{"id": "1601.02377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2016", "title": "Implicit Look-alike Modelling in Display Ads: Transfer Collaborative Filtering to CTR Estimation", "abstract": "user behaviour targeting is essential in online advertising. compared with sponsored ads keyword targeting and contextual group page content targeting, user behaviour targeting builds users'interest profiles via tracking their online behaviour and then delivers the brand ads according across each user's interest, which leads to higher targeting accuracy and thus more improved advertising performance. while current category profiling methods include what keywords and topic numbers or mapping categories propose a descriptive taxonomy. however, to our knowledge, there is no previous work that explicitly investigates the user online visits similarity and incorporates mutual similarity into their ad response prediction. in this work, we propose a general plan which learns the user profiles models on their online browsing behaviour, typically transfers the learned knowledge onto prediction around their ad response. technically, we propose a transfer learning model based on the probabilistic latent factor graphic models, where more users'page response cues resemble generated from their online browsing profiles. the large - scale experiments based on real - world data demonstrate significant improvement of our solution over some strong baselines.", "histories": [["v1", "Mon, 11 Jan 2016 10:12:17 GMT  (857kb)", "http://arxiv.org/abs/1601.02377v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["weinan zhang", "lingxi chen", "jun wang"], "accepted": false, "id": "1601.02377"}, "pdf": {"name": "1601.02377.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Weinan Zhang", "Lingxi Chen", "Jun Wang"], "emails": ["w.zhang@cs.ucl.ac.uk", "lingxi.chen@cs.ucl.ac.uk", "j.wang@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n02 37\n7v 1\n[ cs\n.L G\n] 1\n1 Ja\nn 20"}, {"heading": "1 Introduction", "text": "Targeting technologies have been widely adopted in various online advertising paradigms during the recent decade. According to the Internet advertising revenue report from IAB in 2014 [22], 51% online advertising budget is spent on sponsored search (search keywords targeting) and contextual advertising (page content targeting), while 39% is spent on display advertising (user demographics and behaviour targeting), and the left 10% is spent on other ad formats like classifieds. With the rise of ad exchanges [19] and mobile advertising, user behaviour targeting has now become essential in online advertising.\nCompared with sponsored search or contextual advertising, user behaviour targeting explicitly builds the user profiles and detects their interest segments via tracking their online behaviour, such as browsing history, search keywords and ad clicks etc. Based on user profiles, the advertisers can detect the users with similar interests to the known customers and then deliver the relevant ads to them. Such technology is referred as look-alike modelling [17], which efficiently provides higher targeting accuracy and thus brings more customers to the advertisers [29]. The current user profiling methods include building keyword and\ntopic distributions [1] or clustering users onto a (hierarchical) taxonomy [29]. Normally, these inferred user interest segments are then used as target restriction rules or as features leveraged in predicting users\u2019 ad response [32].\nHowever, the two-stage profiling-and-targeting mechanism is not optimal (despite its advantages of explainability). First, there is no flexible relationship between the inferred tags or categories. Two potentially correlated interest segments are regarded as separated and independent ones. For example, the users who like cars tend to love sports as well, but these two segments are totally separated in the user targeting system. Second, the first stage, i.e., the user interest segments building, is performed independently and with little attention of its latter use of ad response prediction [29,7], which is suboptimal. Third, the effective tag system or taxonomy structure could evolve over time, which makes it much difficult to update them.\nIn this paper, we propose a novel framework to implicitly and jointly learn the users\u2019 profiles on both the general web browsing behaviours and the ad response behaviours. Specifically, (i) Instead of building explicit and fixed tag system or taxonomy, we propose to directly map each user, webpage and ad into a latent space where the shape of the mapping is automatically learned. (ii) The users\u2019 profiles on general browsing and ad response behaviour are jointly learned based on the heterogeneous data from these two scenarios (or tasks). (iii) With a maximum a posteriori framework, the knowledge from the user browsing behaviour similarity can be naturally transferred to their ad response behaviour modelling, which in turn makes an improvement over the prediction of the users\u2019 ad response. For instance, our model could automatically discover that the users with the common behaviour on www.bbc.co.uk/sport will tend to click automobile ads. Due to its implicit nature, we call the proposed model implicit look-alike modelling.\nComprehensive experiments on a real-world large-scale dataset from a commercial display ad platform demonstrate the effectiveness of our proposed model and its superiority over other strong baselines. Additionally, with our model, it is straightforward to analyse the relationship between different features and which features are critical and cost-effective when performing transfer learning."}, {"heading": "2 Related Work", "text": "Ad Response Prediction aims at predicting the probability that a specific user will respond (e.g., click) to an ad in a given context [4,18]. Such context can be either a search keyword [8], webpage content [2], or other kinds of real-time information related to the underlying user [31]. From the modelling perspective, many user response prediction solutions are based on linear models, such as logistic regression [24,14] and Bayesian probit regression [8]. Despite the advantage of high learning efficiency, these linear models suffer from the lack of feature interaction and combination [9]. Thus non-linear models such as tree models [9] and latent vector models [30,20] are proposed to catch the data non-linearity and interactions between features. Recently the authors in [12] proposed to first learn combination features from gradient boosting decision trees (GBDT) and,\nbased on the tree leaves as features, learn a factorisation machine (FM) [23] to build feature interactions to improve ad click prediction performance.\nCollaborative Filtering (CF) on the other hand is a technique for personalised recommendation [26]. Instead of exploring content features, it learns the user or/and item similarity based on their interactions. Besides the user(item)based approaches [25,28], latent factor models, such as probabilistic latent semantic analysis [10], matrix factorisation [13] and factorisation machines [23], are widely used model-based approaches. The key idea of the latent factor models is to learn a low-dimensional vector representation of each user and item to catch the observed user-item interaction patterns. Such latent factors have good generalisation and can be leveraged to predict the users\u2019 preference on unobserved items [13]. In this paper, we explore latent models of collaborative filtering to model user browsing patterns and use them to infer users\u2019 ad click behaviour.\nTransfer Learning deals with the learning problem where the learning data of the target task is expensive to get, or easily outdated, via transferring the knowledge learned from other tasks [21]. It has been proven to work on a variety of problems such as classification [6], regression [16] and collaborative filtering [15]. Different from multi-task learning, where the data from different tasks are assumed to drawn from the same distribution [27], transfer learning methods may allow for arbitrary source and target tasks. In online advertising field, the authors in a recent work [7] proposed a transfer learning scheme based on logistic regression prediction models, where the parameters of ad click prediction model were restricted with a regularisation term from the ones of user web browsing prediction model. In this paper, we consider it as one of the baselines."}, {"heading": "3 Implicit Look-alike Modelling", "text": "In performance-driven online advertising, we commonly have two types of observations about underlying user behaviours: one from their browsing behaviours (the interaction with webpages) and one from their ad responses, e.g., conversions or clicks, towards display ads (the interactions with the ads) [7]. There are two predictions tasks for understanding the users:\n\u2013 Web Browsing Prediction (CF Task). Each user\u2019s online browsing behaviour is logged as a list containing previously visited publishers (domains or URLs). A common task of using the data is to leverage collaborative filtering (CF) [28,23] to infer the user\u2019s profile, which is then used to predict whether the user is interested in visiting any given new publisher. Formally, we denote the dataset for CF as Dc and an observation is denoted as (xc, yc) \u2208 Dc, where xc is a feature vector containing the attributes from the user and the publisher and yc is the binary label indicating whether the user visits the publisher or not. \u2013 Ad Response Prediction (CTR Task). Each user\u2019s online ad feedback behaviour is logged as a list of pairs of ad impression events and their corresponding feedbacks (e.g., click or not). The task is to build a click-through rate (CTR) prediction model [5] to estimate how likely it is that the user will\nclick a specific ad impression in the future. Each ad impression event consists of various information, such as user data (cookie ID, location, time, device, browser, OS etc.), publisher data (domain, URL, ad slot position etc.), and advertiser data (ad creative, creative size, campaign etc.). Mathematically, we denote the ad CTR dataset as Dr and its data instance as (xr, yr), where xr is a feature vector and yr is the binary label indicating whether the user clicks a given ad or not.\nThis paper focuses on the latter task: ad response prediction. We, however, observe that although they are different prediction tasks, the two tasks share a large proportion of users, publishers and their features. We can thus build a user-publisher interest model jointly from the two tasks. Typically we have a large number of observations about user browsing behaviours and we can use the knowledge learned from publisher CF recommendation to help infer display advertising CTR estimation."}, {"heading": "3.1 The Joint Conditional Likelihood", "text": "In our solution, the prediction models on CF task and CTR task are learned jointly. Specifically, we build a joint data discrimination framework. We denote \u0398 as the parameter set of the joint model with prior P (\u0398), and the conditional likelihood of an observed data instance is the probability of predicting the correct binary label given the features P (y|x;\u0398). As such, the conditional likelihood of the two datasets are \u220f\n(xc,yc)\u2208Dc P (y c|xc;\u0398) and\n\u220f\n(xr,yr)\u2208Dr P (y r|xr;\u0398). Max-\nimising a posteriori (MAP) estimation gives\n\u0398\u0302 = max \u0398\nP (\u0398) \u220f\n(xc,yc)\u2208Dc\nP (yc|xc;\u0398) \u220f\n(xr,yr)\u2208Dr\nP (yr|xr;\u0398). (1)\nJust like most solutions on CF recommendation [13,10] and CTR estimation [24,14], in this discriminative framework, \u0398 is only concerned with the mapping from the features to the labels (the conditional probabilities) rather than modelling the prior distribution of features [11].\nThe details of the conditional likelihood P (yc|xc;\u0398), P (yr|xr;\u0398) and the parameter prior P (\u0398) will be discussed in the latter subsections."}, {"heading": "3.2 CF Prediction", "text": "For the CF task, we use a factorisation machine [23] as our prediction model. We further define the features xc \u2261 (xu,xp), where xu \u2261 {xui } is the set of features for a user and xp \u2261 {xpj} is the set of features for a publisher 1. The parameter \u0398 \u2261 (wc0,w c,V c), where wc0 \u2208 R is the global bias term and w c \u2208 RI c+Jc is the weight vector of the Ic-dimensional user features and Jc-dimensional publisher features. Each user feature xui or publisher feature x p j is associated with a Kdimensional latent vector vci or v c j . Thus V c \u2208 R(I c+Jc)\u00d7K .\n1 All the features studied in our work are one-hot encoded binary features.\nWith such setting, the conditional probability for CF in Eq. (1) can be reformulated as:\n\u220f\n(xc,yc)\u2208Dc\nP (yc|xc;\u0398) = \u220f\n(xu,xp,yc)\u2208Dc\nP (yc|xu,xp;wc0,w c,V c). (2)\nLet y\u0302cu,p be the predicted probability of whether user u will be interested in visiting publisher p. With the FM model, the likelihood of observing the label yc given the features (xu,xp) and parameters is\nP (yc|xu,xp;wc0,w c,V c) = (y\u0302cu,p) yc \u00b7 (1\u2212 y\u0302cu,p) (1\u2212yc), (3)\nwhere the prediction y\u0302cu,p is given by an FM with a logistic function:\ny\u0302cu,p = \u03c3 ( wc0 + \u2211\ni\nwcix u i +\n\u2211\nj\nwcjx p j +\n\u2211\ni\n\u2211\nj\n\u3008vci ,v c j\u3009x u i x p j\n)\n, (4)\nwhere \u03c3(x) = 1/(1 + e\u2212x) is the sigmoid function and \u3008\u00b7, \u00b7\u3009 is the inner product of two vectors: \u3008vi,vj\u3009 \u2261 \u2211K\nf=1 vi,f \u00b7 vj,f , which models the interaction between a user feature i and a publisher feature j."}, {"heading": "3.3 CTR Task Prediction Model", "text": "For a data instance (xr, yr) in ad CTR task dataset Dr, its features xr \u2261 (xu,xp,xa) can be divided into three categories: the user features xu (cookie, location, time, device, browser, OS, etc.), the publisher features xp (domain, URL etc.), and the ad features xa (ad slot position, ad creative, creative size, campaign, etc.). Each feature has potential influence to another one in a different category. For example, a mobile phone user might prefer square-sized ads instead of banner ads; users would like to click the ad on the sport websites during the afternoon etc.\nBy the same token as CF prediction, we leverage factorisation machine and the model parameter thus is \u0398 \u2261 (wr0,w r,V r). Specifically, xal is one of the L rdimensional ad features xa, wrl is the corresponding bias weight for the feature, and the feature is also associated with a K-dimensional latent vector vrl . Thus V r \u2208 R(I r+Jr+Lr)\u00d7K . Similar to CF task, the CTR data likelihood is: \u220f\n(xr,yr)\u2208Dr\nP (yr|xr;\u0398) = \u220f\n(xu,xp,xa,yr)\u2208Dr\nP (yr|xu,xp,xa;wr0,w r,V r). (5)\nThen the factorisation machine with logistic activation function \u03c3(\u00b7) is adopted to model the click probability over a specific ad impression:\nP (yr|xu,xp,xa;wr0,w r,V r) = (y\u0302ru,p,a) yr + (1\u2212 y\u0302ru,p,a) (1\u2212yr), (6)\nwhere y\u0302ru,p,a is modelled by interactions among 3-side features\ny\u0302ru,p,a = \u03c3 ( wr0 + \u2211\ni\nwrix u i +\n\u2211\nj\nwrjx p j +\n\u2211\nl\nwrlx a l + (7)\n\u2211\ni\n\u2211\nj\n\u3008vri,v r j\u3009x u i x p j +\n\u2211\ni\n\u2211\nl\n\u3008vri ,v r l\u3009x u i x a l +\n\u2211\nj\n\u2211\nl\n\u3008vrj ,v r l\u3009x p jx a l\n)\n."}, {"heading": "3.4 Dual-Task Bridge", "text": "To model the dependency between the two tasks, the weights of the user features and publisher features in CTR task are assumed to be generated from the counterparts in CF task (as a prior):\nwr \u223c N (wc, \u03c32wdI), (8)\nwhere \u03c32 wd\nis the assumed variance of the Gaussian generation process between each pair of feature weights of CF and CTR tasks and the weight generation is assumed to be independent across features. Similarly, the latent vectors of CTR task are assumed to be generated from the counterparts of CF task:\nvri \u223c N (v c i , \u03c3 2 V d I) (9)\nwhere i is the index of a user or publisher feature; \u03c32 V d is defined similarly. The rational behind the above bridging model is that the users\u2019 interest towards webpage content is relatively general and the displayed ad can be regarded as a special kind of webpage content. One can infer user interests from their browsing behaviours, while their interests on commercial ads can be regarded as a modification or derivative from the learned general interests.\nThe graphic representation for the proposed transferred factorisation machines is depicted in Figure 1. It illustrates the relationship among model parameters and observed data. The left part is for the CF task: xc, wc0, w c and V c work together to infer our CF task target yc, i.e., whether the user would visit a specific publisher or not. The right part illustrates the CTR task. Corresponding to CF task, wr and V r here represent user and publisher features\u2019 weights and latent vectors, while wr,a and V r,a are separately depicted to represent ad features\u2019 weights and latent vectors. All these factors work together to predict CTR task target yr, i.e., whether the user would click the ad or not. On top of that, for each (user or publisher) feature i of the CF task, its weight wci and latent vector vci act as a prior of the counterparts w r i and v r i in CTR task while learning the model. Considering the datasets of the two tasks might be seriously unbalanced, we choose to focus on the averaged log-likelihood of generating each data instance\nfrom the two tasks. In addition, we add a hyperparameter \u03b1 for balancing the task relative importance. As such, the joint conditional likelihood in Eq. (1) is written as\n[\n\u220f\n(xc,yc)\u2208Dc\nP (yc|xc;\u0398) ]\n\u03b1 |Dc|\n\u00b7 [\n\u220f\n(xr,yr)\u2208Dr\nP (yr|xr;\u0398) ]\n1\u2212\u03b1 |Dr|\n(10)\nand its log form is\n\u03b1\n|Dc|\n\u2211\n(xc,yc)\u2208Dc\n[\nyc log y\u0302cu,p + (1\u2212 y c) log(1\u2212 y\u0302cu,p)\n]\n+ 1\u2212 \u03b1\n|Dr|\n\u2211\n(xr,yr)\u2208Dr\n[\nyr log y\u0302ru,p,a + (1 \u2212 y r) log(1\u2212 y\u0302ru,p,a)\n]\n. (11)\nMoreover, from the graphic model, the prior of model parameters can be specified as\nP (\u0398) =P (wc)P (V c)P (wr|wc)P (V r|V c)P (wr,a)P (V r,a) (12)\nlogP (\u0398) = \u2211\ni\nlogN (wci ;\u00b5wc , \u03c3 2 wc) +\n\u2211\ni\nlogN (vci ;\u00b5V c , \u03c3 2 V cI)\n+ \u2211\ni\nlogN (wri ;w c i , \u03c3 2 wd) +\n\u2211\ni\nlogN (vri;v c i , \u03c3 2 V d I) (13)\n+ \u2211\nl\nlogN (wr,al ;\u00b5wr,a , \u03c3 2 wr,a) +\n\u2211\nl\nlogN (vr,al ;\u00b5V r,a , \u03c3 2 V r,aI)."}, {"heading": "3.5 Learning the Model", "text": "Given the detailed implementations of the MAP solution (Eq. (1)) components in Eqs. (11) and (13), for each data instance (x, y), the gradient update of \u0398 is\n\u0398 \u2190 \u0398 + \u03b7 ( \u03b2 \u2202\n\u2202\u0398 logP (y|x;\u0398) +\n\u2202\n\u2202\u0398 logP (\u0398)\n)\n, (14)\nwhere P (y|x;\u0398) is as Eqs. (3) and (6) for (xc, yc) \u2208 Dc and (xr, yr) \u2208 Dr, respectively; \u03b7 is the learning rate; \u03b2 is the instance weight parameter depending on which task the instance belongs to, as given in Eq. (11). The detailed gradient for each specific parameter can be calculated routinely and thus are omitted here due to the page limit."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "Our experiments are conducted based on a real-world dataset provided by Adform, a global digital media advertising technology company based in Copenhagen, Denmark. It consists of two weeks of online display ad logs across different campaigns during March 2015. Specifically, there are 42.1M user domain\nbrowsing events and 154.0K ad display/click events. To fit the data into the joint model, we group useful data features into three categories: user features xu (user cookie, hour, browser, os, user agent and screen size), publisher features xp (domain, url, exchange, ad slot and slot size), ad features xa (advertiser and campaign). Detailed unique value numbers for each attribute are given as below.\nAttribute user cookie hour browser os user agent screen size domain\nUnique number 4,180,170 24 71 37 29,488 118 38,495 Attribute url exchange position size advertiser campaign Unique number 1,100,523 140 3 55 486 2,665\nIn order to perform stable knowledge transfer, we have down-sampled the negative instances to make the ratio of positive over negative instances as 1:5.2"}, {"heading": "4.2 Experiment Protocol", "text": "We conduct a two-stage experiment to verify the effectiveness of our proposed models. First, in a very clean setting, we only focus on user cookie and domain to check whether the knowledge of users\u2019 behaviour on webpage browsing can be transferred to model their behaviour on clicking the ads in these webpages. Second, we start to append various features in the first setting to observe the performance change and check which features lead to better transfer learning. Specifically, we try appending a single side feature into the baseline setting: 1. appending user feature xu, 2. appending publisher feature xp, 3. appending ad feature xa. Finally, all features are added into the model to perform the transfer learning.\nFor each experiment stage, there are three datasets:CF dataset (Dc), CTR dataset (Dr) and Joint dataset (Dc, Dr). Each dataset is split into two parts: the first week data as training data and the second one as test data."}, {"heading": "4.3 Evaluation Metrics", "text": "To evaluate the performance of proposed model, area under the ROC curve (AUC) [8] and root mean square error (RMSE) [13] are adopted as performance metrics. As we focus on ad click prediction performance improvement, we only report the performance of the CTR estimation task."}, {"heading": "4.4 Compared Models", "text": "We implement the following models for experimental comparison.\n\u2013 Base: This baseline model only considers the ad CTR task, without any transfer learning. The parameters are learned by max\u0398 \u220f (xr,yr)\u2208Dr P (y r|xr;\u0398)P (\u0398).\n2 It is common to perform negative down sampling to balance the labels in ad CTR estimation [9]. Calibration methods [3] are then leveraged to eliminate the model bias."}, {"heading": "4.5 Result", "text": "Basic Setting Performance. Figure 2 presents the AUC and RMSE performance of Base, Disjoint and Joint and the improvement of Joint against the hyperparameter \u03b1 in Eq. (11) based on the basic experiment setting. As can be observed clearly, for a large region of \u03b1, i.e., [0.1, 0.7], Joint consistently outperforms the baselines Base and Disjoint on both AUC and RMSE, which demonstrates the effectiveness of our model to transfer knowledge from webpage browsing data to ad click data. Note that when \u03b1 = 0, the CF side model wc does not learn but Joint still outperforms Disjoint and Base. This is due to the different prior of wr and V r in Joint compared with those of Disjoint and Base.\nIn addition, when \u03b1 = 1, i.e., no learning on CTR task, the performance of Joint reasonably gets back to initial guess, i.e., both AUC and RMSE are 0.5.\nTable 1 shows the transfer learning performance comparison between Joint and the state-of-the-art DisjointLR with both models setting optimal hyperparameters. The improvement of Joint over DisjointLR indicates the success of 1) the joint optimisation on the two tasks to perform knowledge transfer and 2) the non-linear factorisation machine relevance model on catching feature interactions.\nAppending Side Information Performance. From the Joint model as in Eq. (11) we see when \u03b1 is large, e.g., 0.8, the larger weight is allocated on the CF task to optimise the joint likelihood. As such, if a large-value \u03b1 leads to the optimal CTR estimation performance, it means the transfer learning takes effect. With such method, we try adding different features into the Joint model to obtain the optimal hyperparameter \u03b1 leading to the highest AUC to check whether a certain feature helps transfer learning. On the contrary, if a low-value or 0 \u03b1 leads to the optimal performance of Joint model when adding a certain feature, it means such feature has no effect of performing transfer learning.\nTable 2 collects the AUC improvement of the Joint model for the conducted experiments. We observe that user browsing hour, ad slot position in the webpage are the most valuable features that help transfer learning, while the user screen size does not bring any transfer value. When adding all these features into Joint model, the optimal \u03b1 is around 0.5 for AUC improvement and 0.6 for RMSE drop (see Figure 3), which means these features along with the basic user, webpage IDs provide an overall positive value of knowledge transfer from webpage browsing behaviour to ad click behaviour."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a transfer learning framework with factorisation machines to build implicit look-alike models on user ad click behaviour prediction\ntask with the knowledge successfully transferred from the rich data of user webpage browsing behaviour. The major novelty of this work lies in the joint training on the two tasks and making knowledge transfer based on the non-linear factorisation machine model to build the user and other feature profiles. Comprehensive experiments on a large-scale real-world dataset demonstrated the effectiveness of our model as well as some insights of detecting which specific features help transfer learning. In the future work, we plan to explore on the user profiling utilisation based on the learned latent vector for each user. We also plan to extend our model to cross-domain recommendation problems."}, {"heading": "Acknowledgement", "text": "We would like to thank Adform for allowing us to use their data in experiments. We would also like to thank Thomas Furmston for his feedback on the paper. Weinan thanks Chinese Scholarship Council for the research support."}], "references": [{"title": "Scalable distributed inference of dynamic user interests for behavioral targeting", "author": ["A. Ahmed", "Y. Low", "M. Aly", "V. Josifovski", "A.J. Smola"], "venue": "KDD", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A semantic approach to contextual advertising", "author": ["A. Broder", "M. Fontoura", "V. Josifovski", "L. Riedel"], "venue": "SIGIR. pp. 559\u2013566. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": "ICML. pp. 161\u2013168. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Modeling delayed feedback in display advertising", "author": ["O. Chapelle"], "venue": "KDD. pp. 1097\u2013 1105. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple and scalable response prediction for display advertising", "author": ["O Chapelle"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Transferring naive bayes classifiers for text classification", "author": ["W. Dai", "G.R. Xue", "Q. Yang", "Y. Yu"], "venue": "AAAI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Scalable hands-free transfer learning for online advertising", "author": ["B. Dalessandro", "D. Chen", "T. Raeder", "C. Perlich", "M. Han Williams", "F. Provost"], "venue": "KDD", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "ICML. pp. 13\u201320", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Practical lessons from predicting clicks on ads at facebook", "author": ["X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "T. Xu", "Y. Shi", "A. Atallah", "R. Herbrich", "S Bowers"], "venue": "ADKDD. pp. 1\u20139. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering via gaussian probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "SIGIR. pp. 259\u2013266. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Machine learning: discriminative and generative, vol", "author": ["T. Jebara"], "venue": "755. Springer Science & Business Media", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "3 idiots approach for display advertising challenge", "author": ["Y.C. Juan", "Y. Zhuang", "W.S. Chin"], "venue": "Internet and Network Economics, pp. 254\u2013265. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer (8), 30\u201337", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating conversion rate in display advertising from past performance data", "author": ["Lee", "K.c.", "B. Orten", "A. Dasdan", "W. Li"], "venue": "KDD. pp. 768\u2013776. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer learning for collaborative filtering via a ratingmatrix generative model", "author": ["B. Li", "Q. Yang", "X. Xue"], "venue": "ICML. pp. 617\u2013624. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Logistic regression with an auxiliary data source", "author": ["X. Liao", "Y. Xue", "L. Carin"], "venue": "ICML. pp. 505\u2013512. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "A feature-pair-based associative classification approach to look-alike modeling for conversion-oriented user-targeting in tail campaigns", "author": ["A. Mangalampalli", "A. Ratnaparkhi", "A.O. Hatch", "A. Bagherjeiran", "R. Parekh", "V. Pudi"], "venue": "WWW. pp. 85\u201386. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "The design of advertising exchanges", "author": ["R.P. McAfee"], "venue": "Review of Industrial Organization 39(3), 169\u2013185", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Ad exchanges: Research issues", "author": ["S. Muthukrishnan"], "venue": "Internet and network economics, pp. 1\u201312. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting response in mobile advertising with hierarchical importance-aware factorization machine", "author": ["R.J. Oentaryo", "E.P. Lim", "D.J.W. Low", "D. Lo", "M. Finegold"], "venue": "WSDM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 22(10), 1345\u20131359", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "ICDM. pp. 995\u20131000. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Predicting clicks: estimating the clickthrough rate for new ads", "author": ["M. Richardson", "E. Dominowska", "R. Ragno"], "venue": "WWW. pp. 521\u2013530. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Riedl"], "venue": "WWW. pp. 285\u2013295. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Collaborative filtering recommender systems", "author": ["J.B. Schafer", "D. Frankowski", "J. Herlocker", "S. Sen"], "venue": "The adaptive web, pp. 291\u2013324. Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "J. Mach. Learn. Res. 10, 1633\u20131685", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Unifying user-based and item-based collaborative filtering approaches by similarity fusion", "author": ["J. Wang", "A.P. De Vries", "M.J. Reinders"], "venue": "SIGIR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "How much can behavioral targeting help online advertising? In: WWW", "author": ["J. Yan", "N. Liu", "G. Wang", "W. Zhang", "Y. Jiang", "Z. Chen"], "venue": "pp. 261\u2013270. ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Coupled group lasso for web-scale ctr prediction in display advertising", "author": ["L. Yan", "W.J. Li", "G.R. Xue", "D. Han"], "venue": "ICML. pp. 802\u2013810", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time bidding for online advertising: measurement and analysis", "author": ["S. Yuan", "J. Wang", "X. Zhao"], "venue": "ADKDD. p. 3. ACM", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time bidding benchmarking with ipinyou dataset", "author": ["W. Zhang", "S. Yuan", "J. Wang"], "venue": "arXiv preprint arXiv:1407.7073", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "With the rise of ad exchanges [19] and mobile advertising, user behaviour targeting has now become essential in online advertising.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Such technology is referred as look-alike modelling [17], which efficiently provides higher targeting accuracy and thus brings more customers to the advertisers [29].", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Such technology is referred as look-alike modelling [17], which efficiently provides higher targeting accuracy and thus brings more customers to the advertisers [29].", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "topic distributions [1] or clustering users onto a (hierarchical) taxonomy [29].", "startOffset": 20, "endOffset": 23}, {"referenceID": 27, "context": "topic distributions [1] or clustering users onto a (hierarchical) taxonomy [29].", "startOffset": 75, "endOffset": 79}, {"referenceID": 30, "context": "Normally, these inferred user interest segments are then used as target restriction rules or as features leveraged in predicting users\u2019 ad response [32].", "startOffset": 148, "endOffset": 152}, {"referenceID": 27, "context": ", the user interest segments building, is performed independently and with little attention of its latter use of ad response prediction [29,7], which is suboptimal.", "startOffset": 136, "endOffset": 142}, {"referenceID": 6, "context": ", the user interest segments building, is performed independently and with little attention of its latter use of ad response prediction [29,7], which is suboptimal.", "startOffset": 136, "endOffset": 142}, {"referenceID": 3, "context": ", click) to an ad in a given context [4,18].", "startOffset": 37, "endOffset": 43}, {"referenceID": 17, "context": ", click) to an ad in a given context [4,18].", "startOffset": 37, "endOffset": 43}, {"referenceID": 7, "context": "Such context can be either a search keyword [8], webpage content [2], or other kinds of real-time information related to the underlying user [31].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Such context can be either a search keyword [8], webpage content [2], or other kinds of real-time information related to the underlying user [31].", "startOffset": 65, "endOffset": 68}, {"referenceID": 29, "context": "Such context can be either a search keyword [8], webpage content [2], or other kinds of real-time information related to the underlying user [31].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "From the modelling perspective, many user response prediction solutions are based on linear models, such as logistic regression [24,14] and Bayesian probit regression [8].", "startOffset": 128, "endOffset": 135}, {"referenceID": 13, "context": "From the modelling perspective, many user response prediction solutions are based on linear models, such as logistic regression [24,14] and Bayesian probit regression [8].", "startOffset": 128, "endOffset": 135}, {"referenceID": 7, "context": "From the modelling perspective, many user response prediction solutions are based on linear models, such as logistic regression [24,14] and Bayesian probit regression [8].", "startOffset": 167, "endOffset": 170}, {"referenceID": 8, "context": "Despite the advantage of high learning efficiency, these linear models suffer from the lack of feature interaction and combination [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "Thus non-linear models such as tree models [9] and latent vector models [30,20] are proposed to catch the data non-linearity and interactions between features.", "startOffset": 43, "endOffset": 46}, {"referenceID": 28, "context": "Thus non-linear models such as tree models [9] and latent vector models [30,20] are proposed to catch the data non-linearity and interactions between features.", "startOffset": 72, "endOffset": 79}, {"referenceID": 19, "context": "Thus non-linear models such as tree models [9] and latent vector models [30,20] are proposed to catch the data non-linearity and interactions between features.", "startOffset": 72, "endOffset": 79}, {"referenceID": 11, "context": "Recently the authors in [12] proposed to first learn combination features from gradient boosting decision trees (GBDT) and,", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "based on the tree leaves as features, learn a factorisation machine (FM) [23] to build feature interactions to improve ad click prediction performance.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "Collaborative Filtering (CF) on the other hand is a technique for personalised recommendation [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Besides the user(item)based approaches [25,28], latent factor models, such as probabilistic latent semantic analysis [10], matrix factorisation [13] and factorisation machines [23], are widely used model-based approaches.", "startOffset": 39, "endOffset": 46}, {"referenceID": 26, "context": "Besides the user(item)based approaches [25,28], latent factor models, such as probabilistic latent semantic analysis [10], matrix factorisation [13] and factorisation machines [23], are widely used model-based approaches.", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "Besides the user(item)based approaches [25,28], latent factor models, such as probabilistic latent semantic analysis [10], matrix factorisation [13] and factorisation machines [23], are widely used model-based approaches.", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "Besides the user(item)based approaches [25,28], latent factor models, such as probabilistic latent semantic analysis [10], matrix factorisation [13] and factorisation machines [23], are widely used model-based approaches.", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "Besides the user(item)based approaches [25,28], latent factor models, such as probabilistic latent semantic analysis [10], matrix factorisation [13] and factorisation machines [23], are widely used model-based approaches.", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "Such latent factors have good generalisation and can be leveraged to predict the users\u2019 preference on unobserved items [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Transfer Learning deals with the learning problem where the learning data of the target task is expensive to get, or easily outdated, via transferring the knowledge learned from other tasks [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 5, "context": "It has been proven to work on a variety of problems such as classification [6], regression [16] and collaborative filtering [15].", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "It has been proven to work on a variety of problems such as classification [6], regression [16] and collaborative filtering [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "It has been proven to work on a variety of problems such as classification [6], regression [16] and collaborative filtering [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 25, "context": "Different from multi-task learning, where the data from different tasks are assumed to drawn from the same distribution [27], transfer learning methods may allow for arbitrary source and target tasks.", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "In online advertising field, the authors in a recent work [7] proposed a transfer learning scheme based on logistic regression prediction models, where the parameters of ad click prediction model were restricted with a regularisation term from the ones of user web browsing prediction model.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": ", conversions or clicks, towards display ads (the interactions with the ads) [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 26, "context": "A common task of using the data is to leverage collaborative filtering (CF) [28,23] to infer the user\u2019s profile, which is then used to predict whether the user is interested in visiting any given new publisher.", "startOffset": 76, "endOffset": 83}, {"referenceID": 21, "context": "A common task of using the data is to leverage collaborative filtering (CF) [28,23] to infer the user\u2019s profile, which is then used to predict whether the user is interested in visiting any given new publisher.", "startOffset": 76, "endOffset": 83}, {"referenceID": 4, "context": "The task is to build a click-through rate (CTR) prediction model [5] to estimate how likely it is that the user will", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "Just like most solutions on CF recommendation [13,10] and CTR estimation [24,14], in this discriminative framework, \u0398 is only concerned with the mapping from the features to the labels (the conditional probabilities) rather than modelling the prior distribution of features [11].", "startOffset": 46, "endOffset": 53}, {"referenceID": 9, "context": "Just like most solutions on CF recommendation [13,10] and CTR estimation [24,14], in this discriminative framework, \u0398 is only concerned with the mapping from the features to the labels (the conditional probabilities) rather than modelling the prior distribution of features [11].", "startOffset": 46, "endOffset": 53}, {"referenceID": 22, "context": "Just like most solutions on CF recommendation [13,10] and CTR estimation [24,14], in this discriminative framework, \u0398 is only concerned with the mapping from the features to the labels (the conditional probabilities) rather than modelling the prior distribution of features [11].", "startOffset": 73, "endOffset": 80}, {"referenceID": 13, "context": "Just like most solutions on CF recommendation [13,10] and CTR estimation [24,14], in this discriminative framework, \u0398 is only concerned with the mapping from the features to the labels (the conditional probabilities) rather than modelling the prior distribution of features [11].", "startOffset": 73, "endOffset": 80}, {"referenceID": 10, "context": "Just like most solutions on CF recommendation [13,10] and CTR estimation [24,14], in this discriminative framework, \u0398 is only concerned with the mapping from the features to the labels (the conditional probabilities) rather than modelling the prior distribution of features [11].", "startOffset": 274, "endOffset": 278}, {"referenceID": 21, "context": "For the CF task, we use a factorisation machine [23] as our prediction model.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "To evaluate the performance of proposed model, area under the ROC curve (AUC) [8] and root mean square error (RMSE) [13] are adopted as performance metrics.", "startOffset": 78, "endOffset": 81}, {"referenceID": 12, "context": "To evaluate the performance of proposed model, area under the ROC curve (AUC) [8] and root mean square error (RMSE) [13] are adopted as performance metrics.", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "2 It is common to perform negative down sampling to balance the labels in ad CTR estimation [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "Calibration methods [3] are then leveraged to eliminate the model bias.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "\u2013 DisjointLR: The transfer learning model proposed in [7] is considered as stateof-the-art transfer learning methods in display advertising.", "startOffset": 54, "endOffset": 57}], "year": 2016, "abstractText": "User behaviour targeting is essential in online advertising. Compared with sponsored search keyword targeting and contextual advertising page content targeting, user behaviour targeting builds users\u2019 interest profiles via tracking their online behaviour and then delivers the relevant ads according to each user\u2019s interest, which leads to higher targeting accuracy and thus more improved advertising performance. The current user profiling methods include building keywords and topic tags or mapping users onto a hierarchical taxonomy. However, to our knowledge, there is no previous work that explicitly investigates the user online visits similarity and incorporates such similarity into their ad response prediction. In this work, we propose a general framework which learns the user profiles based on their online browsing behaviour, and transfers the learned knowledge onto prediction of their ad response. Technically, we propose a transfer learning model based on the probabilistic latent factor graphic models, where the users\u2019 ad response profiles are generated from their online browsing profiles. The large-scale experiments based on real-world data demonstrate significant improvement of our solution over some strong baselines.", "creator": "LaTeX with hyperref package"}}}