{"id": "1601.02705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding", "abstract": "there is a large variety generating objects and appliances in human environments, such as switches, coffee dispensers, fluid extractors, and so on. it is challenging for a roboticist to program a robot incorporating each of these object types and for each of their instantiations. in machine work, we present a novel approach to capturing planning based into the idea that many household objects share similarly - shaped object parts. we treat the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point - cloud, trap language, and manipulation trajectory data into a shared embedding space using dense deep neural network. in order to learn semantically meaningful spaces throughout our network, we introduce a method for pre - training its local counterpart for static feature creation and a method based fine - tuning this embedding space using a loss - based margin. in order to collect a large number of manipulation demonstrations for different targets, we develop a simpler crowd - recruited simulation through robobarista. we test our model that our dataset consisting of 116 objects and appliances with 249 parts along with 250 procedure instructions, for which there are 67 crowd - sourced manipulation demonstrations. we further show that our robot with our model can even assemble a cup of a latte with grease he has never seen before.", "histories": [["v1", "Tue, 12 Jan 2016 00:56:30 GMT  (7318kb,D)", "http://arxiv.org/abs/1601.02705v1", "Journal Version"]], "COMMENTS": "Journal Version", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["jaeyong sung", "seok hyun jin", "ian lenz", "ashutosh saxena"], "accepted": false, "id": "1601.02705"}, "pdf": {"name": "1601.02705.pdf", "metadata": {"source": "CRF", "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding", "authors": ["Jaeyong Sung", "Seok Hyun Jin", "Ian Lenz", "Ashutosh Saxena"], "emails": ["@cs.cornell.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nConsider the espresso machine in Figure 1 \u2014 even without having seen this machine before, a person can prepare a cup of latte by visually observing the machine and reading an instruction manual. This is possible because humans have vast prior experience with manipulating differently-shaped objects that share common parts such as \u2018handles\u2019 and \u2018knobs.\u2019 In this work, our goal is to give robots the same capabilites \u2013 specifically, to enable robots to generalize their manipulation ability to novel objects and tasks (e.g. toaster, sink, water fountain, toilet, soda dispenser, etc.). We build an algorithm that uses a large knowledge base of manipulation demonstrations to infer an appropriate manipulation trajectory for a given pair of point-cloud and natural language instructions.\nIf the robot\u2019s sole task is to manipulate one specific espresso machine or just a few types of \u2018handles\u2019, a roboticist could manually program the exact sequence to be executed. However, human environments contain a huge variety of objects, which makes this approach un-scalable and infeasible. Classification of objects or object parts (e.g. \u2018handle\u2019) alone does not provide enough information for robots to actually manipulate them, since semantically-similar objects\nAuthors are with Department of Computer Science, Cornell University. Email: {jysung,sj372,ianlenz,asaxena} @cs.cornell.edu Jaeyong Sung is also a visiting student researcher at Stanford University. Ashutosh Saxena is also with Brain of Things, Inc.\n1Parts of this work were presented at ISRR 2015 (Sung et al. [61])\nor parts might be operated completely differently \u2013 consider, for example, manipulating the \u2018handle\u2019 of a urinal, as opposed to a door \u2018handle\u2019. Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.\nThe key idea of our work is that objects designed for use by humans share many similarly-operated object parts such as \u2018handles\u2019, \u2018levers\u2019, \u2018triggers\u2019, and \u2018buttons\u2019; thus, manipulation motions can be transferred even between completely different objects if we represent these motions with respect to these parts. For example, even if the robot has never seen an espresso machine before, it should be able to manipulate it if it has previously seen similarly-operated parts of other objects such as a urinal, soda dispenser, or restroom sink, as illustrated in Figure 2. Object parts that are operated in similar fashion may not necessarily carry the same part name (e.g. \u2018handle\u2019) but should have some similarity in their shapes that allows motions to be transferred between completely different objects.\nGoing beyond manipulation based on simple semantic classes is a significant challenge which we address in this work. Instead, we must also make use of visual information\nar X\niv :1\n60 1.\n02 70\n5v 1\n[ cs\n.R O\n] 1\n2 Ja\nn 20\n16\n(point-clouds), and a natural language instruction telling the robot what to do since many possible affordances can exist for the same part. Designing useful features for either of these modalities alone is already difficult, and designing features which combine the two for manipulation purposes is extremely challenging.\nObtaining a good common representation between different modalities is challenging for two main reasons. First, each modality might intrinsically have very different statistical properties \u2013 for example, most trajectory representations are inherently dense, while a bag-of-words representation of language is by nature sparse. This makes it challenging to apply algorithms designed for unimodal data, as one modality might overpower the others. Second, even with expert knowledge, it is extremely challenging to design joint features between such disparate modalities. Humans are able to map similar concepts from different sensory system to the same concept using common representation between different modalities [15]. For example, we are able to correlate the appearance with feel of a banana, or a language instruction with a real-world action. This ability to fuse information from different input modalities and map them to actions is extremely useful to a household robot.\nIn this work, we use deep neural networks to learn a shared embedding between the combination of object parts in the environment (point-cloud) and natural language instructions, and manipulation trajectories (Fig. 3). This means that all three modalities are projected to the same feature space. We introduce an algorithm that learns to pull semantically similar environment/language pairs and their corresponding trajectories to the same regions, and push environment/language pairs away from irrelevant trajectories based on how irrelevant these trajectories are. Our algorithm allows for efficient inference because, given a new instruction and point-cloud, we only need to find the nearest trajectory to the projection of this pair in the learned embedding space, which can be done using fast nearest-neighbor algorithms [45].\nIn the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57]. In contrast to these existing methods, here we present a new pretraining algorithm for initializing networks to be used for joint embedding of different modalities.\nSuch deep learning algorithms require a large dataset for training. However, collecting a large enough dataset of expert demonstrations on a large number of objects is very expensive as it requires joint physical presence of the robot, an expert, and the object to be manipulated. In this work, we show that we can crowd-source the collection of manipulation demonstrations to the public over the web through our Robobarista platform. Since crowd-sourced demonstrations might be noisy and sub-optimal, we present a new learning algorithm which handles this noise. With our noise handling algorithm, our model trained with crowdsourced demonstrations outperforms the model trained with expert demonstrations, even with the significant amount of\nnoise in crowd-sourced manipulation demonstrations. Furthermore, in contrast to previous approaches based on learning from demonstration (LfD) that learn a mapping from a state to an action [4], our work complements LfD as we focus on the entire manipulation motion, as opposed to a sequential state-action mapping.\nOur Robobarista web-based crowdsourcing platform (http://robobarista.cs.cornell.edu) allowed us to collect a large dataset of 116 objects with 250 natural language instructions for which there are 1225 crowd-sourced manipulation trajectories from 71 non-expert users, which we use to validate our methods. We also present experiments on our robot using our approach. In summary, the key contributions of this work are: \u2022 We present a novel approach to manipulation planning\nvia part-based transfer between different objects that allows manipulation of novel objects. \u2022 We introduce a new deep learning model that handles three modalities with noisy labels from crowd-sourcing. \u2022 We introduce a new algorithm, which learns an embedding space while enforcing a varying and loss-based margin, along with a new unsupervised pre-training method which outperforms standard pre-training algorithms [20]. \u2022 We develop an online platform which allows the incorporation of crowd-sourcing to manipulation planning and introduces a large-scale manipulation dataset. \u2022 We evaluate our algorithms on this dataset, showing significant improvement over other state-of-the-art methods."}, {"heading": "II. RELATED WORK", "text": "Improving robotic perception and teaching manipulation strategies to robots has been a major research area in recent years. In this section, we describe related work in various aspects of learning to manipulate novel objects. Scene Understanding. In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35]. Similar to our idea of using part-based transfers, the deformable part model [19] was effective in object detection. However, classification of objects, object parts, or human activities alone does not provide enough information for a robot to reliably plan manipulation. Even a simple category such as \u2018kitchen sinks\u2019 has a huge amount of variation in how different instances are manipulated \u2013 for example, handles and knobs must be manipulated differently, and different orientations and positions of these parts require very different strategies such as pulling the handle upwards, pushing upwards, pushing sideways, and so on. On the other hand, direct perception approaches [18, 34] skip the intermediate object labels and directly perceive affordances based on the shape of the object. These works focus on detecting the part known to afford certain actions, such as \u2018pour,\u2019 given the object, while we focus on predicting the correct motion given the object part.\nManipulation Strategy. Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14]. Others [60, 43] focus on sequencing manipulation tasks assuming perfect manipulation primitives such as grasp and pour are available. Instead, here, we use learning to generalize to manipulating novel objects never seen before by the robot, without relying on preprogrammed motion primitives.\nFor the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm. Many objects that humans operate daily have small parts such as \u2018knobs\u2019, which leads to significant occlusion as manipulation is demonstrated. Another approach using part-based transfer between objects has been shown to be successful for grasping [11, 13]. We extend this approach and introduce a deep learning model that enables part-based transfer of trajectories by automatically learning relevant features. Our focus is on the generalization of manipulation trajectory via partbased transfer using point-clouds without knowing objects a priori and without assuming any of the sub-steps (\u2018approach\u2019, \u2018grasping\u2019, and \u2018manipulation\u2019).\nA few recent works use deep learning approaches for robotic manipulation. Levine et al. [38] use a Gaussian mixture model to learn system dynamics, then use these to learn a manipulation policy using a deep network. Lenz et al. [37] use a deep network to learn system dynamics for realtime model-predictive control. Both these works focus on learning low-level controllers, whereas here we learn highlevel manipulation trajectories.\nLearning from Demonstration. Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4]. Although LfD allows end users to demonstrate a manipulation task by simply taking control of the robot\u2019s arms, it focuses on learning individual actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48]. We believe that learning a single model for an action like \u2018turning on\u2019 is impossible because human environments have so many variations.\nUnlike learning a model from demonstration, instancebased learning [2, 17] replicates one of the demonstrations. Similarly, we directly transfer one of the demonstrations, but focus on generalizing manipulation planning to completely new objects, enabling robots to manipulate objects they have never seen before.\nMetric Embedding. Several works in machine learning make use of the power of shared embedding spaces. LMNN [72] learns a max-margin Mahalanobis distance for a unimodal input feature space. Weston et al. [73] learn linear mappings from image and language features to a common embedding space for automatic image annotation. Moore\net al. [44] learn to map songs and natural language tags to a shared embedding space. However, these approaches learn only a shallow, linear mapping from input features, whereas here we learn a deep non-linear mapping which is less sensitive to input representations.\nDeep Learning. In recent years, deep learning algorithms have enjoyed huge successes, particularly in the domains of of vision and natural language processing (e.g. [33, 54]). In robotics, deep learning has previously been successfully used for detecting grasps for novel objects in multi-channel RGB-D images [36] and for classifying terrain from longrange vision [21].\nNgiam et al. [47] use deep learning to learn features incorporating both video and audio modalities. Sohn et al. [56] propose a new generative learning algorithm for multimodal data which improves robustness to missing modalities at inference time. In these works, a single network takes all modalities as inputs, whereas here we perform joint embedding of multiple modalities using multiple networks.\nSeveral previous works use deep networks for joint embedding between different feature spaces. Mikolov et al. [41] map different languages to a joint feature space for translation. Srivastava and Salakhutdinov [57] map images and natural language \u201ctags\u201d to the same space for automatic annotation and retrieval. While these works use conventional pre-training algorithms, here we present a new pre-training approach for learning embedding spaces and show that it outperforms these existing methods (Sec. VIII-C.) Our algorithm trains each layer to map similar cases to similar areas of its feature space, as opposed to other methods which either perform variational learning [22] or train for reconstruction [20].\nHu et al. [24] also use a deep network for metric learning for the task of face verification. Similar to LMNN [72], Hu et al. [24] enforces a constant margin between distances among inter-class objects and among intra-class objects. In Sec. VIII-C, we show that our approach, which uses a loss-dependent variable margin, produces better results for our problem. Our work builds on deep neural network to embed three different modalities of point-cloud, language, and trajectory into shared embedding space while handling lots of label-noise originating from crowd-sourcing.\nCrowd-sourcing. Many approaches to teaching robots manipulation and other skills have relied on demonstrations by skilled experts [4, 1]. Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al. [17] employs a similar approach towards crowd-sourcing but collects multiple instances of similar table-top manipulation with same object. Others also build web-based platform for crowdsourcing manipulation [65, 66]. However, these approaches either depend on the presence of an expert (due to required special software), or require a real robot at a remote location. Our Robobarista platform borrows some components of work from Alexander et al. [3], but works on any standard web browser with OpenGL support and incorporates real pointclouds of various scenes."}, {"heading": "III. OUR APPROACH", "text": "Our goal is to build an algorithm that allows a robot to infer a manipulation trajectory when it is introduced to a new object or appliance and its natural language instruction manual. The intuition for our approach is that many differentlyshaped objects share similarly-operated object parts; thus, the manipulation trajectory of an object can be transferred to a completely different object if they share similarly-operated parts.\nFor example, the motion required to operate the handle of the espresso machine in Figure 2 is almost identical to the motion required to flush a urinal with a handle. By identifying and transferring trajectories from prior experience with parts of other objects, robots can manipulate even objects they have never seen before.\nWe first formulate this problem as a structured prediction problem as shown in Figure 2. Given a point-cloud for each part of an espresso machine and a natural language instruction such as \u2018Push down on the handle to add hot water\u2019, our algorithm outputs a trajectory which executes the task, using a pool of prior motion experience.\nThis is a challenging problem because the object is entirely new to the robot, and because it must jointly consider the point-cloud, natural language instruction, and each potential trajectory. Moreover, manually designing useful features from these three modalities is extremely challenging.\nWe introduce a deep multimodal embedding approach that learns a shared, semantically meaningful embedding space between these modalities, while dealing with a noise in crowd-sourced demonstration data. Then, we introduce our Robobarista crowd-sourcing platform, which allows us to easily scale the collection of manipulation demonstrations to non-experts on the web."}, {"heading": "A. Problem Formulation", "text": "Our goal is to learn a function f that maps a given pair of point-cloud p \u2208 P of an object part and a natural language\ninstruction l \u2208 L to a trajectory \u03c4 \u2208 T that can manipulate the object part as described by free-form natural language l:\nf : P \u00d7 L \u2192 T (1)\nFor instance, given the handle of the espresso machine in Figure 2 and an natural language instruction \u2018Push down on the handle to add hot water\u2019, the algorithm should output a manipulation trajectory that will correctly accomplish the task on the object part according to the instruction. Point-cloud Representation. Each instance of a point-cloud p \u2208 P is represented as a set of n points in three-dimensional Euclidean space where each point (x, y, z) is represented with its RGB color (r, g, b):\np = {p(i)}ni=1 = {(x, y, z, r, g, b)(i)} n i=1\nThe size of this set varies for each instance. These points are often obtained by stitching together a sequence of sensor data from an RGBD sensor [26]. Trajectory Representation. Each trajectory \u03c4 \u2208 T is represented as a sequence of m waypoints, where each waypoint consists of gripper status g, translation (tx, ty, tz), and rotation (rx, ry, rz, rw) with respect to the origin:\n\u03c4 = {\u03c4 (i)}mi=1 = {(g, tx, ty, tz, rx, ry, rz, rw)(i)} m\ni=1\nwhere g \u2208 {\u201copen\u201d, \u201cclosed\u201d, \u201cholding\u201d}. g depends on the type of the end-effector, which we have assumed to be a twofingered parallel-plate gripper like that of PR2 or Baxter. The rotation is represented as quaternions (rx, ry, rz, rw) instead of the more compact Euler angles to prevent problems such as gimbal lock. Smooth Trajectory. To acquire a smooth trajectory from a waypoint-based trajectory \u03c4 , we interpolate intermediate waypoints. Translation is linearly interpolated and the quaternion is interpolated using spherical linear interpolation (Slerp) [53]."}, {"heading": "B. Data Pre-processing", "text": "Each of the point-cloud, language, and trajectory (p, l, \u03c4) can have any length. Thus, we fit raw data from each modality into a fixed-length vector.\nWe represent point-cloud p of any arbitrary length as an occupancy grid where each cell indicates whether any point lives in the space it represents. Because point-cloud p consists of only the part of an object which is limited in size, we can represent p using two occupancy grid-like structures of size 10 \u00d7 10 \u00d7 10 voxels with different scales: one with each voxel spanning a cube of 1\u00d7 1\u00d7 1(cm) and the other with each cell representing 2.5\u00d7 2.5\u00d7 2.5(cm).\nEach language instruction is represented as a fixed-size bag-of-words representation with stop words removed. Finally, for each trajectory \u03c4 \u2208 T , we first compute its smooth interpolated trajectory \u03c4s \u2208 Ts (Sec. III-A), and then normalize all trajectories Ts to the same length while preserving the sequence of gripper states such as \u2018opening\u2019, \u2018closing\u2019, and \u2018holding\u2019."}, {"heading": "C. Direct manipulation trajectory transfer", "text": "Even if we have a trajectory to transfer, a conceptually transferable trajectory is not necessarily directly compatible if it is represented with respect to an inconsistent reference point.\nTo make a trajectory compatible with a new situation without modifying the trajectory, we need a representation method for trajectories, based on point-cloud information, that allows a direct transfer of a trajectory without any modification. Challenges. Making a trajectory compatible when transferred to a different object or to a different instance of the same object without modification can be challenging depending on the representation of trajectories and the variations in the location of the object, given in point-clouds.\nMany approaches which control high degree of freedom arms such as those of PR2 or Baxter use configurationspace trajectories, which store a time-parameterized series of joint angles [64]. While such approaches allow for direct control of joint angles during control, they require costly recomputation for even a small change in an object\u2019s position or orientation.\nOne approach that allows execution without modification is representing trajectories with respect to the object by aligning via point-cloud registration (e.g. [17]). However, a large object such as a stove might have many parts (e.g. knobs and handles) whose positions might vary between different stoves. Thus, object-aligned manipulation of these parts would not be robust to different stoves, and in general would impede transfer between different instances of the same object.\nLastly, it is even more challenging if two objects require similar trajectories, but have slightly different shapes. And this is made more difficult by limitations of the point-cloud data. As shown in left of Fig. 2, the point-cloud data, even when stitched from multiple angles, are very noisy compared to the RGB images.\nOur Solution. Transferred trajectories become compatible across different objects when trajectories are represented 1) in the task space rather than the configuration space, and 2) relative to the object part in question (aligned based on its principal axis), rather than the object as a whole.\nTrajectories can be represented in the task space by recording only the position and orientation of the end-effector. By doing so, we can focus on the actual interaction between the robot and the environment rather than the movement of the arm. It is very rare that the arm configuration affects the completion of the task as long as there is no collision. With the trajectory represented as a sequence of gripper position and orientation, the robot can find its arm configuration that is collision free with the environment using inverse kinematics.\nHowever, representing the trajectory in task space is not enough to make transfers compatible. The trajectory must also be represented in a common coordinate frame regardless of the object\u2019s orientation and shape.\nThus, we align the negative z-axis along gravity and align the x-axis along the principal axis of the object part using PCA [23]. With this representation, even when the object part\u2019s position and orientation changes, the trajectory does not need to change. The underlying assumption is that similarly operated object parts share similar shapes leading to a similar direction in their principal axes."}, {"heading": "IV. DEEP MULTIMODAL EMBEDDING", "text": "In this work, we use deep learning to find the most appropriate trajectory for a given point-cloud and natural language instruction. This is much more challenging than the uni-modal binary or multi-class classification/regression problems (e.g. image recognition [33]) to which deep learning has mostly been applied [6]. We could simply convert our problem into a binary classification problem, i.e. \u201cdoes\nthis trajectory match this point-cloud/language pair?\u201d Then, we can use a multimodal feed-forward deep network [47] to solve this problem [61].\nHowever, this approach has several drawbacks. First, it requires evaluating the network over every combination of potential candidate manipulation trajectory and the given point-cloud/language pair, which is computationally expensive. Second, this method does nothing to handle noisy labels, a significant problem when dealing with crowdsourced data as we do here. Finally, while this method is capable of producing reasonable results for our problem, we show in Section VIII-C that a more principled approach to our problem is able to improve over it.\nInstead, in this work, we present a new deep architecture and algorithm which is a better fit for this problem, directly addressing the challenges inherent in multimodal data and the noisy labels obtained from crowdsourcing. We learn a joint embedding of point-cloud, language, and trajectory data into the same low dimensional space. We learn nonlinear embeddings using a deep learning approach which maps raw data from these three different modalities to a joint embedding space.\nWe then use this space to find known trajectories which are good matches for new combinations of object parts and instructions. Compared to previous work that exhaustively runs a full network over all these combinations [61], our approach allows us to pre-embed all candidate trajectories into this common feature space. Thus, the most appropriate trajectory can be identified by embedding only a new pointcloud/language pair and then finding its nearest neighbor.\nIn our joint feature space, proximity between two mapped points should reflect how relevant two data-points are to each other, even if they are from completely different modalities. We train our network to bring demonstrations that would manipulate a given object according to some language instruction closer to the mapped point for that object/instruction pair, and to push away demonstrations that would not correctly manipulate the object. Trajectories which have no semantic relevance to the object are pushed much further away than trajectories that have some relevance, even if the latter would not manipulate the object according to the instruction.\nPrior to learning a full joint embedding of all three modalities, we pre-train embeddings of subsets of the modalities to learn semantically meaningful embeddings for these modalities, leading to improved performance as shown in Section VIII-C.\nTo solve this problem of learning to manipulate novel objects and appliance as defined in equation (1), we learn two different mapping functions that map to a common space\u2014one from a point-cloud/language pair and the other from a trajectory. More formally, we want to learn \u03a6P,L(p, l) and \u03a6T (\u03c4) which map to a joint feature space RM :\n\u03a6P,L(p, l) : (P,L)\u2192 RM\n\u03a6T (\u03c4) : T \u2192 RM\nHere, we represent these mappings with a deep neural network, as shown in Figure 3.\nThe first, \u03a6P,L, which maps point-clouds and trajectories, is defined as a combination of two mappings. The first of these maps to a joint point-cloud/language space RN2,pl \u2014 \u03a6P(p) : P \u2192 RN2,pl and \u03a6L(l) : L \u2192 RN2,pl . Once each is mapped to RN2,pl , this space is then mapped to the joint space shared with trajectory information: \u03a6P,L(p, l) : ((P,L)\u2192 RN2,pl)\u2192 RM ."}, {"heading": "A. Model", "text": "We use two separate multi-layer deep neural networks, one for \u03a6P,L(p, l) and one for \u03a6T (\u03c4). Take Np as the size of point-cloud input p, Nl as similar for natural language input l, N1,p and N1,l as the number of hidden units in the first hidden layers projected from point-cloud and natural language features, respectively, and N2,pl as the number of hidden units in the combined point-cloud/language layer. With W \u2019s as network weights, which are the learned parameters of our system, and a(\u00b7) as a rectified linear unit (ReLU) activation function [78], our model for projecting from point-cloud and language features to the shared embedding h3 is as follows:\nh1,pi = a (\u2211Np j=0W 1,p i,j pj ) h1,li = a (\u2211Nl j=0W 1,l i,j lj\n) h2,pli = a (\u2211N1,p j=0 W 2,p i,j h 1,p j + \u2211N1,l j=0 W 2,l i,j h 1,l j\n) h3i = a (\u2211N2,pl j=0 W 3,pl i,j h 2,pl j\n) The model for projecting from trajectory input \u03c4 is similar, except it takes input only from a single modality.\nB. Inference.\nOnce all mappings are learned, we solve the original problem from equation (1) by choosing, from a library of prior trajectories, the trajectory that gives the highest similarity (closest in distance) to the given point-cloud p and language l in our joint embedding space RM . As in previous work [73], similarity is defined as sim(a, b) = a \u00b7 b, and the trajectory that maximizes the magnitude of similarity is selected:\nargmax \u03c4\u2208T sim(\u03a6P,L(p, l),\u03a6T (\u03c4))\nThe previous approach to this problem [61] required projecting the combination of the current point-cloud and natural language instruction with every trajectory in the training set through the network during inference. Here, we pre-compute the representations of all training trajectories in h3, and need only project the new point-cloud/language pair to h3 and find its nearest-neighbor trajectory in this embedding space. As shown in Section VIII-C, this significantly improves both the runtime and accuracy of our approach and makes it much more scalable to larger training datasets like those collected with crowdsourcing platforms."}, {"heading": "V. LEARNING JOINT POINT-CLOUD/LANGUAGE/TRAJECTORY MODEL", "text": "The main challenge of our work is to learn a model which maps three disparate modalities \u2013 point-clouds, natural language, and trajectories \u2013 to a single semantically meaningful space. We introduce a method that learns a common point-cloud/language/trajectory space such that all trajectories relevant to a given task (point-cloud/language combination) should have higher similarity to the projection of that task than task-irrelevant trajectories. Among these irrelevant trajectories, some might be less relevant than others, and thus should be pushed further away.\nFor example, given a door knob that needs to be grasped normal to the door surface and an instruction to rotate it clockwise, a trajectory that correctly approaches the door knob but rotates counter-clockwise should have higher similarity to the task than one which approaches the knob from a completely incorrect angle and does not execute any rotation.\nFor every training point-cloud/language pair (pi, li), we have two sets of demonstrations: a set of trajectories Ti,S that are relevant (similar) to this task and a set of trajectories Ti,D that are irrelevant (dissimilar) as described in Sec. V-C. For each pair of (pi, li), we want all projections of \u03c4j \u2208 Ti,S to have higher similarity to the projection of (pi, li) than \u03c4k \u2208 Ti,D. A simple approach would be to train the network to distinguish these two sets by enforcing a finite distance (safety margin) between the similarities of these two sets [72], which can be written in the form of a constraint:\nsim(\u03a6P,L(pi, li),\u03a6T (\u03c4j)) \u2265 1+sim(\u03a6P,L(pi, li),\u03a6T (\u03c4k))\nRather than simply being able to distinguish two sets, we want to learn semantically meaningful embedding spaces from different modalities. Recalling our earlier example where one incorrect trajectory for manipulating a door knob was much closer to correct than another, it is clear that our learning algorithm should drive some of incorrect trajectories to be more dissimilar than others. The difference between the similarities of \u03c4j and \u03c4k to the projected point-cloud/language pair (pi, li) should be at least the loss \u2206(\u03c4j , \u03c4k). This can be written as a form of a constraint:\n\u2200\u03c4j \u2208 Ti,S ,\u2200\u03c4k \u2208 Ti,D sim(\u03a6P,L(pi, li),\u03a6T (\u03c4j))\n\u2265 \u2206(\u03c4j , \u03c4k) + sim(\u03a6P,L(pi, li),\u03a6T (\u03c4k))\nIntuitively, this forces trajectories with higher DTW-MT distance from the ground truth to embed further than those with lower distance. Enforcing all combinations of these constraints could grow exponentially large. Instead, similar to the cutting plane method for structural support vector machines [68], we find the most violating trajectory \u03c4 \u2032 \u2208 Ti,D for each training pair of (pi, li, \u03c4i \u2208 Ti,S) at each iteration. The most violating trajectory has the highest similarity augmented with the loss scaled by a constant \u03b1:\n\u03c4 \u2032i = arg max \u03c4\u2208Ti,D (sim(\u03a6P,L(pi, li),\u03a6T (\u03c4)) + \u03b1\u2206(\u03c4i, \u03c4))\nThe cost of our deep embedding space h3 is computed as the hinge loss of the most violating trajectory.\nLh3(pi, li, \u03c4i) = |\u2206(\u03c4 \u2032i , \u03c4i)+sim(\u03a6P,L(pi, li),\u03a6T (\u03c4 \u2032i)) \u2212sim(\u03a6P,L(pi, li),\u03a6T (\u03c4i))|+\nThe average cost of each minibatch is back-propagated through all the layers of the deep neural network using the AdaDelta [77] algorithm."}, {"heading": "A. Pre-training Joint Point-cloud/Language Model", "text": "One major advantage of modern deep learning methods is the use of unsupervised pre-training to initialize neural network parameters to a good starting point before the final supervised fine-tuning stage. Pre-training helps these highdimensional networks to avoid overfitting to the training data.\nOur lower layers h2,pl and h2,\u03c4 represent features extracted exclusively from the combination of point-clouds and language, and from trajectories, respectively. Our pretraining method initializes h2,pl and h2,\u03c4 as semantically meaningful embedding spaces similar to h3, as shown later in Section VIII-C.\nFirst, we pre-train the layers leading up to these layers using spare de-noising autoencoders [71, 78]. Then, our process for pre-training h2,pl is similar to our approach to fine-tuning a semantically meaningful embedding space for h3 presented above, except now we find the most violating language l\u2032 while still relying on a loss over the associated optimal trajectory:\nl\u2032 = argmax l\u2208L (sim(\u03a6P(pi),\u03a6L(l)) + \u03b1\u2206(\u03c4, \u03c4 \u2217 i ))\nLh2,pl(pi, li, \u03c4i) = |\u2206(\u03c4i, \u03c4 \u2032)+sim(\u03a6P(pi),\u03a6L(l\u2032)) \u2212sim(\u03a6P(pi),\u03a6L(li))|+\nNotice that although we are training this embedding space to project from point-cloud/language data, we guide learning using trajectory information.\nAfter the projections \u03a6P and \u03a6L are tuned, the output of these two projections are added to form the output of layer h2,pl in the final feed-forward network."}, {"heading": "B. Pre-training Trajectory Model", "text": "For our task of inferring manipulation trajectories for novel objects, it is especially important that similar trajectories \u03c4 map to similar regions in the feature space defined by h2,\u03c4 , so that trajectory embedding h2,\u03c4 itself is semantically meaningful and they can in turn be mapped to similar regions in h3. Standard pretraining methods, such as sparse denoising autoencoder [71, 78] would only pre-train h2,\u03c4 to reconstruct individual trajectories. Instead, we employ pretraining similar to Sec. V-A, except now we pre-train for only a single modality \u2013 trajectory data.\nAs shown on right hand side of Fig. 4, the layer that embeds to h2,\u03c4 is duplicated. These duplicated embedding layers are treated as if they were two different modalities, but all their weights are shared and updated simultaneously. For every trajectory \u03c4 \u2208 Ti,S , we can again find the most violating \u03c4 \u2032 \u2208 Ti,D and the minimize a similar cost function as we do for h2,pl."}, {"heading": "C. Label Noise", "text": "When our data contains a significant number of noisy trajectories \u03c4 , e.g. due to crowd-sourcing (Sec. VII), not all trajectories should be trusted as equally appropriate, as will be shown in Sec. VIII.\nFor every pair of inputs (pi, li), we have Ti = {\u03c4i,1, \u03c4i,2, ..., \u03c4i,ni}, a set of trajectories submitted by the crowd for (pi, li). First, the best candidate label \u03c4\u2217i \u2208 Ti for (pi, li) is selected as the one with the smallest average trajectory distance to the others:\n\u03c4\u2217i = argmin \u03c4\u2208Ti\n1\nni ni\u2211 j=1 \u2206(\u03c4, \u03c4i,j)\nWe assume that at least half of the crowd tried to give a reasonable demonstration. Thus a demonstration with the smallest average distance to all other demonstrations must be a good demonstration. We use the DTW-MT distance function (described later in Sec. VI) for our loss function \u2206(\u03c4, \u03c4\u0304), but it could be replaced by any function that computes the loss of predicting \u03c4\u0304 when \u03c4 is the correct demonstration.\nUsing the optimal demonstration and a loss function \u2206(\u03c4, \u03c4\u0304) for comparing demonstrations, we find a set of trajectories Ti,S that are relevant (similar) to this task and a set of trajectories Ti,D that are irrelevant (dissimilar.) We can use thresholds (tS , tD) determined by the expert to generate two sets from the pool of trajectories:\nTi,S = {\u03c4 \u2208 T |\u2206(\u03c4\u2217i , \u03c4) < tS}\nTi,D = {\u03c4 \u2208 T |\u2206(\u03c4\u2217i , \u03c4) > tD}\nThis method allows our model to be robust against noisy labels and also serves as a method of data augmentation by also considering demonstrations given for other tasks in both sets of Ti,S and Ti,D."}, {"heading": "VI. LOSS FUNCTION FOR MANIPULATION TRAJECTORY", "text": "For both learning and evaluation, we need a function which accurately represents distance between two trajectories. Prior metrics for trajectories consider only their translations (e.g. [31]) and not their rotations and gripper status. We propose a new measure, which uses dynamic time warping, for evaluating manipulation trajectories. This measure nonlinearly warps two trajectories of arbitrary lengths to produce a matching, then computes cumulative distance as the sum of cost of all matched waypoints. The strength of this measure is that weak ordering is maintained among matched waypoints and that every waypoint contributes to the cumulative distance.\nFor two trajectories of arbitrary lengths, \u03c4A = {\u03c4 (i)A } mA i=1 and \u03c4B = {\u03c4 (i)B } mB i=1 , we define a matrix D \u2208 RmA\u00d7mB , where D(i, j) is the cumulative distance of an optimallywarped matching between trajectories up to index i and j, respectively, of each trajectory. The first column and the first row of D is initialized as:\nD(i, 1) = i\u2211 k=1 c(\u03c4 (k) A , \u03c4 (1) B ) \u2200i \u2208 [1,mA]\nD(1, j) = j\u2211 k=1 c(\u03c4 (1) A , \u03c4 (k) B ) \u2200j \u2208 [1,mB ]\nwhere c is a local cost function between two waypoints (discussed later). The rest of D is completed using dynamic programming:\nD(i, j) =c(\u03c4 (i) A , \u03c4 (j) B )\n+ min{D(i\u2212 1, j \u2212 1), D(i\u2212 1, j), D(i, j \u2212 1)}\nGiven the constraint that \u03c4 (1)A is matched to \u03c4 (1) B , the formulation ensures that every waypoint contributes to the final cumulative distance D(mA,mB). Also, given a matched pair (\u03c4 (i)A , \u03c4 (j) B ), no waypoint preceding \u03c4 (i) A is matched to a waypoint succeeding \u03c4 (j)B , encoding weak ordering. The pairwise cost function c between matched waypoints \u03c4 (i) A and \u03c4 (j) B is defined:\nc(\u03c4 (i) A , \u03c4 (j) B ;\u03b1T , \u03b1R,\u03b2, \u03b3) = w(\u03c4 (i) A ; \u03b3)w(\u03c4 (j) B ; \u03b3)(\ndT (\u03c4 (i) A , \u03c4 (j) B )\n\u03b1T + dR(\u03c4\n(i) A , \u03c4 (j) B ) \u03b1R\n)( 1 + \u03b2dG(\u03c4 (i) A , \u03c4 (j) B ) ) where dT (\u03c4 (i) A , \u03c4 (j) B ) = ||(tx, ty, tz) (i) A \u2212 (tx, ty, tz) (j) B ||2\ndR(\u03c4 (i) A , \u03c4 (j) B ) = angle difference between \u03c4 (i) A and \u03c4 (j) B dG(\u03c4 (i) A , \u03c4 (j) B ) = 1(g (i) A = g (j) B )\nw(\u03c4 (i); \u03b3) = exp(\u2212\u03b3 \u00b7 ||\u03c4 (i)||2)\nThe parameters \u03b1, \u03b2 are for scaling translation and rotation errors, and gripper status errors, respectively. \u03b3 weighs the importance of a waypoint based on its distance to the object part. 2 Finally, as trajectories vary in length, we normalize D(mA,mB) by the number of waypoint pairs that contribute\n2In this work, we assign \u03b1T , \u03b1R, \u03b2, \u03b3 values of 0.0075 meters, 3.75\u25e6, 1 and 4 respectively.\nto the cumulative sum, |D(mA,mB)|path\u2217 (i.e. the length of the optimal warping path), giving the final form:\ndistance(\u03c4A, \u03c4B) = D(mA,mB)\n|D(mA,mB)|path\u2217\nThis distance function is used for noise-handling in our model and as the final evaluation metric."}, {"heading": "VII. ROBOBARISTA: CROWD-SOURCING PLATFORM", "text": "In order to collect a large number of manipulation demonstrations from the crowd, we built a crowd-sourcing web platform that we call Robobarista (see Fig. 5). It provides a virtual environment where non-expert users can teach robots via a web browser, without expert guidance or physical presence with a robot and a target object.\nThe system simulates a situation where the user encounters a previously unseen target object and a natural language instruction manual for its manipulation. Within the web browser, users are shown a point-cloud in the 3-D viewer on the left and a manual on the right. A manual may involve several instructions, such as \u201cPush down and pull the handle to open the door\u201d. The user\u2019s goal is to demonstrate how to manipulate the object in the scene for each instruction.\nThe user starts by selecting one of the instructions on the right to demonstrate (Fig. 5). Once selected, the target object part is highlighted and the trajectory edit bar appears below the 3-D viewer. Using the edit bar, which works like a video editor, the user can playback and edit the demonstration. The trajectory representation, as a set of waypoints (Sec. III-A), is directly shown on the edit bar. The bar shows not only the set of waypoints (red/green) but also the interpolated waypoints (gray). The user can click the \u2018play\u2019 button or hover the cursor over the edit bar to examine the current demonstration. The blurred trail of the current trajectory (ghosted) demonstration is also shown in the 3-D viewer to show its full expected path.\nGenerating a full trajectory from scratch can be difficult for non-experts. Thus, similar to Forbes et al. [17], we provide a trajectory that the system has already seen for another object as the initial starting trajectory to edit.3\nIn order to simulate a realistic experience of manipulation, instead of simply showing a static point-cloud, we have overlaid CAD models for parts such as \u2018handle\u2019 so that functional parts actually move as the user tries to manipulate the object.\nA demonstration can be edited by: 1) modifying the position/orientation of a waypoint, 2) adding/removing a waypoint, and 3) opening/closing the gripper. Once a waypoint is selected, the PR2 gripper is shown with six directional arrows and three rings, used to modify the gripper\u2019s position and orientation, respectively. To add extra waypoints, the user can hover the cursor over an interpolated (gray) waypoint on the edit bar and click the plus(+) button. To remove an existing waypoint, the user can hover over it on the edit bar and click minus(-) to remove. As modification occurs, the edit bar and ghosted demonstration are updated with a new interpolation. Finally, for editing the status (open/close) of the gripper, the user can simply click on the gripper.\nFor broader accessibility, all functionality of Robobarista, including 3-D viewer, is built using Javascript and WebGL. We have made the platform available online (http:// robobarista.cs.cornell.edu)"}, {"heading": "VIII. EXPERIMENTS", "text": ""}, {"heading": "A. Robobarista Dataset", "text": "In order to test our model, we have collected a dataset of 116 point-clouds of objects with 249 object parts (examples shown in Figure 6). Objects range from kitchen appliances such as stoves and rice cookers to bathroom hardware such as sinks and toilets. Figure 14 shows a sample of 70 such\n3We have made sure that it does not initialize with trajectories from other folds to keep 5-fold cross-validation in experiment section valid.\nobjects. There are also a total of 250 natural language instructions (in 155 manuals).4 Using the crowd-sourcing platform Robobarista, we collected 1225 trajectories for these objects from 71 non-expert users on the Amazon Mechanical Turk. After a user is shown a 20-second instructional video, the user first completes a 2-minute tutorial task. At each session, the user was asked to complete 10 assignments where each consists of an object and a manual to be followed.\nFor each object, we took raw RGB-D images with the Microsoft Kinect sensor and stitched them using Kinect Fusion [26] to form a denser point-cloud in order to incorporate different viewpoints of objects. Objects range from kitchen appliances such as \u2018stove\u2019, \u2018toaster\u2019, and \u2018rice cooker\u2019 to \u2018urinal\u2019, \u2018soap dispenser\u2019, and \u2018sink\u2019 in restrooms. The dataset is made available at http://robobarista.cs. cornell.edu"}, {"heading": "B. Baselines", "text": "We compared our model against many baselines: 1) Random Transfers (chance): Trajectories are selected at random from the set of trajectories in the training set. 2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points. Using this classifier, we first classify the target object part p into an object part category (e.g. \u2018handle\u2019, \u2018knob\u2019), then use the same feature space to find its nearest neighbor p\u2032 of the same class from the training set. Then the trajectory \u03c4 \u2032 of p\u2032 is transferred to p. 3) Structured support vector machine (SSVM): We used SSVM to learn a discriminant scoring function F : P \u00d7 L \u00d7 T \u2192 R. At test time, for target point-cloud/language\n4Although not necessary for training our model, we also collected trajectories from the expert for evaluation purposes.\npair (p, l), we output the trajectory \u03c4 from the training set that maximizes F . To train SSVM, we use a joint feature mapping \u03c6(p, l, \u03c4) = [\u03c6(\u03c4);\u03c6(p, \u03c4);\u03c6(l, \u03c4)]. \u03c6(\u03c4) applies Isomap [63] to interpolated \u03c4 for non-linear dimensionality reduction. \u03c6(p, \u03c4) captures the overall shape when trajectory \u03c4 is overlaid over point-cloud p by jointly representing them in a voxel-based cube similar to Sec. III-B, with each voxel holding count of occupancy by p or \u03c4 . Isomap is applied to this representation to get the final \u03c6(p, \u03c4). Finally, \u03c6(l, \u03c4) is the tensor product of the language features and trajectory features: \u03c6(l, \u03c4) = \u03c6(l) \u2297 \u03c6(\u03c4). We used our loss function (Sec. VI) to train SSVM and used the cutting plane method to solve the SSVM optimization problem [28].\n4) Latent Structured SVM (LSSVM) + kinematic structure: The way in which an object is manipulated largely depends on its internal structure \u2013 whether it has a \u2018revolute\u2019, \u2018prismatic\u2019, or \u2018fixed\u2019 joint. Instead of explicitly trying to learn this structure, we encoded this internal structure as latent variable z \u2208 Z , composed of joint type, center of joint, and axis of joint [58]. We used Latent SSVM [76] to train with z, learning the discriminant function F : P \u00d7 L \u00d7 T \u00d7 Z \u2192 R. The model was trained with feature mapping \u03c6(p, l, \u03c4, z) = [\u03c6(\u03c4);\u03c6(p, \u03c4);\u03c6(l, \u03c4);\u03c6(l, z);\u03c6(p, \u03c4, z)], which includes additional features that involve z. \u03c6(l, z) captures the relation between l, a bag-of-words representation of language, and bag-of-joint-types encoded by z (vector of length 3 indicating existence of each joint type) by computing the tensor product \u03c6(l)\u2297\u03c6(z), then reshaping the product into a vector. \u03c6(p, \u03c4, z) captures how well the portion of \u03c4 that actually interacts with p abides by the internal structure h. \u03c6(p, \u03c4, z) is a concatenation of three types of features, one for each joint type. For \u2018revolute\u2019 type joints, it includes deviation of trajectory from plane of rotation defined by z, the maximum angular rotation while maintaining pre-defined proximity to the plane of rotation, and the average cosine similarity between rotation axis of \u03c4\nand axis defined by z. For \u2018prismatic\u2019 joints, it includes the average cosine similarity between the extension axis and the displacement vector between waypoints. Finally, for \u2018fixed\u2019 joints, it includes whether the uninteracting part of \u03c4 has collision with the background p since it is important to approach the object from correct angle. 5) Task-Similarity Transfers + random: We compute the pairwise similarities between the test case (ptest, ltest) and each training example (ptrain, ltrain), then transfer a trajectory \u03c4 associated with the training example of highest similarity. Pairwise similarity is defined as a convex combination of the cosine similarity in bag-of-words representations of language and the average mutual point-wise distance of two pointclouds after a fixed number of iterations of the ICP [7] algorithm. If there are multiple trajectories associated with (ptrain, ltrain) of highest similarity, the trajectory for transfer is selected randomly. 6) Task-similarity Transfers + weighting: The previous method is problematic when non-expert demonstrations for a single task (ptrain, ltrain) vary in quality. Forbes et al. [17] introduces a score function for weighting demonstrations based on weighted distance to the \u201cseed\u201d (expert) demonstration. Adapting to our scenario of not having any expert demonstrations, we select \u03c4 that has the lowest average distance from all other demonstrations for the same task, with each distance measured with our loss function (Sec. VI.) This is similar to our noise handling approach in Sec. V-C. 7) Deep Network without Embedding: We train a deep neural network to learn a similar scoring function F : P\u00d7L\u00d7T \u2192 R to that learned for SSVM above. This model discriminatively projects the combination of point-cloud, language, and trajectory features to a score which represents how well the trajectory matches that point-cloud/language combination. Note that this is much less efficient than our joint embedding approach, as it must consider all combinations of a new point-cloud/language pair and every training trajectory to perform inference, as opposed to our model which need only project this new pair to our joint embedding space. This deep\nlearning model concatenates all the input of three modalities and learns three hidden layers before the final layer. 8) Deep Multimodal Network without Embedding: The same approach as \u2018Deep Network without Embedding\u2019 with layers per each modality before concatenating as shown in Figure 7. More details about the model can be found in [61]. 9) LMNN [72]-like cost function: For all top layer fine-tuning and lower layer pre-training, we define the cost function without loss augmentation. Similar to LMNN [72], we give a finite margin between similarities. For example, as cost function for h3:\nLh3(pi, li, \u03c4i) = |1+sim(\u03a6P,L(pi, li),\u03a6T (\u03c4 \u2032)) \u2212sim(\u03a6P,L(pi, li),\u03a6T (\u03c4i))|+\n10) Our Model without Pretraining: Our full model finetuned without any pre-training of lower layers \u2013 all parameters are randomly initialized. 11) Our Model with SDA: Our full model without pretraining h2,pl and h2,\u03c4 as defined in Section V-A and\nSection V-B. Instead, we pre-train each layer as stacked denoising autoencoders [71, 78]. 12) Our Model without Noise Handling: Our model is trained without noise handling as presented in Section V-C. All of the trajectories collected from the crowd are trusted as a ground-truth labels. 13) Our Model with Experts: Our model is trained only using trajectory demonstrations from an expert which were collected for evaluation purposes. 14) Our Full Model - Deep Multimodal Embedding: Our full model as described in this paper with network size of h1,p, h1,l, h1,\u03c4 , h2,pl, h2,\u03c4 , and h3 respectively having a layer with 150, 175, 100, 100, 75, and 50 nodes."}, {"heading": "C. Results and Discussions", "text": "We evaluated all models on our dataset using 5-fold crossvalidation and the results are in Table I. All models which required hyper-parameter tuning used 10% of the training data as the validation set.\nRows list the models we tested including our model and baselines. Each column shows one of three evaluation metrics. The first two use dynamic time warping for manipulation trajectory (DTW-MT) from Sec. VI. The first column shows averaged DTW-MT for each instruction manual consisting of one or more language instructions. The second column shows averaged DTW-MT for every test pair (p, l).\nAs DTW-MT values are not intuitive, we also include a measure of \u201caccuracy,\u201d which shows the percentage of transferred trajectories with DTW-MT value less than 10. Through expert surveys, we found that when DTW-MT of manipulation trajectory is less than 10, the robot came up with a reasonable trajectory and will very likely be able to accomplish the given task. Additionally, Fig. 11 shows accuracies obtained by varying the threshold on the DTWMT measure. Can manipulation trajectories be transferred from completely different objects? Our full model gave 65.1% accuracy (Table I), outperforming every other baseline approach tested.\nFig. 8 shows two examples of successful transfers and one unsuccessful transfer by our model. In the first example,\nthe trajectory for pulling down on a cereal dispenser is transferred to a coffee dispenser. Because our approach to trajectory representation is based on the principal axis (Sec. III-C), even though the cereal and coffee dispenser handles are located and oriented differently, the transfer is a success. The second example shows a successful transfer from a DC power supply to a slow cooker, which have \u201cknobs\u201d of similar shape. The transfer was successful despite the difference in instructions (\u201cTurn the switch..\u201d and \u201cRotate the knob..\u201d) and object type. This highlights the advantages of our end-to-end approach over relying on semantic classes for parts and actions.\nThe last example in Fig. 8 shows a potentially unsuccessful transfer. Despite the similarity in two instructions and similarity in required counterclockwise motions, the transferred motion might not be successful. While the knob on radiator must be grasped in the middle, the rice cooker has a handle that extends sideways, requiring it to be grasped off-center. For clarity of visualization in figures, we have overlaid CAD models over some noisy point-clouds. Many of the object parts were too small and/or too glossy for the Kinect sensor. We believe that a better 3-D sensor would allow for more accurate transfers. On the other hand, it is interesting to note that the transfer in opposite direction from the radiator knob to the rice cooker handle may have yielded a correct manipulation. Can we crowd-source the teaching of manipulation trajectories? When we trained our full model with expert demonstrations, which were collected for evaluation purposes, it performed at 56.5% compared to 65.1% by our model trained with crowd-sourced data. Even though nonexpert demonstrations can carry significant noise, as shown in last two examples of Fig. 6, our noise-handling approach allowed our model to take advantage of the larger, less accurate crowd-sourced dataset. Note that all of our crowd users are true non-expert users from Amazon Mechanical Turk. Is segmentation required for the system? Even with the state-of-the-art techniques [16, 33], detection of \u2018manipulable\u2019 object parts such as \u2018handles\u2019 and \u2018levers\u2019 in a point-\nInput Deep Multimodal Network without Embedding [60] Deep Multimodal Embedding (Our Model)\ncloud is by itself a challenging problem [35]. Thus, we rely on human experts to pre-label parts of the object to be manipulated. The point-cloud of the scene is over-segmented into thousands of supervoxels, from which the expert chooses the part of the object to be manipulated. Even with expert input, such segmented point-clouds are still extremely noisy because of sensor failures, e.g. on glossy surfaces. Is intermediate object part labeling necessary? A multiclass SVM trained on object part labels was able to obtain\nover 70% recognition accuracy in classifying five major classes of object parts (\u2018button\u2019, \u2018knob\u2019, \u2018handle\u2019, \u2018nozzle\u2019, \u2018lever\u2019.) However, the Object Part Classifier baseline, based on this classification, performed at only 23.3% accuracy for actual trajectory transfer, outperforming chance by merely 12.1%, and significantly underperforming our model\u2019s result of 65.1%. This shows that object part labels alone are not sufficient to enable manipulation motion transfer, while our model, which makes use of richer information, does a much better job. Can features be hand-coded? What does learned deep embedding space represent? Even though we carefully designed state-of-the-art task-specific features for the SSVM and LSSVM models, these models only gave at most 40.8% accuracy. The task similarity method gave a better result of 53.7%, but it requires access to all of the raw training data (point-clouds, language, and trajectories) at test time, which leads to heavy computation at test time and requires a large amount of storage as the size of training data increases. Our approach, by contrast, requires only the trajectory data, and a low-dimensional representation of the point-cloud and language data, which is much less expensive to store than the raw data.\nThis shows that it is extremely difficult to find a good set of features which properly combines these three modalities. Our multimodal embedding model does not require handdesigning such features, instead learning a joint embedding space as shown by our visualization of the top layer h3 in Figure 12. This visualization is created by projecting all training data (point-cloud/language pairs and trajectories) of one of the cross-validation folds to h3, then embedding them to 2-dimensional space using t-SNE [69]. Although previous work [61] was able to visualize several nodes in the top layer, most were difficult to interpret. With our model, we can embed all our data and visualize all the layers (see Figs. 12 and 13).\nOne interesting result is that our system was able to naturally learn that \u201cnozzle\u201d and \u201cspout\u201d are effectively synonyms for purposes of manipulation. It clustered these together in the lower-right of Fig. 12 based solely on the fact that both are associated with similar point-cloud shapes and manipulation trajectories. At the same time, it also identified one exception, a small cluster of \u201cnozzles\u201d in the center of Fig. 12 which require different manipulation motions.\nIn addition to the aforementioned cluster in the bottomright of Fig. 12, we see several other logical clusters. Importantly, we can see that our embedding maps vertical and horizontal rotation operations to very different regions of the space \u2013 roughly 12 o\u2019clock and 8 o\u2019clock in Fig. 12, respectively. Even though these have nearly identical language instructions, our algorithm learns to map them differently based on their point-clouds, mapping nearby the appropriate manipulation trajectories. Should cost function be loss-augmented? When we changed the cost function for pre-training h2 and finetuning h3 to use a constant margin of 1 between relevant Ti,S and irrelevant Ti,D demonstrations [72], performance drops to 55.5%. This loss-augmentation is also visible in our embedding space. Notice the purple cluster around the 6 o\u2019clock region of Fig. 12, and the lower part of the cluster in the 5 o\u2019clock region. The purple cluster represents tasks and demonstrations related to pushing a bar (often found on soda fountains), and the lower part of the red cluster represents the task of holding a cup below the nozzle. Although the motion required for one task would not be replaceable by the other, the motions and shapes are very similar, especially compared to most other motions e.g. turning a horizontal knob. Is pre-embedding important? As seen in Table I, without any pre-training our model gives an accuracy of only 54.2%. Pre-training the lower layers with the conventional stacked de-noising auto-encoder (SDA) algorithm [71, 78] increases performance to 62.6%, still significantly underperforming our pre-training algorithm, which gives 65.1%. This shows that our metric embedding pre-training approach provides a better initialization for an embedding space than SDA.\nFig. 13 shows the joint point-cloud/language embedding h2,pl after the network is initialized using our pre-training algorithm and then fine-tuned using our cost function for h3. While this space is not as clearly clustered as h3 shown in Fig. 12, we note that point-clouds tend to appear in the more general center of the space, while natural language instructions appear around the more-specific edges. This\nmakes sense because one point-cloud might afford many possible actions, while language instructions are much more specific. Does embedding improve efficiency? The previous model [61] had 749, 638 parameters to be learned, while our model has only 418, 975 (and still gives better performance.)\nThe previous model had to compute joint pointcloud/language/trajectory features for all combinations of the current point-cloud/language pair with each candidate trajectory (i.e. all trajectories in the training set) to infer an optimal trajectory. This is inefficient and does not scale well with the number of training datapoints. However, our model pre-computes the projection of all trajectories into h3. Inference in our model then requires only projecting the new point-cloud/language combination to h3 once and finding the trajectory with maximal similarity in this embedding.\nIn practice, this results in a significant improvement in efficiency, decreasing the average time to infer a trajectory from 2.3206ms to 0.0135ms, a speed-up of about 171x. Time was measured on the same hardware, with a GPU (GeForce GTX Titan X), using the Theano library [5]. We measured inference times 10000 times for first test fold, which has a pool of 962 trajectories. Time to preprocess the data and time to load into GPU memory was not included in this measurement. We note that the only part of our algorithm\u2019s runtime which scales up with the amount of training data is the nearest-neighbor computation, for which there exist many efficient algorithms [45]. Thus, our algorithm could be scaled to much larger datasets, allowing it to handle a wider variety of tasks, environments, and objects."}, {"heading": "D. Robotic Validation", "text": "To validate the concept of part-based manipulation trajectory transfer in a real-world setting, we tested our algorithm on our PR2 robot. To ensure real transfers, we tested with four objects the algorithm had never seen before \u2013 a coffee dispenser, coffee grinder, lamp, and espresso machine.\nThe PR2 robot has two 7DoF arms, an omni-directional base, and many sensors including a Microsoft Kinect, stereo cameras, and a tilting laser scanner. For these experiments, a point-cloud is acquired from the head mounted Kinect sensor and each motion is executed on the specified arm using a Cartesian end-effector stiffness controller [9] in ROS [51].\nFor each object, the robot is presented with a segmented point-cloud along with a natural language text manual, with each step in the manual associated with a segmented part in the point-cloud. Once our algorithm outputs a trajectory (transferred from a completely different object), we find the manipulation frame for the part\u2019s point-cloud by using its principal axis (Sec. III-B). Then, the transferred trajectory can be executed relative to the part using this coordinate frame, without any modification to the trajectory.\nThe chosen manipulation trajectory, defined as a set of waypoints, is converted to a smooth and densely interpolated trajectory (Sec. III-A.) The robot first computes and execute a collision-free motion to the starting point of the manipulation trajectory. Then, starting from this first waypoint, the\ninterpolated trajectory is executed. For these experiments, we placed the robot in reach of the object, but one could also find a location using a motion planner that would make all waypoints of the manipulation trajectory reachable.\nSome of the examples of successful execution on a PR2 robot are shown in Fig. 10 and in video at the project website: http://robobarista.cs.cornell.edu/. For example, a manipulation trajectory from the task of \u201cturning on a light switch\u201d is transferred to the task of \u201cflipping on a switch to start extracting espresso\u201d, and a trajectory for turning on DC power supply (by rotating clockwise) is transferred to turning on the floor lamp. These demonstrations shows that partbased transfer of manipulation trajectories is feasible without any modification to the source trajectories by carefully choosing their representation and coordinate frames (Sec. IIIC)."}, {"heading": "IX. CONCLUSION AND FUTURE WORK", "text": "In this work, we introduce a novel approach to predicting manipulation trajectories via part based transfer, which allows robots to successfully manipulate even objects they have never seen before. We formulate this as a structured-output problem and approach the problem of inferring manipulation\ntrajectories for novel objects by jointly embedding pointcloud, natural language, and trajectory data into a common space using a deep neural network. We introduce a method for learning a common representation of multimodal data using a new loss-augmented cost function, which learns a semantically meaningful embedding from data. We also introduce a method for pre-training the network\u2019s lower layers, learning embeddings for subsets of modalities, and show that it outperforms standard pre-training algorithms. Learning such an embedding space allows efficient inference by comparing the embedding of a new point-cloud/language pair against pre-embedded demonstrations. We introduce our crowd-sourcing platform, Robobarista, which allows nonexpert users to easily give manipulation demonstrations over the web. This enables us to collect a large-scale dataset of 249 object parts with 1225 crowd-sourced demonstrations, on which our algorithm outperforms all other methods tried. We also verify on our robot that even manipulation trajectories transferred from completely different objects can be used to successfully manipulate novel objects the robot has never seen before.\nWhile our model is able to give correct manipulation trajectories for most of the objects we tested it on, outperform-\ning all other approaches tried, open-loop execution of a pose trajectory may not be enough to correctly manipulate some objects. For such objects, correctly executing a transferred manipulation trajectory may require incorporating visual and/or force feedback [74, 70] in order for the execution to adapt exactly to the object. For example, some dials or buttons have to be rotated or pushed until they click, and each might require a different amount of displacement to accomplish this task. For such tasks, the robot would have to use this feedback to adapt its trajectory in an online fashion.\nOur current model also only takes into account the object part and desired action in question. For some objects, a correct trajectory according to these might still collide with other parts of the environment. Once again, solving this problem would require adapting the manipulation trajectory after it\u2019s selected, and is an interesting direction for future work."}, {"heading": "ACKNOWLEDGMENT", "text": "We thank Joshua Reichler for building the initial prototype of the crowd-sourcing platform. We thank Ross Knepper and Emin Gu\u0308n Sirer for useful discussions. We thank NVIDIA Corporation for the donation of the Tesla K40 GPU used for this research. This work was supported by NRI award 1426452, ONR award N00014-14-1-0156, and by Microsoft Faculty Fellowship and NSF Career Award to one of us (Saxena)."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A. Ng"], "venue": "International Journal of Robotics Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Instance-based learning algorithms", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}, {"title": "Robot web tools [ros topics", "author": ["B. Alexander", "K. Hsiao", "C. Jenkins", "B. Suay", "R. Toris"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A survey of robot learning from demonstration", "author": ["B. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "RAS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Method for registration of 3-d shapes. In Robotics-DL tentative, pages 586\u2013606", "author": ["P.J. Besl", "N.D. McKay"], "venue": "International Society for Optics and Photonics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Learning to localize objects with structured output regression", "author": ["M. Blaschko", "C. Lampert"], "venue": "In ECCV", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Bakebot: Baking cookies with the pr2", "author": ["M. Bollini", "J. Barry", "D. Rus"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Examples of objects and object parts. Each image shows the point cloud representation of an object. We overlaid some of its parts by CAD models for online Robobarista crowd-sourcing platform. Note that the actual underlying point-cloud of object parts contains much more noise and is not clearly segmented, and none of the models have access to overlaid model for inferring manipulation trajectory.  Human and robot perception in large-scale learning from demonstration", "author": ["C. Crick", "S. Osentoski", "G. Jay", "O.C. Jenkins"], "venue": "In HRI. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Semantic grasping: Planning robotic grasps functionally suitable for an object manipulation task", "author": ["H. Dang", "P.K. Allen"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Learning concurrent motor skills in versatile solution spaces", "author": ["C. Daniel", "G. Neumann", "J. Peters"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning a dictionary of prototypical grasp-predicting parts from grasping experience", "author": ["R. Detry", "C.H. Ek", "M. Madry", "D. Kragic"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Learning the dynamics of doors for robotic manipulation", "author": ["F. Endres", "J. Trinkle", "W. Burgard"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Transfer of object shape knowledge across visual and haptic modalities", "author": ["G. Erdogan", "I. Yildirim", "R.A. Jacobs"], "venue": "In Proceedings of the 36th Annual Conference of the Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Robot programming by demonstration with crowdsourced action fixes", "author": ["M. Forbes", "M.J.-Y. Chung", "M. Cakmak", "R.P. Rao"], "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The ecological approach to visual perception", "author": ["J.J. Gibson"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "Object detection with grammar models", "author": ["R. Girshick", "P. Felzenszwalb", "D. McAllester"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Deep belief net learning in a long-range vision system for autonomous offroad driving", "author": ["R. Hadsell", "A. Erkan", "P. Sermanet", "M. Scoffier", "U. Muller", "Y. LeCun"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Reducing the dimensionality of data with neural", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Contact-reactive grasping of objects with partial shape information", "author": ["K. Hsiao", "S. Chitta", "M. Ciocarlie", "E. Jones"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["J. Hu", "J. Lu", "Y.-P. Tan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Learning to recognize human activities from soft labeled data", "author": ["N. Hu", "Z. Lou", "G. Englebienne", "B. Krse"], "venue": "In Proceedings of Robotics: Science and Systems, Berke-  ley,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera", "author": ["S. Izadi", "D. Kim", "O. Hilliges", "D. Molyneaux", "R. Newcombe", "P. Kohli"], "venue": "In ACM Symposium on UIST,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Learning preferences for manipulation tasks from online coactive feedback", "author": ["A. Jain", "B. Wojcik", "T. Joachims", "A. Saxena"], "venue": "In International Journal of Robotics Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Interactive segmentation, tracking, and kinematic modeling of unknown 3d articulated objects", "author": ["D. Katz", "M. Kazemi", "J. Andrew Bagnell", "A. Stentz"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Cloud-based robot grasping with the google object recognition engine", "author": ["B. Kehoe", "A. Matsukawa", "S. Candido", "J. Kuffner", "K. Goldberg"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H. Koppula", "A. Saxena"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "A kernelbased approach to direct action perception", "author": ["O. Kroemer", "E. Ugur", "E. Oztop", "J. Peters"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Unsupervised feature learning for 3d scene labeling", "author": ["K. Lai", "L. Bo", "D. Fox"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "Robotics: Science and Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["I. Lenz", "R. Knepper", "A. Saxena"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["S. Levine", "N. Wagener", "P. Abbeel"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework", "author": ["L.-J. Li", "R. Socher", "L. Fei-Fei"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Unsupervised learning of simultaneous motor primitives through imitation", "author": ["O. Mangin", "P.-Y. Oudeyer"], "venue": "In IEEE ICDL-EPIROB,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "A geometric approach to robotic laundry folding", "author": ["S. Miller", "J. Van Den Berg", "M. Fritz", "T. Darrell", "K. Goldberg", "P. Abbeel"], "venue": "International Journal of Robotics Research,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions", "author": ["D. Misra", "J. Sung", "K. Lee", "A. Saxena"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Learning to embed songs and tags for playlist prediction", "author": ["J. Moore", "S. Chen", "T. Joachims", "D. Turnbull"], "venue": "In Conference of the International Society for Music Information Retrieval (ISMIR),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D.G. Lowe"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["K. M\u00fclling", "J. Kober", "O. Kroemer", "J. Peters"], "venue": "International Journal of Robotics Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Learning and generalization of motor skills by learning from demonstration", "author": ["P. Pastor", "H. Hoffmann", "T. Asfour", "S. Schaal"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2009}, {"title": "Learning to plan for constrained manipulation from demonstrations", "author": ["M. Phillips", "V. Hwang", "S. Chitta", "M. Likhachev"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "Learning articulated motions from visual demonstration", "author": ["S. Pillai", "M. Walter", "S. Teller"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Ros: an opensource robot operating system", "author": ["M. Quigley", "K. Conley", "B. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng"], "venue": "In ICRA workshop on open source software,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2009}, {"title": "3D is here: Point Cloud Library (PCL)", "author": ["R. Rusu", "S. Cousins"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2011}, {"title": "Animating rotation with quaternion curves", "author": ["K. Shoemake"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1985}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E. Huang", "A. Ng", "C. Manning"], "venue": "In EMNLP,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Convolutional-recursive deep learning for 3d object classification", "author": ["R. Socher", "B. Huval", "B. Bhat", "C. Manning", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2012}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "H. Lee"], "venue": "In NIPS,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2012}, {"title": "A probabilistic framework for learning kinematic models of articulated objects", "author": ["J. Sturm", "C. Stachniss", "W. Burgard"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "Un-  structured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "Synthesizing manipulation sequences for under-specified tasks using unrolled markov random fields", "author": ["J. Sung", "B. Selman", "A. Saxena"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Robobarista: Object part-based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["J. Sung", "S.H. Jin", "A. Saxena"], "venue": "In International Symposium on Robotics Research (ISRR),", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2015}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R. Knepper", "A. Li", "T. Howard", "D. Rus", "N. Roy"], "venue": "Robotics: Science and Systems,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. De Silva", "J. Langford"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2000}, {"title": "Robotsfor. me and robots for you", "author": ["R. Toris", "S. Chernova"], "venue": "In Proceedings of the Interactive Machine Learning Workshop, Intelligent User Interfaces Conference,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "The robot management system: A framework for conducting human-robot interaction studies through crowdsourcing", "author": ["R. Toris", "D. Kent", "S. Chernova"], "venue": "Journal of Human-Robot Interaction,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2014}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML. ACM,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2004}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun", "Y. Singer"], "venue": null, "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2005}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2008}, {"title": "Predicting slippage and learning manipulation affordances through gaussian process regression", "author": ["F. Vina", "Y. Bekiroglu", "C. Smith", "Y. Karayiannidis", "D. Kragic"], "venue": "In Humanoids,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2008}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In NIPS,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2005}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "In IJCAI,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2011}, {"title": "Combining force and visual feedback for physical interaction tasks in humanoid robots", "author": ["S. Wieland", "D. Gonzalez-Aguirre", "N. Vahrenkamp", "T. Asfour", "R. Dillmann"], "venue": "In Humanoid Robots,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2009}, {"title": "Hierarchical semantic labeling for task-relevant rgb-d perception", "author": ["C. Wu", "I. Lenz", "A. Saxena"], "venue": "In Robotics:  Science and Systems,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2014}, {"title": "Learning structural svms with latent variables", "author": ["C.-N. Yu", "T. Joachims"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2009}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga"], "venue": "In ICASSP,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2013}], "referenceMentions": [{"referenceID": 60, "context": "[61]) Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.", "startOffset": 60, "endOffset": 71}, {"referenceID": 38, "context": "Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.", "startOffset": 60, "endOffset": 71}, {"referenceID": 18, "context": "Thus, rather than relying on scene understanding techniques [8, 39, 19], we directly use 3D point-clouds for manipulation planning using machine learning algorithms.", "startOffset": 60, "endOffset": 71}, {"referenceID": 14, "context": "Humans are able to map similar concepts from different sensory system to the same concept using common representation between different modalities [15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 44, "context": "Our algorithm allows for efficient inference because, given a new instruction and point-cloud, we only need to find the nearest trajectory to the projection of this pair in the learned embedding space, which can be done using fast nearest-neighbor algorithms [45].", "startOffset": 259, "endOffset": 263}, {"referenceID": 32, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 116, "endOffset": 128}, {"referenceID": 53, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 116, "endOffset": 128}, {"referenceID": 20, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 116, "endOffset": 128}, {"referenceID": 56, "context": "In the past, deep learning methods have shown impressive results for learning features in a wide variety of domains [33, 54, 21] and even learning cross-domain embeddings for, for example, language and image features [57].", "startOffset": 217, "endOffset": 221}, {"referenceID": 3, "context": "Furthermore, in contrast to previous approaches based on learning from demonstration (LfD) that learn a mapping from a state to an action [4], our work complements LfD as we focus on the entire manipulation motion, as opposed to a sequential state-action mapping.", "startOffset": 138, "endOffset": 141}, {"referenceID": 19, "context": "\u2022 We introduce a new algorithm, which learns an embedding space while enforcing a varying and loss-based margin, along with a new unsupervised pre-training method which outperforms standard pre-training algorithms [20].", "startOffset": 214, "endOffset": 218}, {"referenceID": 38, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 31, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 32, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 73, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 91, "endOffset": 107}, {"referenceID": 58, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 134, "endOffset": 142}, {"referenceID": 54, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 191, "endOffset": 199}, {"referenceID": 34, "context": "In recent years, there has been significant research focus on semantic scene understanding [39, 32, 33, 75], human activity detection [59, 25], and features for RGB-D images and point-clouds [55, 35].", "startOffset": 191, "endOffset": 199}, {"referenceID": 18, "context": "idea of using part-based transfers, the deformable part model [19] was effective in object detection.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "On the other hand, direct perception approaches [18, 34] skip the intermediate object labels and directly perceive affordances based on the shape of the object.", "startOffset": 48, "endOffset": 56}, {"referenceID": 33, "context": "On the other hand, direct perception approaches [18, 34] skip the intermediate object labels and directly perceive affordances based on the shape of the object.", "startOffset": 48, "endOffset": 56}, {"referenceID": 8, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 133, "endOffset": 136}, {"referenceID": 41, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 29, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "Most works in robotic manipulation focus on task-specific manipulation of known objects\u2014for example, baking cookies with known tools [9] and folding the laundry [42] \u2013 or focus on learning specific motions such as grasping [30] and opening doors [14].", "startOffset": 246, "endOffset": 250}, {"referenceID": 59, "context": "Others [60, 43] focus on sequencing manipulation tasks assuming perfect manipulation primitives such as grasp and pour are available.", "startOffset": 7, "endOffset": 15}, {"referenceID": 42, "context": "Others [60, 43] focus on sequencing manipulation tasks assuming perfect manipulation primitives such as grasp and pour are available.", "startOffset": 7, "endOffset": 15}, {"referenceID": 57, "context": "For the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm.", "startOffset": 117, "endOffset": 125}, {"referenceID": 49, "context": "For the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm.", "startOffset": 117, "endOffset": 125}, {"referenceID": 28, "context": "For the more general task of manipulating new instances of objects, previous approaches rely on finding articulation [58, 50] or using interaction [29], but they are limited by tracking performance of a vision algorithm.", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "Another approach using part-based transfer between objects has been shown to be successful for grasping [11, 13].", "startOffset": 104, "endOffset": 112}, {"referenceID": 12, "context": "Another approach using part-based transfer between objects has been shown to be successful for grasping [11, 13].", "startOffset": 104, "endOffset": 112}, {"referenceID": 37, "context": "[38] use a Gaussian mixture model to learn system dynamics, then use these to learn a manipulation policy using a deep network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] use a deep network to learn system dynamics for realtime model-predictive control.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 45, "context": "Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "Several successful approaches for teaching robots tasks, such as helicopter maneuvers [1] or table tennis [46], have been based on Learning from Demonstration (LfD) [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 39, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 61, "endOffset": 69}, {"referenceID": 11, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 61, "endOffset": 69}, {"referenceID": 48, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 117, "endOffset": 125}, {"referenceID": 47, "context": "actions and separately relies on high level task composition [40, 12] or is often limited to previously seen objects [49, 48].", "startOffset": 117, "endOffset": 125}, {"referenceID": 1, "context": "Unlike learning a model from demonstration, instancebased learning [2, 17] replicates one of the demonstrations.", "startOffset": 67, "endOffset": 74}, {"referenceID": 16, "context": "Unlike learning a model from demonstration, instancebased learning [2, 17] replicates one of the demonstrations.", "startOffset": 67, "endOffset": 74}, {"referenceID": 70, "context": "LMNN [72] learns a max-margin Mahalanobis distance for a unimodal input feature space.", "startOffset": 5, "endOffset": 9}, {"referenceID": 71, "context": "[73] learn linear mappings from image and language features to a common embedding space for automatic image annotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] learn to map songs and natural language tags to a shared embedding space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33, 54]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 53, "context": "[33, 54]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "In robotics, deep learning has previously been successfully used for detecting grasps for novel objects in multi-channel RGB-D images [36] and for classifying terrain from longrange vision [21].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "In robotics, deep learning has previously been successfully used for detecting grasps for novel objects in multi-channel RGB-D images [36] and for classifying terrain from longrange vision [21].", "startOffset": 189, "endOffset": 193}, {"referenceID": 46, "context": "[47] use deep learning to learn features incorporating both video and audio modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[56] propose a new generative learning algorithm for multimodal data which improves robustness to missing modalities at inference time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] map different languages to a joint feature space for translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Srivastava and Salakhutdinov [57] map images and natural language \u201ctags\u201d to the same space for automatic annotation and retrieval.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": ") Our algorithm trains each layer to map similar cases to similar areas of its feature space, as opposed to other methods which either perform variational learning [22] or train for reconstruction [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 19, "context": ") Our algorithm trains each layer to map similar cases to similar areas of its feature space, as opposed to other methods which either perform variational learning [22] or train for reconstruction [20].", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "[24] also use a deep network for metric learning for the task of face verification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 70, "context": "Similar to LMNN [72], Hu et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 23, "context": "[24] enforces a constant margin between distances among inter-class objects and among intra-class objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Many approaches to teaching robots manipulation and other skills have relied on demonstrations by skilled experts [4, 1].", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "Many approaches to teaching robots manipulation and other skills have relied on demonstrations by skilled experts [4, 1].", "startOffset": 114, "endOffset": 120}, {"referenceID": 9, "context": "Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 61, "context": "Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 26, "context": "Among previous efforts to scale teaching to the crowd [10, 62, 27], Forbes et al.", "startOffset": 54, "endOffset": 66}, {"referenceID": 16, "context": "[17] employs a similar approach towards crowd-sourcing but collects multiple instances of similar table-top manipulation with same object.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "Others also build web-based platform for crowdsourcing manipulation [65, 66].", "startOffset": 68, "endOffset": 76}, {"referenceID": 64, "context": "Others also build web-based platform for crowdsourcing manipulation [65, 66].", "startOffset": 68, "endOffset": 76}, {"referenceID": 2, "context": "[3], but works on any standard web browser with OpenGL support and incorporates real pointclouds of various scenes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "These points are often obtained by stitching together a sequence of sensor data from an RGBD sensor [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 52, "context": "Translation is linearly interpolated and the quaternion is interpolated using spherical linear interpolation (Slerp) [53].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "[17]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Thus, we align the negative z-axis along gravity and align the x-axis along the principal axis of the object part using PCA [23].", "startOffset": 124, "endOffset": 128}, {"referenceID": 32, "context": "image recognition [33]) to which deep learning has mostly been applied [6].", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "image recognition [33]) to which deep learning has mostly been applied [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 46, "context": "this trajectory match this point-cloud/language pair?\u201d Then, we can use a multimodal feed-forward deep network [47] to solve this problem [61].", "startOffset": 111, "endOffset": 115}, {"referenceID": 60, "context": "this trajectory match this point-cloud/language pair?\u201d Then, we can use a multimodal feed-forward deep network [47] to solve this problem [61].", "startOffset": 138, "endOffset": 142}, {"referenceID": 60, "context": "Compared to previous work that exhaustively runs a full network over all these combinations [61], our approach allows us to pre-embed all candidate trajectories into this common feature space.", "startOffset": 92, "endOffset": 96}, {"referenceID": 76, "context": "With W \u2019s as network weights, which are the learned parameters of our system, and a(\u00b7) as a rectified linear unit (ReLU) activation function [78], our model for projecting from point-cloud and language features to the shared embedding h is as follows:", "startOffset": 141, "endOffset": 145}, {"referenceID": 71, "context": "As in previous work [73], similarity is defined as sim(a, b) = a \u00b7 b, and the trajectory that maximizes the magnitude of similarity is selected:", "startOffset": 20, "endOffset": 24}, {"referenceID": 60, "context": "The previous approach to this problem [61] required projecting the combination of the current point-cloud and natural language instruction with every trajectory in the training set through the network during inference.", "startOffset": 38, "endOffset": 42}, {"referenceID": 70, "context": "A simple approach would be to train the network to distinguish these two sets by enforcing a finite distance (safety margin) between the similarities of these two sets [72], which can be written in the form of a constraint:", "startOffset": 168, "endOffset": 172}, {"referenceID": 66, "context": "Instead, similar to the cutting plane method for structural support vector machines [68], we find the most violating trajectory \u03c4 \u2032 \u2208 Ti,D for each training pair of (pi, li, \u03c4i \u2208 Ti,S) at each iteration.", "startOffset": 84, "endOffset": 88}, {"referenceID": 75, "context": "The average cost of each minibatch is back-propagated through all the layers of the deep neural network using the AdaDelta [77] algorithm.", "startOffset": 123, "endOffset": 127}, {"referenceID": 69, "context": "First, we pre-train the layers leading up to these layers using spare de-noising autoencoders [71, 78].", "startOffset": 94, "endOffset": 102}, {"referenceID": 76, "context": "First, we pre-train the layers leading up to these layers using spare de-noising autoencoders [71, 78].", "startOffset": 94, "endOffset": 102}, {"referenceID": 69, "context": "Standard pretraining methods, such as sparse denoising autoencoder [71, 78] would only pre-train h to reconstruct individual trajectories.", "startOffset": 67, "endOffset": 75}, {"referenceID": 76, "context": "Standard pretraining methods, such as sparse denoising autoencoder [71, 78] would only pre-train h to reconstruct individual trajectories.", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "[31]) and not their rotations and gripper status.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], we provide a trajectory that the system has already seen for another object as the initial starting trajectory to edit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "For each object, we took raw RGB-D images with the Microsoft Kinect sensor and stitched them using Kinect Fusion [26] to form a denser point-cloud in order to incorporate different viewpoints of objects.", "startOffset": 113, "endOffset": 117}, {"referenceID": 65, "context": "2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points.", "startOffset": 203, "endOffset": 207}, {"referenceID": 31, "context": "2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points.", "startOffset": 268, "endOffset": 272}, {"referenceID": 51, "context": "2) Object Part Classifier: To test our hypothesis that classifying object parts as an intermediate step does not guarantee successful transfers, we trained an object part classifier using multiclass SVM [67] on point-cloud features \u03c6(p) including local shape features [32], histogram of curvatures [52], and distribution of points.", "startOffset": 298, "endOffset": 302}, {"referenceID": 62, "context": "\u03c6(\u03c4) applies Isomap [63] to interpolated \u03c4 for non-linear dimensionality reduction.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "VI) to train SSVM and used the cutting plane method to solve the SSVM optimization problem [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 57, "context": "Instead of explicitly trying to learn this structure, we encoded this internal structure as latent variable z \u2208 Z , composed of joint type, center of joint, and axis of joint [58].", "startOffset": 175, "endOffset": 179}, {"referenceID": 74, "context": "We used Latent SSVM [76] to train with z, learning the discriminant function F : P \u00d7 L \u00d7 T \u00d7 Z \u2192 R.", "startOffset": 20, "endOffset": 24}, {"referenceID": 57, "context": "6) Latent SSVM + Kinematic [58] 17.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "9) Task Similarity + Weights [17] 13.", "startOffset": 29, "endOffset": 33}, {"referenceID": 60, "context": "0) Deep Multimodal Network with Noise-handling without Embedding [61] 13.", "startOffset": 65, "endOffset": 69}, {"referenceID": 70, "context": "1) LMNN-like Cost Function [72] 15.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Pairwise similarity is defined as a convex combination of the cosine similarity in bag-of-words representations of language and the average mutual point-wise distance of two pointclouds after a fixed number of iterations of the ICP [7] algorithm.", "startOffset": 232, "endOffset": 235}, {"referenceID": 16, "context": "[17] introduces a score function for weighting demonstrations based on weighted distance to the \u201cseed\u201d (expert) demonstration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "More details about the model can be found in [61].", "startOffset": 45, "endOffset": 49}, {"referenceID": 70, "context": "9) LMNN [72]-like cost function: For all top layer fine-tuning and lower layer pre-training, we define the cost function without loss augmentation.", "startOffset": 8, "endOffset": 12}, {"referenceID": 70, "context": "Similar to LMNN [72], we give a finite margin between similarities.", "startOffset": 16, "endOffset": 20}, {"referenceID": 69, "context": "Instead, we pre-train each layer as stacked denoising autoencoders [71, 78].", "startOffset": 67, "endOffset": 75}, {"referenceID": 76, "context": "Instead, we pre-train each layer as stacked denoising autoencoders [71, 78].", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "Is segmentation required for the system? Even with the state-of-the-art techniques [16, 33], detection of \u2018manipulable\u2019 object parts such as \u2018handles\u2019 and \u2018levers\u2019 in a point-", "startOffset": 83, "endOffset": 91}, {"referenceID": 32, "context": "Is segmentation required for the system? Even with the state-of-the-art techniques [16, 33], detection of \u2018manipulable\u2019 object parts such as \u2018handles\u2019 and \u2018levers\u2019 in a point-", "startOffset": 83, "endOffset": 91}, {"referenceID": 59, "context": "without Embedding [60] Deep Multimodal Embedding", "startOffset": 18, "endOffset": 22}, {"referenceID": 60, "context": "Comparisons of transfers between our model and the baseline (deep multimodal network without embedding [61]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 34, "context": "cloud is by itself a challenging problem [35].", "startOffset": 41, "endOffset": 45}, {"referenceID": 67, "context": "This visualization is created by projecting all training data (point-cloud/language pairs and trajectories) of one of the cross-validation folds to h, then embedding them to 2-dimensional space using t-SNE [69].", "startOffset": 206, "endOffset": 210}, {"referenceID": 60, "context": "Although previous work [61] was able to visualize several nodes in the top layer, most were difficult to interpret.", "startOffset": 23, "endOffset": 27}, {"referenceID": 60, "context": "[61]", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "Our algorithm consistently outperforms the previous approach [61] and an LMNN-like cost function [72].", "startOffset": 61, "endOffset": 65}, {"referenceID": 70, "context": "Our algorithm consistently outperforms the previous approach [61] and an LMNN-like cost function [72].", "startOffset": 97, "endOffset": 101}, {"referenceID": 70, "context": "Should cost function be loss-augmented? When we changed the cost function for pre-training h and finetuning h to use a constant margin of 1 between relevant Ti,S and irrelevant Ti,D demonstrations [72], performance drops to 55.", "startOffset": 197, "endOffset": 201}, {"referenceID": 69, "context": "Pre-training the lower layers with the conventional stacked de-noising auto-encoder (SDA) algorithm [71, 78] increases performance to 62.", "startOffset": 100, "endOffset": 108}, {"referenceID": 76, "context": "Pre-training the lower layers with the conventional stacked de-noising auto-encoder (SDA) algorithm [71, 78] increases performance to 62.", "startOffset": 100, "endOffset": 108}, {"referenceID": 60, "context": "Does embedding improve efficiency? The previous model [61] had 749, 638 parameters to be learned, while our model has only 418, 975 (and still gives better performance.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Time was measured on the same hardware, with a GPU (GeForce GTX Titan X), using the Theano library [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 44, "context": "We note that the only part of our algorithm\u2019s runtime which scales up with the amount of training data is the nearest-neighbor computation, for which there exist many efficient algorithms [45].", "startOffset": 188, "endOffset": 192}, {"referenceID": 8, "context": "For these experiments, a point-cloud is acquired from the head mounted Kinect sensor and each motion is executed on the specified arm using a Cartesian end-effector stiffness controller [9] in ROS [51].", "startOffset": 186, "endOffset": 189}, {"referenceID": 50, "context": "For these experiments, a point-cloud is acquired from the head mounted Kinect sensor and each motion is executed on the specified arm using a Cartesian end-effector stiffness controller [9] in ROS [51].", "startOffset": 197, "endOffset": 201}, {"referenceID": 67, "context": "Learned Deep Point-cloud/Language/Trajectory Embedding Space: Joint embedding space h3 after the network is fully fine-tuned, visualized in 2d using t-SNE [69] .", "startOffset": 155, "endOffset": 159}, {"referenceID": 67, "context": "Learned Point-cloud/Language Space: Visualization of the point-cloud/language layer h2,lp in 2d using t-SNE [69] after the network is fully fine-tuned.", "startOffset": 108, "endOffset": 112}, {"referenceID": 72, "context": "For such objects, correctly executing a transferred manipulation trajectory may require incorporating visual and/or force feedback [74, 70] in order for the execution to adapt exactly to the object.", "startOffset": 131, "endOffset": 139}, {"referenceID": 68, "context": "For such objects, correctly executing a transferred manipulation trajectory may require incorporating visual and/or force feedback [74, 70] in order for the execution to adapt exactly to the object.", "startOffset": 131, "endOffset": 139}], "year": 2016, "abstractText": "There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before.", "creator": "LaTeX with hyperref package"}}}