{"id": "1601.02939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "The minimal hitting set generation problem: algorithms and computation", "abstract": "finding * - minimal \" hitting sets \" for identifying given string of sets is a fundamental combinatorial problem with applications in domains as diverse as boolean algebra, computational biology, and data mining. much towards the algorithmic literature focuses on the problem of * recognizing * the concept of minimal detection sets ; however, in many of the applications, context is physically important to * generate * these tool kinds. we survey twenty algorithms from across a variety of domains, considering their history, classification, useful features, and computational performance on a variety at synthetic and real - world inputs. we also provide our mix of implementations underlying these algorithms with a ready - to - use, platform - agnostic interface based on docker containers and the algorun framework, so that interested computational scientists can easily perform input tests with inputs linking their own research areas on their own computers also through a convenient web interface.", "histories": [["v1", "Tue, 5 Jan 2016 19:24:25 GMT  (814kb,D)", "http://arxiv.org/abs/1601.02939v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.CC", "authors": ["andrew gainer-dewar", "paola vera-licona"], "accepted": false, "id": "1601.02939"}, "pdf": {"name": "1601.02939.pdf", "metadata": {"source": "CRF", "title": "The minimal hitting set generation problem: algorithms and computation", "authors": ["Andrew Gainer-Dewar", "Paola Vera-Licona"], "emails": [], "sections": [{"heading": null, "text": "problem with applications in domains as diverse as Boolean algebra, computational biology, and data mining. Much of the algorithmic literature focuses on the problem of recognizing the collection of minimal hitting sets; however, in many of the applications, it is more important to generate these hitting sets. We survey twenty algorithms from across a variety of domains, considering their history, classification, useful features, and computational performance on a variety of synthetic and real-world inputs. We also provide a suite of implementations of these algorithms with a ready-to-use, platform-agnostic interface based on Docker containers and the AlgoRun framework, so that interested computational scientists can easily perform similar tests with inputs from their own research areas on their own computers or through a convenient Web interface."}, {"heading": "1 Introduction", "text": "Fix a family S of sets S1, S2, . . . . A hitting set T of S is a set which intersects each of the sets Si; T is a minimal hitting set (hereafter \u201cMHS\u201d) if no proper subset of T is a hitting set of S.\nThe problem of generating the collection of MHSes for a given set family is of interest in a wide variety of domains, and it has been explicitly studied (under a variety of names) in the contexts of combinatorics (Section 2.1, [1]), Boolean algebra (Section 2.2, [2, 3, 4]), fault diagnosis (Section 2.3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.5, [12, 13, 14, 15]), and computational biology (Section 2.4, [16, 17, 18, 19, 20, 21]), among others. While some interesting results have been obtained for the associated decision problem, the computational complexity of this problem is currently unknown. Nevertheless, there is an extensive literature of algorithms to generate minimal hitting sets.\nIn this paper, we survey the state of the art of algorithms for enumerating MHSes, with particular attention dedicated to publicly-available software implementations and their performance on problems derived from various applied domains. We begin in Section 2 with discussion of the cognate problems that emerge in several applied domains, including explanations of how each can be translated back to our MHS generation problem. In Section 3, we survey what is known about the complexity of the problem, both in general and in specialized tractable cases.\nThe bulk of the paper is dedicated in Section 4 to surveying the history of some twenty algorithms for enumerating MHSes. For each algorithm, we discuss its structure, relevant properties, any known bounds on its complexity, and information about available software implementations.\nIn Section 5, we present the results of extensive benchmarks run on the available implementations of these algorithms, focusing on running time for large examples derived from specific applications. Additionally, we provide these implementations in a unified, ready-to-use framework of Docker images based on the AlgoRun framework, which are available to download or to use through a Web interface. Interested readers and \u2217gainerdewar@uchc.edu \u2020veralicona@uchc.edu\nar X\niv :1\n60 1.\n02 93\n9v 1\n[ cs\n.D S]\n5 J\nan 2\ncomputational scientists may use these containers to try out the algorithms on their own input data and incorporate them into processing pipelines.\nWe conclude in Section 6 with an overview of our results and the state of the art in MHS generation algorithms."}, {"heading": "2 Cognate problems", "text": "There are many important problems in numerous domains that can be reduced or translated to MHS problems. We survey a few of them here, organized by domain."}, {"heading": "2.1 Combinatorics", "text": "Transversal hypergraph problem Given a set V of vertices and a set E of sets E1, E2, \u00b7 \u00b7 \u00b7 \u2286 V of hyperedges, the pair H = (V,E) is a hypergraph. H is finite if V , E, and all the Ei are finite sets. A hypergraph is simple if none of its edges is a subset of any other edge. The hypergraph H = (V,E) can naturally be identified with the set family E, so the theory of hypergraphs is similar to that of set families. Readers interested in the full theory of hypergraphs should consult Berge\u2019s 1984 monograph [22] on the subject.\nIn the finite hypergraph setting, a (minimal) hitting set of E is called a (minimal) transversal of H. The collection of all transversals of H is its transversal hypergraph TrH. TrH is also sometimes called the dual of H because, in the case that H is simple, Tr(TrH) = H. (More generally, Tr(TrH) = minH, the hypergraph obtained from H by removing all non-inclusion-minimal edges.)\nThere is an extensive literature on the problem of determining whether two hypergraphs H1 and H2 are transversal of each other. We will consider a number of algorithms developed for this purpose in Section 4. Interested readers can consult the recent survey of Eiter [1] and Ph.D. thesis of Hagen [2] for more details about this subject.\nSet cover problem Fix a hypergraph H = (V,E). A set cover of H is a set E\u2032 \u2286 E of edges of H with the property that every vertex in V is in some edge in E\u2032. Of course, E is a set cover, but there may be others, and it is of particular interest to compute the inclusion-minimal ones.\nWe can construct a new hypergraph G whose vertex set is E and whose edge set is V (that is, with a vertex in G for every edge of H and an edge in G for each collection of edges in H with a common vertex). Then the MHSes of G are exactly the minimal set covers of H. Thus, an algorithm for generating MHSes can be used to enumerate minimal set covers, and vice versa.\nIndependent set problem Fix a hypergraph H = (V,E). An independent set of H is a set V \u2032 of vertices of H with the property that no edge of H is a subset of V \u2032. Of course, the empty vertex set \u2205 is a set cover, but there may be others, and it is of particular interest to compute the inclusion-maximimal ones. It is easy to show that an independent set is the complement of a hitting set and thus that a maximal independent set is the complement of a minimal hitting set. Thus, we can enumerate maximium independent sets of H simply by applying some algorithm for MHS generation and then taking complements."}, {"heading": "2.2 Boolean algebra", "text": "Definition 2.1. A Boolean function is a function f = f(x1, x2, . . . , xk) : Bk \u2192 B, where B = {0, 1} and k is a non-negative integer.\nA k-ary Boolean function is described by an algebraic expression, called a Boolean expression or Boolean formula, which consists of the binary variables x1, . . . , xk, the binary conjunction operator \u2227 (often written and), the binary disjunction operator \u2228 (often written or), and the unary negation operator \u00ac (often written not).\nIf a Boolean function f has the property that f(X) \u2264 f(Y ) (resp. f(X) \u2265 f(Y )) for any inputs X \u2264 Y , we say that f is positive (resp. negative). A function which is either positive or negative is monotone. Monotone Boolean functions appear in a wide variety of formal and applied settings. (See [23] for an extensive survey.)\nWe can easily ensure that this property holds by restricting the formulas considered:\nDefinition 2.2. A formula for a Boolean function is monotone if it contains only variables, conjunctions, and disjunctions (i.e. no negations).\nTheorem 2.3. Any monotone formula gives a monotone Boolean function.\nHowever, representation in these propositional formulas is not unique; the same function may be given by many formulas, even after commutativity is considered. Accordingly, restrictions are often placed on the formulas to ensure uniqueness.\nDefinition 2.4. A formula for a Boolean function is in conjunctive normal form (CNF) if it is a conjunction of disjunctions of literals and disjunctive normal form (DNF) if it is a disjunction of conjunctions of literals.\nExample 2.5. The formula (x1 \u2228 x2) \u2227 x3 is in CNF, while (x2 \u2227 x3) \u2228 (x1 \u2227 x3) is in DNF. Both normal forms are of great interest for computational applications because they are easy to decompose and compare. In addition, a given monotone Boolean function has exactly one each of CNF and DNF formulas satisfying an irredundancy condition.\nIf two given formulas C in CNF and D in DNF represent the same function, we say they are dual. Deciding whether this is true of a given pair (C,D) is widely studied in the literature of complexity theory; see the excellent survey in the Ph.D. thesis of Hagen [2], which gives this problem the picturesque name monet. We may also consider the generation problem mongen, which asks for the DNF formula D equivalent to a given CNF formula C.\nSurprisingly, mongen is computationally equivalent to the MHS generation problem. To illustrate why, consider the function (x2 \u2228 x3) \u2227 (x1 \u2228 x3), which is evidently in CNF. Its clauses (i.e. the disjunctive components) are formed from the variable sets {x2, x3} and {x1, x3}. Since these clauses are conjoined, to satisfy the formula we must satisfy each clause, which means we must set to 1 at least one variable in each clause; for example, we might take x2 and x1 or take x3 alone.\nIndeed, this is exactly equivalent to finding hitting sets of the set family {{2, 3}, {1, 3}}. Moreover, the irredundancy requirement for a DNF is equivalent to requiring the hitting sets to be minimal. The collection {{2, 1}, {3}} of MHSes corresponds to the DNF (x2 \u2227 x1) \u2228 x3 of the function."}, {"heading": "2.3 Model-based fault diagnosis", "text": "Modern engineered systems may involve incredible numbers of interconnected components; for example, the recently-retired NASA Space Shuttle reportedly had over 2.5 million moving parts. When such a system fails to perform as intended, it is infeasible to check or replace every part and connection; thus, diagnostic procedures are needed to narrow attention to some subset of components which may have caused the observed failure. In a celebrated 1987 paper [5], Reiter developed the foundation for a formal theory of model-based diagnosis (MBD), which we will introduce briefly.\nConsider a system made up of some finite set V of components, each of which may be either active or inactive during any given transaction or activity. We make a series of observations of transactions of the system, recording which components are active and whether the behavior is normal or anomalous. If any of the observed transactions are anomalous, we assume this is due to one or more faulty components. A diagnosis of the faulty system is a set D of components which, if all are faulty, would explain all the anomalous transactions. Under a parsimony restriction, the interesting diagnoses are those which are inclusion-minimal or irredundant. (Reiter only applies the term \u201cdiagnosis\u201d to these minimal examples.)\nLet F be the set of faulty transactions, where each such transaction is represented as the set of components involved. A diagnosis is then a hitting set of F , and the parsimonious diagnoses are exactly the MHSes of F . Thus, any algorithm for generating MHSes is directly applicable to fault diagnosis. Reiter proposed an algorithm for exactly this purpose; we will discuss it and its successors in Section 4."}, {"heading": "2.4 Computational biology", "text": "Minimal hitting sets have appeared as an important combinatorial motif in numerous problems in computational biology. We survey four of them here.\nMinimal cut sets in metabolic networks A metabolic reaction network is the system by which metabolic and physical processes that determine the physiological and biochemical properties of a cell, are represented. As such, these networks comprise the chemical reactions of metabolism, the metabolic pathways and the regulatory interactions that guide these reactions. In its graph representation, a metabolic network is a directed hypergraph in which each of m vertices represents a metabolite and each of n directed hyperedges represents a biochemical reaction. Information about the reactions can be encoded in a stoichiometric matrix M in which each row represents a metabolite, each column represents a biochemical reaction, and the entry Si,j represents the coefficient of metabolite i in reaction j. If the coefficient Si,j is positive, metabolite i is produced by reaction j; if the coefficient is negative, the metabolite is consumed; and if the coefficient is zero, the metabolite is not involved in the reaction at all and thus is not in its hyperedge. An m-dimensional vector may be employed to represent concentrations of metabolites, while an n-dimensional vector may represent rates of reactions.\nIt is typically assumed that some internal metabolites are at steady state and thus must have a net zero rate of change in the system. Such a steady state corresponds to an n-dimensional flux vector ~x with the property that S~x = ~0; we call a vector satisfying this condition an admissible flux mode. We call such a flux mode elementary if its support (the set of reactions with nonzero fluxes) is inclusion-minimal.\nThe notion of elementary flux modes (\u201cEFMs\u201d) was introduced by Schuster and Hilgetag in [24]; subsequent work has developed numerous techniques from linear algebra and computational geometry to find the EFMs. see the recent surveys [25, 26] for overview of the problem, the methods and software which are used to solve it, and various applications.\nOne important area of applications of metabolic network analysis is metabolic engineering, in which the metabolic network of an organism is modified to adjust its production. In [27], Klamt and Gilles focus on blocking a target reaction through cut sets, which they define as a set of reactions whose removal from the network leaves no feasible balanced flux distribution involving the target reaction. To do this, they first compute all the elementary flux modes which involve the target reaction. They then construct a set family whose elements are the reactions of the original network and whose sets are the relevant elementary flux modes. Finally, they compute the minimal hitting sets of this family. Subsequently, Haus et al. developed in [28] a specialized version of the FK-A algorithm (cf. Section 4.2.2) which greatly improves on other methods available at the time.\nOptimal combinations of interventions in signal transduction networks Signal transduction describes the process of conversion of external signals to a specific internal cellular response, such as gene expression, cell division, or even cell suicide. This process begins at the cell membrane where an external stimulus initiates a cascade of enzymatic reactions inside the cell that typically includes phosphorylation of proteins as mediators of downstream processes. A signal transduction network is represented as a signed directed graph in which each of the vertices is a signaling component (such as a protein, gene in a cell) and each edge represents an interaction which the source induces in the target (either positive-signed activation or negative-signed inhibition). Biological signaling networks typically [29] exhibit a natural decomposition into input, intermediate, and output nodes; engineering and control of these networks then typically depends on adjusting the input and intermediate layers to obtain some outcome at the outputs. As an analogous of elementary flux modes (\u201cEFMs\u201d) in metabolic networks, in [30], Wang and Albert introduced the notion of elementary signaling modes (ESMs). ESMs are minimal sets of signaling components which can perform signal transduction from inputs to outputs; ESMs are the natural analogues of EFMs for signal transduction networks. They also provide algorithms for generating the ESMs of a given network.\nOnce the topology and ESMs of a signal transduction network are known, it may be of interest to control how signal flows from a given set of source nodes to a given set of targets, perhaps avoiding interfering with certain side effect nodes along the way. Vera-Licona et al. introduced the OCSANA framework to study this problem in [16]. They begin by computing the ESMs which pass from the specified sources to the specified targets. They then construct a set family whose elements are the signalling components and whose sets are these ESMs. They next compute MHSes of this family, which they term combinations of interventions (CIs). Finally, they apply their \u201cOCSANA\u201d scoring, which encodes heuristics about control of the targets and influence on the side effects, and use the results to identify the most promising CIs.\nSince discovery of hitting sets is a crucial step in the algorithm, the authors of [16] performed an experimental comparison. In particular, they tested Berge\u2019s algorithm (cf. Section 4.1.1) and an approach\nsimilar to MTMiner (cf. Section 4.3.1) which incorporates the OCSANA score into the generation process. They found their new algorithm to improve substantially on Berge\u2019s algorithm.\nReverse-engineering biological networks from high-throughput data In some situations, the network structure itself is not known. In this case, it is useful to reconstruct the network from experimental data through reverse engineering. Broadly, the biological reverse-engineering problem is that of \u201canalyzing a given system in order to identify, from biological data, the components of the system and their relationships\u201d [31]. This may result in either a topological representation of the network structure (typically expressed as an enriched graph) or a full dynamic model (in terms of a mathematical model). Numerous approaches to the topological reverse-engineering problem have been introduced, including at least two which use MHS generation techniques. See [31] for a comparative survey of these two approaches and the relative performance of the specialized algorithms developed for each; we give here only a brief overview of each.\nIdeker In 2000, Ideker et al. introduce a method to infer the topology of a network of gene regulatory interactions in [21]. They first perform a series of experiments in which various genes in the system are forced to high or low expression and use standard microarray techniques to measure the expression of the other genes. For each gene i, they then consider all pairs {a, b} of experiments for which that gene\u2019s expression values differ, then find the set Sa,b of all other genes whose expression values also changed.\nUnder their assumptions, one or more genes in Sa,b must cause the change in i between experiments a and b. A set of genes which intersects all the sets Sa,b is a candidate to explain the observed variation in i over the whole suite of experiments, and thus to be the set of genes connected to i in the regulatory network. Thus, they generate a collection of hitting sets for the family Sa,b; in particular, they develop an algorithm based on the standard branch and bound optimization technique which gives sets of minimal cardinality. Since this collection may be large, they use an entropy-based approach to iteratively generate new experiments which will discriminate among the candidates and re-apply the algorithm to the enlarged data set to improve the accuracy of the predicted networks.\nJarrah In 2007, Jarrah et al. introduce another method to infer the topology of a gene regulatory network in [19] which focuses on time series data within a single experiment. They associate to each gene i a variable xi from some finite field k, then represent each time point t in the experiment as an assignment xt. For a given gene i, they then consider how the values of xt determine the value xt+1i for each t.\nUnder their assumptions, if xi is observed to take different values at times t1 and t2, it must be due to some other variables being different at times t1 \u2212 1 and t2 \u2212 1. Thus, for each pair t1 6= t2 with xt1i 6= x t2 i , they construct a set St1,t2 = {j|j 6= i, x t1\u22121 j 6= x t2\u22121 j } which encodes the variables which may be responsible for the observed change in xi. Like Ideker et al., they then compute minimal hitting sets of this collection, but their algorithm is formalized in terms of computational algebra and gives a complete enumeration of the family of MHSes. (We discuss this approach in Section 4.5.1.) They also present a heuristic scoring function which may help to select the most viable model from the generated hitting sets.\nDrug cocktail development Many widely-used antibiotics are effective against some bacterial strains but ineffective against others. Thus, in cases where more than one strain may be present or the specific strain is unknown, it is necessary to deploy a \u201ccocktail\u201d of several drugs to increase the number of strains covered. A similar situation applies in cancer chemotherapy, where different chemotherapeutic agents are known to be effective only against certain cell lines. In either case, it is desirable to minimize the number of drugs used at once, to minimize the cost of the therapy and the risk of emergent multiple-drug-resistant strains.\nGiven a set of drugs (say, antibiotics) and a set of targets (say, bacterial strains), we can assign to each target the set of drugs that are effective against it. A (minimal) hitting set of this set family is then a (minimal) cocktail of drugs that, taken together, affects all the targets.\nThis application has been studied in detail by Vazquez in [17], using a greedy algorithm to search for very small effective combinations from the NCI60 collection ([32]) of 45334 drugs and 60 cancer cell lines. He is then able to recommend some of these small MHSes as targets for further research."}, {"heading": "2.5 Data mining", "text": "A number of problems in data mining can be formalized in terms of MHS generation. We survey two of them here.\n(In)frequent itemset discovery One fundamental problem in data mining, introduced by Agrawal et al. in [33] and developed further in [34], is the discovery of frequent itemsets in a database of transactions. We adopt the formal setting of the problem presented by Boros et al. in [35]. Fix a finite set A of m transactions, each of which is a finite subset of a set I of n items. Fix an integer threshold 1 \u2264 t \u2264 m. A set C of items is t-frequent if at least t transactions are supersets of C and t-infrequent if no more than t transactions are supersets of C. The maximal frequent (resp. minimal infrequent) itemset problem is to enumerate inclusion-maximal (resp. inclusion-minimal) sets C which are t-frequent (resp. t-infrequent).\nLet Ft denote the hypergraph whose edges are maximal t-frequent itemsets in A and It denote the hypergraph whose edges are minimal infrequent itemsets in A. (Both have the common vertex set I.) Any element of It must intersect the complement of every element of Ft, and in fact as hypergraphs we have that It = Tr(Ft{) exactly. Thus, if either It or Ft is known, the other can be computed using any algorithm for MHS generation. This connection is explored by Manilla and Toivonen in [36]; more algorithmic details are given by Toivonen in [37]. An application of these ideas to database privacy is given by Stavropoulos et al. in [38].\nFurthermore, so-called \u201cjoint-generation\u201d algorithms inspired by the FK algorithms of Fredman and Khachiyan [4] (cf. Section 4.2.2) can generate It and Ft simultaneously. This is developed by Gunopulos et al. in [39] and its complexity implications explored by Boros et al. in [40].\nEmerging pattern discovery Another important data mining problem is the discovery of emerging patterns, which represent the differences between two subsets of the transactions in a database. We adopt the formal setting of the problem introduced by Bailey et al. in [13]. Consider two sets A and B of transactions, where each transaction is itself a set of items. A minimal contrast is an inclusion-minimal set of items which appears in some transaction in A but no transaction in B. Fix a transaction a \u2208 A and construct a set family whose underlying elements are the items in a and with a set a \\ b for each b \u2208 B. Then the minimal contrasts supported by a are exactly the minimal hitting sets of this set family. Thus, any algorithm for MHS generation can be applied to emerging pattern discovery. Indeed, two of the algorithms we study, DL (Section 4.1.4, [12]) and BMR (Section 4.1.5, [13]), were developed for this purpose."}, {"heading": "2.6 Minimal Sudoku puzzles", "text": "The Sudoku family of puzzles is widely published in newspapers and magazines and is played by millions worldwide. An instance of Sudoku is a 9\u00d7 9 grid of boxes, a few of which already contain digits (\u201cclues\u201d) from the range 1\u20139; a solution is an assignment of digits to the remaining boxes so that each of the nine 3\u00d7 3 subgrids and each row and each column of the whole puzzle contains each digit exactly once. An example with seventeen clues is shown in Fig. 1.\nOf course, not every possible placement of clues into the grid yields a valid puzzle. There may be inherent contradictions, such as two identical clues in the same column, so that the puzzle has no solutions. There may also be ambiguities, in which more than one solution is possible. A mathematical question of particular interest, then, is: what is the smallest number of clues in an unambiguous valid Sudoku puzzle? Many such puzzles with 17 clues are known, but none with 16 have ever been identified. In [41], McGuire et al. show that an exhaustive search for such puzzles can be formulated in our terms by constructing set families that represent the effects of clues in solved puzzles and then searching for hitting sets of size 16 or less. They ran this search on a supercomputing cluster and proved conclusively that there are no 16-clue Sudoku puzzles. They use an algorithm similar to HST from [8], discussed in detail in Section 4.1.3; for speed, they modify the algorithm to essentially build the set families and their hitting sets simultaneously."}, {"heading": "3 Complexity results", "text": ""}, {"heading": "3.1 Asymptotic complexity", "text": "Before considering specific algorithms for MHS generation, we should consider the current state of knowledge about the asymptotic complexity of the problem. It has been known since Karp\u2019s seminal 1972 paper [42] that the problem of determining whether a given set family has a hitting set of size no greater than some k is NP-complete. However, we are more concerned with the collection of all hitting sets than with the existence of a single one. We therefore consider two other separate but related problems."}, {"heading": "3.1.1 Recognition problem", "text": "Much of the literature on complexity analysis focuses on \u201cdecision problems\u201d, which must have a \u201cyes\u201d or \u201cno\u201d answer. The natural decision variant of the MHS problem is recognition: given two hypergraphs H and G, to decide whether H = TrG. Fredman and Khachiyan present in [4] an algorithm (discussed in Section 4.2.2) which tests this in time no(logn) (for n the sum of the number of hyperedges in F and G). This time bound is notable in that it is quasi-polynomial\u2014it is worse than a polynomial bound, but better than an exponential bound or even certain sub-exponential bounds. The BM algorithm introduced by Boros and Makino in [43] improves on this bound in parallel cases. It is a long-standing open problem to determine whether recognition is possible in polynomial time."}, {"heading": "3.1.2 Generation problem", "text": "For many applications, however, we need to generate the MHSes of a given set family rather than recognize them. It is straightforward to show that no algorithm can do this in time polynomial in the size of the input. Consider the example of the matching graph Mn = {(1, 2), (3, 4), . . . , (2n\u2212 1, 2n)} as a set family. A minimal hitting set contains a choice of one of the two elements of each edge; thus, there are evidently 2n of them. Simply writing out this result therefore requires at least o(2n) time, so no algorithm can in subexponential time in general. This is not necessarily to say that the MHS generation problem is intractable; rather, it suggests that it is inappropriate to analyze its complexity input-polynomiality solely in terms of input size.\nJohnson et al. introduced a formalism to deal with this issue in [44]. Instead of considering only the size of the input to the problem, we can incorporate the size of the output as well. If a given set family S has n sets and m MHSes, an algorithm for generating those MHSes is said to be output-polynomial (or to run in polynomial total time) if its running time is o(poly(n+m)). Unfortunately, this is not known to be achieved by any current algorithm, and Hagen showed in [45] that several important algorithms are not output-polynomial.\nIf we shift our attention to incremental generation, we may instead ask whether an algorithm can generate one MHS at a time with reasonable delay between outputs. Johnson et al. introduced two suitable formalisms in [44]. First, a generation algorithm may run in incremental-polynomial time; in this case, given a set family\nand a set of MHSes for it, it should yield a new MHS in time polynomial in the combined size of both of these inputs. Second, the algorithm may run with polynomial delay; this stronger variant of incremental-polynomial time requires that the time required depend only on the size of the set family and not on the number of MHSes already known. Crucially, if an algorithm runs with polynomial delay, it is guaranteed to run in output-polynomial total time, but incremental-polynomial time gives no such guarantee ([44]).\nIncremental time analysis is of particular interest in some classes of applications, where we may wish to generate only some MHSes or to perform further processing on each one as it emerges. No algorithm is known to solve the MHS generation problem in incremental polynomial time (much less with polynomial delay) in general, but there are some interesting special cases, considered in Section 3.2.1."}, {"heading": "3.2 Tractable cases", "text": "Crucially, the complexity results in Section 3.1 concern the performance of algorithms in general, which is to say, for the class of all possible hypergraphs or set families. Another thread of research has focused on demonstrating that, in such restricted cases, much better complexity results are possible."}, {"heading": "3.2.1 Fixed-parameter tractability", "text": "In some cases, algorithms are available which \u201cfactor out\u201d some of the complexity of the problem with respect to a particular parameter of the hypergraphs. Specifically, letting k be the parameter of interest and n be the number of edges of the hypergraph, we may find an algorithm that generates all MHSes in time f(k) \u00b7 nO(1) for some arbitrary function f (which is to say, the time is polynomial in n once k is fixed, though it may depend arbitrarily on k). In this case, we say that the problem is fixed-parameter tractable (\u201cFPT\u201d) with respect to that parameter k, since fixing k yields a complexity function that depends polynomially on n.\nFixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]). For a more complete survey, the interested reader may consult [2, \u00a74, \u00a77]."}, {"heading": "3.2.2 Acyclicity", "text": "A graph is acyclic if it contains no cycles\u2014that is, if no non-self-repeating path in the edges leads back to where it starts. Beeri et al. introduced in [52] a notion of acyclicity in hypergraphs, now known as \u03b1-acyclicity, in the context of the study of relational database schemes. Fagin subsequently introduced in [53] the notions of \u03b2-acyclicity and \u03b3-acyclicity, which are successively more restrictive and correspond to desirable tractability problems in databases. Eiter showed that the transversal recognition is solvable in polynomial time for \u03b2-acyclic hypergraphs in [54] and for \u03b1-acyclic hypergraphs in [49]."}, {"heading": "3.3 Limited nondeterminism", "text": "Another important line of inquiry for studies of complexity is how its solution improves with nondeterminism\u2014 that is, if the algorithm is allowed access to some \u201cfree\u201d information. The crucial question is how many nondeterministic bits are required to achieve a better solution. Kavvadias and Stavropoulos showed in [55] that the recognition problem is in the class co-NP[log2 n] for n the total number of edges in H and TrH, meaning that only O ( log2 n ) nondeterministic bits are required to demonstrate that two hypergraphs are not transversals of each other. Since log2 n is subpolynomial, this suggests that the recognition problem is not as hard as the well-known NP-complete problems."}, {"heading": "4 Existing algorithms", "text": "A wide array of algorithms have been developed to generate MHSes (either explicitly or in the language of various cognate problems). If we strip away the details of the various domains and applications by casting all the algorithms in the language of MHS generation, we find that they fall naturally into a few high-level categories:\nset iteration appproaches which work through the input set family one set at a time, building MHSes as they go;\ndivide and conquer approaches which partition the input family into disjoint subfamilies, find their MHSes separately, and then combine them;\nMHS buildup approaches which build candidate MHSes one element at a time, keeping track of un-hit sets as they go; and\nfull cover approaches which improve on the divide-and-conquer approach with a technical hypergraph lemma that allows more efficient recombination.\nWe survey these categories, including discussions of a few published algorithms for each. A summary of these algorithms, giving their taxonomic classifications, original problem domains, and relevant characteristics is presented in Table 1. Whenever possible, we use the terminology of set families and minimal hitting sets, since this language is typically the most straightforward to understand.\nIt can be shown (cf. [56]) that an algorithm for the transversal hypergraph recognition problem (cf. Section 2.1) can be used to generate MHSes of a given set family with a number of runs that is polynomial in the size of the transversal hypergraph. This conversion is possible because, if the input hypergraphs H and G are not transversals of each other, any recognition algorithm must return a \u201cwitness\u201d of this, which can be translated into an edge which must be added to either H or G. Thus, beginning with some given set family S and an empty collection T of MHSes, we can interpret S as a hypergraph and apply any recognition algorithm to find a new MHS to add to T , then repeat until eventually the complete collection is generated and the algorithm confirms S = TrT . As a result, we will consider algorithms for both recognition and generation interchangeably.\nFor nearly all of these algorithms, software implementations are available to perform the calculations on a computer. We have collected eighteen of these implementations into a public repository at github.com/VeraLiconaResearchGroup/MHSGenerationAlgorithms. Source code and information about the implementations are available there. In addition, we provide a ready-to-use Docker container for each using the AlgoRun framework (cf. [57]) and a Web interface to instances of the software running on the AlgoRun project\u2019s servers at algorun.org. This framework is used for experimental benchmarks which are presented in Section 5."}, {"heading": "4.1 Set iteration approaches", "text": "One type of approach to computing hitting sets of a set family S is to begin with a small subfamily of S\u2032 ( S, find the MHSes for S\u2032, and then iteratively add more sets to S\u2032 and update the MHS collection. The methods in this section all follow this approach; they differ in how they select the subfamilies S\u2032 and in the details of how they update the MHS collection."}, {"heading": "4.1.1 Berge (1984)", "text": "The first systematic algorithm for computing transversals of hypergraphs was presented by Berge in [22], a monograph on the theory of hypergraphs. Although this algorithm is called the \u201cSequential Algorithm\u201d in some literature, we will refer to it as Berge. The core idea of the algorithm is proceed inductively over the hyperedges of the hypergraph, alternately adding a new edge to the intermediate hypergraph under consideration and extending the known transversals.\nWe first introduce three important operations on hypergraphs.\nDefinition 4.1. Let H1 = (V1, E1) and H2 = (V2, E2) be two hypergraphs. Their vee H1 \u2228 H2 is the hypergraph with vertex set V = V1 \u222a V2 and edge set E = E1 \u222a E2. Their wedge H1 \u2227H2 is the hypergraph with vertex set V = V1 \u222a V2 and edge set E = {e1 \u222a e2|e1 \u2208 E1, e2 \u2208 E2}.\nDefinition 4.2. LetH be a hypergraph with vertex set V and edge set E. Its minimization (or simplification), minH, is the hypergraph with vertices V and edges {e \u2208 E|@f \u2208 E \\ e : e \u2282 f}. In other words, minH retains exactly the inclusion-minimal edges of H. H is simple if H = minH.\nThese two operations interact nicely with the transversal construction:\nLemma 4.3. Let H1 and H2 be hypergraphs. Then the following relations hold:\nTr(H1 \u2228H2) = min(TrH1 \u2227 TrH2) (1)\nand Tr(H1 \u2227H2) = min(TrH1 \u2228 TrH2). (2)\nAlgorithm Berge then proceeds as follows:\n1. Let H be a hypergraph with edge set E = {e1, e2, . . . , en} (for an arbitrarily chosen ordering), and for each i let Hi be the subhypergraph of H with all its vertices and its first i edges e1, . . . , ei.\n2. For each i in order, compute TrHi inductively: TrH1 = {v|v \u2208 e1}, and TrHi = min(TrHi\u22121 \u2227 Tr ei) = min{t \u222a {v}|t \u2208 TrHi\u22121, v \u2208 ei} by equation (1).\n3. When the iteration is finished, TrHn = TrH by construction.\nPseudocode for the algorithm (in the lanugage of set families) is given in Algorithm 1.\nAlgorithm 1 Berge\u2019s algorithm Input: A finite set family S = {s1, s2, . . . , sn} Output: The set of MHSes of S\n1: function Berge(S) 2: T \u2190 {{e}|e \u2208 s1} 3: for all s \u2208 S \\ s1 do 4: T \u2190 {t \u222a {e}|t \u2208 T, e \u2208 s} 5: T \u2190 minT . Remove non-minimal elements of T 6: end for 7: return T 8: end function\nAs suggested in [18], Berge\u2019s algorithm can be adapted to search only for MHSes of cardinality bounded by some k by simply discarding candidates larger than k at Lines 4 and 5 in each stage of the algorithm.\nThis algorithm is straightforward to implement in code and to study theoretically. Unfortunately, it also has the potentially to be extremely inefficient. The complexity is studied thoroughly by Boros et al. in [58]. In particular, for a set family S with n sets and a total of m underlying elements, if the sets are ordered so that the collection T reaches maximum size k during the algorithm, the running time of this algorithm is O(kmn \u00b7min(m, k)).\nAccordingly, it is clear that the ordering of the edges matters a great deal, since this determines the value of k. Takata showed in [59] that there exists a family of hypergraphs for which no edge ordering yields output-polynomial running time, and thus that Berge is not output-polynomial in general, even if the edge ordering is optimal. Boros et al. demonstrate in [58], however, that the worst case is still sub-exponential.\nWe provide a C++ implementation of Berge which supports enumeration of small MHSes in the repository.\n4.1.2 Reiter (1987) and Greiner et al. (1989)\nAs discussed in Section 2.3, Reiter introduced the formal theory of model-based diagnosis as an application of MHS enumeration in [5]. His approach proceeds through a set family inductively, alternately picking a set which is not yet hit and an element which hits it, until a hitting set for the whole family is found. It then backtracks to the most recent step where another valid choice was available and repeats. The intermediate data are stored in a structure Reiter calls a \u201chitting set tree\u201d.\nHowever, it was shown by Greiner et al. in [60] that this algorithm is incomplete; the hitting sets it generates are guaranteed to be minimal, but in certain circumstances some MHSes may be missed. They\nrepair the algorithm, but in the process they sacrifice the acyclicity of Reiter\u2019s hitting set tree; the result is a directed acyclic graph. We will refer to the algorithm as HS-DAG (for Hitting Set Directed Acyclic Graph). It is straightforward to implement and is widely studied and cited in the MBD literature.\nThe authors are not aware of a formal complexity analysis of this algorithm. It is possible to search only for hitting sets of bounded cardinality with HS-DAG simply by restricting the\ndepth of the search DAG. A Python implementation of this algorithm by the authors of [7] is available in the repository."}, {"heading": "4.1.3 Wotawa (2001)", "text": "Wotawa returned to Reiter\u2019s approach in [8], reviewing the HS-DAG algorithm of [60] (see Section 4.1.2) and adjusting it to reduce the number of set containment checks required. These improvements render the DAG generalization unnecessary, so the underlying data structure is once again a hitting set tree as originally envisioned by Reiter. We will refer to the algorithm as HST (for Hitting Set Tree).\nThe authors are not aware of a formal complexity analysis of this algorithm. It is possible to search only for hitting sets of bounded cardinality with HST simply by restricting the\ndepth of the search tree. A Python implementation of this algorithm by the authors of [7] is available in the repository."}, {"heading": "4.1.4 Dong and Li (2005)", "text": "Dong and Li considered in [12] the \u201cemerging patterns problem\u201d discussed in Section 2.5. Although their work was essentially independent of the literature on hypergraph transversals, they developed an algorithm thematically very similar to Berge. Their algorithm incorporates some optimizations to the minimization calculation in equation (2) to speed up the loop step. We will refer to the algorithm as DL (for its authors).\nHowever, the running time of Berge is dominated by the need to search the intermediate transversals, not the complexity of generating them, so the DL optimization should not be expected to improve the worst-case behavior of Berge. Hagen shows in [2, 45] that Takata\u2019s time bounds on Berge in [59] apply to DL as well, so it is not output-polynomial. Nevertheless, for families with relatively few sets, DL performs well, so it is useful as a subroutine to be used in base cases of other algorithms.\nA C implementation of this algorithm by the authors of [61] is available in the repository.\n4.1.5 Bailey et al. (2003)\nContinuing with the study of emerging patterns, Bailey et al. developed in [13] an algorithm which decomposes the input set family more carefully than Berge\u2019s algorithm. Rather than simply considering one new set at a time, their approach attempts to partition the set family into components with few sets, then use the DL algorithm of [12] as a subroutine to compute their MHSes before combining them using equation (1). We will refer to their algorithm as BMR (for Bailey, Manoukian, and Ramamohanarao).\nHagen shows in [2, 45] that BMR is not output-polynomial. Furthermore, he shows that its complexity is n\u2126(log logn), where the \u2126 indicates that this is a lower bound instead of the upper bound indicated by O and where n = |H|+ |TrH|.\nA C implementation of this algorithm by the authors of [61] is available in the repository."}, {"heading": "4.1.6 Kavvadias and Stavropoulos (2005)", "text": "Returning to the explicit study of hypergraph transversals, Kavvadias and Stavropoulos introduced in [62] another algorithm, which seeks to reduce the memory requirements of Berge with two optimizations. First, they preprocess the input set family to combine elements which occur only in the same sets. Second, they carefully reorganize the processing steps so that many intermediate MHSes can be forgotten without jeopardizing the correctness of the algorithm, allowing them to output MHSes early in the algorithm\u2019s run and then discard them. We will refer to this algorithm as KS (for its authors).\nThe algorithm is designed to run in polynomial memory by avoiding regeneration of candidate hitting sets. Hagen shows in [2, 45] that KS does not run in output-polynomial time. Furthermore, he shows that its\ncomplexity is n\u2126(log logn), where n is the sum of the number of sets in the family and the total number of minimal hitting sets that it admits.\nThe organization of the search routine in KS makes it possible to search for hitting sets of bounded cardinality to save time. The authors are not aware of an implementation that offers this feature.\nA Pascal implementation of this algorithm by the authors of [62] is available in the repository."}, {"heading": "4.2 Divide and conquer approaches", "text": "Another type of approach to computing MHSes of a set family S is to partition S into several subfamilies, find their MHSes separately (perhaps recursing until the subfamilies are sufficiently small), and then combine the results. The algorithms in this section all follow this approach; they differ primarily in how they partition S."}, {"heading": "4.2.1 Lin and Jiang (2003)", "text": "Lin and Jiang return in [9] to the problem of model-based diagnosis. They cast the problem in the Boolean algebra framework, but their algorithm is a straightforward example of the divide-and-conquer approach. We will refer to this algorithm as BOOL (since Lin and Jiang call it the \u201cBoolean algorithm\u201d). Their recursive decomposition algorithm proceeds as follows.\n1. Let S be a finite set family. If |S| < 2, it is trivial to find the MHSes of S directly. Thus, we assume that |S| \u2265 2.\n2. If there is an element e which is present in every set s \u2208 S, construct a new set family S\u2032 = {s\\ e|s \u2208 S}. Recursively find the MHSes of S\u2032, add {e}, and return the result.\n3. If there is a set s \u2208 S with |s| = 1, let e be the unique element of s and construct a new set family S\u2032 = S \\ s. Recursively find the MHSes of S\u2032, add e to each, and return the result.\n4. Otherwise, choose some e \u2208 \u22c3 S arbitrarily. Let S1 = {s \\ e|s \u2208 S, e \u2208 s} and S2 = {s|s \u2208 S, e /\u2208 s}.\nRecursively find the MHSes of S1 and S2. Add e to each MHS of S2, then take the union of the results and return.\nPseudocode for the algorithm is given in Algorithm 2. They call this the \u201cBoolean algorithm\u201d; we will denote it hereafter by BOOL.\nThe Boolean algorithm was subsequently optimized by Pill and Quaritsch in [11]. In particular, improved its performance in cases that only MHSes of size bounded by some k are desired.\nThe authors are not aware of a formal complexity analysis of this algorithm. If desired, this algorithm can search for hitting sets of bounded cardinality to save time. A Python implementation of this algorithm by the authors of [7] is available in the repository."}, {"heading": "4.2.2 Fredman and Khachiyan (1996)", "text": "Fredman and Khachiyan introduced two iterative algorithms in [4] to study the recognition version of the MHS problem in the Boolean algebra context. Like BOOL, these two algorithms both proceed by choosing one element, considering sets that do and do not contain that element separately with recursive calls, and then combining the results. However, they first apply several algebraically-motivated degeneracy tests. If the tests fail, a new hitting set can be found very efficiently. If, however, they succeed, it guarantees that an element can be found which is present in many (specifically, logarithmically many) sets but missing from many others. Considering the sets which do and do not contain this element separately decomposes the problem into two large disjoint sub-problems, which can be considered recursively; the large size of each subproblem ensures that the recursion does not go too deep. This bound on the recursion depth allows Fredman and Khachian to prove running-time bounds which are the strongest known on any sequential algorithm to date. We will refer to these two algorithms as FK-A and FK-B (\u201cFK\u201d for the authors, who use the names A and B in [4]).\nThe first algorithm, FK-A, runs in time nO(log 2 n) and is relatively straightforward to implement. The algorithm is modified in [63] to improve its runtime slightly and adapt it to MHS generation. An implementation in (compiled) C is provided by those authors and is available in the repository.\nAlgorithm 2 The Boolean algorithm (BOOL) Input: A finite set family S = {s1, s2, . . . , sn} Output: The set of MHSes of S\n1: function Bool(S) 2: E \u2190 \u22c3 s\u2208S s\n3: if |S| = 0 then 4: T \u2190 \u2205 5: else if |S| = 1 then 6: let S = {s} 7: T \u2190 s 8: else if there is some e \u2208 E such that e \u2208 s\u2200s \u2208 S then 9: T \u2190 {e} \u2228Bool({s \\ e|s \u2208 S}) 10: else if there is some s \u2208 S such that |s| = 1 then 11: G \u2190 s \u2227Bool(S \\ s) 12: else 13: choose e \u2208 E . can be arbitrary 14: S1 \u2190 {s \\ e|s \u2208 S, e \u2208 s} 15: T1 \u2190 Bool(S1) 16: S2 \u2190 {s|s \u2208 S, e /\u2208 s} 17: T2 \u2190 {e} \u2227Bool(S2) 18: T \u2190 T1 \u222a T2 19: end if 20: return T 21: end function\nThe second algorithm, FK-B, runs in time nO(logn). (More exactly, its time bound is n4\u03c7(n)+O(1) where \u03c7(n)\u03c7(n) = n.) However, its implementation is significantly more complex than that of FK-A. As a result, most authors (including [63]) have disregarded FK-B in comparative studies. However, analysis in [64] suggests that this assumption may be inaccurate. The authors are aware of no publicly-available implementations of FK-B.\nIf desired, both algorithms can search for hitting sets of bounded cardinality to save time. The authors are not aware of an implementation that supports this feature.\nThe algorithms also allow for \u201cjoint generation\u201d of a set family and its MHSes if subsets of both are known. This can be advantageous in situations where the set family is not known a priori but it is possible to check whether a given set is a member of the family. For example, Haus et al. apply this approach in [28], as discussed in Section 2.4."}, {"heading": "4.2.3 Abreu and Gemund (2009)", "text": "Model-based diagnosis often involves finding hitting sets of extremely large set families, so approximation algorithms are particularly attractive in this field. Abreu and Gemund presented such an algorithm in [10]. They use a divide-and-conquer approach similar to that of BOOL, but which considers the elements in an order determined by a statistical heuristic. They also define mechanisms to stop the algorithm early to obtain a partial set of approximately minimal hitting sets. We will refer to this algorithm as STACCATO (the name used by its authors in [10]).\nThe authors of [10] claim that, for a set family with N sets andM total elements, the algorithm guarantees to find a hitting set of cardinality C in O ( (M \u00b7 (N + logM))C ) worst-case time and O(C \u00b7M) space, with\nimproved expected times based on their heuristic and tested experimentally. If desired, this algorithm can search for hitting sets of bounded cardinality to save time. A Python implementation of this algorithm by the authors of [7] is available in the repository.\n4.2.4 Leiserson et al. (2010)\nSome recent research has focused on parallelizing the search for minimal hitting sets. Leiserson et al. cast this issue in a very abstract setting in [65], developing a framework to parallelize any algorithm that searches for minimal elements of a poset and then applying it to the lattice of hitting sets of a set family. We will refer to this algorithm as ParTran (the name used by its authors in [65]).\nTreated as a sequential algorithm by running it in a single execution thread, ParTran is similar to BOOL; its primary distinction is that the two subfamilies S1 and S2 are carefully chosen to be of similar size to improve parallel efficiency. The authors are not aware of a formal analysis of its complexity in either sequential or parallel settings.\nA Cilk++ implementation of this algorithm by the authors of [65] is available in the repository."}, {"heading": "4.2.5 Knuth (2011)", "text": "Binary decision diagrams (BDDs) are a graph-based structure for representing boolean functions and hypergraphs originally introduced by Bryant in [66]. Given a set family S, it is computationally expensive to compress S into a BDD or to decompress that BDD back into S. Nevertheless, BDDs are a powerful data structure for certain combinatorial algorithms. Many logical operations on hypergraphs, such as the \u2227 and \u2228 operations of Definition 4.1, are inexpensive to perform on their BDDs. In exercises 236 and 237 of [67, \u00a77.1.4], Knuth asks the reader to devise an algorithm for MHS generation using BDD operations, and in the solutions he presents a simple one. We will refer to this algorithm as Knuth.\nThe authors are not aware of a formal complexity analysis of KNUTH, and Knuth asserts that the worst-case runtime is unknown.\nA C implementation of this algorithm by the author of [68] is available in the repository."}, {"heading": "4.2.6 Toda (2013)", "text": "In 2013, Toda improved on the KNUTH algorithm in [68] by incorporating a variation on the BDD data structure\u2013the zero-suppressed binary decision diagram (ZDD). After compressing a given set family S into a ZDD, Toda recursively applies a simple divide-and-conquer algorithm to obtain a BDD of all hitting sets of S. He then uses a minimization algorithm to obtain a ZDD of the MHSes of S, which he finally decompresses. We will refer to this algorithm as HTC-BDD (the name given to it by Toda).\nToda gives a formal complexity analysis of HTC-BDD in [68], but the resulting bounds are expressed in terms of the intermediate BDD and ZDD data structures and are incommensurable with bounds like those known for FK-A and FK-B. One important factor is that the decompression of the output from ZDD format into a list of sets can be very time-consuming. Details are explored in Section 5.3. However, the ZDD intermediate data structure makes it possible to determine the number of MHSes without decompressing, which may be of interest for some applications.\nA C implementation of this algorithm by the author of [68] is available in the repository."}, {"heading": "4.2.7 Cardoso and Abreu (2014)", "text": "Cardoso and Abreu revisted the STACCATO approach in [6]. They present several optimizations to reduce wasted computation. In addition, their new algorithm is distributed using the widely-used Map-Reduce paradigm, so in principle it can be deployed over very large message-passing distributed computing systems. It is also designed so that early termination will return a useful approximate result; a collection of hitting sets will be obtained, although they may not be minimal and some may be missing. We will refer to this algorithm as MHS2 (the name given to it by its authors).\nThe authors are not aware of a formal analysis of the complexity of MHS2. A C++ implementation of the algorithm by the authors of [6] is available in the repository."}, {"heading": "4.3 MHS buildup approaches", "text": "A third type of approach to computing MHSes of a set family S is to construct sets of elements which are expected or guaranteed to be subsets of MHSes, then iteratively add elements until they are hitting sets.\nThis approach fits into the standard scheme of \u201cbacktracking\u201d combinatorial algorithms. The approaches in this section all follow this approach; they differ primarily in the conditions used to identify candidate sub-MHSes and the strategies used to avoid redundant calculation.\n4.3.1 H\u00e9bert et al. (2007)\nH\u00e9bert et al. take an approach in [69] that brings insights from data mining to bear on the MHS generation problem. We follow the explanation of the algorithm in [70], which avoids the algebraic complexity of the original. We will refer to this algorithm as MTMiner (the name given to its software implementation by its authors); it is also called HBC (for H\u00e9bert, Bretto, and Cr\u00e9milleux) in some literature (e.g. [70]).\nFix a set family S = {s1, s2, . . . , sn} with underlying element set E = \u22c3 S = {e1, e2, . . . , em}. The MTMiner algorithm is initialized with the set C1 = {{e}|e \u2208 E} of element sets of size 1. At each step of the algorithm, the set Ci of candidate hitting sets of size i is processed. First, any set in Ci which is a hitting set is removed and outputted; as will be seen, it is guaranteed to be minimal. The remaining sets in Ci are extended by combining all pairs (a, b) which overlap in i\u2212 1 elements into their union a\u222a b. For each of these extended sets (of size i+ 1), the algorithm checks whether more sets are hit by a\u222a b than by a or b. If so, a\u222a b is added to Ci+1. The algorithm terminates no later than i = n, by which time all MHSes have been output.\nPseudocode of the algorithm is given in Algorithm 3.\nAlgorithm 3 MTMiner algorithm Input: A family of sets S = {s1, s2, . . . , sn} Output: The set of MHSes of S\n1: function MTMiner(S) 2: C1 \u2190 \u2205 . initial candidate set 3: for all e \u2208 \u22c3 S do\n4: if e \u2208 s for all s \u2208 S then 5: output {e} 6: else 7: C1 \u2190 C1 \u222a {e} 8: end if 9: end for 10: i \u2190 1 . Size of candidates under consideration 11: while Ci 6= \u2205 do 12: Ci+1 \u2190 \u2205 . candidates of size i+ 1 13: for all a, b \u2208 Ci such that |a \u222a b| = i+ 1 do 14: c \u2190 a \u222a b 15: if c \\ {e} \u2208 Ci and c \\ {e} hits fewer sets than c for all e \u2208 c then 16: if c is a hitting set of S then 17: output c 18: else 19: Ci+1 \u2190 Ci+1 \u222a {c} 20: end if 21: end if 22: end for 23: i \u2190 i+ 1 24: end while 25: end function\nThe authors claim a running time bound of O(2x \u00b7 y) where x is the size of the largest hitting set and y is the number of hitting sets of S. However, Hagen shows in [70] that this bound is incorrect. He shows that MTMiner is not output-polynomial and that its complexity is n\u2126(log logn), where n = |S|+ y.\nIt is possible to search only for MHSes of bounded cardinality with MTMiner by discarding any candidate that is too large. The second author and collaborators apply this approach as a \u201cgreedy algorithm\u201d in [16] to\nstudy minimal interventions in a biochemical signalling network. They take a different approach to element search than that in Lines 13 and 14; they instead loop over all candidate sets a of a given size and consider a \u222a {e} for every element e /\u2208 a which does not form a singleton hitting set. They also consider sets and elements in orders determined by a heuristic score called OCSANA to optimize the quality of approximate results in cases where complete enumeration is infeasible.\nA C++ implementation of this algorithm by the authors of [69] is available in the repository."}, {"heading": "4.3.2 Murakami and Uno (2014)", "text": "Murakami and Uno take a somewhat different approach in [61] in two new algorithms. We will refer to these algorithms as MMCS and RS (the names given to them by their authors). Both rely on a crucial observation which makes possible efficient bottom-up searches for minimal hitting sets.\nFirst, we require two definitions. For a given family of sets S, a sub-MHS is a set M which is a subset of some MHS of S. For a given set E of elements of S, an element e \u2208 E is critical in E if there is at least one set s \u2208 S which contains e but no other elements of E.\nThen we have the following proposition, appearing in various forms in cf. [69, 61]:\nProposition 4.4. A set M of elements of a set family S is a sub-MHS if and only if every m \u2208M is critical in M . In this case, we say that M satisfies the minimality condition.\nThus, the MHSes of a set family are exactly the maximal element sets satisfying the minimality condition. Both algorithms MMCS and RS proceed by building up sets that satisfy the minimality condition until they are hitting sets, making clever use of intermediate data structures to ensure that no redundant checks are performed.\nLet k = \u2016S\u2016 be the sum of the sizes of the sets in a set family S. Then MMCS runs in O(k) time per iteration of its main loop, but the authors of [61] do not give bounds for the number of iterations required.\nFor RS, each iteration also takes O(k) time, but the number of iterations can be bounded explicitly: it is O( \u2211 yi) for yi the number of MHSes of the subfamily S\u2264i = {s1, . . . , si}. Thus, the total running time is\nO(k \u00b7 \u2211 yi).\nIt is possible to search only for MHSes of bounded cardinality with MMCS or RS by simply discarding any candidate that is too large. Furthermore, it is straightforward to parallelize the algorithm using the task model. However, the shd program distributed by the authors of [61] does not support either of these modes.\nA C implementation of MMCS and RS by the authors of [61] is available in the repository. A C++ implementation of the parallel versions pMMCS and pRS which supports efficient enumeration of small MHSes is also included."}, {"heading": "4.4 Full cover approaches", "text": "A fourth type of approach to computing the MHSes of a set family S is to decompose the underlying elements into several subsets such that every set in S lies entirely in one of them. Formally, a full cover of S is another set family C with the property that every s \u2208 S is a subset of some c \u2208 C.\nFor any dual cover, we have the following decomposition result, given in cf. [43, Lemma 3], which we express in the algebraic language of hypergraphs:\nLemma 4.5. Let H be a simple hypergraph and let C be a full cover of H. Then the transversal hypergraph TrH of H satisfies\nTrH = \u2227 c\u2208C Tr(Hc). (3)\nwhere \u2227 is the wedge operation defined in Definition 4.1 and Hc is the subhypergraph of H containing only the edges that are subsets of c.\nGiven a set family S and a full cover C of S, we can use equation (3) to break down the dualization computation into several independent computations which can be run in parallel, then merge the results using the hypergraph wedge operation. (Each of these computations can in turn be decomposed recursively.)\nThere are two easy-to-find full covers of any set family S: the family S itself and the singleton family { \u22c3 S}. The approaches given below use more refined full covers to ensure that the recursion of Lemma 4.5 is efficient. Pseudocode of this approach is given in Algorithm 4 in the language of set families.\nAlgorithm 4 Full cover algorithm Input: A set family S = {s1, s2, . . . , sn} and a full cover C of S Output: The set of MHSes of S\n1: function FullCoverDualize(S,C) 2: for all c \u2208 C do . can be considered in parallel 3: Sc \u2190 min({s|s \u2286 c}) 4: Cc \u2190 some full cover of Sc . details vary by algorithm 5: Tc \u2190 FullCoverDualize(Sc, Cc) 6: end for 7: T \u2190 min ( { \u22c3 c\u2208C tc|tc \u2208 Tc} ) . hypergraph wedge 8: return T 9: end function\nOf course, the efficiency of this algorithm depends on the choice of full cover in Line 4. In particular, the procedure to choose this full cover C should have three properties:\n1. C should have many components, to spread the load over many processors;\n2. the individual computations FullCoverDualize(Sc, Cc) should be substantially smaller in scale than the full computation, so no one processor has too much work to do; and\n3. the merge operation in Line 7 (and in particular the minimization step) should not be too complex, so the sequential part of the algorithm does not dominate the running time.\nSeveral published algorithms fit into this scheme; they differ primarily in how they approach the construction of C.\n4.4.1 Khachiyan et al. (2007)\nKhachiyan et al. introduced the full cover decomposition approach in [50, 46]. They focus on hypergraphs with a curious property: the restriction of the hypergraph to any vertex subset V \u2032 admits a full cover with the property that each covering edge has size less than (1\u2212 )|V \u2032| for a fixed threshold parameter 0 < < 1. They show that using such a collection of full covers in Lemma 4.5 yields an efficient recursive algorithm. They then are able to show that this procedure runs in polylogarithmic time on polynomially many processors, with coefficients determined by the value of . We will refer to this algorithm as pKBEG (for the Parallel algorithm of Khachiyan, Boros, Elbassioni, and Gurvich).\nOf course, this analysis only applies if such a family of full covers can be found. They demonstrate constructions (and give explicit values of ) for several important families: hypergraphs with bounded edge size (\u201cdimension\u201d), bounded \u201cdual-conformality\u201d (a condition related to intersections in the transversal), or bounded edge-transversal intersection size.\nThe authors are not aware of a public implementation of this algorithm. Since it does not apply in generality, we will not study it in Section 5."}, {"heading": "4.4.2 Elbassioni (2008)", "text": "Following up on [50, 46], Elbassioni presents in [71] two parallel decomposition approaches for the transversal recognition problem. The first is essentially a rearrangement of FK-B to make the search tree broader and shallower so parallel computation is efficient. The second is a variant of a full-cover decomposition algorithm; given a transversal T of a hypergraph H with vertex set V , it uses\nC(T ) = {V \\ {i}|i \u2208 T} \u222a {T} (4)\nas a full cover of TrH to decompose the problem. (He also incorporates a special divide-and-conquer case similar to the FK algorithms under certain circumstances.) We will refer to this algorithm as pELB (for the Parallel algorithm of Elbassioni).\nElbassioni shows that this algorithm runs in polylogarithmic time on quasipolynomially many processors in polynomial space for any hypergraph; in particular, letting n be the number of vertices, x be the number of edges of H, and y be the size of TrH, the running time is bounded by both n2xo(log y) and n2yo(log x), so any asymmetry in the sizes of H and TrH reduces the runtime. (The exact bounds are cumbersome to state but may be found in [71].)\nThe authors are not aware of any public implementation of this algorithm."}, {"heading": "4.4.3 Boros and Makno (2009)", "text": "Boros and Makino present in [43] a full cover algorithm which improves on the asymptotic complexity bounds of [71] for transversal recognition. To do this, they introduce another full cover in addition to that in equation (4), which they incorporate into an FK-like recursive duality-testing framework. Fix a hypergraph H and an edge e \u2208 H; then\nC(e) = {(V \\ f) \u222a {i}|f \u2208 H, i \u2208 f \u2229 e} (5)\nis a full cover of TrA. By carefully choosing when to use a full cover from equation (4) or equation (5), Boros and Makino are able to obtain very strong bounds on parallel runtime. We will refer to this algorithm as pBM (for the Parallel algorithm of Boros and Makino).\nFix a hypergraph H with n vertices and x edges for which TrH has y edges. Then pBM runs in O(logn+ log x log y) time using O ( nxy1+log x ) processors.\nA C++ implementation of this algorithm for MHS generation, written by the first author, is available in the repository."}, {"heading": "4.5 Other", "text": "Some authors have used approaches that translate the MHS generation problem into other domains for which specialized algorithms already exist. We outline these below."}, {"heading": "4.5.1 Primary decomposition of squarefree monomial ideals", "text": "The MHS generation problem can be translated into a problem in computational algebra. Fix a set family S = {s1, s2, . . . , sn} with underlying element set E = \u22c3 i si = {e1, . . . , em} To each element ei associate a\nvariable xi in a polynomial ring over Q. To each set si, associate a monomial mi = \u220f ej\u2208si xj . (For example, the set {1, 2, 5} becomes the monomial x1x2x5). We can then construct a monomial ideal IS generated by the monimials ms, which encodes the set family algebraically. By construction, IS is squarefree. It then turns out that the generators of the associated primes of IS correspond exactly to the minimal hitting sets of H. We will refer to this approach as PrimDecomp.\nThis approach was used by Jarrah et al. in [19] for an application in computational biology. They calculate the associated primes of IS using Alexander duality [72] as provided in Macaulay2 [73].\nA container which uses Macaulay2 to perform this calculation is provided in the repository."}, {"heading": "4.5.2 Integer programming", "text": "The MHS generation problem can be interpreted as an integer programming problem. Fix a set family S = {s1, s2, . . . , sn} with underlying element set E = \u22c3 i si = {e1, . . . , em} We declare n variables xi, each of which may take values from {0, 1}. A subset T of the vertices then corresponds to an assignment x of the x-variables. For each set si, we impose a constraint \u2211 ej\u2208s xj \u2265 1; an assignment x corresponds to a hitting set if it satisfies all these constraints. Enumeration of inclusion-minimal assignments that satisfy the constraints is then exactly the MHS generation problem. (Indeed, it was shown by Boros et al. in [74] that MHS generation is equivalent to the general problem of enumerating minimal solutions to the linear system Ax = b for 0 \u2264 x \u2264 c where A is a binary matrix, x is a binary vector, and b and c are all-ones vectors.) We will refer to this approach as IntProg.\nBecause linear programming solvers are so diverse and many widely-used ones are proprietary, we do not provide an implementation of this approach."}, {"heading": "4.6 Feature comparison", "text": "We summarize in Table 1 the salient features of the algorithms introduced in Section 4."}, {"heading": "4.7 Algorithm miscellany", "text": "A genetic algorithm for finding many (but not necessarily all) small (but not necessarily minimal) hitting sets is studied by Li and Yunfei in [15]. Vinterbo and \u00d8hrn study in [14] the more refined problem of finding weighted r-approximate hitting sets, which are sets which hit some fraction 0 \u2264 r \u2264 1 of the target sets according to assigned weights; they also apply a genetic algorithm with promising results.\nJelassi et al. consider the efficacy of pre-processing methods in [75]. They find that, for many common classes of set families, it is worthwhile to compute from the family S a new family S\u2032 which combines elements\nwhich occur only in the same sets into so-called generalized nodes. (This optimization was also used by Kavvadias and Stavropoulos in [62] for their algorithm KS.) Their algorithm Irred-Engine performs this preprocessing, applies a known MHS algorithm (in their case, MMCS from [61]) to the resulting family S\u2032, and then expands the results into MHSes for the original S. We will not study this approach separately here, but it may be of interest for applications where many vertices may be redundant."}, {"heading": "5 Time-performance comparison of the algorithms", "text": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13]. However, we find that there is need for a new, comprehensive survey for several reasons:\n1. Each published comparison involves only a few algorithms, and differences in data sets and environment make the results incompatible. Thus, it is not possible to assemble a systematic overview of the relative performance of these algorithms.\n2. Many existing comparisons overlook published algorithms in domains far from the authors\u2019 experience. An algorithm designed for monotone dualization may prove to be useful for data mining, for example, but authors in that field may be unaware of it due to translational issues in the literature.\n3. Most existing comparisons are not published alongside working code and do not provide methodological details so that the results can be reproduced or extended. (Murakami and Uno\u2019s work in [61] is a notable exception, and indeed their publicly-available implementations are used for several algorithms here.)"}, {"heading": "5.1 Methodology", "text": "We have assembled a repository of software implementations of existing algorithms. Each is wrapped in a Docker container using the Algorun framework and using standardized JSON formats for input and output. Details, code, and containers are available from https://github.com/VeraLiconaResearchGroup/ MHSGenerationAlgorithms, including complete instructions for reproducing the experimental environment and running new experiments. These containers are easy to deploy on any computer supporting the Docker container environment; they do not require compiling any code or downloading libraries. Interested readers are encouraged to run similar experiments on their own data sets.\nWe have run each implemented algorithm on a variety of input set families (discussed in detail in Section 5.2). Each was allowed to run for up to one hour (3600 s) before termination; at least one algorithm ran to termination on every data set with this timeout. Algorithms which did not time out were run a total of three times and the median runtime used for analysis, presented in Section 5.3. Algorithms which support cutoff enumeration (that is, finding only hitting sets of size up to some fixed c) were run with c = 5, 7, and 10 as well as full enumeration. Algorithms which support multiple threads were run with t = 1, 2, 4, 6, 8, 12, and 16 threads. All experiments were performed on a workstation with an Intel Xeon E5-2630v3 processor with eight cores at 2.4 GHz (with Hyperthreading enabled, allowing 16 concurrent threads) and 32 GB of ECC DDR4 RAM.\nIn all cases, the generated hitting sets were compared to ensure that the algorithms were running correctly. This revealed errors in several published implementations, which are discussed in Section 4 and Table 1. In cases where an algorithm\u2019s results are only slightly incorrect, we have included its benchmark timing in the results below, since we believe these still give a useful impression of the relative performances of these algorithms."}, {"heading": "5.2 Data sets used for time-performance comparison", "text": "We apply each algorithm to a variety of set families derived from real-world data. We briefly discuss each data set here. We have focused on data sets that provide large, heterogenous set families, since these cases highlight the performance differences among algorithms; for smaller families, the differences may be negligible in practice.\naccident Anonymized information about several hundred thousand accidents in Flanders during the period 1991\u20132000. Originally published in [76]. Converted by the authors of [61] into a set family whose sets are the complements of maximal frequent itemsets with specified threshold 1000\u03b8 for \u03b8 \u2208 {70, 90, 110, 130, 150, 200}; MHSes of this set family then correspond to minimal infrequent itemsets. All of the set families have 441 underlying elements; numbers of sets range from 81 (for \u03b8 = 200) to 10968 (for t = 70). This formulation was downloaded from [77].\necoli Metabolic reaction networks from E. coli. Reaction networks for producing acetate, glucose, glycerol, and succinate, along with the combined network, were analyzed to find their \u201celementary modes\u201d using Metatool [78], which are given as set families. MHSes of these set families correspond to \u201cminimal cut sets\u201d of the original networks, which are of interest in studying and controlling these networks. Statistics for these set families are given in Table 2.\nocsana Interventions in cell signalling networks. Two cell signalling networks (EGFR from [79] and HER2+ from [20]) were analyzed to find their \u201celementary pathways\u201d using OCSANA, which are given as set families. MHSes of these set families correspond to \u201coptimal combinations of interventions\u201d in the original networks, which are of interest in studying and controlling these networks. Each network has been preprocessed to find these elementary pathways using three different algorithms of increasing resolution: shortest paths only (SHORT), including \u201csuboptimal\u201d paths (SUB), and including all paths up to length 20 (ALL). This results in six set families. Statistics for these families are given in Table 3."}, {"heading": "5.3 Results", "text": "We present below the results of the benchmarking experiments on the data sets described in Section 5.2. All experiments were performed on a workstation with an Intel Xeon E5-2630v3 processor with eight cores at 2.4 GHz (with Hyperthreading enabled, allowing 16 concurrent threads) and 32 GB of ECC DDR4 RAM. Each algorithm was allowed to run for up to 3600 s; algorithms that did not complete in this time are marked with \u2013, while algorithms that crashed due to memory exhaustion are marked with !."}, {"heading": "5.3.1 Full enumeration", "text": "We first consider the general problem of enumerating all MHSes of a given set family. Timing results for the full enumeration cases are given in Tables 4 to 6, with algorithms sorted in approximately increasing order of speed."}, {"heading": "5.3.2 Multithreaded full enumeration", "text": "Although many of the published algorithms are serial, a few can be parallelized. For the algorithms for which multithreaded implementations were available, we have run tests with t \u2208 {1, 2, 4, 6, 8, 12, 16} threads on our workstation with eight true cores and Hyperthreading support. Timing results for selected full enumeration cases with various numbers of threads are shown in Tables 7 and 8."}, {"heading": "5.3.3 Cutoff enumeration", "text": "In many applications, only small MHSes are relevant. We consider here the enumeration of MHSes of size no greater than some \u201ccutoff\u201d c; we have run benchmarks for c \u2208 {5, 7, 10} using the algorithms which support cutoff mode. Timing results for selected cutoff enumeration cases are given in Tables 9 and 10."}, {"heading": "5.4 Discussion", "text": "As shown in Section 5.3, the algorithms MMCS and RS from [61] and HTC-BDD from [68] are far faster than their competitors across a variety of input set families.\nHTC-BDD is extremely fast on inputs for which it terminates, outperforming its closest competitors by a factor of 4 to 10 on many inputs. However, it frequently exhausted the 32GB available memory on our workstation. In addition, it does not support cutoff enumeration. Thus, we recommend HTC-BDD for situations where all the MHSes of moderately-sized set families must be found quickly\u2014for example, when many such families must be processed. Since the core algorithm takes a ZDD representation of the input set family and returns either a BDD or a ZDD of its hitting sets, it is also very suitable for processing pipelines where BDDs are already used.\nMMCS and RS are also very fast, and they support both cutoff1 and full enumeration. They have the additional benefit of very low memory requirements\u2014in principle, the space required for a run depends only on the size of the input set family. This is especially useful for inputs like HER2+.SHORT where S is small (\u2248 500 sets) but has an enormous collection of MHSes (\u2248 128 million). Thus, we recommend these algorithms for situations where very large set families are studied or where only the small MHSes are required.\nWe note, however, that the provided implementations (both those by Murakami and Uno and by the first author) store the result MHSes in memory before writing them to disk, which did result in memory exhaustion for some inputs in our experiments. It would be straightforward to modify the implementations of MMCS or RS to stream the result MHSes to disk rather than storing them in memory or to count them without storing them at all, as we did to compute the number of MHSes of size \u2264 10 for HER2.short and HER2.all in Table 9. In addition, the implementations mmcs and rs of Uno and pMMCS and pRS of the first author varied dramatically in performance depending on the input, highlighting the importance of implementation. Researchers planning to use any of these algorithms should certainly benchmark all the available implementations on data drawn from their application before adopting one.\nWe also find that parallel algorithms for MHS generation can be highly effective. For example, the MHS2 algorithm (cf. Section 4.2.7) of Cardoso and Abreu [6] shows a 2.33\u00d7 improvement in running time\n1Supported by the first author\u2019s implementations pMMCS and pRS.\nA lg or ith\nm ac\nci de\nnt (t hr es ho\nld \u03b8 in\nth ou\nsa nd\ns of\nin ci de\nnt s,\nsm al le r \u03b8 gi ve s la rg er\nda ta\nse t)\n\u03b8 =\n20 0\n\u03b8 =\n15 0\n\u03b8 =\n13 0\n\u03b8 =\n11 0\n\u03b8 =\n90 \u03b8\n= 70\n\u03b8 =\n50 \u03b8\n= 30\nm m cs\n0. 00\n0. 01\n0. 02\n0. 06\n0. 23\n0. 66\n2. 28\n20 .2 6 rs 0. 00 0. 01 0. 03 0. 06 0. 23 0. 65 2. 27 19 .8 4 pM M C S 0. 01 0. 02 0. 03 0. 06 0. 22 0. 56 1. 82 17 .3 2 pR S 0. 01 0. 02 0. 03 0. 07 0. 26 0. 75 3. 05 46 .8 1 m tm in er 0. 01 0. 02 0. 04 0. 08 0. 32 0. 94 3. 45 28 .5 9 bm r 0. 01 0. 05 0. 10 0. 18 1. 40 3. 44 13 .6 0 10 8. 56 ht cb dd 0. 38 0. 39 0. 46 0. 44 0. 59 0. 83 ! ! kn ut h 0. 32 0. 35 0. 38 0. 41 0. 73 1. 34 ! ! m hs 2 0. 01 0. 04 0. 12 0. 29 2. 67 14 .7 2 12 5. 86 \u2013 dl 0. 01 0. 07 0. 23 0. 70 3. 06 17 .3 8 14 6. 44 27 63 .5 9 fk abe gk 0. 26 1. 12 2. 88 7. 08 64 .2 3 32 1. 60 22 71 .2 2 ! bo ol 0. 04 0. 24 1. 26 3. 08 11 1. 49 49 3. 72 \u2013 \u2013 hs t 0. 12 2. 73 12 .2 6 54 .8 0 28 3. 95 18 55 .8 3 \u2013 \u2013 pr im de co m p 0. 53 1. 19 3. 66 7. 38 27 2. 63 88 8. 39 \u2013 ! hs da g 0. 26 2. 11 11 .9 1 41 .3 0 23 44 .9 8 \u2013 \u2013 \u2013 be rg e 0. 38 6. 71 52 .7 1 29 0. 00 \u2013 \u2013 \u2013 \u2013 pa rt ra n 0. 84 16 .4 6 16 7. 95 72 7. 06 \u2013 \u2013 \u2013 \u2013 pb m 3. 20 17 58 .7 1 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 st ac ca to 45 .5 9 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 |v er tic es | 64 64 81 81 33 5 33 6 33 6 44 2 |e dg es | 81 44 7 99 0 20 00 43 22 10 96 8 32 20 7 13 54 39 |M H Se s| 25 3 10 39 19 16 35 47 76 17 17 48 6 47 13 7 18 52 18\nTa bl e 4:\nC om\npu ta tio\nn tim\ne (in\ns) to\nen um\ner at e al lM\nH Se\ns fo r\nac ci\nde nt\nw ith\nva rio\nus cu\nto ff va lu es \u03b8\n(\u2013 in di ca te s tim\neo ut . ! in di ca te s m em\nor y ex ha\nus tio\nn. )\nA lg or ith\nm HE R2 sh or t\nal l\nc =\n5 c\n= 7\nc =\n10 c\n= 5\nc =\n7 c\n= 10\npM M C S\n0. 10\n1. 08\n! 5.\n71 64 .8\n9 !\npR S\n0. 08\n2. 23\n! 1.\n88 94 .6\n9 !\nm hs 2\n64 9.\n81 \u2013\n\u2013 \u2013\n\u2013 \u2013\nbo ol\n1. 23\n42 .1\n8 !\n20 1.\n71 ! ! hs t 26 .9 3 \u2013 \u2013 10 .0 0 \u2013 \u2013 hs da g 0. 84 12 18 .2 3 \u2013 2. 21 66 6. 13 \u2013 be rg e 5. 18 \u2013 \u2013 42 .0 2 \u2013 \u2013 st ac ca to \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 |v er tic es | 12 4 12 4 12 4 32 0 32 0\n32 0\n|e dg\nes |\n53 4\n53 4\n53 4\n69 80 5\n69 80 5\n69 80 5\n|M H Se\ns| 88\n26 43 6\n87 44 33 3\n40 18 92\n28 53 02 6\nTa bl e 9:\nC om\npu ta tio\nn tim\ne (in\ns) to\nen um\ner at e al lM\nH Se\ns up\nto siz\ne c fo r\noc sa\nna -H\nER 2+\nw ith\npa th -fi nd\nin g st ra te gi es\nsh or\nt an\nd al l (\u2013 in di ca te s tim eo ut . ! in di ca te s m em or y ex ha us tio n. )\nA lg or ith\nm ec\nol i\nac et\nat e\nco mb\nin ed\nc =\n5 c\n= 7\nc =\n10 c\n= 5\nc =\n7 c\n= 10\npM M C S\n0. 01\n0. 01\n0. 03\n0. 40\n1. 90\n17 .9 4 pR S 0. 01 0. 03 0. 10 0. 27 4. 12 11 8. 49 m hs 2 1. 32 12 .1 4 78 .9 3 87 0. 36 ! ! bo ol 0. 02 0. 09 0. 73 21 .4 5 19 8. 68 ! hs t 0. 02 8. 23 \u2013 8. 92 ! ! hs da g 0. 02 0. 55 45 .9 4 6. 96 54 6. 50 ! be rg e 0. 20 0. 93 3. 80 28 0. 45 \u2013 \u2013 st ac ca to \u2013 \u2013 \u2013 ! ! ! |v er tic es | 10 3 10 3 10 3 11 0 11 0 11 0 |e dg es | 26 6 26 6 26 6 27 50 3 27 50 3 27 50 3 |M H Se s| 39 19 5 73 5 93 7 92 12 49 06 1\nTa bl e 10 : C om\npu ta tio\nn tim\ne (in\ns) to\nen um\ner at e al lM\nH Se\ns up\nto siz\ne c fo r\nec ol\ni ne\ntw or ks\nac et\nat e an\nd co\nmb in ed (\u2013 in di ca te s tim eo ut . ! in di ca te s m em or y ex ha us tio n. )\nwhen passing from one thread to eight on the ecoli-acetate set family, while the first author\u2019s parallel implementation pMMCS of the MMCS algorithm (cf. Section 4.3.2) of Murakami and Uno [61] shows a 4.04\u00d7 speedup when passing from one thread to eight on the ecoli-combined set family. Unfortunately, the first author\u2019s implementation of the BM full-cover-based parallel algorithm (cf. Section 4.4.3) of Boros and Makino [43] was too slow to yield useful results."}, {"heading": "6 Conclusion", "text": "In this paper, we have surveyed the history and literature concerning the problem of generating minimal hitting sets. The computational complexity of this task is a long-standing open problem. However, since many applications (cf. Section 2) depend on generating MHSes, a variety of algorithms (cf. Section 4) have been developed to solve it across numerous pure and applied research domains.\nWe have presented extensive benchmarks (cf. Section 5.3) comparing the computation time required by nearly two dozen of these algorithms on a variety of inputs derived from real-world data. These experiments consistently show that the MMCS and RS algorithms (cf. Section 4.3.2) of Murakami and Uno [61] and the HTC-BDD algorithm (cf. Section 4.2.6) of Toda [68] are far faster than other available algorithms across a variety of inputs. We have provided our benchmarking framework and code in easy-to-install Docker containers (cf. Section 5.1), so researchers wishing to analyze the performance of these algorithms on their own inputs can do so easily. Further details are available on our software repository at https: //github.com/VeraLiconaResearchGroup/MHSGenerationAlgorithms."}], "references": [{"title": "Computational aspects of monotone dualization: A brief survey", "author": ["T. Eiter", "K. Makino", "G. Gottlob"], "venue": "Discrete Applied Mathematics 156 (11) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithmic and computational complexity issues of MONET", "author": ["M. Hagen"], "venue": "Dr. rer. nat., Friedrich-Schiller- Universit\u00e4t Jena ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient read-restricted monotone CNF/DNF dualization by learning with membership queries", "author": ["C. Domingo", "N. Mishra", "L. Pitt"], "venue": "Machine learning 37 (1) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "On the complexity of dualization of monotone disjunctive normal forms", "author": ["M.L. Fredman", "L. Khachiyan"], "venue": "Journal of Algorithms 21 (3) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "A theory of diagnosis from first principles", "author": ["R. Reiter"], "venue": "Artificial intelligence 32 (1) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1016}, {"title": "Mhs2: A map-reduce heuristic-driven minimal hitting set search algorithm", "author": ["N. Cardoso", "R. Abreu"], "venue": "in: J. a. M. Louren\u00c3\u011fo, E. Farchi (Eds.), Multicore Software Engineering, Performance, and Tools, Vol. 8063 of Lecture Notes in Computer Science, Springer Berlin Heidelberg", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A variant of reiter\u2019s hitting-set algorithm", "author": ["F. Wotawa"], "venue": "Information Processing Letters 79 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1016}, {"title": "The computation of hitting sets: Review and new algorithms", "author": ["L. Lin", "Y. Jiang"], "venue": "Information Processing Letters 86 (4) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1016}, {"title": "A", "author": ["R. Abreu"], "venue": "J. van Gemund, A low-cost approximate minimal hitting set algorithm and its application to model-based diagnosis., in: SARA, Vol. 9", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "T", "author": ["I. Pill"], "venue": "Quaritsch, Optimizations for the boolean approach to computing minimal hitting sets., in: ECAI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining border descriptions of emerging patterns from dataset pairs", "author": ["G. Dong", "J. Li"], "venue": "Knowledge and Information Systems 8 (2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast algorithm for computing hypergraph transversals and its application in mining emerging patterns", "author": ["J. Bailey", "T. Manoukian", "K. Ramamohanarao"], "venue": "in: Proceedings of the Third IEEE International Conference on Data Mining, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Minimal approximate hitting sets and rule templates", "author": ["S. Vinterbo", "A. \u00d8hrn"], "venue": "International Journal of approximate reasoning 25 (2) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1016}, {"title": "Computing minimal hitting sets with genetic algorithm", "author": ["L. Li", "J. Yunfei"], "venue": "Tech. rep., DTIC Document ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "OCSANA: optimal combinations of interventions from network analysis", "author": ["P. Vera-Licona", "E. Bonnet", "E. Barillot", "A. Zinovyev"], "venue": "Bioinformatics 29 (12) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal drug combinations and minimal hitting sets", "author": ["A. Vazquez"], "venue": "BMC systems biology 3 (1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Computing complex metabolic intervention strategies using constrained minimal cut sets", "author": ["O. H\u00e4dicke", "S. Klamt"], "venue": "Metabolic engineering 13 (2) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Reverse-engineering of polynomial dynamical systems", "author": ["A.S. Jarrah", "R. Laubenbacher", "B. Stigler", "M. Stillman"], "venue": "Advances in Applied Mathematics 39 (4) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "A pathway-based design of rational combination therapies for cancer", "author": ["P. Vera-Licona", "A. Zinovyev", "E. Bonnet", "I. Kuperstein", "O. Kel", "A. Kel", "T. Dubois", "G. Tucker", "E. Barillot"], "venue": "european journal of cancer 48 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1016}, {"title": "Discovery of regulatory interactions through perturbation: inference and experimental design", "author": ["T.E. Ideker", "V. Thorsson", "R.M. Karp"], "venue": "in: Pacific symposium on biocomputing, Vol. 5", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Hypergraphs: combinatorics of finite sets", "author": ["C. Berge"], "venue": "Vol. 45, Elsevier", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Monotone boolean functions", "author": ["A.D. Korshunov"], "venue": "Russian Mathematical Surveys 58 (5) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "On elementary flux modes in biochemical reaction systems at steady state", "author": ["S. Schuster", "C. Hilgetag"], "venue": "Journal of Biological Systems 2 (02) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Elementary mode analysis: a useful metabolic pathway analysis tool for characterizing cellular metabolism", "author": ["C.T. Trinh", "A. Wlaschin", "F. Srienc"], "venue": "Applied microbiology and biotechnology 81 (5) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Elementary flux modes in a nutshell: Properties", "author": ["J. Zanghellini", "D.E. Ruckerbauer", "M. Hanscho", "C. Jungreuthmayer"], "venue": "calculation and applications, Biotechnology journal 8 (9) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimal cut sets in biochemical reaction networks", "author": ["S. Klamt", "E.D. Gilles"], "venue": "Bioinformatics 20 (2) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Computing knock-out strategies in metabolic networks", "author": ["U.-U. Haus", "S. Klamt", "T. Stephen"], "venue": "Journal of Computational Biology 15 (3) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A theoretical framework for detecting signal transfer routes in signalling networks", "author": ["I. Zevedei-Oancea", "S. Schuster"], "venue": "Computers & Chemical Engineering 29 (3) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Elementary signaling modes predict the essentiality of signal transduction network components", "author": ["R.-S. Wang", "R. Albert"], "venue": "BMC systems biology 5 (1) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Reverse engineering of molecular networks from a common combinatorial approach", "author": ["B. DasGupta", "P. Vera-Licona", "E. Sontag"], "venue": "in: M. Elloumi, A. Y. Zomaya (Eds.), Algorithms in Computational Molecular Biology: Techniques, Approaches and Applications, Wiley Online Library", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "The nci60 human tumour cell line anticancer drug screen", "author": ["R.H. Shoemaker"], "venue": "Nature Reviews Cancer 6 (10) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Mining association rules between sets of items in large databases", "author": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "in: ACM SIGMOD Record, Vol. 22-2, ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1993}, {"title": "Fast discovery of association rules", "author": ["R. Agrawal", "H. Mannila", "R. Srikant", "H. Toivonen", "A.I. Verkamo"], "venue": "in: U. M. Fayyad, G. Piatestky-Shapiro, P. Smyth, R. Uthursamy (Eds.), Advances in knowledge discovery and data mining, AAAI/MIT Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1996}, {"title": "On maximal frequent and minimal infrequent sets in binary matrices", "author": ["E. Boros", "V. Gurvich", "L. Khachiyan", "K. Makino"], "venue": "Annals of Mathematics and Artificial Intelligence 39 (3) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "On an algorithm for finding all interesting sentences extended abstract", "author": ["H. Mannila", "H. Toivoneny"], "venue": "in: Proceedings of the 13th European Meeting on Cybernetics and Systems Research, Citeseer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Sampling large databases for association", "author": ["H. Toivonen"], "venue": "rules, in: VLDB,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1996}, {"title": "A transversal hypergraph approach for the frequent itemset hiding problem", "author": ["E.C. Stavropoulos", "V.S. Verykios", "V. Kagklis"], "venue": "Knowledge and Information Systems ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Data mining", "author": ["D. Gunopulos", "H. Mannila", "R. Khardon", "H. Toivonen"], "venue": "hypergraph transversals, and machine learning, in: Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems, ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1997}, {"title": "On the complexity of generating maximal frequent and minimal infrequent sets", "author": ["E. Boros", "V. Gurvich", "L. Khachiyan", "K. Makino"], "venue": "in: STACS 2002, Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "There is no 16-clue Sudoku: Solving the Sudoku minimum number of clues problem via hitting set enumeration", "author": ["G. McGuire", "B. Tugemann", "G. Civario"], "venue": "Experimental Mathematics 23 (2) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducibility among combinatorial problems", "author": ["R.M. Karp"], "venue": "in: R. E. Miller, J. W. Thatcher, J. D. Bohlinger (Eds.), Complexity of Computer Computations, The IBM Research Symposia Series, Springer US", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1972}, {"title": "A fast and simple parallel algorithm for the monotone duality problem", "author": ["E. Boros", "K. Makino"], "venue": "in: Automata, Languages and Programming, Springer", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "On generating all maximal independent sets", "author": ["D.S. Johnson", "M. Yannakakis", "C.H. Papadimitriou"], "venue": "Information Processing Letters 27 (3) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1988}, {"title": "Lower bounds for three algorithms for transversal hypergraph generation", "author": ["M. Hagen"], "venue": "Discrete Applied Mathematics 157 (7) ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Computing many maximal independent sets for hypergraphs in parallel", "author": ["L. Khachiyan", "E. Boros", "V. Gurvich", "K. Elbassioni"], "venue": "Parallel processing letters 17 (02) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Generating all maximal independent sets of bounded-degree hypergraphs", "author": ["N. Mishra", "L. Pitt"], "venue": "in: Proceedings of the tenth annual conference on Computational learning theory, ACM", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "Some fixed-parameter tractable classes of hypergraph duality and related problems", "author": ["K. Elbassioni", "M. Hagen", "I. Rauf"], "venue": "in: Parameterized and Exact Computation, Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "New results on monotone dualization and generating hypergraph transversals", "author": ["T. Eiter", "G. Gottlob", "K. Makino"], "venue": "SIAM Journal on Computing 32 (2) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2003}, {"title": "A global parallel algorithm for the hypergraph transversal problem", "author": ["L. Khachiyan", "E. Boros", "K. Elbassioni", "V. Gurvich"], "venue": "Information Processing Letters 101 (4) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "A new algorithm for the hypergraph transversal problem", "author": ["L. Khachiyan", "E. Boros", "K. Elbassioni", "V. Gurvich"], "venue": "in: COCOON, Springer", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "On the desirability of acyclic database schemes", "author": ["C. Beeri", "R. Fagin", "D. Maier", "M. Yannakakis"], "venue": "Journal of the ACM (JACM) 30 (3) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1983}, {"title": "Degrees of acyclicity for hypergraphs and relational database schemes", "author": ["R. Fagin"], "venue": "Journal of the ACM (JACM) 30 (3) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1983}, {"title": "Identifying the minimal transversals of a hypergraph and related problems", "author": ["T. Eiter", "G. Gottlob"], "venue": "SIAM Journal on Computing 24 (6) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1995}, {"title": "Monotone boolean dualization is in co-NP [log 2", "author": ["D.J. Kavvadias", "E.C. Stavropoulos"], "venue": "Information Processing Letters 85 (1) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1016}, {"title": "Complexity of identification and dualization of positive boolean functions", "author": ["J.C. Bioch", "T. Ibaraki"], "venue": "Information and Computation 123 (1) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1995}, {"title": "Left-to-right multiplication for monotone boolean dualization", "author": ["E. Boros", "K. Elbassioni", "K. Makino"], "venue": "SIAM Journal on Computing 39 (7) ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "A worst-case analysis of the sequential method to list the minimal hitting sets of a hypergraph", "author": ["K. Takata"], "venue": "SIAM Journal on Discrete Mathematics 21 (4) ", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "A correction to the algorithm in Reiter\u2019s theory of diagnosis", "author": ["R. Greiner", "B.A. Smith", "R.W. Wilkerson"], "venue": "Artificial Intelligence 41 (1) ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1016}, {"title": "Efficient algorithms for dualizing large-scale hypergraphs", "author": ["K. Murakami", "T. Uno"], "venue": "Discrete Applied Mathematics 170 ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "E", "author": ["D.J. Kavvadias"], "venue": "C. Stavropoulos, An efficient algorithm for the transversal hypergraph generation., J. Graph Algorithms Appl. 9 (2) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2005}, {"title": "An efficient implementation of a quasi-polynomial algorithm for generating hypergraph transversals and its application in joint generation", "author": ["L. Khachiyan", "E. Boros", "K. Elbassioni", "V. Gurvich"], "venue": "Discrete Applied Mathematics 154 (16) ", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2006}, {"title": "M", "author": ["M. Hagen", "P. Horatschek"], "venue": "Mundhenk, Experimental comparison of the two Fredman-Khachiyanalgorithms., in: ALENEX, SIAM", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2009}, {"title": "Parallel computation of the minimal elements of a poset", "author": ["C.E. Leiserson", "M. Moreno Maza", "L. Li", "Y. Xie"], "venue": "in: Proceedings of the 4th International Workshop on Parallel and Symbolic Computation, ACM", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "Computers, IEEE Transactions on 100 (8) ", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1986}, {"title": "Combinatorial Algorithms: Part 1", "author": ["D.E. Knuth"], "venue": "Vol. 4A of The Art of Computer Programming, Addison-Wesley, Boston", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Hypergraph transversal computation with binary decision diagrams", "author": ["T. Toda"], "venue": "in: 12th International Symposium, SEA 2013, Rome, Italy, June 5-7, 2013, Springer", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "A data mining formalization to improve hypergraph minimal transversal computation", "author": ["C. H\u00e9bert", "A. Bretto", "B. Cr\u00e9milleux"], "venue": "Fundamenta Informaticae 80 (4) ", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2007}, {"title": "I", "author": ["K.M. Elbassioni", "M. Hagen"], "venue": "Rauf, A lower bound for the hbc transversal hypergraph generation., Fundam. Inform. 130 (4) ", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2014}, {"title": "On the complexity of monotone dualization and generating minimal hypergraph transversals", "author": ["K.M. Elbassioni"], "venue": "Discrete Applied Mathematics 156 (11) ", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2008}, {"title": "Alexander duality for monomial ideals and their resolutions", "author": ["E. Miller"], "venue": "available from http://arxiv. org/abs/math/9812095 ", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1998}, {"title": "Dual-bounded generating problems: All minimal integer solutions for a monotone system of linear inequalities", "author": ["E. Boros", "K. Elbassioni", "V. Gurvich", "L. Khachiyan", "K. Makino"], "venue": "SIAM Journal on Computing 31 (5) ", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2002}, {"title": "Concise representation of hypergraph minimal transversals: Approach and application on the dependency inference problem", "author": ["M.N. Jelassi", "C. Largeron", "S. Ben Yahia"], "venue": "in: Research Challenges in Information Science (RCIS), 2015 IEEE 9th International Conference on, IEEE", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2015}, {"title": "Profiling of high-frequency accident locations by use of association rules", "author": ["K. Geurts", "G. Wets", "T. Brijs", "K. Vanhoof"], "venue": "Transportation Research Record: Journal of the Transportation Research Board 1840 ", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2003}, {"title": "T", "author": ["K. Murakami"], "venue": "Uno, Hypergraph dualization repository ", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["A. Von Kamp"], "venue": "Schuster, Metatool 5.0: fast and flexible elementary modes analysis, Bioinformatics 22 (15) ", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2006}, {"title": "The logic of EGFR/ErbB signaling: theoretical properties and analysis of high-throughput data", "author": ["R. Samaga", "J. Saez-Rodriguez", "L.G. Alexopoulos", "P.K. Sorger", "S. Klamt"], "venue": "PLoS Comput Biol 5 (8) ", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "1, [1]), Boolean algebra (Section 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "2, [2, 3, 4]), fault diagnosis (Section 2.", "startOffset": 3, "endOffset": 12}, {"referenceID": 2, "context": "2, [2, 3, 4]), fault diagnosis (Section 2.", "startOffset": 3, "endOffset": 12}, {"referenceID": 3, "context": "2, [2, 3, 4]), fault diagnosis (Section 2.", "startOffset": 3, "endOffset": 12}, {"referenceID": 4, "context": "3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.", "startOffset": 3, "endOffset": 26}, {"referenceID": 5, "context": "3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": "3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.", "startOffset": 3, "endOffset": 26}, {"referenceID": 7, "context": "3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.", "startOffset": 3, "endOffset": 26}, {"referenceID": 8, "context": "3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.", "startOffset": 3, "endOffset": 26}, {"referenceID": 9, "context": "3, [5, 6, 7, 8, 9, 10, 11]), data mining (Section 2.", "startOffset": 3, "endOffset": 26}, {"referenceID": 10, "context": "5, [12, 13, 14, 15]), and computational biology (Section 2.", "startOffset": 3, "endOffset": 19}, {"referenceID": 11, "context": "5, [12, 13, 14, 15]), and computational biology (Section 2.", "startOffset": 3, "endOffset": 19}, {"referenceID": 12, "context": "5, [12, 13, 14, 15]), and computational biology (Section 2.", "startOffset": 3, "endOffset": 19}, {"referenceID": 13, "context": "5, [12, 13, 14, 15]), and computational biology (Section 2.", "startOffset": 3, "endOffset": 19}, {"referenceID": 14, "context": "4, [16, 17, 18, 19, 20, 21]), among others.", "startOffset": 3, "endOffset": 27}, {"referenceID": 15, "context": "4, [16, 17, 18, 19, 20, 21]), among others.", "startOffset": 3, "endOffset": 27}, {"referenceID": 16, "context": "4, [16, 17, 18, 19, 20, 21]), among others.", "startOffset": 3, "endOffset": 27}, {"referenceID": 17, "context": "4, [16, 17, 18, 19, 20, 21]), among others.", "startOffset": 3, "endOffset": 27}, {"referenceID": 18, "context": "4, [16, 17, 18, 19, 20, 21]), among others.", "startOffset": 3, "endOffset": 27}, {"referenceID": 19, "context": "4, [16, 17, 18, 19, 20, 21]), among others.", "startOffset": 3, "endOffset": 27}, {"referenceID": 20, "context": "Readers interested in the full theory of hypergraphs should consult Berge\u2019s 1984 monograph [22] on the subject.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Interested readers can consult the recent survey of Eiter [1] and Ph.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "thesis of Hagen [2] for more details about this subject.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "(See [23] for an extensive survey.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "thesis of Hagen [2], which gives this problem the picturesque name monet.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "In a celebrated 1987 paper [5], Reiter developed the foundation for a formal theory of model-based diagnosis (MBD), which we will introduce briefly.", "startOffset": 27, "endOffset": 30}, {"referenceID": 22, "context": "The notion of elementary flux modes (\u201cEFMs\u201d) was introduced by Schuster and Hilgetag in [24]; subsequent work has developed numerous techniques from linear algebra and computational geometry to find the EFMs.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "see the recent surveys [25, 26] for overview of the problem, the methods and software which are used to solve it, and various applications.", "startOffset": 23, "endOffset": 31}, {"referenceID": 24, "context": "see the recent surveys [25, 26] for overview of the problem, the methods and software which are used to solve it, and various applications.", "startOffset": 23, "endOffset": 31}, {"referenceID": 25, "context": "In [27], Klamt and Gilles focus on blocking a target reaction through cut sets, which they define as a set of reactions whose removal from the network leaves no feasible balanced flux distribution involving the target reaction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "developed in [28] a specialized version of the FK-A algorithm (cf.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "Biological signaling networks typically [29] exhibit a natural decomposition into input, intermediate, and output nodes; engineering and control of these networks then typically depends on adjusting the input and intermediate layers to obtain some outcome at the outputs.", "startOffset": 40, "endOffset": 44}, {"referenceID": 28, "context": "As an analogous of elementary flux modes (\u201cEFMs\u201d) in metabolic networks, in [30], Wang and Albert introduced the notion of elementary signaling modes (ESMs).", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "introduced the OCSANA framework to study this problem in [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Since discovery of hitting sets is a crucial step in the algorithm, the authors of [16] performed an experimental comparison.", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "Broadly, the biological reverse-engineering problem is that of \u201canalyzing a given system in order to identify, from biological data, the components of the system and their relationships\u201d [31].", "startOffset": 187, "endOffset": 191}, {"referenceID": 29, "context": "See [31] for a comparative survey of these two approaches and the relative performance of the specialized algorithms developed for each; we give here only a brief overview of each.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "introduce a method to infer the topology of a network of gene regulatory interactions in [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "introduce another method to infer the topology of a gene regulatory network in [19] which focuses on time series data within a single experiment.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "This application has been studied in detail by Vazquez in [17], using a greedy algorithm to search for very small effective combinations from the NCI60 collection ([32]) of 45334 drugs and 60 cancer cell lines.", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "This application has been studied in detail by Vazquez in [17], using a greedy algorithm to search for very small effective combinations from the NCI60 collection ([32]) of 45334 drugs and 60 cancer cell lines.", "startOffset": 164, "endOffset": 168}, {"referenceID": 31, "context": "in [33] and developed further in [34], is the discovery of frequent itemsets in a database of transactions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "in [33] and developed further in [34], is the discovery of frequent itemsets in a database of transactions.", "startOffset": 33, "endOffset": 37}, {"referenceID": 33, "context": "in [35].", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "This connection is explored by Manilla and Toivonen in [36]; more algorithmic details are given by Toivonen in [37].", "startOffset": 55, "endOffset": 59}, {"referenceID": 35, "context": "This connection is explored by Manilla and Toivonen in [36]; more algorithmic details are given by Toivonen in [37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "in [38].", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "Furthermore, so-called \u201cjoint-generation\u201d algorithms inspired by the FK algorithms of Fredman and Khachiyan [4] (cf.", "startOffset": 108, "endOffset": 111}, {"referenceID": 37, "context": "in [39] and its complexity implications explored by Boros et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "in [40].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "in [13].", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "4, [12]) and BMR (Section 4.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "5, [13]), were developed for this purpose.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [41], McGuire et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "They use an algorithm similar to HST from [8], discussed in detail in Section 4.", "startOffset": 42, "endOffset": 45}, {"referenceID": 40, "context": "It has been known since Karp\u2019s seminal 1972 paper [42] that the problem of determining whether a given set family has a hitting set of size no greater than some k is NP-complete.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "Fredman and Khachiyan present in [4] an algorithm (discussed in Section 4.", "startOffset": 33, "endOffset": 36}, {"referenceID": 41, "context": "The BM algorithm introduced by Boros and Makino in [43] improves on this bound in parallel cases.", "startOffset": 51, "endOffset": 55}, {"referenceID": 42, "context": "introduced a formalism to deal with this issue in [44].", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Unfortunately, this is not known to be achieved by any current algorithm, and Hagen showed in [45] that several important algorithms are not output-polynomial.", "startOffset": 94, "endOffset": 98}, {"referenceID": 42, "context": "introduced two suitable formalisms in [44].", "startOffset": 38, "endOffset": 42}, {"referenceID": 42, "context": "Crucially, if an algorithm runs with polynomial delay, it is guaranteed to run in output-polynomial total time, but incremental-polynomial time gives no such guarantee ([44]).", "startOffset": 169, "endOffset": 173}, {"referenceID": 44, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 178, "endOffset": 197}, {"referenceID": 2, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 178, "endOffset": 197}, {"referenceID": 45, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 178, "endOffset": 197}, {"referenceID": 46, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 178, "endOffset": 197}, {"referenceID": 45, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 178, "endOffset": 197}, {"referenceID": 47, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 237, "endOffset": 249}, {"referenceID": 46, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 237, "endOffset": 249}, {"referenceID": 48, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 237, "endOffset": 249}, {"referenceID": 49, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 305, "endOffset": 313}, {"referenceID": 46, "context": "Fixed-parameter tractability results have been obtained for the transversal hypergraph recognition problem with a wide variety of parameters, including vertex degree parameters ([46, 3, 47, 48, 47]), hyperedge size or number parameters ([49, 48, 50]), and hyperedge intersection or union size parameters ([51, 48]).", "startOffset": 305, "endOffset": 313}, {"referenceID": 50, "context": "introduced in [52] a notion of acyclicity in hypergraphs, now known as \u03b1-acyclicity, in the context of the study of relational database schemes.", "startOffset": 14, "endOffset": 18}, {"referenceID": 51, "context": "Fagin subsequently introduced in [53] the notions of \u03b2-acyclicity and \u03b3-acyclicity, which are successively more restrictive and correspond to desirable tractability problems in databases.", "startOffset": 33, "endOffset": 37}, {"referenceID": 52, "context": "Eiter showed that the transversal recognition is solvable in polynomial time for \u03b2-acyclic hypergraphs in [54] and for \u03b1-acyclic hypergraphs in [49].", "startOffset": 106, "endOffset": 110}, {"referenceID": 47, "context": "Eiter showed that the transversal recognition is solvable in polynomial time for \u03b2-acyclic hypergraphs in [54] and for \u03b1-acyclic hypergraphs in [49].", "startOffset": 144, "endOffset": 148}, {"referenceID": 53, "context": "Kavvadias and Stavropoulos showed in [55] that the recognition problem is in the class co-NP[log2 n] for n the total number of edges in H and TrH, meaning that only O ( log2 n ) nondeterministic bits are required to demonstrate that two hypergraphs are not transversals of each other.", "startOffset": 37, "endOffset": 41}, {"referenceID": 54, "context": "[56]) that an algorithm for the transversal hypergraph recognition problem (cf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "1 Berge (1984) The first systematic algorithm for computing transversals of hypergraphs was presented by Berge in [22], a monograph on the theory of hypergraphs.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": "As suggested in [18], Berge\u2019s algorithm can be adapted to search only for MHSes of cardinality bounded by some k by simply discarding candidates larger than k at Lines 4 and 5 in each stage of the algorithm.", "startOffset": 16, "endOffset": 20}, {"referenceID": 55, "context": "in [58].", "startOffset": 3, "endOffset": 7}, {"referenceID": 56, "context": "Takata showed in [59] that there exists a family of hypergraphs for which no edge ordering yields output-polynomial running time, and thus that Berge is not output-polynomial in general, even if the edge ordering is optimal.", "startOffset": 17, "endOffset": 21}, {"referenceID": 55, "context": "demonstrate in [58], however, that the worst case is still sub-exponential.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "3, Reiter introduced the formal theory of model-based diagnosis as an application of MHS enumeration in [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 57, "context": "in [60] that this algorithm is incomplete; the hitting sets it generates are guaranteed to be minimal, but in certain circumstances some MHSes may be missed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "3 Wotawa (2001) Wotawa returned to Reiter\u2019s approach in [8], reviewing the HS-DAG algorithm of [60] (see Section 4.", "startOffset": 56, "endOffset": 59}, {"referenceID": 57, "context": "3 Wotawa (2001) Wotawa returned to Reiter\u2019s approach in [8], reviewing the HS-DAG algorithm of [60] (see Section 4.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "4 Dong and Li (2005) Dong and Li considered in [12] the \u201cemerging patterns problem\u201d discussed in Section 2.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "Hagen shows in [2, 45] that Takata\u2019s time bounds on Berge in [59] apply to DL as well, so it is not output-polynomial.", "startOffset": 15, "endOffset": 22}, {"referenceID": 43, "context": "Hagen shows in [2, 45] that Takata\u2019s time bounds on Berge in [59] apply to DL as well, so it is not output-polynomial.", "startOffset": 15, "endOffset": 22}, {"referenceID": 56, "context": "Hagen shows in [2, 45] that Takata\u2019s time bounds on Berge in [59] apply to DL as well, so it is not output-polynomial.", "startOffset": 61, "endOffset": 65}, {"referenceID": 58, "context": "A C implementation of this algorithm by the authors of [61] is available in the repository.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "developed in [13] an algorithm which decomposes the input set family more carefully than Berge\u2019s algorithm.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Rather than simply considering one new set at a time, their approach attempts to partition the set family into components with few sets, then use the DL algorithm of [12] as a subroutine to compute their MHSes before combining them using equation (1).", "startOffset": 166, "endOffset": 170}, {"referenceID": 1, "context": "Hagen shows in [2, 45] that BMR is not output-polynomial.", "startOffset": 15, "endOffset": 22}, {"referenceID": 43, "context": "Hagen shows in [2, 45] that BMR is not output-polynomial.", "startOffset": 15, "endOffset": 22}, {"referenceID": 58, "context": "A C implementation of this algorithm by the authors of [61] is available in the repository.", "startOffset": 55, "endOffset": 59}, {"referenceID": 59, "context": "6 Kavvadias and Stavropoulos (2005) Returning to the explicit study of hypergraph transversals, Kavvadias and Stavropoulos introduced in [62] another algorithm, which seeks to reduce the memory requirements of Berge with two optimizations.", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "Hagen shows in [2, 45] that KS does not run in output-polynomial time.", "startOffset": 15, "endOffset": 22}, {"referenceID": 43, "context": "Hagen shows in [2, 45] that KS does not run in output-polynomial time.", "startOffset": 15, "endOffset": 22}, {"referenceID": 59, "context": "A Pascal implementation of this algorithm by the authors of [62] is available in the repository.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "1 Lin and Jiang (2003) Lin and Jiang return in [9] to the problem of model-based diagnosis.", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "The Boolean algorithm was subsequently optimized by Pill and Quaritsch in [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "2 Fredman and Khachiyan (1996) Fredman and Khachiyan introduced two iterative algorithms in [4] to study the recognition version of the MHS problem in the Boolean algebra context.", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "We will refer to these two algorithms as FK-A and FK-B (\u201cFK\u201d for the authors, who use the names A and B in [4]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 60, "context": "The algorithm is modified in [63] to improve its runtime slightly and adapt it to MHS generation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 60, "context": "As a result, most authors (including [63]) have disregarded FK-B in comparative studies.", "startOffset": 37, "endOffset": 41}, {"referenceID": 61, "context": "However, analysis in [64] suggests that this assumption may be inaccurate.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "apply this approach in [28], as discussed in Section 2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "Abreu and Gemund presented such an algorithm in [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "We will refer to this algorithm as STACCATO (the name used by its authors in [10]).", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "The authors of [10] claim that, for a set family with N sets andM total elements, the algorithm guarantees to find a hitting set of cardinality C in O ( (M \u00b7 (N + logM)) ) worst-case time and O(C \u00b7M) space, with improved expected times based on their heuristic and tested experimentally.", "startOffset": 15, "endOffset": 19}, {"referenceID": 62, "context": "cast this issue in a very abstract setting in [65], developing a framework to parallelize any algorithm that searches for minimal elements of a poset and then applying it to the lattice of hitting sets of a set family.", "startOffset": 46, "endOffset": 50}, {"referenceID": 62, "context": "We will refer to this algorithm as ParTran (the name used by its authors in [65]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 62, "context": "A Cilk++ implementation of this algorithm by the authors of [65] is available in the repository.", "startOffset": 60, "endOffset": 64}, {"referenceID": 63, "context": "5 Knuth (2011) Binary decision diagrams (BDDs) are a graph-based structure for representing boolean functions and hypergraphs originally introduced by Bryant in [66].", "startOffset": 161, "endOffset": 165}, {"referenceID": 65, "context": "A C implementation of this algorithm by the author of [68] is available in the repository.", "startOffset": 54, "endOffset": 58}, {"referenceID": 65, "context": "6 Toda (2013) In 2013, Toda improved on the KNUTH algorithm in [68] by incorporating a variation on the BDD data structure\u2013the zero-suppressed binary decision diagram (ZDD).", "startOffset": 63, "endOffset": 67}, {"referenceID": 65, "context": "Toda gives a formal complexity analysis of HTC-BDD in [68], but the resulting bounds are expressed in terms of the intermediate BDD and ZDD data structures and are incommensurable with bounds like those known for FK-A and FK-B.", "startOffset": 54, "endOffset": 58}, {"referenceID": 65, "context": "A C implementation of this algorithm by the author of [68] is available in the repository.", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "7 Cardoso and Abreu (2014) Cardoso and Abreu revisted the STACCATO approach in [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "A C++ implementation of the algorithm by the authors of [6] is available in the repository.", "startOffset": 56, "endOffset": 59}, {"referenceID": 66, "context": "take an approach in [69] that brings insights from data mining to bear on the MHS generation problem.", "startOffset": 20, "endOffset": 24}, {"referenceID": 67, "context": "We follow the explanation of the algorithm in [70], which avoids the algebraic complexity of the original.", "startOffset": 46, "endOffset": 50}, {"referenceID": 67, "context": "[70]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "However, Hagen shows in [70] that this bound is incorrect.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "The second author and collaborators apply this approach as a \u201cgreedy algorithm\u201d in [16] to", "startOffset": 83, "endOffset": 87}, {"referenceID": 66, "context": "A C++ implementation of this algorithm by the authors of [69] is available in the repository.", "startOffset": 57, "endOffset": 61}, {"referenceID": 58, "context": "2 Murakami and Uno (2014) Murakami and Uno take a somewhat different approach in [61] in two new algorithms.", "startOffset": 81, "endOffset": 85}, {"referenceID": 66, "context": "[69, 61]:", "startOffset": 0, "endOffset": 8}, {"referenceID": 58, "context": "[69, 61]:", "startOffset": 0, "endOffset": 8}, {"referenceID": 58, "context": "Then MMCS runs in O(k) time per iteration of its main loop, but the authors of [61] do not give bounds for the number of iterations required.", "startOffset": 79, "endOffset": 83}, {"referenceID": 58, "context": "However, the shd program distributed by the authors of [61] does not support either of these modes.", "startOffset": 55, "endOffset": 59}, {"referenceID": 58, "context": "A C implementation of MMCS and RS by the authors of [61] is available in the repository.", "startOffset": 52, "endOffset": 56}, {"referenceID": 48, "context": "introduced the full cover decomposition approach in [50, 46].", "startOffset": 52, "endOffset": 60}, {"referenceID": 44, "context": "introduced the full cover decomposition approach in [50, 46].", "startOffset": 52, "endOffset": 60}, {"referenceID": 48, "context": "2 Elbassioni (2008) Following up on [50, 46], Elbassioni presents in [71] two parallel decomposition approaches for the transversal recognition problem.", "startOffset": 36, "endOffset": 44}, {"referenceID": 44, "context": "2 Elbassioni (2008) Following up on [50, 46], Elbassioni presents in [71] two parallel decomposition approaches for the transversal recognition problem.", "startOffset": 36, "endOffset": 44}, {"referenceID": 68, "context": "2 Elbassioni (2008) Following up on [50, 46], Elbassioni presents in [71] two parallel decomposition approaches for the transversal recognition problem.", "startOffset": 69, "endOffset": 73}, {"referenceID": 68, "context": "(The exact bounds are cumbersome to state but may be found in [71].", "startOffset": 62, "endOffset": 66}, {"referenceID": 41, "context": "3 Boros and Makno (2009) Boros and Makino present in [43] a full cover algorithm which improves on the asymptotic complexity bounds of [71] for transversal recognition.", "startOffset": 53, "endOffset": 57}, {"referenceID": 68, "context": "3 Boros and Makno (2009) Boros and Makino present in [43] a full cover algorithm which improves on the asymptotic complexity bounds of [71] for transversal recognition.", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "in [19] for an application in computational biology.", "startOffset": 3, "endOffset": 7}, {"referenceID": 69, "context": "They calculate the associated primes of IS using Alexander duality [72] as provided in Macaulay2 [73].", "startOffset": 67, "endOffset": 71}, {"referenceID": 70, "context": "in [74] that MHS generation is equivalent to the general problem of enumerating minimal solutions to the linear system Ax = b for 0 \u2264 x \u2264 c where A is a binary matrix, x is a binary vector, and b and c are all-ones vectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "7 Algorithm miscellany A genetic algorithm for finding many (but not necessarily all) small (but not necessarily minimal) hitting sets is studied by Li and Yunfei in [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "Vinterbo and \u00d8hrn study in [14] the more refined problem of finding weighted r-approximate hitting sets, which are sets which hit some fraction 0 \u2264 r \u2264 1 of the target sets according to assigned weights; they also apply a genetic algorithm with promising results.", "startOffset": 27, "endOffset": 31}, {"referenceID": 71, "context": "consider the efficacy of pre-processing methods in [75].", "startOffset": 51, "endOffset": 55}, {"referenceID": 59, "context": "(This optimization was also used by Kavvadias and Stavropoulos in [62] for their algorithm KS.", "startOffset": 66, "endOffset": 70}, {"referenceID": 58, "context": ") Their algorithm Irred-Engine performs this preprocessing, applies a known MHS algorithm (in their case, MMCS from [61]) to the resulting family S\u2032, and then expands the results into MHSes for the original S.", "startOffset": 116, "endOffset": 120}, {"referenceID": 58, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 62, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 59, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 66, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 61, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 10, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 5, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 11, "context": "Numerous previous papers have included experimental comparisons of some algorithms, including [61, 65, 62, 69, 64, 12, 6, 13].", "startOffset": 94, "endOffset": 125}, {"referenceID": 58, "context": "(Murakami and Uno\u2019s work in [61] is a notable exception, and indeed their publicly-available implementations are used for several algorithms here.", "startOffset": 28, "endOffset": 32}, {"referenceID": 72, "context": "Originally published in [76].", "startOffset": 24, "endOffset": 28}, {"referenceID": 58, "context": "Converted by the authors of [61] into a set family whose sets are the complements of maximal frequent itemsets with specified threshold 1000\u03b8 for \u03b8 \u2208 {70, 90, 110, 130, 150, 200}; MHSes of this set family then correspond to minimal infrequent itemsets.", "startOffset": 28, "endOffset": 32}, {"referenceID": 73, "context": "This formulation was downloaded from [77].", "startOffset": 37, "endOffset": 41}, {"referenceID": 74, "context": "Reaction networks for producing acetate, glucose, glycerol, and succinate, along with the combined network, were analyzed to find their \u201celementary modes\u201d using Metatool [78], which are given as set families.", "startOffset": 170, "endOffset": 174}, {"referenceID": 75, "context": "Two cell signalling networks (EGFR from [79] and HER2+ from [20]) were analyzed to find their \u201celementary pathways\u201d using OCSANA, which are given as set families.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "Two cell signalling networks (EGFR from [79] and HER2+ from [20]) were analyzed to find their \u201celementary pathways\u201d using OCSANA, which are given as set families.", "startOffset": 60, "endOffset": 64}, {"referenceID": 58, "context": "3, the algorithms MMCS and RS from [61] and HTC-BDD from [68] are far faster than their competitors across a variety of input set families.", "startOffset": 35, "endOffset": 39}, {"referenceID": 65, "context": "3, the algorithms MMCS and RS from [61] and HTC-BDD from [68] are far faster than their competitors across a variety of input set families.", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "7) of Cardoso and Abreu [6] shows a 2.", "startOffset": 24, "endOffset": 27}, {"referenceID": 58, "context": "2) of Murakami and Uno [61] shows a 4.", "startOffset": 23, "endOffset": 27}, {"referenceID": 41, "context": "3) of Boros and Makino [43] was too slow to yield useful results.", "startOffset": 23, "endOffset": 27}, {"referenceID": 58, "context": "2) of Murakami and Uno [61] and the HTC-BDD algorithm (cf.", "startOffset": 23, "endOffset": 27}, {"referenceID": 65, "context": "6) of Toda [68] are far faster than other available algorithms across a variety of inputs.", "startOffset": 11, "endOffset": 15}], "year": 2016, "abstractText": "Finding inclusion-minimal hitting sets for a given collection of sets is a fundamental combinatorial problem with applications in domains as diverse as Boolean algebra, computational biology, and data mining. Much of the algorithmic literature focuses on the problem of recognizing the collection of minimal hitting sets; however, in many of the applications, it is more important to generate these hitting sets. We survey twenty algorithms from across a variety of domains, considering their history, classification, useful features, and computational performance on a variety of synthetic and real-world inputs. We also provide a suite of implementations of these algorithms with a ready-to-use, platform-agnostic interface based on Docker containers and the AlgoRun framework, so that interested computational scientists can easily perform similar tests with inputs from their own research areas on their own computers or through a convenient Web interface.", "creator": "LaTeX with hyperref package"}}}