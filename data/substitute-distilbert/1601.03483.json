{"id": "1601.03483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "A survey on feature weighting based K-Means algorithms", "abstract": "over a data - world data set there is always the possibility, rather high in our opinion, that important features may have different rules of relevance. most information learning algorithms deal with this fact by individually selecting or deselecting features in the data preprocessing phase. third, we maintain that even among relevant features there may be lesser degrees of relevance, and this should be taken into account during the clustering process. with over 50 years of history, k - means is arguably the most popular partitional clustering implementations there exist. the first k - means based clustering algorithm to compute feature weights was designed just over 30 years ago. various such algorithms have been designed since where there has not been, to our knowledge, our report integrating vital evidence of complexity recovery ability, common flaws, and possible drivers for future research. this paper elaborates on the concept of diversity weighting and addresses general issues by critically analysing some of eight historically popular, or innovative, feature weighting mechanisms based in k - means.", "histories": [["v1", "Tue, 22 Sep 2015 08:46:39 GMT  (46kb)", "http://arxiv.org/abs/1601.03483v1", "Journal of Classification (to appear)"]], "COMMENTS": "Journal of Classification (to appear)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["renato cordeiro de amorim"], "accepted": false, "id": "1601.03483"}, "pdf": {"name": "1601.03483.pdf", "metadata": {"source": "CRF", "title": "A survey on feature weighting based K-Means algorithms", "authors": ["Renato Cordeiro de Amorim"], "emails": ["r.amorim@herts.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n03 48\n3v 1\n[ cs\n.L G\n] 2\n2 Se\np 20\nWith over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research. This paper elaborates on the concept of feature weighting and addresses these issues by critically analysing some of the most popular, or innovative, feature weighting mechanisms based in K-Means.\nKeywords Feature weighting \u00b7 K-Means \u00b7 partitional clustering \u00b7 feature selection."}, {"heading": "1 Introduction", "text": "Clustering is one of the main data-driven tools for data analysis. Given a data set Y composed of entities yi \u2208 Y for i = 1, 2, ..., N, clustering algorithms aim to partition Y into K clusters S = {S 1, S 2, ..., S K} so that the entities yi \u2208 S k are homogeneous and entities between clusters are heterogeneous, according to some notion of similarity. These algorithms address a non-trivial problem whose scale sometimes goes unnoticed. For instance, a data set containing 25 entities can have approximately\nRC de Amorim Department of Computer Science, University of Hertfordshire, College Lane, Hatfield AL10 9AB, UK. Tel.: +44 01707 284345 Fax: +44 01707 284115 E-mail: r.amorim@herts.ac.uk\n4.69x1013 different partitions if K is set to four (Steinley 2006). Clustering has been used to solve problems in the most diverse fields such as computer vision, text mining, bioinformatics, and data mining (Vedaldi and Fulkerson 2010; Steinley 2006; Jain 2010; Sturn, Quackenbush, and Trajanoski 2002; Huang et al. 2008; Gasch and Eisen 2002; Mirkin 2012).\nClustering algorithms follow either a partitional or hierarchical approach to the assignment of entities to clusters. The latter produces a set of clusters S as well as a tree-like relationship between these clusters, which can be easily visualised with a dendogram. Hierarchical algorithms allow a given entity yi to be assigned to more than one cluster in S , as long as the assignments occur at different levels in the tree. This extra information regarding the relationships between clusters comes at a considerable cost, leading to a time complexity of O(N2), or even O(N3) depending on the actual algorithm in use (Murtagh 1984; Murtagh and Contreras 2011). Partitional algorithms tend to converge in less time by comparison (details in Section 2), but provide only information about the assignment of entities to clusters. Partitional algorithms were originally designed to produce a set of disjoint clusters, in which an entity yi \u2208 Y could be assigned to a single cluster S k \u2208 S . K-Means (MacQueen 1967; Ball and Hall 1967; Steinhaus 1956) is arguably the most popular of such algorithms (for more details see Section 2). Among the many extensions to K-Means, we have Fuzzy C-Means (Bezdek 1981) which applies Fuzzy set theory (Zadeh 1965) to allow a given entity yi to be assigned to each cluster in S at different degrees of membership. However, Fuzzy C-Means introduces other issues to clustering, falling outside the scope of this paper.\nThe popularity of K-Means is rather evident. A search in scholar.google.com for \u201cK-Means\u201d in May 2014 found just over 320, 000 results, the same search in May 2015 found 442, 000 results. Adding to these impressive numbers, implementations of this algorithm can be found in various software packages commonly used to analyse data, including SPSS, MATLAB, R, and Python. However, K-Means is not without weaknesses. For instance, K-Means treats every single feature in a data set equally, regardless of its actual degree of relevance. Clearly, different features in the same data set may have different degrees of relevance, a prospect we believe should be supported by any good clustering algorithm. With this weakness in mind research effort has happened over the last 30 years to develop K-Means based approaches supporting feature weighting (more details in Section 4). Such effort has lead to various different approaches, but unfortunately not much guidance on the choice of which to employ in practical applications.\nIn this paper, we provide the reader with a survey of K-Means based weighting algorithms. We find this survey to be unique because it does not simply explain some of the major approaches to feature weighting in K-Means, but also provides empirical evidence of their cluster recovery ability. We begin by formally introducing K-Means and the concept of feature weighting in Sections 2 and 3, respectively. We then critically analyse some of the major methods for feature weighting in K-Means in Section 4. We chose to analyse those methods we believe are the most used or innovative, but since it is impossible to analyse all existing methods we are possibly guilty of omissions. The setting and results of our experiments can be found in Sections 5 and 6.\nThe paper ends by presenting our conclusions and discussing common issues with these algorithms that could be addressed in future research, in Section 7."}, {"heading": "2 K-Means clustering", "text": "K-Means is arguably the most popular partitional clustering algorithm (Jain 2010; Steinley 2006; Mirkin 2012). For a given data set Y, K-Means outputs a disjoint set of clusters S = {S 1, S 2, ..., S K}, as well as a centroid ck for each cluster S k \u2208 S . The centroid ck is set to have the smallest sum of distances to all yi \u2208 S k, making ck a good general representation of S k, often called a prototype. K-Means partitions a given data set Y by iteratively minimising the sum of the within-cluster distance between entities yi \u2208 Y and respective centroids ck \u2208 C. Minimising the equation below allows K-Means to show the natural structure of Y.\nW(S ,C) = K \u2211\nk=1\n\u2211\ni\u2208S k\n\u2211\nv\u2208V\n(yiv \u2212 ckv)2, (1)\nwhere V represents the set of features used to describe each yi \u2208 Y. The algorithm used to iteratively minimise (1) may look rather simple at first, with a total of three steps, two of which iterated until the convergence. However, this minimisation is a non-trivial problem, being NP-Hard even if K = 2 (Aloise et al. 2009).\n1. Select the values of K entities from Y as initial centroids c1, c2, ..., cK . Set S \u2190 \u2205. 2. Assign each entity yi \u2208 Y to the cluster S k represented by its closest centroid. If\nthere are no changes in S , stop and output S and C. 3. Update each centroid ck \u2208 C to the centre of its cluster S k. Go to Step 2.\nThe K-Means criterion we show, (1), applies the squared Euclidean distance as in its original definition (MacQueen 1967; Ball and Hall 1967). The use of this particular distance measure makes the centroid update in Step three of the algorithm above rather straightforward. Given a cluster S k with |S k| entities, ckv = 1|S k | \u2211\ni\u2208S k yiv, for each v \u2208 V .\nOne can clearly see that K-Means has a strong relation with the Expectation Maximisation algorithm (Dempster, Laird, and Rubin 1977). Step two of K-Means can be seen as the expectation by keeping C fixed and minimising (1) in respect to S , and Step three can be seen as the maximisation in which one fixes S and minimises (1) in relation to C. K-Means also has a strong relation with Principal Component Analysis, the latter can be seen as a relaxation of the former (Zha et al. 2001; Drineas et al. 2004; Ding and He 2004).\nK-Means, very much like any other algorithm in machine learning, has weaknesses. These are rather well-known and understood thanks to the popularity of this algorithm and the considerable research effort done by the research community. Among these weaknesses we have: (i) the fact that the number of clusters K has to be known beforehand; (ii) K-Means will partition a data set Y into K partitions even if there is no clustering structure in the data; (iii) this is a greedy algorithm that may get trapped\nin local minima; (iv) the initial centroids, found at random in Step one heavily influence the final outcome; (v) it treats all features equally, regardless of their actual degree of relevance.\nHere we are particularly interested in the last weakness. Regardless of the problem at hand and the structure of the data, K-Means treats each feature v \u2208 V equally. This means that features that are more relevant to a given problem may have the same contribution to the clustering as features that are less relevant. By consequence K-Means can be greatly affected by the presence of totally irrelevant features, including features that are solely composed of noise. Such features are not uncommon in real-world data. This weakness can be addressed by setting weights to each feature v \u2208 V , representing its degree of relevance. We find this to be a particularly important field of research, we elaborate on the concept of feature weighting in the next section."}, {"heading": "3 Feature Weighting", "text": "New technology has made it much easier to acquire vast amounts of real-world data, usually described over many features. The curse of dimensionality (Bellman 1957) is a term usually associated with the difficulties in analysing such high-dimensional data. As the number of features v \u2208 V increases, the minimum and maximum distances become impossible to distinguish as their difference, compared to the minimum distance, converges to zero (Beyer et al. 1999).\nlim |V |\u2192\u221e distmax \u2212 distmin distmin = 0 (2)\nApart from the problem above, there is a considerable consensus in the research community that meaningful clusters, particularly those in high-dimensional data, occur in subspaces defined by a specific subset of features (Tsai and Chiu 2008; Liu and Yu 2005; Chen et al. 2012; De Amorim and Mirkin 2012). In cluster analysis, and in fact any other pattern recognition task, one should not simply use all features available as clustering results become less accurate if a significant number of features are not relevant to some clusters (Chan et al. 2004). Unfortunately, selecting the optimal feature subset is NP-Hard (Blum and Rivest 1992).\nFeature weighting can be thought of as a generalization of feature selection (Wettschereck, Aha, and Mohri 1997; Modha and Spangler 2003; Tsai and Chiu 2008). The latter has a much longer history and it is used to either select or deselect a given feature v \u2208 V , a process equivalent to assigning a feature weight wv of one or zero, respectively. Feature selection methods effectively assume that each of the selected features has the same degree of relevance. Feature weighting algorithms do not make such assumption as there is no reason to believe that each of the selected features would have the same degree of relevance in all cases. Instead, such algorithms allow for a feature weight, normally in the interval [0, 1]. This may be a feature weight wv, subject to \u2211\nv\u2208V wv = 1, or even a cluster dependant weight wkv, subject to \u2211 v\u2208V wkv = 1 for k = 1, 2, ..., K. The idea of cluster dependant weights is well aligned with the intuition that a given feature v may have different degrees of relevance at different clusters.\nFeature selection methods for unlabelled data follow either a filter or wrapper approach (Dy 2008; Kohavi and John 1997). The former uses properties of the data itself to select a subset of features during the data pre-processing phase. The features are selected before the clustering algorithm is run, making this approach usually faster. However, this speed comes at price. It can be rather difficult to define whether a given feature is relevant without applying clustering to the data. Methods following a wrapper approach make use of the information given by a clustering algorithm when selecting features. Often, these methods lead to better performance when compared to those following a filter approach (Dy 2008). However, these also tend to be more computationally intensive as the clustering and the feature selection algorithms are run. The surveys by Dy (2008), Steinley and Brusco (2008), and Guyon and Elisseeff (2003) are, in our opinion, a very good starting point for those readers in need of more information.\nFeature weighting and feature selection algorithms are not competing methods. The former does not dismiss the advantages given by the latter. Feature weighting algorithms can still deselect a given feature v by setting its weight wv = 0, bringing benefits traditionally related to feature selection. Such benefits include those discussed by Guyon and Elisseeff (2003) and Dy (2008), such as a possible reduction in the feature space, reduction in measurement and storage requirements, facilitation of data understanding and visualization, reduction in algorithm utilization time, and a general improvement in cluster recovery thanks to the possible avoidance of the curse of dimensionality.\nClustering algorithms recognise patterns under an unsupervised learning framework, it is only fitting that the selection or weighting of features should not require labelled samples. There are a considerable amount of unsupervised feature selection methods, some of which can be easily used in the data pre-processing stage (Devaney and Ram 1997; Talavera 1999; Mitra, Murthy, and Pal 2002) to either select or deselect features from V . Feature weighting algorithms for K-Means have thirty years of history, in the next section we discuss some what we believe to be the main methods."}, {"heading": "4 Major approaches to feature weighting in K-Means", "text": "Work on feature weighting in clustering has over 40 years of history (Sneath and Sokal 1973), however, only in 1984 (DeSarbo et al. 1984) feature weighting was applied to K-Means, arguably the most popular partitional clustering algorithm. Many feature weighting algorithms based on K-Means have been developed since, here we chose nine algorithms for our discussion. These are either among the most popular, or introduce innovative new concepts."}, {"heading": "4.1 SYNCLUS", "text": "Synthesized Clustering (SYNCLUS) (DeSarbo et al. 1984) is, to our knowledge, the first K-Means extension to allow feature weights. SYNCLUS employs two types of weights by assuming that features, as well as groups of features, may have different\ndegrees of relevance. This algorithm requires the user to meaningfully group features into T partitions G = {G1,G2, ...,GT }. We represent the degree of relevance of the feature group Gt with \u03c9t, where 1 \u2264 t \u2264 T . The feature weight of any given feature v \u2208 V is represented by wv.\nIn its first step, very much like K-Means, SYNCLUS requests the user to provide a data set Y and the desired number of partitions K. Unlike K-Means, the user is also requested to provide information regarding how the features are grouped, and a vector \u03c9 containing the weights of each feature group. This vector \u03c9 is normalised so that \u2211T\nt \u03c9t = 1. DeSarbo suggests that each wv, the weight of a given feature v \u2208 V , can be initialised so that it is inversely proportional to the variance of v over all entities yi \u2208 Y, or are all equal.\nThe distance between two objects yi and y j is defined, in each feature group, as their weighted squared distance d(yi, y j)(t) = \u2211 v\u2208Gt wtv (yiv \u2212 y jv) 2. Given \u03c9, w, Y, K, and d(yi, y j)(t), for i, j = 1, 2, ..., N, SYNCLUS optimises the weighted mean-square, stress-like objective function below.\nW(S ,C,w, \u03c9) =\n\u2211T t \u03c9t \u2211 i\u2208Y \u2211 j\u2208Y (\u03b4i j \u2212 d(yi, y j) (t))\n\u2211 i\u2208Y \u2211 j\u2208Y \u03b4 2 i j\n, (3)\nsubject to a disjoint clustering so that S k \u2229 S l = \u2205 for k, l = 1, 2, ..., K and k , l, as well as \u2211\ni\u2208Y \u2211 j\u2208Y \u03b4 2 i j , 0, \u03b4i j = \u03b1a \u2217 i j + \u03b2 (details regarding \u03b1 and \u03b2 in DeSarbo et al.\n1984) where,\na\u2217i j =\n       1 |S k | , if {yi, y j} \u2286 S k, 0, otherwise. (4)\nAlthough an icon of original research, SYNCLUS does have some weaknesses. This computationally expensive algorithm presented mixed results on empirical data sets (Green, Kim, and Carmone 1990), and there have been other claims of poor performance (Gnanadesikan, Kettenring, and Tsao 1995). SYNCLUS is not appropriate for clusterwise regression context with both dependent and independent variables (DeSarbo and Cron 1988).\nNevertheless, SYNCLUS has been a target to various extensions. DeSarbo and Mahajan (1984) extended this method to deal with constraints, different types of clustering schemes, as well as a general linear transformation of the features. It has also been extended by Makarenkov and Legendre (2001) by using the Polak-Ribiere optimisation procedure (Polak 1971) to minimise (3). However, this latter extension seemed to be particularly useful only when \u2018noisy\u2019 features (those without cluster structure) existed. The authors recommended using equal weights (ie. the original K-Means) when data are error-perturbed or contained outliers.\nThe initial work on SYNCLUS also expanded into a method to find optimal feature weights for ultrametric and additive tree fitting (De Soete 1986; De Soete 1988). However, this work lies outside the scope of this paper as the method was applied in hierarchical clustering.\nSYNCLUS marked the beginning of research on feature weighting in K-Means, and it is possible to see its influences in nearly all other algorithms in this particular field.\n4.2 Convex K-Means\nModha and Spangler (2003) introduced the convex K-Means (CK-Means) algorithm. CK-Means presents an interesting approach to feature weighting by integrating multiple, heterogeneous feature spaces into K-Means. Given the two entities {yi, y j} \u2286 Y, each described over the features v \u2208 V , the dissimilarity between these two entities is given by the distortion measure below.\nDw(yi, y j) = \u2211\nv\u2208V\nwvDv(yiv, y jv), (5)\nwhere Dv depends on the feature space in use. Modha and Spangler present two generic examples.\nDv(yiv, y jv) =\n       (yiv \u2212 y jv)T (yiv \u2212 y jv), in the Euclidean case\n2(1 \u2212 yTivy jv), in the Spherical case. (6)\nEquation (5) allows calculating the distortion of a specific cluster \u2211\nyi\u2208S k Dw(yi, ck), and the quality of the clustering S = {S 1, S 2, ..., S K}, given by \u2211K k=1 \u2211\nyi\u2208S k Dw(yi, ck). CK-Means determines the optimal set of feature weights that simultaneously minimises the average within-cluster dispersion and maximises the average betweencluster dispersion along all of the feature spaces, by consequence minimising the criterion below.\nW(S ,C,w) = K \u2211\nk=1\n\u2211\nyi\u2208S k\nDw(yi, ck). (7)\nThis method finds the optimal weight wv for each v \u2208 V from a pre-defined set of feature weights \u2206 = {w : \u2211\nv\u2208V wv = 1,wv \u2265 0, v \u2208 V}. Each partition S (w) =\n{S (w)1 , S (w) 2 , ..., S (w) K } generated by minimising (7) with a different set of weights w \u2208 \u2206 is then evaluated with a generalization of Fisher\u2019s discriminant analysis. In this, one aims to minimise the ratio between the average within-cluster distortion and the average between-cluster distortion.\nCK-Means can be thought of as a gradient descent method that never increases (7), and eventually converges to a local minima solution. This method has introduced a very interesting way to cluster entities described over different feature spaces, something we would dare say is a common characteristic of modern real-world data sets. CK-Means has also shown promising results in experiments (Modha and Spangler 2003), however, the way it finds feature weights has led to claims that generating \u2206 would be difficult in high-dimensional data (Tsai and Chiu 2008; Huang et al. 2005), and that there is no guarantee the optimal weights would be in \u2206 (Huang et al. 2005).\n4.3 Attribute weighting clustering algorithm\nAnother extension to K-Means to support feature weights was introduced by Chan et al. (2004). This algorithm generates a weight wkv for each feature v \u2208 V at each cluster in S = {S 1, S 2, ..., S k, ..., S K}, within the framework of K-Means. This method\nsupports the intuitive idea that different features may have different degrees of relevance at different clusters. This Attribute Weighting algorithm (AWK, for short) attempts to minimise the weighted squared distance between entities yi \u2208 Y and their respective centroids ck \u2208 C, as per the criterion below.\nW(S ,C,w) = K \u2211\nk=1\n\u2211\ni\u2208S k\n\u2211\nv\u2208V\nw\u03b2kvd(yiv, ckv), (8)\nwhere \u03b2 is a user-defined parameter that is greater than 1, d(yiv, ckv) = |yiv \u2212 ckv|2 for a numerical v, and its the simple matching dissimilarity measure below for a categorical v.\nd(yiv, ckv) =\n       0, if yiv = ckv 1, if yiv , ckv.\n(9)\nThe criterion (8) has a computational complexity complexity of O(NMK) (Chan et al. 2004), where M = |V | and is subject to:\n1. A disjoint clustering, in which S k \u2229 S l = \u2205 for k, l = 1, 2, ..., K and k , l. 2. A crisp clustering, given by\n\u2211K k=1 |S k| = N.\n3. \u2211\nv\u2208V wkv = 1 for a given cluster S k. 4. {wkv} \u2265 0 for k = 1, 2, ..., K and v \u2208 V .\nChan et al. (2004) minimises (8) under the above constraints by using partial optimisation for S , C and w. The algorithm begins by setting each wkv = 1/|V |, fixing C and w in order to find the necessary conditions so S minimises (8). Then one fixes S and w, minimising (8) in respect to C. Next, one fixes S and C and minimises (8) in respect to w. This process is repeated until convergence.\nThe minimisations of the first and second steps are rather straight forward. The assignment of entities to the closest cluster S k uses the weighted distance d(yi, ck) = \u2211\nv\u2208V wkv(yiv \u2212 ckv) 2, and since (8) clearly uses the squared Euclidean distance, ckv =\n1 |S k | \u2211 i\u2208S k yiv. The minimisation of (8) is respect to w depends on \u2211 i\u2208S k (yiv \u2212 ckv) 2, generating the three possibilities below.\nwkv =\n                   1 v\u2217 , if \u2211 i\u2208S k (yiv \u2212 ckv) 2 = 0, and v\u2217 = |{v\u2032 : \u2211 i\u2208S k (yiv\u2032 \u2212 ckv\u2032) 2 = 0}|, 0, if \u2211 i\u2208S k (yiv \u2212 ckv) 2 , 0, but \u2211 i\u2208S k (yiv\u2032 \u2212 ckv\u2032) 2 = 0, for some v\u2032 \u2208 V, 1 \u2211\nj\u2208V\n[ \u2211\ni\u2208S k (yiv\u2212ckv )\n2\n\u2211\ni\u2208S k (yi j\u2212ck j )\n2\n] 1 \u03b2\u22121 , if\n\u2211 i\u2208S k (yiv \u2212 ckv) 2 , 0.\n(10) The experiments in Chan et al. (2004) deal solely with \u03b2 > 1. This is probably to avoid the issues related to divisions by zero that \u03b2 = 1 would present in (10), and the behaviour of (8) at other values (for details see Section 4.4). It is interesting to see that DeSarbo et al. (1984) suggested two possible cases for initial weights in SYNCLUS (details in Section 4.1), either to set all weights to the same number, or to be inversely proportional to the variance of the feature in question. It seems to us that Chan\u2019s method have used both suggestion, by initializing each weight wkv to 1/|V | and by optimising wkv so that it is higher when the dispersion of v in yiv \u2208 S k is lower, as the third case in (10) shows.\nThere are some issues to have in mind when using this algorithm. The use of (9) may be problematic in certain cases as the range of d(yiv, ckv) will be different depending on whether v is numerical or categorical. Based on the work of Huang (1998) and Ng and Wong (2002), Chan et al. introduces a new parameter to balance the numerical and categorical parts of a mixed data set, in an attempt to avoid favouring either part. In their paper they test AWK using different values for this parameter and the best is determined as that resulting in the highest cluster recovery accuracy. This approach is rather hard to follow in real-life clustering scenarios as no labelled data would be present. This approach was only discussed in the experiments part of the paper, not being present in the AWK description so it is ignored in our experiments.\nAnother point to note is that their experiments using real-life data sets, despite all explanations about feature weights, use two weights for each feature. One of these relates to the numerical features while the other relates to those that are categorical. This approach was also not explained in the AWK original description and is ignored in our experiments as well.\nA final key issue to this algorithm, and in fact various others, is that there is no clear method to estimate the parameter \u03b2. Instead, the authors state that their method is not sensitive to a range of values of \u03b2, but unfortunately this is demonstrated with experiments on synthetic data in solely two real-world data sets.\n4.4 Weighted K-Means\nHuang et al. (2005) introduced the Weighted K-Means (WK-Means) algorithm. WKMeans attempts to minimise the object function below, which is similar to that of Chan et al. (2004), discussed in Section 4.3. However, unlike the latter, WK-Means originally sets a single weight wv for each feature v \u2208 V .\nW(S ,C,w) = K \u2211\nk=1\n\u2211\ni\u2208S k\n\u2211\nv\u2208V\nw\u03b2vd(yiv, ckv), (11)\nThe Equation above is minimised using an iterative method, optimising (11) for S , C, and w, one at a time. During this process Huang et al. presents the two possibilities below for the update of wv, with S and C fixed, subject to \u03b2 > 1.\nwv =\n           0, if Dv = 0 1\n\u2211h j=1 Dv D j\n1 \u03b2\u22121\n, if Dv , 0, (12)\nwhere,\nDv = K \u2211\nk=1\n\u2211\ni\u2208S k\nd(yiv, ckv), (13)\nand h is the number of features where Dv , 0. If \u03b2 = 1, the minimisation of (11) follows that wv\u2032 = 1, and wv = 0, where v\u2032 , v, and Dv\u2032 \u2264 Dv, for each v \u2208 V (Huang et al. 2005).\nThe weight w\u03b2v in (11) makes the final clustering S , and by consequence the centroids in C, dependant of the value of \u03b2. There are two possible critical values for \u03b2, 0 and 1. If \u03b2 = 0, Equation (11) becomes equivalent to that of K-Means (1). At \u03b2 = 1, the weight of a single feature v \u2208 V is set to one (that with the lowest Dv), while all the others are set to zero. Setting \u03b2 = 1 is probably not desirable in most problems.\nThe above critical values generate three intervals of interest. When \u03b2 < 0, wv increases with an increase in Dv. However, the negative exponent makes w \u03b2 v smaller, so that v has less of an impact on distance calculations. If 0 < \u03b2 < 1, wv increases with an increase in Dv, so does w \u03b2 v . This goes against the principle that a feature with a small dispersion should have a higher weight, proposed by Chan et al. (2004) (perhaps inspired by SYNCLUS, see Section 4.1), and followed by Huang et al. (2005). If \u03b2 > 1, wv decreases with an increase in Dv, and so does w \u03b2 v , very much the desired effect of decreasing the impact of a feature v in (11) whose Dv is high.\nWK-Means was later extended to support fuzzy clustering (Li and Yu 2006), as well as cluster dependant weights (Huang et al. 2008). The latter allows WKMeans to support weights with different degrees of relevance at different clusters, each represented by wkv. This required a change in the criterion to be minimised to W(S ,C,w) =\n\u2211K k=1 \u2211 i\u2208S k \u2211 v\u2208V w \u03b2\nkvd(yiv, ckv), and similar changes to other related equations.\nIn this new version, the dispersion of a variable v \u2208 V at a cluster S k is given by Dkv = \u2211\ni\u2208S k (d(yiv, ckv) + c), where c is a user-defined constant. The authors suggest that in practice c can be chosen as the average dispersion of all features in the data set. More importantly, the adding of c addresses a considerable shortcoming. A feature whose dispersion Dkv in a particular cluster S k is zero should not be assigned a weight of zero when in fact Dkv = 0 indicates that v may be an important feature to identify cluster S k. An obvious exception is if \u2211K k=1 Dkv = 0 for a given v, however, such feature should normally be removed in the data pre-processing stage.\nAlthough there have been improvements, the final clustering is still highly dependant on the exponent exponent \u03b2. It seems to us that the selection of \u03b2 depends on the problem at hand, but unfortunately there is no clear strategy for its selection. We also find that the lack of relationship between \u03b2 and the distance exponent (two in the case of the Euclidean squared distance) avoids the possibility of seen the final weights as feature re-scaling factors. Finally, although WK-Means supports cluster-dependant features, all features are treated as if they were a homogeneous feature space, very much unlike CK-Means (details in Section 4.2).\n4.5 Entropy Weighting K-Means\nThe Entropy Weighting K-Means algorithm (EW-KM) (Jing, Ng, and Huang 2007) minimises the within cluster dispersion while maximising the negative entropy. The reasoning behind this is to stimulate more dimensions to contribute to the identification of clusters in high-dimensional sparse data, avoiding problems related to identifying such clusters using only a few dimensions.\nWith the above in mind, Jing, Ng, and Huang (2007) devised the following criterion for EW-KM:\nW(S ,C,w) = K \u2211\nk=1\n        N \u2211\ni\u2208S k\n\u2211\nv\u2208V\nwkv(yiv \u2212 ckv)2 + \u03b3 \u2211\nv\u2208V\nwkvlogwkv\n        , (14)\nsubject to \u2211\nv\u2208V wkv = 1, {wkv} \u2265 0, and a crisp clustering. In the criterion above, one can easily identify that the first term is the weighted sum of the within cluster dispersion. The second term, in which \u03b3 is a parameter controlling the incentive for clustering in more dimensions, is the negative weight entropy.\nThe calculation of weights in EW-KM occurs as an extra step in relation to KMeans, but still with a time complexity of O(rNMK) where r is the number of iterations the algorithm takes to converge. Given a cluster S k, the weight of each feature v \u2208 V is calculated one at a time with the equation below.\nwkv = exp(\u2212Dkv \u03b3 )\n\u2211 j\u2208V exp( \u2212Dk j \u03b3\n) , (15)\nwhere Dkv represents the dispersion of feature v in the cluster S k, given by Dkv = \u2211\ni\u2208S k (yiv \u2212 ckv) 2. As one would expect, the minimisation of (14) uses partial optimisation for w, C, and S . First, C and w are fixed and (14) is minimised in respect to S . Next, S and w are fixed and (14) is minimised in respect to C. In the final step, S and C are fixed, and (14) is minimised in respect to w. This adds a single step to K-Means, used to calculate feature weights.\nThe R package weightedKmeans found at CRAN includes an implementation of this algorithm, which we decided to use in our experiments (details in Sections 5 and 6). Jing, Ng, and Huang (2007) presents extensive experiments, with synthetic and real-world data. These experiments show EW-KM outperforming various other clustering algorithms. However, there are a few points we should note. First, it is somewhat unclear how a user should choose a precise value for \u03b3. Also, most of the algorithms used in the comparison required a parameter as well. Although we understand it would be too laborious to analyse a large range of parameters for each of these algorithms, there is no much indication on reasoning behind the choices made.\n4.6 Improved K-Prototypes\nJi et al. (2013) have introduced the Improved K-Prototypes clustering algorithm (IKP), which minimises the WK-Means criterion (11), with influences from k-prototype (Huang 1998). IK-P introduces the concept of distributed centroid to clustering, allowing the handling of categorical features by adjusting the distance calculation to take into account the frequency of each category.\nIK-P treats numerical and categorical features differently, but it is still able to represent the cluster S k of a data set Y containing mixed type, data with a single centroid ck = {ck1, ck2, ..., ck|V |}. Given a numerical feature v, ckv = 1|S k | \u2211 i\u2208S k yiv, the center\ngiven by the Euclidean distance. A categorical feature v containing L categories a \u2208 v, has ckv = {{a1v , \u03c9 1 kv}, {a 2 v , \u03c9 2 kv}, ..., {a l v, \u03c9 l kv}, ..., {a L v , \u03c9 L kv}}. This representation for a categorical v allows each category a \u2208 v to have a weight \u03c9lkv = \u2211\ni\u2208S k \u03b7(yiv), directly related to its frequency in the data set Y.\n\u03b7(yiv) =\n       1 \u2211 i\u2208S k 1 , if yiv = a l v,\n0, if yiv , alv. (16)\nSuch modification also requires a re-visit of the distance function in (11). The distance is re-defined to the below.\nd(yiv, ckv) =\n       |yiv \u2212 ckv|, if v is numerical,\n\u03d5(yiv \u2212 ckv), if v is categorical, (17)\nwhere \u03d5(yiv \u2212 ckv) = \u2211K k=1 \u03d1(yiv, a l v),\n\u03d1(yiv, akv) =\n       0, if yiv = alv,\n\u03c9kiv, if yiv , a l v,\n(18)\nIK-P presents some very interesting results (Ji et al. 2013), outperforming other popular clustering algorithms such as k-prototype, SBAC, and KL-FCM-GM (Chatzis 2011; Ji et al. 2012). However, the algorithm still leaves some open questions.\nFor instance, Ji et al. (2013) present experiments on six data sets (two of which being different versions of the same data set) setting \u03b2 = 8, but it is not clear whether the good results provided by this particular \u03b2 would generalize to other data sets. Given a numerical feature, IK-P applies the Manhattan distance (17), however, centroids are calculated using the mean. The center of the Manhattan distance is given by the median rather than the mean, this is probably the reason why Ji et al. found it necessary to allow the user to set a maximum numbers of iterations to their algorithm. Now, even if the algorithm converges, most likely it would converge in a smaller number of iterations if the distance used for the assignments of entities was aligned to that used for obtaining the centroids. Finally, while d(yiv, ckv) for a categorical v has a range in the interval [0, 1], the same is not true if v is numerical, however, Ji et al. (2013) make no mention to data standardization.\n4.7 Intelligent Minkowski Weighted K-Means\nPreviously, we have extended WK-Means (details in Section 4.4) by introducing the intelligent Minkowski Weighted K-Means (iMWK-Means) (De Amorim and Mirkin 2012). In its design, we aimed to propose a deterministic algorithm supporting nonelliptical clusters with weights that could be seen as feature weighting factors. To do so, we combined the Minkowski distance and intelligent K-Means (Mirkin 2012), a method that identifies anomalous patterns in order find the number of clusters in a data set, as well as good initial centroids.\nBelow, we show the Minkowski distance between the entities yi and y j, described over features v \u2208 V .\nd(yi, y j) = ( \u2211\nv\u2208V\n|yiv \u2212 y jv| p)1/p, (19)\nwhere p is a user-defined parameter. If p equals 1, 2, or \u221e, Equation (19) is equivalent to the the Manhattan, Euclidean and Chebyshev distances, respectively. Assuming a given data set has two dimensions (for easy visualisation), the distance bias of a clustering algorithm using (19) would be towards clusters whose shape are any interpolation between a diamond (p = 1) and a square (p = \u221e), clearly going through a circle (p = 2). This is considerably more flexible than algorithms based solely on the squared Euclidean distance, as these recover clusters biased towards circles only. One can also see the Minkowski distance as a multiple of the power mean of the feature-wise differences between yi and y j.\nThe iMWK-Means algorithm calculates distances using (20), a weighted version of the pth root of (19). The use of a root is analogous to the frequent use of the squared Euclidean distance in K-Means.\nd(yi, y j) = \u2211\nv\u2208V\nwpkv|yiv \u2212 y jv| p, (20)\nwhere the user-defined parameter p scales the distance as well as well as the cluster dependent weight wkv. This way the feature weights can be seen as feature re-scaling factors, this is not possible for WK-Means when \u03b2 , 2. Re-scaling a data set with these feature re-scaling factors increases the likelihood of various cluster validity indices to lead to the correct number of clusters (De Amorim and Hennig 2015). With (20) one can reach the iMWK-Means criterion below.\nW(S ,C,w) = K \u2211\nk=1\n\u2211\ni\u2208S k\n\u2211\nv\u2208V\nwpkv|yiv \u2212 ckv| p. (21)\nThe update of wkv, for each v \u2208 V and k = 1, 2, ..., K, follows the equation below.\nwkv = 1\n\u2211 u\u2208V Dkvp Dkup\n1 p\u22121\n, (22)\nwhere the dispersion of feature v in cluster k is now dependant on the exponent p, Dkvp = \u2211 i\u2208S k |yiv \u2212 ckv| p + c, and c is a constant equivalent to the average dispersion. The update of the centroid of cluster S k on feature v, ckv also depends on the value of p. At values of p 1, 2, and \u221e, the center of (19) is given by the median, mean and midrange, respectively. If p < {1, 2,\u221e} then the center can be found using a steepest descend algorithm (De Amorim and Mirkin 2012).\nThe iMWK-Means algorithm deals with categorical features by transforming them in numerical, following a method described by Mirkin (2012). In this method, a given categorical feature v with L categories is replaced by L binary features, each representing one of the original categories. For a given entity yi, only the binary feature representing yiv is set to one, all others are set to zero. The concept of distributed centroid (Ji et al. 2013) can also be applied to our algorithm (De Amorim and\nMakarenkov to appear), however, in order to show a single version of our method we decided not to follow the latter here.\nClearly, the chosen value of p has a considerable impact on the final clustering given by iMWK-Means. De Amorim and Mirkin (2012) introduced a semi-supervised algorithm to estimate a good p, requiring labels for 20% of the entities in Y. Later, the authors showed that it is indeed possible to estimate a good value for p using only 5% of labelled data under the same semi-supervised method, and presented a new unsupervised method to estimate p, requiring no labelled samples (De Amorim and Mirkin 2014).\nThe iMWK-Means proved to be superior to various other algorithms, including WK-Means with cluster dependant weights (De Amorim and Mirkin 2012). However, iMWK-Means also has room for improvement. Calculating a centroid for a p < {1, 2,\u221e} requires the use of a steepest descent method. This can be time consuming, particularly when compared with other algorithms defining ckv = 1|S k | \u2211\ni\u2208S k yiv. Although iMWK-Means allows for a distance bias towards non-elliptical clusters, by setting p , 2, it assumes that all clusters should be biased towards the same shape.\n4.8 Feature Weight Self-Adjustment K-Means\nTsai and Chiu (2008) integrated a feature weight self-adjustment mechanism (FWSA) to K-Means. In this mechanism finding wv for v \u2208 V is modelled as an optimisation problem to simultaneously minimise the separations within clusters and maximise the separation between clusters. The former is measured av = \u2211K k=1 \u2211\ni\u2208S k d(yiv, ckv), where d() is a function returning the distance between the feature v of entity yi and that of centroid ck. The separation between clusters of a given feature v is measured by bv = \u2211K k=1 Nkd(ckv, cv), where Nk is the number of entities in S k, and cv is the center of feature v over yi \u2208 Y. With av and bv one can evaluate how much the feature v contributes to the clustering quality, and in a given iteration j calculate w( j+1)v .\nw( j+1)v = 1 2 (w( j)v + b( j)v /a ( j) v \u2211\nv\u2208V (b ( j) v /a ( j) v )\n), (23)\nwhere the multiplication by 1/2 makes sure that wv \u2208 [0, 1] so it can satisfy the constrain \u2211 v\u2208V w ( j+1) v = 1. With wv one can then minimise the criterion below.\nW(S ,C,w) = K \u2211\nk=1\n\u2211\ni\u2208S k\n\u2211\nv\u2208V\nwv(yiv \u2212 ckv) 2, (24)\nsubject to \u2211\nv\u2208V wv = 1 and {wv}v\u2208V \u2265 0, and a crisp clustering. Experiments in synthetic and real-world data sets compare FWSA K-Means favourably in relation to WK-Means. However, it assumes a homogeneous feature space, and like the previous algorithms it still evaluates a single feature at a time. This means that that a group of features, each irrelevant on its own, but informative if in a group, would be discarded.\nFWSA has already been compared to WK-Means in three real-world data sets (Tsai and Chiu 2008). This comparison shows FWSA reaching an adjusted Rand\nindex (ARI) 0.77 when applied to the Iris data set, while WK-Means reached only 0.75. The latter ARI was obtained by setting \u03b2 = 6, but our experiments (details in Section 6) show that WK-Means may reach 0.81. The difference may be related to how the data was standardised, as well as how \u03b2 was set as here we found the best at each run. Of course, the fact that FWSA does not require a user-defined parameter is a considerable advantage.\n4.9 FG-K-Means\nThe FG-K-Means (FGK) algorithm (Chen et al. 2012) extends K-Means by applying weights at two levels, features and clusters of features. This algorithm has been designed to deal with large data sets whose data comes from multiple sources. Each of the T data sources provides a subset of features G = {G1,G2, ...,GT }, where Gt , \u2205, Gt \u2282 V , Gt \u2229 Gs = \u2205 for t , s and 1 \u2264 t, s \u2264 T , and \u222aGt = V . Given a cluster S k, FGK identifies the degree of relevance of a feature v, represented by wkv, as well as the relevance of a group of features Gt, represented by \u03c9kt. Unlike SYNCLUS (details in Section 4.1), FGK does not require the weights of the groups of features to be entered by the user. FGK updates the K-Means criterion (1) to include both wkv and \u03c9kt, as we show below.\nW(S ,C,w, \u03c9) = K \u2211\nk=1\n        \u2211\ni\u2208S k\nT \u2211\nt=1\n\u2211\nv\u2208Gt\n\u03c9ktwkvd(yiv, ckv) + \u03bb T \u2211\nt=1\n\u03c9ktlog(\u03c9kt) + \u03b7 \u2211\nv\u2208V\nwkvlog(wkv)\n        ,\n(25) where \u03bb and \u03b7 are user-defined parameters, adjusting the distributions of the weights related to the groups of features in G, and each of the features v \u2208 V , respectively. The minimisation of (25) is subject to a crisp clustering in which any given entity yi \u2208 Y is assigned to a single cluster S k. The feature group weights are subject to \u2211K\nk=1 \u03c9kt = 1, 0 < \u03c9kt < 1, for 1 \u2264 t \u2264 T . The feature weights are subject to \u2211\nv\u2208Gt wkv = 1, 0 < wkv < 1, for 1 \u2264 k \u2264 K and 1 \u2264 t \u2264 T . Given a numerical v, the function d in (25) returns the squared Euclidean distance between yiv and ckv, given by (yiv \u2212 ckv)2. A categorical v leads to d returning one if yiv = ckv, and zero otherwise, very much like (9). The update of each feature weight follows the equation below.\nwkv = exp(\u2212Ekv \u03b7 )\n\u2211 h\u2208Gt exp( \u2212Ekh \u03b7\n) , (26)\nwhere Ekv = \u2211\ni\u2208S k \u03c9ktd(yiv, ckv), and t is the index of the feature group to which feature v is assigned to. The update of the feature group weights follows.\n\u03c9kt = exp(\u2212Dkt \u03bb )\n\u2211T s=1 exp( \u2212Dks \u03bb\n) , (27)\nwhere, Dkt = \u2211\ni\u2208S k\n\u2211\nv\u2208Gt wkvd(yiv, ckv). Clusterings generated by FGK are heavily dependant on \u03bb and \u03b7. These parameters must set to positive real values. Large values for \u03bb and \u03b7 lead to weights to be more evenly distributed, so more subspaces\ncontribute to the clustering. Low values lead to weights being more concentrated on fewer subspaces, each of these having a larger contribution to the clustering.\nFGK has a time complexity of O(rNMK), where r is the number of iterations this algorithm takes to complete (Chen et al. 2012). It has outperformed K-Means, WKMeans (see Section 4.4), LAC (Domeniconi et al. 2007) and EW-KM (see Section 4.5), but it also introduces new open questions. This particular method was designed aiming to deal with high-dimensional data, however, it is not clear how \u03bb and \u03b7 should be estimated. This issue makes it rather hard to use FGK in real-world problems. We find that it would also be interesting to see a generalization of this method to use other distance measures, allowing a different distance bias."}, {"heading": "5 Setting of the experiments", "text": "In our experiments we have used real-world as well as synthetic data sets. The former were obtained from the popular UCI machine learning repository (Lichman 2013) and include data sets with different combinations of numerical and categorical features, as we show in Table 1.\nThe synthetic data sets contain spherical Gaussian clusters so that the covariance matrices are diagonal with the same diagonal value \u03c32 generated at each cluster randomly between 0.5 and 1.5. All centroid components were generated independently from a Gaussian distribution with zero mean and unity variance. The cardinality of each cluster was generated following an uniformly random distribution, constrained to a minimum of 20 entities. We have generated 20 data sets under each of the following configurations: (i) 500x4-2, 500 entities over four features partitioned into two clusters; (ii) 500x10-3, 500 entities over 10 features partitioned into three clusters; (iii) 500x20-4, 500 entities over 20 features partitioned into four clusters; (iv) 500x50-5, 500 entities over 50 features partitioned into five clusters.\nUnfortunately, we do not know the degree of relevance of each feature in all of our data sets. For this reason we decided that our experiments should also include data sets to which we have added noise features. Given a data set, for each of its categorical features we have added a new feature composed entirely of uniform random integers (each integer simulates a category). For each of its numerical features we have added a new feature composed entirely of uniform random values. In both cases the new noise feature has the same domain as the original feature. This approach has effectively doubled the number of features in each data set, as well as the number of data sets used in our experiments.\nPrior to our experiments we have standardised the numerical features of each of our data sets as per the equation below.\nyiv = yiv \u2212 yv\nrange(yv) , (28)\nwhere yv = 1N \u2211N i=1 yiv, and range(yv) = max({yiv} N i=1) \u2212 min({yiv} N i=1). Our choice of using (28) instead of the popular z-score is perhaps easier to explain with an example. Lets imagine two features, unimodal v1 and multimodal v2. The standard deviation of v2 would be higher than that of v1 which means that the z-score of v2 would be lower. Thus, the contribution of v2 to the clustering would be lower than that of v1 even so it is v2 that has a cluster structure. Arguably, a disadvantage of using (28) is that it can be detrimental to algorithms based on other standardisation methods (Steinley and Brusco 2008b), not included in this paper.\nWe have standardised the categorical features for all but those experiments with the Attribute weighting and Improved K-Prototypes algorithms (described in Sections 4.3 and 4.6, respectively). These two algorithms define distances for categorical features, so transforming the latter to numerical is not required. Given a categorical feature v containing q categories we substitute v by q new binary features. In a given entity yi only a single of these new features is set to one, that representing the category originally in yiv. We then numerically standardise each of the new features by subtracting it by its mean. The mean of a binary feature is in fact its frequency, so a binary feature representing a category with a high frequency contributes less to the clustering than one with a low frequency. In terms of data pre-processing, we also made sure that all features in each data set had a range higher than zero. Features with a range of zero are not meaningful so they were removed.\nUnfortunately, we found it very difficult to set a fair comparison including all algorithms we describe in this paper. SYNCLUS and FG-K-Means(described in Sections 4.1 and 4.9, respectively) go a step further in feature weighting by allowing weights for feature groups. However, they both require the user to meaningfully group features v \u2208 V into T partitions G = {G1,G2, ...,GT } with SYNCLUS also requesting the user to set a weight for each of these groups. Even if we had enough information to generate G it would be unfair to provide this extra information to some algorithms and not to others. If we were to group features randomly we would be providing these two algorithms with misleading information more often than not, which would surely have an impact on their cluster recovery ability. If we were to set a single group of features and give this group a weight of one then we would be removing the main advantage of using these algorithms, and in fact FG-K-Means would be equivalent to\nEW-KM. Convex K-Means (Section 4.2) also goes a step further in feature weighting, it does so by integrates multiple, heterogeneous feature spaces. Modha and Spangler (2003) demonstrates that with the Euclidean and Spherical cases. However, there is little information regarding the automatic detection of the appropriate space given a feature v \u2208 V , a very difficult problem indeed. For these reasons we decided not to include these three algorithms in our experiments."}, {"heading": "6 Results and discussion", "text": "In our experiments we do have a set of labels for each data set. This allows us to measure the cluster recovery of each algorithm in terms of the adjusted Rand index (ARI) (Hubert and Arabie 1985) between the generated clustering and the known labels.\nARI =\n\u2211\ni j\n(\nni j 2\n)\n\u2212 [ \u2211\ni\n(\nai 2\n)\n\u2211\nj\n(\nb j 2\n) ]/ ( n 2 )\n1 2 [ \u2211 i\n(\nai 2\n)\n+\n\u2211\nj\n(\nb j 2\n)\n] \u2212 [ \u2211\ni\n(\nai 2\n)\n\u2211\nj\n(\nb j 2\n) ]/ ( n 2\n) , (29)\nwhere ni j = |S i \u2229 S j|, ai = \u2211K j=1 |S i \u2229 S j| and bi = \u2211K\ni=1 |S i \u2229 S j|. FWSA is the only algorithm we experiment with that does not require an extra parameter from the user. This is clearly an important advantage as estimating optimum parameters is not a trivial problem, and in many cases the authors of the algorithms do not present a clear estimation method.\nThe experiments we show here do not deal with parameter estimation. Instead, we determine the optimum parameter for a given algorithm by experimenting with values from 1.0 to 5.0 in steps of 0.1. The only exception are the experiments with EW-KM where we apply values from 0 to 5.0 in steps of 0.1, this is because EW-KM is the only algorithm in which a parameter between zero and one is also appropriate. Most of the algorithms we experiment with are non-deterministic (iMWK-Means is the only exception), so we run each algorithm at each parameter 100 times and select as the optimum parameter that with the highest average ARI.\nTables 2 and 3 show the results of our experiments on the real-world data sets without noise features added to them (see Table 1). We show the average (together with the standard deviation) and maximum ARI for what we found to be the optimum parameter for each data set we experiment with. There are different comparisons we can make, particularly when one of the algorithms is deterministic, iMWK-Means. If we compare the algorithms in terms of their expected ARI, given a good parameter, then we can see that in 9 data sets iMWK-Means reaches the highest ARI. We run each non-deterministic algorithm 100 times for each parameter value. If we take into account solely the highest ARI over these 100 runs then the EW-KM reaches the highest ARI overall in 9 data sets, while IK-P does so in six. Still looking only at the highest ARI, the FWSA algorithm (the only algorithm not to require an extra parameter) reaches the highest ARI in four data sets, the same number as AWK and WK-Means. Another point of interest is that the best parameter we could find for iMWK-Means was the same in four data sets.\nTables 4 and 5 show the results of our experiments on the real-world data sets with noise features added to them. Given a data set Y, for each v \u2208 V we add a new\nfeature to Y composed entirely of uniform random values (integers in the case of a categorical v) with the same domain as v. This effectively doubles the cardinality of V . In this set of experiments the Improved K-Prototype was unable to find eight clusters in the Ecoli data set. We believe this issue is related to the data spread. The third feature of this particular data set has only 10 entities with a value other than 0.48. The fourth feature has a single entity with a value other than 0.5. Clearly on the top of these two issues we have an extra seven noise features. Surely one could argue that features three and four could be removed from the data set as they are unlikely to be informative. However, we decided not to start opening concessions to\nalgorithms. Instead we expect the algorithms to find these issues and deal with them. This turn, when comparing expected ARI values given a good parameter, iMWKMeans reaches the highest ARI value in 8 data sets. It ceased to reach the highest ARI in the Australian data set in which it now reaches 0.22 while EW-KM reaches 0.34 (that is 0.3 more than in the experiments with no noise features, but such small inconsistencies are to be expected in experiments with non-deterministic algorithms). When comparing the maximum possible for each algorithm the WK-Means algorithm does reach the highest ARI in six data sets, while EW-KM does so in five.\nTables 6 and 7 show the results of our experiments on the synthetic data sets with and without noise features. Given a data set Y, for each v \u2208 V we have added a new feature to Y containing uniformly random noise in the same domain as that of v, very much like what we did in the real-world data sets. The only difference is that in the synthetic data sets we do not have categorical features and we know that they contain Gaussian clusters (see Section 5). We have 20 data sets for each of the data set configurations, hence, the values under max represent the average of the maximum ARI obtained in each of the 20 data sets, as well as the standard deviation of these values.\nIn this set of experiments iMWK-Means reached the highest expected ARI in all data sets, with and without noise features. If we compare solely the maximum possible ARI per algorithm WK-Means reaches the highest ARI in three data set configurations with no noise features added to them, and in two of the data sets with noise features. AWK also reaches the highest ARI in two of the configurations Clearly,\nthere are other comparisons we can make using all algorithms described in Section 4. Based on the information we present in Section 4 about each algorithm, as well as the cluster recovery results we present in this section, we have defined eight characteristics we believe are desirable for any K-Means based clustering algorithm that implements feature weighting. Table 8 shows our comparison, which we now describe one characteristic at a time.\nNo extra user-defined parameter. Quite a few of the algorithms we describe in Section 4 require an extra parameter to be defined by the user. By tuning this parameter (or these parameters, in the case of FGK) each of these algorithms is able to achieve high accuracy in terms of cluster recovery. However, it seems to us that this parameter estimation is a non-trivial task, particularly because the optimum value is problem dependant. This makes it very difficult to suggest a generally good parameter value (of course this may not be the case if one knows how the data is distributed). Since different values for a parameter tend to result in different clusterings, one could attempt to estimate the best clustering by applying a clustering validation index (Arbelaitz et al. 2013; De Amorim and Mirkin 2014), consensus clustering (Goder and\nFilkov 2008), or even a semi-supervised learning approach (De Amorim and Mirkin 2012). Regarding the latter, we have previously demonstrated that with as low as 5% of the data being labelled it is still possible to estimate a good parameter for iMWKMeans (De Amorim and Mirkin 2014).\nIt is deterministic. A K-Means generated clustering heavily depends on the initial centroids this algorithm uses. These initial centroids are often found at random, meaning that if K-Means is run twice, it may generate very different clusterings. It is often necessary to run this algorithm a number of times and then somehow identify which clustering is the best (again, perhaps using a clustering validation index, a consensus approach, or in the case of this particular algorithm the output of its criterion). If a K-Means based feature weighting algorithm is also non-deterministic, chances are one will have to determine the best parameter and then the best run when applying that parameter. One could also run the algorithm many times per parameter and apply a clustering validation index to each of the generated clusterings. In any case, this can be a very computationally intensive task. We find it that the best approach would be to have a feature weighting algorithm that is deterministic, requiring the algorithm to be run a single time. The iMWK-Means algorithm applies a weighted Minkowski metric based version of the intelligent K-Means (Mirkin 2012). The latter algorithm finds anomalous clusters in a given data set and uses this information to generate initial centroids, making iMWK-Means a deterministic algorithm.\nAccepts different distance bias. Any distance in use will lead to a bias in the clustering. For instance, the Euclidean distance is biased towards spherical shapes while the Manhattan distance is biased towards diamond shapes. A good clustering algorithm should allow for the alignment of its distance bias to the data at hand. Two of the algorithms we analyse address this issue, but in very different ways. CK-Means is able to integrate multiple, heterogeneous feature spaces into K-Means, this means that each feature may use a different distance measure, and by consequence have a different bias. This is indeed a very interesting, and intuitive approach, as features measure\ndifferent things so they may be in different spaces. The iMWK-Means also allows for different distance bias, it does so by using the Lp metric, leaving the exponent p as a user-defined parameter (see Equation 20). Different values for the exponent p lead to different distance biases. However, this algorithm still assumes that all clusters in the data set have the same bias.\nSupports at least two weights per feature. In order to model the degree of relevance of a particular feature one may need more than a single weight. There are two very different cases that one should take into consideration: (i) a given feature v \u2208 V may be considerably informative when attempting to discriminate a cluster S k, but not so for other clusters. This leads to the intuitive idea that v should in fact have K weights. This approach is followed by AWK, WK-Means (in its updated version, see Huang et al. 2008), EWK-Means, iMWK-Means and FGK; (ii) a given feature v \u2208 V may be not be, on its own, informative to any cluster S k \u2208 S . However, the same feature may be informative when grouped with other features. Generally speaking, two (or more) features that are useless by themselves may be useful together (Guyon and Elisseeff 2003). FGK is the only algorithm we analyse that calculates weights for groups of features.\nFeatures grouped automatically. If a feature weighting algorithm should take into consideration the weights of groups of features, it should also be able to group features on its own. This is probably the most controversial of the characteristics we analyse because none of the algorithms we deal with here is able to do so. We present this characteristic in Table 8 to emphasise its importance. Both algorithms that deal with weights for groups of features, SYNCLUS and FGK, require the users to group the features themselves. We believe that perhaps an approach based on bi-clustering (Mirkin 1998) could address this issue.\nCalculates all used feature weights. This is a basic requirement of any feature weighting algorithm. It should be able to calculate all feature weights it needs. Of course a given algorithm may support initial weights being provided by the user, but it should also be able to optimise these if needed. SYNCLUS requires the user to input the weights for groups of features and does not optimise these. CK-Means requires all possible weights to be put in a set \u2206 = {w : \u2211\nv\u2208V wv = 1,wv \u2265 0, v \u2208 V} and then tests each possible subset of \u2206, the weights are not calculated. This approach can be very time consuming, particularly in high-dimensional data.\nSupports categorical features. Data sets often contain categorical features. These features may be transformed to numerical values, however, such transformation may lead to loss of information and considerable increase in dimensionality. Most of the analysed algorithms that support categorical features do so by setting a simple matching dissimilarity measure (eg. AWK, WK-Means and FGK). This binary dissimilarity is zero iff both features have exactly the same category (see for instance Equation 9), and one otherwise. IK-P presents a different and interesting approach taking into account the frequency of each category at a categorical v. This allows for a continuous dissimilarity measure in the interval [0,1].\nAnalyses groups of features. Since two features that are useless by themselves may be useful together (Guyon and Elisseeff 2003), a feature weighting algorithm should be able to calculate weights for groups of features. Only a single algorithm we have analysed is able to do so, FGK. SYNCLUS also uses weights for groups of\nfeatures, however, these are input by the user rather than calculated by the algorithm."}, {"heading": "7 Conclusion and future directions", "text": "Recent technology has made it incredibly easy to acquire vast amounts of real-world data. Such data tend to be described in high-dimensional spaces, forcing data scientists to address difficult issues related to the curse of dimensionality. Dimensionality reduction in machine learning is commonly done using feature selection algorithms, in most cases during the data pre-processing stage. This type of algorithm can be very useful to select relevant features in a data set, however, they assume that all relevant features have the same degree of relevance, which is often not the case.\nFeature weighting is a generalisation of feature selection. The former models the degree of relevance of a given feature by giving it a weight, normally in the interval [0, 1]. Feature weighting algorithms can also deselect a feature, very much like feature selection algorithms, by simply setting its weight to zero. K-Means is arguably the most popular partitional clustering algorithm. Efforts to integrate feature weighting in K-Means have been done for the last 30 years (for details, see Section 4).\nIn this paper we have provided the reader with a discussion on nine of the most popular or innovative feature weighting mechanisms for K-Means. Our survey also\npresents an empirical comparison including experiments in real-world and synthetic data sets, both with and without noise features. Because of the difficulties of presenting a fair empirical comparison (see Section 5) we experimented with six of the nine algorithms discussed. Our survey shows some issues that are somewhat common in these algorithms and could be addressed in future research. For instance, each of the algorithms we discuss presents at least one of the following issues:\n(i) the criterion to be minimised includes a new parameter (or more), but unfortunately there is no clear strategy for the selection of a precise value for this parameter. This issue applies to most algorithms we discussed. Future research could address this issue in different ways. For instance, a method could use one or more clustering validation indices (for a recent comparison of these, see Arbelaitz et al. 2013) to measure the quality of clusterings obtained applying different parameter values. It could also apply a consensus clustering based approach (Goder and Filkov 2008), assuming that two entities that should belong to the same cluster are indeed clustered together by a given algorithm more often than not, over different parameter values. methods developed in future research could also apply a semi-supervised approach, this could require as low as 5% of the data being labelled in order to estimate a good parameter (De Amorim and Mirkin 2014).\n(ii) the method treats all features as if they were in the same feature space, often not the case in real-world data. CK-Means is an exception to this rule, it integrates multiple, heterogeneous feature spaces. It would be interesting to see this idea expanded in future research to other feature weighting algorithms. Another possible approach to this issue would be to measure dissimilarities using different distance measures but compare them using a comparable scale, for instance the distance scaled by the sum of the data scatter. Of course this could lead to new problems, such as for instance defining what distance measure should be used at each feature.\n(iii) the method assumes that all clusters in a given data set should have the same distance bias. It is intuitive that different clusters in a given data set may have different shapes. However, in the algorithms we discuss when a dissimilarity measure is chosen it introduces a shape bias that is the same for all clusters in the data set. Future research could address this issue by allowing different distance measures at different clusters, leading to different shape biases. However, this could be difficult to achieve given what we argue in (ii) and that one would need to align each cluster to the bias of a distance measure.\n(iv) features are evaluated one at a time, presenting difficulties for cases when the discriminatory information is present in a group of features, but not in any single feature of this group. In order to deal with this issue a clustering method should be able to group such features and calculate a weight for the group. Perhaps the concept of bi-clustering (Mirkin 1998) could be extended in future research by clustering features and entities, but also weighting features and groups of features.\nThe above ideas for future research address indeed some of the major problems we have today in K-Means based feature weighting algorithms. Of course this does not mean they are easy to implement, in fact we acknowledge quite the opposite."}], "references": [{"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. DEMPSTER", "N.M. LAIRD", "D.B. RUBIN"], "venue": "In: Journal of the Royal statistical Society 39.1, pp. 1\u201338.", "citeRegEx": "DEMPSTER et al\\.,? 1977", "shortCiteRegEx": "DEMPSTER et al\\.", "year": 1977}, {"title": "A maximum likelihood methodology for clusterwise linear regression", "author": ["W.S. DESARBO", "W.L. CRON"], "venue": "In: Journal of classification 5.2, pp. 249\u2013282.", "citeRegEx": "DESARBO and CRON,? 1988", "shortCiteRegEx": "DESARBO and CRON", "year": 1988}, {"title": "Constrained classification: the use of a priori information in cluster analysis", "author": ["W.S. DESARBO", "V. MAHAJAN"], "venue": "In: Psychometrika 49.2, pp. 187\u2013215.", "citeRegEx": "DESARBO and MAHAJAN,? 1984", "shortCiteRegEx": "DESARBO and MAHAJAN", "year": 1984}, {"title": "Synthesized clustering: A method for amalgamating alternative clustering bases with differential weighting of variables", "author": ["W.S. DESARBO", "J.D. CARROLL", "L.A. CLARK", "P.E. GREEN"], "venue": "In: Psychometrika 49.1, pp. 57\u201378.", "citeRegEx": "DESARBO et al\\.,? 1984", "shortCiteRegEx": "DESARBO et al\\.", "year": 1984}, {"title": "Efficient feature selection in conceptual clustering", "author": ["M. DEVANEY", "A. RAM"], "venue": "In: Proceedings of the 14th International Conference in Machine Learning. Nashville, TN, pp. 92\u201397.", "citeRegEx": "DEVANEY and RAM,? 1997", "shortCiteRegEx": "DEVANEY and RAM", "year": 1997}, {"title": "K-means clustering via principal component analysis", "author": ["C. DING", "X. HE"], "venue": "In: Proceedings of the twenty-first international conference on Machine learning. ACM, p. 29.", "citeRegEx": "DING and HE,? 2004", "shortCiteRegEx": "DING and HE", "year": 2004}, {"title": "Locally adaptive metrics for clustering high dimensional data", "author": ["C. DOMENICONI", "D. GUNOPULOS", "S. MA", "B. YAN", "M. AL-RAZGAN", "D. PAPADOPOULOS"], "venue": "In: Data Mining and Knowledge Discovery 14.1, pp. 63\u201397.", "citeRegEx": "DOMENICONI et al\\.,? 2007", "shortCiteRegEx": "DOMENICONI et al\\.", "year": 2007}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["P. DRINEAS", "A. FRIEZE", "R. KANNAN", "S. VEMPALA", "V. VINAY"], "venue": "In: Machine learning 56.1-3, pp. 9\u201333.", "citeRegEx": "DRINEAS et al\\.,? 2004", "shortCiteRegEx": "DRINEAS et al\\.", "year": 2004}, {"title": "Unsupervised feature selection", "author": ["DY J.G."], "venue": "In: Computational Methods of Feature Selection. Ed. by LIU, H. and MOTODA, H. Data Mining & Knowledge Discovery. Chapman & Hall/CRC, pp. 19\u201339.", "citeRegEx": "G.,? 2008", "shortCiteRegEx": "G.", "year": 2008}, {"title": "Exploring the conditional coregulation of yeast gene expression through fuzzy k-means clustering", "author": ["A.P. GASCH", "M.B. EISEN"], "venue": "In: Genome Biol 3.11, pp. 1\u201322.", "citeRegEx": "GASCH and EISEN,? 2002", "shortCiteRegEx": "GASCH and EISEN", "year": 2002}, {"title": "Weighting and selection of variables for cluster analysis", "author": ["R. GNANADESIKAN", "J.R. KETTENRING", "S.L. TSAO"], "venue": "In: Journal of Classification 12.1, pp. 113\u2013136.", "citeRegEx": "GNANADESIKAN et al\\.,? 1995", "shortCiteRegEx": "GNANADESIKAN et al\\.", "year": 1995}, {"title": "Consensus Clustering Algorithms: Comparison and Refinement.", "author": ["A. GODER", "V. FILKOV"], "venue": "In: ALENEX", "citeRegEx": "GODER and FILKOV,? \\Q2008\\E", "shortCiteRegEx": "GODER and FILKOV", "year": 2008}, {"title": "A preliminary study of optimal variable weighting in k-means clustering", "author": ["P.E. GREEN", "J. KIM", "F.J. CARMONE"], "venue": "In: Journal of Classification 7.2, pp. 271\u2013285.", "citeRegEx": "GREEN et al\\.,? 1990", "shortCiteRegEx": "GREEN et al\\.", "year": 1990}, {"title": "An introduction to variable and feature selection", "author": ["I. GUYON", "A. ELISSEEFF"], "venue": "In: The Journal of Machine Learning Research 3, pp. 1157\u20131182.", "citeRegEx": "GUYON and ELISSEEFF,? 2003", "shortCiteRegEx": "GUYON and ELISSEEFF", "year": 2003}, {"title": "Automated variable weighting in k-means type clustering", "author": ["HUANG J.Z.", "NG M.K.", "RONG H.", "LI", "Z."], "venue": "In: IEEE Transactions on Pattern Analysis and Machine Intelligence 27.5, pp. 657\u2013668.", "citeRegEx": "Z. et al\\.,? 2005", "shortCiteRegEx": "Z. et al\\.", "year": 2005}, {"title": "Weighting method for feature selection in K-means", "author": ["HUANG J.Z.", "XU J.", "NG M.", "YE", "Y."], "venue": "In: Computational Methods of Feature Selection. Ed. by LIU, Huan and MOTODA, Hiroshi. Data Mining & Knowledge Discovery. Chapman & Hall/CRC, pp. 193\u2013209.", "citeRegEx": "Z. et al\\.,? 2008", "shortCiteRegEx": "Z. et al\\.", "year": 2008}, {"title": "Extensions to the k-means algorithm for clustering large data sets with categorical values", "author": ["Z. HUANG"], "venue": "In: Data mining and knowledge discovery 2.3, pp. 283\u2013 304.", "citeRegEx": "HUANG,? 1998", "shortCiteRegEx": "HUANG", "year": 1998}, {"title": "Comparing partitions", "author": ["L. HUBERT", "P. ARABIE"], "venue": "In: Journal of classification 2.1, pp. 193\u2013218.", "citeRegEx": "HUBERT and ARABIE,? 1985", "shortCiteRegEx": "HUBERT and ARABIE", "year": 1985}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. JAIN"], "venue": "In: Pattern Recognition Letters 31.8, pp. 651\u2013666. DOI: 10.1016/j.patrec.2009.09.011.", "citeRegEx": "JAIN,? 2010", "shortCiteRegEx": "JAIN", "year": 2010}, {"title": "A fuzzy k-prototype clustering algorithm for mixed numeric and categorical data", "author": ["J. JI", "W. PANG", "C. ZHOU", "X. HAN", "Z. WANG"], "venue": "In: KnowledgeBased Systems 30, pp. 129\u2013135. DOI: 10.1016/j.knosys.2012.01.006.", "citeRegEx": "JI et al\\.,? 2012", "shortCiteRegEx": "JI et al\\.", "year": 2012}, {"title": "An improved k-prototypes clustering algorithm for mixed numeric and categorical data", "author": ["JI J.", "BAI T.", "ZHOU C.", "MA C.", "WANG Z."], "venue": "In: Neurocomputing 120, pp. 590\u2013596. DOI: 10.1016/j.neucom.2013.04.011.", "citeRegEx": "J. et al\\.,? 2013", "shortCiteRegEx": "J. et al\\.", "year": 2013}, {"title": "An entropy weighting k-means algorithm for subspace clustering of high-dimensional sparse data", "author": ["L. JING", "M.K. NG", "J.Z. HUANG"], "venue": "In: IEEE Transactions on Knowledge and Data Engineering 19.8, pp. 1026\u20131041.", "citeRegEx": "JING et al\\.,? 2007", "shortCiteRegEx": "JING et al\\.", "year": 2007}, {"title": "Wrappers for feature subset selection", "author": ["R. KOHAVI", "G.H. JOHN"], "venue": "In: Artificial intelligence 97.1, pp. 273\u2013324.", "citeRegEx": "KOHAVI and JOHN,? 1997", "shortCiteRegEx": "KOHAVI and JOHN", "year": 1997}, {"title": "A novel fuzzy c-means clustering algorithm", "author": ["LI C.", "YU J."], "venue": "In: Rough Sets and Knowledge Technology. Springer, pp. 510\u2013515.", "citeRegEx": "C. and J.,? 2006", "shortCiteRegEx": "C. and J.", "year": 2006}, {"title": "UCI Machine Learning Repository", "author": ["M. LICHMAN"], "venue": "URL: http://archive.ics.uci.edu/ml.", "citeRegEx": "LICHMAN,? 2013", "shortCiteRegEx": "LICHMAN", "year": 2013}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["LIU H.", "YU L."], "venue": "In: IEEE Transactions on Knowledge and Data Engineering 17.4, pp. 491\u2013502.", "citeRegEx": "H. and L.,? 2005", "shortCiteRegEx": "H. and L.", "year": 2005}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MACQUEEN"], "venue": "In: Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. 14. Bekerley, CA, USA: University of California Press, pp. 281\u2013297.", "citeRegEx": "MACQUEEN,? 1967", "shortCiteRegEx": "MACQUEEN", "year": 1967}, {"title": "Optimal variable weighting for ultrametric and additive trees and K-means partitioning: Methods and software", "author": ["V. MAKARENKOV", "P. LEGENDRE"], "venue": "In: Journal of Classification 18.2, pp. 245\u2013271.", "citeRegEx": "MAKARENKOV and LEGENDRE,? 2001", "shortCiteRegEx": "MAKARENKOV and LEGENDRE", "year": 2001}, {"title": "Mathematical classification and clustering: From how to what and why", "author": ["B. MIRKIN"], "venue": "Springer.", "citeRegEx": "MIRKIN,? 1998", "shortCiteRegEx": "MIRKIN", "year": 1998}, {"title": "Unsupervised feature selection using feature similarity", "author": ["P. MITRA", "C.A. MURTHY", "PAL", "S.K."], "venue": "In: IEEE transactions on pattern analysis and machine intelligence 24.3, pp. 301\u2013312.", "citeRegEx": "MITRA et al\\.,? 2002", "shortCiteRegEx": "MITRA et al\\.", "year": 2002}, {"title": "Feature weighting in k-means clustering", "author": ["D.S. MODHA", "W.S. SPANGLER"], "venue": "In: Machine learning 52.3, pp. 217\u2013237.", "citeRegEx": "MODHA and SPANGLER,? 2003", "shortCiteRegEx": "MODHA and SPANGLER", "year": 2003}, {"title": "Complexities of hierarchic clustering algorithms: State of the art", "author": ["F. MURTAGH"], "venue": "In: Computational Statistics Quarterly 1.2, pp. 101\u2013113.", "citeRegEx": "MURTAGH,? 1984", "shortCiteRegEx": "MURTAGH", "year": 1984}, {"title": "Methods of hierarchical clustering", "author": ["F. MURTAGH", "P. CONTRERAS"], "venue": "In: arXiv preprint arXiv:1105.0121.", "citeRegEx": "MURTAGH and CONTRERAS,? 2011", "shortCiteRegEx": "MURTAGH and CONTRERAS", "year": 2011}, {"title": "Clustering categorical data sets using tabu search techniques", "author": ["M.K. NG", "J.C. WONG"], "venue": "In: Pattern Recognition 35.12, pp. 2783\u20132790.", "citeRegEx": "NG and WONG,? 2002", "shortCiteRegEx": "NG and WONG", "year": 2002}, {"title": "Computational methods in optimization: a unified approach", "author": ["E. POLAK"], "venue": "Academic press.", "citeRegEx": "POLAK,? 1971", "shortCiteRegEx": "POLAK", "year": 1971}, {"title": "Numerical taxonomy", "author": ["P.H.A. SNEATH", "R.R. SOKAL"], "venue": "The principles and practice of numerical classification. W.H.Freeman & Co Ltd.", "citeRegEx": "SNEATH and SOKAL,? 1973", "shortCiteRegEx": "SNEATH and SOKAL", "year": 1973}, {"title": "Sur la division des corp materiels en parties", "author": ["H. STEINHAUS"], "venue": "In: Bull. Acad. Polon. Sci 1, pp. 801\u2013804.", "citeRegEx": "STEINHAUS,? 1956", "shortCiteRegEx": "STEINHAUS", "year": 1956}, {"title": "K-means clustering: a half-century synthesis", "author": ["D. STEINLEY"], "venue": "In: British Journal of Mathematical and Statistical Psychology 59.1, pp. 1\u201334.", "citeRegEx": "STEINLEY,? 2006", "shortCiteRegEx": "STEINLEY", "year": 2006}, {"title": "Selection of variables in cluster analysis: An empirical comparison of eight procedures", "author": ["D. STEINLEY", "BRUSCO", "Michael J."], "venue": "In: Psychometrika 73.1, pp. 125\u2013144.", "citeRegEx": "STEINLEY et al\\.,? 2008a", "shortCiteRegEx": "STEINLEY et al\\.", "year": 2008}, {"title": "A new variable weighting and selection procedure for K-means cluster analysis", "author": ["STEINLEY", "Douglas", "BRUSCO", "Michael J"], "venue": "In: Multivariate Behavioral Research 43.1, pp. 77\u2013108.", "citeRegEx": "STEINLEY et al\\.,? 2008b", "shortCiteRegEx": "STEINLEY et al\\.", "year": 2008}, {"title": "Genesis: cluster analysis of microarray data", "author": ["A. STURN", "J. QUACKENBUSH", "Z. TRAJANOSKI"], "venue": "In: Bioinformatics 18.1, pp. 207\u2013208.", "citeRegEx": "STURN et al\\.,? 2002", "shortCiteRegEx": "STURN et al\\.", "year": 2002}, {"title": "Feature selection as a preprocessing step for hierarchical clustering", "author": ["L. TALAVERA"], "venue": "In: Proceedings of the 16th International Conference in Machine Learning. Bled, Slovenia, pp. 389\u2013397.", "citeRegEx": "TALAVERA,? 1999", "shortCiteRegEx": "TALAVERA", "year": 1999}, {"title": "Developing a feature weight self-adjustment mechanism for a K-means clustering algorithm", "author": ["C.Y. TSAI", "C.C. CHIU"], "venue": "In: Computational statistics & data analysis 52.10, pp. 4658\u20134672.", "citeRegEx": "TSAI and CHIU,? 2008", "shortCiteRegEx": "TSAI and CHIU", "year": 2008}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. VEDALDI", "B. FULKERSON"], "venue": "In: Proceedings of the international conference on Multimedia. ACM, pp. 1469\u20131472.", "citeRegEx": "VEDALDI and FULKERSON,? 2010", "shortCiteRegEx": "VEDALDI and FULKERSON", "year": 2010}, {"title": "A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms", "author": ["D. WETTSCHERECK", "D.W. AHA", "T. MOHRI"], "venue": "In: Artificial Intelligence Review 11.1-5, pp. 273\u2013314.", "citeRegEx": "WETTSCHERECK et al\\.,? 1997", "shortCiteRegEx": "WETTSCHERECK et al\\.", "year": 1997}, {"title": "Fuzzy sets", "author": ["L.A. ZADEH"], "venue": "In: Information and control 8.3, pp. 338\u2013353.", "citeRegEx": "ZADEH,? 1965", "shortCiteRegEx": "ZADEH", "year": 1965}, {"title": "Spectral relaxation for k-means clustering", "author": ["ZHA H.", "HE X.", "DING C.", "GU M.", "SIMON H.D."], "venue": "In: Advances in neural information processing systems, pp. 1057\u20131064.", "citeRegEx": "H. et al\\.,? 2001", "shortCiteRegEx": "H. et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 37, "context": "69x1013 different partitions if K is set to four (Steinley 2006).", "startOffset": 49, "endOffset": 64}, {"referenceID": 43, "context": "Clustering has been used to solve problems in the most diverse fields such as computer vision, text mining, bioinformatics, and data mining (Vedaldi and Fulkerson 2010; Steinley 2006; Jain 2010; Sturn, Quackenbush, and Trajanoski 2002; Huang et al. 2008; Gasch and Eisen 2002; Mirkin 2012).", "startOffset": 140, "endOffset": 289}, {"referenceID": 37, "context": "Clustering has been used to solve problems in the most diverse fields such as computer vision, text mining, bioinformatics, and data mining (Vedaldi and Fulkerson 2010; Steinley 2006; Jain 2010; Sturn, Quackenbush, and Trajanoski 2002; Huang et al. 2008; Gasch and Eisen 2002; Mirkin 2012).", "startOffset": 140, "endOffset": 289}, {"referenceID": 18, "context": "Clustering has been used to solve problems in the most diverse fields such as computer vision, text mining, bioinformatics, and data mining (Vedaldi and Fulkerson 2010; Steinley 2006; Jain 2010; Sturn, Quackenbush, and Trajanoski 2002; Huang et al. 2008; Gasch and Eisen 2002; Mirkin 2012).", "startOffset": 140, "endOffset": 289}, {"referenceID": 9, "context": "Clustering has been used to solve problems in the most diverse fields such as computer vision, text mining, bioinformatics, and data mining (Vedaldi and Fulkerson 2010; Steinley 2006; Jain 2010; Sturn, Quackenbush, and Trajanoski 2002; Huang et al. 2008; Gasch and Eisen 2002; Mirkin 2012).", "startOffset": 140, "endOffset": 289}, {"referenceID": 31, "context": "This extra information regarding the relationships between clusters comes at a considerable cost, leading to a time complexity of O(N2), or even O(N3) depending on the actual algorithm in use (Murtagh 1984; Murtagh and Contreras 2011).", "startOffset": 192, "endOffset": 234}, {"referenceID": 32, "context": "This extra information regarding the relationships between clusters comes at a considerable cost, leading to a time complexity of O(N2), or even O(N3) depending on the actual algorithm in use (Murtagh 1984; Murtagh and Contreras 2011).", "startOffset": 192, "endOffset": 234}, {"referenceID": 26, "context": "K-Means (MacQueen 1967; Ball and Hall 1967; Steinhaus 1956) is arguably the most popular of such algorithms (for more details see Section 2).", "startOffset": 8, "endOffset": 59}, {"referenceID": 36, "context": "K-Means (MacQueen 1967; Ball and Hall 1967; Steinhaus 1956) is arguably the most popular of such algorithms (for more details see Section 2).", "startOffset": 8, "endOffset": 59}, {"referenceID": 45, "context": "Among the many extensions to K-Means, we have Fuzzy C-Means (Bezdek 1981) which applies Fuzzy set theory (Zadeh 1965) to allow a given entity yi to be assigned to each cluster in S at different degrees of membership.", "startOffset": 105, "endOffset": 117}, {"referenceID": 18, "context": "K-Means is arguably the most popular partitional clustering algorithm (Jain 2010; Steinley 2006; Mirkin 2012).", "startOffset": 70, "endOffset": 109}, {"referenceID": 37, "context": "K-Means is arguably the most popular partitional clustering algorithm (Jain 2010; Steinley 2006; Mirkin 2012).", "startOffset": 70, "endOffset": 109}, {"referenceID": 26, "context": "The K-Means criterion we show, (1), applies the squared Euclidean distance as in its original definition (MacQueen 1967; Ball and Hall 1967).", "startOffset": 105, "endOffset": 140}, {"referenceID": 7, "context": "K-Means also has a strong relation with Principal Component Analysis, the latter can be seen as a relaxation of the former (Zha et al. 2001; Drineas et al. 2004; Ding and He 2004).", "startOffset": 123, "endOffset": 179}, {"referenceID": 5, "context": "K-Means also has a strong relation with Principal Component Analysis, the latter can be seen as a relaxation of the former (Zha et al. 2001; Drineas et al. 2004; Ding and He 2004).", "startOffset": 123, "endOffset": 179}, {"referenceID": 42, "context": "Apart from the problem above, there is a considerable consensus in the research community that meaningful clusters, particularly those in high-dimensional data, occur in subspaces defined by a specific subset of features (Tsai and Chiu 2008; Liu and Yu 2005; Chen et al. 2012; De Amorim and Mirkin 2012).", "startOffset": 221, "endOffset": 303}, {"referenceID": 30, "context": "Feature weighting can be thought of as a generalization of feature selection (Wettschereck, Aha, and Mohri 1997; Modha and Spangler 2003; Tsai and Chiu 2008).", "startOffset": 77, "endOffset": 157}, {"referenceID": 42, "context": "Feature weighting can be thought of as a generalization of feature selection (Wettschereck, Aha, and Mohri 1997; Modha and Spangler 2003; Tsai and Chiu 2008).", "startOffset": 77, "endOffset": 157}, {"referenceID": 22, "context": "Feature selection methods for unlabelled data follow either a filter or wrapper approach (Dy 2008; Kohavi and John 1997).", "startOffset": 89, "endOffset": 120}, {"referenceID": 4, "context": "There are a considerable amount of unsupervised feature selection methods, some of which can be easily used in the data pre-processing stage (Devaney and Ram 1997; Talavera 1999; Mitra, Murthy, and Pal 2002) to either select or deselect features from V .", "startOffset": 141, "endOffset": 207}, {"referenceID": 41, "context": "There are a considerable amount of unsupervised feature selection methods, some of which can be easily used in the data pre-processing stage (Devaney and Ram 1997; Talavera 1999; Mitra, Murthy, and Pal 2002) to either select or deselect features from V .", "startOffset": 141, "endOffset": 207}, {"referenceID": 7, "context": "The former uses properties of the data itself to select a subset of features during the data pre-processing phase. The features are selected before the clustering algorithm is run, making this approach usually faster. However, this speed comes at price. It can be rather difficult to define whether a given feature is relevant without applying clustering to the data. Methods following a wrapper approach make use of the information given by a clustering algorithm when selecting features. Often, these methods lead to better performance when compared to those following a filter approach (Dy 2008). However, these also tend to be more computationally intensive as the clustering and the feature selection algorithms are run. The surveys by Dy (2008), Steinley and Brusco (2008), and Guyon and Elisseeff (2003) are, in our opinion, a very good starting point for those readers in need of more information.", "startOffset": 82, "endOffset": 751}, {"referenceID": 7, "context": "The former uses properties of the data itself to select a subset of features during the data pre-processing phase. The features are selected before the clustering algorithm is run, making this approach usually faster. However, this speed comes at price. It can be rather difficult to define whether a given feature is relevant without applying clustering to the data. Methods following a wrapper approach make use of the information given by a clustering algorithm when selecting features. Often, these methods lead to better performance when compared to those following a filter approach (Dy 2008). However, these also tend to be more computationally intensive as the clustering and the feature selection algorithms are run. The surveys by Dy (2008), Steinley and Brusco (2008), and Guyon and Elisseeff (2003) are, in our opinion, a very good starting point for those readers in need of more information.", "startOffset": 82, "endOffset": 779}, {"referenceID": 7, "context": "The former uses properties of the data itself to select a subset of features during the data pre-processing phase. The features are selected before the clustering algorithm is run, making this approach usually faster. However, this speed comes at price. It can be rather difficult to define whether a given feature is relevant without applying clustering to the data. Methods following a wrapper approach make use of the information given by a clustering algorithm when selecting features. Often, these methods lead to better performance when compared to those following a filter approach (Dy 2008). However, these also tend to be more computationally intensive as the clustering and the feature selection algorithms are run. The surveys by Dy (2008), Steinley and Brusco (2008), and Guyon and Elisseeff (2003) are, in our opinion, a very good starting point for those readers in need of more information.", "startOffset": 82, "endOffset": 811}, {"referenceID": 7, "context": "The former uses properties of the data itself to select a subset of features during the data pre-processing phase. The features are selected before the clustering algorithm is run, making this approach usually faster. However, this speed comes at price. It can be rather difficult to define whether a given feature is relevant without applying clustering to the data. Methods following a wrapper approach make use of the information given by a clustering algorithm when selecting features. Often, these methods lead to better performance when compared to those following a filter approach (Dy 2008). However, these also tend to be more computationally intensive as the clustering and the feature selection algorithms are run. The surveys by Dy (2008), Steinley and Brusco (2008), and Guyon and Elisseeff (2003) are, in our opinion, a very good starting point for those readers in need of more information. Feature weighting and feature selection algorithms are not competing methods. The former does not dismiss the advantages given by the latter. Feature weighting algorithms can still deselect a given feature v by setting its weight wv = 0, bringing benefits traditionally related to feature selection. Such benefits include those discussed by Guyon and Elisseeff (2003) and Dy (2008), such as a possible reduction in the feature space, reduction in measurement and storage requirements, facilitation of data understanding and visualization, reduction in algorithm utilization time, and a general improvement in cluster recovery thanks to the possible avoidance of the curse of dimensionality.", "startOffset": 82, "endOffset": 1274}, {"referenceID": 7, "context": "The former uses properties of the data itself to select a subset of features during the data pre-processing phase. The features are selected before the clustering algorithm is run, making this approach usually faster. However, this speed comes at price. It can be rather difficult to define whether a given feature is relevant without applying clustering to the data. Methods following a wrapper approach make use of the information given by a clustering algorithm when selecting features. Often, these methods lead to better performance when compared to those following a filter approach (Dy 2008). However, these also tend to be more computationally intensive as the clustering and the feature selection algorithms are run. The surveys by Dy (2008), Steinley and Brusco (2008), and Guyon and Elisseeff (2003) are, in our opinion, a very good starting point for those readers in need of more information. Feature weighting and feature selection algorithms are not competing methods. The former does not dismiss the advantages given by the latter. Feature weighting algorithms can still deselect a given feature v by setting its weight wv = 0, bringing benefits traditionally related to feature selection. Such benefits include those discussed by Guyon and Elisseeff (2003) and Dy (2008), such as a possible reduction in the feature space, reduction in measurement and storage requirements, facilitation of data understanding and visualization, reduction in algorithm utilization time, and a general improvement in cluster recovery thanks to the possible avoidance of the curse of dimensionality.", "startOffset": 82, "endOffset": 1288}, {"referenceID": 35, "context": "Work on feature weighting in clustering has over 40 years of history (Sneath and Sokal 1973), however, only in 1984 (DeSarbo et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 3, "context": "Work on feature weighting in clustering has over 40 years of history (Sneath and Sokal 1973), however, only in 1984 (DeSarbo et al. 1984) feature weighting was applied to K-Means, arguably the most popular partitional clustering algorithm.", "startOffset": 116, "endOffset": 137}, {"referenceID": 3, "context": "Synthesized Clustering (SYNCLUS) (DeSarbo et al. 1984) is, to our knowledge, the first K-Means extension to allow feature weights.", "startOffset": 33, "endOffset": 54}, {"referenceID": 1, "context": "SYNCLUS is not appropriate for clusterwise regression context with both dependent and independent variables (DeSarbo and Cron 1988).", "startOffset": 108, "endOffset": 131}, {"referenceID": 34, "context": "It has also been extended by Makarenkov and Legendre (2001) by using the Polak-Ribiere optimisation procedure (Polak 1971) to minimise (3).", "startOffset": 110, "endOffset": 122}, {"referenceID": 1, "context": "SYNCLUS is not appropriate for clusterwise regression context with both dependent and independent variables (DeSarbo and Cron 1988). Nevertheless, SYNCLUS has been a target to various extensions. DeSarbo and Mahajan (1984) extended this method to deal with constraints, different types of clustering schemes, as well as a general linear transformation of the features.", "startOffset": 109, "endOffset": 223}, {"referenceID": 1, "context": "SYNCLUS is not appropriate for clusterwise regression context with both dependent and independent variables (DeSarbo and Cron 1988). Nevertheless, SYNCLUS has been a target to various extensions. DeSarbo and Mahajan (1984) extended this method to deal with constraints, different types of clustering schemes, as well as a general linear transformation of the features. It has also been extended by Makarenkov and Legendre (2001) by using the Polak-Ribiere optimisation procedure (Polak 1971) to minimise (3).", "startOffset": 109, "endOffset": 429}, {"referenceID": 8, "context": "Modha and Spangler (2003) introduced the convex K-Means (CK-Means) algorithm.", "startOffset": 14, "endOffset": 26}, {"referenceID": 30, "context": "CK-Means has also shown promising results in experiments (Modha and Spangler 2003), however, the way it finds feature weights has led to claims that generating \u2206 would be difficult in high-dimensional data (Tsai and Chiu 2008; Huang et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 42, "context": "CK-Means has also shown promising results in experiments (Modha and Spangler 2003), however, the way it finds feature weights has led to claims that generating \u2206 would be difficult in high-dimensional data (Tsai and Chiu 2008; Huang et al. 2005), and that there is no guarantee the optimal weights would be in \u2206 (Huang et al.", "startOffset": 206, "endOffset": 245}, {"referenceID": 8, "context": "Another extension to K-Means to support feature weights was introduced by Chan et al. (2004). This algorithm generates a weight wkv for each feature v \u2208 V at each cluster in S = {S 1, S 2, .", "startOffset": 51, "endOffset": 93}, {"referenceID": 8, "context": "A disjoint clustering, in which S k \u2229 S l = \u2205 for k, l = 1, 2, ..., K and k , l. 2. A crisp clustering, given by \u2211K k=1 |S k| = N. 3. \u2211 v\u2208V wkv = 1 for a given cluster S k. 4. {wkv} \u2265 0 for k = 1, 2, ..., K and v \u2208 V . Chan et al. (2004) minimises (8) under the above constraints by using partial optimisation for S , C and w.", "startOffset": 20, "endOffset": 238}, {"referenceID": 3, "context": "It is interesting to see that DeSarbo et al. (1984) suggested two possible cases for initial weights in SYNCLUS (details in Section 4.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "There are some issues to have in mind when using this algorithm. The use of (9) may be problematic in certain cases as the range of d(yiv, ckv) will be different depending on whether v is numerical or categorical. Based on the work of Huang (1998) and Ng and Wong (2002), Chan et al.", "startOffset": 47, "endOffset": 248}, {"referenceID": 8, "context": "There are some issues to have in mind when using this algorithm. The use of (9) may be problematic in certain cases as the range of d(yiv, ckv) will be different depending on whether v is numerical or categorical. Based on the work of Huang (1998) and Ng and Wong (2002), Chan et al.", "startOffset": 47, "endOffset": 271}, {"referenceID": 8, "context": "Huang et al. (2005) introduced the Weighted K-Means (WK-Means) algorithm.", "startOffset": 4, "endOffset": 20}, {"referenceID": 8, "context": "Huang et al. (2005) introduced the Weighted K-Means (WK-Means) algorithm. WKMeans attempts to minimise the object function below, which is similar to that of Chan et al. (2004), discussed in Section 4.", "startOffset": 4, "endOffset": 177}, {"referenceID": 8, "context": "The weight w\u03b2v in (11) makes the final clustering S , and by consequence the centroids in C, dependant of the value of \u03b2. There are two possible critical values for \u03b2, 0 and 1. If \u03b2 = 0, Equation (11) becomes equivalent to that of K-Means (1). At \u03b2 = 1, the weight of a single feature v \u2208 V is set to one (that with the lowest Dv), while all the others are set to zero. Setting \u03b2 = 1 is probably not desirable in most problems. The above critical values generate three intervals of interest. When \u03b2 < 0, wv increases with an increase in Dv. However, the negative exponent makes w \u03b2 v smaller, so that v has less of an impact on distance calculations. If 0 < \u03b2 < 1, wv increases with an increase in Dv, so does w \u03b2 v . This goes against the principle that a feature with a small dispersion should have a higher weight, proposed by Chan et al. (2004) (perhaps inspired by SYNCLUS, see Section 4.", "startOffset": 7, "endOffset": 849}, {"referenceID": 8, "context": "The weight w\u03b2v in (11) makes the final clustering S , and by consequence the centroids in C, dependant of the value of \u03b2. There are two possible critical values for \u03b2, 0 and 1. If \u03b2 = 0, Equation (11) becomes equivalent to that of K-Means (1). At \u03b2 = 1, the weight of a single feature v \u2208 V is set to one (that with the lowest Dv), while all the others are set to zero. Setting \u03b2 = 1 is probably not desirable in most problems. The above critical values generate three intervals of interest. When \u03b2 < 0, wv increases with an increase in Dv. However, the negative exponent makes w \u03b2 v smaller, so that v has less of an impact on distance calculations. If 0 < \u03b2 < 1, wv increases with an increase in Dv, so does w \u03b2 v . This goes against the principle that a feature with a small dispersion should have a higher weight, proposed by Chan et al. (2004) (perhaps inspired by SYNCLUS, see Section 4.1), and followed by Huang et al. (2005). If \u03b2 > 1, wv decreases with an increase in Dv, and so does w \u03b2 v , very much the desired effect of decreasing the impact of a feature v in (11) whose Dv is high.", "startOffset": 7, "endOffset": 933}, {"referenceID": 8, "context": "With the above in mind, Jing, Ng, and Huang (2007) devised the following criterion for EW-KM:", "startOffset": 27, "endOffset": 51}, {"referenceID": 8, "context": "This adds a single step to K-Means, used to calculate feature weights. The R package weightedKmeans found at CRAN includes an implementation of this algorithm, which we decided to use in our experiments (details in Sections 5 and 6). Jing, Ng, and Huang (2007) presents extensive experiments, with synthetic and real-world data.", "startOffset": 15, "endOffset": 261}, {"referenceID": 16, "context": "(2013) have introduced the Improved K-Prototypes clustering algorithm (IKP), which minimises the WK-Means criterion (11), with influences from k-prototype (Huang 1998).", "startOffset": 155, "endOffset": 167}, {"referenceID": 20, "context": "IK-P presents some very interesting results (Ji et al. 2013), outperforming other popular clustering algorithms such as k-prototype, SBAC, and KL-FCM-GM (Chatzis 2011; Ji et al.", "startOffset": 44, "endOffset": 60}, {"referenceID": 19, "context": "2013), outperforming other popular clustering algorithms such as k-prototype, SBAC, and KL-FCM-GM (Chatzis 2011; Ji et al. 2012).", "startOffset": 98, "endOffset": 128}, {"referenceID": 8, "context": "IK-P presents some very interesting results (Ji et al. 2013), outperforming other popular clustering algorithms such as k-prototype, SBAC, and KL-FCM-GM (Chatzis 2011; Ji et al. 2012). However, the algorithm still leaves some open questions. For instance, Ji et al. (2013) present experiments on six data sets (two of which being different versions of the same data set) setting \u03b2 = 8, but it is not clear whether the good results provided by this particular \u03b2 would generalize to other data sets.", "startOffset": 34, "endOffset": 273}, {"referenceID": 8, "context": "IK-P presents some very interesting results (Ji et al. 2013), outperforming other popular clustering algorithms such as k-prototype, SBAC, and KL-FCM-GM (Chatzis 2011; Ji et al. 2012). However, the algorithm still leaves some open questions. For instance, Ji et al. (2013) present experiments on six data sets (two of which being different versions of the same data set) setting \u03b2 = 8, but it is not clear whether the good results provided by this particular \u03b2 would generalize to other data sets. Given a numerical feature, IK-P applies the Manhattan distance (17), however, centroids are calculated using the mean. The center of the Manhattan distance is given by the median rather than the mean, this is probably the reason why Ji et al. found it necessary to allow the user to set a maximum numbers of iterations to their algorithm. Now, even if the algorithm converges, most likely it would converge in a smaller number of iterations if the distance used for the assignments of entities was aligned to that used for obtaining the centroids. Finally, while d(yiv, ckv) for a categorical v has a range in the interval [0, 1], the same is not true if v is numerical, however, Ji et al. (2013) make no mention to data standardization.", "startOffset": 34, "endOffset": 1195}, {"referenceID": 20, "context": "The concept of distributed centroid (Ji et al. 2013) can also be applied to our algorithm (De Amorim and", "startOffset": 36, "endOffset": 52}, {"referenceID": 8, "context": "where the dispersion of feature v in cluster k is now dependant on the exponent p, Dkvp = \u2211 i\u2208S k |yiv \u2212 ckv| p + c, and c is a constant equivalent to the average dispersion. The update of the centroid of cluster S k on feature v, ckv also depends on the value of p. At values of p 1, 2, and \u221e, the center of (19) is given by the median, mean and midrange, respectively. If p < {1, 2,\u221e} then the center can be found using a steepest descend algorithm (De Amorim and Mirkin 2012). The iMWK-Means algorithm deals with categorical features by transforming them in numerical, following a method described by Mirkin (2012). In this method, a given categorical feature v with L categories is replaced by L binary features, each representing one of the original categories.", "startOffset": 160, "endOffset": 618}, {"referenceID": 8, "context": "Makarenkov to appear), however, in order to show a single version of our method we decided not to follow the latter here. Clearly, the chosen value of p has a considerable impact on the final clustering given by iMWK-Means. De Amorim and Mirkin (2012) introduced a semi-supervised algorithm to estimate a good p, requiring labels for 20% of the entities in Y.", "startOffset": 54, "endOffset": 252}, {"referenceID": 42, "context": "FWSA has already been compared to WK-Means in three real-world data sets (Tsai and Chiu 2008).", "startOffset": 73, "endOffset": 93}, {"referenceID": 6, "context": "4), LAC (Domeniconi et al. 2007) and EW-KM (see Section 4.", "startOffset": 8, "endOffset": 32}, {"referenceID": 24, "context": "The former were obtained from the popular UCI machine learning repository (Lichman 2013) and include data sets with different combinations of numerical and categorical features, as we show in Table 1.", "startOffset": 74, "endOffset": 88}, {"referenceID": 8, "context": "2) also goes a step further in feature weighting, it does so by integrates multiple, heterogeneous feature spaces. Modha and Spangler (2003) demonstrates that with the Euclidean and Spherical cases.", "startOffset": 8, "endOffset": 141}, {"referenceID": 17, "context": "This allows us to measure the cluster recovery of each algorithm in terms of the adjusted Rand index (ARI) (Hubert and Arabie 1985) between the generated clustering and the known labels.", "startOffset": 107, "endOffset": 131}, {"referenceID": 13, "context": "Generally speaking, two (or more) features that are useless by themselves may be useful together (Guyon and Elisseeff 2003).", "startOffset": 97, "endOffset": 123}, {"referenceID": 28, "context": "We believe that perhaps an approach based on bi-clustering (Mirkin 1998) could address this issue.", "startOffset": 59, "endOffset": 72}, {"referenceID": 13, "context": "Since two features that are useless by themselves may be useful together (Guyon and Elisseeff 2003), a feature weighting algorithm should be able to calculate weights for groups of features.", "startOffset": 73, "endOffset": 99}, {"referenceID": 28, "context": "Perhaps the concept of bi-clustering (Mirkin 1998) could be extended in future research by clustering features and entities, but also weighting features and groups of features.", "startOffset": 37, "endOffset": 50}], "year": 2016, "abstractText": "In a real-world data set there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process. With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research. This paper elaborates on the concept of feature weighting and addresses these issues by critically analysing some of the most popular, or innovative, feature weighting mechanisms based in K-Means.", "creator": "LaTeX with hyperref package"}}}