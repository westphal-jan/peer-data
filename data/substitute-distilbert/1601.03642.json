{"id": "1601.03642", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2016", "title": "Creativity in Machine Learning", "abstract": "recent image learning simulations can be modified better produce creative results. those results did not exist before ; it is not a trivial combination called the data which was fed into the adaptive learning system. the obtained results come in multiple forms : as images, as text and as audio.", "histories": [["v1", "Tue, 12 Jan 2016 23:28:07 GMT  (1110kb,D)", "http://arxiv.org/abs/1601.03642v1", "5 pages, 4 figures"]], "COMMENTS": "5 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["martin thoma"], "accepted": false, "id": "1601.03642"}, "pdf": {"name": "1601.03642.pdf", "metadata": {"source": "CRF", "title": "Creativity in Machine Learning", "authors": ["Martin Thoma", "Tom Mitchel [Mit"], "emails": ["info@martin-thoma.de"], "sections": [{"heading": null, "text": "This paper gives a high level overview of how they are created and gives some examples. It is meant to be a summary of the current work and give people who are new to machine learning some starting points.\nI. INTRODUCTION\nAccording to [Gad06] creativity is \u201cthe ability to use your imagination to produce new ideas, make things etc.\u201d and imagination is \u201cthe ability to form pictures or ideas in your mind\u201d.\nRecent advances in machine learning produce results which the author would intuitively call creative. A high-level overview over several of those algorithms are described in the following.\nThis paper is structured as follows: Section II introduces the reader on a very simple and superficial level to machine learning, Section III gives examples of creativity with images, Section IV gives examples of machines producing textual content, and Section V gives examples of machine learning and music. A discussion follows in Section VI."}, {"heading": "II. BASICS OF MACHINE LEARNING", "text": "The traditional approach of solving problems with software is to program machines to do so. The task is divided in as simple sub-tasks as possible, the subtasks are analyzed and the machine is instructed to process the input with human-designed algorithms to produce the desired output. However, for some tasks like object recognition this approach is not feasible. There are way to many different objects, different lighting situations, variations in rotation and the arrangement of a scene for a human to think of all of them and model them. But with the internet, cheap computers, cameras, crowd-sourcing platforms like Wikipedia and lots of Websites, services like Amazon Mechanical Turk and several other changes in the past decades a lot of data has become available. The idea of machine learning is to make use of this data.\nA formal definition of the field of Machine Learning is given by Tom Mitchel [Mit97]:\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experience E.\nThis means that machine learning programs adjust internal parameters to fit the data they are given. Those computer programs are still developed by software developers, but the developer writes them in a way which makes it possible to adjust them without having to re-program everything. Machine learning programs should generally improve when they are fed with more data.\nThe field of machine learning is related to statistics. Some algorithms directly try to find models which are based on wellknown distribution assumptions of the developer, others are more general.\nA common misunderstanding of people who are not related in this field is that the developers don\u2019t understand what their machine learning program is doing. It is understood very well in the sense that the developer, given only a pen, lots of paper and a calculator could calculate the same result as the machine does when he gets the same data. And lots of time, of course. It is not understood in the sense that it is hard to make predictions how the algorithm behaves without actually trying it. However, this is similar to expecting from an electrical engineer to explain how a computer works. The electrical engineer could probably get the knowledge he needs to do so, but the amount of time required to understand such a complex system from basic building blocks is a time-intensive and difficult task.\nAn important group of machine learning algorithms was inspired by biological neurons and are thus called artificial neural networks. Those networks are based on mathematical functions called artificial neurons which take n \u2208 N numbers x1, . . . , xn \u2208 R as input, multiply them with weights w1, . . . , wn \u2208 R, add them and apply a so called activation function \u03d5 as visualized in Figure 1(a). One example of such an activation function is the sigmoid function \u03d5(x) = 11+e\u2212x . Those functions act as building blocks for more complex systems as they can be chained and grouped in layers as visualized in Figure 1(b). The interesting question is how the parameters wi are learned. This is usually done by an optimization technique called gradient descent. The gradient descent algorithm takes a function which has to be derivable, starts at any point of the surface of this error function and ar X\niv :1\n60 1.\n03 64\n2v 1\n[ cs\n.C V\n] 1\n2 Ja\nn 20\n16\n2 makes a step in the direction which goes downwards. Hence it tries to find a minimum of this high-dimensional function.\nThere is, of course, a lot more to say about machine learning. The interested reader might want to read the introduction given by Mitchell [Mit97].\nIII. IMAGE DATA\nApplying a simple neural network on image data directly can work, but the number of parameters gets extraordinary large. One would take one neuron per pixel and channel. This means for 500 px\u00d7 500 px RGB images one would get 750,000 input signals. To approach this problem, so called Convolutional Neural Networks (CNNs) were introduced. Instead of learning the full connection between the input layer and the first hidden layer, those networks make use of convolution layers. Convolution layers learn a convolution; this means they learn the weights of an image filter. An additional advantage is that CNNs make use of spacial relationships of the pixels instead of flattening the image to a stream of single numbers.\nAn excellent introduction into CNNs is given by [Nie15]."}, {"heading": "A. Google DeepDream", "text": "The gradient descent algorithm which optimizes most of the parameters in neural networks is well-understood. However, the effect it has on the recognition system is difficult to estimate. [MOT15] proposes a technique to analyze the weights learned by such a network. A similar idea was applied by [VKMT13].\nFor example, consider a neural network which was trained to recognize various images like bananas. This technique turns the network upside down and starts with random noise. To analyze what the network considers bananas to look like, the random noise image is gradually tweaked so that it generates the output \u201cbanana\u201d. Additionally, the changes can be restricted in a way that the statistics of the input image have to be similar to natural images. One example of this is that neighboring pixels are correlated.\nAnother technique is to amplify the output of layers. This was described in [MOT15]:\nWe ask the network: \u201cWhatever you see there, I want more of it!\u201d This creates a feedback loop: if a cloud looks a little bit like a bird, the network will make it look more like a bird. This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere.\nThe name \u201cInceptionism\u201d in the title of [MOT15] comes from the science-fiction movie \u201cInception\u201d (2010). One reason it might be chosen is because neural networks are structured in layers. Recent publications tend to have more and more layers [HZRS15]. The used jargon is to say they get \u201cdeeper\u201d. As this technique as published by Google engineers, the technique is called Google DeepDream.\nFig. 2: Aurelia aurita\nFig. 3: DeepDream impression of Aurelia aurita\nIt has become famous in the internet [Red]. Usually, the images are generated in iterations and in each iteration it is zoomed into the image. Images and videos published by the Google engineers can be seen at [goo15]. Figure 2 shows the original image from which Figure 3 was created with the deep dream algorithm."}, {"heading": "B. Artistic Style Imitation", "text": "A key idea of neural networks is that they learn different representations of the data in each layer. In the case of CNNs, this can easily be visualized as it was done in various papers [ZF14]. Usually, one finds that the network learned to build edge detectors in the first layer and more complex structures in the upper layers.\nGatys, Ecker and Bethge showed in [GEB15] that with a clever choice of features it is possible to separate the general style of an image in terms of local image appearance from the content of an image. They support their claim by applying the style of different artists to an arbitrary image of their choice.\n3 (a) Original Image (b) Style image\n(c) The artistic style of Van Gogh\u2019s \u201cStarry Night\u201d applied to the photograph of a Scottish Highland Cattle.\nFig. 4: The algorithm takes both, the original image and the style image to produce the result.\nThis artistic style imitation can be seen itself as creative work. An example is given by Figure 4. The code which created this example is available under [Joh16].\nSomething similar was done by [SPB+14], where the style of a portrait photograph was transferred to another photograph. A demo can be seen on [Shi14]."}, {"heading": "C. Drawing Robots", "text": "Patrick Tresset and Frdric Fol Leymarie created a system called AIKON (Automatic IKONic drawing) which can automatically generated sketches for portraits [TL05]. AIKON takes a digital photograph, detects faces on them and sketches them with a pen-plotter.\nTresset and Leymaire use k-means clustering [KMN+02] to segment regions of the photograph with similar color which, in turn, will get a similar shading.\nSuch a drawing robot could apply machine learning techniques known from computer vision for detecting the human. It could apply self-learning techniques to draw results most similar to the artists impression of the image. However, the system described in [TL05] seems not to be a machine learning computer program according to the definition by Tom Mitchell [Mit97]."}, {"heading": "IV. TEXT DATA", "text": "Digital text is the first form of natural communication which involved computers. It is used in the form of chats, websites, on collaborative projects like Wikipedia, in scientific literature. Of course, it was used in pre-digital times, too: In newspaper, in novels, in dramas, in religious texts like the bible, in books for education, in notes from conversations.\nThis list could be continued and most of these kinds of texts are now available in digital form. This digital form can be used to teach machines to generate similar texts.\nThe most simple language model which is of use is an n-gram model. This model makes use of sequences of the length n to model language. It can be used to get the probability of a third word, given the previous two words. This way, a complete text can be generated word by word. Refinements and extensions to this model are discussed in the field of Natural Language Processing (NLP).\nHowever, there are much more sophisticated models. One of those are character predictors based on Recurrent Neural Networks (RNNs). Those character predictors take a sequence of characters as input and predict the next character. In that sense they are similar to the n-gram model, but operate on a lower level. Using such a predictor, one can generate texts character by character. If the model is good, the text can have the correct punctuation. This would not be possible with a word predictor.\nCharacter predictors can be implemented with RNNs. In contrast to standard feed-forward neural networks like multilayer Perceptrons (MLPs) which was shown in Figure 1(b), those networks are trained to take their output at some point as well as the normal input. This means they can keep some information over time. One of the most common variant to implement RNNs is by using so called Long short-term memory (LSTM) cells [HS97].\nRecurrent networks apply two main ideas in order to learn: The first is called unrolling and means that an recurrent network is imagined to be an infinite network over time. At each time step the recurrent neurons get duplicated. The second idea is weight sharing which means that those unrolled neurons share the same weight."}, {"heading": "A. Similar Texts Generation", "text": "Karpathy trained multiple character RNNs on different datasets and gave an excellent introduction [Kar15b]. He trained it on Paul Graham\u2019s essays, all the works of Shakespeare, the Hutter Prize [hut] 100 MB dataset of raw Wikipedia articles, the raw LATEX source file of a book about algebraic stacks and geometry and Linux C code.\nWith that training data, the models can generate similar texts. New works which look like Shakespeare plays, new Wikipedia articles, new Linux code and new papers about algebraic geometry can thus automatically be generated. At a first\n4 glance, they do look authentic. The syntax was mostly used correctly, the formatting looks as expected, the sentences are grammatically correct. However, when one looks at the broader context it is easy to recognize that the algorithm has no insight in what it is doing. It does match patterns really well, but it fails to follow a central theme. In the context of C code this means that new variables are introduced, but not used. At the same time, variables which were not declared are used. In the context of Shakespear plays this means that a lot of new characters are introduced, but they don\u2019t speak with each other or about each other.\nThe code used to generate these examples is available and ready to use through [Kar15a]. A couple of examples are in Section A."}, {"heading": "B. Chatbots", "text": "Chatbots are computer programs which participate in chat rooms as autonomous agents. This means they have similar permissions and possibilities as usual human users have, but users can trigger a set of commands to make the bot give them valuable information or features.\nA special category of chatbots are such bots which actively participate in the conversation, which is usually not the case. One of the earliest programs in this category is ELIZA, a bot created by Joseph Weizenbaum in the 1960s [Wei76]. This program had a set of patterns implemented to which it would reply in a seemingly smart way in a psychologists fashion. This means quite often the program would simply repeat the last sentence and add something meaningless like \u201cHow do you feel about it?\u201d. According to [Cur14], Weizenbaum once found his secretary \u2014 who was aware of the fact that this is a computer program \u2014 chatting with the machine. When he looked over her shoulder, she asked him \u201cwould you mind leaving the room\u201d.\nToday, much more sophisticated chatbots exist. They make use of the vast amount of data which is available by millions of Wikipedia articles, chat protocols, websites, help desk protocols, subtitles of movies as well as the astonishing increase in computing power to train RNNs and language models similar to the ones described before.\nInteresting results like the following were obtained by [VL15]:\nHuman: what is the purpose of life ? Machine: to serve the greater good . Human: what is the purpose of living ? Machine: to live forever ."}, {"heading": "V. AUDIO DATA", "text": "Common machine learning tasks which involve audio data are speech recognition, speaker identification, identification of songs. This leads to some less-common, but interesting topics: The composition of music, the synthesizing of audio as art. While the composition might be considered in Section IV,\nwe will now investigate the work which was done in audio synthesization."}, {"heading": "A. Emily Howell", "text": "David Cope created a project called \u201cExperiments in Musical Intelligence\u201d (short: EMI or Emmy) in 1984 [Cop87]. He introduces the idea of seeing music as a language which can be analyzed with natural language processing (NLP) methods. Cope mentions that EMI was more useful to him, when he used the system to \u201ccreate small phrase-size textures as next possibilities using its syntactic dictionary and rule base\u201d [Cop87].\nIn 2003, Cope started a new project which was based on EMI: Emily Howell [Cop13]. This program is able to \u201ccreat[e] both highly authentic replications and novel music compositions\u201d. The reader might want to listen to [Cop12] to get an impression of the beauty of the created music.\nAccording to Cope, an essential part of music is \u201ca set of instructions for creating different, but highly related selfreplications\u201d. Emmy was programmed to find this set of instructions. It tries to find the \u201csignature\u201d of a composer, which Cope describes as \u201ccontiguous patterns that recur in two or more works of the composer\u201d.\nThe new feature of Emily Howell compared to Emmy is that Emily Howell does not necessarily remain in a single, already known style.\nEmily Howell makes use of association network. Cope emphasizes that this is not a form of a neural network. However, it is not clear from [Cop13] how exactly an association network is trained. Cope mentions that Emily Howell is explained in detail in [Cop05]."}, {"heading": "B. GRUV", "text": "Recurrent neural networks \u2014 LSTM networks, to be exact \u2014 are used in [NV15] together with Gated Recurrent Units (GRU) to build a network which can be trained to generate music. Instead of taking notes directly or MIDI files, Nayebi and Vitelli took raw audio waveforms as input. Those audio waveforms are feature vectors given for time steps 0, 1, . . . , t\u2212 1, t. The network is given those feature vectors X1, . . . , Xt and has to predict the following feature vector Xt+1. This means it continues the music. As the input is continuous, the problem was modeled as a regression task. Discrete Fourier Transformation (DFT) was used on chunks of length N of the music to obtain features in the frequency domain.\nAn implementation can be found at [VN15] and a demonstration can be found at [Vit15]."}, {"heading": "C. Audio Synthesization", "text": "Audio synthesization is generating new audio files. This can either be music or speech. With the techniques described before,\n5 neural networks can be trained to generate music note by note. However, it is desirable to allow multiple notes being played at the same time.\nThis idea and some others were applied by Daniel Johnson. He wrote a very good introduction into neural networks for music composition which explains those ideas [Joh15b]. Example compositions are available there, too. He also made the code for his Biaxial Recurrent Neural Network available under [Joh15a]."}, {"heading": "VI. DISCUSSION", "text": "What does these examples mean for our understanding of creativity? Does it influence how much we value art? Could we define art and creativity better after having those and similar results?\nI think we might readjust our understanding of creativity just like we adjusted our understanding of algorithmically hard problems after Deep Blue won against the reigning world chess champion Garry Kasparov in 1997.\nHowever, by now it is obvious that machine learning algorithms cannot compete with human artists. Today\u2019s state of the art algorithms which are purely based on machine learning don\u2019t follow a central theme. They lack the ability to plan. Although clever algorithms were implemented for composing music, it seems as if there is still a lot of supervision involved."}, {"heading": "A. Shakespeare", "text": "PANDARUS: Alas, I think he shall be come approached and the day When little srain would be attain\u2019d into being never fed, And who is but a chain and subjects of his death, I should not sleep.\nSecond Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.\nDUKE VINCENTIO: Well, your wit is in the care of side and that.\nSecond Lord: They would be ruled after this chamber, and my fair nues begun out of the fact, to be conveyed, Whose noble souls I\u2019ll have the heart of the wars.\nClown: Come, sir, I will make did behold your worship.\nVIOLA: I\u2019ll drink it."}, {"heading": "B. Wikipeda", "text": "Naturalism and decision for the majority of Arab countries\u2019 capitalide was grounded by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated with Guangzham\u2019s sovereignty. His generals were the powerful ruler of the Portugal in the [[Protestant Immineners]], which could be said to be directly in Cantonese Communication, which followed a ceremony and set inspired prison, training. The emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth\u2019s Dajoard]], known in western [[Scotland]], near Italy to the conquest of India with the conflict. Copyright was the succession of independence in the slop of Syrian influence that was a famous German movement based on a more popular servicious, non-doctrinal and sexual power post. Many governments recognize the military housing of the [[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]], that is sympathetic to be to the [[Punjab Resolution]] (PJS)[http://www.humah.yahoo.com/guardian. cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery was swear to advance to the resources for those Socialism\u2019s rule, was starting to signing a major tripad of aid exile.]]"}, {"heading": "C. Linux Code, 1", "text": "/*\n7 * Increment the size file of the new incorrect UI_FILTER group information * of the size generatively.\n*/ static int indicate_policy(void) {\nint error; if (fd == MARN_EPT) {\n/* * The kernel blank will coeld it to userspace.\n*/ if (ss->segment < mem_total)\nunblock_graph_and_set_blocked(); else\nret = 1; goto bail;\n} segaddr = in_SB(in.addr); selector = seg / 16; setup_works = true; for (i = 0; i < blocks; i++) {\nseq = buf[i++]; bpf = bd->bd.next + i * search; if (fd) {\ncurrent = blocked; }\n} rw->name = \"Getjbbregs\"; bprm_self_clearl(&iv->version); regs->new = blocks[(BPF_STATS << info->historidac)] | PFMR_CLOBATHINC_SECONDS << 12; return segtable;\n}\nD. Linux Code, 2\n/* * Copyright (c) 2006-2010, Intel Mobile Communications. All rights reserved.\n* * This program is free software; you can redistribute it and/or modify it * under the terms of the GNU General Public License version 2 as published by * the Free Software Foundation.\n* * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n* * GNU General Public License for more details.\n* * You should have received a copy of the GNU General Public License * along with this program; if not, write to the Free Software Foundation, * Inc., 675 Mass Ave, Cambridge, MA 02139, USA. */\n#include <linux/kexec.h> #include <linux/errno.h> #include <linux/io.h> #include <linux/platform_device.h> #include <linux/multi.h>\n8 #include <linux/ckevent.h>\n#include <asm/io.h> #include <asm/prom.h> #include <asm/e820.h> #include <asm/system_info.h> #include <asm/setew.h> #include <asm/pgproto.h>\n#define REG_PG vesa_slot_addr_pack #define PFM_NOCOMP AFSR(0, load) #define STACK_DDR(type) (func)\n#define SWAP_ALLOCATE(nr) (e) #define emulate_sigs() arch_get_unaligned_child() #define access_rw(TST) asm volatile(\"movd %%esp, %0, %3\" : : \"r\" (0)); \\\nif (__type & DO_READ)\nstatic void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \\ pC>[1]);\nstatic void os_prefix(unsigned long sys) { #ifdef CONFIG_PREEMPT\nPUT_PARAM_RAID(2, sel) = get_state_state(); set_pid_sum((unsigned long)state, current_state_str(),\n(unsigned long)-1->lr_full; low; }"}], "references": [{"title": "Experiments in music intelligence (emi),", "author": ["D. Cope"], "venue": null, "citeRegEx": "Cope,? \\Q1987\\E", "shortCiteRegEx": "Cope", "year": 1987}, {"title": "Now then,", "author": ["A. Curtis"], "venue": "BBC, Jul. 2014. [Online]. Available: http://www.bbc.co.uk/blogs/adamcurtis/entries/", "citeRegEx": "Curtis,? \\Q2014\\E", "shortCiteRegEx": "Curtis", "year": 2014}, {"title": "A neural algorithm of artistic style,", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Long short-term memory,", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Deep residual learning for image recognition,", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Biaxial recurrent neural network for music composition,", "author": ["D. Johnson"], "venue": "GitHub, Aug", "citeRegEx": "Johnson,? \\Q2015\\E", "shortCiteRegEx": "Johnson", "year": 2015}, {"title": "An efficient k-means clustering algorithm: analysis and implementation,", "author": ["T. Kanungo", "D. Mount", "N. Netanyahu", "C. Piatko", "R. Silverman", "A. Wu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Kanungo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2002}, {"title": "Inceptionism: Going deeper into neural networks,", "author": ["A. Mordvintsev", "C. Olah", "M. Tyka"], "venue": "googleresearch.blogspot.co.uk, Jun", "citeRegEx": "Mordvintsev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordvintsev et al\\.", "year": 2015}, {"title": "Neural Networks and Deep Learning", "author": ["M.A. Nielsen"], "venue": "Determination Press,", "citeRegEx": "Nielsen,? \\Q2015\\E", "shortCiteRegEx": "Nielsen", "year": 2015}, {"title": "GRUV: Algorithmic music generation using recurrent neural networks,", "author": ["A. Nayebi", "M. Vitelli"], "venue": null, "citeRegEx": "Nayebi and Vitelli,? \\Q2015\\E", "shortCiteRegEx": "Nayebi and Vitelli", "year": 2015}, {"title": "Style transfer for headshot portraits,", "author": ["Y. Shih"], "venue": "[Online]. Available: https://www.youtube.com/watch?v=", "citeRegEx": "Shih,? \\Q2014\\E", "shortCiteRegEx": "Shih", "year": 2014}, {"title": "Style transfer for headshot portraits,", "author": ["Y. Shih", "S. Paris", "C. Barnes", "W.T. Freeman", "F. Durand"], "venue": "ACM Transactions on Graphics (TOG), vol. 33,", "citeRegEx": "Shih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shih et al\\.", "year": 2014}, {"title": "Algorithmic music generation with recurrent neural networks,", "author": ["M. Vitelli"], "venue": null, "citeRegEx": "Vitelli,? \\Q2015\\E", "shortCiteRegEx": "Vitelli", "year": 2015}, {"title": "Hoggles: Visualizing object detection features,", "author": ["C. Vondrick", "A. Khosla", "T. Malisiewicz", "A. Torralba"], "venue": "Computer Vision (ICCV),", "citeRegEx": "Vondrick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2013}, {"title": "A neural conversational model,", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le", "year": 2015}, {"title": "Computer Power and Human Reason: From Judgement to Calculation", "author": ["J. Weizenbaum"], "venue": "W.H.Freeman & Co Ltd,", "citeRegEx": "Weizenbaum,? \\Q1976\\E", "shortCiteRegEx": "Weizenbaum", "year": 1976}, {"title": "Visualizing and understanding convolutional networks,", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Vision\u2013ECCV", "citeRegEx": "Zeiler and Fergus,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Recent machine learning techniques can be modified to produce creative results. Those results did not exist before; it is not a trivial combination of the data which was fed into the machine learning system. The obtained results come in multiple forms: As images, as text and as audio. This paper gives a high level overview of how they are created and gives some examples. It is meant to be a summary of the current work and give people who are new to machine learning some starting points.", "creator": "LaTeX with hyperref package"}}}