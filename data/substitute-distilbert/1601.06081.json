{"id": "1601.06081", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2016", "title": "Why Do Urban Legends Go Viral?", "abstract": "urban legends are a genre of modern folklore, representations of stories for rare and exceptional events, just plausible enough to be believed, which tend to propagate trends across communities. essentially our view, like urban legends represent a form of \" sticky \" deceptive text, they are marked by a tension between the imaginary into reality. they should be credible like no news article and incredible like a romance tale to go viral. in particular we will focus on the idea that urban legends should mimic the context of facts ( who, where, when ) to be credible, while they should be emotional and readable like a dickens tale to be catchy and memorable. using nlp tools we will understand a quantitative analysis of these prototypical societies. we would lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these supernatural features.", "histories": [["v1", "Fri, 22 Jan 2016 17:33:28 GMT  (28kb)", "http://arxiv.org/abs/1601.06081v1", "Preprint of paper in Journal of Information Processing and Management Volume 52, Issue 1, January 2016, Pages 163-172"]], "COMMENTS": "Preprint of paper in Journal of Information Processing and Management Volume 52, Issue 1, January 2016, Pages 163-172", "reviews": [], "SUBJECTS": "cs.CL cs.CY cs.SI", "authors": ["marco guerini", "carlo strapparava"], "accepted": false, "id": "1601.06081"}, "pdf": {"name": "1601.06081.pdf", "metadata": {"source": "CRF", "title": "Why Do Urban Legends Go Viral?", "authors": ["Marco Guerini"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n06 08\n1v 1\n[ cs\n.C L\n] 2\n2 Ja\nn 20\nUrban legends are a genre of modern folklore, consisting of stories about rare and exceptional events, just plausible enough to be believed, which tend to propagate inexorably across communities. In our view, while urban legends represent a form of \u201csticky\u201d deceptive text, they are marked by a tension between the credible and incredible. They should be credible like a news article and incredible like a fairy tale to go viral. In particular we will focus on the idea that urban legends should mimic the details of news (who, where, when) to be credible, while they should be emotional and readable like a fairy tale to be catchy and memorable. Using NLP tools we will provide a quantitative analysis of these prototypical characteristics. We also lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these simple features."}, {"heading": "1. Introduction", "text": "Urban legends are a genre of modern folklore consisting of stories told as true \u2013 and plausible enough to be believed \u2013 about some rare and exceptional events that supposedly happened to a real person or in a real place.\nWhether urban legends are produced by individual authors or emerge spontaneously, they typically spread \u201cvirally\u201d across communities and tend to change over time with repetition and embellishment, like memes (Dawkins, 2006). For example the sewer alligator, that originally \u201cappeared\u201d in New York City (Coleman, 1979), also appeared in different cities to suit regional variations. Though it is considered synonymous of \u201cfalse belief,\u201d the term urban legend\nJournal of Information Processing & Management. doi:10.1016/j.ipm.2015.05.003 c\u00a9 2016. This manuscript version is made available under the CC-BY-NC-ND 4.0 license\nhttp://creativecommons.org/licenses/by-nc-nd/4.0/\nrefers to a subtler and more complex phenomenon. The crucial factor is that the story is told as true in the absence of verification. Folklorists are generally more interested in the social context and meaning of urban legends than their truth value. From an NLP point of view, instead, it is interesting to computationally explore those linguistic characteristics that make them appealing and bring people to circulate them. With the advent of the Internet, urban legends gained new lifeblood, as they began to be circulated by e-mail.\nIn (Heath & Heath, 2007), the authors discuss the idea of \u201cstickiness\u201d popularized by the book \u201cThe Tipping Point\u201d (Gladwell, 2000), seeking to explain what makes an idea or concept memorable or interesting. They also focus on urban legends and claim that, by following the acronym \u201cSUCCES\u201d (each letter referring to a characteristic that makes an idea \u201csticky\u201d), it is possible to describe their prototypical structure:\n\u2022 Simple \u2013 find the core of any idea\n\u2022 Unexpected \u2013 grab people\u2019s attention by surprising them\n\u2022 Concrete \u2013 make sure an idea can be grasped and remembered later\n\u2022 Credible \u2013 give an idea believability\n\u2022 Emotional \u2013 help people see the importance of an idea\n\u2022 Stories \u2013 empower people to use an idea through narrative\nSuch features are allegedly placed at the core of persuasive and viral language; urban legends constitute an ideal framework with which to computationally verify these assertions. Table 1 displays a few examples of urban legends claims.\nIn particular we will investigate some of the prototypical characteristics that can be found in urban legends as compared to similar literary genres. In our view, urban legends are viral since they are stressed by a tension between credible and incredible: credible like a news and incredible like a fairy tale. We will focus on the idea that urban legends should mimic the details of news\n(who, where, when) to be credible, and they should be emotional and readable like the story of a fairy tale to be catchy and memorable. We will verify these psychological hypotheses \u2013 appeared in the literature \u2013 using NLP tools, to drive a quantitative analysis of these qualitative theories. For example, the idea that urban legends derive much of their credibility from details concerning the location where the situation took place, is presented in (Brunvand, 1981). Anecdotically, the television series \u201c1000 Ways to Die\u201d \u2013 that recreates unusual supposed deaths and debunked urban legends in a way similar to the Darwin Awards1 \u2013 introducing each story with the location and date of each supposed incident, to render it more credible.\nIn the tension between credible and incredible, details should be neither too specific, like in the news, nor too few, as in fairy tales: effective urban legends\n1The Darwin Awards are an ironical honor, granted to individuals who have contributed to human evolution by \u201cself-selecting themselves out of the gene pool\u201d via incredibly foolish actions; Darwin Awards explicitly try to disallow urban legends from the awards. See darwinawards.com\nshould be credible but not verifiable. Similarly, emotions should be enough to make it sticky/catchy but not too much to render it not-credible. Finally urban legends should be easy to read, similar to fairy tales, to render them more memorable. As an example consider the following excerpt, taken from the \u201cKidney Theft\u201d urban legend, as reported by snopes.com:\nDear Friends: I wish to warn you about a new crime ring that is targeting business travelers. This ring is well organized [. . . ] and is currently in most major cities and recently very active in New Orleans. The crime begins when a business traveler goes to a lounge for a drink [. . . ] A person in the bar walks up as they sit alone and offers to buy them a drink. The last thing the traveler remembers until they wake up in a hotel room bath tub, their body submerged to their neck in ice, is sipping that drink. There is a note taped to the wall instructing them not to move and to call 911. [. . . ] The business traveler is instructed by the 911 operator to very slowly and carefully reach behind them and feel if there is a tube protruding from their lower back. The business traveler finds the tube and answers, \u201cYes.\u201d The 911 operator tells them to remain still, having already sent paramedics to help. The operator knows that both of the business traveler\u2019s kidneys have been harvested. This is not a scam, it is real. It is documented and confirmable. If you travel, please be careful.\nRegard\nJerry Mayfield\nThere is no very strong emotional wording in this example, it is the situation itself that is scary; on the contrary the email contains locations, the signature of a presumed Jerry Mayfield, and \u2013 noticeably \u2013 credibility is also explicitly addressed in the text with the adjectives \u201creal\u201d, \u201cdocumented\u201d and \u201cconfirmable\u201d.\nIn the following sections we first review relevant work that addresses the problem of deceptive language and behavior both in online and offline scenarios, followed by an overview of work that addresses the virality of online content. Then we describe the data collected for our experiments and the features extracted to model the aforementioned prototypical characteristics of urban legends. We use these features in both descriptive statistics and generalization tasks and we report the best performing features. Finally we discuss future\nresearch on further prototypical characteristics of urban legends."}, {"heading": "2. Related Work", "text": "The topic of deceptive and/or false messages is a burning topic within the NLP community. A seminal work on the linguistic recognition of lies can be found in (Mihalcea & Strapparava, 2009). Still, defense from subtle persuasive language in broadcast messages, including social networks, is needed in many applied scenarios. Viral messages have become a very important factor for persuasion and are currently almost entirely out of control. So, protection from fraudulent communication is needed, especially in competitive commercial situations. Two main approaches are currently under investigation in the literature:\n1) Recognizing the linguistic characteristics of deceptive content in the social web: for example preventing deceptive consumer reviews (Ott et al., 2011) on sites like Trip Advisor is fundamental both for consumers seeking genuine reviews, and for the reputation of the site itself. Deceptive consumer reviews are fictitious opinions that have been deliberately written to sound authentic. Another example concerns online advertising (Sculley et al., 2011): detecting fraudulent ads is in the interest of users, of service providers (e.g. Google AdWords system), and other advertisers. An interesting phenomenon at the crossroad of viral phenomena and deceptive customer reviews, where ironic reviews (such as the case of the mountain three wolf moon) create phenomena of social contagion, is discussed in Reyes & Rosso (2012).\n2) Recognizing on-line behavioral patterns of deceptive users: For example recognizing groups of propagandists or fake accounts that are used to push the virality of content (Lumezanu et al., 2012). Four main patterns are recognized: (i) sending high volumes of tweets over short periods of time, (ii) retweeting while publishing little original content, (iii) quickly retweeting, and (iv) colluding with other, seemingly unrelated, users to send duplicate or near-duplicate messages on the same topic simultaneously. Another example is (Feng et al., 2012) where the authors hypothesize that there is a set of representative dis-\ntributions of review rating scores. Deceptive business entities that hire people to write fake reviews can then be recognized since they will necessarily distort distribution of review scores, leaving \u201cdistributional footprints\u201d behind.\nWe want to consider a third point, which is linked to the previous two but different at the same time: deceptive content that spreads quickly but without an explicit strategy of making them spread, which is the case with urban legends.\nFinally, the spreading dynamics of an urban legend on one hand closely re-\nsembles those of memes that undergo many variations while spreading (Simmons et al., 2011); on the other hand their characteristics resemble those of viral content. Several researchers have studied information flow, community building and similar processes using Social Networking sites as a reference (Lerman & Ghosh, 2010; Khabiri et al., 2009; Aaditeshwar Seth & Cohen, 2008). However, the great majority concentrate on network-related features without taking into account the actual content spreading within the network (Lerman & Galstyan, 2008). A hybrid approach focusing on both product characteristics and network related features is presented in (Aral & Walker, 2011): in particular, the authors study the effect of passive-broadcast and active-personalized notifications embedded in an application to foster word of mouth.\nRecently, the correlation between content characteristics and virality has begun to be investigated, especially with regard to textual content; in (Jamali, 2009), for example, features derived from sentiment analysis of comments are used to predict stories\u2019 popularity. The work in (Berger & Milkman, 2012) uses New York Times articles to examine the relationship between emotions evoked by the content and virality, using semi-automated sentiment analysis to quantify the affectivity and emotionality of each article. Results suggest a strong relationship between affect and virality, where virality corresponds to the number of times the article was email forwarded.\nThe relevant work in (Danescu-Niculescu-Mizil et al., 2012) measures a different form of content spreading by analyzing which features of a movie quote make it \u201cmemorable\u201d online. Another approach to content virality, somehow complementary to the previous one, is presented in (Simmons et al., 2011), and\ntakes the perspective of understanding which modification dynamics make a meme spread from one person to another (while movie quotes spread remaining exactly the same). More recently, some works tried to investigate how different textual contents give rise to different reactions in the audience: the work presented in (Guerini et al., 2011) correlates several viral phenomena with the wording of a post, while (Guerini et al., 2012) shows that specific content features variations (like the readability level of an abstract) differentiate among virality level of downloads, bookmarking, and citations."}, {"heading": "3. Datasets", "text": "To explore the characteristics of urban legends and understand the effectiveness of our ideas we collected a specific dataset. It is composed of roughly 8000 textual examples: 2518 Urban Legends (UL), 1860 Fairy Tales (FT) and 3575 Google News articles (GN). The description of how the datasets have been created follows.\n\u2022 Urban Legends have been harvested from the website snopes.com.\nWhile almost 5 thousand urban legends were collected and discussed on the website, we considered only those that were reported along with a textual example, usually e-mail circulated on the Internet2. We then kept only those textual examples when at least thirty tokens long.\n\u2022 News Articles have been selected from a corpus of about 400.000 Google\nNews articles, from the years 2009-2012. We collected those with the highest similarity among the titles of the Urban Legends, to grant that textual content is comparable. The similarity scores were computed in a Latent Semantic space, built from the British National Corpus using 400 dimensions. The typical categories of GN articles are science, health, entertainment, economy and sports, news from world.\n2In our dataset, roughly 60% of the cases are emails, 40% are examples collected from\nother sources (e.g. websites, local newspapers, forums).\n\u2022 Fairy Tales We exploit a corpus of fairy tales collected and preprocessed\nby (Lobo & de Matos, 2010) that were downloaded from Project Gutenberg (Hart, 2000). Since the corpus ranges from very short tales (the shortest is 75 words) to quite long ones (the longest is 15,000 words) we split the longest tales to get a total of 1860 documents. The mean length of the resulting documents is about 400 words."}, {"heading": "4. Feature Extraction", "text": "After collecting the datasets we extracted four different groups of features,\nrelevant to the prototypical characteristics we want to analyze.\nNamed Entities, (NE ). To annotate named entities we used the TextPro toolkit (Pianta et al., 2008), and in particular its Named Entities recognition module. The output of the tool is in the IOB2 format and includes the tags Person (PER), Organization (ORG), Location (LOC ) and Miscellaneous (MISC ).\nTemporal Expressions, (TIMEX ). To annotate temporal expressions we used the toolkit TTK (Verhagen & Pustejovsky, 2008). The output of the tool is in TimeML annotation language format (Pustejovsky et al., 2003). In particular time expressions are flagged with TIMEX3 tags (tern.mitre.org). The tags considered are DATE, DURATION and TIME.\nTo compute the importance of the aforementioned features, and to explore the characteristics of urban legend texts, we used the method proposed in (Mihalcea & Strapparava, 2009). We calculate a score associated with a given set of entities (features), as a measure of saliency for the given word class inside the text, called coverage.\nMore formally, given a set of feature instances present in a text, C={W1, W2, . . . , WN}, we define the feature coverage in that text (or corpus) A as the percentage of words from A belonging to the feature set C:\nCoverageA(C) =\n\u2211\nWi\u2208C FrequencyA(Wi)\nWordsA (1)\nwhere FrequencyA(Wi) represents the total number of feature occurrences Wi inside the text A, and WordsA represents the total size (in words) of the text. Note that we computed features\u2019 coverage regardless of their actual length: \u201cNew York City\u201d or \u201cParis\u201d both count as one LOC even if the former is composed of three tokens while the latter only of one. Note also that this approach normalizes according to text length, avoiding biases due to different corpus characteristics.\nSentiment (SENT ). Since the three corpora have different characteristics, rather than computing word polarity using specialized bag-of-words approaches, we resort to words\u2019 prior polarity - i.e. if a word out of context evokes something positive or something negative. This technique, even if less precise, guarantee that the same score is given to the same word in different contexts, and that none of the corpora is either overestimated or underestimated. To this end, we follow the methodology proposed in (Gatti & Guerini, 2012), using SentiWordNet 3.0 (Esuli & Sebastiani, 2006), that assigns prior polarities to words starting from their posterior polarities. In particular we choose the best performing approach. This formula uses a weighted mean, i.e. each sense weight is chosen according to a harmonic series. The rationale behind this choice is based on the assumption that more frequent senses should bear more \u201caffective weight\u201d than very rare senses when computing the prior polarity of a word. In particular, for each word we returned its positive (POS ) and negative (NEG) prior polarity score:\nPOS =\n\u2211n\ni=1( 1 i \u00d7 posScorei)\n\u2211n\ni=1( 1 i )\n(2)\nwhere posScorei represents the modulus of the positive polarity of the ith sense of that word. The NEG score is computed following the same procedure.\nEmotions (EMO). To sense emotions from text we used the methodology described in (Strapparava & Mihalcea, 2008). The idea underlying the method is the distinction between direct and indirect affective words. For direct affective words, we refer to the WordNet Affect (Strapparava & Valitutti, 2004) lexicon,\nan extension of the WordNet database which employs six basic emotion labels (anger, disgust, fear, joy, sadness, surprise) to annotate WordNet synsets. LSA is then used to learn, in an unsupervised setting, a vector space from the British National Corpus. In the LSA space, each emotion label can be represented in various way. In particular, we employ the \u2018LSA Emotion Synset\u2019 setting, in which the synsets of direct emotion words are considered. The affective load is computed in terms of its lexical similarity with respect to one of the six emotion labels. The overall affective load of a text is then calculated as the average of its similarity with each emotion label.\nEmotions and Sentiment features are grouped under the labelAffect (AFF ).\nReadability (READ). We further analyzed the texts in the three datasets according to readability indices, to understand whether there is a difference in the language difficulty among them. Basically, the task of readability assessment consists of quantifying how difficult a text is for a reader. This kind of assessment has been widely used for several purposes, such as evaluating the reading level of children and impaired persons and improving Web content accessibility, see for example what reported in (Tonelli et al., 2012).\nWe use three indices to compute the difficulty of a text: the Gunning Fog (Gunning, 1952), Flesch (Flesch, 1946) and Kincaid (Kincaid et al., 1975) indices. These metrics combine factors such as word and sentence length that are easy to compute and approximate the linguistic elements that have an impact on readability. In the following formulae, SentA represents the number of sentences in text A, CpxA the number of complex words (those with three or more syllables), and SyllA the total number of syllables.\nThe Fog index is a rough measure of how many years of schooling it would take someone to understand the content; higher scores indicate material that is harder to read. Texts requiring near-universal understanding have an index less than 8. Academic papers usually have a score between 15 and 20. The score, for a given text A, is calculated according to the formula:\nFogA = 0.4 ( WordsA\nSentA + 100\nCpxA\nWordsA\n)\n(3)\nThe Flesch Index rates texts on a 100-point scale. Higher scores indicate material that is easier to read while lower numbers mark passages that are more difficult to read. Scores can be interpreted as: 90-100 for content easily understood by an average 11-year-old student, while 0-30 for content best understood by university graduates. The score is calculated with the following formula:\nF leschA = 206.835 \u2212 1.015 WordsA\nSentA \u2212 84.6\nSyllA\nWordsA (4)\nThe Kincaid Index or \u201cFlesch\u2013Kincaid Grade Level Formula\u201d translates the 0-100 score of the Flesch Index to a U.S. grade level. It can be interpreted as the number of years of education required to understand this text, similar to the Gunning Fog index. The grade level is calculated with the following formula:\nKincaidA = 0.39 WordsA\nSentA + 11.8\nSyllA\nWordsA \u2212 15.59 (5)"}, {"heading": "5. Descriptive Statistics", "text": "As can be seen from Tables 2 (Named Entities) and 3 (Temporal Expressions), urban legends place half-way between fairy tales and news, as we expected. While fairy tales represent out-of-time, out-of-place and always-true stories (\u201ca long time ago in a faraway land\u201d), news represent circumstantial description of events. This is reflected by the overall use of named entities (respectively almost three and four times more in UL and GN) and of temporal expressions (respectively almost two and three times more). Interestingly the use of person names is the only case where FT reduce the lead of UL and GN, and can be explained by the fact that characters in FT are usually addressed with proper names (e.g. \u201cHansel and Gretel\u201d).\nIn Table 4, statistics for sentiment and emotion coverage are reported. As can be seen, in the SENT group of features the differences are less marked and, quite surprisingly, ULs have the lowest scores. As we would expect, FTs have the highest score. Sentiment does not meet our initial expectation and seems in contrast with previous works \u2013 see for example what reported in (Heath et al., 2001) on UL and evoked emotions; still the results on sentiment as compared to emotions can be explained by the distinction between affective impact and affective language. In fact, affective impact can either derive from the wording of the text itself (usage of strong affect words), or from the depicted situation (i.e. emotions are evoked by describing a vivid situation with a plain language). In our experiment we tested the \u2018wording\u2019 using SENT features while the \u2018evoked emotions\u2019 with the EMO features. So, UL seem to use a plain and objective language, similar to GN, to gain credibility, but tend to evoke strong emotions (similar to FT) to be catchy. Let us consider the \u201cKidney Theft\u201d excerpt described in Section 1, as stated, there is no very strong emotional wording in this UL, it is the depicted situation that is scary per se.\nIn Table 5, statistics for readability are reported. As can be seen, ULs are readable in a way similar to fairy tales. Still, depending on the readability indices, that grasp different aspects of text difficulty, ULs are either slightly easier than FTs or half-way between FTs and ULs similar to the cases of Tables 2 and 3.\nThis behavior can be explained by the fact that ULs have a simpler syntax than FTs but a more complex lexicon. In fact, inspecting the individual elements of the formulae, as reported in the second part of Table 5, we see that while the percentage of complex words (either CpxA WordsA or SyllA WordsA ) puts UL halfway between FT and GN, the average length of sentences (WordsA SentA ) is surprisingly\nhigher for FT than GN and in turn UL. So, depending on the weight given either to complex words or to sentence length, the results in Table 5 can be interpreted.\nAll differences in the means reported in the tables are statistically significant (Student\u2019s t-test, p < 0.001) apart from TIME, between UL and FT, and DURATION, between UL and GN, (signalled with * in Table 3).\nTurning to the analysis of variance, we see that FT is \u2013 on average \u2013 a more cohesive genre, with lower standard deviations, while GN and UL have higher and closer standard deviations. In fact, all differences in the standard deviations reported in the tables are statistically significant (f-test, p < 0.001) apart between UL and GN in Fog, Kincaid and in ALL sentiment (signalled with * in the respective Tables)."}, {"heading": "6. Classification Experiments", "text": "The goal of our experiments is to understand to what extent it is possible to assign a text to one of the aforementioned classes using just the prototypical characteristics (features) discussed above, and whether there is a subset of features that stands out among the others in this classification task. For every feature combination we conducted a binary classification experiment with ten-fold cross validation on the dataset. We always randomly downsampled the majority class in order to make the dataset balanced, i.e. 50% of positive examples and 50% of negative examples; this accounts for a random baseline of 0.5. We also normalized all features according to z-score. Experiments were carried out using SVM (Vapnik, 1995), in particular libSVM (Chang & Lin, 2011) under its default settings. Results are reported in Table 6; all significance tests discussed below are computed using an approximate randomization test (Yeh, 2000).\nUrban Legends vs News. In the UL vs. GN classification task, while all the features together performed well (F1 = 0.833), improving over all other subgroups of features (p < 0.001), no single group of features performed so well,\napart from READ (F1 = 0.763, p < 0.001). Particularly, the temporal features (TIMEX ) performed worse than AFF and NE (p < 0.001). Still, all features improved over the baseline (p < 0.001).\nUrban Legends vs Fairy Tales. In the UL vs. FT classification task, all the features together performed better than the previous experiment (F1 = 0.897), again improving over all the other subgroups of features alone (p < 0.001). Interestingly, the best discriminative subgroup of features (still READ, F1 = 0.868) in this case reduces the lead with respect to all the features together (ALL) and improves over the others subgroups (p < 0.001) apart from the AFF group \u2013 from which has no significant difference \u2013 that in this case performs better than in the previous experiment. On the contrary, the TIMEX group had similar performances as the previous experiment, while NE improved its performance. Finally, all groups of features had a statistically significant improvement over the baseline (p < 0.001).\nIn Table 7 we report the performances of the various classification tasks in term of precision, recall and F1 over the single classes. Interestingly, for almost all feature combinations the classifiers had slightly higher precision than recall for UL, while the contrary holds for FT and GN.\nNews vs Fairy Tales. Finally, we wanted to check whether UL being \u201chalfway\u201d between GN and FT can be observed in our classification experiments as well. If this hypothesis is correct, by classifying GN vs. FT we would expect\nto find higher performance than previous experiments. Results show that this is in fact the case. All features together performed better than all previous experiment and incredibly well (F1= 0.978), again improving over all the other subgroups of features alone (p < 0.001) apart from READ that performs equally well (F1=0.973, no statistically significant difference). Notably, all other groups of features improves over the UL vs. GN and the UL vs. FT tasks. Finally, all groups of features had a statistically significant improvement over the random\nbaseline (p < 0.001).\nThree Class Classification. Finally we also tested feature predictivity on a three class classification task (UL vs GN vs FT). Since in this case we did not performed downsampling, we use the ZeroR classifier as a baseline. For the sake of interpretability of results, along with precision, recall and F1 we also provide the Matthews Correlation Coefficient (MCC) which is useful for unbalanced datasets, as presented in (Gorodkin, 2004) for the multiclass case. MCC returns a value between -1 and +1, where +1 represents a perfect prediction, 0 no better than random and -1 indicates total disagreement. Results are consistent with previous experiments. In Table 8, all feature configurations show an improvement over the baseline (p < 0.001) but the temporal features (TIMEX ) have far lower discriminative power as compared to others groups of features (MCC=0.339)."}, {"heading": "7. Discussion", "text": "While between UL and GN the discrimination is given by a skillful mixture of all the prototypical features together, where none has a clear predominance over the others, between UL and FT, readability (READ) and affect (AFF ) play a major role. From the summary in Table 9 we see that while ALL features together have the highest averaged F1, READ is the best performing subset of features in all experiments, followed by AFF, NER and TIMEX that perform reasonably well. n general, these experiments proved the goodness of our features in discriminating UL against FT and GN in a machine learning framework, confirming the results emerged from the quantitative analysis part. In particular, as expected, these features gave the best results in the GN vs FT experiments, showing that these two genres represent the extremes of a continuum where ULs are placed.\nFeatures F1\u00b5 F1\u03c3 ALL 0.868 0.070 SENT 0.848 0.106 READ 0.819 0.100 NE 0.740 0.100 TIMEX 0.675 0.069\nTable 9: Overall Feature performances"}, {"heading": "8. Ecological Experiments in the News Domain", "text": "As a final validation of our feature importance we also set up experiments where we controlled for the medium where the message is delivered, specifically the online news domain. Since Newspapers exist for all kinds of stories and with all sorts of reputations for reliability we focused on two specific websites. One is the Weekly World News (WWN), a news website with very low reliability where many of the stories have the qualities of urban legends (the WWN was famous for stories about Bigfoot and UFOs, etc.). The other website is The New York Times (NYT), known for its high reliability and fact-checking procedures.\nWe scraped the WWN for a total of 225 stories, and then randomly selected an equal amount of stories from the NYT. For both datasets we extracted the same set of features discussed in the previous sections. For every feature combination we conducted a binary classification experiment with ten-fold cross validation on the dataset. Since the dataset is balanced, this accounts for a random baseline of 0.5. We also normalized all features according to z-score. Results are reported in Table 10.\nAlso in this case our features are able to discriminate between reliable and non-reliable stories (namely those coming from NYT and WWN). In particular, all the features together performed very well (F1 = 0.864), improving over all other subgroups of features (p < 0.001), and NE, TIMEX, READ performed equally well improving over AFF that was the least effective (p < 0.001). Still, AFF improves over the random baseline (p < 0.001).\nWith this last experiment we were able to show that stories from different newspapers of differing reliability might be classified correctly using the features learned for discriminating regular news from urban legends. So, also in more applicative and ecological scenarios, where stories come from the same medium (online news) these features are useful in discriminating stories on the basis of their UL-ness or GN-ness."}, {"heading": "9. Conclusions", "text": "In this paper we have presented a study on urban legends, a genre of modern folklore consisting of stories about some rare and exceptional events plausible enough to be believed. We argued that urban legends represent a form of \u201csticky\u201d deceptive text, marked by a tension between the credible and incredible. To be credible they should resemble a news article while being incredible like a fairy tale. In particular we focused on the idea that ULs should mimic the details of news (who, where, when) to be credible, while being emotional and readable like a fairy tale to be catchy and memorable. Using NLP tools we presented a quantitative analysis of these simple yet effective features and provided some machine learning experiments showing that it is possible to recognize an urban legend using just these prototypical characteristics. In the future we want to explore other prototypical aspects of urban legends like, for example, linguistic style (Pennebaker & Francis, 2001; Louis & Nenkova, 2013). With regard to sentiment, besides the simple word polarities we used, we will explore\nthe emotions expressed in UL, FT and GN, using an approach similar to the one described in (Strapparava & Mihalcea, 2008). Exploiting knowledge-based and corpus-based methods, that approach deals with automatic recognition of affect, annotating texts with six basic emotions. We believe that fine-grained emotion annotation of urban legends could shed more light in the understanding the mechanisms behind persuasive language."}], "references": [{"title": "A multi-disciplinary approach for recommending weblog messages", "author": ["J.Z. Aaditeshwar Seth", "R. Cohen"], "venue": "In The AAAI 2008 Workshop on Enhanced Messaging", "citeRegEx": "Seth and Cohen,? \\Q2008\\E", "shortCiteRegEx": "Seth and Cohen", "year": 2008}, {"title": "Creating social contagion through viral product design: A randomized trial of peer influence in networks", "author": ["S. Aral", "D. Walker"], "venue": "Management Science,", "citeRegEx": "Aral and Walker,? \\Q2011\\E", "shortCiteRegEx": "Aral and Walker", "year": 2011}, {"title": "What makes online content viral", "author": ["J. Berger", "K.L. Milkman"], "venue": "Journal of Marketing Research,", "citeRegEx": "Berger and Milkman,? \\Q2012\\E", "shortCiteRegEx": "Berger and Milkman", "year": 2012}, {"title": "The vanishing hitchhiker: American urban legends and their meanings", "author": ["J. Brunvand"], "venue": null, "citeRegEx": "Brunvand,? \\Q1981\\E", "shortCiteRegEx": "Brunvand", "year": 1981}, {"title": "Libsvm: a library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Alligators-in-the-sewers: a journalistic origin", "author": ["L. Coleman"], "venue": "Journal of American Folklore,", "citeRegEx": "Coleman,? \\Q1979\\E", "shortCiteRegEx": "Coleman", "year": 1979}, {"title": "You had me at hello: How phrasing affects memorability", "author": ["C. Danescu-Niculescu-Mizil", "J. Cheng", "J. Kleinberg", "L. Lee"], "venue": "In Proceedings of the ACL", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2012}, {"title": "The Selfish Gene", "author": ["R. Dawkins"], "venue": null, "citeRegEx": "Dawkins,? \\Q2006\\E", "shortCiteRegEx": "Dawkins", "year": 2006}, {"title": "SentiWordNet: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "In Proceedings of LREC-2006 (pp. 417\u2013422)", "citeRegEx": "Esuli and Sebastiani,? \\Q2006\\E", "shortCiteRegEx": "Esuli and Sebastiani", "year": 2006}, {"title": "Distributional footprints of deceptive product reviews", "author": ["S. Feng", "L. Xing", "A. Gogar", "Y. Choi"], "venue": "In Proceedings of ICWSM-12", "citeRegEx": "Feng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2012}, {"title": "The Art of plain talk", "author": ["R. Flesch"], "venue": null, "citeRegEx": "Flesch,? \\Q1946\\E", "shortCiteRegEx": "Flesch", "year": 1946}, {"title": "Assessing sentiment strength in words prior polarities", "author": ["L. Gatti", "M. Guerini"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics,", "citeRegEx": "Gatti and Guerini,? \\Q2012\\E", "shortCiteRegEx": "Gatti and Guerini", "year": 2012}, {"title": "The tipping point: How little things can make a big difference", "author": ["M. Gladwell"], "venue": null, "citeRegEx": "Gladwell,? \\Q2000\\E", "shortCiteRegEx": "Gladwell", "year": 2000}, {"title": "Comparing two k-category assignments by a k-category correlation coefficient", "author": ["J. Gorodkin"], "venue": "Computational biology and chemistry,", "citeRegEx": "Gorodkin,? \\Q2004\\E", "shortCiteRegEx": "Gorodkin", "year": 2004}, {"title": "Do linguistic style and readability of scientific abstracts affect their virality", "author": ["M. Guerini", "A. Pepe", "B. Lepri"], "venue": "In Proceedings of ICWSM-12", "citeRegEx": "Guerini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2012}, {"title": "Exploring text virality in social networks", "author": ["M. Guerini", "C. Strapparava", "G. \u00d6zbal"], "venue": "In Proceedings of ICWSM-11", "citeRegEx": "Guerini et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2011}, {"title": "The technique of clear writing", "author": ["R. Gunning"], "venue": null, "citeRegEx": "Gunning,? \\Q1952\\E", "shortCiteRegEx": "Gunning", "year": 1952}, {"title": "Project gutenberg. Project Gutenberg", "author": ["M. Hart"], "venue": null, "citeRegEx": "Hart,? \\Q2000\\E", "shortCiteRegEx": "Hart", "year": 2000}, {"title": "Emotional selection in memes: The case of urban legends", "author": ["C. Heath", "C. Bell", "E. Sternberg"], "venue": "Journal of personality and social psychology,", "citeRegEx": "Heath et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Heath et al\\.", "year": 2001}, {"title": "Made to stick: Why some ideas survive and others die", "author": ["C. Heath", "D. Heath"], "venue": "Random House", "citeRegEx": "Heath and Heath,? \\Q2007\\E", "shortCiteRegEx": "Heath and Heath", "year": 2007}, {"title": "Comment Mining, Popularity Prediction, and Social Network", "author": ["S. Jamali"], "venue": "Analysis . Master\u2019s thesis George Mason University", "citeRegEx": "Jamali,? \\Q2009\\E", "shortCiteRegEx": "Jamali", "year": 2009}, {"title": "Analyzing and predicting community preference of socially generated metadata: A case study on comments in the digg community", "author": ["E. Khabiri", "Hsu", "C.-F", "J. Caverlee"], "venue": null, "citeRegEx": "Khabiri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Khabiri et al\\.", "year": 2009}, {"title": "Derivation of New Readability Formulas (Automated Readability Index, Fog Count, and Flesch Reading Ease formula) for Navy Enlisted Personnel", "author": ["J. Kincaid", "R. Fishburne", "R. Rogers", "B. Chissom"], "venue": "Research Branch Report", "citeRegEx": "Kincaid et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Analysis of social voting patterns on digg", "author": ["K. Lerman", "A. Galstyan"], "venue": "In Proceedings of the first workshop on Online social networks WOSP", "citeRegEx": "Lerman and Galstyan,? \\Q2008\\E", "shortCiteRegEx": "Lerman and Galstyan", "year": 2008}, {"title": "Information contagion: an empirical study of the spread of news on digg and twitter social networks", "author": ["K. Lerman", "R. Ghosh"], "venue": null, "citeRegEx": "Lerman and Ghosh,? \\Q2010\\E", "shortCiteRegEx": "Lerman and Ghosh", "year": 2010}, {"title": "Fairy tale corpus organization using latent semantic mapping and an item-to-item top-n recommendation algorithm", "author": ["P.V. Lobo", "D.M. de Matos"], "venue": null, "citeRegEx": "Lobo and Matos,? \\Q2010\\E", "shortCiteRegEx": "Lobo and Matos", "year": 2010}, {"title": "What makes writing great? first experiments on article quality prediction in the science journalism", "author": ["A. Louis", "A. Nenkova"], "venue": "domain. Transactions of ACL,", "citeRegEx": "Louis and Nenkova,? \\Q2013\\E", "shortCiteRegEx": "Louis and Nenkova", "year": 2013}, {"title": "The lie detector: Explorations in the automatic recognition of deceptive language", "author": ["R. Mihalcea", "C. Strapparava"], "venue": "In Proceedings of ACL-09 (pp. 309\u2013312)", "citeRegEx": "Mihalcea and Strapparava,? \\Q2009\\E", "shortCiteRegEx": "Mihalcea and Strapparava", "year": 2009}, {"title": "The textpro tool suite", "author": ["E. Erlbaum Publishers. Pianta", "C. Girardi", "R. Zanoli"], "venue": null, "citeRegEx": "Pianta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pianta et al\\.", "year": 2008}, {"title": "Timeml: Robust specification of event", "author": ["G. Katz", "D. Radev"], "venue": null, "citeRegEx": "Katz and Radev,? \\Q2003\\E", "shortCiteRegEx": "Katz and Radev", "year": 2003}, {"title": "Making objective decisions from subjective data", "author": ["A. Reyes", "P. Rosso"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Learning to identify emotions in text", "author": ["C. Spain. Strapparava", "R. Mihalcea"], "venue": null, "citeRegEx": "Strapparava and Mihalcea,? \\Q2008\\E", "shortCiteRegEx": "Strapparava and Mihalcea", "year": 2008}, {"title": "WordNet-Affect: an affective extension", "author": ["pp. 1556\u20131560). Fortaleza", "C. Brazil. Strapparava", "A. Valitutti"], "venue": null, "citeRegEx": "Fortaleza et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fortaleza et al\\.", "year": 2004}, {"title": "Making readability indices readable. In Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations (pp. 40\u201348)", "author": ["S. Tonelli", "K.T. Manh", "E. Pianta"], "venue": null, "citeRegEx": "Tonelli et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tonelli et al\\.", "year": 2012}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "Temporal processing with the tarsqi toolkit", "author": ["M. Verhagen", "J. Pustejovsky"], "venue": "In 22nd International Conference on on Computational Linguistics: Demonstration Papers (pp. 189\u2013192)", "citeRegEx": "Verhagen and Pustejovsky,? \\Q2008\\E", "shortCiteRegEx": "Verhagen and Pustejovsky", "year": 2008}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["A. Yeh"], "venue": "In Proceedings of the 18th conference on Computational linguisticsVolume", "citeRegEx": "Yeh,? \\Q2000\\E", "shortCiteRegEx": "Yeh", "year": 2000}], "referenceMentions": [{"referenceID": 7, "context": "Whether urban legends are produced by individual authors or emerge spontaneously, they typically spread \u201cvirally\u201d across communities and tend to change over time with repetition and embellishment, like memes (Dawkins, 2006).", "startOffset": 208, "endOffset": 223}, {"referenceID": 5, "context": "For example the sewer alligator, that originally \u201cappeared\u201d in New York City (Coleman, 1979), also appeared in different cities to suit regional variations.", "startOffset": 77, "endOffset": 92}, {"referenceID": 12, "context": "In (Heath & Heath, 2007), the authors discuss the idea of \u201cstickiness\u201d popularized by the book \u201cThe Tipping Point\u201d (Gladwell, 2000), seeking to explain what makes an idea or concept memorable or interesting.", "startOffset": 115, "endOffset": 131}, {"referenceID": 3, "context": "For example, the idea that urban legends derive much of their credibility from details concerning the location where the situation took place, is presented in (Brunvand, 1981).", "startOffset": 159, "endOffset": 175}, {"referenceID": 9, "context": "Another example is (Feng et al., 2012) where the authors hypothesize that there is a set of representative dis-", "startOffset": 19, "endOffset": 38}, {"referenceID": 21, "context": "Several researchers have studied information flow, community building and similar processes using Social Networking sites as a reference (Lerman & Ghosh, 2010; Khabiri et al., 2009; Aaditeshwar Seth & Cohen, 2008).", "startOffset": 137, "endOffset": 213}, {"referenceID": 20, "context": "Recently, the correlation between content characteristics and virality has begun to be investigated, especially with regard to textual content; in (Jamali, 2009), for example, features derived from sentiment analysis of comments are used to predict stories\u2019 popularity.", "startOffset": 147, "endOffset": 161}, {"referenceID": 6, "context": "The relevant work in (Danescu-Niculescu-Mizil et al., 2012) measures a different form of content spreading by analyzing which features of a movie quote make it \u201cmemorable\u201d online.", "startOffset": 21, "endOffset": 59}, {"referenceID": 15, "context": "More recently, some works tried to investigate how different textual contents give rise to different reactions in the audience: the work presented in (Guerini et al., 2011) correlates several viral phenomena with the wording of a post, while (Guerini et al.", "startOffset": 150, "endOffset": 172}, {"referenceID": 14, "context": ", 2011) correlates several viral phenomena with the wording of a post, while (Guerini et al., 2012) shows that specific content features variations (like the readability level of an abstract) differentiate among virality level of downloads, bookmarking, and citations.", "startOffset": 77, "endOffset": 99}, {"referenceID": 17, "context": "\u2022 Fairy Tales We exploit a corpus of fairy tales collected and preprocessed by (Lobo & de Matos, 2010) that were downloaded from Project Gutenberg (Hart, 2000).", "startOffset": 147, "endOffset": 159}, {"referenceID": 28, "context": "To annotate named entities we used the TextPro toolkit (Pianta et al., 2008), and in particular its Named Entities recognition module.", "startOffset": 55, "endOffset": 76}, {"referenceID": 33, "context": "This kind of assessment has been widely used for several purposes, such as evaluating the reading level of children and impaired persons and improving Web content accessibility, see for example what reported in (Tonelli et al., 2012).", "startOffset": 211, "endOffset": 233}, {"referenceID": 16, "context": "We use three indices to compute the difficulty of a text: the Gunning Fog (Gunning, 1952), Flesch (Flesch, 1946) and Kincaid (Kincaid et al.", "startOffset": 74, "endOffset": 89}, {"referenceID": 10, "context": "We use three indices to compute the difficulty of a text: the Gunning Fog (Gunning, 1952), Flesch (Flesch, 1946) and Kincaid (Kincaid et al.", "startOffset": 98, "endOffset": 112}, {"referenceID": 22, "context": "We use three indices to compute the difficulty of a text: the Gunning Fog (Gunning, 1952), Flesch (Flesch, 1946) and Kincaid (Kincaid et al., 1975) indices.", "startOffset": 125, "endOffset": 147}, {"referenceID": 18, "context": "Sentiment does not meet our initial expectation and seems in contrast with previous works \u2013 see for example what reported in (Heath et al., 2001) on UL and evoked emotions; still the results on sentiment as compared to emotions can be explained by the distinction between affective impact and affective language.", "startOffset": 125, "endOffset": 145}, {"referenceID": 34, "context": "Experiments were carried out using SVM (Vapnik, 1995), in particular libSVM (Chang & Lin, 2011) under its default settings.", "startOffset": 39, "endOffset": 53}, {"referenceID": 36, "context": "Results are reported in Table 6; all significance tests discussed below are computed using an approximate randomization test (Yeh, 2000).", "startOffset": 125, "endOffset": 136}, {"referenceID": 13, "context": "For the sake of interpretability of results, along with precision, recall and F1 we also provide the Matthews Correlation Coefficient (MCC) which is useful for unbalanced datasets, as presented in (Gorodkin, 2004) for the multiclass case.", "startOffset": 197, "endOffset": 213}], "year": 2016, "abstractText": "Urban legends are a genre of modern folklore, consisting of stories about rare and exceptional events, just plausible enough to be believed, which tend to propagate inexorably across communities. In our view, while urban legends represent a form of \u201csticky\u201d deceptive text, they are marked by a tension between the credible and incredible. They should be credible like a news article and incredible like a fairy tale to go viral. In particular we will focus on the idea that urban legends should mimic the details of news (who, where, when) to be credible, while they should be emotional and readable like a fairy tale to be catchy and memorable. Using NLP tools we will provide a quantitative analysis of these prototypical characteristics. We also lay out some machine learning experiments showing that it is possible to recognize an urban legend using just these simple features.", "creator": "LaTeX with hyperref package"}}}