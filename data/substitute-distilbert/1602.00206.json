{"id": "1602.00206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2016", "title": "Unsupervised Deep Hashing for Large-scale Visual Search", "abstract": "learning node hashing plays a pivotal link in software - scale visual search. however, most existing algorithm algorithms tend slowly learn shallow models that don't seek representative binary codes. in this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features regarding hash codes. within the heterogeneous deep hashing framework, orthogonal autoencoder layers with strict constraints are formed to model the nonlinear dependence between features and tree codes. then, a restricted boltzmann machine ( rbm ), with constraints is utilized to reduce the dimension in the hamming space. extensive experiments on the modelling of sequential search examines the competitiveness of our proposed approach compared to state - of - the - art.", "histories": [["v1", "Sun, 31 Jan 2016 06:36:47 GMT  (108kb)", "http://arxiv.org/abs/1602.00206v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["zhaoqiang xia", "xiaoyi feng", "jinye peng", "abdenour hadid"], "accepted": false, "id": "1602.00206"}, "pdf": {"name": "1602.00206.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhaoqiang Xia", "Xiaoyi Feng", "Jinye Peng", "Abdenour Hadid"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n00 20\n6v 1\n[ cs\n.C V\n] 3\n1 Ja\nn 20\nIndex Terms\u2014 Learning based hashing, Unsupervised learning, Deep learning, Autoencoder, RBM"}, {"heading": "1. INTRODUCTION", "text": "In the era of big data, large-scale visual search is vital for accessing and processing a huge amount of images and this is of great importance in many fields of computer vision. Compared to tree based approaches, hashing based methods utilize several hash functions to project image features into binary codes and are more suitable for large-scale visual search due to compact representations [1, 2]. Therefore, the hashing approaches are becoming appealing for dealing with highdimensional data.\nBroadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9]. Dataindependent methods basically project the data into a hamming space by random hash functions whereas data-dependent methods (also referred to as learning based hashing) usually learn hash functions from training datasets by optimizing an objective function. In the case of data-independent methods, the random projections require many binary bits to achieve good performance. With the growing size of data, these methods tend to suffer from memory constraints. The learning based methods, on the other hand, can learn more\ndiscriminative hash functions with different objective functions while the hash bits are not linearly growing with the data size. Hence, learning based hashing is clearly more suitable for large-scale data and is the focus of recent research.\nIt appears that most hashing approaches use linear models to map the data into binary codes. Hence, most existing methods using learning techniques do not capture well the nonlinear relationships within images. Although several improvements have been proposed, e.g. by adding kernelization [10], it is still challenging to select an appropriate kernel function for specific data. As deep learning techniques are shown to capture well the nonlinear relationships within data, a deep architecture can effectively boost the learning of hash functions.\nIn this paper, we propose a novel deep hashing method to learn hierarchy and nonlinear hash functions for obtaining compact binary codes. Our proposed deep architecture includes two heterogeneous layers: autoencoder layers and an RBM (a Restricted Boltzmann Machine) layer. The autoencoder layers are used to generate the initial binary codes whereas the RBM layer is utilized to reduce the dimensionality of the binary codes. For learning the deep autoencoders and RBM, we introduce new objective functions minimizing the reconstruction error and energy function under the constraints of balanced and uncorrelated bits. Extensive experimental analysis on the problem of large-scale visual search demonstrates the validity and competitiveness of our proposed approach compared to state-of-the-art methods."}, {"heading": "2. RELATED WORK", "text": "Learning-Based Hashing. According to whether the semantic information is used or not, learning based hashing can be divided into three categories: unsupervised hashing, semisupervised hashing and supervised hashing. Unsupervised hashing approaches do not use semantic information (such as tags) whereas supervised hashing approaches learn hash functions with semantic information. Semi-supervised approaches model the data with labeled as well as unlabeled data. For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-\nnarization loss or/and the variance of binary bits. For the second category, the Semi-Supervised Hashing (SSH) by Wang et al. [2] constructed an objective function minimizing binarization loss of labeled data and maximizing the variance of unlabeled data. The approach was later extended by employing nonlinear hash functions [9]. For the third category, Linear Discriminant Analysis (LDA) [11] and multiple linearSVMs [12] were used as hash functions and trained with large margin criterion. While most methods seek a single linear mapping, we propose a new solution based on a deep learning framework to explore the hierarchy and nonlinear hash mapping.\nDeep Learning. Recently, several deep learning algorithms have been proposed in machine learning and applied to visual object detection and recognition, image classification, face verification and many other research problems [13]. Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks. Some deep learning approaches have been applied for learning binary codes. Liong et al. [17] presented a framework minimizing a global quantization loss function with two constraints to learn binary codes. In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning. In this context, we propose a novel deep learning approach with a heterogeneous architecture and specific constraints for image hashing. In the architecture, we use the layer-wise unsupervised learning to learn the model parameters."}, {"heading": "3. DEEP HASHING", "text": "Fig. 1 illustrates the framework of our proposed deep hashing method. The framework contains two heterogeneous layers: (1) several deep autoencoder layers; (2) an RBM layer. Given a feature vector x = (x1, x2, ..., xd)T , the deep hashing framework can transform the input vector into a binary vector b = (b1, b2, ..., bk)T , where k \u226a d."}, {"heading": "3.1. SAE Layers", "text": "Let us assume that there are L layers in our deep autoencoder layers, and the hash function in lth layer is H l(ul) = tanh(W lvl\u22121+bl). ul represents the input vector in lth layer and u0 is the initial input x. The output vector in lth layer is denoted as vl = (v1, v2, ..., vq)T . To learn multiple-layers autoencoder, layer-by-layer training has been proposed [15] to minimize the reconstruction error. As shown in Fig. 2, the deep autoencoder (i.e. SAE) can be divided into several three-layers autoencoders for each hidden layer of SAE. v\u0303 represents the reconstructed vector of v. The optimization problem of a conventional autoencoder is to minimize the re-\nconstruction error for each hidden layer:\nminE =\nN\u2211\nn=1\n||v\u0303(n)\u2212 v(n)||22 (1)\nwhere N is the number of training samples. Besides preserving the similarity in the projected space by minimizing the reconstruction error, the representative hash codes should be balanced and uncorrelated [6]. For a balanced ith bit, we should have \u2211N n=1 vi(n) = 0. In order to be more informative for each bit, the code bits should also be uncorrelated. This is satisfied by setting \u2211N n=1 vi(n)vj(n) = 0(i 6= j). The solution to the problem (Eq. 1) with above constraints is non-trivial as the problem is NP-hard. Since our goal is to obtain the most balanced and uncorrelated bits of hash codes, we propose to add above constraints as regularization terms to seek the suboptimal solution. The regularized optimization problem is defined by\nmin W l,bl\nR = 1\n2\nN\u2211\nn=1\n||v\u0303l\u22121(n)\u2212 vl\u22121(n)||22 + \u03bb\n2 ||\nN\u2211\nn=1\nv l(n)||22\n+ \u00b5\n2\nN\u2211\nn=1\n|| 1\nN v l(n)vl T (n)\u2212 I||2F\n(2)\nwhere ||\u00b7||F represents the Frobenius norm. v\u0303l\u22121 is the reconstructed vector and computed as v\u0303l\u22121 = tanh(W\u0303 lvl + b\u0303l). v l is the output vector of hidden layers and computed as vl = tanh(W lvl\u22121 + bl). To learn the model parameters {W l, bl}L1 for the L-layers autoencoders, we employ the BackPropagation (BP) algorithm to solve the optimization problem (Eq. 2). W l and bl\nare updated as\nW l := W l \u2212 \u03b1 \u2202R\n\u2202W l\nbl := bl \u2212 \u03b1 \u2202R\n\u2202bl\n(3)\nwhere \u03b1 is a learning rate. The gradients of parameters are derived as\n\u2202R\n\u2202W l =\nN\u2211\nn=1\nv l\u22121 n \u03b4\nlT\nn + \u03bb\nN\u2211\nn=1\nv l n \u25e6\nN\u2211\nn=1\nf \u2032(uln)v l\u22121 n\n+ \u00b5\nN\nN\u2211\nn=1\n( 1\nN v l nv\nlT\nn \u2212 I)v l n \u25e6 f \u2032(uln)v l\u22121 n\n\u2202R \u2202bl =\nN\u2211\nn=1\n\u03b4ln + \u03bb\nN\u2211\nn=1\nv l n \u25e6\nN\u2211\nn=1\nf \u2032(uln)\n+ \u00b5\nN\nN\u2211\nn=1\n( 1\nN v l nv\nlT\nn \u2212 I)v l n \u25e6 f \u2032(uln)\n(4)\nwhere \u201d\u25e6\u201d denotes element-wise multiplication and the sample index \u201dn\u201d is marked as subscripts for clarify. In Eq. 4, the local gradient \u03b4l(n) and the derivative of the activation function f \u2032(x) are computed as\n\u03b4l(n) = (W\u0303 l)T \u03b4\u0303ln \u25e6 f \u2032(uln)\n\u03b4\u0303ln = f \u2032(u\u0303ln) \u25e6 (v\u0303 l\u22121 n \u2212 v l\u22121 n ) f \u2032(x) = 1\u2212 tanh2(x)\n(5)\nIn Eq. 5, the parameters W\u0303 l and b\u0303l need to be learned when the reconstruction errors are back-propagated. These parameters can be learned similarly to the parameters W l and bl."}, {"heading": "3.2. RBM Layer", "text": "We further employ an RBM layer (Fig. 3) to reduce the dimension of the binary codes. Since the variables are binary in the RBM layer, the sign function is used to transform the output vector of SAE layers so that each input unit of the RBM layer can be valued as {0, 1}.\nLet us assume that the visible layer and the hidden layer are denoted as v and h respectively whereas a,b and W are the bias and weights of visible layer and hidden layer. The energy of the RBM model is defined as E(v,h) = \u2212aTv \u2212\nb T h\u2212hTWv and the joint probability of (v,h) is P (v,h) = 1 Z e\u2212E(v,h).\nThe optimization problem of a conventional RBM is to maximize the likelihood of training samples as follows\nmaxL =\nN\u220f\nn=1\nP (v(n)) =\nN\u220f\nn=1\n\u2211\nj\nP (v(n),hj(n)) (6)\nIn order to keep the binary codes balanced and uncorrelated, we integrate above constraints into the optimization problem (Eq. 6). Thus, the regularized problem is defined as\nmin W,a,b\nJ = \u2212lnL+ \u03bb\n2 ||\nN\u2211\nn=1\nh(n)||22+ \u00b5\n2\nN\u2211\nn=1\n|| 1\nN h(n)hT (n)\u2212I||2F\n(7) where h = sgn(Wv+b)+12 . Since the derivative of sign function is an impulse function, the problem (Eq. 7) is intractable to compute. To seek the approximate solution, we replace the sign function with a derivable function f(x) = tanh(\u03b2x)+12 (\u03b2 \u226b 1). Through computing the gradients \u2202J\n\u2202W , \u2202J \u2202b and \u2202J \u2202a , the parameters are updated as\nW := W\u2212\u03b1 \u2202J\n\u2202W\nb := b\u2212 \u03b1 \u2202J\n\u2202b , a := a\u2212 \u03b1\n\u2202J\n\u2202a\n(8)\nWe utilize the Contrastive Divergence (CD) algorithm [21] to seek the numerical solution of the problem (Eq. 7).\nThe gradients are estimated with Gibbs sampling as follows\n\u2202J\n\u2202W \u2248\nN\u2211\nn=1\n(Pn(hi = 1|v (r) n )v (k) n \u2212 Pn(hi = 1|v (0) n )v (0) n )\n+ \u03bb\nN\u2211\nn=1\nhn \u25e6\nN\u2211\nn=1\nf \u2032(Wv(0)n + b)v (0) n\n+ \u00b5\nN\nN\u2211\nn=1\n( 1\nN v (0) n v\n(0)T\nn \u2212 I)v (0) n \u25e6 f \u2032(Wv(0)n + b)v (0) n\n\u2202J \u2202b \u2248\nN\u2211\nn=1\n(Pn(hi = 1|v (r) n )\u2212 Pn(hi = 1|v (0) n ))\n+ \u03bb\nN\u2211\nn=1\nhn \u25e6\nN\u2211\nn=1\nf \u2032(Wv(0)n + b)\n+ \u00b5\nN\nN\u2211\nn=1\n( 1\nN v (0) n v\n(0)T\nn \u2212 I)v (0) n \u25e6 f \u2032(Wv(0)n + b)\n\u2202J \u2202a \u2248 v(r) \u2212 v(0)\n(9)\nwhere v(r) represents the r-step Gibbs sampling and P (h = 1|v) = sigmoid(Wv + b). The derivate of f(x) is f \u2032(x) = \u03b2 \u2212 \u03b2 tanh2(\u03b2x).\nAlgorithm 1 The Heterogeneous Deep Hashing (HetDH) 1: Initialization: Set up \u03bb, \u00b5, \u03b2, \u03b1, L; Set up iteration times\nT and convergence errors \u01eb1, \u01eb2; Randomly initialize elements of W, b, a in [0, 1]; Split the training set into M epochs, each having N images.\n2: Optimization: for t = 1, 2, ..., T\nfor m = 1, 2, ..., M (a) train all layers of SAE successively for l = 1, 2, ..., L\nUpdate parameters W l and bl by (Eqs. 3, 4, 5); end (b) train single layers of RBM Update parameters W , b and a by (Eqs. 8 and 9);\nend if 1 < t < T\nif |Rt \u2212Rt\u22121| > \u01eb1, do repeat (a) ; if |Jt \u2212 Jt\u22121| > \u01eb2, do repeat (b) ;\nend end\n3: Return: All model parameters.\nThe detailed deep hashing algorithm is summarized in Algorithm 1."}, {"heading": "4. EXPERIMENTAL ANALYSIS", "text": "To evaluate our proposed method, we performed extensive experiments on two datasets: CIFAR-101 and MIRFLICKR25K2. The CIFAR-10 dataset consists of 60,000 32\u00d732 color images in 10 classes with 50,000 training images and 10,000 test images. The MIRFLICKR-25K dataset contains 25,000 color images in 26 classes in which 20,000 training images and 5,000 test images are randomly selected. Moreover, the cascaded 512-D GIST [22] and 512-D Bag-of-Features (BoF) [23] are used for image representation.\nFor comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods. Our approach (denoted as HetDH) uses 3 hidden layers (600\u2212 256\u2212 256(128) architecture) for SAE and 1 hidden layer (16 \u223c 128 neurons) for RBM due to the dimensions of the images. To gain insight into the impact of constraints in our proposed deep learning framework, we performed experiments with constraints (denoted as HetDH) and without constraints (denoted as HetDH-WC). We report the results of all the approaches in terms of precision and recall (precision-recall curves).\nFig. 4 shows the precision-recall curves on the CIFAR-10 and MIRFLICKR-25K datasets at 32 and 64 bits for all considered methods. It can be observed that our proposed method (HetDH) outperforms all other methods in all configurations.\n1http://www.cs.toronto.edu/ kriz/cifar.html 2http://press.liacs.nl/mirflickr/ 3The authors have shared their codes on Internet.\nThe results also point out that all the learning based methods work better than the data-independent method (KLSH). Compared to shallow learning models (SH, ITQ, KMH), both HetDH and HetDH-WC achieve good performance due to the hierarchy representation of deep learning in our proposed approach. Comparing HetDH with HetDH-WC, the obtained results show that the constraints for each layer effectively boost the conventional deep learning and improve the searching performance. This assesses the effectiveness of our proposed algorithm.\nIt is worth noting that the dimension of the hash codes (the number of binary bits) affects the performance of image search (see the experimental results at 32 bits vs. 64 bits). The results indicate that larger dimensions improve the precision and recall but at the cost of more memory storage. Finally, the experiments also show that all the hashing methods seem to work better on the CIFAR-10 dataset compared to MIRFLICKR-25K dataset. This is perhaps due to the diverse nature of the images in MIRFLICKR-25K dataset compared to the images in the CIFAR-10 dataset."}, {"heading": "5. CONCLUSION", "text": "We proposed a heterogeneous deep learning architecture for learning hash functions. With two constraints for balanced and uncorrelated binary codes, we learned the parameters of SAE and RBM layers. Experimental results and extensive comparative analysis on the problem of large-scale image search assessed the effectiveness of our proposed approach which outperformed state-of-the-art unsupervised methods."}, {"heading": "6. REFERENCES", "text": "[1] L. Pauleve\u0301, H. Je\u0301gou, and L. Amsaleg, \u201cLocality sensitive hashing: A comparison of hash function types and querying mechanisms,\u201d Pattern Recognition Letters, vol. 31, no. 11, pp. 1348\u20131358, 2010.\n[2] J. Wang, S. Kumar, and S.F. Chang, \u201cSemi-supervised hashing for large-scale search.,\u201d IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 12, pp. 2393\u20132406, 2012.\n[3] A. Gionis, P. Indyk, and R. Motwani, \u201cSimilarity search in high dimensions via hashing,\u201d in Proceedings of the 25th International Conference on Very Large Data Bases, 1999, pp. 518\u2013529.\n[4] M. Slaney and M. Casey, \u201cLocality-sensitive hashing for finding nearest neighbors,\u201d IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128\u2013131, 2008.\n[5] B. Kulis and K. Grauman, \u201cKernelized locality-sensitive hashing for scalable image search,\u201d in IEEE International Conference on Computer Vision (ICCV), 2009, pp. 2130 \u2013 2137.\n[6] Y. Weiss, A. Torralba, and R. Fergus, \u201cSpectral hashing.,\u201d Advances in Neural Information Processing Systems, vol. 282, no. 3, pp. 1753\u20131760, 2008.\n[7] Y. Gong and S. Lazebnik, \u201cIterative quantization: A procrustean approach to learning binary codes,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 817 \u2013 824.\n[8] K. He, F. Wen, and J. Sun, \u201cK-means hashing: An affinity-preserving quantization method for learning binary compact codes,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2938\u20132945.\n[9] C. Wu, J. Zhu, D. Cai, C. Chen, and J. Bu, \u201cSemisupervised nonlinear hashing using bootstrap sequential projection learning,\u201d IEEE Transactions on Knowledge & Data Engineering, vol. 25, no. 6, pp. 1380\u20131393, 2013.\n[10] S.F. Chang, \u201cSupervised hashing with kernels,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2074\u20132081.\n[11] P. Fua, M. Bronstein, and C. Bronstein, A.and Strecha, \u201cLdahash: Improved matching with smaller descriptors,\u201d IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 1, pp. 66\u201378, 2011.\n[12] M. Rastegari, A. Farhadi, and D. Forsyth, \u201cAttribute discovery via predictable discriminative binary codes,\u201d Lecture Notes in Computer Science on ECCV, vol. 7577, no. 1, pp. 876\u2013889, 2012.\n[13] Y. Lecun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, pp. 436\u201344, 2015.\n[14] A. Krizhevsky, I. Sutskever, and G. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in Neural Information Processing Systems, 2012, pp. 1090\u20131098.\n[15] P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol, \u201cExtracting and composing robust features with denoising autoencoders,\u201d in Proceedings of the 25th international conference on Machine learning, 2008, pp. 1096\u20131103.\n[16] R. Salakhutdinov and G. Hinton, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[17] V.E. Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou, \u201cDeep hashing for compact binary codes learning,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 2475\u20132483.\n[18] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, \u201cSupervised hashing for image retrieval via image representation learning,\u201d in Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014, pp. 2156\u20132162.\n[19] K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen, \u201cDeep learning of binary hash codes for fast image retrieval,\u201d in IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2015, pp. 27\u201335.\n[20] H. Lai, Y. Pan, Y. Liu, and S. Yan, \u201cSimultaneous feature learning and hash coding with deep neural networks,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3270\u20133278.\n[21] A. Fischer and C. Igel, \u201cTraining restricted boltzmann machines: An introduction,\u201d Pattern Recognition, vol. 47, no. 1, pp. 25\u201339, 2014.\n[22] A. Oliva and A. Torralba, \u201cModeling the shape of the scene: A holistic representation of the spatial envelope,\u201d International Journal of Computer Vision, vol. 42, no. 3, pp. 145\u2013175, 2001.\n[23] J. Sivic and A. Zisserman, \u201cEfficient visual search of videos cast as text retrieval,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 4, pp. 591\u2013606, 2009."}], "references": [{"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["L. Paulev\u00e9", "H. J\u00e9gou", "L. Amsaleg"], "venue": "Pattern Recognition Letters, vol. 31, no. 11, pp. 1348\u20131358, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised hashing for large-scale search", "author": ["J. Wang", "S. Kumar", "S.F. Chang"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 12, pp. 2393\u20132406, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases, 1999, pp. 518\u2013529.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Locality-sensitive hashing for finding nearest neighbors", "author": ["M. Slaney", "M. Casey"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128\u2013131, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2009, pp. 2130 \u2013 2137.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, vol. 282, no. 3, pp. 1753\u20131760, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 817 \u2013 824.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "K-means hashing: An affinity-preserving quantization method for learning binary compact codes", "author": ["K. He", "F. Wen", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2938\u20132945.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Semisupervised nonlinear hashing using bootstrap sequential projection learning", "author": ["C. Wu", "J. Zhu", "D. Cai", "C. Chen", "J. Bu"], "venue": "IEEE Transactions on Knowledge & Data Engineering, vol. 25, no. 6, pp. 1380\u20131393, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised hashing with kernels", "author": ["S.F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2074\u20132081.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["P. Fua", "M. Bronstein", "A.C. Bronstein", "Strecha"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 34, no. 1, pp. 66\u201378, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D. Forsyth"], "venue": "Lecture Notes in Computer Science on ECCV, vol. 7577, no. 1, pp. 876\u2013889, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. Lecun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436\u201344, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1090\u20131098.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, 2008, pp. 1096\u20131103.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep hashing for compact binary codes learning", "author": ["V.E. Liong", "J. Lu", "G. Wang", "P. Moulin", "J. Zhou"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 2475\u20132483.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014, pp. 2156\u20132162.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning of binary hash codes for fast image retrieval", "author": ["K. Lin", "H.-F. Yang", "J.-H. Hsiao", "C.-S. Chen"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2015, pp. 27\u201335.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3270\u20133278.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Training restricted boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition, vol. 47, no. 1, pp. 25\u201339, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision, vol. 42, no. 3, pp. 145\u2013175, 2001.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient visual search of videos cast as text retrieval", "author": ["J. Sivic", "A. Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 4, pp. 591\u2013606, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Compared to tree based approaches, hashing based methods utilize several hash functions to project image features into binary codes and are more suitable for large-scale visual search due to compact representations [1, 2].", "startOffset": 215, "endOffset": 221}, {"referenceID": 1, "context": "Compared to tree based approaches, hashing based methods utilize several hash functions to project image features into binary codes and are more suitable for large-scale visual search due to compact representations [1, 2].", "startOffset": 215, "endOffset": 221}, {"referenceID": 2, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 110, "endOffset": 119}, {"referenceID": 3, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 110, "endOffset": 119}, {"referenceID": 4, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 110, "endOffset": 119}, {"referenceID": 1, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 5, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 6, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 7, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 8, "context": "Broadly speaking, existing hashing approaches can be classified into two categories: data-independent methods [3, 4, 5] and data-dependent methods [2, 6, 7, 8, 9].", "startOffset": 147, "endOffset": 162}, {"referenceID": 9, "context": "by adding kernelization [10], it is still challenging to select an appropriate kernel function for specific data.", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "For the first category, the Spectral Hashing (SH) [6], ITerative Quantization (ITQ) [7] and K-Means Hashing (KMH) [8] used different objective functions with constraints of bi-", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "[2] constructed an objective function minimizing binarization loss of labeled data and maximizing the variance of unlabeled data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The approach was later extended by employing nonlinear hash functions [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "For the third category, Linear Discriminant Analysis (LDA) [11] and multiple linearSVMs [12] were used as hash functions and trained with large margin criterion.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "For the third category, Linear Discriminant Analysis (LDA) [11] and multiple linearSVMs [12] were used as hash functions and trained with large margin criterion.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "Recently, several deep learning algorithms have been proposed in machine learning and applied to visual object detection and recognition, image classification, face verification and many other research problems [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks.", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "Since several foundational deep learning frameworks, such as Convolutional Neural Networks (CNN) [14], Stacked AutoEncoders (SAE) [15] and Deep Belief Network (DBN) [16], have been presented, numerous deep learning approaches are developed based on these frameworks.", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "[17] presented a framework minimizing a global quantization loss function with two constraints to learn binary codes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning.", "startOffset": 3, "endOffset": 15}, {"referenceID": 18, "context": "In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning.", "startOffset": 3, "endOffset": 15}, {"referenceID": 19, "context": "In [18, 19, 20], the convolutional neural networks were utilized to extract visual features and a hashing layer was combined to learn binary codes through supervised learning.", "startOffset": 3, "endOffset": 15}, {"referenceID": 14, "context": "To learn multiple-layers autoencoder, layer-by-layer training has been proposed [15] to minimize the reconstruction error.", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "Besides preserving the similarity in the projected space by minimizing the reconstruction error, the representative hash codes should be balanced and uncorrelated [6].", "startOffset": 163, "endOffset": 166}, {"referenceID": 20, "context": "We utilize the Contrastive Divergence (CD) algorithm [21] to seek the numerical solution of the problem (Eq.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "Algorithm 1 The Heterogeneous Deep Hashing (HetDH) 1: Initialization: Set up \u03bb, \u03bc, \u03b2, \u03b1, L; Set up iteration times T and convergence errors \u01eb1, \u01eb2; Randomly initialize elements of W, b, a in [0, 1]; Split the training set into M epochs, each having N images.", "startOffset": 191, "endOffset": 197}, {"referenceID": 21, "context": "Moreover, the cascaded 512-D GIST [22] and 512-D Bag-of-Features (BoF) [23] are used for image representation.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "Moreover, the cascaded 512-D GIST [22] and 512-D Bag-of-Features (BoF) [23] are used for image representation.", "startOffset": 71, "endOffset": 75}, {"referenceID": 4, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "For comparative analysis, the KLSH [5], SH [6], ITQ [7] and KMH [8] algorithms3 are considered and used as baseline methods.", "startOffset": 64, "endOffset": 67}], "year": 2016, "abstractText": "Learning based hashing plays a pivotal role in large-scale visual search. However, most existing hashing algorithms tend to learn shallow models that do not seek representative binary codes. In this paper, we propose a novel hashing approach based on unsupervised deep learning to hierarchically transform features into hash codes. Within the heterogeneous deep hashing framework, the autoencoder layers with specific constraints are considered to model the nonlinear mapping between features and binary codes. Then, a Restricted Boltzmann Machine (RBM) layer with constraints is utilized to reduce the dimension in the hamming space. Extensive experiments on the problem of visual search demonstrate the competitiveness of our proposed approach compared to stateof-the-art.", "creator": "LaTeX with hyperref package"}}}