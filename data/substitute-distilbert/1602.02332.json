{"id": "1602.02332", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "Scalable Text Mining with Sparse Generative Models", "abstract": "the information cycle has brought a deluge of databases. much of this is predominantly plain form, insurmountable in scope for humans and incomprehensible in structure for computers. text mining is an academic field of analytics that seeks to utilize the information contained in vast document collections. general data mining methods based on machine learning face challenges with the recovery of text data, posing a need for agile text mining methods.", "histories": [["v1", "Sun, 7 Feb 2016 02:49:27 GMT  (4651kb,D)", "http://arxiv.org/abs/1602.02332v1", "PhD Thesis, Computer Science, University of Waikato, 2016"]], "COMMENTS": "PhD Thesis, Computer Science, University of Waikato, 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["antti puurula"], "accepted": false, "id": "1602.02332"}, "pdf": {"name": "1602.02332.pdf", "metadata": {"source": "CRF", "title": "Scalable Text Mining with Sparse Generative Models", "authors": ["Antti Puurula"], "emails": [], "sections": [{"heading": null, "text": "The information age has brought a deluge of data. Much of this is in text form, insurmountable in scope for humans and incomprehensible in structure for computers. Text mining is an expanding field of research that seeks to utilize the information contained in vast document collections. General data mining methods based on machine learning face challenges with the scale of text data, posing a need for scalable text mining methods.\nThis thesis proposes a solution to scalable text mining: generative models combined with sparse computation. A unifying formalization for generative text models is defined, bringing together research traditions that have used formally equivalent models, but ignored parallel developments. This framework allows the use of methods developed in different processing tasks such as retrieval and classification, yielding effective solutions across different text mining tasks. Sparse computation using inverted indices is proposed for inference on probabilistic models. This reduces the computational complexity of the common text mining operations according to sparsity, yielding probabilistic models with the scalability of modern search engines.\nThe proposed combination provides sparse generative models: a solution for text mining that is general, effective, and scalable. Extensive experimentation on text classification and ranked retrieval datasets are conducted, showing that the proposed solution matches or outperforms the leading task-specific methods in effectiveness, with a order of magnitude decrease in classification times for Wikipedia article categorization with a million classes. The developed methods were further applied in two 2014 Kaggle data mining prize competitions with over a hundred competing teams, earning first and second places.\nii\nContents\nAbstract ii\nList of Abbreviations and Acronyms vi\nNotation and Nomenclature vii\nList of Notations viii"}, {"heading": "1 Introduction 1", "text": "1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Contributions of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.4 Published Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.5 Structure of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Text Mining and Scalability 7", "text": "2.1 Introduction to Text Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Defining Text Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.2 Related Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.1.3 Application Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2 Text Mining Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.2.1 Text Documents as Multiply Structured Data . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.2 Structured Representations for Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.2.3 Text Mining Applications as Machine Learning Tasks . . . . . . . . . . . . . . . . . . . 21 2.2.4 Linear Models as Methods for Text Mining . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.2.5 Text Mining Architectures as KDD Processes . . . . . . . . . . . . . . . . . . . . . . . 28\n2.3 The Scalability Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.3.1 Scale of Text Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.3.2 Views on Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.3.3 Approaches to Scalable Text Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\niii\nCONTENTS"}, {"heading": "3 Multinomial Naive Bayes for Text Mining 38", "text": "3.1 Multinomial Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.1.2 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.1.3 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.2 Generative Models Extending MNB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.2.1 Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.2.2 N-grams and Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.2.3 Directed Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.2.4 Factor Graphs and Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.2.5 Inference and Estimation with Directed Generative Models . . . . . . . . . . . . . . . . 55\n3.2.5.1 Overview of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 3.2.5.2 Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.2.5.3 Expectation Maximization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58"}, {"heading": "4 Reformalizing Multinomial Naive Bayes 61", "text": "4.1 Formalizing Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.1.1 Smoothing Methods for Multinomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 4.1.2 Formalizing Smoothing with Two-State Hidden Markov Models . . . . . . . . . . . . . 65\n4.2 Extending MNB for Fractional Counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n4.2.1 TF-IDF and Feature Transforms with MNB . . . . . . . . . . . . . . . . . . . . . . . . 70 4.2.2 Methods for Fractional Counts with Multinomial Models . . . . . . . . . . . . . . . . . 71 4.2.3 Formalizing Feature Transforms and Fractional Counts with Probabilistic Data . . . . . 73\n4.3 Formalizing MNB as a Generative Directed Graphical Model . . . . . . . . . . . . . . . . . . . 75 4.4 Extending MNB with Prior Scaling and Document Length Modeling . . . . . . . . . . . . . . . 79"}, {"heading": "5 Sparse Inference 81", "text": "5.1 Basic Case: Sparse Posterior Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.2 Extension to Joint Inference on Hierarchically Smoothed Sequence Models . . . . . . . . . . . 83 5.3 Extension to Joint Inference on Mixtures of Sequence Models . . . . . . . . . . . . . . . . . . . 87 5.4 Further Specialized Efficiency Improvements for Sparse Inference . . . . . . . . . . . . . . . . . 88 5.5 Tied Document Mixture: A Sparse Generative Model . . . . . . . . . . . . . . . . . . . . . . . 89"}, {"heading": "6 Experiments 93", "text": "6.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n6.1.1 Experimental Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.1.2 Performance Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 6.1.3 Baseline Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 6.1.4 Parameter Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 6.1.5 Significance Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\niv\nCONTENTS\n6.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n6.2.1 Dataset Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 6.2.2 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 6.2.3 Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 6.2.4 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.3 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.3.1 Evaluated Linear Model Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 6.3.2 Smoothing Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 6.3.3 Feature Weighting and the Extended MNB . . . . . . . . . . . . . . . . . . . . . . . . . 112 6.3.4 Tied Document Mixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 6.3.5 Comparison with Strong Linear Model Baselines . . . . . . . . . . . . . . . . . . . . . . 116 6.3.6 Scalability and Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120"}, {"heading": "7 Conclusion 126", "text": "7.1 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 7.2 Implications of Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 7.3 Revisiting the Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 7.4 Limitations of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 7.5 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129"}, {"heading": "Appendix A Tables of Results 131", "text": ""}, {"heading": "Appendix B Kaggle LSHTC4 Winning Solution 153", "text": "v\nList of Abbreviations and Acronyms\nBM25 Best Match 25 BNB Bernoulli Naive Bayes DBN Dynamic Bayes Network DM data mining EM expectation maximization HMM Hidden Markov Model IDF inverse document frequency IE information extraction IID independent and identically distributed IR information retrieval KDD knowledge discovery in databases KDT knowledge discovery in textual databases LM language model LR Logistic Regression LSHTC large-scale hierarchical text classification MAP mean average precision Micro-F1 micro-averaged F1-score ML machine learning MNB Multinomial Naive Bayes NB Naive Bayes NDCG normalized discounted cumulative gain NLP natural language processing SVM Support Vector Machine TDM Tied Document Mixture TF term frequency TF-IDF term frequency-inverse document frequency TM text mining VSM Vector Space Model\nvi\nNotation and Nomenclature\nThe notation used in the thesis follows closely the linear algebra notation used in statistical natural language processing and machine learning, graphical models literature in particular [Manning and Schu\u0308tze, 1999, Bishop, 2006]. Counts of words in a document are represented using a word vector w, and a sequence of words is represented using a word sequence w. Mixture model notation is used to denote models conditional on variables, e.g. pl(n) = p(n|l) and plikj(wj) = p(wj|l, i, kj) . Apostrophe is used to reduce notation, by indicating derived functions and variables explained in the context, e.g. \u2211 n\u2032 indicates a sum over the same variable type as n and p\u2032(n) indicates a function related to p(n). Information retrieval and natural language processing terminology is reduced, e.g. \u201dword\u201d is used ambiguously to refer to word types and tokens.\nvii\nList of Notations\nw vector of word counts w = [w1, ..., wn, ..., wN ], ordinarily integer counts wn \u2208 N0 n word variable, word vector index 1 \u2264 n \u2264 N N number of distinct words, dimension of a word vector N = |w| w sequence of words w = [w1, ..., wj, ..., wJ ] j word sequence index 1 \u2264 j \u2264 J , 1 \u2264 wj \u2264 N J length of document J = |w|1 = |w| l label index variable 1 \u2264 l \u2264 L c label vector variable c = [c1, ..., cl, ..., cL], cl \u2208 {0, 1} L number of distinct labels in a collection D collection, a dataset of text documents i document index variable 1 \u2264 i \u2264 I, D(i) I number of distinct documents D(i) document i of dataset D, including possible meta-data, D(i) = w(i), D(i) = (w(i), l(i)) m mixture component index 1 \u2264 m \u2264M , \u03b1m, p(m), pm(n) M number of mixture components, maximum n-gram order k sequence of mixture assignment indicators k = [k0, , kj, , kJ ], 1 \u2264 kj \u2264M \u03b8 vector of model parameters, for a multi-class linear\nmodel y(\u03b8l,w) = \u03b8l0 + \u2211N\nn=1 \u03b8lnwn, with bias \u03b8l0 C(l, n) count of joint occurrences of variables in collection,\nC(l, n) = \u2211\ni:l(i)=l w (i) n\nD(l, n) discount applied to a count C(l, n) pul (n) unsmoothed multinomial p u l (n) = C(l,n)\u2212D(l,n)\u2211\u2032 n C(l,n\n\u2032)\u2212D(l,n\u2032) \u03b1 back-off weight, determined by the smoothing method r sequence of word weights, interpreted as probabilities of\nwords occurring, r = [r1, ..., rj, ..., rJ ], rj \u2208 [0, 1]\nviii\nChapter 1\nIntroduction\nThis chapter introduces the topic of the thesis and motivation for research. It presents a thesis statement based on the results of the research, lists the contributions of the thesis compared to the existing literature, references publications by the author related to the thesis, and describes the structure of the thesis."}, {"heading": "1.1 Motivation", "text": "The information age has brought an information overflow. The Internet provides a highway where vast amounts of data can be instantly searched and retrieved. This data presents a source that can be mined for knowledge about the world and to improve decision making. A considerable, and perhaps the most valuable, portion of this data is in text form, such as newspapers, web pages, books, emails, blogs and chat messages.\nThe field of artificial intelligence known as text mining has become an intersection between data mining, information retrieval, machine learning and natural language processing. Since its birth in the mid 90\u2019s, it has shown consistent growth in research publications, and has been applied in numerous ways in both industry and academia. Companies use text mining to monitor opinions related to their brands, while traditionally qualitative sciences such as the humanities use it as an empirical methodology for research. However, the fragmentation of text mining research has resulted in a variety of tools specialized for different applications.\nText mining applications are mapped into general statistical and machine learning tasks, such as classification, ranking, regression and clustering. The increase of data and computing power enables performing previously impossible tasks using statistical models. E-mail spam filters can be trained with trillions of text documents, and cover billions of words. Documents can be classified into a Wikipedia article hierarchy with millions of categories. A major limit on the possibilities of text mining is the scalability in the dimensions of data. General data mining models have not been developed for the sparse and increasingly high dimensional data encountered in text processing tasks. Data mining models that can easily operate on thousands of documents and words can be unusable on datasets with millions of documents and words. The main solution to scalability is to redesign algorithms that have no more than linear computational complexities in terms of\n1\nCHAPTER 1. INTRODUCTION\ndocuments, words, and class variables.\nGiven these problems of fragmentation and scalability, it would be useful to have models that are both versatile and scalable. A versatile model for text mining would have to applicable to different task types with high performance. A scalable model for text mining would have to scale in all of the relevant dimensions of the applied task.\nFollowing an overview of multinomial generative models of text, the thesis proposes extensions of the common Multinomial Naive Bayes (MNB) model as a family of versatile models for text mining. It is shown that when MNB is modified to correct its modeling assumptions, it forms a versatile solution to text mining that is far from the \u201cpunching bag of machine learning\u201d that basic Naive Bayes models have been called [Lewis, 1998, Rennie et al., 2003]. By using inverted index data structures, it is shown that many of the processing operations with MNB and its graphical model extensions can be solved as a function of sparsity, turning the \u201ccurse of dimensionality\u201d [Bellman, 1952, Feldman and Sanger, 2006] with sparse text data into a blessing. Sparse generative models combining generative models with sparse inference offer a versatile and scalable solution to text mining."}, {"heading": "1.2 Thesis Statement", "text": "The thesis statement is as follows:\nGenerative models of text combined with inference using inverted indices provide sparse generative models for text mining that are both versatile and scalable, providing state-of-the-art effectiveness and high scalability for various text mining tasks."}, {"heading": "1.3 Contributions of the Thesis", "text": "The thesis presents a synthesis of the fragmented text mining literature, and describes the generative multinomial models of text used across various text mining tasks. Based on the literature, the thesis proposes a solution to text mining that is both scalable and applicable to diverse tasks. It is shown that many of the multinomial models of text can be formalized as instances and extensions of the MNB model, including the n-gram language models that are widely applied across a variety of text processing applications. This common formalization enables the transfer of modeling innovations across different tasks and research literatures, and provides a high-performing baseline across different task types. Connections to linear and graphical models are shown, and extensions within these frameworks offer MNB even greater flexibility.\nLack of a proper formalization of a computational method means that the behaviour of a method is not explained by any existing theory, and it can behave erratically. This thesis formalizes modifications to MNB\n2\nCHAPTER 1. INTRODUCTION\nsuch as smoothing, feature weighting and structural extensions, showing that for most uses the modified models are not heuristic, but well-defined graphical models that extend the basic MNB model, in some cases using approximate maximum likelihood estimates. Formalizing these modifications means that the models are welldefined in a probabilistic sense, and their behaviour can be understood from the underlying probability theory.\nThe proposed sparse inference builds on the use of inverted indices for information retrieval with multinomial models of documents. It is shown that this type of inference is not limited to computing ranking scores for document retrieval, but can be used to compute exact posterior probabilities from any linear model for various uses, such as ranking, classification and clustering. Furthermore, sparse inference is extended to structural models, yielding the same improvements in scalability and computational complexity for many practical cases.\nFinally, the thesis presents an extensive empirical evaluation of the proposed models and inference over tens of standard datasets used for text classification and ranked retrieval. The experiments give strong evidence in support of the thesis statement. In both types of tasks, modifications of MNB outperform or rival strong baseline methods commonly used for these tasks in effectiveness. Scalability experiments are conducted on a large Wikipedia article classification task, showing that the proposed sparse inference improves scaling of the models into tasks with a million words, documents, and classes.\nAn extensive literature review has been conducted to verify the originality of the contributions. As expected, some of the contributions have prior and concurrent work. The following list highlights the most fundamental contributions that are not found in the prior literature, to the best of the author\u2019s knowledge:\nSynthesis of text mining methodology Surveys of text mining have limited their perspectives to mostly\na few of its influences, such as data mining [Witten, 2004, Hotho et al., 2005, Feldman and Sanger, 2006, Stavrianou et al., 2007, Weiss et al., 2012], machine learning [Weiss et al., 2012, Aggarwal and Zhai, 2012], information retrieval [Aggarwal and Zhai, 2012], and natural language processing [Hearst, 1999, Black, 2006]. Chapter 2 gives a concise synthesis of current text mining methodology, incorporating the perspectives of various practitioners in a coherent framework. Text data is shown to be multiply structured data from a linguistic point of view, many of the core algorithms used for text mining are shown to be cases of linear models, and possible views and solutions to the scalability problem are discussed.\nFormalization of smoothing with two-state Hidden Markov Models Early, but discontinued research\nshowed that the linearly smoothed n-gram language models used for information retrieval could be formalized with two-state Hidden Markov Models (HMM) [Miller et al., 1999, Xu and Weischedel, 2000, Hiemstra, 2001]. Chapter 4 shows that all of the smoothing methods for multinomial generative models can be expressed as exact inference on a two-state HMM. The formalization further shows that most of the parameter estimates for the smoothed multinomials derive from expected log-likelihood parameter estimation on a two-state HMM. Heuristically defined backoff models for Kneser-Ney [Kneser and Ney, 1995] and power-law [Huang and Renals, 2010] discounting are shown to derive from constraints applied to the two-state HMM.\n3\nCHAPTER 1. INTRODUCTION\nFormalization of feature weighting with probabilistic data Concurrent research has derived the ex-\npected log-likelihood estimation of n-gram language models from probabilistically weighted data [Zhang and Chiang, 2014]. Chapter 4 extends this probabilistic data view to inference, showing that both estimation and inference is well defined for probabilistically weighted word sequences and non-negative fractional word counts. This greatly extends the versatility of generative models of text, as data can be weighted to correct modeling assumptions, without losing the probabilistic formalization.\nSparse inference Earlier research has applied inverted indices for reducing the classification times for K-\nnearest Neighbours [Yang, 1994] and Centroid [Shanks et al., 2003]. The same reductions are gained for computing posterior probabilities for linearly interpolated language models in information retrieval [Hiemstra, 1998, Zhai and Lafferty, 2001b]. Chapter 5 shows that inverted indices can be used to reduce the inference complexity according to sparsity for a variety of processing tasks, including linear models and structural extensions of linear models. Applied to Wikipedia classification with a million possible categories, an order of magnitude reduction of classification times is obtained for Tied Document Mixture, a novel structural extension of MNB.\nEvaluation of modified generative models Evaluation of text mining methods has mostly been limited\nto experiments in one task type with a few possible methods [Sebastiani, 2002, Huston and Croft, 2014]. Chapter 6 presents a consistent framework for evaluation of text mining models applied for both text classification and ranked retrieval tasks. Model parameters are searched using Gaussian random search optimization, avoiding the problem of local optima. Statistical testing is conducted between datasets, measuring the strength of the discovered effects across text collections. Experiments are conducted over tens of text classification and retrieval datasets, comparing a large set of modified MNB variants with strong baseline methods for the tasks."}, {"heading": "1.4 Published Work", "text": "The thesis includes and expands on earlier work by the author, most published in peer-reviewed conferences:\n[Puurula, 2011a] Mixture Models for Multi-label Text Classification, New Zealand Computer Science Re-\nsearch Student Conference in Palmerston North, New Zealand, 2010. An analysis of generative mixture models for text modeling\n[Puurula, 2011b] Large Scale Text Classification with Multi-label Naive Bayes, Second Smart Information\nTechnology Applications Conference in Seoul, South Korea, 2011. A generative multi-label mixture model for modeling text\n[Puurula, 2012a] Scalable Text Classification with Sparse Generative Modeling, Pacific Rim International\nConference on Artificial Intelligence in Kuching, Malaysia, 2012. Earliest form of sparse posterior inference for MNB and a multi-label mixture model extension\n4\nCHAPTER 1. INTRODUCTION\n[Puurula and Bifet, 2012] Ensembles of Sparse Multinomial Classifiers for Scalable Text Classification,\nECML/PKDD - PASCAL Workshop on Large-Scale Hierarchical Classification in Bristol, United Kingdom, 2012. An ensemble of sparse generative models used successfully in a machine learning competition\n[Puurula, 2012b] Combining Modifications to Multinomial Naive Bayes for Text Classification, Asian In-\nformation Retrieval Symposium in Tianjin, China, 2012. Combinations of modifications to Multinomial Naive Bayes examined\n[Puurula and Myaeng, 2013] Integrated Instance- and Class-based Generative Modeling for Text Classifi-\ncation, Australasian Document Computing Symposium in Brisbane, Australia, 2013. Early version of the Tied Document Mixture model presented with sparse posterior inference\n[Puurula, 2013] Cumulative Progress in Language Models for Information Retrieval, Australasian Language\nTechnology Association Workshop in Brisbane, Australia, 2013. Combinations of modifications to information retrieval language models examined\n[Puurula et al., 2014] Kaggle LSHTC4 Winning Solution, Arxiv.org preprint, 2014. A description of the\nwinning solution to the Kaggle Large Scale Hierarchical Text Classification competition, with an ensemble of sparse generative models\n[Tsoumakas et al.] WISE 2014 Challenge: Multi-label Classification of Print Media Articles to Topics,\nWeb Information Systems Engineering in Thessaloniki, Greece, 2014. A report on the Kaggle WISE competition, where an ensemble using sparse generative models as components came second\n[Trotman et al., 2014] Improvements to BM25 and Language Models Examined, Australasian Document\nComputing Symposium in Melbourne, Australia, 2014. Exploration of recent information retrieval ranking functions, including generative language models discussed in the thesis. Best paper award\nOpen source code related to the thesis is distributed online, making the methods presented here available for wider use. The SGMWeka1 open source toolkit for sparse generative modeling is available through SourceForge.net, as well as dataset preprocessing scripts required to reproduce the results shown in the thesis. The Kaggle LSHTC4 winning solution2 is available via the Kaggle website, making it possible to replicate the winning methods. The competition description of the LSHTC4 solution is included in Appendix B of the thesis."}, {"heading": "1.5 Structure of the Thesis", "text": "The thesis is written to be accessible to readers of differing backgrounds. The introductory chapters, as well as the experiments and conclusion are intended to be readable by most. The chapters introducing novel\n1http://sourceforge.net/projects/sgmweka/ 2http://www.kaggle.com/c/lshtc/forums/t/7980/winning-solution-description\n5\nCHAPTER 1. INTRODUCTION\nmathematical ideas require extensive background knowledge in probability theory and statistical mathematics, and are recommended mainly for researchers. The rest of the thesis is structured as follows:\nChapter 2 introduces the topic of text mining, covering the terminology and methodology of text mining\nthat will be used in the following chapters. The emerging field of text mining is highly fragmented, and the used terms and methods differ widely. The chapter includes an extensive literature review of the topic, and presents the many facets of text mining in an integrated framework that is accessible to readers without extensive mathematical background.\nChapter 3 introduces the Multinomial Naive Bayes model for text mining and its extensions with genera-\ntive graphical models. This chapter establishes the mathematical notation that will be used throughout the rest of the thesis, and defines concepts such as graphical models and dynamic programming. This chapter is written to be accessible to readers with basic understanding of probability theory.\nChapter 4 presents a more detailed analysis of the MNB model for text mining. It is shown that all of the\ncommonly used smoothing methods for correcting data sparsity with multinomial text models can be formalized as approximate maximum likelihood estimation on a constrained Hidden Markov Model. It is shown that feature weighting can be equally formalized for MNB models and its extensions. Furthermore, practical graphical model extensions of MNB are proposed that maintain the efficiency of the model, while providing greater effectiveness and modeling flexibility. This chapter is accessible to readers with experience in derivations for graphical models.\nChapter 5 presents the idea of sparse inference for MNB, and more generally for linear models and struc-\ntured extensions of linear models. The complexity of inference is reduced as a function of sparsity, by using inverted index representation of model parameters. This chapter is the most technically demanding in the thesis, and contains novel algorithms and derivations.\nChapter 6 presents an extensive empirical evaluation of the modeling ideas presented in Chapters 4 and 5\nin the context of text classification and retrieval. In terms of effectiveness, it is demonstrated that the proposed extensions of MNB models greatly improve on the commonly used generative models for these tasks, providing results competitive with strong baseline methods for both tasks. In terms of scalability, it is shown that MNB with sparse inference easily scales to classification with a million features, documents and classes. Sparse inference on structured extensions of MNB scale with a similarly reduced time complexity, reducing inference times by an order of magnitude in the highest-dimensional cases examined.\nChapter 7 concludes the thesis with a summary of a thesis, revisits the thesis statement, and discusses the\nimplications of the findings, limitations of the thesis, and possible future work.\n6\nChapter 2\nText Mining and Scalability\nThis chapter presents a brief introduction to text mining, followed by a comprehensive overview of text mining methodology, and a discussion on the scalability problem in text mining. The introduction discusses the variety of definitions for text mining, related fields preceding text mining, and domains that apply text mining. The overview of text mining methodology provides a synthesis of viewpoints on text mining, starting from the linguistic properties and representation of text data, followed by mapping of text mining problems into machine learning tasks, and finally comparing text mining architectures to knowledge discovery processes. The discussion on scalability describes the scalability problem in text mining with examples, implicit views on scalability taken by researchers and practitioners, and existing approaches to scalability."}, {"heading": "2.1 Introduction to Text Mining", "text": ""}, {"heading": "2.1.1 Defining Text Mining", "text": "Progress in information technology has brought an information overflow, with transformative societal implications that affect all aspects of human life. A considerable and possibly the most significant portion of this information is in the form of text data, such as books, news articles, microblogs and instant messages. These vast quantities of text data can only be accessed and utilized using computers, but the automated processing of text is only possible using technology specialized for human language. Text mining (TM) in a broad sense refers to technology that allows the utilization of large quantities of text data. In the following, this working definition will be amended by a more concise one.\nText mining originates from several earlier research fields, such as data mining, machine learning, information retrieval, natural language processing. Like these fields, TM has a foundation in computer science, with considerable influence from applied artificial intelligence [Fayyad et al., 1996, Witten, 2004]. It is highly related and sometimes used interchangeably with terms such as information extraction, opinion mining and text analytics. TM is used in a variety of application domains, such as biomedical TM and business intelligence. The related fields have influenced TM in terminology and methodology, whereas the application\n7\nCHAPTER 2. TEXT MINING AND SCALABILITY\nTechnologies\nInformation Retrieval\nData Mining\nNatural Language Processing\nMachine Learning\nText Mining\nApplications\nDigital Humanities\nText Mining\nBusiness Intelligence Biomedical Text Mining\nNews Analytics\nLegal Text Mining\nSocial Sciences\nFigure 2.1: Relationship of TM to the major related fields (technologies) and application domains (applications)\ndomains have been influenced by TM. Figure 2.1 illustrates the relationship of TM to the major related fields and application domains. These relationships will be discussed in detail in Sections 2.1.2 and 2.1.3.\nThe term \u201ctext mining\u201d originated in the data mining publications of mid 1990\u2019s. Feldman et al. wrote a series of publications starting from 1995 under the term \u201cknowledge discovery in textual databases\u201d (KDT) [Feldman and Dagan, 1995, Feldman et al., 1997, 1998, Feldman and Sanger, 2006], and by 1997 a number of authors used the term text mining [Ahonen et al., 1997a, Rajman et al., 1997, Feldman et al., 1997, Tkach, 1997]. The early proponents of TM considered it to be an application of data mining to text data, and saw text data as \u201cunstructured data\u201d that needs to be structured for use in data mining: \u201cbefore we can perform any kind of knowledge discovery in texts we must extract some structured information from them\u201d [Feldman and Dagan, 1995]. This KDT definition viewed TM as data mining, with natural language processing and information retrieval as preprocessing and indexing steps in the mining process [Feldman and Dagan, 1995, Feldman et al., 1997, 1998, Ahonen et al., 1997a,b, Albrecht et al., 1998, Do\u0308rre et al., 1999, Tkach, 1997, Liddy, 2000]. A slight variation of this KDT definition was TM as a different type of process from general data mining [Rajman et al., 1997, Rajman and Besanon, 1998, Merkl, 1998, Witten et al., 1999, Tan, 1999, Witten, 2000a,b].\nWithin a few years TM started to interest other research communities. Natural language processing and computational linguistics researchers saw TM as a potential application [Hearst, 1999]. Many of the leading machine learning methods of the next decade were developed and popularized in the context of modeling text [Elkan, 1997, Joachims, 1998, Lewis, 1998, Lafferty et al., 2001, Hofmann, 1999, Blei et al., 2003], providing TM practitioners an advanced toolkit. Information retrieval had extended earlier into information extraction, and its similarities to TM were discovered at this time as well [Nahm, 2004, Mooney and Bunescu, 2005]. The definitions of TM gradually diversified away from Feldman\u2019s KDT definition of TM, as the term slowly started to be used in exceptionally diverse contexts in both academia and business. Witten [2004] in his review discusses the problems of defining TM, and explicitly avoids providing a concise definition. Both Black\n8\nCHAPTER 2. TEXT MINING AND SCALABILITY\n[2006] and Hotho et al. [2005] give a definition of TM close to the KDT definition, while noting the diversity of definitions. Cohen and Hersh [2005] in their survey of biomedical TM avoid providing an explicit definition. Stavrianou et al. [2007] in their survey give the KDT definition, while Weiss et al. [2012] appears to implicitly use the KDT definition.\nAlthough the KDT definition is a very simple characterization of TM, it is not very descriptive in practice. Perhaps the biggest problem with the definition is that it does not capture what is unique about TM. TM overlaps many fields to the extent that any of its applications could equally be considered as problems of the related fields. What makes TM is unique is not the tasks and problems it shares with the related fields, but the interdisciplinarity and integration of methods for solving the problems.\nAn example of a TM problem could be a web monitoring system for analyzing sentiment related to a brand. A system of this type would require methods from information retrieval to search text data related to the brand, natural language processing and information extraction for extracting parts of the text that refer to the brand, and machine learning for predicting the sentiment. The system would further need text visualization and statistical tests for confirming the reliability of the predictions. An integrated architecture for constructing such a system would most accurately be called a TM solution.\nThe view of TM as integration of artificial intelligence-based text processing technologies captures the main novelty of TM. Perhaps the first definition of TM from this point of view was reflected in the title of the KDD\u20192000 workshop on TM: \u201cText Mining as Integration of Several Related Research Areas\u201d [Grobelnik et al., 2000]. Feinerer et al. [2008] avoids choosing a definition, but seems to support this view as well: \u201cIn general, text mining is an interdisciplinary field of activity amongst data mining, linguistics, computational statistics, and computer science\u201d. For the purpose of this thesis, a concise definition is proposed:\nText mining is an interdisciplinary field of research on the automatic processing of large quantities of text\ndata for valuable information."}, {"heading": "2.1.2 Related Fields", "text": "Text mining originates from earlier and well-established fields grounded in computer science and artificial intelligence, the four major ones being data mining (DM), information retrieval (IR), natural language processing (NLP) and machine learning (ML). All of these fields are interdisciplinary with a considerable amount of participation from a diverse range of academic subjects related to computer science. As information technology is becoming increasingly prevalent in the modern world, much of the research in these fields is becoming distributed and applied across every discipline in the academic world, including the \u201csoft sciences\u201d of the humanities that have previously relied on qualitative methodologies. Outside academia, these fields exist as viable industries, with a global market of start-up and large-cap companies alike. A comparison of TM and related fields is given in the following.\n9\nCHAPTER 2. TEXT MINING AND SCALABILITY\nData mining and knowledge discovery in databases (KDD) [Fayyad et al., 1996, Chen et al., 1996] deal with the discovery of useful patterns in large databases, and have origins in statistics, machine learning and databases. Within KDD, DM constitutes the algorithms used for discovery of patterns, whereas KDD refers to the overall interactive process, where the user explores a dataset [Fayyad et al., 1996]. The term TM originated in DM research, and DM certainly remains one of the major influences in current TM. Many of the definitions used for KDD apply equally to TM today: \u201cThe KDD process can be viewed as a multidisciplinary activity that encompasses techniques beyond the scope of any one particular discipline such as machine learning.\u201d [Fayyad et al., 1996] and \u201cKDD also emphasizes scaling and robustness properties of modeling algorithms for large noisy data sets.\u201d [Fayyad et al., 1996]. Multidisciplinarity and scalability are equally defining qualities of TM. To some extent, TM shares the idea of process models that are applied in different tasks types [Ahonen et al., 1997a,b, Liddy, 2000]. An early view was that TM is simply DM with a text-specific preprocessing phase and an additional document filtering phase [Ahonen et al., 1997a, Do\u0308rre et al., 1999], but current TM systems are better described as architectures than a process [Feldman and Sanger, 2006]. A TM system neither requires a user discovering new patterns: TM can be used to automatically monitor existing well-known patterns in text, such as sentiment and topic. The DM characterization of text as \u201cunstructured data\u201d is also a broad generalization: text has shown to be a unique type of data structured in multiple ways, requiring specialized methods very different from general DM. The goals of TM and DM sometimes differ: the TM output is not necessarily hard facts or quantifiable values, but \u201csoft information\u201d in the form of text. Overall, there is surprisingly little interaction between TM and DM today, although much of TM can be situated in the context of KDD.\nMachine learning deals with systems that learn from data, and has origins in statistics, artificial intelligence and computer science. For a given learning task and performance measure, a learning system improves its performance using data [Mitchell, 1997]. This contrasts with statistics, where the emphasis is on finding the correct models for data, and not on directly optimizing performance [Breiman, 2001a]. The division in goals has led to a division between the \u201ctwo cultures\u201d of traditional statistics and machine learning [Breiman, 2001a]. The success of ML has lead to it being adopted as a general framework in a variety of application domains requiring artificial intelligence. Much of ML has dealt with text data [Joachims, 1998, Sebastiani, 2002, Lafferty et al., 2001, Blei et al., 2003], and much of TM is based on the application of ML methods; text classification in particular. The division of TM into distinct task types [Feldman and Sanger, 2006, Aggarwal and Zhai, 2012] also follows the general ML framework. Like with DM, the main reason for not considering TM as simply an application of ML is the uniqueness of text data. Techniques such as inverted indices have proven crucial for processing text, yet these are virtually unknown in ML. Although the majority of TM methods originate in ML, TM systems also require tools that are specialized for text, originating from a variety of disciplines, some requiring no learning, and some constructed with highly specialized human expertise.\nInformation retrieval deals with systems for retrieval and ranking of documents for a given information need. The ubiquitous case is web search, where the information need is expressed as a query consisting of\n10\nCHAPTER 2. TEXT MINING AND SCALABILITY\nwords, and the ranked set of documents consists of webpage links. Modern research into IR started in the context of computerized library indexing systems [Maron, 1961, Manning et al., 2008], where the number of paper documents was increasing rapidly. This parallels the arrival of vast amounts of digital documents and the development of TM half a century later. Although IR is more general and deals with different types of data, text is the main type of information used to index most forms of data, and text retrieval using word vectors and inverted indices constitutes the main methodology of current IR [Manning et al., 2008]. These two text retrieval techniques also constitute key components for TM, since they enable scalable search of documents. IR has become possibly the main influence on TM, as it has expanded into more complex tasks after progress in ad-hoc text retrieval was considered to be stagnant at the end of the millennium [Voorhees and Harman, 1999].\nNatural language processing or computational linguistics deals with the processing of text data using algorithms based on linguistic theory, and originated in the considerable efforts to develop machine translation during the early years of the cold war [Pierce, 1966]. Basic NLP tasks are segmenting, parsing, and annotating text according to the underlying linguistic structure of the data [Manning and Schu\u0308tze, 1999]. Corpus linguistics refers to the goal of producing well-defined computational theories to understand natural language, whereas the term NLP is closer to the goal of developing practical language technology. TM was proposed early as an additional goal for computational linguistics [Hearst, 1999], and NLP has certainly had a large influence on TM. Statistical NLP in particular predated TM by integrating IR, statistics and NLP [Manning and Schu\u0308tze, 1999]. Research in NLP has relied on annotated digital corpora such as the Brown corpus [Kucera and Francis, 1967]. In contrast, TM often uses unannotated, large scale, and noisy datasets, with the goal of extracting information of value. NLP is not as interdisciplinary as TM; TM research exists across a wide variety of problem domains, whereas NLP research is seldom conducted in other fields.\nRelated to statistical NLP is another field at the intersection of NLP and IR deserving a mention. Information extraction (IE) research started in the context of the Message Understanding Conference series starting from 1987 [Grishman and Sundheim, 1996, Sarawagi, 2008]. The goal of IE is the extraction of predetermined patterns from text collections, such as names, places and events. IE therefore has a more limited and concise goal than TM, but the two fields overlap to the extent that sometimes little difference is seen between the two [Sebastiani, 2002, Stavrianou et al., 2007]. One viewpoint is considering IE as an intermediate step to TM, where the extracted patterns are used for discovery of more complex information [Nahm, 2004, Mooney and Bunescu, 2005]. Text summarization deals with the extraction of human-readable summaries of text, and is a type of TM task that falls outside the scope of IE [Witten, 2004, Sarawagi, 2008, Das and Martins, 2007].\nOne way to measure the development of TM compared to the related fields is to count the number of publications that use the term over the years. Figure 2.2 shows the number of publications for years 1998- 2013 indexed by Google Scholar1 and ScienceDirect2 containing the names of the fields. As can be seen, for a decade TM has grown at a steady rate of 800 more publications each year according to Google Scholar,\n1scholar.google.com 2sciencedirect.com\n11\nCHAPTER 2. TEXT MINING AND SCALABILITY\n1\n10\n100\n1000\n10000\n2013200820031998\nSc ie\nn ce\nD ir\nec t\nEn tr\nie s\nYear\nScienceDirect\n\"text mining\" \"data mining\" \"machine learning\" \"natural language processing\" \"information retrieval\"\n(a) ScienceDirect\nGoogle Scholar\n12\nCHAPTER 2. TEXT MINING AND SCALABILITY\nand 50 according to ScienceDirect. Analyzing these graphs relies on a couple of assumptions: the quality of indexed publications is roughly similar, and that terminological ambiguity such as the use of \u201ccomputational linguistics\u201d for \u201cnatural language processing\u201d does not distort the results. Since both indices agree on the main effect compared to the four other fields, this is most likely correct: published TM research has grown to a volume that the related fields had around year 2000."}, {"heading": "2.1.3 Application Domains", "text": "TM has propagated into a wide variety of domains both in academia and business. In academia, TM is enabling new forms of research by providing a methodology for meta-research of large quantities of publications [Jensen et al., 2006], as well as providing computational methods for fields that have lacked quantitative methodologies [Schreibman et al., 2008]. Aside from the sheer volume shown in Figure 2.2, the diversity of TM applications is challenging and lacks a clear-cut categorization. Surveys in TM have suggested different application areas over the years. Black [2006] cites business intelligence and biomedical TM as the main applications. Feldman and Sanger [2006] give corporate finance, patent research, and life sciences as the most successful applications. Fan et al. [2006] categorizes applications into medicine, business, government, and education. Weiss et al. [2012] gives a number of case studies: web market intelligence, digital libraries, help desk applications, news article categorization, email filtering, search engines, named entity extraction, and customized newspapers. Overall, the general categories of biomedical TM and business intelligence capture the two most established application domains of TM.\nBiomedical TM was proposed in the context of information retrieval as \u201cthe discovery of hidden connections in the scientific literature\u201d [Swanson, 1988, 1991, Hearst, 1999]. Biomedical TM has become an exceedingly popular application, since TM has provided a meta-analysis methodology to discover new facts by combining evidence from the vast biomedical research literature [Cohen and Hersh, 2005, Jensen et al., 2006, Rzhetsky et al., 2008, Zhou et al., 2010, Korhonen et al., 2012, Van Landeghem et al., 2013, Shemilt et al., 2013]. The overall output of scientific publications has increased exponentially for the last century, with some current estimates of annual growth at 4.73% [Larsen and von Ins, 2010] and 8\u22129% [Bornmann and Mutz, 2014]. In 2014, the biomedical article index PubMed3 contains entries for over 24 million publications, and this exponentially growing literature can only be comprehended using new methods. The popularity of biomedical TM has lead to a common perception of TM as simply biomedical literature mining.\nThe origins of TM in business intelligence can be traced to a 1958 IBM paper [Luhn, 1958], that proposed an automated system for managing information in documents. Business TM applications are diverse, including financial TM [Kloptchenko et al., 2004, Gro\u00df-Klu\u00dfmann and Hautsch, 2011], marketing [Decker and Trusov, 2010, Pehlivan et al., 2011], and market intelligence [Archak et al., 2007, Godbole and Roy, 2008, Baumgartner et al., 2009, Pehlivan et al., 2011, Ghose and Ipeirotis, 2011, Archak et al., 2011, Netzer et al., 2012]. Perhaps the most common business use of TM is sentiment analysis, and more generally opinion mining\n3http://www.ncbi.nlm.nih.gov/pubmed\n13\nCHAPTER 2. TEXT MINING AND SCALABILITY\n[Dave et al., 2003, Pang and Lee, 2008], that seeks to analyze text data in order to monitor opinions related to companies, brands and products. Some business TM publications are starting to use the term text analytics as a synonym for TM [Gruhl et al., 2004, Gro\u00df-Klu\u00dfmann and Hautsch, 2011, Basole et al., 2013], following the trend in the industry where the term \u201canalytics\u201d has become increasingly common over the past decade.\nOutside these two major groups of applications, there are application domains that have recently adopted TM methodology. Domains such as law [Coscia and Rios, 2012], political science [Grimmer and Stewart, 2013], humanities [Schreibman et al., 2008], social science [Brier and Hopp, 2011] and intelligence [Maciolek and Dobrowolski, 2013] have combined TM methodology with traditional research methodologies. Many of these domains are applying TM to online data sources such as blogs and micro-blogs, but some use TM on digitized publications. The \u201csoft sciences\u201d such as humanities and social science in particular see TM as providing a new methodology for performing quantitative research on issues that previously relied only on qualitative methods [Schreibman et al., 2008].\nThe extreme interdisciplinarity and variety makes assessing the overall scope of TM in great detail daunting, due to differences in: 1) terminologies, 2) methodologies, and 3) traditions of publication. Firstly, a large portion of TM research occurs under related terms, such as text analytics, opinion mining, etc. The different fields use their own terminology in addition to TM terms. This makes it difficult to find the relevant publications, and to synthesize a coherent picture from manuscripts written to address different issues using different terms. Secondly, TM is often mixed with the methodologies of the application domain. Comparing TM research often requires expertise of the theory and methodologies of both TM and the application domain. Thirdly, academic communities have varying traditions on publishing: computer scientists publish foremost in conferences, whereas humanities publish in the form of books, while most other disciplines prefer journal publication. A TM book written in the context of digital humanities could prove influential, yet lack the impact factors used in the natural sciences. This complicates assessing the quality of TM publications from external indicators such as citation counts and impact factors. A further complication is grey literature: influential discourse on TM takes place not only in established and peer-reviewed academic contexts, but also in contexts such as non-reviewed articles4, blogs5, and white papers6."}, {"heading": "2.2 Text Mining Methodology", "text": ""}, {"heading": "2.2.1 Text Documents as Multiply Structured Data", "text": "Text data is commonly described as \u201cunstructured data\u201d. This phrase originated in the earliest data mining enquiries into text, and has since been used in almost every description of TM. From a data mining point of view, raw text data is not organized into a database of numeric values, hence it can be considered to be\n4http://www.nature.com/news/trouble-at-the-text-mine-1.10184 5http://breakthroughanalysis.com/ 6http://hurwitz.com/index.php/component/content/article/394\n14\nCHAPTER 2. TEXT MINING AND SCALABILITY\nunstructured. The purpose of TM was to convert text into a structured form, where data mining could be applied [Ahonen et al., 1997a, Tan, 1999]. The unstructured data description provides a simple introduction to TM from a data mining perspective, but is unfortunately misleading.\nText is more accurately called multiply structured data. The English word \u201ctext\u201d comes from the Latin etymology \u201ctextus\u201d meaning \u201cwoven\u201d, related to the words \u201ctextile\u201d and \u201ctexture\u201d. Not only is language structured, but it occurs in documents and collections that have additional varying structure. Written text can be understood as sequential statements organized in hierarchies of structures such as sentences, passages, sections, chapters, and so forth. Explicit structure in the form of metadata is virtually always available, whereas the \u201cunstructured data\u201d of human language has implicit structure, arguably among the most complex phenomena to have evolved in nature. The orthographic and typographic structure of written text is further complicated by the structured document mark-ups used in digital text, including hyperlinks that turn text data into hypertextual data better understood in terms of graphs.\nA collection of text data consists of documents that can number in millions or more. The collections can be static, with no time component, or dynamic, with documents ordered by time. The collections can be fixed datasets, or streams that are not retained in memory, but processed in an online manner. Collection and document metadata is typically very rich in TM applications, and can include internal and external hyperlink structures of the documents, locations and languages of the documents, author identities, years and dates of authorship, subcategories and ontologies of the documents, etc. The metadata can be unique to the dataset, or highly standardized [Bargmeyer and Gillman, 2000]. Additional explicit metadata can be constructed by\n15\nCHAPTER 2. TEXT MINING AND SCALABILITY\napplying TM and ML methods on the dataset [Pierre, 2002]. The document content is organized into fields such as titles, text sections and possibly link sections, and often contains non-text and partly textual media such as figures, illustrations, tables and multi-media. The layout, typography and mark-ups define the visual look of text and hyperlinks connect the text within the document, to other documents, and to resources outside the collection. Figure 2.3 illustrates document structure from the beginning of a Wikipedia document.\nA collection used for a specific task has an associated domain of background knowledge [Anand et al., 1995, Feldman and Sanger, 2006]. Feldman and Sanger [2006] define domains in TM loosely as: \u201ca specialized area of interest for which formal ontologies, lexicons, and taxonomies of information may be created\u201d. The availability and usefulness of domain knowledge is one of the defining properties of TM [Feldman and Sanger, 2006]. The domains depend on the use of the collection, and knowledge from more than one domain can be beneficial. For example, a collection of Twitter microblog messages and newspaper articles used for monitoring a company\u2019s public image could benefit from having domain knowledge for spelling correction, normalization, and named entity recognition, for both types of data. The simplest form of domain knowledge is text collections of billions of words in the domain language that can be used to construct models for TM [Napoles et al., 2012, Buck et al., 2014]. Natural language consists of languages, dialects, ethnolects and sociolects. It is common that TM collections and domains only cover a particular subset of one language.\nThe actual text content of a text document consists of sequential information in the form of natural language. From the last century of linguistics research into the structure of natural language, it has been established that language consists of structures at various levels. Starting from the highest level, these are discourse, pragmatics, semantics, syntax, morphology, phonology and phonetics [Jurafsky and Martin, 2008]. Linguistics itself has divided into subfields that each specialize in one of these levels. Discourse deals with\n16\nCHAPTER 2. TEXT MINING AND SCALABILITY\ntopics and discussions, pragmatics with contextual meanings and interpretations, and semantics with the meanings of linguistic constructions. Syntax deals with the generation of sentences from words, and morphology with the generation of words from morphemes. Phonology and phonetics deal with phonemes and phones, the atomic units of speech. Figure 2.4 shows the structure at the syntactic level for the definition of TM used in this thesis, as provided by the NCLT wide-coverage parser7 [Cahill et al., 2004].\nThe lowest levels of language are easiest to describe formally, and syntax and the higher levels were considered too complex for formal descriptions until the advent of computational linguistics. While syntax and the lower levels have clearly defined elementary units such as words and morphemes, no consensus exists for elementary units in the higher levels of language. The levels are not strictly separated: morphosyntax, morphophonology, and morphosemantics study some of the interactions between these levels. The levels are neither strictly hierarchical, but parallel. For example, within a word the morphological and syllabic segments commonly overlap: the word \u201crated\u201d has the morphological boundaries \u201crate + d\u201d, and the syllable boundaries \u201cra - ted\u201d. Text and speech analysis thus commonly annotates linguistic data with overlapping description tiers.\nA further complication in natural language is ambiguity in the various levels. Ambiguous structures are very common in natural language. A common example of ambiguity is the sentence \u201cTime flies like an arrow; fruit flies like a banana\u201d [Burck, 1965]. The words \u201cflies\u201d and \u201clike\u201d are both used ambiguously, \u201cflies\u201d as a verb in the first clause and as a noun the second, \u201clike\u201d as an adverb in the first and as a verb in the second. The sentence is also a \u201cgarden path sentence\u201d, since reading it forces the reader to disambiguate the word \u201cflies\u201d in the first clause, only to realize that \u201ctime flies\u201d is not used as a noun phrase, and that the two clauses must be non-related. Resolving ambiguities requires understanding of the larger context, but generally provides higher efficiency to language as a form of communication [Piantadosi et al., 2012]."}, {"heading": "2.2.2 Structured Representations for Text", "text": "The view of text as unstructured data is misleading, but it considerably simplifies the overwhelming complexity of processing text documents. While human readers can easily understand most types of text documents, the simplest types are undecipherable for general computer algorithms. It is therefore necessary to use simplified representations of text to perform any processing of text that is natural for humans. TM uses different representations of text that depend on the use, also called intermediate forms [Tan, 1999] or representational models [Feldman and Sanger, 2006]. In the following discussion, formal notation is introduced that will be extensively used in the later chapters of the thesis.\nNLP commonly uses text processed into some type of normalized form. This is a form of text with all nonlinguistic elements removed or separated from the text content, and the text is encoded using a standard such as ASCII or Unicode. The text can then be further normalized to remove unwanted variance [Zhu et al., 2007,\n7http://lfg-demo.computing.dcu.ie/lfgparser.html\n17\nCHAPTER 2. TEXT MINING AND SCALABILITY\nDemuynck et al., 2009, Yang and Eisenstein, 2013]. A common normalization is expansion of abbreviations and number words. Another common normalization is spelling correction. The normalized form depends on the intended use. For modeling text as word sequences using n-gram models, modeling effort can be reduced by removing sentence-initial capitalization and punctuation, and placing sentence boundary tokens. Alternatively, the text can be normalized to recover the capitalization and punctuation instead, if the original text is missing these. A word sequence variable representing a normalized document of J words can be formally expressed as w, where each integer variable wj : 1 \u2264 j \u2264 J in the sequence indexes a word in a dictionary of N possible words. Normally the dictionary size N doesn\u2019t need to be defined. The dictionary can be easily updated by maintaining a hash table, and mapping each previously unseen word to the integer value N+1.\nThe normalized word sequences can be further processed according to the intended use. For uses such as text classification, clustering and retrieval, there exists a set of common normalizations: stemming, stopwording and short word removal. Stemming removes word endings, so that for example the words \u201cconnect, connected, connecting, connection, connections\u201d are all mapped to the same word \u201cconnect\u201d [Lovins, 1968, Porter, 1980]. This reduces variability by performing a heuristic clustering of the words. Short words of less than three characters are removed, along with words that occur in a stop-word list: a list of usually around 1000 common words that are not useful for the task. These linguistic processing methods depend on the language, and in most languages advanced morphological processing is required [Kurimo et al., 2010, Zhao and Liu, 2010]. The linguistic processing can also enrich the words with tagging information, such as part-of-speech and dependency tags.\nThe normalized sequence forms used in NLP are insufficient for the common ML and DM methods that rely on data organized into vector forms. The majority of TM applications use a feature vector representation of text documents originating from information retrieval [Salton, 1963, Salton et al., 1975]. The most basic type of a feature vector for a document is the \u201cbag-of-words\u201d, where a feature vector w consists of the counts of each word in the document wn, where n indexes the dictionary of N words. The norms of a vector are used\nto measure document length. L1-norm is the length of the word sequence: |w(i)|1 = J (i) = \u2211 nw (i) n , whereas\nthe \u201dL0-norm\u201d is the number of non-zero counts: |w(i)|0 = \u2211 n min(1, w (i) n ). Sparsity of the vector equals the proportion of non-zero counts: |w(i)|0/N . More complex feature vectors differ from the bag-of-words in how the features are chosen, and how the counts or weights of each feature are computed. Table 2.1 shows a comparison of word sequences and word vector representations using integer counts.\nEarlier text mining research believed that simple weighted words are not easily outperformed for most tasks [Salton and Buckley, 1988, Sebastiani, 2002]. Possible alternative features include word pairs [Lesk, 1969], linguistically tagged words [Dave et al., 2003, Gamon, 2004], factor concepts and topics [Borko and Bernick, 1963, 1964, Deerwester, 1988, Hofmann, 1999, Blei et al., 2003], phrases [Salton, 1988] and parse trees [Chubak and Rafiei, 2012]. Some applications such as authorship detection and essay scoring rely on nontypical features such as document length [Larkey, 1998, Madigan et al., 2005]. More recently, word sequence\n18\nCHAPTER 2. TEXT MINING AND SCALABILITY\nCHAPTER 2. TEXT MINING AND SCALABILITY\nfeatures8 have become a crucial part in some text classification tasks [Dave et al., 2003, Gamon, 2004, Xia et al., 2011, Lui, 2012, Tsoumakas et al., 2013]. The recent results finding considerable improvements from combining other features with word vectors are due to the availability of more data, and advanced models for combining the feature sets.\nFor some uses binary weights \u2200n : wn \u2208 0, 1 are sufficient. For most uses it is beneficial to weight the features so that the words relevant for the modeling purpose are weighted higher. This results in non-negative fractional counts \u2200n : wn \u2208 R \u2227 wn \u2265 0. With word features, the weighting functions typically dampen high count values, normalize the counts for varying document lengths, and weight the words according to rarity in the collection. A variety of possible weighting functions exist for choosing the weights, the most common being Term Frequency - Inverse Document Frequency (TF-IDF) [Salton and Buckley, 1988]. When weighting functions are used, the transformed counts can be denoted w, whereas the original counts can be denoted w\u2032.\nA collection of I documents can be formalized as a set D, where the document variable D(i) for each document identifier i consists of the structured variables used to represent the document. With word vectors for representation and no label information or other metadata, the document variable consists of the word vector: D(i) = (w(i)). With word sequences and label variables l(i) : 1 \u2264 l(i) \u2264 L of L possible labels, the document variables would be D(i) = (l(i),w(i)).\nThe dictionary size N for word vectors in a collection of millions of documents could typically be in the hundreds of thousands. Out of the possible words, typically only some tens or hundreds of words occur in a document. This means that the word vectors are extremely sparse, and both the dimensionality and sparsity increases as larger collections are processed. If counts are accumulated from all documents corresponding to a label, the label-conditional counts are almost as sparse. Like word vectors, most useful representations of text are high-dimensional sparse data.\nRepresenting a collection of high-dimensional sparse data can be done with an inverted index [Zobel and Moffat, 2006], enabling scalable retrieval of documents as well as other types of inference [Yang, 1994, Shanks et al., 2003, Kudo and Matsumoto, 2003, Puurula, 2012a]. The scalability of modern web search engines is largely due to the representation of web pages using inverted indices [Witten et al., 1994, Zobel and Moffat, 2006]. An inverted index stores a document collection as a table of dictionary words or terms, and a postings list of document occurrences of the term n. Table 2.2.2 illustrates an inverted index representation for the example documents shown in Table 2.1. The inverted index representation is highly efficient, since the term occurrences are sparse and zero counts do not need to be considered when constructing, storing or using the index. Normally a posting contains a document identifier and the number of occurrences of the term in the document. Position information is sometimes included in the postings, for ranking functions that benefit from proximity information. The postings lists are commonly compressed for additional storage savings, and methods for further improving the efficiency of indices constitute an extensive literature [Witten et al., 1994,\n8called n-grams in publications, but these are word sequence features\n20\nCHAPTER 2. TEXT MINING AND SCALABILITY\nword n postings list the 1 (1, 3) (2, 2) book 2 (1,1) was 3 (1, 1) released 4 (1, 1) in 5 (1, 1) (2, 1) 1985 6 (1, 1) after 7 (1, 1) publication 8 (1, 1) of 9 (1, 1) first 10 (1, 1) bachman 11 (2, 1) books 12 (2, 1) is 13 (2, 1) still 14 (2, 1) print 15 (2, 1) united 16 (2, 1) kingdom 17 (2, 1) although 18 (2, 1)\nTable 2.2: Inverted index representation for the documents shown in Table 2.1. The postings lists are non-positional and unweighted, containing only the document identifiers and word counts contained in the document word vectors\nZobel and Moffat, 2006]. Use of inverted indices can be described as a type of sparse matrix computation applied to text, although this view is not ordinarily taken in IR."}, {"heading": "2.2.3 Text Mining Applications as Machine Learning Tasks", "text": "TM is applied in numerous ways across application domains. One possible way to categorize the applications is to use ML terminology and consider the underlying learning problem that is solved in each application [Feldman and Sanger, 2006, Aggarwal and Zhai, 2012]. This makes it possible to compare solutions used in different applications, and attempt solutions used for non-text data of the same task type. The basic framework of ML is described next, followed by a mapping of many common TM tasks into ML problems.\nA commonly accepted definition of ML is: \u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experience E.\u201d [Mitchell, 1997]. For an example application of e-mail spam filtering, the experience E could be a training dataset of spam/non-spam e-mail examples, the task T could be the binary classification of new examples into spam/non-spam, and the measure P could be the percentage of correctly classified e-mails.\nWe can consider document collections D a type of dataset for ML algorithms. The first division in learning\n21\nCHAPTER 2. TEXT MINING AND SCALABILITY\nproblems is between inductive and transductive learning [Gammerman et al., 1998, Joachims, 1999]. Given a training dataset D, inductive learning attempts to learn a function for general prediction, usually for making predictions on unseen data. Transductive learning does not attempt to learn a general function, but rather attempts to transfer properties found in a training dataset D to a test dataset D\u2217. The transduced properties are normally the label information available for dataset D, but not for D\u2217. Transduction improves prediction quality, but the solutions will be optimized only for the used test set.\nA second division in learning problems is between supervised, semisupervised and unsupervised learning. In supervised learning, a training dataset is provided with label variables, whereas in unsupervised learning no label variables are provided. Supervised learning is considerably easier than unsupervised learning, as the label variables are usually reliable information for learning the function of interest. Unsupervised learning has to constrain the learning problem to compensate for the lack of label information. For example, in a text clustering task the number of clusters is often fixed and prior distributions can be used to guide the learning towards a more plausible clustering. The corresponding supervised task of text classification would have the label variables provided, so that the number of labels and their assignments to documents would not need to be learned, unlike in the unsupervised case. In semisupervised learning some portion of the label variables are provided. Semisupervised learning is very common in TM tasks, for example in many text classification applications there are a small number of labeled documents compared to large quantities of unlabeled documents.\nThe type of variables to be predicted divides ML problems further. Classification deals with the prediction of discrete label variables. Ranking deals with the ordering of discrete label variables. Regression deals with prediction of continuous label variables. The predicted variables can be structured. Binary classification deals with binary label variables l \u2208 0, 1, multi-class classification with categorical label variables l : 1 \u2264 l \u2264 L, multi-label classification with label vectors c = [c1, ..., cL] of binary variables [Tsoumakas et al., 2010] and multi-dimensional classification with label vectors of categorical variables [Bielza et al., 2011]. Corresponding divisions exist for ranking and regression problems, as well as multi-output or multi-target prediction problems that contain mixed output variables.\nMore complex structured prediction problems arise when the input variable is not a simple vector. Sequence labeling classifies sequence variables into corresponding structured variables. For example, syntactic parsers map word sequences into parse trees, and speech recognition maps sequences of acoustic feature vectors into word sequences. Solutions for structurally complex problems often have a number of uses, and produce models that provide information about the learning problem. Multi-task learning attempts to solve different tasks at the same time, taking advantage of the related optimization problems to find better solutions [Caruana, 1993].\nTable 2.3 shows a sample of common TM applications and their mapping into ML tasks. An application can be mapped into a ML task in several different ways. For example text regression problems [Archak et al., 2007, Joshi et al., 2010, Wang et al., 2010a, Archak et al., 2011, Ghose and Ipeirotis, 2011, Cao et al., 2011, Higgins et al., 2014, Lee et al., 2014] such as stock price prediction can often be solved using regression or\n22\nCHAPTER 2. TEXT MINING AND SCALABILITY\nclassification. Commonly, the main improvements in ML come from defining the task well and choosing the features useful for that task, rather than the choice of learning algorithms. Multiple ways of approaching a problem can work, and the combination of different types of solutions is highly beneficial. Complex learning approaches are not necessary for applying the ML framework: classifiers such as K-nearest Neighbours [Cover and Hart, 1967] and Naive Bayes (NB) [Maron, 1961] can operate on the basis of counted training dataset statistics, without the use of iteratively learned parameters.\nThe performance measures P depend on the application and task. For general classification tasks, accuracy is defined as the percentage of correctly classified instances for a given test set. This might be ill-suited, if the use of the classification system is to find some relevant documents for each possible label. Extensively studied tasks have highly specialized measures that attempt to quantify the usefulness of the ML system in the applications, such as NDCG that is used to measure ranking performance in web search engines [Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002].\nThe ML framework involves segmentation of datasets into training and test portions, so that the performance is not measured on the same data that is used to learn parameters. The most typical split is between a training portion for learning parameters, a development portion for calibrating meta-parameters of the algorithms, and a final test portion that is used for evaluation. Alternatively, cross-validation segments a dataset into a small number of exclusive training and development portions, and the performance measure can be averaged across the folds. More complex solutions can have a number of nested dataset segmentations, reserving unused testing data to optimize each layer in a system."}, {"heading": "2.2.4 Linear Models as Methods for Text Mining", "text": "Mapping TM applications into established tasks enables the use of existing methods for solving problems. Earlier methods for TM relied on linguistic methods for performing text preprocessing, and algebraic meth-\n23\nCHAPTER 2. TEXT MINING AND SCALABILITY\nods for performing text retrieval. The more recent methods for TM are algorithmic, often model-based, and predominantly originate in statistics and ML. The new algorithmic methods based on statistics and learning have brought a paradigm shift in the field of artificial intelligence, and there remain few areas of TM where solutions based solely on domain expert-knowledge are preferred. Commonly, domain knowledge such as stemmers and sentiment lexicons are used as additional information for learning algorithms.\nMost algorithms on word vectors and related representations are applications of linear models, that perform predictions using linear combinations of feature values weighted by learned parameters. The tasks that are commonly solved using linear models include regression, classification, ranking and clustering. In text regression regularized linear regression can be applied [Archak et al., 2007, Joshi et al., 2010, Higgins et al., 2014]. In text classification classifiers such as Centroid [Rocchio, 1971, Han and Karypis, 2000], Bernoulli Naive Bayes (BNB) [Maron, 1961], Logistic Regression (LR) and Support Vector Machines (SVM) [Joachims, 1998, Fan et al., 2008] are linear models. In text ranking and text retrieval, all of the common scoring functions are linear models, including the Vector Space Model (VSM) [Salton et al., 1975], language models [Kalt, 1996, Hiemstra and Kraaij, 1998], as well as more recent discriminative ranking models [Metzler and Croft, 2007]. Text clustering commonly uses linear models, such as multinomial and Cosine distances [Pavlov et al., 2004, Zhong and Ghosh, 2005, Banerjee et al., 2005, Rigouste et al., 2007]. The following presents a succinct overview of the linear model framework for TM.\nA basic type of statistical model for solving modeling problems is the linear regression model, as used for solving text regression problems. Let us assume word vector features w, with a dictionary of N words. Let \u03b8 denote a parameter vector of weights for the regression model, where \u03b80 is called the \u201cbias\u201d parameter and \u03b8n for values 1 \u2264 n \u2264 N are the regression weights for each word feature n9. A linear regression predicting scores y(\u03b8,w) of a predicted continuous variable y takes the form:\ny(\u03b8,w) = \u03b80 + N\u2211 n=1 \u03b8nwn (2.1)\nThe weights \u03b8n decide how much the predictors wn explain the observed variations of y seen in a training dataset, where y and w are available, and \u03b8 needs to be estimated. A basic way to estimate the parameters \u03b8 is the method of least squares, so that the sum-of-squares error function over the dataset\nED(\u03b8) = 1/2 \u2211 i(y (i)\u2212\u03b8Tw(i))2 is minimized. This is equivalent to maximum likelihood estimation assuming that the errors are generated by a Gaussian noise model.\nMost applications of linear models use regularization to control overfitting [Frank and Friedman, 1993]. This adds a regularization term R(\u03b8) to the error function, so that the total error function becomes E \u2032D(\u03b8) =\n9Using w for the weight vector is the common notation with regression models [Bishop, 2006]. The different notation \u03b8 is used here to keep the notation consistent throughout the thesis.\n24\nCHAPTER 2. TEXT MINING AND SCALABILITY\nED(\u03b8) + \u039bR(\u03b8), where \u039b is a weight for the regularization. Regularization often takes the form 1/2 \u2211\nn |\u03b8n|q, where q is the L-norm of the regularizer. A common case q = 1 is called the Lasso regularizer, and the case q = 2 is the ridge or Tikhonov regularization. Regularization causes parameter estimates to shrink towards more conservative values, to zero in the case of the sparsity-inducing Lasso regularizer.\nLinear models for classification and ranking apply a further decision rule on the scores y(\u03b8,w) to map the scores into categories and rankings. Binary classification maps the scores based on the sign of the score: with a classification threshold of 0, if y(\u03b8,w) \u2265 0, l = 1, else l = 2. Multi-class classification involves labeldependent parameter vectors \u03b8l, and maps the scores by maximizing the score: argmaxl(y(\u03b8l,w)). Ranking sorts the scores for the parameter vectors, and maps the order of labels into ranks [Metzler and Croft, 2007]. The multi-class linear scoring function can be expressed as:\ny(\u03b8l,w) = \u03b8l0 + N\u2211 n=1 \u03b8lnwn (2.2)\nThis elementary function covers a swath of modeling approaches, with highly different semantics for the bias parameters \u03b8l0 and the label-dependent parameters \u03b8ln. In classification, models that can be expressed in the form of Equation 2.2 are called linear classifiers, since they form linear decision boundaries as a function of the feature vectors. Figure 2.5 illustrates the decision boundaries of a linear classifier compared to a non-linear classifier. With probabilistic approaches, the scores are related to the posterior probability of the label given the data through a link function. For example, with exponential-family models such as LR, BNB and Multinomial Naive Bayes (MNB), the posterior probabilities are p(l|w) = exp(y(\u03b8l,w)). With non-probabilistic\n25\nCHAPTER 2. TEXT MINING AND SCALABILITY\napproaches such as SVM, the scores are optimized entirely for classification and do not represent probabilities.\nEstimating the linear model parameters depends on the models and the strategies used to reduce overfitting. With algebraic methods such as the Centroid Classifier and generative probabilistic models such as MNB, the label-dependent parameters \u03b8ln are estimated for each label independently, while the bias parameters \u03b8l0 are assumed uniform or estimated separately. With generative models, \u03b8ln = log(pl(n)) are label-conditional log-probabilities, \u03b8l0 = log(p(l)) are label prior log-probabilities, and both types of parameters can be smoothed and scaled to correct for overfitting. With discriminative classifiers such as LR and SVM, the parameters are estimated by minimizing a regularized error function [Fan et al., 2008], similarly to learning the regularized linear regression.\nTable 2.4 summarizes the parameter estimates for the commonly used linear models in TM. For BM25, the\nCroft-Harper IDF function is often used: IDF (n) = log I\u2212In+0.5 In+0.5 [Manning et al., 2008], where In is the number of documents where the word n occurs. Another common IDF function with BM25 is IDF (n) = log I+1 In+0.5 [Fang et al., 2004]. The soft length normalization for BM25 is given by LN(i) = w (i=l) n /(1\u2212 b+ b(|w(i=l)|1/A))\n, where the average document length A is \u2211\ni |w(i)|1/I. The loss function L(\u03b8, D(i)) for SVM/LR is log(1 + \u2212l\n(i)\u03b8Tw(i)) for LR, max(0, 1 \u2212 l(i)\u03b8Tw(i)) for L1-loss SVM and max(0, 1 \u2212 l(i)\u03b8Tw(i))2 for L2-loss SVM [Fan et al., 2008]. The regularization R(\u03b8) for SVM/LR is 1\n2 |\u03b8|22 for L2 regularization and |\u03b8|1 for L1 regularization.\nBM25 requires the meta-parameters k1, k3 and b, LR/SVM requires the meta-parameter C for regularization. Use of feature transforms and smoothing for VSM, BNB, and MNB modifies the equations in Table 2.4, and introduces additional meta-parameters.\nInference for different uses with parameters in the form of Equation 2.2 is a trivial summation and maximization for classification, and summation and sorting for ranking. The algorithms used for estimation differ widely. For the Centroid Classifier and MNB, the estimation is a simple closed-form linear-complexity pro-\n26\nCHAPTER 2. TEXT MINING AND SCALABILITY\ncedure of summing, normalizing and possibly smoothing the word count statistics. For LR and SVM, the algorithms depend on the error function and regularization [Yuan et al., 2012]. Many of the practical models have the property of a convex error function, so that the estimation can be performed using efficient Gradient Descent algorithms, including the online version Stochastic Gradient Descent [Bottou, 2010, Duchi et al., 2011, Bottou, 2012]. Stochastic Gradient Descent is applied on a large class of models, including L2 and L1-regularized LR [Carpenter, 2008, Tsuruoka et al., 2009], and often outperforms methods tailored for the particular problem.\nLinear models can be extended in a number of ways to represent non-linear decision surfaces [Keysers et al., 2003, Bishop, 2006, Chang et al., 2010]. The simplest way is mapping the original feature vectors to transformed ones, examples of which are factor decompositions [Borko and Bernick, 1964, Blei et al., 2003], word pair features [Lesk, 1969], and explicit polynomial mappings [Chang et al., 2010]. Generalized linear models apply an implicit link function to transform different types of prediction tasks into linear regression modeling problems, LR using the logit function being one example. Other types of generalized linear models are not necessarily linear models in the sense of linear classifiers and the definition of Equation 2.2. Replacing the multinomial event model in MNB with a Gaussian model would likewise result in non-linear boundaries.\nA second type of extension into non-linear decision boundaries is utilizing information present in individual documents of the collection. K-Nearest Neighbours [Cover and Hart, 1967], Kernel Density Classifiers [Parzen, 1962] and Mixture Models [Li and Yamanishi, 1997] are models that maintain parameters for a set of prototypes for each class, and combine scores for each class from the prototype scores. These models can capture properties of local neighbourhoods in the documents that would be lost with the representation of a single parameter vector. Kernel learning methods [Boser et al., 1992, Joachims, 1998] use feature transformations called kernels into arbitrary spaces that are not explicitly computed, but rather evaluated implicitly by the learning algorithm. This gives kernel learning a great deal of flexibility in learning decision surfaces, but with a computational cost that is not always preferable over a linear kernel maintaining the original feature space [Fan et al., 2008, Yuan et al., 2012].\nA third type of non-linear extension is multi-layer methods, such as tree-based learning [Breiman et al., 1984, Quinlan, 1986], neural networks [Rosenblatt, 1958, Widrow, 1960], and ensemble learning [Breiman, 2001b, Friedman, 2002, Sill et al., 2009]. All of these combine layers of elementary base-learner algorithms, often dividing the original documents and feature vectors into different subsets for the base-learners. Treebased methods combine component learners similar to mixture models, but combine the components using hard decisions based on rules that best segment the data, rather than performing soft combination with fixed mixture weights assigned to each component. Neural networks extend simple base-learners such as LR with hidden layers of learners, with higher modeling flexibility, but also introducing a difficult learning problem that is commonly approached using Stochastic Gradient Descent [Widrow, 1960, Bottou, 2012] combined with heuristics. Ensemble methods combine a set of diverse base-learners to optimize a performance measure, commonly selecting the optimal set of base-learners for the task and learning the best possible combination\n27\nCHAPTER 2. TEXT MINING AND SCALABILITY\n[Sill et al., 2009, Puurula and Bifet, 2012].\nA further extension of linear models in TM is prediction and modeling in tasks that require structured variables, some of which cannot be accurately solved by decomposing the problems into simple linear problems. Examples of such tasks are entity and event detection performed in information extraction, and syntactic tree generation in parsing sentences. However, a majority of the methods used for solving these problems are extensions of basic linear models into structured prediction: Conditional Random Fields [Lafferty et al., 2001] extend LR, Hidden Markov Models [Baum et al., 1970, Kupiec, 1992] extend Naive Bayes, and Max-margin Markov Networks [Taskar et al., 2003] extend SVM. Structured prediction models extending Naive Bayes are described in Chapter 3, and the methods developed in Chapters 4 and 5 can be equally extended into structured modeling."}, {"heading": "2.2.5 Text Mining Architectures as KDD Processes", "text": "Preprocessing data to structured forms and applying ML to solve tasks forms the basic building blocks of TM. Combining these into complex solutions for TM applications often requires integration of the available components into an architecture for the TM application [Feldman and Sanger, 2006, Villalo\u0301n and Calvo, 2013, Maciolek and Dobrowolski, 2013]. The concept of a TM architecture originates from viewing TM as a case of the KDD process [Feldman and Dagan, 1995, Feldman et al., 1997, Ahonen et al., 1997a].\nA basic KDD process is defined as consisting of five steps [Fayyad et al., 1996]: 1) selection, 2) preprocessing, 3) transformation, 4) data mining and 5) interpretation/evaluation. Selection chooses documents and variables of interest for further analysis. Preprocessing consists of modeling noise and missing data. Transformation reduces and transforms the number of considered variables to a form more suited for analysis. Data mining applies algorithms to find interesting patterns. Interpretation/evaluation performs interpretation of the discoveries, possibly visualizing the models or the data using the models. The number of five steps is not fixed, but an example of a possible basic process. All of the steps are interactive, with the user iteratively modifying the steps and cycling through the process to discover more knowledge from the database.\nTM was proposed in its earliest forms as KDD with an additional text-specific preprocessing step [Ahonen et al., 1997a, Do\u0308rre et al., 1999]. This text preprocessing step consisted of preprocessing each document into a feature vector, and filtering of the feature vector to a form more easily processed by standard DM algorithms. It was also suggested that the filtering step was needed for scalability, as the resulting feature vectors would be exceedingly high dimensional for the usual DM algorithms. Text datasets have since grown thousands of times in all relevant dimensions, and architectural decisions have been suggested to maintain scalability [Villalo\u0301n and Calvo, 2013, Maciolek and Dobrowolski, 2013].\nCurrent TM architectures can perform the selection step by applying a search engine [Villalo\u0301n and Calvo, 2013], or applying a web crawler [Maciolek and Dobrowolski, 2013] to retrieve documents related to the TM\n28\nCHAPTER 2. TEXT MINING AND SCALABILITY\napplication. Unlike in typical KDD, the whole collection is therefore not necessarily known or available, but a sample of the vast amount of possible data is gathered in the first step. The preprocessing step can use extensive linguistic processing [Villalo\u0301n and Calvo, 2013], such as tagging words and phrases according to syntactic roles, identifying named entities and events, and categorizing documents into ontologies. The remaining basic steps of transformation, data mining and interpretation/evaluation largely follow the general KDD process, but with some text specific solutions: transformation can be done with topic modeling [Hofmann, 1999, Blei et al., 2003] instead of general matrix factorization methods, data mining is done with algorithms that operate well on high-dimensional sparse data such as Naive Bayes, and visualization is done with tools such as word clouds [S\u030cilic\u0301 and Bas\u030cic\u0301, 2010]. Feldman and Sanger [2006] considers domain knowledge sources as universally important for TM applications and presents extensive use of domain knowledge throughout TM architectures.\nAs an example of a TM architecture for an application, a framework presented in a survey of opinion mining [Pang and Lee, 2008] can be illustrated. This divides the construction of an opinion search engine system into four problems: 1) classification of queries into opinion/non-opinion related queries, 2) finding matching documents and segmenting parts of opinionated content, 3) classifying the opinion related to the query in the relevant parts of documents, and 4) presenting the gathered opinion information with summarization and visualization. Figure 2.6 shows a basic TM architecture for this application, decomposed into ML tasks.\nThe KDD process appears to encompass everything contained in TM, but it can also be too general to describe some TM applications. Many applications of TM involve systems that use the methods of TM such as inverted indices and domain knowledge resources, but do not aim at discovering new types of knowledge, or require an interactive user. Similarly to information extraction, the outcome for TM can be one of known patterns, produced by a fully automatic system. A typical case is text classification applications such as email spam and newspaper topic classification, that produce expected results automatically.\n29\nCHAPTER 2. TEXT MINING AND SCALABILITY"}, {"heading": "2.3 The Scalability Problem", "text": ""}, {"heading": "2.3.1 Scale of Text Data", "text": "The information explosion has brought an overwhelming amount of data available to ordinary people and large institutions alike, much of it in the form of text. A common truism originating from business intelligence research is that 80%-90% of data in corporate databases is in the form of unstructured text [Rajman et al., 1997, Do\u0308rre et al., 1999, Godbole and Roy, 2008]. Certainly a large majority of consumed data is in the form of written text such as newspapers, books, blogs, micro-blogs and chat messages. There is more textual data produced electronically in a single day than any individual person can digest in a lifetime, and the rate of production is increasing rapidly.\nWhat has changed is not only the scale of data, but its availability and accessability. For example, the largest library to date is the The British Library, containing 170 million items, closely followed by the Library of Congress with 152 million items10. Although considerable in scope, these repositories of data are not so readily accessed as databases existing digitally. The information explosion is not only making vastly larger amounts of data available, but making them rapidly accessible through technologies such as IR and TM.\nThe available text data is continuously expanding and of vast scale in several ways:\nNumber of Documents. The number of documents in many datasets and streams is measured in millions,\nand in some cases billions. The online encyclopedia Wikipedia has 4.5 million articles in English as of 201411. Google Books had digitized 30 million books by 201312. The micro-blog provider Twitter announced in 2013 that its 200 million users were sending 400 million tweets per day13, more than the global SMS mobile text message traffic combined. The popular social messaging app developer WhatsApp14 announced in 2014 that its 430 million users send 50 billion messages per day. The scale of data is such that the nascent field of stream mining has emerged as a possible solution, because in many cases merely storing all this data is not practical or even feasible.\nStructural Metadata. Aside from the sheer number of documents available in databases and streams, text\ndata in most cases comes with implicit and explicit metadata. Implicit metadata is unstructured information such as topics, sentiment and named entities that can be discovered using text mining methods and incorporated into the document. Explicit metadata is information such as document hierarchy categorization, link data and various forms of tags that are attached to the document. For example, a Wikipedia article has category labels and different types of external and internal links. The internal links alone can point to any of the millions of documents in Wikipedia, whereas the number of categories\n10http://en.wikipedia.org/wiki/List of largest libraries 11http://en.wikipedia.org/wiki/Wikipedia:Size of Wikipedia 12http://en.wikipedia.org/wiki/Google Books 13https://blog.twitter.com/2013/celebrating-twitter7 14http://techcrunch.com/2014/01/20/whatsapp-dld/\n30\nCHAPTER 2. TEXT MINING AND SCALABILITY\nis close to half a million in the English Wikipedia. Various types of metadata are practically always present with text data, and the dimensionality of each type can reach millions or more.\nRepresentation Dimensionality and Sparsity. Once extracted from documents, unstructured text needs\nto be represented with a structured representation such as the word vector for most types of processing. With most useful structured representations, the dimensionality of the representation grows with the number of documents. Word vectors can grow to millions of words. At the same time, most useful representations are inherently sparse: while word vectors grow to millions, each individual document typically contains only some tens of unique words and the corresponding word vector is almost entirely empty.\nVast datasets have become common in TM research as well. Google N-grams15 released in 2006 contains 5-gram models estimated from 1 trillion words of web text [Brants et al., 2007], and has been widely used for a great variety of TM tasks. The Annotated Gigaword16 is a collection of over 4 billion words of English text news, enriched with sentence segmentation, parse trees, dependency trees, named entities and in-document coreference chains [Napoles et al., 2012]. Less annotated gigaword corpora have been produced for Arabic, Mandarin, Spanish and French. The 1B Word Language Modeling Benchmark17 started at the end of 2013 is a freely available billion word dataset for comparing progress in language modeling [Chelba et al., 2013]. Text classification has started to tackle online ontology classifications such as Wikipedia article categorization18, where the number of categories reaches hundreds of thousands, and combinations of categories reach millions [Puurula and Bifet, 2012]. Classification and retrieval tasks for web ontologies can have hundreds of millions of documents, accessed via cloud-based storage systems19 [Gross et al., 2013]."}, {"heading": "2.3.2 Views on Scalability", "text": "By definition TM seeks methods that discover new information by examining large quantities of data, as opposed to small-scale analysis that can be done by trained human specialists. There are three ways to view scalability in TM. Scalability can as an avoidable challenge, as a beneficial factor in building TM systems, or as necessary for many TM tasks. This thesis argues that scalability is both beneficial and necessary. These three views are discussed next.\nScalability can be seen be seen as something to be avoided in the simplest way possible. It affects both system-level architectural and component-level algorithmic design. At the component level, most methods that originate in the related technical fields have problems adapting to the TM domain. IR has mostly worked\n15http://catalog.ldc.upenn.edu/LDC2006T13 16http://catalog.ldc.upenn.edu/LDC2012T21 17http://code.google.com/p/1-billion-word-language-modeling-benchmark/ 18http://lshtc.iit.demokritos.gr 19http://trec-kba.org/\n31\nCHAPTER 2. TEXT MINING AND SCALABILITY\nwith scalable methods for accessing text, but most methods originating within NLP, ML and DM are less scalable.\nAlthough text is the natural domain of NLP, most NLP methods have been designed on much smaller corpora and emphasize theory, not practical large scale processing. Some methods such as shallow parsers and taggers can operate with linear complexity, but methods such as deep parsers and discriminative models are mostly non-linear and scale poorly to vast numbers of documents [Lafferty et al., 2001, Blei et al., 2003, Collobert et al., 2011]. Non-NLP ML methods have mostly been defined for tasks on much lower dimensions. SVM excel in text classification effectiveness, and are scalable in both documents and features, but less so in the number of classes. Instance-based learning methods such as K-nearest Neighbours scale well to any amount of training data and classes, but are not generally competitive in effectiveness with very sparse high-dimensional vectors, and can become inefficient in inference with vast numbers of training documents. Decision trees in general do not scale to large numbers of classes or documents with high effectiveness. In brief, the majority of ML and DM methods are ill-suited to the scalability required in the text domain.\nSome of the problems with unscalable components can be corrected by well designed system-level architectures. If a simple directional processing pipeline is used, the scalability of the TM system equals the scalability of the weakest component in the pipeline. Good decisions at the system level can reduce the bottlenecks in processing. Architectural design solutions can improve scalability, but designing better architectural solutions can be much harder than using better components. This is evidenced in web-scale search engines, where answers to many fundamental questions in architectural design are still largely unknown, very difficult to exhaustively answer, and subject to change as the web evolves [Cambazoglu and Baeza-Yates, 2013, Asadi and Lin, 2013]. Before embarking on the difficult task of developing techniques for scalable TM, it is therefore necessary to ask whether scalability is warranted, or whether TM should be confined to smaller problems using less scalable tools from the related technical fields. Certainly for most practitioners experienced in the related fields, the obvious solution for TM would be to work on smaller problems with well-understood and widely adopted tools, coupled with data and feature selection, and the affordable large-scale parallelization enabled by cloud computing.\nScalability can also be seen as desirable. NLP has seen an increasing appreciation of scalable processing methods in the last decade. Motivated by the success of large-scale language models used in speech recognition and machine translation [Buck et al., 2014], the usefulness of large datasets for other NLP tasks has become widely acknowledged in a variety of supervised tasks, including disambiguation [Banko and Brill, 2001], parsing [Pitler et al., 2010], spelling correction [Bergsma et al., 2010], segmentation [Huang et al., 2010] and punctuation recovery [Lui and Wang, 2013]. A general observation has been that effectiveness in tasks improves log-linearly with the amount of data [Pitler et al., 2010], so that each multiplication of the amount of data produces a constant relative improvement. This is in line with the improvement observed in large scale n-gram language models; there seems to be no limit to how much improvement comes from more data, as long as the models have sufficient complexity. For example, bigrams seem to saturate practically at some\n32\nCHAPTER 2. TEXT MINING AND SCALABILITY\nhundreds of millions of words, whereas trigrams should saturate at some billions of words [Rosenfeld, 2000]. Higher-order n-grams and other sufficiently complex models can learn from text data without saturating in the same manner [Brants et al., 2007, Huang et al., 2010, Buck et al., 2014].\nMany TM tasks have small amounts of labeled data available compared to large quantities of unlabeled training data. Text classification tasks often require this type of semi-supervised learning. Both estimation based on the EM-algorithm [Nigam et al., 2006] and semi-supervised estimation [Su et al., 2011] enable the utilization of unlabeled data. Large-scale data can also be used in unsupervised learning for tasks that do not require label information, such as the numerous uses of n-gram model counts [Lapata and Keller, 2005]. Alternatively, large-scale unlabeled datasets can be automatically labeled, producing reusable machine-annotated resources for supervised tasks [Napoles et al., 2012].\nThe most recent view is that scalability is necessary for TM [R\u030ceh\u030aur\u030cek, 2011]. Traditional IR methods have been developed to scale as much data as possible, since IR systems are intended to retrieve as many relevant documents as possible. Unlike in NLP, scalability has always been a requirement of IR. Similarly, some recent text classification tasks require classification into potentially millions of classes, as in the case of Wikipedia categorization [Puurula, 2012a]. Tasks such as these require scaling to large dimensions, and compromising dimensionality would reduce effectiveness of the methods considerably. It can be further argued that some technologies, such as those utilizing machine translation, only became popular with the arrival of n-gram models trained on trillions of words [Brants et al., 2007].\nThis view of scalability as necessary stems from the definition of TM. In contrast to text analysis, TM connotes a process of sifting through large quantities of less valuable material to find material of interest. Without data of vast scale, TM reduces to a mere intersection of NLP and ML. Many new text processing tasks are starting to require processing at vast scale. The resulting processing systems are best called TM systems, as they incorporate methods from a variety of disciplines, and do not fit neatly into any of the related technical fields."}, {"heading": "2.3.3 Approaches to Scalable Text Mining", "text": "A common solution to TM is to treat it as any large-scale processing problem: using general data processing components for processing, and managing scalability with architectural decisions. This can be called a generic approach to scalable TM, and it has a couple of benefits. Components used in other types of data processing can be used in the text domain, and scalability can likewise be managed using well-known solutions. Components of this type include models such as Decision Trees and SVM, while architectural solutions include parallelization solutions such as Map-Reduce. Using generic solutions for both the components and architecture means that the solutions are to a large degree better understood, and both the required software and expertise is more widely available.\n33\nCHAPTER 2. TEXT MINING AND SCALABILITY\nA number of generic techniques for scalability can be used at the architectural level:\nSelection can be applied to documents, features, classes in other possible dimensions. For the example of\nfeature selection, selection removes from the data the least important features under some measure of importance [Lewis, 1992, Yang and Pedersen, 1997, Forman, 2003, Elsayed et al., 2008]. Documents can be selected by removing documents classified as spam or noise from the collection [Chekuri et al., 1997, Manning et al., 2008].\nTransformation can be likewise applied to variables to reduce dimensionality, as well as to the processing\nproblems themselves. For example, a multi-label classification problem can be approximated by transforming it into a sequence of binary-label classification problems, a multiclass classification problem, or a label ranking problem [Tsoumakas et al., 2010]. Features can be combined or transformed using topic modeling and matrix decomposition methods [Hofmann, 1999].\nPrecomputing reduces inference time processing by computing and storing as much as possible of the\nprocessing offline [Zhang and Poole, 1994, Mohri, 2004].\nCaching stores and reuses solutions computed earlier [Skobeltsyn et al., 2008]. Multiple caches can be used\nto store different types of subsolutions.\nPruning and regularization can be imposed on most generic modeling components, reducing the number\nof stored parameters and required computation [Zhang and Poole, 1994, Skobeltsyn et al., 2008].\nStreaming processes data as a sequence of instances, rather than storing a full dataset in memory as a\nsingle batch. Mini-batch processing stores smaller parts of the data, trading memory requirements for model performance. Many algorithms have online versions for stream training, such as Stochastic Gradient Descent [Bottou, 2010], Online EM [Liang and Klein, 2009], and approximations for topic models [R\u030ceh\u030aur\u030cek, 2011].\nParallelization solves computing problems by using several processors simultaneously to solve sub-problems.\nTwo basic types of parallelization should be distinguished: parallel computing and concurrent computing. Parallel computing solves problems as an array of smaller identical subproblems, and combines the results. Concurrent computing solves problems as a pipeline, forwarding data from one component to another continuously. Multi-core processors, computing clusters, grid computing and cloud computing are possible configurations for utilizing parallelization.\nRecent general scientific literature commonly equates scalability with parallelization [Hill, 1990, Kargupta et al., 1997, Dean and Barroso, 2013]. This has become especially prominent due to the popularity of Big Data and cloud computing as topics over the last decade. Parallelization gives reductions in processing time, up to the number of processors used. For parallel computing, an upper bound of this reduction is known as Amdahl\u2019s law [Amdahl, 1967], stating that the maximum speedup gained from parallel computing is dependent on how much of the problem can be solved by parallel computations. The statement can be extended\n34\nCHAPTER 2. TEXT MINING AND SCALABILITY\nfor concurrent computing. Interestingly, Amdahl\u2019s 1967 paper is one of the most cited publications in parallel computing, but is very critical of parallel computing. The original reason for Amdahl\u2019s law was to present a rigid proof that parallel computing is not a panacea for scalability [Amdahl, 1967].\nMuch of the TM literature advocates general architectural methods for scalability, rather than ones taking the properties of text data into account. In particular, parallelization and cloud computing are considered by many to be solutions to TM: Baumgartner et al. [2009], Chard et al. [2011], R\u030ceh\u030aur\u030cek [2011] and Tablan et al. [2013] present TM systems relying exclusively on cloud computing for scalability, Dunlavy et al. [2010] presents a system relying on parallelization and feature selection, while Villalo\u0301n and Calvo [2013] presents a TM software library advocating parallelization, but implicitly using concurrency and document selection. For most TM tasks, selection is implicitly used, as document selection is commonly considered one of the main stages in TM [Ahonen et al., 1997b], and in data mining generally [Fayyad et al., 1996]. Agichtein [2005] reviews solutions that have been used in scalable information extraction, identifying four main approaches for scalability: 1) scanning the collection using rules, 2) selecting documents using search engines, 3) using customized indexes and 4) distributed processing. The first two are cases of selection and the last one refers to parallelization. The use of customized indexes is more specific to information extraction, and is one type of non-generic solution.\nThe downside of this generic approach to scalability is that it does not always scale well to TM, and simplifying a problem to suit generic components cannot always be done without unacceptable approximations. As an example, consider categorization of large-scale multi-label ontologies. Tsoumakas et al. [2013] shows a leading solution to BioASQ20 biomedical article categorization of documents into combinations of 26k possible labels. A majority voting ensemble of four SVMs was trained for ranking the labels, and one for predicting the number of labels per document. The scalability was managed by document selection from 10.8M to 3.9M documents, feature selection by removing words occurring less than six times in the collection, and parallelization with a cluster of 40 processors. Training the SVM classifiers took one and a half days on the 40 processors. While this solution is still state-of-the-art for 26k labels, real-world ontologies are becoming larger in all relevant dimensions. A similar solution based on regularized hierarchical SVMs [Gopal and Yang, 2013] was proposed for the LSHTC421 datasets for Wikipedia categorization with 325k labels. Using a few approximations in the optimization, this resulted in training times of 37 hours with 512 processors, multiplying estimation times from a smaller Wikipedia dataset one tenth in size by a factor of 25. The real-world datasets and streams mentioned earlier in this chapter are on a totally different scale. Clearly, there is a limit to how much can be accomplished by generic data processing techniques. More fundamental innovations for text processing must be applied, if the emerging vast datasets are to be fully utilized.\nA second approach to scalability in TM is to use algorithms intended to scale well with text data by taking the properties of text into account. This can be called the specialized approach to TM. The advantage\n20http://bioasq.org 21http://lshtc.iit.demokritos.gr/\n35\nCHAPTER 2. TEXT MINING AND SCALABILITY\nof using specialized domain expertise is that highly developed solutions for the text domain can be utilized across different types of processing tasks. For example, statistical n-gram language models offer practical and efficient solutions across a variety of text processing tasks.\nTo some extent many of the applied generic algorithms have already taken properties of text data into account, as the development of the applied algorithms and TM are intertwined in many places. For example, SVMs were proposed for text classification explicitly due to features of word count vectors: high dimensionality, sparsity, high proportion of irrelevant to relevant features, and linear separability in the text classification datasets available at that time [Joachims, 1998]. Likewise, the first use of the Map-Reduce parallelization framework was the computation of web-scale n-grams used at Google [Dean and Ghemawat, 2008]. The vector space model was developed in the context of word vectors, and afterwards applied to other tasks [Salton, 1963]. Many of the related technical fields were developed in the context of processing text data, IR and IE in particular.\nTaking the properties of text data into account does not warrant scalable processing. NLP has dealt exclusively with text data, but much of the research has focused on finding computational models that work with small sets of text data, while possibly testing associated linguistic theories, such as formal frameworks for describing the grammar of natural languages. The emergence of vast text datasets has made scalable processing methods more common, in particular shallow parsing methods in combination with machine learning [Neumann and Piskorski, 2002, Lui and Wang, 2013].\nSpecialized solutions for scalable TM can be broadly categorized into shallow processing, hierarchical in-\nference and inverted indices, explained next in detail.\nShallow processing refers to NLP methods that attempt to approximate theoretically grounded deep grammatical processing methods. The typical case is syntactic parsing, where the complexity of the correct model is still unknown, but is at least context free [Chomsky, 1956]. Finite state models provide an approximation to parsing with lower complexity [Koskenniemi, 1990]. Many processing tasks, such as part of speech tagging, phrase chunking and named entity recognition, can be done with low-complexity algorithms such as Hidden Markov Models [Church, 1988]. From a NLP perspective, shallow processing can be seen as an extension of text normalization [Neumann and Piskorski, 2002], whereas from the IE perspective it can be seen as a form of data enrichment [Stajner et al., 2010]. Shallow processing components themselves do not necessarily need to scale in training, since they can be estimated from smaller amounts of data [Tandon and de Melo, 2010, Collobert et al., 2011, Lui and Wang, 2013]. Some linguistic theories argue that language operates on gradually enriched semantic representations [Neumann and Piskorski, 2002, Daum et al., 2003, Sagae et al., 2007]. By constraining deeper processing methods, shallow processing both improves efficiency and effectiveness of further processing stages.\nHierarchical modeling and inference can be applied in numerous ways to utilize the rich structure of text\n36\nCHAPTER 2. TEXT MINING AND SCALABILITY\ncollections. As discussed earlier, text data is embedded with multiple types of implicit and explicit structure. Hierarchical text classification organizes classes into a hierarchy, and classifies documents by traversing the hierarchy from the root towards the leaf classes. This reduces the number of classes that need to be considered to the logarithm of the number of classes [Koller and Sahami, 1997, Tsoumakas et al., 2010, Gopal and Yang, 2013]. Hierarchical clustering can be used for clustering documents, reducing the complexity of clustering to comparisons within each subcluster in the hierarchy [McCallum et al., 2000]. Hierarchical ranking in text retrieval uses a cascade of inverted indices with increasing degrees of granularity [Wang et al., 2011]. Scalability in parsing and entity recognition can be improved by using hierarchical representations [Petrov and Klein, 2007, Kiddon and Domingos, 2010, Singh et al., 2011]. All of these cases of text processing use the same idea of coarse-to-fine processing, improving modeling effectiveness and scalability by organizing variables into a hierarchy. Graph-structured variables can be approximated using hierarchies, and unsupervised modeling can discover many types of variables implicitly present in text documents that can be used for hierarchical modeling.\nInverted indices have constituted the main data structure for efficient text retrieval for several decades [Zobel and Moffat, 2006]. A more recent development is the use of inverted indices in IE, starting from an IBM TM system called WebFountain [Gruhl et al., 2004]. Indices can be enriched with a variety of information obtained through shallow processing of the documents. Words can be indexed after enrichment with person identification, location [Gruhl et al., 2004], part-of-speech and dependency information [Cafarella et al., Cafarella and Etzioni, 2005], relation tuples [Banko et al., 2007], entity types, predicate-argument relationships, semantic frames, frame roles, frame-denoting elements, events, attributes and relations [Hickl et al., 2007]. Virtually any type of information can be included in an enriched inverted index [Gruhl et al., 2004, Agichtein, 2005]. This increases the index size, but lowers the complexity of retrieving documents matching an indexed annotation. For example, looking up documents that refer to a certain entity can be done simply by going through a postings list that contains documents classified to refer to that entity. This lowers the complexity of some types of processing, and has been considered the most promising method for making information extraction scalable [Agichtein, 2005].\nThese three strategies improve scalability of text processing by utilizing properties of text data. Shallow processing utilizes the structural nature of implicit variables in text data. Hierarchical processing utilizes hierarchical representations of both implicit and explicit variables associated with text. Enriched inverted indices and sparse processing utilize the sparsity of common representations of text, such as word vectors. Chapter 5 shows how hierarchical processing and enriched inverted indices can be combined to perform scalable probabilistic inference for a variety of TM tasks.\n37\nChapter 3\nMultinomial Naive Bayes for Text Mining\nThis chapter gives an overview of the Multinomial Naive Bayes (MNB) model for text mining, and its generative and graphical model extensions. A basic broad definition of MNB is first given. Generative models related to MNB are described, including mixture models, topic models, n-gram models, Hidden Markov Models, and Dynamic Bayes Networks. The notation of graphical models is introduced, and the connection of directed generative graphical models to the more general factor graphs is discussed. Dynamic programming algorithms for operating with directed graphical models are described, including Viterbi, forward, and expectation maximization."}, {"heading": "3.1 Multinomial Naive Bayes", "text": ""}, {"heading": "3.1.1 Introduction", "text": "Multinomial Naive Bayes (MNB) is a probabilistic model of count data commonly used for various tasks in text mining. MNB originates in text classification research, but the model goes under different names in fields related to text mining. In text clustering, the equivalent model is called a generative multinomial model [Zhong and Ghosh, 2005]. In information retrieval, a special case of MNB is the query likelihood language model (LM) [Kalt, 1996, Hiemstra and Kraaij, 1998, Ponte and Croft, 1998, Zhai and Lafferty, 2001a]. Many other methods can be related to MNB, either as extensions or modifications. MNB and related methods can be said to form one of the core statistical models of text mining.\nThe MNB model originates from the Naive Bayes (NB) model for text classification, which is a Bayes model that simplifies model estimation by making strong independence assumptions on features. NB was suggested in a 1961 paper by Maron [Maron, 1961]. This paper was pioneering and even visionary in multiple ways. The paper introduced the idea of automatic text classification, the Bernoulli NB model for text classification, a correction to the zero-frequency problem of NB models, evaluation using held-out test data, and used modern terminology as well as vector notation for describing the model. Maron\u2019s work received limited continuation until the early 90s, when text classification started to become a major topic in the machine learning (ML)\n38\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nand data mining fields. Text classification and models related to NB were extensively researched for a decade, until learned linear classifiers such as Support Vector Machines (SVM) became popular due to their superior accuracy [Joachims, 1998].\nBy the end of the 90s, MNB was identified to be considerably better than Bernoulli NB for most text classification uses [Lewis, 1998, McCallum and Nigam, 1998, Rennie et al., 2003], but generally less accurate than discriminative classifiers such as Logistic Regression (LR) and SVMs [Joachims, 1998, Rennie et al., 2003]. It was also noted that the strong modeling assumptions in MNB reduced performance, and modifying MNB could bring its performance closer to the discriminative classifiers [Rennie et al., 2003, Schneider, 2005, Frank and Bouckaert, 2006]. During the next decade, MNB and related methods spread to various other text mining tasks, in many cases becoming baseline methods, while research interest in generative models for text started to diversify into extensions such as mixture models [Li and Yamanishi, 1997, Monti and Cooper, 1999, Toutanova et al., 2001, Novovicova and Malik, 2003] and topic models [Hofmann, 1999, Blei et al., 2003].\nMNB and generative models of text have remained popular due to several advantages, specifically:\nSimplicity NB models are very simple to describe and implement. They are among the first models taught\nto students in ML, prior to learned linear models such as SVM and LR. Simplicity also means that the estimated model parameters can be intuitively understood. More complex ensemble methods can be used to combine a set of NB classifiers, providing a high performance solution that is not a black-box [Elkan, 1997].\nProbabilistic Formulation The probabilistic formulation of MNB confers several advantages. The pa-\nrameters and posterior probabilities of MNB can be easily visualized and interpreted. Text mining is sometimes used as a component for general data mining and statistical analysis. Probabilistic models can be better integrated into complex modeling than non-probabilistic components.\nVersatility Text mining applications vary greatly, and most text mining tools are specialized into solving\nparticular problems. MNB can be directly used in text classification, ranking and clustering, among other uses. Graphical model extensions such as HMMs, mixture models, and conditional N-gram models can be used for modeling structured data. Specialized extensions can be made for handling different types of structured data, such as multi-label outputs [McCallum, 1999] and multi-field documents [Wang et al., 2010b].\nRobustness The variability in text mining applications causes different types of modeling challenges, such\nas limited training data and mismatch between training and test datasets. Despite making strong assumptions, NB models seem to perform well empirically [Domingos and Pazzani, 1997] and MNB is commonly used as a baseline model in text mining applications.\nScalability The amount of available data is growing at an exponential rate. Text datasets as word vectors\nare high-dimensional in the number of documents, words and labels. MNB has a simple form that results in linear time and space complexity for both estimation and inference. Unlike many methods for\n39\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\ntext mining, NB models scale linearly in the number of words, documents and labels. This means that MNB can be used on vast datasets, where anything exceeding linear scaling is intractable.\nOn-line training The vast text datasets that have become available can no longer be stored in the memory\nof a single computer. This is causing a shift from batch processing to on-line processing, where the data is not kept in memory, but processed as a stream. Many data streams are time-ordered, so that older documents are less useful for building predictive models. Examples of data streams are news stories, microblog messages and other forms of media used for rapid communication. Models working on data streams should support on-line training and down-weighting of older data points. These are trivially implemented for NB models.\nParallelization Large-scale data processing can be tackled with parallelization across multiple processors.\nParallel computing frameworks such as MapReduce and Hadoop have become popular solutions for dealing with scalability. One of the main original uses for the MapReduce framework was training large-scale language models by parallelized count accumulation and combination [Dean and Ghemawat, 2008]. Other types of NB models can be parallelized in the same fashion in both estimation and inference.\nThe disadvantage of MNB is the effectiveness compared to more complex models. Even in earlier text classification research, NB was used as a \u201cstraw man\u201d baseline for comparing more complex models [Domingos and Pazzani, 1997, Lewis, 1998, Rennie et al., 2003]. The introduction of SVM [Joachims, 1998] brought about a gradual decline of interest in MNB and other generative models for text classification, following a general trend in ML towards discriminative classifiers. Currently, discriminative classifiers such as SVMs, LR and Maximum Entropy models are considered the most effective solution for text classification uses such as spam classification, sentiment analysis and document categorization."}, {"heading": "3.1.2 Definition", "text": "The MNB model considers word count vectors w as being generated by underlying multinomial distributions of words n associated with label variables l. Usually the instances of text are documents and the label variables are classes, document identifiers, or clusters, depending on the task. The label-dependent multinomial distributions pl(n) are called conditional distributions, and are combined with a categorical prior distribution p(l) of the label variables. A common intuitive explanation is a \u201cgenerative process\u201d, where documents are generated by first sampling a label from the categorical distribution, and then sampling words from the multinomial associated with the label.\nA generative model in ML terminology is a model of the joint distribution p(w, l) of input and output variables [Bishop, 2006, Klinger and Tomanek, 2007, Sutton and McCallum, 2007]. For MNB the inputs are word vectors w and the outputs are document labels l. A generative Bayes model factorizes the joint distribution as p(w, l|\u03b8) = pl(w|\u03bb)p(l|\u03c0), so that the parameters \u03b8 are assumed to factorize into parameters \u03bb for the conditionals pl(w|\u03bb) and \u03c0 for the prior p(l|\u03c0). Posterior inference can be done by applying Bayes theorem: p(l|w) = p(w|l)p(l)/p(w), where p(l|w) is called the posterior and p(w) = \u2211l p(w, l) the marginal.\n40\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nFor ranking and classification, the marginal can be omitted and the inference done by maximizing the joint p(w, l) instead. In classification, the optimization becomes argmaxl pl(w)p(l). Regression can be performed with continuous variables for labels [Frank et al., 1998].\nA problem with Bayes classifiers is estimating the dependencies of the input variables or features n for computing pl(w). By making independence assumptions, computing pl(w) can be simplified. A common assumption is \u201cnaive independence\u2019, which assumes that the conditional probabilities pl(n,wn) are independent,\nthat is, pl(w) = \u220f n pl(n,wn). Models of this type are commonly called NB models. The parameterization of pl(w) can take many forms. A common type of NB model is the multivariate Bernoulli NB [Maron, 1961], where the counts wn are restricted to binary values \u2200n : wn \u2208 {0, 1} and each class conditional probability pl(n,wn) is modeled by a Bernoulli distribution. The Bernoulli distribution models biased \u201ccoin-flip\u201d outcomes of a variable by using a single parameter describing how biased the coin flips are. For example, with parameter \u03bbln = log(0.1), the probability of the word n occurring in a document for label l would be 0.1. The Bernoulli parameters can be estimated simply by counting the training documents w(i) for label l with the word n, and dividing by the number of documents for that label.\nThe multivariate Bernoulli model for NB [Maron, 1961, Robertson and Jones, 1976, Domingos and Pazzani, 1997, Lewis, 1998, McCallum and Nigam, 1998, Craven et al., 2000] is also known as the Binary Independence Model and Bernoulli NB, and was the first type of NB model suggested for text mining [Maron, 1961]. Other possible models for pl(w) in text mining include Gaussian [Domingos and Pazzani, 1997], multinomial [Lewis, 1998, McCallum and Nigam, 1998, Craven et al., 2000, Rennie et al., 2003, Schneider, 2005, Frank and Bouckaert, 2006, Puurula, 2012b], Poisson [Church and Gale, 1995, Kim et al., 2006, Li and Zha, 2006], Von Mises-Fisher [Banerjee et al., 2005], asymmetric distributions [Bennett, 2003], kernel densities [Ciarelli et al., 2009], and finite mixtures of distributions [Church and Gale, 1995, Banerjee et al., 2005, Li and Zha, 2006].\nFor most text mining uses the multivariate Bernoulli model has been replaced by the multinomial model of text and its extensions. A multinomial distribution can be seen as a generalization of the Bernoulli distribution into multiple coin-flip outcomes and multiple coin tosses, much like multiple rolls of a biased dice\nwith N sides. A multinomial models the sums of n possible outcomes from J = \u2211\nnwn dice rolls. The\nexact order of the dice rolls in a sequence of J rolls wj is not needed for counting the sums of outcomes wn. The probability mass function for a label-conditional multinomial distribution of word vectors becomes\npl(w) = Z(w) \u220f n pl(n) wn . The normalizer Z(w) = ( \u2211 n wn)!\u220f n wn! takes into account that word vectors can correspond to a number of different word sequences. As it is constant for a given word vector, it can be omitted in most uses. A common special case of the multinomial is the binomial distribution N = 2. Another special\ncase of note is the categorical distribution \u2211\nnwn = 1.\nUsing multinomials for Bayes model conditional distributions, we get the joint probability distribution for\n41\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nMNB:\np(w, l) =p(l)pl(w) =p(l)Z(w) \u220f n pl(w) wn , (3.1)\nwhere p(l) and pl(w) are parameterized with a categorical and a multinomial, respectively.\nSince document lengths J vary, models are defined over all possible lengths, and the multinomials for different lengths have shared parameters. The shared parameters are \u201ctied\u201d, since they are constrained to be equal regardless of length. In addition, a distribution such as Poisson must be assumed for generating different document lengths, so that the model can generate the joint distribution over documents of all lengths. The length factor has no practical effect in most applications, and is omitted for posterior inference uses such as clustering, ranking and classification. Therefore both the length factor and the multinomial parameter tying are commonly omitted in the MNB literature [McCallum and Nigam, 1998].\nA problem with varying document lengths is that the posterior probabilities for MNB models get increasingly close to either 0 or 1 as the document length increases, since the conditional probability pl(w) is computed by multiplying the N probabilities pl(w) independently [Frank et al., 1998, Monti and Cooper, 1999, Bennett, 2000, Craven et al., 2000]. This scale distortion of the posteriors does not affect classification, ranking or hard clustering, since the rank-order of probabilities for different labels is preserved. For uses such as soft clustering the posterior probabilities can be improved with feature transforms [Pavlov et al., 2004], feature selection [Rigouste et al., 2007, Pinto et al., 2007], and using KL-divergence instead of posterior probabilities to correct for the document lengths [Craven et al., 2000, Schneider, 2005, Pinto et al., 2007]. Nevertheless, for some applications the indirectly estimated posterior probabilities from MNB can be insufficient, and a model directly optimizing the posterior probabilities is preferred, such as LR."}, {"heading": "3.1.3 Estimation", "text": "Estimation of MNB parameters is commonly done by applying maximum likelihood estimation [McCallum, 1999, Rennie, 2001, Juan and Ney, 2002, Vilar et al., 2004, Madsen et al., 2005, Frank and Bouckaert, 2006]. Due to the data sparsity problem with text data, various smoothing methods are commonly used to correct maximum likelihood parameter estimates. In some cases the smoothed estimation is presented as maximum a posteriori estimation [Rennie, 2001, Schneider, 2005, Smucker and Allan, 2007]. Despite the name \u201cBayes\u201d, NB models are commonly not Bayesian in the sense of Bayesian estimation, where a distribution over parameters is maintained instead of a point estimate of parameters. A fully Bayesian version of MNB has been proposed, but shown to be less suitable than maximum likelihood point estimates [Rennie, 2001]. The most common type of estimation is supervised estimation, where a training dataset D consists of I pairs D(i) = (w(i), l(i)) of word vectors and labels, assumed to be independent and identically distributed (IID).\n42\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nThe unsmoothed maximum likelihood estimation approach for the supervised case is described next.\nThe maximum likelihood method of statistical estimation selects a vector of parameters for a model that maximizes the likelihood of the parameters given the data L(\u03b8|D). This equals the probability of the data given the parameters p(D|\u03b8). A conceptual difference between these two is that likelihood is a function of parameters for given training data, whereas probability assumes a model with parameters and can refer to both seen and future data. The MNB likelihood function can be derived:\nL(\u03b8|D) = p(D|\u03b8) = \u220f i p(l(i)|\u03b8)pl(i)(w(i)|\u03b8) IID data assumption\n= \u220f i p(l(i)|\u03bb)pl(i)(w(i)|\u03c0) Bayes model\n= \u220f i p(l(i)|\u03c0) \u220f n pl(i)(n|\u03bb)w (i) n ( \u2211 nw (i) n )!\u220f nw (i) n !\nMultinomial conditional (3.2)\nMaximizing the likelihood can be simplified by noting that the log of the likelihood has the same maximum, but is easier to handle computationally. The MNB log-likelihood decomposes into terms that can be separately optimized. The maximization of the log-likelihood function can be derived:\nargmax \u03b8 (L(\u03b8|D)) = argmax \u03b8 (logL(\u03b8|D)) = argmax \u03b8 (log p(D|\u03b8))\n= argmax (\u03bb,\u03c0) (log( \u220f i p(l(i)|\u03c0) \u220f n pl(i)(n|\u03bb)w (i) n ( \u2211 nw (i) n )!\u220f nw (i) n ! ))\n= argmax (\u03bb,\u03c0) ( \u2211 i (log p(l(i)|\u03c0) + \u2211 n w(i)n log pl(i)(n|\u03bb) + log( ( \u2211 nw (i) n )!\u220f nw (i) n ! ))) = argmax (\u03bb,\u03c0) ( \u2211 i log p(l(i)|\u03c0) + \u2211 i \u2211 n w(i)n log pl(i)(n|\u03bb)) = argmax (\u03bb,\u03c0) ( \u2211 l C(l) log p(l|\u03c0) + \u2211 l \u2211 n C(l, n) log pl(n|\u03bb)), (3.3)\nwhere C(l) and C(l, n) refer to the accumulated counts of the variables in the training data.\nOptimizing the parameters for the prior and conditionals can now be done separately, and for both cases this consists of choosing the vector of parameters that most likely generated the accumulated vector of counts. The maximum likelihood solution to this estimation of a categorical distribution p(l|\u03c0) is the relative frequency estimate C(l)/ \u2211 l\u2032 C(l \u2032): \u03c0l = log(C(l)/ \u2211 l\u2032 C(l \u2032)) and \u03bbln = log(C(l, n)/ \u2211 n\u2032 C(l, n \u2032)). This is a standard result in statistics and commonly proven using the method of Lagrange multipliers [Bilmes, 1998, Juan and Ney, 2002].\nIn practice, estimating the MNB model consists of accumulating the counts in training data and normal-\n43\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nizing by the sums of counts, with time complexity O(IN) and space complexity O(LN). Taking sparsity into account, the space complexity of estimation is reduced to O(L + \u2211\nl \u2211 n:exp(\u03bbln)>0 1) and time complexity to\nO( \u2211\ni |w(i)|0). The parameters can be represented efficiently using sparse matrix representations. A hash table with (l, n) pairs as keys and parameters as values is one popular choice, with amortized constant time complexities for updating the counts. Another common choice is sparse vectors of word counts and periodic merging of the accumulated vectors with list merge operations. With large-scale datasets this can be done using the map-reduce framework [Dean and Ghemawat, 2008].\nGiven labeled training data, maximum likelihood estimation of parameters for MNB can be done 1) exactly, 2) with a closed form solution, 3) as online learning, and 4) in linear time complexity in terms of features, documents and classes. These four advantages make MNB highly useful in practical applications. The maximum likelihood estimates are exact, so there are no approximations required, and any system using MNB does not have to consider possible errors from approximations. The closed form solution means that the estimates can be computed by applying elementary mathematical operators, such as summation and division. The estimation can be done as online learning from streams of text documents, and the effect of older documents can be removed from the estimates trivially. Lastly, the maximum likelihood estimation has linear complexities in all relevant dimensions. Among the common text mining methods, only Centroid and K-nearest Neighbours classifiers have the same scalability in training.\nExtending MNB estimation to weighted data can be done by weighting the accumulated counts from each document. In some cases documents are softly labeled, by using a distribution of weights over labels instead of a single label. Extending the estimation to soft labeling can be done similarly, by weighting the counts by the label weights. In case of unsupervised and semi-supervised estimation, the expectation maximization (EM) algorithm [Dempster et al., 1977, Bailey and Elkan, 1994, Bilmes, 1998, Zhong and Ghosh, 2005, Nigam et al., 2006, Gupta and Chen, 2011] can be used to estimate parameters. This consists of initialization of parameters followed by EM-iterations of computing the soft labeling p(l|w(i)) (expectation step) and re-estimating the model from the soft labeling (maximization step). Each iteration improves the log-likelihood until a stationary point of the likelihood function is reached. Combining EM with multiple random initializations reduces the probability of not reaching a global optimum of the likelihood. Improvements of EM such as online EM [Liang and Klein, 2009] can be used to estimate parameters from stream data and reduce the required amount of EM-iterations."}, {"heading": "3.2 Generative Models Extending MNB", "text": ""}, {"heading": "3.2.1 Mixture Models", "text": "Mixture modeling techniques are commonly used to extend MNB and multinomial models of text, providing both improved modeling precision and a means for incorporating structure into the models. The use of mix-\n44\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\ntures enables the modeling of multi-modal distributions, with accuracy increasing as a function of the number of added components and the amount of available data to estimate the components.\nA basic type of mixture is the Finite Mixture Model [Pearson, 1894]. This models data as being generated by a normalized linear combination of L component distributions, so that for each component l a weight p(l) and a component-conditional distribution pl() is estimated. The component weights are constrained\n0 \u2264 p(l) \u2264 1 and \u2211l p(l) = 1. For example, a finite mixture of multinomials takes the form: p(w) =\n\u2211 l p(l)Z(w) \u220f n pl(n) wn (3.4)\nBy replacing the component variable with the label variable in the MNB model of Equation 3.1, supervised Bayes models can be viewed as mixture models with known component probabilities p(m|w(i)) for each training document [McCallum and Nigam, 1998, Novovicova and Malik, 2003, Nigam et al., 2006]. Commonly the components are unknown variables which are estimated in training using approximate algorithms such as EM and optimization on held-out data. Depending on the application and the type of mixture modeling, different optimization algorithms and constraints on the parameters can be used to simplify the estimation problem.\nMultinomial models of text can be extended by adding mixtures over both documents and words. Document-\nclustering mixtures treat documents as being generated by a mixture, with each component corresponding to a prototypical document. Word-clustering mixtures treat words similarly, with each component corresponding to a prototypical word. The document-clustering components can be called themes, while word-clustering components can be called topics [Keller and Bengio, 2004]. Combination and extension of these two basic types of mixture for text data result in the various mixture and topic models of text.\nThe earliest proposed document-clustering mixture extension of MNB conditions the label variables on the components [Kontkanen et al., 1996, Monti and Cooper, 1999]. In this use the mixture components cluster the training data into soft partitions:\np(w, l) = \u2211 m p(m)pm(l)Z(w) \u220f n pml(n) wn (3.5)\nReplacing each conditional multinomial pl(w) in MNB by a finite mixture of M multinomial components produces the more common Multinomial Mixture Bayes Model [Monti and Cooper, 1999, Novovicova and Malik, 2003, Nigam et al., 2006]:\np(w, l) = p(l) \u2211 m pl(m)Z(w) \u220f n plm(n) wn (3.6)\n45\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nWord-clustering mixtures are used in topic models and multi-label mixture models [Li and Yamanishi, 1997, Hofmann, 1999, Li and Yamanishi, 2000, Blei et al., 2003, McCallum, 1999, Ueda and Saito, 2002a]. A basic model of this type extends the multinomial:\np(w) = Z(w) \u220f n ( \u2211 m p(m)pm(n)) wn (3.7)\nThe earliest proposed topic model of this form [Li and Yamanishi, 1997] replaced the multinomial in MNB, using hard clustering of words to form shared components pm(n), with separate distributions pl(m). A related model called the Stochastic Topic Model [Li and Yamanishi, 2000] used this type of modeling without label variables, performing inference using the components directly for topic segmentation and analysis. Multilabel classification models use this type of topic model as well, but learn the components from multi-label data. The Multi-label Mixture Model [McCallum, 1999] uses Equation 3.7, but performs inference by greedily adding label components l and estimates pl(m) for each document, using a prior p(c) over the labelsets c = [1, ..., l, ..., L] instead of labels. Parametric Mixture Model [Ueda and Saito, 2002a] is similar, but uses a uniform distributions for pl(m) and the labelset prior p(c). Further multi-label mixture models have built on these two models [Ueda and Saito, 2002b, Kaneda et al., 2004, Sato and Nakagawa, 2007, Wang et al., 2008, Ramage et al., 2009].\nProbabilistic topic models became more widely known with Probabilistic Latent Semantic Analysis [Hof-\nmann, 1999]. This uses a unique label variable for each document, so that the joint probability becomes:\np(w, l) \u221d p(l) \u220f n ( \u2211 m pl(m)pm(n)) wn (3.8)\nAlthough popular, the Probabilistic Latent Semantic Analysis model is not a fully generative model of documents [Blei et al., 2003, Keller and Bengio, 2004], since it does not generate new document variables l. The number of components M needs to be optimized, and L is tied to the number of training set documents. Thus the model is not very scalable and is prone to overfitting [Blei et al., 2003]. To address these issues, a model called Latent Dirichlet Allocation [Blei et al., 2003, Minka and Lafferty, 2002] was proposed by replacing the document variables in Probabilistic Latent Semantic Analysis with a Dirichlet distribution p(\u03c9) of the component weights p(m|\u03c9) = \u03c9m:\np(w) = \u222b p(\u03c9|\u03c4 )Z(w) \u220f n ( \u2211 m p(m|\u03c9)pm(n))wnd\u03c9, (3.9)\nwhere p(\u03c9|\u03c4 ) is modeled by a Dirichlet distribution, given by:\n46\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\np(\u03c9|\u03c4 ) \u221d \u0393( \u2211\nm \u03c4m)\u220f m \u0393(\u03c4m) \u220f m \u03c9\u03c4mm , (3.10)\nwhere \u03c4 are the parameters for the Dirichlet distribution and \u0393 is the gamma function.\nThe integration over possible component weight vectors has no closed form solution, and must be approximated using algorithms such as variational Bayes, Gibbs sampling, and expectation propagation [Minka and Lafferty, 2002, Asuncion et al., 2009]. The Latent Dirichlet Allocation model proved exceptionally popular and turned probabilistic topic modeling into an active field of research [Griffiths and Steyvers, 2004, Blei and Lafferty, 2006, Li and McCallum, 2006, Asuncion et al., 2009, Blei, 2012].\nA substantial literature exists on various models based on Latent Dirichlet Allocation. One conceptually useful model is the Theme Topic Mixture Model [Keller and Bengio, 2004]. This presents a discretized version of Latent Dirichlet Allocation that does not require approximate inference. The Dirichlet over component weights is replaced by a document-clustering mixture:\np(w) = \u2211 l p(l)Z(w) \u220f n ( \u2211 m pl(m)pm(n)) wn (3.11)\nTheme Topic Mixture Model presents a direct combination of the document clustering and word clustering finite mixture models. Combinations and extensions of these two types of mixture modeling are used throughout text mining applications, both with multinomial models and distributions other than the multinomial."}, {"heading": "3.2.2 N-grams and Hidden Markov Models", "text": "The multinomial model of text considers words within documents to be distributed independent of their context. Consequently all phrase- and sentence-level information present in the documents is left unmodeled. This sequence information is crucial for many applications of generative models of text, including machine translation, speech recognition, optical character recognition and text compression. Even in tasks where multinomial text models are considered sufficient, sequence modeling has been shown to be beneficial [Song and Croft, 1999, Miller et al., 1999, Peng and Schuurmans, 2003, Medlock, 2006]. Incorporating sequence information is most commonly done using higher order sequential models called n-grams, also known as Markov chain models. These models originate from the early days of computer science, and aside from the many practical uses they have been instrumental in the development of computer science and information theory [Shannon, 1948, Chomsky, 1956, Markov, 1971].\n47\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nAn n-gram model generalizes the multinomial model to take the preceding sequence of M \u2212 1 words into account, where M is the order of the n-gram. Each history of preceding M \u2212 1 words models a separate categorical. The models of the first three orders are commonly called unigram, bigram and trigram models, corresponding to zeroth, first and second order Markov chain models, respectively. Over the last decades higher order models such as 4-grams and 5-grams have become standard, with the availability of web-scale text datasets and the development of computer processors and memory. A full n-gram model of order M and vocabulary size of N requires NM parameters, i.e. counts, for the NM\u22121 categorical distributions. Due to the Zipf-law distribution of text data, the models will be extremely sparse, and only a fraction of these counts will be seen in any amount of training data. For these reasons the main foci in n-gram research have been efficiency of implementation [Siivola et al., 2007, Brants et al., 2007, Watanabe et al., 2009, Pauls and Klein, 2011] and methods for smoothing high order n-grams with lower order estimates [Jelinek and Mercer, 1980, Ney et al., 1994, Chen and Goodman, 1996, 1999, Goodman, 2000, Huang and Renals, 2010, Schu\u0308tze, 2011].\nLet w denote any word sequence that corresponds to the counts in the word vector w: wn = \u2211\nj:wj=n 1.\nLet wj\u2212M+1...wj denote a subsequence of M words ending at word j. The history or context of an n-gram is the sequence wj\u2212M+1...wj\u22121 of preceding M \u2212 1 words, a sequence of 0 words in the unigram case M = 1. The probability of a word sequence is given by:\np(w) = \u220f j pM(wj|j,w)\n= \u220f j pM(wj|w1...wj\u22121) Markov chain\n= \u220f j pM(wj|wj\u2212M+1...wj\u22121) finite history (3.12)\nThe probabilities for the first M \u2212 1 n-grams are undefined, since their histories would span over the word sequence boundaries. To correct this, the sequence can be \u201cpadded\u201d by adding start symbols \u201c<s>\u201d at the beginning of the sequence. Sequence end symbols \u201c</s>\u201d can be added, and both of these improve modeling accuracy at the boundaries.\nThere is a considerable literature on methods for smoothing the n-gram language models. Virtually all of these interpolate n-grams hierarchically with lower order n-grams [Chen and Goodman, 1999]. Let pm() denote the smoothed m-th order model in the hierarchy and pum() denote the unsmoothed model of the same order. The interpolation smoothed n-gram probabilities can be expressed as:\npm(wj|wj\u2212m+1...wj\u22121) =(1\u2212 \u03b1m)pum(wj|wj\u2212m+1...wj\u22121) + \u03b1m pm\u22121(wj|wj\u2212m+2...wj\u22121), (3.13)\nwhere \u03b1m are backoff weights for order m chosen by the smoothing method. With Jelinek-Mercer smoothing\n48\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\n[Jelinek and Mercer, 1980, Chen and Goodman, 1999], \u03b1m are simply fixed parameters estimated on held-out data.\nOften a uniform zerogram model is included to end the recursion. Without a zerogram, the different n-grams in hierarchical interpolation methods form a hierarchy of smoothing with M levels. The smoothing weights can be expanded:\np(w) = \u220f j pM(wj|wj\u2212M+1...wj\u22121)\n= \u220f j \u2211 m p(m)pum(wj|wj\u2212m+1...wj\u22121)\n= \u220f j \u2211 m ( M\u220f m\u2032=m+1 \u03b1m\u2032 \u2212 M\u220f m\u2032=m \u03b1m\u2032)p u m(wj|wj\u2212m+1...wj\u22121), (3.14)\nIn this form it is seen that n-gram smoothing methods utilize a word-level mixture, generating each word in a sequence as a finite mixture model of the different order models. Hierarchical smoothing means that the mixture component weights are generated dynamically as a product of the higher order back-off weights:\np(m) = \u220fM m\u2032=m+1 \u03b1m\u2032 \u2212 \u220fM m\u2032=m \u03b1m\u2032 [Bell et al., 1989]. Writing a word-clustering mixture of Equation 3.7 in the sequence form shows a further surprising connection:\np(w) = \u220f j \u2211 m p(m)pm(wj) (3.15)\nComparing the word-clustering mixture in this form to the expanded n-gram model of Equation 3.14, we can note the similarity between topic models and hierarchically smoothed n-grams. Both models generate words as a mixture of components. In topic modeling the components correspond to topics, and weights are generated by the chosen topic modeling method. In n-gram modeling the components correspond to the n-gram smoothing hierarchy, and weights are generated dynamically by the chosen smoothing method.\nUnlike mixture models, n-gram models have no hidden (unknown) variables in estimation and are efficiently estimated by normalizing the known count statistics. Extending n-grams with hidden variables leads to a more powerful class of models called Hidden Markov Models (HMM) [Baum and Petrie, 1966, Rabiner, 1989, Bilmes, 1998, Miller et al., 1999]. A categorical HMM considers text to be generated as categorical outputs of a hidden Markov chain model, so that only the outputs wj of the Markov chain are seen. A HMM can be seen as an extension of a mixture model, so that the weights of each component become dependent on the history of the last M \u2212 1 components that generated an output. In HMM terminology the outputs are called emissions or observations, and the hidden variables are called states. In text modeling the outputs are commonly words, and the hidden variables are structural variables such as parts of speech, named entities,\n49\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nsections, topics and authors that are to be discovered using the HMM.\nDue to the abundance of applications for HMMs, a number of variants exist that can be mentioned. Arcemission HMMs output a variable on each transition to a state, whereas state-emission HMMs have the output variables attached to the states. Historically HMMs were defined more commonly as arc-emission HMMs, rather than state-emission HMMs. These two types of HMMs are equivalent, in the sense that either can be transformed to the other. In many applications the output distributions are Gaussian or mixture models, instead of categoricals. Epsilon or -states can be used, that have no attached output distribution. HMMs of this type are called -transition HMMs and are more powerful in the distributions that can be represented, since -states can be used to model complex sequences of hidden variables behind each output. But these also pose problems for inference, since each output word can be generated by arbitrarily long loops of -transitions.\nLet k denote a hidden state sequence of states m corresponding to a word sequence w. Let M be the number of hidden states, pkj(wj) the categorical output distribution of state kj, and p(kj|kj\u22121) the transition probability to state kj given the previous state kj\u22121. A first-order categorical state-emission HMM without\n-transitions produces the joint probabilities:\np(w,k) = \u220f j p(kj|kj\u22121)pkj(wj), (3.16)\nwhere the hidden p(k1|k0) = p(k1) is provided by a categorical distribution p(k1) for the initial states k1. Alternatively, the sequence can be padded with boundary symbols the same way as with n-gram models."}, {"heading": "3.2.3 Directed Graphical Models", "text": "MNB and the discussed extensions can be described in the general framework of graphical models [Pearl, 1986, Lauritzen and Spiegelhalter, 1988, Loeliger, 2004, Frey and Jojic, 2005, Klinger and Tomanek, 2007, Parikh and Drezde, 2007, Sutton and McCallum, 2007] as generative directed graphical models. A graphical model is a model of a joint distribution of variables that factorizes according to an underlying graph. Commonly this is an independency graph that encodes the independence assumptions of the model. Nodes in an independency graph represent the variables, and lack of an edge between two variables indicates an independence assumption. Since computation is only required for related variables, factorizing a joint distribution according to the assumptions greatly simplifies modeling. Algorithms developed for the estimation and inference of graphical models are applicable to new types of models, reducing the time required for research and development, while the graphical representation reduces the time required for presentation of new models.\nThe various types of graphical models use different factorizations and graphical notations. Traditionally, graphical models come from two types, called Bayesian Networks [Pearl, 1986] and Markov Random Fields [Kindermann and Snell, 1980]. Both types visualize the variable nodes in the graph with a circle, and encode the independence assumptions in a model by omissions of edges. Both represent the assumptions using an independency graph G = (V,E) of variable nodes V and edges E. Bayesian Networks are called directed\n50\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nw\nl\n(a) Multinomial Naive Bayes\nw\nl\n(b) Logistic Regression\nFigure 3.1: Independency graphs for a Multinomial Naive Bayes and Logistic Regression models. Multinomial Naive Bayes is a directed graphical model, Logistic Regression is an undirected graphical model\ngraphical models, as the edges, depicted as arrows, encode a directional conditionality of variables. Markov Random Fields are called undirected graphical models, and the undirected edges imply dependency, but not directionality. The two types of models are different in the distributions they can represent and the used algorithms. MNB forms a basic directed graphical model, while LR is a corresponding undirected graphical model. Figure 3.1 shows a comparison of MNB and LR using elementary independency graph notation, with the word vector w and label variables l forming the variable nodes x of the graph.\nGraphical models factorize a joint distribution p(x) to T factors \u03a8t: p(x) = \u220f t \u03a8t(nd(t)), where nd(t) is the subset of the variable nodes xt connected to factor \u03a8t. The factors for undirected graphical models are also called cliques, and are constrained to be arbitrary non-negative functions \u03a8t \u2265 0, where \u03a80 = Z = 1/ \u2211 \u03a8Tt=1(nd(t)) is an additional normalization factor, also called the partition function. The factors for a directed graphical model are conditional probability distributions of the form \u03a8t(nd(t)) = p(xt|nd(t)) for each node t, with the special case nd(t) = \u2205 producing marginal probability distributions p(xt). No additional normalization factor is required for directed graphs, as the conditional distributions are normalized probabilities. Since the factors for directed graphs are defined for each node t, the factorization can be read directly from the graph, whereas the clique factors for undirected graphs are visualized less directly by the independency graph notation.\nThe notation for graphical models is ongoing constant evolution, and additional notation has been introduced [Minka and Winn, 2008, Dietz, 2010, Andres et al., 2012]. Usually shaded nodes represent known variables, and clear nodes represent hidden ones. Additional plate notation is standard for repeating parts in graphical models, such as repeated segments in variable sequences. This represents \u201cunrolling\u201d of the graph, so that the segment is repeated a certain number of times. Nevertheless, in practice the plate notation is inconvenient for representing dependencies in sequences. Visualizations of sequence graphical models depict repeated fragments of the models instead [Murphy, 2002, Deviren et al., 2004, 2005, Frey and Jojic, 2005, Wiggers and Rothkrantz, 2006, Sutton and McCallum, 2007, Parikh and Drezde, 2007, Klinger and Tomanek, 2007], sometimes indicating the repetition by use of ellipses, or combining the fragment notation with the plate notation [Wang et al., 2007].\n51\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nAs an example of a directed graphical model, we can first take the MNB p(l,w) = \u03a81(l)\u03a82(l,w). In this case the factors are categoricals \u03a81(l) = p(l) and multinomials \u03a82(l,w) = pl(w). An equivalent definition can\nbe done using sequence variables and graph unrolling. In this case p(l,w) = \u220f\nt \u03a8t(nd(t)), where \u03a81(l) = p(l)\nis categorical and \u03a8t+1(l, wt) = pl(wt) for 2 \u2264 t \u2264 J + 1 are categorical draws from the multinomial, corresponding to the unrolled variables. Figure 3.2 shows directed independency graph notations for MNB, with varying degrees of explicitness.\nGraphical model notations are most useful for expressing an overview of a statistical model, compared to similar models. The mixture models discussed in this section are compared Figure 3.3. When some properties of the model are not crucial for presenting the main modeling ideas, they can be omitted from the illustration.\n52\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nFor example, the models discussed in this chapter all share common modeling ideas, such as the use of categorical distributions and assumption of IID data. Inclusion of these types of properties in the visualization would cause the graphical notations to be less accessible.\nThe use of unrolled Bayes Networks for sequence modeling has been called Dynamic Bayes Network models (DBN) [Deviren et al., 2004, 2005, Wiggers and Rothkrantz, 2006]. Graph unrolling enables describing sequence models such as topic models, HMMs, and n-grams in the notation of directed graphical models. Topic variables, HMM hidden states and n-gram order interpolation weights can be described as\nvariables in the graph. For example, a categorical HMM can be expressed as p(w,k) = \u220f\nt \u03a8(nd(t)), where\n\u03a8t(nd(t)) = pkt(wt) for 1 \u2264 t \u2264 J and \u03a8t(nd(t)) = p(kt%M |kt%M\u22121) for J + 1 \u2264 t \u2264 (J + 1)M , where M is the number of HMM hidden states m.\nDBNs extend this graphical model view of HMMs, so that a number of hidden variables can underlie an observed word output, instead of a single hidden state variable. For example, a model of text can have topic\n53\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nvariables, dialog types, word history, word clusters, parts of speech etc. as the hidden variables [Deviren et al., 2004, 2005, Wiggers and Rothkrantz, 2006]. Including some of these variable types can considerably improve over simple n-gram models of text. Interpolated n-gram models can be included in DBNs by linking to each word the different n-gram order nodes, and an interpolation variable node that outputs the n-gram order mixture weights. With DBNs any conceivable variables can be used to condition the sequence variables, as long as the conditioning does not form cycles in the graph. Figure 3.4 shows a comparison of the sequence model extensions of multinomials."}, {"heading": "3.2.4 Factor Graphs and Gates", "text": "Graphical models using both directed and undirected edges include chain graphs [Frydenberg, 1990] and ancestral graph Markov models [Richardson and Spirtes, 2002]. More recently, factor graphs [Kschischang et al., 2001, Frey, 2003, Loeliger, 2004, Frey and Jojic, 2005, Lazic et al., 2013] has been proposed as a superset of the earlier frameworks. The original formalism itself has been followed by a number of extensions [Loeliger, 2004, Frey, 2003, Minka and Winn, 2008, McCallum et al., 2009, Dietz, 2010, Andres et al., 2012], such as directed factors [Frey, 2003, Dietz, 2010], gates [Frey, 2003, Minka and Winn, 2008, Dietz, 2010, Oberhoff et al., 2011] and factor templates [McCallum et al., 2009].\nThe factor graph notation visualizes any graphical model using a graph G = (V, F,E) of variable nodes V , factor nodes F and edges E. The graph is bipartite, so that each edge connects a variable node to a factor node. The variable nodes are visualized using circles, the factor nodes using small squares and the edges using lines. Figure 3.5 shows MNB as a factor graph. Factor graphs are a strict superset of undirected and directed graphical models, since both types of models can be represented as factor graphs, while many factor graphs cannot be represented by either types of models [Frey, 2003].\nGates are a proposed extension of factor graph notation that allows more explicit visualization of mixture models and context-dependent models [Minka and Winn, 2008, Winn, 2012]. Gates notation uses a gate\n54\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nor switch function [Frey, 2003] to select the behaviour of a sub-graph based on a key variable, such as a mixture model component indicator. For example, labels for MNB can be written as a vector of variables c:\n\u2200l : 0 \u2264 cl \u2264 1, and \u2211 l cl = 1. The joint distribution for MNB can then be rewritten as a gate:\np(w, c) = \u220f l (p(w, l))cl (3.17)\nThe label indicator variables cl in Equation 3.17 performs the role of a switch, changing the output of the gate to 1 for all labels l with key value cl = 0. The gate formalism enables expression of factor graphs where more general functions are used for the key variables, such as context-dependent variables. For a given key value, only the parts of the gated sub-graph with key value cl > 0 need to be computed.\nGates are visualized using dashed rectangles, either as a single rectangle similar to a plate, or in an expanded form with separate rectangles for the different values of the key. The latter is useful in complex cases, when the key values result in different types of computations within the gate. The key variable is shown connected to the gate rectangle by a line. Figure 3.6 shows gated factor graphs for MNB with gates and expanded gates. The gate notation is unnecessary for simple models such as MNB, but becomes useful for illustrating inference in more complex graphical models, as will be shown in Chapter 5 of the thesis."}, {"heading": "3.2.5 Inference and Estimation with Directed Generative Models", "text": ""}, {"heading": "3.2.5.1 Overview of Algorithms", "text": "Directed generative models commonly have efficient algorithms for both inference and estimation. MNB has linear time and space complexity algorithms for both, as do constrained extensions of multinomials such as\n55\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nn-grams. Mixture extensions generally complicate the estimation, and multiply the inference complexities by the number of components in each mixture. Dynamic programming [Bellman, 1952, Viterbi, 1967] is an algorithmic innovation that can be used to reduce the complexities for HMM and DBN extensions. Latent Dirichlet Allocation and some of the more complex directed graphical models require other approximate algorithms for both inference and estimation, such as Variational Bayes and Gibbs Sampling [Asuncion et al., 2009, Blei, 2012].\nThe joint p(w, l) for MNB is computed by the product p(l) \u220f\nn pl(n) wn over the prior and the label condi-\ntionals, and the naive inference algorithm for this is a trivial product over the terms. Inferring the marginal p(w) from the joint and posterior p(l|w) with the Bayes rule can be done with the closed form operations of sums and products. The naive inference time and space complexities for the MNB posterior p(l|w) is O(|w|0L) [Manning et al., 2008]. Document-clustering mixtures introduce sums over documents, and wordclustering mixtures introduce sums over words; both multiply the space and time complexities of inference by the number of components J . Replacing the multinomial in MNB with a HMM introduces a sum over all possible hidden state sequences. This can be reduced using dynamic programming techniques. Inference on more complex directed graphical models can require other algorithms, such as the approximate algorithms for Latent Dirichlet Allocation [Asuncion et al., 2009, Blei, 2012].\nMNB is estimated by gathering and normalizing the sufficient statistics of counts. When sparsity is utilized, this has the time complexity O( \u2211 i |w(i)|0) and space complexity O(L+ \u2211 l \u2211 n:exp(\u03bbln)>0 1). Extension with mixtures complicates the estimation, due to a sum in the likelihood function that does not decompose into separate optimizations. A common practical solution to this problem is the EM algorithm based on dynamic programming. This treats the mixture components as random variables, iteratively optimizing the expected conditional log-likelihood until a stationary point of the likelihood function is reached. As with inference, more complex graphical model extensions can require approximations such as Variational Bayes and Gibbs Sampling. Aside from parameter estimation, sometimes the graphical model structure itself is unknown and learned using a variety of approximate methods [Friedman et al., 1997, Lazic et al., 2013].\nAdditional meta-optimization in estimation is commonly required, to choose the meta-parameters required by the models, and to avoid bad local minima of the EM-estimated likelihood function. Meta-parameters required by the models can include the number of components in the mixtures and smoothing parameters. Basic solutions for setting these are heuristic values and grid searches. Local minima are encountered with complex multimodal likelihood functions, such as those for mixture models. A basic solution is random restarts for lowering the probability of a low quality local optima. Chapter 6 of the thesis describes a Gaussian random search algorithm that can be used to solve the meta-optimization problem in a principled manner."}, {"heading": "3.2.5.2 Dynamic Programming", "text": "Dynamic programming [Bellman, 1952] is an algorithmic innovation that can be used to solve problems with overlapping subproblems efficiently. The application of dynamic programming makes HMMs practical, by\n56\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nlowering the complexity of the required computations. These are next described in brief for the case of a first-order categorical HMM. The probability of a word sequence w for the HMM of Equation 3.16 can be marginalized:\np(w) = \u2211 k \u220f j p(kj|kj\u22121)pkj(wj) (3.18)\nMarginalizing p(w) by summation over the possible state sequences is not usually feasible, as there are MJ possible state sequences. Computing this observation state probability efficiently is considered the first of the three main computational problems with HMMs [Rabiner, 1989]. The second problem is optimizing argmaxk p(k|w), used for segmenting data to the hidden state sequences. The third problem is estimating the model, given that the hidden states are unknown in training data.\nThe summation over the state sequences \u2211 k in computing p(w) can be considered a brute-force solution to the problem. A dynamic programming solution is computing the probabilities p(w) over the sequence indices j instead, summing for each possible state m the probability of all sequences leading to the state, called the forward probability \u03bej(m). This is known as the forward algorithm. Using the forward variables, the marginal probability for the HMM can be rewritten in a recursive form:\np(w) = \u2211 k \u220f j p(kj|kj\u22121)pkj(wj)\n= \u2211 m \u03beJ(m)\n\u03bej(m) = p(m)pm(wj), if j = 1\u2211 m\u2032(\u03bej\u22121(m \u2032)p(m|m\u2032))pm(wj), otherwise, (3.19)\nwhere p(m) = p(kj) and pm(wj) = pkj(wj).\nThe forward algorithm solves p(w) recursively by computing \u03bej(m) from j = 1 to j = J , reducing the time complexity from O(JMJ) to O(JM2). The space complexity is also reduced to O(2K), since only the forward variables for the current and previous sequence index need to be kept in memory. Alternatively, the algorithm can be run in the reverse order. This is called the backward algorithm and it produces exactly the same results. These algorithms can be extended to virtually all types of HMMs. The zeroth order HMM can be shown to be a special case, requiring only O(JM):\np(w) = \u2211 m \u03beJ(m)\n= \u2211 m \u2211 m\u2032 ((\u03beJ\u22121(m \u2032)p(m))pm(wJ))\n57\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\n= \u2211 m\u2032 (\u03beJ\u22121(m \u2032)) \u2211 m (p(m)pm(wJ))\n= \u220f j \u2211 m p(m)pm(wj)) (3.20)\nRelated dynamic programming algorithms are used for solving the other two problems with HMMs. Changing the sum over m in Equation 3.20 to a max returns the probability of the word sequence by the single most likely sequence of states, solving the second problem. This is known as the Viterbi algorithm [Viterbi, 1967] and its efficient implementations underlie many of the applications of HMMs into sequence classification. Lastly, the forward and backward algorithms can be combined to compute the posterior probabilities p(kj|w) of each state kj for each sequence index j. This is known as the forward-backward algorithm, and the posteriors can be used for the Expectation step in EM estimation [Baum et al., 1970, Rabiner, 1989, Bilmes, 1998].\nIn case a directed graphical model has no cycles of edges, efficient exact dynamic programming inference is possible using extensions of the forward, Viterbi and forward-backward algorithms to general graphs [Pearl, 1986, Kschischang et al., 2001, Loeliger, 2004]. For more complex graphical models a variety of less efficient exact and approximate inference algorithms exist. Most of these build on the idea of variable elimination [Zhang and Poole, 1994], that works by marginalizing variables away to arrive at the inferred probability distribution. The generalized case of forward algorithm to graphs is the sum-product algorithm or belief propagation. Analogously the generalized case of Viterbi algorithm is the max-product algorithm. These extend the use of the forward and maximum variables \u03bej(m), so that a message variable is computed for each node in the graph and the inference is done by passing the messages in an efficient order."}, {"heading": "3.2.5.3 Expectation Maximization", "text": "The training data for the HMM of Equation 3.16 consists of instances D(i) = (w(i),k(i)) of known and hidden variables, where w(i) is the known sequence and k(i) is unknown variables for the i-th instance of training data. The likelihood function becomes:\nL(\u03b8|D) = p(D|\u03b8) = \u220f i \u2211 k(i) p(w(i),k(i)|\u03b8)\n= \u220f i \u2211 k(i) \u220f j (p(k (i) j |k(i)(j\u22121),\u03b8) pk(i)j (w (i) j |\u03b8)) (3.21)\nThe summation over components causes a problem for optimizing the log-likelihood of the model, since the log-likelihood no longer factorizes into parts that can be separately optimized. If the hidden states were known and each word was generated by a single component, the log-likelihood function would factorize easily.\n58\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nIn this case the likelihood function is:\nL(\u03b8|D) = \u220f i p(w(i),k(i)|\u03b8)\n= \u220f i \u220f j (p(k (i) j |k(i)(j\u22121),\u03b8) pk(i)j (w (i) j |\u03b8)) (3.22)\nIn most cases the hidden states are not known. With mixture models and HMMs the main solution to this problem is using the EM algorithm to estimate the model. Instead of attempting to maximize the log-likelihood, the EM algorithm treats the components as random variables and iteratively optimizes the conditional expectation of the log-likelihood Q(\u03b8|D, \u03b8\u0302). Let p(k(i)|w(i), \u03b8\u0302) indicate the expectation of a hidden variable sequence k(i) for word sequence w(i) according to some prior parameters \u03b8\u0302: p(k(i)|w(i), \u03b8\u0302) = p(k(i),w(i)|\u03b8\u0302)/\u2211k(i) p(k(i),w(i)|\u03b8\u0302) The Q-function becomes:\nQ(\u03b8|D, \u03b8\u0302) = E(log(L(\u03b8|D, \u03b8\u0302))) = \u2211 i \u2211 k(i) p(k(i)|w(i), \u03b8\u0302) log(p(w(i),k(i)|\u03b8))\n= \u2211 i \u2211 k(i) \u2211 j p(k(i)|w(i), \u03b8\u0302) log(p(k(i)j |k(i)(j\u22121),\u03b8) pk(i)j (w (i) j |\u03b8))) (3.23)\nSince p(k(i)|w(i), \u03b8\u0302) is treated as a random variable, Q(\u03b8|D, \u03b8\u0302) becomes a random variable as well. With random variables the sum of expectations equals the expectation of the sum, so that for computing L(\u03b8|D, \u03b8\u0302)) the expectation can be pushed inside the sums, placing the expectation p(k(i)|w(i), \u03b8\u0302) as a weight for each possible hidden state sequence. Let \u03bbm indicate the conditional parameters for component m, \u03bb the conditional parameters for all components and \u03b1 the parameters for component weights. The conditional log-likelihood can be optimized:\nargmax \u03b8\n(Q(\u03b8|D, \u03b8\u0302))\n= argmax (\u03bb,\u03b1) ( \u2211 i \u2211 k(i) \u2211 j p(k(i)|w(i), \u03b8\u0302) log(p(k(i)j |k(i)(j\u22121),\u03b1) pk(i)j (w (i) j |\u03bb)))\n= argmax (\u03bb,\u03b1) ( \u2211 i \u2211 k(i) \u2211 j p(k(i)|w(i), \u03b8\u0302) log(p(k(i)j |k(i)(j\u22121),\u03b1))\n+ \u2211 i \u2211 k(i) \u2211 j p(k(i)|w(i), \u03b8\u0302) log(p k (i) j (w (i) j |\u03bb))) (3.24)\nReplacing the current set of parameters \u03b8\u0302 with the estimated parameters \u03b8\u0302\u2032 = argmax\u03b8(Q(\u03b8|D, \u03b8\u0302)) and repeating the optimization until Q(\u03b8|D, \u03b8\u0302) = Q(\u03b8|D, \u03b8\u0302\u2032) produces the EM-estimated stationary point of the\n59\nCHAPTER 3. MULTINOMIAL NAIVE BAYES FOR TEXT MINING\nlikelihood function. Depending on the likelihood function and initialization, the stationary point can remain far from a global optimum, and EM is often repeated with randomly sampled initial parameters. Other practical variants include online versions of EM [Liang and Klein, 2009] and combination with genetic algorithms [Mart\u0301\u0131nez and Virtria\u0301, 2000, Pernkopf and Bouchaffra, 2005, Puurula and Compernolle, 2010].\nFor more scalable algorithmic implementation, EM is implemented using the forward-backward variables p(k (i) j |w(i), \u03b8\u0302) instead of the state sequence variables p(k(i)|w(i), \u03b8\u0302). Summing these weighted posterior probabilities produces expected counts E(C()) in place of the counts C() used in Equation 3.3. Using the forwardbackward algorithm, the posteriors can be computed in the same time complexity as the forward algorithm, but require O(JM) in space complexity, as the full JM matrix needs to be stored for estimating with the weights.\n60\nChapter 4\nReformalizing Multinomial Naive Bayes\nThis chapter presents a thorough reformalization of Multinomial Naive Bayes (MNB) as a probabilistic model. The issue of smoothing is first discussed, noting that most of the multinomial smoothing methods necessary for MNB are not correctly formalized under maximum likelihood estimation. A unifying framework for the methods is proposed, and a two-state Hidden Markov Model (HMM) formalization is shown to derive the smoothed parameter estimates. Feature weighting is formalized for both estimation and inference as welldefined approximation with expected log-probabilities, given probabilistically weighted word sequences. A formalization of MNB is defined that takes these corrections into account, followed by a more general graphical model extension that includes label-conditional document length modeling, and scaling the influence of label priors."}, {"heading": "4.1 Formalizing Smoothing", "text": ""}, {"heading": "4.1.1 Smoothing Methods for Multinomials", "text": "The sparsity of text data causes problems for maximum likelihood estimation of multinomials. Words occurring zero times for a label will cause the corresponding parameter estimates to be zero as well, resulting in 0-probabilities when computing the probabilities with unsmoothed models pul (w|\u03bbu). This complication is known as the zero-frequency problem in statistics, and smoothing methods for solving this problem have been extensively researched. However, the smoothed models pl(w|\u03bb) are no longer justified by maximum likelihood, and only a subset of the smoothing methods can be formalized under other principles, such as maximum a posteriori [MacKay and Peto, 1995, Rennie, 2001]. This section proposes a unified framework for smoothing generative models of text, and shows that practically all of the smoothing methods for multinomials can be formalized as approximate maximum likelihood estimation on a constrained Hidden Markov Model (HMM).\nAn early solution to the zero-frequency problem dates two centuries and is known as Laplace correction [Laplace, 1814]. On discussing the probability of the sun rising tomorrow, Laplace argued that despite the event of the sun not rising has never been seen, it should still have a probability assigned to it. The proposed\n61\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nLaplace correction adds a single count to each possible event, thereby avoiding the zero frequency problem. In language modeling this correction is known as Laplace smoothing, and a parametric generalization adding a fractional count \u00b5 instead was proposed by Lidstone in 1920 [Lidstone, 1920]. By further multiplying the added count by a prior background model pu(n), this correction becomes the Dirichlet prior method for smoothing.\nMaximum a posteriori estimation of multinomials with a Dirichlet prior takes the form:\n\u03bbln = log( C(l, n) + \u00b5 pu(n)\u2211 n\u2032 C(l, n \u2032) + \u00b5 pu(n\u2032) ), (4.1)\nwhere the special case pu(n) = 1/N is Lidstone smoothing and pu(n) = 1/N , \u00b5 = N is Laplace correction [Rennie, 2001, Smucker and Allan, 2007].\nAnother basic way to avoid the zero-frequency problem is to linearly interpolate parameter estimates with a background model. This is known as Jelinek-Mercer smoothing [Jelinek and Mercer, 1980], and takes the form:\n\u03bbln = log((1\u2212 \u03b2) C(l, n)\u2211 n\u2032 C(l, n \u2032) + \u03b2 pu(n)) (4.2)\nDirichlet prior and Jelinek-Mercer differ only in how the weight for the background model is chosen. A\ngeneral interpolation function covers both types of smoothing [Johnson, 1932, Smucker and Allan, 2007]:\n\u03bbln = log((1\u2212 \u03b1l)pul (n) + \u03b1lpu(n)), (4.3)\nwhere pul (n) is the unsmoothed label-conditional multinomial for label l. Fixing \u03b1l to a pre-determined\nvalue \u03b1l = \u03b2 produces Jelinek-Mercer smoothing. Fixing \u03b1l = \u00b5 \u00b5+ \u2211 n C(l,n)\n= 1\u2212 \u2211 n C(l,n)\n\u00b5+ \u2211 n C(l,n) produces Dirichlet\nprior smoothing. Combining these as \u03b1l = 1\u2212 \u2211 n(C(l,n)\u2212\u03b2C(l,n)) \u00b5+ \u2211 n C(l,n) results in two-stage smoothing, a smoothing method suggested for information retrieval [Zhai and Lafferty, 2001a, Smucker and Allan, 2007].\nThe background model is commonly a uniform distribution pu(n) = 1 N , or a label-independent collection model pu(n) = \u2211 l C(l,n)\u2211\nn\u2032 \u2211 l C(l,n \u2032) . A uniform-smoothed collection model with smoothing weight \u03a5 interpolates\nbetween a uniform and a collection model: pu(n) = (1\u2212\u03a5) \u2211 l C(l,n)\u2211\nn\u2032 \u2211 l C(l,n \u2032) + \u03a5 1 N . In n-gram language modeling\nliterature the uniform background model is called the zerogram, and the collection model is called the unigram background model [Chen and Goodman, 1999]. Alternatively, if several documents per label exist, the counts from each document can be normalized by length, or weighted according to usefulness. External datasets can\n62\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nlikewise be used to estimate the background model, such as large text datasets of the same language. The choice of background model can also be motivated by the task, such as using a collection model to introduce relevance information in ranked retrieval [Zhai and Lafferty, 2001a].\nJelinek-Mercer and Dirichlet prior are the two most common types of smoothing for MNB models. One view of smoothing is that smoothing methods discount seen occurrences of words in order to redistribute the subtracted probability mass al to the background model. Under this view both of these discount the seen counts linearly by \u03b1l. A third basic type of smoothing is called is called absolute discounting [Ney et al., 1994, Chen and Goodman, 1996, 1999, Zhai and Lafferty, 2001b, 2004]. This works similar to JelinekMercer smoothing, but subtracts a parameter value \u03b4 : 0 \u2264 \u03b4 \u2264 1 from all counts for a label, and uses the subtracted probability mass for choosing the smoothing coefficient. The discounted counts can be denoted C \u2032(l, n) = C(l, n) \u2212 \u03b4. Using Equation 4.3 and choosing pul (n) = C \u2032(l,n)\u2211\nn\u2032 C \u2032(l,n\u2032)\n, and \u03b1l = 1 \u2212 \u2211 n C\n\u2032(l,n)\u2211 n C(l,n) produces\nabsolute discounting.\nA problem with absolute discounting is that usually separate discount values are optimal for different counts, with higher discounts for higher counts [Ney et al., 1994]. Empirical analyses [Chen and Goodman, 1999, Durrett and Klein, 2011, Schu\u0308tze, 2011, Neubig, 2012] have shown that optimal discount values seem to follow a power-law distribution, rather than the constant ones in absolute discounting. A recent improvement over absolute discounting is power-law discounting [Momtazi and Klakow, 2010, Momtazi, 2010, Huang and Renals, 2010]. This method discounts according to a power function C \u2032(l, n) = C(l, n) \u2212 \u03b4C(l, n)\u03b4, with 0 \u2264 \u03b4 \u2264 1. Combining this with a Dirichlet prior and reorganizing terms produces Pitman-Yor smoothing, with pul (n) = C(l,n)\u2212\u03b4C(l,n)\u03b4\u2211 n\u2032 (C(l,n \u2032)\u2212\u03b4C(l,n\u2032)\u03b4) and \u03b1l = 1\u2212 \u2211 n(C(l,n)\u2212\u03b4C(l,n)\u03b4) \u00b5+ \u2211 n C(l,n) , that approximates inference on a Pitman-Yor process [Momtazi and Klakow, 2010, Momtazi, 2010, Huang and Renals, 2010].\nThe discussed smoothing methods can be covered by a general function that is called here generalized smoothing. By choosing \u03b1l = 1\u2212 \u2211 n(C(l,n)\u2212D(l,n)) \u00b5+ \u2211 n C(l,n) and pul (n) = C(l,n)\u2212D(l,n)\u2211 n\u2032 (C(l,n \u2032)\u2212D(l,n\u2032)) , and D(l, n) according to the chosen discounting, we recover all of the smoothing methods as special cases of Equation 4.3. Generalized\n63\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nsmoothing with \u00b5 = 0 and D(l, n) = \u03b2C(l, n) implements Jelinek-Mercer smoothing as linear discounting, D(l, n) = C(l, n) \u2212 \u03b4 implements absolute discounting and D(l, n) = \u03b4C(l, n)\u03b4 implements power-law discounting. A discounting function combining Jelinek-Mercer and power-law discounting can be defined as: D(l, n) = \u03b4C(l, n)\u03b4 + \u03b2C \u2032(l, n), where C \u2032(l, n) = C(l, n)\u2212 \u03b4C(l, n)\u03b4. Chapter 6 of the thesis experiments with this combined discounting function. Table 4.1 summarizes the smoothing methods in terms of smoothing weights al and unsmoothed multinomials p u l (n).\nThe parameters can be chosen to maximize the likelihood of held-out data, or a performance measure related to the task. Closed form approximations requiring no held-out data are possible for some parameters. The discounting parameter \u03b4 for absolute and power-law discounting can be approximated using a leave-one-\nout likelihood estimate [Ney et al., 1994]. Denoting the frequency of 1-counts as n1 = \u2211 n:( \u2211 l C(l,n))=1 1 and\n2-counts as n2 = \u2211 n:( \u2211 l C(l,n))=2 1, the discount parameters can be approximated as \u03b4 = n1/(n1 + 2n2) [Ney et al., 1994, Chen and Goodman, 1999, Huang and Renals, 2010, Zhang and Chiang, 2014]. This Kneser-Ney estimate provides an approximate upper bound of the optimal discount value, and has been demonstrated to work well in practice [Chen and Goodman, 1999, Goodman, 2000, Vilar et al., 2004, Zhang and Chiang, 2014].\nThe smoothing methods for multinomial text models can be extended into the smoothing methods used for higher-order n-gram language models. A substantial literature exists for advanced n-gram smoothing techniques [Chen and Goodman, 1999, Rosenfeld, 2000]. These extend the multinomial smoothing techniques hierarchically, by placing the lower-order m\u22121 n-gram as the background model for each order m. For example, Witten-Bell smoothing [Moffat, 1990] is hierarchical linear interpolation with a nonparametric estimate for the interpolation weights [Chen and Goodman, 1999]. Using the label-conditional model as a higher order model and the background distribution as a lower-order model, the Witten-Bell estimate for the smoothing\nweight is \u03b1l = 1 \u2212 \u2211 n C(l,n)\u2211\nn:C(l,n)>0 1+ \u2211 n C(l,n) . We can note that Witten-Bell smoothing is a case of Dirichlet prior smoothing with a heuristic estimate for the Dirichlet parameter: \u00b5l = \u2211 n:C(l,n)>0 1 = \u2211 n min(1, C(l, n)). Witten-Bell smoothing originates from text compression modelling, where linearly interpolated n-gram models are known as Prediction by Partial Matching (PPM) models [Cleary and Witten, 1984]. The Witten-Bell smoothed PPM is known as PPM-C and has been a baseline for text compression for over two decades. In general applications of n-grams more effective smoothing methods can be applied.\nInterpolated Kneser-Ney smoothing [Chen and Goodman, 1999, James, 2000, Goodman, 2000, Siivola and Pellom, 2005, Goldwater et al., 2006, Teh, 2006, Heafield et al., 2013] has been the standard LM smoothing method for n-grams for over a decade. This combines n-grams hierarchically using absolute discounting, but replaces the lower order m < M estimates of counts by the number of contexts the count occurs in [Kneser\nand Ney, 1995]. For a multinomial, the modified background model becomes pu(n) = \u2211 l:C(l,n)>0 1\u2211\nn\u2032 \u2211 l:C(l,n\u2032)>0 1 . A\ncommon example for this method is the phrase \u201cSan Francisco\u201d. A unigram-model estimate for \u201cFrancisco\u201d could be very high, but since the unigram is likely to occur in only this one context, the bigram Kneser-Ney estimate for this lower-order n-gram count would likely be 1. The modified model would correctly consider the unigram \u201cFrancisco\u201d as very unlikely to occur outside this context. All but the highest-order unsmoothed\n64\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nmodels are replaced by the modified counts before discounting, providing a considerable improvement in modeling precision [Chen and Goodman, 1999, James, 2000].\nSome improvements over interpolated Kneser-Ney have been suggested over the years, with limited acceptance. Modified Kneser-Ney smoothing [Chen and Goodman, 1999, Siivola and Pellom, 2005, Heafield et al., 2013, Zhang and Chiang, 2014] replaces the discount for each order with three different discounts for counts 1, 2 and 3+, optimized together on held-out data for perplexity [Chen and Goodman, 1999, Siivola and Pellom, 2005]. For both interpolated and modified Kneser-Ney, the discount parameters can be estimated on held-out data, or approximated with heuristics such as the discussed discount estimate \u03b4 = n1/(n1 + 2n2). Power-law discounting LM [Huang and Renals, 2010] replaces the absolute discounting in interpolated Kneser-Ney with Pitman-Yor Process smoothing.\nSmoothing the parameter estimates directly as in Equation 4.3 would cause zero-value parameters to become non-zeros, resulting in loss of the parameter sparsity. The parameter estimates pul (n) = \u03bb u ln and pu(n) = \u03bbun are often kept separate, so that the complexity of storing pl(n) is not increased. The space\ncomplexity of estimation for a smoothed MNB model is O(L + N + \u2211\nl |\u03bbul |0) and the time complexity is O( \u2211 i |w(i)|0)."}, {"heading": "4.1.2 Formalizing Smoothing with Two-State Hidden Markov Models", "text": "Parameter interpolation is the standard formalization of parameter smoothing for multinomial and n-gram language models [Jelinek and Mercer, 1980, Chen and Goodman, 1996, 1999, Zhai and Lafferty, 2001b, 2004, Smucker and Allan, 2007]. Although parameter interpolation provides a principled method for estimating multinomial parameters, the estimated parameters are not strictly speaking maximum likelihood estimates of the multinomial model, but rather ad-hoc estimates [Hiemstra et al., 2004]. The parameter interpolation introduced in Equation 4.3 and Table 4.1 shows that all smoothing methods can be expressed as a normalized mixture of an unsmoothed multinomial and a background distribution. This analysis is extended next by showing that the smoothing methods can be formulated as maximum expected log-likelihood estimation on a constrained generative model. The proof proceeds by showing that the interpolated parameters used in the smoothed multinomials can be implemented by a categorical HMM, and that constraining the HMM appropriately in parameter estimation reproduces a model with the same joint probabilities as the smoothed multinomial. Compared to formalization of smoothing methods as approximate inference on a Pitman-Yor process and other Bayesian models [MacKay and Peto, 1995, Teh, 2006, Goldwater et al., 2006, Neubig, 2012], the proposed HMM formalization has the advantage that inference using the estimated models is exact.\nAn early formulation of smoothed document models for IR used a two-state HMM for formalizing JelinekMercer smoothing [Miller et al., 1999]. This model used a HMM with one hidden state for the document distribution and one for collection smoothing [Miller et al., 1999, Xu and Weischedel, 2000]. In addition, this work showed how to integrate bigram models, feature weighting, translation models and relevance feedback\n65\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nusing the HMM model. However, the parameter estimation for this model was not well defined, the model was considered to be very different and unrelated to multinomial LMs, and model smoothing was limited to Jelinek-Mercer smoothing. The connection to LMs was discovered shortly afterwards [Hiemstra, 2001], as was the need to use constraints such as parameter tying and fixed parameters for the formalization [Hiemstra, 2001]. Using parameter tying, Jelinek-Mercer smoothed higher-order LMs could be implemented as HMMs [Manning and Schu\u0308tze, 1999]. Despite considerable interest at the time, the two-state HMM model never became popular in IR, nor was its connection to LM smoothing methods explored. We show in the following that this model can be used to formalize all of the discussed smoothing methods in the maximum likelihood framework.\nLet l be a label variable indicating one of the L multinomials sharing a background model. Let w be any sequence of J words, and wj : 1 \u2264 wj \u2264 N correspond to the words counted in w, so that wn = \u2211\nj:wj=n 1.\nLet k be an unknown sequence of M = 2 hidden states kj : 1 \u2264 kj \u2264 M generating the sequence w. A probability model over the joint probabilities (l,w,k) can be defined:\np(l,w,k) = p(l) \u220f j pl(kj)p u lkj (wj), (4.4)\nwhere pl(kj) is a categorical and p u lkj (wj) is a categorical conditional on the label l and the hidden state kj.\nThe model of Equation 4.4 is a special case of a 0th order categorical HMM, where p(l) correspond to initial\nstate probabilities, pl(kj) to HMM state transition probabilities, and p u lkj (wj) to state emission probabilities. Transitions between the label-conditional states are not allowed, so that each word sequence is produced by a single label. Figure 4.1 shows the graphical model for the two-state HMM.\n66\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nThe conditional probabilities pl(w) for the model can be derived:\npl(w) = Z(w) \u2211 k \u220f j pl(kj)p u lkj (wj)\n= Z(w) \u220f j \u2211 m (pl(m)p u lm(wj))\n= Z(w) \u220f n ( \u2211 m pl(m)p u lm(n)) wn , (4.5)\nwhere pl(m) = pl(kj), p u lm(n) = p u lkj (wj), and Z(w) is the multinomial normalizer.\nThe multinomial normalizer Z(w) = ( \u2211 n wn)!\u220f n wn! accounts for the fact that a count vector w can be generated by the number Z(w) of permutations of the sequence w. As discussed in Chapter 3, computing the product\nof sums \u220f\nj \u2211 m corresponds to the forward algorithm, whereas computing the equivalent sum of products\u2211\nk \u220f j gives the brute-force solution to the same problem [Rabiner, 1989].\nThe conditional probabilities pl(w) = Z(w) \u220f n( \u2211 m pl(m)p u lm(n)) wn can be implemented by a multinomial\npl(w|\u03bb) = Z(w) \u220f n exp(\u03bbln) wn , with parameter vector \u03bbl: \u03bbln = log( \u2211 m pl(m)p u lm(n)). From this form it can readily be seen that all of the smoothing methods can be implemented with the two-state HMM of Equation 4.4, by choosing pulm=1(n) = p u l (n), p u lm=2(n) = p u(n), and pl(m = 2) = \u03b1l as the smoothing weight.\nThe training data for model of Equation 4.4 consists of documents D(i) = (w(i), l(i),k(i)), where w(i) are known word sequences, l(i) are label indicators, and k(i) are unknown component assignments for the i-th document of training data. The likelihood function becomes:\nL(\u03b8|D) = p(D|\u03b8) = \u220f i p(l(i)|\u03b8) \u2211 k(i) pl(i)(w (i),k(i)|\u03b8)\n= \u220f i p(l(i)|\u03b8) \u2211 k(i) \u220f j pl(i)(k (i) j |\u03b8) pul(i)k(i)j (w (i) j |\u03b8)\n= \u220f i p(l(i)|\u03b8) \u220f j \u2211 m pl(i)(m|\u03b8) pul(i)m(w (i) j |\u03b8) (4.6)\nThis is difficult to optimize, due to the sum within the product. Similarly to the EM-algorithm, k can be treated as random variables. Given an initial distribution over component assignments pl(i)(k (i)|w(i), \u03b8\u0302), the conditional expectation of the log-likelihood becomes:\nQ(\u03b8|D, \u03b8\u0302) =E(log(L(\u03b8|D, \u03b8\u0302))) = \u2211 i log(p(l(i)|\u03b8)) + \u2211 k(i) pl(i)(k (i)|w(i), \u03b8\u0302) log(pl(i)(w(i),k(i)|\u03b8))\n= \u2211 i log(p(l(i)|\u03b8))\n67\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\n+ \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pl(i)(k(i)j |\u03b8) pul(i)k(i)j (w (i) j |\u03b8)) (4.7)\nMaximizing the conditional expected likelihood parameters for the unsmoothed multinomial pulm=1(n), background model pulm=2(n), and weights pl(m) decouples into separate optimizations, given the distribution over component assignments pl(i)(k (i)|w(i), \u03b8\u0302). Let \u03bblm = [\u03bbl1, ..., \u03bblN ] indicate the conditional parameters for component m of label l, \u03b1lm = [al1, ..., alM ] the parameters for component weights of label l, and \u03c0 = [\u03c01, ..., \u03c0L] parameters for the label priors. Let \u03bb and \u03b1 indicate the combined parameters for the labelconditionals and component weights. Given pl(i)(k (i)|w(i), \u03b8\u0302), the maximization decouples:\nargmax \u03b8\n(Q(\u03b8|D, \u03b8\u0302))\n= argmax (\u03bb,\u03b1,\u03c0) ( \u2211 i log(p(l(i)|\u03c0))\n+ \u2211 i \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pl(i)(k(i)j |\u03b1) pul(i)k(i)j (w (i) j |\u03bb))\n= argmax (\u03bb,\u03b1,\u03c0) ( \u2211 i log(p(l(i)|\u03c0))\n+ \u2211 i \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pl(i)(k(i)j |\u03b1))\n+ \u2211 i \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pu l(i)k (i) j (w (i) j |\u03bb))) (4.8)\nThe parameter estimates for the smoothing methods derive from different assumed distributions\npl(i)(k (i)|w(i), \u03b8\u0302), except the background model for some smoothing methods. Both discounting and linear interpolation imply distributing the expected count mass from a label-conditional model to a background model. Estimation of a shared background model requires further tying of parameters, removing the labelconditional dependency for the second component: \u2200l : pulm=2(n) = pu(n). The label-conditional unsmoothed models pulm=1(n) and the shared background model p u(n) become estimated from the count mass distributed by pl(i)(k (i)|w(i), \u03b8\u0302), while the smoothing weight pl(m = 2) equals the proportion of count mass distributed to the background model for the label l.\nFor some common smoothing methods, the background model is correctly estimated as derived from the two-state HMM formalization. Table 4.2 shows parameter estimates, when pl(i)(k (i)|w(i), \u03b8\u0302) distributes to form the parameter estimates for Jelinek-Mercer smoothing, absolute discounting and power-law discounting. With Jelinek-Mercer, the collection model estimates derive as the background model. With absolute discounting and power-law discounting, the derived background model estimates equal the modified background models [Kneser and Ney, 1995, Huang and Renals, 2010] proposed for these methods. These modified background models were proposed to satisfy marginal constraints of interpolating a single background model to\n68\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nlabel-conditional models, but are commonly used to form all M \u2212 1 order background models in hierarchical n-gram smoothing [Goodman, 2000, Zhang and Chiang, 2014]. According to the two-state HMM derivation, the models for M \u2212 2 orders should be estimated from the expected count mass recursively distributed from the higher order models, instead of observed counts [Goodman, 2000], or expected counts [Zhang and Chiang, 2014] of each order independently.\nSmoothing methods including a Dirichlet prior will result in untypical background models, since a Dirichlet prior gives weight to the background model in proportion of the sum of counts: \u03b1l = 1 \u2212 \u2211 n C(l,n)\n\u00b5+ \u2211 n C(l,n) .\nThis distributes the residual count mass for estimating pu(n) unevenly, so that labels with more accumulated counts contribute less to the background model: pu(n) = \u2211 l \u03b1lC(l,n)\u2211\nn\u2032 \u2211 l \u03b1lC(l,n \u2032) . Using a uniform background\nmodel will analogously imply untypical label-conditional models. A uniform background model would distribute the same count mass for each word type n from the label-conditional models. Discounting a fraction \u03b4 from the first observation of a word would result in a uniform background model, as would discount-\ning uniformly from each observation of a word: D(l, n) = \u03b4/ \u2211\nl\u2032 C(l \u2032, n). A uniform background model\nconstrains the choices for the label-conditional and smoothing weight. For example, if a single smoothing weight is used for all labels, the weight will be upper bound by the proportion of discounted count mass:\npl(m = 2) \u2264 argminn( \u2211 l\u2032 D(l \u2032, n))/( \u2211 l\u2032 C(l \u2032, n)). Common practice chooses the background model as either a collection or uniform distribution, regardless of the label-conditional models and smoothing weights. Alternatively, the background model and the smoothing method hyperparameters can be optimized together for a performance measure of the task [Puurula, 2012b].\nMaximizing the expected conditional likelihood corresponds to a single iteration of the EM algorithm on the HMM. Full EM estimation of the parameters would replace \u03b8\u0302 by \u03b8, and iterate the expectation step of computing pl(i)(k (i)|w(i), \u03b8\u0302) and the maximization step of solving Equation 4.8 with the updated expectation [Hiemstra et al., 2004]. Nevertheless, the EM parameter estimates would not necessarily reach a global optimum of the model likelihood [Baum et al., 1970, Rabiner, 1989, Bilmes, 1998]. The Q-function parameters estimates can be exact maximum likelihood estimates in some cases, and exact closed form estimates can exist for some sets of constraints. For example, if segmentations of words into states k(i) are provided, the parameters are exact maximum likelihood estimates. The case of linear interpolation with the background model and smoothing parameters fixed has a closed form exact maximum likelihood solution [Zhang and Xu,\n69\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\n2008].\nThe HMM framework for formalizing smoothed multinomials can be extended for formalizing smoothing in more structured generative models. This involves constraining the transitions according to the model [Miller et al., 1999, Manning and Schu\u0308tze, 1999, Hiemstra, 2001], but the approximate maximum likelihood estimation remains the same. Due to tied parameters, the resulting HMM topologies can become complicated, especially if the model structure and parameter tying is complex. Visualizing the HMMs can be simplified by presenting label-dependent parts of the HMM topology separately [Miller et al., 1999] and illustrating fragments of the model [Manning and Schu\u0308tze, 1999, Hiemstra, 2001]."}, {"heading": "4.2 Extending MNB for Fractional Counts", "text": ""}, {"heading": "4.2.1 TF-IDF and Feature Transforms with MNB", "text": "Parameter smoothing constitutes the primary means for correcting assumptions in models for text such as MNB. In some fields of text mining this is seen as sufficient. For example, in information retrieval the common view is that collection smoothing performs the same task as Inverse Document Frequency (IDF) weighting, and as such integration of IDF to LMs is not necessary [Zhai and Lafferty, 2001a, Hiemstra et al., 2004, Zhai, 2008]. This has been challenged by both experimental results [Smucker and Allan, 2006, Momtazi et al., 2010, Puurula, 2013] and analyses of IDF compared to collection smoothing [Robertson, 2004]. In text classification and clustering, MNB has been combined with Term Frequency (TF) and IDF transforms (TFIDF) [Rennie et al., 2003, Kibriya et al., 2004, Pavlov et al., 2004, Frank and Bouckaert, 2006, Puurula, 2012b].\nVarious versions of both IDF and TF-IDF have been proposed for text data [Robertson and Jones, 1976, Salton and Buckley, 1988]. TF-IDF functions comprise three transforms: term count normalization, document length normalization and document frequency weighting. The first two are commonly performed with a combined function and form the TF-part of feature weighting. Document frequency weighting is usually considered an independent factor, and forms the IDF-part of TF-IDF. A combined TF-IDF feature transform is given by:\nwn = log(1 + w\u2032n |w\u2032|0 ) log I In , (4.9)\nwhere w\u2032 is the original unweighted word vector, and In is the number of collection documents having the word n.\nThe choice of log1+ transform for TF normalization can be justified as a correction to multinomials for better modeling the power-law distributions seen in text data [Rennie et al., 2003]. The use of \u201cL0 norm\u201d or the number of non-zeros in the word vector is called unique length normalization, and has been shown to be robust across datasets [Singhal et al., 1996]. The IDF factor log I In is called Robertson-Walker IDF [Robertson, 2004], and forms the most commonly used version of IDF.\n70\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nA large number of variations exist for each of the three components and for how they are combined. Term count normalization can be omitted, or use stronger damping [Singhal et al., 1998]. Length normalization can use L1 norm or L2 norm, and can be applied before or after term count normalization. Document frequency weighting by IDF can take a number of forms, one common variant being Croft-Harper IDF [Croft and Harper, 1979], which downweights common words more severely. Parameterized versions also exist, adding versatility to the transforms [Lee, 2007]. A generalized version of Equation 4.9 can be defined [Puurula, 2012b]:\nwn = log(1 + w\n\u2032 n\n|w\u2032|\u03c60 )\n|w|1\u2212\u03c60 log(max(1, \u03c5 +\nI\nIn )), (4.10)\nwhere \u03c6 controls length scaling and \u03c5 IDF lifting. \u03c6 = 1 performs length normalization before term count normalization, \u03c6 = 0 performs it after. Values 0 < \u03c6 < 1 produce smooth combinations of term count and length normalization, while \u03c6 > 1 and \u03c6 < 0 produce more extreme normalizations. \u03c5 = 0 produces Robertson-Walker IDF, while \u03c5 = \u22121 produces unsmoothed Croft-Harper IDF. Values \u03c5 > 1 produce weaker IDF normalizations, while \u03c5 < \u22121 produces stronger IDF normalizations."}, {"heading": "4.2.2 Methods for Fractional Counts with Multinomial Models", "text": "Transforming data is standard practice for correcting model assumptions in statistical modeling. This enables the use of well-understood simple models with complex data. Common transforms include flooring and ceiling values to accepted bounds, binning, log transforms, and standard score normalization. Feature transforms are less commonly used in text modeling since the models are defined on count data, and most normalizations of count data would produce fractional counts that are undefined for multinomial and categorical models [Juan and Ney, 2002, Vilar et al., 2004, Tam and Schultz, 2008, Bisani and Ney, 2008, Zhang and Chiang, 2014]. This means that the common use of TF-IDF with MNB produces models that are not well-defined in a probabilistic sense. However, there are a few methods in common use that allow fractional counts in restricted uses.\nFor inference uses, a method commonly used to weight words in ranking is the Kullback-Leibler (KL) divergence [Zhai and Lafferty, 2001c]. In ranking, KL-divergence is mostly used to incorporate feedback information into test documents, but it has also been used to integrate IDF weighting of words [Smucker and Allan, 2006, Momtazi et al., 2010, Momtazi, 2010]. In soft classification it has been used to correct differences in scores caused by varying document lengths [Craven et al., 2000, Schneider, 2005]. KL-divergence is a measure between two probability functions. For the case of two multinomial distributions pl(w) and p \u2032(w), the negative KL-divergence is:\n\u2212D(pl(w)||p\u2032(w)) = \u2212 \u2211 n p\u2032(n) log p\u2032(n) pl(n)\n= \u2211 n p\u2032(n) log pl(n)\u2212 \u2211 n p\u2032(n) log p\u2032(n) (4.11)\n71\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nThe second term \u2212\u2211n p\u2032(n) log p\u2032(n) is the entropy for model p\u2032(w). When KL-divergence is used for ranking or posterior scoring, the model p\u2032(w) is the test document or query model, and its entropy can be omitted since it has constant effect on each label model pl(w). If the model p \u2032(w) is estimated as the unsmoothed estimate p\u2032(w) = wn\u2211 n wn for a test document, then scoring by negative KL-divergence gives rank-\nequivalent scores to the posterior log-probabilities \u2212D(pl(w)||p\u2032(w)) rank= log(p(l)) \u2211 nwn log(pl(n)).\nKL-divergence thus provides a framework for generalizing posterior inferences p(l|w) by replacing the counts for a test document by model parameters. A common use is to incorporate pseudo-feedback information from the top ranked labels to the document [Zhai and Lafferty, 2001c], in order to rerank the document with the updated model. A more recent use is transforming features [Smucker and Allan, 2006, Momtazi et al., 2010], so that parameters are weighted and renormalized, according to a weighting such as Inverse Collection Frequency [Smucker and Allan, 2006, Momtazi et al., 2010] or IDF [Momtazi et al., 2010]. For example, using IDF would replace the test document model as p\u2032(n) = Z wn IDF (n), where Z is a normalization term and IDF (n) the IDF-weight of word n. A problem with the KL-divergence framework is that it is not probabilistic in a strict sense, since the KL-divergence scores are not probabilities. In addition, it cannot be used to incorporate feature transforms or normalizations in model estimation.\nA fully probabilistic alternative to KL-divergence is to define a model that directly uses feature weights. The query term weighting model has been proposed [Momtazi, 2010] for weighting MNB conditional prob-\nabilities, so that p(w|l, r) = Zr \u220f j pl(wj) rj , Zr is a document-dependent normalizer and rj is an arbitrary non-negative weight, for example rj = IDF (wj). This can be seen as a log-linear model [Darroch and Ratcliff, 1972], where the weights for each feature in the sequence are fixed. The same method was demonstrated earlier for the two-state categorical HMM models in IR, for the special case of weighting query sections, without considering normalization [Miller et al., 1999]. This method presents a simple modification that provides well-defined posterior probabilities for infererence. Like KL-divergence inference, this method cannot be used for model estimation.\nFor model estimation, static model interpolation [Stolcke, 2002] enables weighting of training data for n-gram LMs. This works identically to Jelinek-Mercer method used for smoothing, but combines weighted components from different training data sources. For example, using K component datasets with multinomial\nparameters pk(n) and weights p(k), the interpolated parameters would be p(n) = \u2211 k p(k)pk(n). Basic linear interpolation can be equally implemented by weighting and storing fractional counts from each dataset. Details of the interpolation can vary, and some smoothing heuristics such as the Kneser-Ney discounting estimate require integer count information [Ney et al., 1994, Zhang and Chiang, 2014]. In general this method allows integration of fractional counts in estimation, while maintaining the probabilistic framework.\n72\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES"}, {"heading": "4.2.3 Formalizing Feature Transforms and Fractional Counts with Probabilis-", "text": "A method that enables transformed features for both estimation and inference is the formalization of fractional counts as probabilistic data. Concurrent research in estimation of n-grams for machine translation has formalized fractional counts as expectations of counts, given a probability distribution over possible word sequences [Zhang and Chiang, 2014]. This method can be extended for estimation and inference of generative models in a variety of applications, as discussed next in detail.\nA weight sequence r = [r1, ..., rJ ] matching a word sequence w can be interpreted as probabilities of words occurring, similar to the distribution over hidden components provided by the EM-algorithm. Each weight rj indicates the probability of the corresponding word wj to have occurred in the data, so that the weights define a distribution over possible word sequences. A possible word sequence w\u0302 given w and r can be called a realization, a special case being a realization with no words w\u0302 = . A sequence of binary indicator variables r\u0302 called an occurrence sequence indicates a draw from distribution defined by the weight variables r. A realization can be generated by different occurrence sequences, and the mapping can be denoted w\u0302 = d(r\u0302,w).\nThe probability of an occurrence sequence r\u0302 can be computed by multiplying the weights: p(r\u0302|w, r) =\u220f j r r\u0302j j (1 \u2212 rj)|r\u0302j\u22121|. The probability of each realization p(w\u0302) can be computed by summing its occurrence\nsequences: p(w\u0302|w, r) = \u2211w\u0302=d(r\u0302,w) p(r\u0302|w, r). Assuming the words occur independently, the probabilities of counts p(wn = c) are distributed according to a Poisson-binomial distribution: p(wn = c) = \u2211 j:wj=n rj. The\nexpectations of count frequencies E( \u2211\nn:wn=c 1) can then be computed with a recursive algorithm [Zhang and\nChiang, 2014]. Table 4.3 summarizes these basic statistics that can be computed by treating weights for words as probabilities.\n73\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nThe method proposed by Zhang and Chiang [2014] uses the expectations of count frequencies E( \u2211\nn:wn=c 1)\nin place of the n1 and n2 statistics for the Kneser-Ney estimate, and the expected counts E(wn) in place of the counts wn. The resulting LMs maximize the expected conditional likelihood given the distribution over realizations, similarly to the two-state HMMs in Section 4.1.2. Let training data D for a multinomial distribution consist of word and weight sequences: D(i) = (w(i), r(i)). The expected conditional log-likelihood can be written:\nQ(\u03b8|D, \u03b8\u0302) = E(log(L(\u03b8|D, \u03b8\u0302))) = \u2211 i \u2211 w\u0302(i) p(w\u0302(i)|w(i), r(i)) log(p(w\u0302(i)))\n= \u2211 i \u2211 w\u0302(i) \u2211 w\u0302\n(i) j\np(w\u0302(i)|w(i), r(i)) log(p(w\u0302(i)j ))\n= \u2211 n E(C(n)|D) log(p(n)) (4.12)\nThis method formalizes the use of fractional counts for estimation, but contains one flaw. With higher order n-grams different realizations have different word histories, since omission of a word causes an n-gram history to skip a word. For example, with a realization w\u0302 = [93] for a weighted word sequence w = [92453], the subsequence [245] would not be realized, and an n-gram history for the last word w5 = 3 in the sequence would have to start with w1 = 9. Different realizations will yield different sets of n-gram histories, and assuming fixed histories would become increasingly incorrect with long word sequences and low weights rj. Correct estimation should take the differing histories into account. The method presented by Zhang and Chiang [2014] sidesteps this issue by allowing weighting only at the level of sentences. Nevertheless, experimental improvements are demonstrated from the expected Kneser-Ney smoothing [Zhang and Chiang, 2014].\nProbabilistic data can be applied equally for inference. The expectation of log-probability E(log(p(w)))\nequals the Q-function for a single document:\nE(log(p(w))) = \u2211 w\u0302 p(w\u0302|w, r) log(p(w\u0302))\n= \u2211 w\u0302 |w\u0302|\u2211 j=1 p(w\u0302|w, r) log(p(w\u0302j))\n= \u2211 j rj log(p(wj)) (4.13)\n74\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nThe expectation of probability E(p(w)) takes the form:\nE(p(w)) = \u2211 w\u0302 p(w\u0302|w, r)p(w\u0302)\n= \u2211 w\u0302 p(w\u0302|w, r) |w\u0302|\u220f j=1 p(w\u0302j)\n= \u2211 r\u0302 \u220f j p(r\u0302j) \u220f j p(wj) r\u0302j\n= \u2211 r\u0302 \u220f j p(r\u0302j)p(wj) r\u0302j (4.14)\nThese two are not necessarily equivalent, as demonstrated by Jensen\u2019s inequality: E(log(p(w))) \u2264 log(E(p(w))). It is not clear which one to use for inference. E(p(w)) seems to be the natural choice, since it equals the mean of the probability over the distribution. If a model is estimated using the Q-function of equation 4.13, then E(log(p(w))) is consistent with the estimation. E(log(p(w))) is trivially computed from the weighted counts, whereas E(p(w)) is more complicated, but can be computed using the forward algorithm from the word and weight sequences in time linear to the sequence length. For this thesis, we will use E(log(p(w))), since it gives results that equal the use of fractional counts in existing literature, while no results using E(p(w)) exist."}, {"heading": "4.3 Formalizing MNB as a Generative Directed Graphical Model", "text": "The present chapter has formalized the smoothing and feature weighting commonly used with MNB, but there are several omissions in the standard descriptions of MNB. It can be argued that the MNB model is not multinomial, naive, or Bayesian:\n1) MNB is usually not a Bayesian model since no distribution over parameters is kept, but rather a generative\nmodel using the Bayes rule for posterior inference.\n2) MNB is not a Naive Bayes model, since the conditional distribution is modeled by a single multinomial,\nnot by conditionally independent models for each feature.\n3) The conditional multinomials in MNB are in fact tied multinomials with parameters shared for all possible\ndocuments lengths, combined with a length-generating distribution such as Poisson.\n4) The conditional multinomials themselves have not been formalized correctly, but using a variety of smooth-\ning methods with no connection to maximum likelihood.\n5) Feature weighting such as TF-IDF has not been generally formalized for either the training or test set\ndocuments, despite being very commonly used with MNB.\n75\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nGiven the common misconceptions about MNB, it is useful to formalize it with a precise definition. We can redefine MNB as a generative model over sequences that factorizes into distributions for labels, word sequence lengths and 2-state HMMs for label-conditionals. Feature weighting can be formalized as estimation and inference using expectations of log likelihoods and log probabilities over probabilistic data. The joint probability for the model factorizes as:\np(w, l,k) = p(l)p(J)p(w,k| l, J) = p(l)p(J) \u220f j pl(kj)p u lkj (wj), (4.15)\nwhere the prior p(l) is categorical, the length generation factor p(J) is Poisson and label-conditionals p(w,k|l, J) are modeled by the 2-state HMMs with M = 2 and categoricals pl(kj) for each l, and plkj(wj) for each l and m.\nThe two-state HMM terms correspond to the original MNB terms: pulm=1(n) = p u l (n) for the labelconditional models, pulm=2(n) = p u(n) for the shared background model, and p(m = 2) = \u03b1l for the smoothing weight. Figure 4.2 shows the graphical model notation for the correctly formalized MNB.\nThe hidden states can be marginalized away:\np(w, l) = p(l)p(J) \u2211 k \u220f j (pl(kj)p u lkj (wj))\n= p(l)p(J) \u220f j \u2211 m (pl(m)p u lm(wj))\n76\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\n= p(l)p(J) \u220f n ((1\u2212 \u03b1l)pul (n) + \u03b1l pu(n))wn (4.16)\nThe label posterior p(l|w) given a word sequence becomes:\np(l|w) = p(w, l)\u2211 l\u2032 p(w, l \u2032)\n\u221d p(l) \u220f n ((1\u2212 \u03b1l)pul (n) + \u03b1l pu(n))wn (4.17)\nThe label posterior p(l|w) given a word vector becomes:\np(l|w) = p(l)p(J)Z(w)\n\u220f j \u2211 m(pl(m)p\nu lm(wj))\u2211\nl\u2032 p(l \u2032)p(J)Z(w) \u220f j \u2211 m(pl\u2032(m)p u l\u2032k(wj))\n\u221d p(l) \u220f n ((1\u2212 \u03b1l)pul (n) + \u03b1l pu(n))wn , (4.18)\nwhere Z(w) is the multinomial normalizer.\nThe word vectors can be used to compute exactly the same label posteriors, meaning that for posterior inference only the word vectors are required, and not the word sequence information. The joint can be marginalized to produce the probability of a word sequence:\np(w) = \u2211 l \u2211 k p(w, l,k)\n= p(J) \u2211 l p(l) \u2211 k pl(w,k)\n= p(J) \u2211 l p(l) \u220f n ((1\u2212 \u03b1l)pul (n) + \u03b1l pu(n))wn (4.19)\nFrom the marginal probabilities MNB can be seen as a mixture of label-conditional models [McCallum and Nigam, 1998, Novovicova and Malik, 2003, Nigam et al., 2006], where the label variables are known for each document in a training dataset. The MNB factorization is simplified by a number of independence assumptions. The length factor p(J) is assumed to be mutually independent with the other factors [Juan and Ney, 2002].\n77\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nThe training data consists of documents D(i) = (l(i),w(i),k(i)), where l(i) and w(i) are known, and k(i) is\nunknown. The likelihood function is:\nL(\u03b8|D) = p(D|\u03b8) = \u220f i p(l(i)|\u03c0)p(|w|(i)0 |\u03c7) \u2211 k(i) \u220f j pl(i)(k (i) j |\u03b1) pul(i)k(i)j (w (i) j |\u03bb) (4.20)\nTreating k as a random variable distributed according to a prior distribution pl(i)(k (i)|w(i), \u03b8\u0302), the condi-\ntional expectation of the log-likelihood becomes:\nQ(\u03b8|D, \u03b8\u0302) = E(log(L(\u03b8|D, \u03b8\u0302))) = \u2211 i (log(p(l(i)|\u03b8)) + log(p(|w|(i)0 |\u03b8))\n+ \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pl(i)(k(i)j |\u03b8) pul(i)k(i)j (w (i) j |\u03b8))) (4.21)\nThe maximization decouples into separate optimizations:\nargmax \u03b8 (Q(\u03b8|D, \u03b8\u0302))) = argmax (\u03c0,\u03bb,\u03b1,\u03c7) ( \u2211 i log(p(l(i)|\u03c0))\n+ \u2211 i log(p(|w|(i)0 |\u03c7))\n+ \u2211 i \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pl(i)(k(i)j |\u03b1))\n+ \u2211 i \u2211 k(i) \u2211 j pl(i)(k (i)|w(i), \u03b8\u0302) log(pu l(i)k (i) j (w (i) j |\u03bb))) (4.22)\nThe parameters \u03c0 and \u03c7 are invariant to the expectation over the hidden states, and are therefore exact maximum likelihood estimates. For many uses the parameters \u03c7 for word sequence lengths can be omitted, since these have no effect on the posterior probabilities. The parameters \u03bb and \u03b1 are expected log-likelihood estimates given pl(i)(k (i)|w(i), \u03b8\u0302), as discussed in Section 4.1.2. Choosing pl(i)(k(i)|w(i), \u03b8\u0302) and a form for the shared background model pu(n) implements smoothing.\nFeature weighting is incorporated by performing estimation and inference over probabilistic data, as described in Section 4.2.3. Given a probabilistic weight sequence r matching the word sequence w, probabilities can be approximated by the expectations of log-probabilities given r. For both inference and estimation, this reproduces the results that come from simply using fractional counts in algorithms instead of integer counts, as has been done with applications of MNB using weighted words. In Equations 4.16, 4.17, 4.18, and\n78\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\n4.19, approximation with expected log-probabilities replaces the integer vector w by the fractional expected counts E(w|r), that are provided by any chosen feature weighting function. Maximum expected log-likelihood estimation introduces the weight terms rj to Equations 4.21 and 4.22. The multinomials for the two-state HMM become weighted by both the distribution of occurring/non-occurring terms defined by r, and the assumed distribution over the HMM component assignments pl(i)(k (i)|w(i), \u03b8\u0302). The length-modeling factor p(|w|(i)0 |\u03c7)) should also take the weighted distribution over occurring word sequences into account, unless the length model is approximated by ignoring weights. Further derivation of Poisson length model estimation on expected sequence lengths is omitted, as the experiments conducted in the thesis evaluate length modeling separately to feature weighting."}, {"heading": "4.4 Extending MNB with Prior Scaling and Document Length", "text": "Modeling\nThe extension of MNB so far has formalized smoothing methods as a directed graphical model. Further useful extensions to MNB can be defined by modifying the graphical model factorization. Two such extensions are label-conditional document length modeling and scaling of the label prior.\nDocument lengths in MNB are assumed to be generated by a shared distribution such as Poisson [McCallum and Nigam, 1998, Blei et al., 2003], that can be omitted for most uses. Use of label-conditional distributions has been suggested in the literature [McCallum and Nigam, 1998], but has not experimented. Since document lengths are known to be among the strongest features for some tasks, such as automatic essay scoring [Larkey, 1998], explicit length modeling could prove useful.\nPrior scaling is applied to LMs in uses such as speech recognition, where a LM models the prior distribution of possible word sequences in a Naive Bayes framework [Gales and Young, 2007]. In these uses the prior has a very different scale than the conditional distribution, and a scaling factor is applied to match the contributions of the prior and conditional optimally. Prior scaling can be applied to TM tasks equally, but this has not been attempted.\nAn Extended MNB incorporating prior scaling and document length modeling can be defined as:\np(w, l,k) = p(l)p(J |l)p(w,k| l, J) = p(l)p(J |l) \u220f j pl(kj)p u lkj (wj), (4.23)\nwhere the label prior p(l) and length model p(J |l) are scaled and renormalized versions of the original distributions p\u2032(l) and p\u2032(J |l) : p(l) = Z(\u03d1) p\u2032(l)\u03d1 and p(J |l) = Z(\u03c2) p\u2032(J |l)\u03c2 . Z(\u03d1) and Z(\u03c2) normalize the factors to be probability distributions, and \u03d1 and \u03c2 are meta-parameters to be estimated on held-out data.\n79\nCHAPTER 4. REFORMALIZING MULTINOMIAL NAIVE BAYES\nThe Extended MNB model with fixed scaling factors \u03d1 = 1 and \u03c2 = 1 constitutes a directed generative model, and the corresponding graphical model is illustrated in Figure 4.3. With scaling weights other than 1 and 0, the parameter estimates of the scaled factors no longer derive from a directed factorization. One solution to formalize the model is to consider the combination of factors as a type of log-linear model combination [Klakow, 1998, Hinton, 2002, Bouchard and Triggs, 2004, Suzuki et al., 2007]:\np(w, l,k) = Z p\u2032(l)\u03d1 p\u2032(J |l)\u03c2 p(w,k| l, J) (4.24)\nWith the log-linear model of Equation 4.24, the factors for label priors, document lengths and labelconditional word distributions become feature functions combined using the log-linear weights \u03d1 and \u03c2. Maximum likelihood estimation of the model becomes complicated. One approximate solution is to keep the maximum likelihood estimates for the directed model, and directly optimize the new parameters \u03d1 and \u03c2 for a performance measure on held-out development data [Metzler and Croft, 2005]. This approximation is used for the experiments conducted in Chapter 6. It maintains the simplicity of directed models, while allowing the additional parameters to improve model performance.\n80\nChapter 5\nSparse Inference\nThis chapter proposes computation based on sparse model representations for scalable inference. This reduces the time and space complexity of inference for a variety of linear models and structural extensions. First a basic case of sparse posterior inference is derived for uses such as Multinomial Naive Bayes (MNB) classification, ranking and clustering. This is extended into a more general case of joint inference on hierarchically smoothed sequence models, and further into joint inference on mixtures of such models. Additional efficiency improvements for the inference are discussed, and a structural extension of MNB benefiting from sparse inference is proposed, called Tied Document Mixture."}, {"heading": "5.1 Basic Case: Sparse Posterior Inference", "text": "Once estimated, a generative model such as Multinomial Naive Bayes (MNB) can be used for several types of inference, depending on the task and application. The most common types are classification in text categorization, ranking in information retrieval, soft classification in document clustering, and model combination. All of these perform inference related to the posterior p(l|w). In classification, the most likely label to generate a document is selected: argmaxl p(l|w). In ranking, scores are computed for each label: y(\u03b8l,w) rank = p(l|w). In soft classification and model combination, the posterior is used directly.\nThe time and space complexity of the inference is crucial in many applications of MNB, since this determines the scalability of the model to large-scale tasks. The numbers of labels, words and training documents can exceed millions in many practical tasks involving web-scale text datasets. The required computation can be reduced depending on the type of inference. For example, for posterior inference the multinomial\nnormalizer can be omitted, since p(l|w) = Z p(l) \u220f n p(wn|l)\u2211\nl Z p(l) \u220f n p(wn|l) = p(l)\n\u220f n p(wn|l)\u2211\nl p(l) \u220f n p(wn|l) .\nA naive algorithm for performing posterior inference with MNB computes p(l|w) = p(l,w)\u2211 l\u2032 p(l \u2032,w) = Zp(l) \u220f n:wn 6=0 pl(n) wn\nfor each label l by computing the joint probabilities p(l,w) as a product over terms n for each label l, and normalizing by the sum over the joints Z = \u2211\nl\u2032 p(l \u2032,w). This has complexity O(|w|0L), and this is widely\nconsidered to be optimal for linear models [Manning et al., 2008]. However, the time complexity for posterior\n81\nCHAPTER 5. SPARSE INFERENCE\ninference can be substantially reduced by taking into account sparsity in the parameters. Earlier work with inverted indices has shown that classifier scores for Centroid classifier [Shanks et al., 2003] and K-nearest Neighbours [Yang, 1994] can be done as a function of sparsity. The posteriors for MNB with uniform priors can be computed similarly [Hiemstra, 1998, Zhai and Lafferty, 2001b], for both Jelinek-Mercer [Hiemstra, 1998] and other basic smoothing methods [Zhai and Lafferty, 2001b]. The sparse inference proposed here generalizes this inference to MNB with categorical priors, a variety of linear models and structural extensions. A basic sparse posterior inference algorithm for MNB is described next. It combines three techniques for efficient computation:\nLog-domain computation refers to use of log probability values, a standard practice with large-scale statis-\ntical models. This changes products into sums and exponents into products in the log-domain, which are much cheaper to compute with modern processors. For example, log(p(l)pl(w)) = log(p(l))+ log(pl(w)) and log(pl(n) wn) = log(pl(n))wn. Another benefit of log-computation is reducing numerical inaccuracy that comes from computing with small probabilities. Alternatives such as scaling [Rabiner, 1989] for correcting inaccuracy with large-scale models are less useful in general.\nPrecomputing organizes an algorithm and necessary data structures so that less computation is necessary\nwhen the algorithm is used. A basic sparse posterior inference algorithm computes first the smoothing distribution probabilities pu(n), and then updates these for each l by multiplying with (1\u2212\u03b1l)pul (n)+\u03b1lp u(n)\npu(n) .\nThe original parameter values pul (n) can be replaced by precomputed ones, since for computing p(l,w), pul (n) are only needed for updating p u(n) for each l. The parameters for p(l), pu(n), \u03b1l and p u l (n) can be replaced by the precomputed log values p\u2032(l) = log p(l), p\u2032u(n) = log(pu(n)), \u03b1\u2032l = log(\u03b1l), and p\u2032ul (n) = log((1\u2212 \u03b1l)pul (n) + \u03b1lpu(n))\u2212 \u03b1\u2032l \u2212 p\u2032u(n) for all pul (n) > 0.\nInverted index data structures have formed the core technique of information retrieval for the last decades\n[Zobel and Moffat, 2006]. An inverted index uses a vector \u03b6 of postings lists for each word, so that for each word a list \u03b6n of occurrence information can be accessed in constant time. For sparse inference with MNB, it is sufficient to maintain the label and parameter in the postings, so that each posting is a pair (l, p\u2032ul (n)). pl(n) can first be computed by p u(n), and updated for each l with pul (n) 6= 0. Using an inverted index, the set of parameters to update can be retrieved with a time complexity\nO(|w|0 + \u2211\nn \u2211 l:pul (n) 6=0 1) instead of O(L|w|0).\nA basic sparse posterior inference algorithm computes p\u2032u(w), updates this for all l with p\u2032(l) and p\u2032ul (w), and normalizes to obtain the posterior p(l|w). This has the time complexity O(L + |w|0 + \u2211\nn \u2211 l:pul (n) 6=0 1).\nUsing the precomputed values, we can compute joint probabilities as p(w, l) \u221d exp(p\u2032(l) + \u2211nwn(p\u2032u(n) + \u03b1\u2032l) + \u2211 n:pul (n)6=0 wnp \u2032u l (n))). The posterior can be computed by normalizing p(l|w) = p(w, l)/ \u2211 l\u2032 p(w, l \u2032). A key algorithmic idea here is that the conditional probabilities p(w|l) decompose into separately computed terms for the smoothing distribution, the smoothing weight, and the label-conditional: p(w|l) \u221d\n82\nCHAPTER 5. SPARSE INFERENCE exp( \u2211\nn:wn 6=0 p \u2032u(n) + |w|1\u03b1\u2032l + \u2211 n:pu(n)6=0 p \u2032u l (n)).\nAlgorithm 5.1 Sparse Posterior Inference for MNB\n1: smooth logprob = 0 2: for all n : wn 6= 0 do 3: smooth logprob+ = p\u2032u(n) \u2217 wn 4: for all l do 5: logprobsl = p\n\u2032(l) + |w|1\u03b1\u2032l + smooth logprob 6: for all n : wn 6= 0 do 7: for all (l, p\u2032ul (n)) \u2208 \u03b6n do 8: logprobsl+ = p \u2032u l (n) \u2217 wn\n9: normalizer = \u22121000000 10: for all l do 11: normalizer = log(exp(normalizer) + exp(logprobsl)) 12: for all l do 13: logprobsl\u2212 = normalizer\nreturn logprobs\nPseudo-code for the resulting sparse posterior inference algorithm returning p(l|w) is given in Algorithm 5.1. Although the sparse inference algorithm is described here for MNB posterior inference, it is applicable to any linear model. The inference algorithm with precompiled values corresponds to a sparsely computed\ndot product y(\u03b8,w) = \u03b80 + \u2211N n=1 \u03b8nwn, which is used for all linear classifiers, including Centroid, Perceptron, Logistic Regression and Support Vector Machine classifiers. Moreover, it can be generalized into structured models such as hierarchical mixture models. Therefore the textbook statement [Manning et al., 2008] on optimality of O(|w|0L) posterior inference is not only incorrect for MNB, but for machine learning methods in general."}, {"heading": "5.2 Extension to Joint Inference on Hierarchically Smoothed Se-", "text": "quence Models\nThe sparse inference in Algorithm 5.1 can be extended to hierarchically smoothed models, while retaining the same benefits in computational complexity. As discussed in Chapter 3, there are many cases where multinomials and MNB models are extended so that the probabilities in a sequence model back off to less context-dependent models. A variety of hierarchically smoothed sequence models are used with text, such as interpolated n-gram models. The smoothing hierarchy can come from sources such as word clusters [Zitouni and Zhou, 2008], the local word context [Chen and Goodman, 1999], or collection structure [McCallum and Nigam, 1999, Zhang et al., 2002, Krikon and Kurland, 2011]. Any combination of hierarchies can equally be used for backing-off. For example, a label-conditional passage bigram could first back-off to a label-conditional passage unigram, then to a label-conditional document unigram, and finally to a collection unigram model.\n83\nCHAPTER 5. SPARSE INFERENCE\nLet M denote the depth of hierarchical smoothing for a sequence extension of MNB, so that the labelconditional probabilities for the model are smoothed by the M\u22121 back-off layers in the hierarchy, where each node for layer m is a categorical distribution that is used in the smoothing. An example of this would be a label-conditional n-gram model, where M is the n-gram order. Node for layer m = 1 corresponds to the root node of the back-off hierarchy, m = M to the leaf node to be smoothed. The joint probability of a sequence w and label l for the model becomes:\np(w, l) = p(l) \u220f j pl(wj|w)\n= p(l) \u2211 k \u220f j pl(kj) p u lkj (wj|w)\n= p(l) \u220f j \u2211 m pl(m)p u lm(wj|w)\n= p(l) \u220f j \u2211 m ( M\u220f m\u2032=m+1 \u03b1lm\u2032 \u2212 M\u220f m\u2032=m \u03b1lm\u2032)p u lm(wj|w), (5.1)\nwhere pl(wj|w) is the smoothed label-conditional probability, pulm(wj|w) the component-conditional probabilities, and \u03b1lm the back-off weight of component m. The conditioning variables l, m and w for the probabilities can be extended to include more variables, or can be tied to reduce the number of variables. For a labelconditional n-gram these would be tied as pulm(wj|w) = pulm(wj|wj\u2212m+1...wj\u22121).\nSparsity can be utilized by storing the non-zero parameters for each node in a precomputed form, by first smoothing and then precomputing the parameters for each node. Smoothed parameters are computed first for the root node m = 1 : p\u2032\u2032ulm(wj|w) = pulm(wj|w), and then for each m > 1 up to to the leaf nodes: p\u2032\u2032ulm(wj|w) = \u03b1lmp\u2032\u2032ul(m\u22121)(wj|w) + (1 \u2212 \u03b1lm)pulm(wj|w). Precomputed smoothed log-parameters are computed starting from the leafs p\u2032ulm(wj|w) = log(p\u2032\u2032ulm(wj|w))\u2212 log(\u03b1lm p\u2032\u2032ul(m\u22121)(wj|w)) for the nodes m > 1, and finally for the root-node m = 1 : p\u2032ulm(wj|w) = log(p\u2032\u2032ulm(wj|w)). The smoothing weights and label priors can be precomputed: \u03b1\u2032lm = log(\u03b1lm) and p \u2032(l) = log(p(l)).\nWith the precomputed values, joint probabilities of sequences p(l|w, l) can be computed scalably by utilizing sparsity. The joint probability in Equation 5.1 can be expressed in a factorized form as:\np(w, l) = p(l) \u220f j \u2211 m ( M\u220f m\u2032=m+1 \u03b1lm\u2032 \u2212 M\u220f m\u2032=m \u03b1lm\u2032)p u lm(wj|w)\n= p(l) \u220f m \u03be(l,m)\n= exp(p\u2032(l) + \u2211 m \u03be\u2032(l,m))\n84\nCHAPTER 5. SPARSE INFERENCE\n\u03be\u2032(l,m) =  \u2211 j:pulm(wj |w)6=0 p\u2032ulm(wj|w), if m = 1\n|w| \u03b1\u2032lm + \u2211 j:pulm(wj |w)6=0 p\u2032ulm(wj|w), otherwise,\n(5.2)\nwhere \u03be(l,m) are factors for nodes (l,m) explained in the following, and \u03be\u2032(l,m)= log(\u03be(l,m)).\nEquation 5.2 can be solved directly using dynamic programming. The factors \u03be(l,m) provide updates to computing p(w|l) given its m \u2212 1 ancestor nodes. Starting from the root node, \u03be(l,m = 1) computes log p(w|l) assuming that no descendant nodes exist. This is then updated iteratively by the descendant nodes m > 1. The complexity of inference is reduced, because for each node only the non-zero unsmoothed parameters need to be considered, and these can be stored in postings lists retrieved from an inverted index.\nThe time complexity is reduced from the dense O(LMJ) to the sparse O(LM+|w|0+ \u2211\nl \u2211 m \u2211 j:pulm(wj |w)6=0 1).\nBy using the shared hierarchical components between l and the other conditioning variables, this complexity can be further reduced. Since in a hierarchy the auxiliary variable values \u03be(l,m) are the same for all children of a node, \u03be(l,m) needs to be computed only once for each shared node h = (l,m) in the hierarchy, and updated according to the children. Exact hierarchical computation reduces the complexity of computing\np(w, l) for all l from O(LM + |w|0 + \u2211\nl \u2211 m \u2211 j:pulm(wj |w)6=0 1) to O(L+ |w|0 + \u2211 h(1 + \u2211 j:puh(wj |w)6=0 1)).\nThe hierarchical complexity can be further reduced in the case of constrained models or approximation. Assume that p(l) are uniform in the following, and only the highest probability label is needed. If a node has no words that match the word sequence \u2200j : wj = 0\u2228pulm(wj|w) = 0, then its update will be \u03be\u2032(l,m) = |w|\u03b1\u2032lm. These nodes can be called the no-match nodes for a given word sequence. If Jelinek-Mercer smoothing is used for estimating \u03b1lm, the back-off weights are the same for all children and |w| \u03b1\u2032lm can be added directly to the parent node \u03be\u2032(l,m \u2212 1). Assuming matching nodes have no no-match ancestors, the sum \u2211m \u03be\u2032(l,m)) can be done only over the matching nodes m. Otherwise, a gap in the sum can be filled by computing \u03be(l,m \u2212 1) iteratively down to the first matching node. With the assumptions of no gaps in the hierarchy, Jelinek-Mercer for smoothing, and uniform p(l), it suffices to compute the maximum probability l by\nargmaxl p(w, l) = argmaxl \u2211 m \u03be(l,m), where the sums \u2211 m \u03be(l,m) can be done dynamically over the shared\nnodes h. The resulting time complexity is reduced to O(|w|0 + \u2211\nh \u2211 j:puh(wj |w)6=0 1), the sum of non-zero\nfeatures and matching nodes.\nFigure 5.1 illustrates the resulting three types of sparse inference algorithms using factor graph notation. There are several other cases where the hierarchical time complexity can be reduced using constraints and approximation. If Dirichet prior or discounting methods are used for estimating \u03b1lm, the back-off weights differ for the children. In this case the probabilities for the no-match children can be either computed exactly or approximated. A simple approximation is to precompute the mean back-off weight of the children for each node, and compute the probabilities for the no-match children using this mean weight. The priors p(l) can be approximated by 0 for the labels that have only no-match nodes in the smoothing mixture. Using these two weaker approximations for \u03b1lm and p(l), the same complexity reduction can be obtained by computing\n85\nCHAPTER 5. SPARSE INFERENCE\n86\nCHAPTER 5. SPARSE INFERENCE argmaxl p(w, l) = argmaxl \u2211 m \u03be(l,m) as before over the shared nodes. Other similar cases exist where the required computation is reduced by the number of no-match nodes. One practical case is ranked retrieval, where a set of top ranked labels for documents can be inferred given the input word sequence for a a query."}, {"heading": "5.3 Extension to Joint Inference on Mixtures of Sequence Models", "text": "The sparse inference discussed so far has mainly dealt with the scalable computation of hierarchically smoothed sequence models. It can be extended to cases where mixtures are defined over the sequence models, as well as to cases where mixtures are used both within and over sequences. A basic implementation for inference with mixtures over sequence models would multiply the inference time complexity by the number of components, but the inference complexity can be reduced again using sparsity, constraints and approximation.\nSimilar to the mixture model view of MNB, we can consider the previous example of Equation 5.1 as a\nmixture model over the label-conditional hierarchically smoothed sequence models:\np(w) = \u2211 l p(w, l)\np(w, l) = p(l) \u220f j \u2211 m ( M\u220f m\u2032=m+1 \u03b1lm\u2032 \u2212 M\u220f m\u2032=m \u03b1lm\u2032)p u lm(wj|w)\n= exp(p\u2032(l) + \u2211 m \u03be\u2032(l,m)) (5.3)\nLets assume a simple two-state HMM case of M = 2, where m = 2 is a label-independent background model and Jelinek-Mercer smoothing is used for selecting \u03b1\u2032lM . The factor for the smoothing background model becomes shared: \u2200l : \u03be\u2032(l,m = 2) = \u03be\u2032(1,m = 2). Given a model where most of the leaf nodes pulM(wj|w) are sparse, many of the labels l will have no-match leaf nodes (l,M), so that p(w|l) = exp(\u03be\u2032(l = 1,m = 2) + |w| \u03b1\u2032lM). Let L\u2032(w) indicate the number of no-match leaf nodes in Equation 5.3, l\u2032 labels with leaf nodes matching the word sequence \u2203j : wJ > 0 \u2227 pulm(wj|w) > 0, and \u03b1\u2032\u2032 the back-off weight for the leaf nodes. The marginalization can be computed as:\np(w) = L\u2032(w)(1\u2212 \u2211 l\u2032 p(l\u2032)) exp(\u03be\u2032(l = 1,m = 2) + |w|\u03b1\u2032\u2032) + \u2211 l\u2032 p(w, l), (5.4)\nreducing marginalization time complexity from O(L) to O(L\u2212 L\u2032(w)).\nFigure 5.2 illustrates the sparse marginalization using factor graphs. The complexity of marginalizations can be reduced in more complex cases as well, such as: different smoothing methods, deeper smoothing hierarchies, and multi-layer mixtures over the labels variables. If smoothing other than Jelinek-Mercer is\n87\nCHAPTER 5. SPARSE INFERENCE\nused, the back-off weights of the no-match children can be approximated by a mean value, or grouped in bins to reduce the approximation. If deeper smoothing hierarchies are used, the marginalizations can be conducted iteratively for each layer m from M to 1. If multi-layer mixtures over the label variables are used, the marginalizations can be conducted similarly by iterating from the leaf nodes to the root."}, {"heading": "5.4 Further Specialized Efficiency Improvements for Sparse In-", "text": "ference\nSparse inference can be made substantially more efficient for many uses. Combination with parallelization and stream processing is trivial, as subsets of the precomputed parameters p\u2032(l) and \u03be\u2032(l,m) in Equation 5.2 can be stored and processed by separate computing nodes, each node containing a shard of the full inverted index for the parameters. Aside from the generic methods for improving efficiency discussed earlier in Chapter 2, methods more specific to text mining can be applied. For ranking or classification only a subset of the labels is required, and therefore a number of further efficiency improvements are possible. These can be categorized as within-node pruning, between-node pruning, and search network minimization:\n88\nCHAPTER 5. SPARSE INFERENCE\nWithin-node Pruning Computing a factor score \u03be(l,m) can be halted if it is unlikely to affect the result.\nIf classification or ranking with top-scoring labels is required, a ranked list of the top-scoring labels and their scores can be maintained, and evaluation of each label can be terminated once it cannot reach the top labels. In information retrieval, heuristics using top-ranked lists form one of the main techniques for improving search efficiency, one notable algorithm being the MaxScore query evaluation algorithm [Strohman et al., 2005]. The postings lists can be sorted in an order such as decreasing magnitude of the parameters, enabling pruning of labels as early as possible. The inverted index can likewise be split into several indices, so that the apriori most likely labels are fully evaluated first; the labels in the following indices can be evaluated with much tighter pruning bounds.\nBetween-node Pruning Computing the factor score \u03be(l,m) can be avoided altogether, if it is likely (or\nbound) to have no effect on the result. If top-scoring labels are requested instead of the full posterior, the search can be organized hierarchically. Tree-based searches are commonly used with instance-based learning algorithms such as k-nearest neighbour classifiers. In tree-based search a hierarchy is constructed based on clustering label similarities, and a branch-and-bound search is used to retrieve the top labels [Ram and Gray, 2012]. Algorithm 5.1 can be extended into tree-based search by splitting the index into indices for each layer in a clustered label hierarchy, and skipping a branch in the evaluation if their higher-level node does not reach the current score bound. This performs between-node pruning, since nodes are discarded even before their scoring is considered. Hierarchical top-ranking could be combined with hierarchical smoothing either separately or together, so that the same hierarchy would be used both for smoothing models and for scoring the labels efficiently.\nSearch Network Minimization Search network minimization is used in complex graph search problems\nsuch as speech recognition systems [Aubert, 2002]. For example, Finite State Machines and Hidden Markov Models are minimized by combining nodes where possible. Examples of this would be merging all nodes with a single child with the child node, or approximating similar nodes with a single clustered node. Different types of minimizations of search networks can be used, if a graph of inverted indices is used for structured sparse inference."}, {"heading": "5.5 Tied Document Mixture: A Sparse Generative Model", "text": "The sparse inference algorithms described in this chapter provide improved scalability for a variety of structured models. Mixture variable nodes can be introduced at any structural level of a model, while incurring only modest increases in computational requirements. The nodes can be organized in layers, or form any type of a directed acyclic graph. The nodes can correspond to linguistic units, document structure, or any available metadata. Layers of nodes can include: subword units, phrases, sentences, passages, sections, fields, documents, labels, and labelsets.\n89\nCHAPTER 5. SPARSE INFERENCE\nConsidering MNB and multinomial models for text, the most practical inclusion for most modeling purposes is a document node. These models are commonly used on datasets stored in the form of word vectors, where other metadata is not available, has been discarded, or varies between tasks and datasets. Document identifiers, in contrast, are available in every application of these models. MNB and multinomial models are usually estimated by either averaging the document counts, or treating all documents as a single large document. Both types of modeling are unnatural in the sense that they assume the documents to be identically distributed for a given label. This is a strong assumption, and one that does not generally hold with text data [Puurula and Myaeng, 2013]. Explicitly modeling the documents avoids this problem.\nIntroducing a document node to MNB between the label and label conditional probabilities would in a general case increase the computational requirements considerably. Document mixture models [Novovicova and Malik, 2003, Nigam et al., 2006] introduce a mixture over documents and learn the soft assignments of documents to mixture components using the EM algorithm. This requires the estimation of the number of components, the component weights and the component-conditional probabilities. In general, optimizing the number of components has to be done by evaluation on development data, while optimizing the other introduced parameters with EM will produce a local maximum of the model likelihood. The overall estimation is therefore approximate, and the time complexity is multiplied by the number of restarts used to find the number of components and to avoid local minima.\nThe use of approximate estimation with iterative algorithms can be avoided by constraining the mixture over documents in a suitable way. One such way is to use a mixture with a component assigned for each document. This constrains the number of components to the number of documents for the label, the component assignments to the documents, and the component-conditional probabilities to document-conditional probabilities. This type of extension of MNB was proposed as Tied Document Mixture (TDM) [Puurula and Myaeng, 2013]. In addition to these modeling choices, TDM smoothes the document models hierarchically, and uses a uniform distribution over the document component weights.\nThe original version of TDM presented used simple hierarchical Jelinek-Mercer smoothing [Puurula and Myaeng, 2013]. With the theory of smoothing presented in Chapter 4, more refined smoothing methods can be attempted. The earlier version was also presented in word vector form, whereas here it can be presented in the word sequence form together with the corresponding directed graphical model. Formally the TDM model takes the form:\np(w, l) = p(l) \u2211 i\u2208Il pl(i) \u220f j pli(wj)\n= p(l) 1 |Il| \u2211 i\u2208Il \u220f j pli(wj)\n90\nCHAPTER 5. SPARSE INFERENCE\n= p(l) 1 |Il| \u2211 i\u2208Il \u220f j \u2211 m pli(m)plim(wj) = p(l) 1\n|Il| \u2211 i\u2208Il \u220f n \u2211 m pli(m)plim(n) wn , (5.5)\nwhere p(l), pl(i), pli(m) and plim(wj) are all categoricals, and Il indicates the set of documents corresponding to label l in the collection. Since a single document exists for each document indicator i, pl(i) = 1 |Il| .\nThe document models pli(wj) = \u2211 m pli(m)plim(wj) are hierarchically tied for smoothing with four levels of nodes: m = 1 uniform background model, m = 2 collection model, m = 3 label-conditional categoricals, and m = 4 document-conditional categoricals. The original TDM used hierarchically Jelinek-Mercer smoothed document models, with a uniform distribution for the root-level smoothing [Puurula and Myaeng, 2013]. An extension can be attempted so that both the document- and label-conditional models are smoothed with any of the methods described in Chapter 4. The document models are defined as:\npli(wj) = \u2211 m pli(m)p u lim(wj)\n=pli(m = 4)p u li(wj) + pl(m = 3)p u l (wj)\n+ p(m = 2)pu(wj) + p(m = 1)U(wj) (5.6)\nThe label-conditional and collection models are estimated by discounting and normalizing averaged nor-\nmalized document counts:\npul (wj = n) \u221d max(0, ( \u2211 i\u2208Il w (i) n |w(i)|1 )\u2212D(l, n)) (5.7)\npu(wj = n) \u221d \u2211 l max(0, ( \u2211 i w (i) n |w(i)|1 )\u2212D(l, n)), (5.8)\nwhere D(l, n) is the given by the chosen discounting method for the hierarchy level, if any.\nNormalized counts E(C(l, n)) = \u2211\ni w\n(i) n\n|w(i)|1 are treated as expected fractional counts for determining\nthe smoothing and discounting values. Length normalization is used for the background models, to reduce the effect of untypically long documents on mean statistics. The weights for the mixture components are computed dynamically from backoff-weights \u03b1 for the different levels in the hierarchy: pli(m) =\u220fM m\u2032=m+1 \u03b1lim\u2032\u2212 \u220fM m\u2032=m \u03b1lim\u2032 , where each \u03b1lim\u2032 is produced by the smoothing method, as described in Chapter 4.\nFigure 5.3 shows a directed graph for TDM. Estimating the smoothed model parameters has O( \u2211 i |w(i)|0)\n91\nCHAPTER 5. SPARSE INFERENCE\nwj\nkj\ni\nl\nj = 1..J\n\u2200i \u2208 Il l = 1..L\nFigure 5.3: Directed graph for the Tied Document Mixture\ntime and space complexity. Using sparse inference, time and space complexity of approximate hierarchical inference for classification is O(|w|0+ \u2211\nl \u2211 i\u2208Il \u2211 n:puli(n)6=0 1). Complexities of exact inference can be increased\na little by the smoothing and type of inference. Computing the prior probabilities p(l) exactly adds a L term to the inference complexities, and using discounting or Dirichlet prior smoothing exactly adds a L term for the label nodes and I for the document nodes.\nThe uniform distribution over document components can be seen as a type of kernel density model, where hierarchically smoothed multinomials are used instead of a Parzen kernel density using Gaussians [Parzen, 1962]. The hierarchical smoothing can be formalized as a mean-shift [Fukunaga and Hostetler, 1975] or data sharpening [Choi and Hall, 1999] method, which shifts the document-conditional models closer to labelconditional models. Under this view, the classifier is a form of a Kernel Density Classifier [Specht, 1988, John and Langley, 1995, Pe\u0301rez et al., 2009]. However, this kernel density formalization is complicated. The smoothed multinomial kernels are discrete, asymmetric, multivariate, bounded kernel functions, as well as local for each class. Each of these properties is treated as a deviation to a standard Parzen kernel density, and there is no known prior work on multinomial or class-smoothed local kernel densities.\n92\nChapter 6\nExperiments\nThis chapter presents a large-scale evaluation of the methods developed in the thesis, showing improvements in both efficiency and scalability. A unified experimental framework is proposed for evaluation of classification and ranked retrieval, as well as optimization of model parameters with Gaussian random searches on development data. The performance measures, baseline methods, and statistical significance measurement across collections are discussed. The set of text collections is described, along the chosen preprocessing, segmentation and statistics. Five sets of experiments are conducted, demonstrating considerable improvements from the methods developed in the thesis."}, {"heading": "6.1 Methodology", "text": ""}, {"heading": "6.1.1 Experimental Framework", "text": "Text mining (TM) applications are commonly decomposed into tasks solved using the methods of machine learning and statistics. The general task types are classification, ranking, clustering, regression and sequence labeling, of which the first two have been most extensively researched. Classification applications include spam classification, sentiment analysis, web page classification, and classification of documents into ontologies. Ranking is mostly applied for information retrieval (IR), where linear models implemented with inverted indices form the basis of modern web search engines. This chapter empirically explores a number of hypotheses drawn from the theory developed in Chapters 4 and 5. Experiments are conducted on ranking and classification datasets using the standard linear models for these tasks as baselines.\nThe experiments explore the following five research questions:\nCommon formalization for smoothing Chapter 4 formalized the various smoothing methods for gener-\native models of text. Does this common formalization improve the effectiveness the models used in the basic TM tasks?\nFeature weighting and Extended MNB Chapter 4 formalized weighting features for Multinomial Naive\n93\nCHAPTER 6. EXPERIMENTS\nBayes (MNB) and an extension of MNB that includes scaling of the prior and length modeling. Do these types of modifications provide improvements in effectiveness?\nStructured generative models Chapter 5 introduced models that add constrained mixture modeling struc-\nture into MNB. Do models of this type improve over MNB and Extended MNB models considerably?\nStrong linear models baselines Chapters 4 and 5 propose generative models in a common framework for\nTM tasks. Do the proposed models improve over strong baselines for these tasks?\nScalability of sparse inference Chapter 5 introduced the idea of performing scalable probabilistic infer-\nence with generative models using inverted indices. How scalable are generative models utilizing sparse inference, compared to discriminative linear models and generative models implemented without inverted indices?\nA common machine learning framework is applied to ranked retrieval and classification, illustrated in Figure 6.1. For both cases datasets are segmented into a development training set Dd, a development test set Dd\u2217, an evaluation training set De, and an evaluation test set De\u2217. The development sets are used for random search optimization of the meta-parameters, such as the smoothing parameters. The evaluation sets are used to produce performance measures for the evaluated models, which are then tested for statistically significant differences between the models across the datasets.\nWithin this framework, text classification and ad-hoc ranked retrieval tasks have fundamental differences only in how the datasets are organized. Classification operates on documents where both the training w and test set w\u2217 documents are distributed in a similar fashion. Ad-hoc ranked retrieval refers to test set documents as queries, and to training set documents simply as documents. The queries often form word vectors much shorter than the retrieved documents, consisting of only a few keywords, a sentence, or a title indicating the search intent. The labels for classification datasets are distributed similarly between the train and test documents. The labels for ranked retrieval are document identifier variables, each training document\n94\nCHAPTER 6. EXPERIMENTS\ncorresponding to a single unique identifier, whereas queries correspond to multiple document labels that have been judged relevant to the query. The labels for multi-label classification can be described as binary indi-\ncator vectors c, \u2200l : cl \u2208 (0, 1), constrained to \u2211 l cl = 1 for multi-class classification, and further L = 2 for binary-label classification. The labels for ranked retrieval can be described as either binary indicator vectors, or integer vectors when graded judgments are available for the queries. Aside from these fundamental differences in terminology and organization of data, ranked retrieval and text classification can be considered in the same experimental framework.\nMulti-label classification tasks are converted into multi-class problems by using the Label Powerset method [Boutell et al., 2004]. This maps each unique label vector seen in training data into a categorical labelset variable, and maps the labelset variables back into label vectors after classification. This simplifies model learning, but also increases the number of possible label variables in learning. Learning and optimizing metaparameters for discriminative models for the large-scale multi-label datasets used in the experiments is not computationally feasible within practical times with Label Powerset or other basic transformations of multilabel learning, and the results for these have not been computed."}, {"heading": "6.1.2 Performance Measures", "text": "There are several commonly used performance measures for both ranking and classification. Classification measures need to consider the unbalanced label distributions common with text data: most labels have few associated documents, while most documents are labeled with one of the most common labels. Ranking measures need to consider the priority of ranking the top ranked labels accurately, compared to ranking all labels accurately. For the experiments conducted in this thesis, Micro-averaged F-score (Micro-F1) is used for evaluating classification, and Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain of top 20 documents (NDCG@20) are used to evaluate ranking. These measures are described in the following.\nFor many classification tasks, F1-scores form the basis of the common evaluation measures. The F1-score is the harmonic mean of the precision and recall. Given binary label vectors of reference labels c and predicted labels c\u0302, precision is defined as the number of true positives TP divided by the number of predicted positives\nPP: Precision = TP/PP, where TP = \u2211 l:cl=1\u2227c\u0302l=1 1 and PP = \u2211 l c\u0302l. Recall is defined as TP divided by the\nnumber of reference positives RP: Recall = TP/RP, where RP = \u2211\nl cl. With these definitions, the F1-score\nfor a single test document can be defined as:\nF1 = 2 Recall \u00b7 Precision Recall + Precision\n(6.1)\nThe Recall, Precision and F1-score measures have been developed in the context of IR, where the measures are commonly averaged across queries to produce corresponding mean measures. Although Mean-F1 averaged\n95\nCHAPTER 6. EXPERIMENTS\nacross test label vectors c can be used for classification, the label imbalance in classification has made other types of averaged measures popular [Tsoumakas et al., 2010]. With macro-averaging a mean F1 is first computed for each label independently, and these are averaged to produce the Macro-F1 measure. With micro-averaging, the statistics of Recall and Precision are computed across the labels, and the Micro-F1 measure is then computed from the micro-averaged Recall and Precision statistics:\nMicro-F1 =2 Micro-recall \u00b7Micro-precision Micro-recall + Micro-precision\n(6.2)\nMicro-precision =\n\u2211 (c,c\u0302) \u2211 l:cl=1\u2227c\u0302l=1 1\u2211\n(c,c\u0302) \u2211 l c\u0302l\n(6.3)\nMicro-recall =\n\u2211 (c,c\u0302) \u2211 l:cl=1\u2227c\u0302l=1 1\u2211\n(c,c\u0302) \u2211 l cl\n(6.4)\nMicro-F1 is one of the most commonly used measures for multi-label text classification. Unlike Micro-F1, Macro-F1 is strongly affected by label imbalance: a label occurring a thousand times has the same weight as one that occurs a single time. Micro-F1 for binary-label and multi-class tasks is also equivalent to mean Accuracy: the proportion of correct classifications [Manning et al., 2008]. This makes Micro-F1 scores for these tasks directly comparable with much of the earlier research literature.\nPrecision forms the basis for MAP, the most common measure used for evaluating ranking in IR systems. MAP is computed as a mean of averaged Precision values over possible levels of Recall. Let y\u0302 denote a vector of binary relevance decisions from the ranked and sorted scores for predicted labels l, and y a matching vector of binary reference scores. MAP is computed as:\nMAP = 1 |D\u2217| \u2211 (y,y\u0302) 1\u2211 k yk L\u2211 k=1 y\u0302kPrecision(k,y) (6.5)\nPrecision(k,y) =\n\u2211k k\u2032=1 yk\u2032\nk (6.6)\nIt can be seen that MAP ignores the actual predicted scores y\u0302, and uses only binary reference labels. MAP has since been supplemented with more sophisticated measures, such as the NDCG [Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002, Wang et al., 2013]. For NDCG the sorted reference scores y can take graded values. Standard NDCG@k is computed as:\nNDCG(k) = 1 |D\u2217| \u2211 (y,y\u0302) Z(k) k\u2211 k\u2032 2yk\u2032 \u2212 1 log2(k \u2032 + 1) (6.7)\n96\nCHAPTER 6. EXPERIMENTS\nZ(k) = max y\u2032 ( k\u2211 k\u2032 2y \u2032 k\u2032 \u2212 1 log2(k \u2032 + 1) ) (6.8)\nThe normalizer Z(k) is the maximum of possible Discounted Cumulative Gains (DCG), giving the DCG for the best possible ranking with k ranked labels, and normalizing NDCG for each test document to a maximum score of 1. Smaller values of k are more effective in discriminating against ranking functions, but become less robust [Wang et al., 2013]. Larger values of k are less brittle, but become less effective in discriminating ranking function performance. The value k = 20 is used in the ranking experiments of this thesis.\nAbsolute differences in basic measures such as Micro-F1, NDCG@20 and MAP are affected by both the baseline model and the dataset. An improvement over a weak baseline or an easy dataset gives larger absolute differences than improvements over strong baselines and hard datasets. In text classification Micro-F1 is close to the perfect score of 1.0 for some tasks, whereas in text retrieval the reverse is the case, with MAP and NDCG results usually below 0.5.\nThe differences in Micro-F1 are further explored using Relative Error Reduction (RER), which is used in the evaluation of multinomial language models (LM) in the context of speech recognition and machine translation [Olive et al., 2011]. The RER can be defined as 1 \u2212 (fmax \u2212 f s2)/(fmax \u2212 f s1), where fmax is the maximum score, f s1 the score for baseline model, and f s2 the score for the new model. Differences in NDCG@20 and MAP are further explored using Relative Improvement (RI), defined as 1 \u2212 f s2/f s1 . The derived measure RI is often used implicitly for describing the differences in IR ranking performance [Zaragoza et al., 2007, Metzler et al., 2009]. Compared to absolute differences, relative differences are often more stable across varying baselines and datasets, and can give more intuitive measures of improvement."}, {"heading": "6.1.3 Baseline Methods", "text": "Many of the leading solutions for TM tasks are instances of linear models, including those for classification and ranking tasks. For classification, linear classifiers such as MNB, Support Vector Machines (SVM), and Logistic Regression (LR) have become the most common solutions. For ranking, the linear models Vector Space Model (VSM), Best Match 25 (BM25), and LM are the standard methods for ranked text retrieval in IR. The connections between linear models for TM were discussed in Chapter 2, and Chapters 3 and 4 examined the MNB model in detail, showing that a common framework for generative models covers the LMs used for ranking as a special case of MNB. The experiments conducted in this chapter use SVM, LR and MNB as baselines for classification, and BM25, VSM and MNB as baselines for ranking.\n97\nCHAPTER 6. EXPERIMENTS\nAs discussed in Chapter 2, the multi-class linear scoring function takes the form:\ny(\u03b8l,w) = \u03b8l0 + N\u2211 n=1 \u03b8lnwn (6.9)\nThe following gives the parameter estimates for each of the baseline models. For most models, results with and without Term Frequency - Inverse Document Frequency (TF-IDF) feature weighting are provided. TF-IDF modifies the training and test documents by applying the generalized TF-IDF presented in Chapter 4: wn = log(1 + w\u2032n |w\u2032|\u03c60 )/|w|1\u2212\u03c60 log(max(1, \u03c5 + IIn )), where w \u2032 n are the original counts, \u03c6 is the parameter for length scaling and \u03c5 is the parameter for IDF lifting. Depending on the model, these parameters are fixed or optimized. With fixed parameters, \u03c6 = 0, and length normalization is done after log normalization of counts. The fixed value \u03c5 = 0 is used for classification and ranking, producing Robertson-Walker IDF, whereas \u03c5 = \u22121 is used for the scalability experiments, producing unsmoothed Croft-Harper IDF and sparsifying the feature vectors to improve scalability.\nMNB and LM are generative probabilistic models based on a multinomial or first-order Markov chain distribution of words conditional on each label variable. An unsmoothed MNB or LM baseline model would\nhave the parameter estimates \u03b8ln = log \u2211 i:li=l w (i) n\u2211\ni \u2211 n w (i) n , while the bias is the log prior probability \u03b8l0 = log p(l) for\nMNB classification and uniform in the case of LM retrieval. The Dirichlet prior and Jelinek-Mercer smoothed MNB/LM models are used as basic baselines for classification and ranking. These modify the unsmoothed log label-conditional probability parameters \u03b8ln as described in Chapter 4. For both classification and ranking these baselines are compared with the proposed combinations of smoothing methods and extended generative models.\nLR and SVM are discriminative linear classifiers, estimated with iterative algorithms that optimize a chosen loss function. With LR the loss function is derived from an underlying discriminative probabilistic model, whereas with SVM the loss function is non-probabilistic. L1 and L2-regularization with L2-SVM and LR are used as baselines for classification. Regularized LR and SVM models estimate the parameters \u03b8ln by\noptimizing \u03b8ln = min\u03b8 R(\u03b8)+C \u2211 i L(\u03b8, D (i)), where R(\u03b8) is the regularization, L(\u03b8, D(i)) is the cost function, and C is the regularization parameter. The loss function L(\u03b8, D(i)) for L2-SVM is max(0, 1 \u2212 l(i)\u03b8Tw(i))2, and log(1 + \u2212l\n(i)\u03b8Tw(i)) for LR. L1-regularization adds the term R(\u03b8) = |\u03b8|1, while L2-regularization adds the term R(\u03b8) = 1\n2 |\u03b8|22. Here L2-SVM is optimized using a coordinate descent algorithm [Hsieh et al., 2008], and\nLR is optimized using a trust region Newton method [Lin et al., 2008].\nThe VSM and BM25 methods used for ranking have a number of variants. VSM was initially defined as a cosine distance on word vector spaces [Rocchio, 1971], and this was later improved by applying TF-IDF feature transforms to training and test documents. Both of these basic models continue to be used in ad-\ndition to more developed versions. The VSM models have the parameter estimates \u03b8ln = \u2211 i:li=l w (i) n\u221a\u2211N\nn\u2032 ( \u2211 i:li=l w (i) n\u2032 ) 2 ,\n98\nCHAPTER 6. EXPERIMENTS\nand the test document vector is L2-normalized: wn = w\u2032n |w\u2032|2 , where w \u2032 is the un-normalized vector. BM25 has been derived as an approximation to a probabilistic model, combined with a soft length normalization of counts [Robertson and Zaragoza, 2009]. The BM25 parameter estimates are \u03b8ln = IDF (n) (k1+1)w (i=l) n\nLN(i)+w (i=l) n\n,\nand the test document counts are normalized wn = (k3+1)w\u2032n k3+w\u2032n , where w\u2032n are the original un-normalized counts [Manning et al., 2008, Robertson and Zaragoza, 2009]. The training document length normalization is given\nby LN(i) = k1((1\u2212 b) + b|w(i=l)|1/A), with the average length A = \u2211\ni |w(i)|1/I. The IDF for this standard BM25 is given by the smoothed Croft-Harper IDF (n) = (I \u2212 In + 0.5)/(In + 0.5) [Manning et al., 2008]. BM25 does not benefit from a further TF-IDF feature transform, as it includes an IDF function and has implicit count and document length normalization."}, {"heading": "6.1.4 Parameter Optimization", "text": "The meta-parameters required by the models and TF-IDF are optimized on a development set for each dataset. A common practice is to either use a grid search of parameter estimates, or heuristic values [Robertson and Zaragoza, 2009]. Neither of these is guaranteed to provide optimal performance, and the results produced by unoptimized models can be misleading. The experiments shown in this thesis use random search optimization of the meta-parameters, an approach that makes few assumptions about the optimized function and is efficient for the small-dimensional optimization problems encountered when optimizing TM linear model meta-parameters.\nGrid search works by defining a grid of permissible parameter ranges (minq, maxq) with small constant steps \u2206q for each meta-parameter q, such as increments of 0.1 from 0.0 to 1.0. There are two main problems with this. First, the number of points to sample in the parameter space is an exponential function the dimension Q of the parameter vector. With Q = 2 parameters, a grid with range from 0.0 to 1.0 and increments of 0.1 would require evaluation of 112 points, while with Q = 5 parameters the number of points would increase to 115 = 161051. This makes grid search efficient only when there are few parameters. Second, if the steps for any of the parameters do not cover the optimum value, the optimization fails. With new models and data the permissible ranges and steps are not well known, and grid search can miss the optimal values.\nDirect search optimization [Powell, 1998], also known as metaheuristics [Luke, 2009] and black-box optimization, is a more complex method of parameter optimization. This seeks to optimize a function f using a limited number of point evaluations f(a), when very little is known about the properties of the function, such as smoothness, unimodality or convexity. Direct search problems of different types are encountered in a number of scientific disciplines, and as a result hundreds of methods have been extensively investigated. Some commonly known cases are genetic algorithms and simulated annealing [Luke, 2009].\nRandom search [Favreau and Franks, 1958, White, 1971] offers a type of direct search algorithm that is well suited for the small-dimensional, non-smooth and multi-modal functions encountered with the linear models in TM. A random search operates by improving the currently best point a by randomly generating\n99\nCHAPTER 6. EXPERIMENTS\nnew points d = a+ \u2206 with steps \u2206, bounding the points within the permissible ranges maxq and minq, and replacing a by d if the new point is good or better, i.e. f(d) \u2265 f(a). Generating the steps by a Gaussian distribution produces the Random Optimization algorithm [Matyas, 1965], which can be improved by several commonly used heuristics:\n\u2022 Decreasing step sizes The step sizes can be gradually reduced by modifying the variance of the Gaussian distribution. The\nvariance for each parameter can be initialized to be half the permissible range 0.5 \u00b7 (maxq \u2212minq), and multiplied by 0.9 after each iteration. This produces a log-curve decrease in step sizes, sampling most of the permissible ranges initially and searching locally later\n\u2022 Multiple parallel steps The point evaluation can be parallelized, evaluating a subiteration of Q points simultaneously and\nchoosing the best point dq = a + \u2206q as the new best point. This enables direct use of multiple processors for optimization, without any parallelization overhead\n\u2022 Multiple best points In case of a tie, the best X points from each subiteration can be used to replace the current best point,\nand sampling can be done uniformly from the this set ax: dq = aq%X + \u2206q. This enables the search to spread out to X different locations, in case a plateau is reached\nAll of the above modifications are common variations to random searches, and generally improve search efficiency without introducing additional flaws into random search, such as vulnerability to local optima. A parallelized random search of 50 iterations and 8 subiterations can be denoted a 50x8 random search. Figure 6.2 visualizes the evaluated points for a 40x40 search using this Gaussian random search. The models for retrieval were optimized with 50x8 searches, the models for classification with 40x40 searches. LR and SVM models were optimized with 40x8 searches, due to longer estimation times and the use of only up to three meta-parameters. For all model types, the random searches were iterated a few times with different permissible ranges."}, {"heading": "6.1.5 Significance Tests", "text": "The parameters optimized on the development sets are used for measuring performance on the evaluation sets. Comparison of performance can be done within a dataset or between datasets. Within-dataset comparisons are more common when a limited number of datasets are available, as used to be the case in IR [Hull, 1993, Sanderson and Zobel, 2005, Smucker et al., 2007, Cormack and Lynam, 2007, Smucker et al., 2009]. Across-dataset comparisons are more common in fields where standardized datasets are publicly available, as is the case in machine learning [Dietterich, 1998, Dems\u030car, 2006]. These have the important advantage of measuring performance on a group of datasets, avoiding problems encountered when using folds of the same dataset for testing significance [Dems\u030car, 2006]. If the chosen datasets are distributed in the same way as a\n100\nCHAPTER 6. EXPERIMENTS\n101\nCHAPTER 6. EXPERIMENTS\ntypical dataset for the task, then the discovered effects will hold on new datasets of the same task. Here a number of datasets have been segmented for both classification and ranking, and the methods are compared across the datasets.\nStatistical significance of the evaluation set performance measures is assessed by performing one-tailed paired t-tests on the absolute between-dataset differences. The paired Student\u2019s t-test [Gosset, 1908] is a basic test for comparing differences, recently advocated for evaluation of IR results [Sanderson and Zobel, 2005, Smucker et al., 2007, Cormack and Lynam, 2007, Smucker et al., 2009]. It compares the means and variances of two groups of observations, and assumes both groups have a Gaussian distribution. The null-hypothesis for a t-test posits that the difference between the means of the two groups is the result of the Gaussian variance. If the difference exceeds what the variance allows, it is considered to be statistically significant. Significance is computed by comparing the t-value of the test to a t-distribution, and discarding the null-hypothesis if the t-statistic deviates too far from the t-distribution for the chosen p-value of statistical significance.\nA paired t-test compares observations from matched pairs, so that instead of the difference between means, the mean of the paired differences is compared. This makes the test more powerful, because the variance caused by datasets is subtracted from the comparison. A one-tailed t-test compares only the difference in the t-statistic in a single direction, instead of deviation in both directions. This makes the test twice as powerful in p-value, with the prior assumption that variance in one direction will not be significant. Paired one-tailed t-tests are conducted with two significance levels on the absolute differences in the measures: 0.005 (\u2020) and 0.05 (\u2021).\nA problem with large-scale statistical testing is that the risk of false positive results is multiplied by the number of conducted comparisons. Modifying test statistics to penalize for the number of comparisons in turn weakens the significances of the individual comparisons. The strategy adopted here is to constrain significance tests only on the differences that attempt to answer the research questions set out in advance, at the beginning of the chapter. This both simplifies the description of findings, and reduces the risk of false positives. Nevertheless, the datasets in question have been used to iteratively develop the models, and a prior literature exists on the performance of the baseline linear models on these datasets. For these reasons the experiments and significance tests are exploratory, and confirmatory evaluation of the discovered effects is left for future research on new datasets. For further exploration and confirmation of the findings, the full evaluation set results are provided as tables in Appendix A.\n102\nCHAPTER 6. EXPERIMENTS"}, {"heading": "6.2 Datasets", "text": ""}, {"heading": "6.2.1 Dataset Overview", "text": "A total of 13 datasets are used for ranking and 16 for classification. Additional experiments are conducted on a large Wikipedia dataset, which allows the scalability of linear classifiers to be assessed when the numbers of documents, features and labels are each scaled up to a million. Aside from the TREC1-8 datasets for ranking, all datasets are publicly available for research use, and the pre-processing scripts and pre-processed datasets are made available1.\nThe ranking datasets consist of the TREC 1-82 collections split according to data source into 11 datasets, OHSU-TREC3 [Hersh et al., 1994, Robertson and Hull, 2001] and FIRE 2008-20114 datasets. TREC1-8 [Voorhees and Harman, 1999] contains the ad-hoc retrieval collections that were used in the 1990s to establish modern ranking functions and performance measures, most notably the BM25 ranking function [Robertson et al., 1996] and the MAP evaluation measure [Voorhees and Harman, 1999]. TREC was followed by other programs of for evaluating IR technology, such as NTCIR, INEX and FIRE. While the TREC ad-hoc track was suspended in 1999 in favor of more diverse tasks, some of these can be considered collections for ad-hoc ranked retrieval. The OHSU-TREC collection is a publicly available dataset of medical articles from PubMed, used for TREC9. The FIRE 2008-2011 English collections were constructed to evaluate IR in the major languages spoken in India, following the TREC ad-hoc evaluation paradigm.\nSix binary-label, five multi-class, and five multi-label datasets are used. The binary-label datasets are TREC065 [Cormack, 2006], ECUE1 and ECUE26 [Delany et al., 2006] for spam classification, and ACLIMDB7 [Maas et al., 2011], TripAdvisor128 [Bespalov et al., 2012], and Amazon128 [Blitzer et al., 2007] for sentiment analysis. These have been made available in the last 10 years, during which period spam classification and sentiment analysis became popular topics. The multi-class datasets are R8, R52, WebKb, 20Ng and Cade9 [Cardoso-Cachopo, 2007]. These are older datasets, and versions of the first four provided the benchmarks used for early comparisons of text classification algorithms [Lewis, 1992, Joachims, 1998, McCallum and Nigam, 1998]. The multi-label datasets, which are more recent and large-scale, are RCV1-v2Ind10, EUR-Lex11 [Menc\u0301\u0131a and Fu\u0308rnkranz, 2010], OHSU-TREC12 [Robertson and Hull, 2001], DMOZ213 and WikipMed213.\n1http://www.cs.waikato.ac.nz/\u02dcasp12/ 2http://trec.nist.gov/data/test coll.html 3http://trec.nist.gov/data/t9 filtering.html 4http://www.isical.ac.in/\u02dcclia/ 5http://plg.uwaterloo.ca/\u02dcgvcormac/treccorpus06/ 6http://www.dit.ie/computing/staff/sjdelany/Dataset.htm 7http://ai.stanford.edu/\u02dcamaas/data/sentiment/ 8http://www.cs.virginia.edu/yanjun/paperA14/ecml12-cikm11-deepSC.htm 9http://web.ist.utl.pt/\u02dcacardoso/datasets/\n10http://www.daviddlewis.com/resources/testcollections/rcv1/ 11http://www.ke.tu-darmstadt.de/resources/eurlex 12http://trec.nist.gov/data/t9 filtering.html 13http://lshtc.iit.demokritos.gr\n103\nCHAPTER 6. EXPERIMENTS\nThe large-scale Wikipedia dataset WikipLarge13 was used in the Large Scale Hierarchical Text Classification (LSHTC) evaluation of scalable multi-label classification of articles into the Wikipedia categorization hierarchy. It has word vector features with millions of words, documents and labels. The scalable probabilistic models developed in this thesis were used to win the 2014 LSHTC competition [Puurula et al., 2014] on this data. For the systematic comparisons of scalability, the original dataset is pruned in terms of features, documents and labels, as described in the following subsections."}, {"heading": "6.2.2 Preprocessing", "text": "Datasets were pre-processed from the original form into word vectors; however, some datasets are provided in word vector forms, making it impossible to perform identical processing steps. In all cases text was first lowercased. Next, stop-words were removed on most datasets, as well as short words (< 3 characters) and long words (> 20 characters). This was followed by Porter-stemming [Porter, 1980] of the remaining words. The scripts used for both pre-processing and segmentation are publicly available14.\nThe binary-label datasets ecue1 and ecue2 are provided as pre-processed integer word vectors. Stop-word removal or stemming is not performed on the text [Delany et al., 2006], but documentation for any other pre-processing is not available. The trec06 dataset is provided as raw emails, including the metadata header. The header was removed, the remaining text was Porter-stemmed, and stop-words, short words, long words, non-words, and numbers were removed. The aclimdb dataset is provided in pre-processed form, in lower-case with numbers removed, but without stemming or removal of stop-words or non-words [Delany et al., 2006]. Opinion grades 1-4 and 1-7 were mapped to negative and positive labels, respectively [Maas et al., 2011]. The tripa12 and amazon12 datasets are provided in pre-processed form, with numbers replaced by \u201cNUMBER\u201d, but without stemming, stop-word, short-word or non-word removal. Opinion grades 1-2 were mapped to negative label, 4-5 to positive [Bespalov et al., 2012].\nThe single-label datasets 20ng, cade, r52, r8, and webkb are provided as pre-processed word vectors. These use Porter-stemming, removal of 524 SMART stop-words, removal of short and truncation of long words [Cardoso-Cachopo, 2007]. Where available, titles of documents are concatenated to text bodies. Here no further processing was done aside from format conversion of the files.\nThe multi-label dataset rcv1 is provided as pre-processed word vectors, with punctuation removal, Porterstemming and SMART stop word removal [Lewis et al., 2004]. The eurlex dataset is provided with lowercasing, Porter-stemming, stop-word removal and number removal [Menc\u0301\u0131a and Fu\u0308rnkranz, 2010]. The ohsu-trec dataset is provided in the original OHSU-MED format [Hersh et al., 1994]. Here we use the MEDLINE subject field as labels, and concatenate the title and description fields to form the word vectors, with Porter-stemming and short word removal as pre-processing. The DMOZ2, wikip med2 and wikip large datasets are provided\n14http://www.cs.waikato.ac.nz/\u02dcasp12/\n104\nCHAPTER 6. EXPERIMENTS\nin pre-processed word vector forms, and no further pre-processing was made.\nThe TREC1-8 collections \u201ctrec *\u201d for ranked retrieval were pre-processed by stop-word, xml-tag, nonword, number, and short word removal, followed by Porter-stemming. For queries the description fields of queries were used to form the query word vectors, and relevance judgements were converted into binary label vectors of relevant document identifiers. The fire en dataset was processed identically to the TREC datasets. For ohsu trec the pre-processing was performed identically, but the queries were concatenated from title and description fields, and the provided graded relevance judgments were preserved as integer-weighted label vectors with relevance judgement grades 0, 1 and 2."}, {"heading": "6.2.3 Segmentation", "text": "The segmentation scripts first partitioned the datasets into a development set for optimizing parameters and an evaluation set for computing the performance measures of the optimized models. Dataset-dependent segmentation choices were made to make the parameter optimization reliable, and keep processing complexity within practical bounds. Pre-existing dataset partitions were used where available. Otherwise random sampling was used to further segment the data. All datasets were mapped into the same framework of segmentation, with the development sets used for optimizing parameters and the evaluation sets used for conducting the evaluated experiment results. Both sets were further divided into a training sets (Dd, De) for learning the linear models, and test sets (Dd\u2217, De\u2217) for measuring performance.\nMany of the classification datasets are provided with existing development and evaluation partitions. The original partitions for ohsu-trec and rcv1 were swapped, as this provided more data for learning models. The evaluation training set for all classification datasets was concatenated from the development training and test sets, while the evaluation test set composed of a held-out portion. The single-label and binary-label datasets ecue1 and ecue2 had insufficient data to form reliable development sets. For these datasets, 5-fold cross-validation over the development partition was used, with 200 documents preserved for each fold as the development test set, and the rest as the development train set. The classification datasets amazon12, rcv1, ohsu-trec, wikip med2, and DMOZ2 had the lowest and highest document frequency words pruned, to reduce the total number of counts to 8 million per dataset, enabling efficient experimentation with less memory use.\nThe TREC datasets are commonly divided by combinations of data source, year, TIPSTER disk, and query number to form smaller segments for experiments. Here the datasets were segmented by data source to form the 11 trec * datasets. The trec * and fire en datasets were further segmented according to queries to form the development and evaluation sets, so that the first 20 queries from each year were concatenated to form the development test set, and the remaining 30 queries from each year were concatenated to form the evaluation test set. The ohsu trec dataset was segmented according to the existing document partition, reserving ohsumed.87 for development and ohsumed.88-91 for evaluation. For all of the retrieval datasets, documents not given a relevance judgement for any of the queries in the test set were removed from the\n105\nCHAPTER 6. EXPERIMENTS\ntraining set, greatly improving the efficiency for performing experiments.\nThe wikip large dataset was segmented different from the general framework. The original training dataset provided for LSHTC4 was segmented by random sampling to reserve 1% of the data as an evaluation test set, and the remaining 99% was used as the evaluation training set. These evaluation sets were then further pruned by documents, features and labelsets so that each of these dimensions scaled up to a million. Documents were pruned in the order they occurred in a shuffled training dataset, with the number of preserved documents varied with the thresholds (10, 100, 1000, 10000, 100000, 1000000). Features were pruned to preserve the most frequent words, with the number or preserved features varied with the thresholds (10, 100, 1000, 10000, 100000, 1000000). Labelsets were similarly pruned to preserve the most common labelsets with the thresholds (1, 10, 1000, 1000000). Overall, these pruning choices resulted in 144 pruned versions of wikip large for testing scalability of the models. The scalability experiments were then conducted on the evaluation sets using fixed parameters, and no development sets were constructed for optimization."}, {"heading": "6.2.4 Dataset Statistics", "text": "The common framework for classification and ranking tasks enables direct comparison of the dataset properties. Table 6.1 shows the basic dataset statistics of numbers of documents, features and labels for the development and evaluation sets. Table 6.2 shows the mean numbers of features and labels per document. For the datasets that use 5-fold cross-validation for development, the first fold is used to compute the statistics for the tables. For the retrieval datasets, label variables for labeled non-relevant documents are not included for showing the number of document labels per query, as these are assumed to be as relevant as non-labeled documents.\nWhen compared under the same framework, it can be seen from Table 6.2 that the training and test set documents in retrieval have very different properties. The test set documents are queries, which are often 20 times shorter than the training documents, and have a large number of labels; whereas the retrieved documents have each one document identifier label. In stark contrast, text classification training and test set documents are generally drawn from the same type of data. Both types of tasks can have significant differences between development and evaluation conditions, although generally these are defined to be similar enough for meta-parameter optimization to be possible."}, {"heading": "6.3 Experiments and Results", "text": ""}, {"heading": "6.3.1 Evaluated Linear Model Modifications", "text": "The experiments compare a number of models across datasets and measures. Modifications to models are denoted by acronym affixes separated by underscores, for example \u201cjm\u201d for Jelinek-Mercer smoothed models,\n106\nCHAPTER 6. EXPERIMENTS\nCHAPTER 6. EXPERIMENTS\nCHAPTER 6. EXPERIMENTS\nand \u201cu dp\u201d for uniform-background Dirichlet prior smoothed models. Throughout the experiments only a subset of possible modifications is attempted, because there are 39 combinations for the basic smoothing methods, and far more when feature weighting and structural models are included. Table 6.3 summarizes the modifications used in the experiments, with references to descriptions. None of the MNB or TDM modifications increase the time or space complexities of estimation or inference, or introduce considerable additional constants to processing requirements. IDF lifting can decrease the complexities, sparsifying documents further by weighting frequently occurring words to 0."}, {"heading": "6.3.2 Smoothing Methods", "text": "The first set of experiments evaluates the usefulness of the common framework for multinomial models of text presented in Chapter 4. Four methods are used for discounting and smoothing: Dirichlet prior (dp), Jelinek-Mercer (jm), absolute discounting (ad), and power-law discounting (pd). These are combined with three choices for background distribution: uniform (u), collection (c), and uniform-smoothed collection (uc). Absolute discounting proved early in the experiments to be inferior to power-law discounting, and further combinations with the other methods are not shown. This was expected from the literature [Huang and Renals, 2010]. Combinations for the other models were explored based on the initial performance of the collection-smoothed models.\nFigure 6.3 shows the mean Micro-F1 across the text classification datasets for the different smoothing methods. Figures 6.4 and 6.5 shows the mean NDCG@20 and MAP across the text retrieval datasets for the different smoothing methods, respectively. The first visible effect is the overall improvement from the combi-\n109\nCHAPTER 6. EXPERIMENTS\n110\nCHAPTER 6. EXPERIMENTS\n111\nCHAPTER 6. EXPERIMENTS\nnations, compared to the individual smoothing methods in standard use, such as c dp and c jm. Overall, the differences are small but consistent.\nThe main hypothesis to test is whether the generalized smoothing method (uc pd jm dp) improves over the baselines Dirichlet prior (c dp) and Pitman-Yor process smoothing (c pd dp). Comparing the combined smoothing method uc pd jm dp to the baseline c dp, a relative reduction of 2.80%\u2020 in Micro-F1 and relative improvements of 3.37% in NDCG@20 and 1.53% in MAP are seen. Comparing to the stronger baseline of c pd dp, the corresponding reductions are 1.03%\u2021, 0.03%, and 0.28%."}, {"heading": "6.3.3 Feature Weighting and the Extended MNB", "text": "The second set of experiments compares feature weighting and the Extended MNB model that adds document length modeling and scaling of the label prior. Feature weighting in text retrieval has been proposed in the form of query weighting [Smucker and Allan, 2006, Momtazi et al., 2010], whereas in text classification both training and test set documents are weighted similarly [Rennie et al., 2003]. Document length modeling and prior scaling are applicable for the text classification experiments, where test documents can be assumed to have the same length distributions as training documents, and the prior probabilities of labels have varying degrees of usefulness for prediction.\nThe experiments first compared the usefulness of Poisson length modeling (po) and prior scaling (ps). Parameters for both were allowed to vary within the permissible range from 0.0 to 2.0. Prior scaling was noticed to improve classification considerably on most datasets. Poisson length modeling gave no significant improvement on average, and no additional gain was observed in combination with prior scaling. The following experiments on text classification used prior scaling, but not length modeling.\nFeature weighting was attempted with both query idf weighting (qidf) and TF-IDF training and test document weighting (ti). Parameterized versions used IDF lifting (tiX, qidfX) in the range -1.0 to 50.0, length scaling (tXi) in the range -1.0 to 2.0, or both (tXiX). A limited selection of the best-performing smoothing models were chosen for these experiments, with uniform or uniform-smoothed collection distributions for background models, and prior scaling for the text classification datasets.\nFigures 6.6, 6.7 and 6.8 show the results for the second set of experiments, averaged across the datasets. For reference, the baselines c dp and c pd dp are included in the figures. Compared to the smoothing method variants, the improvements from feature weighting and prior scaling are substantial. Averaged over the datasets, uc pd dp tXiX ps produces a relative error reduction of 7.50%\u2020 in Micro-F1 over c pd dp in text classification, and u pd qidf produces relative improvements over c pd dp of 8.26%\u2021 in NDCG@20 and 11.03%\u2021 in MAP.\n112\nCHAPTER 6. EXPERIMENTS\n113\nCHAPTER 6. EXPERIMENTS\n114\nCHAPTER 6. EXPERIMENTS\n115\nCHAPTER 6. EXPERIMENTS\n0.715 0.720 0.725 0.730 0.735 0.740 0.745 0.750 0.755\ntdm_uc_jm_dp_kpd_kdp tdm_u_jm_dp_kpd_kjm_kdp tdm_uc_jm_dp_kpd_kjm_kdp\ntdm_u_pd_jm_dp_kpd_kdp\ntdm_u_jm_dp_kpd_kdp tdm_u_jm_dp_kjm_kdp\ntdm_u_jm_kpd_kdp tdm_u_pd_jm_kpd_kdp mnb_uc_pd_dp_tXiX_ps\nmnb_uc_pd_dp\nmnb_c_pd_dp\nmnb_c_dp\nMicro-F1\nFigure 6.9: Mean Micro-F1 across the text classification datasets with different TDM models. MNB baseline models included for comparison"}, {"heading": "6.3.4 Tied Document Mixture", "text": "The third set of experiments explored the Tied Document Mixture (TDM) model proposed in Chapter 5 for text classification. Smoothing the TDM kernel densities was done with Jelinek-Mercer (kjm), power-law discounting (kpd), and Dirichlet prior (kdp). Due to longer processing times on the largest datasets, as well as a much larger number of possible combinations for smoothing, a small number of combinations successful on single-label datasets were chosen for a full set of experiments. For simplifying the comparisons, the feature weighting and prior scaling combinations were also excluded from the TDM experiments, and left for future experimentation.\nFigure 6.9 shows the Micro-F1 results averaged across the text classification datasets. For comparison, MNB baselines from the previous sets of experiments have been included, and results for the two models are separated by the \u201ctdm\u201d and \u201cmnb\u201d affixes. The best performing model tdm uc jm dp kpd kdp produces a relative Micro-F1 improvement of 2.55% over mnb uc pd dp tXiX ps and 8.67%\u2020 over mnb uc pd dp."}, {"heading": "6.3.5 Comparison with Strong Linear Model Baselines", "text": "The fourth set of experiments compared strong linear model baselines to the results from the first three sets of experiments. For ad-hoc text retrieval a strong baseline is the BM25 model (bm25), while results from the earlier VSMs can be included for comparison. For text classification tasks the strong baselines are LR (lr) and l2-SVM (l2svm) models, combined with the parameterized TF-IDF feature weighting used with the Extended MNB models.\nFigure 6.10 summarizes the Micro-F1 results for the LR and SVM baselines compared to MNB and\n116\nCHAPTER 6. EXPERIMENTS\n0.850 0.860 0.870 0.880 0.890 0.900 0.910\nl2svm_l2r_tXiX\nlr_l2r_tXiX l2svm_l1r_tXiX\nlr_l1r_tXiX\nlr_l2r\ntdm_uc_jm_dp_kpd_kdp\nl2svm_l2r\nmnb_uc_pd_dp_tXiX_ps\nl2svm_l1r\nlr_l1r\nmnb_uc_pd_dp\nmnb_c_pd_dp\nmnb_c_dp\nMicro-F1\nFigure 6.10: Mean Micro-F1 across the binary-label and multi-class text classification datasets for the baseline LR and SVM models, compared to MNB and TDM models\nTDM, averaged across the binary-label and multi-class datasets. Overall, the feature weighted SVM and LR models seem to outperform the generative models, l2svm l2r tXiX in particular showing exceptionally high performance. The difference of l2svm l2r tXiX is 11.32% to tdm uc jm dp kpd kdp, 15.43%\u2020 to mnb uc pd - dp tXiX ps, and 28.66%\u2020 to mnb uc pd dp. Despite the difference in mean scores, none of the LR and SVM models significantly improve over TDM, when tested across datasets.\nThe differences within each dataset can be further examined. Figure 6.11 shows the differences within each dataset. On both cade and amazon12, tdm uc jm dp kpd kdp outperforms l2svm l2r tXiX, while mnb - uc pd dp tXiX ps never outperforms l2svm l2r tXiX. The most likely reason for this is that both feature weighted MNB and SVM models are linear models, and the parameter estimation for learned linear models provides more accurate classifiers in linearly separable classification problems. Datasets such as cade and amazon12 are possibly more non-linear, and TDM outperforms in these cases. With large-scale multi-label datasets SVM and LR are not directly usable in reasonable processing time, and on rcv1 and wikip med2 TDM improves on MNB by a small margin.\nFigures 6.12 and 6.13 show comparisons of the MNB models to VSM and BM25 models in mean NDCG@20 and MAP across the datasets. The results are nearly identical under both measures. VSM models fall behind MNB models, while BM25 outperforms basic MNB models, but not the MNB models using query weighting. The improvement of mnb u pd qidf over vsm tXiX is 23.42%\u2020 in NDCG@20 and 24.33%\u2020 in MAP. The improvement over bm25 is 3.12%\u2021 in NDCG@20 and 4.76%\u2021 in MAP.\nThe NDCG@20 and MAP differences within each retrieval dataset are illustrated in Figures 6.14 and 6.15. While mnb u pd qidf and bm25 have similar performance, on several datasets mnb u pd qidf outperforms bm25 by a margin on both measures, resulting in the significant differences across the datasets.\n117\nCHAPTER 6. EXPERIMENTS\n118\nCHAPTER 6. EXPERIMENTS\n119\nCHAPTER 6. EXPERIMENTS"}, {"heading": "6.3.6 Scalability and Efficiency", "text": "The fifth set of experiments explored the scalability of linear models for classification in estimation and inference. The large Wikipedia dataset from the LSHTC4 competition was pruned to scale the number documents, features, and labelsets, each up to a million. The Label Powerset method [Boutell et al., 2004] was used to map the multi-label learning problems into a multi-class problem directly learnable by the LR and SVM models. For improving scalability, all evaluated models used TF-IDF feature weighting with the unsmoothed CroftHarper IDF, further pruning the words occurring in more than half of the training set documents. Each model configuration was allowed to run for four hours on a 3.40GHz i7-2600 processor with 16GB of RAM memory, and runs taking longer were terminated. The learned models lr l2r tXiX, l2svm l2r tXiX, l2svm l1r tXiX, lr l1r tXiX were evaluated, as well as mnb u jm tXiX and tdm u jm kjm tXiX. Hash table implementations of the generative models instead of an inverted index were tested to evaluate the significance of the sparse posterior inference. These perform MNB inference by updating the conditional probabilities for each label, for each word in the test document. This gives the commonly considered \u201coptimal\u201d time complexity for MNB [Manning et al., 2008].\nFigure 6.16 shows the estimation times for l2svm l2r tXiX and Figure 6.17 for mnb u jm tXiX. Figure 6.16 for the SVM model shows exponentially scaling estimation times, with the times increasing rapidly with more documents and labelsets. Figure 6.17 for the MNB model shows linear estimation times, unaffected by the number of labels, and only marginally affected by the number of features. The SVM model does not\n120\nCHAPTER 6. EXPERIMENTS\n121\nCHAPTER 6. EXPERIMENTS\n122\nCHAPTER 6. EXPERIMENTS\ncomplete the task in the allowed time when the allowed documents and labelsets both number over 10000, while the MNB model completes in all but few of the largest configurations. The estimation times of the other learner linear models behave similarly to l2svm l2r tXiX, while estimation for tdm u jm kjm tXiX behaves similarly to mnb u jm tXiX. The constant difference in small numbers of documents in favor of SVM is due to a pre-processing difference: the LR and SVM models were implemented in C with LibLinear, with Python feature reading times subtracted from the training times, while the generative models were implemented in Java with SGMWeka and the training times include a constant from reading the feature files.\nThe inference times for the models depend on both sparsity of the parameters and their representation. Application of sparse inference reduces the inference complexities for both linear and non-linear models according to sparsity of the parameters. Figure 6.18 compares the inference times for mnb u jm tXiX and tdm u jm kjm tXiX using an inverted index and a hash table, with labelsets pruned to 1000000.\nFigure 6.18 exhibits several overlapping effects. With a hash table, inference for MNB and TDM has similar complexity, producing nearly identical scaling. With an inverted index, the inference becomes more scalable for both models. While most of the configurations with over 100000 training documents do not complete in time with a hash table, the inverted index implementations scale more easily to 1000000 training documents. For both MNB and TDM, the sparse inference times scale linearly with the number of training documents, since this also increases the number of seen labelsets closer to 1000000. For both models, with high numbers of documents the number of features induces an exponential growth with the hash table, whereas with inverted index the growth becomes less exponential. Figure 6.19 shows this effect in more detail. The highest-dimensional TDM model (10000 features, 10000 documents) to complete the task within four hours with the hash table implementation took 106.8 ms per classification. The corresponding inverted index implementation took 9.1 ms per classification, an order of magnitude reduction in mean classification times. The gap between a common hash table implementation and the sparse inference with an inverted index will only increase from this in higher dimensional tasks.\n123\nCHAPTER 6. EXPERIMENTS\n124\nCHAPTER 6. EXPERIMENTS\n125\nChapter 7\nConclusion\nThis chapter concludes the thesis with a discussion. First a summary of the thesis results and implications of the findings are discussed. The thesis statement is revisited, arguing that models extending Multinomial Naive Bayes offer both a versatile solution in terms of both effectiveness and scalability. Limitations of the thesis and future work are discussed, considering current developments related to text mining."}, {"heading": "7.1 Summary of Results", "text": "This thesis proposed generative models of text using sparse computation as a general solution for text mining applications. The problems of fragmentation of research and scalability of models were identified as central problems for text mining. A solution based on modified generative multinomial models of text combined with a novel type of exact sparse inference was proposed as a solution for a variety of text mining tasks.\nBuilding on overviews of both text mining and generative multinomial models for text, the thesis showed the connection of the Multinomial Naive Bayes (MNB) models to linear models and directed generative graphical models. Modifications and extensions to MNB such as smoothing and feature weighting were formalized as constrained graphical models estimated with the maximum likelihood principle.\nInference using inverted indices was shown to reduce the complexity of inference with linear models according to the sparsity of the model representation. This sparse inference was shown to be equally applicable to structured extensions of linear models. A hierarchical extension of MNB called Tied Document Mixture (TDM) was proposed as a basic extension of MNB with document-level nodes. Empirical evaluation of the TDM and modified MNB models showed that the models offer highly competitive performance across text classification and ranking tasks, and sparse inference reduced the inference times by an order of magnitude in the largest considered experiments that completed within the allowed time.\n126\nCHAPTER 7. CONCLUSION"}, {"heading": "7.2 Implications of Findings", "text": "Current research in machine learning considers generative Bayes models for classification generally inferior to discriminative models. The formalization of model modifications and the experiment results show that the Bayes model framework is considerably more flexible and effective than thought. Moreover, these findings directly extend to other types of text mining tasks, such as ranked retrieval of documents. Structured generative models such as the proposed TDM were shown to further improve modeling effectiveness, leading to new types of scalable generative models for text mining.\nThe generalized smoothing function presents virtually all of the commonly used smoothing functions for multinomial and n-gram models of text in a common mixture model framework. The methods differ only in the chosen discounting method, and how the smoothing coefficient is chosen. This describes the decades of statistical language modeling research in a concise form, as well as simplifies the development and analysis of new smoothing methods. Formalizing all the smoothing methods as approximate maximum likelihood estimation on a Hidden Markov Model re-establishes a probabilistic formulation for the functions. The formalization of feature transforms and weighted words as inference over probabilistic data similarly re-establishes the use of these methods in a probabilistic framework. Feature transforms were shown to greatly improved MNB performance for classification and ranking, and has potential implications for other types of generative text models.\nScalability limits the range of applications for probabilistic models. The naive inference used here as baseline is widely considered to be optimal: \u201cBecause we have to look at the data at least once, NB can be said to have optimal time complexity.\u201d [Manning et al., 2008]. The presented sparse inference enables improved scaling of linear models and structured extensions to different types of tasks. Unlike parallelization or approximation, sparse inference reduces the total required computation non-linearly and provides exact results. The inference can be further combined with parallelization and many other efficiency improvements used in information retrieval and machine learning. Especially the application of structured sparse models becomes more scalable compared to naive inference.\nThe experimental results show that the commonly used generative models for text classification and ranking are comparatively weak baselines, whereas the modified and extended generative models have performance on par with strong task-specific methods. Among the possible models, the combination of TF-IDF weighting with uniform Jelinek-Mercer smoothing is one single-parameter option that performs well in both types of tasks. With more parameters and optimization for the specific dataset, a number of stronger models can be learned. The results show that the models developed in the thesis provide improved solutions for a variety of common applications, such as classification of sentiment, spam, e-mails, news, web-pages and Wikipedia articles, and different types of ranked retrieval of text documents. The obtained improvements should extend naturally to other tasks that process text with generative models, as well as to future text mining applications.\n127\nCHAPTER 7. CONCLUSION"}, {"heading": "7.3 Revisiting the Thesis Statement", "text": "The thesis statement was posed in Chapter 1 as: Generative models of text combined with inference using inverted indices provide sparse generative models for text mining that are both versatile and scalable, providing state-of-the-art effectiveness and high scalability for various text mining tasks.\nThe theory and experiment results provide strong support to the thesis statement. Modifications and extensions to MNB models of text were formalized as well-defined graphical models. The experiments showed high effectiveness of the developed models for a variety of text classification and clustering tasks, and the obtained improvements should hold in many current and future applications of generative text modeling. The idea of Naive Bayes models as \u201cpunching bags of machine learning\u201d should therefore be reconsidered. The theory for sparse inference was developed to produce scalability as a function of model sparsity to linear models and their structural extensions. In practice this reduced the processing times of the largest completed experiments by an order of magnitude. Given the theory and results of the thesis, the \u201ccurse of dimensionality\u201d of high-dimensional sparse text data should perhaps be considered a useful property."}, {"heading": "7.4 Limitations of the Thesis", "text": "The experiments in the thesis were restricted to the main applications of text classification and ad-hoc text retrieval, where performance of modified MNB models were shown to be competitive with high-performing task-specific solutions. These are by far the most successful applications of text mining, but do not cover all of the possible types of current and future text mining tasks. Some applications are less suited for the assumptions of generative multinomial models than others. Generative models estimated to maximize likelihood are neither guaranteed to give sufficient performance, if the perfomance measure is very different from maximum likelihood. For example, discriminative models directly optimizing posterior probabilities can be a more suitable choice, if high precision of posterior probabilities is required.\nOver the last years a number of research directions have been proposed as widely applicable solutions for machine learning. Deep learning combining modern parallel computing hardware with developments in optimization has brought a resurgence of interest in multi-layer neural networks [Bengio, 2009, Poon and Domingos, 2011, Mikolov et al., 2011, Collobert et al., 2011]. Probabilistic programming combined with factor graphs is enabling flexible development of complex graphical model architectures [McCallum et al., 2009, Minka et al., 2012, Andres et al., 2012]. Gaussian processes are developing into a high-performing solution to a wide variety of applications [Rasmussen, 2003, Preotiuc-Pietro and Cohn, 2013]. Connections to these research directions is outside the scope of the thesis. Research in these topics will continue, but none of these frameworks currently form a paradigm for performing a variety of text processing mining tasks with high scalability.\nScalability has become an increasingly prominent topic in machine learning during the time used for con-\n128\nCHAPTER 7. CONCLUSION\nducting the thesis research. Particularly, parallelized and online stream learning algorithms have become popular in the context of the \u201cBig Data\u201d and data science movement. Online algorithms such as adaptive stochastic gradient descent [Duchi et al., 2011, McMahan et al., 2013] are used to learn deep neural networks, and parallelization frameworks such as MapReduce [Dean and Ghemawat, 2008] combined with cloud computing are enabling new types of applications. Both types of improvements are increasingly popular for text mining. Linear models combining parallelized online learning with approximations [Li et al., 2013, McMahan et al., 2013, Agarwal et al., 2014] have been proposed as one highly scalable solution. Extensive comparison to these developments is outside the scope of this thesis. As discussed in Chapters 4 and 5, both parallelization and stream processing can be trivially combined in estimation and inference with the algorithms presented in this thesis. Furthermore, unlike the sparse inference developed in this thesis, these developments do not reduce the computational complexity of scaling to large numbers of labels and other latent variable nodes."}, {"heading": "7.5 Future Work", "text": "Experiments in text mining tasks other than ranking and classification should prove the models developed in this thesis useful across text mining applications. Clustering uses the posterior probabilities from generative models in the same way as classification and ranking, and there is no reason to doubt that the shown performance improvements extend to clustering. Regression with Bayes models [Frank et al., 1998] is likely improved substantially, since TDM enables both modeling of each continuous variable value using a documentconditional distribution, and hierarchical smoothing of the value-conditional models.\nOne highly useful application of the models is in n-gram language models that extend the multinomial models with context variables. These have a variety of uses such as speech recognition, machine translation, text compression, text prediction and optical character recognition. The generalized smoothing and feature weighting combined with random search for metaparameters could provide superior models to basic n-gram models using Kneser-Ney smoothing. Sparse inference could be applied to language model decoding, although the most common operation with language models is the query of individual conditional probabilities of words given the context, not the computation of marginals for the Bayes rule.\nThe use of weighted words from feature transforms was shown to improve classification and ranking performance, but the use of arbitrary weighted words for generative models can have other uses. Topic models are commonly estimated from word count or sequence data, but the derivation presented in Chapter 5 allows the use of fractional counts for these models as well. Alternatively, the posterior probabilities of a topic model or outputs of a Non-Negative Matrix Factorization [Paatero and Tapper, 1994] could be used as features for a generative model. Non-probabilistic topic models such as Latent Semantic Analysis [Deerwester, 1988] have traditionally used TF-IDF to improve topic separation, and the same should work for generative topic models.\nSparse inference as well as the modifications to generative models were provided for the case of MNB.\n129\nCHAPTER 7. CONCLUSION\nHowever, sparse inference is applicable whenever the parameters of any linear model can be represented with sparse vectors for each label and less sparse vectors for back-off nodes, with the most basic case being a single background distribution node. Inference of kernel densities over sparse data is particularly scalable. As shown with structured extensions of MNB, many types of graphs for the back-off nodes can be efficiently computed. It would therefore be valuable to know which types of linear and non-linear models can be represented in sparse forms, and how much the scalability of inference is improved.\nSparse inference algorithms reduce the complexity of inference with large numbers of labels, and more generally with large numbers of hidden variables in structured models. The TDM model examined in this thesis was an elementary expansion of a linear model with a mixture over document-conditional models. Extension of these findings into more structured cases can yield rich models that both have higher performance and offer the possibility to make different types of inference using the joint probabilities of hidden variables. For example, adding layers of topic variables could enable highly scalable topic modeling.\nCombining the sparse inference with other improvements for scalability was outlined, but empirical experimentation was left for future work. The combination with parallelization should provide further linear improvements in processing speeds, but in practice parallelization of tasks across networks introduces many complications. Combining the algorithm with the other efficiency improvements for text mining mentioned in Chapter 5 can provide more than linear reductions, but the exact scale of these improvements depend on the data and structure of the model. Tree-based searches can provide considerable reductions, if the nodeconditional models separate into different clusters [Ram and Gray, 2012]. Search network minimization can produce considerably smaller models, if nodes can be merged or removed with minimal loss in performance [Aubert, 2002]. A considerable literature exists in information retrieval to improve the efficiency of inverted indices, that can be directly leveraged for sparse inference [Zobel and Moffat, 2006].\nExtensions of the TDM model were used by the author for the 2014 Kaggle competitions LHSTC41 [Puurula et al., 2014] and WISE2 [Tsoumakas et al.]. Both competitions were large-scale multi-label text classification tasks with over a hundred competing teams. The submission to LSHTC4 won the competition, while the submission for WISE placed narrowly second. TDM extended with label hierarchy nodes proved to be the most useful model, and an ensemble of sparse generative models proved to be a high-performing solution. Future work will apply models such as TDM to other cases where ground-breaking effectiveness and scalability is required.\n1http://www.kaggle.com/c/lshtc/ 2http://www.kaggle.com/c/wise-2014\n130"}, {"heading": "Appendix A", "text": "Tables of Results\nThis appendix contains the tables of results for the experiments summarized in Chapter 6. Tables A.1 to A.9 show the results for the MNB smoothing methods. Tables A.10 to A.22 show the results for the Extended MNB models. Table A.23 shows the results for the TDM model. Tables A.24 to A.25 shows the results for the strong linear model baselines. Tables A.26 to A.33 show the training times for the scalability experiments, and tables A.34 to A.41 show the corresponding testing times. In tables A.26 to A.41 the modifier appendix \u201dht\u201d indicates a hash table implementation for inference instead of an inverted index.\n131"}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "APPENDIX A. TABLES OF RESULTS", "text": ""}, {"heading": "Appendix B", "text": "Kaggle LSHTC4 Winning Solution\nThe appended document describes the winning solution to Kaggle LSHTC41 competition organized in 2014, that had 119 participating teams. The solution was based on the methods presented in the thesis, including the sparse inference, the modified and extended MNB models, the TDM model, and the random search development framework. Some additional techniques such as model-based feedback, label thresholding, and ensemble learning were developed for the competition, that fall outside the scope of the thesis. The developed ensemble learning is a natural continuation of the ideas presented in the thesis. It combines an ensemble of sparse generative model base-classifiers using a mixture model, where the weight of each component is dynamically predicted using a large number of meta-features, and the outputs of the base-classifiers are restricted to predicting a single most likely output for each input. The LSHTC ensemble solution was later extended for the Kaggle WISE20142 competition, where it came second out of 120 competing teams.\n1http://www.kaggle.com/c/lshtc/ 2http://www.kaggle.com/c/wise-2014/\n153\nKaggle LSHTC4 Winning Solution\nAntti Puurula1, Jesse Read2, and Albert Bifet3\n1 Department of Computer Science, The University of Waikato, Private Bag 3105, Hamilton 3240, New Zealand\n2 Department of Information and Computer Science, Aalto University, FI-00076 Aalto, Espoo, Finland 3 Huawei Noah\u2019s Ark Lab, Hong Kong Science Park, Shatin, Hong Kong, China\n1 Overview\nOur winning submission to the 2014 Kaggle competition for Large Scale Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of sparse generative models extending Multinomial Naive Bayes. The base-classifiers consist of hierarchically smoothed models combining document, label, and hierarchy level Multinomials, with feature pre-processing using variants of TF-IDF and BM25. Additional diversification is introduced by different types of folds and random search optimization for different measures. The ensemble algorithm optimizes macroFscore by predicting the documents for each label, instead of the usual prediction of labels per document. Scores for documents are predicted by weighted voting of base-classifier outputs with a variant of Feature-Weighted Linear Stacking. The number of documents per label is chosen using label priors and thresholding of vote scores.\nThis document describes the models and software used to build our solution. Reproducing the results for our solution can be done by running the scripts included in the Kaggle package4. A package omitting precomputed result files is also distributed5. All code is open source, released under GNU GPL 2.0, and GPL 3.0 for Weka and Meka dependencies.\n2 Data Segmentation\nSource files: MAKE FILES, nfold sample corpus.py, fast sample corpus.py, shuffle data.py, count labelsets2.py\nTraining data segmentation is done by the script MAKE FILES, included in the code package. This segments the original training dataset train.txt by random sampling into portions for base-classifier training and for ensemble training.\n4 https://kaggle2.blob.core.windows.net/competitions/kaggle/3634/media/ LSHTC4 winner solution.zip 5 https://kaggle2.blob.core.windows.net/competitions/kaggle/3634/media/ LSHTC4 winner solution omit resultsfiles.zip"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "154\nII\n2,341,782 documents are segmented for the former portion and 23,654 documents for the latter. The base-classifier training dataset dry train.txt is further sampled into 10 different folds, each with a 1000 document held-out portion dry dev.txt for parameter optimization. Folds 0-2 have exclusive and different sampled sets for dry dev.txt. Folds 3-5 sample dry train.txt randomly into 3 exlusive training subsets, with a shared optimization portion. Folds 6-9 segment dry train.txt in the original data order into 4 exclusive training subsets, with a shared optimization portion. For all folds, the training datasets are further shuffled to improve the online pruning of parameters in training.\n3 Base-classifiers\nSource files: SGM-45l/, SGM-45l je/, Metaopt2.py, Make templates.py, results/, RUN DEVS, RUN EVALS, meka.jar\nThe base-classifiers consist mostly of sparse generative model extensions of Multinomial Naive Bayes (MNB). These extend MNB by introducing constrained finite mixtures at the document and hierarchy level nodes, and performing inference from the Multinomial node-conditional models using hierarchical smoothing, and kernel densities in case of document-conditional nodes. A special case is models using BM25 for kernel densities and no hierarchical smoothing. The models are stored in a sparse precomputed format, and inference using inverted indices is used to reduce the inference complexity according to the sparsity of the model. The constrained mixture modeling and sparse inference makes the models as scalable for text modeling as Naive Bayes and KNN, but with higher modelling accuracy. A detailed description of basic models of this type are given in [1, 2]. Since the LSHTC models can contain up to 100 million parameters for word counts, the models are provided as configuration files in the package. Estimating the models from training data takes negligible time more compared to reading saved model files.\nA development version of the SGMWeka toolkit6 was customized to implement the models. The customized version is included as the Java source directory SGM-45l, and the program SGM Tests.java used for training and testing the models can be compiled without external dependencies. The documentation for SGMWeka version 1.4.47 is accurate, but the development version contains additional functionalities. A modified version is in the directory SGM-45l je. This includes the Meka toolkit8 for doing multi-label decomposition used by one of the base-classifiers.\n6 http://sourceforge.net/projects/sgmweka/ 7 http://sourceforge.net/p/sgmweka/wiki/SGMWeka%20Documentation%20v.1.4.4/ 8 http://meka.sourceforge.net/"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "155\nIII\nThe script Metaopt2.py optimizes a base-classifier on a development set according to a chosen performance measure, by iteratively estimating the classifier and classifying the development data portion. The script RUN DEVS runs the development and compresses the log files. The configuration files for Metaopt2.py describes all the parameters provided to a SGM Tests call, as well as the optimization measure to extract from the last line of the SGM Tests log file. Metaopt2.py performs a Gaussian Random Search [3] for the chosen parameters, constrained and transformed according to the configuration file. The directories results * contain the first and last parameter configuration file for each base-classifier type, after a 40x8 iteration random search. Some classifiers were constructed by copying the parameters for similar folds (3,4,5), and some used manually chosen parameter configurations. These classifiers have the final iteration parameter file wikip large X params.txt 39 0, but not the initial file wikip large X params.txt. The script Make templates.py makes the parameter template files as specified in the global variable \u201dconfigs\u201d.\nThe template files describe the model by suffixing the file name with modifications. For example, \u201dmnb mafs2 s8 lp u jm2 bm18ti pct0 ps5 thr16.template\u201d modifies a Multinomial Naive Bayes by optimizing the parameters for a modified version of macro-Fscore ( mafs2), uses data fold 8 ( s8), the Label Powerset method for multi-label classification ( lp), smoothing by a uniform background distribution ( u), a BM25 variant for feature weighting ( bm18ti), uses a safe pruning of pre-computed parameters ( pct0), constrains the scaling of label prior ( ps5) and uses 16 threads for parallel classification.\nSome of the modifications have little influence on the results, such as thr16 that instructs SGM Tests to use 16 threads. More detailed explanations of the important modifications are given in the following sections. A total of 54 baseclassifiers are used in the ensemble, selected down to 42 base-classifiers by model selection. Table 1 shows the base-classifiers sorted according to comb dev.txt macro-averaged Fscore. It should be noted that the parameter ranges for some of the modifications were adjusted during the competition, and the parameter ranges in the individual template files can differ from those in Make templates.py.\nThe word count vectors for LSHTC were preprocessed by the organizers to remove common words, stopwords and short words, as can be seen from looking at the distributions of words in the vectors. This causes problems for some models such as Multinomial models of text, that assume word vectors to distribute normally. Feature transforms and weighting can be used to correct this. Feature weighting is done by each base-classifier separately, using variants of TF-IDF and BM25. All models use 1-3 parameters to optimize the feature weighting on the dry dev.txt portion of the fold. A variant of BM25 that proved most successful has the suffix \u201d bm18ti\u201d. As seen in TFIDF.java, this combines the term count normalization of BM25 with the parameterized length normalization and"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "156\nIV"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "157\nV idf weighting from TF-IDF that has been used earlier [3].\nThe Multinomials use hierarchical smoothing with a uniform background distribution [2]. The variant \u201d uc1\u201d uses a uniform distribution interpolated with a collection model, improving the accuracy by a small amount. All models use Jelinek-Mercer \u201d jmX\u201d for smoothing label and hierarchy level Multinomials, and Dirichlet Prior smoothing \u201d kdpX\u201d for smoothing kernel density document models. The feature selection done by the organizers cause very unusual smoothing parameter configurations to be optimal. With Jelinek-Mercer values less than a heavy amount such as 0.98 become rapidly worse, with some models using a smoothing coefficient of 0.999.\nParameter pruning is chosen by the modifiers \u201d mcX\u201d, \u201d pciX\u201d, \u201d pctX\u201d, \u201d mlcX\u201d. \u201d mcX\u201d prunes word features based on their frequency. \u201d pciX\u201d selects on-line pruning of conditional parameters, \u201d pctX\u201d performs mostly safe pruning of precomputed conditional parameters, \u201d mlcX\u201d prunes labels based on their frequency.\nOne special classifier is the variant using the modifer \u201d je\u201d. This requires a development version of the Meka toolkit and the other files in the directory /SGM-45l je. This model does classification with label powersets decomposed into meta-labels, and transforms the meta-labels back into labelsets after classification. The labelset decompositions are stored in a precomputed file loaded by the modified version of SGM Tests.\nKernel densities are selected with the modifier \u201d kd\u201d, passing -kernel densities to SGM Tests. This constructs document-conditional models, and computes label-conditional probabilities using the document-conditionals as kernel densities [2]. The modifiers \u201d csX\u201d load the LSHTC4 label hierarchy, and use random parent nodes to smooth the label-conditional Multinomials. The Label Powerset method for mapping a multi-label problem into a multi-class problem is done by the modifier \u201d lp\u201d, passing -label powerset to SGM Tests.\nThe modifier \u201d nobo\u201d combined with \u201d kd\u201d produces models for document instances with no back-off smoothing by label-conditional models. The modifiers \u201d bm25X\u201d use BM25 instead of Multinomial distances. Combined with \u201d kd\u201d and \u201d nobo\u201d, this produces a model that uses BM25 for kernel densities of each label.\nThe modifiers \u201d ndcg5\u201d, \u201d mjac\u201d, \u201d mifs\u201d and \u201d mafsX\u201d choose the optimization measure for MetaOpt2.py. These correspond to NDCG@5, Mean of Jaccard scores per instance, micro-averaged Fscore, macro-averaged Fscore and surrogate measures for maFscore. It was noticed early in the competition that computing and optimizing maFscore is problematic, since not all labels are present in the training set, and any subset chosen for optimization will contain only a tiny fraction of the 325k+ labels, with the rest being missing labels. Since most la-"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "158\nVI\nbels are missing labels, and any number of false positives for a missing label will equal an fscore of 0, optimizing maFscore becomes problematic. The \u201d mafsX\u201d surrogates used two attempts to penalize for false positives of missing labels, but these was abandoned for a method that allows optimizing macroFscore better without producing too many instances per label.\nThe modifiers \u201d iwX\u201d select a method developed in this competition. This causes the base-classifier to predict instances per label, instead of labels per instance. A sorted list of the best scores for each label is stored, and for each classified instance the lists for labels are updated. A full distribution of labels is computed for each instance, and the label\u2192instance scores are computed from the rank of the label for each instance. After classification of the dataset, the sparse label\u2192instances scores are transposed and outputted and evaluated in the instance\u2192labels format. The arguments -instantiate weight X and -instantiate threshold X passed to SGM Tests control the number of top scoring instances stored for each label. The ensemble combination uses transposed prediction of the same kind to do the classification.\n4 Ensemble Model\nSource files: RUN METACOMB, MetaComb2.java, TransposeFile.py, SelectClassifiers.py, SelectDevLabels.py, comb dev results/, eval results/, weka.jar\nThe ensemble model is built on our earlier LSHTC3 ensemble [3], but performs classification by predicting instances per label. The classifier vote weight prediction is a case of Feature-Weighted Linear Stacking [4], but the regression models are trained separately for each base-classifier, using reference weights that approximate optimal weights per label in a development set.\nThe base-classifier result files are tranposed from a document\u2192labels per line format to a label\u2192documents per line format. After prediction the ensemble result file is transposed back to the document\u2192labels per line .csv format used by the competition. The script RUN METACOMB performs all the required steps, using the result files stored in /comb dev results for training the ensemble and /eval results to do the classifier combination.\nMetacomb2.java perfoms the ensemble classification. The ensemble uses linear regression models to predict the weight of each base-classifier, using metafeatures computed from label information and classifier outputs to predict the optimal classifier weight for each label. The most useful metafeatures in the LSHTC3 submission used labelset correlation features between the base-classifiers for each document instance [3]. This ensemble uses instance-set correlation features for each label analogously."}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "159\nVII\nThe regression models use Weka LinearRegression for implementing the variant of Feature-Weighted Linear Stacking. For each label in comb dev.txt, optimal reference weights are approximated by distributing a weight of 1 uniformly to the base-classifiers that score highest on the performance measure. Initially fscore was used as the measure, as averaging the fscores across the labels gives maFscore. This however doesn\u2019t use rank information in the instance sets. A small improvement in maFscore was gained by using a similar measure that takes rank information into account. approximateOracleWeights() and updateEvaluationResults() in MetaComb2.java show how the reference vote weights are constructed.\nFollowing vote weight prediction, the label\u2192instances scores are summed for each instance from the weighted votes in the function voteFold(). A combination of label prior information and thresholding similar to one used in the base-classifiers is used to choose the number of instances per label. The label prior information selects a number of instances for the label proportional to the"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "160\nVIII\nlabel frequency in training data, multiplied by the parameter 0.95 passed to set instantiate(). The thresholding then includes to the set all instances with score more than 0.5 of the mean of the initial instance set scores. Figure 1 illustrates the ensemble combination and selection of instances.\nDevelopment of the ensemble by n-fold cross-validation can be done by changing the global variable \u201ddevelopmentRun\u201d in MetaComb2.java to 1. Selection of base-classifiers can be done by giving the classifiers to remove as integer arguments to MetaComb2. The list of removed classifiers used in the final evaluation run in RUN METACOMB was developed by running the classifier selection script SelectClassifiers.py with the n-fold crossvalidation. SelectClassifiers.py performs hill-climbing searches, maximizing the output of MetaComb2 by removing and adding classifiers to the ensemble.\n5 How to Generate the Solution\nThe programs and scripts described above can be run to produce the winning submission file. Some of the programs can take considerable computing resources to produce. Both optimizing the base-classifier parameters and classifying the 452k document test set can take several days or more, depending on the model. We used a handful of quadcore i7-2600 CPU processors with 16GB RAM over"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "161\nIX\nthe competition period to develop and optimize the models. At least 16GB RAM is required to store the word counts reaching 100M parameters. Ensemble combination takes less than 8GB memory, and can be computed from the provided .results files. The base-classifier result files are included in the distribution, as computing these takes considerable time.\nFor optimizing base-classifiers, compile SGM Tests.java with javac, configure Make templates.py or copy an existing template, and run RUN DEVS. For classifying the comb dev.txt and test.txt results with a base-classifier, configure and run RUN EVALS. For combining the base-classifier results with the ensemble, run RUN METACOMB. The global variables in each script can be modified to change configurations.\n6 What Gave us the First Place?\nThe competition posed a number of complications different from usual Kaggle competitions. Most of our tools were developed over the last LSHTC challenges, and this gave us a big advantage. The biggest complication in the competition was scalability of both the base-classifiers and ensemble. Our solution uses sparse storage and inverted indices for inference, a modeling idea that enabled us to use an ensemble of tens of base-classifiers. With the SGMWeka toolkit we could combine parameterized feature weighting [3], hierarchical smoothing [2], kernel densities [2], model-based feedback [6], etc. Other participants used KNN with inverted indices, but our solution provides a diversity of structured probabilistic base-classifiers with much better modeling accuracies.\nAnother complication was the preprocessed pruned feature vectors. This made usual Multinomial or Language Model solutions usable only with very untypical and heavy use of linear interpolation smoothing. The commonly used TF-IDF feature transforms also corrected the problem only somewhat. Our solutions for smoothing and feature weighting with a customized BM25 variant took extensive experimentation to discover, but improved the accuracy considerably. It is likely that the other teams had less sophisticated text similarity measures available, and the ones having good measures scored better in the contest.\nA second difficult complication in the contest was the choice of maFscore for evaluation measure, in contrast to earlier LSHTC competitions. What surprised the contestants was that optimization of maFscore with high numbers of labels is problematic, since most labels will be missing. With maFscore a label occurring once is just as important as one occurring 1000 times, and a label never predicted and one predicted by a 1000 false positives have the same effect on the score. Combined with most labels missing, normal optimization of classifiers proved difficult. It took us some time to figure out the right way to solve this problem, but the solution made it possible for us to compete for the win. Before"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "162\ndeveloping the transposed prediction used in both the base-classifiers and the ensemble, our leaderboard score was around 22%. A couple of simple corrections for maximizing maFscore correctly brought the ensemble combination close to 27%, and using the transposed prediction with a larger and more diverse ensemble gave us the final score close to 34%. Other participants noticed this problem of optimizing maFscore, but likely most of them did not find a good solution.\nThe use of metafeature regression in the ensemble instead of majority voting provided a moderate improvement of about 0.5%, and this much was needed for the win. It is likely that the metafeatures optimized on the 23k comb dev.txt documents looked different from the metafeatures computed for the 452k test.txt documents, even though the metafeatures were chosen or normalized to be stable to change in the number of documents. The optimal amount of regularization for the Weka LinearRegression was untypically high at 1000. More complicated Weka regression models for the vote weight prediction failed to improve the test set score, likely due to overfitting the somewhat unreliable features. Another reason could be the small size of the comb dev.txt for ensemble combination. The ensemble fits the parameters for 55 metafeatures to predict the vote weight of each of the 42 base-classifiers, using only 23k points of data shared by the 42 regression models. The improvement from Feature Weighted Linear Stacking could have been considerably larger, if a larger training set had been segmented for the ensemble."}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "163\nXI\n7 Acknowledgements\nWe\u2019d like to thank Kaggle and the LSHTC organizers for their work in making the competition a success, and the machine learning group at the University of Waikato for the computers we used for our solution.\nReferences\n[1] Puurula, A.: Scalable text classification with sparse generative modeling. In: Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelligence. PRICAI\u201912, Berlin, Heidelberg, Springer-Verlag (2012) 458\u2013469 [2] Puurula, A., Myaeng, S.H.: Integrated instance- and class-based generative modeling for text classification. In: Proceedings of the 18th Australasian Document Computing Symposium. ADCS \u201913, New York, NY, USA, ACM (2013) 66\u201373 [3] Puurula, A.: Combining modifications to multinomial naive bayes for text classification. In Hou, Y., Nie, J.Y., Sun, L., Wang, B., Zhang, P., eds.: Information Retrieval Technology. Volume 7675 of Lecture Notes in Computer Science. Springer Berlin Heidelberg (2012) 114\u2013125 [4] Sill, J., Takcs, G., Mackey, L., Lin, D.: Feature-weighted linear stacking. CoRR abs/0911.0460 (2009) [5] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The weka data mining software: An update. SIGKDD Explor. Newsl. 11(1) (November 2009) 10\u201318 [6] Puurula, A.: Cumulative progress in language models for information retrieval. In: Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013), Brisbane, Australia (December 2013) 96\u2013100"}, {"heading": "APPENDIX B. KAGGLE LSHTC4 WINNING SOLUTION", "text": "164"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The information age has brought a deluge of data. Much of this is in text form, insurmountable in scope for humans and incomprehensible in structure for computers. Text mining is an expanding field of research that seeks to utilize the information contained in vast document collections. General data mining methods based on machine learning face challenges with the scale of text data, posing a need for scalable text mining methods. This thesis proposes a solution to scalable text mining: generative models combined with sparse computation. A unifying formalization for generative text models is defined, bringing together research traditions that have used formally equivalent models, but ignored parallel developments. This framework allows the use of methods developed in different processing tasks such as retrieval and classification, yielding effective solutions across different text mining tasks. Sparse computation using inverted indices is proposed for inference on probabilistic models. This reduces the computational complexity of the common text mining operations according to sparsity, yielding probabilistic models with the scalability of modern search engines. The proposed combination provides sparse generative models: a solution for text mining that is general, effective, and scalable. Extensive experimentation on text classification and ranked retrieval datasets are conducted, showing that the proposed solution matches or outperforms the leading task-specific methods in effectiveness, with a order of magnitude decrease in classification times for Wikipedia article categorization with a million classes. The developed methods were further applied in two 2014 Kaggle data mining prize competitions with over a hundred competing teams, earning first and second places.", "creator": "LaTeX with hyperref package"}}}