{"id": "1602.02450", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Loss factorization, weakly supervised learning and label noise robustness", "abstract": "algorithms prove that the empirical risk of most well - known loss functions factors into determining linear term aggregating all labels with \u03b3 term that is label free, and can further easily expressed by sums of the loss. this holds true even for 0 - smooth, non - convex losses and in analytic rkhs. the first term is a ( kernel ) mean operator - - assuming focal quantity of this class - - which we characterize as the sufficient dimension for the labels. the result tightens known generalization bounds and has new light out their interpretation.", "histories": [["v1", "Mon, 8 Feb 2016 01:50:43 GMT  (104kb)", "https://arxiv.org/abs/1602.02450v1", null], ["v2", "Tue, 9 Feb 2016 23:10:23 GMT  (107kb)", "http://arxiv.org/abs/1602.02450v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["giorgio patrini", "frank nielsen", "richard nock", "marcello carioni"], "accepted": true, "id": "1602.02450"}, "pdf": {"name": "1602.02450.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Giorgio Patrini"], "emails": ["giorgio.patrini@anu.edu.au", "nielsen@lix.polytechnique.fr", "richard.nock@nicta.com.au", "marcello.carioni@mis.mpg.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 45\nFactorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like sgd and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results."}, {"heading": "1 Introduction", "text": "Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice. In fact while the amount of available data grows continuously, its relative training labels \u2013often derived by human effort\u2013 become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, weakly supervised learning (wsl) has attracted much research. In this work, we focus on binary classification under weak supervision. Traditionally, wsl problems are attacked by designing ad-hoc loss functions and optimization algorithms tied to the particular learning setting. Instead, we advocate to \u201cdo not reinvent the wheel\u201d and present an unifying treatment. In summary, we show that, under a mild decomposability assumption,\nAny loss admitting a minimizing algorithm over fully labelled data, can also be minimized in wsl setting with provable generalization and noise robustness guarantees. Our proof is constructive: we show that a simple change in the input and of one line of code is sufficient."}, {"heading": "1.1 Contribution and related work", "text": "We introduce linear-odd losses (lols), a definition that not does demand smoothness or convexity, but that a loss l is such that l(x)\u2212 l(\u2212x) is linear. Many losses of practical interest are such, e.g. logistic and square. We prove a theorem reminiscent of Fisher-Neyman\u2019s factorization [Lehmann and Casella, 1998] of the exponential family which lays the foundation of this work: it shows how empirical l-risk factors (Figure 1) in a label free term with another incorporating a sufficient statistic of the labels, the mean operator. The interplay of the two components is still apparent on newly derived generalization bounds, that also improve on known ones [Kakade et al., 2009]. Aside from factorization, the above linearity is known to guarantee convexity to certain losses used in learning on positive and unlabeled data (pu) in the recent [du Plessis et al., 2015].\nAn advantage of isolating labels comes from applications on wsl. In this scenario, training labels are only partially observable due to an unknown noise process [Garc\u0131a-Garc\u0131a and Williamson, 2011, Hernandez-Gonzalez et al., 2016]. For example, labels may be noisy [Natarajan et al., 2013], missing as with semi-supervision [Chapelle et al., 2006] and pu [du Plessis et al., 2015], or aggregated as it happens in multiple instance learning [Dietterich et al., 1997] and learning from label proportions (llp) [Quadrianto et al., 2009]. As the success of those areas shows, labels are not strictly needed for learning. However, most wsl methods implicitly assumes that labels must be recovered in training, as pointed out by [Joulin and Bach, 2012]. Instead, sufficiency supports a more principled two-step approach: (1) estimate the mean operator from weakly supervised data and (2) plug it into any lol and resort to known procedures for empirical risk minimization (erm). Thus, (1) becomes the only technical obstacle in adapting an algorithm, although often easy to surpass. Indeed, this approach unifies a growing body of literature [Quadrianto et al., 2009, Patrini et al., 2014, van Rooyen et al., 2015, Gao et al., 2016]. As a showcase, we implement (2) by adapting stochastic gradient descent (sgd) to wsl. The upgrade only require to transform input by a \u201cdouble sample trick\u201d and to sum the mean operator in the model update.\nFinally, we concentrate on learning with asymmetric label noise. We connect and extend previous work of [Natarajan et al., 2013, van Rooyen et al., 2015] by designing an unbiased mean operator estimator, for which we derive generalization bounds independent on the chosen lol and algorithm. Recent results [Long and Servedio, 2010] have shown that requiring the strongest form of robustness \u2013on any possible noisy sample\u2013 rules out most losses commonly used, and have drifted research focus on non-convex [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010] or linear losses [van Rooyen et al., 2015]. Our approach is more pragmatic, as we show that any\nlols enjoy an approximate form of noise robustness which converges, on a data-dependent basis, to the strongest one. The mean operator is still central here, being the data-dependent quantity that shapes the bound. The theory is validated with experimental analysis, for which we call the adapted sgd as a black box.\nElements of this work appeared in an early version [Patrini et al., 2015], mostly interested in elucidating the connection between loss factorization and \u03b1-label differential privacy [Chaudhuri and Hsu, 2011].\nNext, Section 2 settles notations and background. Section 3 states the Factorization Theorem. Sections 4 and 5 focus on wsl and noisy labels. Section 6 discusses the paper. Proofs and additional results are given in Appendix."}, {"heading": "2 Learning setting and background", "text": "We denote vectors in bold as x and 1{p} the indicator of p being true. We define [m] .= {1, . . . ,m} and [x]+ . = max(0, x). In binary classification, a learning sample S = {(xi, yi), i \u2208 [m]} is a sequence of (observation, label) pairs, the examples, drawn from an unknown distribution D over X\u00d7 Y, with X \u2286 Rd and Y = {\u22121, 1}. Expectation (or average) over (x, y) \u223c D (S) is denoted as ED (ES).\nGiven a hypothesis h \u2208 H, h : X \u2192 R, a loss is any function l : Y\u00d7 R \u2192 R. A loss gives a penalty l(y, h(x)) when predicting the value h(x) and the observed label is y. We consider margin losses, i.e. l(y, h(x)) = l(yh(x)) [Reid and Williamson, 2010], which are implicitly symmetric: l(yh(x)) = l(\u2212y\u00b7(\u2212h(x))). For notational convenience, we will often use a generic scalar argument l(x). Examples are 01 loss 1{x < 0}, logistic loss log(1 + e\u2212x), square loss (1 \u2212 x)2 and hinge loss [1\u2212 x]+.\nThe goal of binary classification is to select a hypothesis h \u2208 H that generalizes on D. That is, we aim to minimize the true risk on the 01 loss RD,01(h) . = ED[1{yh(x) < 0}]. In practice, we only learn from a finite learning sample S and thus minimize the empirical l-risk RS,l(h) . = ES[l(yh(x))] = 1 m \u2211\ni\u2208[m] l(yih(xi)), where l is a tractable upperbound of 01 loss. Finally, we discuss the meaning of wsl \u2013and in particular of weakly supervised binary classification. The difference with the above is at training time: we learn on a sample S\u0303 drawn from a noisy distribution D\u0303 that may flip, aggregate or suppress labels, while observations are the same. Still, the purpose of learning is unchanged: to minimize the true risk. A rigorous definition will not be relevant in our study."}, {"heading": "2.1 Background: exponential family and logistic loss", "text": "Some background on the exponential family is to come. We can learn a binary classifier fitting a model in the conditional exponential family parametrized by \u03b8: p\u03b8(y|x) = exp(\u3008\u03b8, yx\u3009\u2212log \u2211\ny\u2208Y exp\u3008\u03b8, yx\u3009), with y random variable. The two terms in the exponent are the log-partition function and the sufficient statistic yixi, which fully summarizes one example (x, y). The Fisher-Neyman theorem [Lehmann and Casella, 1998, Theorem 6.5] gives a sufficient and necessary condition for sufficiency of a statistic T (y): the probability distribution factors in two functions, such that \u03b8 interacts with the y only through T :\np\u03b8(y) = g\u03b8(T (y))g \u2032(y) .\nIn our case, it holds that g\u2032(y|x) = 1, T (y|x) = yx and g\u03b8(\u00b7|x) = exp(\u3008\u03b8, \u00b7\u3009 \u2212 log \u2211\ny\u2208Y exp(\u3008\u03b8, yx\u3009), since the value of y is not needed to compute g\u03b8. This shows how yx is indeed sufficient (for y). Now,\nunder the i.i.d. assumption, the log-likelihood of \u03b8 is (the negative of)\nm\u2211\ni=1\nlog \u2211\ny\u2208Y ey\u3008\u03b8,xi\u3009 \u2212\nm\u2211\ni=1\n\u3008\u03b8, yixi\u3009 (1)\n= m\u2211\ni=1\nlog \u2211\ny\u2208Y ey\u3008\u03b8,xi\u3009 \u2212\nm\u2211\ni=1\nlog eyi\u3008\u03b8,xi\u3009\n=\nm\u2211\ni=1\nlog\n( e\u3008\u03b8,xi\u3009 + e\u2212\u3008\u03b8,xi\u3009\neyi\u3008\u03b8,xi\u3009\n)\n= m\u2211\ni=1\nlog ( 1 + e\u22122yi\u3008\u03b8,xi\u3009 ) . (2)\nStep (2) is true because y \u2208 Y. At last, by At last, by re-parameterizing \u03b8 and normalizing, we obtain the empirical risk of logistic loss. Equation (1) shows how the loss splits into a linear term aggregating the labels and another, label free term. We translate this property for classification with erm, by transferring the ideas of sufficiency and factorization to a wide set of losses including the ones of [Patrini et al., 2014]."}, {"heading": "3 Loss factorization and sufficiency", "text": "The linear term just encountered in logistic loss integrates a well-studied statistical object.\nDefinition 1 The (empirical) mean operator of a learning sample S is \u00b5S . = ES [yx] .\nWe drop the S when clear by the context. The name mean operator, or mean map, is borrowed from the theory of Hilbert space embedding [Quadrianto et al., 2009]1. Its importance is due to the injectivity of the map \u2013under conditions on the kernel\u2013 which is used in applications such as twosample and independence tests, feature extraction and covariate shift [Smola et al., 2007]. Here, \u00b5 will play the role of sufficient statistic for labels w.r.t. a set of losses.\nDefinition 2 A function T (S) is said to be a sufficient statistic for a variable z w.r.t. a set of losses L and a hypothesis space H when for any l \u2208 L, any h \u2208 H and any two samples S and S\u2032 the empirical l-risk is such that\nRS,l(h)\u2212RS\u2032,f (h) does not depend on z \u21d4 T (S) = T (S\u2032) .\nThe definition is motivated by the one in Statistics, taking log-odd ratios [Patrini et al., 2014]. We aim to establish sufficiency of mean operators for a large set of losses. The next theorem is our main contribution.\nTheorem 3 (Factorization) Let H be the space of linear hypotheses. Assume that a loss l is such that lo(x) . = (l(x) \u2212 l(\u2212x))/2 is linear. Then, for any sample S and hypothesis h \u2208 H the empirical l-risk can be written as\nRS,l(h) = 1\n2 RS2x,l(h) + lo(h(\u00b5S)) ,\nwhere S2x . = {(xi, \u03c3), i \u2208 [m], \u2200\u03c3 \u2208 Y}.\n1We decide to keep the lighter notation of linear classifiers, but nothing should prevent the extension to nonparametric models, exchanging x with an implicit feature map h(x).\nProof We write RS,l(h) = ES[l(yh(x)) ] as\n1 2 ES\n[ l(yh(x)) + l(\u2212yh(x)) + l(yh(x))\u2212 l(\u2212yh(x)) ]\n= 1\n2 ES\n[\u2211\n\u03c3\u2208Y l(\u03c3h(x))\n]\n+ ES\n[ lo(yh(x)) ]\n= 1\n2 ES2x\n[ l(\u03c3h(x)) ]\n+ ES\n[ lo(h(yx)) ] . (3)\nStep 3 is due to the definition of S2x and linearity of h. The Theorem follows by linearity of lo and expectation.\nFactorization splits l-risk in two parts. A first term is the l-risk computed on the same loss on the \u201cdoubled sample\u201d S2x that contains each observation twice, labelled with opposite signs, and hence it is label free. A second term is a loss lo of h applied to the mean operator \u00b5S, which aggregates all sample labels. Also observe that lo is by construction an odd function, i.e. symmetric w.r.t. the origin. We call the losses satisfying the Theorem linear-odd losses.\nDefinition 4 A loss l is a-linear-odd (a-lol) when lo(x) = (l(x)\u2212 l(\u2212x))/2 = ax, for any a \u2208 R.\nNotice how this does not exclude losses that are not smooth, convex, or proper [Reid and Williamson, 2010]. The definition puts in a formal shape the intuition behind [du Plessis et al., 2015] for pu \u2013 although unrelated to factorization. From now on, we also consider H as the space linear hypotheses h(\u00b7) = \u3008\u03b8, \u00b7\u3009. (Theorem 3 applies beyond lols and linear models; see Section 6.) As a consequence of Theorem 3, \u00b5 is sufficient for all labels.\nCorollary 5 The mean operator \u00b5 is a sufficient statistic for the label y with regard to lols and H.\n(Proof in A.1) The corollary is at the core of the applications in the paper: the single vector \u00b5 \u2208 Rd summarizes all information concerning the linear relationship between y and x for losses that are lol (see also Section 6). Many known losses belong to this class; see Table 3. For logistic loss it holds that (Figure 1(a)):\nlo(x) = 1\n2 log\n1 + e\u2212x\n1 + ex =\n1 2 log\ne\u2212 x 2 (e x 2 + e\u2212 x 2 )\ne x 2 (e\u2212 x 2 + e x 2 ) = \u2212x 2\nThis \u201csymmetrization\u201d is known in the literature [Jaakkola and Jordan, 2000, Gao et al., 2016]. Another case of lol is unhinged loss l(x) = 1 \u2212 x [van Rooyen et al., 2015] \u2013while standard hinge loss does not factor in a linear term.\nThe Factorization Theorem 3 generalizes [Patrini et al., 2014, Lemma 1] that works for symmetric proper losses (spls), e.g. logistic, square and Matsushita losses. Given a permissible generator l [Kearns and Mansour, 1996, Nock and Nielsen, 2009], i.e. dom(l) \u2287 [0, 1], l is strongly convex, differentiable and symmetric with respect to 1/2, spls are defined as l(x) = al + l\n\u22c6(\u2212x)/bl, where l\u22c6 is the convex conjugate of l. Then, since l\u22c6(\u2212x) = l\u22c6(x) \u2212 x:\nlo(x) = 1\n2\n(\nal + l\u22c6(\u2212x)\nbl \u2212 al \u2212\nl\u22c6(x)\nbl\n)\n= \u2212 x 2bl .\nA similar result appears in [Masnadi-Shirazi, 2011, Theorem 11]. A natural question is whether the classes spl and lol are equivalent. We answer in the negative.\nLemma 6 The exhaustive class of linear-odd losses is in 1-to-1 mapping with a proper subclass of even functions, i.e. le(x)\u2212 ax, with le any even function.\n(Proof in A.2) Interestingly, the proposition also let us engineer losses that always factor: choose any even function le with desired properties \u2013it need not be convex nor smooth. The loss is then l(x) = le(x)\u2212ax, with a to be chosen. For example, let le(x) = \u03c1|x|+1, with \u03c1 > 0. l(x) = le(x)\u2212\u03c1x is a lol; furthermore, l upperbounds 01 loss and intercepts it in l(0) = 1. Non-convex l can be constructed similarly. Yet, not all non-differentiable losses can be crafted this way since they are not lols. We provide in B sufficient and necessary conditions to bound losses of interest, including hinge and Huber loss, by lols.\nFrom the optimization viewpoint, we may be interested in keeping properties of l after factorization. The good news is that we are dealing with the same l plus a linear term. Thus, if the property of interest is closed under summation with linear functions, then it will hold true. An example is convexity: if l is lol and convex, so is the factored loss.\nThe next Theorem sheds new light on generalization bounds on Rademacher complexity with linear hypotheses.\nTheorem 7 Assume l is a-lol and L-Lipschitz. Suppose Rd \u2287 X = {x : \u2016x\u20162 \u2264 X < \u221e} and H = {\u03b8 : \u2016\u03b8\u20162 \u2264 B < \u221e}. Let c(X,B) .= maxy\u2208Y l(yXB) and \u03b8\u0302 .= argmin\u03b8\u2208H RS,l(\u03b8). Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4:\nRD,l(\u03b8\u0302)\u2212 inf \u03b8\u2208H\nRD,l(\u03b8) \u2264 (\u221a 2 + 1\n4\n)\n\u00b7 XBL\u221a m +\nc(X,B)L 2 \u00b7 \u221a 1 m log ( 1 \u03b4 ) + 2|a|B \u00b7 \u2016\u00b5D \u2212 \u00b5S\u20162 ,\nor more explicity\nRD,l(\u03b8\u0302)\u2212 inf \u03b8\u2208H\nRD,l(\u03b8) \u2264 (\u221a 2 + 1\n4\n)\n\u00b7 XBL\u221a m +\n( c(X,B)L\n2 + 2|a|XB\n\u221a\nd log d\n) \u221a\n1 m log\n( 2\n\u03b4\n)\n.\n(Proof in A.3) The term \u221a 2+1 4 \u00b7 XBL\u221am is derived by an improved upperbound to the Rademacher complexity of H computed on S2x (A.3, Lemma 14); we call it in short the complexity term. The\nAlgorithm 1 \u00b5sgd\nInput: S2x,\u00b5 , l \u2208 lol; \u03bb > 0; T > 0 m\u2032 \u2190 |S2x| \u03b80 \u2190 0 For any t = 1, . . . , T :\nPick it \u2208 [m\u2032] uniformly at random \u03b7t \u2190 (1 + \u03bbt)\u22121 Pick any v \u2208 \u2202l(yi\u3008\u03b8t,xi\u3009) \u03b8t+1 \u2190 (1\u2212 \u03b7t\u03bb)\u03b8t \u2212 \u03b7t(v +a\u00b5/2 ) \u03b8t+1 \u2190 min { \u03b8t+1, \u03b8t+1 \u221a \u03bb\u22121/\u2016\u03b8t+1\u20162) }\nOutput: \u03b8t+1\nformer expression displays the contribution of the non-linear part of the loss, keeping aside what is missing: a deviation of the empirical mean operator from its population mean. When \u00b5S is not known because of partial label knowledge, the choice of any estimator would affect the bound only through that norm discrepancy. The second expression highlights the interplay of the two loss components. c(X,B) is the only non-linear term, which may well be predominant in the bound for fast-growing losses, e.g. strongly convex. Moreover, we confirm that the linear-odd part does not change the complexity term and only affects the usual statistical penalty by a linear factor. A last important remark comes from comparing the bound with the one due to [Kakade et al., 2009, Corollary 4]. Our factor in front of the complexity term is ( \u221a 2 + 1)/4 \u2248 0.6 instead of 2, that is three times smaller. A similar statement may be derived for rkhs on top of [Bartlett and Mendelson, 2002, Altun and Smola, 2006]."}, {"heading": "4 Weakly supervised learning", "text": "In the next two Sections we discuss applications to wsl. Recall that in this scenario we learn on S\u0303 with partially observable labels, but aim to generalize to D. Assume to know an algorithm that can only learn on S. By sufficiency (Corollary 5), a principled two-step approach to use it on S\u0303 is: (1) estimate \u00b5 from S\u0303; (2) run the algorithm with the lol computed on the estimated \u00b5. This direction was explored by work on llp [Quadrianto et al., 2009, logistic loss] and [Patrini et al., 2014, spl] and in the setting of noisy labels [van Rooyen et al., 2015, unhinged loss] and [Gao et al., 2016, logistic loss]. The approach contrasts with ad-hoc losses and optimization procedures, often trying to recover the latent labels by coordinate descent and EM [Joulin and Bach, 2012]. Instead, the only difficulty here is to come up with a well-behaved estimator of \u00b5 \u2013a statistic independent on both h and l. Thereom 7 then assures bounded l-risk and, in turn, true risk. On stricter conditions on l [Altun and Smola, 2006, Section 4] and [Patrini et al., 2014, Theorem 6] hold finite-sample guarantees.\nAlgorithm 1, \u00b5sgd, adapts sgd for weak supervision. For the sake of presentation, we work on a simple version of sgd based on subgradient descent with L2 regularization, inspired by PEGASO [Shalev-Shwartz et al., 2011]. Given \u00b5 changes are trivial: (i) construct S2x from S\u0303 and (ii) sum \u2212a\u00b5/2 to the subgradients of each example of S2x. In Section 6 upgrades proximal algorithms with the same minimal-effort strategy. The next Section shows an estimator of \u00b5 in the case of noisy labels and specializes \u00b5sgd. We also analyze the effect of noise through the lenses of Theorem 7 and discuss a non-standard notion of noise robustness."}, {"heading": "5 Asymmetric label noise", "text": "In learning with noisy labels, S\u0303 is a sequence of examples drawn from a distribution D\u0303, which samples from D and flips labels at random. Each example (xi, y\u0303i) is (xi,\u2212yi) with probability at most 1/2 or it is (xi, yi) otherwise. The noise rates are label dependent\n2 by (p+, p\u2212) \u2208 [0, 1/2)2 respectively for positive and negative examples, that is, asymmetric label noise (aln) [Natarajan et al., 2013].\nOur first result builds on [Natarajan et al., 2013, Lemma 1] that provides a recipe for unbiased estimators of losses. Thanks to the Factorization Theorem 3, instead of estimating the whole l we act on the sufficient statistic:\n\u00b5\u0302S . = ES [ y \u2212 (p\u2212 \u2212 p+) 1\u2212 p\u2212 \u2212 p+ x ] . (4)\nThe estimator is unbiased, that is, its expectation over the noise distribution D\u0303 is the population mean operator: \u00b5\u0302\nD\u0303 = \u00b5D. Denote then the risk computed on the estimator as R\u0302S,l(\u03b8)\n. =\n1 2RS2x,l(\u03b8) + a\u3008\u03b8, \u00b5\u0302S\u3009. Unbiasedness transfers to l-risk: R\u0302D\u0303,l(\u03b8) = RD,l(\u03b8), \u2200\u03b8 (Proofs in A.4). We have therefore obtained a good candidate as input for any algorithm implementing our 2-step approach, like \u00b5sgd. However, in the context of the literature, there is more. On one hand, the estimators of [Natarajan et al., 2013] may not be convex even when l is so, but this is never the case with lols; in fact, l(x)\u2212 l(\u2212x) = 2ax may be seen as alternative sufficient condition to [Natarajan et al., 2013, Lemma 4] for convexity, without asking l differentiable, for the same reason in [du Plessis et al., 2015]. On the other hand, we generalize the approach of [van Rooyen et al., 2015] to losses beyond unhinged and to asymmetric noise. We now prove that any algorithm minimizing lols that uses the estimator in Equation 4 has a non-trivial generalization bound. We further assume that l is Lipschitz.\nTheorem 8 Consider the setting of Theorem 7, except that here \u03b8\u0302 = argmin\u03b8\u2208H R\u0302S\u0303,l(\u03b8). Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4:\nRD,l(\u03b8\u0302)\u2212 inf \u03b8\u2208H\nRD,l(\u03b8) \u2264 (\u221a 2 + 1\n4\n)\n\u00b7 XBL\u221a m +\n( c(X,B)L\n2 + 2|a|XB 1\u2212 p\u2212 \u2212 p+ \u221a d log d\n) \u221a\n1 m log\n( 2\n\u03b4\n)\n.\n(Proof in A.5) Again, the complexity term is tighter than prior work. [Natarajan et al., 2013, Theorem 3] proves a factor of 2L/(1 \u2212 p\u2212 \u2212 p+) that may even be unbounded due to noise, while our estimate shows a constant of about 0.6 < 2 and it is noise free. In fact, lols are such that noise affects only the linear component of the bound, as a direct effect of factorization. Although we are not aware of any other such results, this is intuitive: Rademacher complexity is computed regardless of sample labels and therefore is unchanged by label noise. Furthermore, depending on the loss, the effect of (limited) noise on generalization may be also be negligible since c(X,B) could be very large for losses like strongly convex. This last remark fits well with the property of robustness that we are about to investigate."}, {"heading": "5.1 Every lol is approximately noise-robust", "text": "The next result comes in pair with Theorem 8: it holds regardless of algorithm and (linear-odd) loss of choice. In particular, we demonstrate that every learner enjoys a distribution-dependent property of robustness against asymmetric label noise. No estimate of \u00b5 is involved and hence the theorem applies to any na\u0308\u0131ve supervised learner on S\u0303. We first refine the notion of robustness of [Ghosh et al., 2015, van Rooyen et al., 2015] in a weaker sense.\n2While being independent on the observation.\nDefinition 9 Let (\u03b8\u22c6, \u03b8\u0303\u22c6) respectively be the minimizers of (RD,l(\u03b8), RD\u0303,l(\u03b8)) in H. l is said \u01eb-aln robust if for any D, D\u0303, R D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8\u0303 \u22c6) \u2264 \u01eb.\nThe distance of the two minimizers is measured by empirical l-risk under expected label noise. 0-aln robust losses are also aln robust: in fact if R\nD\u0303,l(\u03b8 \u22c6) = R D\u0303,l(\u03b8\u0303 \u22c6) then \u03b8\u22c6 \u2208 argmin \u03b8 R D\u0303,l(\u03b8).\nAnd hence if R D\u0303,l(\u03b8) has a unique global minimum, that will be \u03b8\n\u22c6. More generally\nTheorem 10 Assume {\u03b8 \u2208 H : ||\u03b8||2 \u2264 B}. Then every a-lol is \u01eb-aln. That is\nR D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8\u0303 \u22c6) \u2264 4|a|Bmax{p+, p\u2212}\u2016\u00b5D\u20162\nMoreover: (1) If \u2016\u00b5D\u20162 = 0 for D then every lol is aln for any D\u0303. (2) Suppose that l is also once differentiable and \u03b3-strongly convex. Then \u2016\u03b8\u22c6 \u2212 \u03b8\u0303\u22c6\u201622 \u2264 2\u01eb/\u03b3 .\n(Proof in A.6) Unlike Theorem 8, this bound holds in expectation over the noisy risk RD\u0303,l. Its shape depends on the population mean operator, a distribution-dependent quantity. There are two immediate corollaries. When \u2016\u00b5D\u20162 = 0, we obtain optimality for all lols. The second corollary goes further, limiting the minimizers\u2019 distance when losses are differentiable and strongly convex. But even more generally, under proper compactness assumptions on the domain of l, Theorem 10 tells us much more: in the case R\nD\u0303,l(\u03b8) has a unique global minimizer, the smaller \u2016\u00b5D\u20162, the closer the minimizer on noisy data \u03b8\u0303\u22c6 will be to the minimizer on clean data \u03b8\u22c6. Therefore, assuming to know an efficient algorithm that computes a model not far from the global optimum \u03b8\u0303\u22c6, that will be not far from \u03b8\u22c6 either. This is true in spite of the presence of local minima and/or saddle points. [Long and Servedio, 2010] proves that no convex potential3 is noise tolerant, that is, 0-ALN robust. This is not a contradiction. To show the negative statement, the authors craft a case of D breaking any of those losses. And in fact that choice of D does not meet optimality in our bound, because \u2016\u00b5D\u20162 = 14 (18\u03b32 + 6\u03b3 + 1) > 0, with \u03b3 \u2208 (0, 1/6). In contrast, we show that every element of the broad class of lols is approximately robust, as opposed to a worst-case statement. Finally, compare our \u01eb-robustness to the one of [Ghosh et al., 2015]: RD,l(\u03b8\u0303\n\u2217) \u2264 (1 \u2212 2max(p\u2212, p+))\u22121RD,l(\u03b8\u2217). Such bound, while relating the (non-noisy) l-risks, is not data-dependent and may be not much informative for high noise rates."}, {"heading": "5.2 Experiments", "text": "We analyze experimentally the theory so far developed. From now on, we assume to know p+ and p\u2212 at learning time. In principle they can be tuned as hyper-parameters Natarajan et al. [2013], at least for small |Y| [Sukhbaatar and Fergus, 2014]. While being out of scope, practical noise estimators are studied [Bootkrajang and Kaba\u0301n, 2012, Liu and Tao, 2014, Menon et al., 2015, Scott, 2015].\n3A convex potential is a loss l \u2208 C1, convex, such that l(0) < 0 and l(x) \u2192 0 for x \u2192 \u221e. Many convex potentials are lols but not all, but there is no inclusion. An example is e\u2212x.\nAlgorithm 2 \u00b5sgd applied on noisy labels\nWe begin by building a toy planar dataset to probe the behavior of Theorem 10. It is made of four observations: (0, 1) and (\u03c6/3, 1/3) three times, with the first example the only negative, repeated 5 times. We consider this the distribution D so as to calculate \u2016\u00b5D\u20162 = \u03c62/4. We fix p+, p\u2212 = 0.2 = p and control \u03c6 to measure the discrepancy dnoisy . = R D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8\u0303 \u22c6), its counterpart dclean computed on D, and how the two minimizers \u201cdiffer in sign\u201d by dmodels . = \u3008\u03b8\u22c6, \u03b8\u0303\u22c6\u3009/\u2016\u03b8\u22c6\u20162\u2016\u03b8\u0303\u22c6\u20162. The same simulation is run varying the noise rates with constant \u03c6 = 10\u22124. We learn with \u03bb = 10\u22126 by standard square loss. Results are in Figure 2. The closer the parameters to 0, the smaller dclean \u2212 dnoisy, while they are equal when each parameter is individually 0. dmodels is negligible, which is good news for the 01-risk on sight.\nAlgorithm 2 learns with noisy labels on the estimator of Equation 4 and by calling the black box of \u00b5sgd. The next results are based on UCI datasets [Bache and Lichman, 2013]. We learn with logistic loss, without model\u2019s intercept and set \u03bb = 10\u22126 and T = 4 \u00b7 2m (4 epochs). We measure dclean and RD,01, injecting symmetric label noise p \u2208 [0, 0.45) and averaging over 25 runs. Again, we consider the whole distribution so as to play with the ingredients of Theorem 10. Figure 3(a) confirms how the combined effect of p\u2016\u00b5D\u20162 can explain most variation of dclean. While this is not strictly implied by Theorem 10 that only involves dnoisy, the observed behavior is expected. A similar picture is given in Figure 3(b) which displays the true risk RD,01 computed on the minimizer \u03b8\u0303 \u22c6 of S\u0303. From 3(a) and 3(b) we also deduce that large \u2016\u00b5D\u20162 is a good proxy for generalization with linear classifiers; see the relative difference between points at the same level of noise. Finally, we have also monitored \u00b5\nD\u0303 . Figure 3(c) shows that large \u2016\u00b5 D\u0303 \u20162 indicates small dclean as well. This remark can be useful in practice, when the norm can be accurately estimated from S\u0303, as opposite to p and \u00b5D, and used to anticipate the effect of noise on the task at hand.\nWe conclude with a systematic study of hold-out error of \u00b5sgd. The same datasets are now split in 1/5 test and 4/5 training sets once at random. In contrast with the previous experimental setting we perform cross-validation of \u03bb \u2208 10{\u22123,...,+3} on 5-folds in the training set. We compare with vanilla sgd run on corrupted sample S\u0303 and measure the gain from estimating \u00b5\u0302\nS\u0303 . The other parameters\nl, T, \u03bb are the same for both algorithms; the learning rate \u03b7 is untouched from [Shalev-Shwartz et al., 2011] and not tuned for \u00b5sgd. The only differences are in input and gradient update. Table 2 reports test error for sgd and its difference with \u00b5sgd, for a range of values of (p\u2212, p+). \u00b5sgd beats systematically sgd with large noise rates, and still performs in pair with its competitor under low or null noise. Interestingly, in the presence of very intense noise p+ \u2248 .5, while the contender is often doomed to random guess, \u00b5sgd still learns sensible models and improves up to 55% relatively to the error of sgd."}, {"heading": "6 Discussion and conclusion", "text": "Mean and covariance operators The intuition behind the relevance of the mean operator becomes clear once we rewrite it as follows.\nLemma 11 Let \u03c0+ . = ES1{y > 0} be the positive label proportion of S. Then \u00b5S = CovS[x, y] + (2\u03c0+ \u2212 1)ES[x] .\n(Proof in A.7) We have come to the unsurprising fact that \u2013when observations are centered\u2013 the covariance CovS[x, y] is what we need to know about the labels for learning linear models. The rest of the loss may be seen as a data-dependent regularizer. However, notice how the condition \u2016\u00b5D\u20162 = 0 does not implies CovD[x, y] = 0, which would make linear classification hard and limit Theorem 10\u2019s validity to degenerate cases. A kernelized version of this Lemma is given in [Song et al., 2009].\nThe generality of factorization Factorization is ubiquitous for any (margin) loss, beyond the theory seen so far. A basic fact of real analysis supports it: a function l is (uniquely) the sum of an even function le and an odd lo:\nl(x) = 1\n2 (l(x) + l(\u2212x) + l(x)\u2212 l(\u2212x)) = le(x) + lo(x) .\nOne can check that le and lo are indeed even and odd (Figure 1). This is actually all we need for the factorization of l.\nTheorem 12 (Factorization) For any sample S and hypothesis h the empirical l-risk can be written as\nRS,l(h) = 1\n2 ES\n[\u2211\n\u03c3\u2208Y l(\u03c3h(x))\n]\n+ ES\n[ lo(yh(x)) ]\nwhere lo(\u00b7) is odd and le(\u00b7) .= \u2211 \u03c3\u2208Y l(\u03c3h(\u00b7)) is even and both uniquely defined.\nIts range of validity is exemplified by 01 loss, a non-convex discontinuous piece-wise linear function, which factors as\nle(x) = { 1 2 x 6= 0 1 otherwise , lo(x) = \u2212 1 2 sign(x) .\nIt follows immediately that ES[lo(\u00b7)] is sufficient for y. However, lo is a function of model \u03b8. This defeats the purpose a of sufficient statistic, which we aim to be computable from data only and it is the main reason to define lols. The Factorization Theorem 3 can also be stated for rkhs. To show that, notice that we satisfy all hypotheses of the Representer Theorem [Scho\u0308lkopf and Smola, 2002].\nTheorem 13 Let h(x) : X \u2192 H be a feature map into a Reproducing Kernel Hilbert Space (rkhs) H with symmetric positive definite kernel k : X\u00d7 X \u2192 R, such that h : x \u2192 k(\u00b7,x). For any learning sample S, the empirical l-risk RS,l(h) with \u2126 : ||h||H \u2192 R+ regularization can be written as\n1 2 ES\n[\u2211\n\u03c3\u2208Y l(\u03c3h(x))\n]\n+ ES\n[ lo(yh(x)) ] +\u2126(||h||H)\nand the optimal hypothesis admits a representation of the form h(x) = \u2211\ni\u2208[m] \u03b1ik(x,xi).\nAll paper may be read in the context of non-parametric models, with the kernel mean operator as sufficient statistic. Finally, it is simple to show factorization for square loss for regression (C). This finding may open further applications of our framework.\nThe linear-odd losses of [du Plessis et al., 2015] This recent work in the context of pu shows how the linear-odd condition on a convex l allows one to derive a tractable, i.e. still convex, loss for learning with pu. The approach is conceptually related to ours as it isolates a label-free term in the loss, with the goal of leveraging on the unlabelled examples too. Interestingly, the linear term of their Equation 4 can be seen as a mean operator estimator like \u00b5\u0302\n. = P(y = 1) \u00b7 ES+ [x], where S+\nis the set of positive examples. Their manipulation of the loss is not equivalent to the Factorization Theorem 3 though, as explained with details in (D. Beside that, since we reason at the higher level of wsl, we can frame a solution for pu simply calling \u00b5sgd on \u00b5\u0302 defined above or on estimators improved by exploiting results of [Patrini et al., 2014].\nLearning reductions Solving a machine learning problem by solutions to other learning problems is a learning reduction [Beygelzimer et al., 2015]. Our work does fit into this framework. Following [Beygelzimer et al., 2005], we define a wsl task as a triple (K,Y, l), with weakly supervised advice K, predictions space Y and loss l, and we reduce to binary classification (Y,Y, l). Our reduction is somehow simple, in the sense that Y does not change and neither does l. Although, Algorithm 1 modifies the internal code of the \u201coracle learner\u201d which contrasts with the concept of reduction. Anyway, we could as well write subgradients as\n1\n2\n( \u2202l(\u3008\u03b8t,xi\u3009) + \u2202l(\u2212\u3008\u03b8t,xi\u3009) + a\u00b5 ) ,\nwhich equals \u2202l, and thus the oracle would be untouched.\nBeyond \u00b5sgd meta-\u00b5sgd is intimately similar to stochastic average gradient (sag) [Schmidt et al., 2013]. Let denote gi,te (\u03b8) \u2208 \u2202le(yi\u3008\u03b8,xi\u3009) if i = it (example i picked at time t), otherwise = gi,t\u22121e (\u03b8). Define the same for lo accordingly. Then, sag\u2019s model update is:\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7 m\n\u2211\ni\u2208[m] gi,te (\u03b8 t)\u2212 \u03b7 m\n\u2211\ni\u2208[m] gi,to (\u03b8 t) ,\nand recalling that a\u00b5S = ES[\u2202lo(\u03b8)], \u00b5sgd\u2019s update is\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7 \u2202lie(\u03b8t)\u2212 \u03b7\nm\n\u2211\ni\u2208[m] \u2202lio(\u03b8 t) .\nFrom this parallel, the two algorithms appear to be variants of a more general sampling mechanism of examples and gradient components, at each step. More generally, stochastic gradient is just one suit of algorithms that fits into our 2-step learning framework. Proximal methods [Bach et al., 2012] are another noticeable example. The same modus operandi leads to a proximal step of the form:\n\u03b8t+1 \u2190 prox\u0398 ( \u03b8t + \u03b7 ( \u2202RS2x,l(\u03b8 t) + a 2 \u00b5 ))\nwith proxg(x) = argminx\u2032 g(x \u2032) + 12\u2016x \u2212 x\u2032\u201622) and \u0398(\u00b7) the regularizer. Once again, the adaptation works by summing \u00b5 in the gradient step and changing the input to S2x.\nA better (?) picture of robustness The data-dependent worst-case result of [Long and Servedio, 2010], like any extreme-case argument, should be handled with care. It does not give the big picture for all data we may encounter in a real world, but only the most pessimistic. We present such a global view which appears better than expected: learning the minimizer from noisy data does not necessarily\nreduce convex losses to a singleton [van Rooyen et al., 2015] but depends on the mean operator for a large number of them (not necessarily linear, convex or smooth). Quite surprisingly, factorization also marries the two opposite views in one formula4:\nl(x) = 1\n2 ( l(x) + l(\u2212x) \ufe38 \ufe37\ufe37 \ufe38\n=const \u21d2 0-aln\n+ l(x)\u2212 l(\u2212x) \ufe38 \ufe37\ufe37 \ufe38\n=ax \u21d2 \u01eb-aln\n) .\nTo conclude, we have seen how losses factor in a way that we can isolate the contribution of supervision. This has several implications both on theoretical and practical grounds: learning theory, formal analysis of label noise robustness, and adaptation of algorithms to handle poorly labelled data. An interesting question is whether factorization would let one identify what really matters in learning that is instead completely unsupervised, and to do so with more complex models than the ones considered here, as for example deep architectures."}, {"heading": "Acknowledgements", "text": "The authors thank Aditya Menon for insightful feedback on an earlier draft. NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "Appendices", "text": ""}, {"heading": "A Proofs", "text": ""}, {"heading": "A.1 Proof of Lemma 5", "text": "We need to show the double implication that defines sufficiency for y. \u21d2) By Factorization Theorem (3), RS,l(h)\u2212RS\u2032,l(h) is label independent only if the odd part cancels out. \u21d0) If \u00b5S = \u00b5\u2032S then RS,l(h)\u2212 RS\u2032,l(h) is independent of the label, because the label only appears in the mean operator due to Factorization Theorem (3)."}, {"heading": "A.2 Proof of Lemma 6", "text": "Consider the class of lols satisfying l(x)\u2212 l(\u2212x) = 2ax. For any element of the class, define le(x) = l(x)\u2212 ax, which is even. In fact we have\nle(\u2212x) = l(\u2212x) + ax = l(x)\u2212 2ax+ ax = l(x)\u2212 ax = le(x) ."}, {"heading": "A.3 Proof of Theorem 7", "text": "We start by proving two helper Lemmas. The next one provides a bound to the Rademacher complexity computed on the sample S2x . = {(xi, \u03c3), i \u2208 [m], \u2200\u03c3 \u2208 Y}.\nLemma 14 Suppose m even. Suppose X = {x : \u2016x\u20162 \u2264 X} be the observations space, and H = {\u03b8 : \u2016\u03b8\u20162 \u2264 B} be the space of linear hypotheses. Let Y2m .= \u00d7j\u2208[2m]Y. Then the empirical Rademacher complexity\nR(H \u25e6 S2x) .= E\u03c3\u223cY2m\n\n sup \u03b8\u2208H\n1\n2m\n\u2211\ni\u2208[2m] \u03c3i\u3008\u03b8,xi\u3009\n\n\nof H on S2x satisfies:\nR(H \u25e6 S2x) \u2264 v \u00b7 BX\u221a 2m , (5)\nwith v . = 12 + 1 2 \u221a 1 2 \u2212 1m .\nProof Suppose without loss of generality that xi = xm+i. The proof relies on the observation that \u2200\u03c3 \u2208 Y2m,\narg sup \u03b8\u2208H\n{ES[\u03c3(x)\u3008\u03b8,x\u3009]} = 1\n2m arg sup\n\u03b8\u2208H\n{ \u2211\ni\n\u03c3i\u3008\u03b8,xi\u3009 }\n= supH \u2016\u03b8\u20162 \u2016\u2211i \u03c3ixi\u20162 \u2211\ni\n\u03c3ixi . (6)\nSo,\nR(H \u25e6 S2x) = EY2m sup h\u2208H {ES2x [\u03c3(x)h(x)]}\n= supH \u2016\u03b8\u20162\n2m \u00b7 EY2m\n\n \n( \u22112m\ni=1 \u03c3ixi )\u22a4 (\u22112m i=1 \u03c3ixi )\n\u2016\u22112mi=1 \u03c3ixi\u20162\n\n \n= sup H\n\u2016\u03b8\u20162 \u00b7 EY2m [ 1 2m \u00b7 \u2225 \u2225 \u2225 \u2225 \u2225 2m\u2211\ni=1\n\u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2 ] . (7)\nNow, remark that whenever \u03c3i = \u2212\u03c3m+i, xi disappears in the sum, and therefore the max norm for the sum may decrease as well. This suggests to split the 22m assignations into 2m groups of size 2m, ranging over the possible number of observations taken into account in the sum. They can be factored by a weighted sum of contributions of each subset of indices I \u2286 [m] ranging over the non-duplicated observations:\nEY2m\n[\n1 m \u00b7 \u2225 \u2225 \u2225 \u2225 \u2225 2m\u2211\ni=1\n\u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2 ] = 1 22m \u2211 I\u2286[m] 2m\u2212|I| 2m \u00b7 \u2211 \u03c3\u2208Y|I| \u221a 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211 i\u2208I \u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2 . (8)\n=\n\u221a 2 2m \u2211\nI\u2286[m]\n1 2m \u00b7 1 2|I| \u00b7 \u2211\n\u03c3\u2208Y|I|\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2211\ni\u2208I \u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2\n\ufe38 \ufe37\ufe37 \ufe38\nu|I|\n. (9)\nThe \u221a 2 factor appears because of the fact that we now consider only the observations of S. Now, for any fixed I, we renumber its observations in [|I|] for simplicity, and observe that, since \u221a 1 + x \u2264 1+x/2,\nu|I| = 1\n2|I|\n\u2211\n\u03c3\u2208Y|I|\n\u221a \u2211\ni\u2208I \u2016xi\u201622 +\n\u2211\ni1 6=i2 \u03c3i1\u03c3i2x\n\u22a4 i1 xi2 (10)\n=\n\u221a \u2211\ni\u2208I \u2016xi\u201622 2|I| \u2211\n\u03c3\u2208Y|I|\n\u221a\n1 +\n\u2211\ni1 6=i2 \u03c3i1\u03c3i2x \u22a4 i1 xi2\n\u2211 i\u2208I \u2016xi\u201622 (11)\n\u2264\n\u221a \u2211\ni\u2208I \u2016xi\u201622 2|I| \u2211\n\u03c3\u2208Y|I|\n(\n1 +\n\u2211\ni1 6=i2 \u03c3i1\u03c3i2x \u22a4 i1xi2\n2 \u2211\ni\u2208I \u2016xi\u201622\n)\n(12)\n=\n\u221a \u2211\ni\u2208I \u2016xi\u201622 +\n1\n2|I| \u00b7 2 \u2211 i\u2208I \u2016xi\u201622 \u00b7\n\u2211\n\u03c3\u2208Y|I|\n\u2211\ni1 6=i2 \u03c3i1\u03c3i2x\n\u22a4 i1xi2 (13)\n=\n\u221a \u2211\ni\u2208I \u2016xi\u201622 +\n1 2|I| \u00b7 2\u2211i\u2208I \u2016xi\u201622 \u00b7 \u2211 i1 6=i2 x\u22a4i1xi2 \u00b7\n  \u2211\n\u03c3\u2208Y|I| \u03c3i1\u03c3i2\n\n\n\ufe38 \ufe37\ufe37 \ufe38\n=0\n(14)\n=\n\u221a \u2211\ni\u2208I \u2016xi\u201622 (15)\n\u2264 \u221a\n|I| \u00b7X . (16) Plugging this in eq. (9) yields\n1\nX \u00b7 EY2m\n[\n1 m \u00b7 \u2225 \u2225 \u2225 \u2225 \u2225 2m\u2211\ni=1\n\u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2 ] \u2264 \u221a 2 2m m\u2211 k=0 \u221a k 2m ( m k ) . (17)\nSince m is even:\nEY2m\n[\n1 2m \u00b7 \u2225 \u2225 \u2225 \u2225 \u2225 2m\u2211\ni=1\n\u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2 ] \u2264 \u221a 2 2m (m/2)\u22121 \u2211 k=0 \u221a k 2m ( m k ) + \u221a 2 2m m\u2211 k=m/2 \u221a k 2m ( m k ) . (18)\nNotice that the left one trivially satisfies\n\u221a 2 2m (m/2)\u22121 \u2211\nk=0\n\u221a k\n2m\n( m\nk\n) \u2264 \u221a 2\n2m\n(m/2)\u22121 \u2211\nk=0\n1 2m \u00b7 \u221a m\u2212 2 2 ( m k )\n= 1 2 \u00b7 \u221a 1 m \u2212 2 m2 \u00b7 1 2m (m/2)\u22121 \u2211\nk=0\n( m\nk\n)\n\u2264 1 4 \u00b7 \u221a 1 m \u2212 2 m2 (19)\nAlso, the right one satisfies: \u221a 2\n2m\nm\u2211\nk=m/2\n\u221a k\n2m\n( m\nk\n) \u2264 \u221a 2\n2m\nm\u2211\nk=m/2\n\u221a m\n2m\n( m\nk\n)\n= 1\u221a 2m \u00b7 1 2m\nm\u2211\nk=m/2\n( m\nk\n)\n= 1 2 \u00b7 1\u221a 2m . (20)\nWe get\n1\nX \u00b7 EY2m\n[\n1 m \u00b7 \u2225 \u2225 \u2225 \u2225 \u2225 2m\u2211\ni=1\n\u03c3ixi \u2225 \u2225 \u2225 \u2225 \u2225 2 ] \u2264 1 4 \u00b7 \u221a 1 m \u2212 2 m2 + 1 2 \u00b7 \u221a 1 2m (21)\n= 1\u221a 2m\n\u00b7 ( 1\n2 +\n1\n2\n\u221a\n1 2 \u2212 1 m\n)\n. (22)\nAnd finally:\nR(H \u25e6 S2x) \u2264 v \u00b7 BX\u221a 2m , (23)\nwith\nv . =\n1 2 + 1 2\n\u221a\n1 2 \u2212 1 m , (24)\nas claimed.\nThe second Lemma is a straightforward application of McDiarmid \u2019s inequality [McDiarmid, 1998] to evaluate the convergence of the empirical mean operator to its population counterpart.\nLemma 15 Suppose Rd \u2287 X = {x : \u2016x\u20162 \u2264 X < \u221e} be the observations space. Then for any \u03b4 > 0 with probability at least 1\u2212 \u03b4\n\u2016\u00b5D \u2212 \u00b5S\u20162 \u2264 X \u00b7 \u221a d\nm log\n( d\n\u03b4\n)\n.\nProof Let S and S\u2032 be two learning samples that differ for only one example (xi, yi) 6= (xi\u2032 , yi\u2032). Let first consider the one-dimensional case. We refer to the k-dimensional component of \u00b5 with \u00b5k. For any S, S\u2032 and any k \u2208 [d] it holds\n\u2223 \u2223\u00b5kS \u2212 \u00b5kS\u2032 \u2223 \u2223 = 1\nm\n\u2223 \u2223xki yi \u2212 xki\u2032yi\u2032 \u2223 \u2223\n\u2264 X m |yi \u2212 yi\u2032 | \u2264 2X m .\nThis satisfies the bounded difference condition of McDiarmid\u2019s inequality, which let us write for any k \u2208 [d] and any \u01eb > 0 that\nP (\u2223 \u2223\u00b5kD \u2212 \u00b5kS \u2223 \u2223 \u2265 \u01eb ) \u2264 exp\n(\n\u2212m\u01eb 2\n2X2\n)\nand the multi-dimensional case, by union bound\nP ( \u2203k \u2208 [d] : \u2223 \u2223\u00b5kD \u2212 \u00b5kS \u2223 \u2223 \u2265 \u01eb ) \u2264 d exp\n(\n\u2212m\u01eb 2\n2X2\n)\n.\nThen by negation\nP ( \u2200k \u2208 [d] : \u2223 \u2223\u00b5kD \u2212 \u00b5kS \u2223 \u2223 \u2264 \u01eb ) \u2265 1\u2212 d exp\n(\n\u2212m\u01eb 2\n2X2\n)\n,\nwhich implies that for any \u03b4 > 0 with probability 1\u2212 \u03b4\nX\n\u221a\n2 m log\n( d\n\u03b4\n)\n\u2265 \u2016\u00b5D \u2212 \u00b5S\u2016\u221e \u2265 d\u22121/2 \u2016\u00b5D \u2212 \u00b5S\u20162 .\nThis concludes the proof.\nWe now restate and prove Theorem 7.\nTheorem 7 Assume l is a-lol and L-Lipschitz. Suppose Rd \u2287 X = {x : \u2016x\u20162 \u2264 X < \u221e} be the observations space, and H = {\u03b8 : \u2016\u03b8\u20162 \u2264 B < \u221e} be the space of linear hypotheses. Let c(X,B)\n. = maxy\u2208Y l(yXB). Let \u03b8\u0302 = argmin\u03b8\u2208H RS,l(\u03b8). Then for any \u03b4 > 0, with probability at least\n1\u2212 \u03b4\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 (\u221a 2 + 1\n4\n)\n\u00b7 XBL\u221a m + c(X,B)L 2 \u00b7 \u221a 1 m log ( 1 \u03b4 ) + 2|a|B \u00b7 \u2016\u00b5D \u2212 \u00b5S\u20162 ,\nor more explicitly\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 (\u221a 2 + 1\n4\n)\n\u00b7 XBL\u221a m +\n( c(X,B)L\n2 + 2|a|XB\n\u221a\nd log d\n) \u221a\n1 m log\n( 2\n\u03b4\n)\n.\nProof Let \u03b8\u22c6 = argmin\u03b8\u2208H RD,l(\u03b8). We have\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) = 1\n2 RD2x,l(\u03b8\u0302) + a\u3008\u03b8\u0302,\u00b5D\u3009 \u2212\n1 2 RD2x,l(\u03b8 \u22c6)\u2212 a\u3008\u03b8\u22c6,\u00b5D\u3009 (25)\n= 1\n2\n( RD2x,l(\u03b8\u0302)\u2212RD2x,l(\u03b8\u22c6) ) + a\u3008\u03b8\u0302 \u2212 \u03b8\u22c6,\u00b5D\u3009\n= 1\n2\n( RS2x,l(\u03b8\u0302)\u2212RS2x,l(\u03b8\u22c6) ) + a\u3008\u03b8\u0302 \u2212 \u03b8\u22c6,\u00b5D\u3009\n+ 1\n2\n( RD2x,l(\u03b8\u0302)\u2212RS2x,l(\u03b8\u0302)\u2212RD2x,l(\u03b8\u22c6) +RS2x,l(\u03b8\u22c6) ) } A1 . (26)\nStep 25 is obtained by the equality RD,l(\u03b8) = 1 2RD2x,l(\u03b8)+a\u3008\u03b8,\u00b5D\u3009 for any \u03b8. Now, rename Line\n26 as A1. Applying the same equality with regard to S, we have\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 RS,l(\u03b8\u0302)\u2212RS,l(\u03b8\u22c6) \ufe38 \ufe37\ufe37 \ufe38\nA2\n+ a\u3008\u03b8\u0302 \u2212 \u03b8\u22c6,\u00b5D \u2212 \u00b5S\u3009 \ufe38 \ufe37\ufe37 \ufe38\nA3\n+A1 .\nNow, A2 is never more than 0 because \u03b8\u0302 is the minimizer of RS,l(\u03b8). From the Cauchy-Schwarz inequality and bounded models it holds true that\nA3 \u2264 |a| \u2225 \u2225 \u2225\u03b8\u0302 \u2212 \u03b8\u22c6 \u2225 \u2225 \u2225 2 \u00b7 \u2225 \u2225 \u2225\u00b5D \u2212 \u00b5S \u2225 \u2225 \u2225 2 \u2264 2|a|B \u2225 \u2225 \u2225\u00b5D \u2212 \u00b5S \u2225 \u2225 \u2225 2 . (27)\nWe could treat A1 by calling standard bounds based on Rademacher complexity on a sample with size 2m [Bartlett and Mendelson, 2002]. Indeed, since the complexity does not depend on labels, its value would be the same \u2013modulo the change of sample size\u2013 for both S and S2x, as they are computed with same loss and observations. However, the special structure of S2x allows us to obtain a tighter\nstructural complexity term, due to some cancellation effect. The fact is proven by Lemma 14. In order to exploit it, we first observe that\nA1 \u2264 1\n2\n( RD2x,l(\u03b8\u0302)\u2212RS2x,l(\u03b8\u0302)\u2212RD2x,l(\u03b8\u22c6) +RS2x,l(\u03b8\u22c6) )\n\u2264 sup \u03b8\u2208H |RD2x,l(\u03b8)\u2212RS2x,l(\u03b8)|\nwhich by standard arguments [Bartlett and Mendelson, 2002] and the application of Lemma 14 gives a bound with probability at least 1\u2212 \u03b4, \u03b4 > 0\nA1 \u2264 2L \u00b7 R(H \u25e6 S2x) + c(X,B)L \u00b7 \u221a 1\n4m log\n( 1\n\u03b4\n)\n\u2264 L \u00b7 \u221a 2 + 1\u221a 2 \u00b7 BX\u221a 2m + c(X,B)L \u00b7 \u221a 1 4m log ( 1 \u03b4 )\nwhere c(X,B) . = maxy\u2208Y l(yXB) and because 1 2 + 1 2 \u221a 1 2 \u2212 1m < (\u221a 2+1\u221a 2 )\n, \u2200m > 0. We combine the results and get with probability at least 1\u2212 \u03b4, \u03b4 > 0 that\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 (\u221a 2 + 1\n2\n)\n\u00b7 XBL\u221a m + c(X,B)L 2 \u00b7 \u221a 1 m log ( 1 \u03b4 ) + 2|a|B \u00b7 \u2016\u00b5D \u2212 \u00b5S\u20162 . (28)\nThis proves the first part of the statement. For the second one, we apply Lemma 15 that provides the probabilistic bound for the norm discrepancy of the mean operators. Consider that both statements are true with probability at least 1\u2212 \u03b4/2. We write\nP\n({ RD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 (\u221a 2 + 1\n2\n)\n\u00b7 XBL\u221a m + c(X,B)L 2 \u00b7 \u221a 1 m log ( 2 \u03b4 ) + 2|a|B \u00b7 \u2016\u00b5D \u2212 \u00b5S\u20162 }\n\u2227 { \u2016\u00b5D \u2212 \u00b5S\u20162 \u2264 X \u00b7 \u221a d\nm log\n( 2d\n\u03b4\n)})\n\u2265 1\u2212 \u03b4/2\u2212 \u03b4/2 = 1\u2212 \u03b4 ,\nand therefore with probability 1\u2212 \u03b4\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 (\u221a 2 + 1\n2\n)\n\u00b7 XBL\u221a m + c(X,B)L 2 \u00b7 \u221a 1 m log ( 2 \u03b4 ) + 2|a|XB \u00b7 \u221a d m log ( 2d \u03b4 )\n=\n(\u221a 2 + 1\n2\n)\n\u00b7 XBL\u221a m +\n( c(X,B)L\n2 + 2|a|XB\n\u221a\nd log d\n) \u221a\n1 m log\n( 2\n\u03b4\n)\n.\nA.4 Unbiased estimator for the mean operator with asymmetric label noise\n[Natarajan et al., 2013, Lemma 1] provides an unbiased estimator for a loss l(x) computed on x of the form:\nl\u0302(y\u3008\u03b8,xi\u3009) .= (1\u2212 p\u2212y) \u00b7 l(\u3008\u03b8,xi\u3009) + py \u00b7 l(\u2212\u3008\u03b8,xi\u3009)\n1\u2212 p\u2212 \u2212 p+\nWe apply it for estimating the mean operator instead of, from another perspective, for estimating a linear (unhinged) loss as in [van Rooyen et al., 2015]. We are allowed to do so by the very result of the Factorization Theorem, since the noise corruption has effect on the linear-odd term of the loss only. The estimator of the sufficient statistic of a single example yx is\nz\u0302 . = 1\u2212 p\u2212y + py 1\u2212 p\u2212 \u2212 p+ yx\n= 1\u2212 (p\u2212 \u2212 p+)y 1\u2212 p\u2212 \u2212 p+ yx = y \u2212 (p\u2212 \u2212 p+) 1\u2212 p\u2212 \u2212 p+ x ,\nand its average, i.e. the mean operator estimator, is\n\u00b5\u0302S . = ES [ y \u2212 (p\u2212 + p+) 1\u2212 p\u2212 \u2212 p+ x ] ,\nsuch that in expectation over the noisy distribution it holds E D\u0303 [z\u0302] = \u00b5D. Moreover, the corresponding risk enjoys the same unbiasedness property. In fact\nR\u0302 D\u0303,l(\u03b8) =\n1 2 RD2x,l(\u03b8) + ED\u0303 [a\u3008\u03b8, z\u0302\u3009]\n= 1\n2 RD2x,l(\u03b8) + a\u3008\u03b8, \u00b5\u0302D\u0303\u3009 (29)\n= 1\n2 RD2x,l(\u03b8) + a\u3008\u03b8,\u00b5D\u3009\n= RD,l(\u03b8) ,\nwhere we have also used the independency on labels (and therefore of label noise) of RD2x,l."}, {"heading": "A.5 Proof of Theorem 8", "text": "This Theorem is a version of Theorem 7 applied to the case of asymmetric label noise. Those results differ in three elements. First, we consider the generalization property of a minimizer \u03b8\u0302 that is learnt on the corrupted sample S\u0303. Second, the minimizer is computed on the basis of the unbiased estimator of \u00b5\u0302\nS\u0303 and not barely \u00b5 S\u0303 . Third, as a consequence, Lemma 15 is not valid in this scenario. Therefore, we first prove a version of the bound for the mean operator norm discrepancy while considering label noise.\nLemma 16 Suppose Rd \u2287 X = {x : \u2016x\u20162 \u2264 X < \u221e} be the observations space. Let S\u0303 is a learning sample affected by asymmetric label noise with noise rates (p+, p\u2212) \u2208 [0, 1/2). Then for any \u03b4 > 0 with probability at least 1\u2212 \u03b4\n\u2225 \u2225\u00b5\u0302\nD\u0303 \u2212 \u00b5\u0302 S\u0303 \u2225 \u2225 2 \u2264 X 1\u2212 p\u2212 \u2212 p+ \u00b7 \u221a d m log ( d \u03b4 ) .\nProof Let S\u0303 and S\u0303\u2032 be two learning samples from the corrupted distribution D\u0303 that differ for only one example (xi, y\u0303i) 6= (xi\u2032 , y\u0303i\u2032). Let first consider the one-dimensional case. We refer to the k-dimensional\ncomponent of \u00b5 with \u00b5k. For any S\u0303, S\u0303\u2032 and any k \u2208 [d] it holds \u2223 \u2223\u00b5\u0302k\nS\u0303 \u2212 \u00b5\u0302k S\u0303\u2032\n\u2223 \u2223 = 1\nm\n\u2223 \u2223 \u2223 \u2223 ( y\u0303i \u2212 (p\u2212 \u2212 p+) 1\u2212 p\u2212 \u2212 p+ ) xki \u2212 ( y\u0303i\u2032 \u2212 (p\u2212 \u2212 p+) 1\u2212 p\u2212 \u2212 p+ ) xki\u2032 \u2223 \u2223 \u2223 \u2223\n= 1\nm\n\u2223 \u2223 \u2223 \u2223\ny\u0303ix k i\n1\u2212 p\u2212 \u2212 p+ \u2212 y\u0303i\u2032x\nk i\u2032\n1\u2212 p\u2212 \u2212 p+\n\u2223 \u2223 \u2223 \u2223\n\u2264 X m(1\u2212 p\u2212 \u2212 p+) |y\u0303i \u2212 y\u0303i\u2032 | \u2264 2X m(1\u2212 p\u2212 \u2212 p+) .\nThis satisfies the bounded difference condition of McDiarmid\u2019s inequality, which let us write for any k \u2208 [d] and any \u01eb > 0 that\nP (\u2223 \u2223\u00b5\u0302kD \u2212 \u00b5\u0302kS \u2223 \u2223 \u2265 \u01eb ) \u2264 exp\n(\n\u2212(1\u2212 p\u2212 \u2212 p+)2 m\u01eb2\n2X2\n)\nand the multi-dimensional case, by union bound\nP ( \u2203k \u2208 [d] : \u2223 \u2223\u00b5\u0302kD \u2212 \u00b5\u0302kS \u2223 \u2223 \u2265 \u01eb ) \u2264 d exp\n(\n\u2212(1\u2212 p\u2212 \u2212 p+)2 m\u01eb2\n2X2\n)\n.\nThen by negation\nP ( \u2200k \u2208 [d] : \u2223 \u2223\u00b5\u0302kD \u2212 \u00b5\u0302kS \u2223 \u2223 \u2264 \u01eb ) \u2265 1\u2212 d exp\n(\n\u2212(1\u2212 p\u2212 \u2212 p+)2 m\u01eb2\n2X2\n)\n,\nwhich implies that for any \u03b4 > 0 with probability 1\u2212 \u03b4\nX\n(1 \u2212 p\u2212 \u2212 p+)\n\u221a\n2 m log\n( d\n\u03b4\n)\n\u2265 \u2016\u00b5\u0302D \u2212 \u00b5\u0302S\u2016\u221e \u2265 d\u22121/2 \u2016\u00b5D \u2212 \u00b5S\u20162 .\nThis concludes the proof.\nThe proof of Theorem 8 follows the structure of Theorem 7\u2019s and elements of [Natarajan et al.,\n2013, Theorem 3]\u2019s. Let \u03b8\u0302 = argmin\u03b8\u2208H R\u0302D\u0303,l(\u03b8) and \u03b8 \u22c6 = argmin\u03b8\u2208H RD,l(\u03b8). We have\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) = R\u0302D\u0303,l(\u03b8\u0302)\u2212 R\u0302D\u0303,l(\u03b8\u22c6) (30)\n= 1\n2 RD2x,l(\u03b8\u0302) + a\u3008\u03b8\u0302, \u00b5\u0302D\u0303\u3009 \u2212\n1 2 RD2x,l(\u03b8 \u22c6)\u2212 a\u3008\u03b8\u22c6, \u00b5\u0302 D\u0303 \u3009\n= 1\n2\n( RD2x,l(\u03b8\u0302)\u2212RD2x,l(\u03b8\u22c6) ) + a\u3008\u03b8\u0302 \u2212 \u03b8\u22c6, \u00b5\u0302 D\u0303 \u3009\n= 1\n2\n( RS2x,l(\u03b8\u0302)\u2212RS2x,l(\u03b8\u22c6) ) + a\u3008\u03b8\u0302 \u2212 \u03b8\u22c6, \u00b5\u0302 D\u0303 \u3009\n+ 1\n2\n( RD2x,l(\u03b8\u0302)\u2212RS2x,l(\u03b8\u0302)\u2212RD2x,l(\u03b8\u22c6) +RS2x,l(\u03b8\u22c6) ) } A1 . (31)\nStep 30 is due to unbiasedness shown in Section A.4. Again, rename Line 31 as A1, which this time is bounded directly by Theorem 7. Next, we proceed as within the proof of Theorem 7 but now exploiting the fact that 12RS2x,l(\u03b8) = R\u0302S\u0303,l(\u03b8)\u2212 a\u3008\u03b8, \u00b5\u0302D\u0303\u3009\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 R\u0302S\u0303,l(\u03b8\u0302)\u2212 R\u0302S\u0303,l(\u03b8\u22c6) \ufe38 \ufe37\ufe37 \ufe38\nA2\n+ a\u3008\u03b8\u0302 \u2212 \u03b8\u22c6, \u00b5\u0302 D\u0303 \u2212 \u00b5\u0302 S\u0303 \u3009\n\ufe38 \ufe37\ufe37 \ufe38\nA3\n+A1 .\nNow, A2 is never more than 0 because \u03b8\u0302 is the minimizer of R\u0302S\u0303,l(\u03b8). From the Cauchy-Schwarz inequality and bounded models it holds true that\nA3 \u2264 |a| \u2225 \u2225 \u2225\u03b8\u0302 \u2212 \u03b8\u22c6 \u2225 \u2225 \u2225 2 \u00b7 \u2225 \u2225 \u2225\u00b5\u0302D\u0303 \u2212 \u00b5\u0302S \u2225 \u2225 \u2225 2 \u2264 2|a|B \u2225 \u2225 \u2225\u00b5\u0302D\u0303 \u2212 \u00b5\u0302S\u0303 \u2225 \u2225 \u2225 2 , (32)\nfor which we can call Lemma 16. Finally, by a union bound we get that for any \u03b4 > 0 with probability 1\u2212 \u03b4\nRD,l(\u03b8\u0302)\u2212RD,l(\u03b8\u22c6) \u2264 (\u221a 2 + 1\n2\n)\n\u00b7 XBL\u221a m +\n( c(X,B)L\n2 + 2|a|XB 1\u2212 p+ \u2212 p\u2212 \u221a d log d\n) \u221a\n1 m log\n( 2\n\u03b4\n)\n."}, {"heading": "A.6 Proof of Theorem 10", "text": "We now restate and prove Theorem 8. The reader might question the bound for the fact that the quantity on the right-hand side can change by rescaling \u00b5D by X , i.e. the max L2 norm of observations in the space X. Although, such transformation would affect l-risks on the left-hand side as well, balancing the effect. With this in mind, we formulate the result without making explicit dependency on X .\nTheorem 10 Assume {\u03b8 \u2208 H : ||\u03b8||2 \u2264 B}. Let (\u03b8\u22c6, \u03b8\u0303\u22c6) respectively the minimizers of (RD,l(\u03b8), RD\u0303,l(\u03b8)) in H. Then every a-lol is \u01eb-aln. That is\nR D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8\u0303 \u22c6) \u2264 4|a|Bmax(p\u2212, p+) \u00b7 \u2016\u00b5D\u20162 ."}, {"heading": "Moreover:", "text": "1. If \u2016\u00b5D\u20162 = 0 for D then every lol is aln for any D\u0303. 2. Suppose that l is also once differentiable and \u03b3-strongly convex. Then \u2016\u03b8\u22c6 \u2212 \u03b8\u0303\u22c6\u201622 \u2264 2\u01eb/\u03b3 .\nProof The proof draws ideas from [Manwani and Sastry, 2013]. Let us first assume the noise to be symmetric, i.e. p+ = p\u2212 = p. For any \u03b8 we have\nR D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8) = (1\u2212 p) (RD,l(\u03b8\u22c6)\u2212RD,l(\u03b8))\n+ p (RD,l(\u03b8 \u22c6)\u2212RD,l(\u03b8) + 2a\u3008\u03b8\u22c6 \u2212 \u03b8,\u00b5D\u3009) (33) \u2264 (RD,l(\u03b8\u22c6)\u2212RD,l(\u03b8)) + 4|a|Bp\u2016\u00b5D\u20162 (34) \u2264 4|a|Bp\u2016\u00b5D\u20162 . (35)\nWe are working with lols, which are such that l(x) = l(\u2212x) + 2ax and therefore we can take Step 33. Step 34 follows from Cauchy-Schwartz inequality and bounded models. Step 35 is true because \u03b8\u22c6 is the minimizer of RD,l(\u03b8). We have obtained a bound for any \u03b8 and so for the supremum with regard to \u03b8. Therefore:\nsup \u03b8\u2208H\n(\nR D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8)\n)\n= R D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8\u0303) .\nTo lift the discussion to asymmetric label noise, risks have to be split into losses for negative and positive examples. Let RD+,l be the risk computed over the distribution of the positive examples D + and RD\u2212,l the one of the negatives, and denote the mean operators\u00b5D+ ,\u00b5D\u2212 accordingly. Also, define the probability of positive and negative labels in D as \u03c0\u00b1 = P(y = \u00b11). The same manipulations for\nthe symmetric case let us write\nR D\u0303,l(\u03b8 \u22c6)\u2212R D\u0303,l(\u03b8) = \u03c0\u2212 ( RD\u2212,l(\u03b8 \u22c6)\u2212RD\u2212,l(\u03b8) ) + \u03c0+ ( RD+,l(\u03b8 \u22c6)\u2212RD+,l(\u03b8) )\n+ 2ap\u2212\u03c0\u2212\u3008\u03b8\u22c6 \u2212 \u03b8,\u00b5D\u2212\u3009+ 2ap+\u03c0+\u3008\u03b8\u22c6 \u2212 \u03b8,\u00b5D+\u3009 \u2264 (RD,l(\u03b8\u22c6)\u2212RD,l(\u03b8)) + 2a\u3008\u03b8\u22c6 \u2212 \u03b8, p\u2212\u00b5D\u2212 + p+\u00b5D+\u3009 \u2264 4|a|B \u00b7 \u2016p\u2212\u03c0\u2212\u00b5D\u2212 + p+\u03c0+\u00b5D+\u20162 \u2264 4|a|Bmax(p\u2212, p+) \u00b7 \u2016\u03c0\u2212\u00b5D\u2212 + \u03c0+\u00b5D+\u20162 = 4|a|Bmax(p\u2212, p+) \u00b7 \u2016\u00b5D\u20162 .\nThen, we conclude the proof by the same argument for the symmetric case. The first corollary is immediate. For the second, we first recall the definition of a function f strongly convex.\nDefinition 17 A differentiable function f(x) is \u03b3-strongly convex if for all x, x\u2032 \u2208 Dom(f) we have\nf(x)\u2212 f(x\u2032) \u2265 \u3008\u2207f(x\u2032), x\u2212 x\u2032\u3009+ \u03b3 2 \u2016x\u2212 x\u2032\u201622 .\nIf l is differentiable once and \u03b3-strongly convex in the \u03b8 argument, so it the risk RD\u0303,l by composition with linear functions. Notice also that \u2207R D\u0303,l(\u03b8\u0303 \u22c6) = 0 because \u03b8\u0303\u22c6 is the minimizer. Therefore:\n\u01eb \u2265 R D\u0303,l(\u03b8 \u22c6)\u2212 R D\u0303,l(\u03b8\u0303 \u22c6)\n\u2265 \u2329\n\u2207R D\u0303,l(\u03b8\u0303\n\u22c6), \u03b8\u22c6 \u2212 \u03b8\u0303\u22c6 \u232a + \u03b3\n2\n\u2225 \u2225 \u2225\u03b8 \u22c6 \u2212 \u03b8\u0303\u22c6 \u2225 \u2225 \u2225 2\n2\n\u2265 \u03b3 2 \u2225 \u2225 \u2225\u03b8 \u22c6 \u2212 \u03b8\u0303\u22c6 \u2225 \u2225 \u2225 2 2 ,\nwhich means that \u2225 \u2225 \u2225\u03b8 \u22c6 \u2212 \u03b8\u0303\u22c6 \u2225 \u2225 \u2225 2\n2 \u2264 2\u01eb \u03b3 ."}, {"heading": "A.7 Proof of Lemma 11", "text": "CovS[x, y] = ES[yx]\u2212 ES[y]ES[x]\n= \u00b5S \u2212\n\n 1\nm\n\u2211\ni:yi>0\n1\u2212 1 m\n\u2211\ni:yi<0\n1\n\nES[x]\n= \u00b5S \u2212 (2\u03c0+ \u2212 1)ES[x] .\nThe second statement follows immediately."}, {"heading": "B Factorization of non linear-odd losses", "text": "When lo is not linear, we can find upperbounds in the form of affine functions. It suffices to be continuous and have asymptotes at \u00b1\u221e.\nLemma 18 Let the loss l be continuous. Suppose that it has asymptotes at \u00b1\u221e, i.e. there exist c1, c2 \u2208 R and d1, d2 \u2208 R such that\nlim x\u2192+\u221e f(x)\u2212 c1x\u2212 d1 = 0, lim x\u2192\u2212\u221e f(x)\u2212 c2x\u2212 d2 = 0\nthen there exists q \u2208 R such that lo(x) \u2264 c1+c22 x+ q .\nProof One can compute the limits at infinity of lo to get\nlim x\u2192+\u221e\nlo(x) \u2212 c1 + c2\n2 x = d1 \u2212 d2 2\nand\nlim x\u2192\u2212\u221e\nlo(x)\u2212 c1 + c2\n2 x = d2 \u2212 d1 2 .\nThen q . = sup{lo(x)\u2212 c1+c22 x} < +\u221e as lo is continuous. Thus lo(x)\u2212 c1+c2 2 x \u2264 q.\nThe Lemma covers many cases of practical interest outside the class of lols, e.g. hinge, absolute and Huber losses. Exponential loss is the exception since lo(x) = \u2212sinh(x) cannot be bounded. Consider now hinge loss: l(x) = [1 \u2212 x]+ is not differentiable in 1 nor proper [Reid and Williamson, 2010], however it is continuous with asymptotes at \u00b1\u221e. Therefore, for any \u03b8 its empirical risk is bounded as\nRS,hinge(\u03b8) \u2264 1\n2 RS2x,hinge(\u03b8)\u2212\n1 2 \u3008\u03b8,\u00b5\u3009+ q ,\nsince c1 = 0 and c2 = 1. An alternative proof of this result on hinge is provided next, giving the exact value of q = 1/2. The odd term for hinge loss is\nlo(x) = 1\n2 ([1\u2212 x]+ \u2212 [1 + x]+)\n= 1\n4 (\u22122x+ |1\u2212 x| \u2212 |1 + x|)\ndue to an arithmetic trick for the max function: max(a, b) = (a+ b)/2 + |b\u2212 a|/2. Then for any x\n|1\u2212 x| \u2264 |x|+ 1, |1 + x| \u2265 |x| \u2212 1\nand therefore\nlo(x) \u2264 1 4 (\u22122x+ |x|+ 1\u2212 |x|+ 1) = 1 2 (1\u2212 x) .\nWe also provide a \u201cif-and-only-if\u201d version of Lemma 18 fully characterizing which family of losses can be upperbounded by a lol.\nLemma 19 Let l : R \u2192 R a continuous function. Then there exists c1, d1, d2 \u2208 R such that\nlim sup x\u2192+\u221e\nlo(x)\u2212 c1x\u2212 d1 = 0 (36)\nand lim sup x\u2192\u2212\u221e lo(x)\u2212 c1x\u2212 d2 = 0 , (37) if and only if there exists q, q\u2032 \u2208 R such that lo(x) \u2264 q\u2032x+ q for every x \u2208 R.\nProof \u21d2) Suppose that such limits exist and they are zero for some c1, d1, d2. Let prove that lo is bounded from above by a line.\nq = sup x\u2208R\n{lo(x)\u2212 c1x} < \u221e ,\nbecause lo is continuous. So for every x \u2208 R\nlo(x) \u2264 c1x+ q .\nIn particular we can take c1 as the angular coefficient of the line. \u21d0) Vice versa we proceed by contradiction. Suppose that there exists q, q\u2032 \u2208 R such that lo is bounded from above by l(x) = q\u2032x + q. Suppose in addition that the conditions on the asymptotes (36) and (37) are false. This implies either the existence of a sequence xn \u2192 +\u221e such that\nlim n\u2192\u221e\nlo(xn)\u2212 q\u2032xn \u2192 \u00b1\u221e ,\nor the existence of another sequence x\u2032n \u2192 \u2212\u221e\nlim n\u2192\u221e\nlo(yn)\u2212 q\u2032x\u2032n \u2192 \u00b1\u221e .\nOn one hand, if at least one of these two limits is +\u221e then we already reach a contradiction, because lo(x) is supposed to be bounded from above by l(x) = q\n\u2032x + q. Suppose on the other hand that xn \u2192 +\u221e is such that\nlim n\u2192+\u221e\nlo(xn)\u2212 q\u2032xn \u2192 \u2212\u221e .\nThen defining x\u2032n = \u2212xn we have\nlim n\u2192+\u221e\nlo(wn)\u2212mx\u2032n \u2192 +\u221e ,\nand for the same reason as above we reach a contradiction."}, {"heading": "C Factorization of square loss for regression", "text": "We have formulated the Factorization Theorem for classification problems. However, a similar property holds for regression with square loss: f(\u3008\u03b8,xi\u3009, y) = (\u3008\u03b8,xi\u3009 \u2212 yi)2 factors as\nES[(\u3008\u03b8,x\u3009 \u2212 y)2] = ES [ \u3008\u03b8,x\u30092 ] + ES [ y2 ] \u2212 2\u3008\u03b8,\u00b5\u3009 .\nTaking the minimizers on both sides we obtain\nargmin \u03b8 ES[f(\u3008\u03b8,x\u3009, y)] = argmin \u03b8 ES\n[ \u3008\u03b8,x\u30092 ] \u2212 2\u3008\u03b8,\u00b5\u3009\n= argmin \u03b8\n\u2016X\u22a4\u03b8\u201622 \u2212 2\u3008\u03b8,\u00b5\u3009 ."}, {"heading": "D The role of lols in [du Plessis et al., 2015]", "text": "Let \u03c0+ . = P(y = 1) and let D+ and D\u2212 respectively the set of positive and negative examples in D. Consider first\nE(x,\u00b7)\u223cD [l(\u2212\u3008\u03b8,x\u3009)] = \u03c0+E(x,\u00b7)\u223cD+ [l(\u2212\u3008\u03b8,x\u3009)] + (1\u2212 \u03c0+)E(x,\u00b7)\u223cD\u2212 [l(\u2212\u3008\u03b8,x\u3009)] . (38)\nThen, it is also true that\nE(x,y)\u223cD [l(y\u3008\u03b8,x\u3009)] = \u03c0+E(x,y)\u223cD+ [l(y\u3008\u03b8,x\u3009)] + (1\u2212 \u03c0+)E(x,y)\u223cD\u2212 [l(y\u3008\u03b8,x\u3009)] . (39)\nNow, solve Equation 38 for (1 \u2212 \u03c0+)E(x,y)\u223cD\u2212 [l(y\u3008\u03b8,x\u3009)] = (1 \u2212 \u03c0+)E(x,y)\u223cD\u2212 [\u2212l(\u2212\u3008\u03b8,x\u3009)] and substitute it into 39 so as to obtain:\nE(x,y)\u223cD [l(y\u3008\u03b8,x\u3009)] = = \u03c0+E(x,y)\u223cD+ [l(y\u3008\u03b8,x\u3009)] + E(x,\u00b7)\u223cD [l(\u2212\u3008\u03b8,x\u3009)]\u2212 \u03c0+E(x,\u00b7)\u223cD+ [l(\u2212\u3008\u03b8,x\u3009)] = \u03c0+ ( E(x,y)\u223cD+ [l(+\u3008\u03b8,x\u3009)]\u2212 E(x,\u00b7)\u223cD+ [l(\u2212\u3008\u03b8,x\u3009)] ) + E(x,\u00b7)\u223cD [l(\u2212\u3008\u03b8,x\u3009)]\n= 2\u03c0+E(x,y)\u223cD+ [lo(+\u3008\u03b8,x\u3009)] + E(x,\u00b7)\u223cD [l(\u2212\u3008\u03b8,x\u3009)]\nBy our usual definition of lo(x) = 1 2 (l(x) \u2212 l(\u2212x)). Recall that one of the goal of the authors is to conserve the convexity of this new crafted loss function. Then, [du Plessis et al., 2015, Theorem 1] proceeds stating that when lo is convex, it must also be linear. And therefore they must focus on lols. The result of [du Plessis et al., 2015, Theorem 1] is immediate from the point of view of our theory: in fact, an odd function can be convex or concave only if it also linear. The resulting expression based on the fact l(x)\u2212 l(\u2212x) = 2ax simplifies into\nE(x,y)\u223cD [l(y\u3008\u03b8,x\u3009)] = a\u03c0+E(x,y)\u223cD+ [y\u3008\u03b8,x\u3009] + E(x,\u00b7)\u223cD [l(\u2212\u3008\u03b8,x\u3009)] = a\u03c0+\u00b5D+ + E(x,\u00b7)\u223cD [l(\u2212\u3008\u03b8,x\u3009)] .\nwhere \u00b5D+ is a mean operator computed on positive examples only. Notice how the second term is instead label independent, although it is not an even function as in our Factorization Theorem."}, {"heading": "E Additional examples of loss factorization", "text": "\u22122 \u22121 0 1 2 x\n\u22122\n\u22121\n0\n1\n2\n1{x< 0} le(x)\nlo(x)\n(a) 0-1 loss\n\u22122 \u22121 0 1 2 x\n\u22122\n0\n2\n4\n6 \u221a 1+ x2 \u2212 x\nle(x)\nlo(x)\n(b) Matsushita loss\n\u22122 \u22121 0 1 2 x\n\u22122\n0\n2\n4 6 |x| \u2212 x+1 le(x)\nlo(x)\n(c) \u03c1-loss, \u03c1=1\n\u22122 \u22121 0 1 2 x\n\u22122\n\u22121\n0\n1\n2 3 2\u2212hinge le(x)\nlo(x)\n(d) 2-hinge loss\n\u22122 \u22121 0 1 2 x\n\u22122\n\u22121\n0\n1\n2\n3\n[0, 1\u2212 x] + le(x)\nlo(x)\n(e) hinge loss\n\u22122 \u22121 0 1 2 x\n\u22122\n0\n2\n4\n6 Huber\nle(x)\nlo(x)\n(f) Huber loss"}], "references": [{"title": "Theory of point estimation, volume 31", "author": ["E.L. Lehmann", "G. Casella"], "venue": "Springer Science & Business Media,", "citeRegEx": "Lehmann and Casella.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann and Casella.", "year": 1998}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In NIPS*23,", "citeRegEx": "Kakade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2009}, {"title": "Convex formulation for learning from positive and unlabeled data", "author": ["M C. du Plessis", "G. Niu", "M. Sugiyama"], "venue": "In 32 th ICML,", "citeRegEx": "Plessis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plessis et al\\.", "year": 2015}, {"title": "Degrees of supervision", "author": ["D. Garc\u0131a-Garc\u0131a", "R.C. Williamson"], "venue": "In NIPS*25 workshop on Relations between machine learning problems,", "citeRegEx": "Garc\u0131a.Garc\u0131a and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Garc\u0131a.Garc\u0131a and Williamson.", "year": 2011}, {"title": "Weak supervision and other non-standard classification problems: a taxonomy", "author": ["J. Hernandez-Gonzalez", "I. Inza", "J.A. Lozano"], "venue": "In Pattern Recognition Letters,", "citeRegEx": "Hernandez.Gonzalez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hernandez.Gonzalez et al\\.", "year": 2016}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"], "venue": "In NIPS*27,", "citeRegEx": "Natarajan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2013}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": "MIT press Cambridge,", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Estimating labels from label proportions", "author": ["N. Quadrianto", "A.J. Smola", "T.S. Caetano", "Q.V. Le"], "venue": "JMLR, 10:2349\u20132374,", "citeRegEx": "Quadrianto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2009}, {"title": "Almost) no label no cry", "author": ["G. Patrini", "R. Nock", "P. Rivera", "T. Caetano"], "venue": "In NIPS*28,", "citeRegEx": "Patrini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Patrini et al\\.", "year": 2014}, {"title": "Learning with symmetric label noise: The importance of being unhinged", "author": ["B. van Rooyen", "A.K. Menon", "R.C. Williamson"], "venue": "In NIPS*29,", "citeRegEx": "Rooyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rooyen et al\\.", "year": 2015}, {"title": "Risk minimization in the presence of label noise", "author": ["W. Gao", "L. Wang", "Y.-F. Li", "Z.-H Zhou"], "venue": "In Proc. of the 30 AAAI Conference on Artificial Intelligence,", "citeRegEx": "Gao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2016}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["P.M. Long", "R.A. Servedio"], "venue": "Machine learning,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Learning SVMs from sloppily labeled data", "author": ["G. Stempfel", "L. Ralaivola"], "venue": "In Artificial Neural Networks (ICANN),", "citeRegEx": "Stempfel and Ralaivola.,? \\Q2009\\E", "shortCiteRegEx": "Stempfel and Ralaivola.", "year": 2009}, {"title": "On the design of robust classifiers for computer vision", "author": ["H. Masnadi-Shirazi", "V. Mahadevan", "N. Vasconcelos"], "venue": "In Proc. of the 23 IEEE CVPR,", "citeRegEx": "Masnadi.Shirazi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Masnadi.Shirazi et al\\.", "year": 2010}, {"title": "t-logistic regression", "author": ["N. Ding", "S.V.N. Vishwanathan"], "venue": "In NIPS*24,", "citeRegEx": "Ding and Vishwanathan.,? \\Q2010\\E", "shortCiteRegEx": "Ding and Vishwanathan.", "year": 2010}, {"title": "Bridging weak supervision and privacy aware learning via sufficient statistics. NIPS*29, Workshop on learning and privacy with incomplete data and weak supervision", "author": ["G. Patrini", "F. Nielsen", "R. Nock"], "venue": null, "citeRegEx": "Patrini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patrini et al\\.", "year": 2015}, {"title": "Sample complexity bounds for differentially private learning", "author": ["K. Chaudhuri", "D. Hsu"], "venue": "In JMLR,", "citeRegEx": "Chaudhuri and Hsu.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri and Hsu.", "year": 2011}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Bregman divergences and surrogates for learning", "author": ["R. Nock", "F. Nielsen"], "venue": "IEEE Trans.PAMI,", "citeRegEx": "Nock and Nielsen.,? \\Q2009\\E", "shortCiteRegEx": "Nock and Nielsen.", "year": 2009}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "Jaakkola and Jordan.,? \\Q2000\\E", "shortCiteRegEx": "Jaakkola and Jordan.", "year": 2000}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["M.J. Kearns", "Y. Mansour"], "venue": "In 28 th ACM STOC,", "citeRegEx": "Kearns and Mansour.,? \\Q1996\\E", "shortCiteRegEx": "Kearns and Mansour.", "year": 1996}, {"title": "The design of bayes consistent loss functions for classification", "author": ["H. Masnadi-Shirazi"], "venue": "PhD thesis, University of California at San Diego,", "citeRegEx": "Masnadi.Shirazi.,? \\Q2011\\E", "shortCiteRegEx": "Masnadi.Shirazi.", "year": 2011}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.-L. Bartlett", "S. Mendelson"], "venue": "JMLR, 3:463\u2013482,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Unifying divergence minimization and statistical inference via convex duality", "author": ["Y. Altun", "A.J. Smola"], "venue": "In 19 th COLT,", "citeRegEx": "Altun and Smola.,? \\Q2006\\E", "shortCiteRegEx": "Altun and Smola.", "year": 2006}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Making risk minimization tolerant to label", "author": ["A. Ghosh", "N. Manwani", "P.S. Sastry"], "venue": "noise. Neurocomputing,", "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Learning from noisy labels with deep neural networks", "author": ["S. Sukhbaatar", "R. Fergus"], "venue": null, "citeRegEx": "Sukhbaatar and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Sukhbaatar and Fergus.", "year": 2014}, {"title": "Label-noise robust logistic regression and its applications", "author": ["J. Bootkrajang", "A. Kab\u00e1n"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bootkrajang and Kab\u00e1n.,? \\Q2012\\E", "shortCiteRegEx": "Bootkrajang and Kab\u00e1n.", "year": 2012}, {"title": "Classification with noisy labels by importance reweighting", "author": ["T. Liu", "D. Tao"], "venue": null, "citeRegEx": "Liu and Tao.,? \\Q2014\\E", "shortCiteRegEx": "Liu and Tao.", "year": 2014}, {"title": "Learning from corrupted binary labels via class-probability estimation", "author": ["A. Menon", "B. Van Rooyen", "C.S. Ong", "B. Williamson"], "venue": null, "citeRegEx": "Menon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Menon et al\\.", "year": 2015}, {"title": "A rate of convergence for mixture proportion estimation, with application to learning from noisy labels", "author": ["C. Scott"], "venue": "AISTATS,", "citeRegEx": "Scott.,? \\Q2015\\E", "shortCiteRegEx": "Scott.", "year": 2015}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu"], "venue": "In 26 th ICML. ACM,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Learning reductions that really work", "author": ["A. Beygelzimer", "H. Daum\u00e9 III", "J. Langford", "P. Mineiro"], "venue": null, "citeRegEx": "Beygelzimer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2015}, {"title": "Error limiting reductions between classification tasks", "author": ["A. Beygelzimer", "V. Dani", "T. Hayes", "J. Langford", "B. Zadrozny"], "venue": "In 22 th ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2005}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "Probabilistic Methods for Algorithmic Discrete Mathematics, pages 1\u201354", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1998\\E", "shortCiteRegEx": "McDiarmid.", "year": 1998}, {"title": "Noise tolerance under risk minimization", "author": ["N. Manwani", "P.S. Sastry"], "venue": "Cybernetics, IEEE Transactions on,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We prove a theorem reminiscent of Fisher-Neyman\u2019s factorization [Lehmann and Casella, 1998] of the exponential family which lays the foundation of this work: it shows how empirical l-risk factors (Figure 1) in a label free term with another incorporating a sufficient statistic of the labels, the mean operator.", "startOffset": 64, "endOffset": 91}, {"referenceID": 1, "context": "The interplay of the two components is still apparent on newly derived generalization bounds, that also improve on known ones [Kakade et al., 2009].", "startOffset": 126, "endOffset": 147}, {"referenceID": 5, "context": "For example, labels may be noisy [Natarajan et al., 2013], missing as with semi-supervision [Chapelle et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 6, "context": ", 2013], missing as with semi-supervision [Chapelle et al., 2006] and pu [du Plessis et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 7, "context": ", 2015], or aggregated as it happens in multiple instance learning [Dietterich et al., 1997] and learning from label proportions (llp) [Quadrianto et al.", "startOffset": 67, "endOffset": 92}, {"referenceID": 8, "context": ", 1997] and learning from label proportions (llp) [Quadrianto et al., 2009].", "startOffset": 50, "endOffset": 75}, {"referenceID": 12, "context": "Recent results [Long and Servedio, 2010] have shown that requiring the strongest form of robustness \u2013on any possible noisy sample\u2013 rules out most losses commonly used, and have drifted research focus on non-convex [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 16, "context": "Elements of this work appeared in an early version [Patrini et al., 2015], mostly interested in elucidating the connection between loss factorization and \u03b1-label differential privacy [Chaudhuri and Hsu, 2011].", "startOffset": 51, "endOffset": 73}, {"referenceID": 17, "context": ", 2015], mostly interested in elucidating the connection between loss factorization and \u03b1-label differential privacy [Chaudhuri and Hsu, 2011].", "startOffset": 117, "endOffset": 142}, {"referenceID": 9, "context": "We translate this property for classification with erm, by transferring the ideas of sufficiency and factorization to a wide set of losses including the ones of [Patrini et al., 2014].", "startOffset": 161, "endOffset": 183}, {"referenceID": 8, "context": "The name mean operator, or mean map, is borrowed from the theory of Hilbert space embedding [Quadrianto et al., 2009].", "startOffset": 92, "endOffset": 117}, {"referenceID": 18, "context": "Its importance is due to the injectivity of the map \u2013under conditions on the kernel\u2013 which is used in applications such as twosample and independence tests, feature extraction and covariate shift [Smola et al., 2007].", "startOffset": 196, "endOffset": 216}, {"referenceID": 9, "context": "The definition is motivated by the one in Statistics, taking log-odd ratios [Patrini et al., 2014].", "startOffset": 76, "endOffset": 98}, {"referenceID": 19, "context": "Table 1: Factorization of linear-odd losses: spl (including logistic, square and Matsushita) [Nock and Nielsen, 2009], double \u201c2\u201d-hinge and perceptron [du Plessis et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 25, "context": "For the sake of presentation, we work on a simple version of sgd based on subgradient descent with L2 regularization, inspired by PEGASO [Shalev-Shwartz et al., 2011].", "startOffset": 137, "endOffset": 166}, {"referenceID": 5, "context": "The noise rates are label dependent 2 by (p+, p\u2212) \u2208 [0, 1/2) respectively for positive and negative examples, that is, asymmetric label noise (aln) [Natarajan et al., 2013].", "startOffset": 148, "endOffset": 172}, {"referenceID": 5, "context": "On one hand, the estimators of [Natarajan et al., 2013] may not be convex even when l is so, but this is never the case with lols; in fact, l(x)\u2212 l(\u2212x) = 2ax may be seen as alternative sufficient condition to [Natarajan et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 12, "context": "[Long and Servedio, 2010] proves that no convex potential is noise tolerant, that is, 0-ALN robust.", "startOffset": 0, "endOffset": 25}, {"referenceID": 26, "context": "Finally, compare our \u01eb-robustness to the one of [Ghosh et al., 2015]: RD,l(\u03b8\u0303 \u2217) \u2264 (1 \u2212 2max(p\u2212, p+))\u22121RD,l(\u03b8\u2217).", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": "[2013], at least for small |Y| [Sukhbaatar and Fergus, 2014].", "startOffset": 31, "endOffset": 60}, {"referenceID": 5, "context": "In principle they can be tuned as hyper-parameters Natarajan et al. [2013], at least for small |Y| [Sukhbaatar and Fergus, 2014].", "startOffset": 51, "endOffset": 75}, {"referenceID": 25, "context": "The other parameters l, T, \u03bb are the same for both algorithms; the learning rate \u03b7 is untouched from [Shalev-Shwartz et al., 2011] and not tuned for \u03bcsgd.", "startOffset": 101, "endOffset": 130}, {"referenceID": 32, "context": "A kernelized version of this Lemma is given in [Song et al., 2009].", "startOffset": 47, "endOffset": 66}, {"referenceID": 33, "context": "To show that, notice that we satisfy all hypotheses of the Representer Theorem [Sch\u00f6lkopf and Smola, 2002].", "startOffset": 79, "endOffset": 106}, {"referenceID": 9, "context": "Beside that, since we reason at the higher level of wsl, we can frame a solution for pu simply calling \u03bcsgd on \u03bc\u0302 defined above or on estimators improved by exploiting results of [Patrini et al., 2014].", "startOffset": 179, "endOffset": 201}, {"referenceID": 34, "context": "Learning reductions Solving a machine learning problem by solutions to other learning problems is a learning reduction [Beygelzimer et al., 2015].", "startOffset": 119, "endOffset": 145}, {"referenceID": 35, "context": "Following [Beygelzimer et al., 2005], we define a wsl task as a triple (K,Y, l), with weakly supervised advice K, predictions space Y and loss l, and we reduce to binary classification (Y,Y, l).", "startOffset": 10, "endOffset": 36}, {"referenceID": 36, "context": "Beyond \u03bcsgd meta-\u03bcsgd is intimately similar to stochastic average gradient (sag) [Schmidt et al., 2013].", "startOffset": 81, "endOffset": 103}, {"referenceID": 12, "context": "A better (?) picture of robustness The data-dependent worst-case result of [Long and Servedio, 2010], like any extreme-case argument, should be handled with care.", "startOffset": 75, "endOffset": 100}], "year": 2016, "abstractText": "We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the loss. This holds true even for non-smooth, non-convex losses and in any rkhs. The first term is a (kernel) mean operator \u2013the focal quantity of this work\u2013 which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like sgd and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.", "creator": "LaTeX with hyperref package"}}}