{"id": "1602.02658", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Graying the black box: Understanding DQNs", "abstract": "in recent writing there is a growing interest in using deep representations for reinforcement learning. in this paper, we present a methodology for tools to analyze deep representation - networks ( dqns ) in a non - conceptual matter. using our tools we reveal that the features learned by dqns aggregate the state space in a hierarchical fashion, reducing its success. moreover we are able to understand and process the characteristics learned by builders for three massive atari2600 games and define ways to interpret, debug and optimize of coherent neural networks in reinforcement learning.", "histories": [["v1", "Mon, 8 Feb 2016 17:27:31 GMT  (7148kb,D)", "http://arxiv.org/abs/1602.02658v1", null], ["v2", "Tue, 9 Feb 2016 16:13:00 GMT  (7148kb,D)", "http://arxiv.org/abs/1602.02658v2", null], ["v3", "Wed, 17 Feb 2016 19:15:55 GMT  (7149kb,D)", "http://arxiv.org/abs/1602.02658v3", null], ["v4", "Mon, 24 Apr 2017 09:57:21 GMT  (7660kb,D)", "http://arxiv.org/abs/1602.02658v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["tom zahavy", "nir ben-zrihem", "shie mannor"], "accepted": true, "id": "1602.02658"}, "pdf": {"name": "1602.02658.pdf", "metadata": {"source": "META", "title": "Graying the black box: Understanding DQNs", "authors": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "emails": ["TOMZAHAVY@CAMPUS.TECHNION.AC.IL", "BETZINIR@GMAIL.COM", "SHIE@EE.TECHNION.AC.IL"], "sections": [{"heading": "1. Introduction", "text": "In the reinforcement learning (RL) paradigm, an agent autonomously learns from experience in order to maximize some reward signal. Learning to control agents directly from high-dimensional inputs like vision and speech is a hard problem in RL, known as the curse of dimensionality. Countless solutions to this problem have been offered including linear function approximators (Tsitsiklis & Van Roy, 1997), hierarchical representations (Dayan & Hinton, 1993) state aggregation (Singh et al., 1995) and options (Sutton et al., 1999). These methods rely upon engineering problem specific state representations, thus making the agent not fully autonomous and reducing its flexibility. Therefore, there is a growing interest in using nonlinear function approximators, that are more general and require less domain specific knowledge, e.g., TD-gammon (Tesauro, 1995). Unfortunately, such methods are known to be unstable or even to diverge when used to represent the action-value function (Tsitsiklis & Van Roy, 1997; Gordon, 1995; Riedmiller, 2005). The Deep Q-Network (DQN) algorithm (Mnih et al., 2015) increased training stability by introducing the target network and by using Experi-\n* These authors have contributed equally\nence Replay (ER) (Lin, 1993). Its promise was demonstrated in the Arcade Learning Environment (ALE) (Bellemare et al., 2012), a challenging framework composed of dozens of Atari games used to evaluate general competency in AI. It achieved dramatically better results than earlier approaches, showing a robust ability to learn representations.\nWhile using Deep Learning (DL) in RL looks promising, there is no free lunch. Training a Deep Neural Network (DNNs) is a complex optimization process. At the inner level we fix a hypothesis class and learn it using some gradient descent algorithm. The optimization of the outer level addresses the choice of network architecture, setting hyperparameters, and even the choice of optimization algorithm. In Deep Reinforcement Learning (DRL) we also need to choose how to model the environment as a Markov Decision Process (MDP), i.e., to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015). While the optimization of the inner level is analytic to a high degree, the optimization of the outer level is far from being so. Currently, most practitioners tackle this problem either using a trial and error methodology, or by exhaustively searching over the space of possible configurations (moreover, the lack of interpretability regarding the success or failure of a DNN makes it a manually tedious task and diminishes it\u2019s wide use).\nA key issue in RL is that of representation, i.e., allowing the agent to locally generalize the policy across similar states. Unfortunately, spatially similar states often induce different control rules in contrast to other machine learning benchmarks that enjoy local generalization. As a consequence, learning a policy (or a value function) using a function approximator is often hindered by discontinuities in the state. For example, in the Atari2600 game Seaquest, whether or not a diver has been collected (represented by a few pixels) controls the outcome of the submarine surface action (without-loose a life, with-fill air). Another cause for these discontinuities is that for a given problem, two states with\nar X\niv :1\n60 2.\n02 65\n8v 1\n[ cs\n.L G\n] 8\nF eb\n2 01\n6\nsimilar representations may in fact be far from each other in terms of the number of state transitions required to reach one from the other.\nThus, methods that focus on the temporal structure of the policy has been proposed. Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; S\u0327ims\u0327ek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998). Given a good representation of the states, varying levels of convergence guarantees has been promised (Dean & Lin, 1995; Parr, 1998; Hauskrecht et al., 1998; Dietterich, 2000).\nIn this work we analyze the representation of states learned by DRL agents. We argue that DQN is learning a hierarchical spatio-temporal aggregation of the state space using different sub manifolds to represent the policy thus explaining its success. We show that by doing so we can give a qualitative understanding of learned policies that can help the outer optimization loop and help in the debugging process. Our methodology is to extract the neural activations of the last DQN hidden layer, while it is playing the game, and then apply t-Distributed Stochastic Neighbor Embedding (t-SNE) (Van der Maaten & Hinton, 2008) for dimensionality reduction and visualization. On top of this low dimensional representation we display the dynamics, different game measures and hand crafted features, so that we are able to describe what each sub-manifold represents. We also use saliency maps to analyze the influence of different features on the network. In particular, our main contributions are the following:\n\u2022 We suggest an explanation for the success of DQN based on its learned representation, by showing that DQN is mapping states to sub-manifolds with mutual spatial and temporal structure and learning specific policies at each cluster, so it can tackle state discontinuities.\n\u2022 We give an interpretation for the agents policy in a clear way, thus allowing to understand what are its weaknesses and strengths.\n\u2022 We propose methods for debugging and reducing the hyper parameters grid search of DRL and give examples on game modeling, termination and initial states, score over-fitting, etc."}, {"heading": "2. Related work", "text": "The goal behind visualization of DNN is to give better understanding of these inscrutable black boxes. While some approaches study the type of computation performed at each layer as a group (Yosinski et al., 2014), others try to explain the function computed by each individual neuron\n(similar to the \u201dJennifer Aniston Neuron\u201d (Quiroga et al., 2005)). Dataset-centric approaches display images from the training or test set that cause high or low activations for individual units, e.g., the deconvolution method (Zeiler & Fergus, 2014) highlights the portions of a particular image that are responsible for the firing of each neural unit. Network-centric approaches investigate a network directly without any data from a dataset, e.g., Erhan et al. (Erhan et al., 2009) synthesized images that cause high activations for particular units. Other works used the input gradient to find images that cause strong activations (e.g., (Simonyan & Zisserman, 2014); (Nguyen et al., 2014); (Szegedy et al., 2013)).\nSince RL research was mostly focused on linear function approximations and policy gradient methods, we are less familiar with visualization techniques and attempts to understand the structure learnt by an agent. Wang et al. (Wang et al., 2015) suggested to use saliency maps and analyzed which pixels are more active at network predications. Using this method they compared between the standard DQN and their dueling network architecture. Engel & Mannor (2001) learnt an embedded map of Markov processes and visualized it on 2 dimensions, their analysis is based on the state transition distribution while we will focus on distances between the features learned by DQN."}, {"heading": "3. Background", "text": "This section includes relative background on RL, DQN and t-SNE."}, {"heading": "3.1. Reinforcement Learning", "text": "The goal of RL agents is to maximizes its expected total reward by learning an optimal policy (mapping from states to actions). At time t the agent observes a state st, selects an action at, and receives a reward rt, following the agents decision it observes the next state st+1 . We consider infinite horizon problems where the cumulative return is discounted by a factor of \u03b3 \u2208 [0, 1] and the return at time t is given by Rt = \u2211T t\u2032=t \u03b3\nt\u2032\u2212trt, where T is the termination step. The action-value function Q\u03c0(s, a) measures the expected return after observing state st and taking an action under a policy \u03c0, Q(s, a) = E[Rt|st = s, at = a, \u03c0]. The optimal action-value obeys a fundamental recursion known as the Bellman equation,\nQ\u2217(st, at) = E [ rt + \u03b3max\na\u2032 Q\u2217(st+1, a\n\u2032) ]"}, {"heading": "3.2. Deep Q Networks", "text": "The DQN algorithm (Mnih et al., 2015; 2013) approximates the optimal Q function with a Convolutional Neural Network (CNN), by optimizing the network weights such\nthat the expected TD error of the optimal bellman equation is minimized:\nEst,at,rt,st+1 \u2016Q\u03b8 (st, at)\u2212 yt\u2016 2 2\nyt = 0 st is terminal\u03b3 [rt + max a\u2019 Q\u03b8target ( st+1, a \u2032 )] else\nNotice that this is an off-line algorithm, meaning that the tuples {st,at, rt, st+1, \u03b3} are collected from the agents experience, stored in the ER and later used for training. The DQN maintains two separate Q-networks with current parameters \u03b8 and target parameters \u03b8target that are updated from \u03b8 every fixed number of iterations. In order to capture the game dynamics, DQN represents a state by a sequence of history frames and pads initial states with zero frames.\nt-SNE is a visualization technique for high dimensional data by giving each data point a location in a two or threedimensional map. It has been proven to outperform linear dimensionality reduction methods and non-linear embedding methods such as ISOMAP (Tenenbaum et al., 2000) in several research fields including machine-learning benchmark datasets and hyper-spectral remote sensing data (Lunga et al., 2014). The technique is relatively easy to optimize, and reduces the tendency to crowd points together in the center of the map. It is known to be particularly good at creating a single map that reveals structure at many different scales, which is particularly important for highdimensional data that lie on several different, but related, low-dimensional manifolds."}, {"heading": "4. Methods", "text": "Our tool is displayed in Figure 1. It contains a visualization of the t-SNE map (Left), tools for visualizing global features (top right), tools for visualizing game specific features (middle right), and visualization of the chosen state and its corresponding gradient image (bottom right)."}, {"heading": "4.1. t-SNE", "text": "To produce the t-SNE figures, we train a DQN for each Atari2600 game and then let it play with an epsilon greedy scheme. We collect 120k game states, for each we store the neural activations of the last hidden layer, and apply the Barnes Hut t-SNE (Van Der Maaten, 2014) which is useful for large data sets. We also pre-process the data using Principal Component Analysis to dimensionality of 50. All experiments were performed with 3000 iterations perplexity of 30."}, {"heading": "4.2. Coloring the t-SNE map with different measures", "text": "We assign each state with its measure such as value estimates (value, Q, advantage), generation time, termination, (decide how too show the gui and detail all measure we use), and use it to color the t-SNE map. We also extract hand-crafted features, directly from the emulator raw frames, such as player position, enemy position, number of lives, and so on. We use these features to filter points in the t-SNE image. The filtered images reveal insightful patterns that cannot be seen otherwise. From our experience, handcrafted features are very powerful, however the drawback of using them is that they require manual work. Similar to (Engel & Mannor, 2001) we visualize the dynamics of the learned policy. However we use a 3d t-SNE state representation and display transitions upon it with arrows."}, {"heading": "4.3. Saliency maps", "text": "We generate Saliency maps, similar to (Simonyan & Zisserman, 2014), by computing the Jacobians of the trained value with respect to the input video. The Jacobians image is presented above the input image itself and helps to understand which pixels in the image affect the value prediction the most."}, {"heading": "4.4. Analysis", "text": "Using these measures we are able to understand the common attributes of a given cluster (e.g Figure 3 in the apendix). From the analysis of the agent dynamics between clusters we can point a hierarchical aggregation of the state space. We define the clusters that has a clear entrance and termination areas as option clusters and we are able to give an interpretation for the agent policy in those clusters. For some options we are able to derive rules for initiation and\ntermination, e.g., landmark options (Mann et al., 2015) has unique termination state."}, {"heading": "5. Experiments", "text": "We applied our methodology on three ATARI games: Breakout, Pacman and Seaquest. For each one we give a short description of the game, analyze the optimal policy, detail the features we designed, interpret the DQN\u2019s policy and derive conclusions. We finish by analyzing initial and terminal states and the influence of score pixels."}, {"heading": "5.1. Breakout", "text": "In Breakout, a layer of bricks lines the top of the screen. A ball travels across the screen, bouncing off the top and side walls of the screen. When a brick is hit, the ball bounces away, the brick is destroyed and the player receives reward based on the bricks layer. The player loses a turn when the ball touches the bottom of the screen. To prevent this from happening, the player has a movable paddle to bounce the ball upward, keeping it in play. The highest score achievable for one player is 896; this is done by eliminating exactly two screens of bricks worth 448 points each.\nWe extract features for the player (paddle) position, ball\u2019s position, balls direction of movement, number of missing bricks, and a tunnel feature. We say that a tunnel exists when there is a clear path between the area below the bricks and the area above it. We approximate this event by looking\nfor at least one clear column of bricks.\nA good strategy for Breakout is leading the ball above the bricks by digging a tunnel in one side of the bricks block. Doing so enables the ball to bounce on the bricks achieving reward while the agent is safe from loosing it. By introducing a discount factor to Breakout\u2019s MDP, this strategy becomes even better since it achieves high immediate reward.\nFigure 2 presents the t-SNE of Breakout. The agent learns a hierarchical policy: first carve a tunnel in the left side of the screen and then keep the ball above the bricks as long as possible. In clusters (1-3) the agent is carving the left tunnel. Once the agent enters those clusters it will not\nleave them until the tunnel is carved (see Figure 14), thus, we define them as a landmark option (Mann et al., 2015). The clusters are separated from each other by the ball position and direction. In cluster 1 the ball is heading toward the tunnel, in cluster 2 the ball is at the right side and in cluster 3 the ball bounces back from the tunnel after hitting bricks. As less bricks remain in the tunnel the value is gradually rising till the tunnel is carved and the value is maximal (this makes sense since the agent is enjoying high reward rate straight after reaching it). Once the tunnel is carved, the option is terminated and the agent moves to clusters 4-7 (dashed red line), sorted by the ball position with regard to the bricks (see Figure 2). Again the clusters are distinguished by the ball position and direction. In cluster 4 and 6 the ball is above the bricks and in 5 it is below them. Clusters 8 and 9 represent termination and initial states respectively (See Figure 2 in the appendix for examples of states along the option path).\nCluster 7 is created due to a bug in the emulator that allows the ball to pass the tunnel without completely opening the tunnel, however the agent learned to represent this case to its own cluster and assigned it high value estimates (same as the other tunnel clusters). This observation is interesting since it indicates that the agent is learning a representation based on the game dynamics and not only from the pixels.\nWe also identify two policy downsides. First, by coloring the t-SNE based on time, we note that states with only a few bricks are visited for multiple times along the trajectory, indicating that the agent is stuck in a local optima (see Figure 1 in the appendix). We discuss the inability of the\nagent to perform well in the second level of the game in Section 5.5."}, {"heading": "5.2. Seaquest", "text": "In Seaquest, the player\u2019s goal is to retrieve as many treasure-divers, while dodging and blasting enemy subs and killer sharks before the oxygen runs out. When the game begins, each enemy is worth 20 points and a rescued diver worth 50. Every time the agent surface with six divers, killing an enemy (rescuing a diver) is increased by 10 (50) points up to a maximum of 90 (1000). Moreover the agent is awarded with an extra bonus based on its remaining oxygen level and receives an extra life. However if it surface with less than six divers the oxygen fills up with no bonus, and if it surface with none it loses a life.\nDQN\u2019s performance on Seaquest(\u223c5k) is inferior to human experts(\u223c100k). What makes Seaquest hard for DQN is that the shooting enemies is rewarded immediately, while rescuing divers is rewarded only once six divers are collected and rescued to sea level, therefore requires much longer planning.\nWe extract features for player position, direction of movement (ascent/descent), oxygen level, number of enemies, number of available divers to collect, number of available divers to use, and number of lives.\nFigure 4 shows the t-SNE map divided into different clusters. We notice two main partitions of the t-SNE clusters, between low (clusters 5-7) and high (cluster 1-3) oxygen\nlevels, and by the amount of collected divers (clusters 2 and 11 both represent having a diver). We also identified other partitions between clusters such as refueling clusters (1: pre-episode refueling and 2: in-episode refueling), various termination clusters (8: agent appears in black and white, 9: agent\u2019s figure is replaced with drops, 10: agent\u2019s figure disappears) and low oxygen clusters characterized by flickering in the oxygen bar (4 and 7).\nWhile the agent distinguishes states that are with a collected diver, Figure 6 implies that the agent did not understand the concept of collecting a diver and sometimes treats them as enemies. Moreover we see that the clusters share a similar value estimate that is highly correlated with the amount of remaining oxygen and the number of present enemies. However states with an available or collected diver do not raise the value estimates nor do states that are close to refueling. Moreover, the agent never explores the bottom of the screen, nor collects more than two divers. Thus it never experienced surfacing to sea level with 6 divers and never achieving the high reward bonus.\nAs a result, the agent\u2019s policy is to kill as many enemies as possible while avoiding being shot at. If it hasn\u2019t collected\na diver, the agent follows a sub-optimal policy and ascends near to sea level as the oxygen decreases. There it continues to shoot at enemies (but not collecting divers as it should). However it also learned not to surface entirely without a diver.\nIf the agent collects a diver it moves to the blue shaded clusters (all clusters in this paragraph refer to Figure 5), where we identify a refuel option. We noticed that the option can be initiated from two different zones based on the oxygen level but has a singular termination cluster (3b). If the diver is taken while having a high level of oxygen, then it enters the option at the northern (red) part of cluster 1. Otherwise it will enter on a later point along the direction of the option (red arrows). In cluster 1, the agent keeps following the normal shooting policy. Eventually the agent reaches a critical level of oxygen (cluster 3) and ascends to sea level. From there the agent jumps to the fueling cluster (area 1b). The fueling cluster is identified by its rainbow appearance because the level of oxygen is increasing rapidly. However, the refuel option was not learned perfectly. Area 2 is another refuel cluster, there, the agent does not exploit its oxygen before ascending to get air (area 2b)."}, {"heading": "5.3. Pacman", "text": "In Pacman, an agent navigates in a maze while being chased by two ghosts. The agent is positively rewarded (+1) for collecting bricks. An episode ends when a preda-\ntor catches the agent, or when the agent collects all bricks. There are also 4 bonus bricks, one at each corner of the maze. The bonus bricks provide larger reward (+5), and more importantly, they make the ghosts vulnerable for a short period of time, during which they cannot kill the agent. Occasionally, a bonus box appears for a short time providing high reward (+100) once collected.\nWe extract features for player position, direction of movement, number of left bricks to eat, minimal distance (L1) between the player and the predators, number of lives, ghost mode that indicates that the predators are vulnerable, and bonus box feature that indicate when a highly valued box appears.\nFigure 8 shows the t-SNE colored by value function, while Figure 7 shows the t-SNE colored by the number of left bricks with examples of states from each cluster. We can see that the clusters are well partitioned by the number of remaining bricks and value estimates. Moreover, examining the states in each cluster (Figure 7) we see that the clusters share specific bricks pattern and agent location. From Figure 9 we also learn that the agent collects the bonus bricks at very specific times. Thus, we understand that the agent has learned a location-based policy that is focused on collecting the bonus bricks, similar to maze problems, but one that is synchronous with avoiding the ghosts.\nThe agent starts at cluster 1, heading for the bottom-left bonus brick and collecting it in cluster 2. Once collected, it moves to cluster 3 where it is heading to the top-right bonus brick and collects it in cluster 4. The agent is then moving to clusters 5 and 6 where it is taking the bottom-right and\ntop-left bonus bricks respectively. We also identified cluster 7 and 9 as termination clusters and cluster 8 as a hiding cluster in which the agent hides from the ghosts in the topleft corner since very few bricks remains.\nThe main downside of the learned policy is observed from looking at cluster 10. In this cluster, the bonus box is visible indicating that the agent learned to separate this situation from others, however we can see that the cluster has a low value estimate indicating that the agent did not learn the right value function yet."}, {"heading": "5.4. Enviroment modeling", "text": "The DQN algorithm requires specific treatment for initial (padded with zero frames) and terminal (receive target value of zero) states, however it is not clear how to check if this treatment works well. Therefore we show t-SNE maps for different games with a focus on termination and initial\nstates in Figure 10. We can see that all terminal states are mapped successfully into a singular zone, however, initial states are also mapped to singular zones and assigned with wrong value predictions. Following these observations we suggest to investigate different ways to model initial states, i.e., replicating a frame instead of feeding zeros and test it using our methodology. We also note that while terminal states seems to be modeled correctly, there seems to be a better way to model them, e.g., by setting the target to be zero if the next state is terminal."}, {"heading": "5.5. Score pixels", "text": "Some Atari2600 games include multiple repetitions of the same game. Once the agent finished the first screen it is presented with another one, distinguished only by the score pixels. Therefore, an agent might encounter problems with generalizing to the new screen if it over-fits the score pixels. In breakout, for example, the current state of the art architectures achieves a game score of around 450 points\nwhile the maximal available points are 896 suggesting that the agent is somehow not learning to generalize for the new level. We investigated the affect that score pixels has on the network predictions. Figure 11 shows the saliency maps of different games supporting our claim that DQN is basing its estimates using these pixels, and Figure 12 shows that the network maps states with different amount of score digits to different zones in the game of Seaquest. We suggest to further investigate these affects, for example we suggest to train an agent that does not receive those pixels as input."}, {"heading": "6. Conclusions", "text": "Our experiments on ALE indicate that the features learned by DQN maps the state space to different sub-manifolds, in each, different features are present. By analyzing the dynamics in these clusters and between them we were able to identify hierarchical structures. In particular we are able to identify options with defined initial and termination rules. The states aggregation gives the agent the ability to learn specific policies for the different regions, thus giving an explanation to the success of DRL agents.\nSimilar to Neuro-Science, where reverse engineering methods like fMRI reveal structure in brain activity, we demonstrated how to describe the agent\u2019s policy with simple logic rules by processing the network\u2019s neural activity. This is important since often humans can understand the optimal policy and therefore understand what are the agent\u2019s weaknesses. Moreover, the ability to understand the hierarchical structure of the policy can help in distilling it into a simpler architecture (Rusu et al., 2015; Parisotto et al., 2015).\nDebugging the network using our methodology discovered that the network is over-fitting specific pixels, therfore, we suggest to crop the redundant pixels from the input frames. We also noticed problems in modeling initial and terminal states. We suggest to model initial states by replicating the state instead of padding it with zeroes and to model the terminal states in the network itself and not the target function.\nThe state aggregation learned by the agent may be used to design better algorithms. One possibility is to learn a classifier from a states to clusters based on the t-SNE map and then learn a different control rule at each cluster. Another option is to use human understanding of the learned clusters and to motivate the network to join redundant clusters and create new clusters. For example, in Seaquest, clusters that are separated by the amount of score digits may be joined to encourage better generalization, and a cluster that distinguishes states with an available diver should be created."}, {"heading": "7.1. Breakout", "text": ""}, {"heading": "7. Suplemantary figures", "text": "7.2. Seaquest\n7.3. Pacman"}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "arXiv preprint arXiv:1207.4708,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Bellemare", "Marc G", "Ostrovski", "Georg", "Guez", "Arthur", "Thomas", "Philip S", "Munos", "R\u00e9mi"], "venue": "arXiv preprint arXiv:1512.04860,", "citeRegEx": "Bellemare et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2015}, {"title": "Feudal reinforcement learning. pp. 271\u2013271", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Dayan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1993}, {"title": "Decomposition techniques for planning in stochastic domains", "author": ["Dean", "Thomas", "Lin", "Shieu-Hong"], "venue": "Citeseer,", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Dietterich and G.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich and G.", "year": 2000}, {"title": "Learning embedded maps of markov processes", "author": ["Engel", "Yaakov", "Mannor", "Shie"], "venue": "In in Proceedings of ICML 2001. Citeseer,", "citeRegEx": "Engel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2001}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Dept. IRO, Universite\u0301 de Montre\u0301al, Tech. Rep,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Stable function approximation in dynamic programming", "author": ["Gordon", "Geoffrey J"], "venue": null, "citeRegEx": "Gordon and J.,? \\Q1995\\E", "shortCiteRegEx": "Gordon and J.", "year": 1995}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Manifold-learning-based feature extraction for classification of hyperspectral data: a review of advances in manifold learning", "author": ["Lunga", "Dalton", "Prasad", "Santasriya", "Crawford", "Melba M", "Ersoy", "Ozan"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Lunga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lunga et al\\.", "year": 2014}, {"title": "Approximate value iteration with temporally extended actions", "author": ["Mann", "Timothy A", "Mannor", "Shie", "Precup", "Doina"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Mann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2015}, {"title": "Dynamic abstraction in reinforcement learning via clustering", "author": ["Mannor", "Shie", "Menache", "Ishai", "Hoze", "Amit", "Klein", "Uri"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Q-cutdynamic discovery of sub-goals in reinforcement learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Machine Learning: ECML", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "arXiv preprint arXiv:1412.1897,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision problems", "author": ["Parr", "Ronald"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Parr and Ronald.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Ronald.", "year": 1998}, {"title": "Invariant visual representation by single neurons in the human", "author": ["Quiroga", "R Quian", "Reddy", "Leila", "Kreiman", "Gabriel", "Koch", "Christof", "Fried", "Itzhak"], "venue": "brain. Nature,", "citeRegEx": "Quiroga et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Quiroga et al\\.", "year": 2005}, {"title": "Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Riedmiller", "Martin"], "venue": "In Machine Learning: ECML", "citeRegEx": "Riedmiller and Martin.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller and Martin.", "year": 2005}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Wolfe", "Alicia P", "Barto", "Andrew G"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Reinforcement learning with soft state aggregation", "author": ["Singh", "Satinder P", "Jaakkola", "Tommi", "Jordan", "Michael I"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Singh et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1995}, {"title": "Automated discovery of options in reinforcement learning", "author": ["Stolle", "Martin"], "venue": "PhD thesis, McGill University,", "citeRegEx": "Stolle and Martin.,? \\Q2004\\E", "shortCiteRegEx": "Stolle and Martin.", "year": 2004}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "De Silva", "Vin", "Langford", "John C"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Temporal difference learning and TDGammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Learning metric-topological maps for indoor mobile robot navigation", "author": ["Thrun", "Sebastian"], "venue": "Artificial Intelligence,", "citeRegEx": "Thrun and Sebastian.,? \\Q1998\\E", "shortCiteRegEx": "Thrun and Sebastian.", "year": 1998}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["Tsitsiklis", "John N", "Van Roy", "Benjamin"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Tsitsiklis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1997}, {"title": "Accelerating t-SNE using treebased algorithms", "author": ["Van Der Maaten", "Laurens"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Maaten and Laurens.,? \\Q2014\\E", "shortCiteRegEx": "Maaten and Laurens.", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer Vision\u2013 ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Countless solutions to this problem have been offered including linear function approximators (Tsitsiklis & Van Roy, 1997), hierarchical representations (Dayan & Hinton, 1993) state aggregation (Singh et al., 1995) and options (Sutton et al.", "startOffset": 194, "endOffset": 214}, {"referenceID": 24, "context": ", 1995) and options (Sutton et al., 1999).", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": "Its promise was demonstrated in the Arcade Learning Environment (ALE) (Bellemare et al., 2012), a challenging framework composed of dozens of Atari games used to evaluate general competency in AI.", "startOffset": 70, "endOffset": 94}, {"referenceID": 19, "context": ", to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015).", "startOffset": 149, "endOffset": 258}, {"referenceID": 33, "context": ", to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015).", "startOffset": 149, "endOffset": 258}, {"referenceID": 1, "context": ", to choose the discount factor, the amount of history frames that represent a state, and to choose between the various algorithms and architectures (Nair et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015).", "startOffset": 149, "endOffset": 258}, {"referenceID": 12, "context": "Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998).", "startOffset": 88, "endOffset": 152}, {"referenceID": 11, "context": "Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998).", "startOffset": 88, "endOffset": 152}, {"referenceID": 21, "context": "Such methods decompose the learning task into simpler subtasks using graph partitioning (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005) and path processing mechanisms (Stolle, 2004; Thrun, 1998).", "startOffset": 88, "endOffset": 152}, {"referenceID": 34, "context": "While some approaches study the type of computation performed at each layer as a group (Yosinski et al., 2014), others try to explain the function computed by each individual neuron (similar to the \u201dJennifer Aniston Neuron\u201d (Quiroga et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 17, "context": ", 2014), others try to explain the function computed by each individual neuron (similar to the \u201dJennifer Aniston Neuron\u201d (Quiroga et al., 2005)).", "startOffset": 121, "endOffset": 143}, {"referenceID": 6, "context": "(Erhan et al., 2009) synthesized images that cause high activations for particular units.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": ", (Simonyan & Zisserman, 2014); (Nguyen et al., 2014); (Szegedy et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 25, "context": ", 2014); (Szegedy et al., 2013)).", "startOffset": 9, "endOffset": 31}, {"referenceID": 33, "context": "(Wang et al., 2015) suggested to use saliency maps and analyzed which pixels are more active at network predications.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": ", Erhan et al. (Erhan et al., 2009) synthesized images that cause high activations for particular units. Other works used the input gradient to find images that cause strong activations (e.g., (Simonyan & Zisserman, 2014); (Nguyen et al., 2014); (Szegedy et al., 2013)). Since RL research was mostly focused on linear function approximations and policy gradient methods, we are less familiar with visualization techniques and attempts to understand the structure learnt by an agent. Wang et al. (Wang et al., 2015) suggested to use saliency maps and analyzed which pixels are more active at network predications. Using this method they compared between the standard DQN and their dueling network architecture. Engel & Mannor (2001) learnt an embedded map of Markov processes and visualized it on 2 dimensions, their analysis is based on the state transition distribution while we will focus on distances between the features learned by DQN.", "startOffset": 2, "endOffset": 732}, {"referenceID": 26, "context": "It has been proven to outperform linear dimensionality reduction methods and non-linear embedding methods such as ISOMAP (Tenenbaum et al., 2000) in several research fields including machine-learning benchmark datasets and hyper-spectral remote sensing data (Lunga et al.", "startOffset": 121, "endOffset": 145}, {"referenceID": 9, "context": ", 2000) in several research fields including machine-learning benchmark datasets and hyper-spectral remote sensing data (Lunga et al., 2014).", "startOffset": 120, "endOffset": 140}, {"referenceID": 10, "context": ", landmark options (Mann et al., 2015) has unique termination state.", "startOffset": 19, "endOffset": 38}, {"referenceID": 10, "context": "leave them until the tunnel is carved (see Figure 14), thus, we define them as a landmark option (Mann et al., 2015).", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": "Moreover, the ability to understand the hierarchical structure of the policy can help in distilling it into a simpler architecture (Rusu et al., 2015; Parisotto et al., 2015).", "startOffset": 131, "endOffset": 174}], "year": 2017, "abstractText": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning.", "creator": "LaTeX with hyperref package"}}}