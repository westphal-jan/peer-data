{"id": "1602.03001", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "A Convolutional Attention Network for Extreme Summarization of Source Code", "abstract": "attention mechanisms in neural networks have proved useful for problems in which topical input and output act not have fixed dimension. often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. we introduce functional attentional neural network that establishes convolution on the input tokens to detect local time - invariant and long - range topical attention features in a context - dependent way. we apply this architecture to the problem of biased summarization of source code resources into short, descriptive source name - like summaries. using those tags, thinking machine gradually generates a summary by marginalizing over two filtering mechanisms : one that includes only next assigned token based on the partial weights of the input tokens and another that is able to compose a code token as - is directly observing the summary. we demonstrate our convolutional attention neural network'own performance on 10 popular java installations showing that it achieves highest performance compared to previous attentional mechanisms.", "histories": [["v1", "Tue, 9 Feb 2016 14:36:49 GMT  (57kb)", "http://arxiv.org/abs/1602.03001v1", null], ["v2", "Wed, 25 May 2016 12:18:28 GMT  (67kb)", "http://arxiv.org/abs/1602.03001v2", "Code, data and visualization atthis http URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.SE", "authors": ["miltiadis allamanis", "hao peng", "charles a sutton"], "accepted": true, "id": "1602.03001"}, "pdf": {"name": "1602.03001.pdf", "metadata": {"source": "META", "title": "A Convolutional Attention Network  for Extreme Summarization of Source Code", "authors": ["Miltiadis Allamanis", "Hao Peng"], "emails": ["M.ALLAMANIS@ED.AC.UK", "PENGHAO.PKU@GMAIL.COM", "CSUTTON@INF.ED.AC.UK"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n03 00\n1v 1\n[ cs\n.L G\n] 9\nF eb"}, {"heading": "1. Introduction", "text": "Deep learning for structured prediction problems, in which a sequence (or more complex structure) of predictions need to be made given an input sequence, presents special dif-\n\u2020Work partially done while author was an intern at the University of Edinburgh.\nficulties, because not only are the input and output highdimensional, but the dimensionality is not fixed in advance. Recent research has tackled these problems using neural models of attention (Mnih et al., 2014), which have had great recent successes in machine translation (Bahdanau et al., 2015) and image captioning (Xu et al., 2015). Attentional models have been successful because they separate two different concerns: predicting which input locations are most relevant to each location of the output; and actually predicting an output location given the most relevant inputs.\nIn this paper, we suggest that many domains contain translation-invariant features that can help to determine the most useful locations for attention. For example, in a research paper, the sequence of words \u201cin this paper, we suggest\u201d often indicates that the next few words will be important to the topic of the paper. As another example, suppose a neural network is trying to predict the name of a method in the Java programming language from its body. If we know that this method name begins with get and the method body contains a statement return ____ ; , then whatever token fills in the blank is likely to be useful for predicting the rest of the method name. Previous architectures for neural attention are not constructed to learn translation-invariant features specifically.\nWe introduce a neural convolutional attentional model, that includes a convolutional network within the attention mechanism itself. Convolutional models are a natural choice for learning translation-invariant features while using only a small number of parameters and for this reason have been highly successful in non-attentional models for images (LeCun et al., 1998; Krizhevsky et al., 2012) and text classification (Blunsom et al., 2014). But to our knowledge they have not been applied within an attentional mechanism. Our model uses a set of convolutional layers \u2014 without any pooling \u2014 to detect patterns in the input and\nidentify \u201cinteresting\u201d locations where attention should be focused.\nWe apply this network to an \u201cextreme\u201d summarization problem: We ask the network to predict a short and descriptive name of a source code snippet (e.g. a method body) given solely its tokens. Source code has two distinct roles: it is a means of instructing a CPU to perform a computation but also acts as an essential means of communication among developers who need to understand, maintain and evolve software systems. For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013). Additionally, learning to summarize source code has important applications in software engineering, such as in code understanding and in code search. The highly structured form of source code makes convolution naturally suited for the purpose of extreme summarization. Our choice of problem is inspired by previous work (Allamanis et al., 2015a) that tries to name existing methods (functions) using a large set of hard-coded features, such as features from the containing class and the method signature. But these hard-coded features that may not be available for arbitrary code snippets and in dynamically typed languages. In contrast, in this paper we consider a more general problem: given an arbitrary snippet of code \u2014 without any hard-coded features \u2014 provide a summary, in the form of a descriptive method name.\nThis problem resembles a summarization task, where the name is viewed as a natural language summary to be extracted from source code. However, extreme source code summarization is drastically different from natural language summarization, because unlike natural language, source code is unambiguous and highly structured. Furthermore, a good summary needs to explain how the code instructions compose into a higher-level meaning and not na\u00efvely explain what the code does. This necessitates learning higher-level patterns in the source code that use both the structure of the code and the identifiers to detect and explain complex code constructs. Our extreme summarization problem may also be viewed as a translation task, in the same way that any summarization problem can be viewed as translation. But a significant difference from translation is that the input source code sequence tends to be very large (72 on average in our datasets) and the output summary very small (3 on average in our dataset). The length of the input sequence necessitates the extraction of both temporally invariant attention features and topical sentence-wide features and \u2014 as we show in this paper \u2014 existing techniques from neural machine translation yield sub-optimal results.\nFurthermore, source code presents the additional challenge of out-of-vocabulary words, that is, each new software\nproject and each new source file introduces new vocabulary about aspects of the software\u2019s domain, data structures, and so on, vocabulary which often does not appear in the training set. To address this problem, we introduce a copy mechanism, which uses the convolutional attentional mechanism to identify important tokens in the input even if they are outof-vocabulary tokens that do not appear in the training set. The decoder can then choose to copy tokens directly from the input sequence to the output sequence, resembling the functionality of Vinyals et al. (2015).\nThe key contributions of our paper are: (a) a novel convolutional attentional network that can successfully perform extreme summarization of source code; (b) a comprehensive approach to the extreme code summarization problem, with interest both in the machine learning and software engineering community; and (c) a comprehensive evaluation of four competing algorithms on real-world data that demonstrates the advantage of our method compared to standard attentional mechanisms."}, {"heading": "2. Convolutional Attention Model", "text": "Our convolutional attentional model receives as an input a sequence of code subtokens1 c = [c<S>, c1, . . . cN , c</S>] and outputs an extreme summary in the form of a concise method name. The summary is a sequence of subtokens m = [m<s>,m1 . . .mM ,m</s>]. For example, in the shouldRender method (shown in the top left of Table 3) the input code subtokens are c = [<S>, try, {, return, render, requested, . . .] while the target output is m = [<s>, should, render, </s>]. The neural network predicts each summary subtoken sequentially and models P (mt|m<s>, . . . ,mt\u22121, c). Information about the previously produced tokens m<s>, . . . ,mt\u22121 is passed into a recurrent neural network that represents the current state with a vector ht\u22121 vector. Our convolutional attentional neural network (Figure 1) uses the current state ht\u22121 and a series of convolutions over the embeddings of the tokens c to compute a set of attention features. The resulting features are used to compute one or more normalized attention vectors (e.g. \u03b1 in Figure 1) which are distributions over input token locations containing a weight (in R(0,1)) for each subtoken in c. Finally, given these weights, a representation of the context is computed and is used to predict the probability distribution over the targets mi. This model may also be thought as a generative bimodal model of summary text given a code snippet."}, {"heading": "2.1. Learning Attention Features", "text": "The basic building block of our model is a convolutional network (LeCun et al., 1990; Collobert & Weston, 2008) for extracting position and context dependent features. The input to attention_features is a sequence of code tokens c of length LEN(c) and each location is mapped to a set of attention features into Lfeat, a matrix of size (LEN(c) + const) \u00d7 k2 where the constant is a fixed amount of padding. The intuition behind attention_features is that given the input c, it uses convolution to compute k2 features for each location. By then using ht\u22121 as a multiplicative gating-like mechanism, only the currently relevant features are kept in L2. In the final stage, we normalize L2. attention_features is described with the following pseudocode:\nattention_features (code tokens c, context ht\u22121) C \u2190 LOOKUPANDPAD(c, E) L1 \u2190 RELU(CONV1D(C, Kl1)) L2 \u2190 CONV1D(L1,Kl2)\u2299 ht\u22121 Lfeat \u2190 L2/ \u2016L2\u20162 return Lfeat\n1By subtokens we refer to the parts of a complex source code token e.g. getInputStream contains the get, Input and Stream subtokens.\nHere E \u2208 R|V |\u00d7D contains the D dimensional embedding of each code token and name subtoken (i.e. all possible cis and mis). The two convolution kernels are Kl1 \u2208 R\nD\u00d7w1\u00d7k1 and Kl2 \u2208 Rk1\u00d7w2\u00d7k2 , where w1, w2 are the window sizes of the convolutions. The vector ht\u22121 \u2208 Rk2 represents information from the previous subtokens m0 . . .mt\u22121. CONV1D performs a onedimensional (throughout the length of sentence c) narrow convolution. Note that the input sequence c is padded by LOOKUPANDPAD. The size of the padding is such that with the narrow convolutions, the attention vector (returned by attention_weights) has exactly LEN(c) components. The \u2299 operator is the elementwise multiplication of a vector and a matrix, i.e. B = A \u2299 v for v \u2208 RM and A a M \u00d7 N matrix, Bij = Aijvi. We found the normalization of L2 into Lfeat to be useful during training. We believe it helps because of the widely varying lengths of inputs c. Note that no pooling happens in this model; the input sequence c is of the same length as the output sequence (modulo the padding).\nTo compute the final attention weight vector \u2014 a vector with non-negative elements and unit norm \u2014 we define attention_weights as a function that accepts Lfeat from attention_features and a convolution kernel K of size k2 \u00d7 w3\u00d71. attention_weights returns the normalized attention weights vector with length LEN(c) and is described by the following pseudocode:\nattention_weights (attention features Lfeat, kernel K) return SOFTMAX(CONV1D(Lfeat,K))\nComputing the State ht. Predicting the full summary m is a sequential prediction problem, where each subtokenmt is sequentially predicted given the previous state containing information about the previous subtokens m0 . . .mt\u22121. The state is passed through the vector ht \u2208 Rk2 which is computed using a Gated Recurrent Unit (Cho et al., 2014) i.e.\nGRU(current input xt, previous state ht\u22121) rt \u2190 \u03c3(xtWxr + ht\u22121Whr + br) ut \u2190 \u03c3(xtWxu + ht\u22121Whu + bu) ct \u2190 tanh(xtWxc + rt \u25e6 (ht\u22121Whc) + bc) ht \u2190 (1\u2212 ut) \u25e6 ht\u22121 + ut \u25e6 ct return ht\nwhere \u25e6 denotes the Hadamard product. During testing the next state is computed by ht = GRU(Emt ,ht\u22121) i.e. using the embedding of the current output subtoken mt. For regularization during training, we use a trick similar to Bengio et al. (2015) and with probability equal to the dropout rate we compute the next state as ht = GRU(n\u0302,ht\u22121), where n\u0302 is the predicted embedding."}, {"heading": "2.2. Simple Convolutional Attentional Model", "text": "We now use the components described above as our building block for our extreme summarization model. We first build conv_attention, a convolutional attentional model that uses an attention vector \u03b1 computed from attention_weights to weight the embeddings of the tokens in c and compute the predicted target embedding n\u0302 \u2208 RD. The model returns a probability distribution n \u2208 R|V | over all possible target tokensmt.\nconv_attention (code c, previous state ht\u22121) \u03b1 \u2190attention_weights (Lfeat,Katt) n\u0302 \u2190 \u2211 i \u03b1iEci\nn \u2190 SOFTMAX(E n\u0302\u22a4 + b) return n\nwhere b \u2208 R|V | is a bias vector. We train this model by using maximum likelihood. This model works as follows, we start with the special m0 = <s> subtoken and h0. Then at each timestep t, we sequentially generate each subtoken mt using the probability distribution n returned by calling conv_attention (c,ht\u22121). Given the newly generated token mt the new state ht = GRU(Emt ,ht\u22121) is then computed. The process stops when the special </s> subtoken is generated."}, {"heading": "2.3. Copy Convolutional Attentional Model", "text": "We extend conv_attention by using an additional attention vector \u03ba in a novel copying mechanism that can suggest out-of-vocabulary subtokens. In our data a significant proportion of the output subtokens (about 35%) also appear in c. Motivated by this, we extend conv_attention and allow a direct copy from the input sequence c into the summary. Now the neural network when predicting mt with probability \u03bb copies a token from c into mt and with probability 1 \u2212 \u03bb predicts the target subtoken as in conv_attention. When copying a subtoken ci is copied into mt with probability equal to the attention weight \u03bai. This process is described by the following pseudocode:\ncopy_attention (code c, previous state ht\u22121) Lfeat \u2190 attention_features (c,ht\u22121) \u03b1 \u2190attention_weights (Lfeat,Katt) \u03ba \u2190attention_weights (Lfeat,Kcopy) \u03bb \u2190 max(\u03c3(CONV1D(Lfeat,K\u03bb))) n\u0302 \u2190 \u2211 i \u03b1iEci\nn \u2190 SOFTMAX(E n\u0302\u22a4 + b) return \u03bbPOS2VOC(\u03ba, c) + (1\u2212 \u03bb)TOMAP(n, V )\nwhere \u03c3 is the sigmoid function, Katt, Kcopy and K\u03bb are different convolutional kernels, n \u2208 R|V |, \u03b1,\u03ba \u2208 R\nLEN(c), POS2VOC is a method that maps each subtoken ci to the probability \u03bai assigned by the copy mechanism. TOMAP maps each subtoken in V to its probabil-\nity ni. Finally, the two maps are merged, returning a map that contains all potential target subtokens in V \u222a c and marginalizing over the copy probability \u03bb. Note that \u03b1 and \u03ba are analogous attention weights but are computed from different kernels. Note that n is computed exactly as in conv_attention.\nObjective. To obtain signal from the copying mechanism and \u03bb for each prediction step, we input to copy_attention a binary vector Ic=mt of size LEN(c) where each component is one if the code token is identical to the current target subtoken mt. We can then compute the probability of a correct copy and train using maximum likelihood over the marginalization of the two mechanisms, i.e.\nP (mt|ht\u22121, c) = \u03bb\n\u2211 i \u03baiIci=mt\u2211\ni \u03bai + (1\u2212 \u03bb)\u00b5rmt\nwhere the first term is the probability of a correct copy (weighted by \u03bb) and the second term is the probability of the target token mt (weighted by 1\u2212\u03bb). We use \u00b5 \u2208 (0, 1] to penalize the model when the simple attention part of the model needs to predict the special UNK subtoken that could have been predicted exactly by the copy mechanism, otherwise \u00b5 = 1. In this work, we arbitrarily used \u00b5 = e\u221210, although variations do not affect the performance."}, {"heading": "2.4. Predicting Names", "text": "To predict a full method name, we use a hybrid BFS and beam search method. We start all predictions from the special m0 = <s> subtoken, while we maintain a (max-)heap of the highest probability partial predictions so far. At each step, we pick the highest probability prediction and predict its next possible subtokens, pushing them back to the heap. If the special </s> subtoken is produced the suggestion is moved onto the list of suggestions. Since we are interested in producing the list of the top k suggestions, at each point, we prune partial suggestions that have a probability less than the current best kth full suggestion. To make this process tractable, we limit the size of the partial suggestion heap and stop iterating after 100 steps."}, {"heading": "3. Evaluation", "text": "Dataset Collection. We are interested in the extreme summarization problem where we summarize a source code snippet into a short and concise method-like name. Although such a dataset does not exist for arbitrary snippets of source code, it is natural to consider existing method (function) bodies as our snippets and the method names picked by the developers as our target extreme summaries.\nTo collect a good quality dataset, we cloned 11 open source Java projects found on GitHub. We obtained the most popular projects by taking the sum of the z-scores of\nthe number of watchers and forks of each project, using GHTorrent (Gousios & Spinellis, 2012). We selected the top 11 projects that contained more than 10MB of source code files each and use libgdx as a development set. These projects have thousands of forks and stars, being widely known among software developers. The projects along with short descriptions are shown in Table 1. We used this procedure to select a mature, large, and diverse corpus of real source code. For each file, we extract the Java methods, removing methods that are overridden, are abstract or are the constructors of a class. We find the overridden methods by an approximate static analysis that checks for inheritance relationships and the @Override annotation. Overridden methods are removed, since they are highly repetitive and their names are easy to predict. Any full tokens that are identical to the method name (e.g. in recursion) are replaced with a special SELF token. We split and lowercase each method name and code token into subtokens {mi} and {ci} on camelCase and snake_case. The dataset can be found at http://groups.inf.ed.ac.uk/cup/codeattention/.\nExperimental Setup. To measure the quality of our suggestions we compute two scores. Exact match shows the percent of the method names predicted exactly, while the F1 score is computed in a per-subtoken basis. When suggesting summaries, each model returns a ranked list of suggestions. We compute exact match and F1 at rank 1 and 5, as the best score achieved by any one of the top suggestions (i.e. if the fifth suggestion achieves the best F1 score, we use this one for computing F1 at rank 5). Using the BLEU score (Papineni et al., 2002) would have been possible, but it would not be different from F1 given the short lengths of our output sequences (3 on average). We use each project separately, training one neural network for each project and testing on the respective test set. This is because each project\u2019s domain varies widely and little information can be transferred among them, due to the principle of code reusability of software engineering. We note that we attempted to train a single model using all project training sets and tested on each project separately but this yielded significantly worse results for all algorithms.\nFor each project, we split the files uniformly at random into a training (65%), a validation (5%) and a test (30%) set. Each file is a top-level Java class. We perform hyperparameter optimizations using Bayesian optimization with spearmint (Snoek et al., 2012) maximizing the F1 at rank 5.\nFor comparison, we use two different algorithms: a tf-idf algorithm that computes a tf-idf vector from the code snippet subtokens and suggests the names of the nearest neighbors using cosine similarity. We also use the standard attention model of Bahdanau et al. (2015) that uses a biRNN\nand fully connected components, that has been successfully used in machine translation. We perform hyperparameter optimizations following the same protocol on libgdx.\nTraining. To train conv_attention and copy_attention we optimize the objective using stochastic gradient descent with RMSProp and Nesterov momentum (Sutskever et al., 2013; Hinton et al., 2012). We use dropout (Srivastava et al., 2014) on all parameters, parametric leaky RELUs (Maas et al., 2013; He et al., 2015) and gradient clipping. Each of the parameters of the model is initialized with normal random noise around zero, except for b that is initialized to the log of the empirical frequency of each target token in the training set. For conv_attention the optimized hyperparameters are k1 = k2 = 8, w1 = 24, w2 = 29, w3 = 10, dropout rate 50% and D = 128. For copy_attention the optimized hyperparameters are k1 = 32, k2 = 16, w1 = 18, w2 = 19, w3 = 2, dropout rate 40% and D = 128."}, {"heading": "3.1. Quantitative Evaluation", "text": "Table 1 shows the F1 scores achieved by the different methods for each project while Table 2 shows a quantitative evaluation, averaged across all projects. \u201cStandard Attention\u201d refers to the machine translation model of Bahdanau et al. (2015). The tf-idf algorithm seems to be performing very well, showing that the bag-of-words representation of the input code is a strong indicator of its name. Interestingly, the standard attention model performs worse than tf-idf in this domain, while conv_attention and copy_attention perform the best. The copy mechanism gives a good F1 improvement at rank 1 and a larger improvement at rank 5. Although our convolutional attentional models have an exact match similar to tf-idf, they achieve a much higher precision compared to all other algorithms.\nThese differences in the data characteristics could be the cause of the low performance achieved by the model of Bahdanau et al. (2015). Although source code snippets resemble natural language sentences, they are more structured, much longer and vary greatly in length. In our training sets, each method has on average 72 tokens (median 25 tokens, standard deviation 156) and the output method names are made up from 3 subtokens on average (\u03c3 = 1.7).\nOOV Accuracy. We measure the OOV word accuracy as the percent of the out-of-vocabulary subtokens that are correctly predicted by copy_attention. On average, across our dataset, 4.4% of the test method name subtokens are outof-vocabulary. Naturally, the standard attention model and tf-idf have an OOV accuracy of zero, since they are unable to predict those tokens. On average we get a 10.5% OOV accuracy at rank 1 and 19.4% at rank 5. This shows that the copying mechanism is useful in this domain and especially in smaller projects that tend to have more OOV tokens. We\nalso note that OOV accuracy varies across projects, presumably due to different coding styles.\nTopical vs. Time-Invariant Feature Detection. The difference of the performance between the copy_attention and the standard attention model of Bahdanau et al. (2015) raises an interesting question. What does copy_attention learn that cannot be learned by the standard attention model? One hypothesis is that the biRNN of the standard attention model fails to capture long-range features, especially in very long inputs. To confirm our hypothesis, we shuffle the subtokens in libgdx, essentially removing all features that depend on the sequential information. Without any local features all models should reduce to achieving performance similar to tf-idf. Indeed, copy_attention now has an F1 at rank 1 that is +1% compared to tf-idf (presumably thanks to the language model-like structure of the output), while the standard attention model worsens its performance getting an F1 score (rank 1) of 26.2%, compared to the original 41.8%. This suggests that the biRNN fails to capture long-range topical attention features.\nA simpler ht\u22121. Since the target summaries are quite short, we tested a simpler alternative to the GRU, assigning ht\u22121 = W \u00d7 [Gmt\u22121 , Gmt\u22122 ], where G \u2208 R\nD\u00d7|V | is a new embedding matrix (different from the embeddings in E) and W is a k2\u00d7D\u00d72 tensor. This model is simpler and slightly faster to train and achieves similar performance to copy_attention, reducing F1 by less than 1%."}, {"heading": "3.2. Qualitative Evaluation", "text": "Figure 2 shows a visualization of a small method that illustrates how copy_attention typically works. At the first step, it focuses its attention at the whole method and de-\ncides upon the first subtoken. In a large number of cases this includes subtokens such as get, set, is, create etc. In the next steps the copying mechanism is highly confident (\u03bb = 0.97 in Figure 2) and sequentially copies the correct subtokens from the code snippet into the name. We note that across many examples the copying mechanism tends to have a significantly more focused attention vector \u03ba, compared to the attention vector \u03b1. Presumably, this happens because of the different training signals of the attention mechanisms.\nA second example of copy_attention is seen in Figure 3. Although due to space limitations this is a relatively short method, it illustrates how the model has learned both timeinvariant features and topical features. It correctly detects the == operator and predicts that the method has a high probability of starting with is. Furthermore, in the next step (prediction of the m2 bullets subtoken) it successfully learns to ignore the e prefix (prepended on all enumeration variables in that project) and the flag subtoken that does not provide useful information for the summary.\nTable 3 presents a set of hand-picked examples from libgdx that show interesting challenges of the domain and how our copy_attention handles them. Understandably, the model does not distinguish between should and is \u2014 both implying a boolean return value \u2014 and instead of shouldRender, isRender is suggested. The getAspectRatio, surfaceArea and minRunLength examples show the challenges of describing a previously unseen abstraction. Interestingly, the model correctly recognizes that a novel (UNK) token should be predicted after get in getAspectRatio. Most surprisingly, reverseRange is predicted correctly, because of the struc-\nture of the code, even though no code tokens contain the summary tokens."}, {"heading": "4. Related Work", "text": "Convolutional neural networks have been used for image classification with great success (Krizhevsky et al., 2012; Szegedy et al., 2015; LeCun et al., 1990; 1998). More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014). Closely related is the work of Denil et al. (2014) that learns representations of documents using convolution but uses the network activations to summarize a document rather than an attentional model. Rush et al. (2015) use an attention-based encoder to summarize sentences, but do not use convolution for their attention mechanism. Our work is also related to other work in attention mechanisms for text (Hermann et al., 2015) and images (Xu et al., 2015; Mnih et al., 2014) that does not use convolution to\nprovide the attention values. Finally, pointer networks (Vinyals et al., 2015) are similar to our copy mechanism but use an RNN for providing attention. Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).\nIn recent years, thanks to the insight of Hindle et al. (2012) the use of probabilistic models for software engineering applications has grown. Research has mostly focused on token-level (Nguyen et al., 2013; Tu et al., 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014). Movshovitz-Attias et al. (2013) learns to predict code comments using a source code topic model. Allamanis et al. (2015b) create a generative model of source code snip-\npets given a natural language query and Oda et al. (2015) use machine translation to convert source code into pseudocode. Closer to our work, Raychev et al. (2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming. Additionally, compared to Piech et al. (2015) and Mou et al. (2016) our work looks into highly diverse, non-student submission code that performs a wide range of real-world tasks."}, {"heading": "5. Discussion & Conclusions", "text": "Modeling and understanding source code artifacts through machine learning can have a direct impact in software engineering research. The problem of extreme code summarization is a first step towards the more general goal of developing machine learning representations of source code that will allow machine learning methods to reason probabilistically about code resulting in useful software engineering tools that will help code construction and maintenance.\nAdditionally, source code \u2014 and its derivative artifacts \u2014 represent a new modality for machine learning with very different characteristics compared to images and natural language. Therefore, models of source code necessitate research into new methods that could have interesting parallels to images and natural language. This work is a step towards this direction: our neural convolutional attentional model attempts to \u201cunderstand\u201d the highly-structured source code text by learning both long-range features and localized patterns, achieving the best performance among other competing methods on real-world source code."}, {"heading": "Acknowledgements", "text": "This work was supported by Microsoft Research through its PhD Scholarship Programme. Charles Sutton was supported by the Engineering and Physical Sciences Research Council [grant number EP/K024043/1]. We would like to thank Daniel Tarlow and Krzysztof Geras for their insightful comments and suggestions."}], "references": [{"title": "Learning natural coding conventions", "author": ["Allamanis", "Miltiadis", "Barr", "Earl T", "Bird", "Christian", "Sutton", "Charles"], "venue": "In Symposium on the Foundations of Software Engineering (FSE),", "citeRegEx": "Allamanis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2014}, {"title": "Suggesting accurate method and class names", "author": ["Allamanis", "Miltiadis", "Barr", "Earl T", "Bird", "Christian", "Sutton", "Charles"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering", "citeRegEx": "Allamanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Bimodal modelling of source code and natural language", "author": ["Allamanis", "Miltiadis", "Tarlow", "Daniel", "Gordon", "Andrew", "Wei", "Yi"], "venue": "In ICML,", "citeRegEx": "Allamanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "arXiv preprint arXiv:1506.03099,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "The impact of identifier style on effort and comprehension", "author": ["Binkley", "Dave", "Davis", "Marcia", "Lawrie", "Dawn", "Maletic", "Jonathan I", "Morrell", "Christopher", "Sharif", "Bonita"], "venue": "Empirical Software Engineering,", "citeRegEx": "Binkley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Binkley et al\\.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom", "Phil", "Grefenstette", "Edward", "Kalchbrenner", "Nal"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Denil", "Misha", "Demiraj", "Alban", "Kalchbrenner", "Nal", "Blunsom", "Phil", "de Freitas", "Nando"], "venue": "arXiv preprint arXiv:1406.3830,", "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer", "Chris", "Ballesteros", "Miguel", "Ling", "Wang", "Matthews", "Austin", "Smith", "Noah A"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "GHTorrent: Github\u2019s data from a firehose", "author": ["Gousios", "Georgios", "Spinellis", "Diomidis"], "venue": "In Mining Software Repositories (MSR),", "citeRegEx": "Gousios et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gousios et al\\.", "year": 2012}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: surpassing humanlevel performance on ImageNet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In NIPS,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "On the naturalness of software", "author": ["Hindle", "Abram", "Barr", "Earl T", "Su", "Zhendong", "Gabel", "Mark", "Devanbu", "Premkumar"], "venue": "In International Conference on Software Engineering (ICSE),", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Neural networks for machine learning. Online Course at coursera", "author": ["Hinton", "Geoffrey", "Srivastava", "Nitsh", "Swersky", "Kevin"], "venue": "org, Lecture,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Phrase-based statistical translation of programming languages", "author": ["Karaivanov", "Svetoslav", "Raychev", "Veselin", "Vechev", "Martin"], "venue": "In Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming & Software. ACM,", "citeRegEx": "Karaivanov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karaivanov et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Nips. chapter Handwritten Digit Recognition with a Back-propagation Network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "R.E. Howard", "W. Habbard", "L.D. Jackel", "D. Henderson"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Cognitive perspectives on the role of naming in computer programs", "author": ["Liblit", "Ben", "Begel", "Andrew", "Sweetser", "Eve"], "venue": "In Proceedings of the 18th Annual Psychology of Programming Workshop,", "citeRegEx": "Liblit et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liblit et al\\.", "year": 2006}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing (WDLASL),", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Structured generative models of natural source code", "author": ["Maddison", "Chris", "Tarlow", "Daniel"], "venue": "In ICML,", "citeRegEx": "Maddison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou", "Lili", "Li", "Ge", "Zhang", "Lu", "Wang", "Tao", "Jin", "Zhi"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (to appear),", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Natural language models for predicting programming comments", "author": ["Movshovitz-Attias", "Dana", "WW Cohen", "W. Cohen", "William"], "venue": "In Association for Computational Linguistics (ACL),", "citeRegEx": "Movshovitz.Attias et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Movshovitz.Attias et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Migrating code with statistical machine translation", "author": ["Nguyen", "Anh Tuan", "Tung Thanh", "Tien N"], "venue": "In Companion Proceedings of the 36th International Conference on Software Engineering", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Learning to generate pseudo-code from source code using statistical machine translation", "author": ["Oda", "Yusuke", "Fudaba", "Hiroyuki", "Neubig", "Graham", "Hata", "Hideaki", "Sakti", "Sakriani", "Toda", "Tomoki", "Nakamura", "Satoshi"], "venue": "In Automated Software Engineering (ASE),", "citeRegEx": "Oda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oda et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning program embeddings to propagate feedback on student code", "author": ["Piech", "Chris", "Huang", "Jonathan", "Nguyen", "Andy", "Phulsuksombati", "Mike", "Sahami", "Mehran", "Guibas", "Leonidas J"], "venue": "In ICML,", "citeRegEx": "Piech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Piech et al\\.", "year": 2015}, {"title": "Predicting program properties from \u201cbig code", "author": ["Raychev", "Veselin", "Vechev", "Martin", "Krause", "Andreas"], "venue": "In Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages", "citeRegEx": "Raychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raychev et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "The effects of comments and identifier names on program comprehensibility: an experimental investigation", "author": ["Takang", "Armstrong A", "Grubb", "Penny A", "Macredie", "Robert D"], "venue": "J. Prog. Lang.,", "citeRegEx": "Takang et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Takang et al\\.", "year": 1996}, {"title": "On the localness of software", "author": ["Tu", "Zhaopeng", "Su", "Zhendong", "Devanbu", "Premkumar"], "venue": "In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "Recent research has tackled these problems using neural models of attention (Mnih et al., 2014), which have had great recent successes in machine translation (Bahdanau et al.", "startOffset": 76, "endOffset": 95}, {"referenceID": 3, "context": ", 2014), which have had great recent successes in machine translation (Bahdanau et al., 2015) and image captioning (Xu et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 41, "context": ", 2015) and image captioning (Xu et al., 2015).", "startOffset": 29, "endOffset": 46}, {"referenceID": 22, "context": "Convolutional models are a natural choice for learning translation-invariant features while using only a small number of parameters and for this reason have been highly successful in non-attentional models for images (LeCun et al., 1998; Krizhevsky et al., 2012) and text classification (Blunsom et al.", "startOffset": 217, "endOffset": 262}, {"referenceID": 20, "context": "Convolutional models are a natural choice for learning translation-invariant features while using only a small number of parameters and for this reason have been highly successful in non-attentional models for images (LeCun et al., 1998; Krizhevsky et al., 2012) and text classification (Blunsom et al.", "startOffset": 217, "endOffset": 262}, {"referenceID": 6, "context": ", 2012) and text classification (Blunsom et al., 2014).", "startOffset": 32, "endOffset": 54}, {"referenceID": 23, "context": "For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013).", "startOffset": 103, "endOffset": 167}, {"referenceID": 39, "context": "For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013).", "startOffset": 103, "endOffset": 167}, {"referenceID": 5, "context": "For these reasons, software engineering research has found that good names are important to developers (Liblit et al., 2006; Takang et al., 1996; Binkley et al., 2013).", "startOffset": 103, "endOffset": 167}, {"referenceID": 21, "context": "The basic building block of our model is a convolutional network (LeCun et al., 1990; Collobert & Weston, 2008) for extracting position and context dependent features.", "startOffset": 65, "endOffset": 111}, {"referenceID": 7, "context": "The state is passed through the vector ht \u2208 R2 which is computed using a Gated Recurrent Unit (Cho et al., 2014) i.", "startOffset": 94, "endOffset": 112}, {"referenceID": 4, "context": "For regularization during training, we use a trick similar to Bengio et al. (2015) and with probability equal to the dropout rate we compute the next state as ht = GRU(n\u0302,ht\u22121), where n\u0302 is the predicted embedding.", "startOffset": 62, "endOffset": 83}, {"referenceID": 32, "context": "Using the BLEU score (Papineni et al., 2002) would have been possible, but it would not be different from F1 given the short lengths of our output sequences (3 on average).", "startOffset": 21, "endOffset": 44}, {"referenceID": 36, "context": "We perform hyperparameter optimizations using Bayesian optimization with spearmint (Snoek et al., 2012) maximizing the F1 at rank 5.", "startOffset": 83, "endOffset": 103}, {"referenceID": 3, "context": "We also use the standard attention model of Bahdanau et al. (2015) that uses a biRNN and fully connected components, that has been successfully used in machine translation.", "startOffset": 44, "endOffset": 67}, {"referenceID": 38, "context": "To train conv_attention and copy_attention we optimize the objective using stochastic gradient descent with RMSProp and Nesterov momentum (Sutskever et al., 2013; Hinton et al., 2012).", "startOffset": 138, "endOffset": 183}, {"referenceID": 17, "context": "To train conv_attention and copy_attention we optimize the objective using stochastic gradient descent with RMSProp and Nesterov momentum (Sutskever et al., 2013; Hinton et al., 2012).", "startOffset": 138, "endOffset": 183}, {"referenceID": 37, "context": "We use dropout (Srivastava et al., 2014) on all parameters, parametric leaky RELUs (Maas et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 24, "context": ", 2014) on all parameters, parametric leaky RELUs (Maas et al., 2013; He et al., 2015) and gradient clipping.", "startOffset": 50, "endOffset": 86}, {"referenceID": 14, "context": ", 2014) on all parameters, parametric leaky RELUs (Maas et al., 2013; He et al., 2015) and gradient clipping.", "startOffset": 50, "endOffset": 86}, {"referenceID": 3, "context": "\u201cStandard Attention\u201d refers to the machine translation model of Bahdanau et al. (2015). The tf-idf algorithm seems to be performing very well, showing that the bag-of-words representation of the input code is a strong indicator of its name.", "startOffset": 64, "endOffset": 87}, {"referenceID": 3, "context": "These differences in the data characteristics could be the cause of the low performance achieved by the model of Bahdanau et al. (2015). Although source code snippets resemble natural language sentences, they are more structured, much longer and vary greatly in length.", "startOffset": 113, "endOffset": 136}, {"referenceID": 3, "context": "Standard attention refers to the model of Bahdanau et al. (2015).", "startOffset": 42, "endOffset": 65}, {"referenceID": 3, "context": "Standard Attention refers to the work of Bahdanau et al. (2015).", "startOffset": 41, "endOffset": 64}, {"referenceID": 3, "context": "The difference of the performance between the copy_attention and the standard attention model of Bahdanau et al. (2015) raises an interesting question.", "startOffset": 97, "endOffset": 120}, {"referenceID": 20, "context": "Convolutional neural networks have been used for image classification with great success (Krizhevsky et al., 2012; Szegedy et al., 2015; LeCun et al., 1990; 1998).", "startOffset": 89, "endOffset": 162}, {"referenceID": 21, "context": "Convolutional neural networks have been used for image classification with great success (Krizhevsky et al., 2012; Szegedy et al., 2015; LeCun et al., 1990; 1998).", "startOffset": 89, "endOffset": 162}, {"referenceID": 6, "context": "More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014).", "startOffset": 94, "endOffset": 116}, {"referenceID": 15, "context": "Our work is also related to other work in attention mechanisms for text (Hermann et al., 2015) and images (Xu et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 41, "context": ", 2015) and images (Xu et al., 2015; Mnih et al., 2014) that does not use convolution to provide the attention values.", "startOffset": 19, "endOffset": 55}, {"referenceID": 26, "context": ", 2015) and images (Xu et al., 2015; Mnih et al., 2014) that does not use convolution to provide the attention values.", "startOffset": 19, "endOffset": 55}, {"referenceID": 12, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 13, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 10, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 29, "context": "Finally, distantly related to this work is research on creating neural network architectures that learn code-like behaviors (Graves et al., 2014; Zaremba & Sutskever, 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2015; Neelakantan et al., 2015).", "startOffset": 124, "endOffset": 293}, {"referenceID": 6, "context": "More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014). Closely related is the work of Denil et al. (2014) that learns representations of documents using convolution but uses the network activations to summarize a document rather than an attentional model.", "startOffset": 95, "endOffset": 169}, {"referenceID": 6, "context": "More related to this work is the use of convolutional neural networks for text classification (Blunsom et al., 2014). Closely related is the work of Denil et al. (2014) that learns representations of documents using convolution but uses the network activations to summarize a document rather than an attentional model. Rush et al. (2015) use an attention-based encoder to summarize sentences, but do not use convolution for their attention mechanism.", "startOffset": 95, "endOffset": 338}, {"referenceID": 40, "context": "Research has mostly focused on token-level (Nguyen et al., 2013; Tu et al., 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al.", "startOffset": 43, "endOffset": 81}, {"referenceID": 19, "context": ", 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014).", "startOffset": 121, "endOffset": 167}, {"referenceID": 30, "context": ", 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014).", "startOffset": 121, "endOffset": 167}, {"referenceID": 13, "context": "In recent years, thanks to the insight of Hindle et al. (2012) the use of probabilistic models for software engineering applications has grown.", "startOffset": 42, "endOffset": 63}, {"referenceID": 13, "context": "In recent years, thanks to the insight of Hindle et al. (2012) the use of probabilistic models for software engineering applications has grown. Research has mostly focused on token-level (Nguyen et al., 2013; Tu et al., 2014) and syntax-level (Maddison & Tarlow, 2014) language models of code and translation between programming languages (Karaivanov et al., 2014; Nguyen et al., 2014). Movshovitz-Attias et al. (2013) learns to predict code comments using a source code topic model.", "startOffset": 42, "endOffset": 419}, {"referenceID": 0, "context": "Allamanis et al. (2015b) create a generative model of source code snip-", "startOffset": 0, "endOffset": 25}, {"referenceID": 27, "context": "pets given a natural language query and Oda et al. (2015) use machine translation to convert source code into pseudocode.", "startOffset": 40, "endOffset": 58}, {"referenceID": 27, "context": "pets given a natural language query and Oda et al. (2015) use machine translation to convert source code into pseudocode. Closer to our work, Raychev et al. (2015) aim to predict names and types of variables, whereas Allamanis et al.", "startOffset": 40, "endOffset": 164}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al.", "startOffset": 60, "endOffset": 84}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes.", "startOffset": 60, "endOffset": 113}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.", "startOffset": 60, "endOffset": 199}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming.", "startOffset": 60, "endOffset": 338}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming.", "startOffset": 60, "endOffset": 556}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming. Additionally, compared to Piech et al. (2015) and Mou et al.", "startOffset": 60, "endOffset": 758}, {"referenceID": 0, "context": "(2015) aim to predict names and types of variables, whereas Allamanis et al. (2014) and Allamanis et al. (2015a) suggest names for variables, methods and classes. Similar to Allamanis et al. (2015a), we aim to predict method names but using only the tokens within a method and no other features (e.g. method signature). Mou et al. (2016) use syntaxlevel convolutional neural networks, ignoring identifiers, to learn a vector representations of code and classify student submissions into their original tasks without considering naming. Piech et al. (2015) also learn program embeddings from student submissions using the program state, to assist MOOC students debug their submissions but do not consider naming. Additionally, compared to Piech et al. (2015) and Mou et al. (2016) our work looks into highly diverse, non-student submission code that performs a wide range of real-world tasks.", "startOffset": 60, "endOffset": 780}], "year": 2016, "abstractText": "Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model\u2019s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network\u2019s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.", "creator": "LaTeX with hyperref package"}}}