{"id": "1602.04450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics", "abstract": "nonlinear algorithms typically depend on various parameters, the choice of which significantly affects mechanical robot's performance. while an initial guess for the parameters may refer obtained forming dynamic models of the robot, parameters are usually tuned manually on the real system towards achieve the best performance. optimization algorithms, such on bayesian algebra, still become used to automate this process. however, these methods periodically evaluate parameters around their optimization process that lead to safety - critical system failures. nevertheless, a safe bayesian optimization algorithm, generic safeopt, has been developed as applied in robotics, which guarantees that the performance of the system never falls yield proper critical value ; simpler is, safety is defined based on the performance function. however, coupling software requires safety is not desirable in most cases. in this paper, we define separate functions for performance and safety. we present a generalized safeopt algorithm that, given an initial safe guess for the parameters, maximizes performance but only evaluates parameters that satisfy all safety constraints with high probability. it achieves this by modeling generally safe and unknown performance and constraint graphs as gaussian processes. we provide a theoretical analysis and demonstrate in experiments concerning a quadrotor vehicle that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters. moreover, we show an extension to context - based constraints - static, safe optimization in the experiments.", "histories": [["v1", "Sun, 14 Feb 2016 13:30:43 GMT  (864kb,D)", "http://arxiv.org/abs/1602.04450v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG cs.SY", "authors": ["felix berkenkamp", "reas krause", "angela p schoellig"], "accepted": false, "id": "1602.04450"}, "pdf": {"name": "1602.04450.pdf", "metadata": {"source": "CRF", "title": "Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics", "authors": ["Felix Berkenkamp", "Andreas Krause", "Angela P. Schoellig"], "emails": ["krausea}@ethz.ch,", "schoellig@utias.utoronto.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nSafety and the ability to operate within given constraints of the environment are critical pre-requisites for any algorithm that is applied on a real system. This holds true especially in robotics, where systems often face large prior uncertainties [1]. Typically, safety guarantees are provided with respect to a potentially uncertain model. When accurate models are not available, either conservative parameters with low performance are used, or the parameters are tuned manually on the real system, where the engineer carefully selects parameters that are safe. Recently, this manual tuning approach in robotics was automated by using safe Bayesian optimization [2]. However, in this work safety was strictly defined in terms of a minimum performance below which the system is not allowed to go, rather than allowing general safety conditions. This has limited its applicability to robotics.\nIn this paper, we tackle the problem of safe Bayesian optimization under arbitrary constraints by explicitly decoupling the objective to maximize performance from the safety requirements. By doing so, we allow for multiple, practically relevant, and intuitive safety constraints. We generalize the theoretical framework of SAFEOPT (Safe Optimization) from [3] to our\nsetting with multiple constraints. We show that the resulting algorithm, SAFEOPT-MC (for multiple constraints), performs well in practice and provide theoretical guarantees about its performance.\nIn the robotics literature, optimization algorithms have previously been applied with the goal of maximizing a userdefined performance function through iterative experiments. This is especially powerful when no prior model of the robot is available. However, typical algorithms in the literature do not consider safety of the optimization process, and make other restrictive assumptions such as requiring gradients [4], [5], which are difficult to obtain from noisy data, or an impractical number of experiments [6]. In the reinforcement learning literature on policy gradients, safety has mostly been considered in terms of heuristics that disallow large steps along the gradient into areas of the state space that have not been explored before [7].\nOne recent category of optimization algorithms that has been successfully applied to robotics is Bayesian optimization [8]. In Bayesian optimization, rather than modeling the objective function as a black-box about which we can only obtain point-wise information, regularity assumptions are made. The resulting algorithms are practical and provably find the global optimum of the objective function while evaluating the function at only few parameters [9], [10]. Bayesian optimization methods often model the unknown function as a Gaussian process (GP) [11], which in turn is used to guide function evaluations to locations that are informative about the optimum of the unknown function [8], [12]. Moreover, GP models are highly flexible, allow to encode as much prior knowledge as desired, and explicitly model noise in the performance function evaluations.\nExample applications of Bayesian optimization in robotics include gait optimization of legged robots [13], [14] and\nar X\niv :1\n60 2.\n04 45\n0v 1\n[ cs\n.R O\n] 1\n4 Fe\nb 20\n16\nthe optimization of the controller parameters of a snake-like robot [15]. In [16] the weighting matrices of an LQR controller for an inverted pendulum are optimized. Several different Bayesian optimization methods were compared in [17] for bipedal locomotion. While these examples illustrate the potential of Bayesian optimization methods in robotics, none of them explicitly considers safety as a requirement.\nRecently, the concept of safety has been incorporated into Bayesian optimization. In [18] an algorithm to optimize an unknown function subject to an unknown constraint was introduced. However, this constraint was not considered safetycritical; that is, the evaluation of unsafe parameters was allowed. The case of safety-critical constraints was considered in [3] and [19]. The algorithm in [3], called SAFEOPT, has been successfully applied to robotics in [2]. However, both mentioned safe optimization algorithms consider safety as a minimum performance requirement. In robotics, safety constraints are typically functions of the states or inputs that are independent of the performance.\nIn this paper, we present an algorithm that considers multiple, arbitrary safety constraints decoupled from the performance objective. This generalization retains the desirable sample-efficient properties of normal Bayesian optimization, but carefully explores the parameter space in order to maximize performance while guaranteeing safety with high probability. We extend the theory of SAFEOPT in [3] to account for these additional constraints and show that similar theoretical guarantees can be obtained for the new setting. We then relax the assumptions used in the proofs to obtain a more practical version of the algorithm similar to [2], but additionally show that the safety guarantees carry over to this case. In our experiments, we consider the problem of safely optimizing a nonlinear control law for a quadrotor vehicle. Our experiments demonstrate that the proposed approach is able to safely optimize parameters of a nonlinear control law while respecting safety constraints with high probability.\nIn control theory, safety in the presence of unmodeled dynamics is often interpreted as stability of the underlying control law with respect to an uncertain model [20]. In this setting, controllers can be gradually improved by estimating the unmodeled dynamics and updating the control law based on this estimate. Safety can be guaranteed by ensuring that either the controller is robustly stable for all possible models within the uncertainty specification [21] or the system never leaves a safe subset of the state space [22], [23], [24]. Both methods require a system model and uncertainty specification to be known a priori, which must be accurate enough to guarantee stability. In contrast, our method does not use a model of the system directly, but models uncertainty on a higher level, as part of the constraints and optimization objective. Furthermore, our algorithm is applicable to general algorithms beyond control."}, {"heading": "II. PROBLEM STATEMENT", "text": "We consider a given algorithm that is used to achieve a certain task with a robot. In general, this algorithm is arbitrary\nand may contain several components including vision, state estimation, planning, and control laws. The algorithm depends on tuning parameters a \u2208 A in some specified, discrete domain A.\nThe goal is to find the parameters within A that maximize a given, scalar performance measure, f . For example, this performance measure may represent the negative tracking error of a robot [2], the average walking speed of a bipedal robot [13], or any other quantity that can be computed over a finite time horizon. While the performance measure can be evaluated in experiments for any parameter set a, the function dependence on a is unknown a priori. In the following, we write f(a) : A 7\u2192 R, even though the computation of the performance measure will typically depend on states, control inputs and external signals as well.\nWe assume that the underlying system is safety-critical; that is, there are constraints that the system must respect when evaluating parameters. Similarly to the performance measure, f(a), these constraints can represent any quantity and may depend on states, inputs, or even environment variables. There are q safety constraints of the form gi(a) \u2265 0, i = 1 . . . q, which together define the safety conditions. Again, these functions are unknown a priori but can be evaluated through experiments for a given parameter set a. In the case that there are constraints for which the functional dependence on the parameters a is known, these unsafe parameters could simply be excluded from the parameter space A.\nThe overall optimization problem can be written as\nmax a\u2208A\nf(a) subject to gi(a) \u2265 0 \u2200 i = 1, . . . , q. (1)\nThe goal is to iteratively find the global maximum of this constrained optimization problem by, at each iteration n, sampling and evaluating parameters an until the optimal parameters are found. In particular, since the constraints define the safety of the underlying system, only parameters that are inside the feasible region of (1) are allowed to be evaluated; that is, only parameters that fulfill these safety requirements on the real system. Solving (1) by sampling without violating the constraints is impossible without further assumptions. In the following, we assume that the functions in (1) can be modeled as a Gaussian process and require the safety constraints to hold with high probability. Moreover, we assume that an initial safe set of parameters, S0 \u2286 A, is known for which the constraints are fulfilled. These serve as a starting point for the exploration of the safe region in (1). As a result, we do not find the global optimum of (1), but instead restrict ourselves to the part of the feasible region that is connected to S0.\nFurthermore, whenever we evaluate parameters on the real system ,we only obtain noisy estimates of both the performance function and the constraints, since both depend on noisy sensor data. At each iteration, we obtain measurements f\u0302(an) = f(an) + \u03c90 and g\u0302i(an) = gi(an) + \u03c9i, where \u03c9i \u223c N (0, \u03c32i ), i = 0, . . . , q, is zero-mean, Gaussian noise. In general, these noise variables may be correlated,\nbut we do not consider this case in our theoretical analysis in Sec. IV-B."}, {"heading": "III. BACKGROUND", "text": "In this section, we review Gaussian processes (GPs) and Bayesian Optimization, which form the foundation of our safe Bayesian optimization algorithm. The introduction to GPs is standard and taken from [2] and [11]."}, {"heading": "A. Gaussian Process (GP)", "text": "Both the function f(a) and the safety constraints gi(a) in Sec. II are unknown a priori. We use GPs as a nonparametric model to approximate these unknown functions over their common domain A. In the following, we focus on a single function, the performance function, but will extend this model to multiple functions in order to represent both performance and constraints of Sec. III-A1.\nGPs are a popular choice for nonparametric regression in machine learning, where the goal is to find an approximation of a nonlinear map, f(a) : A 7\u2192 R, from an input vector a \u2208 A to the function value f(a). This is accomplished by assuming that function values f(a), associated with different values of a, are random variables and that any finite number of these random variables have a joint Gaussian distribution depending on the values of a [11].\nFor the nonparametric regression, we need to define a prior mean function and a covariance function, k(a,a\u2032), which defines the covariance of any two function values, f(a) and f(a\u2032), a,a\u2032 \u2208 A. The latter is also known as the kernel. In this work, the mean is assumed to be zero, without loss of generality. The choice of kernel function is problem-dependent and encodes assumptions about smoothness and rate of change of the unknown function. A review of potential kernels can be found in [11]. More information about the kernels used in this paper can be found in Sec. V.\nThe GP framework can be used to predict the function value, f(a\u2217), for an arbitrary parameter set, a\u2217 \u2208 A, based on a set of n past observations, Dn = {ai, f\u0302(ai)}ni=1. We assume that observations are noisy measurements of the true function value, f(a); that is, f\u0302(a) = f(a) + \u03c9 with \u03c9 \u223c N (0, \u03c32\u03c9). Conditioned on these observations, the mean and variance of the prediction are given by\n\u00b5n(a \u2217) = kn(a \u2217)(Kn + In\u03c3 2 \u03c9) \u22121f\u0302n, (2) \u03c32n(a \u2217) = kn(a \u2217,a\u2217)\u2212 kn(a\u2217)(Kn + I\u03c32\u03c9)\u22121kTn (a\u2217), (3)\nwhere f\u0302n = [ f\u0302(a1), . . . , f\u0302(an) ]T is the vector of observed, noisy function values, the covariance matrix Kn \u2208 Rn\u00d7n has entries [Kn](i,j) = k(ai,aj), i, j \u2208 {1, . . . , n}, and the vector kn(a \u2217) = [ k(a\u2217,a1), . . . , k(a \u2217,an) ]\ncontains the covariances between the new input a\u2217 and the observed data points in Dn. The matrix In \u2208 Rn\u00d7n denotes the identity matrix.\n1) GPs with multiple outputs: So far, we have focused on GPs that model a single scalar function. In order to model not only the performance, f(a), but also the safety constraints, gi(a), we have to consider multiple, possibly\ncorrelated functions. In the GP literature, these are usually treated by considering a matrix of kernel functions, which models the correlation between different functions [25]. Here instead, we use an equivalent representation by considering a surrogate function,\np(a, i) = { f(a) if i = 0 gi(a) if i > 0,\n(4)\nwhich returns either the performance function or the individual safety constraints depending on the additional input i \u2208 I with I = {0, . . . , q}. This is a single-output function and can be modeled as a GP with scalar output in the extended parameter space A \u00d7 I. For example, the kernel for the performance function f(a) and one safety constraint, g(a), could look like\nk((a, i), (a\u2032, j)) =\n{ kf (a,a\n\u2032) + (1\u2212 \u03b4ij)kfg(a,a\u2032) if i = 0 kg(a,a\n\u2032) + (1\u2212 \u03b4ij)kfg(a,a\u2032) if i = 1, (5)\nwhere \u03b4ij is the Kronecker delta. This kernel models the functions, f(a) and g(a), with independent kernels, kf and kg , respectively, but also introduces a covariance function kfg that models the covariance between the two function outputs. By extending the training data by the extra parameter i, we can use the normal GP framework and predict function values and corresponding uncertainties using (2) and (3). When observing the function values, the index i is added to the parameter set a for each observation. Including noise parameters inside the kernel, allows to model noise correlations between the individual functions.\nImportantly, using this surrogate function rather than the framework in [25] allows us to bound the information gain and provide theoretical guarantees in Sec. IV-B."}, {"heading": "B. Bayesian Optimization", "text": "Bayesian optimization aims to find the global maximum of an unknown function [8]. The assumption is that evaluating the function is expensive, while computational resources are cheap. This fits our problem in Sec. II where each evaluation of the performance function corresponds to an experiment on the real system, which takes time and causes wear.\nIn general, Bayesian optimization models the objective function as a random function and uses this model to determine informative sample locations. A popular approach is to model the underlying function as a GP, see Sec. III-A. GP-based methods use the mean and variance predictions in (2) and (3) to compute the next sample location. For example, according to the GP-UCB (GP-Upper Confidence Bound) algorithm in [10], the next sample location is\nan = argmax a\u2208A\n\u00b5n\u22121(a) + \u03b2 1/2 n \u03c3n\u22121(a), (6)\nwhere \u03b2n is an iteration-dependent scalar that reflects the confidence interval of the GP. Intuitively, (6) selects new evaluation points at locations where the upper bound of the confidence interval of the GP estimate is maximal. Repeatedly\nevaluating the system at locations given by (6) improves the mean estimate of the underlying function and decreases the uncertainty at candidate locations for the maximum, such that the global maximum is found eventually, cf. [10].\nWhile (6) is also an optimization problem, its solution does not require any evaluations on the real system but only uses the GP model. This reflects the assumption of cheap computational resources.\nC. Safe Bayesian Optimization (SAFEOPT) In this paper, we extend the safe optimization algorithm SAFEOPT [3] to multiple constraints. SAFEOPT is a Bayesian optimization algorithm, see Sec. III-B. However, instead of optimizing the underlying function f(a) globally, it restricts itself to a safe set of parameters that achieve a certain minimum performance with high probability. This safe set is not known initially, but is estimated after each function evaluation. In this context, the challenge is to find an appropriate evaluation strategy similar to (6), which at each iteration n not only aims to find the global maximum within the currently known safe set (exploitation), but also aims to increase the set of controllers that are known to be safe (exploration). SAFEOPT [3] trades off between these two sets by choosing for the next experiment the parameters inside the safe set about whose performance we are most uncertain."}, {"heading": "IV. SAFEOPT-MC (MULTIPLE CONSTRAINTS)", "text": "In this section, we introduce the SAFEOPT-MC algorithm for multiple constraints and discuss its theoretical properties. Since the goal of the algorithm is to solve (1) by sampling, we will consider two important properties: the ability (i) to expand the region of the optimization problem that is known to be feasible or safe as much as possible without violating the constraints, and (ii) to find the optimal parameters within this safe set.\nThe theoretical guarantees of the algorithm rely on the Lipschitz continuity of the underlying function. Many commonly used kernels, such as the squared exponential (Gaussian) kernel, satisfy this condition with high probability [26]. In the following, we assume that f(a) and gi(a) are Lipschitz continuous with respect to some norm, with Lipschitz constant L.\nSince we only observe noisy estimates of both the performance function and the constraints, we cannot expect to find the entire safe region encoded by the constraints within a finite number of evaluations. Instead, we follow [3] and consider learning the safety constraint to some accuracy, . This assumption is equivalent to allowing a minimum slack of on the constraints in (1).\nAs mentioned in Sec. II, we assume that we have access to initial, safe parameters S0 \u2286 A, for which the constraints are known to be satisfied. Starting from some initial safe set S, if we knew the function up to everywhere, we could expand the safe set to R (S) := S\u222a\u22c2\ni\u22651\n{a \u2208 A | \u2203a\u2032 \u2208 S : gi(a\u2032)\u2212 \u2212 L\u2016a\u2212a\u2032\u2016 \u2265 0} , (7)\nwhere R represents the number of states that can be classified as safe using the Lipschitz continuity. The baseline that we compare against is the limit of repeatedly applying this operator on S0; that is, with Rn (S) = R (R n\u22121 (S)) and R1 (S) = R (S) the baseline is R\u0304 (S0) := limn\u2192\u221eR n (S0). This set contains all the parameters in A that could be classified as safe starting from S0 if we knew the function up to error. This set does not include all the states that potentially fulfill the constraints in (1), but is the best we can do without violating the safety constraints. Hence the optimal value that we compare against is not the one in (1), but\nf\u2217 = max a\u2208R\u0304 (S0) f(a), (8)\nwhich is the maximum performance value over the set that we could hope to classify as safe starting from the initial safe set, S0."}, {"heading": "A. The Algorithm", "text": "In this section, we present the algorithm that allows us to achieve the previously set baseline. The most critical aspect of the algorithm is safety. However, once safety is ensured, the second challenge is to find an evaluation criterion that enables trading off between trying to further expand the current estimate of the safe set and trying to improving the estimate of the best parameters within the current set.\nTo ensure safety, we use the confidence intervals of our posterior GP estimate given the data observed so far. The confidence intervals for the surrogate function in (4) are defined as\nQn(a, i) := [ \u00b5n\u22121(a, i)\u00b1 \u03b21/2n \u03c3n\u22121(a, i) ] , (9)\nwhere \u03b2n is a scalar that determines the desired confidence interval. This set contains all possible function values between the lower and upper confidence interval of the GP posterior. The probability of the true function value lying within this interval depends on the choice of \u03b2n, as well as on the assumptions made about the functions. We provide more details about this choice in Sec. IV-B, (17), and Sec. IV-C.\nRather than defining the lower and upper bounds based on (9), the following analysis requires that consecutive estimates of the lower and upper bounds are contained within each other. This assumption ensures that the safe set does not shrink from one iteration to the next, which we require to prove our results. For the practical experiments we relax this assumption in Sec. IV-C.\nWe define the contained set at iteration n as Cn(a, i) = Cn\u22121(a, i) \u2229Qn(a, i), where C0(a, i) is [0,\u221e] for all a \u2208 S0 and R otherwise. This ensures that parameters in the initial safe set S0 will remain safe according to the estimate. The lower and upper bounds on this set are defined as lin(a) := minCn(a, i) and u i n(a) := maxCn(a, i), respectively. For notational clarity, we write lfn(a) := l 0 n(a) and ufn(a) := u 0 n(a) for the performance bounds.\nBased on these confidence intervals for the function values, the estimate of the safe set is given by\nSn = \u22c2 i\u22651 \u22c3 a\u2208Sn\u22121 { a\u2032 \u2208 A | lin(a)\u2212 L\u2016a\u2212a\u2032\u2016 \u2265 0 } . (10)\nThis set contains all points in Sn\u22121, as well as all additional points that fulfill all safety constraints given the GP estimate and the Lipschitz constant.\nWith the set of safe parameters defined, the last remaining challenge is to trade off between exploration and exploitation. One could, similar to [19], simply select the most uncertain element over the entire set. However, this approach is not sample-efficient, since it involves learning about the entire function rather than restricting evaluations to the relevant parameters. To avoid this, we first define subsets of Sn that correspond to parameters that could either improve the estimate of the maximum or could expand the safe set, similar to [3]. The set of maximizers is defined as\nMn := { a \u2208 Sn |ufn(a) \u2265 max\na\u2032\u2208Sn lfn(a \u2032)\n} , (11)\nwhich contains all points where the upper bound of the current estimate is above the best lower bound. The parameters in Mn are candidates for the optimum, since they could obtain performance values above the current conservative estimate of the optimal performance.\nSimilarly, an optimistic set of parameters that could potentially enlarge the safe set is\nGn := {a \u2208 Sn | en(a) \u2265 0} , (12) en(a) := \u2223\u2223{a\u2032 \u2208 A \\ Sn | \u2203i \u2265 1 : uin(a)\u2212 L\u2016a\u2212a\u2032\u2016 \u2265 0}\u2223\u2223 . (13)\nThe function en enumerates the number of parameters that could additionally be classified as safe if all safety functions obtained values equal to their upper confidence bound. Thus, the set Gn is an optimistic set of parameters that could potentially expand the safe set.\nWe trade off between the two sets, Mn and Gn, by selecting the most uncertain element across all performance and safety functions; that is, at each iteration n we select\nan = argmax a\u2208Gn\u222aMn,i\u2208I wn(a, i), (14)\nwn(a, i) = u i n(a)\u2212 lin(a) (15)\nas the next parameter set to be evaluated on the real system. The implications of this selection criterion will become more apparent in the next section, but from a high-level view this criterion leads to a behavior that focuses almost exclusively on exploration initially, as the most uncertain points will typically lie on the boundary of the safe set for many commonly used kernels. This changes once the constraint evaluations return results closer to the safety constraints. At this point, the algorithm keeps switching between selecting parameters that are potential maximizers, and parameters that could expand the safe set and lead to a new area in the parameter space with even higher function values. A summary of the algorithm is\nAlgorithm 1: SAFEOPT-MC Inputs: Domain A\nGP prior (k((a, i), (a\u2032, j)) Lipschitz constant L, Initial safe set S0 \u2286 A\n1 for n = 1, . . . do 2 Sn \u2190 \u22c2 i\u22651 \u22c3 a\u2208Sn\u22121 { a\u2032 \u2208 A | lin(a)\u2212 L\u2016a\u2212a\u2032\u2016 \u2265 0 } 3 Mn \u2190 { a \u2208 Sn |ufn(a) \u2265 maxa\u2032\u2208Sn lfn(a\u2032)\n} 4 Gn \u2190 {a \u2208 Sn | en(a) \u2265 0} 5 an \u2190 argmaxa\u2208Gn\u222aMn,i\u2208I wn(a, i) 6 Noisy measurements: f\u0302(an), g\u0302i(an)\u2200i = 0, . . . , q 7 Update GP with new data 8 end\nfound in Algorithm 1 and an example run of the algorithm is illustrated in Fig. 2.\nAt any iteration, we can obtain an estimate for the current best parameters from\na\u0302n = argmax a\u2208Sn\nlfn(a). (16)"}, {"heading": "B. Theoretical Results", "text": "In this section, we show that the same theoretical frame work from the SAFEOPT algorithm [3] can be extended to multiple constraints and the evaluation criterion (14). Here, we only provide the results and high-level ideas of the proofs. The mathematical details are provided separately in [27]. In the following, we assume all function evaluations are corrupted by noise with the same variance, \u03c3.\nIn order to provide guarantees for safety, we need the confidence intervals in (9) to hold for all iterations and functions. The following Lemma allows us to choose a scaling factor, \u03b2n for (9), so that we achieve a specific probability of the true function being contained in the confidence intervals for all iterations, under the assumption that the surrogate function, p(a, i) is sampled from a GP.\nLemma 1. Assume that p(a, i) is sampled from a GP. If \u03b2n = 2 log(|I| |A|\u03c0n/\u03b4), where \u2211 n\u22651 \u03c0 \u22121 n = 1 with \u03c0n > 0. Then the following holds with probability at least 1\u2212 \u03b4 for all time steps n \u2265 1, parameters a \u2208 A and functions i \u2208 I:\n|p(a, i)\u2212 \u00b5n\u22121(a, i)| \u2264 \u03b21/2n \u03c3n\u22121(a, i). (17)\nProof: See Lemma 5.1 in [10]. The main idea in (17) is that we require the confidence intervals to hold for all data points and iterations. We scale up the uncertainty in order to reach the desired confidence. The parameter \u03c0n can be chosen as a constant with \u03c0n = 1/Tmax, where Tmax is an upper bound on the number of iterations, if known. If not known, \u03c0n = n2\u03c02/6 or any function that fulfills the requirements in (17). For a different result for the case when the function p has bounded norm in the Reproducing Kernel Hilbert Space (RKHS) associated with the kernel, and\nbounded noise see [10]. In this case, the probability does not depend on the size of A.\nSince the confidence intervals hold with probability 1\u2212 \u03b4 and the safe set is not empty, it is possible to prove safety of the algorithm. In order to prove that we reach our baseline, we must ensure that we are able to learn the true function up to confidence in both the sets Mn and Gn. The number of samples required to achieve this depends on the information gain, \u03b3n, which is the maximum amount of mutual information that can be obtained about the GP prior from n data samples. It has been shown in [10] that the mutual information has a sublinear dependence on the number of samples for commonly used kernels. As a result, we can use it to bound the number of samples required to know the function up to accuracy. Using this, we can obtain the following result:\nTheorem 1. Assume that p(a, i) is L-Lipschitz continuous and sampled from a GP with zero-mean Gaussian noise with variance \u03c32. Also, assume that S0 6= \u2205 and gi(a) \u2265 0 for all a \u2208 S0 and i \u2208 I. Choose \u03b2n as in (17), define a\u0302n as in (16), and let n\u2217 be the smallest positive integer satisfying\nn\u2217 \u03b2n\u2217\u03b3|I|n\u2217 \u2265 C1(|R\u03040(S0)|+ 1) 2 , (18)\nwhere C1 = 8/ log(1 + \u03c3\u22122). For any \u2265 0 and \u03b4 \u2208 (0, 1), when running Algorithm 1 the following inequalities jointly hold with probability at least 1\u2212 \u03b4:\n1) \u2200n \u2265 1,\u2200i \u2208 I : gi(an) \u2265 0 2) \u2200n \u2265 n\u2217, f(a\u0302n) \u2265 f\u2217 \u2212\nProof: See [27]. Item 2 states that, given the assumptions we made about the underlying function, Algorithm 1 explores the state space without violating the safety constraints and, after n\u2217 samples, finds an estimate that is -close to the optimal value over the safely reachable region. The information gain, \u03b3|I|n\u2217 , grows\nat a faster rate of |I|n, since the information gain is defined in terms of selecting |I| observations that greedily maximize the mutual information, while in our setting all functions are evaluated at the same parameter set a at each iteration. However, \u03b3|I|n remains sublinear in n, c.f., [27]."}, {"heading": "C. Practical Implementation", "text": "In this section, we discuss possible changes to Algorithm 1 that make the algorithm more practical, at the expense of loosing some of the theoretical guarantees. The main motivation behind this is that defining a Lipschitz constant is impractical in many situations, and specifying the wrong constant can lead to conservativeness or unsafe actions. Moreover, for many commonly used kernels, high-probability Lipschitz constants are already encoded in the GP model. In practice, we use the GP directly to ensure safety [2]; that is, we define lin(a) = minQn(a, i) and uin(a, i) = maxQn(a, i) in terms of the confidence intervals. In this case, we can define the safe set without a Lipschitz constant as\nSn = S0 \u222a { a \u2208 A | \u2200i \u2265 1: lin(a) \u2265 0 } . (19)\nWhile it is difficult to proof the full exploration of the safely reachable set as in Item 2, the resulting algorithm remains safe:\nLemma 2. With the assumptions of (17) and S0 6= \u2205 for all n \u2265 0, and gi(a) \u2265 0 for all a \u2208 S0 and i \u2208 I, when running Algorithm 1 with the safe set defined as in (19), the following holds with probability at least 1\u2212 \u03b4:\n\u2200n \u2265 1, \u2200i \u2208 I : gi(an) \u2265 0 (20)\nProof: The confidence intervals hold with probability 1\u2212 \u03b4 following (17). Since Sn in (19) is defined as the set of parameters that fulfill the safety constraint and the safe set is never empty since S0 6= \u2205, the claim follows.\nSimilarly, the set of expanders can be defined in terms of the GP directly, by adding optimistic measurements and counting the number of new states that are classified as safe, see [2]. However, this adds a large computational burden.\nThe parameter \u03b2n, which determines the GP\u2019s confidence interval in (17), may be impractically conservative for experiments. However, depending on the application, one may consider setting \u03b2n to a constant value, which bounds the failure probability per iteration, rather than over all iterations.\nLearning all the different functions, f and gi, up to the same accuracy, , may be restrictive if they are scaled differently. A possible solution is to either scale the observed data, or to scale the uncertainties in (14) by the prior variances of the kernels for that specific output."}, {"heading": "V. QUADROTOR EXPERIMENTS", "text": "In this section, we demonstrate Algorithm 1 (with the changes discussed in Sec. IV-C) in experiments on a quadrotor vehicle, a Parrot AR.Drone 2.0. During the experiments, measurements of all vehicle states were obtained from an overhead motion capture camera system. The quadrotor\u2019s dynamics can be described by six states: positions, x = (x, y, z), velocities, x\u0307 = (x\u0307, y\u0307, z\u0307), ZYX Euler angles, (\u03c6, \u03b8, \u03c8), and body angular velocities (\u03c9x, \u03c9y, \u03c9z). The control inputs, u, are the desired roll and pitch angles, \u03b8des and \u03c6des, the desired zvelocity, z\u0307des, and the desired yaw angular velocity, \u03c9z,des, which in turn are inputs to an unknown, proprietary, on-board controller.\nThe position dynamics in the global coordinate frame are\nx\u0308 = RZYX(\u03c6, \u03b8, \u03c8)~f \u2212 ~g, (21)\nwhere RZYX is the rotation matrix from the body frame to the inertial frame, ~f = (0, 0, c) is the mass-normalized thrust and ~g = (0, 0, g) is the gravitational force. In order to control the system, we use measurements of the angles and accelerations to solve for the thrust, c. Horizontal position control loops are shaped to behave in the manner of a secondorder systems with time constant \u03c4 and damping ratio \u03b6. Based on a given desired reference trajectory, commanded accelerations are computed\nWe define the desired accelerations with respect to a desired reference trajectory as a second order system,\nx\u0308c = 1\n\u03c42 (xdes \u2212 x) +\n2\u03b6\n\u03c4 (x\u0307des \u2212 x\u0307), (22)\ny\u0308c = 1\n\u03c42 (ydes \u2212 y) +\n2\u03b6\n\u03c4 (y\u0307des \u2212 y\u0307), (23)\nfrom which we then obtain the control inputs for the desired roll and pitch angles, by solving (21) for the angles. Here, the optimization variables are a = (\u03c4, \u03b6). The yaw rate and zvelocity are controlled by independent PI-controllers. For details regarding the controllers see [28], [29]."}, {"heading": "A. Step Response", "text": "In a first experiment, the goal is to minimize the root-meansquare error (RMSE) over a time horizon of 5 s (N = 350 sam-\nples) during a 1-meter reference position change in xdirection. We define the performance function,\nf(an) = C(an)\u2212 0.75C(a0), (24)\nC(an) = 1\u221a N ( N\u2211 k=1 \u2016xk \u2212 xdes,k\u201622 )1/2 , (25)\nas the performance relative to 75% of the performance of the initial parameters, a0 = (0.9, 0.8). Here, the subscript k indicates the kth position sample. For the GP model, we choose \u03b21/2n = 2 to define the confidence interval in (9). What remains is to define the GP model associated with this performance requirement. The most important aspect is the choice of kernel, which defines the properties of the mean functions. Here, we choose the Mate\u0300rn kernel with parameter \u03bd = 3/2 [11],\nk(a,a\u2032) = \u03c32\u03b7\n( 1+ \u221a 3 r(a,a\u2032) ) exp ( \u2212 \u221a 3 r(a,a\u2032) ) , (26)\nr(a,a\u2032) = \u221a\n(a\u2212 a\u2032)TM\u22122(a\u2212 a\u2032), (27) which encodes that mean functions are once differentiable. The kernel is parameterized by three hyperparameters: measurement noise, \u03c32\u03c9 in (2) and (3), prior variance, \u03c3 2 \u03b7 , and postitive lengthscales, l \u2208 RA+ , which are the diagonal elements of the diagonal matrix M, M = diag(l). These hyperparameters have intuitive interpretations. The variance of the measurement noise, \u03c3\u03b7 , corresponds to the noise in the observations, which includes any randomness in the algorithm and initial conditions, as well as random disturbances. The prior variance, \u03c32\u03b7 , determines the expected magnitude of function values; that is, |f(a)| \u2264 \u03c3\u03b7 with probability 0.68 according to the Gaussian confidence intervals. Lastly, the lengthscales l determine how quickly the covariance between neighboring values deteriorates with their distance. The smaller the lengthscales, the faster the function values can change from one parameter set to the next. In particular, the high-probability Lipschitz constant\nencoded by this kernel depends on the ratio between the prior variance and the lengthscales, \u03c32\u03b7/l 2 max.\nIn our example, measurement noise is minimal, since the positions are measured reasonably accurately by the overhead camera system. However, to capture errors in the initial position, we define \u03c3\u03c9 = 0.05C(a0). We assume that we can improve the initial controller by roughly 20%, so we set \u03c3\u03b7 = 0.2C(a0). The lengthscales are set to 0.05 in order to encourage cautious exploration. These parameters turned out to be conservative for the real system.\nIf, as was done in [2], one were to set the safety constraint as g1(a) = f(a), the algorithm would classify the blue shaded region in Fig. 3 as safe. This region includes time constants as low as \u03c4 = 0.3, which encourage highly aggressive maneuvers, as would be expected from a performance function that encourages changing position as fast as possible. However, these high gains amplify noise in the measurements, which can lead to crashes; that is, the performance-based constraint cannot properly encode safety. Notice that the blue shaded area does not correspond to full exploration, since the experiment was aborted after the first serious crash.\nHowever, if in addition, we define as safety constraint on the maximum angular velocity, g2(a) = 0.5\u2212maxk |\u03c9x,k|, with \u03c32 = 0.1, l = 0.2, and \u03c9\u03b7 = 0.25, the algorithm explores the parameter space and stops before the safety constraints are violated, as can be seen in Fig. 3. Instead, the algorithm explores increasing the damping ratio, which allows slightly smaller values of \u03c4 and therefore higher performance without violating the safety constraints."}, {"heading": "B. Circle Trajectory", "text": "In a second experiment, we define the optimization criterion as the RMSE with respect to a circle trajectory of radius 1 m at a speed of 1 m/s, using the same hyperparameters as in Sec. V-A. Feasibility of such motions has been analyzed in [30]. We define safety as a constraint on the RMSE (0.2 m) and a constraint on the maximum angular velocity around the x and y axis (0.5 rad/s). The yaw-angle always point at the center of the circle, which ideally should lead to zero\nangular velocity. Deviations from this are an indication of unsafe behavior.\nThe trajectories that result from running the optimization algorithm are shown in Fig. 4. Only safe parameters that keep the vehicle within the constraints on RMSE and angular velocity are evaluated. The resulting optimized trajectory (in red) is the best that can be obtained given the safety constraints and controller structure above."}, {"heading": "C. Context-Dependent Optimization", "text": "An additional aspect of using GPs to model the performance is that we can transfer knowledge to different environmental situations, called contexts [31]. Contexts are determined outside of the algorithm, and may correspond to environment conditions such as weather or external signals that cannot be affected by the algorithm. We model how the performance and constraint functions change with respect to these contexts, by multiplying the kernel function, kp, with another kernel, kz , over the contexts,\nk((a, i, z), (a\u2032, i\u2032, z\u2032)) = kp((a, i), (a \u2032, i\u2032)) \u2217 kz(z, z\u2032). (28)\nThis kernel structure implies that function values are correlated when both parameters and the context are similar. In our circle experiment, we model how performance and constraints vary with desired speed by defining a kernel kz(x\u0307des, x\u0307\u2032des) with parameters \u03c3\u03b7 = 1 and l = 0.25. Based on the data from Sec. V-B, the new model defines speeds which are known to be safe. Starting from Sec. V-B, we run SAFEOPTMC while increasing the speed to maximum safe values. This allows us to find optimal parameters for increasingly higher speeds, which satisfy the constraints. We can increase the speed safely up to 1.8 m/s. We show the mean performance function estimates for two speeds in Fig. 5. For lower speeds, the best controller parameters track the reference position more aggressively (low \u03c4 ), while for higher speeds the optimal parameters put higher relative gains on the velocity (1/\u03c42 < 2\u03b6/\u03c4 ). Additionally, as expected, high speeds lead to higher reference tracking errors. Overall, this approach allows us to find context-dependent parameters, while remaining within the safety constraints."}, {"heading": "VI. CONCLUSION", "text": "We presented a generalization of the Safe Bayesian Optimization algorithm in [3] that allows for multiple, separate safety constraints and applied it to nonlinear control problems on a quadrotor vehicle. Overall, the algorithm enabled efficient and automatic optimization of parameters without violating the safety constraints, which would lead to system failures."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was supported in part by SNSF grant 200020 159557, NSERC grant RGPIN-2014-04634, and the Connaught New Researcher Award."}], "references": [{"title": "Learning control in robotics", "author": ["S. Schaal", "C.G. Atkeson"], "venue": "IEEE Robotics & Automation Magazine, vol. 17, no. 2, pp. 20\u201329, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Safe controller optimization for quadrotors with Gaussian processes", "author": ["F. Berkenkamp", "A.P. Schoellig", "A. Krause"], "venue": "Proc. of the IEEE International Conference on Robotics and Automation (ICRA), 2016, (to appear).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Safe exploration for optimization with Gaussian processes", "author": ["Y. Sui", "A. Gotovos", "J.W. Burdick", "A. Krause"], "venue": "Proc. of the International Conference on Machine Learning (ICML), 2015, pp. 997\u20131005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "PID tuning using extremum seeking: online, model-free performance optimization", "author": ["N.J. Killingsworth", "M. Krsti\u0107"], "venue": "IEEE Control Systems, vol. 26, no. 1, pp. 70\u201379, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic tuning and adaptation for PID controllers - a survey", "author": ["K.J. \u00c5str\u00f6m", "T. H\u00e4gglund", "C.C. Hang", "W.K. Ho"], "venue": "Control Engineering Practice, vol. 1, no. 4, pp. 699\u2013714, 1993.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Genetic algorithms and robotics: a heuristic strategy for optimization", "author": ["Y. Davidor"], "venue": "World Scientific,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "Policy Gradient Methods for Robotics", "author": ["J. Peters", "S. Schaal"], "venue": "Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems, 2006, pp. 2219\u20132225.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian approach to global optimization: theory and applications", "author": ["J. Mockus"], "venue": "Springer Science & Business Media,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Convergence rates of efficient global optimization algorithms", "author": ["A.D. Bull"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2879\u20132904, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian process optimization in the bandit setting: no regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "Proc. of the International Conference on Machine Learning (ICML), 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A taxonomy of global optimization methods based on response surfaces", "author": ["D.R. Jones"], "venue": "Journal of Global Optimization, vol. 21, no. 4, pp. 345\u2013383, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian gait optimization for bipedal locomotion", "author": ["R. Calandra", "N. Gopalan", "A. Seyfarth", "J. Peters", "M.P. Deisenroth"], "venue": "Learning and Intelligent Optimization. Springer, 2014, pp. 274\u2013290.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic gait optimization with Gaussian process regression.", "author": ["D.J. Lizotte", "T. Wang", "M.H. Bowling", "D. Schuurmans"], "venue": "in Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Using response surfaces and expected improvement to optimize snake robot gait parameters", "author": ["M. Tesch", "J. Schneider", "H. Choset"], "venue": "Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2011, pp. 1069\u20131074.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic LQR tuning based on Gaussian process optimization", "author": ["A. Marco", "P. Hennig", "J. Bohg", "S. Schaal", "S. Trimpe"], "venue": "Proc. of the IEEE International Conference on Robotics and Automation (ICRA), 2016, (to appear).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "An experimental comparison of Bayesian optimization for bipedal locomotion", "author": ["R. Calandra", "A. Seyfarth", "J. Peters", "M.P. Deisenroth"], "venue": "Proc. of the IEEE International Conference on Robotics and Automation (ICRA), 2014, pp. 1951\u20131958.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian optimization with unknown constraints", "author": ["M.A. Gelbart", "J. Snoek", "R.P. Adams"], "venue": "Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI), 2014, pp. 250\u2013259.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Safe exploration for active learning with Gaussian processes", "author": ["J. Schreiter", "D. Nguyen-Tuong", "M. Eberts", "B. Bischoff", "H. Markert", "M. Toussaint"], "venue": "Proc. of the European Conference on Machine Learning (ECML), vol. 9284, 2015, pp. 133\u2013149.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Essentials of robust control", "author": ["K. Zhou", "J.C. Doyle"], "venue": "Prentice Hall Upper Saddle River, NJ,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Safe and robust learning control with Gaussian processes", "author": ["F. Berkenkamp", "A.P. Schoellig"], "venue": "Proc. of the European Control Conference (ECC), 2015, pp. 2501\u20132506.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Provably safe and robust learning-based model predictive control", "author": ["A. Aswani", "H. Gonzalez", "S.S. Sastry", "C. Tomlin"], "venue": "Automatica, vol. 49, no. 5, pp. 1216\u20131226, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Reachability-based safe learning with Gaussian processes", "author": ["A.K. Akametalu", "S. Kaynama", "J.F. Fisac", "M.N. Zeilinger", "J.H. Gillula", "C.J. Tomlin"], "venue": "Proc. of the IEEE Conference on Decision and Control (CDC), 2014, pp. 1424\u20131431.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Safe exploration in Markov decision processes", "author": ["T.M. Moldovan", "P. Abbeel"], "venue": "Proc. of the International Conference on Machine Learning (ICML), 2012, pp. 1711\u20131718.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernels for Vector- Valued Functions: A Review", "author": ["M.A. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 3, pp. 195\u2013266, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Posterior consistency of Gaussian process prior for nonparametric binary regression", "author": ["S. Ghosal", "A. Roy"], "venue": "The Annals of Statistics, vol. 34, no. 5, pp. 2413\u20132429, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Feed-forward parameter identification for precise periodic quadrocopter motions", "author": ["A. Schoellig", "C. Wiltsche", "R. D\u2019Andrea"], "venue": "Proc. of the American Control Conference (ACC), 2012, pp. 4313\u20134318.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A platform for aerial robotics research and demonstration: The Flying Machine Arena", "author": ["S. Lupashin", "M. Hehn", "M.W. Mueller", "A.P. Schoellig", "M. Sherback", "R. D\u2019Andrea"], "venue": "Mechatronics, vol. 24, no. 1, pp. 41\u201354, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Feasiblity of motion primitives for choreographed quadrocopter flight", "author": ["A. Schollig", "M. Hehn", "S. Lupashin", "R. D\u2019Andrea"], "venue": "Proc. of the American Control Conference (ACC), 2011, pp. 3843\u20133849.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextual Gaussian process bandit optimization", "author": ["A. Krause", "C.S. Ong"], "venue": "Proc. of Neural Information Processing Systems (NIPS), 2011, pp. 2447\u20132455.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "This holds true especially in robotics, where systems often face large prior uncertainties [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Recently, this manual tuning approach in robotics was automated by using safe Bayesian optimization [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "framework of SAFEOPT (Safe Optimization) from [3] to our Fig.", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "However, typical algorithms in the literature do not consider safety of the optimization process, and make other restrictive assumptions such as requiring gradients [4], [5], which are difficult to obtain from noisy data, or an impractical number of experiments [6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 4, "context": "However, typical algorithms in the literature do not consider safety of the optimization process, and make other restrictive assumptions such as requiring gradients [4], [5], which are difficult to obtain from noisy data, or an impractical number of experiments [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "However, typical algorithms in the literature do not consider safety of the optimization process, and make other restrictive assumptions such as requiring gradients [4], [5], which are difficult to obtain from noisy data, or an impractical number of experiments [6].", "startOffset": 262, "endOffset": 265}, {"referenceID": 6, "context": "In the reinforcement learning literature on policy gradients, safety has mostly been considered in terms of heuristics that disallow large steps along the gradient into areas of the state space that have not been explored before [7].", "startOffset": 229, "endOffset": 232}, {"referenceID": 7, "context": "One recent category of optimization algorithms that has been successfully applied to robotics is Bayesian optimization [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "global optimum of the objective function while evaluating the function at only few parameters [9], [10].", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "global optimum of the objective function while evaluating the function at only few parameters [9], [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "Bayesian optimization methods often model the unknown function as a Gaussian process (GP) [11], which in turn is used to guide function evaluations to locations that are informative about the optimum of the unknown function [8], [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Bayesian optimization methods often model the unknown function as a Gaussian process (GP) [11], which in turn is used to guide function evaluations to locations that are informative about the optimum of the unknown function [8], [12].", "startOffset": 224, "endOffset": 227}, {"referenceID": 11, "context": "Bayesian optimization methods often model the unknown function as a Gaussian process (GP) [11], which in turn is used to guide function evaluations to locations that are informative about the optimum of the unknown function [8], [12].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "include gait optimization of legged robots [13], [14] and ar X iv :1 60 2.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "include gait optimization of legged robots [13], [14] and ar X iv :1 60 2.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "the optimization of the controller parameters of a snake-like robot [15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "In [16] the weighting matrices of an LQR controller", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Several different Bayesian optimization methods were compared in [17] for bipedal locomotion.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "In [18] an algorithm to optimize an unknown function subject to an unknown constraint was introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "The case of safety-critical constraints was considered in [3] and [19].", "startOffset": 58, "endOffset": 61}, {"referenceID": 18, "context": "The case of safety-critical constraints was considered in [3] and [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "The algorithm in [3], called SAFEOPT, has been successfully applied to robotics in [2].", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "The algorithm in [3], called SAFEOPT, has been successfully applied to robotics in [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "We extend the theory of SAFEOPT in [3] to account for these additional constraints and show that similar theoretical guarantees can be obtained for the new setting.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "We then relax the assumptions used in the proofs to obtain a more practical version of the algorithm similar to [2], but additionally show that the safety guarantees carry over to this case.", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "In control theory, safety in the presence of unmodeled dynamics is often interpreted as stability of the underlying control law with respect to an uncertain model [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "Safety can be guaranteed by ensuring that either the controller is robustly stable for all possible models within the uncertainty specification [21] or the system never leaves a safe subset of the state space [22], [23], [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "Safety can be guaranteed by ensuring that either the controller is robustly stable for all possible models within the uncertainty specification [21] or the system never leaves a safe subset of the state space [22], [23], [24].", "startOffset": 209, "endOffset": 213}, {"referenceID": 22, "context": "Safety can be guaranteed by ensuring that either the controller is robustly stable for all possible models within the uncertainty specification [21] or the system never leaves a safe subset of the state space [22], [23], [24].", "startOffset": 215, "endOffset": 219}, {"referenceID": 23, "context": "Safety can be guaranteed by ensuring that either the controller is robustly stable for all possible models within the uncertainty specification [21] or the system never leaves a safe subset of the state space [22], [23], [24].", "startOffset": 221, "endOffset": 225}, {"referenceID": 1, "context": "For example, this performance measure may represent the negative tracking error of a robot [2], the average walking speed of a bipedal robot [13], or any other quantity that can be computed over a finite time horizon.", "startOffset": 91, "endOffset": 94}, {"referenceID": 12, "context": "For example, this performance measure may represent the negative tracking error of a robot [2], the average walking speed of a bipedal robot [13], or any other quantity that can be computed over a finite time horizon.", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "The introduction to GPs is standard and taken from [2] and [11].", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "The introduction to GPs is standard and taken from [2] and [11].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "This is accomplished by assuming that function values f(a), associated with different values of a, are random variables and that any finite number of these random variables have a joint Gaussian distribution depending on the values of a [11].", "startOffset": 237, "endOffset": 241}, {"referenceID": 10, "context": "A review of potential kernels can be found in [11].", "startOffset": 46, "endOffset": 50}, {"referenceID": 24, "context": "treated by considering a matrix of kernel functions, which models the correlation between different functions [25].", "startOffset": 110, "endOffset": 114}, {"referenceID": 24, "context": "Importantly, using this surrogate function rather than the framework in [25] allows us to bound the information gain and provide theoretical guarantees in Sec.", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "Bayesian optimization aims to find the global maximum of an unknown function [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "in [10], the next sample location is", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "In this paper, we extend the safe optimization algorithm SAFEOPT [3] to multiple constraints.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "SAFEOPT [3] trades off between these two sets by choosing for the next experiment the parameters inside the safe set about whose performance we are most uncertain.", "startOffset": 8, "endOffset": 11}, {"referenceID": 25, "context": "Many commonly used kernels, such as the squared exponential (Gaussian) kernel, satisfy this condition with high probability [26].", "startOffset": 124, "endOffset": 128}, {"referenceID": 2, "context": "Instead, we follow [3] and consider learning the safety constraint to some accuracy, .", "startOffset": 19, "endOffset": 22}, {"referenceID": 18, "context": "One could, similar to [19], simply select the most uncertain element over the entire set.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "To avoid this, we first define subsets of Sn that correspond to parameters that could either improve the estimate of the maximum or could expand the safe set, similar to [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "In this section, we show that the same theoretical frame work from the SAFEOPT algorithm [3] can be extended to multiple constraints and the evaluation criterion (14).", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "1 in [10].", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "bounded noise see [10].", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "It has been shown in [10] that the mutual information has a sublinear dependence on the number of samples for commonly used kernels.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "In practice, we use the GP directly to ensure safety [2]; that is, we define l n(a) = minQn(a, i) and un(a, i) = maxQn(a, i) in terms of the confidence intervals.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "the number of new states that are classified as safe, see [2].", "startOffset": 58, "endOffset": 61}, {"referenceID": 26, "context": "For details regarding the controllers see [28], [29].", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "For details regarding the controllers see [28], [29].", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Here, we choose the Mat\u00e8rn kernel with parameter \u03bd = 3/2 [11],", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "If, as was done in [2], one were to set the safety constraint as g1(a) = f(a), the algorithm would classify the blue shaded region in Fig.", "startOffset": 19, "endOffset": 22}, {"referenceID": 28, "context": "Feasibility of such motions has been analyzed in [30].", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "An additional aspect of using GPs to model the performance is that we can transfer knowledge to different environmental situations, called contexts [31].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "We presented a generalization of the Safe Bayesian Optimization algorithm in [3] that allows for multiple, separate safety constraints and applied it to nonlinear control problems on a quadrotor vehicle.", "startOffset": 77, "endOffset": 80}], "year": 2016, "abstractText": "Robotics algorithms typically depend on various parameters, the choice of which significantly affects the robot\u2019s performance. While an initial guess for the parameters may be obtained from dynamic models of the robot, parameters are usually tuned manually on the real system to achieve the best performance. Optimization algorithms, such as Bayesian optimization, have been used to automate this process. However, these methods may evaluate parameters during the optimization process that lead to safety-critical system failures. Recently, a safe Bayesian optimization algorithm, called SAFEOPT, has been developed and applied in robotics, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is not desirable in most cases. In this paper, we define separate functions for performance and safety. We present a generalized SAFEOPT algorithm that, given an initial safe guess for the parameters, maximizes performance but only evaluates parameters that satisfy all safety constraints with high probability. It achieves this by modeling the underlying and unknown performance and constraint functions as Gaussian processes. We provide a theoretical analysis and demonstrate in experiments on a quadrotor vehicle that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters. Moreover, we show an extension to contextor environmentdependent, safe optimization in the experiments.", "creator": "LaTeX with hyperref package"}}}