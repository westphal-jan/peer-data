{"id": "1602.05127", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2016", "title": "A Harmonic Extension Approach for Collaborative Ranking", "abstract": "we present a new perspective on graph - based methods ignoring information ranking for recommender systems. unlike user - adaptive or item - based schemes that compute a weighted average of priorities given by the nearest neighbors, or low - rank approximation methods around convex optimization and the nuclear norm, we formulate component completion as a series of semi - supervised learning problems, and propagate the known ratings to the missing input supplying the user - user or item - item graph globally. the semi - supervised learning problems are introduced as laplace - beltrami equations on a problem, or namely, harmonic extension, and can be discretized by a point integral measure. we show that our approach does not impose a low - rank euclidean subspace on the data points, but instead minimizes the price of the appropriate manifold. our method, named kernel ( low dimensional manifold ), turns out to be particularly restrictive against generating rankings of items, showing decent computational efficiency ; optimal analytical quality compared to state - of - the - art methods.", "histories": [["v1", "Tue, 16 Feb 2016 18:35:25 GMT  (173kb,D)", "http://arxiv.org/abs/1602.05127v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["da kuang", "zuoqiang shi", "stanley osher", "rea bertozzi"], "accepted": false, "id": "1602.05127"}, "pdf": {"name": "1602.05127.pdf", "metadata": {"source": "CRF", "title": "A Harmonic Extension Approach for Collaborative Ranking", "authors": ["Da Kuang", "Zuoqiang Shi", "Stanley Osher", "Andrea Bertozzi"], "emails": ["dakuang@math.ucla.edu.", "sjo@math.ucla.edu.", "bertozzi@math.ucla.edu.", "zqshi@math.tsinghua.edu.cn."], "sections": [{"heading": "1 Introduction", "text": "Recommender systems are crucial components in contemporary e-commerce platforms (Amazon, eBay, Netflix, etc.), and were popularized by the Netflix challenge. Detailed surveys of this field can be found in [11, 13]. Recommendation algorithms are commonly based on collaborative filtering, or \u201ccrowd of wisdom\u201d, and can be categorized into memory-based and model-based approaches. Memory-based approaches include user-based and item-based recommendation [18]. For example, for a user u, we retrieve the highly-rated items from the nearest neighbors of u, and recommend those items that have not been consumed by u. Memory-based methods are actually based on a graph, where a user-user or item-item similarity matrix defines the nearest neighbors of each user or item. In contrast, model-based methods are formulated as matrix completion problems which assume that the entire user-by-item rating matrix is low-rank [3], and the goal is to predict the missing ratings given the observed ratings. While memory-based methods are typically more computationally efficient, model-based methods can achieve much higher quality for collaborative filtering.\nPopular model-based methods such as regularized SVD [3] minimize the sum-of-squares error over all the observed ratings. When evaluating the predictive accuracy of these algorithms, we often divide the whole data set into a training set and a test set. After obtaining a model on the training set, we evaluate the accuracy of the model\u2019s prediction on the test set in order to see how well it generalizes to unseen data. However, the measure for evaluating success in a practical recommender system is very different. What we care more about is whether the top recommended\n\u2217Department of Mathematics, University of California, Los Angeles (UCLA), Los Angeles, CA, 90095. Email: {dakuang,sjo,bertozzi}@math.ucla.edu. \u2020Yau Mathematical Sciences Center, Tsinghua University, Beijing, China, 100084. Email: zqshi@math.tsinghua.edu.cn.\nar X\niv :1\n60 2.\n05 12\n7v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n16\nitems for a user u will actually be \u201cliked\u201d by u. In an experimental setting, the evaluation measure for success in this context is the resemblance between the ranked list of top recommended items and the ranked list of observed ratings in the test set. Thus, this measure that compares two rankings is more relevant to the performance in real scenarios. The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.\nWe start with the matrix completion problem and formulate it as a series of semi-supervised learning problems, or in particular, harmonic extension problems on a manifold that can be solved by label propagation [24, 20]. For each item, we want to know the ratings by all the users, and the goal of the semi-supervised learning problem is to propagate the known labels for this item (observed ratings) to the unknown labels on the user-user graph; and reversely, for each user, to propagate the known labels given by this user to the unknown labels on the item-item graph.\nWithout loss of generality, we assume that there exists a user manifold, denoted as M, which consists of an infinite number of users. In a user-by-item rating system with n items, each user is identified by an n-dimensional vector that consists of the ratings to n items. Thus, the user manifold M is a submanifold embedded in Rn. For the i-th item, we define the rating function fi :M\u2192 R that maps a user into the rating of this item.\nOne basic observation is that for a fixed item, similar users should give similar ratings. This implies that the function fi, 1 \u2264 i \u2264 n is a smooth function on M. Therefore, it is natural to find the rating function fi by minimizing the following energy functional:\nE(f) = \u222b M \u2016\u2207Mf(u)\u20162du, (1)\nwhere \u2207Mf(u) is the gradient at u defined on the manifold M. Using standard variational approaches, minimizing the above functional (1) is reduced to solving the Laplace-Beltrami equation on the user manifold M. Then the Laplace-Beltrami equation can be solved by a novel point integral method [20].\nFor the harmonic extension model, we also have an interpretation based on the low dimensionality of the user manifold, after which we call our method LDM. The user manifold is a manifold embedded in Rn, and n is usually a large number. Compared to n, the intrinsic dimension of the user manifold is typically much smaller. Based on this observation, we use the dimension of the user manifold as a regularization to recover the rating matrix. This idea implies the following optimization problem:\nmin X\u2208Rm\u00d7n, M\u2282Rn\ndim(M), (2)\nsubject to: P\u2126(X) = P\u2126(A), U(X) \u2282M,\nwhere dim(M) is the dimension of the manifoldM, and U(X) is the user set corresponding to the rows of X. \u2126 = {(i, j) : user i rated item j} is the index set of the observed ratings, and P\u2126 is the projection operator to \u2126,\nP\u2126(X) = { xij , (i, j) \u2208 \u2126, 0, (i, j) /\u2208 \u2126.\nBy referring to the theory in differential geometry, this optimization problem is reduced to the same formulation as that in harmonic extension (1) which gives our model a geometric interpretation.\nAnother important aspect of our proposed method is the weight matrix that defines the useruser or item-item graph. Because the information given in the rating matrix is incomplete, we can\nonly assume that the weight matrix used in harmonic extension is a good guess. We will propose an efficient way to construct the weight matrix based on incomplete data.\nOur main contribution is summarized as follows:\n\u2022 We propose an algorithm that exploits manifold structures to solve the harmonic extension problem for collaborative ranking, representing a new perspective on graph-based methods for recommender systems.\n\u2022 On real data sets, our method achieves robust ranking quality with reasonable run-time, compared to state-of-the-art methods for large-scale recommender systems.\nThe rest of this paper is organized as follows. In Section 2, we formulate the matrix completion problem as harmonic extension. In Section 3, we describe the point integral method to rigorously solve the discretized harmonic extension problem. In Section 4, we show that our approach seeks to minimize the dimension of the underlying manifold. In Section 5, we describe our more efficient way to compute the similarity matrix. In Section 6, we empirically demonstrate the efficiency and ranking quality of our method. In Section 7, we explain the connection and difference between our method and previous work. In Section 8, we discuss our proposed method and its implication on other methods.\nHere are some notations we will use. For a vector x = [x1, \u00b7 \u00b7 \u00b7 , xm]T , we call y = [xi1 , xi2 , \u00b7 \u00b7 \u00b7 , xir ]T a subvector of length r by extracting the elements of x in the index set {i1, \u00b7 \u00b7 \u00b7 , ir}, where i1 < i2 < \u00b7 \u00b7 \u00b7 < ir. For a matrix M , a vector x, integers i, j, and sets of row and column indices S, S\u2032, we use Mi,j ,MS,S\u2032 ,M:,j ,MS,j , xS to denote an entry of M , a submatrix of M , the j-th column of M , a subvector of the j-th column of M , and a subvector of x, respectively."}, {"heading": "2 Harmonic Extension Formulation", "text": "Consider a user-by-item rating matrix A = (aij) \u2208 Rm\u00d7n, where rows correspond to m users, and columns correspond to n items. The observed ratings are indexed by the set \u2126 = {(i, j) : user i rated item j}. Let \u2126i = {1 \u2264 j \u2264 m : (i, j) \u2208 \u2126}, 1 \u2264 i \u2264 n. Suppose there exists a \u201ctrue\u201d rating matrix A\u2217 given by an oracle with no missing entries, which is not known to us, and A|\u2126 = A\u2217|\u2126.\nAs mentioned in the introduction, we formulate matrix completion as a harmonic extension problem on a manifold. Recall the user manifold in Section 1, denoted as M, which is embedded in Rn. The set of m users in our user-by-item rating system is represented as U = {uj , 1 \u2264 j \u2264 m} where uj is the j-th row of A\n\u2217 and U \u2282 M is a sample of M. Let Ui = {uj \u2208 U : j \u2208 \u2126i} be the collection of users who rate the i-th item.\nThen we compute the rating function fi for all the users by minimizing the energy functional in (1):\nmin fi\u2208H1(M) E(fi) subject to: fi(uj)|Ui = aij , (3)\nwhere H1 is the Sobolev space. Hence, we need to solve the following type of optimization problem for n times.\nmin f\u2208H1(M)\nE(f) subject to: f(u)|\u039b = g(u), (4)\nwhere \u039b \u2282M is a point set. To solve the above optimization problem, we first use the Bregman iteration to enforce the constraint [15].\nAlgorithm 1 Algorithm for solving (3)\n1: repeat 2: Get estimate for \u2206M based on f1, f2, \u00b7 \u00b7 \u00b7 , fn 3: for i = 1 to n do 4: Solve (6) to obtain fi 5: end for 6: until some stopping criterion is satisfied\n\u2022 Solve fk+1 = arg min\nf E(f) + \u00b5\u2016f \u2212 g + dk\u20162L2(\u039b), (5)\nwhere \u2016f\u20162L2(\u039b) = \u2211 u\u2208\u039b |f(u)|2, dn is a function defined on \u039b.\n\u2022 Update dk, dk+1(u) = dk(u) + fk+1(u)\u2212 g(u), \u2200u \u2208 \u039b.\n\u2022 Repeat above process until convergence.\nUsing a standard variational approach, the solution to (5) can be reduced to the following Laplace-Beltrami equation: \u2206Mf(x)\u2212 \u00b5 \u2211 y\u2208\u039b \u03b4(x\u2212 y)(f(y)\u2212 h(y)) = 0, x \u2208M, \u2202f\n\u2202n (x) = 0, x \u2208 \u2202M,\n(6)\nwhere \u03b4 is the Dirac-\u03b4 function inM, h = g\u2212dn is a given function on \u039b, and n is the outer normal vector. That is to say, the function f that minimizes (4) is a harmonic function on M\\\u2202M, and (6) is called the harmonic extension problem in the continuous setting.\nIf the underlying manifold M (the true rating matrix A\u2217 in the discrete setting) were known, the n problems in (3) would be independent with each other and could be solved individually by (6). However,M is not known, and therefore we have to get a good estimate for the operator \u2206M based on fj \u2019s. Our algorithm for solving (3) is described on a high level in Algorithm 1, where we iteratively update fj \u2019s and our estimate for \u2206M.\nIn the next section, we use the point integral method (PIM) to solve the Laplace-Beltrami equation (6).\nRemark. The update of fj \u2019s in Algorithm 1 follows the \u201cJacobi\u201d scheme. We could also use the \u201cGauss-Seidel\u201d scheme, i.e. re-estimate \u2206M after the update of each fj , but that would be much slower."}, {"heading": "3 Point Integral Method (PIM)", "text": "Integral Equation: The key observation in PIM is that the Laplace-Beltrami operator has the following integral approximation: \u222b\nM wt(x,y)\u2206Mf(y)dy\n\u2248\u2212 1 t \u222b M (f(x)\u2212 f(y))wt(x,y)dy\n+ 2 \u222b \u2202M \u2202f(y) \u2202n wt(x,y)d\u03c4y, (7)\nwhere wt(x,y) = exp(\u2212 |x\u2212y| 2\n4t ). The following theorem gives the accuracy of the integral approximation.\nTheorem 3.1. [19] If f \u2208 C3(M) is a smooth function on M, then for any x \u2208M,\n\u2016r(f)\u2016L2(M) = O(t 1/4), (8)\nwhere\nr(f) = \u222b M wt(x,y)\u2206Mf(y)dy\n+ 1\nt \u222b M (f(x)\u2212 f(y))wt(x,y)dy\n\u2212 2 \u222b \u2202M \u2202f(y) \u2202n wt(x,y)d\u03c4y.\nApplying the integral approximation (7) to the Laplace-Beltrami equation (6), we get an integral equation\n1\nt \u222b M (f(x)\u2212 f(y))wt(x,y)dy\n+ \u00b5 \u2211 y\u2208\u039b wt(x,y)(f(y)\u2212 h(y)) = 0, (9)\nIn this integral equation, there are no derivatives, and therefore it is easy to discretize over the point cloud.\nDiscretization: We notice that the closed form of the user manifold M is not known, and we only have a sample of M, i.e. U . Next, we discretize the integral equation (7) over the point set U .\nAssume that the point set U is uniformly distributed over M. The integral equation can be discretized easily, as follows:\n|M| m m\u2211 j=1 wt(xi,xj)(f(xi)\u2212 f(xj))+\n\u00b5t \u2211 y\u2208\u039b wt(xi,y)(f(y)\u2212 h(y)) = 0 (10)\nAlgorithm 2 Harmonic Extension\nInput: Initial rating matrix A. Output: Rating matrix R. 1: Set R = A. 2: repeat 3: Estimate the weight matrix W = (wij) from the user set U (Algorithm 3). 4: Compute the graph Laplacian matrix: L = D \u2212W 5: for i = 1 to n do 6: repeat 7: Solve the following linear systems\nLfi + \u00b5\u0304W:,Ui(fi)Ui = \u00b5\u0304W:,UihUi ,\nwhere h = gi \u2212 dk. 8: Update dk, dk+1 = dk + fk+1 \u2212 gi 9: until some stopping criterion for the Bregman iterations is satisfied\n10: end for 11: rij = fi(uj) and R = (rij). 12: until some stopping criterion is satisfied\nwhere |M| is the volume of the manifold M. We can rewrite (10) in the matrix form.\nLf + \u00b5\u0304W:,\u039bf\u039b = \u00b5\u0304W:,\u039bh. (11)\nwhere h = (h1, \u00b7 \u00b7 \u00b7 , hm) and \u00b5\u0304 = \u00b5tm|M| . L is a m\u00d7m matrix which is given as\nL = D \u2212W (12) where W = (wij), i, j = 1, \u00b7 \u00b7 \u00b7 ,m is the weight matrix and D = diag(di) with di = \u2211m\nj=1wij . Remark. In the harmonic extension approach, we use a continuous formulation based on the underlying user manifold. And the point integral method is used to solve the Laplace-Beltrami equation on the manifold. If a graph model were used at the beginning, the natural choice for harmonic extension would be the graph Laplacian. However, it has been observed that the graph Laplacian is not consisitent in solving the harmonic extension problem [20, 16], and PIM gives much better results.\nRemark. The optimization problem we defined in (1) can be viewed as a continuous analog of the discrete harmonic extension problem [24], which we write in our notations:\nmin fi n\u2211 j,j\u2032=1 wjj\u2032 ( (fi)j \u2212 (fi)j\u2032 )2 subject to: (fi)Ui = AUi,i. (13)\nThe formulation (13) in the context of collaborative ranking can be seen as minimizing the weighted sum of squared error in pairwise ranking. This form of loss function considers all the possible pairwise rankings of items, which is different from the loss function in previous work on collaborative ranking [9, 17]: \u2211\nj,j\u2032\u2208Ui\nL ( [aji \u2212 aj\u2032i]\u2212 [(fi)j \u2212 (fi)j\u2032 ] ) , (14)\nwhere L is a loss function such as hinge loss and exponential loss. Only the pairwise rankings of items in the training set are considered in (14)."}, {"heading": "4 Low Dimensional Manifold (LDM) Interpretation", "text": "In this section, we emphasize the other interpretation of our method based on the low dimensionality of the user manifold. In the user-by-item rating system, a user is represented by an n-dimensional vector that consists of the ratings to n items, and the user manifold is a manifold embedded in Rn. Usually, n, the number of items, is a large number in the order of 103 \u223c 106. The intrinsic dimension of the user manifold is much less than n. Based on this observation, it is natural to recover the rating matrix by looking for the user manifold with the lowest dimension, which implies the optimization problem in (2):\nmin X\u2208Rm\u00d7n, M\u2282Rn\ndim(M),\nsubject to: P\u2126(X) = P\u2126(A), U(X) \u2282M.\nwhere dim(M) is the dimension of the manifoldM, and U(X) is the user set corresponding to the rows of X.\nNext, we need to give a mathematical expression to compute dim(M). Here we assume M is a smooth manifold embedded in Rn. Let \u03b1i, i = 1, \u00b7 \u00b7 \u00b7 , d be the coordinate functions on M, i.e.\n\u03b1i(x) = xi, \u2200x = (x1, \u00b7 \u00b7 \u00b7 , xn) \u2208M (15)\nUsing differential geometry, we have the following formula [16].\nProposition 4.1. Let M be a smooth submanifold isometrically embedded in Rn. For any x \u2208M,\ndim(M) = n\u2211 i=1 \u2016\u2207M\u03b1i(x)\u20162\nwhere \u2207M is the gradient in the manifold M.\nWe can clearly see that \u03b1i corresponds to the rating function fi. Using the above proposition, the manifold dimension minimization problem (2) can be rewritten as\nmin X\u2208Rm\u00d7n, M\u2282Rd d\u2211 i=1 \u2016\u2207Mfi\u20162L2(M), (16)\nsubject to: fi(xj)|Ui = aij , U(X) \u2282M,\nwhere\n\u2016\u2207Mfi\u2016L2(M) = (\u222b M \u2016\u2207Mfi(x)\u20162dx )1/2 . (17)\nThis is the same optimization problem we solved in Section 2 and Section 3."}, {"heading": "5 Weight Matrix", "text": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7]. We employ the typical user-user or item-item graph with cosine similarity used in existing memory-based approaches for recommendation [18]. However, we have made substantial changes to make the procedure efficient for large sparse rating matrices. Our algorithm for building the weight matrix is described in detail in Algorithm 3. Again, we consider the user-user graph without loss of generality.\nFirst, as usual, we can only afford to compute and store a sparse nearest-neighbor weight matrix. To get the K nearest neighbors for a target user u, traditional algorithms in memory-based methods require computing the distances between u and every other user, and selecting the K closest ones, where most of the computation is wasted if K << m. In our algorithm, we first identify the nearest neighbors approximately, without computing the actual distances or similarities, and then compute the similarities between u and its nearest neighbors only. We use a binary rating matrix RB that records \u201crated or not-rated\u201d information (Algorithm 3, line 1-2), and determine the K nearest neighbors using an kd-tree based approximate nearest neighbor algorithm (line 3, line 5) [12]. That is to say, two users who have rated similar sets of movies are more likely to be considered to be in each other\u2019s neighborhood, regardless of their numeric ratings for those movies. Neither of the ways to build the kd-tree and to find nearest neighbors based on the tree are as precise as a na\u0308\u0131ve search; however, empirical results in the next section have shown that our approximate strategy does not compromise the quality.\nSecond, we extended the VLFeat package [21] to enable building a kd-tree from a sparse data matrix (in our case, RB) and querying the tree with sparse vectors. kd-tree uses a space partitioning scheme for efficient neighbor search [2]. For high-dimensional data, we employ the greedy way that chooses the most varying dimension for space partitioning at each step of building the tree [12], and the procedure terminates when each leaf partition has one data point. Thus, the complexity of building the tree is not exponential, contrary to common understanding; and the practical performance of kd-tree can be very efficient and better than that of locality sensitive hashing [1, 14]. For example, in our case with m data points in n dimensions, the complexity of building the tree is O(m), rather than O(2n). In addition, when querying the tree, we put an upper bound D on the maximum number of distance comparisons, and therefore the overall complexity of finding K nearest neighbors for all the m data points is O(Dm logK) (the logK factor comes from maintaining a heap data structure for the K nearest neighbors).\nNote that the resulting graph is not symmetric. Also the cosine similarity for two data points with incomplete information is defined by using the co-rated items only (Algorithm 3, line 8-9) [18]."}, {"heading": "6 Experiments", "text": "In this section, we evaluate our proposed method LDM in terms of both run-time and ranking quality. Since the volume of literature on recommender systems, collaborative filtering, and matrix completion is huge, we select only a few existing methods to compare with. All the experiments are run on a Linux laptop with one Intel i7-5600U CPU (4 physical threads) and 8 GB memory.\nAlgorithm 3 Building weight matrix from incomplete rating data Input: Incomplete rating matrix R \u2208 Rm\u00d7n, number of nearest neighbors K 1: Generate binary rating matrix RB \u2208 Rm\u00d7n:\n(RB)j,j\u2032 =\n{ 1, Rj,j\u2032 is not missing\n0, Rj,j\u2032 is missing\n2: Normalize each row of RB such that \u2016(RB)j,:\u20162 = 1, \u2200j, 1 \u2264 j \u2264 m 3: Build a kd-tree on the data points (rows) in RB 4: Initialize a sparse matrix W \u2190 0m\u00d7m 5: for j = 1 to m do 6: NB \u2190 The set of K approximate nearest neighbors of (RB)j,:, found by querying the kd-tree 7: for j\u2032 \u2208 NB (j\u2032 6= j) do 8: Set of co-rated items C \u2190 {i : Rj,i is not missing, and Rj\u2032,i is not missing} 9: Wj,j\u2032 \u2190 cosine(Rj,C , Rj\u2032,C)\n10: end for 11: end for Output: Sparse weight matrix W \u2208 Rm\u00d7m"}, {"heading": "6.1 Data Sets", "text": "We use three MovieLens1 data sets in our experiments: MovieLens-100k, MovieLens-1m, and MovieLens-10m. In each of the data sets, each user has at least 20 ratings. Following the convention in previous work on collaborative ranking, we randomly select N ratings for each user as the training set, and the other ratings were used for testing. To keep at least 10 ratings in the testing set for each user, we remove the users with fewer than N + 10 ratings. After this preprocessing, we can generate several versions of these data sets with different N \u2019s, whose information is summarized in Table 1.\n1http://grouplens.org/datasets/movielens/"}, {"heading": "6.2 Methods for Comparison", "text": "We compare our LDM method with singular value decomposition (SVD) as a baseline, and two state-of-the-art methods that are designed specifically for collaborative ranking. All the three methods for comparison optimize a pairwise ranking loss function. We do not perform hyperparameter selection on a separate validation set because it is time-consuming, but we investigate the effect of the hyperparameters in Section 6.4; and in Section 6.5, we use a fixed set of hyperparameters that can achieve a good balance between run-time and ranking quality, which are found empirically across several data sets. We list these methods below (their program options that will be used in Section 6.5 is in the footnote):\n\u2022 SVD: We use the Java implementation of ranking-based SVD in the PREA toolkit2,3. This version uses gradient descent to optimize the ranking-based loss function (14), as opposed to the squared loss function in regularized SVD.\n\u2022 LCR: Local collaborative ranking [9], as implemented in the PREA toolkit4.\n\u2022 AltSVM: Alternating support vector machine [17], as implemented in the collranking package5. We employ the default configurations.\nLastly, our proposed method LDM (Algorithm 2) is implemented in Matlab, and the construction and querying of the kd-tree (Algorithm 3) is implemented in C, for which we extended the VLFeat6 package to build a kd-tree with a sparse input matrix efficiently. In Algorithm 2, we set \u00b5 = 1 and run one inner iteration and one outer iteration only, since we empirically found that the weight matrix constructed from the incomplete input ratings by Algorithm 3 is often good enough for the algorithm to converge in one iteration. Algorithm 2 typically accounts for most (\u223c 95%) of the run-time in our method.\nAll the programs except SVD use 4 threads in our experiments, and are applied to the same training and testing random splits."}, {"heading": "6.3 Evaluation Measure", "text": "We evaluate the ranking quality by normalized discounted cumulative gain (NDCG) @K [6], averaged over all the users. Given a ranked list of t items i1, \u00b7 \u00b7 \u00b7 , it and their ground-truth relevance score ri1 , \u00b7 \u00b7 \u00b7 , rit , the DCG@K score (K \u2264 t) is computed as\nDCG@K(i1, \u00b7 \u00b7 \u00b7 , it) = K\u2211 j=1 2rij \u2212 1 log2(j + 1) . (18)\nThen, we sort the list of items in the decreasing order of the relevance score and obtain the list i\u22171, \u00b7 \u00b7 \u00b7 , i\u2217t , where ri\u22171 \u2265 \u00b7 \u00b7 \u00b7 \u2265 ri\u2217t , and this sorted list achieves the maximum DCG@K score over all the possible permutations. The NDCG score is defined as the normalized version of (18):\nNDCG@K(i1, \u00b7 \u00b7 \u00b7 , it) = DCG@K(i1, \u00b7 \u00b7 \u00b7 , it) DCG@K(i\u22171, \u00b7 \u00b7 \u00b7 , i\u2217t ) . (19)\n2http://prea.gatech.edu 3The command-line options for SVD is -a ranksvd exp add 5 5 25. 4The command-line options for LCR is -a pgllorma exp add 5 5 13. 5https://github.com/dhpark22/collranking 6http://www.vlfeat.org/\nWe only evaluate NDCG@10 due to limited space. Note that for a user u, the list of items that is provided to compute (19) is all the items with observed ratings given by u in the test set, not only the highest-ranking ones.\nIn contrast to previous work on top-N recommender systems [8, 5], we discourage the use of Precision@K in the context of collaborative ranking for recommender systems, which measures the proportion of actually rated items out of the top K items in the ranked list of all the items in the data set. Contemporary recommender systems typically use numeric ratings rather than binary ratings. In a 1-to-5 rating system, for example, a 1-star item should be less favorable than an unrated item with an expected 3-star rating. However, a recommender system that ranks 1-star items at the top positions would get a higher Precision@K score than one that ranks unrated items at the top positions. Thus, Precision@K is not a valid measure for collaborative ranking with a non-binary rating system."}, {"heading": "6.4 Effect of Parameter Selection", "text": "First, we examine the influence of the kd-tree parameters on the performance of LDM, namely the number of nearest neighbors k and the maximum number of distance comparisons D. Fig. 1 shows the change in NDCG@10 when varying k and D on several small data sets (due to time constraints). In general, the ranking quality is much better with moderately large k,D values\nthan with very small k,D values, but does not improve much when further increasing k and D. Therefore, we can use sufficiently large k,D values to get good ranking quality, but not too large to be computationally efficient. In the experimental comparison in Section 6.5, we fix the parameters to k = 64 and D = 256.\nNext, we vary the hyperparameters in each of the four methods, and compare simultaneously the ranking quality and run-time under different hyperparameters. Ideally, a good performance of a collaborative ranking method means producing higher NDCG@10 scores in less time. Fig. 2 plots NDCG@10 against the run-time for several small data sets (due to time constraints). LDM achieves the highest NDCG@10 in a reasonable amount of time compared to the other methods. AltSVM is efficient but produces unsatisfactory ranking quality, which is also sensitive to its hyperparameters. For LCR, the ranking quality is acceptable but it takes considerably longer time, especially when the size of training set increases. On MovieLens-100k (N = 20), SVD and LDM achieve similar NDCG@10 scores but LDM costs much shorter run-time."}, {"heading": "6.5 Results", "text": "Now we fix the hyperparameters as described in Section 6.2. Table 2 reports the run-time and NDCG@10 scores for all the compared methods on the larger data sets. LDM does not achieve the highest NDCG@10 scores in every case, but produces robust ranking quality with decent run-time (except MovieLens-10m, N = 50). For LCR and SVD, the time cost increases dramatically on larger data sets. AltSVM achieves superior ranking quality when the number of training ratings N is large, but its performance is sensitive to the number of iterations, which in turn depends on the data set and the given tolerance parameter. We conclude that LDM is an overall competitive method that is efficient and robust to hyperparameters and the underlying data sets. Also, LDM has particular advantages when the available information is relatively few, i.e. when N is small, which we consider is a more difficult problem than the cases with richer training information.\nWe can clearly see that the run-time of LDM increases with the number of users (due to the reliance on the user-user similarity matrix, as expected), while the run-time of AltSVM increases with the number of ratings in the training set. Further optimizing the code for LDM is an important direction in our future work to make it efficient for large-scale data sets.\nWe note that our method predicts all the missing ratings in the label propagation process, which is included in the timing of LDM, and therefore our methods takes a negligible amount of time for testing, especially in the real scenario where a recommendation algorithm would have to predict the ratings for all the items and return the top-rated ones to the user."}, {"heading": "7 Related Work", "text": "Both user-based and item-based collaborative filtering [4, 18] can be considered as graph-based local label propagation methods. The idea of discrete harmonic extension for semi-supervised learning in general was originally proposed in [24]. Graph-based methods were recently considered for matrix completion [7] and top-N recommendation [23] as well. Given all the existing work, what we have presented is a continuous harmonic extension formulation for label propagation and a rigorous manifold learning algorithm for collaborative ranking."}, {"heading": "8 Conclusion and Discussion", "text": "In this paper, we have proposed a novel perspective on graph-based methods for matrix completion and collaborative ranking. For each item, we view the user-user graph as a partially labeled data set (or vice versa), and our method propagates the known labels to the unlabeled graph nodes through the graph edges. The continuous harmonic extension problem associated with the above semi-supervised learning problem is defined on a user or item manifold solved by a point integral method. Our formulation can be seen as minimizing the dimension of the user or item manifold, and thus builds a smooth model for the users or items but with higher complexity than low-rank matrix approximation. Also, our method can be fully parallelized on distributed machines, since the linear systems that need to be solved for all the items are independent with one another. Experimental results have shown that our method has particular strength when the number of available ratings in the training set is small, which makes it promising when combined with other state-of-the-art methods like AltSVM. Our formulation for harmonic extension in the context of matrix completion can be extended to include other constraints or regularization terms and side information as well. An important direction is to further improve the efficiency of the algorithm to be comparable with recent large-scale matrix completion methods [22, 25]."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM, vol. 51, no. 1, pp. 117\u2013122, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Commun. ACM, vol. 18, no. 9, pp. 509\u2013517, 1975.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1975}, {"title": "Learning collaborative information filters", "author": ["D. Billsus", "M.J. Pazzani"], "venue": "ICML \u201998: Proc. of the 15th Int. Conf. on Machine learning, 1998, pp. 46\u201354.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["J.S. Breese", "D. Heckerman", "C. Kadie"], "venue": "UAI \u201998: Proc. of the 14th Conf. on Uncertainty in Artificial Intelligence, 1998, pp. 43\u201352.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Item-based top-n recommendation algorithms", "author": ["M. Deshpande", "G. Karypis"], "venue": "ACM Trans. Inf. Syst., vol. 22, no. 1, pp. 143\u2013177, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Trans. Inf. Syst., vol. 20, no. 4, pp. 422\u2013446, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Matrix completion on graphs", "author": ["V. Kalofolias", "X. Bresson", "M. Bronstein", "P. Vandergheynst"], "venue": "NIPS Workshop on \u201cOut of the Box: Robustness in High Dimension\u201d, 2014. 13", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of item-based top-n recommendation algorithms", "author": ["G. Karypis"], "venue": "CIKM \u201901: Proc. of the 10th Int. Conf. on Information and Knowledge Management, 2001, pp. 247\u2013254.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Local collaborative ranking", "author": ["J. Lee", "S. Bengio", "S. Kim", "G. Lebanon", "Y. Singer"], "venue": "WWW \u201914: Proc. of the 23rd Int. Conf. on World Wide Web, 2014, pp. 85\u201396.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Local low-rank matrix approximation", "author": ["J. Lee", "S. Kim", "G. Lebanon", "Y. Singer"], "venue": "ICML \u201913: Proc. of the 30th Int. Conf. on Machine learning, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparative study of collaborative filtering algorithms", "author": ["J. Lee", "M. Sun", "G. Lebanon"], "venue": "2012, http://arxiv.org/abs/1205.3193.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP: Proc. of the Int. Conf. on Computer Vision Theory and Applications, 2009, pp. 331\u2013340.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Recent advances in recommender systems and future directions", "author": ["X. Ning", "G. Karypis"], "venue": "Pattern Recognition and Machine Intelligence, ser. Lecture Notes in Computer Science. Springer, 2015, vol. 9124, pp. 3\u20139.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Are you using the right approximate nearest neighbor algorithm?", "author": ["S. O\u2019Hara", "B.A. Draper"], "venue": "WACV \u201913: Workshop on Applications of Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An iterative regularization method for total variation-based image restoration", "author": ["S. Osher", "M. Burger", "D. Goldfarb", "J. Xu", "W. Yin"], "venue": "Multiscale Modeling & Simulation, vol. 4, no. 2, pp. 460\u2013489, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Low dimensional manifold model for image processing", "author": ["S. Osher", "Z. Shi", "W. Zhu"], "venue": "UCLA, Tech. Rep. CAM report 16-04, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Preference completion: Largescale collaborative ranking from pairwise comparisons", "author": ["D. Park", "J. Neeman", "J. Zhang", "S. Sanghavi", "I. Dhillon"], "venue": "ICML \u201915: Proc. of the 32th Int. Conf. on Machine learning, 2015, pp. 1907\u20131916.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Riedl"], "venue": "WWW \u201901: Proc. of the 10th Int. Conf. on World Wide Web, 2001, pp. 285\u2013295.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Convergence of the point integral method for the poisson equation on manifolds I: the Neumann boundary", "author": ["Z. Shi", "J. Sun"], "venue": "2014, http://arxiv.org/abs/1509.06458.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Harmonic extension", "author": ["Z. Shi", "J. Sun", "M. Tian"], "venue": "2015, http://arxiv.org/abs/1509.06458.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "2008, http://www.vlfeat.org/.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Nomad: Non-locking, stochastic multi-machine algorithm for asynchronous and decentralized matrix completion", "author": ["H. Yun", "H.-F. Yu", "C.-J. Hsieh", "S.V.N. Vishwanathan", "I. Dhillon"], "venue": "Proc. VLDB Endow., vol. 7, no. 11, pp. 975\u2013986, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Random walk models for top-n recommendation task", "author": ["Y. Zhang", "J.-q. Wu", "Y.-t. Zhuang"], "venue": "Journal of Zhejiang University SCIENCE A, vol. 10, no. 7, pp. 927\u2013936, 2009. 14", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "ICML \u201903: Proc. of the 20th Int. Conf. on Machine learning, 2003, pp. 912\u2013919.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "A fast parallel sgd for matrix factorization in shared memory systems", "author": ["Y. Zhuang", "W.-S. Chin", "Y.-C. Juan", "C.-J. Lin"], "venue": "RecSys \u201913: Proc. of the 7th ACM Conf. on Recommender Systems, 2013, pp. 249\u2013256. 15", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Detailed surveys of this field can be found in [11, 13].", "startOffset": 47, "endOffset": 55}, {"referenceID": 12, "context": "Detailed surveys of this field can be found in [11, 13].", "startOffset": 47, "endOffset": 55}, {"referenceID": 17, "context": "Memory-based approaches include user-based and item-based recommendation [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "In contrast, model-based methods are formulated as matrix completion problems which assume that the entire user-by-item rating matrix is low-rank [3], and the goal is to predict the missing ratings given the observed ratings.", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "Popular model-based methods such as regularized SVD [3] minimize the sum-of-squares error over all the observed ratings.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.", "startOffset": 156, "endOffset": 159}, {"referenceID": 8, "context": "The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.", "startOffset": 200, "endOffset": 207}, {"referenceID": 16, "context": "The problem that places priority on the top recommended items rather than the absolute accuracy of predicted ratings is referred to as top-N recommendation [5], or more recently collaborative ranking [9, 17], and is our focus in this paper.", "startOffset": 200, "endOffset": 207}, {"referenceID": 23, "context": "We start with the matrix completion problem and formulate it as a series of semi-supervised learning problems, or in particular, harmonic extension problems on a manifold that can be solved by label propagation [24, 20].", "startOffset": 211, "endOffset": 219}, {"referenceID": 19, "context": "We start with the matrix completion problem and formulate it as a series of semi-supervised learning problems, or in particular, harmonic extension problems on a manifold that can be solved by label propagation [24, 20].", "startOffset": 211, "endOffset": 219}, {"referenceID": 19, "context": "Then the Laplace-Beltrami equation can be solved by a novel point integral method [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "To solve the above optimization problem, we first use the Bregman iteration to enforce the constraint [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "[19] If f \u2208 C3(M) is a smooth function on M, then for any x \u2208M, \u2016r(f)\u2016L2(M) = O(t ), (8)", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "However, it has been observed that the graph Laplacian is not consisitent in solving the harmonic extension problem [20, 16], and PIM gives much better results.", "startOffset": 116, "endOffset": 124}, {"referenceID": 15, "context": "However, it has been observed that the graph Laplacian is not consisitent in solving the harmonic extension problem [20, 16], and PIM gives much better results.", "startOffset": 116, "endOffset": 124}, {"referenceID": 23, "context": "The optimization problem we defined in (1) can be viewed as a continuous analog of the discrete harmonic extension problem [24], which we write in our notations:", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "This form of loss function considers all the possible pairwise rankings of items, which is different from the loss function in previous work on collaborative ranking [9, 17]: \u2211 j,j\u2032\u2208Ui L ( [aji \u2212 aj\u2032i]\u2212 [(fi)j \u2212 (fi)j\u2032 ] ) , (14)", "startOffset": 166, "endOffset": 173}, {"referenceID": 16, "context": "This form of loss function considers all the possible pairwise rankings of items, which is different from the loss function in previous work on collaborative ranking [9, 17]: \u2211 j,j\u2032\u2208Ui L ( [aji \u2212 aj\u2032i]\u2212 [(fi)j \u2212 (fi)j\u2032 ] ) , (14)", "startOffset": 166, "endOffset": 173}, {"referenceID": 15, "context": "Using differential geometry, we have the following formula [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 17, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 9, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 6, "context": "The weight matrix W plays an important role in our algorithm as well as other graph-based approaches [24, 18, 10, 7].", "startOffset": 101, "endOffset": 116}, {"referenceID": 17, "context": "We employ the typical user-user or item-item graph with cosine similarity used in existing memory-based approaches for recommendation [18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "We use a binary rating matrix RB that records \u201crated or not-rated\u201d information (Algorithm 3, line 1-2), and determine the K nearest neighbors using an kd-tree based approximate nearest neighbor algorithm (line 3, line 5) [12].", "startOffset": 221, "endOffset": 225}, {"referenceID": 20, "context": "Second, we extended the VLFeat package [21] to enable building a kd-tree from a sparse data matrix (in our case, RB) and querying the tree with sparse vectors.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "kd-tree uses a space partitioning scheme for efficient neighbor search [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "For high-dimensional data, we employ the greedy way that chooses the most varying dimension for space partitioning at each step of building the tree [12], and the procedure terminates when each leaf partition has one data point.", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Thus, the complexity of building the tree is not exponential, contrary to common understanding; and the practical performance of kd-tree can be very efficient and better than that of locality sensitive hashing [1, 14].", "startOffset": 210, "endOffset": 217}, {"referenceID": 13, "context": "Thus, the complexity of building the tree is not exponential, contrary to common understanding; and the practical performance of kd-tree can be very efficient and better than that of locality sensitive hashing [1, 14].", "startOffset": 210, "endOffset": 217}, {"referenceID": 17, "context": "Also the cosine similarity for two data points with incomplete information is defined by using the co-rated items only (Algorithm 3, line 8-9) [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "\u2022 LCR: Local collaborative ranking [9], as implemented in the PREA toolkit4.", "startOffset": 35, "endOffset": 38}, {"referenceID": 16, "context": "\u2022 AltSVM: Alternating support vector machine [17], as implemented in the collranking package5.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "3 Evaluation Measure We evaluate the ranking quality by normalized discounted cumulative gain (NDCG) @K [6], averaged over all the users.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "In contrast to previous work on top-N recommender systems [8, 5], we discourage the use of Precision@K in the context of collaborative ranking for recommender systems, which measures the proportion of actually rated items out of the top K items in the ranked list of all the items in the data set.", "startOffset": 58, "endOffset": 64}, {"referenceID": 4, "context": "In contrast to previous work on top-N recommender systems [8, 5], we discourage the use of Precision@K in the context of collaborative ranking for recommender systems, which measures the proportion of actually rated items out of the top K items in the ranked list of all the items in the data set.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "Both user-based and item-based collaborative filtering [4, 18] can be considered as graph-based local label propagation methods.", "startOffset": 55, "endOffset": 62}, {"referenceID": 17, "context": "Both user-based and item-based collaborative filtering [4, 18] can be considered as graph-based local label propagation methods.", "startOffset": 55, "endOffset": 62}, {"referenceID": 23, "context": "The idea of discrete harmonic extension for semi-supervised learning in general was originally proposed in [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "Graph-based methods were recently considered for matrix completion [7] and top-N recommendation [23] as well.", "startOffset": 67, "endOffset": 70}, {"referenceID": 22, "context": "Graph-based methods were recently considered for matrix completion [7] and top-N recommendation [23] as well.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "An important direction is to further improve the efficiency of the algorithm to be comparable with recent large-scale matrix completion methods [22, 25].", "startOffset": 144, "endOffset": 152}, {"referenceID": 24, "context": "An important direction is to further improve the efficiency of the algorithm to be comparable with recent large-scale matrix completion methods [22, 25].", "startOffset": 144, "endOffset": 152}], "year": 2016, "abstractText": "We present a new perspective on graph-based methods for collaborative ranking for recommender systems. Unlike user-based or item-based methods that compute a weighted average of ratings given by the nearest neighbors, or low-rank approximation methods using convex optimization and the nuclear norm, we formulate matrix completion as a series of semi-supervised learning problems, and propagate the known ratings to the missing ones on the user-user or item-item graph globally. The semi-supervised learning problems are expressed as LaplaceBeltrami equations on a manifold, or namely, harmonic extension, and can be discretized by a point integral method. We show that our approach does not impose a low-rank Euclidean subspace on the data points, but instead minimizes the dimension of the underlying manifold. Our method, named LDM (low dimensional manifold), turns out to be particularly effective in generating rankings of items, showing decent computational efficiency and robust ranking quality compared to state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}