{"id": "1602.06064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2016", "title": "On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation", "abstract": "we choose to train bi - directional neural network language model ( nnlm ) with noise contrastive estimation ( nce ). experiments are also involving a rescore task on the ptb data set. it is shown that nce - trained mixed - directional users outperformed the one tested by conventional maximum likelihood training. but unfortunately ( 2014 ), shannon did not out - perform the baseline uni - directional nnlm.", "histories": [["v1", "Fri, 19 Feb 2016 07:27:49 GMT  (453kb,D)", "http://arxiv.org/abs/1602.06064v1", null], ["v2", "Mon, 22 Feb 2016 02:51:34 GMT  (458kb,D)", "http://arxiv.org/abs/1602.06064v2", null], ["v3", "Thu, 25 Feb 2016 02:00:23 GMT  (458kb,D)", "http://arxiv.org/abs/1602.06064v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tianxing he", "yu zhang", "jasha droppo", "kai yu"], "accepted": false, "id": "1602.06064"}, "pdf": {"name": "1602.06064.pdf", "metadata": {"source": "CRF", "title": "On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation", "authors": ["Tianxing He", "Yu Zhang"], "emails": ["cloudygoose@sjtu.edu.cn", "yzhang87@csail.mit.edu", "jdroppo@microsoft.com", "kai.yu@sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Recent years have witnessed exciting performance improvements in the field of language modeling, largely due to introduction of a series of neural network language models(NNLM). Although the conventional back-off n-gram language model has been widely used in the automatic speech recognition (ASR) or machine translation(MT) community for its simplicity and effectiveness, it has long suffered from the curse-of-dimensionality problem caused by huge number of possible word combinations in real-world text. Various smoothing techniques[1] are proposed to address this issue but the improvements have been limited. Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].In neural network based language models, the word context is projected into a continuous space and the projection, represented by the transformation matrices in the neural network, are learned during training. The projected continuous word vectors are also referred to as word embeddings. With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.\nDespite the benefits of effective context representation brought by word embeddings, FNNLM is still a short-span language model and not capable of utilizing long-term (e.g. context that is 5 or 6 words away) word history for the target word prediction. To address this issue, recurrent neural network language model (RNNLM), which introduces a recurrent connection in the hidden layer, is proposed to preserve long-term context. It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.\nHowever, RNN training generally suffered from the \u201cvanishing gradient\u201d problem[13]:the gradient flow will decay sharply through a non-linear operation. The LSTM[14] structure alleviates this problem by introducing a \u201cmemory cell\u201d structure which allows the gradient to travel without being squashed by a non-linear operation. Also, it has a set of gates which enable the model to decide whether to memorize, forget, or output information. By introducing the LSTM strucutre\nar X\niv :1\n60 2.\n06 06\n4v 1\n[ cs\n.C L\n] 1\ninto RNNLM[9], LSTMLM is able to remember longer context information and gains more performance gain. It has also been shown that the dropout [15] can be used to regularize the LSTMLM. Inspired by its success, several variants of LSTM have been proposed, recently the gated recurrent unit(GRU)[16] is gaining increasing popularity becuase it has matching performance with LSTM but has simpler structure. More recently, [17] has proposed to introduce the concept of memory into NNLM. By fetching memories from previous time, the model is able to \u201cexplicitly\u201d utilizing long-term dependency without recurrence structure.\nWhile these research efforts have been focusing on better utilization of history information, it would be desirable if the model can utilize context information from both sides. In literature, very few attempts have been made to train a proper bi-directional neural network language model, even though bi-drectional NN has already been successfully applied to other fields[18]. This is because the bidirectional model won\u2019t be by itself normalized because of the generative nature of language model, which makes the conventional maximum likelihood training framework improper for its training.\nIn this work, attempts have been made to train a bi-directional neural network language model with noise contrastive estimation, an alternative to maximum likelihood training which does not have the constrain that the model to be trained is inherently normalized. The rest of the paper is organized as follows: in section 2, the motivation of this work is discussed, in section 3 the formulation of the model are elaborated in detail, implementation is covered in section 4, finally experiment results are shown in section 5 and related works are discussed in section 6."}, {"heading": "2 Motivation", "text": "Statistical language models assign a probability P (W) to a given sentenceW =< w1, w2, ..., wn >, which can be decomposed into a product of word-level probabilities using the rule of conditional probability:\nP (W) = \u03a0iP (wi|w1..i\u22121) (1)\nLanguage models by this formulation predict the probability distribution of the next word given its former words(history). Since the prediction only depends on history information, in this work, this kind of model is denoted as uni-directional language model. All types of language model mentioned in section 1 fall into this category, but note that shot-span models like N-gram makes the \u201dMarkov Chain\u201d assumption P (wi|w1..i\u22121) \u2248 P (wi|wi\u2212N..i\u22121) to alleviate the data-sparsity problem. For uni-directional language models, as long as each word-level probability is properly normalized, normalization is also guaranteed on sentence level:\u2211\nW PLM (W) = 1 (2)\nThis is the key reason why the \u201dmaximum likelihood estimation\u201d training framework, which requires the model to be inherently probabilistic, has been successfully applied to the parameter estimation(training) of uni-directional language models. And recent years of research effort in the field of neural network language model has been focused on getting a better representation of history context using sophisticated recurrent neural network structures like LSTM[9].\nUnfortunately, while recently bi-directional neural network like BI-RNN or BI-LSTM has been successfully applied to many tasks, it is not trivial to apply this powerful model to language modeling, the main challenge is that the bi-directional information will break the sentence-level normalization1, making the model no longer valid for the MLE training framework(please refer to section 3 for more details ).\nIn this work, noise contrastive estimation(NCE)[19] is used to train a bi-directional neural network based LM, one big advantage of NCE over MLE is that it doesn\u2019t require the model to be self-normalized. This enables the utilization of bi-directional information for word-level scoring. Formulations of this work will be elaborated in the next section.\n1If some bi-directional model like P (wi|w1..i\u22121,i+1..N ) is used as the word-level LM, equation 1, and hence equation 2 won\u2019t hold any more."}, {"heading": "3 Formulation", "text": ""}, {"heading": "3.1 Model Formulation", "text": "In this work, P (W) is the product of word-level scores(similar to uni-directional LM) and a learned normalization scalar c, required by the NCE framework to ensure normalization:\nf \u2032(W) = \u03a0ifi(W) PNCE(W) = f \u2032(W)exp(c)\n(3)\nwhere fi the scoring given by a bi-directional neural network on each word index. And the \u201dNCE\u201d superscript for PNCE(W) is for indicating the normalization is induced by NCE training. In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):\nvi = Wxhxi \u2212\u2192 h 1i = g( \u2212\u2192 h 1i\u22121, vi) \u2190\u2212 h 1i = g( \u2190\u2212 h 1i+1, vi)\nh1i = tanh(W 1 hf \u2212\u2192 h 1i + W 1 hr \u2190\u2212 h 1i + b 1)\nui = exp(Whoh1i + bo)\n(4)\nwhere W\u2217\u2217 and b\u2217 are the transformation matrices and bias vector parameters in the neural network, and xi is the one-hot representation of wi. Finally, fi(W) is obtained after a normalizing operation over the vocabulary(denoted as V) on ui:\nfi(W) = ui(wi)\u2211\nwj\u2208V ui(wj) (5)\nNote that the word-level normalization is not needed in this work(ui(wi) can be used directly as fi(W)), but experiments show that reserving the word-level normalization will give better results. In this work, gated recurrent unit is used as the recurrent structure ht = g(ht\u22121, vt) because it is faster, causes less memory and has matching performance with the LSTM structure[16]. So our NN model is denoted BI-GRULM and the formulation is put below:\nzt = \u03c3(Whzht\u22121 + Wxzvt + bz) rt = \u03c3(Whrht\u22121 + Wxrvt + br)\nh\u0303t = tanh(Wh1(rt \u2217 ht\u22121) + Wh2vt + bh) ht = (1\u2212 zt) \u2217 ht\u22121 + zt \u2217 h\u0303t\n(6)\nwhere \u03c3 is the sigmoid function \u03c3(x) = 11+e\u2212x and \u2217 is element-wise multiplication, and note that a different set of parameter is used for forward and backward connections in the bi-directional neural network.\nFinally, we also write down the formulations of a one-layer uni-directional GRULM(UNI-GRULM) here since it will be used as baseline model:\nh1i =g(h 1 i\u22121,Wxhxi) ui =exp(Whoh1i + bo) (7)\nNote that other than being uni-directional, the only other difference between these two models is the normalization scalar c. And in this work, the dropout operation is applied on h1i for both models."}, {"heading": "3.2 Training of bi-directional NNLM", "text": "As stressed in section 2, the MLE framework is not suitable for training bi-directional NNLM, still, in this work MLE training is tried as a baseline experiment. Denoting the the data distribution as Pdata(W), the MLE objective function is formulated as below:\nJMLE(\u03b8) = EPdata(W)[logf \u2032 \u03b8(W)] (8)\nNote that here the normalization scalar c does not exist in the model.\nIn this work, noise contrastive estimation[19] is applied to train the bi-directional NNLM. NCE introduces a noise distribution Pnoise(W) into training and a \u201dto-be-learned\u201d normalization scalar c into the model, and its basic idea is that instead of maximizing the likelihood of the data samples, the model is asked to discriminative samples from the data distribution against samples from the noise distribution:\nJNCE(\u03b8) = EPdata(W)[logP (D =1|W; \u03b8)] + kEPnoise(W)[logP (D = 0|W; \u03b8)]\nP (D = 1|W; \u03b8) = P NCE \u03b8 (W)\nPNCE\u03b8 (W) + kPnoise(W)\nP (D = 0|W; \u03b8) = kPnoise(W) PNCE\u03b8 (W) + kPnoise(W)\n(9)\nassuming a noise ratio of k.\nAnd the gradients are:\n\u2202logP (D = 1|W; \u03b8) \u2202\u03b8 = kPnoise(W) PNCE\u03b8 (W) + kPnoise(W) \u2202logPNCE\u03b8 (W) \u2202\u03b8\n\u2202logP (D = 0|W; \u03b8) \u2202\u03b8 = \u2212PNCE\u03b8 (W) PNCE\u03b8 (W) + kPnoise(W) \u2202logPNCE\u03b8 (W) \u2202\u03b8\n(10)\nFor NCE to get good performance, a noise distribution that is close to the real data distribution is preferred, so in our case it is natural to use a good uni-directional LM as the noise distribution. In this work, N-gram LM is used as the noise distribution since it is efficient to sample from. Details about implementation and training process will be covered in section 4."}, {"heading": "4 Training and Implementation details", "text": "Mini-batch based stochastic gradient descent(SGD) is used to train bi-directional NNLM in this work. The training process is very similar to [12], but several changes need to be made for the sentence-level bi-directional NNLM training. Since NN training in this work is sentence-level, data(consisted of real data samples and noise model samples) are processed in chunks, illustrated in figure 2. Moreover, a batch of data streams is processed together to utilize the computing power of GPU. It is relatively easy to realize this training process of BI-RNN with the help of neural network training tool-kits like CNTK[22]. In this work, the chunk size is set to 90(which is larger than the longest sentence in the ptb data-set) and the batch size is set to 64.\nAn validation-based learning strategy is used, the learning rate is fixed to a large value at first, and start halving at a rate of 0.6 when no significant improvement on the validation data is observed. And the training is stopped when that happens again. Further, a L2 regularization with coefficient 1e-5 is used.\nFinally, the SRILM[23] Toolkit is used for N-gram LM training in this work. In our training the N-GRAM noise is generated on-the-fly so noise samples won\u2019t be the same between iterations."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Datasets", "text": "In this section, results of experiments designed to test the performance of the proposed bi-directional NNLM trained by NCE. Since the training process is very time-costly when the noise ratio k is large(in our training framework, it will cost at least k times the time for training the baseline UNIGRULM model), we confined our experiments to the Penn Treebank portion(PTB) of the WSJ corpus, which is publicly available and has been used extensively in LM community. There are 930k tokens, 74k tokens, 82k tokens for training, validation and testing, respectively, and the vocabulary size is 10k.\nFurther, since there is no guarantee that the trained model will be properly normalized, the evaluation of perplexity(PPL), which is the most conventional evaluation for LM, can no longer be applied. Instead, we need to resort to some discriminative task in which the LM is asked to tell \u201dgood\u201d sentence from \u201dbad\u201d sentences, like its application in decoding or rescoring in systems like speech recognition or machine translation. But still, we want the training corpus and vocabulary size to be small enough, which will enable us to try a large noise ratio k, since sentence-level sampling is considered in this work, it is expected that k needs to be large enough for the training to work.\nIn light of the above concerns, a rescoring task is created directly on the PTB dataset, denoted as ptbrescore2. In this test, random small errors are introduced to each sentence of the original test corpus of the PTB dataset, and the LM is then asked to recognize the original sentence from the tampered ones by assigning it the highest score. In this work, three types of error, namely substitution, deletion and insertion, are generated. For each error type, 9 decoys(one decoy only has one error) are generated for each test sentence, constituting three test sets. So a uniform guess will have an accuracy of 10%. Further, a mixed set where each decoy can be of any of the three types of error is also added, denoted as test sdi. Some examples are shown in table 1. Note that in this test set all random number(for the position or new word index) are drawn from a uniform distribution, and the s-test set is similar to the MSR sentence completion task [24]."}, {"heading": "5.2 Pseudo-PPL Test", "text": "Although perplexity can not be used to evaluate bi-directional NNLM, it is still interesting what PPL the trained model will assign to the test sentences. Besides the original test set for the PTB data, two additional text are generated, one is sentences sampled from the 4-GRAM baseline model(denoted as 4gram-text), the other one is sentences sampled from a completely uniform distribution(denoted\n2This test set and the scripts for reproducing the N-gram baseline are available at https://cloudygoose@bitbucket.org/cloudygoose/ptb-robust-datagen.git\nas uniform-text). All three sets have around 4,000 sentences. A well-behaved LM is expected to assign lowest PPL to the first set, relatively low PPL to the second, and very high(bad) PPL to the last one. The results are shown in table 2.\nIt is shown that the BI-GRULM(detailed configuration will be discussed in section 5.3) trained with NCE has similiar behavior to the baseline uni-directional model, meaning that NCE is helping the model with sentence-level normalization. On the contrary, BI-GRULM trained with MLE is assigning extremely low PPL to every test set, indicating that the model is not properly normalized. But surprisingly, the relative order of PPL from MLE-trained BI-GRULM is correct."}, {"heading": "5.3 Evaluation on the ptb-rescore task", "text": "In this section accuracy results on the ptb-rescore task is presented. Three models are trained to be baseline models: 4-GRAM, UNI-GRULM, and BI-GRULM trained by MLE. Note that unless otherwise mentioned, all GRULMs trianed in the work has 300 neurons on hidden layer and only one layer(in the BI-GRULM case, one layer means one forward layer and one backward layer) is used. This setting is chosen for the reason that adding more neurons or more layers give no significant on the test PPL for the baseline UNI-GRULM model. Through training, a dropout rate of 50% is applied for the UNI-GRULM, but no dropout is applied for the reported experiments for the BI-GRULM because it is found that dropout won\u2019t give performance gain in that case.\nThe baseline results are shown in the upper part of table 3. Overally, the UNI-GRULM model gives the best performance, as expected. An interesting observation is that all model have extremely poor performance on the test-d set. This behavior, however, is not so surprising since the LM score of a sentence is afterall a product of word-level probabilities, so decoys with one less word will have big advantage. It is found that this problem can be alleviated by a length-norm trick:\nscorelength\u2212norm(W) = score(W) l = logf(W) l =\n\u2211l i logfi(W)\nl (11)\nassuming sentenceW is of length l(including the sentence-end token). Note that this trick is equivalent to ranking the sentences using PPL instead of sentence-level log likelihood and it will do harm to the performance on the test-i set, although not large.\nResults of BI-GRULM trained by NCE are shown in in the lower part of table 3, it is observed that the length-norm trick can also help in this case, and the overall performance is improving with larger and larger noise ratio, however, it became unaffordable for us to run experiments with ratio larger than 100. One strange observation is that performance on the test-d set degrades with larger noise ratio, and this causes performance on the test-sdi to become worse. Also, comparing with the BI-GRULM(MLE) result, BI-GRULMs trained by NCE with a large noise ratio have overally better performance, indicating that NCE has the potential to utilize to power of BI-GRULM structure more properly.\nUnfortunately, the proposed model failed to out-perform the best UNI-GRULM baseline model on every test set. Results on the test-s set show that improvement can only be obtained by growing the noise ratio exponentially, this matches our concern in section 5.1, the sentence-level sampling space may be too sparse for our sampling to properly cover."}, {"heading": "6 Related work", "text": "In [20], bi-directional LSTMLM is trained with MLE and tested by LM rescoring in an ASR task. However, no improvement is observed over the uni-directional baseline model. On the other hand, NCE has been used in uni-directional LM training both for FNNLM[25] and RNNLM[26], the main goal was to speed-up the training and evaluation of these two models because under NCE training the final softmax operation on the output layer is no longer necessary. Note that different from these two work, NCE is applied on the sentence level in this work."}, {"heading": "7 Conclusion", "text": "In this work noise contrastive estimation is used to train a bi-directional neural network language model. Experiments are conducted on a rescore task on the PTB data set. It is shown that NCEtrained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still, it did not out-perform the baseline uni-directional NNLM. The key reason maybe that the sentence-level sampling space is too sparse for our sampling to cover."}], "references": [{"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "Proc. ACL. 1996, pp. 310\u2013318, Association for Computational Linguistics.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal OF Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech Language, vol. 21, no. 3, pp. 492\u2013518, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "AISTATS, 2005, pp. 246\u2013252.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Improved neural network based language modelling andadaptation", "author": ["J. Park", "X. Liu", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. InterSpeech, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "Proc. ICML, 2007, pp. 641\u2013648. 7", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "Proc. InterSpeech, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparison of feedforward and recurrent neural network language models", "author": ["Martin Sundermeyer", "Ilya Oparin", "Ben Freiberg", "Ralf Schlter", "Hermann Ney"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schluter", "Hermann Ney"], "venue": "Proc. InterSpeech, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Cache based recurrent neural network language model inference for first pass speech recognition", "author": ["Zhiheng Huang", "Geoffrey Zweig", "Benoit Dumoulin"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient lattice rescoring using recurrent neural network language models", "author": ["X. Liu", "Y. Wang", "X. Chen", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient gpu-based training of recurrent neural network language models using spliced sentence bunch", "author": ["X. Chen", "Y. Wang", "X. Liu", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. InterSpeech, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jrgen Schmidhuber"], "venue": "2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, vol. abs/1412.3555, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Feedforward sequential memory neural networks without recurrent feedback", "author": ["Shiliang Zhang", "Hui Jiang", "Si Wei", "Li-Rong Dai"], "venue": "CoRR, vol. abs/1510.02693, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, 2014, pp. 1764\u20131772.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael U. Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res., vol. 13, no. 1, pp. 307\u2013361, Feb. 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Bidirectional recurrent neural network language models for automatic speech recognition", "author": ["Ebru Arisoy1", "Abhinav Sethy", "Bhuvana Ramabhadran", "Stanley Chen"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional recurrent neural networks as generative models - reconstructing gaps in time series", "author": ["Mathias Berglund", "Tapani Raiko", "Mikko Honkala", "Leo K\u00e4rkk\u00e4inen", "Akos Vetek", "Juha Karhunen"], "venue": "CoRR, vol. abs/1504.01575, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Proceedings International Conference on Spoken Language Processing, November 2002, pp. 257\u2013286.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "Proceedings of the 29th International Conference on Machine Learning, 2012, pp. 1751\u20131758.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["M.J.F. Gales X. Chen", "X. Liu", "P.C.Woodland"], "venue": "Proc. ICASSP, 2015. 8", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Various smoothing techniques[1] are proposed to address this issue but the improvements have been limited.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 2, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 3, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 4, "context": "Recently, neural network based language models have attracted great interest due to its effective encoding of word context history [2, 3, 4, 5].", "startOffset": 131, "endOffset": 143}, {"referenceID": 1, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 2, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 3, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 4, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 5, "context": "With the continuous context representation, feed-forward neural network language models (FNNLM)[2, 3, 4, 5, 6], have achieved both better perplexity(PPL) and better word error rate (WER) when embedded into a realworld system.", "startOffset": 95, "endOffset": 110}, {"referenceID": 6, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 7, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 8, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 9, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 10, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 11, "context": "It has achieved significant performance gain on perplexity and word error rate (WER) performance on various data sets [7, 8, 9, 10, 11, 12], out-performing traditional back-off n-gram models and FNNLMs.", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "However, RNN training generally suffered from the \u201cvanishing gradient\u201d problem[13]:the gradient flow will decay sharply through a non-linear operation.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "The LSTM[14] structure alleviates this problem by introducing a \u201cmemory cell\u201d structure which allows the gradient to travel without being squashed by a non-linear operation.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "into RNNLM[9], LSTMLM is able to remember longer context information and gains more performance gain.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "It has also been shown that the dropout [15] can be used to regularize the LSTMLM.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Inspired by its success, several variants of LSTM have been proposed, recently the gated recurrent unit(GRU)[16] is gaining increasing popularity becuase it has matching performance with LSTM but has simpler structure.", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "More recently, [17] has proposed to introduce the concept of memory into NNLM.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "In literature, very few attempts have been made to train a proper bi-directional neural network language model, even though bi-drectional NN has already been successfully applied to other fields[18].", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "And recent years of research effort in the field of neural network language model has been focused on getting a better representation of history context using sophisticated recurrent neural network structures like LSTM[9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 18, "context": "In this work, noise contrastive estimation(NCE)[19] is used to train a bi-directional neural network based LM, one big advantage of NCE over MLE is that it doesn\u2019t require the model to be self-normalized.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):", "startOffset": 85, "endOffset": 93}, {"referenceID": 19, "context": "In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "In this work, the same bi-directional neural network structure that has been used in [18, 20] is applied, and is shown in figure 1 and formulated below(we are aware that other variants of BI-RNN exist[21], but they are not fundamentally different with regard to this work):", "startOffset": 200, "endOffset": 204}, {"referenceID": 15, "context": "In this work, gated recurrent unit is used as the recurrent structure ht = g(ht\u22121, vt) because it is faster, causes less memory and has matching performance with the LSTM structure[16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 18, "context": "In this work, noise contrastive estimation[19] is applied to train the bi-directional NNLM.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "The training process is very similar to [12], but several changes need to be made for the sentence-level bi-directional NNLM training.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "Finally, the SRILM[23] Toolkit is used for N-gram LM training in this work.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "In [20], bi-directional LSTMLM is trained with MLE and tested by LM rescoring in an ASR task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "On the other hand, NCE has been used in uni-directional LM training both for FNNLM[25] and RNNLM[26], the main goal was to speed-up the training and evaluation of these two models because under NCE training the final softmax operation on the output layer is no longer necessary.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "On the other hand, NCE has been used in uni-directional LM training both for FNNLM[25] and RNNLM[26], the main goal was to speed-up the training and evaluation of these two models because under NCE training the final softmax operation on the output layer is no longer necessary.", "startOffset": 96, "endOffset": 100}], "year": 2017, "abstractText": "We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE). Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still(regretfully), it did not out-perform the baseline uni-directional NNLM.", "creator": "LaTeX with hyperref package"}}}