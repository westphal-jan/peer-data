{"id": "1602.06586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Recovering Structured Probability Matrices", "abstract": "we consider the problem of accurately recovering a clade b of size m by m, which represents a probability distribution over m ^ 2 outcomes, given access to an observed matrix called \" counts \" generated by taking independent judgments from the distribution b. how are structural properties of each underlying matrix b be leveraged to yield computationally efficient and information theoretically desirable reconstruction algorithms? when can accurate reconstruction manage accomplished in the sparse data regime? this basic problem lies at the core of a selection of questions that remain currently being mined throughout different communities, including community detection in sparse random graphs, learning structured models such even topic models or hidden markov models, encouraging the efforts from the evolutionary language processing community to compute \" word embeddings \".", "histories": [["v1", "Sun, 21 Feb 2016 21:47:45 GMT  (542kb,D)", "http://arxiv.org/abs/1602.06586v1", null], ["v2", "Mon, 29 Feb 2016 22:20:50 GMT  (542kb,D)", "http://arxiv.org/abs/1602.06586v2", null], ["v3", "Sat, 9 Apr 2016 06:59:37 GMT  (518kb,D)", "http://arxiv.org/abs/1602.06586v3", null], ["v4", "Fri, 15 Apr 2016 04:58:58 GMT  (518kb,D)", "http://arxiv.org/abs/1602.06586v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qingqing huang", "sham m kakade", "weihao kong", "gregory valiant"], "accepted": false, "id": "1602.06586"}, "pdf": {"name": "1602.06586.pdf", "metadata": {"source": "CRF", "title": "Recovering Structured Probability Matrices", "authors": ["Qingqing Huang", "Sham M. Kakade", "Weihao Kong", "Gregory Valiant"], "emails": ["qqh@mit.edu.", "sham@cs.washington.edu", "kweihao@gmail.com", "valiant@stanford.edu."], "sections": [{"heading": null, "text": "Our results apply to the setting where B has a particular rank 2 structure. For this setting, we propose an efficient (and practically viable) algorithm that accurately recovers the underlying M \u00d7 M matrix using \u0398(M) samples. This result easily translates to \u0398(M) sample algorithms for learning topic models with two topics over dictionaries of size M , and learning hidden Markov Models with two hidden states and observation distributions supported on M elements. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires \u2126(M) samples. Furthermore, we provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires \u2126(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs. This impossibility of sublinear-sample property testing in these settings is intriguing and underscores the significant differences between these structured settings and the standard setting of drawing i.i.d samples from an unstructured distribution of support size M .\n\u2217MIT. Email: qqh@mit.edu. \u2020University of Washington. Email: sham@cs.washington.edu \u2021Stanford University. Email: kweihao@gmail.com \u00a7Stanford University. Email: valiant@stanford.edu. Gregory and Weihao\u2019s contributions were supported by NSF\nCAREER Award CCF-1351108, and a research grant from the Okawa Foundation.\nar X\niv :1\n60 2.\n06 58\n6v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n16"}, {"heading": "1 Introduction", "text": "Consider an unknown M \u00d7 M matrix of probabilities B, satisfying \u2211i,j Bi,j = 1. Suppose one is given N independently drawn (i, j)-pairs, sampled according to the distribution defined by B. How many draws are necessary to accurately recover B? What can one infer about the underlying matrix based on these samples? How can one accurately test whether the underlying matrix possesses certain properties of interest? How do structural assumptions on B \u2014 for example, the assumption that B has low rank \u2014 affect the information theoretic or computational complexity of these questions? For the majority of these tasks, we currently lack both a basic understanding of the computational and information theoretic lay of the land, as well as algorithms that seem capable of achieving the information theoretic or computational limits.\nThis general question of making accurate inferences about a matrix of probabilities, given a matrix of observed \u201ccounts\u201d of discrete outcomes, lies at the core of a number of problems that disparate communities have been tackling independently. On the theoretical side, these problems include both work on community detection in stochastic block models (where the goal is to infer the community memberships from an adjacency matrix of a graph that has been drawn according to an underlying matrix of probabilities expressing the community structure) as well as the line of work on recovering topic models, hidden Markov models (HMMs), and richer structured probabilistic models (where the model parameters can often be recovered using observed count data). On the practical side, these problems include the recent work in the natural language processing community to infer structure from matrices of word co-occurrence counts for the purpose of constructing good \u201cword embeddings\u201d, as well as latent semantic analysis and non-negative matrix factorization.\nIn this work, we start this line of inquiry by focusing on the estimation problem where the probability matrix B possesses a particular rank 2 structure. While this estimation problem is rather specific, it generalizes the basic community detection problem and also encompasses the underlying problem behind learning 2-state HMMs and learning 2-topic models. Furthermore, this rank 2 case also provides a means to study how property testing and estimation problems are different in this structured setting, as opposed to the simpler rank 1 setting that is equivalent to the standard setting of independent draws from a distribution supported on M elements.\nWe focus on the estimation of B in the sparse data regime, near the information theoretic limit. The motivation for this is that in many practical scenarios involving count samples, we seek algorithms capable of extracting the underlying structure in the sparsely sampled regime. To give two examples, consider forming the matrix of word co-occurrences\u2014the matrix whose rows and columns are indexed by the set of words, and whose (i, j)th element consists of the number of times the ith word follows the jth word in a large corpus of text. One could also consider a matrix whose rows are indexed by customers, and whose columns are indexed by products, with the (i, j)th entry corresponding to the number of times the ith customer has purchased the jth product. In both settings, the structure of the probability matrix underlying these observed counts contains insights into the two domains, and in both domains we only have relatively sparse data. This is inherent in many other natural scenarios involving heavy-tailed distributions where, regardless of how much data one collects, a significant fraction of items (e.g. words, genetic mutations) will only be observed a few times.\nSuch estimation questions have been actively studied in the community detection literature, where the objective is to accurately recover the communities in the regime where the average degree (e.g. the row sums of the adjacency matrix) are constant. In contrast, the recent line of works for recovering highly structured models (such as large topic models, HMMs, etc.) are only applicable to the oversampled regime (where the amount of data is well beyond the information theoretic limits), where achieving the information theoretic limits remains a widely open question. This work begins to bridge the divide between these recent algorithmic advances in both communities. We hope that the rank-2 probability matrix setting that studied here serves as a jumping-off point for the more general questions of developing information theoretically optimal algorithms for estimating higher-rank matrices and\ntensors in general, or recovering low-rank approximations to arbitrary probability matrices, in the sparse data regime. While these settings may be more challenging, we believe that some of our algorithmic techniques can be fruitfully extended.\nIn addition to developing algorithmic tools which we hope are applicable to a wider class of problems, a second motivation for considering this particular rank 2 case is that, with respect to distribution learning and property testing, the entire lay-of-the-land seems to change completely when the probability matrix B has rank larger than 1. In the rank 1 setting \u2014 where a sample consists of 2 independent draws from a distribution supported on {1, . . . ,M} \u2014 the distribution can be learned using \u0398(M) draws. Nevertheless, many properties of interest can be tested or estimated using a sample size that is sublinear in M1. However, in the rank 2 setting, even though the underlying matrix B can be represented with O(M) parameters (and, as we show, it can also be accurately and efficiently recovered with O(M) sample counts), sublinear sample property testing and estimation is generally impossible. This result begs the more general question: what conditions must be true of a structured statistical setting in order for property testing to be easier than learning?"}, {"heading": "1.1 Problem Formulation", "text": "Assume our vocabulary is the index set M = {1, . . . ,M} of M words and that there is an underlying low rank probability matrix B, of size M \u00d7M , with the following structure:\nB = DWD>, where D = [ p, q ] . (1)\nHere, D = RM\u00d72+ is the dictionary matrix parameterized by two M -dimensional probability vectors p, q, supported on the standard (M \u2212 1)-simplex. Also, W is the 2 \u00d7 2 mixing matrix, which is a probability matrix satisfying W \u2208 R2\u00d72+ , \u2211 i,jWi,j = 1.\nDenote wp = W1,1 +W1,2 and wq = W2,1 +W2,2. Note that \u2211\nk Bi,k = wpp+ wqq. Define the covariance matrix of any probability matrix P as:\n[Cov(P )]i,j := Pi,j \u2212 ( \u2211\nk\nPi,k)( \u2211\nk\nPk,j).\nNote that Cov(P )~1 = ~0 and ~1>Cov(P ) = ~0 (where ~1 and ~0 are the all ones and zeros vectors, respectively). This implies that, without loss of generality, the covariance of the mixing matrix, Cov(W ), can be expressed as:\nCov(W ) = [wL,\u2212wL]>[wR,\u2212wR].\nfor some real numbers wL, wR \u2208 [\u22121, 1]. For ease of exposition, we restrict to the symmetric case where wL = wR = w, though our results hold more generally.\nSuppose we obtain N , i.i.d. sample counts from B of the form {(i1, j1) (i2, j2), . . . (iN , jN )}, where each sample (in, jn) \u2208 M \u00d7 M. The probability of obtaining a count (i, j) in a sample is Bi,j . Moreover, assume that the number of samples follows a Poisson distribution: N \u223c Poi(N). The Poisson assumption on the number of samples is made only for the convenience of analysis: if N follows a Poisson distribution, the counts of observing (i, j) follows a Poisson distribution Poi(NBi,j) and is independent from the counts of observing (i\u2032, j\u2032) for (i\u2032, j\u2032) 6= (i, j). This assumption is made only for the convenience of analysis and is not crucial for the correctness of the algorithm. As M is asymptotically large, with high probability, N and N are within a subconstant factor of each other and both upper and lower bounds translate between the Poissonized setting, and the setting of exactly N samples. Throughout, we state our sample complexity results in terms of N rather than N.\n1Distinguishing whether a distribution is uniform versus far from uniform can be accomplished using only O( \u221a M) draws, testing whether two sets of samples were drawn from similar distributions can be done with O(M2/3) draws, estimating the entropy of the distribution to within an additive can be done with O( M\nlogM ) draws, etc.\nNotation Throughout the paper, we use the following standard shorthand notations. Denote [n] , {1, . . . , n}. Let I denote a subset of indices in M. For a M -dimensional vector x, we use vector xI to denote the elements of x restricting to the indices in I; for two index sets I, J , and a M \u00d7M dimensional matrix X, we use XI\u00d7J denote the submatrix of X with rows restricting to indices in I and columns restricting to indices in J .\nWe use Poi(\u03bb) to denote a Poisson distribution with rate \u03bb; we use Ber(\u03bb) to denote a Bernoulli random variable with success probability \u03bb; and we use Mul(x;\u03bb) to denote a multinomial distribution over M outcomes with \u03bb number of trials and event probability vector x \u2208 RM+ such that \u2211 i xi = 1."}, {"heading": "1.2 Main Results", "text": ""}, {"heading": "1.2.1 Recovering Rank-2 Probability Matrices", "text": "Throughout, we focus on a class of well-separated model parameters. The separation assumptions guarantee that the rank 2 matrix B is well-conditioned. Furthermore, this assumption also has natural interpretations in each of the different applications (to that of community detection, topic modeling, and HMMs).\nAll of our order notations are with respect to the vocabulary size M , which is asymptotically large. Also, we say that a statement is true \u201cwith high probability\u201d if the failure probability of the statement is inverse poly in M ; and we say a statement is true \u201cwith large probability\u201d if the failure probability is of some small constant \u03b4, which can be easily boosted to very smaller probabilities with repetitions.\nAssumption 1 (\u2126(1) separation). Assume that W is symmetric, where wL = wR = w (all our results extend to the asymmetric case). Define the marginal probability vector, \u03c1 and the dictionary separation vector as:\n\u03c1i := \u2211\nk\nBi,k, \u2206 := w(p\u2212 q) . (2)\nAssume that wp and wq are lower bounded by some constant Cw = \u2126(1). Assume that the `1-norm of the dictionary separation is lower bounded by some constant C\u2206 = \u2126(1),\n\u2016\u2206\u20161 \u2265 C\u2206. (3)\nNote that while in general the dictionary matrix D and the mixing matrix W are not uniquely identifiable from B, there does exist an identifiable decomposition. Observe that the matrix Cov(B) admits a unique rank-1 decomposition: Cov(B) := B\u2212 \u03c1\u03c1> = \u2206\u2206>, which also implies that:\nB = \u03c1\u03c1> + \u2206\u2206>. (4)\nIt is this unique decomposition we seek to estimate, along with the matrix B. Now we are ready to state our main theorem.\nTheorem 1.1 (Main theorem). Suppose we have access to N i.i.d. samples generated according to the rank 2 probability matrix B with structure given by (1) and satisfying the separation Assumption 1. For > 0, with N = \u0398(M/ 2) samples, our algorithm runs in time poly(M) and returns estimators\nB\u0302, \u03c1\u0302, \u2206\u0302, such that with large probability:\n\u2016B\u0302 \u2212 B\u20161 \u2264 , \u2016\u03c1\u0302\u2212 \u03c1\u20161 \u2264 , \u2016\u2206\u0302\u2212\u2206\u20161 \u2264 .\n(here, the `1-norm of an M \u00d7M matrix P is simply defined as \u2016P\u20161 = \u2211 i,j |Pi,j |).\nFirst, note that we do not bound the spectral error \u2016B\u0302 \u2212 B\u20162 since when the marginal \u03c1 is not roughly uniform, error bounds in terms of spectral distance are not particularly strong. A natural error measure of estimation for probability distributions is total variation distance (equivalent to our `1 norm here). Second, note that naively estimating a distribution over M2 outcomes requires order M2 samples. Importantly, our algorithm utilizes the rank 2 structure of the underlying matrix B to achieve a sample complexity which is precisely linear in the vocabulary size M (without any additional log factors). The proof of the main theorem draws upon recent advances in the concentration of sparse random matrices from the community detection literature; the well characterized problem in the community detection literature can be viewed as a simple and special case of our problem, where the marginals are homogeneous (which we discuss later).\nWe now turn to the implications of this theorem to testing and learning problems."}, {"heading": "1.2.2 Topic Models and Hidden Markov Models", "text": "One of the main motivations for considering the specific rank 2 structure on the underlying matrix B is that this structure encompasses the structure of the matrix of expected bigrams generated by both 2-topic models and two state HMMs. We now make these connections explicit.\nDefinition 1.2. A 2-topic model over a vocabulary of size M is defined by a pair of distributions, p and q supported over M words, and a pair of topic mixing weights \u03c0p and \u03c0q = 1 \u2212 \u03c0p. The process of drawing a bigram (i, j) consists of first randomly picking one of the two \u201ctopics\u201d according to the mixing weights, and then drawing two independent words from the word distribution corresponding to the chosen topic. Thus the probability of seeing bigram (i, j) is (\u03c0ppipj + \u03c0qqiqj), and so the expected\nbigram matrix can be written as B = DWD> with D = [p, q], and W = [ \u03c0p 0 0 \u03c0q ] .\nThe following corollary shows that estimation is possible with sample size linear in M :\nCorollary 1.3. (Learning 2-topic models) Suppose we are in the 2-topic model setting. Assume that \u03c0p(1 \u2212 \u03c0p)\u2016p \u2212 q\u20161 = \u2126(1). There exists an algorithm which, given N = \u2126(M/ 2) bigrams, runs in time poly(M) and with large probability returns estimates \u03c0\u0302p, p\u0302, q\u0302 such that\n|\u03c0\u0302p \u2212 \u03c0p| < , \u2016p\u0302\u2212 p\u20161 \u2264 , \u2016q\u0302 \u2212 q\u20161 \u2264 .\nDefinition 1.4. A hidden Markov model with 2 hidden states (sp, sq) and a size M observation vocabulary is defined by a 2 \u00d7 2 transition matrix T for the 2 hidden states, and two distributions of observations, p and q, corresponding to the 2 states.\nA sequence of N observations is sampled as follows: First, select an initial state according to the stationary distribution of the underlying Markov chain [\u03c0p, \u03c0q]; Then evolve the Markov chain according to the transition matrix T for N steps; For each n \u2208 {1, . . . , N}, the n-th observation in the sequence is generated by making an independent draw from either distribution p or q according to whether the Markov chain is in state sp or sq at the n-th timestep.\nThe probability that seeing a bigram (i, j) for the n and the (n + 1)-th observation is given by \u03c0ppi(Tp,ppj + Tp,qqj) + \u03c0qqi(Tq,ppj + Tq,qqj), and hence the expected bigram matrix can be written as\nB = DWD> with D = [p, q], and W = [ \u03c0p 0 0 \u03c0q ] [ Tp,p 1\u2212 Tp,p 1\u2212 Tq,q Tq,q ] .\nWe have the following learning result:\nCorollary 1.5. (Learning 2-state HMMs) Suppose we are in the 2-state HMM setting. Assume that \u2016p\u2212q\u20161 \u2265 C1 and that \u03c0p, Tp,p, Tq,q are lower bounded by C2 and upper bounded by 1\u2212C2, where both C1 and C2 are \u2126(1). There exists an algorithm which, given a sampled chain of length N = \u2126(M/ 2),\nruns in time poly(M) and returns estimates \u03c0\u0302p, T\u0302 , p\u0302, q\u0302 such that, with high probability, we have (that there is exists a permutation of the model such that)\n|\u03c0\u0302p \u2212 \u03c0p| < , |T\u0302p,p \u2212 Tp,p| < , |T\u0302q,q \u2212 Tq,q| < , \u2016p\u0302\u2212 p\u20161 \u2264 , \u2016q\u0302 \u2212 q\u20161 \u2264 . Furthermore, it is sufficient for this algorithm to only utilize \u2126(M/ 2) random bigrams and only \u2126(1/ 2) random trigrams from this chain.\nIt is worth noting that the matrix of bigram probabilities does not uniquely determine the underlying HMM. However, one can recover the model parameters using sampled trigram sequences; this last step is straightforward (and sample efficient as it uses only an additional \u2126(1/ 2) trigrams) when given an accurate estimate of B (see [6] for the moment structure in the trigrams)."}, {"heading": "1.2.3 Testing vs. Learning", "text": "The above theorem and corollaries are tight in an extremely strong sense. Both for the topic model and HMM settings, while we can learn the models using \u2126(M) samples/observations, in both settings, it is information theoretically impossible to perform even the most basic property tests using fewer than \u0398(M) samples.\nIn the case of 2-topic models, the community detection lower bounds [41][32][52] imply that \u0398(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words. We prove a stronger lower bound for the case of HMMs with two states, where we permit an estimator to have more information, namely the full sequence of observations (not merely bigram and trigram counts). Perhaps surprisingly, even with this extra information, we have the following lower bound:\nTheorem 1.6. Given a sequence of observations from a HMM with two states and emission distributions p, q supported on M elements, even if the underlying Markov process is symmetric, with transition probability 1/4, it is information theoretically impossible to distinguish the case that the two emission distributions, p = q = Uniform[M ] from the case that ||p \u2212 q||1 \u2265 1/2 using a sequence of fewer than \u0398(M) observations.\nThe proof of this theorem is given in Appendix 5.2, and amounts to a careful comparison of the processes of generating a uniformly random path in a graph, versus generating a path corresponding to a 2-state HMM for which there is significant correlation between consecutive observations. As an immediate corollary of this theorem, it follows that many natural properties of HMMs cannot be estimated using a sublinear length sequence of observations.\nCorollary 1.7. For HMMs with 2 states and emission distributions supported on a domain of size at most M , to estimate the entropy rate up to an additive constant c \u2264 1 requires a sequence of \u2126(M) observations.\nThese strong lower bounds for property testing and estimation of HMMs are striking for several reasons. First, the core of our learning algorithm is a matrix reconstruction step that uses only the set of bigram counts (though we do use trigram counts for the final parameter recovery). Conceivably, one could benefit significantly from considering longer sequences of observations \u2014 even in HMMs that mix in constant time, there are detectable correlations between observations separated by O(logM) steps. Regardless, our lower bound shows that this is not the case. No additional information from such longer k-grams can be leveraged to yield sublinear sample property testing or estimation.\nA second notable point is the brittleness of sublinear property testing and estimation as we deviate from the standard (unstructured) i.i.d sampling setting. In particular, while it may be natural to expect that testing and estimation would become rapidly more difficult as the number of hidden states of an HMM increase, we see here a (super-constant) increase in the difficulty of testing and estimation problems between the one state setting to the two state setting."}, {"heading": "1.3 Related Work", "text": "As mentioned earlier, the general problem of reconstructing an underlying matrix of probabilities given access to a count matrix drawn according to the corresponding distribution, lies at the core of questions that are being actively pursued by several different communities. We briefly describe these questions, and their relation to the present work. Community Detection. With the increasing prevalence of large scale social networks, there has been a flurry of activity from the algorithms and probability communities to both model structured random graphs, and understand how (and when it is possible) to examine a graph and infer the underlying structures that might have given rise to the observed graph. One of the most well studied community models is the stochastic block model [27]. In its most basic form, this model is parameterized by a number of individuals, M , and two probabilities, \u03b1, \u03b2. The model posits that the M individuals are divided into two equal-sized \u201ccommunities\u201d, and such a partition defines the following random graph model: for each pair of individuals in the same community, the edge between them is present with probability \u03b1 (independently of all other edges); for a pair of individuals in different communities, the edge between them is present with probability \u03b2 < \u03b1. Phrased in the notation of our setting, the adjacency matrix of the graph is generated by including each potential edge (i, j) independently, with probability Bi,j , with Bi,j = \u03b1 or \u03b2 according to whether i and j are in the same community. Note that B has rank 2 and is expressible in the form of Equation 1 as B = DWD> where D = [p, q] for vectors p = 2M I1 and q = 2 M I2 where I1 is the indicator vector for membership in the first community, and I2 is defined analogously, and W is the 2\u00d7 2 matrix with \u03b1M 2 4 on the diagonal and \u03b2 M2\n4 on the off-diagonal.\nWhat values of \u03b1, \u03b2, and M enable the community affiliations of all individuals to be accurately recovered with high probability? What values of \u03b1, \u03b2, and M allow for the graph to be distinguished from an Erdos-Renyi random graph (that has no community structure)? The crucial regime is where \u03b1, \u03b2 = O( 1M ), and hence each person has a constant, or logarithmic expected degree. The naive spectral approaches will fail in this regime, as there will likely be at least one node with degree \u2248 logM/ log logM , which will ruin the top eigenvector. Nevertheless, in the past four years a number of transformations of the adjacency matrix have been proposed, after which the spectral approach will enable constant factor optimal detection (the problem of distinguishing the community setting from G(n, p) and reconstruction in this constant degree regime (see e.g. [22, 40, 32, 33]). In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36]. More recently, there has been further research investigating more complex stochastic block models, consisting of three or more components, components of unequal sizes, etc. (see e.g. [19, 2]).\nWord Embeddings. On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9]. The main idea is to map every word w to a vector vw \u2208 Rd (typically d \u2248 500) in such a way that the geometry of the vectors captures the semantics of the word.2 One of the main constructions for such embeddings is to form the M \u00d7M matrix whose rows/columns are indexed by words, with i, jth entry corresponding to the total number of times the ith and jth word occur next to (or near) each other in a large corpus of text (e.g. wikipedia). The word embedding is then computed as the rows of the singular vectors corresponding to the top rank d approximation to this empirical count matrix.3 These embeddings have proved to be extremely effective, particularly when used as a way to map\n2The goal of word embeddings is not just to cluster similar words, but to have semantic notions encoded in the geometry of the points: the example usually given is that the direction representing the difference between the vectors corresponding to \u201cking\u201d and \u201cqueen\u201d should be similar to the difference between the vectors corresponding to \u201cman\u201d and \u201cwoman\u201d, or \u201cuncle\u201d and \u201caunt\u201d, etc.\n3A number of pre-processing steps have been considered, including taking the element-wise square roots of the entries, or logarithms of the entries, prior to computing the SVD.\ntext to features that can then be trained in downstream applications. Despite their successes, current embeddings seem to suffer from sampling noise in the count matrix (where many transformations of the count data are employed, e.g. see [45])\u2014this is especially noticeable in the relatively poor quality of the embeddings for relatively rare words. The recent theoretical work [10] sheds some light on why current approaches are so successful, yet the following question largely remains: Is there a more accurate way to recover the best rank-d approximation of the underlying matrix than simply computing the best rank-d approximation for the (noisy) matrix of empirical counts?\nEfficient Algorithms for Latent Variable Models. There is a growing body of work from the algorithmic side (as opposed to information theoretic) on how to recover the structure underlying various structured statistical settings. This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5]. A number of these methods essentially can be phrased as solving an inverse moments problem, and the work in [6] provides a unifying viewpoint for computationally efficient estimation for many of these models under a tensor decomposition perspective. In general, this body of work has focussed on the computational issues and has considered these questions in the regime in which the amount of data is plentiful\u2014well above the information theoretic limits.\nSublinear Sample Testing and Estimation. In contrast to the work described in the previous section on efforts to devise computationally efficient algorithms for tackling complex structural settings in the \u201cover\u2013sampled\u201d regime, there is also significant work establishing information theoretically optimal algorithms and (matching) lower bounds for estimation and distributional hypothesis testing in the most basic setting of independent samples drawn from (unstructured) distributions. This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc. While many of these results are optimal in a worst-case (\u201cminimax\u201d) sense, there has also been recent progress on instance optimal (or \u201ccompetitive\u201d) estimation and testing, e.g. [3, 4, 50], with stronger information theoretic optimality guarantees. There has also been significant work on these tasks in \u201csimply structured\u201d settings, e.g. where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21]."}, {"heading": "2 Outline of our estimation algorithm", "text": "In this section, we sketch the outline of our algorithm and explain the intuition behind the key ideas. Given N samples drawn according to the probability matrix B. Let BN denote the matrix of empirical counts, and let 1NBN denote the average. By the Poisson assumption on sample size, we have that [BN ]i,j \u223c Poi(NBi,j).\nFirst, note that it is straightforward to obtain an estimate \u03c1\u0302 which is close to the true marginal \u03c1 with accuracy in `1 norm with sample complexity N = \u2126(M). Also, recall that B \u2212 \u03c1\u03c1> = \u2206\u2206> as per (4), hence after subtracting off the (relatively accurate) rank 1 matrix of marginals, we are essentially left with a rank 1 matrix recovery problem. Our algorithm seeks to accurately perform this rank-1 decomposition using a linear sample size, N = \u0398(M).\nBefore introducing our algorithm, let us consider the naive approach of estimating \u2206 by taking the rank-1 truncated SVD of the matrix ( 1NBN \u2212 \u03c1\u0302\u03c1\u0302>), which concentrates to \u2206\u2206> in spectral distance asymptotically. Unfortunately, this approach leads to a sample complexity as large as \u0398(M2 logM). In the linear sample size regime, the empirical counts matrix is a poor representation of the underlying distribution. Intuitively, due to the high sampling noise, the rows and columns of BN corresponding to words with larger marginal probabilities have higher row and column sums in expectation, as well as higher variances that undermine the spectral concentration of the matrix as a whole. This\nobservation leads to the idea of pre-scaling the matrix so that every word (i.e. row/column) is roughly of unit variance. Indeed, with a slight modification of the truncated SVD, we can improve the sample complexity of this approach to \u0398(M log(M)), which is nearly linear. It is, however, not obvious how to further improve this. Appendix E provides a detailed analysis of these aforementioned truncated SVD approaches.\nNext, we briefly discuss the important ideas of our algorithm that lead to the linear sample complexity. Our algorithm consists of two phases: For a small constant 0, Phase I of our algorithm returns estimates \u03c1\u0302 and \u2206\u0302 both up to 0 accuracy in `1 norm with \u0398(M) samples; After the first phase gets us off the ground, Phase II builds upon the output of Phase I to refine the estimation to any target accuracy with \u0398(M/ 2) samples. The outline of the two phases are given in Algorithm 1 and Algorithm 2, separately, and the detailed analysis of the algorithms are deferred to Section 3 and 4.\nThe guarantees of the main algorithm follows immediately from Theorem 2.1 and 2.2.\nPhase I: \u201cbinning\u201d and \u201cregularization\u201d In Section 1, we drew the connection between our problem and the community detection problem in sparse random graphs. Recall that when the word marginals are roughly uniform, namely all in the order of O( 1M ), the linear sample regime corresponds to the stochastic block model setup where the expected row / column sums are all in the order of d0 = N M = \u2126(1). It is well-known that in this sparse regime, the adjacency matrix, or the empirical count matrix BN in our problem, does not concentrate to the expectation matrix in the spectral distance. Due to some heavy rows with row sum in the order of \u2126( logMlog logM ), the leading eigenvectors are polluted by the local properties of these heavy nodes and do not reveal the global structure of the graph, which are precisely the desired information in expectation.\nIn order to enforce spectral concentration in the linear sample size regime, one of the many techniques is to tame the heavy rows and columns by setting them to 0. This simple idea was first introduced by [23], and followed by analysis works in [22] and many others. Recently in [33] and [34] the authors provided clean and clever proofs to show that any such \u201cregularization\u201d essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.\nIs it possible to leverage such \u201cregularization\u201d ideas in our problem where the marginal probabilities are not uniform? A natural candidate solution would be to partition the vocabulary M into bins of words according to the word marginals, so that the words in the same bin have roughly uniform marginals. Restricting our attention to the diagonal blocks of B whose indices are in the same bin, the expected row / column sums are indeed roughly uniform. Then we can regularize each diagonal block separately to guarantee spectral concentration, to which truncated SVD should then apply. Figure 1a visualizes the two operations of \u201cbinning\u201d and \u201cregularization\u201d.\nEven though the \u201cbinning\u201d idea seems natural, there are three concerns one needs to rigorously address in order to implement the idea:\n1. We do not have access to the exact marginal \u03c1. With linear sample size, we only can estimate \u03c1 up to constant accuracy in `1 norm. If we implement binning according to the empirical marginals, there is considerable probability that words with large marginals are placed in a bin intended for words with small marginals \u2014 which we call \u201cspillover effect\u201d. When directly applied to the empirical bins with such spillover, the existing results of \u201cregularization\u201d in [34] do not lead to the desired concentration result.\n2. When restricting to each diagonal block corresponding to a bin, we are throwing away all the sample counts outside the block. This greatly reduces the effective sample size, and it is not obvious that we retain enough samples in each diagonal block to guarantee meaningful concentration results and subsequent estimation. This is particularly worrying because we may use a super-constant number of bins, and hence throw away all but a subconstant fraction of data for some words.\nInput: 2N sample counts.\nOutput: Estimator \u03c1\u0302, \u2206\u0302, B\u0302.\nDivide the sample counts into two independent batches of equal size N , and construct two empirical count matrices BN1 and BN2.\n1. (Estimate the marginal) \u03c1\u0302i = 1 N \u2211 j\u2208[M ][BN1]i,j .\n2. (Binning) Partition the vocabulary M into: I\u03020 = { i : \u03c1\u0302i <\n0 M } , I\u0302log = { i : \u03c1\u0302i > log(M) M } , I\u0302k = { i : e k M \u2264 \u03c1\u0302i \u2264 e k+1 M } , k = 1 : log log(M).\n3. (Estimate the separation \u2206\u0302)\n(a) (Estimate \u2206\u0302I\u0302log) (up to sign flip)\nIf \u2211 \u03c1\u0302I\u0302log < 0, set \u2206\u0302I\u0302log = 0, else\ni. (Rescaling): Set E = diag(\u03c1\u0302I\u0302log) \u22121/2[BN2 \u2212 \u03c1\u0302\u03c1\u0302>]I\u0302log\u00d7I\u0302logdiag(\u03c1\u0302I\u0302log) \u22121/2.\nii. (t-SVD): Let ulogu > log be the rank-1 truncated SVD of E.\niii. Set vlog = diag(\u03c1\u0302I\u0302log) 1/2ulog.\n(b) (Estimate \u2206\u0302I\u0302k) (up to sign flip)\nIf \u2211 \u03c1\u0302I\u0302k < 0e \u2212k, set \u2206\u0302I\u0302k = 0, else\ni. (Regularization): Set B\u0303 = [BN2]I\u0302k\u00d7I\u0302k , set d max k = N( \u2211 \u03c1\u0302I\u0302k) ek+\u03c4 M .\nIf a row/column of B\u0303 has sum larger than 2dmaxk , set the entire row/column to 0. Set E = (B\u0303 \u2212 \u03c1\u0302I\u0302k \u03c1\u0302 > I\u0302k ).\nii. (t-SVD): Let vkv > k be the rank-1 truncated SVD of E.\n(c) (Estimate \u2206\u0302I\u03020) Set \u2206\u0302I\u03020 = 0. (d) (Stitching \u2206\u0302I\u0302k \u2019s ) Fix k \u2217 = arg maxk \u2016vk\u2016, set \u2206\u0302I\u0302\u2217k = vk\u2217 .\nFor all k, define I+k = {i : i \u2208 Ik : \u2206\u0302i > 0} and I\u2212k = Ik\\I+k .\nSet \u2206\u0302I\u0302k = vk if\n\u2211 i\u2208I+\nk\u2217 ,j\u2208I + k [BN2]i,j\u2211 i\u2208M,j\u2208I+\nk [BN2]i,j\n>\n\u2211 i\u2208I+\nk\u2217 ,j\u2208I \u2212 k [BN2]i,j\u2211 i\u2208M,j\u2208I\u2212\nk [BN2]i,j\n,\nand \u2206\u0302I\u0302k = \u2212vk otherwise.\n4. Return \u03c1\u0302, \u2206\u0302, and B\u0302 = \u03c1\u0302\u03c1\u0302> + \u2206\u0302\u2206\u0302>.\nAlgorithm 1: Phase I\n3. Even if the \u201cregularization\u201d trick works for each diagonal block, we need to extract the useful information and \u201cstitch\u201d together this information from each block to provide an estimator for the entire matrix, which includes the off-diagonal blocks.\nPhase I (Algorithm 1) capitalizes on these natural ideas of \u201cbinning\u201d and \u201cregularization\u201d, and avoids the above potential pitfalls. We show that the algorithm satisfies the following guarantees:\nInput: Estimator \u03c1\u0302 and \u2206\u0302 from Phase I. N sample counts.\nOutput: Refined estimator \u03c1\u0302, \u2206\u0302, B\u0302.\n1. (Construct anchor partition)\nSet A = \u03c6. For k = 1, . . . , log logM, log: If \u2016\u2206\u0302I\u0302k\u20162 \u2264 ( \u221a dmaxk N ) 1/2, skip the bin, else, set A = A \u222a {i \u2208 I\u0302k : \u2206\u0302i > 0}.\n2. (Estimate anchor matrix)\nSet BA =\n[ \u2211 i\u2208A,j\u2208A[BN ]i,j \u2211 i\u2208A,j\u2208Ac [BN ]i,j\u2211\ni\u2208Ac,j\u2208A[BN ]i,j \u2211 i\u2208Ac,j\u2208Ac [BN ]i,j\n] . Set vector b = [ \u2211 i\u2208A,j\u2208M[BN ]i,j\u2211 i\u2208Ac,j\u2208M[BN ]i,j ] .\nSet aa> to be rank-1 truncated SVD of the 2\u00d7 2 matrix (BA \u2212 bb>).\n3. (Refine the estimation:)\nSet\n[ \u03c1\u0302>\n\u2206\u0302>\n] = [a, b]\u22121 [ \u2211 i\u2208A[BN ]i,M\u2211 i\u2208Ac [BN ]iM ]\n4. (Return) \u03c1\u0302, \u2206\u0302 and B\u0302 = \u03c1\u0302\u03c1\u0302> + \u2206\u0302\u2206\u0302>.\nAlgorithm 2: Phase II\nTheorem 2.1 (Main theorem 1. \u0398(M) sample complexity for achieving constant accuracy in `1 norm). Fix 0 to be a small constant. Given N = \u0398(M) samples, with large probability, Phase I (Algorithm 1) estimates \u03c1 and \u2206 up to 0 accuracy in `1 norm, namely\n\u2016\u03c1\u0302\u2212 \u03c1\u20161 < 0, \u2016\u2206\u0302\u2212\u2206\u20161 < 0, \u2016B\u0302 \u2212 B\u20161 < 0.\nPhase II: \u201cAnchor partition\u201d Under the Assumption of \u2126(1) separation, Phase II of our algorithm makes use of the estimates of \u2206 computed by Phase I, to refine the estimates of \u03c1 and \u2206.\nThe key to this refining process is to construct an \u201canchor partition\u201d, which is a bi-partition of the vocabulary M based on the signs of the estimate of separation \u2206\u0302 given by Phase I. We collapse the M \u00d7M matrix BN into a 2\u00d7 2 matrix corresponding to the bi-partition, and accurately estimate the 2 \u00d7 2 matrix with the N samples. Given this extremely accurate estimate of this 2 \u00d7 2 anchor matrix, we can now iteratively refine our estimates of \u03c1i and \u2206i for each word i by solving a simple least square fitting problem.\nThe above description may seem opaque, but similar ideas \u2014 estimation refinement based on some crude global information \u2014 has appeared in many works for different problems. For example, in a recent paper [19] on community detection, after obtaining a crude classification of nodes using spectral algorithm, one round of a \u201ccorrection\u201d routine is applied to each node based on its connections to the graph partition given by the first round. This refinement immediately leads to an optimal rate of recovery. Another example is given in [18] in the context of solving random quadratic equations, where local refinement of the solution follows the spectral method initialization. Figure 1b visualize the example of community detection. In our problem, the nodes are the M words, the edges are the sample counts, and instead of re-assigning the label to each node in the refinement routine, we re-estimate the \u03c1i and \u2206i for each word i.\nTheorem 2.2 (Main theorem 2. \u0398(M/ 2) sample complexity to achieve accuracy in `1 norm). Assume that B satisfies the \u2126(1) separation assumption. Given N samples, with large probability, Phase II of our algorithm (Algorithm 2) estimates \u03c1 and \u2206 up to accuracy in `1 norm:\n\u2016\u03c1\u0302\u2212 \u03c1\u20161 < O( \u221a M\nN ), \u2016\u2206\u0302\u2212\u2206\u20161 < O(\n\u221a M\nN ), \u2016B\u0302 \u2212 B\u20161 < O(\n\u221a M\nN )."}, {"heading": "3 Algorithm Phase I, achieving constant 0 accuracy", "text": "In this section, we outline the proof for Theorem 2.1, the detailed proofs are provided in Section A and B in the appendix.\nWe denote the ratio between sample size and the vocabulary size by\nd0 = N\nM . (5)\nThroughout our discussion for Phase I algorithm, we assume that d0 = \u0398(1) to be some fixed large constant.\nGiven N samples, the goal is to estimate the word marginal vector \u03c1 as well as the dictionary separation vector \u2206 up to constant accuracy in `1 norm. We denote the estimates by \u03c1\u0302 and \u2206\u0302. Also, we estimate the underlying probability matrix B with\nB\u0302 = \u03c1\u0302\u03c1\u0302> + \u2206\u0302\u2206\u0302>.\nNote that since \u2016\u2206\u20161 \u2264 \u2016\u03c1\u20161 = 1, constant `1 norm accuracy in \u03c1\u0302 and \u2206\u0302 immediately lead to constant accuracy of B\u0302 also in `1 norm.\nFirst, we show that it is easy to estimate the marginal probability vector \u03c1 up to constant accuracy.\nLemma 3.1 (Estimate the word marginal probability \u03c1). Given the empirical count matrix BN1 constructed with the first batch of N sample counts, consider the estimate of the marginal probabilities:\n\u03c1\u0302i = 1\nN\n\u2211 j\u2208M [BN1]i,j . (6)\nWith large probability, we can bound the estimation accuracy by:\n\u2016\u03c1\u0302\u2212 \u03c1\u20161 \u2264 O( \u221a 1\nd0 ). (7)\nThe hard part is to estimate the separation vector \u2206 up to constant accuracy in `1 norm with linear number of sample counts, namely d0 = \u0398(1). Recall that naively taking the rank-1 truncated SVD of ( 1NBN \u2212 \u03c1\u0302\u03c1\u0302>) fails to reveal any information about \u2206\u2206>, since in the linear sample size regime, the leading eigenvectors of BN are mostly dominated by the statistical noise of the words with large marginal probabilities. Our Phase I algorithm achieves this with more delicate steps. We analyze each step in the next 5 subsections, structured as follows:\n1. In Section 3.1, we introduce the binning argument and the necessary notations for the rest of the section. We bin the vocabularyM according to the estimates of word marginals, i.e. \u03c1\u0302, and we call a bin heavy or light according to the typical word marginals in that bin.\n2. In Section 3.2 we analyze how to estimate the entries of \u2206 restricted to the heaviest bin (up to some common sign flip). Because the marginal probabilities of words in this bin are sufficiently large, truncated SVD can be applied to the properly scaled diagonal block of the empirical average count matrix.\n3. In Section 3.3 we analyze how to estimate the entries of \u2206 restricted to all the other bins (up to some common sign flip), by examining the corresponding diagonal blocks in the empirical average count matrix.\nThe main challenge here is that due to the estimation error of the word marginal vector \u03c1\u0302, the binning is not perfect, in the sense that a lighter bin may include some heavy words by chance. Lemma 3.4 is the key technical lemma, which shows that with high probability, such spillover effect is very small for all bins with high probability. Then we leverage the clever proof techniques from [34] to show that if spillover effect is small, regularized truncated SVD can be applied to estimate the entries of \u2206 restricted to each bin.\n4. In Section 3.4, we analyze the lightest bin.\n5. In Section 3.5 we show how to fix the sign flips across different bins, by using the off-diagonal blocks, so that we can concatenate different sections of \u2206\u0302 to obtain an estimator for the entire separation vector \u2206."}, {"heading": "3.1 Binning according to empirical marginal distribution", "text": "Instead of tackling the empirical count matrix BN as a whole, we focus on its diagonal blocks and analyze the spectral concentration restricted to each block separately. Since the entries Bi,j restricted to each diagonal block are roughly uniform, the block hopefully concentrates well, so that we can estimate segments of the separation vector \u2206 by using truncated SVD with the \u201cregularization\u201d trick.\nFor any set of words I, we use [BN ]I,I to denote the diagonal block of BN whose indices are in the set I. Note that when restricting to the diagonal block, the rank 2 decomposition in (4) is given by BI,I = \u03c1I\u03c1>I + \u2206I\u2206>I .\nEmpirical binning We partition the vocabulary M according to the empirical marginal \u03c1\u0302 in (6):\nI\u03020 = { i : \u03c1\u0302i <\n0 M\n} , I\u0302k = { i : ek\u22121\nM \u2264 \u03c1\u0302i \u2264\nek\nM\n} , I\u0302log = { i : \u03c1\u0302i > log(M)\nM\n} .\nWe call this empirical binning to emphasize the dependence on the empirical estimator \u03c1\u0302, which is a random variable built from the first batch of N sample counts. We call I\u03020 the lightest empirical bin, and I\u0302log the heaviest empirical bin, and I\u0302k for 1 \u2264 k \u2264 log logM the moderate empirical bins.\nFor the analysis, we further define the exact bins according to the exact marginal probabilities:\nI0 = { i : \u03c1i <\n0 M\n} , Ik = { i : ek\u22121\nM \u2264 \u03c1i \u2264\nek\nM\n} , Ilog = { i : \u03c1i > log(M)\nM\n} . (8)\nSpillover effect As N increases asymptotically, we have I\u0302k coincides with Ik for each k. However, in the linear sample size regime, the estimation error in \u03c1\u0302 cause the following two spillover effects:\n1. Words from the heavy bin Ik\u2032 , for k\u2032 much larger than k, are placed in the empirical bin I\u0302k.\n2. Words from the exact bin Ik escape from the corresponding empirical bin I\u0302k;\nThe hope, that we can have good spectral concentration in each diagonal block [BN2]I\u0302k\u00d7I\u0302k , crucially relies on the fact that the entries Bi,j restricted to this block are roughly uniform. However, the hope may be ruined by the spillover effects, especially the first one. In the following sections, we show that with high probability the spillover effects are small for all empirical bins of considerable probability mass, in particular:\n1. The total marginal probability of the words in the empirical bin I\u0302k, that are from faraway exact bins, namely \u222a{k\u2032:k\u2032\u2265k+\u03c4}Ik\u2032 , is small and in the order of O(e\u2212e kd0) (see Lemma 3.4).\n2. Most words of Ik stays within the nearest empirical bins, namely \u222a{k\u2032:k\u2212\u03c4\u2264k\u2032\u2264k+\u03c4}I\u0302k\u2032 , (see Lemma 4.5).\nThroughout the discussion, we fix some small constant number \u03c4 to be:\n\u03c4 = 1 (9)\nNotations To analyze the spillover effects, we define some additional quantities. We define the total marginal probability mass in the empirical bins to be:\nWk = \u2211\ni\u2208I\u0302k\n\u03c1i, (10)\nand let Mk = |I\u0302k| denote the total number of words in the empirical bin. We also define W\u0302k = \u2211\ni\u2208I\u0302k \u03c1\u0302i.\nWe use J\u0302k to denote the set of the words from much heavier bins that are spilled over into the empirical bin I\u0302k (recall that \u03c4 is a small constant):\nJ\u0302k = I\u0302k \u2229 (\u222a{k\u2032:k\u2032\u2265k+\u03c4}Ik\u2032), (11)\nand let L\u0302k denote the \u201cgood words\u201d in the empirical bin I\u0302k:\nL\u0302k = I\u0302k\\J\u0302k. (12)\nwhere \u03c4 We also denote the total marginal probability mass of the heavy spillover words J\u0302k by:\nW k = \u2211\ni\u2208J\u0302k\n\u03c1i. (13)\nNote that these quantities are random variables determined by the randomness of the first batch of N samples, via the empirical binning. These are fixed quantities when we consider the empirical count matrix with the second batch of N samples.\nDefine the upper bound of the \u201ctypical\u201d word marginal in the k-th empirical bin to be:\n\u03c1k = e k+\u03c4/M,\nNote that since wp, wq \u2265 Cw, we can bound each entry in the true matrix by the product of marginals as\nBi,j \u2264 2\nC2w \u03c1i\u03c1j , \u2200i, j.\nLet dmaxk denote the expected max row/column sum of the diagonal block corresponding the k-th bin:\ndmaxk = 2\nC2w NWk\u03c1k. (14)"}, {"heading": "3.2 Estimate \u2206 restricted to the heaviest empirical bin", "text": "First, we show that the empirical marginal probabilities restricting the heaviest bin concentrate much better than what Lemma 3.1 implies.\nLemma 3.2 (Concentration of marginal probabilities in the heaviest bin). With high probability, for all the words with marginal probability \u03c1i \u2265 log(M)/M , we have that for some universal constant C1, C2,\nC1 \u2264 \u03c1\u0302i/\u03c1i \u2264 C2. (15) Lemma 3.2 says that with high probability, we can estimate the marginal probabilities for every words in the heaviest bin with constant multiplicative accuracy. Note that it also suggests that actually we do not need to worry about the spillover effect of the words from Ilog placed in much lighter bins, since with high probability, all the words stay in the bin I\u0302log and a constant number of adjacent empirical bins.\nNext two lemmas show that with square root re-scaling, truncated SVD works to estimate the segment of separation restricted to the empirical heaviest bin. Lemma 3.3 (Estimate \u2206 restricted to the heaviest empirical bin). Suppose that W\u0302log = \u2211 \u03c1\u0302I\u0302log > 0. Define D\u0302I\u0302log = diag(\u03c1\u0302I\u0302log), and consider the diagonal block corresponding to the heaviest empirical bin I\u0302log:\nE = D\u0302 \u22121/2 I\u0302log ( 1 N [BN2]I\u0302log,I\u0302log \u2212 \u03c1\u0302I\u0302log \u03c1\u0302 > I\u0302log )D\u0302 \u22121/2 I\u0302log . (16)\nLet uu> denote the rank 1 truncated SVD of matrix E, set vlog = D\u0302 1/2\nI\u0302log u. With high probability over\nthe second batch of N samples, we can estimate \u2206I\u0302log , the dictionary separation vector restricted to the heaviest empirical bin, with vlog up to sign flip with accuracy:\nmin{\u2016\u2206I\u0302log \u2212 vlog\u20161, \u2016\u2206I\u0302log + vlog\u20161} = O ( min { 1/d 1/2 0\n\u2016\u2206I\u0302log\u20161 , 1/d\n1/4 0\n}) . (17)\nThe two cases in the above bound correspond to whether the separation is large or small, compared to the statistical noise from sampling, which is in the order 1/d 1/4 0 . If the bin contains a large separation, then the bound follows the standard Wedin\u2019s perturbation bound; if the separation is small, i.e. \u2016\u2206I\u0302log\u20161 1/d 1/4 0 , then the bound 1/d 1/4 0 just corresponds to the magnitude of the statistical noise.\n3.3 Estimate \u2206 restricted to the log log(M) moderate empirical bins\nIn this section, we show that the spillover effects are small for all the moderate bins (Lemma 3.4). In particular, we upper bound the total spillover marginal W k for all k with high probability. Provided that the spillover effects are small, we show that (Lemma 3.5 and Lemma 3.6) truncated SVD with regularization can be applied to each diagonal block of the empirical count matrix BN2, to estimate the entries of the separation vector \u2206 restricted to each bin. The proofs of this section are provided in Section B in the appendix.\nLemma 3.4 (With high probability, spillover from much heavier bins is small). With high probability over the first batch of N sample counts, for all empirical bins {I\u03020, I\u03021, . . . I\u0302log log(M)}, we can bound the total marginal probability of spillover from much heavier bins, i.e. W k defined in (13), by:\nW k \u2264 2e\u2212e \u03c4+kd0/2. (18)\nAlso, if Wk > 0e \u2212k, we can bound the number of spillover words Mk by:\nMk \u2264Mk/dmaxk . (19)\nRecall that \u03c4 is set in (9), Wk is defined in (10), and d max k = NWk\u03c1k is defined in (14).\nNow consider [BN2]I\u0302k\u00d7I\u0302k , the diagonal block corresponding to the empirical bin I\u0302k. To ensure the spectral concentration of this diagonal block, we \u201cregularize\u201d it by removing the rows and columns with very large sum. The spectral concentration of the block with the remaining elements leads to an estimate of the separation vector \u2206I\u0302k restricted to L\u0302k, the set of \u201cgood words\u201d defined in (12). To make the operation of \u201cregularization\u201d more precise, we need to introduce some additional notations.\nDefine \u03c1\u0303k to be a vector with the same length as \u03c1I\u0302k , with the same entries for the good words,\nand set the entries corresponding to the spillover set J\u0302k to be 0, namely\n(\u03c1\u0303k)i = \u03c1i1 [ i \u2208 L\u0302k ] .\nSimilarly define vector \u2206\u0303k to be the separation restricted to the good words in the empirical bins:\n(\u2206\u0303k)i = \u2206i1 [ i \u2208 L\u0302k ] . (20)\nWe define the matrix B\u0303k (of the same size as [BN2]I\u0302k\u00d7I\u0302k):\nB\u0303k = \u03c1\u0303k\u03c1\u0303>k + \u2206\u0303k\u2206\u0303>k . (21)\nRecall that the expected max row sum of the diagonal block is given by dmaxk = NWk\u03c1k defined in (14). Let R\u0302k denote the indices of the rows and columns in [BN2]I\u0302k,I\u0302k whose row sum or column sum are larger than 2dmaxk , namely\nR\u0302k =   i \u2208 I\u0302k : \u2211\nj\u2208I\u0302k\n[BN2]i,j > 2d max k or\n\u2211\nj\u2208I\u0302k\n[BN2]j,i > 2d max k    . (22)\nStarting with B\u0303k = [BN2]I\u0302k\u00d7I\u0302k , we set all the rows and columns of B\u0303k indexed by R\u0302k to 0. Note that by definition the rows and columns in B\u0303k and B\u0303k that are zero-ed out do not necessarily coincide. However, the next lemma shows that B\u0303k concentrates to B\u0303k in the spectral distance.\nLemma 3.5 (Concentration of regularized diagonal block B\u0303k.). Suppose that the marginal of the bin\nI\u0302k is large enough Wk = \u2211 \u03c1I\u0302k > 0e\n\u2212k. With high probability at least (1 \u2212M\u2212rk ), where r is some universal constant, we have\n\u2225\u2225\u2225\u2225 1\nN B\u0303k \u2212 B\u0303k \u2225\u2225\u2225\u2225 2 \u2264 Cr1.5 \u221a dmaxk log 2 dmaxk N , (23)\nProof. Detailed proof of this lemma is provided in Section B in the appendix. Here we highlight the key steps of the proof.\nIn Figure 2, the rows and the columns of [BN ]I\u0302k\u00d7I\u0302k are sorted according to the exact marginal probabilities of the words in ascending order. The rows and columns that are set to 0 by regularization are shaded. Consider the block decomposition according to the good words L\u0302k and the spillover words J\u0302k. We bound the spectral distance of the 4 blocks (A1, A2, A3, A4) separately. The bound for the entire matrix B\u0302k is then an immediate result of triangle inequality.\nFor block A1 whose rows and columns all correspond to the \u201cgood words\u201d with roughly uniform marginals, we show its concentration by applying the result in [34]. For block A2 and A3, we want to show that after regularization the spectral norm of these two blocks are small. Intuitively, the expected row sum and the expected column sum of block A2 are bounded by 2d max k and 2d max k Wk Wk = O(1), as a result of the bound on the spillover words W k in Lemma 3.4. Therefore the spectral norm of the block are likely to be bounded by O( \u221a dmaxk ), which we show rigorously with high probability arguments. Lastly for block A4, which rows and columns all correspond to the spillover words. We show that the spectral norm of this block is very small as a result of the small spillover marginal W k.\nIt is also important to note that given Wk > 0e \u2212k and conditional on the high probability even\nthat W k \u2264 2e\u2212e kd0 , we can write dmaxk also as d max k = NMk\u03c1 2 k.\nLemma 3.6 (Given spectral concentration of block B\u0303k, estimate the separation \u2206\u0303k). Suppose that the\nmarginal of the bin I\u0302k is large enough Wk = \u2211 \u03c1I\u0302k > 0e \u2212k. Let vkv > k be the rank-1 truncated SVD\nof the regularized block (\n1 N B\u0303k \u2212 \u03c1\u0302I\u0302k \u03c1\u0302 > I\u0302k\n) . With high probability over the second batch of N samples,\nmin{\u2016\u2206\u0303k \u2212 vk\u20162, \u2016\u2206\u0303k + vk\u20162} = O  min    \u221a dmaxk log 2 dmaxk N\n1\n\u2016\u2206I\u0302k\u20162 ,\n(\u221a dmaxk log\n2 dmaxk N )1/2    .\n(24)\nNamely vk is an estimate of the vector \u2206\u0303k (defined as in (21)), up to some unknown sign flip."}, {"heading": "3.4 Estimate \u2206 restricted to the lightest empirical bin.", "text": "Claim 3.7 (Estimate separation restricted to the lightest bin). Setting \u2206\u0302I\u03020 = 0 only incurs an `1 error of a small constant, and this is simply because the total marginal of words in the lightest bin with high probability can be bounded by a small constant:\n\u2016\u2206I\u03020\u20161 \u2264 \u2016\u03c1I\u03020\u20161 \u2264 \u2016\u03c1I0\u20161 +W 0 \u2264 0 M M + e\u2212d0/2 = O( 0),\nwhere we used the assumption that d0 \u2265 1/ 40."}, {"heading": "3.5 Stitching the segments of \u2206\u0302 to reconstruct an estimation of the matrix", "text": "Given vk for all k as estimation for \u2206I\u0302k \u2019s up to sign flips. Fix k \u2217 to be one good bin (with large bin marginal and large separation). Partition the words into two groups I+k\u2217 = {i : i \u2208 Ik\u2217 : \u2206\u0302i > 0} and I\u2212k\u2217 = Ik\u2217\\I+k\u2217 . Without loss of generality assume that \u2211 i\u2208I+\nk\u2217 \u2206\u0302i \u2265\n\u2211 i\u2208I\u2212\nk\u2217 \u2206\u0302i. We set \u2206\u0302I\u0302k\u2217 = vk\u2217 .\nFor all other good bins k, we similarly define I+k and I\u2212k . The next claim shows how to determine the relative sign flip of vk\u2217 and vk.\nClaim 3.8 (Pairwise comparison of bins to fix sign flips). For all good bins k \u2208 G, we can fix the sign flip to be \u2206\u0302I\u0302k = vk if:\n\u2211 i\u2208I+\nk\u2217 ,j\u2208I + k [BN2]i,j \u2211\ni\u2208M,j\u2208I+k [BN2]i,j\n>\n\u2211 i\u2208I+\nk\u2217 ,j\u2208I \u2212 k [BN2]i,j \u2211\ni\u2208M,j\u2208I\u2212k [BN2]i,j\n,\nand \u2206\u0302I\u0302k = \u2212vk otherwise.\nProof. This claim is straightforward. When restricted to the good bins, the estimates vk are accurate enough. We can determine that the sign flips of k\u2217 and k are consistent if and only if the conditional distribution of the two word tuple (x, y) \u2208 M2 satisfies Pr(x \u2208 I+k\u2217 \u2223\u2223x \u2208 I+k ) > Pr(x \u2208 I+k\u2217 \u2223\u2223x \u2208 I\u2212k ), and we should revert vk otherwise.\nFinally, concatenate the segments of \u2206\u0302, we can summarize to bound the overall estimation error in `1 norm:\nLemma 3.9 (Accuracy of \u2206\u0302 of Phase I). For fixed 0 = \u2126(1) to be a small constant, if d0 =: N/M \u2265 1/ 40, with large probability, Phase I algorithm can estimate the separation vector \u2206 with constant accuracy in `1 norm:\n\u2016\u2206\u0302\u2212\u2206\u2016 = O( 0)."}, {"heading": "4 Algorithm Phase II, achieving arbitrary accuracy", "text": "Given \u03c1\u0302 and \u2206\u0302 from the Phase I of our algorithm. Under the \u2126(1) separation assumptions, we refine the estimation to achieve arbitrary accuracy in Phase II. In this section, we verify the steps of Algorithm 2, and show the correctness of Theorem 2.2."}, {"heading": "4.1 Construct an anchor partition", "text": "Imagine that we have a way to group the M words in the vocabulary into a new vocabulary with a constant number of superwords, and similarly define marginal vector \u03c1A and separation vector \u2206A over the superwords. The new probability matrix (of constant size) corresponds to we sum over the rows/columns of the matrix B according to the grouping. If we group the words in a way such that \u2206A still has \u2126(1) dictionary separation, then with N = \u2126(M) samples we can estimate the constantdimensional vector \u03c1A and \u2206A to arbitrary accuracy (as M 1). Note that accurate estimates of \u03c1A and \u2206A defined over the superwords give us crude \u201cglobal information\u201d about the true \u03c1 and \u2206. Now sum the empirical BN over the rows accordingly, and leave the columns intact, it is easy to recognize that the expected factorization is given by \u03c1A\u03c1 > + \u2206A\u2206 >. Therefore, given accurate \u03c1A and \u2206A, refining \u03c1\u0302 and \u2206\u0302 is as simple as solving a least square problem. We formalize this argument and introduce the definition of anchor partition below.\nDefinition 4.1 (Anchor partition). Consider a partition of the vocabulary into (A,Ac). denote \u03c1A =\u2211 i\u2208A \u03c1i and \u2206A = \u2211 i\u2208A\u2206i. We call it an anchor partition if for some constant CA = O(1),\ncond\n([ \u03c1A, \u2206A\n1\u2212 \u03c1A, \u2212\u2206A\n]) \u2264 CA. (25)\nIn this section, we show that if the dictionary separation \u2016\u2206\u20161 = \u2126(1), the estimator \u2206\u0302 obtained in Phase I with constant `1 accuracy contains enough information for us to construct an anchor partition.\nFirst note that with \u2126(1) separation, it is feasible to find a pair of anchor partition. The next lemma state a sufficient condition for constructing an anchor partition.\nLemma 4.2 (Sufficient condition for constructing an anchor partition). Consider a set of words I, let \u2206I be the vector of \u2206 restricted to the entries in I. Suppose that for some constant C = \u2126(1), we have\n\u2016\u2206I\u20161 \u2265 C\u2016\u2206\u20161, (26)\nand suppose that for some constant C \u2032 \u2264 13C, we can estimate \u2206I up to precision:\n\u2016\u2206\u0302I \u2212\u2206I\u20161 \u2264 C \u2032\u2016\u2206I\u20161. (27)\nDenote A\u0302 = {i \u2208 I : \u2206\u0302i > 0}. We have that (A\u0302,M\\A\u0302) forms an anchor partition defined in 4.1.\nConsider the heaviest bin. If Wlog = \u2126(1) and \u2016\u2206log\u20161 = \u2126(1), then Lemma 3.3 shows that we can estimate the portion of \u2206 restricted to the heaviest bin to constant `1 norm accuracy, and then Lemma 4.2 above shows how to find an anchor partition with set A as a subset of I\u0302log.\nIf either Wlog = o(1) or \u2016\u2206log\u20161 = o(1), there must be at least a constant fraction of marginal probabilities as well as a constant fraction of separation located in the moderate empirical bins (I\u0302k\u2019s). Recall that we can always ignore the lightest bin. If this is the case, in the following we show how to construct an anchor partition using the moderate empirical bins.\nDefinition 4.3 (Good bin). Denote the sum of dictionary separation restricted to the \u201cgood words\u201d(defined in (12)) L\u0302k by:\nSk = \u2211\ni\u2208L\u0302k\n|\u2206i|, for k = 0, . . . , log log(M). (28)\nNote that equivalently we can write Sk = \u2016\u2206\u0303k\u20161. We call an empirical bin I\u0302k to be a \u201cgood bin\u201d if for some fixed constant C1, C2 = \u2126(1) it satisfies the following two conditions:\n1. the marginal probability of the bin Wk \u2265 C1e\u2212k.\n2. the ratio between the separation and the marginal probability of the bin satisfies Sk2Wk \u2265 C2.\nThe next Lemma shows that Phase I algorithm provides a good estimate of the separation restricted to the good bins with high probability.\nLemma 4.4 (Estimate the separation restricted to the k-th good bin). Consider the k-th empirical bin I\u0302k with Mk words. If it is a \u201cgood bin\u201d, then with high probability, the estimate for the separation vector \u2206 restricted to the k-th empirical bin \u2206\u0302I\u0302k = vk given in Lemma 3.6, up to accuracy:\n\u2016\u2206\u0302I\u0302k \u2212 \u2206\u0303k\u20161 \u2264 1\u221a d0 \u2016\u2206I\u0302k\u20161. (29)\nLet G \u2286 {1, . . . , log log(M)} denote the set of all the \u201cgood bins\u201d. Next, we show that there are many good bins when the dictionary separation is large.\nLemma 4.5 (Most words fall in \u201cgood bins\u201d with high probability ). Assume that \u2016\u2206I\u0302log\u20161 < 1 2\u2016\u2206\u20161. Fix constants C1 = C2 = 1 24\u2016\u2206\u20161. Note that C1 = C2 = \u2126(1) by well separation Assumption.\nWith high probability, we can bound the total marginal probability mass in the \u201cgood bins\u201d as:\n\u2211 k\u2208G Wk \u2265 \u2016\u2206\u20161 12 . (30)\nBy definition this implies a bound of total separation in the good words in the good bins:\n\u2211\ni\u2208L\u0303k,k\u2208G\n|\u2206i| = \u2211\nk\u2208G Sk \u2265 2C2\n\u2211 k\u2208G Wk \u2265 1 24 (\u2016\u2206\u20161)2 = \u2126(1). (31)\nLemma 4.5 together with Lemma 4.4 suggest that we can focus on the estimation of separation vector restricted to the \u201cgood words\u201d in the \u201cgood bins\u201d, namely \u2206\u0303I\u0302k for all k \u2208 G. In particular, set I = \u222ak\u2208GL\u0302k in Lemma 4.2. By Lemma 4.5 we know the separation contained in I is at least\u2211\nk\u2208G Sk = C\u2016\u2206\u20161; moreover by Lemma 4.4, with linear number of samples (large d0) we can estimate \u2206I up to constant `1 accuracy. Therefore we can construct a valid anchor partition (A,M\\A) by setting:\nA = {\u2206\u0302i > 0 : k \u2208 G}.\nIdeally, we need to restrict to the \u201cgood words\u201d and set the anchor partition to be {i \u2208 L\u0303k, \u2206\u0302i > 0 : k \u2208 G}. However, empirically we do not know which are the \u201cgood words\u201d instead of spillover from heavier bins. Luckily, the bound on the total marginal of spillover \u2211 kW k = O(e\n\u2212d0) guarantees that even if we mis-classify all the spillover words, it does not ruin the separation in A constructed above."}, {"heading": "4.2 Estimate the anchor matrix", "text": "Now consider grouping the words into two superwords according to the anchor partition we constructed. We define the 2\u00d7 2 matrix DA = [\n\u03c1A, \u2206A 1\u2212 \u03c1A, \u2212\u2206A\n] to be the anchor matrix.\nTo estimate the anchor matrix, we just need to accurately estimate two scalar variables \u03c1A and \u2206A. Apply the standard concentration bound, we can argue that with high probability,\n\u2016 [ \u2211 i\u2208A,j\u2208A[BN ]i,j \u2211\ni\u2208A,j\u2208Ac [BN ]i,j\u2211 i\u2208Ac,j\u2208A[BN ]i,j \u2211 i\u2208Ac,j\u2208Ac [BN ]i,j\n] \u2212DAD>A\u2016 < O(\n1\u221a N )\nMoreover we have that |\u2206A| = \u2126(1) since (A,Ac) is an anchor partition. Therefore we can estimate \u03c1A and \u2206A to accuracy\n1\u221a N , and when N = \u2126(M) and M is asymptotically large, we can essentially\nobtain a close to exact DA."}, {"heading": "4.3 Use anchor matrix to estimate dictionary", "text": "Now given an anchor partition of the vocabulary (A,Ac), and given the exact anchor matrix DA which has \u2126(1) condition number, refining the estimation of \u03c1i and \u2206i for each i is very easy and achieves optimal rate.\nLemma 4.6 (Estimate \u03c1 and \u2206 with accuracy in `2 distance). We have that with probability at least 1\u2212 \u03b4,\n\u2016\u03c1\u0302\u2212 \u03c1\u2016 < \u221a \u03b4/N, \u2016\u2206\u0302\u2212\u2206\u2016 < \u221a \u03b4/N.\nCorollary 4.7 (Estimate \u03c1 and \u2206 in `1 distance). With large probability we can estimate \u03c1 and \u2206 in `1 distance:\n\u2016\u03c1\u0302\u2212 \u03c1\u20161 < \u221a M\u03b4\nN , \u2016\u2206\u0302\u2212\u2206\u20161 <\n\u221a M\nN ."}, {"heading": "5 Sample complexity lower bounds for estimation VS testing", "text": ""}, {"heading": "5.1 Lower bound for estimating probabilities", "text": "We reduce the estimation problem to the community detection for a specific set of model parameters. Consider the following topic model with equal mixing weights, i.e. w = wc = 1/2. For some constant C\u2206 = \u2126(1), the two word distributions are given by:\np = [ 1 + C\u2206 M , . . . , 1 + C\u2206 M , 1\u2212 C\u2206 M , . . . , 1\u2212 C\u2206 M ] ,\nq = [ 1\u2212 C\u2206 M , . . . , 1\u2212 C\u2206 M , 1 + C\u2206 M , . . . , 1 + C\u2206 M ] .\nThe expectation of the sum of samples is given by\nE[BN ] = N 1 2 (pp> + qq>) = N M2 [ 1 + C2\u2206 1\u2212 C2\u2206 1\u2212 C2\u2206 1 + C2\u2206 ] .\nNote that the expected row sum is in the order of \u2126(NM ). When N is small, with high probability the entries of the empirical sum BN only take value either 0 or 1, and BN approximately corresponds to a SBM (G(M,a/M, b/M)) with parameter a = NM (1 + C 2 \u2206) and b = N M (1\u2212 C2\u2206).\nIf the number of sample document is large enough for any algorithm to estimating the dictionary vector p and q up to `1 accuracy for a small constant , it can then be used to achieve partial recovery in the corresponding SBM, namely correctly classify a \u03b3 proportion of all the nodes for some constant \u03b3 = C\u2206 .\nAccording to Zhang & Zhou [52], there is a universal constant C > 0 such that if (a\u2212 b)2/(a+ b) < c log(1/\u03b3), then there is no algorithm that can recover a \u03b3-correct partition in expectation. This suggests that a necessary condition for us to learn the distributions is that\n(2(N/M)C2\u2206) 2\n2(N/M) \u2265 c log(C\u2206/ ),\nnamely (N/M) \u2265 c log(C\u2206/ )/2C4\u2206. In the well separated regime, this means that the sample complexity is at least linear in the vocabulary size M .\nNote that this lower bound is in a sense a worst case constructed with a particular distribution of p and q, and for other choices of p and q it is possible that the sample complexity can be much lower than that \u2126(M)."}, {"heading": "5.2 Lower bound for testing property of HMMs", "text": "In this section, we analyze the information theoretical lower bound to achieve the task of testing whether a sequence of observations is indeed generated by a 2-state HMM. This problem is closely related to the problem of, in our general setup in the main text, testing whether the underlying probability matrix of the observed sample counts is of rank 1 or rank 2. Note that in the context of HMM this would be a stronger lower bound, since we permit an estimator to have more information given the sequence of consecutive observations, instead of merely bigram counts.\nTheorem 5.1 (Theorem 1.6 restated). Consider a sequence of N observations from a HMM with two states {+,\u2212} and emission distributions p, q supported on M elements. For asymptotically large M , using a sequence of N = O(M) observations, it is information theoretically impossible to distinguish the case that the two emission distributions are well separated, i.e. ||p\u2212 q||1 \u2265 1/2, from the case that both p and q are uniform distribution over [M ], namely a degenerate HMM of rank 1.\nIn order to derive a lower bound for the sample complexity, it suffices to show that given a sequence of N consecutive observed words, for N = o(M), one can not distinguish whether it is generated by a random instance from a class of 2-state HMMs (Definition 1.4) with well-separated emission distribution p and q, or the sequence is simply N i.i.d. samples from the uniform distribution over M, namely a degenerate HMM with p = q.\nWe shall focus on a class of well-separated HMMs parameterized as below: a symmetric transition\nmatrix T = [ 1\u2212 t, t t, 1\u2212 t ] , where we set the transition probability to t = 1/4; the initial state distribution is \u03c0p = \u03c0q = 1/2 over the two states sp and sq; the corresponding emission distribution p and q are uniform over two disjoint subsets of the vocabulary, A and M\\A, separately. Moreover, we treat the set A as a random variable, which can be any of the ( M M/2 ) subset of the vocabulary of size M/2, which equal probability 1/ ( M M/2 ) . Note that there is a one to one mapping between the set A and an instance in the class of well-separated HMM. Now consider a random sequence of N words GN1 = [g1, . . . , gN ] \u2208 MN . If this sequence is generated by an instance of 2-state HMM denoted by A, the joint probability of (GN1 ,A) is given by:\nPr2(G N 1 ,A) = Pr2(GN1 |A)Pr2(A) = Pr2(GN1 |A) 1( M M/2 ) (32)\nMoreover, given A, since the support of p and q are disjoint over A and M\\A by our assumption, we can perfectly infer about the sequence of hidden states SN1 (G N 1 ,A) = [s1, . . . , sN ] \u2208 {sp, sq}N simply by the rule si = sp if gi \u2208 A and si = sq otherwise. Thus we have:\nPr2(G N 1 |A) = Pr2(GN1 , SN1 |A) =\n1/2\nM/2\nM\u220f\ni=2\n(1\u2212 t)1[si = si\u22121] + t1[si 6= si\u22121] M/2 . (33)\nOn the other hand, if the sequence GN1 is simply i.i.d. samples from uniform distribution overM, its probability is given by\nPr1(G N 1 ) =\n1\nMN . (34)\nWe further define a joint distribution rule Pr1(G N 1 ,A) such that the marginal probability agrees with Pr1(G N 1 ). In particular, we define:\nPr1(G N 1 ,A) = Pr1(A|GN1 )Pr1(GN1 ) \u2261\nPr2(G N 1 |A)\u2211\nB\u2208( MM/2) Pr2(GN1 |B)\nPr1(G N 1 ), (35)\nwhere we define the conditional probability Pr1(A|GN1 ) using the properties of the 2-state HMM class. The main idea of the proof to Theorem 5.1 is to show that for N = o(M), the total variation distance between Pr1 and Pr2, is small. It follows immediately from the connection between the error bound of hypothesis testing and total variation distance between two probability rules, that if TV (Pr1(G N 1 ),Pr2(G N 1 )) is too small we are not able to test which probability rule the random sequence GN1 is generated according to. The detailed proofs are provided in Appendix D."}, {"heading": "A Proofs for Algorithm Phase I (Section 3.1, 3.2, 3.4)", "text": "Proof. (to Lemma 3.1 (Estimate the word marginal probability \u03c1)) We analyze how accurate the empirical average \u03c1\u0302 is. Note that under the assumption of Poisson number of samples, we have \u03c1\u0302i \u223c 1NPoi(N\u03c1i), and V ar(\u03c1\u0302i) = \u03c1i/N . Apply Markov inequality:\nPr\n( M\u2211\ni=1\n\u2223\u2223\u2223\u2223 \u03c1\u0302i \u2212 \u03c1i\u221a\n\u03c1i\n\u2223\u2223\u2223\u2223 2 > t ) \u2264 M tN ,\nthus probability at least 1\u2212 \u03b4, we can bound M\u2211\ni=1\n\u2223\u2223\u2223\u2223 \u03c1\u0302i \u2212 \u03c1i\u221a\n\u03c1i\n\u2223\u2223\u2223\u2223 2 < M\nN\u03b4 = O\n( 1\nd0\n) . (36)\nThen apply Cauchy-Schwatz, we have\nM\u2211\ni=1\n|\u03c1\u0302i \u2212 \u03c1i| \u2264 ( M\u2211\ni=1\n\u221a \u03c1i\n2 M\u2211\ni=1\n\u2223\u2223\u2223\u2223 \u03c1\u0302i \u2212 \u03c1i\u221a\n\u03c1i\n\u2223\u2223\u2223\u2223 2 )1/2 = O ( 1\u221a d0 ) .\nProof. (to Lemma 3.2 (Concentration of marginal probabilities in the heaviest bin)) Fix constants C1 = 1 2 and C2 = 10, apply Corollary F.5 of Poisson tail (note that N\u03c1i > d0 logM is very large), the probability that \u03c1\u0302i concentrates well is given by:\nPr(C1N\u03c1i < Poi(N\u03c1i) < C2N\u03c1i) \u2265 1\u2212 4e\u2212N\u03c1i/2 \u2265 1\u2212 4e\u2212N logM/(2M),\nwhere we used the fact that \u03c1i \u2265 log(M)/M in the heaviest bin. Note that the number of words in the heaviest bin is upper bounded by Mlog \u2264 1mini\u2208Ilog \u03c1i \u2264 M log(M) . Take a union bound, we have that with high probability, all the estimates \u03c1\u0302i\u2019s in the heaviest bin concentrate well:\nPr(\u2200i \u2208 Ilog : C1\u03c1i < \u03c1\u0302i < C2\u03c1i) \u2265 1\u2212 M logM e\u2212N logM/(2M)\n\u2265 1\u2212 4e\u2212N logM/(2M)+logM\u2212log logM\n\u2265 1\u2212M\u2212(d0/2\u22121) \u2265 1\u2212M\u22121,\nsince we pick d0 to be a large constant and d0 > 4.\nProof. (to Lemma 3.3 (Estimate the dictionary separation restricted to the empirical heaviest bin)) (1) First, we claim that with high probability, no word from Ik for k \u2264 log(M)\u2212e2 is placed in I\u0302logM . Namely all the words in I\u0302log have true marginals at least e \u2212e2 logM M , in the same order of \u2126( logM M ).\nThis is easy to show, by the Corollary F.5 of Poisson tail bound, each of the word from the much lighter bins is placed in I\u0302logM with probability less than 2e\u2212N logM/M . Take a union bound over all words with marginal at least 1/M , we can bound the probability that any of the words being placed in I\u0302logM by 2Me\u2212d0 logM = O(M\u2212d0+1). In the following we will just condition on this high probability event and do not consider the words from much lighter bins spillover in to the heaviest empirical bin I\u0302logM .\n(2) The appropriate scaling with the diagonal matrix diag(\u03c1\u0302I\u0302log) \u22121/2 on both sides of the diagonal block is very important, which allows us to apply matrix Bernstein inequality at a sharper rate. Note that with the two independent batches of samples, the empirical count matrix BN2 is independent from the empirical marginal vector \u03c1\u0302. Thus for every fixed realization of \u03c1\u0302, we apply Bernstein matrix concentration we have that with probability at least 1\u2212M\u22121,\n\u2016D\u0302\u22121/2 I\u0302log ( 1 N [BN ]I\u0302log,I\u0302log \u2212 BI\u0302log,I\u0302log)D\u0302 \u22121/2 I\u0302log \u20162 \u2264\n\u221a log(Mlog/\u03b4)\nN +\nM log(M) log(Mlog/\u03b4)\nN\n= O\n( M\nN (1 +\nlog(1/\u03b4)\nlog(M) )\n)\n= O\n( M\nN\n) ,\nwhere we used the fact that the all the marginals in the heaviest bin can be estimated with constant multiplicative accuracy, namely C1 < \u03c1\u0302i/\u03c1i < C2 for all i \u2208 I\u0302log, given by Lemma 3.2; also, note that compared to the Bernsten matrix inequality in (47) without scaling, here with the proper scaling we have V ar \u2264 1 and B \u2264 Mlog(M) , since \u03c1\u0302i > log(M)/M for all i \u2208 I\u0302log. We will show that [BN ]I\u0302log,I\u0302log , the diagonal block of the empirical count matrix, concentrates well enough to ensure that we can estimate the separation restricted to the heaviest bin by the leading eigenvector of ([BN ]I\u0302log,I\u0302log \u2212 \u03c1\u0302I\u0302log \u03c1\u0302 > I\u0302log ).\nNote that\nD\u0302 \u22121/2 I\u0302log BI\u0302log,I\u0302logD\u0302 \u22121/2 I\u0302log = D\u0302 \u22121/2 I\u0302log \u03c1I\u0302log(D\u0302 \u22121/2 I\u0302log \u03c1I\u0302log) > + D\u0302 \u22121/2 I\u0302log \u2206\u2032I\u0302log (D\u0302 \u22121/2 I\u0302log \u2206I\u0302log) >.\nApply triangle inequality we have\n\u2016D\u0302\u22121/2 I\u0302log ( 1 N [BN ]I\u0302log,I\u0302log \u2212 \u03c1\u0302I\u0302log \u03c1\u0302 > I\u0302log )D\u0302 \u22121/2 I\u0302log \u2212 D\u0302\u22121/2 I\u0302log \u2206\u2032I\u0302log (D\u0302 \u22121/2 I\u0302log \u2206I\u0302log) >\u20162\n\u2264 \u2225\u2225\u2225\u2225D\u0302 \u22121/2 I\u0302log ( 1 N [BN ]I\u0302log,I\u0302log \u2212 BI\u0302log,I\u0302log)D\u0302 \u22121/2 I\u0302log \u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2225D\u0302 \u22121/2 I\u0302log (\u03c1\u0302I\u0302log \u03c1\u0302 > I\u0302log \u2212 \u03c1I\u0302log\u03c1 > I\u0302log )D\u0302 \u22121/2 I\u0302log \u2225\u2225\u2225\u2225 2\n=O\n( M\nN\n) + \u221a M\nN\n=O\n(\u221a M\nN\n) ,\nwhere in the last equality we used the fact that d0 = N/M is some large constant. (3) Let uu> be the rank-1 truncated SVD of D\u0302 \u22121/2 I\u0302log ( 1N [BN ]I\u0302log,I\u0302log \u2212 \u03c1\u0302I\u0302log \u03c1\u0302 > I\u0302log )D\u0302 \u22121/2 I\u0302log . Let v = D\u0302 1/2 I\u0302log u be our estimate for \u2206I\u0302log . Apply Wedin\u2019s theorem to rank-1 matrix (Lemma F.1), we can bound the distance between vector u and D\u0302 \u22121/2 I\u0302log \u2206 I\u0302log by:\nmin{\u2016D\u0302\u22121/2 I\u0302log \u2206I\u0302log \u2212 u\u20162, \u2016D\u0302 \u22121/2 I\u0302log \u2206I\u0302log + u\u20162} = O\n min    (M/N)1/2\n\u2016D\u0302\u22121/2 I\u0302log \u2206I\u0302log\u20162 , (M/N)1/4\n     .\nNote that \u2016D\u03021/2 I\u0302log 1\u20162 = \u2016\u03c1\u03021/2I\u0302log\u20162 = 1. Apply Cauchy-Schwatz, for any vector x, we have\n\u2016D\u0302\u22121/2 I\u0302log x\u20162\u2016D\u03021/2I\u0302log1\u20162 \u2265 \u2223\u2223\u2223\u2223< D\u0302 \u22121/2 I\u0302log x, D\u0302 1/2 I\u0302log 1 > \u2223\u2223\u2223\u2223 = \u2016x\u20161,\ntherefore, we can use \u2016D\u0302\u22121/2 I\u0302log x\u20162 \u2265 \u2016x\u20161 to bound\nmin{\u2016\u2206I\u0302log \u2212 v\u20161, \u2016\u2206I\u0302log + v\u20161} \u2264 O ( min { (M/N)1/2\n\u2016\u2206I\u0302log\u20161 , (M/N)1/4\n}) .\nIn the above inequalities we absorb all the universal constants and focus on the scaling factors.\nProof. (of Lemma 3.8 (Pairwise comparison of bins to fix sign flips))\nProof. (to Lemma 3.9 (Accuracy of \u2206\u0302 in Phase I)) Consider for each empirical bin. If W\u0302k < 0e \u2212k set \u2206\u0302I\u0302k = 0. We can bound the total `1 norm error incurred in those bins by 0. Also, for the lightest bin, we can bound the total `1 norm error from setting \u2206\u0302I\u03020 = 0 by 0 small constant. If W\u0302k > 0e \u2212k, we can\napply the concentration bounds in Lemma 3.2, 3.6, and note that \u2016\u2206\u0302I\u0302k\u2212\u2206I\u0302k\u20161 \u2264 \u221a Mk\u2016\u2206\u0302I\u0302k\u2212\u2206I\u0302k\u20162.\nNote that we need to take a union bound of probability that spectral concentration results holds (Lemma 3.5) for all the bins with large enough marginal. This is true because we have at most log logM bins, and each bin\u2019s spectral concentration holds with high probability (1 \u2212 1/poly(M)), thus even after taking the union bound the failure probability is still inverse poly in M .\nActually throughout the paper the small constant failure probability is only incurred when bounding the estimation error of \u03c1\u0302, for the same reason of estimating a simple and unstructured distribution.\nOverall, we can bound the estimation error in `1 norm by:\n\u2016\u2206\u0302\u2212\u2206\u20161 \u2264 0\ufe38\ufe37\ufe37\ufe38 lightest bin + 1/d 1/4 0\ufe38 \ufe37\ufe37 \ufe38\nheaviestbin\n+ 0\ufe38\ufe37\ufe37\ufe38 moderate bins with small marginal\n+ \u2211\nk\n\u221a Mk(\n\u221a dmaxk log\n2 dmaxk N )1/2\n\ufe38 \ufe37\ufe37 \ufe38 moderate bins with large marginal\n\u2264 2 0 + 1/d1/40 + \u2211\nk\n( M2kNWk\u03c1k\nN2 )1/4\n\u2264 2 0 + 1/d1/40 + e\u03c4 \u2211\nk\n( W 2kMk N )1/4\n\u2264 2 + 1/d1/40 (1 + e\u03c4 ) = O( 0)\nwhere in the second last inequality used Cauchy-Schwartz and the factWk \u2264 1, so that \u2211 k( W 2kMk N )\n1/4 \u2264\u2211 k( \u221a Wk \u221a Mk) 1/2 \u2264 (\u2211kWk \u2211 kMk) 1/4 \u2264M1/4, and in the last inequality above we use the assumption that d0 =: N/M \u2265 1/ 40."}, {"heading": "B Proofs for Algorithm Phase I (Section 3.3)", "text": "Proof. (to Lemma 3.4 (Spillover from much heavier bins is small in all bins)) We define dk = e kd0, which is not to be confused with d max k defined in (14). (1) Consider k\u2032 = k + \u03c4 . The probability that a word i from Ik\u2032 falls into I\u0302k is bounded by:\nPr ( N ek\u22121\nM < Poi(N\u03c1i) < N\nek\nM\n) < Pr ( Poi(N e(\u03c4+k)\nM ) < N\nek\nM\n) \u2264 2e\u2212e\u03c4+kd0/2 (37)\nwhere we apply the Poisson tail bound (1) in Corollary F.5, and set c = e\u2212\u03c4 < 1/2 for \u03c4 \u2265 1. Note that this bound is doubly exponentially decreasing in \u03c4 and exponentially decreasing in d0.\nIn expectation, we can bound W k by:\nEW k = E \u2211\ni\u2208M \u03c1i\n\u2211\nk\u2032:k\u2032\u2265k+\u03c4+1 1[i \u2208 Ik\u2032 ] Pr\n( N ek\u22121\nM < Poi(N\u03c1k\u2032) < N\nek\nM\n)\n\u2264 \u2211\ni\u2208M \u03c1i Pr\n( Poi(N e(\u03c4+k)\nM ) < N\nek\nM\n)\n\u2264 2e\u2212e\u03c4+kd0/2.\nSimilarly, apply the Poisson tail bound (2) in Corollary F.5, and set c\u2032 = e\u03c4 \u2265 e for \u03c4 \u2265 1, we can bound the probability with which a word i from much lighter bins, namely \u222a{k\u2032:k\u2032<k\u2212\u03c4}Ik\u2032 , is placed in the empirical bin I\u0302k by:\nPr ( N ek\nM < Poi(N\u03c1i) \u2264 N\nek+1\nM\n) \u2264 Pr ( Poi(N e(k\u2212\u03c4)\nM ) > N\nek\nM\n) \u2264 2e\u2212ekd0 , (38)\nand bound the total marginal probability by:\nEW k \u2264 2e\u2212e kd0 .\n(2) Next, we apply Bernstein\u2019s bound to get a high probability argument. We show that with high probability, for all the log log(M) bins, we can bound the spillover probability mass by W k \u2264 E[W k] + O( 1poly(M)), which implies that asymptotically as the vocabulary size M \u2192 \u221e, we have W k \u2264 2e\u2212e\n\u03c4+kd0/2 for all k. Consider the word i from the exact bin Ik\u2032 , for some k\u2032 \u2265 k + \u03c4 . Let\n\u03bbi = 2e \u2212ek\u2032d0/2\ndenote the upper bound (as shown in (37)) of the independent probability with which word i is placed in the empirical bin I\u0302k (recall the Poisson number of samples assumption). The spillover probability mass is a random variable and can be written as\nW k = \u2211\ni\u2208Ik\u2032 :(k+\u03c4)< k\u2032 \u2264log log(M)\n\u03c1iBer(\u03bbi),\nNote that the summation of word i is over all the bin Ik\u2032 for (k + \u03c4) \u2264 k\u2032 \u2264 log log(M), where recall that in in Lemma 3.2 we showed that with high probability the heaviest words are retained in the empirical bin I\u0302log. Apply Bernstein\u2019s inequality to bound W k:\nPr(W k \u2212 EW k > t) \u2264 e \u2212 t 2/2\u2211 i \u03c1 2 i \u03bbi+maxi \u03c1it/3 .\nTo ensure that the right hand side is bounded by e\u2212 logM (this is to create space for the union bound over the log logM bins), we can fix some large universal constant C and set t to be\nt = 2 ( ( \u2211\ni\n\u03c12i\u03bbi) 1/2 + max\ni \u03c1i\n) log(M).\nwhich right hand side can be bounded by:\n(\u2211\ni\n\u03c12i\u03bbi\n)1/2 + max\ni \u03c1i \u2264\n( max\ni\u2208Ik\u2032 :(k+\u03c4)\u2264k\u2032\u2264log log(M) (\n1 \u03c1i )(\u03c12i )(2e \u2212ek\u2032d0/2)\n)1/2 + logM\nM\n\u2264 (\n2 max (k+\u03c4)\u2264k\u2032\u2264log log(M)\nek \u2032 M e\u2212e k\u2032d0/2\n)1/2 + logM\nM\n\u2264 2e \u2212ek+\u03c4d0/4 \u221a M + logM M ,\nwhere the first inequality is uses the worst case to bound the summation, and the last inequality uses the fact that d0 = \u2126(1) is a large constant. Therefore, we can set t = 2( e\u2212(e\nk+\u03c4 d0/4) logM\u221a M + (logM) 2 M ).\nFinally, take a union bound over at most log log(M) moderate bins, we argue that with high probability (at least 1\u2212O(1/M)), for all the empirical moderate bins, we can bound the spillover marginal by:\nW k \u2264 EW k +O( 1\npoly(M) ).\n(3) Moreover, assume that Wk \u2265 e\u2212k, we can bound the number of the heavy spillover words Mk compared to number of words in the exact bin Mk.\nFirst note that Mk \u2264 Wke\u03c4+k/M . Recall that dmaxk = NWk(e\u03c4+k/M) was defined in (14). Also, since Wk \u2265 e\u2212k W k \u2248 e\u2212e k+\u03c4d0 , we can lower bound the number of words in the empirical bin I\u03021 by:\nMk \u2265 Wk\nek+\u03c4/M .\nThus we can bound\nMk dmaxk Mk \u2264 ( W k ek+\u03c4/M ) (NWk(e k+\u03c4/M))\n( Wk ek+\u03c4/M )\n\u2264 ek+\u03c4d0W k \u2264 2e k+\u03c4d0\neek+\u03c4d0\n\u2264 1,\nwhere the second last inequality we used the high probability upper bound for W k, and in the last inequality we use the fact that ex > 2x for all x.\nProof. (of Lemma 3.5 (Concentration of the regularized diagonal block B\u0303k.)) In Figure 2, the rows and the columns of [BN ]I\u0302k\u00d7I\u0302k are sorted according to the exact marginal probabilities of the words in ascending order. The rows and columns that are set to 0 by regularization are shaded.\nOn the left hand side, it is the empirical matrix without regularization. We denote the removed elements by matrix E \u2208 RMk\u00d7Mk+ , whose only nonzero entries are those that are removed from in the regularization step (in the strips with orange color), namely E = [ [BN2]i,j1[i or j \u2208 R\u0302k] ] . We denote\nthe retained elements by matrix B\u0303k = [BN2]I\u0302k\u00d7I\u0302k\\E = [BN2]I\u0302k\u00d7I\u0302k \u2212 E.\nOn the right hand side it is the same block decomposition applied to the matrix which we want the regularized empirical count matrix converges to. Recall that we defined B\u0303k = \u03c1\u0303k\u03c1\u0303>k + \u2206\u0303k\u2206\u0303>k in (21), where we set entries corresponding to the words in the spillover set J\u0302k to 0. Thus by definition, the only non-zero entries of B\u0303k are equal to those in BL\u0302k\u00d7L\u0302k .\nWe bound the spectral distance of the 4 blocks (A1, A2, A3, A4) separately. The bound for the\nentire matrix B\u0302k is then an immediate result of triangle inequality:\n\u2016B\u0303k \u2212N B\u0303k\u2016 = \u2016[BN2]I\u0302k\u00d7I\u0302k \u2212 E \u2212N B\u0303k\u2016 = \u2016A1\\E +A2\\E +A3\\E +A4\\E \u2212N B\u0303k\u2016 \u2264 \u2016A1\\E \u2212NBL\u0302k\u00d7L\u0302k\u2016+ \u2016A2\\E\u2016+ \u2016A3\\E\u2016+ \u2016A4\\E\u2016.\nWe bound the 4 parts separately below in (a)-(c).\n(a) To bound \u2016A1\\E \u2212NBL\u0302k\u00d7L\u0302k\u2016, we first make a few observations:\n1. By definition of J\u0302k and I\u0302k, every entry of the random matrix A1 is distributed as an independent Poisson variable Poi(\u03bbk), where \u03bbk \u2264 N( e k+\u03c4 M ) 2 \u2264 d0 log(M)M = o(1).\n2. The expected row sum of A1 is bounded by of d max k .\n3. With the regularization of removing the heavy rows and columns in E, every column sum and the row sum of A1 is bounded by 2d max k .\nTherefore, by applying the Lemma F.6 (an immediate extension of the main theorem in [34]), we can argue that with probability at least 1\u2212M\u2212rk for some constant r = \u0398(1),\n\u2016A1\\E \u2212NBL\u0302k\u00d7L\u0302k\u20162 = O( \u221a dmaxk ).\n(b) To bound \u2016A2\\E\u2016 and \u2016A3\\E\u2016, the key observations are:\n1. Every row sum of A2\\E and every column sum of A3\\E is bounded by 2dmaxk .\n2. For every non-zero row of A2, its distribution is entry-wise dominated by a multinomial distri-\nbution Mul ( \u03c1L\u0302k\u2211 i\u2208L\u0302k \u03c1i ; (2dmaxk ) ) , while the entries in E are set to 0, and note that in A2 the columns are restricted to the good words L\u0302k. Moreover, by the Poisson assumption on N (also in the definition of dmaxk = NWk\u03c1k), we have that the distributions of the entries in the row are\nindependently dominated by Poi (\n2dmaxk Mk\n) .\nLemma B.1 (row/column-wise `1 norm to `2 norm bound (Lemma 2.5 in [34])). Consider a matrix B in which each row has L1 norm at most a and each column has L1 norm at most b, then \u2016B\u20162 \u2264 \u221a ab.\nClaim B.2 (Sparse decomposition of (A2\\E)). With high probability, the index subset J\u0302k \u00d7 L\u0302k of (A2\\E) can be decomposed into two disjoint subsets R and C such that: each row of R and each column of C has row/column sum at most (r log(dmaxk )).\nRecall that from regularization we know that each column of R and each row of C in A2\\E has column/row sum at most 2dmaxk . Therefore we can apply Lemma B.1 and conclude that with high probability\n\u2016A2\\E\u20162 \u2264 2 \u221a rdmaxk log(d max k ).\nProof. (to Claim B.2) We sketch the proof of Claim B.2, which mostly follows the sparse decomposition argument in Theorem 6.3 in [34]. We adapt their argument in our setup where the entries are distributed according to independent Poisson distributions. We first show (in (1)) that, with high probability, any square submatrix in (A2\\E) actually contains a sparse column with almost only a constant column sum; then, with this property we can (in (2)) iteratively take out sparse columns and rows from (A2\\E) to construct the R and C. (1) With high probability, in any square submatrix of size m \u00d7 m in (A2\\E), there exists a sparse column whose sum is at most (r log dmaxk ).\nTo show this, consider an arbitrary column in an arbitrary submatrix of size m \u00d7m in (A2\\E). Recall our observation (b).2, that the column sum is dominated by a Poisson with rate\n\u03bb = 2dmaxk m\nMk .\nTherefore, we can bound the column sum by applying the Chernoff bound for Poisson distribution (Lemma F.2):\nPr (a column sum > (r log dmaxk )) \u2264 Pr (Poi(\u03bb) > (r log dmaxk ))\n\u2264 e\u2212\u03bb ( r log dmaxk\ne\u03bb\n)\u2212r log dmaxk\n\u2264 (\nrMk 2dmaxk m\n)\u2212r log dmaxk\n\u2264 ( rMk 2m )\u2212r ,\nwhere in the last inequality we used the fact that for dmaxk and r to be large constant, the following simple inequality holds:\nlog(dmaxk ) log\n( rMk\n2dmaxk m\n) \u2265 log ( rMk m ) .\nThen consider all the m columns in the submatrix of size m\u00d7m, which column sums are independently dominated by Poisson distributions, we have\nPr (every column sum > (r log dmaxk ) ) \u2264 ( rMk 2m )\u2212rm .\nNext, take a union bound over all the m\u00d7m submatrices of (A2\\E) for m ranging between 1 and Mk, and recall that block (A2\\E) is of size Mk \u00d7 (Mk \u2212Mk). We can bound for all the submatrices that:\nPr (for every submatrix in (A2\\E), there exist a column whose sum \u2264 (r log dmaxk ))\n\u22651\u2212 Mk\u2211\nm=1\n( Mk m )( Mk m ) ( rMk 2m )\u2212rm\n\u22651\u2212 Mk\u2211\nm=1\n( Mk m )2m(rMk 2m )\u2212rm\n\u22651\u2212M\u2212(r\u22122)k . (39)\nNote that this is indeed a high probability event, since for Wk \u2265 0e\u2212k, we have shown that Mk \u2265 Me\u22122k+\u03c4 .\n(2) Perform iterative row and column deletion to construct R and C. Given (A2\\E) of size Mk \u00d7Mk, we apply the argument above in (1) iteratively. First select a sparse column and remove it to C, and apply it to remove columns until the remaining number of columns and rows are equal, then apply it alternatively to the rows (move to R) and columns (move to C) until empty. By construction, there are at most Mk such sparse columns in C, each column of C has sum bounded by (r log dmaxk ), and each row of C bounded by 2dmaxk because it is in the regularized (A2\\E); similarly R has at most Mk rows and each row of R has sum at most (r log dmaxk ) and each column has sum at most 2dmaxk .\nThe proof for the other narrow strip (A3\\E) is in parallel with the above analysis for (A2\\E).\n(c) To bound \u2016A4\\E\u2016, the two key observation are:\n1. The total marginal probability mass of spillover heavy words W k = \u2211\ni\u2208J\u0302k \u03c1i \u2264 2e \u2212ek+\u03c4d0 .\n(shown in Lemma 3.4).\n2. Similar to the observation in ((b)).2 above, the distributions of the entries in each row of (A4\\E) are independently dominated by a Poisson variable Poi ( 2dmaxk Wk Wk 1 Mk ) .\nIn parallel with Claim B.2, we make a claim about the spectral norm of the block (A4\\E): Claim B.3 (Sparse decomposition of (A4\\E)). With high probability, the index subset J\u0302k \u00d7 J\u0302k of A2 can be decomposed into two disjoint subsets R and C such that: each row of R and each column of C has sum at most r; each column of R and each row of C has sum at most dmaxk .\nProof. (to Claim B.3) To show this, we construct sparse decomposition similar to that of (A2\\E). The only difference is that, when considering all the m\u00d7m submatrices, we only need to consider all the submatrices contained in the small square (A4\\E) of size J\u0302k \u00d7 J\u0302k, instead of all submatrices\nin the wide strip (A2\\E) of size L\u0302k \u00d7 J\u0302k. In this case, taking the union bound leads to factors of Mk, compared to that of Mk in (39).\nHere we only highlight the difference in the inequalities. Consider an arbitrary column in an arbitrary submatrix of size m\u00d7m in (A4\\E). Recall the observation that this column sum is dominated by a Poisson with rate\n\u03bb = 2dmaxk W k Wk m Mk .\nThus we can bound the probability of having a dense column by:\nPr(a column sum > r) \u2264 Pr(Poi(\u03bb) > r) \u2264 e\u2212\u03bb( r e\u03bb )\u2212r \u2264 ( r e\u03bb )\u2212r.\nTake a union over all the square matrices of size m\u00d7m in block (A4\\E), we can bound:\nPr(for every submatrix in (A4\\E), there exist a column whose sum \u2264 r)\n\u22651\u2212 Mk\u2211\nm=1\n( Mk m )2 ( r e\u03bb )\u2212rm\n\u22651\u2212 Mk\u2211\nm=1\n( Mk m )2m( Mk m\nrWk\ne2dmaxk W k\n)\u2212rm\n\u22651\u2212M\u2212(r\u22122)k ,\nwhere in the last inequality we used the fact that dmaxk = NWk ek+\u03c4 M , and plug in the high probability upper bound of W k \u2264 2e\u2212e k+\u03c4d0 as in (18), we have:\nrWk\ne2dmaxk W k =\nrWkM\n2eNWkek+\u03c4e\u2212e k+\u03c4d0\n= re(e\nk+\u03c4d0)\n2e(ek+\u03c4d0) 1.\nAgain note that given that the bin has significant total marginal probability, thus Mk \u2265 Me\u22122k, the above probability bound is indeed a high probability statement.\nProof. (to Lemma 3.6 (Given spectral concentration of block B\u0303k, estimate the separation \u2206\u0303k )) Recall the result of Lemma 3.5 about the concentration of the diagonal block with regularization. For empirical bin with large enough marginal Wk, we have with high probability,\n\u2225\u2225\u2225\u2225 1\nN B\u0303k \u2212 B\u0303k \u2225\u2225\u2225\u2225 2 \u2264 C \u221a dmaxk log 2 dmaxk N .\nAlso recall that \u03c1\u0302I\u0302k is defined to be the exact marginal vector restricted to the empirical bin I\u0302k. We can also bound\n\u2225\u2225\u2225\u2225( 1\nN B\u0303k \u2212 \u03c1\u0302I\u0302k \u03c1\u0302 > I\u0302k\n)\u2212 \u2206\u0303k\u2206\u0303>k \u2225\u2225\u2225\u2225\n2\n\u2264 \u2225\u2225\u2225\u2225( 1\nN B\u0303k \u2212 \u03c1\u0302I\u0302k \u03c1\u0302 > I\u0302k\n)\u2212 (B\u0303k \u2212 \u03c1\u0303k\u03c1\u0303>k ) \u2225\u2225\u2225\u2225\n2 \u2264 \u2225\u2225\u2225\u2225 1\nN B\u0303k \u2212 B\u0303k \u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225\u03c1\u0302I\u0302k \u03c1\u0302 > I\u0302k \u2212 \u03c1\u0303k\u03c1\u0303>k \u2225\u2225\u2225 2\n\u2264 C \u221a dmaxk log\n2 dmaxk N .\nNote that in the last inequality above we ignored the term \u2016\u03c1\u0302I\u0302k \u2212 \u03c1\u0303k\u20162 as it is small for all bins (with large probability):\n\u2225\u2225\u2225\u03c1\u0302I\u0302k \u03c1\u0302 > I\u0302k \u2212 \u03c1\u0303k\u03c1\u0303>k \u2225\u2225\u2225 2 \u2264 4 \u2225\u2225\u2225\u03c1\u0302I\u0302k \u2212 \u03c1\u0303k \u2225\u2225\u2225 2 \u2225\u2225\u2225\u03c1\u0302I\u0302k \u2225\u2225\u2225 2 \u2264 \u221a\u221a\u221a\u221a\u03c1k(Mk/N)\ufe38 \ufe37\ufe37 \ufe38\nover L\u0302k + \u03c12k(W k/\u03c1k)\ufe38 \ufe37\ufe37 \ufe38 over J\u0302k\n\u221a Mk\u03c1 2 k\n\u2264\n\u221a M2k\u03c1k 3 N \u2264 \u221a dmaxk N ,\nwhere in the second inequality we write \u2225\u2225\u2225\u03c1\u0302I\u0302k \u2212 \u03c1\u0303k \u2225\u2225\u2225 2 2 into two parts over the set of good words L\u0302k and the set of bad words J\u0302k. To bound the sum over L\u0302k we used the Markov inequality as in the proof of Lemma 3.1; and to bound the sum over J\u0302k as well as the term \u2225\u2225\u2225\u03c1\u0302I\u0302k \u2225\u2225\u2225 2 2 we used the fact that if a word i appears in I\u0302k, we must have \u03c1\u0302i \u2264 \u03c1k. The last inequality is due to M2k\u03c13k \u2264W 2k \u03c1k \u2264Wk\u03c1k = dmaxk /N . Let vkv > k be the rank-1 truncated SVD of the regularized block (B\u0303k \u2212 \u03c1\u0302k\u03c1\u0302>k ). Apply Wedin\u2019s theorem to rank-1 matrix (Lemma F.1), we can bound the distance between vector vk and \u2206\u0303k by:\nmin { \u2016\u2206\u0303k \u2212 vk\u2016, \u2016\u2206\u0303k + vk\u2016 } = O(min{\n\u221a dmaxk log 2 dmaxk /N\n\u2016\u2206\u0303k\u2016 , ( \u221a dmaxk log 2 dmaxk /N) 1/2})."}, {"heading": "C Proofs for Algorithm Phase II (Section 4)", "text": "Proof. (to Lemma 4.2 (Sufficient condition for constructing an anchor partition)) (1) First, we show that if for some constant c = \u2126(1), a set of words A satisfy\n\u2223\u2223\u2223\u2223\u2223 \u2211\ni\u2208A \u2206i\n\u2223\u2223\u2223\u2223\u2223 \u2265 c\u2016\u2206\u20161, (40)\nthen (A, [M ]\\A) is a pair of anchor set defined in 4.1. By the assumption of constant separation (C\u2206 = \u2126(1)), \u2211 i\u2208A\u2206i = \u2126(1). We can bound the condition number of the anchor partition matrix by:\ncond\n([ \u03c1A, \u2206A\n1\u2212 \u03c1A, \u2212\u2206A\n]) = \u221a T 2 \u2212 4D + T\u221a T 2 \u2212 4D \u2212 T \u2264 \u221a 1 + 4cC\u2206 + 1\u221a 1 + 4cC\u2206 \u2212 1 = \u2126(1),\nwhere T = \u03c1A \u2212\u2206A \u2264 1 and D = \u2212\u03c1A\u2206A \u2212 (1\u2212 \u03c1A)\u2206A = \u2212\u2206A. (2) Next we show that A\u0302 defined in the lemma statement satisfies (40).\nDenote A\u2217 = {i \u2208 I : \u2206i > 0}. Note that \u2016\u2206I\u20161 = \u2211 i\u2208A\u2217 \u2206i \u2212 \u2211 i\u2208I\\A\u2217 \u2206i. Without loss\nof generality we assume that \u2211\ni\u2208A\u2217 \u2206i \u2265 12\u2016\u2206I\u20161 \u2265 12C\u2016\u2206\u20161, where the last inequality is by the condition in (26).\nGiven \u2206\u0302I that satisfies (27). We look at A\u0302 = {i \u2208 I : \u2206\u0302i > 0}. \u2211\ni\u2208A\u0302\n\u2206i = \u2211\ni\u2208A\u0302\u2229A\u2217 \u2206i \u2212\n\u2211\ni\u2208A\u0302\u2229(I\\A\u2217)\n\u2206i\n= \u2211\ni\u2208A\u2217 \u2206i \u2212\n\u2211\ni\u2208(A\u0302\u2229(I\\A\u2217))\u222a(A\u2217\u2229(I\\A\u0302))\n|\u2206i|\n\u2265 \u2211\ni\u2208A\u2217 \u2206i \u2212 \u2016\u2206\u0302I \u2212\u2206I\u20161\n\u2265 (1 2 C \u2212 C \u2032)\u2016\u2206\u20161 \u2265 1 6 CC\u2206,\nwhere in the second last inequality we used the fact that, if the sign of \u2206\u0302i and \u2206i are different, it must be that |\u2206\u0302i \u2212\u2206i| > |\u2206i|.\nProof. (to Lemma 4.4 (Estimate the separation restricted to the k-th good bin)) Since it is a good bin, we have the `2 bound given by Lemma 3.6 as below (assuming the possible sign flip has been fixed as in Lemma 3.8):\n\u2016\u2206\u0303k \u2212 vk\u20162 \u2264 \u221a dmaxk log\n2 dmaxk N\n1\n\u2016\u2206\u0303k\u20162 .\nThen we can convert the bound to `1 distance by:\n\u2016vk \u2212 \u2206\u0303k\u20161 \u2016\u2206\u0303k\u20161 \u2264 \u221a Mk\u2016vk \u2212 \u2206\u0303k\u20162 \u2016\u2206\u0303k\u20161 \u2264 \u221a Mk\u2016vk \u2212 \u2206\u0303k\u20162\u2016\u2206\u0303k\u20162 \u2016\u2206\u0303k\u20162\u2016\u2206\u0303k\u20161\n\u2264 Mk\u2016vkv > k \u2212 \u2206\u0303k\u2206\u0303>k \u2016 \u2016\u2206\u0303k\u201621 \u2264 CMk W 2k \u221a dmaxk log2(d0) N\n\u2264 CMk W 2k\ne\u03c4 \u221a NWk\nWk Mk\nlog2(d0)\nN\n\u2264 Ce\u03c4 \u221a M log2(d0)\nNWkek = O(\n\u221a log(d0)\nd0 ),\nwhere in the second last inequality, we used the fact that Mk ek M \u2264Wk again, and in the last inequality we used the assumption Wk \u2265 0/ek.\nProof. (to Lemma 4.5 (With \u2126(1) separation, most words fall in \u201cgood bins\u201d with high probability)) This proof is mostly playing around with the probability mass and converting something obviously true in expectation to high probability argument.\n(1) Note that by their definition we know that Wk \u2265 12Sk, and we have \u2211\nk\nWk\n( 1[\nSk 2Wk \u2265 C2] + Sk 2Wk 1[ Sk 2Wk < C2]\n)\n\u2265 \u2211\nk\nWk Sk\n2Wk\n( 1[\nSk 2Wk \u2265 C2] + 1[ Sk 2Wk < C2]\n)\n= \u2211\nk\nWk Sk\n2Wk\n= 1\n2\n\u2211\nk\nSk,\nMoreover, note that by definition of Wk, we have \u2211 kWk = 1, therefore\n\u2211\nk\nWk Sk\n2Wk 1[ Sk 2Wk\n< C2] \u2264 \u2211\nk\nC2Wk = C2.\nFrom the above two inequalities we can bound\n\u2211\nk\nWk1[ Sk\n2Wk \u2265 C2] \u2265\n1\n2\n\u2211\nk\nSk \u2212 C2.\nAlso note that\n\u2211\nk\nWk1[Wk < C1 2k ] \u2264 C1.\nTherefore according to the definition of \u201cgood bins\u201d we have that:\n\u2211 k\u2208G Wk = \u2211 k Wk1\n[ Sk\n2Wk \u2265 C2 and Wk \u2265 C1 2k\n] \u2265 1\n2\n\u2211\nk\nSk \u2212 C2 \u2212 C1. (41)\n(2) We want to lower bound the quantity \u2211\nk Sk to be a constant fraction of \u2016\u2206\u20161. Note that by definition of Sk in (28) we can equivalently write the sum as:\n\u2211\nk\nSk = \u2211\nk\n\u2211\ni\u2208I\u0302k\u2229(\u222a{k\u2032:k\u2032\u2264k+\u03c4}Ik\u2032 )\n|\u2206i| = \u2211\ni\n|\u2206i| \u2211\nk\n1[i \u2208 Ik \u2229 \u222a{k\u2032:k\u2032\u2264k+\u03c4}I\u0302k\u2032 ].\nConsider for each word i. Assume that word i \u2208 Ik. Given N = d0M for some large constant d0, denote dk = e kd0, we can bound the probability Pr(i \u2208 Ik \u2229 \u222a{k\u2032:k\u2032\u2264k+\u03c4}I\u0302k\u2032) as follows:\nPr(i \u2208 Ik \u2229 \u222a{k\u2032:k\u2032\u2264k+\u03c4}I\u0302k\u2032) \u2265 1\u2212 Pr(Poi(N\u03c1i) > e\u03c4\u03c1i)\u2212 Pr(Poi(N\u03c1i) < e\u2212\u03c4N\u03c1i/2)\n\u2265 1\u2212 e \u2212(\u03c4\u22121)e(\u03c4+k)d0 \u221a\n2\u03c0ek+\u03c4d0 \u2212 e\u2212dk\n\u2265 1\u2212 2e\u2212dk .\nTherefore at least in expectation we can lower bound the sum by\nE[ \u2211\nk\nSk] = \u2211\ni\n|\u2206i| \u2211\nk\nPr(i \u2208 Ik \u2229 \u222a{k\u2032:k\u2032\u2264k+\u03c4}I\u0302k\u2032) \u2265 (1\u2212 2e\u2212d0)\u2016\u2206\u20161.\n(3) Restrict to the exact good bins, for which we know that the exact \u2016\u03c1I\u0302k\u20161 \u2265 e \u2212k and \u2016\u2206I\u0302k\u20161/\u2016\u03c1Ik\u20161 \u2265 C. Here we know that if I\u0302k is an good bin, the number of words in this exact good bin is lower bounded by Mk \u2265 e\u2212k/\u03c1k \u2265 M/e\u2212k, and since k \u2264 log logM we have that Mk \u2265 Mlog(M) . This is important for use to apply Bernstein concentration of the words in the bin.\nSince \u2016\u2206I\u0302k\u20161/\u2016\u03c1I\u0302k\u20161 \u2265 C, and that |\u2206i| \u2264 \u03c1i, we have that out of the Mk words there are at least a constant fraction of words with |\u2206i| \u2265 12C\u03c1k. Recall that we denote \u03c1k = ek/M . This is easy to see as x\u03c1k + (Mk \u2212 x)12C\u03c1k \u2265 \u2016\u2206I\u0302k\u20161 \u2265 CMk\u03c1k thus x \u2265 C/2\u2212 CMk.\nThen we bound the probability that out of these cMk words with good separation, a constant fraction of them do not escape from the closest \u03c4 empirical bins. Denote \u03bbk = 2e\n\u2212dk , which is the upper bound of the escaping probability for each of the word, and is very small. By a simple application of Bernstein bounds of the Bernoulli sum, for a small constant c0, we have\nPr( \u2211\ni=1,...cMk\nBeri(\u03bbk) \u2265 c0Mk) \u2264 Pr( \u2211\ni=1,...cMk\nBeri(\u03bbk)\u2212 \u03bbkMk \u2265 (c0 \u2212 \u03bbk)Mk)\n\u2264 e \u2212\n1 2 (c0\u2212\u03bbk) 2M2k Mk\u03bbk+ 1 3 (c0\u2212\u03bbk)Mk\n\u2248 e\u2212c0Mk .\nThen union bound over all the exact good bins. That gives a log logM multiply of the probability. We now know that restricting to the non-escaping good words in the exact good bins, they already contribute a constant fraction (due to constant non-escaping, constant ratio \u2016\u2206I\u0302k\u20161/\u2016\u03c1I\u0302k\u20161, and constant \u2211 k\u2208exact good binWk) of the total separation \u2016\u2206\u20161. Therefore we can conclude that for some universal constant C we have \u2211\nk\nSk \u2265 C\u2016\u2206\u20161.\n(4) Finally plug the above bound of \u2211\nk Sk into (41), and note the assumption on the constants C1 and C2, we can conclude that the total marginal probability mass contained in \u201cgood bins\u201d is large:\n\u2211 k\u2208G Wk \u2265 (C \u2212 1 24 \u2212 1 24 )\u2016\u2206\u20161 = 1 12 \u2016\u2206\u20161.\nProof. (to Lemma 4.6 (Estimate \u03c1 and \u2206 with accuracy in `2 distance)) Consider word i we have that\n[ \u03c1A, \u2206A\n1\u2212 \u03c1A, \u2212\u2206A ] [ \u03c1i \u2206i ] = [ \u2211 j\u2208A Bj,i\u2211 j\u2208Ac Bj,i ]\nSet\n[ \u03c1\u0302i \u2206\u0302i ] = [ \u03c1A, \u2206A 1\u2212 \u03c1A, \u2212\u2206A ]\u22121 [ 1 N \u2211 j\u2208A[BN ]j,i 1 N \u2211 j\u2208Ac [BN ]j,i ]\nSince 1N \u2211 j\u2208A[BN ]j,i \u223c 1NPoi(N(\u03c1A\u03c1i + \u2206A\u2206i)), apply Markov inequality, we have that\nPr( \u2211\ni\n( 1\nN\n\u2211 j\u2208A [BN ]j,i \u2212 \u2211 j\u2208A Bj,i)2 > 2) \u2264 1 N\n\u2211 i(\u03c1A\u03c1i + \u2206A\u2206i)\n2 = \u03c1A N 2\nNote that \u03c1A = \u2126(1) and that cond(DA) = \u2126(1), we can propagate the concentration to the estimation error of \u03c1 and \u2206 as, for some constant C = \u2126(1),\nPr(\u2016\u03c1\u0302\u2212 \u03c1\u2016 > ) \u2264 C N 2 , Pr(\u2016\u2206\u0302\u2212\u2206\u2016 > ) \u2264 C N 2 ."}, {"heading": "D Proofs for HMM testing lower bound (Section 5.2)", "text": "Proof. (to Theorem 5.1)\nTV (Pr1(G N 1 ),Pr2(G N 1 )) \u2264 TV (Pr1(GN1 ,A),Pr2(GN1 ,A)) (42)\n= 1\n2\n\u2211\nGN1 \u2208[M ]N ,A\u2208( M M/2)\n\u2223\u2223Pr2(GN1 ,A)\u2212 Pr1(GN1 ,A) \u2223\u2223\n= 1\n2\n\u2211\nGN1 \u2208[M ]N ,A\u2208( M M/2)\nPr1(G N 1 ,A) \u2223\u2223\u2223\u2223 Pr2(G N 1 ,A) Pr1(GN1 ,A) \u2212 1 \u2223\u2223\u2223\u2223\n(a) \u2264 1 2 \u221a\u221a\u221a\u221a\u221a \u2211\nGN1 \u2208[M ]N ,A\u2208( M M/2)\nPr1(GN1 ,A) (\nPr2(GN1 ,A) Pr1(GN1 ,A)\n\u2212 1 )2\n= 1\n2\n\u221a\u221a\u221a\u221a\u221a \u2211\nGN1 \u2208[M ]N ,A\u2208( M M/2)\nPr1(GN1 ,A) (\nPr2(GN1 ,A) Pr1(GN1 ,A)\n)2 \u2212 1\n(b) =\n1\n2 \u221a\u221a\u221a\u221a\u221a ( MN( M M/2 ) )2 \u2211\nGN1 \u2208[M ]N ,A\u2208( M M/2)\nPr1(GN1 ,A) ( \u2211\nB\u2208( MM/2)\nPr2(GN1 |B) )2 \u2212 1\n(c) =\n1\n2 \u221a\u221a\u221a\u221a\u221a\u221a\u221a MN ( M M/2 )2 \u2211 GN1 \u2208[M ]N ( \u2211 B\u2208( MM/2) Pr2(G N 1 |B) )2\n\ufe38 \ufe37\ufe37 \ufe38 Y\n\u22121, (43)\nwhere inequality (a) used the fact that E[X] \u2264 (E[X2])1/2; equality (b) used the joint distributions in (32) and (35); and equality (c) takes sum over the summand A first and makes use of the marginal probability Pr1(G N 1 ) as in (34).\nIn order to bound the term Y = M N\n( MM/2) 2\n\u2211 GN1 \u2208[M ]N (\u2211 B\u2208( MM/2) Pr2(G N 1 |B) )2 in equation 43, we\nbreak the square and write:\nY = MN ( M M/2 )2 \u2211\nB,B\u2032\u2208( MM/2)\n\u2211\nGN1 \u2208[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B\u2032). (44)\nIn Claim D.1 below we explicitly compute the term \u2211\nGN1 \u2208[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B\u2032) for any two\nsubsets B,B\u2032 \u2208 ( M M/2 ) . Then in Claim D.2 we compute the sum over B,B\u2032 and bound Y \u2264 \u221a 2\n2\u2212 4 3 N M\n.\nTo conclude, we can bound the total variation distance as:\nTV (Pr1(G N 1 ),Pr2(G N 1 )) \u2264\n1\n2\n\u221a\u221a\u221a\u221a \u221a\n2\n2\u2212 43 NM \u2212 1.\nTherefore TV (Pr1(G N 1 ),Pr2(G N 1 )) = o(1) if N = o(M).\nClaim D.1. In the same setup of Theorem 5.1, given two subsets B,B\u2032 \u2208 M and |B| = |B\u2032| = M/2, let B =M\\B,B\u2032 =M\\B\u2032 denote the corresponding complement. Define x to be:\nx = |B \u2229 B\u2032| M/2 . (45)\nNote that we have 0 \u2264 x \u2264 1. Also define a1 = 1+(1\u22122t) 2 4 and a2 = ( 4(1\u22122t) 1+(1\u22122t)2 )2 as functions of the transition probability t. We have:\n\u2211\nGN1 \u2208[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B\u2032) =\n1\n(M/2)N\n( \u03b3N1 (1\u2212 2\u03b32)\u2212 \u03b3N2 (1\u2212 2\u03b31)\n2(\u03b31 \u2212 \u03b32)\n) ,\nwhere \u03b31 = a1\n( 1 + \u221a 1\u2212 x(1\u2212 x)a2 ) , \u03b32 = a1 ( 1\u2212 \u221a 1\u2212 x(1\u2212 x)a2 ) are functions of |B \u2229B\u2032| and t.\nProof. (to Claim D.1) (1) For an instance of 2-state HMM specified by B \u2208 ( M M/2 ) , consider two consecutive outputs (gn\u22121, gn). We first show how to compute the probability Pr2(gn|gn\u22121,B). Given B and B\u2032, we can partition the vocabulary M into four subsets as:\nM1 = B \u2229 B\u2032, M2 = B \u2229 B\u2032, M3 = B \u2229 B\u2032, M4 = B \u2229 B\u2032.\nNote that we have |M1| = |M4| = xM/2 and |M2| = |M3| = (1\u2212 x)M/2. Define a subset of tuples JB \u2282 [4]2 to be\nJB = {(1, 1), (1, 2), (2, 1), (2, 2), (3, 3), (3, 4), (4, 3), (4, 4)}.\nIf gn\u22121 \u2208 Mj\u2032 , gn \u2208 Mj and (j\u2032, j) \u2208 JB, we know that the hidden state does not change, namely sn\u22121 = sn, and that Pr2(gn|gn\u22121,B) = Pr2(sn|sn\u22121,B)Pr2(gn|sn,B) = 1\u2212tM/2 . Also, if (j\u2032, j) \u2208 J cB \u2261 [4]2\\JB, we know that there is the state transition and we have Pr2(gn|gn\u22121,B) = tM/2 .\nSimilarly, for the 2-state HMM specified by B\u2032, we can define\nJB\u2032 = {(1, 1), (1, 3), (3, 1), (3, 3), (2, 2), (2, 4), (4, 2), (4, 4)},\nand Pr2(gn|gn\u22121,B\u2032) = 1\u2212tM/2 if (gn\u22121 \u2208Mj\u2032 , gn \u2208Mj) with (j\u2032, j) \u2208 JB\u2032 and equals tM/2 if (j\u2032, j) \u2208 J cB\u2032 . (2) Next, we show how to compute the target sum of the claim statement in a recursive way.\nDefine Fn,j for n \u2264 N and j = 1, 2, 3, 4 as below\nFn,j = \u2211\nGn1\u2208[M ]n Pr2(G\nn 1 |B)Pr2(Gn1 |B\u2032)1[gn \u2208Mj ],\nand the target sum is \u2211\nGN1 \u2208[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B\u2032) = \u2211 j=1:4 FN,j . Also, we have that\nF1,j = |Mj |/M2.\nMaking use of the recursive property of the probability rule of the 2-state HMM as in (33), we can write the following recursion in terms of Fn,j for n \u2265 2:\nFn,j = \u2211\nGn1\u2208[M ]n Pr2(G\nn 1 |B)Pr2(Gn1 |B\u2032)1[gn \u2208Mj ]\n= \u2211\nGn1\u2208[M ]n Pr2(G\nn\u22121 1 |B)Pr2(Gn\u221211 |B\u2032)Pr2(gn|Gn\u221211 ,B)Pr2(gn|Gn\u221211 ,B\u2032)\n\u2211\nj\u2032=1:4\n1[gn\u22121 \u2208Mj\u2032 , gn \u2208Mj ]\n= \u2211\nGn\u221211 \u2208[M ]n\u22121 Pr2(G\nn\u22121 1 |B)Pr2(Gn\u221211 |B\u2032)\n\u2211\ngn\u2208[M ]\n\u2211\nj\u2032=1:4\n1[gn\u22121 \u2208Mj\u2032 , gn \u2208Mj ]Pr2(gn|gn\u22121,B)Pr2(gn|gn\u22121,B\u2032)\n= |Mj | \u2211\nj\u2032=1:4\nFn\u22121,j\u2032 ( 1\u2212 t M/2 1[(j\u2032, j) \u2208 JB] + t M/2 1[(j, j\u2032) \u2208 J cB] )( 1\u2212 t M/2 1[(j\u2032, j) \u2208 JB\u2032 ] + t M/2 1[(j, j\u2032) \u2208 J cB\u2032 ] )\nwhere we used the probability Pr2(gn|gn\u22121,B) derived in (1). Equivalently we can write the recursion as:\n \nFn,1 Fn,2 Fn,3 Fn,4\n  = 1\nM/2 DxT\n \nFn\u22121,1 Fn\u22121,2 Fn\u22121,3 Fn\u22121,4\n  ,\nfor diagonal matrix Dx =   x 1\u2212 x\n1\u2212 x x\n  and the symmetric stochastic matrix T given by\nT =  \n(1\u2212 t)2 (1\u2212 t)t (1\u2212 t)t t2 (1\u2212 t)t (1\u2212 t)2 t2 (1\u2212 t)t (1\u2212 t)t t2 (1\u2212 t)2 (1\u2212 t)t t2 (1\u2212 t)t (1\u2212 t)t (1\u2212 t)2\n  = 4\u2211\ni=1\n\u03bbiviv > i ,\nwhere the singular values and singular vectors of T are specified as follows: \u03bb1 = 1, \u03bb4 = (1 \u2212 2t)2, and v1 = 1 2 [1, 1, 1, 1] >, v4 = 1 2 [1,\u22121,\u22121, 1]>. And \u03bb2 = \u03bb3 = 1 \u2212 2t with v2 = 1\u221a2 [0, 1,\u22121, 0] > and v2 = 1\u221a 2 [1, 0, 0,\u22121]>.\nNote that we can write (F1,1, F1,2, F1,3, F1,4) > = M/2\nM2 Dx(1, 1, 1, 1)\n>.\n(3) Finally we can compute the target sum as:\n\u2211\nGN1 \u2208[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B\u2032) =\n( 1 1 1 1 ) ( Fn,1 Fn,2 Fn,3 Fn,4 )>\n= ( 1 1 1 1 ) 1\n(M/2)N\u22121 (DxT )\nN\u22121M/2 M2 Dx ( 1 1 1 1 )>\n(a) =\n1\n(M/2)N v>1 (DxT ) Nv1\n= 1\n(M/2)N (1 0)\n( 1/2 (1\u2212 2t)2(x\u2212 1/2)\n(x\u2212 1/2) (1\u2212 2t)2/2\n)\n\ufe38 \ufe37\ufe37 \ufe38 H(x)N\nN\n(1 0)>\n(b) =\n1\n(M/2)N\n( \u03b31(x)\u03b32(x)\nN \u2212 \u03b32(x)\u03b31(x)N \u03b31(x)\u2212 \u03b32(x) + 1 2 \u03b31(x) N \u2212 \u03b32(x)N \u03b31(x)\u2212 \u03b32(x)\n)\n= 1 (M/2)N \u03b3N1 (1\u2212 2\u03b32)\u2212 \u03b3N2 (1\u2212 2\u03b31) 2(\u03b31 \u2212 \u03b32) ,\nwhere in (a) we used the fact that DxTv1 = (x \u2212 1/2)v1 + v4/2 and DxTv4 = (1 \u2212 2t)2(v1/2 + (x \u2212 1/2)v4); in (b) we used the Calley-Hamilton theorem to obtain that for 2 \u00d7 2 matrix H(x) parameterized by x and with 2 distinct eigenvalue \u03b31(x) and \u03b32(x), its power can be written as H(x)N = \u03b31(x)\u03b32(x) N\u2212\u03b32(x)\u03b31(x)N\n\u03b31(x)\u2212\u03b32(x) I2\u00d72 + \u03b31(x)N\u2212\u03b32(x)N \u03b31(x)\u2212\u03b32(x) H(x). Moreover, the distinct eigenvalues of the\n2\u00d7 2 matrix H(x) can be written explicitly as follows:\n\u03b31(x) = 1 + (1\u2212 2t)2\n4\n 1 + \u221a 1\u2212 x(1\u2212 x) ( 4(1\u2212 2t) 1 + (1\u2212 2t)2 )2   ,\n\u03b32(x) = 1 + (1\u2212 2t)2\n4\n 1\u2212 \u221a 1\u2212 x(1\u2212 x) ( 4(1\u2212 2t) 1 + (1\u2212 2t)2 )2   ,\nwhere recall that we defined x = |B\u2229B \u2032|\nM/2 so 0 \u2264 x \u2264 1, also we have the transition probability 0 < t < 1/2 to be a constant, therefore we have \u03b31 > \u03b32 to be two distinct real roots.\nThe next claim makes use of the above claim and bounds the right hand side of equation 44.\nClaim D.2. In the same setup of Theorem 5.1, we have\nY = MN ( M M/2 )2 \u2211\nB,B\u2032\u2208( MM/2)\n\u2211\nGN1 \u2208[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B\u2032) \u2264\n\u221a 2\n2\u2212 43 NM .\nProof. (to Claim D.2)\nDefine f(x) = \u03b31(x) N (1\u22122\u03b32(x))\u2212\u03b32(x)N (1\u22122\u03b31(x))\n2(\u03b31(x)\u2212\u03b32(x)) with \u03b31(x) = a1\n( 1 + \u221a 1\u2212 x(1\u2212 x)a2 ) and \u03b32(x) =\na1\n( 1\u2212 \u221a 1\u2212 x(1\u2212 x)a2 ) as functions of x. Recall that x = |B\u2229B \u2032| M/2 , and a1 =\n1+(1\u22122t)2 4 , a2 =(\n4(1\u22122t) 1+(1\u22122t)2\n)2 are constants determined solely by transition probability t.\nUse the result of Claim D.2 we have:\nY = MN ( M M/2 )2 \u2211\nB,B\u2032\u2208( MM/2)\n1\n(M/2)N f(x)\n(a) = MN ( M M/2 )2 1 (M/2)N\n( M\nM/2\n)M/2\u2211\ni=1\n( M/2\ni\n)2 f ( i\nM/2\n)\n= 2N( M M/2 ) M/2\u2211 i=1\n( M/2\ni\n)2 f ( i\nM/2\n) , (46)\nwhere equality (a) is obtained by counting the number of subsets B,B\u2032 \u2208 ( M M/2 ) with i overlapping entries. Next we approximately bound Y when M is asymptotically large. First, we can bound \u03b31(x) as:\n\u03b31(x) = a1\n( 1 + \u221a 1\u2212 x(1\u2212 x)a2 )\n= a1(1 + \u221a\n(1\u2212 a2/4) + a2(1/2\u2212 x)2) \u2264 C + (1\u2212 2t)|1/2\u2212 x| \u2264 2c(1/2\u2212x)2 ,\nwhere C and c are small constants. Then we bound f(x) for x = iM/2 :\nlim M\u2192\u221e\nf(x) \u2264 \u03b31(x)N (1\u2212 2\u03b32(x))\n2(\u03b31(x)\u2212 \u03b32(x)) \u2264 2c(1/2\u2212x)2N ,\nwhere we used the fact that \u03b31 \u2264 1/2 and that (1\u22122\u03b32(x))2(\u03b31(x)\u2212\u03b32(x)) \u2264 CC. Second, we use Stirling\u2019s approximation for the combinatorial coefficients ( M/2 i )2 and ( M M/2 ) , we have (the entropy function is given by H2(x) = x log2(x) + (1\u2212 x) log2(1\u2212 x)) ( M\nM/2\n) \u2248 2 M\n\u221a \u03c0M/2 ,\n( M/2\ni\n)2 \u2248 ( 2 (M/2)H2( i M/2 ) )2 \u2264 ( 2 1\u22122( i M/2 \u2212 1 2 )2 )M ,\nFinally we can approximately bound Y in (46) as follows:\nY \u2248 2N\u2212M \u221a \u03c0M/2 M/2\u2211\ni=1\n( M/2\ni\n)2 f ( i\nM/2\n)\n\u2264 2N\u2212M \u221a \u03c0M/2M/2\n\u222b 1/2\nx=\u22121/2 2(1\u22122x 2)M+cx2N\n= \u221a \u03c0M/2M/22N\n\u222b 1/2\nx=\u22121/2 2\u2212x 22M\u2212cN\n=\n\u221a 2\n2\u2212 43 NM ,\nwhere in the last equality we take the limit that M \u2192\u221e and set t = 1/4."}, {"heading": "E Analyze truncated SVD", "text": "The reason that truncated SVD does not concentrate at the optimal rate is as follows. What truncated SVD actually optimizes is the spectral distance from the estimator to the empirical average (minimizing \u2016B\u0302 \u2212 1NBN\u20162), yet not to the expected matrix B. It is only \u201coptimal\u201d in some very special setup, for example when ( 1NBN \u2212 B) are entry-wise i.i.d. Gaussian. In the asymptotic regime when N \u2192 \u221e it is indeed true that under mild condition any sampling noise converges to i.i.d Gaussian. However in the sparse regime where N = \u2126(M), the sampling noise from the probability matrix is very different from additive Gaussian noise.\nClaim E.1 (Truncated SVD has sample complexity super linear). In order to achieve accuracy, the sample complexity of rank-2 truncated SVD estimator is in given by N = O(M2 logM).\nExample 1: a = b = w = 1/2, dictionary given by\np = [ 1 + C\u2206 M , . . . , 1 + C\u2206 M , 1\u2212 C\u2206 M , . . . , 1\u2212 C\u2206 M ] ,\nq = [ 1\u2212 C\u2206 M , . . . , 1\u2212 C\u2206 M , 1 + C\u2206 M , . . . , 1 + C\u2206 M ] .\nSample complexity is O(M logM). Example 2: modify Example 1 so that a constant fraction of the probability mass lies in a common word, namely p1 = q1 = 1/2\u03c11 = 0.1, while the marginal probability as well as the separation in all the other words are roughly uniform. Sample complexity is O(M2 logN).\nProof. (to Claim E.1 (Truncated SVD has sample complexity super linear)) (1) We formalize this and examine the sample complexity of t-SVD by applying Bernstein matrix inequality. The concentration of the empirical average matrix at the following rate:\nPr(\u2016 1 N BN \u2212 B\u2016 \u2265 t) \u2264 e\u2212\n(Nt)2\nNV ar+BNt/3 +log(M)\n,\nwhere V ar = \u2016E[eie>i ]\u20162 = \u2016diag(\u03c1)\u20162 = maxi \u03c1i, and B = maxi,j \u2016eiej\u20162 = 1. Therefore, with probability at least 1\u2212 \u03b4, we have that\n\u2016 1 N BN \u2212 B\u2016 \u2264\n\u221a maxi \u03c1i log(M/\u03b4)\nN +\n1\n3\n1\nN log(M/\u03b4). (47)\nSince \u2016x\u20161 \u2264 \u221a M\u2016x\u20162, in order to guarantee that \u2016\u2206\u0302 \u2212 \u2206\u20161 \u2264 , it suffices to ensure that\n\u2016\u2206\u0302\u2212\u2206\u20162 \u2264 / \u221a M . Note that the leading two eigenvectors are given by \u03c31(B) \u2265 \u2016\u03c1\u20162 = 1/ \u221a M and\n\u03c32(B) = \u2016\u2206\u20162 = C\u2206/ \u221a M . Assume that we have the exact marginal probability \u03c1, by Davis-Kahan, it suffices to ensure that\n\u2016 1 N BN \u2212 B\u20162 \u2264 \u2016\u2206\u20162\u221a M .\nExample 1. Consider the example of (p, q) in community detection problem, where the marginal probability \u03c1i is roughly uniform. We have \u2016\u2206\u20162 = C\u2206/ \u221a M and maxi \u03c1i = 1/M , and the concentration bound becomes\n\u2016 1 N BN \u2212 B\u2016 \u2264\n\u221a log(M/\u03b4)\nMN , (48)\nand by requiring\n\u221a log(M/\u03b4)\nMN \u2264 \u2016\u2206\u20162\u221a M = C\u2206 M\nwe get a sample complexity bound N = \u2126(M log(M/\u03b4)), which is worse than the lower bound by a log(M) factor.\nExample 2. Moreover, modify Example 1 so that a constant fraction of the probability mass lies in a common word, namely p1 = q1 = 1/2\u03c11 = 0.1, while the marginal probability as well as the separation in all the other words are roughly uniform. In this case, \u2016\u2206\u20162 is still roughly C\u2206/ \u221a M , however we have maxi \u03c1i = 0.1, and the sample complexity becomes N = \u2126(M 2 log(M/\u03b4)). This is even worse than the first example, as the same separation gets swamped by the heavy common words.\n(2) (square root of the empirical marginal scaling (from 1st batch of samples) on both side of the empirical count matrix (from 2nd batch of samples)).\nTake a closer look at the above proof and we can identify two misfortunes that make the truncated SVD deviate from linear sample complexity:\n1. In the worst case, the nonuniform marginal probabilities costs us an M factor in the first component of Bernstein\u2019s inequality;\n2. We pay another log(M) factor for the spectral concentration of the M \u00d7M random matrix.\nTo resolve these two issues, the two corresponding key ideas of Phase I algorithm are \u201cbinning\u201d and \u201cregularization\u201d:\n1. \u201cBinning\u201d means that we partition the vocabulary according to the marginal probabilities, so that for the words in each bin, their marginal probabilities are roughly uniform. If we are able to apply spectral method in each bin separately, we could possibly get rid of the M factor.\n2. Now restrict our attention to the diagonal block of the empirical average matrix 1NBN whose indices corresponding to the words in a bin. Assume that the bin has sufficiently many words, so that the expected row sum and column sum are at least constant, namely the effective number of samples is at least in the order of the number of of words in the bin.\nWe apply regularized spectral method for the empirical average with indices restricted to the bin. By \u201cregularization\u201d we mean removing the rows and column, whose row and column sum are much higher than the expected row sum, from the empirical. Then we apply t-SVD to the remaining. This regularization idea is motivated by the community detection literature in the sparse regime, where the total number of edges of the random network is only linear in the number of nodes."}, {"heading": "F Auxiliary Lemmas", "text": "Lemma F.1 (Wedin\u2019s theorem applied to rank-1 matrix). Denote symmetric matrix X = vv> + E. Let v\u0302v\u0302> denote the rank-1 truncated SVD of X. There is a positive universal constant C such that\nmin{\u2016v \u2212 v\u0302\u2016, \u2016v + v\u0302\u2016} \u2264 {\nC\u2016E\u2016 \u2016v\u2016 if \u2016v\u20162 > C\u2016E\u2016; C\u2016E\u20161/2 if \u2016v\u20162 < C\u2016E\u2016.\nLemma F.2 (Chernoff Bound for Poisson variables).\nPr(Poi(\u03bb) \u2265 x) \u2264 e\u2212\u03bb ( x e\u03bb )\u2212x , for x > \u03bb, Pr(Poi(\u03bb) \u2264 x) \u2264 e\u2212\u03bb ( x e\u03bb )\u2212x , for x < \u03bb.\nLemma F.3 (Matrix Bernstein). Consider a sequence of N random matrix {Xk} of dimension M\u00d7M which are independent, self-adjoint. Assume that E[Xk] = 0 and \u03bbmax(Xk) \u2264 R almost surely. Denote the total variance by \u03c32 = \u2016\u2211Nk=1 E[X2k ]\u2016. Then the following inequality holds for all t > 0:\nPr ( \u2016 N\u2211\nk=1\nXk\u2016 \u2265 t ) \u2264Me\u2212 t2 \u03c32+Rt/3 \u2264 { Me\u2212 3t2\n8\u03c32 , for t \u2264 \u03c32/R; Me\u2212 3t 8R , for t \u2265 \u03c32/R.\nLemma F.4 (Upper bound of Poisson tails (Proposition 1 in [25])). Assume \u03bb > 0, consider the Poisson distribution Poi(\u03bb).\n(1) if 0 \u2264 n < \u03bb, the left tail can be upper bounded by:\nPr(Poi(\u03bb) \u2264 n) \u2264 (1\u2212 n \u03bb )\u22121 Pr(Poi(\u03bb) = n).\n(2) if n > \u03bb\u2212 1, for any m \u2265 1, the right tail can be upper bounded by:\nPr(Poi(\u03bb) \u2265 n) \u2264 (1\u2212 ( \u03bb n+ 1\n)m)\u22121 n+m\u22121\u2211\ni=n\nPr(Poi(\u03bb) = i).\nCorollary F.5. Let \u03bb > C for some large universal constant C. For any constant c\u2032 > e, 0 \u2264 c < 1/2, we have the following Poisson tail bounds:\nPr(Poi(\u03bb) \u2264 c\u03bb) \u2264 2e\u2212\u03bb/2, Pr(Poi(\u03bb) \u2265 c\u2032\u03bb) \u2264 2e\u2212c\u2032\u03bb.\nProof. Apply Stirling\u2019s bound for \u03bb large, we have \u03bb! \u2265 (\u03bbe )\u03bb. Then, the bound in Lemma F.4 (1) can be written as\nPr(Poi(\u03bb) \u2264 c\u03bb) \u2264 (1\u2212 c)\u22121 Pr(Poi(\u03bb) = c\u03bb) \u2264 2e\u2212\u03bb(\u03bb)c\u03bb/(c\u03bb)! \u2264 2e\u2212\u03bb(\u03bb)c\u03bb/(c\u03bbe\u22121)c\u03bb\n\u2264 2e\u2212\u03bb+c\u03bb log(e/c)\n\u2264 2e\u2212\u03bb/2, where in the second inequality we used the assumption that c < 1/2, and in the last inequality we used the inequality 1\u2212 c log(e/c) \u2265 1/2 for all 0 \u2264 c < 1.\nSimilarly, set m = 1 in Lemma F.4 (2), we can write the bound as\nPr(Poi(\u03bb) \u2265 c\u2032\u03bb) \u2264 (1\u2212 \u03bb c\u2032\u03bb+ 1 )\u22121 Pr(Poi(\u03bb) = c\u2032\u03bb)\n\u2264 (1\u2212 1/c\u2032)\u22121e\u2212\u03bb(\u03bb)c\u2032\u03bb/(c\u2032\u03bb)! \u2264 2e\u2212\u03bb(\u03bb)c\u2032\u03bb/(c\u2032\u03bbe\u22121)c\u2032\u03bb \u2264 2e\u2212c\u2032\u03bb log(c\u2032/e)\u22121\n\u2264 2e\u2212c\u2032\u03bb, where in both the second and the last inequality we used the assumption that c\u2032 > e and \u03bb is a large constant.\nLemma F.6 (Slight variation of Vershynin\u2019s theorem (Poisson instead of Bernoulli)). Consider a random matrix A of size M \u00d7M , where each entry follows an independent Poisson distribution Ai,j \u223c Poi([EA]i,j). Define dmax = M maxi,j [EA]i,j. For any r \u2265 1, the following holds with probability at least 1 \u2212 M\u2212r. Consider any subset consisting of at most 10 Mdmax , and decrease the entries in the rows and the columns corresponding to the indices in the subset in an arbitrary way. Then for some universal large constant c the modified matrix A\u2032 satisfies:\n\u2016A\u2032 \u2212 [EA]\u2016 \u2264 Cr3/2( \u221a dmax + \u221a d\u2032),\nwhere d\u2032 denote the maximal row sum in the modified random matrix."}], "references": [{"title": "Exact recovery in the stochastic block model", "author": ["Emmanuel Abbe", "Afonso S Bandeira", "Georgina Hall"], "venue": "arXiv preprint arXiv:1405.3267,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms", "author": ["Emmanuel Abbe", "Colin Sandon"], "venue": "arXiv preprint arXiv:1503.00609,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Competitive closeness testing", "author": ["J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Competitive classification and closeness testing", "author": ["J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anima Anandkumar", "Yi kai Liu", "Daniel J. Hsu", "Dean P Foster", "Sham M Kakade"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "author": ["Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra"], "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning topic models\u2013going beyond svd", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1502.03520,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "CoRR, abs/1502.03520,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Testing closeness of discrete distributions", "author": ["T. Batu", "L. Fortnow", "R. Rubinfeld", "W.D. Smith", "P. White"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Sublinear algorithms for testing monotone and unimodal distributions", "author": ["T. Batu", "R. Kumar", "R. Rubinfeld"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Smoothed analysis of tensor decompositions", "author": ["Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Testing closeness with unequal sized samples", "author": ["B. Bhattacharya", "G. Valiant"], "venue": "In Neural Information Processing Systems (NIPS) (to appear),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["L. Birge"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency", "author": ["J.T. Chang"], "venue": "Mathematical Biosciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems", "author": ["Yuxin Chen", "Emmanuel J Candes"], "venue": "arXiv preprint arXiv:1505.05114,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery", "author": ["Peter Chin", "Anup Rao", "Van Vu"], "venue": "arXiv preprint arXiv:1501.05021,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Testing k-modal distributions: optimal algorithms via reductions", "author": ["C. Daskalakis", "I. Diakonikolas", "R. Servedio", "G. Valiant", "P. Valiant"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Random Structures & Algorithms,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "On the second eigenvalue of random regular graphs", "author": ["Joel Friedman", "Jeff Kahn", "Endre Szemeredi"], "venue": "In Proceedings of the twenty-first annual ACM symposium on Theory of computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Rong Ge", "Qingqing Huang", "Sham M. Kakade"], "venue": "In Proceedings of the Symposium on Theory of Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Upper bounds on poisson tail probabilities", "author": ["Peter W Glynn"], "venue": "Operations research letters,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1987}, {"title": "Streaming and sublinear approximation of entropy and information distances", "author": ["S. Guha", "A. McGregor", "S. Venkatasubramanian"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Stochastic blockmodels: First steps", "author": ["Paul W Holland", "Kathryn Blackmond Laskey", "Samuel Leinhardt"], "venue": "Social networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1983}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Estimation of a discrete monotone density", "author": ["H.K. Jankowski", "J.A. Wellner"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In Proceedings of the 42nd ACM symposium on Theory of computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Spectral redemption in clustering sparse networks", "author": ["Florent Krzakala", "Cristopher Moore", "Elchanan Mossel", "Joe Neeman", "Allan Sly", "Lenka Zdeborov\u00e1", "Pan Zhang"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Sparse random graphs: regularization and concentration of the laplacian", "author": ["Can M Le", "Elizaveta Levina", "Roman Vershynin"], "venue": "arXiv preprint arXiv:1502.03049,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Concentration and regularization of random graphs", "author": ["Can M Le", "Roman Vershynin"], "venue": "arXiv preprint arXiv:1506.00669,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Community detection thresholds and the weak ramanujan property", "author": ["Laurent Massouli\u00e9"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Learning nonsingular phylogenies and hidden Markov models", "author": ["E. Mossel", "S. Roch"], "venue": "Annals of Applied Probability,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Stochastic block models and reconstruction", "author": ["Elchanan Mossel", "Joe Neeman", "Allan Sly"], "venue": "arXiv preprint arXiv:1202.1499,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Consistency thresholds for binary symmetric block models", "author": ["Elchanan Mossel", "Joe Neeman", "Allan Sly"], "venue": "arXiv preprint arXiv:1407.1591,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Optimal algorithms for testing closeness of discrete distributions", "author": ["S. on Chan", "I. Diakonikolas", "G. Valiant", "P. Valiant"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Estimating entropy on m bins given fewer than m samples", "author": ["L. Paninski"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Strong lower bounds for approximating distribution support size and the distinct elements problem", "author": ["S. Raskhodnikova", "D. Ron", "A. Shpilka", "A. Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "A spectral algorithm for learning class-based n-gram models of natural language", "author": ["Karl Stratos", "Michael Collins Do-Kyum Kim", "Daniel Hsu"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Estimating the unseen: an n/ log n-sample estimator for entropy and support size, shown optimal via new clts", "author": ["G. Valiant", "P. Valiant"], "venue": "In Symposium on Theory of Computing (STOC),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "The power of linear estimators", "author": ["G. Valiant", "P. Valiant"], "venue": "In Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "Estimating the unseen: improved estimators for entropy and other properties", "author": ["G. Valiant", "P. Valiant"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "An automatic inequality prover and instance optimal identity testing", "author": ["G. Valiant", "P. Valiant"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "However, one can recover the model parameters using sampled trigram sequences; this last step is straightforward (and sample efficient as it uses only an additional \u03a9(1/ 2) trigrams) when given an accurate estimate of B (see [6] for the moment structure in the trigrams).", "startOffset": 225, "endOffset": 228}, {"referenceID": 40, "context": "In the case of 2-topic models, the community detection lower bounds [41][32][52] imply that \u0398(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words.", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "In the case of 2-topic models, the community detection lower bounds [41][32][52] imply that \u0398(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words.", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "One of the most well studied community models is the stochastic block model [27].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 39, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 31, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 32, "context": "[22, 40, 32, 33]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 40, "context": "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36].", "startOffset": 167, "endOffset": 178}, {"referenceID": 0, "context": "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36].", "startOffset": 167, "endOffset": 178}, {"referenceID": 35, "context": "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between \u03b1, \u03b2, and M were established, down to subconstant factors [41, 1, 36].", "startOffset": 167, "endOffset": 178}, {"referenceID": 18, "context": "[19, 2]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[19, 2]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 36, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 34, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 45, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on \u201cword embeddings\u201d [37, 35, 46, 9].", "startOffset": 152, "endOffset": 167}, {"referenceID": 44, "context": "see [45])\u2014this is especially noticeable in the relatively poor quality of the embeddings for relatively rare words.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "The recent theoretical work [10] sheds some light on why current approaches are so successful, yet the following question largely remains: Is there a more accurate way to recover the best rank-d approximation of the underlying matrix than simply computing the best rank-d approximation for the (noisy) matrix of empirical counts? Efficient Algorithms for Latent Variable Models.", "startOffset": 28, "endOffset": 32}, {"referenceID": 28, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 49, "endOffset": 61}, {"referenceID": 38, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 49, "endOffset": 61}, {"referenceID": 16, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 49, "endOffset": 61}, {"referenceID": 7, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 6, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 13, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 19, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 50, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 37, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 12, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 27, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 30, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 23, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 197, "endOffset": 225}, {"referenceID": 4, "context": "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].", "startOffset": 287, "endOffset": 290}, {"referenceID": 5, "context": "A number of these methods essentially can be phrased as solving an inverse moments problem, and the work in [6] provides a unifying viewpoint for computationally efficient estimation for many of these models under a tensor decomposition perspective.", "startOffset": 108, "endOffset": 111}, {"referenceID": 42, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 25, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 46, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 48, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 90, "endOffset": 106}, {"referenceID": 43, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 121, "endOffset": 129}, {"referenceID": 46, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 121, "endOffset": 129}, {"referenceID": 46, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 162, "endOffset": 174}, {"referenceID": 48, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 162, "endOffset": 174}, {"referenceID": 47, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 162, "endOffset": 174}, {"referenceID": 10, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 41, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 49, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 14, "context": "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.", "startOffset": 289, "endOffset": 305}, {"referenceID": 2, "context": "[3, 4, 50], with stronger information theoretic optimality guarantees.", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "[3, 4, 50], with stronger information theoretic optimality guarantees.", "startOffset": 0, "endOffset": 10}, {"referenceID": 49, "context": "[3, 4, 50], with stronger information theoretic optimality guarantees.", "startOffset": 0, "endOffset": 10}, {"referenceID": 15, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 11, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 29, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 20, "context": "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].", "startOffset": 109, "endOffset": 125}, {"referenceID": 22, "context": "This simple idea was first introduced by [23], and followed by analysis works in [22] and many others.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "This simple idea was first introduced by [23], and followed by analysis works in [22] and many others.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "Recently in [33] and [34] the authors provided clean and clever proofs to show that any such \u201cregularization\u201d essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "Recently in [33] and [34] the authors provided clean and clever proofs to show that any such \u201cregularization\u201d essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "When directly applied to the empirical bins with such spillover, the existing results of \u201cregularization\u201d in [34] do not lead to the desired concentration result.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "For example, in a recent paper [19] on community detection, after obtaining a crude classification of nodes using spectral algorithm, one round of a \u201ccorrection\u201d routine is applied to each node based on its connections to the graph partition given by the first round.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "Another example is given in [18] in the context of solving random quadratic equations, where local refinement of the solution follows the spectral method initialization.", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "Then we leverage the clever proof techniques from [34] to show that if spillover effect is small, regularized truncated SVD can be applied to estimate the entries of \u2206 restricted to each bin.", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "For block A1 whose rows and columns all correspond to the \u201cgood words\u201d with roughly uniform marginals, we show its concentration by applying the result in [34].", "startOffset": 155, "endOffset": 159}], "year": 2017, "abstractText": "We consider the problem of accurately recovering a matrix B of size M \u00d7M , which represents a probability distribution overM outcomes, given access to an observed matrix of \u201ccounts\u201d generated by taking independent samples from the distribution B. How can structural properties of the underlying matrix B be leveraged to yield computationally efficient and information theoretically optimal reconstruction algorithms? When can accurate reconstruction be accomplished in the sparse data regime? This basic problem lies at the core of a number of questions that are currently being considered by different communities, including community detection in sparse random graphs, learning structured models such as topic models or hidden Markov models, and the efforts from the natural language processing community to compute \u201cword embeddings\u201d. Many aspects of this problem\u2014both in terms of learning and property testing/estimation and on both the algorithmic and information theoretic sides\u2014remain open. Our results apply to the setting where B has a particular rank 2 structure. For this setting, we propose an efficient (and practically viable) algorithm that accurately recovers the underlying M \u00d7 M matrix using \u0398(M) samples. This result easily translates to \u0398(M) sample algorithms for learning topic models with two topics over dictionaries of size M , and learning hidden Markov Models with two hidden states and observation distributions supported on M elements. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires \u03a9(M) samples. Furthermore, we provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires \u03a9(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs. This impossibility of sublinear-sample property testing in these settings is intriguing and underscores the significant differences between these structured settings and the standard setting of drawing i.i.d samples from an unstructured distribution of support size M . \u2217MIT. Email: qqh@mit.edu. \u2020University of Washington. Email: sham@cs.washington.edu \u2021Stanford University. Email: kweihao@gmail.com \u00a7Stanford University. Email: valiant@stanford.edu. Gregory and Weihao\u2019s contributions were supported by NSF CAREER Award CCF-1351108, and a research grant from the Okawa Foundation. ar X iv :1 60 2. 06 58 6v 1 [ cs .L G ] 2 1 Fe b 20 16", "creator": "LaTeX with hyperref package"}}}