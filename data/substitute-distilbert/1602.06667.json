{"id": "1602.06667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "A Motion Planning Strategy for the Active Vision-Based Mapping of Ground-Level Structures", "abstract": "this paper presents a system capable of autonomously mapping the visible part producing a bounded three - dimensional structure using autonomous mobile ground robot equipped with a depth sensor. we describe motion planning strategies as determine appropriate successive viewpoints and attempt to fill holes automatically in a point cloud produced by global sensing boundary navigation layer. we develop a local motion planner having potential fields to maintain a desired distance resembling the structure. recommended emphasis is on accurately reconstructing a 3d structure of a structure of moderate size rather than mapping large open environments, with applications for example in exploration, construction and modeling. the proposed application better not require any initialization in the form of a mesh model or a bounding box. we compare via simulations the performance of our policies to the classic frontier based exploration algorithm. we illustrate the efficacy of adjustment mechanisms for different structure sizes, levels of localization accuracy and range of the directed sensor.", "histories": [["v1", "Mon, 22 Feb 2016 07:05:49 GMT  (11682kb,D)", "https://arxiv.org/abs/1602.06667v1", null], ["v2", "Wed, 1 Mar 2017 17:27:45 GMT  (7599kb,D)", "http://arxiv.org/abs/1602.06667v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.SY", "authors": ["s r manikandasriram", "r\\'e phu-van nguyen", "jerome le ny"], "accepted": false, "id": "1602.06667"}, "pdf": {"name": "1602.06667.pdf", "metadata": {"source": "CRF", "title": "A Motion Planning Strategy for the Active Vision-Based Mapping of Ground-Level Structures", "authors": ["S. R. Manikandasriram", "Andr\u00e9 Phu-Van"], "emails": [], "sections": [{"heading": null, "text": "Note to Practitioners\u2014 The objective of this work is to automate the process of building a 3D model of a structure of interest that is as complete as possible, using a mobile camera or depth sensor, in the absence of any prior information about this structure. Given that increasingly robust solutions for the Visual Simultaneous Localization and Mapping problem (vSLAM) are now readily available, the key challenge that we address here is to develop motion planning policies to control the trajectory of the sensor in a way that improves the mapping performance. We target in particular scenarios were no external absolute positioning system is available, such as mapping certain indoor environments where GPS signals are blocked. In this case, it is often important to revisit previously seen locations relatively quickly, in order to avoid excessive drift in the dead-reckoning localization system. Our system works by first determining the boundaries of the structure, before attempting to fill the holes in the constructed model. Its performance is illustrated through simulations and a real-world experiment performed with a depth sensor carried by a mobile manipulator.\nIndex Terms\u2014Motion Planning, Active Sensing, Active SLAM, Autonomous Mapping, Autonomous Inspection\nI. INTRODUCTION\nACCURATE 3D computer models of large structures havea wide range of practical applications, from inspecting an aging structure to providing virtual tours of cultural heritage\nPart of this work was performed while the first author was visiting Polytechnique Montreal, under a Globalink Fellowship from MITACS. This work was also supported by NSERC (Grant 435905-13) and the Canada Foundation for Innovation (Grant 32848).\nS. R. M. is with the Robotics Institute, University of Michigan, Ann Arbor, MI 48109, USA. A. Phu-Van Nguyen and J. Le Ny are with the Department of Electrical Engineering, Polytechnique Montreal, and GERAD, Montreal, QC H3T 1J4, Canada. e-mails: srmani@umich.edu, andre-phuvan.nguyen@polymtl.ca, jerome.le-ny@polymtl.ca.\nsites [1], [2]. In civil engineering for example, an important problem is that of construction progress monitoring, i.e., comparing the state of a building under construction over time to the project plan. The process of regularly updating the estimate of the state of the building has traditionally been performed manually, but in recent years new methods have been developed to automate it using data obtained from a variety of sensors, e.g., positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].\nThis paper considers the problem of guiding in real-time a mobile autonomous robot carrying a vision sensor, in order to build a 3D model of a structure. For this, we need to address two problems. First, we need a robust mapping system that can build the 3D model in real-time when given a sequence of images or depth maps as input. This is a widely researched problem called Visual Simultaneous Localization and Mapping (vSLAM) or real-time Structure from Motion (SfM), for which several open source packages offer increasingly accurate and efficient solutions [6], [7]. The second problem relates to active sensing [8], as we need motion planning strategies that can guide a mobile sensor to explore the structure of interest. For mapping, monitoring or inspection applications, certain classical strategies such as frontier-based exploration algorithms [9], which guide the robot to previously unexplored regions irrespective of whether it is part of the structure of interest or not, are not necessarily well adapted.\nSome recent work considers the problem of reconstructing a 3D model of arbitrary objects by moving a depth sensor relative to the object using different forms of next best view planning algorithms [10], [11]. Typically, these systems iteratively build a complete 3D model of the object by heuristically choosing the next best viewpoint according to some performance measure. However, much of this work is restricted to building models of relatively small objects that are bounded by the size of the robot workspace. In contrast, our focus is on 3D reconstruction of larger but still bounded structures such as buildings, which can be several orders of magnitude larger than a mobile robot. The related problem of automated inspection deals with large structures such as tall buildings [5] and ship hulls. Bircher et al. [12] assume that a prior 3D mesh of the structure to inspect is available and compute a short path connecting viewpoints that together are guaranteed to cover all triangles in the mesh. In [13], Englot et al. begin by assuming a safe bounding box of the hull and construct a coarse mesh of the hull by tracing along the walls of this box in a fixed trajectory without taking feedback from the actual geometry of the structure. Moreover, this coarse mesh ar X\niv :1\n60 2.\n06 66\n7v 2\n[ cs\n.R O\n] 1\nM ar\n2 01\n7\n2 is manually processed offline to yield an accurate 3D mesh, which is then used to inspect the finer structural details. Yoder and Scherer [14] also assume a bounding box and develop an algorithm combining next best view planning and frontierbased exploration to encourage coverage of the structure. Sheng et al. [15] use a prior CAD model of an aircraft to plan a path for a robotic crawler such that it inspects all the rivets on the surface of the aircraft. In this paper however, we do not assume any prior information in terms of a 3D mesh, CAD model or a bounding box around the structure, and focus on reactive path-planning to build the model online. Our mapping problem is also related to coverage path planning, see, e.g., [16]\u2013[19] and the references therein, which has traditionally focused on developing algorithms ensuring that a mobile robot passes over all points in a 2D environment, assuming a sufficiently accurate localization system.\nIn computer vision and photogrammetry, SfM techniques aim at building a 3D model of a scene from a large number of images [20]\u2013[23], but most of this work focuses on batch post-processing and typically assumes a given dataset, whereas here our focus is essentially on how to acquire an appropriate set of images. Let us mention however the work of Daftry et al. [24], which presents an interactive real-time SfM system providing online feedback to the user taking pictures, alerting him or her when a new picture cannot be properly integrated in the model. Also, Tuite et al. [25] develop a competitive game where players are encouraged to take pictures that help build complete 3D models. We emphasize that we do not discuss in details the task of actually building a model from a collection of pictures or depth maps, which can be executed by one of the available vSLAM or real-time SfM systems, such as the Real-Time Appearance Based Mapping package (RTAB-Map) [6] that we use in our simulations. Our work focuses on actively exploring the environment with an autonomous robot to build a complete model in real-time, with our controller taking at any time the current model as an input. Which package we use for model reconstruction has little influence on our algorithms, for example any vSLAM system based on pose-graph optimization [26] could be used. State-of-the-art batch SfM systems can also be used to postprocess the sequence of images or depth maps captured using our policies in order to obtain a more accurate model offline. Naturally, eventual completeness of the model is limited by the physical characteristics of the robot, and specifically the reachable space of the sensor, see Fig. 1.\nFinally, another line of work in informative path planning relates to autonomous exploration and coverage of relatively large environments, using variants of frontier based exploration algorithms for example [28]\u2013[31]. While these papers focus on path planning to quickly build models of potentially large and complex spaces, they do not address the problem of autonomously delimiting and mapping as completely as possible a specific bounded structure of interest.\nOur contributions can be summarized as follows. After presenting the problem statement in Section II, we develop in Section III a motion planner allowing a ground robot equipped with a camera or depth sensor to autonomously determine the boundaries of an initially unknown structure.\n(a)\n(b)\nFig. 1. Comparison of the a) Simulated Model in Gazebo [27] that needs to be mapped and b) Reconstructed 3D model by a mobile ground robot using our policies. Only the bottom portion is mapped due to the limited reachable space of the sensor.\nThen, Section IV describes an algorithm for detecting missing portions in the 3D model constructed during the boundary determination phase, and an exploration strategy to improve the completeness of the model. In Section V we analyze the level of coverage completeness that can be expected from our strategy. The behavior of the proposed policies is illustrated in Section VI through simulations, and the resulting accuracy of the constructed models compared to that obtained using the classical frontier based exploration algorithm. Experimental results are presented in Section VII to validate the algorithms under more realistic illumination conditions. One justification for our incremental exploration approach is that we focus on using the vSLAM module both for mapping the structure as well as localizing the robot, although an additional deadreckoning system such as wheel odometry could be present as well. In the absence of an independent source of accurate absolute positioning, it is important to close loops relatively frequently with the vSLAM system, i.e., revisit regions that have already been explored, in order to control the growth of the localization errors building up with visual odometry alone. We also help the vSLAM system by following the boundaries of the structure, where visual features are likely to be present."}, {"heading": "II. PROBLEM STATEMENT AND ASSUMPTIONS", "text": "Consider the problem of constructing a 3D model of a given structure of finite size, e.g., a monument or a building, using a mobile ground robot carrying an imaging or depth sensor, such as a Kinect, a monocular or stereo camera, or a LIDAR. Initially, no approximate model of the structure nor map of the environment is available, and the actual size of the structure is also unknown. The sensor (also called camera in the following) provides a sequence of point clouds obtained directly or computed from depth and/or luminance images. These local point clouds, together with an estimate of the sensor trajectory, can then be assembled and registered\n3 (a) (b)\nFig. 2. (a) Starting configuration for the robot and the camera with respect to the structure. (b) The initial image as seen by the camera. Initially, the robot only knows that the structure in the field of view (FOV) of its camera is the one that should be mapped.\nin a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32]. We do not directly address here the model reconstruction problem in vSLAM. Instead, we focus on determining good trajectories for the robot allowing a vSLAM module (and potentially a batch SfM module in post-processing) to produce a high quality model, which ideally should capture the entire visible portion of the structure accurately. A key challenge is to develop strategies that are applicable for any type of structure while respecting the physical limitations of the platform.\nWe assume that initially the robot is positioned along the structure to be mapped, with the camera capturing point clouds mounted on its right and facing the structure at a distance D measured in a horizontal plane, see Fig. 2. In normal operations, we wish to maintain this distance D between the structure and the path of the camera, where D is chosen based on the camera\u2019s resolution. Define a global fixed Frame of Reference (FoR) G := {Og,xg,yg, zg}, in which the global point cloud is to be assembled. Note that we write vectors in bold. The robot FoR R := {Or,xr,yr, zr} (forward, left, up) coincides initially with G, but is attached to a point Or that moves along with the robot. For concreteness to describe our scenario and algorithms, the camera FoR C := {Oc,xc,yc, zc}, is assumed to be rigidly attached to the robot except for the yaw motion, which is left unconstrained.\nAssumption 1: The center Oc of the camera mounted on the mobile ground robot has fixed coordinates (0, 0, hc) in frame R, and in addition we always maintain zr = zc. Such a choice of camera configuration determines which parts of the structure are not visible at all, and hence cannot be mapped by any algorithm implemented on this platform. However, other system configurations could be handled with some of the more generic tools developed in this paper.\nFig. 3 shows our conventions for the different FoR used. The imaging plane of the camera is defined by yczc, with xc pointing towards the front of the camera on the optical axis. Coordinates in the camera, robot and global FoR are denoted using superscripts as vc, vr and vg respectively for a vector v. We assume that initially xc = \u2212yr so that the camera points to the right of the robot.\nWe make two additional assumptions for simplicity of exposition. The first one guarantees that there exists collision free paths around the structure.\nFig. 3. The camera is kept at a constant height above the robot\u2019s base. The red, green and blue lines correspond to the x, y and z axes respectively and both the camera and robot can rotate along their z axes. The yellow region corresponds to the view frustum of the camera.\nAssumption 2: The horizontal distance of the closest obstacle from the structure is at least 2D. The next assumption simplifies the problem of detecting, tracking and removing the ground surface from point clouds.\nAssumption 3: The structure and the robot are placed on the horizontal plane zg = 0. In particular, we have zc = zr = zg.\nIn the following we fix the z-coordinate of Or to be zero. A consequence of these assumptions is that relatively horizontal surfaces that are at the same height or above the camera center for example cannot be mapped, and the maximum height (measured in the R or G frame) of the structure that can be mapped is Hmax = hc + D tan\u03c8/2, where \u03c8 is the vertical angle of view of the camera. Assumption 3 could be removed by using recent classification systems that can differentiate between ground and non-ground regions [33] to pre-process the point clouds before sending them to our system.\nFinally, there are additional implicit assumptions that we state informally. First, since we rely on an external mapping module to build the 3D model, the conditions that allow this module to operate sufficiently reliably must be met. For example, vSLAM generally requires appropriate scene illumination and the presence of a sufficiently rich set of visual features. Second, we concentrate on the reconstruction of the details of the model at a scale comparable with or larger than the typical length of the robot. If features at a smaller scale need to be included, e.g., fine structural details on a wall, our system could be augmented with a more local planner for a robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23]. Finally, for reasons explained in Section III-C, we assume that the robot is equipped with sensors capable of detecting obstacles in a 180\u25e6 region ahead of it and within a distance of D, see Fig. 8.\nWe divide our mapping process into two phases, as depicted on Fig 4. The first is the Perimeter Exploration (PE) phase, during which the robot moves with the structure on its right to determine its boundaries. The robot continuously moves towards previously unseen regions of the structure, with the exploration directed towards finding the limits of the structure and closing a first loop around it relatively quickly rather than trying to map all its details. The PE phase ends when our algorithm detects that the robot has returned to the neighborhood of its starting point Og and the vSLAM module\n4 detects a global loop closure. After completing the PE phase, the system determines the locations of potential missing parts in the constructed 3D model. Next, in the Cavity Exploration (CE) phase, the system explores these missing parts in the model. The following subsections explain each step of our process in detail."}, {"heading": "III. PERIMETER EXPLORATION", "text": "In this section, we present a method to autonomously determine the boundaries of an unknown structure. From Assumptions 2 and 3, zg = 0 and zg = Hmax are bounding horizontal planes for the model. The remaining problem is to determine the expansion of the structure in the xgyg plane. To do this, the robot moves clockwise around the structure by determining online a discrete sequence of successive goals or waypoints. It tries to keep the optical axis of the depth sensor approximately perpendicular to the structure, which maximizes the depth resolution at which a given portion of the structure is captured, and increases the density of captured points. It also tries to maintain the camera center Oc on a smooth path at a fixed distance D from the structure."}, {"heading": "A. Determination of the next goal", "text": "The pseudo-code to determine the next position and orientation of the camera in our perimeter exploration algorithm is shown in Algorithm 1. The algorithm takes as input the current point cloud produced by the camera in its FoR. For its implementation we rely on the Point Cloud Library (PCL) [35]. First, the ground plane is removed, using a simple\nAlgorithm 1 Algorithm for computing the next goal for the camera using the current point cloud in camera FoR.\n1: function COMPUTENEXTGOAL(cloud full) 2: cloud \u2190 PCLremoveGroundPlane(cloud full) 3: cloud slice \u2190 filterForwardSlice(cloud) 4: pc \u2190 PCLcompute3Dcentroid(cloud slice) 5: [v1,v2,v3;\u03bb1, \u03bb2, \u03bb3]\u2190 PCA(cloud slice) 6: n\u0303\u2190 v3\u2212 (v3 \u00b7 zc)v3 . Projection on the xcyc plane 7: n\u2190 n\u0303 sign(n\u0303 \u00b7 \u2212\u2212\u2192 Ocp\nc);n\u2190 n/\u2016n\u2016 8: r\u2190 zc \u00d7 n 9: goal\u2190 pc \u2212D n + step r\n10: return goal,n 11: end function\nthresholding operation, so that the resulting point cloud P contains only those points that belong to the structure. Next, on line 3, we select a subset S of the point cloud referred to as the forward slice, which adjoins the part of the structure that must be explored next, see Fig. 5a. Concretely, we choose S so that its yc-coordinates satisfy ycmax\u2212 ycmax\u2212y c min 3 \u2264 y c \u2264 ycmax, where ycmin and y c max are the minimum and maximum y\nccoordinate values for all points in P . On line 5, following [36], we compute via Principal Component Analysis (PCA) the normal direction to that plane \u03a0 which best fits S. In more details, denote S = {pci : i = 1, 2, . . . ,m} and define the covariance matrix X = 1m \u2211m i=1(p c i \u2212 pc)(pci \u2212 pc)T , where\npc = 1m \u2211m i=1 p c i is the centroid of S computed on line 4. We compute the eigenvectors [v1,v2,v3] of X, ordered here by decreasing value of the eigenvalues \u03bb1, \u03bb2, \u03bb3. The eigenvector v3 for the smallest eigenvalue corresponds to the normal to the plane \u03a0.\nThe algorithm returns n, computed from the projection of the normal vector v3 on the xcyc plane, and taken to point in the direction of the vector \u2212\u2212\u2192 Ocp\nc. This vector n defines the desired orientation of the camera. The algorithm also returns the next goal point goal = pc\u2212D n+step r for the center Oc of the camera, where r = zc \u00d7 n is computed on line 8, and step = ycmax\u2212y c min\n6 . The term step r, which is along the plane \u03a0, is used to shift the goal forward so that both sections of a corner fall in the FOV of the camera, as in the situation shown on Fig. 5a. This prevents the algorithm from making slow progress around corners. Furthermore, the interior angle of a corner could be acute, as shown in Fig. 5b, and consequently the farther section of the corner would not be visible from the camera. Such a case can be detected by monitoring the width of S to fall below a threshold. In this case we modify the computation of the goal to be goal = pc+D r which allows the robot to move around sharp corners of the structure. Finally, the computed camera pose is transformed into the global FoR to obtain the next goal point gg for the camera center Oc. We simplify the notation gg to g in the following, where we work in the global reference frame."}, {"heading": "B. Local path planning to the next goal", "text": "In order to move the camera center to g while keeping it approximately at the desired distance D from the structure\n5\nalong the way, we use a local path planner based on potential fields [37], [38]. A potential function encoding the structure as obstacles in the neighborhood of the camera, as well as the goal g, is sampled in the form of a cost map on a local 2D grid of size 2D\u00d72D centered on the camera\u2019s current position, see Fig. 6. Assumption 2 guarantees that all the occupied cells in this cost map denote the structure itself. For k occupied cells centered at {xj}kj=1, the potential function N(x) is defined as\nN(x) = \u03b1\u2016x\u2212 g\u20162 + k\u2211 j=1 Ij(x)dj(x), (1)\nwith dj(x) = 1\n\u03b2\u2016x\u2212 xj\u2016 ; Ij(x) = { 1 if \u2016x\u2212 xj\u2016 \u2264 D 0 otherwise,\nfor some scalar parameters \u03b1, \u03b2. Here dj is the repulsion from the jth occupied cell, and is limited by Ij to a neighborhood of radius D around the cell. A path for the camera is obtained by following the negative gradient of N , i.e., x\u0307 = \u2212\u2207N(x). Denoting Jx = {j : Ij(x) = 1} the occupied cells in the Dneighborhood of x, we have\n\u2212\u2207N(x) = 2\u03b1(g \u2212 x) + \u2211 i\u2208Jx\n1\n\u03b2\u2016x\u2212 xi\u20163 (x\u2212 xi). (2)\nLet MD = {x : Jx 6= \u2205} denote the region that is at distance at most D from the structure. Assuming a small value of \u03b2, the summation term in (2) is dominant whenever x \u2208 MD and pushes the path away from the structure. However, this term vanishes as soon as x /\u2208 MD. Then, assuming that the camera starts at x0 on the boundary \u2202MD of MD, it remains approximately on \u2202MD if \u2212\u2192xg points toward the interior of MD. It is possible that this condition is not satisfied by the point g computed in the previous subsection, in which case we replace g by g1, which is obtained by selecting a new goal = pc \u2212D\u2032 n + step r for D\u2032 < D such that this condition is satisfied. The path will then slide on \u2202MD until it reaches its goal [39]. Finally, this path for the center of the camera is used to compute a corresponding path for the center of the robot, which then needs to be tracked using a platform specific controller.\nOverall, during the PE phase the robot attempts to maintain a viewpoint orthogonal to the structure, even though it replans\nfor a new goal according to Algorithm 1 only at discrete times. Note that only the computation of the next goal happens at discrete instants but the vSLAM module updates the model at a higher rate as per the capabilities of the hardware."}, {"heading": "C. Replanning due to the structure interferring", "text": "Assumption 2 guarantees that the robot can move sufficiently freely around the structure, but this does not prevent the structure itself from interfering with the path planned above. Consider the situation shown in Fig. 7a. The wall ahead of the robot does not fall into the FOV of the camera due to the limited horizontal angle of view, yet the robot should not approach this wall closer than a distance D. Hence, if the robot detects obstacles in its forward D-neighborhood, it is stopped at its current position and the yaw motion of the camera is\n6 used to scan ahead and face the new section of the structure. More precisely, as illustrated in Fig. 8, we use the costmap from the previous subsection to turn the camera to face along the direction from the robot center Or to the first occupied cell in the D-neighborhood of the robot. The next goal is then recomputed using the newly captured point cloud."}, {"heading": "D. End of the PE phase", "text": "The end of the PE phase corresponds to the robot closing a loop around the structure. Therefore, we require that the vSLAM module detects a loop closure based on the captured images, i.e., recognizes that the robot has returned to the vicinity of a known point. The robot continues traveling on the PE path until this condition is met. This task is not necessarily straightforward because of localization errors, notably the drift accumulating in dead-reckoning systems such as the visual odometry function of the vSLAM module, or the wheel odometry system. If available, absolute positioning sensors such as a GPS receiver in the case of outdoor operations can thus indirectly help improve the loop closure detection. One can also use the measurements of a compass to detect when the robot is traveling along an edge of the structure that has the same orientation as the starting edge, and focus the search for a loop closure along these edges."}, {"heading": "IV. COMPLETING THE MODEL: CAVITY EXPLORATION", "text": "There are two possible types of flaws in the model obtained at the end of the PE phase. Type I flaws correspond to holes that are present in the already explored regions. As noted in Section II, these holes could be due to limitations of the sensor or local occlusions caused by small irregularities in the structure itself, and should be filled using a platform with a more appropriate reachable space, hence we do not consider them further. Type II flaws, called cavities in the following, correspond to regions that were skipped during the PE phase, due to the situation depicted on Fig. 7 in particular. These cavities will be filled during the CE phase, where the robot is allowed to move closer to the structure, although this means that the model will not necessarily be reconstructed up to a height Hmax in some places."}, {"heading": "A. Cavity Entrances", "text": "In this subsection we describe an algorithm to determine the locations of the entrances of the cavities in the model, which will be subsequently used by the CE strategy. We use a voxel based 3D occupancy grid constructed from the global point cloud, and maintained in a hierarchical tree data structure by the OctoMap [32] library. Internally, this library performs ray casting operations, labelling the occupancy measurement of each voxel along the line segment from the camera position to each point in the point cloud as free and the point itself as occupied. For this, we require the vSLAM module to provide the sequence of point clouds and associated estimated camera positions used in assembling the current model. All voxels in the occupancy grid that are not labeled free or occupied are called unknown. Using the constructed OctoMap, we compute a set of frontier voxels, whose definition is adapted from [9].\n(a) With frontier voxels (b) Cavity entrance voxels shown in red\nFig. 9. a) The constructed OctoMap with occupied voxels shown in blue and frontier voxels shown in yellow. These yellow voxels form the boundary of the explored region, but most of them lie along the top and bottom faces of the view frustums. b) The cavity entrance voxels are shown in red.\n(a) Leaf size: 0.05m, depth: 16 (b) Leaf size: 0.4m, depth: 13\nFig. 10. OctoMap queried at depth level 16 and 13 respectively. In this paper, the value of D is 3m and the threshold d0 is chosen as 0.4m.\nDefinition 1: A frontier voxel is a free voxel with at least one neighboring unknown voxel.\nRecall that the camera is constrained to move in a horizontal plane during the PE phase. Consequently, many frontier voxels lie along the top and bottom faces of the view frustums, see Fig. 9a, but do not correspond to cavities to explore. We can ignore them by only considering frontier voxels for which the normal vector n, computed using the nearby frontier voxels [36], makes a sufficiently small angle with the horizontal plane. In other words, we keep only the frontier voxels for which the z-coordinate of the normal n satisfies |ngz| < \u03b1, for some chosen threshold \u03b1. Next, Type I flaws can result in frontier voxels, which we also want to exclude from consideration. Therefore, we require that the distance to the closest occupied voxel should be greater than some threshold d0, which can be chosen as a small fraction of the distance maintained from the structure, say 0.1D. As the number of voxels in a typical structure is very large, we do not perform this thresholding exactly but instead we use an estimate for the distance to the structure obtained from OctoMap. The hierarchical structure of OctoMap allows efficient multi-resolution queries, see Fig. 10, and thus we keep as cavity entrance voxels only those that are marked free at a resolution of approximately d0.\nFinally, we call cavity entrance voxels the frontier voxels that satisfy the two preceding conditions, see Fig. 9b. The cavity entrance voxels are clustered using an Euclidean clustering algorithm from PCL [35] and each cluster is referred to as a cavity entrance. Moreover, there could be some sparsely located cavity entrance voxels, which are removed by setting a minimum size for the cavity entrance."}, {"heading": "B. Cavity Exploration", "text": "Once the cavity entrances have been determined, we can start the CE phase. We explore each detected cavity using a motion analogous to the PE phase, wherein we maintain the\n7\nstructure to the right at a distance \u2206 \u2208 [\u03b4,D] that is determined online based on the available clearance in the cavity and \u03b4 is the minimum required distance for the mapping module. For this, we require a starting viewpoint for each cavity entrance and an algorithm to compute \u2206. The starting viewpoint is chosen from the set of camera poses returned by the vSLAM module during the PE phase and such that the centroid of the cavity entrance lies within the view frustum. Additionally, the centroid should not be occluded by the structure from the camera position. From these camera poses, the one with the earliest timestamp is chosen as starting viewpoint, see Fig. 11.\nThe timestamps of the starting viewpoints of the cavity entrances are used to sort them in increasing order and each of the cavities is explored in sequence. A typical cavity has at least two cavity entrances bordering it, as shown in Fig. 9b and it is possible to have more cavity entrances in some cases. During the CE phase, if the centroid of a cavity entrance falls within the view frustum of the current camera position and is not occluded by the structure, we remove that cavity entrance from our list.\nExploring confined regions during the CE phase requires certain modifications to the PE policy. Recall that the system skipped the cavities during the PE phase as the robot came closer than a distance D from the structure. Therefore during the CE phase, only the region directly ahead of the robot and within a distance \u2206 is checked for interference of the computed path with the structure. Moreover, our potential field-based local path planner now returns paths that maintain a distance \u2206 from the structure. For this, using the notation of Sections III-A and III-B, we modify goal as goal \u2190 pc \u2212\u2206n + step r and transform to the global FoR to obtain the new point g. The distance \u2206 is chosen by starting from the minimum value \u03b4 and increasing it until we reach a local minimum of N\u2206(g) along n, where the definition of N\u2206 is adapted from (1) with \u2206 replacing D. Similarly, when an acute angled corner is encountered during the CE phase, we modify goal as goal\u2190 pc + \u2206 r.\nWhen the robot exits a cavity, the point clouds captured by the camera correspond to parts of the structure that are already present in the model from the PE phase. Consequently, the system can detect that it has finished exploring the current cavity by monitoring the loop closures obtained by the vSLAM module. The robot can then choose the next region to explore from its current list of remaining cavity entrances, and can\ntravel there by following again the PE path. Alternatively, the number of changes in the occupancy measurements of the OctoMap could be used to detect the end of the cavity, as point clouds captured after exiting the cavity ideally would not add new information to the OctoMap. But this solution tends to be less robust because localization errors and sensor noise can induce a large number of changes even when the camera is viewing a region that is already present in the model."}, {"heading": "V. COVERAGE ANALYSIS", "text": "In this section we provide some analysis of the coverage completeness of the PE and CE strategies. To simplify the discussion, we focus on the case of simple structures consisting of vertical walls, potentially supporting hanging structures under which the mobile robot is able to pass. We then analyze the boundary coverage in 2D for the slice M of the structure on the plane zg = hc, see Fig. 4.\nFirst, we analyze the PE phase. We assume that the path planner is able to keep the robot at distance D from M, in other words, the robot\u2019s path remains on the boundary \u2202MD defined in Section III-B, keeping the structure on its right. Note that MD is the Minkowski sum M\u2295BD of M and a closed disk of radius D.\nLemma 1: The path followed by the robot during PE phase cannot self-intersect, except at the initial point Og.\nProof: During the PE phase, the robot keeps the structure at distance D on its right as it moves forward. Fig. 12 then illustrates the impossibility for the robot\u2019s path to intersect itself during PE. Indeed, in case (a) of a counter-clockwise cycle, one can show that before the merging point the robot\u2019s obstacle detector would have seen the structure on its left at distance at most D (point A on Fig. 12), and implemented the left turn as explained in Section III-C. In case (b) of a clockwise cycle, the robot would collide with the structure before closing the path. In both cases we have a contradiction.\nRecall that a simple closed curve (SCC) is a non-selfintersecting, continuous loop. We then have\nCorollary 1: Suppose \u2202MD consists of a finite set of disjoint SCCs. Then, during the PE phase, the robot travels on the SCC of \u2202MD on which it initially started, in the direction that keeps MD on its right. Moreover, assuming the loop closure detection does not incorrectly terminate the PE phase too early, the robot reaches back its starting point Og on this curve.\n8 Proof: The robot progresses along \u2202MD, and its path cannot self-intersect by Lemma 1, so it must eventually reach back its starting point since \u2202MD is finite. It cannot switch to another SCC than the one on which it started, since it would violate the assumption that the planner maintains a distance D with M.\nCorollary 1 characterizes the part of the boundary of MD that the PE strategy covers, assuming the path planner and PE termination algorithm work correctly. The robot ideally travels on an SCC that is part of \u2202MD, which we call the PE curve in the following. We orient this curve in the direction of travel of the robot, with MD on the right.\nLet us now turn to the analysis of the CE phase. LetM\u03b4 = M\u2295B\u03b4 be the Minkowski sum of M and the closed disk of radius \u03b4, where \u03b4 is the minimum horizontal clearance defined in Section IV-B. We work under the mild assumption that both \u2202MD and \u2202M\u03b4 consist of a finite set of disjoint SCCs, although \u2202MD can have a strictly smaller number of such curves in general. The notation of the following proposition is illustrated on Fig. 13.\nProposition 1: Let \u03a3D be the oriented PE curve, and C\u03b4 be a connected component of M\u03b4 in the region on the right of \u03a3D. Let \u0398\u03b4 be one of the SCC forming \u2202C\u03b4 , orient \u0398\u03b4 such that C\u03b4 is on its right, and let \u0398D be the (possibly empty) SCC forming the boundary of \u0398\u03b4 \u2295 BD\u2212\u03b4 on the left of \u0398\u03b4 . If \u0398D \u2229\u03a3D 6= \u2205, then at the end of the PE and the CE phase, the view frustum has covered the curve \u0398\u03b4 .\nReferring to Fig. 4, M has four components Ci, i = 0, . . . , 3. MD has a unique component since by adding a buffer D the components merge into one. Note however that in general, MD does not have to be simply connected, nor even path connected. The dashed line representing the PE curve is also the boundary of MD. Now M\u03b4 has three components, because C0 and C2 merge once we add a buffer \u03b4. C1 and C3 remain disconnected in M\u03b4 however, which allows the robot to enter the passages separating C0 and C1 on the one hand, and C2 and C3 on the other hand. At the end of cavity exploration, the boundary of the components C1 \u2295 B\u03b4 and (C0 \u222aC2)\u2295B\u03b4 will be mapped, but C3\u2295B\u03b4 does not satisfy the hypothesis of Proposition 1 (the boundary of C3 \u2295 BD does not share any point with the PE curve), and in this case its boundary indeed is not mapped. The robot cannot map the whole boundary of C0 or C2 individually, since it cannot pass between these two structures that are less than \u03b4 apart.\nProof: Note that \u0398D is a SCC that forms part or all of \u2202CD, where CD = C\u03b4 \u2295 BD\u2212\u03b4 , i.e., \u0398D consists of points that are at distance D of the portion of the structure in C\u03b4 . As a result, all the points belonging to \u0398D must be either also on \u03a3D or on the right of \u03a3D. A first possibility is that \u0398D = \u03a3D, in which case the curve \u0398\u03b4 is covered at the end of the PE phase.\nIf \u0398\u03b4 is not covered at the end of the PE phase there is a point on \u0398\u03b4 that lies on a cavity entrance (frontier boundary between the free and unknown region) and that is reachable by a path starting from \u03a3D (since C\u03b4 is a connected component of M\u03b4 , a robot could travel along \u0398\u03b4 during the CE phase). Assuming this point is detected by the procedure of Section IV-A, during the CE phase the robot will travel to this point and remove it from its list of cavities to explore, keeping C\u03b4 on its right along the way. It will then continue following a path along C\u03b4 contained in the annulus between \u0398\u03b4 and \u0398D, until \u0398\u03b4 has been entirely covered by the view frustum. The coverage of \u0398\u03b4 terminates since it is a SSC.\nIn conclusion, the cavity entrances computed at the end of the PE phase act as attractors for the robot during the CE phase. However, the robot only covers those frontier voxels that it can reach while still keeping the structure on its right as a guide and remaining at a distance between \u03b4 and D away from it. For example, if it enters a large room after going through a cavity entrance, it will not try to cover the area far away from the walls (hence, it does not try to cover obstacle C3 on Fig. 4). One could potentially attempt to cover these interior areas as well at the same time, e.g., by using a 2D coverage algorithm when we enter a wide cavity, but this would require in general a sufficiently precise absolute positioning system complementing the odometry information of the vSLAM module. This might not be a trivial requirement, for example because the structure itself might obstruct GPS reception. Instead, our algorithm is motivated by the fact that keeping the structure in range helps maintain the accuracy of the visual odometry component of the vSLAM module. By trying to exit a cavity quickly once we enter it, the vSLAM module can also close loops more frequently as the robot returns to the PE curve, before accumulating too much error through the odometry."}, {"heading": "VI. SIMULATION RESULTS", "text": "We illustrate the behavior of our policies via 3D simulations for different sizes of the structure, camera range values and localization accuracy levels for the robot. The implementation of our motion planning policies is integrated with the Robot Operating System (ROS) Navigation Stack [40], which is supported by many mobile ground robots. All the simulations are performed using the Gazebo simulator [27]. The vSLAM algorithm used is RTAB-Map [6].\nThe simulations are carried out with publicly available models of a Clearpath Husky A200 robot and a Kinect depth sensor whose range can be varied [41], see Fig. 2. A UR5 robotic arm is used to carry the sensor, but only yaw motions of the arm are allowed, as described in Section II. For illustration purposes, we consider artificial structures made of\n9\nshort wall-like segments. We refer to the structure used in most of the previous illustrations as the Small \u0393 model. The Large \u0393 model has the same shape as Small \u0393 but is twice the size. We also illustrate the effectiveness of our policy for a realistic model of a house, and compare its performance with that of the classic Frontier-Based Exploration (FBE) algorithm [9]. We have included a supplementary MP4 format video, which shows the simulation and real-world experiments with a Husky robot following our policies for mapping the Small \u0393 model using a Kinect sensor."}, {"heading": "A. Structure Size and Camera Range", "text": "The relative size of the structure with respect to the range of the camera affects the trajectory determined by our algorithms. Fig. 14 shows simulation results for 4 scenarios. With a camera range of 4.5m, the Large \u0393 model is completely mapped at the end of the PE phase. For the Small \u0393 model a cavity remains, which is subsequently explored during the CE phase. Increasing the camera range to say 12m allows the Small \u0393 structure to be mapped at the end of the PE phase as well, see Fig. 14b. Fig. 14d shows that the robot following our policies is able to map large structures with multiple cavities of different sizes. Table I lists the path lengths obtained for the different test cases."}, {"heading": "B. Localization accuracy", "text": "The Husky robot combines data from an Inertial Measurement Unit (IMU), a standard GPS receiver and wheel odometry to achieve a relatively small localization error overall. In order to evaluate the impact of localization accuracy on our algorithms, we simulate the effect of large wheel slippage by introducing a zero mean additive Gaussian white noise to each of the wheel encoder measurements, with a variance equal to k(vx + \u03c9z)/2, where vx is the linear velocity of the robot, \u03c9z is its yaw rate and k is a proportionality constant, also called noise level in the following. Increasing k results in a poorer alignment of the point clouds, but all portions of the structure, except the horizontal faces, are still captured in\nthe reconstructed model, see Fig. 15. Note that our policies compute the next waypoint at discrete times and therefore assume that the drift in localization between waypoints is sufficiently small so that the robot reaches the next waypoint with the camera facing the structure.\nWe use the CloudCompare [42] software to compute the distortion in the reconstructed model Ck, for a noise level k, with respect to a reference point cloud CR which is generated using a different mobile platform with almost perfect localization. First, we register Ck to CR using an Iterative Closest Point (ICP) algorithm [43]. We then define for every point in Ck, its error to be the distance to the nearest neighbor in CR. Table II lists the simulation results for mapping the Small \u0393 model with different noise levels k, where nk is the number of points in Ck and \u00b5, \u03c3,max are respectively the mean, standard deviation and maximum value of the errors of all points in Ck. The table indicates that both the mean and standard deviation of the errors increase with the noise level."}, {"heading": "C. Comparing with Frontier-based Exploration", "text": "The Frontier Exploration [44] package available in ROS relies on a 2D LIDAR to build an occupancy grid that is used to compute the frontiers. The package requires the user to define a 2D polygon that encloses the structure. The algorithm then explores until there are no more frontiers inside the userdefined polygon.\nIn comparison to the FBE algorithm, our algorithms 1) do not require a user defined bounding polygon; 2) maintain as\n10\nmuch as possible a fixed distance from the structure (during the PE phase), thereby ensuring that all portions up to a height of Hmax are mapped; 3) consistently explore the structure while keeping it on the right, which can be important from a user perspective to understand the behavior of the robot. On the other hand, the trajectory prescribed by the FBE algorithm depends on the size of the user-defined bounding polygon. A large bounding polygon will cause the robot to explore areas far away from the structure and will possibly not maintain a fixed direction of exploration. Our strategy produces the same path every time for a given structure whereas the path computed by the FBE algorithm could differ greatly between two trials. The robot also often gets stuck while using the FBE algorithm as the computed waypoints are often too close to the structure.\nIn order to get a quantitative measure of the structure coverage, we use again the CloudCompare software to compute essentially the projection of the reconstructed model C on the reference point cloud CR. Namely, for each point in C we compute the closest point in CR. Note that multiple points in C can have the same closest point in CR. In this case, we remove these duplicate points to obtain the unique closest point set. Then, as long as the reconstructed model aligns relatively well with the reference model, the cardinality of the unique closest point set is taken as our estimate of the structure coverage. Table III compares the level of structure coverage\nachieved by our policies and FBE with a camera range of 4.5m for two of the environments considered. For the Small \u0393 model, our reference point cloud has 6, 116 points with a minimum distance of 0.1m between points. For the House model, our reference point cloud has 10, 889 points with a minimum distance of 0.1m between points. Since the height of the House model is more than Hmax, we only take the portion of the reconstructed model up to the height Hmax for computing the structure coverage and mean error for the two algorithms.\nTable III shows that our policies achieve a higher level of structure coverage than FBE for the environments considered and our proposed coverage metric. Note also that the smooth trajectory prescribed by our policies is beneficial to the vSLAM module to achieve a better alignment and a lower value for the mean error in the reconstructed model, especially if the robot localization accuracy is poor. A visual inspection of Fig. 16a and Fig. 16b shows the improvement in model reconstruction when using our algorithms compared to FBE."}, {"heading": "VII. REAL-WORLD EXPERIMENT", "text": "In this section we show an example of real-world experiment using the Husky robot shown in Fig. 17, which is equipped with a Kinova robotic arm carrying a Kinect v2 depth sensor. A SICK laser range scanner is used only for\n11\nobstacle detection. All computations are done in real-time on an embedded Intel i5 based computer without the help of a dedicated GPU. The structure was built from a series of cork display panels and foam insulation boards. As shown in Fig. 17, it mimics the small \u0393 model used in simulation and its dimensions are 8.2 m on the long edge, and 4 m on the short edges. To help the vSLAM algorithm detect a loop closure, we place a visual marker (a colorful poster seen on Fig. 17) on the structure in front of the starting point of the robot. Since the panels are similar on both sides we also put visual markers inside the structure to confirm that the cavity inspection has correctly mapped all of the inside. The most computationally intensive part of our algorithm is the detection of cavity entrances, see Section IV-A, which takes a few seconds of computation for this structure at the end of the PE phase.\nDue to the limited space available to maneuver around the structure, we reduce the obstacle sensing region to a range of 1.8 m and to a 10 degree cone in front of the robot. The depth camera range is cut at 4.0 m and we set the desired wall distance D to 1.5 m. For odometry we use the extended Kalman filter (EKF) from [45] with IMU and wheel odometry data as inputs. No absolute position fixing system is available. The output of the EKF is sent to RTAB-Map [6] for mapping and localization purposes. Since we use a ground robot on flat terrain, we constrain RTAB-Map\u2019s mapping to 3 degrees of freedom (x, y and yaw angle).\nThe behavior of our algorithm is illustrated on the accompanying video, and the model produced online with the vSLAM module is shown in Fig. 18. Overall, the executed trajectory confirms that our algorithm correctly performs perimeter exploration followed by cavity inspection. Noise in the depth measurements can be an issue if left unfiltered. However, proper calibration and applying a standard speckle and bilateral filter can alleviate the problem. In our experiments, we tuned the parameters of these filters by placing the robot in a location where significant noise was measured. We then increased the maximum speckle size and size of the bilateral filter window until most of the visible noise was removed.\nThe experiment also illustrates some current practical issues that can degrade mapping performance. First, because of the height of the boards used to build the structure and the limited\nspace available to navigate around it, the robotic arm was extended so that the depth sensor was at a height of 1.2m. It then tended to shake during acceleration changes of the robot, making the sensor vulnerable to producing blurry images. This could be mitigated by a better control of the smoothness of the robot trajectories, a stiffer orientable platform to hold the sensor, and by using a stereo camera with global shutters in bright daylight to reduce motion blur. During testing we also noticed that the algorithm can be sensitive to gaps or \u201dwindows\u201d in the structure. In front of a gap, the depth sensor can detect surfaces inside the structure, which can then perturb the goal calculation algorithm described in section III-A. This can be addressed by reducing the range of the sensor measurements to a value close to the desired distance D."}, {"heading": "VIII. CONCLUSIONS", "text": "This paper presents novel motion planning policies that guide a mobile ground robot carrying a camera or depth sensor to autonomously explore the visible portion of a bounded three-dimensional structure. The proposed policies do not assume any prior information about the size or geometry of the structure. Coupled with state-of-art vSLAM systems, our strategies are able to achieve high coverage in the reconstructed model, given the physical limitations of the platform. We illustrate the efficacy of our approach via 3D simulations for different structure sizes, camera range and localization accuracy, and we have tested our system in realworld experiments. In addition, a comparison of our policies with the classical frontier based exploration algorithms clearly shows the improvement in performance for a realistic structure such as a house."}], "references": [{"title": "Autonomous inspection of underwater structures", "author": ["M. Jacobi"], "venue": "Robotics and Autonomous Systems, vol. 67, no. C, pp. 80\u201386, May 2015, special issue on Advances in Autonomous Underwater Robotics.  12", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Detailed 3D reconstruction of large-scale heritage sites with integrated techniques", "author": ["S.F. El-Hakim", "J.A. Beraldin", "M. Picard", "G. Godin"], "venue": "IEEE Computer Graphics and Applications, vol. 24, no. 3, pp. 21\u201329, May-June 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Automated progress tracking using 4D schedule and 3D sensing technologies", "author": ["Y. Turkan", "F. Bosche", "C.T. Haas", "R. Haas"], "venue": "Automation in Construction, vol. 22, pp. 414\u2013421, March 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Progressive 3D reconstruction of infrastructure with videogrammetry", "author": ["I. Brilakis", "H. Fathib", "A. Rashidi"], "venue": "Automation in Construction, vol. 20, no. 7, pp. 884\u2013895, November 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A framework for model-driven acquisition and analytics of visual data using UAVs for automated construction progress monitoring", "author": ["J. Lin", "K. Han", "M. Golparvar-Fard"], "venue": "Computing in Civil Engineering, Austin, Texas, June 2015, pp. 156\u2013164.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Online Global Loop Closure Detection for Large-Scale Multi-Session Graph-Based SLAM", "author": ["M. Labbe", "F. Michaud"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Chicago, IL, September 2014, pp. 2661\u20132666.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "3-D mapping with an RGB-D camera", "author": ["F. Endres", "J. Hess", "J. Sturm", "D. Cremers", "W. Burgard"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 1, pp. 177\u2013187, February 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Active perception", "author": ["R. Bajcsy"], "venue": "Proceedings of the IEEE, vol. 76, no. 8, pp. 966\u20131005, August 1988.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "A frontier-based approach for autonomous exploration", "author": ["B. Yamauchi"], "venue": "Proceedings of the 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation, ser. CIRA \u201997. Washington, DC, USA: IEEE Computer Society, 1997, pp. 146\u2013151.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient next-bestscan planning for autonomous 3D surface reconstruction of unknown objects", "author": ["S. Kriegel", "C. Rink", "T. Bodenm\u00fcller", "M. Suppa"], "venue": "Journal of Real-Time Image Processing, pp. 611\u2013631, December 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous generation of complete 3D object models using next best view manipulation planning", "author": ["M. Krainin", "B. Curless", "D. Fox"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 5031\u20135037.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Structural inspection path planning via iterative viewpoint resampling with application to aerial robotics", "author": ["A. Bircher", "K. Alexis", "M. Burri", "P. Oettershagen", "S. Omari", "T. Mantel", "R. Siegwart"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2015, pp. 6423\u20136430.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Sampling-based coverage path planning for inspection of complex structures", "author": ["B. Englot", "F.S. Hover"], "venue": "International Conference on Automated Planning and Scheduling (ICAPS), Sau Paulo, Brazil, June 2012, pp. 29\u201337.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Autonomous exploration for infrastructure modeling with a micro aerial vehicle", "author": ["L. Yoder", "S. Scherer"], "venue": "Proceedings of the Field and Service Robotics Conference, June 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Navigating a miniature crawler robot for engineered structure inspection", "author": ["W. Sheng", "H. Chen", "N. Xi"], "venue": "IEEE Transactions on Automation Science and Engineering, vol. 5, no. 2, pp. 368\u2013373, April 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "A terrain-covering algorithm for an AUV", "author": ["S. Hert", "S. Tiwari", "V. Lumelsky"], "venue": "Autonomous Robots, vol. 3, no. 2, pp. 91\u2013119, June 1996.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Morse decompositions for coverage tasks", "author": ["E.U. Acar", "H. Choset", "A.A. Rizzi", "P.N. Atkar", "D. Hull"], "venue": "The International Journal of Robotics Research, vol. 21, no. 4, pp. 331\u2013344, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Sensor-based coverage of unknown environments: Incremental construction of Morse decompositions", "author": ["E.U. Acar", "H. Choset"], "venue": "The International Journal of Robotics Research, vol. 21, no. 4, pp. 345\u2013 366, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "A robotic crack inspection and mapping system for bridge deck maintenance", "author": ["R. Lim", "H.M. La", "W. Sheng"], "venue": "IEEE Transactions on Automation Science and Engineering, vol. 11, no. 2, pp. 367\u2013378, April 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the world from internet photo collections", "author": ["N. Snavely", "S.M. Seitz", "R. Szeliski"], "venue": "International Journal of Computer Vision, vol. 80, no. 2, pp. 189\u2013210, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Building Rome on a cloudless day", "author": ["J.-M. Frahm", "P. Fite-Georgel", "D. Gallup", "T. Johnson", "R. Raguram", "C. Wu", "Y.-H. Jen", "E. Dunn", "B. Clipp", "S. Lazebnik", "M. Pollefeys"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part IV, ser. ECCV\u201910. Berlin, Heidelberg: Springer-Verlag, 2010, pp. 368\u2013381.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "VisualSFM: A visual structure from motion system", "author": ["C. Wu"], "venue": "http: //ccwu.me/vsfm/", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Accurate, dense, and robust multiview stereopsis", "author": ["Y. Furukawa", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 8, pp. 1362\u20131376, Aug 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Building with drones: Accurate 3D facade reconstruction using MAVs", "author": ["S. Daftry", "C. Hoppe", "H. Bischof"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, WA, May 2015, pp. 3487\u20133494.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Photocity: Training experts at large-scale image acquisition through a competitive game", "author": ["K. Tuite", "N. Snavely", "D.-y. Hsiao", "N. Tabing", "Z. Popovic"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2011, pp. 1383\u20131392. [Online]. Available: http://doi.acm.org/10.1145/1978942.1979146", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "g2o: A general framework for graph optimization", "author": ["R. Kuemmerle", "G. Grisetti", "H. Strasdat", "K. Konolige", "W. Burgard"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 3607\u20133613.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Choosing where to go: Complete 3D exploration with stereo", "author": ["R. Shade", "P. Newman"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 2806\u20132811.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Autonomous indoor 3D exploration with a micro-aerial vehicle", "author": ["S. Shen", "N. Michael", "V. Kumar"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2012, pp. 9\u201315.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient visual exploration and coverage with a micro aerial vehicle in unknown environments", "author": ["L. Heng", "A. Gotovos", "A. Krause", "M. Pollefeys"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2015, pp. 1071\u20131078.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Decentralized active information acquisition: Theory and application to multi-robot SLAM", "author": ["N. Atanasov", "J. Le Ny", "K. Daniilidis", "G.J. Pappas"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Seattle, Washington, May 2015, pp. 4775\u20134782.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "OctoMap: An efficient probabilistic 3D mapping framework based on octrees", "author": ["A. Hornung", "K.M. Wurm", "M. Bennewitz", "C. Stachniss", "W. Burgard"], "venue": "Autonomous Robots, vol. 34, no. 3, pp. 189\u2013206, April 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-supervised learning to visually detect terrain surfaces for autonomous robots operating in forested terrain", "author": ["S. Zhou", "J. Xi", "M.W. McDaniel", "T. Nishihata", "P. Salesses", "K. Iagnemma"], "venue": "Journal of Field Robotics, vol. 29, no. 2, pp. 277\u2013297, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "A frontier-void-based approach for autonomous exploration in 3D", "author": ["C. Dornhege", "A. Kleiner"], "venue": "Advanced Robotics, vol. 27, pp. 459\u2013468, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "3D is here: Point cloud library (PCL)", "author": ["R.B. Rusu", "S. Cousins"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011, pp. 1\u20134.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating surface normals in noisy point cloud data", "author": ["N.J. Mitra", "A. Nguyen", "L. Guibas"], "venue": "International Journal of Computational Geometry & Applications, vol. 14, no. 04n05, pp. 261\u2013276, 2004.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Real-time obstacle avoidance for manipulators and mobile robots", "author": ["O. Khatib"], "venue": "The International Journal of Robotics Research, vol. 5, no. 1, pp. 90\u201398, 1986.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1986}, {"title": "Principles of Robot Motion: Theory, Algorithms, and Implementations", "author": ["H. Choset", "K.M. Lynch", "S. Hutchinson", "G.A. Kantor", "W. Burgard", "L.E. Kavraki", "S. Thrun"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Discontinuous dynamical systems", "author": ["J. Cortes"], "venue": "IEEE Control Systems Magazine, vol. 28, no. 3, pp. 36\u201373, June 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient variants of the ICP algorithm", "author": ["S. Rusinkiewicz", "M. Levoy"], "venue": "Proceedings of the Third International Conference on 3-D Digital Imaging and Modeling, 2001, Quebec City, Canada, May 2001, pp. 145\u2013 152.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "A generalized extended Kalman filter implementation for the robot operating system", "author": ["T. Moore", "D. Stouch"], "venue": "Proceedings of the 13th International Conference on Intelligent Autonomous Systems (IAS-13). Springer, July 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "sites [1], [2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "sites [1], [2].", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": ", positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": ", positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": ", positioning systems, stationary 3D laser scanners [3], high resolution video cameras [4], or still cameras carried by UAVs [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "This is a widely researched problem called Visual Simultaneous Localization and Mapping (vSLAM) or real-time Structure from Motion (SfM), for which several open source packages offer increasingly accurate and efficient solutions [6], [7].", "startOffset": 229, "endOffset": 232}, {"referenceID": 6, "context": "This is a widely researched problem called Visual Simultaneous Localization and Mapping (vSLAM) or real-time Structure from Motion (SfM), for which several open source packages offer increasingly accurate and efficient solutions [6], [7].", "startOffset": 234, "endOffset": 237}, {"referenceID": 7, "context": "The second problem relates to active sensing [8], as we need motion planning strategies that can guide a mobile sensor to explore the structure of interest.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "For mapping, monitoring or inspection applications, certain classical strategies such as frontier-based exploration algorithms [9], which guide the robot to previously unexplored regions irrespective of whether it is part of the structure of interest or not, are not necessarily well adapted.", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "Some recent work considers the problem of reconstructing a 3D model of arbitrary objects by moving a depth sensor relative to the object using different forms of next best view planning algorithms [10], [11].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Some recent work considers the problem of reconstructing a 3D model of arbitrary objects by moving a depth sensor relative to the object using different forms of next best view planning algorithms [10], [11].", "startOffset": 203, "endOffset": 207}, {"referenceID": 4, "context": "The related problem of automated inspection deals with large structures such as tall buildings [5] and ship hulls.", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "[12] assume that a prior 3D mesh of the structure to inspect is available and compute a short path connecting viewpoints that together are guaranteed", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In [13], Englot et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Yoder and Scherer [14] also assume a bounding box and develop an algorithm combining next best view planning and frontierbased exploration to encourage coverage of the structure.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "[15] use a prior CAD model of an aircraft to plan a path for a robotic crawler such that it inspects all the rivets on the surface of the aircraft.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": ", [16]\u2013[19] and the references therein, which has traditionally focused on developing algorithms ensuring that a mobile robot passes over all points in a 2D environment, assuming a sufficiently accurate localization system.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [16]\u2013[19] and the references therein, which has traditionally focused on developing algorithms ensuring that a mobile robot passes over all points in a 2D environment, assuming a sufficiently accurate localization system.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "In computer vision and photogrammetry, SfM techniques aim at building a 3D model of a scene from a large number of images [20]\u2013[23], but most of this work focuses on batch post-processing and typically assumes a given dataset, whereas here our focus is essentially on how to acquire an appropriate set of images.", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "In computer vision and photogrammetry, SfM techniques aim at building a 3D model of a scene from a large number of images [20]\u2013[23], but most of this work focuses on batch post-processing and typically assumes a given dataset, whereas here our focus is essentially on how to acquire an appropriate set of images.", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "[24], which presents an interactive real-time SfM system providing online feedback to the user taking pictures, alerting him or her when a new picture cannot be properly integrated in the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] develop a competitive game where players are encouraged to take pictures that help build complete 3D models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We emphasize that we do not discuss in details the task of actually building a model from a collection of pictures or depth maps, which can be executed by one of the available vSLAM or real-time SfM systems, such as the Real-Time Appearance Based Mapping package (RTAB-Map) [6] that we use in our simulations.", "startOffset": 274, "endOffset": 277}, {"referenceID": 25, "context": "Which package we use for model reconstruction has little influence on our algorithms, for example any vSLAM system based on pose-graph optimization [26] could be used.", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "Finally, another line of work in informative path planning relates to autonomous exploration and coverage of relatively large environments, using variants of frontier based exploration algorithms for example [28]\u2013[31].", "startOffset": 208, "endOffset": 212}, {"referenceID": 29, "context": "Finally, another line of work in informative path planning relates to autonomous exploration and coverage of relatively large environments, using variants of frontier based exploration algorithms for example [28]\u2013[31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 5, "context": "in a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "in a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32].", "startOffset": 102, "endOffset": 105}, {"referenceID": 30, "context": "in a coordinate frame in real-time using available SLAM algorithms, such as RTAB-Map [6] or RGBD-SLAM [7], and post-processing then allows us to build a dense 3D model or a 3D occupancy grid stored in an OctoMap [32].", "startOffset": 212, "endOffset": 216}, {"referenceID": 31, "context": "Assumption 3 could be removed by using recent classification systems that can differentiate between ground and non-ground regions [33] to pre-process the point clouds before sending them to our system.", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23].", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "robotic arm carrying the sensor [10], [34], as well as targeted computer vision techniques [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "For its implementation we rely on the Point Cloud Library (PCL) [35].", "startOffset": 64, "endOffset": 68}, {"referenceID": 34, "context": "On line 5, following [36], we compute via Principal Component Analysis (PCA) the normal direction to that plane \u03a0 which best fits S.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "along the way, we use a local path planner based on potential fields [37], [38].", "startOffset": 69, "endOffset": 73}, {"referenceID": 36, "context": "along the way, we use a local path planner based on potential fields [37], [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "The path will then slide on \u2202MD until it reaches its goal [39].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "We use a voxel based 3D occupancy grid constructed from the global point cloud, and maintained in a hierarchical tree data structure by the OctoMap [32] library.", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "Using the constructed OctoMap, we compute a set of frontier voxels, whose definition is adapted from [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 34, "context": "We can ignore them by only considering frontier voxels for which the normal vector n, computed using the nearby frontier voxels [36], makes a sufficiently small angle with the horizontal plane.", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "The cavity entrance voxels are clustered using an Euclidean clustering algorithm from PCL [35] and each cluster is referred to as a cavity entrance.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The vSLAM algorithm used is RTAB-Map [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "We also illustrate the effectiveness of our policy for a realistic model of a house, and compare its performance with that of the classic Frontier-Based Exploration (FBE) algorithm [9].", "startOffset": 181, "endOffset": 184}, {"referenceID": 38, "context": "First, we register Ck to CR using an Iterative Closest Point (ICP) algorithm [43].", "startOffset": 77, "endOffset": 81}, {"referenceID": 39, "context": "For odometry we use the extended Kalman filter (EKF) from [45] with IMU and wheel odometry data as inputs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "The output of the EKF is sent to RTAB-Map [6] for mapping and localization purposes.", "startOffset": 42, "endOffset": 45}], "year": 2017, "abstractText": "This paper presents a strategy to guide a mobile ground robot equipped with a camera or depth sensor, in order to autonomously map the visible part of a bounded threedimensional structure. We describe motion planning algorithms that determine appropriate successive viewpoints and attempt to fill holes automatically in a point cloud produced by the sensing and perception layer. The emphasis is on accurately reconstructing a 3D model of a structure of moderate size rather than mapping large open environments, with applications for example in architecture, construction and inspection. The proposed algorithms do not require any initialization in the form of a mesh model or a bounding box, and the paths generated are well adapted to situations where the vision sensor is used simultaneously for mapping and for localizing the robot, in the absence of additional absolute positioning system. We analyze the coverage properties of our policy, and compare its performance to the classic frontier based exploration algorithm. We illustrate its efficacy for different structure sizes, levels of localization accuracy and range of the depth sensor, and validate our design on a realworld experiment. Note to Practitioners\u2014 The objective of this work is to automate the process of building a 3D model of a structure of interest that is as complete as possible, using a mobile camera or depth sensor, in the absence of any prior information about this structure. Given that increasingly robust solutions for the Visual Simultaneous Localization and Mapping problem (vSLAM) are now readily available, the key challenge that we address here is to develop motion planning policies to control the trajectory of the sensor in a way that improves the mapping performance. We target in particular scenarios were no external absolute positioning system is available, such as mapping certain indoor environments where GPS signals are blocked. In this case, it is often important to revisit previously seen locations relatively quickly, in order to avoid excessive drift in the dead-reckoning localization system. Our system works by first determining the boundaries of the structure, before attempting to fill the holes in the constructed model. Its performance is illustrated through simulations and a real-world experiment performed with a depth sensor carried by a mobile manipulator.", "creator": "LaTeX with hyperref package"}}}