{"id": "1602.06872", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Principal Component Projection Without Principal Component Analysis", "abstract": "we show how to efficiently derive a vector onto the top principal components of a matrix, without explicitly computing these components. specifically, planners introduce an agile algorithm that greatly computes the projection using few alternatives to any black - box routine for ridge regression.", "histories": [["v1", "Mon, 22 Feb 2016 17:52:02 GMT  (304kb)", "http://arxiv.org/abs/1602.06872v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG stat.ML", "authors": ["roy frostig", "cameron musco", "christopher musco", "aaron sidford"], "accepted": true, "id": "1602.06872"}, "pdf": {"name": "1602.06872.pdf", "metadata": {"source": "CRF", "title": "Principal Component Projection Without Principal Component Analysis", "authors": ["Roy Frostig", "Christopher Musco"], "emails": ["rf@cs.stanford.edu", "cnmusco@mit.edu", "cpmusco@mit.edu", "asid@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n06 87\n2v 1\n[ cs\n.D S]\nBy avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression.\nTo achieve our results, we first observe that ridge regression can be used to obtain a \u201csmooth projection\u201d onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision."}, {"heading": "1 Introduction", "text": "In machine learning and statistics, it is common \u2013 and often essential \u2013 to represent data in a concise form that decreases noise and increases efficiency in downstream tasks. Perhaps the most widespread method for doing so is to project data onto the linear subspace spanned by its directions of highest variance \u2013 that is, onto the span of the top components given by principal component analysis (PCA). Computing principal components can be an expensive task, a challenge that prompts a basic algorithmic question:\nCan we project a vector onto the span of a matrix\u2019s top principal components without performing principal component analysis?\nThis paper answers that question in the affirmative, demonstrating that projection is much easier than PCA itself. We show that it can be solved using a simple iterative algorithm based on black-box calls to a ridge regression routine. The algorithm\u2019s runtime does not depend on the number of top principal components chosen for projection, a cost inherent to any algorithm for PCA, or even algorithms that just compute an orthogonal span for the top components."}, {"heading": "1.1 Motivation: principal component regression", "text": "To motivate our projection problem, consider one of the most basic downstream applications for PCA: linear regression. Combined, PCA and regression comprise the principal component regression (PCR) problem:\nDefinition 1.1 (Principal component regression (PCR)). Let A \u2208 Rn\u00d7d be a design matrix whose rows are data points and let b \u2208 Rd be a vector of data labels. Let A\u03bb denote the result of projecting each row of A onto the span of the top principal components of A \u2013 in particular the eigenvectors of the covariance matrix 1nA TA whose corresponding variance (eigenvalue) exceeds a threshold \u03bb. The task of PCR is to find a minimizer of the squared loss \u2016A\u03bbx\u2212 b\u201622. In other words, the goal is to compute A\u2020\u03bbb, where A \u2020 \u03bb is the Moore-Penrose pseudoinverse of A\u03bb.\nPCR is a key regularization method in statistics, numerical linear algebra, and scientific disciplines including chemometrics [Hot57, Han87, FF93]. It models the assumption that small principal components represent noise rather than data signal. PCR is typically solved by first using PCA to compute A\u03bb and then applying linear regression. The PCA step dominates the algorithm\u2019s cost, especially if many principal components have variance above the threshold \u03bb.\nWe remedy this issue by showing that our principal component projection algorithm yields a fast algorithm for regression. Specifically, full access to A\u03bb is unnecessary for PCR: A \u2020 \u03bbb can be computed efficiently given only an approximate projection of the vector ATb onto A\u2019s top principal components. By solving projection without PCA we obtain the first PCA-free algorithm for PCR."}, {"heading": "1.2 A first approximation: ridge regression", "text": "Interestingly, our main approach to efficient principal component projection is based on a common alternative to PCR: ridge regression. This ubiquitous regularization method computes a minimizer of \u2016Ax\u2212 b\u201622+\u03bb\u2016x\u201622 for some regularization parameter \u03bb [Tik63]. The advantage of ridge regression is its formulation as a simple convex optimization problem that can be solved efficiently using many techniques (see Lemma 2.1).\nSolving ridge regression is equivalent to applying the matrix (ATA + \u03bbI)\u22121AT, an operation that can be viewed as a smooth relaxation of PCR. Adding the \u21132 norm penalty (i.e. \u03bbI) effectively \u201cwashes out\u201d A\u2019s small principal components in comparison to its large ones and achieves an effect similar to PCR at the extreme ends of A\u2019s spectrum.\nAccordingly, ridge regression gives access to a \u201csmooth projection\u201d operator, (ATA+ \u03bbI)\u22121ATA. This matrix approximates PA\u03bb , the projection matrix onto A\u2019s top principal components. Both have the same singular vectors, but PA\u03bb has a singular value of 1 for each squared singular value \u03c3 2 i \u2265 \u03bb in A and a singular value of 0 for each \u03c32i < \u03bb, whereas (A TA + \u03bbI)\u22121ATA has singular values equal to \u03c32 i\n\u03c32 i +\u03bb\n. This function\napproaches 1 when \u03c32i is much greater than \u03bb and 0 when it is smaller. Figure 1 illustrates the comparison.\nUnfortunately, ridge regression is a very crude approximation to PCR and projection in many settings and may perform significantly worse in certain data analysis applications [DFKU13]. In short, while ridge regression algorithms are valuable tools, it has been unclear how to wield them for tasks like projection or PCR."}, {"heading": "1.3 Main result: from ridge regression to projection", "text": "We show that it is possible to sharpen the weak approximation given by ridge regression. Specifically, there exists a low degree polynomial p(\u00b7) such that p ( (ATA+ \u03bbI)\u22121ATA )\ny provides a very accurate approximation to PA\u03bby for any vector y. Moreover, the polynomial can be evaluated as a recurrence, which translates into a simple iterative algorithm: we can apply the sharpened approximation to a vector by repeatedly applying any ridge regression routine a small number of times.\nTheorem 1.2 (Principal component projection without PCA). Given A \u2208 Rn\u00d7d and y \u2208 Rd, Algorithm 1 uses O\u0303(\u03b3\u22122 log(1/\u01eb)) approximate applications of (ATA+ \u03bbI)\u22121 and returns x with \u2016x\u2212PA\u03bby\u20162 \u2264 \u01eb\u2016y\u20162.\nLike most iterative PCA algorithms, our running time scales inversely with \u03b3, the spectral gap around \u03bb. Notably, it does not depend on the number of principal components in A\u03bb, a cost incurred by any method that applies the projection PA\u03bb directly, either by explicitly computing the top principal components of A, or even by just computing an orthogonal span for these components.\nAs mentioned, the above theorem also yields an algorithm for principal component regression that computes A\u2020\u03bbb without finding A\u03bb. We achieve this result by introducing a robust reduction from projection to PCR, that again relies on ridge regression as a computational primitive.\nCorollary 1.3 (Principal component regression without PCA). Given A \u2208 Rn\u00d7d and b \u2208 Rn, Algorithm 2 uses O\u0303(\u03b3\u22122 log(1/\u01eb)) approximate applications of (ATA+\u03bbI)\u22121 and returns x with \u2016x\u2212A\u2020\u03bbb\u2016ATA \u2264 \u01eb\u2016b\u20162.\nCorollary 1.3 gives the first known algorithm for PCR that avoids the cost of principal component analysis."}, {"heading": "1.4 Related work", "text": "A number of papers attempt to alleviate the high cost of principal component analysis when solving PCR. It has been shown that an approximation to A\u03bb suffices for solving the regression problem [CH90, BMI14]. Unfortunately, even the fastest approximations are much slower than routines for ridge regression and inherently incur a linear dependence on the number of principal components above \u03bb.\nMore closely related to our approach is work on thematrix sign function, an important operation in control theory, quantum chromodynamics, and scientific computing in general. Approximating the sign function often involves matrix polynomials similar to our \u201csharpening polynomial\u201d that converts ridge regression to principal component projection. Significant effort addresses Krylov methods for applying such operators without computing them explicitly [vdEFL+02, FS08].\nOur work differs from these methods in an important way: since we only assume access to an approximate ridge regression algorithm, it is essential that our sharpening step is robust to noise. Our iterative polynomial construction allows for a complete and rigorous noise analysis that is not available for Krylov methods, while at the same time eliminating space and post-processing costs. Iterative approximations to the matrix sign function have been proposed, but lack rigorous noise analysis [Hig08]."}, {"heading": "1.5 Paper layout", "text": "Section 2: Mathematical and algorithmic preliminaries.\nSection 3: Develop a PCA-free algorithm for principal component projection based on a ridge regression subroutine.\nSection 4: Show how our approximate projection algorithm can be used to solve PCR, again without PCA.\nSection 5: Detail our iterative approach to sharpening the smooth ridge regression projection towards true projection via a low degree sharpening polynomial.\nSection 6: Empirical evaluation of our principal component projection and regression algorithms."}, {"heading": "2 Preliminaries", "text": "Singular value decomposition. Any matrix A \u2208 Rn\u00d7d of rank r has a singular value decomposition (SVD) A = U\u03a3VT, where U \u2208 Rn\u00d7r and V \u2208 Rd\u00d7r both have orthonormal columns and \u03a3 \u2208 Rr\u00d7r is a diagonal matrix. The columns of U and V are the left and right singular vectors of A. Moreover, \u03a3 = diag(\u03c31(A), ..., \u03c3r(A)), where \u03c31(A) \u2265 \u03c32(A) \u2265 ... \u2265 \u03c3r(A) > 0 are the singular values of A in decreasing order.\nThe columns of V are the eigenvectors of the covariance matrix ATA \u2013 that is, the principal components of the data \u2013 and the eigenvalues of the covariance matrix are the squares of the singular values \u03c31, . . . , \u03c3r.\nFunctions of matrices. If f : R \u2192 R is a scalar function and S = diag(s1, . . . , sn) is a diagonal matrix, we define by f(S) def = diag(f(s1), . . . , f(sn)) the entrywise application of f to the diagonal. For a non-diagonal matrix A with SVD A = U\u03a3VT we define f(A) def = Uf(\u03a3)VT.\nMatrix pseudoinverse. We define the pseudoinverse of A as A\u2020 = f(A)T where f(x) = 1/x. The pseudoinverse is essential in the context of regression, as the vector A\u2020b minimizes the squared error \u2016Ax\u2212 b\u201622.\nPrincipal component projection. Given a threshold \u03bb > 0 let k be the largest index with \u03c3k(A) 2 \u2265 \u03bb and define:\nA\u03bb def = Udiag(\u03c31, . . . , \u03c3k, 0, . . . , 0)V T.\nThe matrix A\u03bb contains A\u2019s rows projected to the span of all principal components having squared singular value at least \u03bb. We sometimes write A\u03bb = APA\u03bb where PA\u03bb \u2208 Rd\u00d7d is the projection onto these top components. Here PA\u03bb = f(A TA) where f(x) is a step function: 0 if x < \u03bb and 1 if x \u2265 \u03bb.\nMiscellaneous notation. For any positive semidefinite M,N \u2208 Rd\u00d7d we use N M to denote that M\u2212N is positive semidefinite. For any x \u2208 Rd, \u2016x\u2016M def= \u221a xTMx.\nRidge regression. Ridge regression is the problem of computing, given a regularization parameter \u03bb > 0:\nx\u03bb def = argmin x\u2208Rd \u2016Ax\u2212 b\u201622 + \u03bb\u2016x\u201622. (1)\nThe solution to (1) is given by x\u03bb = ( ATA+ \u03bbI )\u22121 ATb. Applying the matrix ( ATA+ \u03bbI )\u22121\nto ATb is equivalent to solving the convex minimization problem:\nx\u03bb def = argmin\nx\u2208Rd\n1 2 xTATAx\u2212 yTx+ \u03bb\u2016x\u201622.\nA vast literature studies solving problems of this form via (accelerated) gradient descent, stochastic variants, and random sketching [Nes83, NN13, SSZ14, LLX14, FGKS15, CLM+15]. We summarize a few, now standard, runtimes achievable by these iterative methods:\nLemma 2.1 (Ridge regression runtimes). Given y \u2208 Rd let x\u2217 = (ATA + \u03bbI)\u22121y. There is an algorithm, ridge(A, \u03bb,y, \u01eb) that, for any \u01eb > 0, returns x\u0303 such that\n\u2016x\u0303\u2212 x\u2217\u2016ATA+\u03bbI \u2264 \u01eb\u2016y\u2016(ATA+\u03bbI)\u22121 .\nIt runs in time Tridge(A, \u03bb, \u01eb) = O ( nnz(A) \u221a \u03ba\u03bb \u00b7 log(1/\u01eb) ) where \u03ba\u03bb = \u03c3 2 1(A)/\u03bb is the condition number of the regularized system and nnz(A) is the number of nonzero entries in A. There is a also stochastic algorithm that, for any \u03b4 > 0, gives the same guarantee with probability 1\u2212 \u03b4 in time\nTridge(A, \u03bb, \u01eb, \u03b4) = O ((nnz(A) + d sr(A)\u03ba\u03bb) \u00b7 log(1/\u03b4\u01eb)) ,\nwhere sr(A) = \u2016A\u20162F /\u2016A\u201622 is A\u2019s stable rank. When nnz(A) \u2265 d sr(A)\u03ba\u03bb the runtime can be improved to\nTridge(A, \u03bb, \u01eb, \u03b4) = O\u0303( \u221a nnz(A) \u00b7 d sr(A)\u03ba\u03bb \u00b7 log(1/\u03b4\u01eb)),\nwhere the O\u0303 hides a factor of log (\nd sr(A)\u03ba\u03bb nnz(A)\n)\n.\nNote that typically, the regularized condition number \u03ba\u03bb will be significantly smaller than the full condition number of ATA."}, {"heading": "3 From ridge regression to principal component projection", "text": "We now describe how to approximately apply PA\u03bb using any black-box ridge regression routine. The key idea is to first compute a soft step function of ATA via ridge regression, and then to sharpen this step to approximate PA\u03bb . Let Bx = (A\nTA+ \u03bbI)\u22121(ATA)x be the result of applying ridge regression to (ATA)x. In the language of functions of matrices, we have B = r(ATA), where\nr(x) def =\nx\nx+ \u03bb .\nThe function r(x) is a smooth step about \u03bb (see Figure 1). It primarily serves to map the eigenvalues of ATA to the range [0, 1], mapping those exceeding the threshold \u03bb to a value above 1/2 and the rest to a value below 1/2. To approximate the projection PA\u03bb , it would now suffice to apply a simple symmetric step function:\ns(x) =\n{\n0 if x < 1/2 1 if x \u2265 1/2\nIt is easy to see that s(B) = s(r(ATA)) = PA\u03bb . For x \u2265 \u03bb, r(x) \u2265 1/2 and so s(r(x)) = 1. Similarly for x < \u03bb, r(x) < 1/2 and hence s(r(x)) = 0. That is, the symmetric step function exactly converts our smooth ridge regression step to the true projection operator."}, {"heading": "3.1 Polynomial approximation to the step function", "text": "While computing s(B) directly is expensive, requiring the SVD of B, we show how to approximate this function with a low-degree polynomial. We also show how to apply this polynomial efficiently and stably using a simple iterative algorithm. Our main result, proven in Section 5, is:\nLemma 3.1 (Step function algorithm). Let S \u2208 Rd\u00d7d be symmetric with every eigenvalue \u03c3 satisfying \u03c3 \u2208 [0, 1] and |\u03c3 \u2212 1/2| \u2265 \u03b3. Let A denote a procedure that on x \u2208 Rd produces A(x) with \u2016A(x) \u2212 Sx\u20162 = O(\u01eb2\u03b32)\u2016x\u20162. Given y \u2208 Rd set s0 := A(y), w0 := s0 \u2212 12y, and for k \u2265 0 set\nwk+1 := 4\n(\n2k + 1\n2k + 2\n)\nA(wk \u2212A(wk))\nand sk+1 := sk +wk+1. If all arithmetic operations are performed with \u2126(log(d/\u01eb\u03b3)) bits of precision then \u2016sq \u2212 s(S)y\u20162 = O(\u01eb)\u2016y\u20162 for q = \u0398(\u03b3\u22122 log(1/\u01eb)).\nNote that the output sq is an approximation to a 2q degree polynomial of S applied to y. In Algorithm 1, we give pseudocode for combining the procedure with ridge regression to solve principal component projection. Set S = B and let A be an algorithm that approximately applies B to any x by applying approximate ridge regression to ATAx. As long as B has no eigenvalues falling within \u03b3 of 1/2, the lemma ensures \u2016sq \u2212PA\u03bby\u20162 = O(\u01eb)\u2016y\u20162. This requires \u03b3 on order of the spectral gap: 1\u2212 \u03c32k+1(A)/\u03c32k(A), where k is the largest index with \u03c32k(A) \u2265 \u03bb.\nAlgorithm 1 (pc-proj) Principal component projection input: A \u2208 Rn\u00d7d, y \u2208 Rd, error \u01eb, failure rate \u03b4, threshold \u03bb, gap \u03b3 \u2208 (0, 1) q := c1\u03b3\n\u22122 log(1/\u01eb) \u01eb\u2032 := c\u221212 \u01eb 2\u03b32/ \u221a \u03ba\u03bb, \u03b4\n\u2032 := \u03b4/(2q) s := ridge(A, \u03bb,ATAy, \u01eb\u2032, \u03b4\u2032) w := s\u2212 12y for k = 0, ..., q \u2212 1 do t := w \u2212 ridge(A, \u03bb,ATAw, \u01eb\u2032, \u03b4\u2032) w := 4 (\n2k+1 2k+2\n)\nridge(A, \u03bb,ATAt, \u01eb\u2032, \u03b4\u2032)\ns := s +w end for return s\nTheorem 3.2. If 11\u22124\u03b3\u03c3k+1(A) 2 \u2264 \u03bb \u2264 (1\u2212 4\u03b3)\u03c3k(A)2 and c1, c2 are sufficiently large constants, pc-proj (Algorithm 1) returns s such that with probability \u2265 1\u2212 \u03b4,\n\u2016s\u2212PA\u03bby\u20162 \u2264 \u01eb\u2016y\u20162.\nThe algorithm requires O(\u03b3\u22122 log(1/\u01eb)) ridge regression calls, each costing Tridge(A, \u03bb, \u01eb \u2032, \u03b4\u2032). Lemma 2.1 yields total cost (with no failure probability)\nO ( nnz(A) \u221a \u03ba\u03bb\u03b3 \u22122 log(1/\u01eb) log (\u03ba\u03bb/(\u01eb\u03b3)) )\nor, via stochastic methods,\nO\u0303 ( (nnz(A+ d sr(A)\u03ba\u03bb) \u03b3 \u22122 log(1/\u01eb) log(\u03ba\u03bb/(\u01eb\u03b3\u03b4)) )\nwith acceleration possible when nnz(A) > d sr(A)\u03ba\u03bb.\nProof. We instantiate Lemma 3.1. Let S = B = (ATA+ \u03bbI)\u22121ATA. As discussed, B = r(ATA) and hence all its eigenvalues fall in [0, 1]. Specifically, \u03c3i(B) = \u03c3i(A) 2\n\u03c3i(A)2+\u03bb . Now, \u03c3k(B) \u2265 \u03bb/(1\u22124\u03b3)\u03bb/(1\u22124\u03b3)+\u03bb = 12\u22124\u03b3 \u2265 12 + \u03b3\nand similarly \u03c3k+1(B) \u2264 \u03bb(1\u22124\u03b3)\u03bb(1\u22124\u03b3)+\u03bb = 1\u22124\u03b3 2\u22124\u03b3 \u2264 12 \u2212 \u03b3, so all eigenvalues of B are at least \u03b3 far from 1/2. By Lemma 2.1, for any x, with probability \u2265 1\u2212 \u03b4\u2032:\n\u2016ridge(A,\u03bb,ATAx, \u01eb\u2032, \u03b4\u2032)\u2212Bx\u2016ATA+\u03bbI \u2264 \u01eb\u2032\u2016ATAx\u2016(ATA+\u03bbI)\u22121 \u2264 \u03c31(A)\u01eb\u2032\u2016x\u20162.\nSince the minimum eigenvalue of ATA+ \u03bbI is \u03bb:\n\u2016ridge(A, \u03bb,ATAx, \u01eb\u2032, \u03b4\u2032)\u2212Bx\u20162\n\u2264 \u03c31(A)\u221a \u03bb\n\u01eb\u2032\u2016x\u20162 \u2264 \u221a \u03ba\u03bb\u01eb 2\u03b32\nc2 \u221a \u03ba\u03bb \u2016x\u20162 = O(\u01eb2\u03b32)\u2016x\u20162.\nApplying the union bound over all 2q calls of ridge, this bound holds for all calls with probability \u2265 1\u2212 \u03b4\u2032 \u00b7 2q = 1\u2212 \u03b4. So, overall, by Lemma 3.1, with probability at least 1\u2212 \u03b4, \u2016s\u2212 s(B)y\u20162 = O(\u01eb)\u2016y\u20162. As discussed, s(B) = PA\u03bb . Adjusting constants on \u01eb (via c1 and c2) completes the proof.\nNote that the runtime of Theorem 3.2 includes a dependence on \u221a \u03ba\u03bb. In performing principal component projection, pc-proc applies an asymmetric step function to ATA. The optimal polynomial for approximating this step also has a \u221a \u03ba\u03bb dependence [EY11], showing that our reduction from projection to ridge regression is optimal in this regard."}, {"heading": "3.2 Choosing \u03bb and \u03b3", "text": "Theorem 3.2 requires \u03c3k+1(A) 2\n1\u22124\u03b3 \u2264 \u03bb \u2264 (1\u2212 4\u03b3)\u03c3k(A)2. If \u03bb is chosen approximately equidistant from the two eigenvalues, we need \u03b3 = O(1 \u2212 \u03c32k+1(A)/\u03c32k(A)).\nIn practice, however, it is unnecessary to explicitly specify \u03b3 or to choose \u03bb so precisely. With q = O(\u03b3\u22122 log(1/\u01eb)) our projection will be approximately correct on all singular values outside the range [(1 \u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb]. If there are any \u201cintermediate\u201d singular values in this range, as shown in Section 5, the approximate step function applied by Lemma 3.1 will map these values to [0, 1] via a monotonically increasing soft step. That is, Algorithm 1 gives a slightly softened projection \u2013 removing any principal directions with value < (1\u2212 \u03b3)\u03bb, keeping any with value > (1 + \u03b3)\u03bb and partially projecting away any in between."}, {"heading": "4 From principal component projection to principal component", "text": "regression\nA major motivation for an efficient, PCA-free method for projecting a vector onto the span of top principal components is principal component regression (PCR). Recall that PCR solves the following problem:\nA \u2020 \u03bbb = argmin x\u2208Rd \u2016A\u03bbx\u2212 b\u201622.\nIn exact arithmetic, A\u2020\u03bbb is equal to (A TA)\u22121PA\u03bbA\nTb. This identity suggests a method for computing the solution to ridge regression without finding A\u03bb explicitly: first apply a principal component projection algorithm to ATb and then solve a linear system to apply (ATA)\u22121.\nUnfortunately, this approach is disastrously unstable, not only when PA\u03bb is applied approximately, but in any finite precision environment. Accordingly, we present a modified method for obtaining PCA-free regression from projection."}, {"heading": "4.1 Stable inversion via ridge regression", "text": "Let y = PA\u03bbA Tb and suppose we have some y\u0303 \u2248 y (e.g. obtained from Algorithm 1). The issue with the first approach mentioned is that since (ATA)\u22121 could have a very large maximum eigenvalue, we cannot guarantee (ATA)\u22121y\u0303 \u2248 (ATA)\u22121y. On the other hand, applying the ridge regression operator (ATA+\u03bbI)\u22121 to y\u0303 is much more stable since it has a maximum eigenvalue of 1/\u03bb, so (ATA + \u03bbI)\u22121y\u0303 will approximate (ATA+ \u03bbI)\u22121y well.\nIn short, it is more stable to apply (ATA+\u03bbI)\u22121y = f(ATA)y, where f(x) = 1x+\u03bb , but the goal in PCR is to apply (ATA)\u22121 = h(ATA) where h(x) = 1/x. So, in order to go from one function to the other, we use a correction function g(x) = x1\u2212\u03bbx . By simple calculation,\nA \u2020 \u03bbb = (A TA)\u22121y = g((ATA+ \u03bbI)\u22121)y.\nAdditionally, we can stably approximate g(x) with an iteratively computed low degree polynomial! Specifically, we use a truncation of the series g(x) =\n\u2211\u221e i=1 \u03bb i\u22121xi. An exact approximation to g(x) would exactly apply (ATA)\u22121, which as discussed, is unstable due to very large eigenvalues (corresponding to small eigenvalues of ATA). Our approximation to g(x) is accurate on the large eigenvalues of ATA but inaccurate on the small eigenvalues. This turns out to be the key to the stability of our algorithm. By not \u201cfully inverting\u201d these eigenvalues, our polynomial approximation avoids the instability of applying the true inverse (ATA)\u22121. We provide a complete error analysis in Appendix B, the upshot of which is the following:\nLemma 4.1 (PCR approximation algorithm). Let A be a procedure that, given x \u2208 Rd, produces A(x) with \u2016A(x)\u2212 (ATA+ \u03bbI)\u22121x\u2016ATA+\u03bbI = O( \u01ebq2\u03c31(A) )\u2016x\u20162. Let B be a procedure that, given x \u2208 R\nd produces B(x) with \u2016B(x)\u2212PA\u03bbx\u20162 = O( \u01ebq2\u221a\u03ba\u03bb )\u2016x\u20162. Given b \u2208 R n set s0 := B(ATb) and s1 := A(s0). For k \u2265 1 set:\nsk+1 := s1 + \u03bb \u00b7 A(sk).\nIf all arithmetic operations are performed with \u2126(log(d/q\u01eb)) bits of precision then \u2016sq\u2212A\u2020\u03bbb\u2016ATA = O(\u01eb)\u2016b\u20162 for q = \u0398(log(\u03ba\u03bb/\u01eb)).\nWe instantiate the iterative procedure above in Algorithm 2. pc-proj(A, \u03bb,y, \u03b3, \u01eb, \u03b4) denotes a call to Algorithm 1.\nAlgorithm 2 (ridge-pcr) Ridge regression-based PCR input: A \u2208 Rn\u00d7d, b \u2208 Rn, error \u01eb, failure rate \u03b4, threshold \u03bb, gap \u03b3 \u2208 (0, 1) q := c1 log(\u03ba\u03bb/\u01eb) \u01eb\u2032 := c\u221212 \u01eb/(q\n2\u221a\u03ba\u03bb), \u03b4\u2032 = \u03b4/2(q + 1) y := pc-proj(A, \u03bb,ATb, \u03b3, \u01eb\u2032, \u03b4/2) s0 := ridge(A, \u03bb,y, \u01eb\n\u2032, \u03b4\u2032), s := s0 for k = 1, ..., q do s := s0 + \u03bb \u00b7 ridge(A, \u03bb, s, \u01eb\u2032, \u03b4\u2032) end for return s\nTheorem 4.2. If 11\u22124\u03b3\u03c3k+1(A) 2 \u2264 \u03bb \u2264 (1\u22124\u03b3)\u03c3k(A)2 and c1, c2 are sufficiently large constants, ridge-pcr (Algorithm 2) returns s such that with probability \u2265 1\u2212 \u03b4,\n\u2016s\u2212A\u2020\u03bbb\u2016ATA \u2264 \u01eb\u2016b\u20162.\nThe algorithm makes one call to pc-proj and O(log(\u03ba\u03bb/\u01eb)) calls to ridge regression, each of which costs Tridge(A, \u03bb, \u01eb \u2032, \u03b4\u2032), so Lemma 2.1 and Theorem 3.2 imply a total runtime of\nO\u0303(nnz(A) \u221a \u03ba\u03bb\u03b3 \u22122 log2 (\u03ba\u03bb/(\u01eb\u03b3))),\nwhere O\u0303 hides log log(1/\u01eb), or, with stochastic methods,\nO\u0303((nnz(A) + d sr(A)\u03ba\u03bb)\u03b3 \u22122 log2 (\u03ba\u03bb/(\u01eb\u03b3\u03b4))).\nProof. We apply Lemma 4.1; A is given by ridge(A, \u03bb,x, \u01eb\u2032, \u03b4\u2032). Since \u2016(ATA+ \u03bbI)\u22121\u20162 < 1/\u03bb, Lemma 2.1 states that with probability 1\u2212 \u03b4\u2032,\n\u2016A(x)\u2212 (ATA+ \u03bbI)\u22121x\u2016ATA+\u03bbI\n\u2264 \u01eb\u2032\u2016x\u2016(ATA+\u03bbI)\u22121 \u2264 c\u221212 \u01eb\nq2 \u221a \u03ba\u03bb\u03bb\n\u2016x\u20162 \u2264 c\u221212 \u01eb\nq2\u03c31(A) \u2016x\u20162.\nNow, B is given by pc-proj(A, \u03bb,x, \u03b3, \u01eb\u2032, \u03b4/2). With probability 1 \u2212 \u03b4/2, if 11\u22124\u03b3\u03c3k+1(A)2 \u2264 \u03bb \u2264 (1\u2212 4\u03b3)\u03c3k(A)2 then by Theorem 3.2, \u2016B(x)\u2212PA\u03bbx\u20162 \u2264 \u01eb\u2032\u2016x\u20162 = \u01eb/(c2q2 \u221a \u03ba\u03bb). Applying the union bound over q+1 calls to A and a single call to B, these bounds hold on every call with probability \u2265 1\u2212\u03b4. Adjusting constants on \u01eb (via c1 and c2) proves the theorem."}, {"heading": "5 Approximating the matrix step function", "text": "We now return to proving our underlying result on iterative polynomial approximation of the matrix step function:\nLemma 3.1 (Step function algorithm). Let S \u2208 Rd\u00d7d be symmetric with every eigenvalue \u03c3 satisfying \u03c3 \u2208 [0, 1] and |\u03c3 \u2212 1/2| \u2265 \u03b3. Let A denote a procedure that on x \u2208 Rd produces A(x) with \u2016A(x) \u2212 Sx\u2016 = O(\u01eb2\u03b32)\u2016x\u20162. Given y \u2208 Rd set s0 := A(y), w0 := s0 \u2212 12y, and for k \u2265 0 set\nwk+1 := 4\n(\n2k + 1\n2k + 2\n)\nA(wk \u2212A(wk))\nand sk+1 := sk +wk+1. If all arithmetic operations are performed with \u2126(log(d/\u01eb\u03b3)) bits of precision and if q = \u0398(\u03b3\u22122 log(1/\u01eb)) then \u2016sq \u2212 s(S)y\u20162 = O(\u01eb)\u2016y\u20162.\nThe derivation and proof of Lemma 3.1 is split into 3 parts. In Section 5.1 we derive a simple low degree polynomial approximation to the sign function:\nsgn(x) def =\n\n \n \n1 if x > 0\n0 if x = 0\n\u22121 if x < 0 In Section 5.2 we show how this polynomial can be computed with a stable iterative procedure. In Section 5.3 we use these pieces and the fact that the step function is simply a shifted and scaled sign function to prove Lemma 3.1. Along the way we give complementary views of Lemma 3.1 and show that there exist more efficient polynomial approximations."}, {"heading": "5.1 Polynomial approximation to the sign function", "text": "We show that for sufficiently large k, the following polynomial is uniformly close to sgn(x) on [\u22121, 1]:\npk(x) def =\nk \u2211\ni=0\n x(1 \u2212 x2)i i \u220f\nj=1\n2j \u2212 1 2j\n\n\nThe polynomial pk(x) can be derived in several ways. One follows from observing that sgn(x) is odd and thereby sgn(x)/x = 1/|x| is even. So, a good polynomial approximation for sgn(x) should be odd and, when divided by x, should be even (i.e. a function of x2). Specifically, given a polynomial approximation q(x) to 1/ \u221a x on the range (0, 1] we can approximate sgn(x) using xq(x2). Choosing q to be the k-th order Taylor approximation to 1/ \u221a x at x = 1 yields pk(x). With this insight we show that pk(x) converges to sgn(x).\nLemma 5.1. sgn(x) = limk\u2192\u221e pk(x) for all x \u2208 [\u22121, 1]. Proof. Let f(x) = x\u22121/2. By induction on k it is straightforward to show that the k-th derivative of f at x > 0 is\nf (k)(x) = (\u22121)k \u00b7 (x)\u2212 1+2k 2\nk \u220f\ni=1\n2i\u2212 1 2 .\nSince (\u22121)i(x\u2212 1)i = (1 \u2212 x)i we see that the degree k Taylor approximation to f(x) at x = 1 is therefore\nqk(x) = k \u2211\ni=0\n\n(1\u2212 x)i \u00b7 1 i!\ni \u220f\nj=1\n2j \u2212 1 2\n  = k \u2211\ni=0\n (1 \u2212 x)i \u00b7 i \u220f\nj=1\n2j \u2212 1 2j\n\n .\nNote that for x, y \u2208 [\u01eb, 1], the remainder f (k)(x)(1 \u2212 y)k/k! has absolute value at most (1 \u2212 \u01eb)k. Therefore the remainder converges to 0 as k \u2192 \u221e and the Taylor approximation converges, i.e. limk\u2192\u221e qk(x) = 1/ \u221a x\nfor x \u2208 (0, 1]. Since pk(x) = x \u00b7 qk(x2) we have limk\u2192\u221e pk(x) = x/ \u221a x2 = sgn(x) for x 6= 0 with x \u2208 [\u22121, 1]. Since pk(0) = 0 = sgn(0), the result follows.\nAlternatively, to derive pk(x) we can consider (1 \u2212 x2)k, which is relatively large near 0 and small on the rest of [\u22121, 1]. Integrating this function from 0 to x and normalizing yields a good step function. In Appendix A we prove that:\nLemma 5.2. For all x \u2208 R pk(x) = \u222b x 0 (1\u2212 y2)kdy\n\u222b 1 0 (1 \u2212 y2)kdy .\nNext, we bound the rate of convergence of pk(x) to sgn(x):\nLemma 5.3. For k \u2265 1 if x \u2208 (0, 1] then pk(x) > 0 and\nsgn(x)\u2212 (x \u221a k)\u22121e\u2212kx 2 \u2264 pk(x) \u2264 sgn(x) . (2)\nIf x \u2208 [\u22121, 0) then pk(x) < 0 and\nsgn(x) \u2264 pk(x) \u2264 sgn(x) + (x \u221a k)\u22121e\u2212kx 2 .\nProof. The claim is trivial when x = 0. Since pk(x) is odd it suffices to consider x \u2208 (0, 1]. For such x, it is direct that pk(x) > 0, and pk(x) \u2264 sgn(x) follows from the observation that pk(x) increases monotonically with k and limk\u2192\u221e pk(x) = sgn(x) by Lemma 5.1. All that remains to show is the left-side inequality of (2). Using Lemma 5.1 again,\nsgn(x)\u2212 pk(x) = \u221e \u2211\ni=k+1\n x(1 \u2212 x2)i i \u220f\nj=1\n2i\u2212 1 2i\n\n\n\u2264 x(1\u2212 x2)k \u221e \u2211\ni=0\n (1 \u2212 x2)i k \u220f\nj=1\n2j \u2212 1 2j\n\n .\nNow since 1 + x \u2264 ex for all x and \u2211ni=1 1i \u2265 lnn, we have\nk \u220f\nj=1\n2j \u2212 1 2j \u2264 exp\n\n\nk \u2211\nj=1\n\u22121 2j\n  \u2264 exp (\u2212 ln k\n2\n)\n= 1\u221a k .\nCombining with \u2211\u221e i=0(1 \u2212 x2)i = x\u22122 and again that 1 + x \u2264 ex proves the left hand side of (2).\nThe lemma directly implies that pk(x) is a high quality approximation to sgn(x) for x bounded away from 0.\nCorollary 5.4. If x \u2208 [\u22121, 1], with |x| \u2265 \u03b1 > 0 and k = \u03b1\u22122 ln(1/\u01eb), then | sgn(x)\u2212 pk(x)| \u2264 \u01eb.\nWe conclude by noting that this proof in fact implies the existence of a lower-degree polynomial approximation to sgn(x). Since the sum of coefficients in our expansion is small, we can replace each (1\u2212x2)q with Chebyshev polynomials of lower degree. In Appendix A, we prove:\nLemma 5.5. There exists an O(\u03b1\u22121 log(1/\u03b1\u01eb)) degree polynomial q(x) such that | sgn(x)\u2212 q(x)| \u2264 \u01eb for all x \u2208 [\u22121, 1] with |x| \u2265 \u03b1 > 0 .\nLemma 5.5 achieves, up an additive log(1/\u03b1)/\u03b1, the optimal trade off between degree and approximation of sgn(x) [EY07]. We have preliminary progress toward making this near-optimal polynomial algorithmic, a topic we leave to explore in future work."}, {"heading": "5.2 Stable iterative algorithm for the sign function", "text": "We now provide an iterative algorithm for computing pk(x) that works when applied with limited precision. Our formula is obtained by considering each term of pk(x). Let\ntk(x) def = x(1\u2212 x2)k\nk \u220f\nj=1\n2j \u2212 1 2j .\nClearly tk+1(x) = tk(x)(1 \u2212 x2)(2k + 1)/(2k + 2) and therefore we can compute the tk iteratively. Since pk(x) = \u2211k i=0 ti(x) we can compute pk(x) iteratively as well. We show this procedure works when applied to matrices, even if all operations are performed with limited precision:\nLemma 5.6. Let B \u2208 Rd\u00d7d be symmetric with \u2016B\u20162 \u2264 1. Let C be a procedure that given x \u2208 Rd produces C(x) with \u2016C(x)\u2212(I\u2212B2)x\u20162 \u2264 \u01eb\u2016x\u20162. Given y \u2208 Rd suppose that we have t0 and p0 such that \u2016t0\u2212By\u20162 \u2264 \u01eb\u2016y\u20162 and \u2016p0 \u2212By\u20162 \u2264 \u01eb\u2016y\u20162. For all k \u2265 1 set\ntk+1 :=\n(\n2k + 1\n2k + 2\n)\nC(tk) and pk+1 := pk + tk+1 .\nThen if arithmetic operations are carried out with \u2126(log(d/\u01eb)) bits of precision we have for 1 \u2264 k \u2264 1/(7\u01eb)\n\u2016tk(B)y \u2212 tk\u20162 \u2264 7k\u01eb and \u2016pk(B)y \u2212 pk\u20162 \u2264 7k\u01eb .\nProof. Let t\u2217k def = tk(B)y, p \u2217 k def = pk(B)y, and C def = I \u2212 B2. Since p\u22170 = t\u22170 = By and \u2016B\u20162 \u2264 1 we see that even if t0 and p0 are truncated to the given bit precision we still have \u2016t0 \u2212 t\u22170\u20162 \u2264 \u01eb\u2016y\u20162 and \u2016p0 \u2212 p\u22170\u20162 \u2264 \u01eb\u2016y\u20162.\nNow suppose that \u2016tk \u2212 t\u2217k\u20162 \u2264 \u03b1\u2016y\u20162 for some \u03b1 \u2264 1. Since |tk(x)| \u2264 |pk(x)| \u2264 | sgn(x)| \u2264 1 for x \u2208 [\u22121, 1] and \u2212I B I we know that \u2016t\u2217k\u20162 \u2264 \u2016y\u20162 and by reverse triangle inequality \u2016tk\u20162 \u2264 (1 + \u03b1)\u2016y\u20162. Using our assumption on C and applying triangle inequality yields\n\u2016C(tk)\u2212Ct\u2217k\u20162 \u2264 \u2016C(tk)\u2212Ctk\u20162 + \u2016C(tk \u2212 t\u2217k)\u20162 \u2264 \u01eb\u2016tk\u20162 + \u2016C\u20162 \u00b7 \u2016(tk \u2212 t\u2217k)\u20162 \u2264 (\u01eb(1 + \u03b1) + \u03b1)\u2016y\u20162 \u2264 (2\u01eb+ \u03b1)\u2016y\u20162 .\nIn the last line we used \u2016C\u20162 \u2264 1 since 0 B2 I. Again, by this fact we know that \u2016Ct\u2217k\u20162 \u2264 \u2016y\u20162 and therefore again by reverse triangle inequality \u2016C(tk)\u20162 \u2264 (1+2\u01eb+\u03b1)\u2016y\u20162. Using C(tk) to compute tk+1 with bounded arithmetic precision will then introduce an additional additive error of \u01eb(1+2\u01eb+\u03b1)\u2016y\u20162 \u2264 4\u01eb\u2016y\u20162. Putting all this together we have that \u2016t\u2217k\u2212 tk\u20162 grows by at most an additive 6\u01eb\u2016y\u20162 every time k increases and by the same argument so does \u2016pk \u2212 p\u2217k\u20162. Including our initial error of \u01eb on t0 and p0, we conclude that \u2016t\u2217k \u2212 tk\u20162 and \u2016pk \u2212 p\u2217k\u20162 are both bounded by 6k\u01eb+ \u01eb \u2264 7k\u01eb."}, {"heading": "5.3 Approximating the step function", "text": "We finally apply the results of Section 5.1 and Section 5.2 to approximate the step function and prove Lemma 3.1. We simply apply the fact that s(x) = (1/2)(1+ sgn(2x\u2212 1)) and perform further error analysis. We first use Lemma 5.6 to show how to compute (1/2)(1 + pk(2x\u2212 1)). Lemma 5.7. Let S \u2208 Rd\u00d7d be symmetric with 0 S I. Let A be a procedure that on x \u2208 Rd produces A(x) with \u2016A(x) \u2212 Sx\u20162 \u2264 \u01eb\u2016x\u20162. Given arbitrary y \u2208 Rd set s0 := A(y), w0 := s0 \u2212 (1/2)y, and for all k \u2265 0 set\nwk+1 := 4\n(\n2k + 1\n2k + 2\n)\nA(wk \u2212A(wk))\nand sk+1 := sk + wk+1. If arithmetic operations are performed with \u2126(log(d/\u01eb)) bits of precision and k = O(1/\u01eb) then \u20161/2(I\u2212 pk(2S\u2212 I))y \u2212 sk\u20162 = O(k\u01eb)\u2016y\u20162.\nProof. Since M def = I \u2212 (2S \u2212 I)2 = 4S(I \u2212 S) we see that wk is the same as (1/2)tk in Lemma 5.6 with B = 2S\u2212 I and C(x) = 4A(x\u2212A(x)), and sk = \u2211k\ni=0(1/2)ti+(1/2)b. Since multiplying by 1/2 everywhere does not increase error and since \u20162S \u2212 I\u20162 \u2264 1 we can invoke Lemma 5.6 to yield the result provided we can show \u20164A(x\u2212A(x))\u2212Mx\u20162 = O(\u01eb)\u2016x\u20162. Computing A(x) and subtracting from x introduces at most additive error 2\u01eb\u2016x\u20162 Consequently by the error guarantee of A, \u20164A(A(x) \u2212 x) \u2212 Mx\u20162 = O(\u01eb)\u2016x\u20162 as desired.\nUsing Lemma 5.7 and Corollary 5.4 we finally have:\nProof of Lemma 3.1. By assumption, 0 S I and \u01eb\u03b32q = O(1). Invoking Lemma 5.7 with error \u01eb\u2032 = \u01eb2\u03b32, letting aq def = 1/2(I\u2212 pq(2S\u2212 I))y we have\n\u2016aq \u2212 sq\u20162 = O(\u03b32\u01eb2q)\u2016y\u20162 = O(\u01eb)\u2016y\u20162 . (3)\nNow, since s(S) = 1/2(I\u2212 sgn(2S \u2212 I)) and every eigenvalue of 2S \u2212 I is in [\u03b3, 1], by assumption on S we can invoke Corollary 5.4 yielding \u2016aq \u2212 s(S)y\u20162 \u2264 12\u2016pq(2S \u2212 I) \u2212 sgn(2S\u2212 I)\u20162\u2016y\u20162 \u2264 2\u01eb\u2016y\u20162. The result follows from combining with (3) via triangle inequality."}, {"heading": "6 Empirical evaluation", "text": "We conclude with an empirical evaluation of pc-proc and ridge-pcr (Algorithms 1 and 2). Since PCR has already been justified as a statistical technique, we focus on showing that, with few iterations, the algorithm recovers an accurate approximation to A\u2020\u03bbb and PA\u03bby.\nWe begin with synthetic data, which lets us control the spectral gap \u03b3 that dominates our iteration bounds (see Theorem 3.2). Data is generated randomly by drawing top singular values uniformly from the range [.5(1 + \u03b3), 1] and tail singular values from [0, .5(1 \u2212 \u03b3)]. \u03bb is set to .5 and A is formed via the SVD U\u03a3VT where U and V are random orthonormal matrices and \u03a3 contains our random singular values. To model a typical PCR application, b is generated by adding noise to the response Ax of a random \u201ctrue\u201d x that correlates with A\u2019s top principal components.\nAs apparent in Figure 2(a), our algorithm performs very well for regression, even for small \u03b3. Error is\nmeasured via the natural ATA-norm and we plot \u2016ridge-pcr(A,b, \u03bb)\u2212A\u2020\u03bbb\u20162ATA/\u2016A \u2020 \u03bbb\u20162ATA.\nFigure 2(b) shows similar convergence for projection, although we do notice a stronger effect of a small gap \u03b3 in this case. Projection error is given with respect to the more natural 2-norm.\nBoth plots confirm the linear convergence predicted by our analysis (Theorems 3.2 and 4.2). To illustrate stability, we include an extended plot for the \u03b3 = .1 data which shows arbitrarily high accuracy as iterations increase (Figure 3).\nFinally, we consider a large regression problem constructed from MNIST classification data [LCB15], with the goal of distinguishing handwritten digits {1,2,4,5,7} from the rest. Input is normalized and 1000 random Fourier features are generated according to a unit RBF kernel [RR07]. Our final data set is both of larger scale and condition number than the original.\nThe MNIST principal component regression was run with \u03bb = .01\u03c321 . Although the gap \u03b3 is very small around this cutoff point (just .006), we see fast convergence for PCR. Convergence for projection is slowed more notably by the small gap, but it is still possible to obtain 0.01 relative error with only 20 iterations (i.e. invocations of ridge regression)."}, {"heading": "A The matrix step function", "text": "Here we provide proofs omitted from Section 5. We prove Lemma 5.2 showing that pq(x) can be viewed alternatively as a simple integral of (1 \u2212 x2)q. We also prove Lemma 5.5 showing the existence of an even lower degree polynomial approximation to sgn(x).\nLemma 5.2. For all x \u2208 R pk(x) = \u222b x 0 (1\u2212 y2)kdy \u222b 1\n0 (1 \u2212 y2)kdy\n.\nProof. Let qk(x) def = \u222b x 0 (1 \u2212 x2)q. Our proof follows from simply recursively computing this integral via integration by parts. Integration by parts with u = (1 \u2212 x2)k and dv = dx yields\nqk(x) = x(1\u2212 x2)k + 2k \u222b x\n0\nx2(1\u2212 x2)k\u22121 .\nSince x2 = 1\u2212 (1\u2212 x2) we have\nqk(x) = x(1 \u2212 x2)k + 2k \u00b7 qk\u22121(x)\u2212 2k \u00b7 qk(x).\nRearranging terms and dividing by 2k + 1 yields\nqk(x) = 1\n2k + 1\n[ x(1\u2212 x2)k + 2k \u00b7 qk\u22121(x) ] .\nSince q0(1) = 1 this implies that qk(1) = \u220fk j=1 2j 2j+1 and\nqk(x) qk(1) =\n1\n2k + 1\nk \u220f\nj=1\n(\n2j + 1\n2j\n)\nx(1\u2212 x2)k + qk\u22121(x) qk\u22121(1) .\nSince 12k+1 \u220fk j=1 2j+1 2j = \u220fk j=1 2j\u22121 2j we have that qk(x)/qk(1) = pk(x) as desired.\nWe now prove the existence of a lower degree polynomial for approximating sgn(x).\nLemma 5.5. There exists an O(\u03b1\u22121 log(1/\u03b1\u01eb)) degree polynomial q(x) such that | sgn(x)\u2212 q(x)| \u2264 \u01eb for all x \u2208 [\u22121, 1] with |x| \u2265 \u03b1 > 0 .\nWe first provide a general result on approximating polynomials with lower degree polynomials.\nLemma A.1 (Polynomial Compression). Let p(x) be an O(k) degree polynomial that we can write as\np(x) = k \u2211\ni=0\nfi(x) (gi(x)) i\nwhere fi(x) and gi(x) are O(1) degree polynomials satisfying |fi(x)| \u2264 ai and |gi(x)| \u2264 1 for all x \u2208 [\u22121, 1]. Then, there exists polynomial q(x) of degree O( \u221a k log(A/\u01eb)) where A = \u2211k\ni=0 ai such that |p(x)\u2212 q(x)| \u2264 \u01eb for all x \u2208 [\u22121, 1].\nThis lemma follows from the well known fact in approximation theory that there exist O( \u221a d) degree polynomials that approximate xd uniformly on the interval [\u22121, 1]. In particular we make use of the following: Theorem A.2 (Theorem 3.3 from [SV14]). For all s and d there exists a degree d polynomial denoted ps,d(x) such that |ps,d(x) \u2212 xs| \u2264 2 exp(\u2212d2/2s) for all x \u2208 [\u22121, 1].\nUsing Theorem A.2 we prove Lemma A.1.\nProof. Let d = \u221a 2k log(A/\u01eb) and let our low degree polynomial be defined as q(x) = \u2211k\ni=1 fi(x)pd,i(gi(x)). By Theorem A.2 we know that q(x) has the desired degree and by triangle inequality for all x \u2208 [\u22121, 1]\n|p(x) \u2212 q(x)| \u2264 k \u2211\ni=1\nai|gi(x)i \u2212 pd,i(gi(x))|\n\u2264 k \u2211\ni=1\nai exp(\u2212d2/2i) \u2264 \u01eb,\nwhere the last line used i \u2264 k and our choice of d.\nUsing Lemma A.1 we can now complete the proof.\nProof of Lemma 5.5. Note that pk(x) can be written in the form of Lemma A.1 with fi(x) = x \u220fi j=1 2j\u22121 2j and gi(x) = 1 \u2212 x2. Clearly |fi(x)| \u2264 1 and |gi(x)| \u2264 1 for x \u2208 [\u22121, 1] and thus we can invoke Lemma A.1 to obtain a degree O( \u221a\nk log(k/\u01eb)) polynomial qk(x) with |qk(x)\u2212 pk(x)| \u2264 12\u01eb for all x \u2208 [\u22121, 1]. By Corollary 5.4 we know that for k = \u03b1\u22122 ln(2/\u01eb) we have | sgn(x) \u2212 pk(x)| \u2264 \u01eb/2 and therefore\n| sgn(x) \u2212 qk(x)| \u2264 \u01eb. Since \u221a\n\u03b1\u22122 ln(2/\u01eb) ln\n( \u03b1\u22122 ln(2/\u01eb)\n\u01eb\n)\n= O(\u03b1\u22121 ln(1/\u03b1\u01eb)),\nwe have the desired result."}, {"heading": "B Principal component regression", "text": "Finally we prove Lemma 4.1, the main result behind our algorithm to convert principal component projection to PCR algorithm. The proof is in two parts. First, letting y = PA\u03bbA Tb, we show how to approximate (ATA)\u22121y = A\u2020\u03bb with a low degree polynomial of the ridge inverse (A TA + \u03bbI)\u22121. Second, we provide an error analysis of our iterative method for computing this polynomial. We start with a very basic polynomial approximation bound:\nLemma B.1. Let g(x) def = x1\u2212\u03bbx and pk(x) def = \u2211k i=1 \u03bb i\u22121xi. For x \u2264 12\u03bb we have: g(x)\u2212 pk(x) \u2264 12k\u03bb Proof. We can expand g(x) =\n\u2211\u221e i=1 \u03bb i\u22121xi. So:\ng(x)\u2212 pk(x) = \u221e \u2211\ni=k+1\n\u03bbi\u22121xi \u2264 x 2k\u22121\n\u221e \u2211\ni=1\n(\u03bbx)i \u2264 1 2k\u03bb .\nWe next extend this lemma to the matrix case:\nLemma B.2. For any A \u2208 Rn\u00d7d and b \u2208 Rd, let y = PA\u03bbATb. Let pk(x) = \u2211k i=1 \u03bb i\u22121xi. Then we have:\n\u2016pk ( (ATA+ \u03bbI)\u22121 ) y \u2212A\u2020\u03bbb\u2016ATA \u2264 \u03ba\u03bb\u2016b\u20162\n2k .\nProof. For conciseness, in the remainder of this section we denote M def = ATA + \u03bbI. Let z = pk ( M\u22121 ) y. Letting g(x) = x/(1\u2212\u03bbx) we have g (\n1 x+\u03bb\n)\n= 1x . So, g(M \u22121)y = (ATA)\u22121y = A\u2020\u03bbb. Define \u03b4k(x) def = g(x)\u2212\npk(x).\n\u2016A\u2020\u03bbb\u2212 z\u2016ATA = \u2016\u03b4k(M\u22121)y\u2016ATA \u2264 \u03c31(A) \u00b7 \u2016\u03b4k(M\u22121)y\u20162. (4)\nThe projection y falls entirely in the span of principal components of A with squared singular values \u2265 \u03bb. M maps these values to singular values \u2264 1\u03bb+\u03bb = 12\u03bb , and hence by Lemma B.1 we have:\n\u2016\u03b4k(M\u22121)y\u20162 \u2264 1\n2k\u03bb \u2016y\u20162\n\u2264 \u03c31(A) 2k\u03bb \u2016b\u20162.\nCombining with (4) and recalling that \u03ba\u03bb def = \u03c31(A) 2/\u03bb gives the lemma.\nWith this bound in place, we are ready to give a full error analysis of our iterative method for applying pk((A TA+ \u03bbI)\u22121).\nLemma 4.1 (PCR approximation algorithm). Let A be a procedure that, given x \u2208 Rd produces A(x) with \u2016A(x)\u2212 (ATA+ \u03bbI)\u22121x\u2016ATA+\u03bbI = O( \u01ebq2\u03c31(A) )\u2016x\u20162. Let B be a procedure that, given x \u2208 R\nd produces B(x) with \u2016B(x)\u2212PA\u03bbx\u20162 = O( \u01ebq2\u221a\u03ba\u03bb )\u2016x\u20162. Given b \u2208 R n set s0 := B(ATb) and s1 := A(s0). For k \u2265 1 set:\nsk+1 := s1 + \u03bb \u00b7 A(sk)\nIf all arithmetic operations are performed with \u2126(log(d/q\u01eb)) bits of precision then \u2016sq\u2212A\u2020\u03bbb\u2016ATA = O(\u01eb)\u2016b\u20162 for q = \u0398(log(\u03ba\u03bb/\u01eb)).\nProof. Let s\u22170 def = PA\u03bbA Tb, and for k \u2265 1\ns\u2217k = k \u2211\ni=1\n\u03bbi\u22121M\u2212is\u22170\nFor ease of exposition, assume our accuracy bound on B gives \u2016s0 \u2212 s\u22170\u20162 \u2264 \u01ebq2\u221a\u03ba\u03bb \u2016A Tb\u20162 \u2264\n\u221a \u03bb\u01eb/q2\u2016b\u20162.\nAdjusting constants, the same proof with give Lemma 4.1 when error is actually O( \u221a \u03bb\u01eb/q2). By triangle inequality:\n\u2016s1 \u2212 s\u22171\u2016M \u2264 \u2016s1 \u2212M\u22121s0\u2016M + \u2016M\u22121(s\u22170 \u2212 s0)\u2016M (5)\n\u2016M\u22121(s\u22170 \u2212 s0)\u2016M \u2264 1\u221a\u03bb\u2016s \u2217 0 \u2212 s0\u20162 \u2264 \u01eb/q2\u2016b\u20162. And by our accuracy bound on A:\n\u2016s1 \u2212M\u22121s0\u2016M \u2264 \u01eb\nq2\u03c31(A) \u2016s0\u20162.\nApplying triangle inequality and the fact that the projection PA\u03bb can only decrease norm we have:\n\u01eb\nq2\u03c31(A) \u2016s0\u20162 \u2264\n\u01eb\nq2\u03c31(A) (\u2016s\u22170\u20162 + \u2016s\u22170 \u2212 s0\u20162)\n\u2264 \u01eb q2\u03c31(A) (\n\u2016ATb\u20162 + \u221a \u03bb\u01eb/q2\u2016b\u20162 )\n\u2264 2\u01eb/q2\u2016b\u20162.\nPlugging back into (5) we finally have: \u2016s1 \u2212 s\u22171\u2016M \u2264 3\u01eb/q2\u2016b\u20162. Suppose we have for any k \u2265 1, \u2016sk \u2212 s\u2217k\u2016M \u2264 \u03b1\u2016b\u20162.\n\u2016sk+1 \u2212 s\u2217k+1\u2016M \u2264 \u2016s1 \u2212 s\u22171\u2016M + \u03bb\u2016A(sk)\u2212M\u22121s\u2217k\u2016M \u2264 3\u01eb/q2\u2016b\u20162 + \u03bb\u2016A(sk)\u2212M\u22121s\u2217k\u2016M.\nWe have:\n\u03bb\u2016A(sk)\u2212M\u22121s\u2217k\u2016M\n\u2264 \u03bb\u01eb q2\u03c31(A) \u2016sk\u20162 + \u03bb\u2016M\u22121(sk \u2212 s\u2217k)\u2016M \u2264 \u03bb\u01eb q2\u03c31(A) (\u2016s\u2217k\u20162 + \u2016sk \u2212 s\u2217k\u20162) + \u03b1\u2016b\u20162 \u2264 \u03bb\u01eb q2\u03c31(A) \u2016sk \u2212 s\u2217k\u20162 + (\u03b1+ k\u01eb/q2)\u2016b\u20162\nwhere the last step follows from: \u03bb\u01ebq2\u03c31(A)\u2016s \u2217 k\u20162 \u2264 \u03bb\u01ebq2\u03c31(A) \u2211k i=1 \u03bb i\u22121\u2016M\u2212is\u22170\u20162 \u2264 k\u01ebq2\u03c31(A)\u2016s \u2217 0\u20162 \u2264 k\u01eb/q2\u2016b\u20162.\nNow, \u03bb\u01ebq2\u03c31(A)\u2016sk \u2212 s \u2217 k\u20162 \u2264 \u01eb\n\u221a \u03bb\nq2\u03c31(A) \u2016sk \u2212 s\u2217k\u2016M \u2264 \u01eb\u03b1/q2\u2016b\u20162 since\n\u221a \u03bb \u2264 \u03c31(A).\nSo overall, presuming \u2016sk \u2212 s\u2217k\u2016M \u2264 \u03b1\u2016b\u20162, we have \u2016sk+1 \u2212 s\u2217k+1\u2016M \u2264 [(1 + \u01eb/q2)\u03b1+ (3 + k)\u01eb/q2]\u2016b\u20162. We know that \u2016s1 \u2212 s\u22171\u2016M \u2264 3\u01eb/q2\u2016b\u20162, so by induction we have:\n\u2016sq \u2212 s\u2217q\u2016M < (1 + \u01eb/q2)q \u00b7 q2\u01eb/q2\u2016b\u20162 < 3\u01eb\u2016b\u20162.\nFinally, applying the above bound, triangle inequality, and the polynomial approximation bound from Lemma B.2 we have:\n\u2016sq \u2212A\u2020\u03bbb\u2016ATA \u2264 \u2016sq \u2212 s\u2217q\u2016ATA + \u2016s\u2217q \u2212A \u2020 \u03bbb\u2016ATA\n\u2264 ( 3\u01eb+ \u03ba\u03bb 2q ) \u2016b\u20162 \u2264 4\u01eb\u2016b\u20162\nsince q = \u0398(log(\u03ba\u03bb/\u01eb))."}], "references": [{"title": "In Proceedings of the 2014 IEEE International Symposium on Information Theory (ISIT)", "author": ["Christos Boutsidis", "Malik Magdon-Ismail. Faster SVD-truncated regularized least-squares"], "venue": "pages 1321\u20131325,", "citeRegEx": "BMI14", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing truncated singular value decomposition least squares solutions by rank revealing qr-factorizations", "author": ["Tony F. Chan", "Per Christian Hansen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Chan and Hansen.,? \\Q1990\\E", "shortCiteRegEx": "Chan and Hansen.", "year": 1990}, {"title": "In Proceedings of the 6th Conference on Innovations in Theoretical Computer Science (ITCS)", "author": ["Michael B. Cohen", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Richard Peng", "Aaron Sidford. Uniform sampling for matrix approximation"], "venue": "pages 181\u2013190,", "citeRegEx": "CLM15", "shortCiteRegEx": null, "year": 2015}, {"title": "The Journal of Machine Learning Research", "author": ["Paramveer S. Dhillon", "Dean P. Foster", "Sham M. Kakade", "Lyle H. Ungar. A risk comparison of ordinary least squares vs ridge regression"], "venue": "14(1):1505\u20131511,", "citeRegEx": "DFKU13", "shortCiteRegEx": null, "year": 2013}, {"title": "Uniform approximation of sgn(x) by polynomials and entire functions", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathmatique,", "citeRegEx": "Eremenko and Yuditskii.,? \\Q2007\\E", "shortCiteRegEx": "Eremenko and Yuditskii.", "year": 2007}, {"title": "Polynomials of the best uniform approximation to sgn(x) on two intervals", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "citeRegEx": "Eremenko and Yuditskii.,? \\Q2011\\E", "shortCiteRegEx": "Eremenko and Yuditskii.", "year": 2011}, {"title": "A statistical view of some chemometrics regression", "author": ["Ildiko E. Frank", "Jerome H. Friedman"], "venue": "tools. Technometrics,", "citeRegEx": "Frank and Friedman.,? \\Q1993\\E", "shortCiteRegEx": "Frank and Friedman.", "year": 1993}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "FGKS15", "shortCiteRegEx": null, "year": 2015}, {"title": "Model Order Reduction: Theory, Research Aspects and Applications, chapter Matrix Functions, pages 275\u2013303", "author": ["Andreas Frommer", "Valeria Simoncini"], "venue": null, "citeRegEx": "Frommer and Simoncini.,? \\Q2008\\E", "shortCiteRegEx": "Frommer and Simoncini.", "year": 2008}, {"title": "BIT Numerical Mathematics", "author": ["Per Christian Hansen. The truncated SVD as a method for regularization"], "venue": "27(4):534\u2013553,", "citeRegEx": "Han87", "shortCiteRegEx": null, "year": 1987}, {"title": "Functions of Matrices: Theory and Computation", "author": ["Nicholas J. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Higham.,? \\Q2008\\E", "shortCiteRegEx": "Higham.", "year": 2008}, {"title": "The relations of the newer multivariate statistical methods to factor analysis", "author": ["Harold Hotelling"], "venue": "British Journal of Statistical Psychology,", "citeRegEx": "Hotelling.,? \\Q1957\\E", "shortCiteRegEx": "Hotelling.", "year": 1957}, {"title": "Burges", "author": ["Yann LeCun", "Corinna Cortes", "Christopher J.C"], "venue": "MNIST handwritten digit database.", "citeRegEx": "LCB15", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems 27 (NIPS)", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao. An accelerated proximal coordinate gradient method"], "venue": "pages 3059\u20133067,", "citeRegEx": "LLX14", "shortCiteRegEx": null, "year": 2014}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Yurii Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L. Nguy\u00ean"], "venue": "In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Mathematical Programming", "author": ["Shai Shalev-Shwartz", "Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization"], "venue": "pages 1\u201341,", "citeRegEx": "SSZ14", "shortCiteRegEx": null, "year": 2014}, {"title": "Faster algorithms via approximation theory", "author": ["Sushant Sachdeva", "Nisheeth K. Vishnoi"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Sachdeva and Vishnoi.,? \\Q2014\\E", "shortCiteRegEx": "Sachdeva and Vishnoi.", "year": 2014}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["Andrey Tikhonov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Tikhonov.,? \\Q1963\\E", "shortCiteRegEx": "Tikhonov.", "year": 1963}, {"title": "Numerical methods for the QCD overlap operator I: Sign-function and error bounds", "author": ["Jasper van den Eshof", "Andreas Frommer", "Thomas Lippert", "Klaus Schilling", "Henk A. van der Vorst"], "venue": "Computer physics communications, 146(2):203\u2013224,", "citeRegEx": "vdEFL02", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 3, "context": "Unfortunately, ridge regression is a very crude approximation to PCR and projection in many settings and may perform significantly worse in certain data analysis applications [DFKU13].", "startOffset": 175, "endOffset": 183}, {"referenceID": 12, "context": "Finally, we consider a large regression problem constructed from MNIST classification data [LCB15], with the goal of distinguishing handwritten digits {1,2,4,5,7} from the rest.", "startOffset": 91, "endOffset": 98}], "year": 2016, "abstractText": "We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a \u201csmooth projection\u201d onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision.", "creator": "LaTeX with hyperref package"}}}