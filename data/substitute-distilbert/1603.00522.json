{"id": "1603.00522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "Solving Combinatorial Games using Products, Projections and Lexicographically Optimal Bases", "abstract": "joining order to find nash - equilibria requires n - player zero - sum collisions where your player plays arithmetic objects like spanning trees, matchings etc, we consider two online learning algorithms : the online mirror descent ( omd ) algorithm and the multiplicative weights update ( mwu ) algorithm. the carpenter algorithm emphasizes the computation of a certain bregman projection, that has closed form solutions for simple convex sets like the euclidean ball or kepler simplex. however, for general polyhedra one often needs to exploit the efficient machinery of convex optimization. we give a novel primal - style algorithm for smoothing bregman projections on restricted base polytopes of \u0192. next, in third case of the fisher algorithm, although algorithm scales logarithmically in the numbers of pure strategies or experts $ n $ given terms of regret, the reconstruction assumes time polynomial in $ n $ ; this especially provides a problem when learning combinatorial objects. we give a small product to simulate the multiplicative weights update algorithm some time polynomial in their natural dimension. this is useful whenever there exists a moderate time generalized counting oracle ( even if approximate ) over these objects. finally, using the combinatorial structure of symmetric nash - equilibria ( np ) when both players play bases of matroids, basically show that these projections be enhanced with a single projection or convex minimization ( without using online learning ).", "histories": [["v1", "Tue, 1 Mar 2016 23:02:38 GMT  (56kb,D)", "http://arxiv.org/abs/1603.00522v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["swati gupta", "michel goemans", "patrick jaillet"], "accepted": false, "id": "1603.00522"}, "pdf": {"name": "1603.00522.pdf", "metadata": {"source": "CRF", "title": "Solving Combinatorial Games using Products, Projections and Lexicographically Optimal Bases", "authors": ["Swati Gupta", "Michel Goemans", "Patrick Jaillet"], "emails": ["SWATIG@MIT.EDU", "GOEMANS@MATH.MIT.EDU", "JAILLET@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "The motivation of our work comes from two-player zero-sum games where both players play combinatorial objects, such as spanning trees, cuts, matchings, or paths in a given graph. The number of pure strategies of both players can then be exponential in a natural description of the problem. These are succinct games, as discussed in the paper of Papadimitriou and Roughgarden (2008) on correlated equilibria. For example, in a spanning tree game in which all the results of this paper apply, pure strategies correspond to spanning trees T1 and T2 selected by the two players in a graph G (or two distinct graphs G1 and G2) and the payoff\u2211\ne\u2208T1,f\u2208T2 Lef is a bilinear function; this allows for example to model classic network interdiction games (see for e.g., Washburn and Wood (1995)), design problems (Chakrabarty et al. (2006)), and the interaction between algorithms for many problems such as ranking and compression as bilinear duels (Immorlica et al. (2011)). To formalize the games we are considering, assume that the pure strategies for player 1 (resp. player 2) correspond to the vertices u (resp. v) of a strategy polytope P \u2286 Rm (resp. Q \u2286 Rn) and that the loss for player 1 is given by the bilinear function uTLv where L \u2208 Rm\u00d7n. A feature of bilinear loss functions is that the bilinearity extends to mixed strategies as well, and thus one can easily see that mixed Nash equilibria (see\nc\u00a9 S. Gupta, M. Goemans & P. Jaillet.\nar X\niv :1\n60 3.\n00 52\n2v 1\n[ cs\n.L G\n] 1\nM ar\n2 01\ndiscussion in Section 3) correspond to solving the min-max problem:\nmin x\u2208P max y\u2208Q xTLy = max y\u2208Q min x\u2208P xTLy. (1)\nWe refer to such games as MSP (Min-max Strategy Polytope) games. Nash equilibria for two-player zero-sum games can be characterized and found by solving a linear program (von Neumann (1928)). However, for succinct games in which the strategies of both players are exponential in a natural description of the game, the corresponding linear program has exponentially many variables and constraints, and as Papadimitriou and Roughgarden (2008) point out in their open questions section, \u201cthere are no standard techniques for linear programs that have both dimensions exponential.\u201d Under bilinear losses/payoffs however, the von Neumann linear program can be reformulated in terms of the strategy polytopes P and Q, and this reformulation can be solved using the equivalence between optimization and separation and the ellipsoid algorithm (Gro\u0308tschel et al. (1981)) (discussed in more detail in Section 3). In the case of the spanning tree game mentioned above, the strategy polytope of each player is simply the spanning tree polytope characterized by Edmonds (1971). Note that Immorlica et al. (2011) give such a reformulation for bilinear games involving strategy polytopes with compact formulations only (i.e., each strategy polytope must be described using a polynomial number of inequalities).\nIn this paper, we first explore ways of solving efficiently this linear program using learning algorithms. As is well-known, if one of the players uses a no-regret learning algorithm and adapts his/her strategies according to the losses incurred so far (with respect to the most adversarial opponent strategy) then the average of the strategies played by the players in the process constitutes an approximate equilibrium (Cesa-Bianchi and Lugosi (2006)). Therefore, we consider the following learning problem over T rounds in the presence of an adversary: in each round the \u201clearner\u201d (or player) chooses a mixed strategy xt \u2208 P . Simultaneously, the adversary chooses a loss vector lt = Lvt where vt \u2208 Q and the loss incurred by the player is xTt lt. The goal of the player is to minimize the cumulative loss, i.e., \u2211t i=1 x T t lt. Note that this setting is similar to the classical full-information online structured learning problem (see for e.g., Audibert et al. (2013)), where the learner is required to play a pure strategy ut \u2208 U (where U is the vertex set of P ) possibly randomized according to a mixed strategy xt \u2208 P , and aims to minimize the loss in expectation, i.e., E(uTt lt).\nOur two main results on learning algorithms are (i) an efficient implementation of online mirror descent when P is the base polytope of a polymatroid and this is obtained by designing a novel algorithm to compute Bregman projections over such a polytope, and (ii) an efficient implementation of the MWU algorithm over the vertices of 0/1 polytopes P , provided we have access to a generalized counting oracle for the vertices. These are discussed in detail below. In both cases, we assume that we have an (approximate) linear optimization oracle for Q, which allows to compute the (approximately) worst loss vector given a mixed strategy in P . Finally, we study the combinatorial properties of symmetric Nash-equilibria for matroid games and show how this structure can be exploited to compute these equilibria."}, {"heading": "1.1. Online Mirror Descent", "text": "Even though the online mirror descent algorithm is near-optimal in terms of regret for most of online learning problems (Srebro et al. (2011)), it is not computationally efficient. One of the crucial steps in the mirror descent algorithm is that of taking Bregman projections on the strategy polytope that has closed form solutions for simple cases like the Euclidean ball or the n-dimensional simplex, and this is why such polytopes have been the focus of attention. For general polyhedra (or convex sets) however, taking a Bregman projection is a separable convex minimization problem. One could exploit the general machinery of convex optimization such as the ellipsoid algorithm, but the question is if we can do better.\nFirst contribution: We give a novel primal-style algorithm, INC-FIX for minimizing separable strongly convex functions over the base polytope P of a polymatroid. This includes the setting in which U forms the bases of a matroid, and cover many interesting examples like k-sets (uniform matroid), spanning trees (graphic matroid), matchable/reachable sets in graphs (matching matroid/gammoid), etc. The algorithm is iterative and maintains a feasible point in the polymatroid (or the independent set polytope for a matroid). This point follows a trajectory that is guided by two constraints: (i) the point must remain in the polymatroid, (ii) the smallest indices (not constrained by (i)) of the gradient of the Bregman divergence increase uniformly. The correctness of the algorithm follows from first order optimality conditions, and the optimality of the greedy algorithm when optimizing linear functions over polymatroids. We discuss special cases of our algorithm under two classical mirror maps, the unnormalized entropy and the Euclidean mirror map, under which each iteration of our algorithm reduces to staying feasible while moving along a straight line. We believe that the key ideas developed in this algorithm may be useful to compute projections under Bregman divergences over other polyhedra, as long as the linear optimization for those is well-understood.\nAs a remark, in order to compute -approximate Nash-equilibria, if both the strategy polytopes are polymatroids then the same projection algorithms apply to the saddle-point mirror prox algorithm (Nemirovski (2004)) and reduce the dependence of the rate of convergence on to O(1/ )."}, {"heading": "1.2. Multiplicative Weights Update", "text": "The multiplicative weights update (MWU) algorithm proceeds by maintaining a probability distribution over all the pure strategies of each player, and multiplicatively updates the probability distribution in response to the adversarial strategy. The number of iterations the MWU algorithm takes to converge to an \u2212approximate strategy1 is O(lnN/ 2), where N is the number of pure strategies of the learner, in our case the size of the vertex set of the combinatorial polytope. However, the running time of each iteration is O(N) due to the updates required on the probabilities of each pure strategy. A natural question is, can the MWU algorithm be adapted to run in logarithmic time in the number of strategies?\nSecond contribution: We give a general framework for simulating the MWU algorithm over the set of vertices U of a 0/1 polytope P efficiently by updating product distributions. A product distribution p over U \u2286 {0, 1}m is such that p(u) \u221d \u220f e:u(e)=1 \u03bb(e) for some multiplier vector \u03bb \u2208 Rm+ . Note that it is easy to start with a uniform distribution over all vertices in this representation, by simply setting \u03bb(e) = 1 for all e. The key idea is that a multiplicative weight update to a product distribution results in another product distribution, obtained by appropriately and multiplicatively updating each \u03bb(e). Thus, in different rounds of the MWU algorithm, we move from one product distribution to another. This implies that we can restrict our attention to product distributions without loss of generality, and this was already known as any point in a 0/1 polytope can be decomposed into a product distribution. Product distributions allow us to maintain a distribution on (the exponentially sized) U by simply maintaining \u03bb \u2208 Rm+ . To be able to use the MWU algorithm together with product distributions, we require access to a generalized (approximate) counting oracle which, given \u03bb \u2208 Rm+ , (approximately) computes \u2211 u\u2208U \u220f e:ue=1\n\u03bb(e) and also, for any element f , computes \u2211 u\u2208U :uf=1 \u220f e:ue=1\n\u03bb(e) allowing the derivation of the corresponding marginals x \u2208 P . For selfreducible structures U (Schnorr (1976)) (such as spanning trees, matchings or Hamiltonian cycles), the latter condition for every element f is superfluous, and the generalized approximate counting oracle can be replaced by a fully polynomial approximate generator as shown by Jerrum et al. (1986).\nWhenever we have access to a generalized approximate counting oracle, the MWU algorithm converges to -approximate in O(ln |U|/ 2) time. A generalized exact counting oracle is available for spanning trees (this\n1. A strategy pair (x\u2217, y\u2217) is called an -approximate Nash-equilibrium if x\u2217TLy\u2212 \u2264 x\u2217TLy\u2217 \u2264 xTLy\u2217+ for all x \u2208 P, y \u2208 Q.\nis Kirchhoff\u2019s determinantal formula or matrix tree theorem) or more generally for bases of regular matroids (see for e.g., Welsh (2009)) and randomized approximate ones for bipartite matchings (Jerrum et al. (2004)) and extensions such as 0\u2212 1 circulations in directed graphs or subgraphs with prespecified degree sequences (Jerrum et al. (2004)).\nAs a remark, if a generalized approximate counting oracle exists for both strategy polytopes (as is the case for the spanning tree game mentioned early in the introduction), the same ideas apply to the optimistic mirror descent algorithm (Rakhlin and Sridharan (2013)) and reduce the dependence of the rate of convergence on to O(1/ ) while maintaining polynomial running time."}, {"heading": "1.3. Structure of symmetric Nash equilibria", "text": "For matroid MSP games, we study properties of Nash equilibria using combinatorial arguments with the hope to find constructive algorithms to compute Nash equilibria. Although we were not able to solve the problem for the general case, we prove the following results for the special case of symmetric Nash equilibria (SNE) where both the players play the same mixed strategy.\nThird contribution: We combinatorially characterize the structure of symmetric Nash equilibria. We prove uniqueness of SNE under positive and negative definite loss matrices. We show that SNE coincide with lexicographically optimal points in the base polytope of the matroid in certain settings, and hence show that they can be efficiently computed using any separable convex minimization algorithm (for example, algorithm INC-FIX in Section 4). Given an observed Nash-equilibrium, we can also construct a possible loss matrix for which that is a symmetric Nash equilibrium.\nComparison of approaches. Both the learning approaches have different applicability and limitations. We know how to efficiently perform the Bregman projection only for polymatroids, and not for bipartite matchings for which the MWU algorithm with product distributions can be used. On the other hand, there exist matroids (Azar et al. (1994)) for which any generalized approximate counting algorithm requires an exponential number of calls to an independence oracle, while an independence oracle is all what we need to make the Bregman projection efficient in the online mirror descent approach. Our characterization of symmetric Nash-equilibria shows that a single projection is enough to compute symmetric equilibria (and check if they exist).\nThis paper is structured as follows. After discussing related work in Section 2, we review background and notation in Section 3 and show a reformulation for the game that can be solved using separation oracles for the strategy polytopes. We give a primal-style algorithm for separable convex minimization over base polytopes of polymatroids in Section 4. We discuss the special cases of computing Bregman projections under the entropy and Euclidean mirror maps, and show how to solve the subproblems that arise in the algorithm. In Section 5, we show how to simulate the multiplicative weights algorithm in polynomial time using generalized counting oracles. We further show that the MWU algorithm is robust to errors in these algorithms for optimization and counting. Finally in Section 6, we completely characterize the structure of symmetric Nash-equilibria for matroid games under symmetric loss matrices. Sections 4, 5 and 6 are independent of each other, and can be read in any order."}, {"heading": "2. Related work", "text": "The general problem of finding Nash-equilibria in 2-player games is PPAD-complete (Chen et al. (2009), Daskalakis et al. (2009)). Restricting to two-player zero-sum games without any assumptions on the structure\nof the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of - approximate Nash equilibria with N pure strategies are known (Altho\u0308fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)). These results perform a search on the support of the Nash equilibria and it is not known how to find Nash equilibria for large two-player zero sum games in time logarithmic in the number of pure strategies of the players. In recent work, Hazan and Koren (2015) show that any online algorithm requires \u2126\u0303( \u221a N) time to approximate the value of a two-player zero-sum game, even when given access to constant time best-response oracles. In this work, we restrict our attention to two-player zero-sum games with bilinear loss functions and give polynomial time algorithms for finding Nash-equilibria (polynomial in the representation of the game).\nOne way to find Nash-equilibria is by using regret-minimization algorithms. We look at the problem of learning combinatorial concepts that has recently gained a lot of popularity in the community (Koolen and Van Erven (2015), Audibert et al. (2013), Neu and Barto\u0301k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Barto\u0301k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting.\nKoolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings.\nOne of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)). Our algorithm is fundamentally different from the decomposition algorithm; the latter generates a sequence of violated inequalities (a dual approach) while our algorithm maintains a feasible point in the polymatroid (a primal approach). The violated inequalities in the decomposition algorithm come from a series of submodular function minimizations, and their computations can be amortized into a parametric submodular function minimization as is shown in Nagano (2007a,b); Suehiro et al. (2012). In the special cases of the Euclidean or the entropy mirror maps, our iterative algorithm leads to problems involving the maintenance of feasibility along lines, which reduces to parametric submodular function minimization problems. In the special case of cardinality-based submodular functions (f(S) = g(|S|) for some concave g), our algorithm can be implemented overall in O(n2) time, matching the running time of a specialized algorithm due to Suehiro et al. (2012).\nWe also consider the multiplicative weights update (MWU) algorithm (Littlestone and Warmuth (1994), Freund and Schapire (1999), Arora et al. (2012)) rediscovered for different settings in game theory, machine learning, and online decision making with a large number of applications. Most of the applications of the MWU algorithm have running times polynomial in the number of pure strategies of the learner, an observation\nalso made in Blum et al. (2008). In order to perform this algorithm efficiently for exponential experts (with combinatorial structure), it does not take much to see that multiplicative updates for linear losses can be made using product terms. However, the analysis of prior works was very specific to the structure of the problem. For example, Takimoto and Warmuth (2003) give efficient implementations of the MWU for learning over general s\u2212 t paths that allow for cycles or over simple paths in acyclic directed graphs. This approach relies on the recursive structure of these paths, and does not generalize to simple paths in an undirected graph (or a directed graph). Similarly, Helmbold and Schapire (1997) rely on the recursive structure of bounded depth binary decision trees. Koo et al. (2007) use the matrix tree theorem to learn over spanning trees by doing largemargin optimization. We give a general framework to analyze these problems, while drawing a connection to sampling or generalized counting of product distributions."}, {"heading": "3. Preliminaries", "text": "In a two-player zero-sum game with loss (or payoff) matrix R \u2208 RM\u00d7N , a mixed strategy x (resp. y) for the row player (resp. column player) trying to minimize (resp. maximize) his/her loss is an assignment x \u2208 \u2206M (resp. y \u2208 \u2206N ) where \u2206K is the simplex {x \u2208 RK , \u2211K i=1 xi = 1, x \u2265 0}. A pair of mixed strategies (x\u2217, y\u2217) is called a Nash-equilibrium if x\u2217TRy\u0304 \u2264 x\u2217TRy\u2217 \u2264 x\u0304TRy\u2217 for all x\u0304 \u2208 \u2206M , y\u0304 \u2208 \u2206N , i.e. there is no incentive for either player to switch from (x\u2217, y\u2217) given that the other player does not deviate. Similarly, a pair of strategies (x\u2217, y\u2217) is called an \u2212approximate Nash-equilibrium if x\u2217TRy\u0304\u2212 \u2264 x\u2217TRy\u2217 \u2264 x\u0304TRy\u2217+ for all x\u0304 \u2208 \u2206M , y\u0304 \u2208 \u2206N . Von Neumann showed that every two-player zero-sum game has a mixed Nashequilibrium that can be found by solving the following dual pair of linear programs:\n(LP1) : min\u03bb (LP2) : max\u00b5\nRTx \u2264 \u03bbe, Ry \u2265 \u00b5e, eTx = 1, x \u2265 0. eT y = 1, y \u2265 0.\nwhere e is a vector of all ones in the appropriate dimension. In our two-player zero-sum MSP games, we let the strategies of the row player be U = vert(P ), where P = {x \u2208 Rm, Ax \u2264 b} is a polytope and vert(P ) is the set of vertices of P and those of the column player be V = vert(Q) where Q = {y \u2208 Rn, Cy \u2264 d} is also a polytope. The numbers of pure strategies, M = |U|, N = |V| will typically be exponential in m or n, and so may be the number of rows in the constraint matrices A and C. The linear programs (LP1) and (LP2) have thus exponentially many variables and constraints. We restrict our attention to bilinear loss functions that are represented as Ruv = uTLv for some m\u00d7 n matrix L. An artifact of bilinear loss functions is that the bilinearity extends to mixed strategies as well. If \u03bb \u2208 \u2206U and \u03b8 \u2208 \u2206V are mixed strategies for the players then the expected loss is equal to xTLy where x = \u2211 u\u2208U \u03bbuu\nand y = \u2211\nv\u2208V \u03b8vv:\nEu,v(Ruv) = \u2211 u\u2208U \u2211 v\u2208V \u03bbu\u03b8v(u TLv) = ( \u2211 u\u2208U \u03bbuu)L( \u2211 v\u2208V \u03b8vv) = x TLy.\nThus the loss incurred by mixed strategies only depend on the marginals of the distributions over the vertices of P and Q; distributions with the same marginals give the same expected loss. This plays a crucial role in our proofs. Thus the Nash equilibrium problem for MSP games reduces to (1): minx\u2208P maxy\u2208Q xTLy = maxy\u2208Q minx\u2208P x\nTLy. As an example of an MSP game, consider a spanning tree game where the pure strategies of each player are the spanning trees of a given graph G = (V,E) with m edges, and L is the m \u00d7m identity matrix. This\ncorresponds to the game in which the row player would try to minimize the intersection of his/her spanning tree with that of the column player, whereas the column player would try to maximize the intersection. For a complete graph, the number of pure strategies for each player is nn\u22122 by Cayley\u2019s theorem, where n is the number of vertices. For the graph G in Figure 1(a), the marginals of the unique Nash equilibrium for both players are given in 1(b) and (c). For graphs whose blocks are uniformly dense, both players can play the same (called symmetric) optimal mixed strategy.\nFor MSP games with bilinear losses, the linear programs (LP1) and (LP2) can be reformulated over the space of marginals, and (LP1) becomes\n(LP1\u2032) : min\u03bb\nxTLv \u2264 \u03bb \u2200 v \u2208 V, (2) x \u2208 P \u2286 Rm, (3)\nand similarly for (LP2): max{\u00b5 : uTLy \u2265 \u00b5 \u2200u \u2208 U , y \u2208 Q}. This reformulation can be used to show that, for these MSP games with bilinear losses (and exponentially many strategies), there exists a Nash equilibrium with small (polynomial) encoding length. A polyhedron K is said to have vertex-complexity at most \u03bd if there exist finite sets V,E of rational vectors such that K = conv(V ) + cone(E) and such that each of the vectors in V and E has encoding length at most \u03bd. A polyhedron K is said to have facet-complexity at most \u03c6 if there exists a system of inequalities with rational coefficients that has solution setK such that the (binary) encoding length of each inequality of the system is at most \u03c6. Let \u03bdP and \u03bdQ be the vertex complexities of polytopes P and Q respectively; if P and Q are 0/1 polytopes, we have \u03bdP \u2264 m and \u03bdQ \u2264 n. This means that the facet complexity of P and Q are O(m2\u03bdP ) and O(n2\u03bdQ) (see Lemma (6.2.4) in Lova\u0301sz et al. (1988)). Therefore the facet complexity of the polyhedron in (LP1\u2032) can be seen to be O(max(m\u3008L\u3009\u03bdQ, n2\u03bdP )), where \u3008L\u3009 is the binary enconding length of L and the first term in the max corresponds to the inequalities (2) and the second to (3). From this, we can derive Lemma 1.\nLemma 1 The vertex complexity of the linear program (LP1\u2032) is O(m2(m\u3008L\u3009\u03bdQ + n2\u03bdP )) where \u03bdP and \u03bdQ are the vertex complexities of P and Q and \u3008L\u3009 is the binary encoding length of L. (If P and Q are 0/1 polytopes then \u03bdP \u2264 m and \u03bdQ \u2264 n.)\nThis means that our polytope defining (LP1\u2032) is well-described (a\u0300 la Gro\u0308tschel et al.). We can thus use the machinery of the ellipsoid algorithm (Gro\u0308tschel et al. (1981)) to find a Nash Equilibrium in polynomial\ntime for these MSP games, provided we can optimize (or separate) over P and Q. Indeed, by the ellipsoid algorithm, we have the equivalence between strong separation and strong optimization for well-described polyhedra. The strong separation over (2) reduces to strong optimization over Q, while a strong separation algorithm over (3), i.e. over P , can be obtained from a strong separation over P by the ellipsoid algorithm.\nWe should also point out at this point that, if the polyhedra P and Q admit a compact extended formulation then (LP1\u2032) can also be reformulated in a compact way (and solved using interior point methods, for example). A compact extended formulation for a polyhedron P \u2286 Rd is a polytope with polynomially many (in d) facets in a higher dimensional space that projects onto P . This allows to give a compact extended formulation for (LP1\u2032) for the spanning tree game as a compact formulation is known for the spanning tree polytope (Martin (1991)) (and any other game where the two strategy polytopes can be described using polynomial number of inequalities). However, this would not work for a corresponding matching game since the extension complexity for the matching polytope is exponential (Rothvo\u00df (2014))."}, {"heading": "4. Online mirror descent", "text": "In this section, we show how to perform mirror descent faster, by providing algorithms for minimizing strongly convex functions over base polytopes of polymatroids.\nConsider a compact convex set X \u2286 Rn, and let D \u2286 Rn be a convex open set such that X is included in its closure. A mirror map (or a distance generating function) is a k-strongly convex function2 and differentiable function \u03c9 : D \u2192 R that satisfies additional properties of divergence of the gradient on the boundary of D, i.e., limx\u2192\u2202D ||\u2207\u03c9(x)|| = \u221e (for details, refer to Nemirovski and Yudin (1983), Beck and Teboulle (2003), Bubeck (2014)). In particular, we consider two important mirror maps in this work, the Euclidean mirror map and the unnormalized entropy mirror map. The Euclidean mirror map is given by \u03c9(x) = 12 ||x||\n2, for D = RE and is 1-strongly convex with respect to the L2 norm. The unnormalized entropy map is given by \u03c9(x) = \u2211 e\u2208E x(e) ln(x(e)) \u2212 \u2211 e\u2208E x(e), for D = RE+ and is 1-strongly convex over the unnormalized simplex with respect to the L1 norm (see proof in Appendix A, Lemma 15). The Online Mirror Descent (OMD) algorithm with respect to the mirror map \u03c9 proceeds as follows. Think ofX as the strategy polytope of the row player or the learner in the online algorithm that is trying to minimize regret over the points in the polytope with respect to loss vectors l(t) revealed by the adversary in each round. The iterative algorithm starts with the first iterate x(1) equal to the \u03c9-center of X given by arg minx\u2208X \u03c9(x). Subsequently, for t > 1, the algorithm first moves in unconstrained way using\n\u2207\u03c9(y(t+1)) = \u2207\u03c9(x(t))\u2212 \u03b7\u2207l(t)(x(t)).\nThen the next iterate x(t+1) is obtained by a projection step:\nx(t+1) = arg min x\u2208X\u2229D D\u03c9(x, y (t+1)), (4)\nwhere the generalized notion of projection is defined by the functionD\u03c9(x, y) = \u03c9(x)\u2212\u03c9(y)\u2212\u2207\u03c9(y)T (x\u2212 y), called the Bregman divergence of the mirror map. Since the Bregman divergence is a strongly convex function in x for a fixed y, there is a unique minimizer x(t+1). For the two mirror maps discussed above, the divergence is D\u03c9(x, y) = 12 ||x \u2212 y||\n2 for y \u2208 RE for the Euclidean mirror map, and is D\u03c9(x, y) =\u2211 e\u2208E x(e) ln(x(e)/y(e))\u2212 \u2211 e\u2208E x(e) + \u2211 e\u2208E y(e) for the entropy mirror map.\n2. A function f is said to be k-strongly convex over domain D with respect to a norm || \u00b7 || if f(x) \u2265 f(y) +\u2207f(y)T (x \u2212 y) + k 2 ||x\u2212 y||2.\nThe regret of the online mirror descent algorithm is known to scale as O(RG \u221a t) where R depends on the geometry of the convex set and is given by R2 = arg maxx\u2208X \u03c9(x) \u2212 arg minx\u2208X \u03c9(x) and G is the Lipschitz constant of the underlying loss functions, i.e., ||\u2207l(i)||\u2217 \u2264 G for all i = 1, . . . , T . We restate the theorem about the regret of the online mirror-descent algorithm (adapted from Bubeck (2011), Ben-Tal and Nemirovski (2001), Rakhlin and Sridharan (2014)).\nTheorem 2 Consider online mirror descent based on a k-strongly convex (with respect to || \u00b7 ||) and differentiable mirror map \u03c9 : D \u2192 R on a closed convex set X. Let each loss function l(i) : X \u2192 R be convex and GLipschitz, i.e. ||\u2207l(i)||\u2217 \u2264 G \u2200i \u2208 {1, . . . , t} and let the radiusR2 = arg maxx\u2208X \u03c9(x)\u2212arg minx\u2208X \u03c9(x). Further, we set \u03b7 = RG \u221a 2k t then:\nt\u2211 i=1 l(i)(xi)\u2212 t\u2211 i=1 l(i)(x\u2217) \u2264 RG \u221a 2t k for all x\u2217 \u2208 X."}, {"heading": "4.1. Convex Minimization on Base Polytopes of Polymatroids", "text": "In this section, we consider the setting in whichX is the base polytope of a polymatroid. Let f be a monotone submodular function, i.e., f must satisfy the following conditions: (i) (monotonicity) f(A) \u2264 f(B) for all A \u2286 B \u2286 E, and (ii) (submodularity) f(A)+f(B) \u2265 f(A\u222aB)+f(A\u2229B) for all A,B \u2286 E. Furthermore, we will assume f(\u2205) = 0 (normalized) and without loss of generality we assume that f(A) > 0 for A 6= \u2205. Given such a function f , the independent set polytope is defined as P (f) = {x \u2208 RE+ : x(U) \u2264 f(U) \u2200 U \u2286 E} and the base polytope as B(f) = {x \u2208 RE+ : x(E) = f(E), x(U) \u2264 f(U) \u2200 U \u2286 E} (Edmonds (1970)). A typical example is when f is the rank function of a matroid, and the corresponding base polytope corresponds to the convex hull of its bases. Bases of a matroid include spanning trees (bases of a graphic matroid), k-sets (uniform matroid), maximally matchable sets of vertices in a graph (matching matroid), or maximal subsets of T \u2286 V having disjoint paths from vertices in S \u2286 V in a directed graph G = (V,E) (gammoid).\nLet us consider any strongly convex separable function h : D \u2192 R, defined over a convex open set D such that P (f) \u2286 D (i.e., closure of D) and\u2207h(D) = RE . We require that either 0 \u2208 D or there exists some x \u2208 P (f) such that\u2207h(x) = c\u03c7(E), c \u2208 R. These conditions hold, for example, for minimizing the Bregman divergence of the two mirror maps (Euclidean and entropy) we discussed in the previous section. We present in this section an algorithm INC-FIX, that minimizes such convex functions h over the base polytope B(f) of a given monotone normalized submodular function f . Our approach can be interpreted as a generalization of Fujishige\u2019s monotone algorithm for finding a lexicographically optimal base to handle general separable convex functions.\nKey idea: The algorithm is iterative and maintains a vector x \u2208 P (f) \u2229 D. When considering x we associate a weight vector given by \u2207h(x) and consider the set of minimum weight elements. We move x within P (f) in a direction such that (\u2207h(x))e increases uniformly on the minimum weight elements, until one of two things happen: (i) either continuing further would violate a constraint defining P (f), or (ii) the set of elements of minimum weight changes. If the former happens, we fix the tight elements and continue the process on non-fixed elements. If the latter happens, then we continue increasing the value of the elements in the modified set of minimum weight elements. The complete description of the INC-FIX algorithm is given in Algorithm 1. We refer to the initial starting point as x(0). The algorithm constructs a sequence of points x(0), x(1), . . . , x(k) = x\u2217 in P (f). At the beginning of iteration i, the set of non-fixed elements whose value can potentially be increased without violating any constraint is referred to as Ni\u22121. The iterate x(i)\nis obtained by increasing the value of minimum weight elements of x(i\u22121) in Ni\u22121 weighted by (\u2207h(x))e such that the resulting point stays in P (f). Iteration i of the main loop ends when some non-fixed element becomes tight and we fix the value on these elements by updating Ni. We continue until all the elements are fixed, i.e., Ni = \u2205. We denote by T (x) : RE \u2192 2E the maximal set of tight elements in x (which is unique by submodularity of f ).\nAlgorithm 1 INC-FIX Input: f : 2E \u2192 R, h = \u2211 e\u2208E he, and input x (0)\nOutput: x\u2217 = arg minz\u2208B(f) \u2211\ne he(z(e)) N0 = E, i = 0 repeat /* Main loop */\ni\u2190 i+ 1 x = x(i\u22121) M = arg mine\u2208Ni\u22121 \u2207(h(x))e while T (x) \u2229M = \u2205 do /* Inner loop */\n1 = max{\u03b4 : (\u2207h)\u22121(\u2207h(x) + \u03b4\u03c7(M)) \u2208 P (f)} 2 = arg mine\u2208Ni\u22121\\M (\u2207h(x))e \u2212 arg mine\u2208Ni\u22121(\u2207h(x))e x\u2190 (\u2207h)\u22121(\u2207h(x) + min( 1, 2)\u03c7(M)); /* Increase */ M = arg mine\u2208Ni\u22121(\u2207h(x))e end x(i) = x, Mi = arg mine\u2208Ni\u22121(\u2207h(x(i)))e Ni = Ni\u22121 \\ (Mi \u2229 T (x(i))) /* Fix */\nuntil Ni = \u2205; Return x\u2217 = x(i).\nChoice of starting point: We let x(0) = 0 unless 0 /\u2208 D; observe that 0 \u2208 P (f) \u2286 D. In the latter case, we let x(0) \u2208 P (f) such that\u2207h(x(0)) = c\u03c7(E) for some c \u2208 R. For example, for the Euclidean mirror map and some y \u2208 RE , \u2207h(x) = \u2207D\u03c9(x, y) = x\u2212 y and D = RE , hence we start the algorithm with x(0) = 0. However, for the entropy mirror map and some positive (component-wise) y \u2208 RE , \u2207h(x) = \u2207D\u03c9(x, y) = ln(xy ) andD = R E >0. We thus start the algorithm with x\n(0) = cy for a small enough c > 0 such that x \u2208 P (f). We now state the main theorem to prove correctness of the algorithm.\nTheorem 3 Consider a k-strongly convex and separable function \u2211\ne\u2208E he(\u00b7) : D \u2192 R whereD is a convex open set in RE , P (f) \u2286 D and \u2207h(D) = RE such that either 0 \u2208 D or there exists x \u2208 P (f) such that \u2207h(x) = c\u03c7(E) for c \u2208 R. Then, the output of INC-FIX algorithm is x\u2217 = arg minz\u2208B(f) \u2211 e he(z(e)).\nThe proof relies on the following optimality conditions, which follows from first order optimality conditions and Edmonds\u2019 greedy algorithm. Theorem 4 Consider any strongly convex separable function h(x) : D \u2192 R where h(x) = \u2211\ne\u2208E he(x(e)), and any monotone submodular function f : 2E \u2192 R with f(\u2205) = 0. Assume P (f) \u2286 D and \u2207h(D) = RE . Consider x\u2217 \u2208 RE . Let F1, F2, . . . , Fk be a partition of the ground set E such that (\u2207h(x\u2217))e = ci for all e \u2208 Fi and ci < cj for i < j. Then, x\u2217 = arg minz\u2208B(f) \u2211 e\u2208E he(z(e)) if and only if x\n\u2217 lies in the face Hopt of B(f) given by\nHopt := {z \u2208 B(f)| z(F1 \u222a . . . \u222a Fi) = f(F1 \u222a . . . \u222a Fi) \u2200 1 \u2264 i \u2264 k}.\nProof By first order optimality conditions, we know that x\u2217 = arg minz\u2208B(f) \u2211\ne he(x(e)) if and only if \u2207h(x\u2217)T (z \u2212 x\u2217) \u2265 0 for all z \u2208 B(f). This is equivalent to x\u2217 \u2208 arg minz\u2208B(f)\u2207h(x\u2217)T z. Now consider the partition F1, F2, . . . , Fk as defined in the statement of the theorem. Using Edmonds\u2019 greedy algorithm Edmonds (1971), we know that any z\u2217 \u2208 B(f) is a minimizer of \u2207h(x\u2217)T z if and only if it is tight (i.e., full rank) on each F1 \u222a . . . Fi for i = 1, . . . , k, i.e., z\u2217 lies in the face Hopt of B(f) given by\nHopt := {z \u2208 B(f)| z(F1 \u222a . . . \u222a Fi) = f(F1 \u222a . . . \u222a Fi) \u2200 1 \u2264 i \u2264 k}.\nNote that at the end of the algorithm, there may be some elements at zero value (specifically in cases where x(0) = 0). In our proof for correctness for the algorithm, we use the following simple lemma about zero-valued elements.\nLemma 5 For x \u2208 P (f), if a subset S of elements is tight then so is S \\ {e : x(e) = 0}.\nProof Let S = S1 \u222a S2 such that x(S2) = 0 and x(e) > 0 for all e \u2208 S1. Then, f(S1) \u2265 x(S1) = x(S1 \u222a S2) = f(S1 \u222a S2) \u2265 f(S1), where the last inequality follows from monotonicity of f , implying that we have equality throughout. Thus, x(S1) = f(S1).\nWe now give the proof for Theorem 3 to show the correctness of the INC-FIX algorithm. Proof Note that since h(x) = \u2211 e he(x(e)) is separable and k-strongly convex, \u2207h is a strictly increasing function (for each component e, h\u2032e(x)\u2212h\u2032e(y) \u2265 k(x\u2212 y) for x > y). Moreover, (\u2207h)\u22121 is well-defined for all points in RE since\u2207h(D) = RE . Consider the output of the algorithm x\u2217 and let us partition the elements of the ground set E into F1, F2, . . . , Fk such that h\u2032e(x\n\u2217(e)) = ci for all e \u2208 Fi and ci < cj for i < j. We will show that Fi = Mi \u2229T (x(i)) and that k is the number of iterations of Algorithm 1. We first claim that in each iteration i \u2265 1 of the main loop, the inner loop satisfies the following invariant, as long as the initial starting point x(0) \u2208 P (f):\n(a). The inner loop returns x(i) \u2208 P (f) such that h\u2032e(x(i)(e)) = max{h\u2032e(x(i\u22121)(e)), (i)} for e \u2208 Ni\u22121 and some (i) \u2208 R, x(i)(e) = x(i\u22121)(e) for e \u2208 E \\Ni\u22121, and T (x(i)) \u2229Mi 6= \u2205.\nNote that M is initialized to be the set of minimum elements in Ni\u22121 with respect to \u2207h(x(i\u22121)) before entering the inner loop. 1 ensures that the potential increase in \u2207h(x) on elements in M is such that the corresponding point x \u2208 P (f) ( 1 exists since x(i\u22121) \u2208 P (f)). 2 ensures that the potential increase in \u2207h(x) on elements in M is such that M remains the set of minimum weighted elements in Ni\u22121. Finally, x is obtained by increasing \u2207h(x) by min( 1, 2) and M is updated accordingly. This ensures that at any point in the inner loop, M = arg mine\u2208Ni\u22121 h \u2032 e(x(e)). This continues till there is a tight set T (x) of the current iterate, x, that intersects with the minimum weighted elements M . Observe that in each iteration of the inner loop either the size of T (x) increases (in the case when 1 = min( 1, 2)) or the size of M increases (in the case when 2 = min( 1, 2)). Therefore, the inner loop must terminate. Note that (i) = mine\u2208Ni\u22121 h \u2032 e(x (i)(e)) = h\u2032f (x (i)(f)) for f \u2208Mi, by definition of Mi.\nRecall that Mi = arg mine\u2208Ni\u22121(\u2207h(x(i)))e and let the set of elements fixed at the end of each iteration be Li = Mi \u2229 T (x(i)). We next prove the following claims at the end of each iteration i \u2265 1.\n(b). x(i\u22121)(e) \u2264 x(i)(e) for all e \u2208 E, as \u2207h is a strictly increasing function and \u2207h(x(i\u22121)) \u2264 \u2207h(x(i)) due to claim (a).\n(c). Next, observe that we always decrease the set of non-fixed elements Ni, i.e., Ni \u2282 Ni\u22121. This follows since Ni = Ni\u22121 \\ Li and \u2205 6= Li \u2286 Ni\u22121 (follows from (a) and definition of Li).\n(d). By construction, the set of elements fixed at the end of each iteration partition E \\ Ni, i.e., E \\ Ni = L1\u222a\u0307 . . . \u222a\u0307Li.\n(e). We claim that the set of minimum elementsMi at the end of any iteration i always contains the left-over minimum elements from the previous iteration, i.e., Mi\u22121 \\ Li\u22121 \u2286Mi. This is clear if Li\u22121 = Mi\u22121, so consider the case when Li\u22121 \u2282 Mi\u22121. At the beginning of the inner loop of iteration i, M = arg mine\u2208Ni\u22121 h \u2032 e(x\n(i\u22121)(e)) = Mi\u22121 \\ Li\u22121. Subsequently, in the inner loop the set of minimum elements can only increase and thus, Mi \u2287Mi\u22121 \\ Li\u22121.\n(f). We next show that (i\u22121) < (i) for i \u2265 2. Consider an arbitrary iteration i \u2265 2. If Li\u22121 = Mi\u22121, then (i) = mine\u2208Ni\u22121=Ni\u22122\\Li\u22121 h \u2032 e(x\n(i)(e))\u2265(\u2217) mine\u2208Ni\u22122\\Mi\u22121 h\u2032e(x(i\u22121)(e)) > mine\u2208Mi\u22121 h\u2032e(x(i)(e)) = (i\u22121) where (*) follows from (b). Otherwise, we have Li\u22121 \u2282 Mi\u22121. This implies that Mi\u22121 \\ Li\u22121 is not tight on x(i\u22121), and it is in fact equal to arg mine\u2208Ni\u22121 h \u2032 e(x\ni\u22121(e)) (= M at the beginning of the inner loop in iteration i). As this set is not tight, the gradient value can be strictly increased and therefore (i) > (i\u22121).\nWe next split the proof into two cases (A) and (B), depending on how x(0) is initialized.\n(A) Proof for x(0) = 0:\n(g). We claim that x(i)(e) = x(i\u22121)(e) for all e \u2208 Ni\\Mi. This follows since the gradient values, h\u2032e(xi(e)), for the edges e \u2208 Ni \\Mi are greater than (i) and thus remain unchanged from x(i\u22121) (due to claim (a)).\n(h). We claim that x(i)(e) = 0 for e \u2208 Ni\\Mi. Using (e) we getNi\\Mi = Ni\u22121\\Mi \u2286 Ni\u22121\\(Mi\u22121\\Li). Since Li \u2229 Ni = \u2205, we get Ni \\Mi \u2286 Ni\u22121 \\Mi\u22121. Now (g) implies that x(i)(e) = x(0)(e) for all e \u2208 Ni \\Mi.\n(i). We next prove that 1 \u2264 j \u2264 i, x(i)(L1 \u222a . . . \u222a Lj) = f(L1 \u222a . . . \u222a Lj). First, for i = 1 note that T (x(1)) can be partitioned into {L1, T (x(1)) \\ M1}. Since T (x(1)) \\ M1 \u2286 N1 \\ M1, we get x(1)(T (x(1)) \\M1) = 0 using (g). Thus, by Lemma 5, we get that x(1) is tight on L1 as well. Next, consider any iteration i > 1. If j < i, then x(j)(L1 \u222a . . .\u222aLj) = f(L1 \u222a . . .\u222aLj) by induction. Since x(i) \u2265 x(j), x(i) must also be tight on L1 \u222a . . . \u222a Lj . Note that T (x(i)) can be partitioned into { ( T (x(i))\u2229 (E \\Ni\u22121 ) , ( T (x(i))\u2229 (Ni\u22121 \\Mi) ) , ( T (x(i))\u2229Mi ) } = { ( L1 \u222a . . .\u222aLi\u22121 ) , ( T (x(i))\u2229\n(Ni \\Mi) ) , Li} using (d). Note that x(i) is zero-value on Ni \\Mi, from (h). By Lemma (5) we get that\nx(i) is also tight on ( L1 \u222a . . . \u222a Li ) .\nClaim (b) implies the termination of the algorithm when for some t, Nt = \u2205. From claim (d), we have obtained a partition ofE into disjoint sets {L1, L2, . . . , Lt}. From claims (a) and (d), we get x(t)(e) = (i) for e \u2208 Li. Claim (f) implies that the partition in the theorem {F1, . . . , Fk} is identical to the partition obtained via the algorithm {L1, . . . , Lt} (hence t = k). Claim (i) implies that x\u2217 = x(t) lies in the face Hopt as defined in Theorem 4.\n(B) Proof for x(0) \u2208 P (f) such that\u2207h(x(0)) = c\u03c7(E) for some c \u2208 R:\n(g\u2032). We claim that Mi = Ni\u22121. For iteration i = 1, h\u2032e(x (1)(e)) = (1) for all e \u2208 E, since h\u2032e(x(0)(e)) = c\nfor all the edges. Thus, indeed, M1 = E = N0. For iteration i > 1, we have Ni\u22121 = Ni\u22122 \\ Li\u22121 = Mi\u22121 \\ Li\u22121 (by induction). This implies that h\u2032e(x(i\u22121)(e)) = (i\u22121) for all the edges in Ni\u22121. Thus, in iteration i, all the edges e must again have the same gradient value, h\u2032e(x (i)(e)), due to invariant (a).\n(h\u2032). We claim that 1 \u2264 j \u2264 i, x(i)(L1 \u222a . . . \u222a Lj) = f(L1 \u222a . . . \u222a Lj). First, for iteration i = 1, since M1 = E by (g\u2032), we have T (x(1)) = L1. So, x(1) is tight on L1.\nNext, consider any iteration i > 1. If j < i, then x(j)(L1 \u222a . . .\u222aLj) = f(L1 \u222a . . .\u222aLj) by induction. Since x(i) \u2265 x(j), x(i) must also be tight on L1 \u222a . . . \u222a Lj . Note that T (x(i)) can be partitioned into { ( T (x(i))\u2229 (E \\Ni\u22121 ) , ( T (x(i))\u2229 (Ni\u22121 \\Mi) ) , ( T (x(i))\u2229Mi ) } = { ( L1\u222a . . .\u222aLi\u22121 ) , \u2205, Li} using (d), the claim follows.\nClaim (b) implies the termination of the algorithm when for some t, Nt = \u2205. From claim (d), we have obtained a partition ofE into disjoint sets {L1, L2, . . . , Lt}. From claims (a) and (d), we get x(t)(e) = (i) for e \u2208 Li. Claim (f) implies that the partition in the theorem {F1, . . . , Fk} is identical to the partition obtained via the algorithm {L1, . . . , Lt} (hence t = k). Claim (h\u2032) implies that x\u2217 = x(t) lies in the face Hopt as defined in Theorem 4."}, {"heading": "4.2. Bregman Projections under the Entropy and Euclidean Mirror Maps", "text": "We next discuss the application of INC-FIX algorithm to two important mirror maps. One feature that is common to both is that the trajectory of x in INC-FIX is piecewise linear, and the main step is find the maximum possible increase of x in a given direction d.\nUnnormalized entropy mirror map This map is given by \u03c9(x) = \u2211 e\u2208E x(e) lnx(e) \u2212 \u2211\ne\u2208E x(e) and its divergence is D\u03c9(x, y) = \u2211 e\u2208E x(e) ln(x(e)/y(e))\u2212 \u2211 e\u2208E x(e) + \u2211 e\u2208E y(e). Note that\u2207D\u03c9(x, y) = ln(xy ) for a given y > 0. Finally, \u2207D \u22121 \u03c9 (x) = ye\n\u2212x. In order to minimize the divergence D\u03c9(x, y) with respect to a given point y \u2208 RE+ using the INC-FIX algorithm, we need to find the maximum possible increase in the gradient \u2207D\u03c9 while remaining in the submodular polytope P (f). In each iteration, this amounts to computing:\n1 = max{\u03b4 : (\u2207D\u03c9)\u22121(\u2207D\u03c9(x) + \u03b4\u03c7(M)) \u2208 P (f)} = max{\u03b4 : ye(\u2207D\u03c9(x)+\u03b4\u03c7(M)) \u2208 P (f)} = max{\u03b4 : x+ z \u2208 P (f), z(e) = (e\u03b4 \u2212 1)x(e) for e \u2208M, z(e) = 0 for e /\u2208M},\nfor someM \u2286 E. For the entropy mirror map, the point to be projected, y, must be positive in each coordinate (otherwise the divergence is undefined). We initialize x(0) = cy \u2208 P (f) for a small enough constant c > 0. Apart from satisfying the conditions of the INC-FIX algorithm, this ensures that there is a well defined direction for increase in each iteration.\nEuclidean mirror map The Euclidean mirror map is given by \u03c9(x) = 12 ||x|| 2 and its divergence is D\u03c9(x, y) = 1 2 ||x \u2212 y||\n2. Here, for a given y \u2208 R, \u2207D\u03c9(x, y) = \u2207\u03c9(x) \u2212 \u2207\u03c9(y) = x \u2212 y. This implies that for any iteration, we need to compute\n1 = max{\u03b4 : (\u2207D\u03c9)\u22121(\u2207D\u03c9(x) + \u03b4\u03c7(M)) \u2208 P (f)} = max{\u03b4 : (\u2207D\u03c9(x) + \u03b4\u03c7(M)) + y \u2208 P (f)} = max{\u03b4 : x+ \u03b4\u03c7(M) \u2208 P (f)},\nfor some M \u2286 E. Notice that in each iteration of both the algorithms for the entropy and the Euclidean mirror maps, we need to find the maximum possible increase to the value on non-tight elements in a fixed given direction d \u2265 0. For the case of the entropy mirror map, de = xe for e \u2208 M and de = 0 otherwise, while, for the Euclidean mirror map, d = \u03c7(M) (for current iterate x \u2208 P (f), and corresponding minimum weighted set of edges M \u2286 E).\nLet LINE be the problem of finding the maximum \u03b4 s.t. x + \u03b4d \u2208 P (f). P1 is equivalent to finding maximum \u03b4 such that minS\u2286E f(S) \u2212 (x + \u03b4d)(S) = 0; this is a parametric submodular minimization problem. For the graphic matroid, feasibility in the forest polytope can be solved by O(|V |) maximum flow problems (Cunningham (1985), Corollary 51.3a in Schrijver (2003)), and as result, LINE can be solved as O(|V |2) maximum flow problems (by Dinkelbach\u2019s discrete Newton method) orO(|V |) parametric maximum flow problems. For general polymatroids over the ground set E, the problem LINE can be solved using Nagano\u2019s parametric submodular function minimization (Nagano (2007c)) that requires O(|E|6 + \u03b3|E|5) running time, where \u03b3 is the time required by the value oracle of the submodular function. Each of the entropy and the Euclidean mirror maps requires O(|E|) (O(|V |) for the graphic matroid) such computations to compute a projection, since each iteration of the INC-FIX algorithm at least one non-tight edge becomes tight. Thus, for the graphic matroid we can compute Bregman projections in O(|V |4|E|) time (using Orlin\u2019s O(|V ||E|) algorithm (Orlin (2013)) for computing the maximum flow) and for general polymatroids the running time is O(|E|7 + \u03b3|E|6) where |E| is the size of the ground set. For cardinality-based submodular functions3, we can prove a better bound on the running time.\nLemma 6 The INC-FIX algorithm takes O(|E|2) time to compute projections over base polytopes of polymatroids when the corresponding submodular function is cardinality-based.\nProof Let f be a cardinality-based submodular function such that f(S) = g(|S|) for all S \u2286 E and some concave g : N \u2192 R. Let T (x) denote the maximal tight set of x \u2208 P (f). Let LINE be the problem to find \u03bb\u2217 = max{\u03bb : x+ \u03bbz \u2208 P (f)} where x \u2208 P (f), z \u2208 RE+. Let S(x) be a sorted sequence of components of x in decreasing order. We will show that for both the mirror maps, we can solve LINE in O(|E|) time.\n(i) We first claim that (x+\u03bb\u2217y)(e) \u2264 mine\u2032\u2208T (x) x(e\u2032) for all e \u2208 E\\T (x). Let e\u2217 \u2208 arg mine\u2032\u2208T (x) x(e\u2032). The claim holds since otherwise the submodular constraint on the set T (x) \\ {e\u2217} \u222a {e\u2032} (which is of the same cardinality as T (x)) will be violated.\n(ii) Let S(x) = {x(e1), x(e2), . . . , x(em)} such that x(ei) \u2265 x(ej) for 1 \u2264 i < j \u2264 m. Then, to check if a vector x is in P (f), one can simply check if \u2211k i=1 x(ei) \u2264 g(k) for each k = 1, . . . ,m as the\nsubmodular function f is cardinality-based.\n(iii) We next claim that for each of the Euclidean and entropy mirror maps, the ordering of the elements remains the same in subsequent iterations, i.e., at the end of each iteration i, the ordering S(x(i)) = S(x(i\u22121)) up to equal elements.\n(A) For the Euclidean mirror map, with x(0) = 0: We first claim that S(y) = S(x(0)) up to equal elements; this holds since x(0) = 0. Next, for each iteration i we claim that S(x(i\u22121)) = S(x(i\u22121) + \u03bbz) up to equal elements for \u03bb > 0. For the Euclidean mirror map, z = \u03c7(M) for M = arg mine\u2208Ni\u22121(x\n(i\u22121)(e) \u2212 y(e)) (or for some intermediate iterate in the inner loop). LetD = {e \u2208 Ni\u22121 : x(i\u22121)(e) > 0}. Note thatD \u2286M using claim (e) of the proof for Theorem\n3. A submodular function is cardinality-based if f(S) = g(|S|) for all S \u2286 E and some concave g : N\u2192 R.\n3. As all elements of D are increased by the same amount \u03bb, while being bound by the value of minimum element in T (x), their ordering remains the same as S(x(i\u22121)) after the increase. Further the zero-elements in M , i.e., M \\D have the highest value of y(e) among zero-elements of x(i\u22121). Therefore, increasing these uniformly respects the ordering with respect to S(y), while being less than the value of elements in D.\n(B) For the entropy mirror map, with x(0) = y \u2208 P (f): For each iteration i, we claim that S(x(i\u22121)) = S(x(i\u22121) + \u03bbz) up to equal elements, for some \u03bb > 0. For the entropy mirror map, the direction z is given by z(e) = x(i)(e) for e \u2208 Ni\u22121 = E\\T (x) (see claim (g\u2032) in the proof for Theorem 3) and z(e) = 0 for e \u2208 T (x). Since we increase x(i\u22121) proportionally to x(i\u22121)(e) on all elements in Ni\u22121, while being bound by the value of the minimum element in T (x), the ordering of the elements S(x(i\u22121) + \u03bbz) is the same as S(x(i\u22121)) (up to equal elements).\n(iv) Since the ordering of the elements remains the same after an increase, we get an easy way to solve the problem LINE. For each k = |T (x)| + 1, . . . , |E|, we can compute the maximum possible increase possible without violating a set of cardinality k which is given by tk = g(k)\u2212 \u2211k\ni=1 xi\u2211k i=|T (x)| z(i) . Then \u03bb\u2217 =\nmink tk and this can be checked in O(|E|) time.\nHence, we require a single sort at the beginning of the INC-FIX algorithm (O(|E| ln |E|) time), and using this ordering (that does not change in subsequent iterations) we can perform LINE in O(|E|) time. Therefore, the running time of INC-FIX for cardinality-based submodular functions is O(|E|2).\nComputing Nash-equilibria Let us now consider the MSP game over strategy polytopes P and Q under a bilinear loss function, such that P is the base polytope of a matroid (and Q is any polytope that one can optimize linear functions over). The online mirror descent algorithm starts with x(0) being the \u03c9\u2212center of the base polytope, that is simply obtained by projecting a vector of ones on the base polytope. Each subsequent iteration x(t) is obtained by projecting an appropriate point y(t) under the Bregman projection of the mirror map. For the entropy map, y(t) = [x(t\u22121)1 e \u2212\u03b7\u2207l(t\u22121)1 ; . . . ;x (t\u22121) m e\u2212\u03b7\u2207l (t\u22121) m ], and for the Euclidean map y(t) is simply given by y(t) = [x(t\u22121)1 \u2212 \u03b7\u2207l (t\u22121) 1 ; . . . ;x (t\u22121) m \u2212 \u03b7\u2207l(t\u22121)m ]. Here, \u2207l(t) = Lv(t) where v(t) = arg maxz\u2208Q x (t)TLz. Note that for the entropy map, each y(t) (that we project) is guaranteed to be strictly greater than zero since x(0) > 0. Assuming the loss functions are G-Lipschitz under the appropriate norm (i.e., L1-norm for the entropy mirror map, and L2-norm for the Euclidean mirror map), after T = O(G2R2/ 2) iterations of the mirror descent algorithm, we obtain a \u2212approximate Nash-equilibrium of the MSP game given by ( \u2211T i=1 x (i)/T, \u2211T i=1 v (i)/T ). For the entropy map, R2 \u2264 r(E) ln(m) and for the Euclidean mirror map, R2 \u2264 r(E). Using the Euclidean mirror map (as opposed to the entropy map) even though we reduce theR2 term in the number of iterations, the Lipschitz constant might be greater with respect to the L2-norm (as opposed to the L1-norm). For example, suppose in a spanning tree game the loss matrix L is scaled such that ||L||\u221e \u2264 1. Then, the loss functions are such that Gentropy = ||\u2207l(i)||\u221e = ||Lv(i)||\u221e \u2264 n and GEuc = ||\u2207li||2 = ||Lv(i)||2 \u2264 n \u221a m and so, the online mirror descent algorithm converges to an - approximate strategy in O(R2G2/ 2) = O(n2 r(E) lnm/ 2) = O(n3 lnm/ 2) rounds (of learning) under the entropy mirror map4, whereas it takes O(n3m/ 2) rounds under the Euclidean map.\n4. Even though this case is identical to the multiplicative weights update algorithm, the general analysis for the mirror descent algorithms gives a better convergence rate with respect to the size of the graph n,m (but the same dependence on )."}, {"heading": "5. The Multiplicative Weights Update Algorithm", "text": "We now restrict our attention to MSP games over 0/1 strategy polytopes P and Q such that U = vert(P ) \u2286 {0, 1}m and V = vert(Q) \u2286 {0, 1}n. The vertices of these polytopes constitute the pure strategies of these games (i.e., combinatorial concepts like spanning trees, matchings, k-sets). We review the Multiplicative Weights Update (MWU) algorithm for MSP games over strategy polytopes P and Q. The MWU algorithm starts with the uniform distribution over all the vertices, and simulates an iterative procedure where the learner (say player 1) plays a mixed strategy x(t) in each round t. In response the ORACLE selects the most adversarial loss vector for the learner, i.e., l(t) = Lv(t) where v(t) = arg maxy\u2208Q x(t)Ly. The learner observes losses for all the pure strategies and incurs loss equal to the expected loss of their mixed strategy. Finally the learner updates their mixed strategy by lowering the weight of each pure strategy u by a factor of \u03b2u\nT l(t)/F for a fixed constant 0 < \u03b2 < 1 and a factor F that accounts for the magnitude of the losses in each round. That is, for each round t \u2265 1, the updates in the MWU algorithm are as follows, starting with w(1)(u) = 1 for all u \u2208 U :\nx(t) =\n\u2211 u\u2208U w\n(t)(u)u\u2211 u\u2208U w (t)(u) , v(t) = arg max y\u2208Q x(t)TLy,w(t+1)(u) = w(t)(u)\u03b2u TLv(t)/F \u2200u \u2208 U .\nStandard analysis of the MWU algorithm shows that an \u2212approximate Nash-equilibrium can be obtained in O(( ln |U|\n( /F )2 ) rounds in the case of MSP games (see for e.g. Arora et al. (2012)).\nNote that in many interesting cases of MSP games, the input of the game is O(ln |U|). Even though the MWU algorithm converges in O(ln |U|) rounds it requires O(|U|) updates per round. We will show in the following sections how this algorithm can be simulated in polynomial time (i.e., polynomial in the input of the game)."}, {"heading": "5.1. MWU in Polynomial Time", "text": "We show how to simulate the MWU algorithm in time polynomial in ln |U| where U is the vertex set of the 0/1 polytope P \u2282 Rm, by the use of product distributions. A product distribution p over the set U is such that p(u) \u221d \u220f e\u2208u \u03bbe for some vector \u03bb \u2208 Rm. We refer to the \u03bb vector as the multiplier vector of the product distribution. The two key observations here are that product distributions can be updated efficiently by updating only the multipliers (for bilinear losses), and multiplicative updates on a product distribution results in a product distribution again.\nTo argue that the MWU can work by updating only product distributions, suppose first that in some iteration t of the MWU algorithm, we are given a product distribution p(t) over the vertex set U implicitly by its multiplier vector \u03bb(t), and a loss vector l(t) \u2208 Rm such that the loss of each vertex u is uT l(t). In order to multiplicatively update the probability of each vertex u as p(t+1)(u) \u221d p(t)(u)\u03b2uT l(t) , note that we can simply update the multipliers with the loss of each component.\np(t+1)(u) \u221d p(t)(u)\u03b2uT l(t) \u221d (\u220f e\u2208u \u03bb(t)(e) ) \u03b2u T l(t) \u221d \u220f e\u2208u ( \u03bb(t)(e)\u03b2l (t)(e) )\nas u \u2208 {0, 1}m.\nHence, the resulting probability distribution p(t+1) is also a product distribution, and we can implicitly represent it in the form of the multipliers \u03bb(t+1)(e) = \u03bb(t)(e)\u03b2l\n(t)(e)(e \u2208 [m]) in the next round of the MWU algorithm.\nSuppose we have a generalized counting oracle M which, given \u03bb \u2208 Rm+ , computes \u2211 u\u2208U \u220f e:ue=1\n\u03bb(e) and also, for any element f , computes \u2211 u\u2208U :uf=1 \u220f e:ue=1 \u03bb(e). Such an oracle can be used to compute\nthe marginals x \u2208 P corresponding to the product distribution associated with \u03bb. Suppose we have also an adversary oracle R that computes the worst-case response of the adversary given a marginal point in the learner\u2019s strategy polytope, i.e., R(x) = arg maxv\u2208V xTLv (the losses depend only on marginals of the learner\u2019s strategy and not the exact probability distribution). Then, we can exactly simulate the MWU algorithm. We can initialize the multipliers to be \u03bb(1)(e) = 1 for all e \u2208 [m], thus effectively starting with uniform weights w(1) across all the vertices of the polytope. Given the multipliers \u03bb(t) for each round, we can compute the corresponding marginal point x(t) = M(\u03bb(t)) and the corresponding loss vector Lv(t) where v(t) = R(x(t)). Finally, we can update the multipliers with the loss in each component, as discussed above. It is easy to see that the standard proofs of convergence of the MWU go through, as we only change the way of updating probability distributions, and we obtain the statement of Theorem 7. We assume here that the loss matrix L \u2265 05. The proof is included in Appendix B.\nTheorem 7 Consider an MSP game with strategy polytopes P , Q and loss(x, y) = xTLy for x \u2208 P, y \u2208 Q, as defined above. Let F = maxx\u2208P,y\u2208Q xTLy and U = vert(P ). Given two polynomial oracles M and R where M(\u03bb) = x is the marginal point corresponding to multipliers \u03bb, and R(x) = arg maxy\u2208Q xTLy, the algorithm MWU with product updates gives anO( )\u2212approximate Nash equilibrium (x\u0304, y\u0304) = (1t \u2211t i=1 x (i), 1t \u2211t i=1 v (i)) in O( ln(|U|) ( /F )2 ) rounds and time polynomial in (n,m).\nHaving shown that the updates to the weights of each pure strategy can be done efficiently, the regret bound follows from known proofs for convergence of the multiplicative weights update algorithm. We would like to draw attention to the fact that, by the use of product distributions, we are not restricting the search of approximate equilibria. This follows from the analysis, and also from the fact that any point in the relative interior of a 0/1 polytope can be viewed as a (max-entropy) product distribution over the vertices of the polytope (Asadpour et al. (2010), Singh and Vishnoi (2014)).\nApproximate Computation. If we have an approximate generalized counting oracle, we would have an approximate marginal oracle M 1 that computes an estimate of the marginals, i.e. M 1(\u03bb) = x\u0303 such that ||M(\u03bb)\u2212x\u0303||\u221e \u2264 1 whereM(\u03bb) is the true marginal point corresponding to \u03bb, and an approximate adversary oracle R 2 that computes an estimate of the worst-case response of the adversary given a marginal point in the learner\u2019s strategy polytope, i.e. R 2(x) = v\u0303 such that xTLv\u0303 \u2265 maxv\u2208V xTLv \u2212 2 (for example, in the case when the strategy polytope is not in P and only an FPTAS is available for optimizing linear functions). We\nAlgorithm 2 The MWU algorithm with approximate oracles Input: M 1 : Rm \u2192 Rm, R 2 : Rm \u2192 Rn, > 0. Output: O( + F 1 + 2)-approximate Nash equilibrium (x\u0304, y\u0304) \u03bb(1) = 1, t = 1, F = maxx\u2208P,y\u2208Q x\nTLy, \u2032 = /F, \u03b2 = 1 1+ \u221a 2 \u2032 repeat x\u0303(t) = M 1(\u03bb(t)) v\u0303(t) = R 2(x\u0303(t)) \u03bb(t+1)(e) = \u03bb(t)(e) \u2217 \u03b2Lv\u0303\n(t)(e)/F \u2200e \u2208 E t\u2190 t+ 1 until t < F 2 ln |U | 2\n; (x\u0304, y\u0304) = ( 1t\u22121 \u2211t\u22121 i=1 x\u0303 (i), 1t\u22121 \u2211t\u22121 i=1 v\u0303 (i))\ngive a complete description of the algorithm in Algorithm 2 and the formal statement of the regret bound in the following lemma (for loss matrices L \u2265 05) (proved in Appendix B). The tricky part in the proof is that since the loss vectors are approximately computed from approximate marginal points, there is a possibility of\n5. This is however an artificial condition, and the regret bounds change accordingly for general losses.\nnot converging at all. However, we show that this is not the case since we maintain the true multipliers \u03bb(t) in each round. It is not clear if there would be convergence, for example, had we gone back and forth between the marginal point and the product distribution.\nLemma 8 Given two polynomial approximate oracles M 1 and R 2 where M 1(\u03bb) = x\u0303 s.t. ||M(\u03bb)\u2212 x\u0303||\u221e \u2264 1, and R 2(x) = v\u0303 s.t. xTLv\u0303 \u2265 maxy\u2208Q xTLy \u2212 2, the algorithm MWU with product updates gives an O( + F 1 + 2)\u2212approximate Nash equilibrium (x\u0304, y\u0304) = (1t \u2211t i=1 x\u0303 (i), 1t \u2211t i=1 v\u0303 (i)) in O(F 2 ln(|U|) 2\n) rounds and time polynomial in (n,m).\nApplications: For learning over the spanning tree polytope, an exact generalized counting algorithm follows from Kirchoff\u2019s matrix theorem (Lyons and Peres (2005)) that states that the value of \u2211 T \u220f e\u2208T \u03bbe is equal to the value of the determinant of any cofactor of the weighted Laplacian of the graph. One can use fast Laplacian solvers (see for e.g., Koutis et al. (2010)) for obtaining a fast approximate marginal oracle. Kirchhoff\u2019s determinantal formula also extends to (exact) counting of bases of regular matroids. For learning over the bipartite matching polytope (i.e., rankings), one can use the randomized generalized approximate counting oracle from (Jerrum et al. (2004)) for computing permanents to obtain a feasible marginal oracle. Note that the problem of counting the number of perfect matchings in a bipartite graph is #P-complete as it is equivalent to computing the permanent of a 0/1 matrix (Valiant (1979)). The problem of approximately counting the number of perfect matchings in a general graph is however a long standing open problem, if solved, it would result in another way of solving MSP games on the matching polytope. Another example of a polytope that admits a polynomial approximate counting oracle is the cycle cover polytope (or 0 \u2212 1 circulations) over directed graphs (Singh and Vishnoi (2014)). Also, we would like to note that to compute Nash-equilibria for MSP games that admit marginal oracles for both the polytopes, the optimistic mirror descent algorithm is simply the exponential weights with a modified loss vector (Rakhlin and Sridharan (2013)), and hence the same framework applies.\nSampling pure strategies: In online learning scenarios that require the learner to play a combinatorial concept (i.e., a pure strategy in each round), we note first that given any mixed strategy (that lies in a strategy polytope \u2208 Rn), the learner can obtain a convex decomposition of the mixed strategy into at most n + 1 vertices by using the well-known Caratheodory\u2019s Theorem. The learner can then play a pure strategy sampled proportional to the convex coefficients in the decomposition. In the case of learning over the spanning tree and bipartite perfect matching polytopes using product distributions however, there exists a more efficient way of sampling due to the self-reducibility6 of the these polytopes (Kulkarni (1990), Asadpour et al. (2010)): Order the edges of the graph randomly and decide for each probabilistically whether to use it in the final object or not. The probabilities of each edge are updated after every iteration conditioned on the decisions (i.e., to include or not) made on the earlier edges. This sampling procedure works as long as there exists a polynomial time marginal oracle (i.e., a generalized counting oracle) to update the probabilities of the elements of the ground set after each iteration and if the polytope is self-reducible (Sinclair and Jerrum (1989)). Intuitively, self-reducibility means that there exists an inductive construction of the combinatorial object from a smaller instance of the same problem. For example, conditioned on whether an edge is taken or not, the problem of finding a spanning tree (or a matching) on a given graph reduces to the problem of finding a spanning tree (or a matching) in a modified graph.\n6. Intuitively, self-reducibility means that there exists an inductive construction of the combinatorial object from a smaller instance of the same problem. For example, conditioned on whether an edge is taken or not, the problem of finding a spanning tree (or a matching) on a given graph reduces to the problem of finding a spanning tree (or a matching) in a modified graph."}, {"heading": "6. Symmetric Nash-equilibria", "text": "In this section we explore purely combinatorial algorithms to find Nash-equilibria, without using learning algorithms. Symmetric Nash-equilibria are a set of optimal strategies such that both players play the exact same mixed strategy at equilibrium. We assume here that the strategy polytopes of the two players are the same. In this section, we give necessary and sufficient conditions for a symmetric Nash-equilibrium to exist in case of matroid MSP games. More precisely, our main result is the following:\nTheorem 9 Consider an MSP game with respect to a matroid M = (E, I) with an associated rank function r : E \u2192 R+. Let L be the loss matrix for the row player such that it is symmetric, i.e. LT = L. Let x \u2208 B(M) = {x \u2208 RE : x(S) \u2264 r(S) \u2200 S \u2282 E, x(E) = r(E), x \u2265 0}. Suppose x partitions the elements of the ground set into {P1, P2, . . . Pk} such that (Lx)(e) = ci \u2200e \u2208 Pi and c1 < c2 . . . < ck. Then, the following are equivalent.\n(i). (x, x) is a symmetric Nash-equilibrium,\n(ii). All bases of matroid M have the same cost with respect to weights Lx,\n(iii). For all bases B of M , |B \u2229 Pi| = r(Pi),\n(iv). x(Pi) = r(Pi) for all i \u2208 {1, . . . , k},\n(v). For all circuits C of M , \u2203i : C \u2286 Pi.\nProof Case (i)\u21d4 (ii). Assume first that (x, x) is a symmetric Nash-equilibrium. Then, the value of the game is maxz\u2208B(M) xTLz = minz\u2208B(M) zTLx = minz\u2208B(M) xTLT z (1) = minz\u2208B(M) x\nTLz, where (1) follows from LT = L. This implies that every base of the matroid has the same cost under the weights Lx. Coversely, if every base has the same cost with respect to weights Lx, then x \u2208 arg maxy\u2208B(M) xTLy and x \u2208 arg miny\u2208B(M) xTLy. Since no player has an incentive to deviate, this implies that (x, x) is a Nashequilibrium.\nCase (ii) \u21d4 (iii). Assume (ii) holds. Suppose there exists a base B such that |B \u2229 Pi| < r(Pi) for some i. We know that there exists a base BT such that |BT \u2229 Pi| = r(Pi). Since B \u2229 Pi, BT \u2229 Pi \u2208 I and |BT \u2229 Pi| > |B \u2229 Pi|, \u2203e \u2208 (BT \\B) \u2229 Pi such that (B \u2229 Pi) + e \u2208 I. Since (B \u2229 Pi) + e \u2208 I and B is a base, \u2203f \u2208 B \\ Pi such that B + e \u2212 f \u2208 I. This gives a base of a different cost as e and f are in different members of the partition. Hence, we reach a contradiction. Thus, (ii) implies (iii). Conversely, assume (iii) holds. Note that the cost of a base B is c(B) = \u2211k i=1 ci|Pi \u2229 B|. Thus, (iii) implies\nthat every base has the same cost \u2211k\ni=1 cir(Pi).\nCase (iii) \u21d4 (iv). Assume (iii) holds. Since x \u2208 B(M), x is a convex combination of the bases of the matroid, i.e. x = \u2211 B lB\u03c7(B) where \u03c7(B) denotes the characteristic vector for the base B. Thus, (iii) im-\nplies that x(Pi) = \u2211 B \u03bbB|B \u2229 Pi| = \u2211\nB \u03bbBr(Pi) = r(Pi) for all i \u2208 {1, . . . , k}. Conversely assume (iv) and consider any base B of the matroid. Then, r(E) = |B| = \u2211k i=1 |B\u2229Pi|\u2264(1) \u2211k i=1 r(Pi)\n(2) = \u2211k\ni=1 x(Pi) = x(E) = r(E), where (1) follows from rank inequality and (2) follows from (iv) for each Pi. Thus, equality holds in (1) and we get that for each base B, |B \u2229 Pi| = r(Pi).\nCase (iii) \u21d4 (v). Assume (iii) and let C be a circuit. Let e \u2208 C and B be a base that contains C \u2212 e.\nHence, the unique circuit in B+e is C. Thus, for any element f \u2208 C\u2212e, B\u2212e+f \u2208 I. Hence, (iii) implies that all the elements of C \u2212 e must lie in the same member of the partition as e does. Hence, \u2203i : C \u2286 Pi. Conversely, assume (v). Consider any two bases B and BT such that B \\BT = {e} and BT \u2212B = {f} for some e, f \u2208 E. Let C be the unique circuit in BT + e and hence f \u2208 C. It follows from (v) that e, f are in the same member of the partition, and hence |B \u2229 Pi| = |BT \u2229 Pi| for all i \u2208 {1, . . . , k}. Since we know there exists a base Bi such that |Bi \u2229 Pi| = r(Pi) for each i, hence all bases must have the same intersection with each Pi and (iii) follows.\nCorollary 10 Consider a game where each player plays a base of the graphic matroid M(G) on a graph G, and the loss matrix of the row player is the identity matrix I \u2208 RE\u00d7E. Then there exists a symmetric Nash-equilibrium if and only if every block of G is uniformly dense.\nProof Since the loss matrix is the identity matrix, x(e) = ci for all e \u2208 Pi. Theorem 9 (v) implies that each Pi is a union of blocks of the graph. Further, as x(Pi) = r(Pi) = ci|Pi|, each Pi (and hence each block contained in Pi) is uniformly dense.\nCorollary 11 Given any point x \u2208 RE, x > 0 in the base polytope of a matroid M = (E, I), one can construct a matroid game for which (x, x) is the symmetric Nash equilibrium.\nProof Let the loss matrix L be defined as Le,e = 1/xe for e \u2208 E and 0 otherwise. Then, Lx(e) = 1 for all e \u2208 E. Thus, all the bases have the same cost under Lx. It follows from Theorem 9 that (x, x) is a symmetric Nash equilibrium of this game.\nUniqueness: Consider a symmetric loss matrix L such that Le,f = 1 for all e, f \u2208 E. Note that any feasible point in the base polytope B(M) forms a symmetric Nash equilibrium. Hence, we need a stronger condition for the symmetric Nash equilibria to be unique. We note that for positive and negative-definite loss matrices, symmetric Nash-equilibria are unique, if they exist (proof in the Appendix).\nTheorem 12 Consider the game with respect to a matroid M = (E, I) with an associated rank function r : E \u2192 R+. Let L be the loss matrix for the row player such that it is positive-definite, i.e. xTLx > 0 for all x 6= 0. Then, if there exists a symmetric Nash equilibrium of the game, it is unique.\nProof Suppose (x, x) and (y, y) are two symmetric Nash equilibria such that x 6= y, then the value of the game is xTLx = yTLy. Then, xTLz \u2264 xTLx \u2264 zTLx \u2200z \u2208 B(M) implying that xTLz \u2264 xTLx \u2264 xTLT z \u2200z \u2208 B(M). Since L is symmetric, we get xTLx = xTLz \u2200z \u2208 B(M). Similarly, we get yTLy = yTLz \u2200z \u2208 B(M). Consider z = x+y2 . Then, z TLz = (x+y)2 T Lz = x2 TLz + y2 TLz = x2 TLx + y2 TLy. This contradicts the strict convexity of the quadratic form of xTLx. Hence, x = y and there exists a unique symmetric Nash equilibrium.\nLexicographic optimality: We further note that symmetric Nash-equilibria are closely related to the concept of being lexicographically optimal as studied in Fujishige (1980). For a matroid M = (E, I), x \u2208 B(M) is called lexicographically optimal with respect to a positive weight vector w if the |E|-tuple of numbers x(e)/w(e) (e \u2208 E) arranged in the order of increasing magnitude is lexicographically maximum among all\n|E|-tuples of numbers y(e)/w(e) (e \u2208 E) arranged in the same manner for all y \u2208 B(M). We evoke the following theorem from Fujishige (1980).\nTheorem 13 Let x \u2208 B(M) and w be a positive weight vector. Define c(e) = x(e)/w(e) (e \u2208 E) and let the distinct numbers of c(e) (e \u2208 E) be given by c1 < c2 < . . . < cp. Futhermore, define Si \u2286 E (i = 1, 2, . . . , p) by\nSi = {e|e \u2208 E, c(e) \u2264 ci} (i = 1, 2, . . . , p).\nThen the following are equivalent:\n(i) x is the unique lexicographically optimal point in B(M) with respect to the weight vector w;\n(ii) x(Si) = r(Si) (i = 1, 2, . . . , p).\nThe following corollary gives an algorithm for computing symmetric Nash-equilibria for matroid MSP games.\nCorollary 14 Consider a matroid game with a diagonal loss matrix L such that Le,e > 0 for all e \u2208 E. If there exists a symmetric Nash-equilibrium for this game, then it is the unique lexicographically optimal point in B(M) with respect to the weights 1/Le,e (e \u2208 E).\nThe proof follows from observing the partition of edges with respect to the weight vector Lx, and proving that symmetric Nash-equilibria satisfy the sufficient conditions for being a lexicographically optimal base. Lexicographically optimal bases can be computed efficiently using Fujishige (1980), Nagano (2007a), or the INC-FIX algorithm from Section 4. Hence, one could compute the lexicographically optimal base x for a weight vector defined as w(e) = 1/Le,e (e \u2208 E) for a positive diagonal loss matrix L, and check if that is a symmetric Nash-equilibrium. If it is, then it is also the unique symmetric Nash-equilibrium and if it is not then there cannot be any other symmetric Nash-equilibrium."}, {"heading": "Appendix A. Mirror Descent", "text": "Lemma 15 The unnormalized entropy map, \u03c9(x) = \u2211n i=1 xi lnxi \u2212 \u2211n\ni=1 xi, is 1-strongly convex with respect to the L1-norm over any matroid base polytope B(f) = {x \u2208 RE : x(E) = r(E), x(E(S)) \u2264 r(S)\u2200S \u2286 E, x \u2265 0}.\nProof We have \u03c9(x)\u2212 \u03c9(y)\u2212\u2207\u03c9(y)T (x\u2212 y) = \u2211 e\u2208E xe lnxe \u2212 \u2211 e\u2208E xe \u2212 \u2211 e\u2208E ye ln ye + \u2211 e\u2208E ye \u2212 \u2211 e\u2208E ln ye(xe \u2212 ye) (5)\n= \u2211 e\u2208E xe ln(xe/ye) \u2265(1) 1 2 ( \u2211 e\u2208E |xe \u2212 ye|)2 = 1 2 ||x\u2212 y||21, (6)\nwhere (1) follows from Pinsker\u2019s inequality."}, {"heading": "Appendix B. Multiplicative Weights Update Algorithm", "text": "Lemma 16 Consider an MSP game with strategy polytopes P \u2286 Rm and Q \u2286 Rn, and let the loss function for the row player be given by loss(x, y) = xTLy for x \u2208 P, y \u2208 Q. Suppose we simulate an online algorithm A such that in each round t the row player chooses decisions from x(t) \u2208 P , the column player reveals an adversarial loss vector v(t) such that x(t)TLv(t) \u2265 maxy\u2208Q x(t)TLy \u2212 \u03b4 and the row player subsequently incurs loss x(t)TLv(t) for round t. If the regret of the learner after T rounds goes down as f(T ), that is,\nRT (A) = T\u2211 i=1 x(i)TLv(i) \u2212min x\u2208P t\u2211 i=1 xTLv(i) \u2264 f(T ) (7)\nthen ( 1T \u2211T i=1 x (i), 1T \u2211T i=1 v (i)) is an O(f(T )T + \u03b4)-approximate Nash-equilibrium for the game.\nProof Let x\u0304 = 1T \u2211T i=1 x (i) and v\u0304 = 1T \u2211T i=1 v\n(i). By the von Neumann minimax theorem, we know that the value of the game is \u03bb\u2217 = minx maxy xTLy = maxy minx xTLy. This gives,\nmin x max y xTLy = \u03bb\u2217 \u2264 max y x\u0304TLy = max y\n1\nT T\u2211 i=1 x(i)Ly \u2264 1 T T\u2211 i=1 max y x(i)TLy (8)\n\u2264 1 T ( T\u2211 i=1 x(i)TLv(i) + \u03b4) (9)\n\u2264 min x\u2208P\n1\nT T\u2211 i=1 xTLv(i) + f(T ) T + \u03b4 (10)\n= min x\u2208P\nxTL 1\nT T\u2211 i=1 v(i) + f(T ) T + \u03b4 = min x\u2208P xTLv\u0304 + f(T ) T + \u03b4\n\u2264 max y\u2208Q min x\u2208P\nxTLy + f(T )\nT + \u03b4 = \u03bb\u2217 +\nf(T )\nT + \u03b4.\nwhere the last inequality in (8) follows from the convexity maxy xTLy in x, (9) follows from the error in the adversarial loss vector, and (10) follows from the given regret equation (7). Thus, we get x\u0304TLv\u0304 \u2264 maxy\u2208Q x\u0304 TLy \u2264 \u03bb\u2217+ f(T )T +\u03b4, and x\u0304 TLv\u0304 \u2265 minx\u2208P xTLv\u0304 \u2265 \u03bb\u2217\u2212 f(T )T \u2212\u03b4. Hence, (x\u0304, v\u0304) is a (2f(T ) T +2\u03b4 ) - approximate Nash-equilibrium for the game.\nProof for Theorem 7. Proof We want to show that the updates to the weights of each pure strategy u \u2208 U can be done efficiently. For the multipliers \u03bb(t) in each round, let w(t)(u) be the unnormalized probability for each vertex u, i.e., w(t)(u) = \u220f e\u2208E \u03bb\n(t)(e)u(e) where E is the ground set of elements such that the strategy polytope P of the row player (learner in the MWU algorithm) lies in R|E|. Recall that the set of vertices of P is denoted by U . Let Z(t) be the normalization constant for round t, i.e., Z(t) = \u2211 u\u2208U w\n(t)(u). Thus, the probability of each vertex u is p(t)(u) = w(t)(u)/Z(t). Let F = maxx\u2208P,y\u2208Q xTLy. For readability, we denote the scaled loss xTLy/F as \u03b7(x, y); \u03b7(x, y) \u2208 [0, 1].\nThe algorithm starts with \u03bb(1)(e) = 1 for all e \u2208 E and thus w(1)(u) = 1 for all u \u2208 U . We claim that w(t+1)(u) = w(t)(u)\u03b2u TLv(t)/F , where v(t) is revealed to be a vector in arg maxy\u2208Q x(t)Ly in each round t.\nw(t+1)(u) = \u220f e\u2208E \u03bb(t+1)(e)u(e) = \u220f e\u2208E \u03bb(t)(e)u(e)\u03b2(Lv (t)/F )e\n= \u03b2(u TLv(t)/F ) \u220f e\u2208E \u03bb(t)(e)u(e) . . . u \u2208 {0, 1}m. = w(t)(u)\u03b2u TLv(t)/F = w(t)(u)\u03b2\u03b7(u,v (t)).\nHaving shown that the multiplicative update can be performed efficiently, the rest of the proof follows as standard proofs the MWU algorithm. We prove that Z(t+1) \u2264 Z(1) exp(\u2212(1\u2212 \u03b2) \u2211t i=1 \u03b7(x (i), v(i))).\nZ(t+1) = \u2211 u\u2208U w(t+1)(u) = \u2211 u\u2208U w(t)(u)\u03b2\u03b7(u,v (t))\n\u2264 \u2211 u\u2208U w(t)(u)(1\u2212 (1\u2212 \u03b2)\u03b7(u, v(t))) . . . (1\u2212 \u03b8)x \u2264 1\u2212 \u03b8x for x \u2208 [0, 1], \u03b8 \u2208 [\u22121, 1].\n= Z(t)(1\u2212 (1\u2212 \u03b2)\u03b7(x(t), v(t))) \u2264 Z(t) exp(\u2212(1\u2212 \u03b2)\u03b7(x(t), v(t))) . . . (1\u2212 \u03b8x) \u2264 e\u2212x\u03b8 \u2200 x, \u2200 \u03b8.\nRolling out the above till the first round, we get\nZ(t+1) \u2264 Z(1) exp(\u2212(1\u2212 \u03b2) t\u2211 i=1 \u03b7(x(i), v(i))). (11)\nSince w(t+1)(u) \u2264 Z(t+1) for all u \u2208 U , using (11), we get\nlnw(1)(u) + ln\u03b2 t\u2211 i=1 \u03b7(u, v(i)) \u2264 lnZ(1) \u2212 (1\u2212 \u03b2) t\u2211 i=1 \u03b7(x(i), v(i))\n\u21d2 t\u2211 i=1 \u03b7(x(i), v(i)) \u2264 \u2212 ln\u03b2 (1\u2212 \u03b2) t\u2211 i=1 \u03b7(u, v(i)) + lnZ(1) \u2212 lnw(1)(u) 1\u2212 \u03b2 . . . \u03b2 < 1\n\u21d2 t\u2211 i=1 \u03b7(x(i), v(i)) \u2264 1 + \u03b2 2\u03b2 t\u2211 i=1 \u03b7(u, v(i)) + lnZ(1) \u2212 lnw(1)(u) 1\u2212 \u03b2\n. . . \u2212 lnx 1\u2212 x \u2264 1 + x 2x for x \u2208 (0, 1]\n\u21d2 1 t t\u2211 i=1 \u03b7(x(i), v(i)) \u2264 1 + \u03b2 2\u03b2t t\u2211 i=1 \u03b7(u, v(i)) + lnZ(1) \u2212 lnw(1)(u) (1\u2212 \u03b2)t\nConsider an arbitrary mixed strategy for the row player p such that \u2211\nu\u2208U p(u)u = x, and multiply the above equation for each vertex u with the p(u) and sum them up -\n1\nt t\u2211 i=1 \u03b7(x(i), v(i)) \u2264 1 + \u03b2 2\u03b2t t\u2211 i=1 \u03b7(x, v(i)) + lnZ1 \u2212 lnw(1)(u) (1\u2212 \u03b2)t\nNote that Z1 = |U|, w(1)(u) = 1. Setting \u2032 = /F , \u03b2 = 11+\u221a2 \u2032 , t = ln |U| \u20322 = F 2 ln(|U|) 2 we get\n1\nt t\u2211 i=1 \u03b7(x(i), v(i)) \u2264 2 + \u221a 2 \u2032 2t t\u2211 i=1 \u03b7(x, v(i)) + \u20322 ln(|U|)(1 + \u221a 2 \u2032)\u221a 2 \u2032 ln(|U|) (12)\n\u21d2 1 t t\u2211 i=1 \u03b7(x(i), v(i)) \u2264 1 t t\u2211 i=1 \u03b7(x, v(i)) + \u221a 2 \u2032 + \u20322 . . . using t\u2211 i=1 \u03b7(x, v(i)) \u2264 t\n(13)\n\u21d2 1 t t\u2211 i=1 x(i)TLv(i) \u2264 1 t t\u2211 i=1 xTLv(i) +O( ) (14)\nThus, we get that the MWU algorithm converges to an -approximate Nash-equilibrium inO(F 2 ln(|U|)/ 2) rounds.\nProof for Lemma 8. Proof Let the multipliers in each round t be \u03bb(t), the corresponding true and approximate marginal points in round t be x(t) and x\u0303(t) respectively, such that ||x(t)\u2212x\u0303(t)||\u221e \u2264 1. Now, we can only compute approximately adversarial loss vectors, v\u0303(t) in each round such that x\u0303(t)Lv\u0303(t) \u2265 maxy\u2208Q x\u0303(t)Ly \u2212 2.\nEven though we cannot compute x(t) exactly, we do maintain the corresponding \u03bb(t)s that correspond to these true marginals. Let us analyze first the multiplicative updates corresponding to approximate loss vectors v\u0303(t) using product distributions (that can be done efficiently) over the true marginal points. Using the proof for Theorem 7, we get the following regret bound with respect to the true marginals corresponding to (14) for t = F 2 ln(|U|) 2 rounds:\n1\nt t\u2211 i=1 x(i)TLv\u0303(i) \u2264 1 t t\u2211 i=1 xTLv\u0303(i) +O( ) (15)\nWe do not have the value for x(i) for i = 1, . . . , t, but only estimates x\u0303(i) for i = 1, . . . , t such that ||x\u0303(i) \u2212 x(i)||\u221e \u2264 1. Since the losses we consider are bilinear, we can bound the loss of the estimated point in each iteration i as follows:\n|x\u0303(i)TLv\u0303(i) \u2212 x(i)TLv\u0303(i)| \u2264 1eLv\u0303(i) \u2264 F 1, (16)\nwhere e = (1, . . . , 1)T . Thus using (15), we get\n1\nt t\u2211 i=1 x\u0303(i)TLv\u0303(i) \u2264 1 t t\u2211 i=1 xTLv\u0303(i) +O( + F 1). (17)\nNow, considering that we played points x\u0303(i) for each round i, and suffered losses v\u0303(i), we have shown that the MWU algorithm achieves O( + F 1) regret on an average. Thus, as R 2 is assumed to have error 2, using Lemma 16 we have that (1t \u2211t i=1 x\u0303 (i), 1t \u2211t i=1 v\u0303 (i)) is an O( + F 1 + 2)-approximate Nash-equilibrium."}], "references": [{"title": "Improved Bounds for Online Learning Over the Permutahedron and Other Ranking Polytopes", "author": ["N. Ailon"], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ailon.,? \\Q2014\\E", "shortCiteRegEx": "Ailon.", "year": 2014}, {"title": "On sparse approximations to randomized strategies and convex combinations", "author": ["I. Alth\u00f6fer"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Alth\u00f6fer.,? \\Q1994\\E", "shortCiteRegEx": "Alth\u00f6fer.", "year": 1994}, {"title": "The Multiplicative Weights Update Method: a Meta-Algorithm and Applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "An O (log n/log log n)approximation Algorithm for the Asymmetric Traveling Salesman Problem", "author": ["A. Asadpour", "M.X. Goemans", "A. Madry", "S. Oveis Gharan", "A. Saberi"], "venue": null, "citeRegEx": "Asadpour et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Asadpour et al\\.", "year": 2010}, {"title": "Regret in online combinatorial optimization", "author": ["J. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "On the problem of approximating the number of bases of a matriod", "author": ["Y. Azar", "A.Z. Broder", "A.M. Frieze"], "venue": "Information processing letters,", "citeRegEx": "Azar et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Azar et al\\.", "year": 1994}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "Beck and Teboulle.,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2003}, {"title": "Lectures on modern convex optimization: analysis, algorithms, and engineering applications, volume", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": null, "citeRegEx": "Ben.Tal and Nemirovski.,? \\Q2001\\E", "shortCiteRegEx": "Ben.Tal and Nemirovski.", "year": 2001}, {"title": "Regret minimization and the price of total anarchy", "author": ["A. Blum", "M.T. Hajiaghayi", "K. Ligett", "A. Roth"], "venue": "Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "Blum et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2008}, {"title": "Introduction to online optimization", "author": ["S. Bubeck"], "venue": "Lecture Notes,", "citeRegEx": "Bubeck.,? \\Q2011\\E", "shortCiteRegEx": "Bubeck.", "year": 2011}, {"title": "Theory of Convex Optimization for Machine Learning", "author": ["S. Bubeck"], "venue": "arXiv preprint arXiv:1405.4980,", "citeRegEx": "Bubeck.,? \\Q2014\\E", "shortCiteRegEx": "Bubeck.", "year": 2014}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Design is as easy as optimization", "author": ["D. Chakrabarty", "A. Mehta", "V.V. Vazirani"], "venue": "In Automata, Languages and Programming,", "citeRegEx": "Chakrabarty et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chakrabarty et al\\.", "year": 2006}, {"title": "Settling the complexity of computing two-player Nash equilibria", "author": ["Xi Chen", "Xiaotie Deng", "Shang-Hua Teng"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Following the perturbed leader for online structured learning", "author": ["A. Cohen", "T. Hazan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Cohen and Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Cohen and Hazan.", "year": 2015}, {"title": "Minimum cuts, modular functions, and matroid", "author": ["W.H. Cunningham"], "venue": "polyhedra. Networks,", "citeRegEx": "Cunningham.,? \\Q1985\\E", "shortCiteRegEx": "Cunningham.", "year": 1985}, {"title": "The complexity of computing a Nash equilibrium", "author": ["C. Daskalakis", "P.W. Goldberg", "C.H. Papadimitriou"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Daskalakis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2009}, {"title": "Submodular functions, matroids, and certain polyhedra", "author": ["J. Edmonds"], "venue": "Combinatorial structures and their applications,", "citeRegEx": "Edmonds.,? \\Q1970\\E", "shortCiteRegEx": "Edmonds.", "year": 1970}, {"title": "Matroids and the greedy algorithm", "author": ["J. Edmonds"], "venue": "Mathematical programming,", "citeRegEx": "Edmonds.,? \\Q1971\\E", "shortCiteRegEx": "Edmonds.", "year": 1971}, {"title": "Approximating Nash equilibria using small-support strategies", "author": ["T. Feder", "H. Nazerzadeh", "A. Saberi"], "venue": "Electronic Commerce,", "citeRegEx": "Feder et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Feder et al\\.", "year": 2007}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Lexicographically optimal base of a polymatroid with respect to a weight vector", "author": ["S. Fujishige"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Fujishige.,? \\Q1980\\E", "shortCiteRegEx": "Fujishige.", "year": 1980}, {"title": "Submodular functions and optimization, volume 58", "author": ["S. Fujishige"], "venue": null, "citeRegEx": "Fujishige.,? \\Q2005\\E", "shortCiteRegEx": "Fujishige.", "year": 2005}, {"title": "Two algorithms for maximizing a separable concave function over a polymatroid feasible region", "author": ["H. Groenevelt"], "venue": "European journal of operational research,", "citeRegEx": "Groenevelt.,? \\Q1991\\E", "shortCiteRegEx": "Groenevelt.", "year": 1991}, {"title": "The ellipsoid method and its consequences in combinatorial optimization", "author": ["M. Gr\u00f6tschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": null, "citeRegEx": "Gr\u00f6tschel et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Gr\u00f6tschel et al\\.", "year": 1981}, {"title": "The computational power of optimization in online learning", "author": ["E. Hazan", "T. Koren"], "venue": "arXiv preprint arXiv:1504.02089,", "citeRegEx": "Hazan and Koren.,? \\Q2015\\E", "shortCiteRegEx": "Hazan and Koren.", "year": 2015}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["D.P. Helmbold", "R.E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Helmbold and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Helmbold and Schapire.", "year": 1997}, {"title": "Learning permutations with exponential weights", "author": ["D.P. Helmbold", "M.K. Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Helmbold and Warmuth.,? \\Q2009\\E", "shortCiteRegEx": "Helmbold and Warmuth.", "year": 2009}, {"title": "A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries", "author": ["M. Jerrum", "A. Sinclair", "E. Vigoda"], "venue": "ACM Symposium of Theory of Computing,", "citeRegEx": "Jerrum et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 2004}, {"title": "Random generation of combinatorial structures from a uniform distribution", "author": ["M.R. Jerrum", "L.G. Valiant", "V.V. Vazirani"], "venue": "Theoretical Computer Science,", "citeRegEx": "Jerrum et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 1986}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Structured prediction models via the matrix-tree theorem", "author": ["T. Koo", "A. Globerson", "X.C. P\u00e9rez", "M. Collins"], "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Koo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2007}, {"title": "Second-order quantile methods for experts and combinatorial games", "author": ["W.M. Koolen", "T. Van Erven"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "Koolen and Erven.,? \\Q2015\\E", "shortCiteRegEx": "Koolen and Erven.", "year": 2015}, {"title": "Approaching optimality for solving SDD linear systems", "author": ["I. Koutis", "G.L. Miller", "R. Peng"], "venue": "Proceedings - Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Koutis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koutis et al\\.", "year": 2010}, {"title": "Generating random combinatorial objects", "author": ["V.G. Kulkarni"], "venue": "Journal of Algorithms,", "citeRegEx": "Kulkarni.,? \\Q1990\\E", "shortCiteRegEx": "Kulkarni.", "year": 1990}, {"title": "Simple strategies for large zero-sum games with applications to complexity theory", "author": ["R.J. Lipton", "N.E. Young"], "venue": "Proceedings of the twenty-sixth annual ACM symposium on Theory of computing.,", "citeRegEx": "Lipton and Young.,? \\Q1994\\E", "shortCiteRegEx": "Lipton and Young.", "year": 1994}, {"title": "Playing large games using simple strategies", "author": ["R.J. Lipton", "E. Markakis", "A. Mehta"], "venue": "Proceedings of the 4th ACM conference on Electronic commerce,", "citeRegEx": "Lipton et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2003}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Geometric algorithms and combinatorial optimization", "author": ["L. Lov\u00e1sz", "M. Gr\u00f6tschel", "A. Schrijver"], "venue": "Berlin: Springer-Verlag,", "citeRegEx": "Lov\u00e1sz et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Lov\u00e1sz et al\\.", "year": 1988}, {"title": "Using separation algorithms to generate mixed integer model reformulations", "author": ["R.K. Martin"], "venue": "Operations Research Letters,", "citeRegEx": "Martin.,? \\Q1991\\E", "shortCiteRegEx": "Martin.", "year": 1991}, {"title": "On convex minimization over base polytopes", "author": ["K. Nagano"], "venue": "Integer Programming and Combinatorial Optimization,", "citeRegEx": "Nagano.,? \\Q2007\\E", "shortCiteRegEx": "Nagano.", "year": 2007}, {"title": "A strongly polynomial algorithm for line search in submodular polyhedra", "author": ["K. Nagano"], "venue": "Discrete Optimization,", "citeRegEx": "Nagano.,? \\Q2007\\E", "shortCiteRegEx": "Nagano.", "year": 2007}, {"title": "A faster parametric submodular function minimization algorithm and applications", "author": ["K. Nagano"], "venue": null, "citeRegEx": "Nagano.,? \\Q2007\\E", "shortCiteRegEx": "Nagano.", "year": 2007}, {"title": "Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski.,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski.", "year": 2004}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A.S. Nemirovski", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits", "author": ["G. Neu", "G. Bart\u00f3k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Neu and Bart\u00f3k.,? \\Q2015\\E", "shortCiteRegEx": "Neu and Bart\u00f3k.", "year": 2015}, {"title": "Max flows in o(nm) time, or better", "author": ["J.B. Orlin"], "venue": "In Proceedings of the forty-fifth annual ACM Symposium on Theory of Computing,", "citeRegEx": "Orlin.,? \\Q2013\\E", "shortCiteRegEx": "Orlin.", "year": 2013}, {"title": "Computing correlated equilibria in multi-player games", "author": ["C.H. Papadimitriou", "T. Roughgarden"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Papadimitriou and Roughgarden.,? \\Q2008\\E", "shortCiteRegEx": "Papadimitriou and Roughgarden.", "year": 2008}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "The matching polytope has exponential extension complexity", "author": ["T. Rothvo\u00df"], "venue": "ACM Symposium of Theory of Computing,", "citeRegEx": "Rothvo\u00df.,? \\Q2014\\E", "shortCiteRegEx": "Rothvo\u00df.", "year": 2014}, {"title": "Optimal algorithms for self-reducible problems", "author": ["C. Schnorr"], "venue": "In ICALP,", "citeRegEx": "Schnorr.,? \\Q1976\\E", "shortCiteRegEx": "Schnorr.", "year": 1976}, {"title": "Combinatorial optimization: polyhedra and efficiency, volume 24", "author": ["A. Schrijver"], "venue": "Springer Science & Business Media,", "citeRegEx": "Schrijver.,? \\Q2003\\E", "shortCiteRegEx": "Schrijver.", "year": 2003}, {"title": "Approximate counting, uniform generation and rapidly mixing markov chains", "author": ["A. Sinclair", "M. Jerrum"], "venue": "Information and Computation,", "citeRegEx": "Sinclair and Jerrum.,? \\Q1989\\E", "shortCiteRegEx": "Sinclair and Jerrum.", "year": 1989}, {"title": "Entropy, optimization and counting", "author": ["M. Singh", "N.K. Vishnoi"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Singh and Vishnoi.,? \\Q2014\\E", "shortCiteRegEx": "Singh and Vishnoi.", "year": 2014}, {"title": "On the universality of online mirror descent", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2011}, {"title": "Online prediction under submodular constraints", "author": ["D. Suehiro", "K. Hatano", "S. Kijima"], "venue": "Algorithmic Learning Theory, pages 260\u2013274,", "citeRegEx": "Suehiro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Suehiro et al\\.", "year": 2012}, {"title": "Path kernels and multiplicative updates", "author": ["E. Takimoto", "M.K. Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth.", "year": 2003}, {"title": "The complexity of computing the permanent", "author": ["L.G. Valiant"], "venue": "Theoretical computer science,", "citeRegEx": "Valiant.,? \\Q1979\\E", "shortCiteRegEx": "Valiant.", "year": 1979}, {"title": "Zur theorie der gesellschaftsspiele", "author": ["J. von Neumann"], "venue": "Mathematische Annalen,", "citeRegEx": "Neumann.,? \\Q1928\\E", "shortCiteRegEx": "Neumann.", "year": 1928}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["M.K. Warmuth", "D. Kuzmin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Warmuth and Kuzmin.,? \\Q2008\\E", "shortCiteRegEx": "Warmuth and Kuzmin.", "year": 2008}, {"title": "Two person zero-sum games for network interdiction", "author": ["A. Washburn", "K. Wood"], "venue": "Operations Research,", "citeRegEx": "Washburn and Wood.,? \\Q1995\\E", "shortCiteRegEx": "Washburn and Wood.", "year": 1995}, {"title": "Some problems on approximate counting in graphs and matroids", "author": ["D. Welsh"], "venue": "In Research Trends in Combinatorial Optimization,", "citeRegEx": "Welsh.,? \\Q2009\\E", "shortCiteRegEx": "Welsh.", "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": null, "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 46, "context": "These are succinct games, as discussed in the paper of Papadimitriou and Roughgarden (2008) on correlated equilibria.", "startOffset": 55, "endOffset": 92}, {"referenceID": 46, "context": "These are succinct games, as discussed in the paper of Papadimitriou and Roughgarden (2008) on correlated equilibria. For example, in a spanning tree game in which all the results of this paper apply, pure strategies correspond to spanning trees T1 and T2 selected by the two players in a graph G (or two distinct graphs G1 and G2) and the payoff \u2211 e\u2208T1,f\u2208T2 Lef is a bilinear function; this allows for example to model classic network interdiction games (see for e.g., Washburn and Wood (1995)), design problems (Chakrabarty et al.", "startOffset": 55, "endOffset": 495}, {"referenceID": 12, "context": ", Washburn and Wood (1995)), design problems (Chakrabarty et al. (2006)), and the interaction between algorithms for many problems such as ranking and compression as bilinear duels (Immorlica et al.", "startOffset": 46, "endOffset": 72}, {"referenceID": 12, "context": ", Washburn and Wood (1995)), design problems (Chakrabarty et al. (2006)), and the interaction between algorithms for many problems such as ranking and compression as bilinear duels (Immorlica et al. (2011)).", "startOffset": 46, "endOffset": 206}, {"referenceID": 52, "context": "Nash equilibria for two-player zero-sum games can be characterized and found by solving a linear program (von Neumann (1928)).", "startOffset": 110, "endOffset": 125}, {"referenceID": 42, "context": "However, for succinct games in which the strategies of both players are exponential in a natural description of the game, the corresponding linear program has exponentially many variables and constraints, and as Papadimitriou and Roughgarden (2008) point out in their open questions section, \u201cthere are no standard techniques for linear programs that have both dimensions exponential.", "startOffset": 212, "endOffset": 249}, {"referenceID": 20, "context": "\u201d Under bilinear losses/payoffs however, the von Neumann linear program can be reformulated in terms of the strategy polytopes P and Q, and this reformulation can be solved using the equivalence between optimization and separation and the ellipsoid algorithm (Gr\u00f6tschel et al. (1981)) (discussed in more detail in Section 3).", "startOffset": 260, "endOffset": 284}, {"referenceID": 15, "context": "In the case of the spanning tree game mentioned above, the strategy polytope of each player is simply the spanning tree polytope characterized by Edmonds (1971). Note that Immorlica et al.", "startOffset": 146, "endOffset": 161}, {"referenceID": 15, "context": "In the case of the spanning tree game mentioned above, the strategy polytope of each player is simply the spanning tree polytope characterized by Edmonds (1971). Note that Immorlica et al. (2011) give such a reformulation for bilinear games involving strategy polytopes with compact formulations only (i.", "startOffset": 146, "endOffset": 196}, {"referenceID": 10, "context": "As is well-known, if one of the players uses a no-regret learning algorithm and adapts his/her strategies according to the losses incurred so far (with respect to the most adversarial opponent strategy) then the average of the strategies played by the players in the process constitutes an approximate equilibrium (Cesa-Bianchi and Lugosi (2006)).", "startOffset": 315, "endOffset": 346}, {"referenceID": 4, "context": ", Audibert et al. (2013)), where the learner is required to play a pure strategy ut \u2208 U (where U is the vertex set of P ) possibly randomized according to a mixed strategy xt \u2208 P , and aims to minimize the loss in expectation, i.", "startOffset": 2, "endOffset": 25}, {"referenceID": 54, "context": "Online Mirror Descent Even though the online mirror descent algorithm is near-optimal in terms of regret for most of online learning problems (Srebro et al. (2011)), it is not computationally efficient.", "startOffset": 143, "endOffset": 164}, {"referenceID": 43, "context": "As a remark, in order to compute -approximate Nash-equilibria, if both the strategy polytopes are polymatroids then the same projection algorithms apply to the saddle-point mirror prox algorithm (Nemirovski (2004)) and reduce the dependence of the rate of convergence on to O(1/ ).", "startOffset": 196, "endOffset": 214}, {"referenceID": 48, "context": "For selfreducible structures U (Schnorr (1976)) (such as spanning trees, matchings or Hamiltonian cycles), the latter condition for every element f is superfluous, and the generalized approximate counting oracle can be replaced by a fully polynomial approximate generator as shown by Jerrum et al.", "startOffset": 32, "endOffset": 47}, {"referenceID": 28, "context": "For selfreducible structures U (Schnorr (1976)) (such as spanning trees, matchings or Hamiltonian cycles), the latter condition for every element f is superfluous, and the generalized approximate counting oracle can be replaced by a fully polynomial approximate generator as shown by Jerrum et al. (1986). Whenever we have access to a generalized approximate counting oracle, the MWU algorithm converges to -approximate in O(ln |U|/ 2) time.", "startOffset": 284, "endOffset": 305}, {"referenceID": 58, "context": ", Welsh (2009)) and randomized approximate ones for bipartite matchings (Jerrum et al.", "startOffset": 2, "endOffset": 15}, {"referenceID": 28, "context": ", Welsh (2009)) and randomized approximate ones for bipartite matchings (Jerrum et al. (2004)) and extensions such as 0\u2212 1 circulations in directed graphs or subgraphs with prespecified degree sequences (Jerrum et al.", "startOffset": 73, "endOffset": 94}, {"referenceID": 28, "context": ", Welsh (2009)) and randomized approximate ones for bipartite matchings (Jerrum et al. (2004)) and extensions such as 0\u2212 1 circulations in directed graphs or subgraphs with prespecified degree sequences (Jerrum et al. (2004)).", "startOffset": 73, "endOffset": 225}, {"referenceID": 28, "context": ", Welsh (2009)) and randomized approximate ones for bipartite matchings (Jerrum et al. (2004)) and extensions such as 0\u2212 1 circulations in directed graphs or subgraphs with prespecified degree sequences (Jerrum et al. (2004)). As a remark, if a generalized approximate counting oracle exists for both strategy polytopes (as is the case for the spanning tree game mentioned early in the introduction), the same ideas apply to the optimistic mirror descent algorithm (Rakhlin and Sridharan (2013)) and reduce the dependence of the rate of convergence on to O(1/ ) while maintaining polynomial running time.", "startOffset": 73, "endOffset": 495}, {"referenceID": 5, "context": "On the other hand, there exist matroids (Azar et al. (1994)) for which any generalized approximate counting algorithm requires an exponential number of calls to an independence oracle, while an independence oracle is all what we need to make the Bregman projection efficient in the online mirror descent approach.", "startOffset": 41, "endOffset": 60}, {"referenceID": 13, "context": "Related work The general problem of finding Nash-equilibria in 2-player games is PPAD-complete (Chen et al. (2009), Daskalakis et al.", "startOffset": 96, "endOffset": 115}, {"referenceID": 13, "context": "Related work The general problem of finding Nash-equilibria in 2-player games is PPAD-complete (Chen et al. (2009), Daskalakis et al. (2009)).", "startOffset": 96, "endOffset": 141}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al.", "startOffset": 163, "endOffset": 179}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al.", "startOffset": 163, "endOffset": 204}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al.", "startOffset": 163, "endOffset": 226}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)).", "startOffset": 163, "endOffset": 247}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)). These results perform a search on the support of the Nash equilibria and it is not known how to find Nash equilibria for large two-player zero sum games in time logarithmic in the number of pure strategies of the players. In recent work, Hazan and Koren (2015) show that any online algorithm requires \u03a9\u0303( \u221a N) time to approximate the value of a two-player zero-sum game, even when given access to constant time best-response oracles.", "startOffset": 163, "endOffset": 510}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)). These results perform a search on the support of the Nash equilibria and it is not known how to find Nash equilibria for large two-player zero sum games in time logarithmic in the number of pure strategies of the players. In recent work, Hazan and Koren (2015) show that any online algorithm requires \u03a9\u0303( \u221a N) time to approximate the value of a two-player zero-sum game, even when given access to constant time best-response oracles. In this work, we restrict our attention to two-player zero-sum games with bilinear loss functions and give polynomial time algorithms for finding Nash-equilibria (polynomial in the representation of the game). One way to find Nash-equilibria is by using regret-minimization algorithms. We look at the problem of learning combinatorial concepts that has recently gained a lot of popularity in the community (Koolen and Van Erven (2015), Audibert et al.", "startOffset": 163, "endOffset": 1118}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)). These results perform a search on the support of the Nash equilibria and it is not known how to find Nash equilibria for large two-player zero sum games in time logarithmic in the number of pure strategies of the players. In recent work, Hazan and Koren (2015) show that any online algorithm requires \u03a9\u0303( \u221a N) time to approximate the value of a two-player zero-sum game, even when given access to constant time best-response oracles. In this work, we restrict our attention to two-player zero-sum games with bilinear loss functions and give polynomial time algorithms for finding Nash-equilibria (polynomial in the representation of the game). One way to find Nash-equilibria is by using regret-minimization algorithms. We look at the problem of learning combinatorial concepts that has recently gained a lot of popularity in the community (Koolen and Van Erven (2015), Audibert et al. (2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al.", "startOffset": 163, "endOffset": 1142}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)). These results perform a search on the support of the Nash equilibria and it is not known how to find Nash equilibria for large two-player zero sum games in time logarithmic in the number of pure strategies of the players. In recent work, Hazan and Koren (2015) show that any online algorithm requires \u03a9\u0303( \u221a N) time to approximate the value of a two-player zero-sum game, even when given access to constant time best-response oracles. In this work, we restrict our attention to two-player zero-sum games with bilinear loss functions and give polynomial time algorithms for finding Nash-equilibria (polynomial in the representation of the game). One way to find Nash-equilibria is by using regret-minimization algorithms. We look at the problem of learning combinatorial concepts that has recently gained a lot of popularity in the community (Koolen and Van Erven (2015), Audibert et al. (2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al.", "startOffset": 163, "endOffset": 1165}, {"referenceID": 0, "context": "of the loss functions, asymptotic upper and lower bounds (of the order O(logN/ 2)) on the support of approximate Nash equilibria with N pure strategies are known (Alth\u00f6fer (1994), Lipton and Young (1994), Lipton et al. (2003), Feder et al. (2007)). These results perform a search on the support of the Nash equilibria and it is not known how to find Nash equilibria for large two-player zero sum games in time logarithmic in the number of pure strategies of the players. In recent work, Hazan and Koren (2015) show that any online algorithm requires \u03a9\u0303( \u221a N) time to approximate the value of a two-player zero-sum game, even when given access to constant time best-response oracles. In this work, we restrict our attention to two-player zero-sum games with bilinear loss functions and give polynomial time algorithms for finding Nash-equilibria (polynomial in the representation of the game). One way to find Nash-equilibria is by using regret-minimization algorithms. We look at the problem of learning combinatorial concepts that has recently gained a lot of popularity in the community (Koolen and Van Erven (2015), Audibert et al. (2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al.", "startOffset": 163, "endOffset": 1189}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al.", "startOffset": 55, "endOffset": 68}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc.", "startOffset": 55, "endOffset": 90}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)).", "startOffset": 55, "endOffset": 406}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)).", "startOffset": 55, "endOffset": 506}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)).", "startOffset": 55, "endOffset": 530}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al.", "startOffset": 55, "endOffset": 693}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al.", "startOffset": 55, "endOffset": 743}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al.", "startOffset": 55, "endOffset": 840}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)).", "startOffset": 55, "endOffset": 864}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component.", "startOffset": 55, "endOffset": 1005}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm.", "startOffset": 55, "endOffset": 1479}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm.", "startOffset": 55, "endOffset": 1542}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)).", "startOffset": 55, "endOffset": 2008}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)).", "startOffset": 55, "endOffset": 2042}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)). Our algorithm is fundamentally different from the decomposition algorithm; the latter generates a sequence of violated inequalities (a dual approach) while our algorithm maintains a feasible point in the polymatroid (a primal approach). The violated inequalities in the decomposition algorithm come from a series of submodular function minimizations, and their computations can be amortized into a parametric submodular function minimization as is shown in Nagano (2007a,b); Suehiro et al. (2012). In the special cases of the Euclidean or the entropy mirror maps, our iterative algorithm leads to problems involving the maintenance of feasibility along lines, which reduces to parametric submodular function minimization problems.", "startOffset": 55, "endOffset": 2541}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)). Our algorithm is fundamentally different from the decomposition algorithm; the latter generates a sequence of violated inequalities (a dual approach) while our algorithm maintains a feasible point in the polymatroid (a primal approach). The violated inequalities in the decomposition algorithm come from a series of submodular function minimizations, and their computations can be amortized into a parametric submodular function minimization as is shown in Nagano (2007a,b); Suehiro et al. (2012). In the special cases of the Euclidean or the entropy mirror maps, our iterative algorithm leads to problems involving the maintenance of feasibility along lines, which reduces to parametric submodular function minimization problems. In the special case of cardinality-based submodular functions (f(S) = g(|S|) for some concave g), our algorithm can be implemented overall in O(n2) time, matching the running time of a specialized algorithm due to Suehiro et al. (2012). We also consider the multiplicative weights update (MWU) algorithm (Littlestone and Warmuth (1994), Freund and Schapire (1999), Arora et al.", "startOffset": 55, "endOffset": 3011}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)). Our algorithm is fundamentally different from the decomposition algorithm; the latter generates a sequence of violated inequalities (a dual approach) while our algorithm maintains a feasible point in the polymatroid (a primal approach). The violated inequalities in the decomposition algorithm come from a series of submodular function minimizations, and their computations can be amortized into a parametric submodular function minimization as is shown in Nagano (2007a,b); Suehiro et al. (2012). In the special cases of the Euclidean or the entropy mirror maps, our iterative algorithm leads to problems involving the maintenance of feasibility along lines, which reduces to parametric submodular function minimization problems. In the special case of cardinality-based submodular functions (f(S) = g(|S|) for some concave g), our algorithm can be implemented overall in O(n2) time, matching the running time of a specialized algorithm due to Suehiro et al. (2012). We also consider the multiplicative weights update (MWU) algorithm (Littlestone and Warmuth (1994), Freund and Schapire (1999), Arora et al.", "startOffset": 55, "endOffset": 3111}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)). Our algorithm is fundamentally different from the decomposition algorithm; the latter generates a sequence of violated inequalities (a dual approach) while our algorithm maintains a feasible point in the polymatroid (a primal approach). The violated inequalities in the decomposition algorithm come from a series of submodular function minimizations, and their computations can be amortized into a parametric submodular function minimization as is shown in Nagano (2007a,b); Suehiro et al. (2012). In the special cases of the Euclidean or the entropy mirror maps, our iterative algorithm leads to problems involving the maintenance of feasibility along lines, which reduces to parametric submodular function minimization problems. In the special case of cardinality-based submodular functions (f(S) = g(|S|) for some concave g), our algorithm can be implemented overall in O(n2) time, matching the running time of a specialized algorithm due to Suehiro et al. (2012). We also consider the multiplicative weights update (MWU) algorithm (Littlestone and Warmuth (1994), Freund and Schapire (1999), Arora et al.", "startOffset": 55, "endOffset": 3139}, {"referenceID": 0, "context": "(2013), Neu and Bart\u00f3k (2015), Cohen and Hazan (2015), Ailon (2014), Koolen et al. (2010)), and has found many real world applications in communication, principal component analysis, scheduling, routing, personalized content recommendation, online ad display etc. There are two popular approaches to learn over combinatorial concepts. The first is the Follow-the-Perturbed-Leader (Kalai and Vempala (2005)), though efficient in runtime complexity it is suboptimal in terms of regret (Neu and Bart\u00f3k (2015), Cohen and Hazan (2015)). The other method for learning over combinatorial structures is the online mirror descent algorithm (online convex optimization is attributed to Zinkevich (2003) and mirror descent to Nemirovski and Yudin (1983)), which fares better in terms of regret but is computationally inefficient (Srebro et al. (2011), Audibert et al. (2013)). In the first part of the work, we give a novel algorithm that speeds up the mirror descent algorithm in some setting. Koolen et al. (2010) introduce the Component Hedge (CH) algorithm for linear loss functions that sum over the losses for each component. They perform multiplicative updates for each component and subsequently perform Bregman projections over certain extended formulations with a polynomial number of constraints, using off-the-shelf convex optimization subroutines. Our work, on the other hand, allows for polytopes with exponentially many inequalities. The works of Helmbold and Warmuth (2009) (for learning over permutations) and Warmuth and Kuzmin (2008) (for learning k-sets) have been shown to be special cases of the CH algorithm. Our work applies to these settings. One of our contributions is to give a new efficient algorithm INC-FIX for performing Bregman projections on base polytopes of polymatroids, a step in the online mirror descent algorithm. Another way of solving the associated separable convex optimization problem over base polytopes of polymatroids is the decomposition algorithm of Groenevelt (1991) (also studied in Fujishige (2005)). Our algorithm is fundamentally different from the decomposition algorithm; the latter generates a sequence of violated inequalities (a dual approach) while our algorithm maintains a feasible point in the polymatroid (a primal approach). The violated inequalities in the decomposition algorithm come from a series of submodular function minimizations, and their computations can be amortized into a parametric submodular function minimization as is shown in Nagano (2007a,b); Suehiro et al. (2012). In the special cases of the Euclidean or the entropy mirror maps, our iterative algorithm leads to problems involving the maintenance of feasibility along lines, which reduces to parametric submodular function minimization problems. In the special case of cardinality-based submodular functions (f(S) = g(|S|) for some concave g), our algorithm can be implemented overall in O(n2) time, matching the running time of a specialized algorithm due to Suehiro et al. (2012). We also consider the multiplicative weights update (MWU) algorithm (Littlestone and Warmuth (1994), Freund and Schapire (1999), Arora et al. (2012)) rediscovered for different settings in game theory, machine learning, and online decision making with a large number of applications.", "startOffset": 55, "endOffset": 3160}, {"referenceID": 8, "context": "also made in Blum et al. (2008). In order to perform this algorithm efficiently for exponential experts (with combinatorial structure), it does not take much to see that multiplicative updates for linear losses can be made using product terms.", "startOffset": 13, "endOffset": 32}, {"referenceID": 8, "context": "also made in Blum et al. (2008). In order to perform this algorithm efficiently for exponential experts (with combinatorial structure), it does not take much to see that multiplicative updates for linear losses can be made using product terms. However, the analysis of prior works was very specific to the structure of the problem. For example, Takimoto and Warmuth (2003) give efficient implementations of the MWU for learning over general s\u2212 t paths that allow for cycles or over simple paths in acyclic directed graphs.", "startOffset": 13, "endOffset": 373}, {"referenceID": 8, "context": "also made in Blum et al. (2008). In order to perform this algorithm efficiently for exponential experts (with combinatorial structure), it does not take much to see that multiplicative updates for linear losses can be made using product terms. However, the analysis of prior works was very specific to the structure of the problem. For example, Takimoto and Warmuth (2003) give efficient implementations of the MWU for learning over general s\u2212 t paths that allow for cycles or over simple paths in acyclic directed graphs. This approach relies on the recursive structure of these paths, and does not generalize to simple paths in an undirected graph (or a directed graph). Similarly, Helmbold and Schapire (1997) rely on the recursive structure of bounded depth binary decision trees.", "startOffset": 13, "endOffset": 713}, {"referenceID": 8, "context": "also made in Blum et al. (2008). In order to perform this algorithm efficiently for exponential experts (with combinatorial structure), it does not take much to see that multiplicative updates for linear losses can be made using product terms. However, the analysis of prior works was very specific to the structure of the problem. For example, Takimoto and Warmuth (2003) give efficient implementations of the MWU for learning over general s\u2212 t paths that allow for cycles or over simple paths in acyclic directed graphs. This approach relies on the recursive structure of these paths, and does not generalize to simple paths in an undirected graph (or a directed graph). Similarly, Helmbold and Schapire (1997) rely on the recursive structure of bounded depth binary decision trees. Koo et al. (2007) use the matrix tree theorem to learn over spanning trees by doing largemargin optimization.", "startOffset": 13, "endOffset": 803}, {"referenceID": 37, "context": "4) in Lov\u00e1sz et al. (1988)).", "startOffset": 6, "endOffset": 27}, {"referenceID": 24, "context": ") This means that our polytope defining (LP1\u2032) is well-described (\u00e0 la Gr\u00f6tschel et al.). We can thus use the machinery of the ellipsoid algorithm (Gr\u00f6tschel et al. (1981)) to find a Nash Equilibrium in polynomial", "startOffset": 71, "endOffset": 172}, {"referenceID": 39, "context": "This allows to give a compact extended formulation for (LP1\u2032) for the spanning tree game as a compact formulation is known for the spanning tree polytope (Martin (1991)) (and any other game where the two strategy polytopes can be described using polynomial number of inequalities).", "startOffset": 155, "endOffset": 169}, {"referenceID": 39, "context": "This allows to give a compact extended formulation for (LP1\u2032) for the spanning tree game as a compact formulation is known for the spanning tree polytope (Martin (1991)) (and any other game where the two strategy polytopes can be described using polynomial number of inequalities). However, this would not work for a corresponding matching game since the extension complexity for the matching polytope is exponential (Rothvo\u00df (2014)).", "startOffset": 155, "endOffset": 433}, {"referenceID": 40, "context": ", limx\u2192\u2202D ||\u2207\u03c9(x)|| = \u221e (for details, refer to Nemirovski and Yudin (1983), Beck and Teboulle (2003), Bubeck (2014)).", "startOffset": 47, "endOffset": 75}, {"referenceID": 6, "context": ", limx\u2192\u2202D ||\u2207\u03c9(x)|| = \u221e (for details, refer to Nemirovski and Yudin (1983), Beck and Teboulle (2003), Bubeck (2014)).", "startOffset": 76, "endOffset": 101}, {"referenceID": 6, "context": ", limx\u2192\u2202D ||\u2207\u03c9(x)|| = \u221e (for details, refer to Nemirovski and Yudin (1983), Beck and Teboulle (2003), Bubeck (2014)).", "startOffset": 76, "endOffset": 116}, {"referenceID": 8, "context": "We restate the theorem about the regret of the online mirror-descent algorithm (adapted from Bubeck (2011), Ben-Tal and Nemirovski (2001), Rakhlin and Sridharan (2014)).", "startOffset": 93, "endOffset": 107}, {"referenceID": 7, "context": "We restate the theorem about the regret of the online mirror-descent algorithm (adapted from Bubeck (2011), Ben-Tal and Nemirovski (2001), Rakhlin and Sridharan (2014)).", "startOffset": 108, "endOffset": 138}, {"referenceID": 7, "context": "We restate the theorem about the regret of the online mirror-descent algorithm (adapted from Bubeck (2011), Ben-Tal and Nemirovski (2001), Rakhlin and Sridharan (2014)).", "startOffset": 108, "endOffset": 168}, {"referenceID": 17, "context": "Given such a function f , the independent set polytope is defined as P (f) = {x \u2208 R+ : x(U) \u2264 f(U) \u2200 U \u2286 E} and the base polytope as B(f) = {x \u2208 R+ : x(E) = f(E), x(U) \u2264 f(U) \u2200 U \u2286 E} (Edmonds (1970)).", "startOffset": 185, "endOffset": 200}, {"referenceID": 17, "context": "Using Edmonds\u2019 greedy algorithm Edmonds (1971), we know that any z\u2217 \u2208 B(f) is a minimizer of \u2207h(x\u2217)T z if and only if it is tight (i.", "startOffset": 6, "endOffset": 47}, {"referenceID": 15, "context": "For the graphic matroid, feasibility in the forest polytope can be solved by O(|V |) maximum flow problems (Cunningham (1985), Corollary 51.", "startOffset": 108, "endOffset": 126}, {"referenceID": 15, "context": "For the graphic matroid, feasibility in the forest polytope can be solved by O(|V |) maximum flow problems (Cunningham (1985), Corollary 51.3a in Schrijver (2003)), and as result, LINE can be solved as O(|V |2) maximum flow problems (by Dinkelbach\u2019s discrete Newton method) orO(|V |) parametric maximum flow problems.", "startOffset": 108, "endOffset": 163}, {"referenceID": 15, "context": "For the graphic matroid, feasibility in the forest polytope can be solved by O(|V |) maximum flow problems (Cunningham (1985), Corollary 51.3a in Schrijver (2003)), and as result, LINE can be solved as O(|V |2) maximum flow problems (by Dinkelbach\u2019s discrete Newton method) orO(|V |) parametric maximum flow problems. For general polymatroids over the ground set E, the problem LINE can be solved using Nagano\u2019s parametric submodular function minimization (Nagano (2007c)) that requires O(|E|6 + \u03b3|E|5) running time, where \u03b3 is the time required by the value oracle of the submodular function.", "startOffset": 108, "endOffset": 472}, {"referenceID": 15, "context": "For the graphic matroid, feasibility in the forest polytope can be solved by O(|V |) maximum flow problems (Cunningham (1985), Corollary 51.3a in Schrijver (2003)), and as result, LINE can be solved as O(|V |2) maximum flow problems (by Dinkelbach\u2019s discrete Newton method) orO(|V |) parametric maximum flow problems. For general polymatroids over the ground set E, the problem LINE can be solved using Nagano\u2019s parametric submodular function minimization (Nagano (2007c)) that requires O(|E|6 + \u03b3|E|5) running time, where \u03b3 is the time required by the value oracle of the submodular function. Each of the entropy and the Euclidean mirror maps requires O(|E|) (O(|V |) for the graphic matroid) such computations to compute a projection, since each iteration of the INC-FIX algorithm at least one non-tight edge becomes tight. Thus, for the graphic matroid we can compute Bregman projections in O(|V |4|E|) time (using Orlin\u2019s O(|V ||E|) algorithm (Orlin (2013)) for computing the maximum flow) and for general polymatroids the running time is O(|E|7 + \u03b3|E|6) where |E| is the size of the ground set.", "startOffset": 108, "endOffset": 961}, {"referenceID": 2, "context": "Arora et al. (2012)).", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "This follows from the analysis, and also from the fact that any point in the relative interior of a 0/1 polytope can be viewed as a (max-entropy) product distribution over the vertices of the polytope (Asadpour et al. (2010), Singh and Vishnoi (2014)).", "startOffset": 202, "endOffset": 225}, {"referenceID": 3, "context": "This follows from the analysis, and also from the fact that any point in the relative interior of a 0/1 polytope can be viewed as a (max-entropy) product distribution over the vertices of the polytope (Asadpour et al. (2010), Singh and Vishnoi (2014)).", "startOffset": 202, "endOffset": 251}, {"referenceID": 30, "context": ", Koutis et al. (2010)) for obtaining a fast approximate marginal oracle.", "startOffset": 2, "endOffset": 23}, {"referenceID": 27, "context": ", rankings), one can use the randomized generalized approximate counting oracle from (Jerrum et al. (2004)) for computing permanents to obtain a feasible marginal oracle.", "startOffset": 86, "endOffset": 107}, {"referenceID": 27, "context": ", rankings), one can use the randomized generalized approximate counting oracle from (Jerrum et al. (2004)) for computing permanents to obtain a feasible marginal oracle. Note that the problem of counting the number of perfect matchings in a bipartite graph is #P-complete as it is equivalent to computing the permanent of a 0/1 matrix (Valiant (1979)).", "startOffset": 86, "endOffset": 352}, {"referenceID": 27, "context": ", rankings), one can use the randomized generalized approximate counting oracle from (Jerrum et al. (2004)) for computing permanents to obtain a feasible marginal oracle. Note that the problem of counting the number of perfect matchings in a bipartite graph is #P-complete as it is equivalent to computing the permanent of a 0/1 matrix (Valiant (1979)). The problem of approximately counting the number of perfect matchings in a general graph is however a long standing open problem, if solved, it would result in another way of solving MSP games on the matching polytope. Another example of a polytope that admits a polynomial approximate counting oracle is the cycle cover polytope (or 0 \u2212 1 circulations) over directed graphs (Singh and Vishnoi (2014)).", "startOffset": 86, "endOffset": 755}, {"referenceID": 27, "context": ", rankings), one can use the randomized generalized approximate counting oracle from (Jerrum et al. (2004)) for computing permanents to obtain a feasible marginal oracle. Note that the problem of counting the number of perfect matchings in a bipartite graph is #P-complete as it is equivalent to computing the permanent of a 0/1 matrix (Valiant (1979)). The problem of approximately counting the number of perfect matchings in a general graph is however a long standing open problem, if solved, it would result in another way of solving MSP games on the matching polytope. Another example of a polytope that admits a polynomial approximate counting oracle is the cycle cover polytope (or 0 \u2212 1 circulations) over directed graphs (Singh and Vishnoi (2014)). Also, we would like to note that to compute Nash-equilibria for MSP games that admit marginal oracles for both the polytopes, the optimistic mirror descent algorithm is simply the exponential weights with a modified loss vector (Rakhlin and Sridharan (2013)), and hence the same framework applies.", "startOffset": 86, "endOffset": 1015}, {"referenceID": 27, "context": ", rankings), one can use the randomized generalized approximate counting oracle from (Jerrum et al. (2004)) for computing permanents to obtain a feasible marginal oracle. Note that the problem of counting the number of perfect matchings in a bipartite graph is #P-complete as it is equivalent to computing the permanent of a 0/1 matrix (Valiant (1979)). The problem of approximately counting the number of perfect matchings in a general graph is however a long standing open problem, if solved, it would result in another way of solving MSP games on the matching polytope. Another example of a polytope that admits a polynomial approximate counting oracle is the cycle cover polytope (or 0 \u2212 1 circulations) over directed graphs (Singh and Vishnoi (2014)). Also, we would like to note that to compute Nash-equilibria for MSP games that admit marginal oracles for both the polytopes, the optimistic mirror descent algorithm is simply the exponential weights with a modified loss vector (Rakhlin and Sridharan (2013)), and hence the same framework applies. Sampling pure strategies: In online learning scenarios that require the learner to play a combinatorial concept (i.e., a pure strategy in each round), we note first that given any mixed strategy (that lies in a strategy polytope \u2208 Rn), the learner can obtain a convex decomposition of the mixed strategy into at most n + 1 vertices by using the well-known Caratheodory\u2019s Theorem. The learner can then play a pure strategy sampled proportional to the convex coefficients in the decomposition. In the case of learning over the spanning tree and bipartite perfect matching polytopes using product distributions however, there exists a more efficient way of sampling due to the self-reducibility6 of the these polytopes (Kulkarni (1990), Asadpour et al.", "startOffset": 86, "endOffset": 1788}, {"referenceID": 3, "context": "In the case of learning over the spanning tree and bipartite perfect matching polytopes using product distributions however, there exists a more efficient way of sampling due to the self-reducibility6 of the these polytopes (Kulkarni (1990), Asadpour et al. (2010)): Order the edges of the graph randomly and decide for each probabilistically whether to use it in the final object or not.", "startOffset": 242, "endOffset": 265}, {"referenceID": 3, "context": "In the case of learning over the spanning tree and bipartite perfect matching polytopes using product distributions however, there exists a more efficient way of sampling due to the self-reducibility6 of the these polytopes (Kulkarni (1990), Asadpour et al. (2010)): Order the edges of the graph randomly and decide for each probabilistically whether to use it in the final object or not. The probabilities of each edge are updated after every iteration conditioned on the decisions (i.e., to include or not) made on the earlier edges. This sampling procedure works as long as there exists a polynomial time marginal oracle (i.e., a generalized counting oracle) to update the probabilities of the elements of the ground set after each iteration and if the polytope is self-reducible (Sinclair and Jerrum (1989)).", "startOffset": 242, "endOffset": 811}, {"referenceID": 21, "context": "Lexicographic optimality: We further note that symmetric Nash-equilibria are closely related to the concept of being lexicographically optimal as studied in Fujishige (1980). For a matroid M = (E, I), x \u2208 B(M) is called lexicographically optimal with respect to a positive weight vector w if the |E|-tuple of numbers x(e)/w(e) (e \u2208 E) arranged in the order of increasing magnitude is lexicographically maximum among all", "startOffset": 157, "endOffset": 174}, {"referenceID": 21, "context": "We evoke the following theorem from Fujishige (1980).", "startOffset": 36, "endOffset": 53}], "year": 2016, "abstractText": "In order to find Nash-equilibria for two-player zero-sum games where each player plays combinatorial objects like spanning trees, matchings etc, we consider two online learning algorithms: the online mirror descent (OMD) algorithm and the multiplicative weights update (MWU) algorithm. The OMD algorithm requires the computation of a certain Bregman projection, that has closed form solutions for simple convex sets like the Euclidean ball or the simplex. However, for general polyhedra one often needs to exploit the general machinery of convex optimization. We give a novel primal-style algorithm for computing Bregman projections on the base polytopes of polymatroids. Next, in the case of the MWU algorithm, although it scales logarithmically in the number of pure strategies or experts N in terms of regret, the algorithm takes time polynomial in N ; this especially becomes a problem when learning combinatorial objects. We give a general recipe to simulate the multiplicative weights update algorithm in time polynomial in their natural dimension. This is useful whenever there exists a polynomial time generalized counting oracle (even if approximate) over these objects. Finally, using the combinatorial structure of symmetric Nash-equilibria (SNE) when both players play bases of matroids, we show that these can be found with a single projection or convex minimization (without using online learning).", "creator": "LaTeX with hyperref package"}}}