{"id": "1603.00772", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Filter based Taxonomy Modification for Improving Hierarchical Classification", "abstract": "large hierarchical classification of data organized as a hierarchy of classes has received significant attention in the literature. top - down ( td ) hierarchical classification ( hc ), which exploits the hierarchical structure during the learning process is theoretically effective method for dealing prediction problems at scale due because its computational advantages. however, forecast accuracy suffers due to error propagation i. e., prediction schemes made at higher sizes in arbitrary hierarchy cannot be corrected at lower levels. one of the main reasons behind errors at the higher levels is the presence of inconsistent nodes and links that are introduced due to the arbitrary process constantly creating these hierarchies by domain experts. through this paper, we propose two efficient data driven filter based approaches emphasizing hierarchical structure modification : ( i ) flattening ( complex and global ) approach to exploits and removes inconsistent nodes present within the hierarchy and ( ii ) rewiring approach modifies parent - child relationships to improve cognitive classification performance of learned models. our simple empirical evaluation of locally proposed approaches on several image and text datasets indicate comparable performance identifying competing approaches.", "histories": [["v1", "Wed, 2 Mar 2016 16:14:49 GMT  (583kb,D)", "https://arxiv.org/abs/1603.00772v1", null], ["v2", "Thu, 9 Jun 2016 06:41:42 GMT  (581kb,D)", "http://arxiv.org/abs/1603.00772v2", null], ["v3", "Sat, 15 Oct 2016 06:21:54 GMT  (650kb,D)", "http://arxiv.org/abs/1603.00772v3", "The conference version of the paper is submitted for publication"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["azad naik", "huzefa rangwala"], "accepted": false, "id": "1603.00772"}, "pdf": {"name": "1603.00772.pdf", "metadata": {"source": "CRF", "title": "Filter based Taxonomy Modification for Improving Hierarchical Classification", "authors": ["Azad Naik", "Huzefa Rangwala"], "emails": [], "sections": [{"heading": null, "text": "Keywords\u2014 Top-Down Hierarchical Classification, Rewiring, Clustering, Flattening"}, {"heading": "1 Introduction", "text": "Taxonomy (hierarchy) is most commonly used to organize large volumes of data. It has been successfully used in different application domains such as bioinformatics1, computer vision2 and web directories3. These application domains (especially highlighed by interest in online prediction challenges such as LSHTC4 and BioASQ5) introduce unique computational and statistical challenges. Given that these datasets have several thousand classes, the developed methods need to scale during the learning and prediction phases. Further, the majority of classes have very few training examples, leading to a class imbalance problem where the learned models (for rare categories) have a tendency to overfit and mispredictions favor the larger classes.\nHierarchies provide useful structural relationships (such as parent-child and siblings) among different classes that can be exploited for learning generalized classification models. In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135]. Utilizing the hierarchical structure has been shown to improve the classification performance for rare categories as well [6]. Top-down HC methods that leverage the hierarchy during the learning and prediction process are effective approaches to deal with large-scale problems [2]. Classification decision for top-down methods involves invoking only the models in the relevant path within the hierarchy. Though computationally efficient, these methods have higher number of misclassifications due to error propagation [7].\nFor several benchmarks, the HC approaches are outperformed by flat classifiers that ignore the hierarchy [9,10]. In majority of the cases, the hierarchy available for training classifiers is manually designed by experts based on domain knowledge and is not consistent for classification. In order to improve performance, we need to restructure the hierarchy to make it more favorable and useful for classification. Motivated by this idea, our main focus in this paper is on generating an improved representation from the expert-defined hierarchy. To summarize, our contributions are as follows:\n\u2217Department of Computer Science, George Mason University. Email: anaik3@gmu.edu; rangwala@cs.gmu.edu 1http://geneontology.org/ 2http://www.image-net.org/ 3http://dir.yahoo.com/ 4http://lshtc.iit.demokritos.gr/ 5http://bioasq.org/\nar X\niv :1\n60 3.\n00 77\n2v 3\n[ cs\n.A I]\n1 5\nO ct\n2 01\n6\n\u2022 We propose an efficient data-driven filter based rewiring approach for hierarchy modification which unlike previous wrapper based approaches [11,12] does not require multiple, expensive computations. Our approach is scalable and can be applied to the HC problems with high-dimensional features, large number of classes and examples.\n\u2022 We perform extensive empirical evaluations and case studies to show the strengths of our approach in comparison to other hierarchy modification approaches such as clustering and flattening. \u2022 The modified hierarchy can be used with any hierarchical classification approaches like top-down HC or stateof-the-art approaches incorporating hierarchical relationships [13]. The modified hierarchy in conjunction with a scalable Top-Down HC approach outperforms the flat classifiers on \u223c65% of the rare categories (i.e., classes with less than 10 training examples) across the DMOZ datasets (See Section 4.5)."}, {"heading": "2 Methods", "text": "2.1 Motivation The manual process of hierarchy creation suffers from various issues. Specifically, (i) Hierarchies are generated by grouping semantically similar categories under a common parent category. However, many different semantically sound hierarchies may exist for same set of classes. For example, in categorizing products, the experts may generate a hierarchy by first separating products based on the company name (e.g., Apple, Microsoft) and then the product type (e.g., phone, tablet) or vice-versa. Both hierarchies are equally good from the perspective of an expert. However, these different hierarchies may lead to different classification results. (ii) Apriori it is not clear to domain experts when to generate new nodes (hierarchy expansion) or merge two or more nodes (link creation) while creating hierarchies, resulting in a certain degree of arbitrariness. (iii) A large number of categories pose a challenge for the manual design of a consistent hierarchy. (iv) Dynamic changes may require hierarchical restructuring.\nTo remove inconsistencies, various approaches for hierarchy modification have been proposed. These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance. Clustering based methods have also been adapted in some of these studies [8,17] where consistent hierarchy is generated from scratch using agglomerative or divisive clustering algorithms. A summary of the various existing methods and their characteristics is shown in Table 1.\nTo understand the qualitative difference between hierarchy generated using various approaches, we performed experiments on the smaller newsgroup6 dataset containing 20 classes. Figure 1(b)-(d) shows the hierarchy structure obtained using clustering, flattening and rewiring based approaches, respectively. Hierarchy generated using clustering completely ignores the expert-defined hierarchy information, which contains valuable prior knowledge for classification [11]. Flattening approaches cannot group together the classes from different hierarchical branches (for e.g, soc.religion.christian and religion.misc). On the contrary, the rewiring approaches provide the flexibility of grouping classes from different sub-branches. More details about Figure 1 are discussed later in a case study (Section 4.1).\n2.2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning\n6http://qwone.com/sim jason/20Newsgroups/\nalgorithm. Modified changes are retained if the performance results improve; otherwise the changes are discarded and the process is repeated. This repeated procedure of hierarchy modification continues until the optimal hierarchy that satisfies certain criteria is reached. As such, wrapper approaches are not scalable for large datasets.\nWe propose an efficient data-driven filter based rewiring approach where the hierarchy is modified based on certain relevance criterion (pairwise sibling similarity) between the different classes within the hierarchy. Our approach is single step and do not require experimental evaluation for multiple iterations. We refer to our proposed rewiring approach as rewHier. Table 2 captures the common notations used in this paper and Algorithm 1 illustrates our approach for hierarchy modification. Specifically, it consists of two steps:\n(i) Grouping Similar Classes Pairs - To ensure classes with high degree of similarity are grouped together under the same parent node in the modified taxonomy, this step identifies the similar classes pairs that exist within the expert-defined hierarchy. Pairwise cosine similarity is used as the similarity measure in our experiments because it is less prone to the curse of dimensionality [20]. Once the similarity scores are computed, we determine the set S of most similar pairs of classes using an empirically defined cut-off threshold \u03c4 for a dataset (detailed analysis regarding \u03c4 selection is discussed in Section 4.4). For example, in Figure 1(a) this step will group together the class pairs with high similarity scores such as S = [ (religion.misc, soc.religion.christian), (electronics,\nwindows.x), (electronics, graphics), \u00b7\u00b7\u00b7 ] .\nPairwise similarity computation between different classes is one of the major bottlenecks of this step. To make it scalable, we distribute the similarity computation across multiple compute nodes.\n(ii) Inconsistency Identification and Correction - To obtain the consistent hierarchy, we group together each of the similar class pairs to a common parent node. Iteratively, starting from the most similar class pairs we check for potential inconsistencies i.e., if the pairs of classes are in different branches (sub-trees). In order to resolve the identified inconsistencies we take corrective measures using three basic elementary operations: (i) node creation, (ii) parent-child rewiring and (iii) node deletion. Figure 2(b)-(d) illustrates the various hierarchical structures that are obtained after the execution of these elementary operations on the expert-defined hierarchy in Figure 2 (a).\nNode Creation (NC) - This operation groups together the identified similar class pairs in different branches\nAlgorithm 1 rewHier Algorithm Data: Original Hierarchy H, input-output (xi, yi) Result: Modified Hierarchy HM /* Initialization */ HM = H; /* Ist step: Grouping Similar Classes Pair */ Compute cosine similarity between all possible class pairs. /* similar class grouping */ Identify the most similar class pairs with similarity scores value greater than empirically defined threshold parameter \u03c4 . Let |c| denotes the number of such pairs represented by the set S = {s1, s2, . . . , s|c|}, where i-th pair si is represented using (s (1) i , s (2) i ). /* IInd step: Inconsistency Identification and Correction */ for i = 1 to |c| do\nrewire[1] = 1; /* check if rewiring is needed for s (1) i */ rewire[2] = 1; /* check if rewiring is needed for s (2) i */ /* Inconsistent pair check */ if \u03c0(s (1) i ) 6= \u03c0(s (2) i ) then\n/* check similarity to all siblings */ foreach j \u2208 \u03b6(s(1)i ) do if ( (j, s\n(2) i ) or (s (2) i , j)\n) /\u2208 S then\nrewire[2] = 0; break;\nend\nend foreach j \u2208 \u03b6(s(2)i ) do if ( (j, s\n(1) i ) or (s (1) i , j)\n) /\u2208 S then\nrewire[1] = 0; break;\nend\nend if (rewire[1] == 0) and (rewire[2] == 0) then\n/* perform node creation */ Nnew = \u03c6 /* create new node */ [HM ] = NC(Nnew\u2192lca(si), si\u2192Nnew,HM ); /* lca denotes lowest common ancestor */\nelse if (rewire[1] == 1) then\n[HM ] = PCRewire(s(1)i \u2192\u03c0(s (2) i ),HM );\nelse\n[HM ] = PCRewire(s(2)i \u2192\u03c0(s (1) i ),HM );\nend\nend\nend\nend /* perform node deletion */ [HM ] = ND(HM ); return HM\n(sub-trees) of the hierarchy using a new node, with parent as the lowest common ancestors of similar classes.\nFigure 2(b) illustrates this operation where the similar class pairs 5 and 6 are grouped together by the newly created node D. This operation is used only when a proper subset of the leaf nodes from different branches are similar (i.e., not similar to all leaf nodes in the branch; otherwise the parent-child rewiring operation is used).\nParent-child Rewiring (PCRewire) - As shown in Figure 2(c), this operation simply assigns (rewires) the leaf node from one parent to another parent node in the hierarchy. It is useful when the leaf node is identified to be similar to all the sibling leaf nodes within the given hierarchy branch. For example, in Figure 2(c), if the computed similarity score determines the leaf node 6 to be more similar to nodes 3, 4 and 5 in comparison to its current siblings 7 and 8, than it is more desirable from a classification perspective to assign 6 as node B child rather than C.\nNode Deletion (ND) - This refers to deletion of nodes in the hierarchy that are deemed useless for classification. In Figure 2(d), node B is deleted because there are no leaf nodes that can be classified by node B. This operation is used as a post-processing step in our algorithm to refine the hierarchy.\nThe rewHier algorithm determines (outer for loop) the best corrective measures (node creation or parentchild rewiring) that need to be taken. Once all the inconsistencies have been addressed, rewHier calls the node deletion procedure as a final modification step where unnecessary nodes are deleted.\nIt should be noted that the new modified hierarchy obtained after inconsistencies removal can be used to train any HC classifier. State-of-the-art HC classification approaches embed the parent-child relationships from the hierarchy either, within the regularization term [21], referred by HR-LR (or HR-SVM) or the loss term, referred by HierCost [13]. The intuition behind Hierarchy Regularized Logistic Regression (HR-LR) [21] approach is that data-sparse child nodes benefit during training from data-rich parent nodes, and this has been shown to achieve the best performance on standard HC benchmarks. However, training these models is computationally expensive due to the coupling between different classes within this formulation. To make this method scalable, distributed computation using hadoop map-reduce was proposed in conjunction with parallel training of odd and even levels. As such, this method requires special hardware and software configurations for large datasets and hence, we did not use this method in our experiments. In case of HierCost [13], a cost-sensitive learning approach was adapted. This method intuitively captures the hierarchical information by treating misclassifications differently based on the commonalities (ancestors) between the true and the predicted labels. Intrinsically, this method scales for large datasets due to the trivial decomposition of learned models for different leaf categories. This method outperforms HR-LR method without any additional parameter configurations. Hence, in this paper we use the HierCost approach for evaluation with our rewired hierarchies.\n2.3 Top-Down Hierarchical Classification We propose to use the Top-Down HC approach with our modified hierarchies because it scales well during training and prediction. Specifically, we train binary onevs-rest classifiers for each of the nodes n \u2208 N \u2014 to discriminate its positive examples from the examples of other nodes (i.e., negative examples) in the hierarchy. In this paper, we use logistic regression (LR) as the underlying base model for training [21]. The LR objective uses logistic loss to minimize the empirical risk and squared l2-norm term (denoted by || \u00b7 ||22) to control model complexity and prevent overfitting. The objective function fn for training a model corresponding to node n is provided in eq. (2.1).\n(2.1) fn = min \u0398n\n[ C\nN\u2211 i=1 log ( 1 + exp ( \u2212yni \u0398n Txi )) + 1 2 \u2016\u0398n\u201622\n]\nFor each node n in the hierarchy, we solve eq. (2.1) to obtain the optimal weight vector denoted by \u0398n. The complete set of parameters for all the nodes [\u0398n]n\u2208N constitutes the learned model for top-down classifier. For LR models, the conditional probability for y\u0302ni \u2208 \u00b11 given its feature vector xi and the weight vector \u0398n is given by eq. (2.2) and the decision function by eq. (2.3).\nP ( y\u0302ni | xi,\u0398n ) = 1 /( 1 + exp ( \u2212yni \u0398Tnxi )) (2.2)\ny\u0302ni =\n{ + 1 fn(xi) = \u0398 T nxi \u2265 0\n\u2212 1 otherwise\n} (2.3)\nFor a test example with feature vector xi, the top-down classifier predicts the class label y\u0302i \u2208 L as shown in eq. (2.4). Essentially, the algorithm starts at the root and recursively selects the best child node until it reaches a terminal node which is the predicted label.\ny\u0302i =  initialize p := root while p /\u2208 L p := argmaxq\u2208C(p) fq(xi)\nreturn p\n(2.4)"}, {"heading": "3 Experimental Protocol", "text": "3.1 Datasets We have used an extensive set of datasets for evaluating the performance of our proposed rewiring approach. Various statistics of the datasets used are listed in Table 3. CLEF [22] and DIATOMS [23] are image datasets and the rest are text datasets. IPC7 is a collection of patent documents and the DMOZ datasets are an archive of web-pages available from LSHTC8 challenge website. For evaluating the DMOZ-2010 and DMOZ-2012 datasets we use the provided test split. The results reported for these two benchmarks are blind prediction (i.e., we do not know the ground truth labels for the test set) obtained from the web-portal interface9,10. For all text datasets we apply the tf-idf transformation with l2-norm normalization on the word-frequency feature vector."}, {"heading": "3.2 Evaluation Metrics", "text": "Flat Measures - Micro-F1 (\u00b5F1) and Macro-F1 (MF1) are used for evaluating the performance. To compute \u00b5F1, we sum up the category specific true positives (TPc), false positives (FPc) and false negatives (FNc) for different classes and compute the score as:\nP = \u2211 c\u2208L TPc\u2211\nc\u2208L(TPc + FPc) , R =\n\u2211 c\u2208L TPc\u2211\nc\u2208L(TPc + FNc)\n\u00b5F1 = 2PR\nP +R\n7http://www.wipo.int/classifications/ipc/en/ 8http://lshtc.iit.demokritos.gr/ 9http://lshtc.iit.demokritos.gr/node/81\n10 http://lshtc.iit.demokritos.gr/LSHTC3 oracleUpload\nUnlike \u00b5F1 that gives equal weight to each instance, MF1 gives equal weight to all classes so that score is not skewed in favor of the larger classes. It is computed as:\nPc = TPc\nTPc + FPc , Rc = TPc TPc + FNc MF1 = 1\n|L| \u2211 c\u2208L 2PcRc Pc +Rc\nHierarchical Measures - Hierarchy is used for evaluating the classifier performance. The hierarchy based measure include hierarchical F1 (hF1) defined by:\nhP = \u2211N i=1 |A(y\u0302i) \u2229 A(yi)|\u2211N\ni=1 |A(y\u0302i)| , hR =\n\u2211N i=1 |A(y\u0302i) \u2229 A(yi)|\u2211N\ni=1 |A(yi)|\nhF1 = 2 \u2217 hP \u2217 hR hP + hR\nwhere A(y\u0302i) and A(yi) are the sets of ancestors of the predicted and true labels which include the label itself, but do not include the root node, respectively.\nNote that for consistent evaluation, we have used the original hierarchy for all methods unless noted."}, {"heading": "3.3 Methods for Comparison", "text": "3.3.1 Hierarchical Methods Based on the hierarchy used during the training process, we use the following methods for comparison.\nTop-Down Logistic Regression (TD-LR): Expert-defined hierarchy provided by domain experts is used for training the classifiers.\nClustering Approach: Hierarchy generated using agglomerative clustering is used. For evaluation, we have restricted the height of clustered hierarchy to the original height by flattening using cluster cohesion [8].\nGlobal Inconsistent Node Flattening (Global-INF) [7]: Hierarchy is modified by flattening (removing) the inconsistent nodes based on optimal optimization objective value obtained at each node (eq. (2.1)) and empirically defined global cut-off threshold.\nOptimal Hierarchy Search [11]: Optimal hierarchy is identified in the hierarchical space by gradually modifying the expert-defined hierarchy using elementary operations \u2013 promote, demote and merge. For reducing the number of operations (and hence hierarchy evaluations), we have restricted the modification to the hierarchy branches where we encountered the maximum classification errors. This modified approach is referred as T-Easy. In the original paper [11], the largest evaluated dataset has 244 classes and 15795 instances.\n3.3.2 Flat Method The hierarchy is ignored and binary one-versus-rest l2-regularized LR classifiers are trained for each of the leaf categories. The prediction decision for unlabeled test instances is based on the maximum prediction score achieved across the several leaf categories classifiers.\n3.3.3 State-of-the-art Cost-sensitive Learning [13] Similar to flat method but with cost value associated with each instance in the loss function as shown in eq. (3.5). This approach is referred as HierCost and for evaluations we have used the best cost function \u201cexponential tree distance (ExTrD)\u201d proposed in the paper.\n(3.5) fn = min \u0398n\n[ C\nN\u2211 i=1 \u03c3i log ( 1 + exp ( \u2212yni \u0398n Txi )) + 1 2 \u2016\u0398n\u201622\n]\nwhere \u03c3i is the cost value assigned to example i.\n3.4 Experimental Settings To make the experimental results comparable to previously published results we use the same train-test split as provided by the public benchmarks. In all the experiments we divide the training dataset into train and a small validation dataset in the ratio 90:10. The final reported testing performance is\ndone on an independent held-out dataset as provided by these benchmarks. The model is trained by choosing the misclassification penalty parameter C in the set [ 0.001, 0.01, 0.1, 1, 10, 100, 1000 ] . The best parameter selected using a validation set is used to retrain the models on the entire training set. For our proposed rewiring approach, we compute the pairwise similarities between classes using the entire training dataset. Additionally, we use the liblinear solver11 for optimization in all the experiments. The source code is made available at our website: http://cs.gmu.edu/\u223cmlbio/TaxMod"}, {"heading": "4 Discussion and Results", "text": "4.1 Case Study To understand the quality of different hierarchical structures (expert-defined, clustered, flattened and rewired) for the newsgroup dataset shown in Figure 1, we perform top-down HC using each of the hierarchy, separately. The dataset has 11269 training instances, 7505 test instances and 20 classes. We evaluate each of the hierarchy by randomly selecting five different sets of training and test split in the same ratio as original dataset.\nThe results of classification performance is shown in Table 4. We can see that using these modified hierarchies substantially improves the classification performance in comparison to the baseline expert-defined hierarchy. On comparing the clustered, flattened and proposed rewired hierarchies, the classification performance obtained from using the rewired hierarchy is found to be significantly better than the flattened and clustered hierarchy. This is because rewired hierarchy can resolve inconsistencies by grouping together the classes from different hierarchical branches."}, {"heading": "4.2 Evaluating Rewiring Approaches", "text": "4.2.1 Performance based on Flat Metrics Table 5 shows the \u00b5F1 and MF1 performance comparison of rewiring approaches against expert-defined, clustered and flattened hierarchy baselines. The rewiring approaches consistently outperform other baselines for all the datasets across all metrics. For image datasets, the relative performance improvement is larger with performance improvement up to \u223c11% using MF1 scores in comparison\n11http://www.csie.ntu.edu.tw/\u223ccjlin/liblinear/\nto the baseline TD-LR method. In Table 5 results with p-values < 0.01 and < 0.05 are denoted by N and M, respectively. We compute the sign-test for \u00b5F1 [24] and non-parametric wilcoxon rank test for MF1 comparing the F1 scores obtained per class for the rewiring methods against the best baseline i.e., Global-INF. Both, the rewiring approaches significantly outperform the Global-INF method across the different datasets.\nThe proposed rewHier approach shows competitive classification performance in comparison to the T-Easy approach. For smaller datasets, the T-Easy approach has better performance because it searches for the optimal hierarchy in the hierarchical space. However, the main drawback of the T-Easy approach is that it requires computationally expensive learning-based evaluations for reaching the optimal hierarchy making it intractable for large, real-world classification benchmarks such as DMOZ (See detailed discussion in Runtime Comparison).\n4.2.2 Performance based on Hierarchical Metrics Hierarchical evaluation metrics such as hF1 computes errors for misclassified examples based on the definition of a defined hierarchy. Table 6 shows the hF1 score for the best baseline method, Global-INF and the rewiring methods evaluated over the original and the modified hierarchy. The rewiring methods shows the best performance for all the datasets because it is able to restructure the hierarchy based on the dataset that is better suited for classification.\n4.2.3 Runtime Comparison In Table 7 we compare the training times of the different models. For training, we learn the models in parallel for different classes using multiple compute nodes which are then combined to obtain the final runtime. For our proposed rewiring approach we also compute the similarity between different classes in parallel. We can see from Table 7 that TD-LR takes the the least time as there is no overhead associated with modifying the hierarchy; followed by the Global-INF model which requires retraining of models after hierarchy flattening. Rewiring approaches are most expensive because of the compute intensive task of either performing similarity computation in our proposed approach or multiple hierarchy evaluations using the T-Easy approach. The T-Easy method takes the longest time due to large number of expensive hierarchy evaluations after each elementary operations until the optimal hierarchy is reached. Table 8 shows the number of elementary operations executed using the T-Easy and the rewHier approach. We can see that T-Easy approach performs large number of operations even for smaller datasets (for e.g., 412 operations for IPC datasets in comparison to 42 for the rewHier).\n4.3 Effect of varying the Training Size Figure 3 shows the MF1 comparison of rewiring approaches with Global-INF approach on CLEF and DMOZ-SMALL datasets with varying percentage of training size. For both datasets we can see that rewiring approaches outperform the flattening approaches. For the CLEF dataset with smaller training percentage, the rewHier approach has better performance. The reason for this behavior might be the over-fitting of the optimal hierarchy with the training data in case of T-Easy approach, which results in poor performance on unseen examples. For training dataset with enough examples as expected, T-Easy method gives the best performance but at the cost of expensive runtime. We cannot run T-Easy on the larger DMOZ datasets.\n4.4 Threshold (\u03c4) Selection to Group Similar Classes Pairs Figure 4 shows the sorted (descending order) class pairs cosine similarity scores for DMOZ-SMALL dataset. We can see that similarity scores become nearly constant after 1000 pairs (and drops further after 6000, not shown in the Figure) that does not provide any interesting similar classes grouping information for taxonomy modification. As such, for this dataset choosing threshold as the similarity score of the 1000-th class pair is a reasonable choice. A similar approach to determine the threshold is applied for other datasets as well.\n4.5 Improvement over Flat and State-of-the-art Approaches. Figure 5 presents the percentage of classes improved for TD-LR and HierCost HC approaches in comparison to the flat approach on DMOZ datasets containing rare categories i.e., less than 10 training examples. For the DMOZ-2010 and DMOZ-2012 benchmarks we use a separate held out test dataset since, we do not have the true labels for the provided test set used for the online competition. From Figure 5 we observe that both the HC approaches outperforms the flat approach irrespective of the hierarchy being used. Rare categories benefit from the utilization of hierarchical relationships, and using the hierarchy improves the accuracy of HC. Moreover, use of rewHier to train the TD-LR and HierCost approaches improves the classification performance in comparison to using the expert-defined hierarchy. Further, the HierCost approach consistently outperforms the TD-LR approach because HierCost penalizes the misclassified instances based on the assignment within the hierarchy. Table 9 gives the more comprehensive results over all classes and Figure 6 gives MF1 and hF1 improvements for rare categories classes.\nIn terms of prediction runtime, the TD approaches outperform the flat and HierCost approaches. The flat and HierCost models invoke all the classifiers trained for the leaf nodes to make a prediction decision. For the DMOZ2012 dataset, the flat and HierCost approaches take \u223c220 minutes for predicting the labels of test instances, whereas the TD-LR model is 3.5 times faster on the same hardware configuration."}, {"heading": "5 Related Work", "text": "Our work is closely related to the rewiring approach developed in Tang et al. [11], where the expert-defined hierarchy is gradually modified. Iteratively, a subset of the hierarchy is modified and evaluated for classification performance improvement using the HC learning algorithm. Modified changes are retained if the performance results improve; otherwise the changes are discarded and the process is repeated. This repeated procedure of hierarchy modification continues until the optimal hierarchy is reached. Expensive evaluation at each step makes this approach intractable for large-scale datasets. Another drawback of this approach is deciding which branch of the hierarchy to explore first (for modification) and which elementary operation (promote, demote, merge) to apply at each step. Other work in similar direction can be found in [12,19].\nEarlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18]. In other work, learning based approach have been proposed [16],\nwhere nodes to flatten are decided based on classification performance improvement on a validation set. This approach although useful for smaller datasets, is not scalable due to the expensive evaluation process after each node removal. Recently, Naik et al. [7] proposed a taxonomy adaptation where some nodes are intelligently flattened based on empirically defined cut-off threshold and objective function values computed at each node. Hierarchy modification using this approach is scalable and beneficial for classification and has been theoretically justified [25].\nOther approaches towards hierarchy modification involves generating hierarchy from scratch, ignoring the expert-defined hierarchy. These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27]. Constructing hierarchy using clustering approaches is not popular due to its sensitivity to predefined parameters such as number of levels."}, {"heading": "6 Conclusion and Future Work", "text": "We propose a data-driven filter based rewired approach for hierarchy modification that is more suited for HC. Our method is robust and can be adapted to work in conjunction with any state-of-the-art HC approaches in the literature that utilize hierarchical relationships. Irrespective of the classifiers being trained, our modified hierarchy consistently gives better performance over use of clustering or flattening to modify the original hierarchy. In comparison to previous rewiring approaches, our method gives competitive results with much better runtime performance that allow HC approaches to scale to significantly large datasets (e.g., DMOZ). Further, experiments on datasets with skewed distribution shows the effectiveness of our proposed method in comparison to flat and state-of-the-art methods, especially for classes with rare categories. In future, we plan to study the effect of our method in conjunction with feature selection and other non-linear classification methods."}, {"heading": "Acknowledgement", "text": "NSF grant #203337 and #202882 to Huzefa Rangwala."}], "references": [{"title": "Hierarchical document categorization with support vector machines", "author": ["L. Cai", "T. Hofmann"], "venue": "In CIKM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Hierarchically classifying documents using very few words", "author": ["D. Koller", "M. Sahami"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Improving text classification by shrinkage in a hierarchy of classes", "author": ["A. McCallum", "R. Rosenfeld", "T. Mitchell", "A. Ng"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Hierarchical classification of web content", "author": ["S. Dumais", "H. Chen"], "venue": "In ACM SIGIR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Hierarchical text classification and evaluation", "author": ["A. Sun", "E. Lim"], "venue": "In ICDM, pages 521\u2013528,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Site abstraction for rare category classification in large-scale web directory", "author": ["T. Liu", "H. Wan", "T. Qin", "Z. Chen", "Y. Ren", "W. Ma"], "venue": "In WWW: Special interest tracks & posters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Inconsistent node flattening for improving top-down hierarchical classification", "author": ["A. Naik", "H. Rangwala"], "venue": "In IEEE DSAA,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Hierarchical document classification using automatically generated hierarchy. JIIS", "author": ["T. Li", "S. Zhu", "M. Ogihara"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Hierarchical classification via orthogonal transfer", "author": ["L. Xiao", "D. Zhou", "M. Wu"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A study of hierarchical and flat classification of proteins", "author": ["A. Zimek", "F. Buchwald", "E. Frank", "S. Kramer"], "venue": "IEEE/ACM TCBB,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Acclimatizing taxonomic semantics for hierarchical content classification", "author": ["L. Tang", "J. Zhang", "H. Liu"], "venue": "In ACM SIGKDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Hierarchy evolution for improved classification", "author": ["X. Qi", "B. Davison"], "venue": "In CIKM, pages 2193\u20132196,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Hiercost: Improving large scale hierarchical classification with cost sensitive learning", "author": ["A. Charuvaka", "H. Rangwala"], "venue": "In ECML PKDD,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Maximum-margin framework for training data synchronization in large-scale hierarchical classification", "author": ["R. Babbar", "I. Partalas", "E. Gaussier", "MR. Amini"], "venue": "In Neural Information Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Flatten hierarchies for large-scale hierarchical text categorization", "author": ["X. Wang", "B. Lu"], "venue": "In ICDIM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On flat versus hierarchical classification in large-scale taxonomies", "author": ["R. Babbar", "I. Partalas", "E. Gaussier", "M. Amini"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Automatically learning document taxonomies for hierarchical classification", "author": ["K. Punera", "S. Rajan", "J. Ghosh"], "venue": "In WWW: Special interest tracks & posters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Improving hierarchical svms by hierarchy flattening and lazy classification", "author": ["H Malik"], "venue": "In Large-Scale HC Workshop of ECIR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Improving taxonomies for large-scale hierarchical classifiers of web docs", "author": ["K. Nitta"], "venue": "In CIKM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "The challenges of clustering high dimensional data", "author": ["M. Steinbach", "L. Ert\u00f6z", "V. Kumar"], "venue": "In New Directions in Statistical Physics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Recursive regularization for large-scale classification with hierarchical & graphical dependencies", "author": ["S. Gopal", "Y. Yang"], "venue": "In ACM SIGKDD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Hierarchical annotation of medical images", "author": ["I. Dimitrovski", "D. Kocev", "S. Loskovska", "S. D\u017eeroski"], "venue": "Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Hierarchical classification of diatom images using predictive clustering trees", "author": ["I. Dimitrovski", "D. Kocev", "S. Loskovska", "S. D\u017eeroski"], "venue": "Ecological Informatics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "In ACM SIGIR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Discriminative learning of relaxed hierarchy for large-scale visual recognition", "author": ["T. Gao", "D. Koller"], "venue": "In ICCV, pages 2072\u20132079,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "On the merits of building categorization systems by supervised clustering", "author": ["C. Aggarwal", "S. Gates", "P. Yu"], "venue": "In SIGKDD,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "A practical web-based approach to generating topic hierarchy for text segments", "author": ["S. Chuang", "L. Chien"], "venue": "In CIKM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 1, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 2, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 3, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 4, "context": "In the past, researchers have demonstrated the usefulness of hierarchies for classification and have obtained promising results [1\u20135].", "startOffset": 128, "endOffset": 133}, {"referenceID": 5, "context": "Utilizing the hierarchical structure has been shown to improve the classification performance for rare categories as well [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Top-down HC methods that leverage the hierarchy during the learning and prediction process are effective approaches to deal with large-scale problems [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "Though computationally efficient, these methods have higher number of misclassifications due to error propagation [7].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "For several benchmarks, the HC approaches are outperformed by flat classifiers that ignore the hierarchy [9,10].", "startOffset": 105, "endOffset": 111}, {"referenceID": 9, "context": "For several benchmarks, the HC approaches are outperformed by flat classifiers that ignore the hierarchy [9,10].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "using various methods: (b) Agglomerative clustering with cluster cohesion to restrict the height to original height [8] (c) Global-INF flattening method [7] (d) Proposed rewiring method.", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "using various methods: (b) Agglomerative clustering with cluster cohesion to restrict the height to original height [8] (c) Global-INF flattening method [7] (d) Proposed rewiring method.", "startOffset": 153, "endOffset": 156}, {"referenceID": 10, "context": "\u2022 We propose an efficient data-driven filter based rewiring approach for hierarchy modification which unlike previous wrapper based approaches [11,12] does not require multiple, expensive computations.", "startOffset": 143, "endOffset": 150}, {"referenceID": 11, "context": "\u2022 We propose an efficient data-driven filter based rewiring approach for hierarchy modification which unlike previous wrapper based approaches [11,12] does not require multiple, expensive computations.", "startOffset": 143, "endOffset": 150}, {"referenceID": 13, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 165, "endOffset": 168}, {"referenceID": 15, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 214, "endOffset": 218}, {"referenceID": 7, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 265, "endOffset": 268}, {"referenceID": 16, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 310, "endOffset": 314}, {"referenceID": 10, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 361, "endOffset": 365}, {"referenceID": 11, "context": "Modification Method Approach Type Scalable Margin-based modification [14] Flattening Filter X Level flattening [15] Flattening Filter X Inconsistent node flattening [7] Flattening Filter X Learning based algorithm [16] Flattening Wrapper \u00d7 Agglomerative clustering [8] Clustering Wrapper \u00d7 Divisive clustering [17] Clustering Wrapper \u00d7 Optimal hierarchy search [11] Rewiring Wrapper \u00d7 Genetic based algorithm [12] Rewiring Wrapper \u00d7", "startOffset": 409, "endOffset": 413}, {"referenceID": 12, "context": "\u2022 The modified hierarchy can be used with any hierarchical classification approaches like top-down HC or stateof-the-art approaches incorporating hierarchical relationships [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 13, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 14, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 15, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 17, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 91, "endOffset": 105}, {"referenceID": 10, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 253, "endOffset": 263}, {"referenceID": 11, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 253, "endOffset": 263}, {"referenceID": 18, "context": "These approaches can be broadly categorized into two categories: (i) Flattening approaches [7, 14\u201316, 18] where some of the identified inconsistent nodes (based on error rate, classification margins) are flattened (removed) and (ii) Rewiring approaches [11,12,19] where parent-child relationships within the hierarchy are modified to improve the classification performance.", "startOffset": 253, "endOffset": 263}, {"referenceID": 7, "context": "Clustering based methods have also been adapted in some of these studies [8,17] where consistent hierarchy is generated from scratch using agglomerative or divisive clustering algorithms.", "startOffset": 73, "endOffset": 79}, {"referenceID": 16, "context": "Clustering based methods have also been adapted in some of these studies [8,17] where consistent hierarchy is generated from scratch using agglomerative or divisive clustering algorithms.", "startOffset": 73, "endOffset": 79}, {"referenceID": 10, "context": "Hierarchy generated using clustering completely ignores the expert-defined hierarchy information, which contains valuable prior knowledge for classification [11].", "startOffset": 157, "endOffset": 161}, {"referenceID": 10, "context": "2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning", "startOffset": 54, "endOffset": 66}, {"referenceID": 11, "context": "2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning", "startOffset": 54, "endOffset": 66}, {"referenceID": 18, "context": "2 Proposed Rewiring Approach Wrapper based approaches [11, 12, 19] modify the hierarchy by making one or few changes, which are then evaluated for classification performance improvement using the HC learning", "startOffset": 54, "endOffset": 66}, {"referenceID": 19, "context": "Pairwise cosine similarity is used as the similarity measure in our experiments because it is less prone to the curse of dimensionality [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "rewire[1] = 1; /* check if rewiring is needed for s (1) i */ rewire[2] = 1; /* check if rewiring is needed for s (2) i */ /* Inconsistent pair check */", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "rewire[1] = 1; /* check if rewiring is needed for s (1) i */ rewire[2] = 1; /* check if rewiring is needed for s (2) i */ /* Inconsistent pair check */", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "if \u03c0(s (1) i ) 6= \u03c0(s (2) i ) then /* check similarity to all siblings */ foreach j \u2208 \u03b6(s i ) do if ( (j, s (2) i ) or (s (2) i , j) ) / \u2208 S then rewire[2] = 0; break; end", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "end foreach j \u2208 \u03b6(s i ) do if ( (j, s (1) i ) or (s (1) i , j) ) / \u2208 S then rewire[1] = 0; break; end", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "end if (rewire[1] == 0) and (rewire[2] == 0) then /* perform node creation */ Nnew = \u03c6 /* create new node */ [HM ] = NC(Nnew\u2192lca(si), si\u2192Nnew,HM ); /* lca denotes lowest common ancestor */", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "end if (rewire[1] == 0) and (rewire[2] == 0) then /* perform node creation */ Nnew = \u03c6 /* create new node */ [HM ] = NC(Nnew\u2192lca(si), si\u2192Nnew,HM ); /* lca denotes lowest common ancestor */", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "else if (rewire[1] == 1) then [HM ] = PCRewire(s i \u2192\u03c0(s (2) i ),HM ); else [HM ] = PCRewire(s i \u2192\u03c0(s (1) i ),HM ); end", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "State-of-the-art HC classification approaches embed the parent-child relationships from the hierarchy either, within the regularization term [21], referred by HR-LR (or HR-SVM) or the loss term, referred by HierCost [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 12, "context": "State-of-the-art HC classification approaches embed the parent-child relationships from the hierarchy either, within the regularization term [21], referred by HR-LR (or HR-SVM) or the loss term, referred by HierCost [13].", "startOffset": 216, "endOffset": 220}, {"referenceID": 20, "context": "The intuition behind Hierarchy Regularized Logistic Regression (HR-LR) [21] approach is that data-sparse child nodes benefit during training from data-rich parent nodes, and this has been shown to achieve the best performance on standard HC benchmarks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "In case of HierCost [13], a cost-sensitive learning approach was adapted.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "In this paper, we use logistic regression (LR) as the underlying base model for training [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "CLEF [22] and DIATOMS [23] are image datasets and the rest are text datasets.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "CLEF [22] and DIATOMS [23] are image datasets and the rest are text datasets.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "For evaluation, we have restricted the height of clustered hierarchy to the original height by flattening using cluster cohesion [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "Global Inconsistent Node Flattening (Global-INF) [7]: Hierarchy is modified by flattening (removing) the inconsistent nodes based on optimal optimization objective value obtained at each node (eq.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "Optimal Hierarchy Search [11]: Optimal hierarchy is identified in the hierarchical space by gradually modifying the expert-defined hierarchy using elementary operations \u2013 promote, demote and merge.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "In the original paper [11], the largest evaluated dataset has 244 classes and 15795 instances.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "3 State-of-the-art Cost-sensitive Learning [13] Similar to flat method but with cost value associated with each instance in the loss function as shown in eq.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "TD-LR Clustering [8] Flattening [7] Proposed Metric Agglomerative Global-INF rewHier [Figure 1(a)] [Figure 1(b)] [Figure 1(c)] [Figure 1(d)] \u03bcF1(\u2191) 77.", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "TD-LR Clustering [8] Flattening [7] Proposed Metric Agglomerative Global-INF rewHier [Figure 1(a)] [Figure 1(b)] [Figure 1(c)] [Figure 1(d)] \u03bcF1(\u2191) 77.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "Dataset Evaluation TD-LR Agglomerative Flattening Rewiring Methods Metrics Clustering [8] Global-INF [7] T-Easy [11] rewHier", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Dataset Evaluation TD-LR Agglomerative Flattening Rewiring Methods Metrics Clustering [8] Global-INF [7] T-Easy [11] rewHier", "startOffset": 101, "endOffset": 104}, {"referenceID": 10, "context": "Dataset Evaluation TD-LR Agglomerative Flattening Rewiring Methods Metrics Clustering [8] Global-INF [7] T-Easy [11] rewHier", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Dataset Hierarchy Flattening Rewiring Methods used Global-INF T-Easy [11] rewHier", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "Dataset Baseline Flattening Rewiring Methods TD-LR Global-INF T-Easy [11] rewHier", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "T-Easy [11] 52 156 412 (promote, demote, merge)", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "We compute the sign-test for \u03bcF1 [24] and non-parametric wilcoxon rank test for MF1 comparing the F1 scores obtained per class for the rewiring methods against the best baseline i.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "[11], where the expert-defined hierarchy is gradually modified.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Other work in similar direction can be found in [12,19].", "startOffset": 48, "endOffset": 55}, {"referenceID": 18, "context": "Other work in similar direction can be found in [12,19].", "startOffset": 48, "endOffset": 55}, {"referenceID": 13, "context": "Earlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18].", "startOffset": 144, "endOffset": 154}, {"referenceID": 14, "context": "Earlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18].", "startOffset": 144, "endOffset": 154}, {"referenceID": 17, "context": "Earlier studies focused on flattening based approaches where some level or nodes are selectively flattened (removed) based on certain criterion [14,15,18].", "startOffset": 144, "endOffset": 154}, {"referenceID": 15, "context": "In other work, learning based approach have been proposed [16],", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "[7] proposed a taxonomy adaptation where some nodes are intelligently flattened based on empirically defined cut-off threshold and objective function values computed at each node.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Hierarchy modification using this approach is scalable and beneficial for classification and has been theoretically justified [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}, {"referenceID": 16, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}, {"referenceID": 25, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}, {"referenceID": 26, "context": "These approaches exploit hierarchical clustering algorithms for generating the hierarchy [8,17,26,27].", "startOffset": 89, "endOffset": 101}], "year": 2016, "abstractText": "Hierarchical Classification (HC) is a supervised learning problem where unlabeled instances are classified into a taxonomy of classes. Several methods that utilize the hierarchical structure have been developed to improve the HC performance. However, in most cases apriori defined hierarchical structure by domain experts is inconsistent; as a consequence performance improvement is not noticeable in comparison to flat classification methods. We propose a scalable data-driven filter based rewiring approach to modify an expertdefined hierarchy. Experimental comparisons of top-down HC with our modified hierarchy, on a wide range of datasets shows classification performance improvement over the baseline hierarchy (i.e., defined by expert), clustered hierarchy and flattening based hierarchy modification approaches. In comparison to existing rewiring approaches, our developed method (rewHier) is computationally efficient, enabling it to scale to datasets with large numbers of classes, instances and features. We also show that our modified hierarchy leads to improved classification performance for classes with few training samples in comparison to flat and state-of-the-art HC approaches. Source code available for reproducibility at: www.cs.gmu.edu/\u223cmlbio/TaxMod Keywords\u2014 Top-Down Hierarchical Classification, Rewiring, Clustering, Flattening", "creator": "LaTeX with hyperref package"}}}