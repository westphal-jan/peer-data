{"id": "1603.01520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Optimized Polynomial Evaluation with Semantic Annotations", "abstract": "in this course we discuss how semantic annotations can be used thereby introduce content and information of as underlying imperative code ways enable compilers to produce code transformations that will enable better performance. by using this approaches not directly good performance is achieved, but also better programmability, maintainability and portability across different hardware architectures. to exemplify this we will use polynomial techniques of different degrees.", "histories": [["v1", "Fri, 4 Mar 2016 16:13:24 GMT  (101kb,D)", "https://arxiv.org/abs/1603.01520v1", null], ["v2", "Thu, 10 Mar 2016 14:17:38 GMT  (102kb,D)", "http://arxiv.org/abs/1603.01520v2", null], ["v3", "Fri, 11 Mar 2016 11:31:59 GMT  (102kb,D)", "http://arxiv.org/abs/1603.01520v3", "Part of the Program Transformation for Programmability in Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March 2016, 7 pages, LaTeX, 4 PNG figures"]], "reviews": [], "SUBJECTS": "cs.PL cs.CL", "authors": ["daniel rubio bonilla", "colin w glass", "jan kuper"], "accepted": false, "id": "1603.01520"}, "pdf": {"name": "1603.01520.pdf", "metadata": {"source": "CRF", "title": "Optimized Polynomial Evaluation with Semantic Annotations", "authors": ["Daniel Rubio Bonilla", "Colin W. Glass", "Jan Kuper"], "emails": ["rubio@hlrs.de", "glass@hlrs.de", "j.kuper@utwente.nl"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors B.1.4 [Microprogram Design Aids]: Languages and compilers\nGeneral Terms Programming, Polynomial, Optimization, Performance\nKeywords programming models, polynomial functions, code optimization"}, {"heading": "1. Introduction", "text": "Code optimization is the process that tries to improve the code by making it consume less resources such us CPU cycles, memory, or communication in distributed systems. The word optimization comes from the root \u201coptimal\u201d (which comes from the Latin word optimus), meaning that it cannot be better. But it is very rare that this process can produce really optimal code. The optimized code can in most cases be optimal for a given use in a determined hardware system. One can often reduce the execution time by making it consume more memory, but in systems where the memory space is scare it might be beneficial to code a slower algorithm which reduces memory usage. In summary, in most cases there is \u201cno size that fits all\u201d code that executes optimal in all cases. Software developers that write code for generic systems must adjust their code to perform reasonably well in most common situations.\nThe code optimization process can also take big amount of time, that can be thought as a cost; it is possible that beyond a certain level of optimization it is not cost effective to invest more time in improving the execution performance. Software developer time is not the only price to pay for code optimization as often the process will lead to source code that is obfuscated, more difficult to maintain or modify and\nreduces the portability across different hardware systems. Another drawback is that it will also reduce the opportunities to reuse certain parts of the code.\nTo deal with all this factors the ideal situation would be when the source code of the application resembles as close as possible the mathematical formulation of the algorithm, leaving the optimization of the code to the compiler. Unfortunately current compilers are not able to make the same level of aggressive transformations on a near to mathematics code because it is not aware of the intention of the computation (i.e. what is the final result that we want to achieved). For this reason the optimizations are limited to a small set of optimizations that guarantee that the computation is correct.\nIntroducing mathematical information to the compiler can allow it to perform optimizations at the algorithmic level, similarly to what a software developer might do, to better exploit the characteristics of the underlying hardware. After this step the usual set of current optimizations can be applied.\nIn this paper we will show how the C resembling the formulation of equivalent polynomial functions affects the execution time and we will discuss how the mathematical information can be introduced through semantic annotations into programming models so that it can be exploited by compilers, allowing them to perform this set of algorithmic optimizations."}, {"heading": "2. Polynomial Representations", "text": "In mathematics, a polynomial is an expression consisting of variables and coefficients that involves only the operations of addition, subtraction, multiplication and non-negative integer exponents. Polynomials appear in a wide range of problem complexity of different areas. In mathematics they are used in calculus and numerical analysis to approximate to other functions, or in advanced mathematics they are used to construct polynomial rings and algebraic varieties, central concepts in algebra. They are also often used in physics and chemistry to, for example, describe the trajectory of projectiles, express equations such as the ideal gas law or, polynomial integrals (the sums of many polynomials), can be used to express energy, inertia and voltage difference, to name a few applications. Other natural sciences also use polynomial, like the construction of astronomical and meteoro-\nPROHA\u201916, March 12, 2016, Barcelona, Spain 1 2016/3/14\nar X\niv :1\n60 3.\n01 52\n0v 3\n[ cs\n.P L\n] 1\n1 M\nar 2\n01 6\nlogical models. As polynomials are very useful to express curves, thus engineers use them to design roads, bridges, railways lines, and roller coasters. Combinations of polynomial functions can be used in more sophisticated analysis to retrieve more data, for this, they are applied in the field of economics to do cost analysis.\nMathematical equations can often be expressed under different formulations that are functionally equivalent, but that if taken directly into code will be executed in a different way, resulting in a different resource usage of the available hardware. To illustrate this we will present a polynomial equations and transform its mathematical representation in different steps. The resulting different, but equivalent, functions will be directly coded in C language and then evaluated against each other.\nLet the quartic polynomial function (degree four) be given by:\nf0(x) = A4x 4 +A3x 3 +A2x 2 +A1x 1 +A0\nIn Figure 1 the execution structure, i.e. data/work flow, of this polynomial function is graphically represented: xi is represented by a sequence of multiplications with x, starting from x \u00b7 x, and the result is multiplied with ai. Then, the results for every i are added.\nListing 1 shows the source code implementation in C that matches the execution model of the equation f0(x).\nListing 1. f0(x) C implementation #define A0 ...\n...\nint polyCalc(int x) {\nint res;\nres = A4*x*x*x*x + A3*x*x*x + A2*x*x + A1*x +\nA0;\nreturn res;\n}\nHowever, this execution model is rather inefficient, as it will be shown in the benchmark section, as the outcomes of all terms xi are calculated separately. Instead, we may cal-\nculate the corresponding values xi = xi incrementally:\nx1 = x, x2 = xx1, x3 = xx2, x4 = xx3\nand then define an improved version of the polynomial function:\nf1(x) = A4x4 +A3x3 +A2x2 +A1x1 +A0\nFigure 2 shows this second execution model where the xi\nterms are being reused.\nAnd Listing 2 shows the source code that represents this execution model directly, with the extra variable x that stores the incrementally exponentiation.\nListing 2. f1(x) C implementation #define A0 ...\n...\nint polyCalc(int x) {\nint res, _x;\nres = A0;\nres += A1*x;\n_x = x*x;\nres += A2*_x;\n_x *=x;\nres += A3*_x;\n_x *=x;\nres += A4*_x;\nreturn res;\n}\nBut it can still done better. The function can also be rewritten to further reduce the number of multiplications. This is the function f2(x) that also specifies an execution model, given in Figure 3. This model is apparently more efficient than the previous ones, in the sense that fewer operations are required, although later we will test this empirically.\nf2(x) = (((A4x+A3)x+A2)x+A1)x+A0\nAnd its execution model is expressed graphically in Figure 3.\nPROHA\u201916, March 12, 2016, Barcelona, Spain 2 2016/3/14\nThe direct implementation of f2(x) in C code is shown in Listing 3.\nListing 3. f2(x) C implementation #define A0 ...\n...\nint polyCalc(int x) {\nint res;\nres = (((A4*x + A3)*x + A2)*x + A1)*x + A0;\nreturn res;\n}\nAs the equivalence of f0(x), f1(x), and f2(x) can be proven and the correspondence between the function definitions, the execution model and the derived C code is direct, this also means that the equivalence of the corresponding execution model and C code is guaranteed (as long as we do not care for the numeric differences that can appear due to the lost precision and rounding of floating point variables or cater for possible overflows in integer operations when executing on CPUs)."}, {"heading": "3. Code Analysis", "text": "The three mathematical-equivalent formulations have a different number of operations to be performed when directly implemented on C code. The number of operations has a direct impact on the performance on configurable hardware, such us FPGAs (Kuper & Wester 2104) but the situation might be different on CPUs due to their complex behavior and diversity of architectures. The impact of a particular code is very difficult to predict in the case of superscalar architectures with out-of-order execution. In this chapter we will analyze the theoretical number of computations to be performed for each version and in next chapters we will measure the performance empirically.\nThe number of operations can be directly counted from the mathematical representations or the C code previously shown for each version. The summary of theoretical operations is summarized in Table 1. As it can be observed the number of additions is constant but the number of multiplications has been significantly reduced with each new representation of the polynomial function.\nWhen generating the binary code that is later going to be executed on a CPU, compilers do not translate directly the C instructions but perform a set of generic and architecturespecific optimizations that aim to improve the performance\nover a naive (or direct) code compilation. In this work we have explored the code generated by GCC and LLVM compilers using aggressive optimizations, -O3, and enabling integer operation re-ordering, -fstrict-overflow. This last flag allows the compiler to assume strict signed overflow rules. For C and C++ this means that overflow when doing arithmetic with signed numbers is undefined, which means that the compiler may assume that it does not happen. This permits various optimizations, for example, the compiler assumes that an expression like i + 10 > i is always true for signed i. This assumption is only valid if signed overflow is undefined, as the expression is false if i+10 overflows when using twos complement arithmetic. For our case it means that the compiler can change, re-order or merge the operations with signed integers (GCC 4.9.2 flags 2015).\nTable 2 reflects the number of multiplications and additions that were found in the binary code, after examining the assembler code, generated by GCC and LLVM for different versions of the C code. If we compare this data with Table 1, which contains the number of operations explicitly written in the C code, we can observe that GCC has preserved all the operations while LLVM has been reduced the amount of multiplications for f0(x) and f1(x).\nFor better understanding of the behavior of the compilers we decided to try the same approaches with larger polynomials, in this case of degree 9. For this we can define the polynomial function\ng0(x) = A9x 9 + ...+A1x 1 +A0\nand we have produce two equivalent representations,\ng1(x) = A9x9 + ...+A1x1 +A0\nwhere\nx1 = x, x2 = xx1, ..., x9 = xx8\nPROHA\u201916, March 12, 2016, Barcelona, Spain 3 2016/3/14\nand finally the version with least operations\ng2(x) = ((A9x+A8)x+ ...+A1)x+A0\nWe are not listing the C code of g0(x), g1(x) and g2(x) functions as these are mechanical extensions over the previously shown for f0(x), f1(x) and f2(x) shown in Listings 1, 2 and 3 respectively. The total amount of operations found in these expressions and their direct C implementation is given in Table3, where we can observe that the additions are kept equal but the multiplications can be greatly reduced.\nAs with the 4th degree polynomial function we analyzed the binary generated by the compilers. Similarly to the previous case GCC made a direct implementation of the C code but LLVM reduced the number of multiplications, interestingly it was able to produce a binary code with less multiplications for g0(x) but for g1(x). But for both compilers the code with less operations comes from the code of g2(x)."}, {"heading": "3.1 LLVM Optimizations", "text": "According to Tables 1 and 3 the LLVM compiler is able to reduce the number of operations on the original implementation of both polynomial functions f0(x) and g0(x) as well as of the modified versions f1(x) and g1(x). In this chapter we analyze the generated code by LLVM and compare it to the generated for the code f2(x) and g2(x)."}, {"heading": "3.1.1 Polynomial Function Degree 4", "text": "The assembler generated by LLVM for the code of the implementation of f0(x) is shown in Listing 4.\nListing 4. LLVM ASM for f0(x) p o l y C a l c :\n. . . i m u l l %edi , %r8d ; i 1 movl %edi , %eax ; i 2 i m u l l %eax , %eax ; i 3 i m u l l %eax , %eax ; i 4\ni m u l l %r9d , %eax ; i 5 a d d l %ecx , %r8d ; i 6 i m u l l %edi , %r8d ; i 7 a d d l %edx , %r8d ; i 8 i m u l l %edi , %r8d ; i 9 l e a l (%rax ,% r s i ) , %eax ; i 1 0 a d d l %r8d , %eax ; i 1 1 . . . r e t q\nThe data dependency between registers and operations have been graphically depicted in Figure 4.\nFrom the assembler code and the dependency diagram we can observe that there are two independent execution branches that merge in the last operation. The left branch calculates A3x3+A2x2+A1x1 and the right branch calculates A4x4+A0, adding them together in the last instruction. The assembler lines i1 and i6 to i9 correspond to the left branch while the lines i2 to i5 and i10 correspond to the right branch of the graph.\nIt is interesting to notice that the maximum path length of this assembler code is 6 instructions, while f2(x) shows a continuous hard dependency on the previous calculated results, making it a single branch of 8 instructions length. At this point one could inevitable think that LLVM could be trying to exploit superscalar execution models that could take advantage of shorter path even when the total amount of instructions is larger."}, {"heading": "3.1.2 Polynomial Function Degree 10", "text": "The assembler generated by LLVM for the code of the implementation of g0(x) is shown in Listing 5.\nPROHA\u201916, March 12, 2016, Barcelona, Spain 4 2016/3/14\nListing 5. LLVM ASM for g0(x) p o l y C a l c :\ni m u l l %edi , %r8d movl %edi , %eax i m u l l %eax , %eax movl %eax , %r11d i m u l l %r11d , %r11d i m u l l %r11d , %r9d movl 24(% rsp ) , %r10d i m u l l %r11d , %r10d i m u l l %edi , %eax i m u l l %eax , %eax movl 32(% rsp ) , %ebx i m u l l %eax , %ebx i m u l l 40(% rsp ) , %eax i m u l l %r11d , %r11d movl 48(% rsp ) , %ebp i m u l l %r11d , %ebp i m u l l 56(% rsp ) , %r11d a d d l %ecx , %r8d i m u l l %edi , %r8d a d d l %edx , %r10d a d d l %eax , %r10d a d d l %r11d , %r10d a d d l %r8d , %r10d i m u l l %edi , %r10d l e a l (%r9 ,% r s i ) , %eax a d d l %ebx , %eax a d d l %ebp , %eax a d d l %r10d , %eax . . . r e t q\nAs happened with the degree 4 polynomial function, the binary generated by LLVM for this case, contains more operations than the compilation of g2(x) but in different dataindependent branches instead of a single branch. The g2(x) branch has a total of 18 operations while the g0(x) LLVM optimized code longest branch has 12 operations. In the next section of this work we will evaluate the performance of the different generated binaries."}, {"heading": "4. Benchmarks", "text": "Although the analysis of the theoretical computational of the different equivalent polynomial representations and the binary code generated by the different compilers is interesting the most important is to actually run the different binaries and measure its performance. All the benchmarks were run on a Intel i7-4770k CPU, based on the Haswell architecture, with energy saving (sleeping states and throttling disabled) and core and uncore parts locked at 2 GHz and 2x8 GiB of DDR3 RAM at 1600 MHz (CL9). As we have the clock of the CPU completely locked we have just measured the cy-\ncles that were needed to execute each polynomial function (the more cycles needed the slower the code was).\nThe cycles were obtained by reading the Time Stamp Counter (TSC). It is a 64-bit register present in most modern x86 processors that counts the number of cycles since reset and can be read using the instruction RDTSC that returns the TSC value in EDX:EAX registers (or RDTSCP that forces every preceding instruction to be completed in out-of-order CPUs). In Haswell based CPUs the TSC register increments at a constant rate set by the maximum resolved frequency at which the processor is booted, 2 GHz in our case, and is synchronized across all cores of the CPU (in older CPUs it was not incrementing constantly but varying with the core\u2019s actual frequency and values could be different on different cores).\nTable 5 shows the cycles needed to resolve 128 polynomial functions of degree 4 when they were written resembling the formulation of f0(x), f1(x) or f2(x) under LLVM and GCC compilers. Note that the results were obtained in a superscalar CPU, it can execute many instructions in a single cycle, thus the amount of cycles used can be lower than the total amount of instructions.\nThe results show that the code that resembles f2(x) formulation of the polynomial function performs the best in both compiler with LLVM being faster by a 1%. LLVM optimizations on f0(x) make the code perform much better that GCC\u2019s versions, which did not reduce the number of operations, making it almost as performant as LLVM\u2019s version of f1(x). This answers the question that was arrisen in the previous section. In this case, for a Haswell CPU, having two shorter execution data-independent branches was not faster than the single and larger fully-dependent branch of f2(x). GCC produced code that performed proportionally inverse to the amount of arithmetic instructions, being always slower than LLVM (with a small margin for f1(x) and in particular for f2(x)).\nSimilarly, Table 6 shows the cycles needed to resolve 128 polynomial functions of degree 9 when they were written resembling the formulation of g0(x), g1(x) or g2(x) under LLVM and GCC compilers.\nIn this case the g2(x) version of the code is the fastest for both compilers, being LLVM almost a 3% faster than GCC. The binary generated by GCC was faster the further the number of arithmetic operations were reduced in the code. This is not the same case for LLVM as g0(x) performs better than g1(x). Even if the C implementation of g0(x) has more operations than g1(x), LLVM is able to optimize\nPROHA\u201916, March 12, 2016, Barcelona, Spain 5 2016/3/14\nbetter g0(x) producing a binary with slightly less arithmetic instrucions, thus performing better. As with the degree 4 polynomial function, for LLVM, g2(x) performs better than the optimized g0(x). In Haswell based CPU, the multipleindependent shorter branched g0(x) binary does not perform as good as the single fully-dependnent branch code of g2(x)."}, {"heading": "5. Semantic Annotations", "text": "The code optimizations needed to move from f0(x) to f1(x) and finally to f2(x) are not possible in current compilers because current programming models do not convey enough information to enable such transformations without the risk of violating correctness of behaviour. We can say that compilers are missing information about the intention (in terms of \u201csemantics\u201d) of the programmer and the structure and principle behaviour of the program.\nFor these reasons, projects such as POLCA are currently researching in providing programming models that convey sufficient information to meet the following goals (POLCA 2013):\n\u2022 provide structural (dependencies and operational behaviour) and mathematical information,\n\u2022 enable transformation of the source code, \u2022 allow the toolchain to assess the \u201cappropriateness\u201d of\nthe algorithm and transformations for specific hardware platforms\n\u2022 and maintain programmability and correctness.\nTo enable transformations exposed in this work we need to provide enough information to the compiler so that it \u201cunderstand\u201d the mathematical properties of the code running, by using semantic annotations, and strongly binding them to the C code so that the later can be correctly manipulated, reformulating the original implemented algorithm and code for an equivalent but with better performance.\nA proposal of the annotations applied to the code of Listing 1 is shown in Listing 6. First the ring prop (Ring Properties) pragma annotation declares that, for the function PolyCalc that we assume a number of algebraic laws to hold: associativity of + and \u2217, that 0 and 1 are neutral elements for + and \u2217, that \u2217 distributes over + for int data types. A mathematical ring is one of the fundamental algebraic structures used in abstract algebra. It consists of a set equipped with two binary operations that generalize the arithmetic operations of addition and multiplication. Through this generalization, theorems from arithmetic are\nextended to non-numerical objects such as polynomials, series, matrices and functions. A ring is an abelian group with a second binary operation that is associative, is distributive over the abelian group operation, and has an identity element. By extension from the integers, the abelian group operation is called addition and the second binary operation is called multiplication (E. Noether 1921)(A. Fraenkel 1914).\nIn summary, a ring is a set R equipped with binary operations + and \u00b7 satisfying the following three sets of axioms, called the ring axioms (Bourbaki 1970) (MacLane & Birkhoff 1967) (Lang 2002):\n1. R is an abelian group under addition, meaning that: \u2022 (a + b) + c = a + (b + c) for all a, b, c in R (+ is\nassociative). \u2022 a+ b = b+ a for all a, b in R (+ is commutative). \u2022 There is an element 0 in R such that a+ 0 = a for all a in R (0 is the additive identity).\n\u2022 For each a in R there exists a in R such that a+(a) = 0 (a is the additive inverse of a).\n2. R is a monoid under multiplication, meaning that: \u2022 (a \u00b7b) \u00b7c = a \u00b7(b \u00b7c) for all a, b, c in R (\u00b7 is associative). \u2022 There is an element 1 in R such that a\u00b71 = aand1\u00b7a = a for all a in R (1 is the multiplicative identity).\n3. Multiplication is distributive with respect to addition: \u2022 a \u00b7 (b + c) = (a \u00b7 b) + (a \u00b7 c) for all a, b, c in R (left\ndistributivity). \u2022 (b+ c) \u00b7 a = (b \u00b7 a) + (c \u00b7 a) for all a, b, c in R (right\ndistributivity).\nBy stating that these mathematical properties apply we enable the compiler to be able to manipulate the code instructions accordingly without taking into consideration possible side effects that could make the code not valid under certain conditions (e.g. integer overflow).\nThen the second pragma annotation, math exp (Mathematical Expression), states the mathematical computation that is performed in the PolyCalc function to which it applies. In it the terms that appear share the name with the constants are variables used in the code, being the evaluation of the expression the value of the function itself, i.e. its return value.\nListing 6. f0(x) implementation with semantic annotations #define A0 ...\n...\n#pragma ring_prop (+, 0, -, *, 1) int\n#pragma math_exp (A0 + A1*x + A2*x^2 + A3*x^3 +\nA4*x^4)\nint polyCalc(int x) {\nint res;\nPROHA\u201916, March 12, 2016, Barcelona, Spain 6 2016/3/14\nres = A4*x*x*x*x + A3*x*x*x + A2*x*x + A1*x +\nA0;\nreturn res;\n}\nBy capturing this information, the mathematical expression and the operators properties, and making the compiler aware of it, the compiler may decide to transform the body of the function, modifying the original code resembling the expression f0(x) into code that resembles f1(x) or f2(x)."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper we have shown, by using polynomial equations, that mathematical expressions may have different but equivalent formulations and that we can write code that resembles each of these formulations. The different versions of the code will perform different depending on the compiler used to generate the binary code and the hardware that will execute it. We have also discussed that it is possible to manually search for a good combination of specific and optimized code, for an specific algorithm, and the hardware where it is going to run. But if later the hardware is changed the performance of the code might be worse that the original notoptimize code due to specific hardware optimizations for the old platform that now ruin performance. For example it has been shown that the g1(x) code looks more optimal (and complex) from a theoretical point of view than the original g0(x), and it behaves better when compiled with GCC, but the performance decreases if compiled with LLVM.\nTo address these issues we have proposed the usage of semantic annotations that introduce mathematical information with the aim to allow the compiler to produce optimize code beyond current capabilities from a generic clean code. This will allow to write programs with less bugs, easier to maintain and easier to extend, while at the same time increasing portability. These annotations have to be further tested and extended to be able to address a wider set of mathematical expressions and enable their usability in future compilers.\nRegarding the polynomial functions, we will extend the analysis of the performance effects, in terms of time to completion and energy used, of using floating point data types and not only of integers. These effects should also be measured in other architecture families, such as ARM. It would also be interesting to determine the effects of even larger polynomial degrees and introduce further manually optimize code with independent executable branches trying to exploit superscalar architectures."}, {"heading": "Acknowledgments", "text": "This project has received funding from the European Union\u2019s Seventh Framework Programme under the POLCA project, Grant number 610686."}], "references": [{"title": "Algebra, Chapter 8", "author": ["Nicolas Bourbaki"], "venue": "Springer-Verlag", "citeRegEx": "Bourbaki,? 1970", "shortCiteRegEx": "Bourbaki", "year": 1970}, {"title": "Algebra", "author": ["Saunders MacLane", "Garrett Birkhoff"], "venue": "AMS Chelsea. p. 85", "citeRegEx": "MacLane and Birkhoff,? 1967", "shortCiteRegEx": "MacLane and Birkhoff", "year": 1967}], "referenceMentions": [{"referenceID": 0, "context": "In summary, a ring is a set R equipped with binary operations + and \u00b7 satisfying the following three sets of axioms, called the ring axioms (Bourbaki 1970) (MacLane & Birkhoff 1967) (Lang 2002):", "startOffset": 140, "endOffset": 155}], "year": 2016, "abstractText": "In this paper we discuss how semantic annotations can be used to introduce mathematical algorithmic information of the underlying imperative code to enable compilers to produce code transformations that will enable better performance. By using this approaches not only good performance is achieved, but also better programmability, maintainability and portability across different hardware architectures. To exemplify this we will use polynomial equations of different degrees.", "creator": "LaTeX with hyperref package"}}}