{"id": "1603.04000", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Mar-2016", "title": "Learning Typographic Style", "abstract": "typography lacks a ubiquitous aesthetic form this affects our understanding, perception, and trust via what we read. thousands of different font - faces have been written representing dynamic variations in the artwork. in this paper, we learn the style of a font by analyzing a small subset contains only static letters. from these four letters, we learn two tasks. the first is a discrimination task : adding the basic letters and a weak candidate letter, does the new letter belong to the same font? second, given the four basis letters, can applicants generate all of the other letters with the same characteristics of those in the basis set? we use deep neural networks to address both tasks, quantitatively and qualitatively measure prediction results in a variety of novel manners, and present a thorough investigation of the weaknesses and strengths of the approach.", "histories": [["v1", "Sun, 13 Mar 2016 05:44:57 GMT  (1706kb,D)", "http://arxiv.org/abs/1603.04000v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["shumeet baluja"], "accepted": false, "id": "1603.04000"}, "pdf": {"name": "1603.04000.pdf", "metadata": {"source": "CRF", "title": "Learning Typographic Style", "authors": ["Shumeet Baluja"], "emails": ["shumeet@google.com"], "sections": [{"heading": null, "text": "Keywords: Style Analysis, Typography, Image Generation, Learning"}, {"heading": "1 Introduction", "text": "The history of fonts and typography is vast, originating back at least to fifteenth century Germany with the creation of the movable type press by Johannes Gutenberg, and the first font \u201cBlackletter.\u201d This was based on the handwriting style of the time, and was used to print the first books [1,2]. Centuries later, numerous studies have consistently shown the large impact that fonts have on not only the readability of text, but also the comprehensibility and trustability of what is written [3,4,5].\nDespite the prevalence of just the few standard fonts used throughout academic literature, there are innumerable creative, stylized and unique fonts available. Many have been created by individual designers as hobbies, or for particular applications such as logos, movies or print advertisements. A small sample of a few of the over 10,000 fonts [6] used in this study are shown in Figure 1.\nThe seminal work of Tenenbaum and Freeman [7] towards separating style from content was applied to letter generation. Our motivation and goals are similar to theirs: we hope that a learner can exploit the structure in samples of related content to extract representations necessary for modeling style. The endgoal is to perform tasks, such as synthesis and analysis, on new styles that have never been encountered. In contrast to [7], we do not attempt to explicitly model style and content separately; rather, through training a learning model (a deep neural network) to reproduce style, content and style are implicitly distinguished.\nar X\niv :1\n60 3.\n04 00\n0v 1\n[ cs\n.C V\n] 1\nWe also extend their work in four directions. First, we demonstrate that a very small subset of characters is required to effectively learn both discriminative and generative models for representing typographic style; we use only 4 instead of the 54 alpha-numeric characters used previously. Second, with these 4 letters, we learn individual-letter and combined-letter models capable of generating all the remaining letters. Third, we broaden the results to thousands of unseen fonts encompassing a far more expansive set of styles than seen previously. Finally, we present novel methods for quantitatively analyzing the generated results that are applicable to any image-creation task.\nFonts are particularly well suited for the examination of style: they provide diversity along many axes: shape, size, stroke weight, slant, texture, and serif details \u2014 all within a constrained environment in which the stylistic elements can be readily distinguished from content. Additionally, unlike many image generation tasks, this task has the enormous benefit of readily available groundtruth data, thereby allowing quantitative measurement of performance. Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].\nThe primary goal of this paper can be succinctly stated as follows: given a small set of letters (the basis set) of a particular font, can we generate the remaining letters in the same style? Before we can address this question, we need to ensure that there is enough information in a small basis set to ascertain style. This is the subject of the next section."}, {"heading": "2 A Discriminative Task: Same Font or Not?", "text": "By addressing the question of whether a set of basis-letters contain enough information to extract \u201cstyle\u201d, we immediately work towards identifying sources of potential difficulties for the overarching goal of generating characters \u2013 i.e. will the generative process have enough information to extract from the basis letters to successfully complete the task? Further, the networks created here will be a vital component in validating the final generated results (Section 3.3).\nThe discriminative task is as follows: Given a set of four basis letters1, e.g. B,A,S,Q, in font-A and another letter \u03a6, can we train a classifier to correctly identify whether \u03a6 is generated in font-A\u2019s style? See Figure 2. Next, we describe how the data is pre-processed and the neural networks used to address the task."}, {"heading": "2.1 Data Specifics", "text": "Due to the large variation in the font-faces examined, normalization of the font images was vital, though minimal. Each letter was input to the network as a 36\u00d736 gray-scale pixel image. In total, 5 letters were used as inputs: the first 4 were the basis letters (all from the same font), and the fifth letter was to be categorized. The output of the network was a single binary value, indicating whether the fifth letter belonged to the same font as the other 4.\nAll the fonts used in this study were True-Type-Font format (TTF), which allows for scaling. Due to the stylistic variations present in the data, the ratio of widths to heights of different font faces vary dramatically. The size of the characters within the 36\u00d736 image is set as follows. For each font, all 26 capital letters are generated with the same point-setting until the bounding box of any one of the 26 characters reaches the 36\u00d736 limit in either dimension. All 26 characters are then generated with that point setting; this ensures that if the font style specifies having a large \u2019R\u2019 while having a little \u2019L\u2019, that stylistic decision is preserved in the training example and that the largest letter fits within the\n1 B,A,S,Q comprised the basis set throughout this study. They were chosen because they contained a diverse set of edges, angles and curves that can be rearranged/analyzed to reveal hints for composing many of the remaining letters.\n36\u00d736 bounding box. Note, that this sometimes creates non-standard positioning of letters when seen together. For example, the character \u2019Q\u2019 sometimes has a descender [19] that extends below the font\u2019s baseline; this will appear raised up in the training examples (see Figure 2).\nA training set of approximately 200,000 examples was created from 10,000 randomly chosen unique fonts. The training set was composed of 100,000 positive examples (5th letter was the same font) and 100,000 negative examples (5th letter was a different font). When generating the examples, each character was randomly perturbed from the center location by up to \u00b12 pixels to introduce variation (this also allowed us to create enough unique positive examples). A disjoint testing set with 1,000 font families not present in the training set was created in exactly the same manner. For both training and testing, negative examples were chosen by randomly pairing different font-families2."}, {"heading": "2.2 Network Architectures", "text": "Building on the successes of deep neural networks for image recognition tasks (see [20] for a recent summary with ImageNet), we explore a variety of neural network architectures for this task. Numerous experiments were conducted to discover effective network architectures to use. Of the over 60 architectures tried, the top performing seven were selected. Though a full description of the experiments is beyond the scope of this paper, a few of the general principles found are provided here to help guide future studies.\n1. Treating inputs as a single large image or as 5 individual images: We compared using the full 36\u00d7(36\u00d75) pixel image as a single input vs. creating individual \u2019towers\u2019 for each of the 5 characters (see [21,22,23,24] for a review of column/tower architectures). As a single image, it may be easier to capture the relative sizes of characters. As multiple images, individualized character transformations can be created with potentially cleaner derivatives. Through numerous experiments, the results consistently favored using individual towers, both in terms of training time and final accuracy. See Figure 3. 2. Network Depth: In contrast to the trend of increasing network depth, deeper nets did not lead to improved performance. Unlike in general object detection tasks in which deep neural networks are able to exploit the property that images can be thoughts of as compositional hierarchies [25], such deep hierarchies may not be present for this task. For this study, two sets of convolution layers were used with fully connected layers prior to the final output (more details in Section 2.3). Additional depth did not increase performance.\n2 To create negative examples, two fonts from the same font-family, i.e. Times-RomanBold and Times-Roman were never used in the same negative example. Because the hierarchy of fonts is not apriori known, font families were estimated by a simple lexicographic analysis of the font names. Importantly, it should be noted that this did not preclude fonts that appear visually nearly indistinguishable (but with dissimilar names) from being used as negative examples.\n3. RELU or Logistic? A variety of activation functions were attempted, including exclusively logistic activations throughout the network (recall that the networks were not as deep as is often used in image-recognition tasks, a condition often cited for using Rectified Linear (RELU) activations [26]). Except for the final logistic-output unit, the rest of the activations throughout the network worked best as RELU. Though the performance difference was not consistent across trials, RELU units trained faster. 4. Convolutions vs. Fully Connected Layers: Convolution layers are frequently employed for two purposes: achieving translation invariance and free parameter reduction, especially when used in conjunction with pooling layers. For this task, translation invariance is not as crucial as in general object detection tasks, as the object of interest (the character) is centered. Further, this task worked well across a number of network sizes and free-parameter ranges. Networks that employed convolutions as well as those that used fully connected layers exclusively performed equally well."}, {"heading": "2.3 Individual and Ensembles Network Results", "text": "As mentioned in the previous section, over 60 architectures were experimented with for this task. Gradient descent with momentum (SGD+Momentum) was used to train the networks and no domain-specific prenormalization of the weights was necessary. From the 60 architectures, seven were chosen (see Table 1). All are based on the tower architecture (Figure 3). The performance on the discrimination task was measured on an independent test set \u2014 the 4-basis letters and \u03a6 were drawn from fonts that were not used for training. The results were consistent across a wide variety of free parameters; in the seven networks, the number of parameters varied by a factor of 18\u00d7, with similar performance.\nIt is illuminating to visualize the types of errors that the networks made. Figure 4 provides examples that were both correctly (Left) and incorrectly (Right)\nclassified by all 7 networks. In the leftmost column, examples in which the 5th character was correctly recognized as belonging to the same font are shown. In the second column, correctly recognized negative examples (the 5th letter was a different font) are given. Note that the 6th letter, the \u201cground-truth\u201d, was not used as input to the network; it is provided here to show the actual letter of the same font. In the second column, note the similarity of the correctly discerned letters from the ground-truth. In particular, in this column, row 1 had very few distinguishing marks between the proposed and real \u2019H\u2019; in row 4, the \u2019U\u2019 was recognized as not being a member of the same font, as was the \u2019V\u2019 in row 7 \u2013 based solely on the weight of the strokes.\nThe third column shows the false-negatives. Several of the mistakes are readily explained: rows 5,6,7 are non-alphabetic fonts (no pre-filtering was done in the font selection process). Rows 2 and 9 are explicitly designed to have a diverse set of characters, e.g. for creating \u2019Ransom Note\u2019 like artistic effects. Also shown\nare color-inverted fonts (row 4); the likely cause of the mistakes was the sparsity of such fonts in the training set.\nFinally, in the last column, false-positives are shown \u2013 different fonts that are mistakenly recognized as the same. Three types of errors were seen. When the fonts have extremely thin strokes (row 2), the failure rate increases. This is due to two factors: under-representation in the training set and less information in the pixel maps from which to make correct discriminations. Second, as before, non-alphabetic fonts appear. The third source of mistakes is that many fonts have characters that appear similar or match the basis font\u2019s style (rows 3,7,8). It is worth emphasizing here that the font design process is not an algorithmic one; the artist/designer can create fonts with as much, or as little, variation as desired. There may be multiple acceptable variations for each character.\nDespite the similar performance of the 7 networks, enough variation in the outputs exists to use them as an ensemble [27]. In 76.7% of examples, all networks were correct. In 8.2%, only 1 network was mistaken, in 4.1%, 2/7 were mistaken and in 3.0%, 3/7 were mistaken. Therefore, by using a simple majority voting scheme, the seven networks can be employed as an ensemble that yields 92.1% accuracy. This ensemble will be used throughout the remainder of the paper."}, {"heading": "3 A Generative Task: Creating Style-Specific Characters", "text": "In the previous section, we determined that there was sufficient information in the 4-Basis letters to correctly determine whether a fifth letter was a member of the same font. In this section, we attempt to construct networks that generate characters. The experiments are broadly divided into two approaches: single and multiple-letter generation. In the first approach, a network is trained to generate only a single letter. In the multiple-letter generation networks, all letters are generated simultaneously by the same network. Though the single letter networks are conceptually simpler, training a network to generate multiple letters simultaneously may allow the hidden units to share useful representations and features \u2013 i.e. serif style, line width, angles, etc. This is a form of transfer learning with a strong basis in multi-task learning [28,29,30,31]."}, {"heading": "3.1 Single Letter Generation", "text": "As in the previous section, experiments with numerous network architectures were conducted. The architectures varied in the number of layers (2-10), units (100s-1000s) and connectivity patterns. The final architecture used is shown in see Figure 6(Left). In the discrimination task described in the previous section, there was a single output node. This allowed the use of large penultimate hidden layers since the number of connections to the final output remained small. In contrast, in image generation tasks, the number of output nodes is the number of pixels desired; in this case it is the size of a full input character (36\u00d736). This drastically increases the number of connections in the network. To alleviate this growth in the number of connections, reverse-retinal connections were used in\nwhich each hidden unit in the penultimate layer connects to a small patch in the output layer. Various patch sizes were tried; patches of both sizes 3 and 4 were finally employed. Unlike other convolution and de-convolution networks, the connection weights are not shared. Sharing was not necessary since translation invariance is not needed in the generated image; each letter should be generated in the center of the output. Similar architectures have been used for superresolution and image deconvolution, often with weight sharing [32,33].\nThe letter \u2019R\u2019 was chosen as the first test case, since, for many fonts, the constituent building blocks for \u2019R\u2019 are present in the basis letters (e.g. copy the \u2019P\u2019 shape from the \u2019B\u2019-basis letter and combine it with the bottom right leg of the \u2019A\u2019). As in the previous experiments, the network was trained with the same set of 9,000 unique fonts with L2 loss in pixel-space, using SGD+Momentum. Numerous experiments, both with and without batch normalization [34], were conducted \u2013 no consistent difference in final error was observed. Results for fonts from the test set are shown in Figure 5.\nUnfortunately, simply measuring the pixel-wise difference to the actual target letter (SSE) does not correlate well to subjective evaluations of performance. Yet, as in all real-world image generation tasks, aesthetic judgment is vital. We asked a human rater3 to ascertain the quality of the result along 3 axis: (1) is the shape generally correct, (2) are the serif appropriately captured, (3) is this an acceptable replacement for the actual letter? The last metric is the hardest to measure, but perhaps the most relevant. The outcome was overall positive, with the caveats noted in Figure 5."}, {"heading": "3.2 Simultaneous Letter Generation - Multi-Task Learning", "text": "In this section, we explore the task of generating all the upper-case letters simultaneously. Unlike the single-letter generation process described in the previous section, the hidden units of the network can share extracted features. This is particularly useful in the character generation process, as many letters have common components (e.g. \u2019P\u2019 & \u2019R\u2019 , \u2019O\u2019 & \u2019Q\u2019, \u2019T\u2019 & \u2019I\u2019). See Figure 6(Right).\nOnce again, numerous architectures were explored (over 30 in total). Comparing the best single-letter-generation networks with the best multi-letter-generation networks, the SSE error was repeatedly reduced (by 5%-6%) by using the multiletter generation networks. Qualitatively, however, the characters generated by both networks appeared similar. Nonetheless, because of the small SSE error improvement coupled with the ease of deployment, the multi-letter-generation network are used going forward. A large set of results are shown in Figure 7. Subjectively evaluating these results would be error prone and yield inconsistent results given the difficulty in evaluating the individual consistency of a large set\n3 The subjective evaluation was conducted by an independent User Experience Researcher (UER) volunteer not affiliated with this project. The UER was given a paper copy of the input letters, the generated letters, and the actual letter. The UER was asked to evaluate the \u2019R\u2019 along the 3 dimensions listed above. Additionally, for control, the UER was also given examples (not shown here) which included real \u2019R\u2019s in order to minimize bias. The UER was not paid for this experiment.\nof characters with each other and with respect to the basis set. Instead, in the next section, we describe novel methods for validating the results."}, {"heading": "3.3 Validating Results", "text": "In the previous section, the results of the font generation network were measured by the network\u2019s SSE error and subsequently by an external human evaluator. Here, we present alternate methods based on using the discriminative networks developed in Section 2. The discriminative networks were used to determine whether an input character \u03a6 was the same font as the basis characters. Recall that by employing a voting-ensemble of 7 networks, 92.1% accuracy was attained.\nHere, to test the generative network G on font f , the results of G(f) are passed into the discriminative network ensemble (D) along with the original BASQ-basis letters, D(Bf , Af , Sf , Qf , G(f)). Consequently, the output of D is used to determine whether the generated letter is the same font as the basis letters. Novel variants of this method, using pairs of generator/discriminator networks have been independently proposed in [35], which is based on the architectures of generative-adversarial models [36,37].\nIn the most straightforward formulation, we test whether the letters generated by G appear the same as the original font. See Table 2 \u2013 column Original Basis, Synthetic Test. Formally, the test is:\nFor each font , f , in the test set: generate all characters:\nG(Bf , Af , Sf , Qf ) \u2192 [Af \u2032 , Bf \u2032 , Cf \u2032 , Df \u2032 , ..., Zf \u2032 ]\ntest each GENERATED character:\nD(Bf , Af , Sf , Qf , Af \u2032) \u2192 [Same/Different] ... D(Bf , Af , Sf , Qf , Zf \u2032) \u2192 [Same/Different]\nRecall that the discriminative network ensemble, D, was trained with tuples of letters {BasisSet, \u03a6} drawn from the training set of fonts and tested on an entirely separate set of fonts. Thus, we are not limited in the set of fonts that we can use as the basis-set. In the next examination, we reverse the question asked above. We generate the letters as before, but ask the question, if the generated letters are used as the basis set, will the original letters be recognized as the same font? See Table 2 \u2013 column Synthetic Basis, Original Test. The generation portion remains the same, but the test is revised to:\ntest each ORIGINAL character:\nD(Bf \u2032 , Af \u2032 , Sf \u2032 , Qf \u2032 , Af ) \u2192 [Same/Different] ... D(Bf \u2032 , Af \u2032 , Sf \u2032 , Qf \u2032 , Zf ) \u2192 [Same/Different]\nIn the previous two experiments, we compared the generated characters to the original font. This gives a measure of how closely the generated characters resemble the original font. In the final experiment, we ask how consistent the generated letters are with each other. If we are given the basis letters from the generated font, will the other generated letters from the same font be classified as the same? See Table 2 \u2013 column Synthetic Basis, Synthetic Test.\ntest GENERATED characters against GENERATED basis:\nD(Bf \u2032 , Af \u2032 , Sf \u2032 , Qf \u2032 , Af \u2032) \u2192 [Same/Different] ... D(Bf \u2032 , Af \u2032 , Sf \u2032 , Qf \u2032 , Zf \u2032) \u2192 [Same/Different]\nLooking at Table 2, from the column Original Basis, Synthetic Test, it can be seen that the generated fonts appear similar to the original fonts on which they are based. An interesting question arises: why, then, does the second test Synthetic Basis, Original Test have a lower recognition rate? The answer lies in the fact that the font designer may introduce non-systematic variations that add visual interest, but may not adhere to strict stylistic cues present in other letters. In the most obvious cases, the \u2019ransom note fonts\u2019, each character is intentionally not representative of other characters. Other themed fonts introduce objects such as trees, animals, and vehicles into the design that reflect each artist\u2019s individual creativity. For example, some fonts replace the empty spaces in the letter \u2019B\u2019 with mini-airplanes, but may not do so in the letter \u2019A\u2019, etc. The generated fonts, when they capture such shapes, will synthesize more consistent characters that reuse many of the same elements. It is likely that the original font\u2019s glyphs may appear outside the more cohesive set generated by the networks.4\nThe third column, Synthetic Basis, Synthetic Test, shows that the characters generated are extremely consistent with each other. Normally, this would be considered a good attribute; however, when viewed in terms of the baseline (Column 4: Original Basis, Original Test), it raises an important question. Why are the generated fonts more homogeneous than the original fonts? Is it for the reason mentioned above, or is it for the, potentially more troubling, reason that all the generated characters (across all fonts) are too much alike? Have all the characters \u201cregressed to the mean?\u201d As a final test, we examine this possibility explicitly using only network generated fonts. First, we set the basis letters (BASQ) from a randomly selected generated font. Second, we randomly select another generated font and character as the candidate test character. 50,000 randomly paired samples are created. Unlike the previous tests, in which accuracy was measured by how many matches were found, the accuracy is measured by how many nonmatches are detected. This explicitly tests whether the generated fonts look too similar to each other. Running this test with the discriminative network ensemble, D, yields a different-font detection rate of 90.7%. For a baseline, we repeat\n4 For completeness, we also analyzed the \u2019R\u2019s generated by the one-letter-at-a-time networks. They had similar performance (when measured with D) to the \u2019R\u2019 row shown in Table 2, with (6%) higher SSE.\nthis experiment with 50,000 pairs generated from the original (non-synthesized) fonts. D yields a correct different-font detection rate of 90.1%. The very close results indicate that the generated fonts have not regressed to the mean, in terms of recognizable serifs and style, and remain as distinguishable as the original fonts. The difference in performance between column 3 and 4 in Table 2 is likely due to the variability introduced by smaller artistic variances inserted by the designers, as described in the paragraph above."}, {"heading": "4 Future Work", "text": "Beyond the straightforward explorations of varying the number and selection of basis letters and also generating lower-case letters, there are many conceptually interesting avenues for future work. First, an alternative approach to using the generator networks is to use discriminative-networks and propagate derivatives back to the inputs to modify the input pixels to maximize the similarity of the hidden states to specified values. This has recently been proposed to create natural images as well as dream like images \u201cDeep Dreams\u201d [38,17].\nSecond, we made a concerted effort to use the simplest networks possible. An extensive empirical search through the space of networks and learning algorithms was conducted to find the most straightforward approach to addressing this task. Recent preprints [9,35] describe concurrent explorations of similar and related problems with much more complex architectures that yield comparably promising results. Extending this work to other architectures is easily done; for\nexample, the evaluation mechanisms used in this study are akin to Generative Adversarial Nets (GAN) [36,39,37] in which a synthetic-vs.-real distinguisher network and a font generator are trained to outperform each other. This study provides a strong, and easily implemented, baseline to which new architectures and learning approaches can be compared.\nThird, the trained networks can be used in novel manners beyond synthesizing characters from existing fonts. Will it be possible to take the attributes of multiple fonts and combine them? For example, if the the basis set is composed of 2 characters from font-A and 2 characters from font-B, will the resulting characters be a combination of the two? Preliminary evidence (see Figure 8) suggests that it may be possible; though perhaps more than 2 examples from each font will be necessary for artistically more innovative combinations to be produced.\nFourth, we have taken an image based approach to font generation. An alternate, more speculative direction, is to use a non-image based approach with recurrent networks, such as Long-Short-Term-Memory (LSTMs). Can LSTMs be used to generate characters directly in TrueType language? For this task, network\u2019s inputs would include the TTF of the basis letters. Similar programmatic learning in which LSTMs compose simple programs and sequences of music have been recently attempted [40,41,42]. If it is possible to generate results directly in TTF encodings, this will produce another, orthogonal, result to complement this study. The feasibility of this approach is open for future research."}, {"heading": "5 Conclusions", "text": "In this paper, we have presented a learning method to analyze typographic style based on a small set of letters and to employ the learned models to both distinguish fonts and produce characters in the same style. The results are quite promising, but fully capturing artistic visual intent is just beginning. Many of the overall shapes, weights, angles and serifs were successfully modeled. In the\nfuture, as learning progresses, some of the more individualized nuances and repeated intricate design patterns, unique to individual fonts, will be captured."}, {"heading": "6 Acknowledgments", "text": ""}], "references": [{"title": "Treasury of Alphabets and Lettering: A Source Book of the Best Letter Forms of Past and Present for Sign Painters, Graphic Artists, Commercial Artists, Typographers, Printers, Sculptors, Architects, and Schools of Art and Design", "author": ["J. Tschichold"], "venue": "A Norton professional book. Norton", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Typeface timeline shows us the history of fonts", "author": ["E. Hutchings"], "venue": "http://www.psfk.com/2012/04/history-of-fonts.html", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An eye tracking study of how font size and type influence online reading", "author": ["D. Beymer", "D. Russell", "P. Orton"], "venue": "Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction-Volume 2, British Computer Society", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "The influence of design factors on trust in a bank\u2019s website", "author": ["S. Ha"], "venue": "Digital Repository@ Iowa State University", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Increasing trust in mobile commerce through design aesthetics", "author": ["Y.M. Li", "Y.S. Yeh"], "venue": "Computers in Human Behavior 26(4)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Download", "author": ["10000Fonts.com"], "venue": "http://www.10000fonts.com/catalog/", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Separating style and content with bilinear models", "author": ["J.B. Tenenbaum", "W.T. Freeman"], "venue": "Neural computation 12(6)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "A fontastic voyage: Generative fonts with adversarial networks", "author": ["M. Bowey"], "venue": "http://multithreaded.stitchfix.com/blog/2016/02/02/a-fontastic-voyage", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators", "author": ["P. Upchurch", "N. Snavely", "K. Bala"], "venue": "ArXiv e-prints", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Analyzing 50k fonts using deep neural networks (2016", "author": ["E. Bernhardsson"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Elements of style: learning perceptual shape style similarity", "author": ["Z. Lun", "E. Kalogerakis", "A. Sheffer"], "venue": "ACM Transactions on Graphics (TOG) 34(4)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Defining pictorial style: Lessons from linguistics and computer graphics", "author": ["J. Willats", "F. Durand"], "venue": "Axiomathes 15(3)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR abs/1308.0850", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognizing image style", "author": ["S. Karayev", "A. Hertzmann", "H. Winnemoeller", "A. Agarwala", "T. Darrell"], "venue": "CoRR abs/1311.3715", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "The interestingness of images", "author": ["M. Gygli", "H. Grabner", "H. Riemenschneider", "F. Nater", "L. Gool"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "What makes an image popular", "author": ["A. Khosla", "A. Das Sarma", "R. Hamid"], "venue": "Proceedings of the 23rd international conference on World wide web,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "CoRR abs/1508.06576", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Representing musical genre: A state of the art", "author": ["J.J. Aucouturier", "F. Pachet"], "venue": "Journal of New Music Research 32(1)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "A 20 minute intro to typography basics", "author": ["M. Bowey"], "venue": "http://design.tutsplus.com/articles/a-20-minute-intro-to-typography-basics\u2013 psd-3326", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M.S. Bernstein", "A.C. Berg", "F. Li"], "venue": "CoRR abs/1409.0575", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Largescale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-path convolutional neural networks for complex image classification", "author": ["M. Wang"], "venue": "CoRR abs/1506.04701", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J Dean"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "Annals of statistics", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning 7", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers", "author": ["J.T. Huang", "J. Li", "D. Yu", "L. Deng", "Y. Gong"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning 28(1)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep convolutional neural network for image deconvolution", "author": ["L. Xu", "J.S. Ren", "C. Liu", "J. Jia"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "Computer Vision\u2013ECCV 2014. Springer", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating images with recurrent adversarial networks", "author": ["D. Jiwoong Im", "C. Dongjoo Kim", "H. Jiang", "R. Memisevic"], "venue": "ArXiv e-prints", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "ArXiv e-prints", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["A. Mordvintsev", "C. Olah", "M. Tyka"], "venue": "http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeperinto-neural.html", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "szlam", "a.", "R. Fergus"], "venue": "In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., Garnett, R., eds.: Advances in Neural Information Processing Systems 28. Curran Associates, Inc.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "A first look at music composition using lstm recurrent neural networks", "author": ["D. Eck", "J. Schmidhuber"], "venue": "Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale 103", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "\u201d This was based on the handwriting style of the time, and was used to print the first books [1,2].", "startOffset": 93, "endOffset": 98}, {"referenceID": 1, "context": "\u201d This was based on the handwriting style of the time, and was used to print the first books [1,2].", "startOffset": 93, "endOffset": 98}, {"referenceID": 2, "context": "Centuries later, numerous studies have consistently shown the large impact that fonts have on not only the readability of text, but also the comprehensibility and trustability of what is written [3,4,5].", "startOffset": 195, "endOffset": 202}, {"referenceID": 3, "context": "Centuries later, numerous studies have consistently shown the large impact that fonts have on not only the readability of text, but also the comprehensibility and trustability of what is written [3,4,5].", "startOffset": 195, "endOffset": 202}, {"referenceID": 4, "context": "Centuries later, numerous studies have consistently shown the large impact that fonts have on not only the readability of text, but also the comprehensibility and trustability of what is written [3,4,5].", "startOffset": 195, "endOffset": 202}, {"referenceID": 5, "context": "A small sample of a few of the over 10,000 fonts [6] used in this study are shown in Figure 1.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "The seminal work of Tenenbaum and Freeman [7] towards separating style from content was applied to letter generation.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "In contrast to [7], we do not attempt to explicitly model style and content separately; rather, through training a learning model (a deep neural network) to reproduce style, content and style are implicitly distinguished.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 82, "endOffset": 90}, {"referenceID": 8, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 82, "endOffset": 90}, {"referenceID": 9, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 82, "endOffset": 90}, {"referenceID": 10, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 223, "endOffset": 233}, {"referenceID": 14, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 223, "endOffset": 233}, {"referenceID": 15, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 223, "endOffset": 233}, {"referenceID": 16, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 243, "endOffset": 247}, {"referenceID": 17, "context": "Recently, growing attention has been devoted to style, not only in terms of fonts [8,9,10], but in perceptual shape similarity for architecture and rigid objects [11], computer graphics [12], cursive text [13], photographs [14,15,16], artwork [17], and music [18].", "startOffset": 259, "endOffset": 263}, {"referenceID": 18, "context": "For example, the character \u2019Q\u2019 sometimes has a descender [19] that extends below the font\u2019s baseline; this will appear raised up in the training examples (see Figure 2).", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "Building on the successes of deep neural networks for image recognition tasks (see [20] for a recent summary with ImageNet), we explore a variety of neural network architectures for this task.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "creating individual \u2019towers\u2019 for each of the 5 characters (see [21,22,23,24] for a review of column/tower architectures).", "startOffset": 63, "endOffset": 76}, {"referenceID": 21, "context": "creating individual \u2019towers\u2019 for each of the 5 characters (see [21,22,23,24] for a review of column/tower architectures).", "startOffset": 63, "endOffset": 76}, {"referenceID": 22, "context": "creating individual \u2019towers\u2019 for each of the 5 characters (see [21,22,23,24] for a review of column/tower architectures).", "startOffset": 63, "endOffset": 76}, {"referenceID": 23, "context": "creating individual \u2019towers\u2019 for each of the 5 characters (see [21,22,23,24] for a review of column/tower architectures).", "startOffset": 63, "endOffset": 76}, {"referenceID": 24, "context": "Unlike in general object detection tasks in which deep neural networks are able to exploit the property that images can be thoughts of as compositional hierarchies [25], such deep hierarchies may not be present for this task.", "startOffset": 164, "endOffset": 168}, {"referenceID": 25, "context": "RELU or Logistic? A variety of activation functions were attempted, including exclusively logistic activations throughout the network (recall that the networks were not as deep as is often used in image-recognition tasks, a condition often cited for using Rectified Linear (RELU) activations [26]).", "startOffset": 292, "endOffset": 296}, {"referenceID": 26, "context": "Despite the similar performance of the 7 networks, enough variation in the outputs exists to use them as an ensemble [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "This is a form of transfer learning with a strong basis in multi-task learning [28,29,30,31].", "startOffset": 79, "endOffset": 92}, {"referenceID": 28, "context": "This is a form of transfer learning with a strong basis in multi-task learning [28,29,30,31].", "startOffset": 79, "endOffset": 92}, {"referenceID": 29, "context": "This is a form of transfer learning with a strong basis in multi-task learning [28,29,30,31].", "startOffset": 79, "endOffset": 92}, {"referenceID": 30, "context": "This is a form of transfer learning with a strong basis in multi-task learning [28,29,30,31].", "startOffset": 79, "endOffset": 92}, {"referenceID": 31, "context": "Similar architectures have been used for superresolution and image deconvolution, often with weight sharing [32,33].", "startOffset": 108, "endOffset": 115}, {"referenceID": 32, "context": "Similar architectures have been used for superresolution and image deconvolution, often with weight sharing [32,33].", "startOffset": 108, "endOffset": 115}, {"referenceID": 33, "context": "Numerous experiments, both with and without batch normalization [34], were conducted \u2013 no consistent difference in final error was observed.", "startOffset": 64, "endOffset": 68}, {"referenceID": 34, "context": "Novel variants of this method, using pairs of generator/discriminator networks have been independently proposed in [35], which is based on the architectures of generative-adversarial models [36,37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 35, "context": "Novel variants of this method, using pairs of generator/discriminator networks have been independently proposed in [35], which is based on the architectures of generative-adversarial models [36,37].", "startOffset": 190, "endOffset": 197}, {"referenceID": 36, "context": "Novel variants of this method, using pairs of generator/discriminator networks have been independently proposed in [35], which is based on the architectures of generative-adversarial models [36,37].", "startOffset": 190, "endOffset": 197}, {"referenceID": 37, "context": "This has recently been proposed to create natural images as well as dream like images \u201cDeep Dreams\u201d [38,17].", "startOffset": 100, "endOffset": 107}, {"referenceID": 16, "context": "This has recently been proposed to create natural images as well as dream like images \u201cDeep Dreams\u201d [38,17].", "startOffset": 100, "endOffset": 107}, {"referenceID": 8, "context": "Recent preprints [9,35] describe concurrent explorations of similar and related problems with much more complex architectures that yield comparably promising results.", "startOffset": 17, "endOffset": 23}, {"referenceID": 34, "context": "Recent preprints [9,35] describe concurrent explorations of similar and related problems with much more complex architectures that yield comparably promising results.", "startOffset": 17, "endOffset": 23}, {"referenceID": 35, "context": "example, the evaluation mechanisms used in this study are akin to Generative Adversarial Nets (GAN) [36,39,37] in which a synthetic-vs.", "startOffset": 100, "endOffset": 110}, {"referenceID": 38, "context": "example, the evaluation mechanisms used in this study are akin to Generative Adversarial Nets (GAN) [36,39,37] in which a synthetic-vs.", "startOffset": 100, "endOffset": 110}, {"referenceID": 36, "context": "example, the evaluation mechanisms used in this study are akin to Generative Adversarial Nets (GAN) [36,39,37] in which a synthetic-vs.", "startOffset": 100, "endOffset": 110}, {"referenceID": 39, "context": "Similar programmatic learning in which LSTMs compose simple programs and sequences of music have been recently attempted [40,41,42].", "startOffset": 121, "endOffset": 131}, {"referenceID": 40, "context": "Similar programmatic learning in which LSTMs compose simple programs and sequences of music have been recently attempted [40,41,42].", "startOffset": 121, "endOffset": 131}, {"referenceID": 41, "context": "Similar programmatic learning in which LSTMs compose simple programs and sequences of music have been recently attempted [40,41,42].", "startOffset": 121, "endOffset": 131}], "year": 2016, "abstractText": "Typography is a ubiquitous art form that affects our understanding, perception, and trust in what we read. Thousands of different font-faces have been created with enormous variations in the characters. In this paper, we learn the style of a font by analyzing a small subset of only four letters. From these four letters, we learn two tasks. The first is a discrimination task: given the four letters and a new candidate letter, does the new letter belong to the same font? Second, given the four basis letters, can we generate all of the other letters with the same characteristics as those in the basis set? We use deep neural networks to address both tasks, quantitatively and qualitatively measure the results in a variety of novel manners, and present a thorough investigation of the weaknesses and strengths of the approach.", "creator": "LaTeX with hyperref package"}}}