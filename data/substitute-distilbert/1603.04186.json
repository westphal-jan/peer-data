{"id": "1603.04186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "Visual Concept Recognition and Localization via Iterative Introspection", "abstract": "convolutional neural networks have been shown to manage internal representations, which correspond closely to semantically perceived objects and parts, although trained solely on class labels. class activation mapping ( arcs ) is a general method that makes it possible to easily highlight the image regions contributing to a network's classification decision. we build upon these two developments to shape a network to re - examine informative identity regions, which we construct introspection. we propose a weakly - supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as detail progresses, by alternating stages of classification and introspection. we evaluate our method and show statistical effectiveness over a sequence encompasses several datasets, ensuring a top - 1 accuracy 84. 71 % cub - 5 - 2011, 2012 is the maximal run - date without using external data enabling stronger supervision. on stanford - 40 actions, we set a new state - of the interest of 87. 89 %, and on avant - aircraft and the stanford dogs dataset, we show consistent improvements over baselines, utilizing traits which include significantly more supervision.", "histories": [["v1", "Mon, 14 Mar 2016 10:18:03 GMT  (669kb,D)", "http://arxiv.org/abs/1603.04186v1", null], ["v2", "Wed, 25 May 2016 13:27:37 GMT  (3273kb,D)", "http://arxiv.org/abs/1603.04186v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "shimon ullman"], "accepted": false, "id": "1603.04186"}, "pdf": {"name": "1603.04186.pdf", "metadata": {"source": "META", "title": "Visual Concept Recognition and Localization via Iterative Introspection", "authors": ["Amir Rosenfeld", "Shimon Ullman"], "emails": ["amir.rosenfeld@weizmann.ac.il", "shimon.ullman@weizmann.ac.il"], "sections": [{"heading": null, "text": "Keywords: Classification, Introspection, Attention, Semi-Supervised Learning"}, {"heading": "1 Introduction", "text": "With the advent of deep convolutional neural networks as the leading method in computer vision, several attempts have been made to understand their inner workings. Examples of pioneering work in this direction include [1,2]; providing glimpses into the representations learned by intermediate levels in the network. Specifically, the recent work of Zhou et al. [3] provides an elegant mechanism to highlight the discriminative image regions that served the CNN for a given task. This can be seen as a form of introspection, highlighting the source of the network\u2019s conclusions. A useful trait we have observed in experiments is that even if the final classification is incorrect, the highlighted image regions are still be informative with respect to the correct target class. This is probably due to the similar appearance of confused classes. see Figures 1 and 4 for some examples. Motivated by this observation, we propose an iterative mechanism of internal\nar X\niv :1\n60 3.\n04 18\n6v 1\n[ cs\nsupervision, termed introspection, which revisits discriminative regions to refine the classification. As the process is repeated, each stage further highlights discriminative sub-regions. Each stage uses its own classifier, as we found this to be beneficial when compared to using the same classifier for all sub-windows.\nWe describe strategies for how to leverage the introspection scheme, and demonstrate how these consistently improve results on several benchmark datasets, while progressively refining the localization of discriminative regions. As shown, our method is particularly beneficial for fine-grained tasks such as species [4,5] or model [6] identification and to challenging cases in e.g., action recognition [7], which requires attention to small and localized details.\nIn the following we will first review some related work. In Section 3 we describe our method in detail. Section 4 contains experiments and analysis to evaluate the proposed method, followed by concluding remarks in 5."}, {"heading": "2 Related Work", "text": "Supervised methods consistently outperform unsupervised or semi-supervised methods, as they allow for the incorporation of prior knowledge into the learning process. There is a trade-off between more accurate classification results and structured output on the one and, the cost of labor-intensive manual annotations, on the other. Some examples are [8,9], where bounding boxes and part annotations are given at train time. Aside from the resources required for large-scale annotations, such methods elude the question of learning from weakly supervised data (and mostly unsupervised data), as is known to happen in human infants, who can learn from limited examples [10]. Following are a few lines of work related to the proposed method."}, {"heading": "2.1 Neural Net Visualization and Inversion", "text": "Several methods have been proposed to visualize the output of a neural net or explore its internal activations. Zeiler et al. [1] found patterns that activate hidden units via deconvolutional neural networks. They also explore the localization ability of a CNN by observing the change in classification as different image regions are masked out. [2] Solves an optimization problem, aiming to generate an image whose features are similar to a target image, regularized by a natural image prior. Zhou et al. [11] aims to explicitly find what image patches activate hidden network units, finding that indeed many of them correspond to semantic concepts and object parts. These visualizations suggest that, despite training solely with image labels, there is much to exploit within the internal representations learned by the network and that the emergent representations can be used for weakly supervised localization and other tasks of fine-grained nature."}, {"heading": "2.2 Semi-Supervised class Localization", "text": "Some recent works attempt to obtain object localization through weak labels, i.e., the net is trained on image-level class labels, but it also learns localization. [12] Localizes image regions pertaining to the target class by masking out subimages and inspecting change in activations of the network. Oquab et al. [13] use global max-pooling to obtain points on the target objects. Recently, Zhou et al. [3] used global average pooling (GAP) to generate a Class-Activation Mapping (CAM), visualizing discriminative image regions and enabling the localization of detected concepts. Our introspection mechanism utilizes their CAMs to iteratively identify discriminative regions and uses them to improve classification without additional supervision."}, {"heading": "2.3 Attention Based Mechanisms", "text": "Recently, some attention based mechanisms have been proposed, which allow focusing on relevant image regions, either for the task of better classification\n[14] or efficient object localization [15]. Such methods benefit from the recent fusion between the fields of deep learning and reinforcement learning [16]. Another method of interest is the spatial-transformer networks in [17]: they designed a network that learns and applies spatial warping to the feature maps, effectively aligning inputs, which results in increased robustness to geometric transformations. This enables fine-grained categorization on the CUB-200-2011 birds [4] dataset by transforming the image so that only discriminative parts are considered (bird\u2019s head, body). Additional works appear in [18], who discovers discriminative patches and groups them to generate part detectors, whose detections are combined with the discovered patches for a final classification. In [19], the outputs of two networks are combined via an outer-product, creating a strong feature representation. [20] discovers and uses parts by using co-segmentation on ground-truth bounding boxes followed by alignment."}, {"heading": "3 Approach", "text": "Our approach is composed of alternating between two main steps: classification and introspection. In the classification step, we apply a trained network to an\nimage region (possibly the entire image). In the introspection step, we use the output of a hidden layer in the network, whose values were set during the classification step. This highlights image regions which are fed to the next iteration\u2019s classification step. This process is iterated a few times (typically 4, see Section 4, Fig. 5), and finally the results of all stages are combined. Training proceeds by learning a specialized classifier for each iteration, as different iterations capture different contexts and levels of detail (but without additional supervision).\nBoth classification/introspection steps utilize the recent Class-Activation Mapping method of [3]. We briefly review the CAM method and then describe how we build upon it. In [3], a deep neural network is modified so that post-classification, it is possible to visualize the varying contribution of image regions, via a so-called Class Activation Mapping (CAM). A global average pooling was used as a penultimate feature representation. This results in a feature vector which is a spatial averaging of each of the feature maps of the last convolutional layer. Using the notation in [3]: let fk(x, y) be the k\u2019th output of the last convolutional layer at grid location (x, y). The results of the global-average pooling results in a vector F = (F1, F2, . . . , Fk), defined as:\nFk = \u2211 x,y fk(x, y) (1)\nThis is followed by a fully connected layer with C outputs (assuming C target classes). Hence the score for class c before the soft-max will be:\nSc = \u2211 k wckFk (2)\n= \u2211 k wck \u2211 x,y fk(x, y) (3)\n= \u2211 x,y \u2211 k wckfk(x, y) (4)\nNow, define\nMc(x, y) = \u2211 k \u03c9ckfk(x, y) (5)\nwhere \u03c9ck are class-specific weights. Hence we can express Sc as a summation of terms over (x, y):\nSc = \u2211 x,y Mc(x, y) (6)\nAnd the class probability scores are computed via soft-max, e.g, Pc = eSc\u2211 t e St . Eq. 5 allows us to measure the contribution of each grid cell Mc(x, y) for each specific class c. Indeed, [3] has shown this method to highlight informative image\nregions (with respect to the task at hand), while being on par with the classification performance obtained by the unmodified network (GoogLeNet [21] in their case). See Fig. 1 for some CAMs. Interestingly, we can use the CAM method to highlight informative image regions for classes other than the correct class, providing intuition on the features it has learned to recognize. This is discussed in more detail in Section 4.2 and demonstrated in Fig. 4. We name a network whose final convolutional layer is followed by a GAP layer as a GAP-network, and the output of the GAP layer as the GAP features. We next describe how this is used in our proposed method."}, {"heading": "3.1 Iterative Classification-Introspection", "text": "The proposed method alternates classification and introspection. Here we provide the outline of the method, with specific details such as values of parameters discussed in Section 4.6.\nFor a given image I and window w (initially the entire image), a learned classifier is applied to the GAP features extracted from the window Iw, resulting in C classification scores Sc, c \u2208 [1 . . . C] and corresponding CAMs Mwc (x, y). The introspection phase employs a strategy to select a sub-window for the next step by applying a beam-search to a set of putative sub-windows. The sequence of windows visited by the method is a route on an exploration-tree, from the root to one of the leaves. Each node represents an image window and the root is the entire image. We next explain how the sub-windows are created, and how the search is applied.\nWe order the current classification scores Sc by descending order and retain the top k scoring classes. Let c\u0302 be one of these classes and Mwc\u0302 (x, y) the corresponding CAM. We extract a square sub-window w\u2032 centered on the maximal value of Mwc\u0302 (x, y). Each such w\n\u2032 (k in total) is added as a child of the current node, which is represented by w. In this way, each iteration of the method expands a selected node in the exploration-tree, corresponding to an image window, until a maximum depth is reached. The tree depth is the number of iterations. We define iteration 0 as the iteration acting on the root. The size of a sub-window w\u2032 is of a constant fraction of the size of its parent w. We next describe how the exploration-tree is used for classification."}, {"heading": "3.2 Feature Aggregation", "text": "Let k be the number of windows generated at iteration t > 0. We denote the set of windows by:\nWt = (w t i) k i=1 (7)\nAnd the entire set of windows as:\nR = (Wt)Tt=0 (8)\nwhere W0 is the entire image. For each window w t i we extract features f t w \u2208 RK , e.g., K = 1024, the dimension of the GAP features, as well as classification\nscores Swti \u2208 R C . The set of windows R for an image I is arranged as nodes in the exploration-tree. The final prediction is a result of aggregating evidence from selected sub-windows along some path from the root to a tree-leaf. We evaluate variants of both early fusion (combining features from different iterations) or later fusion (combining predictions from different iterations)."}, {"heading": "3.3 Training", "text": "Training proceeds in two main stages. The first is to train a sequence of classifiers that will produce an exploration-tree for each training/testing sample. The second is training on feature aggregations along different routes in the explorationtrees, to produce a final model.\nDuring training, we train a classifier for each iteration (for a predefined number of iterations, 5 total) of the introspection/classification sequence. The automatic training of multiple classifiers at different scales contributes directly to the success of the method, as using the same classifier for all iterations yielded no improvement over the baseline results (Section 4.1). For the first iteration, we simply train on entire images with the ground-truth class-labels. For each iteration t > 1, we set the training samples to sub-windows of the original images and the targets to the ground-truth labels. The sub-windows selected for training are always those corresponding to the strongest local maximum in Mc(x, y), where Mc\u0302(x, y) is the CAM corresponding to the highest scoring class. Each classifier is an SVM trained on the features the output of the GAP layer of the network (as was done in [3]). We also checked the effect of fine-tuning the network and using additional features. The Results are discussed in the experiments, Section 4.\nRoutes on Exploration Trees The image is explored by traversing routes on a tree of nested sub-windows. The result of training is a set of classifiers, E = (E)i=1...T . We produce an exploration tree by applying at each iteration j the classifier Ej on the features of the windows produced by the previous iteration. The window of iteration 0 is the entire image. A route along the tree will consist of a sequence of windows w1, w2, ...wT where T is the number of iterations. We found in experiments that more than 5 iterations (including the first) brings negligible boosts in performance. The image score for a given class is given by either (1) summing the scores of classifiers along a route (late fusion), or (2) learning a classifier for the combined features of all visited windows along the route (early fusion). Features are combined via averaging rather than concatenation. This reduces the training time at no significant change to the final performance; such an effect has also been noted by [22]. See Fig. 2 for an overview of the proposed method. Fig. 2 shows some examples of how progressively zooming in on image regions helps correct early classification mistakes."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup:", "text": "In all our experiments, we start with a variant of the VGG-16 network [22] which was fined tuned on ILSVRC by [3]. We chose it over GoogLeNet-GAP as it obtained slightly higher classification results on the ILSVRC validation set. In this network, all layers after conv5-3 have been removed, including the subsequent pooling layer; hence the spatial-resolution of the resultant feature maps/CAM is 14\u00d7 14 for an input of size 224\u00d7 224 (leaving the pooling layer would reduce resolution to be 7 \u00d7 7). A convolutional layer of 1024 filters has been added, followed by a fully-connected layer to predict classes. This is our basic GAPnetwork, called VGG-GAP. Each image window, including the entire original image, is resized so that its smaller dimension is 224 pixels, resulting in a feature map 14 \u00d7 n \u00d7 1024, for which we compute the average along the first two dimensions, to get a feature representation. We resize the images using bilinear interpolation. We train a separate classifier for each iteration of the classification/introspection process; treating all visited image windows with the same classifier yielded a negligible improvement (0.3% in precision) over the baseline.\nAll classifiers are trained with a linear SVM [26] on `2 normalized feature vectors. If the features are a concatenation of two feature vectors, those are `2 normalized before concatenation. Our experiments were carried out using the MatConvNet framework [27], as well as [28,29]. We evaluated our approach on several datasets, including Stanford-40 Actions [7], the Caltech-USCD Birds-200-2011 [4] (a.k.a CUB-200-2011), the Stanford-Dogs dataset, [5] and the FGVC-Aircraft dataset [6]. See Table 1 for a summary of our results compared to recent work. In the following, we shall first show some analysis on the validity of using the CAMs to guide the next step. We shall then describe interesting properties of our method, as well as the effects of different parameterizations of the method."}, {"heading": "4.2 Correlation of Class and Localization", "text": "In this section, we show some examples to verify our observation that CAMs tend to highlight informative image locations w.r.t to the target class despite the fact that the image may have been misclassified at the first iteration (i.e., before zooming in on sub-windows).\nTo do so, we have applied to the test-set of the Stanford-40 actions dataset a classifier learned on the GAP features of VGG-GAP. For each category in turn, we ranked all images in the test set according to the classifier\u2019s score for that category. We then picked the top 5 true positive images and top 5 false\npositive images. See Fig. 4 for some representative images. We can see that the CAMs for a target class tend to be consistent in both positive images and highranking non-class images. Intuitively, this is because the classifier gives more weight to patterns which are similar in appearance. For the \u201cwriting on a book\u201d category (top-left of Fig. 4 ) we can see how in positive images the books and especially the hands are highlighted, as they are for non-class images, such as \u201creading\u201d, or \u201ccutting vegetables\u201d. For \u201ctexting message\u201d (top-right) the hand region is highlighted in all images, regardless of class. For \u201cshooting an arrow\u201d class (bottom-right), elongated structures such as fishing rods are highlighted. A nice confusion appears between an archer\u2019s bow and a violinist\u2019s bow (bottomright block, last row, first image), which are also referred to by the same word in some human languages.\nTo check our claim quantitatively, we computed the extent of two square sub-windows for each image in the test-set: one using the CAM of the true class and one using the CAM of the highest scoring non-true class. For each pair we computed the overlap (intersection over union) score. The mean score all images was 0.638; This is complementary evidence to [3], who shows that the CAMs have good localization capabilities for the correct class."}, {"heading": "4.3 Early and Late Fusion", "text": "In all our experiments, we found that using the features extracted from a window at some iteration can bring worse results on its own compared to those extracted from earlier iterations (which include this window). However, the performance tends to improve as we combine results from several iterations, in a late-fusion manner. Training on the combined (averaged) features of windows from multiple iterations further improves the results (early-fusion). Summing the scores of early-fused features for different route lengths further improves accuracy: if Si is the score of the classifier trained on a route of length i. Then creating a final score from S1 +St + . . . ST tends to improve as T grows, typically stabilizing at T = 5. See Fig. 5 for an illustration of this effect. Importantly, we tried using the classifier from the first iteration (i.e., trained on entire images) for all iterations. This performed worse than learning a classifier per-iteration, especially in later iterations."}, {"heading": "4.4 Fine-Grained vs General Categories", "text": "The Standford-40 Action dataset [7] is a benchmark dataset made of 9532 images of 40 different action classes, with 4000 images for training and the rest for testing. It contains a diverse set of action classes including transitive ones with small objects (smoking, drinking) and large objects (horses), as well as intransitive actions (running, jumping). As a baseline, we used the GAP-network of [3] as a feature extractor and trained a multi-class SVM [26] using the resulting features. Using 5 iterations, we obtain a final precision of 77.08% and 80.06% with fine-tuning. It is particularly interesting to examine the classes for which our method is most beneficial. We have calculated the F-measure for each class\nusing the classification scores from the fourth and first iteration and compared them. Fig. 6 shows this; the largest absolute improvements are on relatively challenging classes such as texting a message (7.89%), drinking (9.24%), smoking (9.38%), etc. For all of these, the discriminative objects are small objects and are relatively hard to detect compared to most other classes. In some cases, performance is harmed by zooming in on too-local parts of an image: for \u201criding a bike\u201d (-3.12%), a small part of the bicycle will not allow disambiguating the image from e.g., \u201cfixing a bike\u201d. Another pair of categories exhibiting similar behavior is \u201criding a horse\u201d vs. \u201cfeeding a horse\u201d."}, {"heading": "4.5 Top-Down vs. Bottom-Up attention", "text": "To further verify that our introspection mechanism highlights regions whose exploration is worthwhile, we evaluated an alternative to the introspection stage by using a generic saliency measure [30]. On the Stanford-40 dataset, instead of using the CAM after the first classification step, we picked the most salient image point as the center of the next sub-window. Then we proceeded with training and testing as usual. This produced a sharp drop in results: on the first iteration performance dropped from 74.47% when using the CAM to 62.31% when using the saliency map. Corresponding drops in performance were measured in the late-fusion and early fusion steps, which improve results in the proposed scheme but made them worse when using saliency as a guide.\nUsage of Complementary Feature Representations The network used for drawing attention to discriminative image regions need not necessarily be the one used for feature representation. We used the VGG-16 [22] network to extract fc6 features along the GAP features for all considered windows. On the Stanford-40 Actions dataset, when used to classify categories using features extracted from entire images, these features we slightly weaker than the GAP features (68% vs 72%). However, training on a concatenated feature representation boosted results significantly, reaching a precision of 85%. We observed a similar effect on all datasets, showing that the two representations are complementary in nature. Combined with our iterative method, we were able to achieve 87.89%, compared to the previous best 81% of [23].\nEffect of Aspect-Ratio Distortion Interestingly, our baseline implementation (using only the VGG-GAP network as a feature extractor for the entire image) got a precision score of 75.23% compared to 72.03% of [3]. We suspect that it may be because in their implementation , they modified the aspect ratio of the images to be square regardless of the original aspect ratio, whereas we did not. Doing so indeed got a score more similar to theirs, which is interesting from a practical viewpoint."}, {"heading": "4.6 Various Parameters & Fine-Tuning", "text": "Our method includes several parameters, including the number of iterations, the width of the beam-search used to explore routes of windows on the image and the ratio between the size of the current window and the next. For the number iterations, we have consistently observed that performance saturates, and even deteriorates a bit, around iteration 4. An example of this can be seen in Fig. 5 showing the performance vs iteration number on the Standford-40 dataset. A similar behavior was observed on all the datasets on which we\u2019ve evaluated the method. This is probably due to the increasingly small image regions considered at each iteration. As for the number of windows to consider at each stage, we tried choosing between 1 and 3 of the windows relating to the highest ranking\nclasses on a validation set. At best, such strategies performed as well as the greedy strategy, which chose only the highest scoring window at each iteration. The size of the sub-window with respect to the current image window was set as \u221a\n2m where m is the geometric mean of the current window\u2019s height and width (in effect, all windows are square, except the entire image). We have experimented with smaller and larger values on a validation set and found this parameter to give a good trade-off between not zooming in too much (risking \u201cmissing\u201d relevant features) and too little (gaining too little information with respect to the previous iteration).\nWe have also evaluated our results when fine-tuning the VGG-GAP network before the first iteration. This improves the results for some of the datasets, i.e., Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).\nFinally, we evaluated the effect of fine-tuning the network for all iterations on the CUB-200-2011. This resulted in a top-1 precision of 84.48%, which to our knowledge the best result to-date, second only to works which added a massive amount of external data mined from the web [31] (91.9%) and/or strong supervision [32] (84.6%)."}, {"heading": "5 Conclusions", "text": "We have presented a method, which by repeatedly examining the source of the current prediction, decides on informative image regions to consider for further examination. The method is based on the observation that a trained CNN can be used to highlight relevant image areas even when its final classification is incorrect. This is a result of training on multiple visual categories using a shared feature representation. We have built upon Class Activation Maps [3] due to their simplicity and elegance, though other methods for identifying the source of the classification decision (e.g., [1]) could probably be employed as well. The proposed method integrates multiple features extracted at different locations and scales. It makes consistent improvements on fine-grained classification tasks and on tasks where classification depends on fine localized details. It obtains state-of-the art results on CUB-200-2011 [4], among methods which avoid strong supervision such as bounding boxes or keypoint annotations. The improvements are shown despite the method being trained using only class labels, avoiding the need for supervision in the form of part annotations or even bounding boxes. In future work, it would be interesting to examine the use of recurrent nets (RNN, LSTM [33]) to automatically learn sequential processes, which incrementally improve classification results, extending the approach described in the current work."}], "references": [{"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Deep Features for Discriminative Localization", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "arXiv preprint arXiv:1512.04150", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Novel Dataset for FineGrained Image Categorization", "author": ["A. Khosla", "N. Jayadevaprakash", "B. Yao", "L. Fei-Fei"], "venue": "First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Fine-grained visual classification of aircraft", "author": ["S. Maji", "E. Rahtu", "J. Kannala", "M. Blaschko", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1306.5151", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei-Fei"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Part-based R-CNNs for finegrained category detection", "author": ["N. Zhang", "J. Donahue", "R. Girshick", "T. Darrell"], "venue": "Computer Vision\u2013ECCV 2014. Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fine-grained pose prediction, normalization, and recognition", "author": ["N. Zhang", "E. Shelhamer", "Y. Gao", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.07063", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science 350(6266)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "arXiv preprint arXiv:1412.6856", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-taught object localization with deep networks", "author": ["A. Bergamo", "L. Bazzani", "D. Anguelov", "L. Torresani"], "venue": "arXiv preprint arXiv:1409.3964", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Is object localization for free?-weaklysupervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A Graves"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Active object localization with deep reinforcement learning", "author": ["J.C. Caicedo", "S. Lazebnik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A Zisserman"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "The application of two-level attention models in deep convolutional neural network for fine-grained image classification", "author": ["T. Xiao", "Y. Xu", "K. Yang", "J. Zhang", "Y. Peng", "Z. Zhang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear CNN models for fine-grained visual recognition", "author": ["T.Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Fine-grained recognition without part annotations", "author": ["J. Krause", "H. Jin", "J. Yang", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep spatial pyramid: The devil is once again in the details", "author": ["B.B. Gao", "X.S. Wei", "J. Wu", "W. Lin"], "venue": "arXiv preprint arXiv:1504.05277", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly Supervised Fine-Grained Image Categorization", "author": ["Y. Zhang", "Wei", "X.s.", "J. Wu", "J. Cai", "J. Lu", "V.A. Nguyen", "M.N. Do"], "venue": "arXiv preprint arXiv:1504.04943", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural activation constellations: Unsupervised part model discovery with convolutional networks", "author": ["M. Simon", "E. Rodner"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research 9", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "MatConvNet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Vlfeat: an open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "In Bimbo, A.D., Chang, S.F., Smeulders, A.W.M., eds.: ACM Multimedia, ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Saliency optimization from robust background detection", "author": ["W. Zhu", "S. Liang", "Y. Wei", "J. Sun"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition", "author": ["J. Krause", "B. Sapp", "A. Howard", "H. Zhou", "A. Toshev", "T. Duerig", "J. Philbin", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.06789", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Augmenting strong supervision using web data for fine-grained categorization", "author": ["Z. Xu", "S. Huang", "Y. Zhang", "D. Tao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Long Short Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Examples of pioneering work in this direction include [1,2]; providing glimpses into the representations learned by intermediate levels in the network.", "startOffset": 54, "endOffset": 59}, {"referenceID": 1, "context": "Examples of pioneering work in this direction include [1,2]; providing glimpses into the representations learned by intermediate levels in the network.", "startOffset": 54, "endOffset": 59}, {"referenceID": 2, "context": "[3] provides an elegant mechanism to highlight the discriminative image regions that served the CNN for a given task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "1: (Top) Class Activation Maps [3] show the source of a network\u2019s classification.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "An SVM trained on features extracted from VGG-GAP [3] misclassified all of these images, while highlighting the discriminative regions.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "As shown, our method is particularly beneficial for fine-grained tasks such as species [4,5] or model [6] identification and to challenging cases in e.", "startOffset": 87, "endOffset": 92}, {"referenceID": 4, "context": "As shown, our method is particularly beneficial for fine-grained tasks such as species [4,5] or model [6] identification and to challenging cases in e.", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": ", action recognition [7], which requires attention to small and localized details.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Some examples are [8,9], where bounding boxes and part annotations are given at train time.", "startOffset": 18, "endOffset": 23}, {"referenceID": 7, "context": "Some examples are [8,9], where bounding boxes and part annotations are given at train time.", "startOffset": 18, "endOffset": 23}, {"referenceID": 8, "context": "Aside from the resources required for large-scale annotations, such methods elude the question of learning from weakly supervised data (and mostly unsupervised data), as is known to happen in human infants, who can learn from limited examples [10].", "startOffset": 243, "endOffset": 247}, {"referenceID": 0, "context": "[1] found patterns that activate hidden units via deconvolutional neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Solves an optimization problem, aiming to generate an image whose features are similar to a target image, regularized by a natural image prior.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] aims to explicitly find what image patches activate hidden network units, finding that indeed many of them correspond to semantic concepts and object parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Localizes image regions pertaining to the target class by masking out subimages and inspecting change in activations of the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] use global max-pooling to obtain points on the target objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] used global average pooling (GAP) to generate a Class-Activation Mapping (CAM), visualizing discriminative image regions and enabling the localization of detected concepts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] or efficient object localization [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] or efficient object localization [15].", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Such methods benefit from the recent fusion between the fields of deep learning and reinforcement learning [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Another method of interest is the spatial-transformer networks in [17]: they designed a network that learns and applies spatial warping to the feature maps, effectively aligning inputs, which results in increased robustness to geometric transformations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Additional works appear in [18], who discovers discriminative patches and groups them to generate part detectors, whose detections are combined with the discovered patches for a final classification.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "In [19], the outputs of two networks are combined via an outer-product, creating a strong feature representation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "[20] discovers and uses parts by using co-segmentation on ground-truth bounding boxes followed by alignment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Both classification/introspection steps utilize the recent Class-Activation Mapping method of [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "In [3], a deep neural network is modified so that post-classification, it is possible to visualize the varying contribution of image regions, via a so-called Class Activation Mapping (CAM).", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Using the notation in [3]: let fk(x, y) be the k\u2019th output of the last convolutional layer at grid location (x, y).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Indeed, [3] has shown this method to highlight informative image", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "regions (with respect to the task at hand), while being on par with the classification performance obtained by the unmodified network (GoogLeNet [21] in their case).", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "Each classifier is an SVM trained on the features the output of the GAP layer of the network (as was done in [3]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 20, "context": "This reduces the training time at no significant change to the final performance; such an effect has also been noted by [22].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "3: Exploration routes on images: Each row shows the original image and 3 iterations of the algorithm, including the resulting Class-Activation Maps [3] used to guide the next iteration.", "startOffset": 148, "endOffset": 151}, {"referenceID": 20, "context": "In all our experiments, we start with a variant of the VGG-16 network [22] which was fined tuned on ILSVRC by [3].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "In all our experiments, we start with a variant of the VGG-16 network [22] which was fined tuned on ILSVRC by [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "VGG-GAP* is our improved baseline using the VGG-GAP network [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "GoogLeNet-GAP[3] 72.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "03[3],81 [23] 79.", "startOffset": 2, "endOffset": 5}, {"referenceID": 21, "context": "03[3],81 [23] 79.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "92[24] 77.", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "9 [18], 81.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "01 [25], 82 [20], 84.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "01 [25], 82 [20], 84.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "1 [17] 84.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": "1[19]", "startOffset": 1, "endOffset": 5}, {"referenceID": 24, "context": "All classifiers are trained with a linear SVM [26] on `2 normalized feature vectors.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "Our experiments were carried out using the MatConvNet framework [27], as well as [28,29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 26, "context": "Our experiments were carried out using the MatConvNet framework [27], as well as [28,29].", "startOffset": 81, "endOffset": 88}, {"referenceID": 5, "context": "We evaluated our approach on several datasets, including Stanford-40 Actions [7], the Caltech-USCD Birds-200-2011 [4] (a.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "a CUB-200-2011), the Stanford-Dogs dataset, [5] and the FGVC-Aircraft dataset [6].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "a CUB-200-2011), the Stanford-Dogs dataset, [5] and the FGVC-Aircraft dataset [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "638; This is complementary evidence to [3], who shows that the CAMs have good localization capabilities for the correct class.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "The Standford-40 Action dataset [7] is a benchmark dataset made of 9532 images of 40 different action classes, with 4000 images for training and the rest for testing.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "As a baseline, we used the GAP-network of [3] as a feature extractor and trained a multi-class SVM [26] using the resulting features.", "startOffset": 42, "endOffset": 45}, {"referenceID": 24, "context": "As a baseline, we used the GAP-network of [3] as a feature extractor and trained a multi-class SVM [26] using the resulting features.", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Performance is shown on the Stanford-40 [7] dataset", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "The figure shows absolute difference in terms of F-measure over the baseline approach on all categories of the Stanford-40 Actions [7] dataset", "startOffset": 131, "endOffset": 134}, {"referenceID": 27, "context": "To further verify that our introspection mechanism highlights regions whose exploration is worthwhile, we evaluated an alternative to the introspection stage by using a generic saliency measure [30].", "startOffset": 194, "endOffset": 198}, {"referenceID": 20, "context": "We used the VGG-16 [22] network to extract fc6 features along the GAP features for all considered windows.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "89%, compared to the previous best 81% of [23].", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "03% of [3].", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": ", Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": ", Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": ", Stanford-40 [7], CUB-200-2011 [4], but did not improve results significantly for others (Dogs [5], Aircraft [6]).", "startOffset": 110, "endOffset": 113}, {"referenceID": 28, "context": "48%, which to our knowledge the best result to-date, second only to works which added a massive amount of external data mined from the web [31] (91.", "startOffset": 139, "endOffset": 143}, {"referenceID": 29, "context": "9%) and/or strong supervision [32] (84.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "We have built upon Class Activation Maps [3] due to their simplicity and elegance, though other methods for identifying the source of the classification decision (e.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": ", [1]) could probably be employed as well.", "startOffset": 2, "endOffset": 5}, {"referenceID": 30, "context": "In future work, it would be interesting to examine the use of recurrent nets (RNN, LSTM [33]) to automatically learn sequential processes, which incrementally improve classification results, extending the approach described in the current work.", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network\u2019s classification decision. We build upon these two developments to enable a network to re-examine informative image regions, which we term introspection. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, obtaining a top-1 accuracy 84.48% CUB-200-2011, which is the highest to-date without using external data or stronger supervision. On Stanford-40 Actions, we set a new state-of the art of 87.89%, and on FGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements over baselines, some of which include significantly more supervision.", "creator": "LaTeX with hyperref package"}}}