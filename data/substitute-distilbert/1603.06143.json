{"id": "1603.06143", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks", "abstract": "we present a deep learning approach for tying up constrained procedural modeling. probabilistic inference algorithms such as sequential monte carlo ( smc ) provide powerful tools for constraining procedural models, but they require newer samples to produce better results. in this game, we show evolutionary mathematicians create procedural models which learn without resources satisfy constraints. we augment procedural models with neural networks : numerical networks control how the model makes random choices based on what output it has generated thus far. we call such a prediction a neurally - guided procedural model. as a pre - computation, why train these models on stress - satisfying example outputs generated via smc. i now then used as efficient importance samplers for smc, particularly high - quality imagery with very few samples. we evaluate our method on l - system - like models with image - based constraints. have a desired quality threshold, neurally - guided models would generate satisfactory results up to moderate faster than unguided models.", "histories": [["v1", "Sat, 19 Mar 2016 20:58:47 GMT  (5959kb,D)", "http://arxiv.org/abs/1603.06143v1", null], ["v2", "Thu, 13 Oct 2016 20:10:09 GMT  (5956kb,AD)", "http://arxiv.org/abs/1603.06143v2", null]], "reviews": [], "SUBJECTS": "cs.GR cs.AI", "authors": ["daniel ritchie", "anna thomas", "pat hanrahan", "noah d goodman"], "accepted": true, "id": "1603.06143"}, "pdf": {"name": "1603.06143.pdf", "metadata": {"source": "META", "title": "Neurally-Guided Procedural Models:earning to Guide Procedural Models with Deep Neural Networks", "authors": ["Daniel Ritchie", "Anna Thomas", "Pat Hanrahan", "Noah D. Goodman"], "emails": ["ngoodman}@stanford.edu"], "sections": [{"heading": null, "text": "We present a deep learning approach for speeding up constrained procedural modeling. Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks: these networks control how the model makes random choices based on what output it has generated thus far. We call such a model a neurally-guided procedural model. As a pre-computation, we train these models on constraint-satisfying example outputs generated via SMC. They are then used as efficient importance samplers for SMC, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.\nCR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling\u2014Geometric algorithms, languages, and systems G.3 [Probability And Statistics]: Probabilistic algorithms (including Monte Carlo) I.2.6 [Artificial Intelligence]: Learning\u2014Connectionism and neural nets\nKeywords: Procedural Modeling, Probabilistic Programming, Sequential Monte Carlo, Deep Learning, Neural Networks"}, {"heading": "1 Introduction", "text": "\u2217e-mail: {dritchie, thomasat, hanrahan, ngoodman}@stanford.edu\nROCEDURAL modeling is a powerful technique for creating graphics content. It facilitates efficient content creation at massive scale, such as procedural cities [Mu\u0308ller\net al. 2006]. It can generate fine detail that would require painstaking effort to create by hand, such as decorative floral patterns [Wong et al. 1998]. It can even generate surprising or unexpected results, helping users to explore large or unintuitive design spaces [Marks et al. 1997; Ritchie et al. 2015a].\nMany applications demand the ability to constrain or control procedural models: making their outputs resemble examples [Stava et al. 2014; Dang et al. 2015], fit a target shape [Prusinkiewicz et al. 1994; Talton et al. 2011; Ritchie et al. 2015b], or respect functional constraints such as physical stability [Ritchie et al. 2015a]. Bayesian probabilistic inference provides a general-purpose framework for imposing such controls: the procedural model specifies a generative prior distribution, and the constraints are encoded as a likelihood function. Samples from the posterior distribution can be drawn via approximate inference algorithms such as Markov Chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) [Brooks et al. 2011; Doucet et al. 2001].\nUnfortunately, these algorithms are often slow, requiring many samples to produce high-quality results. This limits their usability for interactive applications. Performance can be improved through clever sampler design, but this requires time and expertise and is often problem-specific [Schwarz and Wonka 2014; Zhu et al. 2012].\nSampling from constrained procedural models is challenging because the constraints implicitly define complex (often non-local) dependencies not present in the original procedural model. Can we instead make these dependencies explicit by encoding them in the models\u2019 generative logic? Such an explicit model could simply be run forward to generate constraint-satisfying results.\nar X\niv :1\n60 3.\n06 14\n3v 1\nIn this paper, we propose a method for automatically learning an approximation to such a perfect explicit model. Our method leverages advances in deep learning: it augments the procedural model with neural networks that control how the model makes random choices, based on what partial output the model has generated thus far. We call such a model a neurally-guided procedural model. The neural networks are expressive enough to capture many implicit dependencies induced by the constraints.\nWe train neurally-guided procedural models using constraintsatisfying example outputs generated via SMC. Once trained, these models can be used as intelligent SMC important samplers. Our approach thus enables \u2018bootstrapping\u2019 samplers which train on their own outputs and become more efficient over time. Or, the system can invest time up-front generating and training on many examples, effectively \u2018pre-compiling\u2019 an efficient sampler.\nWe demonstrate our method through experiments with L-systemlike procedural models with image-based soft constraints (Figure 1). For a given constraint satisfaction score threshold, our neurally-guided procedural model can generate results which reliably achieve that threshold using 10-20x fewer particles and up to 10x less compute time than an unguided procedural model.\nIn summary, our main contributions are:\n\u2022 A general mathematical framework for defining and training neurally-guided procedural models that make implicit constraints into explicit generative processes.\n\u2022 A specific implementation for L-system-like models with image-based constraints.\n\u2022 Performance evaluation of neurally-guided image-constrained models, showing up to 10x speedups.\nWe give a high-level overview of our approach in Section 3 and then present the mathematical foundations of our method in Section 4. In Section 5, we describe how to implement neurally-guided procedural models with image-matching constraints. Finally, we evaluate the performance of those models in Section 6."}, {"heading": "2 Related Work", "text": "Probabilistic Inference for Procedural Modeling Many research projects have used Bayesian probabilistic inference to control procedural models: constraining the shape of a 3D object [Talton et al. 2011; Ritchie et al. 2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al. 2011; Yeh et al. 2012], coloring in patterns [Lin et al. 2013], and dressing virtual characters [Yu et al. 2012] are a few recent applications. Our work aims to make such systems more efficient: neurally-guided procedural models can capture many of the dependencies introduced by constraint likelihood functions, so samplers need fewer samples to find good results.\nIn recent work similar in spirit to our own, Dang and colleagues built a system which modifies a procedural grammar so that its output distribution reflects user preference scores given to example outputs [Dang et al. 2015]. Like us, they seek a model whose generative logic captures dependencies induced by a likelihood function (in their case, a Gaussian process regression over user-provided examples). Their method works by splitting non-terminal symbols in the original grammar, giving it more degrees of freedom to capture more dependencies. This approach works well for discrete dependencies, such as ensuring all floors of a building have the same architectural style. In contrast, our method captures dependencies using neural networks, making it better suited for complex, continuous constraint functions, such as shape-fitting.\nGuided Procedural Modeling The shape of procedural models can be controlled using purely generative methods. The seminal work on open/environmentally-sensitive L-systems developed a formalism by which L-systems could query their spatial position and orientation [Prusinkiewicz et al. 1994; Me\u030cch and Prusinkiewicz 1996]. This ability allows them to prune their growth to an implicit surface. Recent follow-up work extends this technique to larger models by decomposing them into separate guide regions with limited interaction [Benes\u030c et al. 2011]. These guide methods were carefully designed for the specific problem of fitting procedural models to shapes. In contrast, our method learns how to guide procedural models and is generally applicable to constraints which can be expressed as a likelihood function.\nNeural Networks for Procedural Modeling Previous work has found other ways to apply neural networks to procedural modeling. One recent project uses neural networks as computationally inexpensive proxies for costly scoring functions in an inverse urban procedural modeling setting [Vanegas et al. 2012]. Another uses an autoencoder network to learn a low-dimensional representation space in which it is easy to explore the variability in a procedural model\u2019s output [Yumer et al. 2015]. Our use of neural networks differs from both of the above projects, as we use them to capture constraint-induced dependencies via feedforward functions.\nNeural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014]. These algorithms use neural networks to define more expressive parametric families of probability distributions. They train stochastic deep belief networks and autoencoders, primarily modeling distributions over images for computer vision applications. Our method uses a different learning objective, and we focus on training procedural models with more complex recursive control flow.\nThe Neural Adaptive Sequential Monte Carlo algorithm is most similar to our method; it uses a similar learning objective and aims to train more efficient SMC importance samplers [Gu et al. 2015]. However, they focus on inference in time series models, such as nonlinear state space models."}, {"heading": "3 Approach", "text": "In this section, we motivate and outline the process of creating, training, and using neurally-guided procedural models. Throughout this paper, we represent procedural models as probabilistic programs, i.e. programs that make random choices and support conditional inference queries about their distribution of outputs [Goodman and Stuhlmu\u0308ller 2014]."}, {"heading": "3.1 Motivation", "text": "We motivation our approach using a simple program chain that recursively generates a random sequence of linear segments, constrained to match a target image. Figure 2a shows the text of this program, along with samples generated from it (drawn in black) against several target images (drawn in gray). Chains generated by running the program forward do not match the targets, since forward sampling is oblivious to the constraint. Instead, we can generate constrained samples using Sequential Monte Carlo (SMC) [Ritchie et al. 2015b]. SMC generates multiple samples, or particles, in parallel, resampling them at each step of the program to favor constraint-satisfying partial outputs. This results in final chains that more closely match the target images. However, the algorithm requires many particles\u2014and therefore significant\nfunction chain(pos, ang) { var newang = ang + gaussian(0, PI/8); var newpos = pos + polarToRect(LENGTH, newang); genSegment(pos, newpos); if (flip(0.5)) chain(newpos, newang); }\nForward Samples"}, {"heading": "SMC", "text": "Samples (N = 10)\n(a)\nfunction chain_neural(pos, ang) { var newang = ang + gaussMixture(nn1(...)); var newpos = pos + polarToRect(LENGTH, newang); genSegment(pos, newpos); if (flip(nn2(...))) chain_neural(newpos, newang); }\nForward Samples\nSMC Samples (N = 10)\n(b)\nFigure 2: Transforming a simple linear chain model into a neurally-guided procedural model. (a) The original program. When the program\u2019s output (shown in black) is constrained to match a target image (shown in gray), forward sampling gives poor results. SMC sampling performs better but requires far more than 10 particles to achieve good results for all targets. (b) The neurally-guided program, where parameters of random choices are computed via neural networks. The neural nets receive the target image and all previous random choices as input (abstracted as \u201c...\u201d; see Figure 3b). Once trained, forward sampling from this program adheres closely to the target image, and SMC with 10 particles consistently produces good results.\ncomputation\u2014to produce acceptable results. Figure 2a shows that N = 10 particles is not sufficient.\nIn an ideal world, we would not need costly inference algorithms to generate constraint-satisfying results. Instead, we would have access to an \u2018oracle\u2019 program, chain_perfect, that perfectly fills in the target image when run forward. What form might this program take? At each step, it would need access to the target image, to know where to grow the chain next. It would also need to see the output it has already generated, to know when it has filled the target and can stop growing the chain.\nOur insight is that while oracle programs such as chain_perfect can be difficult or impossible to write by hand, it is possible to learn a program chain_neural that comes close. Figure 2b shows our approach. For each random choice in the program text (e.g. gaussian, flip), we replace the parameters of that choice with the output of a neural network. This neural network\u2019s inputs (abstracted as \u201c...\u201d) include the target image as well the choices the program has made thus far. The network thus shapes the distribution over possible choices, guiding the programs\u2019s future output based on the target image and its past output. These neural nets affect both continuous choices (e.g. angles) as well as control flow decisions (e.g. recursion): they dictate where the chain goes next, as well as whether it keeps going at all. For continuous choices such as gaussian, we also modify the program to sample from a mixture distribution. This helps the program handle situations where the constraints permit multiple distinct choices (e.g. in which direction to start the chain for the circle-shaped target image in Figure 2).\nWhen properly trained, a neurally-guided procedural model such as chain_neural generates constraint-satisfying results more efficiently than its un-guided counterpart. Figure 2b shows example outputs from chain_neural. Forward samples adhere closely to the target images, and SMC with 10 particles is sufficient to produce chains that fully fill the target shape. The next sections of the paper describe the process of building and training these neurally-guided procedural models in more detail."}, {"heading": "3.2 System Overview", "text": "Figure 3 shows a high-level overview of our workflow for defining, training, and using neurally-guided procedural models. It consists of the following steps:\nTransform The procedural model is first transformed by inserting one neural network for each random choice in the program text and turning continuous random choices into mixture distributions (Figure 3a-b). The network receives as input the constraint (e.g. a target image) and all previously-made random choices (shown grayed out in Figure 3a-b) and outputs the parameters for the choice (e.g. Gaussian means, variances, and mixture weights). We perform this transformation manually; it could be automated via source-tosource compilation. The neural networks can capture multiple different constraints, but an appropriate architecture for them depends on the generative paradigm and the output domain of the procedural model (e.g. images, 3D models, etc.) In Section 5, we present an architecture for 2D L-system-like procedural models which generate images. In particular, we describe how our implementation converts the previous random choices into a fixed-width vector appropriate for input to a neural net.\nGenerate Given a constraint, such as a target image, Sequential Monte Carlo generates samples from the constrained procedural model (Figure 3c). Our system uses the version of SMC for probabilistic programs presented by Ritchie et al. [2015b], where particles are resampled after the program generates a new piece of geometry. It also uses the trained models as importance samplers for this SMC algorithm when generating final results.\nTrain The generated samples are then used to train the neural networks: the desired outcome is a set of network parameters that make the model more likely to generate these samples when run forward. We derive the learning objective in Section 4 and the details of our stochastic gradient learning method in Section 5. The trained neurally-guided model can then quickly generate more samples, which can serve as further training data for refining the model, if desired."}, {"heading": "4 Mathematical Foundations", "text": "Having outlined our approach, we now formally define neurallyguided procedural models. For our purposes, a procedural model is a generative probabilistic model of the following form:\nPM(x) = |x|\u220f i=1 pi(xi; \u03a6i(x1, . . . ,xi\u22121))\nHere, x is the vector of random choices the model makes as it executes (the dimensionality of x may be variable, as with recursive procedural models such as stochastic L-systems). The pi\u2019s are local probability distributions from which each successive random choice is drawn. pi is parameterized by a set of parameters (e.g. mean and variance, for a Gaussian distribution), which are determined by some function \u03a6i of the previous random choices x1, . . . ,xi\u22121. The total probability density is the product of these local probabilities, according to the chain rule.\nA constrained procedural model is a procedural model whose probability distribution is modulated by some likelihood function `(x, c), i.e. a scoring function indicating how well an output of the model satisfies some constraint c. For example, c could be an image, with `(\u00b7, c) measuring similarity to that image. By Bayes\u2019 rule:\nPCM(x|c) = 1\nZ \u00b7 PM(x) \u00b7 `(x, c)\nwhere Z is a normalizing constant. The set of all constraints c supported by the procedural model forms the constraint space C (e.g. all images, all binary mask images, etc.)\nA neurally-guided procedural model modifies a procedural model by, for each local probability pi, replacing the parameter function \u03a6i with a neural network:\nPGM(x|c; \u03b8) = |x|\u220f i=1 p\u0303i(xi; NNi(x1, . . . ,xi\u22121, c; \u03b8))\nThe neural nets receive the previous random choice values and the constraint as input, and are themselves parameterized by \u03b8. p\u0303i is a mixture distribution if random choice i is continuous; otherwise, p\u0303i = pi.\nIn training a neurally-guided procedural model, our goal is to find the parameters \u03b8 such that PGM is as close as possible to PCM for all supported constraints. Formally, we seek to minimize the conditional KL divergence DKL(PCM||PGM). Given some prior distribution P (c) over constraints c \u2208 C, our optimization objective is:\nmin \u03b8 DKL(PCM||PGM) (1)\n= min \u03b8\nEP (c) [ EPCM(x|c) [ log\nPCM(x|c) PGM(x|c; \u03b8) ]] = min\n\u03b8 EP (c)\n[ EPCM(x|c) [ logPCM(x|c)\u2212 logPGM(x|c; \u03b8) ]] = max\n\u03b8 EP (c)\n[ EPCM(x|c) [ logPGM(x|c; \u03b8)\u2212 logPCM(x|c) ]] = max\n\u03b8 EP (c)\n[ EPCM(x|c) [ logPGM(x|c; \u03b8) ]] \u2248 max\n\u03b8\n1\nN N\u2211 s=1 logPGM(xs|cs; \u03b8)\nxs \u223c PCM(x) , cs \u223c P (c)\nIn the last step, we approximate the expectations with an average over a finite set of samples xs, cs drawn from the constrained procedural model PCM using SMC and the constraint prior P (c). If we view these samples as a training data set, then this optimization objective is simply maximizing the log-likelihood of the training data under the neurally-guided model PGM.\nWith such a training set in hand, optimization proceeds via stochastic gradient ascent using the gradient\n\u2207 logPGM(x|c; \u03b8)\n= |x|\u2211 i=1 \u2207 log p\u0303i(xi; NNi(x1, . . . ,xi\u22121, c; \u03b8)) (2)\nIs is worth noting that DKL(PCM||PGM) is not the only measure of distance between probability distributions we could have used. In particular, several related works have used the other direction of KL divergence, DKL(PGM||PCM), due to its attractive properties: it requires training samples from PGM, which are much less expensive to generate than samples from PCM. It is the optimization objective used in many variational inference algorithms [Wingate and Weber 2012; J. Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992]. When used for procedural modeling, however, this objective leads to models whose outputs lack diversity, making them unsuitable for generating visually-varied content. This behavior is due to a well-known property of the objective: minimizing it produces approximating distributions that are overly-compact, i.e. concentrating their probability mass in a smaller volume of the state space than the true distribution being approximated [MacKay 2002].\nFully Connected\n(FC) tanh FC Bounds OutputParams\nnf\nna\nnp nf 2\n36c\n36c\nnf 2\nnp\nfunction \u00a0branch( \u00a0pos, \u00a0ang, \u00a0width \u00a0) \u00a0{...} \u00a0\nTarget Image (50x50)"}, {"heading": "Convolve + Downsample", "text": ""}, {"heading": "Convolve +", "text": "Downsample\nCurrent Partial Output (50x50)\nTarget Image Features\nPartial Output Features\nLocal State Features\nFigure 4: Neural network architecture for image-matching procedural models. The network uses a multilayer perceptron which takes a vector of features as input and ouputs the parameters for a random choice probability distribution. The input features come from three sources. Local State Features are the arguments to the function in which the random choice occurs. Target Image Features come from 3x3 pixel windows of the target image, extracted at multiple resolutions, around the procedural model\u2019s current position. Partial Output Features are analogous windows extracted from the partial image the model has generated. All of these features can be computed from the target image and the sequence of random choices made thus far."}, {"heading": "5 Implementation", "text": "In this section, we describe an implementation of neurally-guided accumulative procedural models: models that iteratively or recursively add new geometry to a structure. Most growth models, such as L-systems, are accumulative [Prusinkiewicz and Lindenmayer 1990]. This is in contrast with other modeling paradigms: spatial subdivision, such as architectural split grammars [Mu\u0308ller et al. 2006]; object subdivision, such as fractal terrain [Lewis 1987]; or simulation, such as erosion-based terrain [S\u030ct\u2019ava et al. 2008]. For our purposes, a procedural model is accumulative if, while executing, it provides a \u2018current position\u2019: a point p from which geometry generation will continue. We focus on 2D models (p \u2208 R2), though the techniques we present extend naturally to 3D.\nWe first describe the neural network architecture used by the neurally-guided models before giving details on how we train them."}, {"heading": "5.1 Network Architecture", "text": "Our neural networks take as input the constraint c (in this case, a target image) and all previously-made random choices, and output the parameters of a random choice. Figure 4 shows our network architecture. We use a multilayer perceptron (MLP) architecture, because it is simple, easy to scale, and is a universal function approximator [Rumelhart et al. 1986; Cybenko 1989]. Our MLP takes nf inputs, has one hidden layer of size nf/2 with a tanh nonlinearity, and has np outputs, where np is the number of parameters the random choice expects. Since some parameters are bounded (e.g. Gaussian variance must be positive), each output is remapped via an appropriate bounding transform (e.g. ex for non-negative parameters). We experimented with more hidden layers but found that this did not improve performance.\nThe inputs for the MLP come from several sources, each providing the network with decision-critical information. All of these features can be computed from the target image and the choices-so-far; for efficiency, we compute them incrementally as the program runs.\nLocal State Features The first set of relevant data is the local state of the procedural model: its current position p, the current orientation of any local reference frame, its current recursion depth, etc. Our networks access this information via the arguments of the function in which the random choice occurs. We extract all na scalar arguments, normalize them to [\u22121, 1], and pass them to the MLP.\nTarget Image Features To make appropriate decisions for matching a target image, the network must have access to that image. The raw pixels provide too much data; we need to summarize the relevant image contents. Convolutional neural networks reduce an image to a fixed-width feature vector but are aimed at classification tasks: they detect features but are intentionally invariant to where those features occur [Krizhevsky et al. 2012].\nInstead, we use a different, location-sensitive architecture. We extract a 3x3 window of pixels around the model\u2019s current position p. We do this at four different resolution levels, with each level computed by convolving the previous level with a 3x3 kernel and then downsampling via a 2x2 box filter. For a image with channel depth c, this results in 36c features. Together, these features summarize what the target image looks like from the procedural model\u2019s current position, where resolution decreases with distance. This architecture is similar to the foveated \u2018glimpses\u2019 used in recent work on neural models of visual attention [Mnih et al. 2014].\nPartial Output Features The target image features provide the network with information it needs to generate matching content with accuracy (e.g. how to stay within a target shape) However, they do not provide the information necessary to achieve completeness (e.g. how to completely fill in a target shape). To give the network this capability, we also extract multi-resolution windows from the partial output image generated by the procedural model thus far. This adds another 36c input features.\nThe parameters \u03b8 of this architecture consist of the weights and biases for both fully-connected layers in the MLP, as well as the kernel weights and biases for the three convolution + downsampling\nlayers on each image. Each network typically has around several thousand such parameters. For example, given a program with four local features (position x, position y, angle, width) which targets a one-channel image, the network that predicts the parameters of a four-component Gaussian mixture has 3466 parameters."}, {"heading": "5.2 Training", "text": "We train neurally-guided procedural models by stochastic gradient ascent using the gradient in Equation 2. Our system computes this gradient via backpropagation from the log p\u0303i\u2019s to the neural network parameters \u03b8. We use the Adam algorithm for stochastic gradient optimization, with \u03b1 = \u03b2 = 0.75 and an initial learning rate of 0.01 [Kingma and Ba 2015]. We found that a mini-batch size of one worked best in our experiments: more frequent gradient updates led to faster convergence than less-frequent-but-less-noisy updates. We terminate training after 20000 gradient updates."}, {"heading": "5.3 Implementation Details", "text": "We implemented our prototype system in the Javascript-based probabilistic programming language WebPPL [Goodman and Stuhlmu\u0308ller 2014], with neural networks implemented using an open-source Javascript library for neural computation.1 The source code for our system is available at [LINK ANONYMIZED]."}, {"heading": "6 Experiments", "text": "In this section, we qualitatively and quantitatively evaluate how well our neurally-guided procedural models capture image-based constraints. We first describe our databases of target images before presenting the details of several experiments. All timing data reported in this section was collected on an Intel Core i7-3840QM machine with 16GB RAM running OSX 10.10.5."}, {"heading": "6.1 Image Datasets", "text": "As shown in Equation 1, each training sample from a procedural model must be paired with a constraint c drawn from a prior P (c) over possible constraints. During training, we sample target images uniformly at random from a database of training images. In our experiments, we use the following image collections:\n\u2022 Scribbles: A set of 49 binary mask images drawn by hand with the brush tool in Photoshop. These were designed to\n1https://github.com/dritchie/adnn\ncover a range of possible shapes with thick and thin regions, high and low curvature, and different self-intersections.\n\u2022 Glyphs: A subset of 197 glyphs from the FF Tartine Script Bold typeface. Consists of all glyphs which have only one foreground connected component and at least 500 foreground pixels when rendered at 129x97.\n\u2022 PhyloPic: A set of 35 images from PhyloPic, a database of silhouettes for plants, animals, and other organisms.2\nWhen using these images for training, we augment the datasets by also including a horizontally-mirrored duplicate of each image. We also annotate each image with a starting point and starting direction from which to initialize the execution of a procedural model. Figure 5 shows some representative images from each collection."}, {"heading": "6.2 Shape Matching", "text": "In our first set of experiments, we train neurally-guided procedural models to capture 2D shape matching constraints, where the target shape is specified as a binary mask image. If D is the spatial domain of the image, and I(x) is the function which renders the current partial output defined by random choices x, then the likelihood function for this constraint is\n`shape(x, c) = N ( sim(I(x), c)\u2212 sim(0, c)\n1\u2212 sim(0, c) , 1, \u03c3shape) (3)\nsim(I1, I2) =\n\u2211 p\u2208D w(p) \u00b7 1{I1(p) = I2(p)}\u2211\np\u2208D w(p)\nw(p) =  1 if I2(p) = 0 1 if ||\u2207I2(p)||= 1 wfilled if ||\u2207I2(p)||= 0\nwhere N is the normal distribution. This function encourages the rendered image to be similar to the target mask, where similarity is normalized against the target\u2019s similarity to an empty image 0. Each pixel p\u2019s contribution to the similarity is weighted by w(p), determined by whether the target mask is empty, filled, or has an edge at that pixel. We use wfilled = 0.6\u0304, so that empty and edge pixels are worth 1.5 times as much as filled pixels. This encourages the program to match perceptually-important contours before filling in flat regions. We set \u03c3shape = 0.02 in all experiments.\nWe wrote a WebPPL program which recursively generates vines with leaves and flowers and then trained a neurally-guided version of this program to capture the above likelihood. The model was trained on 10000 sample traces, each generated using SMC with 600 particles. Target images were drawn uniformly at random from the Scribbles dataset. Each sample took on average 17 seconds to generate; parallelized across four CPU cores, the entire set of samples took approximately 12 hours to generate (later in this section, we show that far fewer samples are actually needed). Training took 55 minutes in our single-threaded Javascript implementation. These times could be reduced with more efficient implementations (e.g. leveraging GPUs for training).\nFigure 1 shows example outputs from the vines program. The weighting scheme of `shape causes the geometry to adhere to target shape contours, making the shape recognizable without cluttering interior regions and obscuring the vines\u2019 structural characteristics. This behavior is not easy to achieve with a purely generative space-filling approach such as environmentally-sensitive Lsystems [Prusinkiewicz et al. 1994], but it is simple to specify with constraints. The top row outputs were generated using 10-particle\n2http://phylopic.org\nSMC with the trained neurally-guided model, which reliably produces recognizable results. In contrast, 10-particle SMC with the unguided model produces totally unrecognizable results (middle row). Because the neural networks make the guided model more computationally-expensive to evaluate, a more equitable comparison is to give the unguided model the same amount of wall-clock time as the guided model\u2014this correponds to \u223c 50 particles, in this case (bottom row). While the resulting outputs fare better, the target shape is still obscured. We should also note that the unguided model is unpredictable at such low particle counts; results of even this quality took many tries to obtain at 50 particles. In practice, we find that the unguided model needs\u223c200 particles to reliably match the performance of the guided model. Figure 7 shows more outputs from the vines program, and Figure 8 shows example outputs from a neurally-guided procedural lightning program.\nWe test each model on the images in the Glyph dataset and report the median normalized similarity-to-target achieved (i.e. argument one to the Gaussian in Equation 3). Figure 6a plots this average similarity as the number of SMC particles increases. The performance of the neurally-guided models improves with the addition of more features; at 10 particles, the full model is already near the peak performance asymptote. Figure 6b shows the wall-clock time\neach method requires as the desired average similarity increases. The vertical gap between the two curves shows the speedup given by neural guidance, which can be as high as 10x. Note that we trained on the Scribbles dataset but tested on the Glyphs dataset; these results suggest that our models can generalize to qualitativelydifferent previously-unseen images.\nFigure 9 shows the benefit of using mixture distributions for continuous random choices in the guided model. The experimental setup is the same as in Figure 6. We compare a model which uses 4-component mixture distributions with a no-mixture model. The with-mixtures model provides a noticeable performance boost, which we alluded to in Section 3: when matching complex shapes with junctions and intersections, such as the crossing of the letter \u2018t,\u2019 the program benefits from modeling uncertainty at these points with multi-modal distributions. We found 4 mixture components sufficient for our examples.\nWe also investigate how the number of training samples affects performance. Figure 10 plots the median similarity at 10 particles as training set size increases. Performance increases rapidly for the first few hundred samples before appearing to level off (the noise in the curve is due to randomness in neural net training initialization). This suggests that \u223c1000 sample traces is sufficient, which may seem surprising, as many published deep learning systems require millions of training examples [Krizhevsky et al. 2012]. In our case, each training trace contains up to thousands of random choices, each of which provides a learning signal\u2014in this way, the training data is \u201cbigger\u201d than it appears. Our implementation can generate 1000 samples in just over an hour using four CPU cores. As mentioned previously, this time could be reduced by \u2018boostrapping\u2019 the system: training on smaller subsets of data and using the partially-learned model to generate further data faster."}, {"heading": "6.3 Stylized \u201cCircuit\u201d Design", "text": "Thus far, we have focused on image-matching constraints. However, the architecture we have presented can learn other types of image-based constraints. In this section, we constrain the vines program to generate outputs which resemble stylized circuit designs.\nDense packing of long wire traces is one of the most striking visual characteristics of circuit boards. To achieve dense packing, we encourage the program to fill a certain percentage \u03c4 of the image (\u03c4 = 0.5 in the subsequent results). To mimic the appearance\nof traces, we encourage the output image to have a dense, highmagnitude gradient field, as the vines program can best achieve this result by creating many long rectilinear or diagonal edges. These constraints result in the following likelihood:\n`circ(x) = N (edge(I(x)) \u00b7 (1\u2212 \u03b7(fill(I(x)), \u03c4)), 1, \u03c3circ) (4)\nedge(I) = 1 |D| \u2211 p\u2208D ||\u2207I(p)||\nfill(I) = 1 |D| \u2211 p\u2208D I(p)\nwhere \u03b7(x, x\u0304) is the relative error of x from x\u0304 and \u03c3circ = 0.01. Finally, we also include a separate term that penalizes the program from generating geometry outside the bounds of the image; this encourages the program to fill in a rectangular \u201cdie\u201d-like region.\nTo guide this program, we use the same architecture as before, minus the target image features (since there is no target image). We train the neurally-guided model using 2000 traces generated using SMC with 600 particles. Sample generation took about 10 hours on four CPU cores, and training took just under two hours. Figure 11 shows some outputs from this program, and Figure 12 shows a performance comparison between unguided and neurally-guided models for this task. As with the shape matching examples, the neurally-guided model generates high-scoring results significantly faster than the unguided model."}, {"heading": "7 Discussion and Future Work", "text": "This paper introduced neurally-guided procedural models: constrained procedural models that use neural networks to capture constraint-induced dependencies. We developed a mathematical framework for defining and training such models. We also described a specific neural architecture for accumulative models that generate images. Finally, we evaluated the performance of neurally-guided models, demonstrating that they can generate highquality results significantly faster than unguided models."}, {"heading": "7.1 Limitations", "text": "One limitation of our system is its need for training data, which must be generated via expensive inference. This can be a significant up-front cost, especially for computationally-expensive models. Thus, our method is not well-suited for scenarios where the procedural model changes rapidly, such as speeding up the inner loop of a development and debugging cycle. Instead, it is best suited for scenarios where the model is fixed, such as deploying a finalized procedural model as part of a design tool. It may be particularly attractive for online, multi-user deployments, where the system can gather example results from the community, periodically retrain, and push the updated procedural model to users.\nOur method is also not well-suited for capturing hard constraints, which some visual effects necessitate (e.g. symmetries), as it requires a continuous probability for each training sample. While hard constraints can sometimes be usefully approximated with tight soft constraints, neural networks such as ours are best at approximating noisy and/or random functions, not precise, deterministic relationships. Other techniques are needed for generatively capturing these kinds of constraints."}, {"heading": "7.2 Future Work", "text": "The neural guide architecture we presented applies to imagegenerating programs. How might we extend it to other output domains? The architecture must compactly represent the model\u2019s partial output at any point in the program. For 3D modeling, our architecture extends naturally to 3D, e.g. using voxel grids instead of images. For other output domains, it may be possible to develop architectures that learn a partial output state representation, as in recent work on recurrent sequence generation [Graves 2013].\nOur architecture also focuses on accumulative procedural models, but applications of other generative paradigms could also benefit from neural guidance. One example is texture generation, which repeatedly generates content across its entire domain, often in a multiscale fashion. In such a setting, the guide model cannot rely on a \u201ccurrent position\u201d for extracting decision-critical features. It might\ninstead learn what parts of the current partial output are relevant, as recently-developed visual attention models learn where to look in an image to make classification decisions [Mnih et al. 2014].\nOur method could also be extended to inference algorithms beyond Sequential Monte Carlo. For example, Markov Chain Monte Carlo works better when the likelihood of a partial output is not predictive of the likelihood of the final output (e.g. generating stable arches, where the structure is unstable until finished). Just as neurally-guided models are efficient importance samplers for SMC, they might also serve as efficient proposal distributions for MCMC.\nBy adding neural networks which predict random choice parameters, we have only explored one simple program transformation to enable constraint capture. More extensive transformations, such as changing the control flow of the program, may be necessary for capturing especially complex constraints. One step in this direction would be to combine our approach with the grammar-splitting technique of Dang et al. [2015].\nUltimately, we envision a future in which procedural models learn to encode complex constraints using purely generative methods, so that forward sampling alone produces beautiful results. The work we have presented in this paper takes a step toward that goal, but it is only the beginning. We hope that our work inspires other researchers to develop new and better neural network architectures and program transformations to attack this problem."}, {"heading": "BROOKS, S., GELMAN, A., JONES, G., AND MENG, X. 2011.", "text": "Handbook of Markov Chain Monte Carlo. CRC Press.\nCYBENKO, G. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems."}, {"heading": "DANG, M., LIENHARD, S., CEYLAN, D., NEUBERT, B.,", "text": "WONKA, P., AND PAULY, M. 2015. Interactive Design of Probability Density Functions for Shape Grammars.\nDOUCET, A., DE FREITAS, N., AND GORDON, N., Eds. 2001. Sequential Monte Carlo Methods in Practice. Springer.\nGOODMAN, N. D., AND STUHLMU\u0308LLER, A., 2014. The Design and Implementation of Probabilistic Programming Languages. http://dippl.org. Accessed: 2015-12-23.\nGRAVES, A. 2013. Generating Sequences With Recurrent Neural Networks. CoRR abs/1308.0850.\nGU, S., GHAHRAMANI, Z., AND TURNER, R. E. 2015. Neural Adaptive Sequential Monte Carlo. In NIPS 2015.\nJ. MANNING, R. RANGANATH, K. N., AND BLEI, D. 2014. Black Box Variational Inference. In AISTATS 2014.\nKINGMA, D. P., AND BA, J. 2015. Adam: A Method for Stochastic Optimization. In ICLR 2015.\nKINGMA, D. P., AND WELLING, M. 2014. Auto-Encoding Variational Bayes. In ICLR 2014."}, {"heading": "KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E. 2012.", "text": "ImageNet Classification with Deep Convolutional Neural Networks. In NIPS 2012.\nLEWIS, J. P. 1987. Generalized Stochastic Subdivision. ACM Trans. Graph. 6, 3."}, {"heading": "LIN, S., RITCHIE, D., FISHER, M., AND HANRAHAN, P. 2013.", "text": "Probabilistic Color-by-Numbers: Suggesting Pattern Colorizations Using Factor Graphs. In SIGGRAPH 2013.\nMACKAY, D. J. C. 2002. Information Theory, Inference & Learning Algorithms. Cambridge University Press."}, {"heading": "MARKS, J., ANDALMAN, B., BEARDSLEY, P. A., FREEMAN, W.,", "text": "GIBSON, S., HODGINS, J., KANG, T., MIRTICH, B., PFISTER, H., RUML, W., RYALL, K., SEIMS, J., AND SHIEBER, S. 1997. Design galleries: A general approach to setting parameters for computer graphics and animation. In SIGGRAPH 1997."}, {"heading": "MERRELL, P., SCHKUFZA, E., LI, Z., AGRAWALA, M., AND", "text": "KOLTUN, V. 2011. Interactive Furniture Layout Using Interior Design Guidelines. In SIGGRAPH 2011.\nMNIH, A., AND GREGOR, K. 2014. Neural Variational Inference and Learning in Belief Networks. In ICML 2014.\nMNIH, V., HEESS, N., GRAVES, A., AND KAVUKCUOGLU, K. 2014. Recurrent Models of Visual Attention. In NIPS 2014."}, {"heading": "MU\u0308LLER, P., WONKA, P., HAEGLER, S., ULMER, A., AND", "text": "VAN GOOL, L. 2006. Procedural Modeling of Buildings. In SIGGRAPH 2006.\nME\u030cCH, R., AND PRUSINKIEWICZ, P. 1996. Visual Models of Plants Interacting with Their Environment. In SIGGRAPH 1996.\nPRUSINKIEWICZ, P., AND LINDENMAYER, A. 1990. The Algorithmic Beauty of Plants. Springer-Verlag New York, Inc.\nPRUSINKIEWICZ, P., JAMES, M., AND ME\u030cCH, R. 1994. Synthetic Topiary. In SIGGRAPH 1994."}, {"heading": "REZENDE, D. J., MOHAMED, S., AND WIERSTRA, D. 2014.", "text": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In ICML 2014."}, {"heading": "RITCHIE, D., LIN, S., GOODMAN, N. D., AND HANRAHAN, P.", "text": "2015. Generating Design Suggestions under Tight Constraints with Gradient-based Probabilistic Programming. In Eurographics 2015.\nRITCHIE, D., MILDENHALL, B., GOODMAN, N. D., AND HANRAHAN, P. 2015. Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo. In SIGGRAPH 2015."}, {"heading": "RUMELHART, D. E., HINTON, G. E., AND WILLIAMS, R. J.", "text": "1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1. MIT Press, ch. Learning Internal Representations by Error Propagation.\nSCHWARZ, M., AND WONKA, P. 2014. Procedural Design of Exterior Lighting for Buildings with Complex Constraints. ACM Trans. Graph. 33, 5."}, {"heading": "STAVA, O., PIRK, S., KRATT, J., CHEN, B., MCH, R., DEUSSEN,", "text": "O., AND BENES, B. 2014. Inverse Procedural Modelling of Trees. Computer Graphics Forum 33, 6."}, {"heading": "TALTON, J. O., LOU, Y., LESSER, S., DUKE, J., ME\u030cCH, R., AND", "text": "KOLTUN, V. 2011. Metropolis Procedural Modeling. ACM Trans. Graph. 30, 2."}, {"heading": "VANEGAS, C. A., GARCIA-DORADO, I., ALIAGA, D. G.,", "text": "BENES, B., AND WADDELL, P. 2012. Inverse Design of Urban Procedural Models. In SIGGRAPH Asia 2012.\nS\u030cT\u2019AVA, O., BENES\u030c, B., BRISBIN, M., AND KR\u030cIVA\u0301NEK, J. 2008. Interactive Terrain Modeling Using Hydraulic Erosion. In SCA 2008.\nWILLIAMS, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8.\nWINGATE, D., AND WEBER, T. 2012. Automated Variational Inference in Probabilistic Programming. In NIPS 2012 Workshop on Probabilistic Programming."}, {"heading": "WONG, M. T., ZONGKER, D. E., AND SALESIN, D. H. 1998.", "text": "Computer-generated Floral Ornament. In SIGGRAPH 1998."}, {"heading": "YEH, Y.-T., YANG, L., WATSON, M., GOODMAN, N. D., AND", "text": "HANRAHAN, P. 2012. Synthesizing Open Worlds with Constraints Using Locally Annealed Reversible Jump MCMC. In SIGGRAPH 2012."}, {"heading": "YU, L.-F., YEUNG, S.-K., TERZOPOULOS, D., AND CHAN, T. F.", "text": "2012. DressUp!: Outfit Synthesis Through Automatic Optimization. In SIGGRAPH Asia 2012."}, {"heading": "YUMER, M. E., ASENTE, P., MECH, R., AND KARA, L. B. 2015.", "text": "Procedural Modeling Using Autoencoder Networks. In UIST 2015."}, {"heading": "ZHU, L., XU, W., SNYDER, J., LIU, Y., WANG, G., AND GUO,", "text": "B. 2012. Motion-guided Mechanical Toy Modeling. In SIGGRAPH Asia 2012."}], "references": [{"title": "Guided Procedural Modeling", "author": ["B. BENE\u0160", "O. \u0160AVA", "R. M\u011aCH", "G. MILLER"], "venue": "Eurographics 2011.", "citeRegEx": "BENE\u0160 et al\\.,? 2011", "shortCiteRegEx": "BENE\u0160 et al\\.", "year": 2011}, {"title": "Handbook of Markov Chain Monte Carlo", "author": ["S. BROOKS", "A. GELMAN", "G. JONES", "X. MENG"], "venue": "CRC Press.", "citeRegEx": "BROOKS et al\\.,? 2011", "shortCiteRegEx": "BROOKS et al\\.", "year": 2011}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. CYBENKO"], "venue": "Mathematics of Control, Signals and Systems.", "citeRegEx": "CYBENKO,? 1989", "shortCiteRegEx": "CYBENKO", "year": 1989}, {"title": "Interactive Design of Probability Density Functions for Shape Grammars", "author": ["M. DANG", "S. LIENHARD", "D. CEYLAN", "B. NEUBERT", "P. WONKA", "M. PAULY"], "venue": null, "citeRegEx": "DANG et al\\.,? \\Q2015\\E", "shortCiteRegEx": "DANG et al\\.", "year": 2015}, {"title": "Sequential Monte Carlo Methods in Practice", "author": ["A. DOUCET", "N. DE FREITAS", "N. GORDON", "Eds."], "venue": "Springer.", "citeRegEx": "DOUCET et al\\.,? 2001", "shortCiteRegEx": "DOUCET et al\\.", "year": 2001}, {"title": "The Design and Implementation of Probabilistic Programming Languages. http://dippl.org", "author": ["N.D. GOODMAN", "A. STUHLM\u00dcLLER"], "venue": null, "citeRegEx": "GOODMAN and STUHLM\u00dcLLER,? \\Q2014\\E", "shortCiteRegEx": "GOODMAN and STUHLM\u00dcLLER", "year": 2014}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["A. GRAVES"], "venue": "CoRR abs/1308.0850.", "citeRegEx": "GRAVES,? 2013", "shortCiteRegEx": "GRAVES", "year": 2013}, {"title": "Neural Adaptive Sequential Monte Carlo", "author": ["S. GU", "Z. GHAHRAMANI", "R.E. TURNER"], "venue": "NIPS 2015.", "citeRegEx": "GU et al\\.,? 2015", "shortCiteRegEx": "GU et al\\.", "year": 2015}, {"title": "Black Box Variational Inference", "author": ["J. MANNING", "K.N.R. RANGANATH", "D. BLEI"], "venue": "AISTATS 2014.", "citeRegEx": "MANNING et al\\.,? 2014", "shortCiteRegEx": "MANNING et al\\.", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["KINGMA D.P.", "BA", "J."], "venue": "ICLR 2015.", "citeRegEx": "P. et al\\.,? 2015", "shortCiteRegEx": "P. et al\\.", "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. KINGMA", "M. WELLING"], "venue": "ICLR 2014.", "citeRegEx": "KINGMA and WELLING,? 2014", "shortCiteRegEx": "KINGMA and WELLING", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "NIPS 2012.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Generalized Stochastic Subdivision", "author": ["J.P. LEWIS"], "venue": "ACM Trans. Graph. 6, 3.", "citeRegEx": "LEWIS,? 1987", "shortCiteRegEx": "LEWIS", "year": 1987}, {"title": "Probabilistic Color-by-Numbers: Suggesting Pattern Colorizations Using Factor Graphs", "author": ["S. LIN", "D. RITCHIE", "M. FISHER", "P. HANRAHAN"], "venue": "SIGGRAPH 2013.", "citeRegEx": "LIN et al\\.,? 2013", "shortCiteRegEx": "LIN et al\\.", "year": 2013}, {"title": "Information Theory, Inference & Learning Algorithms", "author": ["D.J.C. MACKAY"], "venue": "Cambridge University Press.", "citeRegEx": "MACKAY,? 2002", "shortCiteRegEx": "MACKAY", "year": 2002}, {"title": "Design galleries: A general approach to setting parameters for computer graphics and animation", "author": ["J. MARKS", "B. ANDALMAN", "P.A. BEARDSLEY", "W. FREEMAN", "S. GIBSON", "J. HODGINS", "T. KANG", "B. MIRTICH", "H. PFISTER", "W. RUML", "K. RYALL", "J. SEIMS", "S. SHIEBER"], "venue": "SIGGRAPH 1997.", "citeRegEx": "MARKS et al\\.,? 1997", "shortCiteRegEx": "MARKS et al\\.", "year": 1997}, {"title": "Interactive Furniture Layout Using Interior Design Guidelines", "author": ["P. MERRELL", "E. SCHKUFZA", "Z. LI", "M. AGRAWALA", "V. KOLTUN"], "venue": "SIGGRAPH 2011.", "citeRegEx": "MERRELL et al\\.,? 2011", "shortCiteRegEx": "MERRELL et al\\.", "year": 2011}, {"title": "Neural Variational Inference and Learning in Belief Networks", "author": ["A. MNIH", "K. GREGOR"], "venue": "ICML 2014.", "citeRegEx": "MNIH and GREGOR,? 2014", "shortCiteRegEx": "MNIH and GREGOR", "year": 2014}, {"title": "Recurrent Models of Visual Attention", "author": ["V. MNIH", "N. HEESS", "A. GRAVES", "K. KAVUKCUOGLU"], "venue": "NIPS 2014.", "citeRegEx": "MNIH et al\\.,? 2014", "shortCiteRegEx": "MNIH et al\\.", "year": 2014}, {"title": "Procedural Modeling of Buildings", "author": ["P. M\u00dcLLER", "P. WONKA", "S. HAEGLER", "A. ULMER", "L. VAN GOOL"], "venue": "SIGGRAPH 2006.", "citeRegEx": "M\u00dcLLER et al\\.,? 2006", "shortCiteRegEx": "M\u00dcLLER et al\\.", "year": 2006}, {"title": "Visual Models of Plants Interacting with Their Environment", "author": ["R. M\u011aCH", "P. PRUSINKIEWICZ"], "venue": "SIGGRAPH 1996.", "citeRegEx": "M\u011aCH and PRUSINKIEWICZ,? 1996", "shortCiteRegEx": "M\u011aCH and PRUSINKIEWICZ", "year": 1996}, {"title": "The Algorithmic Beauty of Plants", "author": ["P. PRUSINKIEWICZ", "A. LINDENMAYER"], "venue": "Springer-Verlag New York, Inc.", "citeRegEx": "PRUSINKIEWICZ and LINDENMAYER,? 1990", "shortCiteRegEx": "PRUSINKIEWICZ and LINDENMAYER", "year": 1990}, {"title": "Synthetic Topiary", "author": ["P. PRUSINKIEWICZ", "M. JAMES", "R. M\u011aCH"], "venue": "SIGGRAPH 1994.", "citeRegEx": "PRUSINKIEWICZ et al\\.,? 1994", "shortCiteRegEx": "PRUSINKIEWICZ et al\\.", "year": 1994}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D.J. REZENDE", "S. MOHAMED", "D. WIERSTRA"], "venue": "ICML 2014.", "citeRegEx": "REZENDE et al\\.,? 2014", "shortCiteRegEx": "REZENDE et al\\.", "year": 2014}, {"title": "Generating Design Suggestions under Tight Constraints with Gradient-based Probabilistic Programming", "author": ["D. RITCHIE", "S. LIN", "N.D. GOODMAN", "P. HANRAHAN"], "venue": "Eurographics 2015.", "citeRegEx": "RITCHIE et al\\.,? 2015", "shortCiteRegEx": "RITCHIE et al\\.", "year": 2015}, {"title": "Controlling Procedural Modeling Programs with Stochastically-Ordered Sequential Monte Carlo", "author": ["D. RITCHIE", "B. MILDENHALL", "N.D. GOODMAN", "P. HANRAHAN"], "venue": "SIGGRAPH 2015.", "citeRegEx": "RITCHIE et al\\.,? 2015", "shortCiteRegEx": "RITCHIE et al\\.", "year": 2015}, {"title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol", "author": ["D.E. RUMELHART", "G.E. HINTON", "R.J. WILLIAMS"], "venue": "1. MIT Press, ch. Learning Internal Representations by Error Propagation.", "citeRegEx": "RUMELHART et al\\.,? 1986", "shortCiteRegEx": "RUMELHART et al\\.", "year": 1986}, {"title": "Procedural Design of Exterior Lighting for Buildings with Complex Constraints", "author": ["M. SCHWARZ", "P. WONKA"], "venue": "ACM Trans. Graph. 33, 5.", "citeRegEx": "SCHWARZ and WONKA,? 2014", "shortCiteRegEx": "SCHWARZ and WONKA", "year": 2014}, {"title": "Inverse Procedural Modelling of Trees", "author": ["O. STAVA", "S. PIRK", "J. KRATT", "B. CHEN", "R. MCH", "O. DEUSSEN", "B. BENES"], "venue": "Computer Graphics Forum 33, 6.", "citeRegEx": "STAVA et al\\.,? 2014", "shortCiteRegEx": "STAVA et al\\.", "year": 2014}, {"title": "Metropolis Procedural Modeling", "author": ["J.O. TALTON", "Y. LOU", "S. LESSER", "J. DUKE", "R. M\u011aCH", "V. KOLTUN"], "venue": "ACM Trans. Graph. 30, 2.", "citeRegEx": "TALTON et al\\.,? 2011", "shortCiteRegEx": "TALTON et al\\.", "year": 2011}, {"title": "Inverse Design of Urban Procedural Models", "author": ["C.A. VANEGAS", "I. GARCIA-DORADO", "D.G. ALIAGA", "B. BENES", "P. WADDELL"], "venue": "SIGGRAPH Asia 2012.", "citeRegEx": "VANEGAS et al\\.,? 2012", "shortCiteRegEx": "VANEGAS et al\\.", "year": 2012}, {"title": "Interactive Terrain Modeling Using Hydraulic Erosion", "author": ["O. \u0160T\u2019AVA", "B. BENE\u0160", "M. BRISBIN", "J. K\u0158IV\u00c1NEK"], "venue": "SCA", "citeRegEx": "\u0160T.AVA et al\\.,? \\Q2008\\E", "shortCiteRegEx": "\u0160T.AVA et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. WILLIAMS"], "venue": "Machine Learning 8.", "citeRegEx": "WILLIAMS,? 1992", "shortCiteRegEx": "WILLIAMS", "year": 1992}, {"title": "Automated Variational Inference in Probabilistic Programming", "author": ["D. WINGATE", "T. WEBER"], "venue": "NIPS 2012 Workshop on Probabilistic Programming.", "citeRegEx": "WINGATE and WEBER,? 2012", "shortCiteRegEx": "WINGATE and WEBER", "year": 2012}, {"title": "Computer-generated Floral Ornament", "author": ["M.T. WONG", "D.E. ZONGKER", "D.H. SALESIN"], "venue": "SIGGRAPH 1998.", "citeRegEx": "WONG et al\\.,? 1998", "shortCiteRegEx": "WONG et al\\.", "year": 1998}, {"title": "Synthesizing Open Worlds with Constraints Using Locally Annealed Reversible Jump MCMC", "author": ["YEH", "Y.-T.", "L. YANG", "M. WATSON", "N.D. GOODMAN", "P. HANRAHAN"], "venue": "SIGGRAPH 2012.", "citeRegEx": "YEH et al\\.,? 2012", "shortCiteRegEx": "YEH et al\\.", "year": 2012}, {"title": "DressUp!: Outfit Synthesis Through Automatic Optimization", "author": ["YU", "L.-F.", "YEUNG", "S.-K.", "D. TERZOPOULOS", "T.F. CHAN"], "venue": "SIGGRAPH Asia 2012.", "citeRegEx": "YU et al\\.,? 2012", "shortCiteRegEx": "YU et al\\.", "year": 2012}, {"title": "Procedural Modeling Using Autoencoder Networks", "author": ["M.E. YUMER", "P. ASENTE", "R. MECH", "L.B. KARA"], "venue": "UIST 2015.", "citeRegEx": "YUMER et al\\.,? 2015", "shortCiteRegEx": "YUMER et al\\.", "year": 2015}, {"title": "Motion-guided Mechanical Toy Modeling", "author": ["ZHU L.", "XU W.", "SNYDER J.", "LIU Y.", "WANG G.", "GUO", "B."], "venue": "SIGGRAPH Asia 2012.", "citeRegEx": "L. et al\\.,? 2012", "shortCiteRegEx": "L. et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 34, "context": "It can generate fine detail that would require painstaking effort to create by hand, such as decorative floral patterns [Wong et al. 1998].", "startOffset": 120, "endOffset": 138}, {"referenceID": 15, "context": "It can even generate surprising or unexpected results, helping users to explore large or unintuitive design spaces [Marks et al. 1997; Ritchie et al. 2015a].", "startOffset": 115, "endOffset": 156}, {"referenceID": 28, "context": "Many applications demand the ability to constrain or control procedural models: making their outputs resemble examples [Stava et al. 2014; Dang et al. 2015], fit a target shape [Prusinkiewicz et al.", "startOffset": 119, "endOffset": 156}, {"referenceID": 22, "context": "2015], fit a target shape [Prusinkiewicz et al. 1994; Talton et al. 2011; Ritchie et al. 2015b], or respect functional constraints such as physical stability [Ritchie et al.", "startOffset": 26, "endOffset": 95}, {"referenceID": 29, "context": "2015], fit a target shape [Prusinkiewicz et al. 1994; Talton et al. 2011; Ritchie et al. 2015b], or respect functional constraints such as physical stability [Ritchie et al.", "startOffset": 26, "endOffset": 95}, {"referenceID": 1, "context": "Samples from the posterior distribution can be drawn via approximate inference algorithms such as Markov Chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) [Brooks et al. 2011; Doucet et al. 2001].", "startOffset": 162, "endOffset": 202}, {"referenceID": 4, "context": "Samples from the posterior distribution can be drawn via approximate inference algorithms such as Markov Chain Monte Carlo (MCMC) or Sequential Monte Carlo (SMC) [Brooks et al. 2011; Doucet et al. 2001].", "startOffset": 162, "endOffset": 202}, {"referenceID": 27, "context": "Performance can be improved through clever sampler design, but this requires time and expertise and is often problem-specific [Schwarz and Wonka 2014; Zhu et al. 2012].", "startOffset": 126, "endOffset": 167}, {"referenceID": 29, "context": "Probabilistic Inference for Procedural Modeling Many research projects have used Bayesian probabilistic inference to control procedural models: constraining the shape of a 3D object [Talton et al. 2011; Ritchie et al. 2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al.", "startOffset": 182, "endOffset": 224}, {"referenceID": 16, "context": "2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al. 2011; Yeh et al. 2012], coloring in patterns [Lin et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 35, "context": "2015b], creating functionally-plausible and aesthetically-pleasing furniture arrangements [Merrell et al. 2011; Yeh et al. 2012], coloring in patterns [Lin et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 13, "context": "2012], coloring in patterns [Lin et al. 2013], and dressing virtual characters [Yu et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 36, "context": "2013], and dressing virtual characters [Yu et al. 2012] are a few recent applications.", "startOffset": 39, "endOffset": 55}, {"referenceID": 22, "context": "The seminal work on open/environmentally-sensitive L-systems developed a formalism by which L-systems could query their spatial position and orientation [Prusinkiewicz et al. 1994; M\u011bch and Prusinkiewicz 1996].", "startOffset": 153, "endOffset": 209}, {"referenceID": 30, "context": "One recent project uses neural networks as computationally inexpensive proxies for costly scoring functions in an inverse urban procedural modeling setting [Vanegas et al. 2012].", "startOffset": 156, "endOffset": 177}, {"referenceID": 37, "context": "Another uses an autoencoder network to learn a low-dimensional representation space in which it is easy to explore the variability in a procedural model\u2019s output [Yumer et al. 2015].", "startOffset": 162, "endOffset": 181}, {"referenceID": 17, "context": "Neural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014].", "startOffset": 97, "endOffset": 165}, {"referenceID": 23, "context": "Neural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014].", "startOffset": 97, "endOffset": 165}, {"referenceID": 10, "context": "Neural Variational Inference Our method is also inspired by recent work in variational inference [Mnih and Gregor 2014; Rezende et al. 2014; Kingma and Welling 2014].", "startOffset": 97, "endOffset": 165}, {"referenceID": 7, "context": "The Neural Adaptive Sequential Monte Carlo algorithm is most similar to our method; it uses a similar learning objective and aims to train more efficient SMC importance samplers [Gu et al. 2015].", "startOffset": 178, "endOffset": 194}, {"referenceID": 24, "context": "Our system uses the version of SMC for probabilistic programs presented by Ritchie et al. [2015b], where particles are resampled after the program generates a new piece of geometry.", "startOffset": 75, "endOffset": 98}, {"referenceID": 33, "context": "It is the optimization objective used in many variational inference algorithms [Wingate and Weber 2012; J. Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992].", "startOffset": 79, "endOffset": 151}, {"referenceID": 17, "context": "It is the optimization objective used in many variational inference algorithms [Wingate and Weber 2012; J. Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992].", "startOffset": 79, "endOffset": 151}, {"referenceID": 32, "context": "Manning and Blei 2014; Mnih and Gregor 2014] as well the REINFORCE algorithm for reinforcement learning [Williams 1992].", "startOffset": 104, "endOffset": 119}, {"referenceID": 14, "context": "concentrating their probability mass in a smaller volume of the state space than the true distribution being approximated [MacKay 2002].", "startOffset": 122, "endOffset": 135}, {"referenceID": 21, "context": "Most growth models, such as L-systems, are accumulative [Prusinkiewicz and Lindenmayer 1990].", "startOffset": 56, "endOffset": 92}, {"referenceID": 12, "context": "2006]; object subdivision, such as fractal terrain [Lewis 1987]; or simulation, such as erosion-based terrain [\u0160t\u2019ava et al.", "startOffset": 51, "endOffset": 63}, {"referenceID": 26, "context": "We use a multilayer perceptron (MLP) architecture, because it is simple, easy to scale, and is a universal function approximator [Rumelhart et al. 1986; Cybenko 1989].", "startOffset": 129, "endOffset": 166}, {"referenceID": 2, "context": "We use a multilayer perceptron (MLP) architecture, because it is simple, easy to scale, and is a universal function approximator [Rumelhart et al. 1986; Cybenko 1989].", "startOffset": 129, "endOffset": 166}, {"referenceID": 11, "context": "Convolutional neural networks reduce an image to a fixed-width feature vector but are aimed at classification tasks: they detect features but are intentionally invariant to where those features occur [Krizhevsky et al. 2012].", "startOffset": 200, "endOffset": 224}, {"referenceID": 18, "context": "This architecture is similar to the foveated \u2018glimpses\u2019 used in recent work on neural models of visual attention [Mnih et al. 2014].", "startOffset": 113, "endOffset": 131}, {"referenceID": 22, "context": "This behavior is not easy to achieve with a purely generative space-filling approach such as environmentally-sensitive Lsystems [Prusinkiewicz et al. 1994], but it is simple to specify with constraints.", "startOffset": 128, "endOffset": 155}, {"referenceID": 33, "context": "This is also known as a partial mean field approximation [Wingate and Weber 2012].", "startOffset": 57, "endOffset": 81}, {"referenceID": 11, "context": "This suggests that \u223c1000 sample traces is sufficient, which may seem surprising, as many published deep learning systems require millions of training examples [Krizhevsky et al. 2012].", "startOffset": 159, "endOffset": 183}, {"referenceID": 6, "context": "For other output domains, it may be possible to develop architectures that learn a partial output state representation, as in recent work on recurrent sequence generation [Graves 2013].", "startOffset": 171, "endOffset": 184}, {"referenceID": 18, "context": "It might instead learn what parts of the current partial output are relevant, as recently-developed visual attention models learn where to look in an image to make classification decisions [Mnih et al. 2014].", "startOffset": 189, "endOffset": 207}], "year": 2017, "abstractText": "We present a deep learning approach for speeding up constrained procedural modeling. Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks: these networks control how the model makes random choices based on what output it has generated thus far. We call such a model a neurally-guided procedural model. As a pre-computation, we train these models on constraint-satisfying example outputs generated via SMC. They are then used as efficient importance samplers for SMC, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling\u2014Geometric algorithms, languages, and systems G.3 [Probability And Statistics]: Probabilistic algorithms (including Monte Carlo) I.2.6 [Artificial Intelligence]: Learning\u2014Connectionism and neural nets", "creator": "LaTeX acmsiggraph.cls (11/2015)"}}}