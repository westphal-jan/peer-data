{"id": "1603.07249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "A Tutorial on Deep Neural Networks for Intelligent Systems", "abstract": "adaptive intelligent networks involves artificial intelligence approaches including artificial neural networks. here, we present a tutorial of research neural networks ( ras ), and some insights about the origin of the term \" deep \" ; references to deep learning are also given. restricted boltzmann matrices, which are the core of dnns, are discussed in detail. an characterization of a simple two - layer network, essentially objective learning for unlabeled data, is shown. deep belief networks ( dbns ), channels are used to compose sequences with more than two layers, are also described. moreover, examples enabling supervised learning with dnns performing simple prediction gradient classification graphs, are presented and explained. this tutorial includes two intelligent signal recognition applications : hand - written digits ( benchmark known as mnist ) and citation recognition.", "histories": [["v1", "Wed, 23 Mar 2016 15:55:20 GMT  (1451kb,D)", "http://arxiv.org/abs/1603.07249v1", "30 pages, 19 figures, unpublished technical report"]], "COMMENTS": "30 pages, 19 figures, unpublished technical report", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["juan c cuevas-tello", "manuel valenzuela-rendon", "juan a nolazco-flores"], "accepted": false, "id": "1603.07249"}, "pdf": {"name": "1603.07249.pdf", "metadata": {"source": "CRF", "title": "A Tutorial on Deep Neural Networks for Intelligent Systems", "authors": ["Juan C. Cuevas-Tello", "Manuel Valenzuela-Rend\u00f3n", "Juan A. Nolazco-Flores", "Eugenio Garza Sada"], "emails": ["cuevastello@itesm.mx", "valenzuela@itesm.mx", "jnolazco@itesm.mx", "cuevas@uaslp.mx"], "sections": [{"heading": null, "text": "Developing Intelligent Systems involves artificial intelligence approaches including artificial neural networks. Here, we present a tutorial of Deep Neural Networks (DNNs), and some insights about the origin of the term \u201cdeep\u201d; references to deep learning are also given. Restricted Boltzmann Machines, which are the core of DNNs, are discussed in detail. An example of a simple two-layer network, performing unsupervised learning for unlabeled data, is shown. Deep Belief Networks (DBNs), which are used to build networks with more than two layers, are also described. Moreover, examples for supervised learning with DNNs performing simple prediction and classification tasks, are presented and explained. This tutorial includes two intelligent pattern recognition applications: handwritten digits (benchmark known as MNIST) and speech recognition."}, {"heading": "1 Introduction", "text": "Intelligent systems involve artificial intelligence approaches including artificial neural networks. This paper focus mainly on Deep Neural Networks (DNNs).\nThe core of DNNs are the Restricted Boltzmann Machines (RBMs) proposed by Smolensky [23, 10], and widely studied by Hinton et al. [13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12]. The next section describes the relationship among RBMs, DBN and DNNs.\nar X\niv :1\n60 3.\n07 24\n9v 1\nNowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22]. However, DL mainly refers to Deep Neural Networks (DNNs) and in particular to DBNs and RBMs [15]. Some work related to DL is focusing on high performance computing to speed up the learning of DNNs, i.e. Graphics Processing Units (known as GPUs), Message Passing Interface (MPI) among other parallelization technologies [3].\nA wide survey on artificial intelligence and in particular DL has been published recently, which covers DNNs, Convolutional Neural Networks, Recurrent Neural Networks, among many other learning strategies [22].\nA Restricted Boltzmann Machine (RBM) is defined as\na single layer of hidden units which are not connected to each other and have undirected, symmetrical connections to a layer of visible units. The visible units and the hidden states are sampled from their conditional distribution using Gibbs sampling by running a Markov chain until it reaches its stationary distribution. The learning rule is the same as the maximum likelihood learning rule [contrastive divergence] for the infinite logistic belief net with tied weights [12].\nProducts of Experts (PoE) and Boltzmann machines are probabilistic generative models, and their intersection comes up with RBMs [10]. Learning by contrastive divergence of PoE is the basis of the learning algorithm of DBNs [10, 12].\nWe recommend [6] as a gentle introduction that explains the training of RBMs and their relationship to graphical models including Markov Random Fields (MRFs); it also presents Markov chains to explain how a RBM draws samples from probability distributions such as Gibbs distribution of a MRF.\nThe building blocks of a RBM are binary stochastic neurons [12]. Nevertheless, there are several ways to define real-valued visible neurons, where Gaussian-Binary-RBM are widely used [6].\nWe use a publicly available MATLAB R\u00a9/Octave toolbox for RBMs developed by Tanaka and Okutomi [24]. This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].\nThe main contribution of this tutorial are the DNNs examples along the source code (Matlab/Octave) to build intelligent systems. Therefore, the spirit of this tutorial is that people can easily execute the examples and see what kind of results are obtained. There are examples with either unsupervised or supervised learning, and examples for prediction and classification tasks are also provided. Moreover, the parameter setting of DNNs with an example is shown.\nThis tutorial is organized as follows: The following section (\u00a72) describes RBMs. Section \u00a73 describes the toolbox developed by Tanaka and Okutomi [24] and the database MNIST. Section \u00a74 presents a discussion about the parameter settings of DNNs. Section \u00a75 explains some simple examples of DNNs. The last section presents speech processing with DNNs; \u00a76. Finally, a summary and references are given."}, {"heading": "2 Restricted Boltzmann Machines (RBMs)", "text": "A RBM is depicted in Fig. 1. The visible layer is the input, unlabeled data, to the neural network. The hidden layer grabs features from the input data, and each neuron captures a different feature [12]. By definition, a RBM is a bipartite undirected graph. A RBM has m visible units ~V = (V1, V2, . . . , Vm), the input data, and n hidden units ~H = (H1, H2, . . . ,Hn), the features [6]. A joint configuration, (~v,~h) of the visible and hidden units has an energy given by [14]\nE(~v,~h) = \u2212 m\u2211 i=1 aivi \u2212 n\u2211 j=1 bjhj \u2212 m\u2211 i=1 n\u2211 j=1 vihjwij , (1)\nwhere vi and hj are the binary states of the visible and hidden units, respectively; ai, bj are the biases, and wij is a real valued weight associated with each edge in the network [11], see Fig. 1.\nThe building block of a RBM is a binary stochastic neuron [12]. Fig. 2 shows how to obtain the state of a hidden neuron given a visible layer (data).\nA RBM can be seen as a stochastic neural network. First, weights wij are randomly initialized. Then, the data to be learned is set at the visible layer; this data can be an image, a signal, etcetera. Now, the state of the neurons at\nthe hidden layer is obtained by\np(hj = 1|~v) = sig ( bj +\n\u2211 i viwij\n) , (2)\nso the conditional probability of hj being 1 is the firing rate of a stochastic neuron with a sigmoid activation function, sig(x) = 1/(1 \u2212 e\u2212x), see Fig. 2. This step, visible to hidden, is represented as \u3008vihj\u30090, at time t = 0 [12, 6, 24]."}, {"heading": "2.1 Contrastive Divergence algorithm", "text": "Learning in a RBM is achieved by the Contrastive Divergence (CD) algorithm, see Fig. 3 [12]. The first step of the CD algorithm is \u3008vihj\u30090, as shown above. The next step is the \u201creconstruction\u201d of the visible layer by\np(vi = 1|~h) = sig ai +\u2211 j hjwi,j  , (3) i.e., hidden to visible. This step is denoted as \u3008hjvi\u30090. The new state of the hidden layer is obtained using the result of the reconstruction as the input data, and this step is denoted as \u3008vihj\u30091; at time t = 1. Finally, the weights and biases are adjusted in the following way [12]:\n\u2206wij = \u03b5 ( \u3008vihj\u30090 \u2212 \u3008vihj\u30091 ) ; (4)\n\u2206ai = \u03b5 ( v0i \u2212 v1i ) ; (5)\n\u2206bj = \u03b5 ( h0j \u2212 h1j ) ; (6)\nwhere \u03b5 is the learning rate. RBMs find better models if more steps of the CD algorithm are performed; CDk is used to denote the learning in k steps/iterations [11]. The CD algorithm is summarized in Algorithm 1, and it uses the complete training data, batch learning [6]."}, {"heading": "2.2 Deep Belief Network", "text": "A Deep Belief Network (DBN) [12] is depicted in Fig. 4. Comparing Fig. 1 with Fig. 4, we can see that a DBN is built by stacking RBMs. Thus, the more levels the DBN has, the deeper the DBN is. The hidden neurons in a RBM1 capture the features from the visible neurons. Then, those features become the input to RBM2, and so on until the RBMr is reached; see also Fig. 5. A DBN extracts features from features in an unsupervised manner (deep learning).\nA hybrid DBN has been proposed for supervised learning, see Fig. 5. This network adds labels to the top layer. The weights ~WL between the top level and the last layer of hidden neurons, associative memory, are learned in a supervised manner. This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8]. This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].\nHinton et al. applied a DNN to the MINST handwritten digits database1 [12], see Fig. 6. At that time, the DNN produced the best performance with an error rate of 1.25% compared with other methods including Support Vector Machines (SVM) which had an error rate of 1.4% [12]."}, {"heading": "3 Toolbox for DNN", "text": "We use a publicly available toolbox for MATLAB R\u00a9 developed by Tanaka and Okutomi [24], and which can be downloaded online2. This toolbox is based on [12]. This toolbox includes sparsity [16], dropout [4] and a novel inference\n1http://yann.lecun.com/exdb/mnist/ 2http://www.mathworks.com/matlabcentral/fileexchange/42853-deep-neural-network\nfor RBM devised by Tanaka [24]. Once the toolbox has been downloaded and unzipped, it will generate the following directories:\n\u2022 /DeepNeuralNetwork/\n\u2022 /DeepNeuralNetwork/mnist"}, {"heading": "3.1 MNIST", "text": "The MNIST database3 of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. Once the MNIST has been downloaded and unzipped, we will come up with the following files:\n\u2022 train-images-idx3-ubyte: training set images\n\u2022 train-labels-idx1-ubyte: training set labels\n\u2022 t10k-images-idx3-ubyte: test set images\n\u2022 t10k-labels-idx1-ubyte: test set labels\nNote that when you uncompress the *.gz files, then you will need to check the file names, and replace \u201c.\u201d by \u201c-\u201d. You must locate the files within the /DeepNeuralNetwork/mnist/ directory.\n3http://yann.lecun.com/exdb/mnist/"}, {"heading": "3.2 Running the example: DNN-MNIST", "text": "The file /mnist/testMNIST.m is the main file of the example provided by the toolbox to train a DNN for the MNIST database. The example uses a hybrid network with only two hidden layers of 800 neurons each layer, see Fig. 7. We have tested the toolbox on Octave4 3.2.4 and MATLAB R\u00a9 7.11.0.584 (2010b), both in Linux Operating Systems.\nThe script testMNIST.m will generate the file mnistbbdbn.mat with the DNN already trained. Once testMNIST.m has finished it will appear something like:\n\u2022 For training data: rmse = 0.0155251; ErrorRate = 0.00196667 (0.196%); Tanaka et al. reported 0.158% [24].\n\u2022 For test data: rmse = 0.0552593; ErrorRate = 0.0161 (1.6%); Tanaka et al. reported 1.76% [24].\nThe computational time required to train 60,000 MNIST examples and 10,000 examples for testing is about 3 days on a computer with 384 GB memory, 4 CPUs 2.3GHz with 16 cores each (total of 64 cores)."}, {"heading": "3.3 Understanding the toolbox example for MNIST", "text": "Once the example script (testMNIST.m) has been successfully executed, we run the script in Fig. 8.\n4https://www.gnu.org/software/octave/\nThis script generates N = 10 images via imshow, see Fig. 9. The images are part of the 10 first testing samples. Each image is stored in a vector of size 784, which corresponds to an image size of 28\u00d728 pixels. And each pixel stores a number between 0 and 255, where 0 means background (white) and 255 means foreground (black); see Fig. 9.\nIn Fig. 10, we depict the inputs and outputs of the DNN for the MNIST database. The variable IN represents the inputs for either training or testing samples. The variable OUT is the output for training, and the variable out the output for testing. Both output variables represent the labels of the digits to learn/recognize (digits from 0 to 9).\nFor example, the script in Fig. 8 generates the following result:\nout =\n0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\nThe variable out represents the labels. Each row correspond to each digit image5, and each column means the activation or not of the digits 0\u20139. That is, the first column represents the digit 0, and the last column the digit 9. For example, see top-left in Fig. 9, the image representing the digit 7 activates only column 8 (i.e. 0 0 0 0 0 0 0 1 0 0) of the first row of variable out. The image at the right side of digit 7 corresponds to digit 2, so the third column is activated on the second row of variable out, and so on. In this example the ErrorRate is zero, because the first ten samples of testing are all recognized successfully. Let us create an hypothetical scenario where the DNN fails to recognize the first image (digit 7), i.e. imagine that the output looks like the following:\nout =\n0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\nLook at the first row, now this row indicates that the first digit image in Fig. 9 corresponds to the digit 6, instead of digit 7. Therefore, Errorrate measures this error via the abs function along the difference between the desired output OUT and the output of the DNN, which is out. Then the error rate is obtained as follows:\n5Note that there are 60,000 images for training, 10,000 images for testing and 10 images for illustration purposes (Fig. 9), i.e. only 10 rows.\nErrorRate = abs(OUT-out) ErrorRate =\n0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nNow, we sum out by row. This is to detect how many differences are found by test sample.\nsum(ErrorRate,2) ans =\n2 0 0 0 0 0 0 0 0 0\nIf an error exists, then the result is divided by 2. This is because if there is some difference per each sample we will have two 1\u2019s as show above.\nsum(ErrorRate,2)/2 ans =\n1 0 0 0 0 0 0 0 0 0\nFinally, the error rate is given as the mean value:\nmean(sum(ErrorRate,2)/2) ans = 0.10000\nSince the DNN fails to recognize one digit out of ten, the error rate is 10% (i.e. 0.10000)."}, {"heading": "4 Parameter Setting", "text": "There are several parameters to set up when working with DNNs, including statistics to monitor the contrastive divergence algorithm, batch sizes, monitoring overfitting (iterations), learning rate \u03b5, initial weights, number of hidden unit and hidden layers, types of units (e.g. binary or Gaussian), dropout, among others [16, 11, 4, 6]. In practice, we can only focus on the following:\n\u2022 Maximum of iterations (MaxIter), which is also know as k for the contrastive divergence algorithm.\n\u2022 The learning rate (StepRatio).\n\u2022 Type units (e.g. Bernoulli or Gaussian distribution).\nWe analyze the impact of these DNN parameters through the XOR example; see \u00a75.2.4 below."}, {"heading": "5 Further Examples", "text": "Besides the MNIST database example, described above, this section presents examples for unsupervised and supervised learning; including prediction and\nclassification tasks."}, {"heading": "5.1 Unsupervised learning", "text": "We now show an example for unsupervised learning. The network architecture is a single RBM with six visible units and eight hidden units; see Fig. 11. The goal is to learn a simple pattern (Pattern) as shown below within the script in Fig. 12. This Pattern is a very simple example of unlabeled data.\nAll the computational times reported throughout this section were obtained running on a personal computer with the following characteristics: 2.3 GHz Intel Core i7 and 4GB memory; Linux-Ubuntu 12.04 and GNU Octave 3.2.4."}, {"heading": "5.1.1 Script", "text": "Fig. 12 shows the Matlab/Octave script for our example of unsupervised learning. We used the same toolbox than for the MNIST database [24]."}, {"heading": "5.1.2 Results", "text": "As the script in Fig. 12 is executed, several data is displayed. First, the training data (our pattern) is shown:\n% Matlab/Octave script\nTrainData =\n1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1\nthen the output after pretrainDBN is:\nout =\n9.9e-01 9.9e-01 1.0e-04 1.2e-04 2.5e-05 2.6e-05 5.0e-04 4.3e-04 9.9e-01 9.9e-01 6.1e-04 6.0e-04 4.2e-06 3.6e-06 5.9e-06 5.4e-06 9.9e-01 9.9e-01\nThis output are the probabilities [0,1], known as reconstructions, so we apply the function round, and then we obtain:\nout =\n1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1\nThe total elapsed time is 0.254730 seconds."}, {"heading": "5.1.3 Discussion", "text": "We found that the DNN in Fig. 11 is able to learn the given pattern in an unsupervised manner. We use a stepRatio of 2.5 because this allow us to have fewer iterations, i.e. MaxIter = 50. Some authors recommend a learning rate (stepRatio) of 0.01 [11, 24], but with this setting, we need at least 1,000 iterations to learn the pattern; see \u00a75.2.4 for parameter setting issues."}, {"heading": "5.2 Predicting Patterns", "text": "The following example simulates a time series prediction scenario. We test two different patterns (Pattern1 and Pattern2). Our training data is a matrix with eight columns, which is the number of variables. We use only six variables as input, and the last two column variables as output. The main idea is that we feed the network only with six variables, then the network must \u201cpredict\u201d the next two variables; see Fig. 13. Compared with the previous example, we use supervised learning as shown above in the MNIST example, see \u00a73. Therefore, the labels are our two last columns of the pattern (outputs), i.e. TrainLabels."}, {"heading": "5.2.1 Script", "text": "The script for this example is shown in Fig. 13."}, {"heading": "5.2.2 Results", "text": "The script in Fig. 14 generates the following output. First, it prints out the TrainData and TrainLabels together with the instruction [TrainData TrainLabels]:\nTraining... ans =\n0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0\nThe reconstructions (probabilities) are given in the last two columns:\nTesting... ans = 0.0000 0.0000 0.0000 0.0000 0.0000 1.0000 0.0031 0.9881 0.0000 0.0000 0.0000 0.0000 1.0000 0.0000 0.9911 0.0011 0.0000 0.0000 0.0000 1.0000 0.0000 1.0000 0.0044 0.0112 0.0000 0.0000 1.0000 0.0000 1.0000 0.0000 0.0092 0.0073 0.0000 1.0000 0.0000 1.0000 0.0000 0.0000 0.0065 0.0002 1.0000 0.0000 1.0000 0.0000 0.0000 0.0000 0.0003 0.0041\n% Matlab/Octave script\nAs in the previous example, we apply the function round, so we obtain:\n[TestData round(out)] ans =\n0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0"}, {"heading": "5.2.3 Discussion", "text": "In this example, we set a DNN to predict two variables given six input variables. We found that the DNN is able to successfully predict the given patterns. Here, we show only results for Pattern1, but the results for Pattern2 are similar. We started to test the DNN with a different pattern, and we came across accidentally with a pattern that the DNN cannot predict (4 inputs, 2 outputs):\nTraining... ans =\n0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\nTesting... ans =\n0.00000 0.00000 0.00000 0.00000 0.49922 0.49922 0.00000 0.00000 0.00000 0.00000 0.49922 0.49922 0.00000 0.00000 0.00000 1.00000 0.00993 0.00993 0.00000 0.00000 1.00000 0.00000 0.01070 0.01070 0.00000 1.00000 0.00000 0.00000 0.01142 0.01142 1.00000 0.00000 0.00000 0.00000 0.01109 0.01109\nThe two first rows of the training data have the same input values, but they have different output. Therefore, the DNN \u201cintelligently\u201d suggest an output of 0.499 (probability)."}, {"heading": "5.2.4 XOR problem", "text": "The XOR problem is a non-linear problem that is typical test for a classifier because it is a problem that a simple linear classifier cannot learn. In the neural networks literature, an example of a linear classifier is the perceptron introduced by Frank Rosenblatt in 1957 [20]. A decade later, Marvin Minsky and Seymour Paper wrote their famous book Perceptrons, and they showed that perceptrons cannot solve the XOR problem [18]. Perhaps partly due to the publication of Perceptrons , there was a decline of research in neural networks until the backpropagation algorithm appeared about twenty year after Minsky and Paper\u2019s publication.\nHere, we analyze the XOR problem with a DNN; see Fig. 15."}, {"heading": "5.2.5 Script", "text": "The script for the XOR problem is shown in Fig. 16.\n%Matlab/Octave script"}, {"heading": "5.2.6 Results", "text": "The script in Fig. 16 involves only two input variables and a single output, so the TrainData and TrainLabels are as follows:\nTrainData =\n0 0 0 1 1 0 1 1\nTrainLabels =\n0 1 1 0\nBefore coming up with the script of Fig. 16, we tested different configurations for the DNN. We started with the following hyperparameter setting:\nnodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.MaxIter = 10; opts.BatchSize = 4; opts.Verbose = true; opts.StepRatio = 0.1; opts.object = \u2019CrossEntropy\u2019; TestData = TrainData;\nThe TrainLabels is a column vector, and out\u2019 is a row vector, where out\u2019 is the transpose of out. Therefore, the desired output is out\u2019 = 0 1 1 0. The output of the DNN for this setting is:\nout\u2019 = 0.49994 0.49991 0.50009 0.50006\nThis setting does not work with the above parameters. If we add more iterations opts.MaxIter = 100, it still does not work properly, and we obtain the output:\nout\u2019 = 0.47035 0.51976 0.49161 0.50654\nIf we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves. The output now is:\nout\u2019 = 0.118510 0.906046 0.878771 0.096262\nThe performance is still better if we add more iterations opts.MaxIter = 1000. The output now is:\nout\u2019 = 0.014325 0.982409 0.990972 0.012630 Elapsed time: 32.607048 seconds\nThe previous experiment takes about 33 seconds. In order to decrease the complexity of the network, we reduce the number of hidden neurons. Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.MaxIter = 1000. In about 24 seconds, the output is:\nout\u2019 = 0.043396 0.950205 0.947391 0.059305 Elapsed time: 23.640984 seconds\nNow, if we reduce the the number of iterations with less neurons as the previous experiments, i.e. opts.MaxIter = 100, it is faster but the performance decays. So the output now is:\nout\u2019 = 0.16363 0.80535 0.82647 0.20440 Elapsed time: 2.617439 seconds\nOther hyperparameter is opts.StepRatio, the learning rate, so we tunned this parameter to opts.StepRatio = 0.01, and the number of iterations is opts.MaxIter = 1000. We found similar results to the previous experiment, i.e. we do not reach the desired output.\nNevertheless, if we use opts.StepRatio = 2.5 and opts.MaxIter = 100, then we obtain a good performance in about one second. The output is:\nout\u2019 = 0.022711 0.955236 0.955606 0.065202 Elapsed time: 0.806343\nAnother important experiment is to test the performance with real values. So we change the test data, and we obtain the following results:\nTestData =\n0.10000 0.10000 0.00000 0.90000 1.00000 0.20000 0.80000 1.00000\nout =\n0.213813 0.956192 0.889679 0.053432\nElapsed time: 0.686760\nFinally, we ran some experiments by tuning the hyperparameter opts.object to \u2018Square\u2019 or \u2018CrossEntorpy\u2019, and we found no difference in performance. The flag opts.verbose is only for showing or not the training performance."}, {"heading": "5.2.7 Discussion", "text": "For the XOR problem, we found that the best performance is with the following combination: opts.StepRatio = 2.5 and opts.MaxIter = 100. A large step ratio with few iterations allow us to obtain results faster than other settings. The performance is good enough to solve the XOR problem. Moreover, this setting classifies correctly when the inputs are real data. For this reason, this setting is used in the previous examples; see \u00a75.1 and \u00a75.2."}, {"heading": "6 Speech Processing", "text": "Speech Processing has several applications including Speech Recognition, Language Identification and Speaker Recognition; see Fig. 17. Sometimes, additional information is stored and associated to speech. Therefore, Speaker Recognition can be either text dependent or text independent. Moreover, Speaker Recognition involves different tasks such as [2]:\n\u2022 Speaker Identification\n\u2022 Speaker Detection\n\u2022 Speaker Verification."}, {"heading": "6.1 Speech Features", "text": "The first step for most speech recognition systems is the feature extraction from the time-domain sampled acoustic waveform (audio); see Figure 18a. The time-domain waveform is represented by overlapping frames. Each frame is generated every 10ms with a duration of 25ms. Then, a feature is extracted for every frame. Several methods have been investigated for feature extraction (acoustic representation) including Linear Prediction Coefficients (LPCs), Perceptual Linear Prediction (PLP) coefficients and Mel-Frequency spaced Cepstral Coefficients(MFCCs) [5, 25]."}, {"heading": "6.2 DNN and Speech Processing", "text": "As we shown above, DNNs have the flexibility to be used as either unsupervised or supervised learning. Therefore, DNNs can be used for regression or classification problems in speech recognition; see Fig. 19.\nNowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].\nVOICEBOX6 is a Speech Processing Toolbox for MATLAB R\u00a9, which is also publicly available.\n6http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html"}, {"heading": "7 Summary", "text": "Neural networks approaches have been used widely to build Intelligent Systems. We introduced Deep Neural Networks (DNNs) and Restricted Boltzmann Machines (RBMs), and their relationship to Deep Learning (DL) and Deep Belief Nets (DBNs). Across the literature, there are some introductory papers for RBMs [11, 6]. One of the contributions of this tutorial are the simple examples for a better understanding of RBMs and DNNs. The examples cover unsupervised and supervised learning, therefore, we cover both unlabeled and labeled data, for prediction and classification. Moreover, we introduce a publicly available MATLAB R\u00a9 toolbox to show the performance of DNNs and RBMs [24]. The toolbox and the examples have been tested on Octave, the open source version of MATLAB R\u00a9. The last example, XOR problem, presents some results by different setting of some hyperparameters of DNNs. Finally, two applications for intelligent pattern recognition are also covered on this tutorial: the MNIST benchmarking and speech recognition."}], "references": [{"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop", "G.E. Hinton"], "venue": "Clarendon Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Campbell", "author": ["J.P. Jr"], "venue": "Speaker recognition: A tutorial. Proceedings of IEEE, 85(6):1437\u20131462", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "D", "author": ["A. Coates", "B. Huval", "T. Wang", "A.Y. Wu"], "venue": "J.and Ng. Deep learning with COTS HPC systems. In Proceedings of the 30th International Conference on Machine Learning ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, volume 28", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1980}, {"title": "Training restricted Boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition, 14:25\u201339", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Frame-by-frame language identification in short utterances using deep neural networks", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Mreno", "P.J. Moreno", "J. Gonzalez- Rodriguez"], "venue": "Neural Networks, 64:49\u201358", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Prentice Hall", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, 29(6):82\u201397", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, 14(8):1711\u20131800", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "A practical guide to training restricted Boltzmann machines version 1", "author": ["G.E. Hinton"], "venue": "Department of Computer Science, University of Toronto", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(1):504\u2013507", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the National Academy of Sciences, volume 79, pages 2554\u2013\u20132558", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "J", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado"], "venue": "Dean, , and A. Ng. Building high-level features using large scale unsupervised learning. In Proceedings of International Conference on Machine Learning ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep learning for acoustic modeling in parametric speech generation", "author": ["Z.H. Ling", "S.Y. Kang", "H. Zen", "A. Senior", "M. Schuster", "X.J. Qian", "H. Meng", "L. Deng"], "venue": "IEEE Signal Processing Magazine, 32(3):35\u201352", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptrons", "author": ["M.L. Minsky", "S.A. Papert"], "venue": "Cambridge, MA: MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1969}, {"title": "Neural Networks: A Systematic Introduction", "author": ["R. Rojas"], "venue": "Springer-Verlag", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "The perceptron\u2013A perceiving and recognizing automaton", "author": ["F. Rosenblatt"], "venue": "Technical Report 85-460-1, Cornell Aeronautical Laboratory", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1957}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(1):533\u2013536", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1986}, {"title": "Deep learning in neural networks: An overview", "author": ["J Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel Distributed Processing", "author": ["P. Smolensky"], "venue": "volume 1, chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194\u2013281. MIT Press, Cambridge", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1986}, {"title": "A novel inference of a restricted Boltzmann machine", "author": ["M. Tanaka", "M. Okutomi"], "venue": "International Conference on Pattern Recognition ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of speaker identification: Accuracy and robustness issues", "author": ["R. Togneri", "D. Pullella"], "venue": "IEEE Circuits and Systems Magazine, 2(1):23\u201361", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Single-channel mixed speech recognition using deep neural networks", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEE International Acoustics, Speech and Signal Processing (ICASSP), pages 5632\u20135636", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "The core of DNNs are the Restricted Boltzmann Machines (RBMs) proposed by Smolensky [23, 10], and widely studied by Hinton et al.", "startOffset": 84, "endOffset": 92}, {"referenceID": 9, "context": "The core of DNNs are the Restricted Boltzmann Machines (RBMs) proposed by Smolensky [23, 10], and widely studied by Hinton et al.", "startOffset": 84, "endOffset": 92}, {"referenceID": 12, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 0, "endOffset": 12}, {"referenceID": 10, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "[13, 12, 11], where the term deep comes from Deep Beliefs Networks (DBN) [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Nowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22].", "startOffset": 93, "endOffset": 104}, {"referenceID": 2, "context": "Nowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22].", "startOffset": 93, "endOffset": 104}, {"referenceID": 21, "context": "Nowadays, the term Deep Learning (DL) is becoming popular in the machine learning literature [15, 3, 22].", "startOffset": 93, "endOffset": 104}, {"referenceID": 14, "context": "However, DL mainly refers to Deep Neural Networks (DNNs) and in particular to DBNs and RBMs [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Graphics Processing Units (known as GPUs), Message Passing Interface (MPI) among other parallelization technologies [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 21, "context": "A wide survey on artificial intelligence and in particular DL has been published recently, which covers DNNs, Convolutional Neural Networks, Recurrent Neural Networks, among many other learning strategies [22].", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "The learning rule is the same as the maximum likelihood learning rule [contrastive divergence] for the infinite logistic belief net with tied weights [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Products of Experts (PoE) and Boltzmann machines are probabilistic generative models, and their intersection comes up with RBMs [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "Learning by contrastive divergence of PoE is the basis of the learning algorithm of DBNs [10, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 11, "context": "Learning by contrastive divergence of PoE is the basis of the learning algorithm of DBNs [10, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 5, "context": "We recommend [6] as a gentle introduction that explains the training of RBMs and their relationship to graphical models including Markov Random Fields (MRFs); it also presents Markov chains to explain how a RBM draws samples from probability distributions such as Gibbs distribution of a MRF.", "startOffset": 13, "endOffset": 16}, {"referenceID": 11, "context": "The building blocks of a RBM are binary stochastic neurons [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Nevertheless, there are several ways to define real-valued visible neurons, where Gaussian-Binary-RBM are widely used [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 23, "context": "We use a publicly available MATLAB R \u00a9/Octave toolbox for RBMs developed by Tanaka and Okutomi [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].", "startOffset": 47, "endOffset": 50}, {"referenceID": 23, "context": "This toolbox implements sparsity [16], dropout [4] and a novel inference for RBM [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "Section \u00a73 describes the toolbox developed by Tanaka and Okutomi [24] and the database MNIST.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "The hidden layer grabs features from the input data, and each neuron captures a different feature [12].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": ",Hn), the features [6].", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "A joint configuration, (~v,~h) of the visible and hidden units has an energy given by [14] E(~v,~h) = \u2212 m \u2211", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "where vi and hj are the binary states of the visible and hidden units, respectively; ai, bj are the biases, and wij is a real valued weight associated with each edge in the network [11], see Fig.", "startOffset": 181, "endOffset": 185}, {"referenceID": 11, "context": "The building block of a RBM is a binary stochastic neuron [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "In this example, visible to hidden, hj is the probability of producing a spike [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "This step, visible to hidden, is represented as \u3008vihj\u3009, at time t = 0 [12, 6, 24].", "startOffset": 70, "endOffset": 81}, {"referenceID": 5, "context": "This step, visible to hidden, is represented as \u3008vihj\u3009, at time t = 0 [12, 6, 24].", "startOffset": 70, "endOffset": 81}, {"referenceID": 23, "context": "This step, visible to hidden, is represented as \u3008vihj\u3009, at time t = 0 [12, 6, 24].", "startOffset": 70, "endOffset": 81}, {"referenceID": 11, "context": "3 [12].", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "Finally, the weights and biases are adjusted in the following way [12]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "RBMs find better models if more steps of the CD algorithm are performed; CDk is used to denote the learning in k steps/iterations [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 5, "context": "The CD algorithm is summarized in Algorithm 1, and it uses the complete training data, batch learning [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 11, "context": "A Deep Belief Network (DBN) [12] is depicted in Fig.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 0, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 18, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 7, "context": "This process is called fine-tunning [12], and it can be achieved by many different algorithms including backpropagation [21, 1, 19, 8].", "startOffset": 120, "endOffset": 134}, {"referenceID": 15, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 3, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 5, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 23, "context": "This hybrid DBN is referred as Deep Neural Networks [16, 4, 6, 24].", "startOffset": 52, "endOffset": 66}, {"referenceID": 11, "context": "applied a DNN to the MINST handwritten digits database [12], see Fig.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "4% [12].", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "3 Toolbox for DNN We use a publicly available toolbox for MATLAB R \u00a9 developed by Tanaka and Okutomi [24], and which can be downloaded online.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "This toolbox is based on [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "This toolbox includes sparsity [16], dropout [4] and a novel inference 1http://yann.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "This toolbox includes sparsity [16], dropout [4] and a novel inference 1http://yann.", "startOffset": 45, "endOffset": 48}, {"referenceID": 11, "context": "Figure 6: An hybrid DBN for supervised learning [12]; the MNIST database.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "for RBM devised by Tanaka [24].", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Figure 7: Another DNN architecture for the MNIST database [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "158% [24].", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "76% [24].", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 10, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 3, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 5, "context": "binary or Gaussian), dropout, among others [16, 11, 4, 6].", "startOffset": 43, "endOffset": 57}, {"referenceID": 23, "context": "We used the same toolbox than for the MNIST database [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "This output are the probabilities [0,1], known as reconstructions, so we apply the function round, and then we obtain:", "startOffset": 34, "endOffset": 39}, {"referenceID": 10, "context": "01 [11, 24], but with this setting, we need at least 1,000 iterations to learn the pattern; see \u00a75.", "startOffset": 3, "endOffset": 11}, {"referenceID": 23, "context": "01 [11, 24], but with this setting, we need at least 1,000 iterations to learn the pattern; see \u00a75.", "startOffset": 3, "endOffset": 11}, {"referenceID": 19, "context": "In the neural networks literature, an example of a linear classifier is the perceptron introduced by Frank Rosenblatt in 1957 [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "A decade later, Marvin Minsky and Seymour Paper wrote their famous book Perceptrons, and they showed that perceptrons cannot solve the XOR problem [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 1, "context": "%Matlab/Octave script TrainData = [0 0; 0 1; 1 0; 1 1]; TrainLabels = [0; 1; 1; 0]; TestData = TrainData; TestLabels = TrainLabels; nodes = [2 12 1]; % [#inputs #hidden #outputs] bbdbn = randDBN( nodes, \u2019BBDBN\u2019 ); % Bernoulli-Bernoulli RBMs nrbm = numel(bbdbn.", "startOffset": 140, "endOffset": 148}, {"referenceID": 11, "context": "%Matlab/Octave script TrainData = [0 0; 0 1; 1 0; 1 1]; TrainLabels = [0; 1; 1; 0]; TestData = TrainData; TestLabels = TrainLabels; nodes = [2 12 1]; % [#inputs #hidden #outputs] bbdbn = randDBN( nodes, \u2019BBDBN\u2019 ); % Bernoulli-Bernoulli RBMs nrbm = numel(bbdbn.", "startOffset": 140, "endOffset": 148}, {"referenceID": 0, "context": "%Matlab/Octave script TrainData = [0 0; 0 1; 1 0; 1 1]; TrainLabels = [0; 1; 1; 0]; TestData = TrainData; TestLabels = TrainLabels; nodes = [2 12 1]; % [#inputs #hidden #outputs] bbdbn = randDBN( nodes, \u2019BBDBN\u2019 ); % Bernoulli-Bernoulli RBMs nrbm = numel(bbdbn.", "startOffset": 140, "endOffset": 148}, {"referenceID": 1, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 2, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 2, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 0, "context": "nodes = [2 3 3 1]; % [#inputs #hidden #hidden #outputs] pts.", "startOffset": 8, "endOffset": 17}, {"referenceID": 1, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 11, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 11, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 0, "context": "If we add more hidden neurons nodes = [2 12 12 1] with the same number of iterations, then the performance improves.", "startOffset": 38, "endOffset": 49}, {"referenceID": 1, "context": "Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.", "startOffset": 42, "endOffset": 50}, {"referenceID": 11, "context": "Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.", "startOffset": 42, "endOffset": 50}, {"referenceID": 0, "context": "Now we have a single hidden layer nodes = [2 12 1], and with the same number of iterations opts.", "startOffset": 42, "endOffset": 50}, {"referenceID": 1, "context": "Moreover, Speaker Recognition involves different tasks such as [2]: \u2022 Speaker Identification \u2022 Speaker Detection \u2022 Speaker Verification.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Several methods have been investigated for feature extraction (acoustic representation) including Linear Prediction Coefficients (LPCs), Perceptual Linear Prediction (PLP) coefficients and Mel-Frequency spaced Cepstral Coefficients(MFCCs) [5, 25].", "startOffset": 239, "endOffset": 246}, {"referenceID": 24, "context": "Several methods have been investigated for feature extraction (acoustic representation) including Linear Prediction Coefficients (LPCs), Perceptual Linear Prediction (PLP) coefficients and Mel-Frequency spaced Cepstral Coefficients(MFCCs) [5, 25].", "startOffset": 239, "endOffset": 246}, {"referenceID": 8, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 97, "endOffset": 104}, {"referenceID": 25, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 97, "endOffset": 104}, {"referenceID": 6, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 130, "endOffset": 133}, {"referenceID": 16, "context": "Nowadays, DNNs have been applied successfully on speech processing including speaker recognition [9, 26], language identification [7] and speech generation [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Across the literature, there are some introductory papers for RBMs [11, 6].", "startOffset": 67, "endOffset": 74}, {"referenceID": 5, "context": "Across the literature, there are some introductory papers for RBMs [11, 6].", "startOffset": 67, "endOffset": 74}, {"referenceID": 23, "context": "Moreover, we introduce a publicly available MATLAB R \u00a9 toolbox to show the performance of DNNs and RBMs [24].", "startOffset": 104, "endOffset": 108}], "year": 2016, "abstractText": "Developing Intelligent Systems involves artificial intelligence approaches including artificial neural networks. Here, we present a tutorial of Deep Neural Networks (DNNs), and some insights about the origin of the term \u201cdeep\u201d; references to deep learning are also given. Restricted Boltzmann Machines, which are the core of DNNs, are discussed in detail. An example of a simple two-layer network, performing unsupervised learning for unlabeled data, is shown. Deep Belief Networks (DBNs), which are used to build networks with more than two layers, are also described. Moreover, examples for supervised learning with DNNs performing simple prediction and classification tasks, are presented and explained. This tutorial includes two intelligent pattern recognition applications: handwritten digits (benchmark known as MNIST) and speech recognition.", "creator": "LaTeX with hyperref package"}}}