{"id": "1603.07828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Privacy-Preserved Big Data Analysis Based on Asymmetric Imputation Kernels and Multiside Similarities", "abstract": "this study presents an efficient reason for incomplete data classification, realizing the entries of samples are missing normally masked due to privacy preservation. to deal exactly these incomplete data, a new kernel specifically encoding asymmetric intrinsic mappings is added in this study. such a new kernel uses three - side similarities for kernel matrix formation. the similarity between a test instance and a training sample relies on retaining only their distance but also the relation between this test sample and the centroid of the class, where the training sample enters. this aids biased estimation compared with typical regression when only one training sample is used for kernel gradient formation. furthermore, of proposed kernel is capable of performing data imputation by using class - dependent averages. this demonstrates fisher test credibility and data discriminability. experiments on two databases were carried out within evaluating the proposed method. the result indicated that reliable accuracy upon the proposed method was higher than that of the baseline. these findings thereby demonstrate the effectiveness of the proposed idea.", "histories": [["v1", "Fri, 25 Mar 2016 06:04:30 GMT  (1174kb)", "http://arxiv.org/abs/1603.07828v1", "8 pages, 11 figures"], ["v2", "Mon, 21 Nov 2016 14:20:34 GMT  (605kb)", "http://arxiv.org/abs/1603.07828v2", "Incomplete data analysis, partial similarity, multiside similarity, privacy preservation, kernel ridge regression (KRR), missing values, data imputation, kernel method, cloud computing, data analytics"]], "COMMENTS": "8 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["bo-wei chen"], "accepted": false, "id": "1603.07828"}, "pdf": {"name": "1603.07828.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["dennisbwc@gmail.com;", "bo-wei.chen@monash.edu)."], "sections": [{"heading": null, "text": "incomplete data classification, where the entries of samples are missing or masked due to privacy preservation. To deal with these incomplete data, a new kernel function with asymmetric intrinsic mappings is proposed in this study. Such a new kernel uses three-side similarities for kernel matrix formation. The similarity between a test instance and a training sample relies on not only their distance but also the relation between this test sample and the centroid of the class, where the training sample belongs. This reduces biased estimation compared with typical methods when only one training sample is used for kernel matrix formation.\nFurthermore, the proposed kernel is capable of performing data imputation by using class-dependent averages. This enhances Fisher Discriminant Ratios and data discriminability.\nExperiments on two databases were carried out for evaluating the proposed method. The result indicated that the accuracy of the proposed method was higher than that of the baseline. These findings thereby demonstrate the effectiveness of the proposed idea.\nIndex Terms\u2014Incomplete data analysis, privacy preservation,\nkernel ridge regression (KRR)\nI. INTRODUCTION\nncomplete data analysis, in addition to complexity problems, has become a research hotspot with the increasing demand for big data processing. For example, data collected by large-scale sensor networks could reach trillions. When sensors fail, defective data are recorded in the dataset, subsequently resulting in biased estimation. In cloud computing, the same problem arises not merely because of erroneous samples, but because of privacy protection. Sensitive personal data, such as health records, faces, and names, are intentionally removed from the original data to avoiding being maliciously manipulated [1]. These defective or masked data form an incomplete dataset. A systematic approach for conquering incomplete data is evitable.\nActually, as far back as the earlier 1970s [2], this problem already aroused much attention from scientists when they dealt with nonresponses in surveys. Missing data usually result from physical mechanical failure or human factors. For instance, subjects neglect questions in questionnaires deliberately or\n1 B.-W. Chen is with the School of Information Technology, Monash University Sunway Campus (email: dennisbwc@gmail.com; bo-wei.chen@monash.edu).\nTo handle such a problem, data imputation is one of the most commonly used techniques when scientists face missing data. Imputation is a statistical term that describes the process of replacing missing data with substituted values [3]. Data deletion is a straightforward and easy approach for incomplete data. Once a missing value appears in an attribute, the entire attribute is removed from the data. However, if the deleted attribute contains key information, discriminability may degrade. To improve such a phenomenon, single imputation and multiple imputation were developed. The former assigns a similar sample to a feature that contains missing entries. This type of approaches include hot decks (i.e., inserted values are selected based on the same dataset), cold decks (i.e., insertion is derived from another dataset), stochastic regression (e.g., interpolation), subspace-based reconstruction [4, 5], and fixed-value replacement (e.g., means or medians). Single imputation has been widely used because of its efficiency and simplicity although it generates values that cannot fully reflect uncertainty. Multiple imputation accommodates such a point by considering the insertion as a set of stochastic numbers, or a value with random noise. Thus, inserted values along with unmissing entries satisfy the distribution of complete data. Multiple imputation is performed by randomly taking a value from a set of stochastic numbers and then filling in the missing dataset L times [6]. Therefore, L complete datasets are duplicated from the original incomplete dataset. Each complete dataset is individually analyzed. The final result comes from fusion of L analyses. Multiple imputation was proposed by Donald B. Rubin, and this technique created a milestone for incomplete data analysis.\nCompared with single imputation, multiple imputation yields results that approximate actual data distribution. Nevertheless, to approach real distribution, Monte Carlo methods [7] or Markov Chain Monte Carlo (MCMC) were frequently adopted to simulate theoretical scenarios by using finite sampling. For small datasets, multiple imputation is an effective and efficient solution to incomplete data. When the volume of data becomes excessively large, generation of L complete datasets requires a delicate mechanism.\nMuch research related to machine learning was examined for the performance comparison of data imputation. For example, Troyanskaya et al. [4] examined data imputation by K-Nearest Neighbors (KNNs) and Singular Value Decomposition (SVD). The result showed that when the\nI\npercentage of missing values reached 20%, the normalized root-mean-squared (RMS) errors were below 0.18, both lower than those by using zero-padding and average-filling methods. Farhangfar et al. [8] compared several imputation methods, including hot decks, polytomous regression, Na\u00efve Bayesian regressors, and mean insertion. Classification on incomplete data was carried out by testing C4.5, SVMs, Na\u00efve Bayesian classifiers, and KNNs. SVMs with polynomial kernels and mean insertion outperformed SVMs with Radial Basis Functions (RBFs) and the other classifiers. In big incomplete data analytics, both effectiveness and efficiency should be received highly attention since data imputation also requires efforts. Anagnostopoulos and Triantafillou [9] investigated the scalability of imputation. They evaluated accuracy and efficiency of data imputation by analyzing where a group of clustered computers worked better than a single powerful machine. They devised a weighted KNN and sequential multivariate regression for imputation. The experiment indicated their proposed distributed method, i.e., weighted imputation, was better than na\u00efve MapReduce mechanisms and superior to centralized processing. Kung and Wu [10] focused on kernelized methods and devised a zero-padding method for single imputation. After filling in substituted values, the Partial Fisher Discriminant-Ratio of each attribute was calculated and sorted, so that discriminant attributes could be selected.\nRegarding big data analysis, two types of machine-learning approaches are frequently investigated in literature. One is divide-and-conquer [11-14], and the other is incremental [15-21]. The former focuses on recursively dividing the original problem into subproblems. The solution to each subproblem is computed in parallel and then combined to give a solution for the original problem [22]. To relieve the computational burden and enhance the performance, entire data are usually divided into smaller sets so that individual computers can share the load. Unlike the divide-and-conquer methods, the latter type does not involve processing subproblems in a distributed way, but data are sequentially fed into the same classifier. Incremental learner requires a mechanism for updating model parameters without retraining the entire model. It is also called online learning. Both divide-and-conquer and incremental methods can be applied to big incomplete data analysis.\nUnlike the abovementioned strategies, this study presents a novel mechanism for big incomplete data analysis, where Fast Kernel Ridge Regression (Fast KRR) powered by the proposed asymmetric imputation kernel is used during data processing. The form aims at big data, and the latter focuses on imputation, where privacy data can be masked and filled in with the proposed asymmetric imputation kernel.\nThe advantage of the proposed methodology is that class information is considered during imputation. This is conducive to discriminablility enhancement when the system fills in missing entries with imputated values. Second, kernel matrices no long reply on the similarity between pairwise samples. Instead, three-side similarities are adopted, where the third side comes from the centroid of a class. This improves robustness against incomplete information.\nThe rest of this paper is organized as follows. Section II introduces the proposed asymmetric data imputation kernel. Sections III then describes the details of Fast KRR. Next, Section IV summarizes the performance of the proposed method and the analytic results. Conclusions are finally drawn in Section VI."}, {"heading": "II. ASYMMETRIC IMPUTATION KERNEL", "text": "This study followed the method \u2015Kernel Approach to Incomplete Data Analysis (KAIDA)\u2016 [10] with several enhanced modifications for data imputation.\nThe original imputation [10] is fulfilled by zero-padding a vector that contains missing values. Let B represent a mask that performs imputation. Then,\n  1 if is given\n0 otherwise\n      x x B (1)\nand\n  x x x B (2)\nwhere \u03b9 denoted the \u03b9\u2013th dimension of a vector, and  is the element-wise operator.\nHowever, zero-padding results in a decrease of Partial Fisher Discriminant-Ratios, shown as follows. Let F denote the Partial Fisher Discriminant-Ratio. Thus,\n \n   \n2\n2 2 F\n \n\n \n \n \n \n \n \n (3)\nwhere \u03bc and \u03c3 are respectively the partial mean and the partial variance of the \u03b9\u2013th dimension. Classes \u2015+\u2016 and \u2015-\u2016 respectively refer to positive and negative samples. The Fisher Discriminant-Ratio measures the discrepancy between two distributions. When the ratio becomes higher, the discrepancy is larger. This implies higher discriminability.\nAssume that the number of unmissing entries of class \u2015+\u2016 for the \u03b9\u2013th dimension is n  . The incremental computation of the mean and the variance is\n1 1\nn n n\nn\n \n   \n\n   \n \n   \n              (4)\nand\n          \n2 2\n2 2\n1 1\n1 1\nn n n n\nn n n n\n     \n      \n \n   \n     \n      \n         \n              \n(5)\nwhere [\u22c5] is the iteration, and \u03b7 is the value of an entry or an attribute in an instance.\nIf \u03b7 is zero as (1) does, (4) and (5) yield\n1 1\nn n n n\nn\n \n     \n\n   \n \n     \n                \nand\n              \n2 21\n2 2 2\n1 1\n1 1 .\nn n n n\nn n n n n\n     \n        \n \n   \n      \n        \n         \n                  \nThis causes a decrease of (3). Namely,\n   1F n F n   . (6)\nThis indicates that when more zero-padding performs, discriminability in the \u03b9\u2013th dimension diminishes.\nAs our focus is data classification, discriminability is the first priority when imputed data are generated. To prevent Partial Fisher Discriminant-Ratios from decreasing, let n        .\nSubsequently, (4) and (5) become\n1n n               \nand\n    2 21 1 n\nn n n\n    \n\n      \n          .\nThis does not decrease Partial Fisher Discriminant-Ratios since the nominator remains unchanged, and the denominator is smaller. In terms of Signal-to-Noise Ratios (SNRs) [23], the denominator, i.e., variance, is noise. When variance diminishes, SNRs enhance.\nAs class information is conducive to the training phase, which was ignored in [10], this study enhances the mechanism in [10] by integrating class information into kernels to benefit incomplete-data training. Nevertheless, test data do not contain class labels. When the system calculates the kernel matrix, i.e., the similarity matrix between test and training instances, no class information is used either.\nA kernel matrix typically measures the similarity between two vectors. Take the cosine similarity function for example.\nThe kernel matrix is formed by calculating\n  T\nCosine ,\ni j\ni j\ni j\nK  x x\nx x x x . (7)\nwhere i and j respectively specify the index of an instance, and T means the conjugate operator. When these two vectors contain missing entries, it involves nonvectorial similarities and results in biased estimation. Thus, the Masked Partial-Cosine (MPC) function mentioned in [10] was used for nonvectorial similarities.\n  T\nMPC ,\ni j\ni j\ni j\nK  x x\nx x x x\n(8)\ni i\ni i\ni i\nj j\n       x x x x x x B B x x B B . (9)\nThe idea of the cosine similarity in (8) can be further extended to Pearson\u2019s correlations, where the similarity is standardized.\n     \nT\nMPP ,\ni r j q\ni j\ni r j q\nK  \n  \nx z x z x x\nx z x z (10)\nwhere r z and qz respectively represent the estimated centroids\nof training class r and q. Besides, i x and jx are the instances of class r and q, respectively. Equation (10) works well during the training phase because class information is given in the dataset. Nonetheless, the testing phase does not provide class information for testing inputs. This requires estimation of centroids, like K-means or spectral clustering. It could create much computational time when data sizes are huge.\nTo alleviate such a problem, this study proposes a new mechanism, where class information is indirectly embedded in the kernel matrix. Rather than using two-side similarities, this study adopts three sides, where the third one is the centroid of a training class. The following equation shows the details of the proposed Masked Partial Three-Side Cosine (MPT) function.\n   T\nMPT ,\ni j q\ni j\ni j q\nK \n \nx x z x x\nx x z (11)\nand (4). At the testing stage, i x signifies a testing vector, and\nj x represents a training sample, of which the class information\nis present. As these centroids are derived from the training data with missing entries, therefore,\nif is missing\n1 otherwise\n1\nq q\nq q q q q\nq q\nq\nn\nn n n\nn\n \n    \n \n\n \n                     \n(12)\nwhere \u03bcq refers to q z in (11). This creates imputation with a\nclass-dependent average in the same attribute. The effect of\nj q x z indicates that the similarity between i x and jx should\nalso considers the similarity between i x and the class centroid\nof j x . Second, the missing entries are filled with q z .\nEquation (11) can be extended into polynomial and radial basis functions (RBFs), i.e.,\n   T 2\nMPT , 1\np\ni j q\ni j\ni j q K            x x z x x x x z (13)\nand\n   \n2\n1 2\nMPT , exp 2\nj qi\ni j\ni j q\nK        \n    \nx zx x x\nx x z (14)\nwhere \u03c42 is the variance of the distribution, and p is the kernel order.\nTypically, K can be split into the product of two identical\nfeature mapping functions  . In our case, the two feature\nmapping functions at the training stage are different."}, {"heading": "III. KERNEL RIDGE REGRESSION", "text": "Kernel Ridge Regression extends linear regression techniques, in which a ridge parameter is imposed on the objective function to regularize a model [23]. KRR has two types of operation modes. One is intrinsic space, and the other is empirical space.\nWhen feature mapping functions  generate vectorial data,\nintrinsic-space computation yields favorable complexity if the number of data N is far larger than the dimension M. Otherwise, empirical-space operations should be used.\nA. Intrinsic Space\nLet {(xi,yi)| i = 1,\u2026,N} denote a pair of an M-dimensional feature vector xi and its corresponding label yi, where i specifies the indices of N training samples. The objective of a linear regressor is to minimize the following cost function of least-square errors (LSEs).\n     2 2T\nKRR , ,\n1\nmin , min N\ni i b b\ni E b b y  \n      \n   u u u u x u (15)\nwhere EKRR is the cost function, u represents a J-by-1 weight\nvector, (xi) denotes the intrinsic-space feature vector of xi, b is a bias term, and \u03c1 specifies the ridge parameter. Besides, T means the conjugate operator, and ||\u2219||2 calculates the norm-two distance. Notable, J is the degree of intrinsic space when feature vectors are transformed by a kernel function.\nEquation (15) can be rewritten as a matrix form, i.e.,\n  2 2T\nKRR , .E b b    u \u03a6 u e y u (16)\nIndividually differentiating (16) with respect to u and b, and subsequently zeroing both equations gives\n  1 T b \n    u \u03a6\u03a6 I \u03a6 y e (17)\nand\n T T T 1\n.b N  e y e \u03a6 u (18)\nNotice that K = \u0424T\u0424 instead of \u0424\u0424T mentioned in (17)."}, {"heading": "B. Empirical Space", "text": "According to the Learning Subspace Property in [23], the weight vector u has the following relation with \u0424 and an unknown N-dimensional vector a.\nu \u03a6a . (19)\nCombining (16) and (19) yields\n  2 T\nKRR , .E b b     a Ka e y a Ka (20)\nRearranging the equations after (20) with respect to a and b yields\n    1 b    a K I y e (21)\nand\n \n \n1T\n1T b\n\n\n\n\n \n\ny K I e e K I e . (22)"}, {"heading": "IV. EXPERIMENTAL RESULT", "text": "Experiments on two datasets were carried out for evaluating the performance. The information of these datasets is listed in Table I. The first column shows the name. The rest columns specify the number of classes, samples, and dimensions, respectively. Dataset \u2015MIT/BIH ECG\u2016 is available at PhysioNet (www.physionet.org), and \u2015ALL-AML\u2016 is downloaded from the UC Irvine (UCI) Machine Learning\nRepository (archive.ics.uci.edu/ml/). The datasets show two typical data, where both N > M and M > N are presented, respectively.\nTable I Attributes of the Datasets\nName #Classes #Instances #Dimensions ECG 2 104033 21\nALL-AML 2 72 7129\nTable II Missing Values\nName Percentage of Missing Values Mask Mode Proposed 00.00%\u201340.00% Double Mask KAIDA 00.00%\u201340.00% Double Mask\nTable III Kernels of KRR\nName Kernels\nProposed MPT Linear, MPT Poly3, & MPT RBF KAIDA MPC, Masked Poly3, & Masked RBFs\nDuring the data imputation, 8/10 of the datasets were randomly selected for training, and the rest were used for testing. Two approaches \u2014 KAIDA [10] and our proposed method \u2014 were employed for assessment. The double mask was used in KAIDA and our method. The percentage of missing values ranged from 0.00% to 40.00%. Missing entries were randomly and uniformly generated. Notably, missing entries for training incomplete data and testing incomplete data were not exactly the same to guarantee randomness.\nRegarding KRR, different kernels, including linear, polynomial 3 (i.e., poly3), and radial basis functions, were applied to the MPC and MPT. The ridge parameter \u03c1 for KRR was 5.0. For high dimensional data, the top 200 dimensions with highest FDRs were selected during testing and training. Table II\u2013Table IV respectively summarize the settings.\nFig. 2\u2013Fig. 7 show the accuracy of KAIDA and our proposed method when ECG data were tested. Fig. 8\u2013Fig. 11 display the results of ALL-AML data. Each figure was generated independently as the training and testing subdatasets were randomly selected. Symbols \u2015I\u2016 and \u2015C\u2016 respectively denote incomplete data and complete data (i.e., without missing values). When they are\ncombined (i.e., II, IC, CI, and CC), the first symbol represents the training stage, and the last symbol signifies the test phase.\nClosely examining the results indicate that compared with KAIDA, our proposed method generated higher accuracy, especial when high-order feature mapping functions were used. However, both methods failed to maintain the accuracy when RBFs were used for testing ALL-AML data. The possible reason is ALL-AML is high-dimensional data, and poly3 kernels already successfully interpreted the distribution of the samples. The RBFs yielded overfitting hyperplanes. Table V\u2013Table IX summarize the numeric results of the figures. The accuracy in these tables was collected based on KAIDA and our proposed method, both with incomplete training data and incomplete testing data.\nAnother finding from our experimental results is the accuracy comparison between KRR and SVMs. The performance of KRR in classification was approximately near that of SVMs, but the computational time was far less than that of SVMs."}, {"heading": "V. CONCLUSION", "text": "This work presents a novel learning method for processing large-scale data with missing values. To effectively recover the discriminability of the data, this study proposes kernel-based data imputation, where the imputation focuses on enhancement of the similarity between test and training samples. The proposed method takes advantage of teacher information in the training phase, where missing entries of the centroids are filled in class-dependent substituted values. Then, a training sample is combined with its class centroid, and this correspondingly generates substituted values for training patterns. The equation shows that such imputation does not reduce Fisher Discriminant-Ratios, which indicate the discrepancy between two distributions. The proposed method can increase the discrepancy of the training data by diminishing the variance. In terms of SNRs, the noise is minimized. Although a test sample still uses masks to zero-pads its missing entries, the three-side kernel matrix compensates the drawback of zero-padding.\nExperiments were conducted by evaluating two datasets. Each dataset represents one type of big data. The result showed that the difference in accuracy between the proposed method and the baseline increased when high-order kernels were used. Besides, our proposed method yielded better results than the baseline did. Such findings indicate that the proposed methods are more effective than the baseline for handling missing data, thereby demonstrating the feasibility of the proposed idea.\nACKNOWLEDGEMENT\nThe author would like to thank Prof. Sun-Yuan Kung for his supervision and teaching when the author studied at Princeton University."}], "references": [{"title": "Geetter, \u2015Privacy and Big Data,", "author": ["B.M. Gaff", "H.E. Sussman"], "venue": "IEEE Computer Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Multiple Imputation for Nonresponse in Surveys", "author": ["D.B. Rubin"], "venue": "New York, NY: Wiley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1987}, {"title": "Missing value estimation methods for DNA microarrays,", "author": ["O. Troyanskaya", "M. Cantor", "G. Sherlock", "P. Brown", "T. Hastie", "R. Tibshirani", "D. Botstein", "R.B. Altman"], "venue": "Bioinformatics, vol. 17,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Multiple imputation for missing data: Concepts and new development,", "author": ["Y.C. Yuan"], "venue": "SAS Institute Incorporation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Analysis of Incomplete Multivariate Data", "author": ["J.L. Schafer"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Scaling out big data missing value imputations", "author": ["C. Anagnostopoulos", "P. Triantafillou"], "venue": "Pythia vs. Godzilla,\u2016 in Proc. 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Kernel approach to incomplete data analysis (KAIDA),", "author": ["S.-Y. Kung", "P.-Y. Wu"], "venue": "Proc. 1st International Conference on Advances in Big Data Analytics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "PSVM: Parallelizing support vector machines on distributed computers,", "author": ["E.Y. Chang", "K. Zhu", "H. Wang", "H. Bai", "J. Li", "Z. Qiu", "H. Cui"], "venue": "in Proc. 21st Annual Conf. Neural Information Processing System (NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Clustered support vector machines,", "author": ["Q. Gu", "J. Han"], "venue": "Proc. 16th Int. Conf. Artificial Intelligence and Statistics (AISTATS), Scottsdale, Arizona, United States,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Divide and conquer kernel ridge regression,", "author": ["Y. Zhang", "J. Duchi", "M. Wainwright"], "venue": "Proc. Conference on Learning Theory (Colt", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A divide-and-conquer solver for kernel support vector machines,", "author": ["C.-J. Hsieh", "S. Si", "I.S. Dhillon"], "venue": "in Proc. 31st International Conference on Machine Learning (ICML", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Incremental support vector machine learning: A local approach,", "author": ["L. Ralaivola", "F. d'Alch\u00e9-Buc"], "venue": "Proc. International Conference on Artificial Neural Networks (ICANN", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Proximal support vector machine classifiers,", "author": ["G. Fung", "O.L. Mangasarian"], "venue": "Proc. 7th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Incremental and decremental support vector machine learning,", "author": ["G. Cauwenberghs", "T. Poggio"], "venue": "Proc. 14th Annual Conf. Neural Information Processing System (NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "SVM incremental learning, adaptation and optimization,", "author": ["C.P. Diehl", "G. Cauwenberghs"], "venue": "Proc. International Joint Conference on Neural Networks (IJCNN", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Incremental support vector learning: Analysis, implementation and applications,", "author": ["P. Laskov", "C. Gehl", "S. Kr\u00fcger", "K.-R. M\u00fcller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Multiple incremental decremental learning of support vector machines,", "author": ["M. Karasuyama", "I. Takeuchi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Incremental training of support vector machines,", "author": ["A. Shilton", "M. Palaniswami", "D. Ralph", "A.C. Tsoi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Kernel Methods and Machine Learning", "author": ["S.-Y. Kung"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Sensitive personal data, such as health records, faces, and names, are intentionally removed from the original data to avoiding being maliciously manipulated [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "Actually, as far back as the earlier 1970s [2], this problem already aroused much attention from scientists when they dealt with nonresponses in surveys.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": ", interpolation), subspace-based reconstruction [4, 5], and fixed-value replacement (e.", "startOffset": 48, "endOffset": 54}, {"referenceID": 3, "context": "Multiple imputation is performed by randomly taking a value from a set of stochastic numbers and then filling in the missing dataset L times [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Nevertheless, to approach real distribution, Monte Carlo methods [7] or Markov Chain Monte Carlo (MCMC) were frequently adopted to simulate theoretical scenarios by using finite sampling.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "[4] examined data imputation by K-Nearest Neighbors (KNNs) and Singular Value Decomposition (SVD).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Anagnostopoulos and Triantafillou [9] investigated the scalability of imputation.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Kung and Wu [10] focused on kernelized methods and devised a zero-padding method for single imputation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 8, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 9, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 10, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 26, "endOffset": 33}, {"referenceID": 11, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 12, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 13, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 14, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 15, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 16, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 17, "context": "One is divide-and-conquer [11-14], and the other is incremental [15-21].", "startOffset": 64, "endOffset": 71}, {"referenceID": 6, "context": "This study followed the method \u2015Kernel Approach to Incomplete Data Analysis (KAIDA)\u2016 [10] with several enhanced modifications for data imputation.", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "The original imputation [10] is fulfilled by zero-padding a vector that contains missing values.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "In terms of Signal-to-Noise Ratios (SNRs) [23], the denominator, i.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "As class information is conducive to the training phase, which was ignored in [10], this study enhances the mechanism in [10] by integrating class information into kernels to benefit incomplete-data training.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "As class information is conducive to the training phase, which was ignored in [10], this study enhances the mechanism in [10] by integrating class information into kernels to benefit incomplete-data training.", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "Thus, the Masked Partial-Cosine (MPC) function mentioned in [10] was used for nonvectorial similarities.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Kernel Ridge Regression extends linear regression techniques, in which a ridge parameter is imposed on the objective function to regularize a model [23].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Empirical Space According to the Learning Subspace Property in [23], the weight vector u has the following relation with \u0424 and an unknown N-dimensional vector a.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Two approaches \u2014 KAIDA [10] and our proposed method \u2014 were employed for assessment.", "startOffset": 23, "endOffset": 27}], "year": 2016, "abstractText": "This study presents an efficient approach for incomplete data classification, where the entries of samples are missing or masked due to privacy preservation. To deal with these incomplete data, a new kernel function with asymmetric intrinsic mappings is proposed in this study. Such a new kernel uses three-side similarities for kernel matrix formation. The similarity between a test instance and a training sample relies on not only their distance but also the relation between this test sample and the centroid of the class, where the training sample belongs. This reduces biased estimation compared with typical methods when only one training sample is used for kernel matrix formation. Furthermore, the proposed kernel is capable of performing data imputation by using class-dependent averages. This enhances Fisher Discriminant Ratios and data discriminability. Experiments on two databases were carried out for evaluating the proposed method. The result indicated that the accuracy of the proposed method was higher than that of the baseline. These findings thereby demonstrate the effectiveness of the proposed idea.", "creator": "Microsoft\u00ae Word 2010"}}}