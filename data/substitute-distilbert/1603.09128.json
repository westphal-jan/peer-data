{"id": "1603.09128", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "abstract": "we present an approach to applying multi - sense word embeddings relying both on original and bilingual information. our model lies of an encoder, which uses monolingual and bilingual context ( i. e. a parallel sentence ) to choose a reader for a given word, and a decoder which predicts context words performed on the chosen sense. the two components are estimated quantitative. we observe that the word representations induced presenting bilingual data outperform the stimulus counterparts to sufficient range of evaluation tasks, even though crosslingual information is not available at optimal time.", "histories": [["v1", "Wed, 30 Mar 2016 11:09:01 GMT  (55kb,D)", "http://arxiv.org/abs/1603.09128v1", "11 pages, to appear at NAACL 2016"]], "COMMENTS": "11 pages, to appear at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["simon suster", "ivan titov", "gertjan van noord"], "accepted": true, "id": "1603.09128"}, "pdf": {"name": "1603.09128.pdf", "metadata": {"source": "CRF", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "authors": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord"], "emails": ["s.suster@rug.nl", "titov@uva.nl", "g.j.m.van.noord@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).\nIn parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings.\nWe propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the\nar X\niv :1\n60 3.\n09 12\n8v 1\n[ cs\n.C L\n] 3\n0 M\nar 2\npredicted senses. We strive to remain flexible as to the form of parallel data used in training and support both the use of word- and sentence-level alignments.\nOur findings are:\n\u2022 The second-language signal effectively improves the quality of multi-sense embeddings as seen on a variety of intrinsic tasks for English, with the results superior to that of the baseline Skip-Gram model, even though the crosslingual information is not available at test time.\n\u2022 This finding is robust across several settings, such as varying dimensionality, vocabulary size and amount of data.\n\u2022 In the extrinsic POS-tagging task, the secondlanguage signal also offers improvements over monolingually-trained multi-sense embeddings, however, the standard Skip-Gram embeddings turn out to be the most robust in this task.\nWe make the implementation of all the models as well as the evaluation scripts available at http: //github.com/rug-compling/bimu."}, {"heading": "2 Word Embeddings with Discrete Autoencoders", "text": "Our method borrows its general structure from neural autoencoders (Rumelhart et al., 1986; Bengio et\nal., 2013). Autoencoders are trained to reproduce their input by first mapping their input to a (lower dimensional) hidden layer and then predicting an approximation of the input relying on this hidden layer. In our case, the hidden layer is not a real-valued vector, but is a categorical variable encoding the sense of a word. Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).\nMore formally, our model consists of two components: an encoding part which assigns a sense to a pivot word, and a reconstruction (decoding) part recovering context words based on the pivot word and its sense. As predictions are probabilistic (\u2018soft\u2019), the reconstruction step involves summation over all potential word senses. The goal is to find embedding parameters which minimize the error in recovering context words based on the pivot word and the sense assignment. Parameters of both encoding and reconstruction are jointly optimized. Intuitively, a good sense assignment should make the reconstruction step as easy as possible. The encoder uses not only words in the first-language sentence to choose the sense but also, at training time, is conditioning its decisions on the words in the second-language sentence. We hypothesize that the injection of crosslingual information will guide learning towards inducing more informative sense-specific word representations. Consequently, using this information at training time would benefit the model even though crosslingual information is not available to the encoder at test time.\nWe specify the encoding part as a log-linear model:\np(s|xi, Ci, C \u2032i, \u03b8) \u221d exp ( \u03d5>i,s(\n1\u2212 \u03bb |Ci| \u2211 j\u2208Ci \u03b3j+\n\u03bb |C \u2032i| \u2211 k\u2208C\u2032i \u03b3\u2032k) ) . (1)\nTo choose the sense s \u2208 S for a word xi, we use the bag of context words Ci from the first language l, as well as the bag of context words C \u2032i from the second language l\u2032.1 The context Ci is defined as a\n1We have also considered a formulation which included a sense-specific bias bxi,s \u2208 R to capture relative frequency of latent senses but it did not seem to affect performance.\nmultiset Ci = {xi\u2212n, . . . , xi\u22121, xi+1, . . . , xi+n}, including words around the pivot word in the window of size n to each side. We set n to 5 in all our experiments. The crosslingual context C \u2032i is discussed in \u00a7 3, where we either rely on word alignments or use the entire second-language sentence as the context. We distinguish between sense-specific embeddings, denoted by \u03d5 \u2208 Rd, and generic sense-agnostic ones, denoted {\u03b3, \u03b3\u2032} \u2208 Rd for first and second language, respectively. The number of sense-specific embeddings is the same for all words. We use \u03b8 to denote all these embedding parameters. They are learned jointly, with the exception of the pre-trained secondlanguage embeddings.\nThe hyperparameter \u03bb \u2208 R, 0 \u2264 \u03bb \u2264 1 weights the contribution of each language. Setting \u03bb = 0 would drop the second-language component and use only the first language. Our formulation allows the addition of new languages easily, provided that the second-language embeddings live in the same semantic space.\nThe reconstruction part predicts a context word xj given the pivot xi and the current estimate of its s:\np(xj |xi, s, \u03b8) = exp(\u03d5>i,s\u03b3j)\u2211\nk\u2208|V| exp(\u03d5 > i,s\u03b3k)\n, (2)\nwhere |V| is the vocabulary size. This is effectively a Skip-Gram model (Mikolov et al., 2013a) extended to rely on senses."}, {"heading": "2.1 Learning and regularization", "text": "As sense assignments are not observed during training, the learning objective includes marginalization over word senses and thus can be written as:\u2211\ni \u2211 j\u2208Cxi log \u2211 s\u2208S p(xj |xi, s, \u03b8)p(s|xi, Ci, C \u2032i, \u03b8),\nin which index i goes over all pivot words in the first language, j over all context words to predict at each i, and s marginalizes over all possible senses of the word xi. In practice, we avoid the costly computation of the normalization factor in the softmax computation of Eq. (2) and use negative sampling (Mikolov et al., 2013b) instead of log p(xj |xi, s, \u03b8):\nlog \u03c3(\u03d5>i,s\u03b3j) + \u2211 x\u2208N log \u03c3(\u2212\u03d5>i,s\u03b3x), (3)\nwhere \u03c3 is the sigmoid non-linearity function and \u03b3x is a word embedding from the sample of negative (noisy) words N . Optimizing the autoencoding objective is broadly similar to the learning algorithm defined for multi-sense embedding induction in some of the previous work (Neelakantan et al., 2014; Li and Jurafsky, 2015). Note though that this previous work has considered only monolingual context.\nWe use a minibatch training regime and seek to optimize the objective function L(B, \u03b8) for each minibatch B. We found that optimizing this objective directly often resulted in inducing very flat posterior distributions. We therefore use a form of posterior regularization (Ganchev et al., 2010) where we can encode our prior expectations that the posteriors should be sharp. The regularized objective for a minibatch is defined as\nL(B, \u03b8) + \u03bbH \u2211 i\u2208B H(qi), (4)\nwhere H is the entropy function and qi are the posterior distributions from the encoder (p(s|xi, Ci, C \u2032i, \u03b8)). This modified objective can also be motivated from a variational approximation perspective, see Marcheggiani and Titov (2016) for details. By varying the parameter \u03bbH \u2208 R, it is easy to control the amount of entropy regularization. For \u03bbH > 0, the objective is optimized with flatter posteriors, while \u03bbH < 0 infers more peaky posteriors. When \u03bbH \u2192 \u2212\u221e, the probability mass needs to be concentrated on a single sense, resulting in an algorithm similar to hard EM. In practice, we found that using hard-update training2, which is closely related to the \u03bbH \u2192 \u2212\u221e setting, led to best performance."}, {"heading": "2.2 Obtaining word representations", "text": "At test time, we construct the word representations by averaging all sense embeddings for a word xi and weighting them with the sense expectations (Li and Jurafsky, 2015)3:\n\u03c9i = \u2211 s\u2208S p(s|xi, Ci)\u03d5i,s. (5)\n2I.e. updating only that embedding \u03d5i,s\u2217 for which s\u2217 = argmaxs p(s|xi, Ci, C\u2032i, \u03b8).\n3Although our training objective has sparsity-inducing properties, the posteriors at test time are not entirely peaked, which makes weighting beneficial.\nUnlike in training, the sense prediction step here does not use the crosslingual contextC \u2032i since it is not available in the evaluation tasks. In this work, instead of marginalizing out the unobservable crosslingual context, we simply ignore it in computation.\nSometimes, even the first-language context is missing, as is the situation in many word similarity tasks. In that case, we just use the uniform average, 1/|S| \u2211 s\u2208S \u03d5i,s."}, {"heading": "3 Word affiliation from alignments", "text": "In defining the crosslingual signal we draw on a heuristic inspired by Devlin et al. (2014). The secondlanguage context words are taken to be the multiset of words around and including the pivot affiliated to xi:\nC \u2032i = {x\u2032ai\u2212m, ..., x \u2032 ai , ..., x \u2032 ai+m}, (6)\nwhere x\u2032ai is the word affiliated to xi and the parameterm regulates the context window size. By choosing m = 0, only the affiliated word is used as l\u2032 context, and by choosing m =\u221e, the l\u2032 context is the entire sentence (\u2248uniform alignment). To obtain the index ai, we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is the index of the aligned word in the middle (and rounding down when necessary). 3) If xi is unaligned, C \u2032i is empty, therefore no l \u2032\ncontext is used. We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora."}, {"heading": "4 Parameters and Set-up", "text": ""}, {"heading": "4.1 Learning parameters", "text": "We use the AdaGrad optimizer (Duchi et al., 2011) with initial learning rate set to 0.1. We set the minibatch size to 1000, the number of negative samples to 1, the sampling factor to 0.001 and the window size parameter m to 5. All the embeddings are 50- dimensional (unless specified otherwise) and initialized by sampling from the uniform distribution between [\u22120.05, 0.05]. We include in the vocabulary all words occurring in the corpus at least 20 times. We set the number of senses per word to 3 (see further discussion in \u00a7 6.4 and \u00a7 7). All other parameters with\ntheir default values can be examined in the source code available online."}, {"heading": "4.2 Bilingual data", "text": "In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data. However, the domain of Europarl is rather constrained, whereas we would like to obtain word representations of more general language, also to carry out an effective evaluation on semantic similarity datasets where domains are usually broader. We therefore use the following parallel corpora: News Commentary (Bojar et al., 2013) (NC), Yandex-1M4 (RU-EN), CzEng 1.0 (Bojar et al., 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al., 2009) (FR-EN). The sizes of the corpora are reported in Table 1. The word representations trained on the NC corpora are evaluated only intrinsically due to the small sizes."}, {"heading": "5 Evaluation Tasks", "text": "We evaluate the quality of our word representations on a number of tasks, both intrinsic and extrinsic."}, {"heading": "5.1 Word similarity", "text": "We are interested here in how well the semantic similarity ratings obtained from embedding comparisons correlate to human ratings. For this purpose, we use a variety of similarity benchmarks for English and report the Spearman \u03c1 correlation scores between the human ratings and the cosine ratings obtained from our word representations. The SCWS benchmark (Huang et al., 2012) is probably the most suitable\n4https://translate.yandex.ru/corpus\nsimilarity dataset for evaluating multi-sense embeddings, since it allows us to perform the sense prediction step based on the sentential context provided for each word in the pair.\nThe other benchmarks we use provide the ratings for the word pairs without context. WS-353 contains 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository."}, {"heading": "5.2 Supersense similarity", "text": "We also evaluate on a task measuring the similarity between the embeddings\u2014in our case uniformly averaged in the case of multi-sense embeddings\u2014and a matrix of supersense features extracted from the English SemCor, using the Qvec tool (Tsvetkov et al., 2015). We choose this method because it has been shown to output scores that correlate well with extrinsic tasks, e.g. text classification and sentiment analysis. We believe that this, in combination with word similarity tasks from the previous section, can give a reliable picture of the generic quality of word embeddings studied in this work."}, {"heading": "5.3 POS tagging", "text": "As our downstream evaluation task, we use the learned word representations to initialize the embedding layer of a neural network tagging model. We use the same convolutional architecture as Li and Juraf-\nsky (2015): an input layer taking a concatenation of neighboring embeddings as input, three hidden layers with a rectified linear unit activation function and a softmax output layer. We train for 10 epochs using one sentence as a batch. Other hyperparameters can be examined in the source code. The multi-sense word embeddings are inferred from the sentential context (weighted average), as for the evaluation on the SCWS dataset. We use the standard splits of the Wall Street Journal portion of the Penn Treebank: 0\u201318 for training, 19\u201321 for development and 22\u201324 for testing."}, {"heading": "6 Results", "text": "We compare three embeddings models, Skip-Gram (SG), Multi-sense (MU) and Bilingual Multi-sense (BIMU), using our own implementation for each of them. The first two can be seen as simpler variants of the BIMU model: in SG we omit the encoder entirely, and in MU we omit the second-language (l\u2032) part of the encoder in Eq. (1). We train the SG and the MU models on the English part of the parallel corpora. Those parameters common to all methods are kept fixed during experiments. The values \u03bb and m for controlling the second-language signal in BIMU are set on the POS-tagging development set (cf. \u00a7 6.3).\nThe results on the SCWS benchmark (Table 2) show consistent improvements of the BIMU model over SG and MU across all parallel corpora, except on the small CZ-EN (NC) corpus. We have also measured the 95% confidence intervals of the difference between the correlation coefficients of BIMU and SG, following the method described in Zou (2007). According to these values, BIMU significantly outperforms SG on RU-EN, and on French, Russian and Spanish NC corpora.5\nNext, ignoring any language-specific factors, we would expect to observe a trend according to which the larger the corpus, the higher the correlation score. However, this is not what we find. Among the largest corpora, i.e. RU-EN, CZ-EN and FR-EN, the models trained on RU-EN perform surprisingly well, practically on par with the 23-times larger FR-EN corpus. Similarly, the quality of the embeddings trained on CZ-EN is generally lower than when trained on the\n5I.e. counting those results in which the CI of the difference does not include 0.\n10 times smaller RU-EN corpus. One explanation for this might be different text composition of the corpora, with RU-EN matching the domain of the evaluation task better than the larger two corpora. Also, FR-EN is known to be noisy, containing webcrawled sentences that are not parallel or not natural language (Denkowski et al., 2012). Furthermore, language-dependent effects might be playing a role: for example, there are signs of Czech being the least helpful language among those studied. But while there is evidence for that in all intrinsic tasks, the situation in POS tagging does not confirm this speculation.\nWe relate our models to previously reported SCWS scores from the literature using 300-dimensional models in Table 3. Even though we train on a much smaller corpus than the previous works,6 the BIMU\n6For example, Li and Jurafsky (2015) use the concatenation of Gigaword and Wikipedia with more than 5B words.\nmodel achieves a very competitive correlation score. The results on similarity benchmarks and qvec largely confirm those on SCWS, despite the lack of sentential context which would allow to weight the contribution of different senses more accurately for the multi-sense models. Why, then, does simply averaging the MU and BIMU embeddings lead to better results than when using the SG embeddings? We hypothesize that the single-sense model tends to over-represent the dominant sense with its generic, one-vector-per-word representation, whereas the uniformly averaged embeddings yielded by the multisense models better encode the range of potential senses. Similar observations have been made in the context of selectional preference modeling of polysemous verbs (Greenberg et al., 2015).\nIn POS tagging, the relationship between MU and BIMU models is similar as discussed above. Overall, however, neither of the multi-sense models outperforms the SG embeddings. The neural network tagger may be able to implicitly perform disambiguation on top of single-sense SG embeddings, similarly to what has been argued in Li and Jurafsky (2015). The tagging accuracies obtained with MU on CZ-EN and FR-EN are similar to the one obtained by Li and Jurafsky with their multi-sense model (93.8), while the accuracy of SG is more competitive in our case (around 94.0 compared to 92.5), although they use a larger corpus for training the word representations.\nIn all tasks, the addition of the bilingual component during training increases the accuracy of the encoder for most corpora, even though the bilingual information is not available during evaluation."}, {"heading": "6.1 The amount of (parallel) data", "text": "Fig. 2a displays how the semantic similarity as measured on SCWS evolves as a function of increasingly\nlarger sub-samples from FR-EN, our largest parallel corpus. The BIMU embeddings show relatively stable improvements over MU and especially over SG embeddings. The same performance as that of SG at 100% is achieved by MU and BIMU sooner, using only around 40/50% of the corpus."}, {"heading": "6.2 The dimensionality and frequent words", "text": "It is argued in Li and Jurafsky (2015) that often just increasing the dimensionality of the SG model suffices to obtain better results than that of their multi-sense model. We look at the effect of dimensionality on semantic similarity in fig. 2b, and see that simply increasing the dimensionality of the SG model (to any of 100, 200 or 300 dimensions) is not sufficient to outperform the MU or BIMU models. When constraining the vocabulary to 6,000 most frequent words, the representations obtain higher quality. We can see that the models, especially SG, benefit slightly more from the increased dimensionality when looking at these most frequent words. This is according to expectations\u2014frequent words need more representational capacity due to their complex semantic and syntactic behavior (Atkins and Rundell, 2008)."}, {"heading": "6.3 The role of bilingual signal", "text": "The degree of contribution of the second language l\u2032 during learning is affected by two parameters, \u03bb for the trade-off between the importance of first and second language in the sense prediction part (encoder) and the value of m for the size of the window around the second-language word affiliated to the pivot. Fig. 3a suggests that the context from the second language\nis useful in sense prediction, and that it should be weighted relatively heavily (around 0.7 and 0.8, depending on the language).\nRegarding the role of the context-window size in sense disambiguation, the WSD literature has reported both smaller (more local) and larger (more topical) monolingual contexts to be useful, see e.g. Ide and Ve\u0301ronis (1998) for an overview. In fig. 3b we find that considering a very narrow context in the second language\u2014the affiliated word only or a m = 1 window around it\u2014performs the best, and that there is little gain in using a broader window. This is understandable since the l\u2032 representation participating in the sense selection is simply an average over all generic embeddings in the window, which means that the averaged representation probably becomes noisy for large m, i.e. more irrelevant words are included in the window. However, the negative effect on the accuracy is still relatively small, up to around \u22120.1 for the models using French and Russian as the second languages, and \u22120.25 for Czech when setting m = \u221e. The infinite window size setting, corresponding to the sentence-only alignment, performs well also on SCWS, improving on the monolingual multi-sense baseline on all corpora (Table 4)."}, {"heading": "6.4 The number of senses", "text": "In our work, the number of senses k is a model parameter, which we keep fixed to 3 throughout the empirical study. We comment here briefly on other choices of k \u2208 {2, 4, 5}. We have found k = 2 to be a good choice on the RU-EN and FR-EN corpora (but not on CZ-EN), with an around 0.2-point improvement over k = 3 on SCWS and in POS tagging. With the larger values of k, the performance tends to degrade. For example, on RU-EN, the k = 5 score on SCWS is about 0.6 point below our default setting."}, {"heading": "7 Additional Related Work", "text": "Multi-sense models. One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010). In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired \u2018centroid\u2019 vectors that keep track of the contexts in which word senses have occurred. They explore two model variants, one in which the number of senses is the same for all words, and another in which a threshold value determines the number of senses for each word. The results comparing the two variants are inconclusive, with the advantage of the dynamic variant being virtually nonexistent.\nIn our work, we use the static approach. Whenever there is evidence for less senses than the number of available sense vectors, this is unlikely to be a serious issue as the learning would concentrate on some of the senses, and these would then be the preferred predictions also at test time. Li and Jurafsky (2015) build upon the work of Neelakantan et al. with a more principled method for introducing new senses using the Chinese Restaurant Processes (CRP). Our experiments confirm the findings of Neelakantan et al. that multi-sense embeddings improve Skip-gram embeddings on intrinsic tasks, as well as those of Li and Jurafsky, who find that multi-sense embeddings offer little benefit to the neural network learner on extrinsic tasks. Our discrete-autoencoding method when viewed without the bilingual part in the encoder has a lot in common with their methods.\nMultilingual models. The research on using multilingual information in the learning of multi-sense embedding models is scarce. Guo et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings. Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of\nLin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; Ta\u0308ckstro\u0308m et al., 2012).\nThe recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embedding dimensionality is stochastic is relevant since it demonstrates that their embeddings exploit different dimensions to encode different word meanings. Just like us, Kawakami and Dyer (2015) use bilingual supervision, but in a more complex LSTM network that is trained to predict word translations. Although they do not represent different word senses separately, their method produces representations that depend on the context. In our work, the second-language signal is introduced only in the sense prediction component and is flexible\u2014it can be defined in various ways and can be obtained from sentence-only alignments as a special case."}, {"heading": "8 Conclusion", "text": "We have presented a method for learning multi-sense embeddings that performs sense estimation and context prediction jointly. Both mono- and bilingual information is used in the sense prediction during training. We have explored the model performance on a variety of tasks, showing that the bilingual signal improves the sense predictor, even though the crosslingual information is not available at test time. In this way, we are able to obtain word representations that are of better quality than the monolingually-trained multi-sense representations, and that outperform the Skip-Gram embeddings on intrinsic tasks. We have analyzed the model performance under several conditions, namely varying dimensionality, vocabulary size, amount of data, and size of the second-language context. For the latter parameter, we find that bilingual information is useful even when using the entire\nsentence as context, suggesting that sentence-only alignment might be sufficient in certain situations."}, {"heading": "Acknowledgments", "text": "We would like to thank Jiwei Li for providing his tagger implementation, and Robert Grimm, Diego Marcheggiani and the anonymous reviewers for useful comments. The computational work was carried out on Peregrine HPC cluster of the University of Groningen. The second author was supported by NWO Vidi grant 016.153.327."}], "references": [{"title": "A study on similarity and relatedness using distributional and WordNet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "NAACL-HLT.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A. Smith."], "venue": "NIPS.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "The Oxford guide to practical lexicography", "author": ["Sue B.T. Atkins", "Michael Rundell."], "venue": "Oxford University Press.", "citeRegEx": "Atkins and Rundell.,? 2008", "shortCiteRegEx": "Atkins and Rundell.", "year": 2008}, {"title": "An unsupervised model for instance level subcategorization acquisition", "author": ["Simon Baker", "Roi Reichart", "Anna Korhonen."], "venue": "EMNLP.", "citeRegEx": "Baker et al\\.,? 2014", "shortCiteRegEx": "Baker et al\\.", "year": 2014}, {"title": "Unsupervised translation sense clustering", "author": ["Mohit Bansal", "John Denero", "Dekang Lin."], "venue": "NAACL-HLT.", "citeRegEx": "Bansal et al\\.,? 2012", "shortCiteRegEx": "Bansal et al\\.", "year": 2012}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "ACL.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In ACL.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Breaking sticks and ambiguities with adaptive skip-gram", "author": ["Sergey Bartunov", "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov."], "venue": "arXiv preprint arXiv:1502.07257.", "citeRegEx": "Bartunov et al\\.,? 2015", "shortCiteRegEx": "Bartunov et al\\.", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "The Joy of Parallelism with CzEng", "author": ["Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd", "Ond\u0159ej Du\u0161ek", "Petra Galu\u0161\u010d\u00e1kov\u00e1", "Martin Majli\u0161", "David Mare\u010dek", "Ji\u0159\u0131\u0301 Mar\u0161\u0131\u0301k", "Michal Nov\u00e1k", "Martin Popel", "Ale\u0161 Tamchyna"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2012}, {"title": "Findings of the 2013 Workshop on Statistical Machine Translation", "author": ["Specia."], "venue": "WMT.", "citeRegEx": "Specia.,? 2013", "shortCiteRegEx": "Specia.", "year": 2013}, {"title": "Word-sense disambiguation using statistical methods", "author": ["Peter F Brown", "Stephen A Della Pietra", "Vincent J Della Pietra", "Robert L Mercer."], "venue": "ACL.", "citeRegEx": "Brown et al\\.,? 1991", "shortCiteRegEx": "Brown et al\\.", "year": 1991}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."], "venue": "ACL.", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Findings of the 2009 Workshop on Statistical Machine Translation", "author": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Josh Schroeder."], "venue": "WMT.", "citeRegEx": "Callison.Burch et al\\.,? 2009", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2009}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar A P", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha."], "venue": "NIPS.", "citeRegEx": "P et al\\.,? 2014", "shortCiteRegEx": "P et al\\.", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Word sense disambiguation using a second language monolingual corpus", "author": ["Ido Dagan", "Alon Itai."], "venue": "Computational Linguistics, 20(4):563\u2013596.", "citeRegEx": "Dagan and Itai.,? 1994", "shortCiteRegEx": "Dagan and Itai.", "year": 1994}, {"title": "The CMU-Avenue French-English Translation System", "author": ["Michael Denkowski", "Greg Hanneman", "Alon Lavie."], "venue": "WMT.", "citeRegEx": "Denkowski et al\\.,? 2012", "shortCiteRegEx": "Denkowski et al\\.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "ACL.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "An unsupervised method for word sense tagging using parallel corpora", "author": ["Mona Diab", "Philip Resnik."], "venue": "ACL.", "citeRegEx": "Diab and Resnik.,? 2002", "shortCiteRegEx": "Diab and Resnik.", "year": 2002}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models", "author": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2010", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "An information theoretic approach to bilingual word clustering", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "ACL.", "citeRegEx": "Faruqui and Dyer.,? 2013", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2013}, {"title": "Community evaluation and exchange of word vectors at wordvectors.org", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In ACL System Demonstrations", "citeRegEx": "Faruqui and Dyer.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "EACL.", "citeRegEx": "Faruqui and Dyer.,? 2014b", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "WWW.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Posterior regularization for structured latent variable models", "author": ["Kuzman Ganchev", "Jo\u00e3o Gra\u00e7a", "Jennifer Gillenwater", "Ben Taskar."], "venue": "The Journal of Machine Learning Research, 11:2001\u20132049.", "citeRegEx": "Ganchev et al\\.,? 2010", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "author": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "arXiv preprint arXiv:1410.2455.", "citeRegEx": "Gouws et al\\.,? 2014", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering", "author": ["Clayton Greenberg", "Asad Sayeed", "Vera Demberg."], "venue": "NAACL.", "citeRegEx": "Greenberg et al\\.,? 2015", "shortCiteRegEx": "Greenberg et al\\.", "year": 2015}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "COLING.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren."], "venue": "KDD.", "citeRegEx": "Halawi et al\\.,? 2012", "shortCiteRegEx": "Halawi et al\\.", "year": 2012}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "ACL.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Embedding word similarity with neural machine translation", "author": ["Felix Hill", "Kyunghyun Cho", "S\u00e9bastien Jean", "Coline Devin", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.6448.", "citeRegEx": "Hill et al\\.,? 2014a", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "arXiv preprint arXiv:1408.3456.", "citeRegEx": "Hill et al\\.,? 2014b", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "ACL.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Introduction to the special issue on word sense disambiguation: the state of the art", "author": ["Nancy Ide", "Jean V\u00e9ronis."], "venue": "Computational linguistics, 24(1):2\u201340.", "citeRegEx": "Ide and V\u00e9ronis.,? 1998", "shortCiteRegEx": "Ide and V\u00e9ronis.", "year": 1998}, {"title": "Cross-lingual sense determination: Can it work", "author": ["Nancy Ide"], "venue": "Computers and the Humanities,", "citeRegEx": "Ide.,? \\Q2000\\E", "shortCiteRegEx": "Ide.", "year": 2000}, {"title": "Word sense acquisition from bilingual comparable corpora", "author": ["Hiroyuki Kaji."], "venue": "NAACL-HLT.", "citeRegEx": "Kaji.,? 2003", "shortCiteRegEx": "Kaji.", "year": 2003}, {"title": "Learning to represent words in context with multilingual supervision", "author": ["Kazuya Kawakami", "Chris Dyer."], "venue": "arXiv preprint arXiv:1511.04623.", "citeRegEx": "Kawakami and Dyer.,? 2015", "shortCiteRegEx": "Kawakami and Dyer.", "year": 2015}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "COLING.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit, volume 5.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "CoNLL.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "In EMNLP", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "Phrase clustering for discriminative learning", "author": ["Dekang Lin", "Xiaoyun Wu."], "venue": "ACL-IJCNLP of AFNLP.", "citeRegEx": "Lin and Wu.,? 2009", "shortCiteRegEx": "Lin and Wu.", "year": 2009}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "NAACL.", "citeRegEx": "Lu et al\\.,? 2015", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Discrete-state variational autoencoders for joint discovery and factorization of relations", "author": ["Diego Marcheggiani", "Ivan Titov."], "venue": "Transactions of the Association for Computational Linguistics, 4.", "citeRegEx": "Marcheggiani and Titov.,? 2016", "shortCiteRegEx": "Marcheggiani and Titov.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "ICLR Workshop Papers.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A Miller", "Walter G Charles."], "venue": "Language and cognitive processes, 6(1):1\u201328.", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Infinite dimensional word embeddings", "author": ["Eric Nalisnick", "Sachin Ravi."], "venue": "arXiv preprint arXiv:1511.05392.", "citeRegEx": "Nalisnick and Ravi.,? 2015", "shortCiteRegEx": "Nalisnick and Ravi.", "year": 2015}, {"title": "Multilingual part-of-speech tagging: Two unsupervised approaches", "author": ["Tahira Naseem", "Benjamin Snyder", "Jacob Eisenstein", "Regina Barzilay."], "venue": "Journal of Artificial Intelligence Research, 36:1\u201345.", "citeRegEx": "Naseem et al\\.,? 2009", "shortCiteRegEx": "Naseem et al\\.", "year": 2009}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "EMNLP.", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Exploiting parallel texts for word sense disambiguation: An empirical study", "author": ["Hwee Tou Ng", "Bin Wang", "Yee Seng Chan."], "venue": "ACL.", "citeRegEx": "Ng et al\\.,? 2003", "shortCiteRegEx": "Ng et al\\.", "year": 2003}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."], "venue": "CoNLL.", "citeRegEx": "Passos et al\\.,? 2014", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "WWW.", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Multiprototype vector-space models of word meaning", "author": ["Joseph Reisinger", "J. Raymond Mooney."], "venue": "NAACL-HLT.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Learning internal representations by error propagation", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams."], "venue": "David E. Rumelhart, James L. McClelland, and PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Mi-", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Climbing the Tower of Babel: Unsupervised Multilingual Learning", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "ICML.", "citeRegEx": "Snyder and Barzilay.,? 2010", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2010}, {"title": "Leveraging monolingual data for crosslingual compositional word representations", "author": ["Hubert Soyer", "Pontus Stenetorp", "Akiko Aizawa."], "venue": "CoRR, abs/1412.6334.", "citeRegEx": "Soyer et al\\.,? 2014", "shortCiteRegEx": "Soyer et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "NAACL-HLT.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu."], "venue": "COLING.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Unsupervised induction of semantic roles within a reconstructionerror minimization framework", "author": ["Ivan Titov", "Ehsan Khoddam."], "venue": "NAACL.", "citeRegEx": "Titov and Khoddam.,? 2015", "shortCiteRegEx": "Titov and Khoddam.", "year": 2015}, {"title": "Crosslingual induction of semantic roles", "author": ["Ivan Titov", "Alexandre Klementiev."], "venue": "ACL.", "citeRegEx": "Titov and Klementiev.,? 2012", "shortCiteRegEx": "Titov and Klementiev.", "year": 2012}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer."], "venue": "EMNLP.", "citeRegEx": "Tsvetkov et al\\.,? 2015", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Verb similarity on the taxonomy of wordnet", "author": ["Dongqiang Yang", "David M.W. Powers."], "venue": "GWC.", "citeRegEx": "Yang and Powers.,? 2006", "shortCiteRegEx": "Yang and Powers.", "year": 2006}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong."], "venue": "ACL.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Zou et al\\.,? 2013", "shortCiteRegEx": "Zou et al\\.", "year": 2013}, {"title": "Toward using confidence intervals to compare correlations", "author": ["Guang Yong Zou."], "venue": "Psychological methods, 12(4).", "citeRegEx": "Zou.,? 2007", "shortCiteRegEx": "Zou.", "year": 2007}], "referenceMentions": [{"referenceID": 48, "context": "They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al.", "startOffset": 39, "endOffset": 108}, {"referenceID": 6, "context": "They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al.", "startOffset": 39, "endOffset": 108}, {"referenceID": 42, "context": "They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al.", "startOffset": 39, "endOffset": 108}, {"referenceID": 67, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 16, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 5, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 55, "context": ", 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014).", "startOffset": 140, "endOffset": 227}, {"referenceID": 35, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 63, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 53, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 15, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 43, "context": "While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015).", "startOffset": 238, "endOffset": 345}, {"referenceID": 40, "context": ", a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 32, "context": ", a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 28, "context": ", a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014).", "startOffset": 117, "endOffset": 215}, {"referenceID": 30, "context": "Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014).", "startOffset": 107, "endOffset": 125}, {"referenceID": 25, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 70, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 65, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 60, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 52, "context": "We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009).", "startOffset": 80, "endOffset": 199}, {"referenceID": 38, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 54, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 20, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 37, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 17, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 11, "context": "Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991).", "startOffset": 151, "endOffset": 256}, {"referenceID": 59, "context": "Our method borrows its general structure from neural autoencoders (Rumelhart et al., 1986; Bengio et al., 2013).", "startOffset": 66, "endOffset": 111}, {"referenceID": 8, "context": "Our method borrows its general structure from neural autoencoders (Rumelhart et al., 1986; Bengio et al., 2013).", "startOffset": 66, "endOffset": 111}, {"referenceID": 1, "context": "Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).", "startOffset": 143, "endOffset": 163}, {"referenceID": 64, "context": ", 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).", "startOffset": 33, "endOffset": 58}, {"referenceID": 47, "context": ", 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016).", "startOffset": 82, "endOffset": 112}, {"referenceID": 48, "context": "This is effectively a Skip-Gram model (Mikolov et al., 2013a) extended to rely on senses.", "startOffset": 38, "endOffset": 61}, {"referenceID": 49, "context": "(2) and use negative sampling (Mikolov et al., 2013b) instead of log p(xj |xi, s, \u03b8):", "startOffset": 30, "endOffset": 53}, {"referenceID": 53, "context": "Optimizing the autoencoding objective is broadly similar to the learning algorithm defined for multi-sense embedding induction in some of the previous work (Neelakantan et al., 2014; Li and Jurafsky, 2015).", "startOffset": 156, "endOffset": 205}, {"referenceID": 43, "context": "Optimizing the autoencoding objective is broadly similar to the learning algorithm defined for multi-sense embedding induction in some of the previous work (Neelakantan et al., 2014; Li and Jurafsky, 2015).", "startOffset": 156, "endOffset": 205}, {"referenceID": 27, "context": "We therefore use a form of posterior regularization (Ganchev et al., 2010) where we can encode our prior expectations that the posteriors should be sharp.", "startOffset": 52, "endOffset": 74}, {"referenceID": 47, "context": "This modified objective can also be motivated from a variational approximation perspective, see Marcheggiani and Titov (2016) for details.", "startOffset": 96, "endOffset": 126}, {"referenceID": 43, "context": "At test time, we construct the word representations by averaging all sense embeddings for a word xi and weighting them with the sense expectations (Li and Jurafsky, 2015)3:", "startOffset": 147, "endOffset": 170}, {"referenceID": 19, "context": "In defining the crosslingual signal we draw on a heuristic inspired by Devlin et al. (2014). The secondlanguage context words are taken to be the multiset of words around and including the pivot affiliated to xi: C \u2032 i = {xai\u2212m, .", "startOffset": 71, "endOffset": 92}, {"referenceID": 22, "context": "We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora.", "startOffset": 24, "endOffset": 43}, {"referenceID": 21, "context": "We use the AdaGrad optimizer (Duchi et al., 2011) with initial learning rate set to 0.", "startOffset": 29, "endOffset": 49}, {"referenceID": 41, "context": "In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data.", "startOffset": 71, "endOffset": 84}, {"referenceID": 9, "context": "0 (Bojar et al., 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 13, "context": ", 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al., 2009) (FR-EN).", "startOffset": 77, "endOffset": 106}, {"referenceID": 35, "context": "The SCWS benchmark (Huang et al., 2012) is probably the most suitable", "startOffset": 19, "endOffset": 39}, {"referenceID": 26, "context": "WS-353 contains 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al.", "startOffset": 43, "endOffset": 69}, {"referenceID": 58, "context": "The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only.", "startOffset": 10, "endOffset": 43}, {"referenceID": 50, "context": "The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only.", "startOffset": 58, "endOffset": 84}, {"referenceID": 56, "context": "The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 31, "context": ", 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT.", "startOffset": 22, "endOffset": 43}, {"referenceID": 12, "context": "Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs.", "startOffset": 15, "endOffset": 35}, {"referenceID": 68, "context": "The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity.", "startOffset": 47, "endOffset": 67}, {"referenceID": 46, "context": "Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs.", "startOffset": 10, "endOffset": 30}, {"referenceID": 34, "context": "Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness.", "startOffset": 20, "endOffset": 40}, {"referenceID": 0, "context": ", 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL).", "startOffset": 15, "endOffset": 36}, {"referenceID": 0, "context": ", 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository.", "startOffset": 15, "endOffset": 912}, {"referenceID": 66, "context": "We also evaluate on a task measuring the similarity between the embeddings\u2014in our case uniformly averaged in the case of multi-sense embeddings\u2014and a matrix of supersense features extracted from the English SemCor, using the Qvec tool (Tsvetkov et al., 2015).", "startOffset": 235, "endOffset": 258}, {"referenceID": 43, "context": "We use the same convolutional architecture as Li and Jurafsky (2015): an input layer taking a concatenation of neighboring embeddings as input, three hidden layers with a rectified linear unit activation function and a softmax output layer.", "startOffset": 46, "endOffset": 69}, {"referenceID": 71, "context": "We have also measured the 95% confidence intervals of the difference between the correlation coefficients of BIMU and SG, following the method described in Zou (2007). According to these values, BIMU significantly outperforms SG on RU-EN, and on French, Russian and Spanish NC corpora.", "startOffset": 156, "endOffset": 167}, {"referenceID": 18, "context": "Also, FR-EN is known to be noisy, containing webcrawled sentences that are not parallel or not natural language (Denkowski et al., 2012).", "startOffset": 112, "endOffset": 136}, {"referenceID": 43, "context": "For example, Li and Jurafsky (2015) use the concatenation of Gigaword and Wikipedia with more than 5B words.", "startOffset": 13, "endOffset": 36}, {"referenceID": 15, "context": "0 Chen et al. (2014) 68.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "0 Chen et al. (2014) 68.4 Neelakantan et al. (2014) 69.", "startOffset": 2, "endOffset": 52}, {"referenceID": 15, "context": "0 Chen et al. (2014) 68.4 Neelakantan et al. (2014) 69.3 Li and Jurafsky (2015) 69.", "startOffset": 2, "endOffset": 80}, {"referenceID": 29, "context": "Similar observations have been made in the context of selectional preference modeling of polysemous verbs (Greenberg et al., 2015).", "startOffset": 106, "endOffset": 130}, {"referenceID": 43, "context": "The neural network tagger may be able to implicitly perform disambiguation on top of single-sense SG embeddings, similarly to what has been argued in Li and Jurafsky (2015). The tagging accuracies obtained with MU on CZ-EN and FR-EN are similar to the one obtained by Li and Jurafsky with their multi-sense model (93.", "startOffset": 150, "endOffset": 173}, {"referenceID": 2, "context": "This is according to expectations\u2014frequent words need more representational capacity due to their complex semantic and syntactic behavior (Atkins and Rundell, 2008).", "startOffset": 138, "endOffset": 164}, {"referenceID": 41, "context": "It is argued in Li and Jurafsky (2015) that often just increasing the dimensionality of the SG model suffices to obtain better results than that of their multi-sense model.", "startOffset": 16, "endOffset": 39}, {"referenceID": 36, "context": "Ide and V\u00e9ronis (1998) for an overview.", "startOffset": 0, "endOffset": 23}, {"referenceID": 35, "context": "One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010).", "startOffset": 138, "endOffset": 186}, {"referenceID": 57, "context": "One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010).", "startOffset": 138, "endOffset": 186}, {"referenceID": 53, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 63, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 43, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 7, "context": "In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015).", "startOffset": 72, "endOffset": 163}, {"referenceID": 48, "context": "(2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired \u2018centroid\u2019 vectors that keep track of the contexts in which word senses have occurred.", "startOffset": 41, "endOffset": 64}, {"referenceID": 7, "context": ", 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al.", "startOffset": 31, "endOffset": 81}, {"referenceID": 7, "context": ", 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired \u2018centroid\u2019 vectors that keep track of the contexts in which word senses have occurred. They explore two model variants, one in which the number of senses is the same for all words, and another in which a threshold value determines the number of senses for each word. The results comparing the two variants are inconclusive, with the advantage of the dynamic variant being virtually nonexistent. In our work, we use the static approach. Whenever there is evidence for less senses than the number of available sense vectors, this is unlikely to be a serious issue as the learning would concentrate on some of the senses, and these would then be the preferred predictions also at test time. Li and Jurafsky (2015) build upon the work of Neelakantan et al.", "startOffset": 31, "endOffset": 926}, {"referenceID": 28, "context": "Guo et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of", "startOffset": 19, "endOffset": 40}, {"referenceID": 45, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 25, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 33, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 69, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 23, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 70, "context": "Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space.", "startOffset": 92, "endOffset": 216}, {"referenceID": 32, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 28, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 61, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 40, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 62, "context": "A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T\u00e4ckstr\u00f6m et al., 2012).", "startOffset": 171, "endOffset": 313}, {"referenceID": 38, "context": "The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest.", "startOffset": 20, "endOffset": 45}, {"referenceID": 38, "context": "The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest.", "startOffset": 20, "endOffset": 75}, {"referenceID": 38, "context": "The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embedding dimensionality is stochastic is relevant since it demonstrates that their embeddings exploit different dimensions to encode different word meanings. Just like us, Kawakami and Dyer (2015) use bilingual supervision, but in a more complex LSTM network that is trained to predict word translations.", "startOffset": 20, "endOffset": 356}], "year": 2016, "abstractText": "We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time.", "creator": "LaTeX with hyperref package"}}}