{"id": "1604.00317", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "A Semisupervised Approach for Language Identification based on Ladder Networks", "abstract": "in this study we highlighted the problem of training a neuralnetwork for language identification using both labeled random modified speech samples in the form of i - vectors. we propose a neural network architecture package can also handle out - q - set languages. scholars utilize some modified version surrounding the recently proposed ladder network semisupervised training hypothesis that optimizes the reconstruction costs of a stack of denoising autoencoders. we show that comparative insight can be successfully applied regarding the case where the training dataset is composed of both labeled and unlabeled acoustic data. the results show simultaneous language identification on the nist 2015 language identification dataset.", "histories": [["v1", "Fri, 1 Apr 2016 16:26:57 GMT  (678kb,D)", "http://arxiv.org/abs/1604.00317v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ehud ben-reuven", "jacob goldberger"], "accepted": false, "id": "1604.00317"}, "pdf": {"name": "1604.00317.pdf", "metadata": {"source": "CRF", "title": "A Semisupervised Approach for Language Identification based on Ladder Networks", "authors": ["Ehud Ben-Reuven", "Jacob Goldberger"], "emails": ["udi@benreuven.com", "jacob.goldberger@biu.ac.il"], "sections": [{"heading": null, "text": "1. Introduction The purpose of automatic language recognition is to identify which language is spoken from a speech sample. There are many characteristics of speech that could be used to identify languages. Languages are made up of different sounds that form phonemes, so it is possible to distinguish languages based on the acoustic features present in the speech signal. There are of course also lexical information. Languages are separable by the vocabulary, or sets of words and syntactic rules. In this study we focus on language identification that is based solely on the acoustic information conveyed by the speech signal. The applications of language identification systems include multilingual translation systems and emergency call routing, where the response time of a fluent native operator might be critical.\nThe impressive performance improvement obtained using deep neural networks (DNNs) for automatic speech recognition (ASR) [1] have motivated the application of DNNs to speaker and language recognition. DNN have been trained for a different purpose to learn frame-level features that were then used to train a secondary classifier for the intended language recognition task [2][3]. DNNs have also been applied to directly train language classification systems [4][5].\nIn this study we applied DNN to language recogni-\ntion and report performance on the NIST 2015 Language Recognition i-vector Machine Learning Challenge [6]. In this task there are i-vectors examples from 50 languages. There are also unlabeled training data and we also need to address the problem of out-of-set languages that can appear in both the unlabeled training data and the test set. In this study we propose a DNN based approach that addresses both issues of unlabeled training data and out-ofset examples. Most previous DNN learning approaches have used unlabeled data only for pre-training which is a way to initialize the weights of the network followed by normal supervised learning. The standard pre-training strategy is based on a greedy layer-wise procedure using either Restricted Boltzmann Machines (RBM) [7] or noisy autoencoders [8]. In contrast, here we aimed to explicitly incorporate the unlabeled data into the costfunction that is optimized in the training step.\nOur approach is based on the recently proposed Ladder Network training procedure that has proven to be very successful, especially in cases where the training dataset is composed of both labeled and unlabeled data [9] [10]. In addition to the supervised objective, the Ladder Network also has an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. However, instead of reconstructing one layer at a time and greedily stacking, an unsupervised objective involving the reconstruction of all layers is optimized jointly by all parameters (with the relative importance of each layer cost controlled by hyper-parameters).\nThe paper proceeds as follows. In the next section we present a brief overview of Ladder Networks. We then describe a semisupervised language identification system based on Ladder Networks. We demonstrate the performance improvement of the proposed method on a dataset from the NIST 2015 Language Recognition ivector Challenge [6]."}, {"heading": "2. The Ladder Network Architecture", "text": "In this section we give a brief overview of the Ladder Network training technique. A detailed description of Ladder Networks can be found in [9] [10]. Assume we\nar X\niv :1\n60 4.\n00 31\n7v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\nare given training data consisting of n labeled examples x1, ..., xn \u2208 Rd with labels c1, ..., cn \u2208 {1, ..., k} and m\u2212n additional unlabeled examples xn+1, ..., xm. We assume that the unrevealed labels belong to the same set {1, ..., k}.\nOur goal is to train a fully connected multi-layer DNN using both the labeled and the unlabeled training data. Each layer of the network is formalized as a linear transformation followed by a non-linear activation function \u03c6:\nh0 = x\nzl =Wlhl\u22121, l = 1, ..., L\nhl = \u03c6(zl)\n(1)\nSoftmax is used as the non-linear activation function in the last layer to compute the output class distribution hL = p(y|x). In the Ladder Network training procedure in addition to the standard forward pass, we also apply a corrupted forward pass which is implemented by adding isotropic Gaussian noise to the input and to all hidden layers:\nh\u03030 = x\u0303 = x+ noise\nz\u0303l =Wlh\u0303l\u22121 + noise, l = 1, ..., L\nh\u0303l = \u03c6(z\u0303l)\n(2)\nDenote the soft-max output obtained by the noisy-pass\nby y\u0303. We view the noisy feed-forward pass as a dataencoding procedure that is followed by a denoising decoding pass whose target is the result of the noise-less forward propagation at each layer. The decoding procedure is defined as follows:\nuL = h\u0303L\nz\u0302L = g(z\u0303L, uL)\nul = Vl+1z\u0302l+1, l = L\u22121, ..., 0 z\u0302l = g(z\u0303l, ul)\nx\u0302 = z\u03020\n(3)\nwhere Vl is a weight matrix from layer l to layer l\u22121, with the same dimensions of W >\nl . The function g(\u00b7, \u00b7) is called the combinator function as it combines the vertical ul and the lateral z\u0303l connections (also called skip connections) in an element-wise fashion in order to estimate zl. The estimation is linear as a function of z\u0303l whose coefficients are non-linear functions of ul. A comparison of other possible choices for the combinator function is presented in [11]. The resulting encoder/decoder architecture thus resembles a ladder (hence the name Ladder Networks). Figure 1 illustrates a ladder structure with two hidden layers.\nThe concept of batch normalization was recently proposed by Ioffe and Szegedy [12] as a method to improve convergence as a result of reduced covariance shift. In\nbatch normalization, we compute the running mean and variance of hidden units over the batch, and use them to \u201cwhiten\u201d hidden units in each mini-batch. In Ladder Networks batch normalization plays another crucial role of preventing encoder and decoder collapse to a trivial solution of a constant function that can be easily denoised. Note that for a standard autoencoder this cannot happen since we cannot change the input feature vector we want to reconstruct. To simplify the presentation of the Ladder Network we ignored the batch normalization step that is incorporated in each layer of the Ladder Network.\nThe cost function for training the network is composed of a supervised objective and an unsupervised autoencoder objective. The supervised cost Cs is the standard likelihood function which is defined as the average negative log probability of the noisy output y\u0303 matching the target ct given the input xt:\nCs = \u2212 1\nn n\u2211 t=1 log p(y\u0303 = ct|xt)\nNote that, unlike standard DNN, the likelihood function is computed here based on the noisy network (and hence the back-propagation procedure is applied to the noisy feed-forward step). The noise is used here to regularize supervised learning in a way similar to the weight noise regularization method [13] and dropout [14].\nThe cost of the standard denoising autoencoder is the input reconstruction error [15] [16]. Here the autoencoder cost function is the reconstruction error of the input along with all the network layers. The error here is defined as the distance of the denoised decoded value from the noise-less forward value at each layer. The unsupervised ladder cost function Cd is:\nCd = L\u2211 l=0 \u03bblC (l) d = 1 m L\u2211 l=0 \u03bbl wl m\u2211 t=1 \u2016zl,t \u2212 z\u0302l,t\u20162 (4)\nwhere wl is the layer\u2019s width, m the number of training samples, and the hyperparameter \u03bbl is a layer-wise multiplier determining the importance of the denoising cost. zl,t is the noise-free hidden layer obtained from the input xt and z\u0302l,t is the corresponding decoded layer. The target of the encode/decode at each layer is the result of the noise-less forward propagation. Note that the ladderautoencoder cost function does not use the labels and therefore it can be applied to both labeled and unlabeled training data.\nA recent study [11] conducted an extensive experimental investigation of variants of the Ladder Network in which individual components were removed or replaced to gain insights into their relative importance. The authors found that all of the components were necessary for optimal performance. Ladder Networks have been found to obtain state-of-the-art results on standard datasets with only a small portion of labeled examples [9]. This confirms a current trend in Deep Learning that, while recent\nprogress in DL (e.g. [12] [14]) applied on large labeled datasets does not rely on any unsupervised pretraining, semisupervised learning might instead be crucial for success when only a small amount of labeled data is available."}, {"heading": "3. Dataset and Evaluation Metrics", "text": "The 2015 language recognition i-vector challenge [6] covers 50 target languages and a set of unnamed \u201cout-ofset\u201d (oos) languages. Labeled training data (300 speech segments per language) were provided for the target languages and a set of approximately 6,500 unlabeled examples covering the target and out-of-set languages was provided for development. The test set consisted of 6,500 test segments covering the target and out-of-set languages. The speech duration of the audio segments used to create the i-vectors for the challenge were sampled from a log-normal distribution with a mean of approximately 35s. The speech segments are derived from conversational telephone and narrow-band broadcast speech data. Each speech segment is represented by an i-vector of 400 components [17].\nThe task consists of classifying each speech segment as either one of the 50 target languages or as an out-ofset. According to the challenge rules [18] the goal is to minimize the following cost function:\nCost = 1\u2212poos k\n\u00b7 k\u2211\ni=1\nperror(i) + poos \u00b7 perror(oos) (5)\nwhere k = 50, poos = 0.23 is (assumed to be) the fraction of out-of-set examples in the test set and\nperror(i) = # errors-class-i # trials-class-i\nis the fraction of examples in the test set with label i that were misclassified."}, {"heading": "4. Model and Training", "text": ""}, {"heading": "4.1. Network and cost function", "text": "Assume we are given training data consisting of n labeled examples x1, ..., xn \u2208 Rd with labels c1, ..., cn \u2208 {1, ..., k}. We also have a development set xn+1, ..., xm with unspecified labels. These unspecified labels include all k target labels and multiple additional out-of-set (oos) labels. Since we need to take care of the oos labels, we construct a DNN whose soft-max output layer has k+1 outputs corresponding to the k labels and to an out-ofset decision. Note that in order to train a network with an oos explicit decision, we cannot rely only on in-set labeled data.\nhL\nx\noosk1\nWe applied the Ladder Network training scheme to take maximum advantage of the information conveyed in the unlabeled data. We first used the same ladder reconstruction error cost function that was described in the previous section. We used additional cost function that is composed of two components. The first score is the negative likelihood score of the labeled data:\nC1 = \u2212 1\nn n\u2211 t=1 log p(y\u0303 = ct|xt). (6)\nwhere y\u0303 is the soft-output of the noisy network. Denote the percentage of oos examples in the unlabeled development set (and in the test set) by poos. We assumed that poos is given. We further assumed that there is an equal number of examples from each one of k classes in the unlabeled part of the training set (and in the test set). Define the estimated average label frequency over all the unlabeled set as follows:\npav(i) = 1\nm n+m\u2211 t=n+1 p(y\u0303 = i|xt), i = 1, ..., k\nand\npav(oos) = 1\nm n+m\u2211 t=n+1 p(y\u0303 = oos|xt).\nThe second score encourages the estimated labels over all the development set to maintain the (assumed to be) known label distribution. The score is defined as follows:\nC2 = \u2212poos log pav(oos)\u2212 1\u2212poos k k\u2211 i=1 log pav(i). (7)\nIn practice when we optimized the network parameter using mini-batches, we computed this score on each minibatch instead of the entire development set. We set the size of the mini-batch to be 1024 which is large enough in order to assume, based on the law of large numbers, that the label frequency within the mini-batch is similar to the overall label frequency. The cost function C2 is not part of the original ladder framework. It represents the assumption that the labeled training data and the unlabeled development set have similar label statistics. It\nalso assumes a predefined relative number of oos examples. We show in the experiment section that, in addition to the contribution of the ladder method, this cost function further improves classification performance.\nThe cost function of a batch is made up of the two elements described above:\nC = C1 + \u03b1C2 (8)\nwhere \u03b1 is an hyper-parameter that is tuned in a crossvalidation procedure that is described below. (We can assume without loss of generality that one of the coefficients of the likelihood score, the label frequency score and the ladder score is 1.)\nWe tried to use another standard unsupervised cost function that is based on minimizing the entropy of the distribution obtained by the soft-max layer, thus encouraging the decision in the unlabeled data to focus on one of the languages [19]:\nCent = \u2212 1\nm n+m\u2211 t=n+1 \u2211 i p(y\u0303 = i|xt) log p(y\u0303 = i|xt)\nsuch that i \u2208 {1, ..., k} \u222a {oos}. We tried computing this score with different importance weights either on the noise-free network or the noisy network. However, there was not any significant performance gain using this cost function."}, {"heading": "4.2. A post processing step", "text": "The soft-max layer of the network provides a distribution over the 51 possible outputs. The final decision is then obtained by selecting the most probable label. Once the predictions are made on the test data, we can measure the ratio of the number of examples for which oos was predicted. If this ratio is below the pre-defined ratio, poos, we can relabel more examples as oos until the expected ratio is met. The candidate examples for relabeling are the ones in which probability of the most probable language is the smallest. If this ratio is above the pre-defined ratio, poos, we can relabel fewer examples as oos until the expected ratio is met. This relabeling is done by multiplying the predicted probability to be oos by a reduction bias which gives other labels a higher likelihood of being picked as the predicted label. In the experiment results we show that this procedure indeed helps when using standard network training based only on the labeled data (i.e. when \u03b1=0). However, there was no improvement when this post-processing was applied on the Ladder Network results or when \u03b1>0 in the standard network."}, {"heading": "4.3. Parameter Tuning", "text": "We created a modified dataset for hyper parameters tuning using only the labeled part of the training dataset. We randomly selected 38 out 50 languages as the languageset and considered the data of the remaining 12 languages\nas out-of-set. We took a subset of the 38\u00d7300 examples as the labeled training set. We then used the remaining labeled data to construct a development set and a test set that both contained examples from all the 50 languages. The development and test sets were constructed in a way that all the 50 languages (either in-set or out-of-set) were in the same proportion.\nThe process of creating a modified set can be repeated many times, and each time we chose another set of languages to be out-of-set, thus giving each language an opportunity to be out-of-set.\nThe cross-validation described above was used to set the hyper-parameters of the Ladder Network. The main parameters were the number of layers, size of each layer and the relative importance of each layer in the reconstruction score. We also tuned the relative weights of the score component C1 and C2."}, {"heading": "4.4. Model Configuration", "text": "The network we implemented1 has an i-vector input of dimension 400 which passes through four ReLU hidden layers of size 500, 500, 500 and 100 and a final soft-max output layer of 51 (or 39 when doing cross validation for parameter tuning). In the encoding step a Gaussian noise with standard deviation of \u03c3 = 0.5 was added to the input data and to all intermediate hidden layers. Training was done with mini-batches having a batch size of 1024. Note that this size has a secondary effect through the loss function C2 which averages the predictions across the minibatch before computing the loss. The training consisted of 1000 epoch iterations. The order of the samples was\n1code available at https://github.com/udibr/LRE\nshuffled before each iteration. It turned out that because of the unsupervised learning the ladder method is insensitive to the number of epochs and having between 800 to 2000 epoch iterations gave similar results. A direct skip of information from the encoder to the decoder was used only on the input layer using the Gaussian method described in [9]. The L2 reconstruction error of the denoising layers compared with the noise-free network layers was assigned a weight of 1 for the input layer and the first hidden layer and a weight of 0.3 for all other layers. The weights of C1 and C2 were set to 1 and 0.15 respectively."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. NIST challenge results", "text": "We next report the results for several variants of the method described above on the NIST 2015 Language Recognition i-vector Challenge [6]. Each combination of models and parameters was applied to the test set of the challenge and submitted to the competition web-site. According to the challenge rules, an unknown subset of 30% of the test samples was used to compute a score for the progress-set and the results for the remaining 70% set are not reported by the web-site. The performance score is defined in Eq. (5) where a smaller number is better. Table 1 shows the progress-set results we achieved for different models and parameters on the competition web site.\nWe examined two training procedures: one with the full ladder structure and one with a baseline variant where we set the weight of the ladder reconstruction score to zero. The baseline method is still based on a noisyforward step which, without the layer-reconstruction cost function, turned out to be a regularized learning method similar to dropout [14].\nFor each model (either baseline or Ladder) we tested the results of using two variants of the training cost function (8). One variant (defined by\u03b1=0) was based only on the likelihood score (6) computed on the labeled data. In the second variant (defined by \u03b1=0.15) we also used the cross entropy of the average probabilities (7) computed on the unlabeled data. The optimal value of \u03b1was chosen using the cross-validation procedure on the labeled data that was described above. Note that the Ladder Network uses the unlabeled data for training even if \u03b1 = 0. In contrast, the baseline method ignores the unlabeled data if \u03b1 = 0. Table 1 shows that it is indeed beneficial to use unlabeled data during training as well. The best results were obtained by Ladder training applied to the cost function which besides a likelihood term also used the cross entropy of the average probabilities. Table 1 reveals the importance of the cost function propose in this study (7) that combined with the Ladder network training method yielded the best results.\nWe next examined the contribution of the postprocessing step described in Section 4.2. Table 1 also shows the ratio of oos decisions in the test data submitted to the challenge site. We tried to improve the results by using the post-processing step (4.2) with a parameter, poos = 0.23, for the oos ratio. The results are shown in the rightmost column of Table 1. The post-processing step only led to improvement in the baseline model and only in the case of \u03b1 = 0 where only the likelihood cost function was in use. Note that in this case the training step is only based on the labeled data. In all the other cases the post-processing did not help."}, {"heading": "5.2. Analysis", "text": "The ladder model exhibited superior regularization, as was evident in the cross validation tests we made. Table 2 shows the cross validation performance results for the ladder and the baseline model (without the ladder reconstruction score). The score was defined in (5) where the number of languages was set to be k = 38. Each row of the table corresponds to a different split of the 50 languages into 38 in-set and 12 out-of-set languages. Selecting different sets of languages to be in-set and outof-set can yield drastically different results. We therefore repeated the process 5 times to allow for each language to be out-of-set at least once.\nTable 2 also reports the number of epochs that were needed to get the best test results. In the baseline method there was a problem of over-fitting and that required early\nstopping to achieve best results. By contrast, the ladder score imposes an aggressive regularization which avoids the need for early stopping. Figure 2 shows the cost function that was minimized during training as a function of the number of epochs. The cost function is plotted for both the training data and the test data. It is clear that, while in the baseline method the problem of overfitting could not be avoided, the Ladder Network has no overfitting problems. This behavior is yet another advantage of the Ladder Network training procedure in that we can train the network up to local optimum without the risk of overfitting.\nTo conclude, in this study we utilized the Ladder Network technique for semi-supervised learning of a language identification system. We showed that the Ladder Network provided an improved framework for integrating unlabeled data into a supervised training scheme. The Ladder Network also uses the unlabeled data in the cost function optimized at the training step. This enabled us to explicitly address out-of-set examples in part of the training process. We also used a new cost function that encourages the statistics of the predicted labels in the unlabeled dataset to be similar to the label statistics of the labeled training data. The joint contribution of the ladder training methods and this cost function provided a significant reduction in classification errors of 15% (from 28.41 to 23.95) when applied to the NIST 2015 Language Recognition Challenge."}, {"heading": "6. References", "text": "[1] G. Hinton et al., \u201cDeep neural networks for acoustic mod-\neling in speech recognition,\u201d IEEE Signal Processing Magazine, vol. 29, no. 8, pp. 82\u201397, 2012.\n[2] Y. Song, B. Jiang, Y. Bao, S. Wei, and L.-R. Dai, \u201cI-vector representation based on bottleneck features for language identification,\u201d Electron. Lett., pp. 1569\u20131580, 2013.\n[3] P. Matejka, L. Zhang, T. Ng, H. S. Mallidi, O. Glembek, J. Ma, and B. Zhang, \u201cNeural network bottleneck features for language identification,\u201d in IEEE Odyssey, 2014, pp. 299\u2013304.\n[4] I. Lopez-Moreno, J. Gonzalez-Dominguez, D. Martinez O. Plchot, J. Gonzalez-Rodriguez, and P. Moreno, \u201cAutomatic language identification using deep neural networks,\u201d in ICASSP, 2014, pp. 5374\u20135378.\n[5] F. Richardson, D. Reynolds, and N. Dehak, \u201cDeep neural network approaches to speaker and language recognition,\u201d IEEE Signal Processing Letters, vol. 22, no. 10, pp. 1671\u2013 1675, 2015.\n[6] \u201cNIST language recognition i-vector machine learning challenge,\u201d https://ivectorchallenge.nist. gov/, 2015.\n[7] G. E. Hinton, S. Osindero, and Y. Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural Computation, vol. 18, pp. 1527\u20131554, 2006.\n[8] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy layer-wise training of deep networks,\u201d in Advances in Neural Information Processing Systems (NIPS), 2007.\n[9] A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko, \u201cSemisupervised learning with ladder network,\u201d arXiv:1507.02672, 2015.\n[10] H. Valpola, \u201cFrom neural PCA to deep unsupervised learning,\u201d arXiv:1411.7783, 2014.\n[11] M. Pezeshki, L. Fan, P. Brakel, A. Courville, and Y. Bengio, \u201cDeconstructing the ladder network architecture,\u201d arXiv:1511.06430, 2015.\n[12] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d in Int. Conference on Machine Learning (ICML), 2015.\n[13] A. Graves, \u201cPractical variational inference for neural networks,\u201d in Advances in Neural Information Processing Systems (NIPS), 2011, pp. 2348\u20132356.\n[14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[15] H. Larochelle Y. Bengio H. Vincent and P.A. Manzagol, \u201cExtracting and composing robust features with denoising autoencoders,\u201d in Int. Conference on Machine Learning (ICML), 2008, pp. 1096\u20131103.\n[16] G.E. Hinton and R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[17] N. Dehak, P. A. Torres-Carrasquillo, D. Reynold, and R. Dehak, \u201cLanguage recognition via Ivectors and dimensionality reduction,\u201d in Interspeech, 2011.\n[18] \u201cNIST language recognition challenge rules,\u201d http://www.nist.gov/itl/iad/mig/ upload/lre_ivectorchallenge_rel_v2.pdf, 2015.\n[19] Y. Grandvalet and Y. Bengio, \u201cSemi-supervised learning by entropy minimization,\u201d in Advances in Neural Information Processing Systems (NIPS), 2005."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 8, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electron. Lett., pp. 1569\u20131580, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural network bottleneck features for language identification", "author": ["P. Matejka", "L. Zhang", "T. Ng", "H.S. Mallidi", "O. Glembek", "J. Ma", "B. Zhang"], "venue": "IEEE Odyssey, 2014, pp. 299\u2013304.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "D. Martinez O. Plchot", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "ICASSP, 2014, pp. 5374\u20135378.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural network approaches to speaker and language recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 10, pp. 1671\u2013 1675, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Semisupervised learning with ladder network", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "arXiv:1507.02672, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "From neural PCA to deep unsupervised learning", "author": ["H. Valpola"], "venue": "arXiv:1411.7783, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deconstructing the ladder network architecture", "author": ["M. Pezeshki", "L. Fan", "P. Brakel", "A. Courville", "Y. Bengio"], "venue": "arXiv:1511.06430, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Int. Conference on Machine Learning (ICML), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2011, pp. 2348\u20132356.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1929}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["H. Larochelle Y. Bengio H. Vincent", "P.A. Manzagol"], "venue": "Int. Conference on Machine Learning (ICML), 2008, pp. 1096\u20131103.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Language recognition via Ivectors and dimensionality reduction", "author": ["N. Dehak", "P.A. Torres-Carrasquillo", "D. Reynold", "R. Dehak"], "venue": "Interspeech, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The impressive performance improvement obtained using deep neural networks (DNNs) for automatic speech recognition (ASR) [1] have motivated the application of DNNs to speaker and language recognition.", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "DNN have been trained for a different purpose to learn frame-level features that were then used to train a secondary classifier for the intended language recognition task [2][3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "DNN have been trained for a different purpose to learn frame-level features that were then used to train a secondary classifier for the intended language recognition task [2][3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "DNNs have also been applied to directly train language classification systems [4][5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "DNNs have also been applied to directly train language classification systems [4][5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "The standard pre-training strategy is based on a greedy layer-wise procedure using either Restricted Boltzmann Machines (RBM) [7] or noisy autoencoders [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "The standard pre-training strategy is based on a greedy layer-wise procedure using either Restricted Boltzmann Machines (RBM) [7] or noisy autoencoders [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "Our approach is based on the recently proposed Ladder Network training procedure that has proven to be very successful, especially in cases where the training dataset is composed of both labeled and unlabeled data [9] [10].", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "Our approach is based on the recently proposed Ladder Network training procedure that has proven to be very successful, especially in cases where the training dataset is composed of both labeled and unlabeled data [9] [10].", "startOffset": 218, "endOffset": 222}, {"referenceID": 7, "context": "A detailed description of Ladder Networks can be found in [9] [10].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "A detailed description of Ladder Networks can be found in [9] [10].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Figure 1: A conceptual illustration of the Ladder Network for L = 2 [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "A comparison of other possible choices for the combinator function is presented in [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "The concept of batch normalization was recently proposed by Ioffe and Szegedy [12] as a method to improve convergence as a result of reduced covariance shift.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "The noise is used here to regularize supervised learning in a way similar to the weight noise regularization method [13] and dropout [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "The noise is used here to regularize supervised learning in a way similar to the weight noise regularization method [13] and dropout [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "The cost of the standard denoising autoencoder is the input reconstruction error [15] [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "The cost of the standard denoising autoencoder is the input reconstruction error [15] [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "A recent study [11] conducted an extensive experimental investigation of variants of the Ladder Network in which individual components were removed or replaced to gain insights into their relative importance.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "Ladder Networks have been found to obtain state-of-the-art results on standard datasets with only a small portion of labeled examples [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 10, "context": "[12] [14]) applied on large labeled datasets does not rely on any unsupervised pretraining, semisupervised learning might instead be crucial for success when only a small amount of labeled data is available.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] [14]) applied on large labeled datasets does not rely on any unsupervised pretraining, semisupervised learning might instead be crucial for success when only a small amount of labeled data is available.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Each speech segment is represented by an i-vector of 400 components [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": ") We tried to use another standard unsupervised cost function that is based on minimizing the entropy of the distribution obtained by the soft-max layer, thus encouraging the decision in the unlabeled data to focus on one of the languages [19]:", "startOffset": 239, "endOffset": 243}, {"referenceID": 7, "context": "A direct skip of information from the encoder to the decoder was used only on the input layer using the Gaussian method described in [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 12, "context": "The baseline method is still based on a noisyforward step which, without the layer-reconstruction cost function, turned out to be a regularized learning method similar to dropout [14].", "startOffset": 179, "endOffset": 183}], "year": 2016, "abstractText": "In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset.", "creator": "LaTeX with hyperref package"}}}