{"id": "1604.01278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy", "abstract": "dialect extensions ultimately achieve amr smatch specification script are presented. the testing tool com - bines the smatch scoring script with the c6. 0 rule - reference dictionary to produce a human - readable report on the error patterns frequency present in the scored amr graphs. this first extension results in 4 % gain over the item - of - concern camr baseline analysis by adding to it a manually crafted construct fixing the lowest camr parser errors. the second extension combines a per - sentence smatch with an en - semble method for selecting the best amr graph among balanced set of amr norms for the same sentence. this phonetic modification au - tomatically yields further 0. 4 % gain when ap - plied upstream outputs of two nondeterministic amr commands : a camr + wrapper parser and a novel character - builder neural translation enhancement parser. for amr parsing task the character - level sequential translation attains surprising 7 % gain over the carefully optimized word - level output translation. overall, we achieve smatch f1 = 62 % on the semeval - 2016 official scor - ing set and f1 = 67 % on the ldc2015e86 computer module.", "histories": [["v1", "Tue, 5 Apr 2016 14:46:18 GMT  (610kb)", "http://arxiv.org/abs/1604.01278v1", "NAACL HLT 2016, SemEval-2016 Task 8 submission"]], "COMMENTS": "NAACL HLT 2016, SemEval-2016 Task 8 submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guntis barzdins", "didzis gosko"], "accepted": false, "id": "1604.01278"}, "pdf": {"name": "1604.01278.pdf", "metadata": {"source": "CRF", "title": "RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy", "authors": ["Guntis Barzdins", "Didzis Gosko"], "emails": ["guntis.barzdins@lu.lv,", "didzis.gosko@leta.lv"], "sections": [{"heading": null, "text": "RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy\nGuntis Barzdins, Didzis Gosko University of Latvia, IMCS and LETA Rainis Blvd. 29, Riga, LV-1459, Latvia\nguntis.barzdins@lu.lv, didzis.gosko@leta.lv"}, {"heading": "Abstract", "text": "Two extensions to the AMR smatch scoring script are presented. The first extension combines the smatch scoring script with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. This first extension results in 4% gain over the state-of-art CAMR baseline parser by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. The second extension combines a per-sentence smatch with an ensemble method for selecting the best AMR graph among the set of AMR graphs for the same sentence. This second modification automatically yields further 0.4% gain when applied to outputs of two nondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level neural translation AMR parser. For AMR parsing task the character-level neural translation attains surprising 7% gain over the carefully optimized word-level neural translation. Overall, we achieve smatch F1=62% on the SemEval-2016 official scoring set and F1=67% on the LDC2015E86 test set.\n1 \u00a0 Introduction\nAbstract Meaning Representation (AMR) (Banarescu et al., 2013) initially was envisioned as an intermediate representation for semantic machine translation, but has found applications in other NLP fields such as information extraction.\n1 Available at http://c60.ailab.lv\nFor SemEval-2016 Task 8 on Meaning Representation Parsing we took a dual approach: besides developing our own neural AMR parser, we also extended the AMR smatch scoring tool (Cai and Knight, 2013) with a rule-based C6.0 classifier1 to guide development of an accuracy-increasing wrapper for the state-of-art AMR parser CAMR (Wang et al., 2015a; 2015b). A minor gain was also achieved by combining these two approaches in an ensemble.\nThe paper starts with the description of our smatch extensions, followed by the description of our AMR parser and wrapper, and concludes with the results section evaluating the contributions of described techniques to our final SemEval result.\n2 \u00a0 Smatch Extensions\nWe describe two extensions2 to the original AMR smatch scoring script. These extensions do not change the smatch algorithm or scores produced, but they extract additional statistical information helpful for improving results of any AMR parser, as will be illustrated in Section 3.\n2.1 \u00a0 Visual Smatch with C6.0 Classifier Rules The original AMR smatch scoring metric produces as output only three numbers: precision, recall and F1. When developing an AMR parser, these three numbers alone do not reveal the actual mistakes in the AMR parser output (we call it silver AMR) when compared to the human-annotated gold AMR.\n2 Available at https://github.com/didzis/smatchTools\nThe first step in alleviating this problem is visualizing the mappings produced by the smatch algorithm as part of the scoring process. Figure 1 shows such smatch alignment visualization where gold and silver AMR graphs are first split into the edges, which are further aligned through variable mapping. The smatch metric measures success of such alignment \u2013 perfect alignment results in F1 score 100% while incomplete alignment produces lower scores.\nThe visualization in Figure 1 is good for manual inspection of incomplete AMR alignments in individual sentences. But it still is only marginally helpful for AMR parser debugging, because the datadriven parsers are expected to make occasional mistakes due to the training data incompleteness rather than due to a bug in the parser.\nTelling apart the repetitive parser bugs from the occasional training data incompleteness induced errors is not easy and to invoke the required statistical mechanisms we resorted to a rule-based C6.0 classifier (Barzdins et al., 2014; 2015), a modification of the legacy C4.5 classifier (Quinlan, 1993). The\nclassifier is asked to find most common patterns (rules) leading to some AMR graph edges to appear mostly in the gold, silver, or matched class after the smatch alignment. The bottom part of Figure 1 illustrates few such rules found by C6.0. For example, the second rule relates to the visualized sentence and should be read as \u201cif the instance has type mountainous, then it appears 1 time in the gold graphs and 0 times in the silver graphs of the entire document\u201d. Similarly the third rule should be read as \u201cword Foreign appears 13 times as :op1 of name in the gold graphs, but only 1 time in the silver graphs of the entire document\u201d \u2013 such 13 to 1 ratio likely points to some capitalization error in the parser pipeline. The generated rules can be sorted by their statistical impact score calculated as Laplace ratio (p+1)/(p+n+2) from the number of correct p and wrong n predictions made by this rule.\nClassifier generated rules were the key instrument we used to create a bug-fixing wrapper for the CAMR parser, described in Section 3.1. We fixed only bugs triggering error-indicating-rules with the\nimpact scores above 0.8, because Laplace ratio strongly correlates with the smatch score impact of the particular error.\n2.2 \u00a0 Smatch Extension for Ensemble Voting The original smatch algorithm is designed to compare only two AMR documents. Meanwhile CAMR parser is slightly non-deterministic in the sense that it produces different AMRs for the same test sentence, if trained repeatedly. Randomly choosing one of the generated AMRs is a suboptimal strategy. A better strategy is to use an ensemble voting inspired approach: among all AMRs generated for the given test sentence, choose the AMR which achieves the highest average pairwise smatch score with all the other AMRs generated for the same test sentence. Intuitively it means that among the non-deterministic options we choose the \u201cprevalent\u201d AMR.\nMultiple AMRs for the same test sentence can be generated also from different AMR parsers with substantially different average smatch accuracy. In this case all AMRs still can participate in the scoring, but weights need to be assigned to ensure that only AMRs from the high-accuracy parser may win.\n3 \u00a0 AMR Parsers\nWe applied the smatch extensions described in the previous Section to two very different AMR parsers.\n3.1 \u00a0 CAMR Parser with Wrapper We applied the debugging techniques from Section 2.1 to the best available open-source AMR parser CAMR3. The identified bug-fixes were almost entirely implemented as a CAMR parser wrapper4, that runs extra pre-processing (normalization) step on input data and extra post-processing step on output data. Only minor modifications to CAMR code itself were made5 to improve the performance on multi-core systems and to fix date normalization problems.\nOur CAMR wrapper tries to normalize the input data to the format recognized well by CAMR and to fix some systematic discrepancies of annotation style between the actual CAMR output and the expected gold AMRs. The overall gain from our wrapper is about 4%.\n3 https://github.com/Juicechuan/AMRParsing 4 https://github.com/didzis/CAMR/tree/wrapper\nThe following normalization actions are taken during pre-processing step, together accounting for about 2% gain:\n1. number normalization from a lexical (e.g. \u201cseventy-eight\u201d), semi-lexical (e.g. \u201c5 million\u201d) or multi-token digital (e.g. \u201c100,000\u201d or \u201c100 000\u201d) format to a single token digital format (e.g. \u201c100000\u201d);\n2. currency normalization from a number (any format mentioned in previous step) together with a currency symbol (e.g. \u201c$ 100\u201d) to a single token digital number with the lexical currency name (e.g. \u201c100 dollars\u201d);\n3. date normalization from any number and lexical mix to an explicit eight-digit dash separated format \u201cyyyy-mm-dd\u201d.\nSmall modifications had to be made to the baseline JAMR (Flanigan et al., 2014) aligner used by CAMR to reliably recognize the \u201cyyyy-mm-dd\u201d date format and to correctly align the date tokens to the graph entries (by default JAMR uses \u201cyymmdd\u201d date format that is ambiguous regarding century and furthermore can be misinterpreted as a number).\nThe rules for date normalization were extracted from the training set semi-automatically using C6.0 classifier by mapping date-entities in the gold AMR graphs and corresponding fragments in input sentences.\nAdditionally, all wiki edges were removed from the AMR graphs prior to training, because CAMR does not handle them well; this step ensures that CAMR is trained without wiki edges and therefore will not insert any wiki entries in the generated AMR. Instead, we insert wiki links deterministically during the post-processing step.\nDuring post-processing step the following modifications are applied to the CAMR parser generated AMR graphs, together accounting for about 2% gain:\n1. nationalities are normalized (e.g. \u201cItalian\u201d to \u201cItaly\u201d);\n2. some redundant graph leafs not carrying any semantic value are removed (e.g. \u201cnull-edge\u201d);\n3. wiki links are inserted deterministically next to \u201cname\u201d edges using gazetteer extracted from the training data and extended with the complete list of countries and nationalities (wiki value is selected\n5 Modified CAMR at https://github.com/didzis/CAMR\nbased on the parent concept and content of the \u201cname\u201d instance);\nAbout 1% additional gain comes from the observation that CAMR parser suffers from overfitting: it achieves optimal results when trained for only 2 iterations and with empty validation set.\n3.2 \u00a0 Neural AMR Parser For neural translation based AMR parsing we used simplified AMRs without wiki links and variables. Prior to deleting variables, AMR graphs were converted into trees by duplicating the co-referring nodes. Such AMR simplification turned out to be nearly loss-less, as a simple co-reference resolving script restores the variables with average F1=0.98 smatch accuracy.\nWe trained a modified TensorFlow seq2seq neural translation model6 with attention (Abadi et al., 2015; Sutskerev et al., 2014; Bahdanau et al., 2015) to \u201ctranslate\u201d plain English sentences into simplified AMRs. For the test sentence in Figure 1 it gives following parsing result:\n(mountain-01 :ARG1 (country :name (name :op1 \"Georgia\"))\nApart from a missing bracket this is a valid (although slightly incorrect) simplified AMR. Note that this translation has been learned in \u201cclosed task\u201d and \u201cend-to-end\u201d manner only from the provided AMR training set without any external knowledge source. This explains overall lower accuracy of the neural AMR parser compared to CAMR, which uses external knowledge from wide coverage parsing models of BLLIP7 parser (Charniak and Johnson, 2005). The neural AMR parser accuracy is close to CAMR accuracy for short sentences up to 100 characters, but degrades considerably for longer sentences.\nWe optimized TensorFlow seq2seq model hyperparameters within the constraints of the available GPU memory: 1 layer or 400 neurons, single bucket of size 480, each input and output character tokenized as a single \u201cword\u201d, vocabulary size 120 (number of distinct characters), batch size 4, trained for 30 epochs (4 days on TitanX GPU).\nOperating seq2seq model on the character-level (Karpathy, 2015; Chung et al., 2016; Luong et al., 2016) rather than standard word-level improved smatch F1 by notable 7%. Follow-up tests (Barzdins 6 https://github.com/didzis/tensorflowAMR\net al., 2016) revealed that character-level translation with attention improves results only if the output is a syntactic variation of the input (as is the case for AMR parsing), but for e.g. English-Latvian translation gives inferior results due to attention mechanism failing to establish character-level mappings between the English and Latvian words.\n4 \u00a0 Results\nTable 1 shows smatch scores for various combinations of parsers and thus quantifies the contributions of all methods described in this paper. We improved upon CAMR rather than JAMR parser due to better baseline performance of CAMR, likely due to its reliance on the wide coverage BLLIP parser.\nThe CAMR parser wrapper (result of Sections 2.1 and 3.1) is the largest contributor to our results. The weighed ensemble of 3 runs of CAMR+wrapper and 1 run of neural AMR parser (Sections 2.2 and 3.2) gave an additional boost to the results. Including the neural AMR parser in the ensemble doubled the gain \u2013 apparently it acted as an efficient tiebreaker between the similar CAMR+wrapper outputs.\n5 \u00a0 Conclusions\nAlthough our results are based on CAMR parser, the described debugging and ensemble approaches are likely applicable also to other AMR parsers."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Latvian National research program SOPHIS under grant agreement Nr.10-4/VPP-4/11 and in part by H2020 SUMMA project under grant agreement 688139/H2020-ICT-2015.\n7 https://github.com/BLLIP/bllip-parser"}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["nanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "Google Research whitepaper. Software available from tensorflow.org", "citeRegEx": "Vi\u00e9gas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vi\u00e9gas et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR\u20192015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Abstract Meaning Representation for Sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": "In Proceedings of the 7th Linguistic An-", "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Using C5.0 and Exhaustive Search for Boosting Frame-Semantic Parsing Accuracy", "author": ["Guntis Barzdins", "Didzis Gosko", "Laura Rituma", "Peteris Paikens"], "venue": "Proceedings of the 9th Language Resources and Evaluation Conference (LREC-2014),", "citeRegEx": "Barzdins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barzdins et al\\.", "year": 2014}, {"title": "Riga: from FrameNet to Semantic Frames with C6.0 Rules", "author": ["Guntis Barzdins", "Peteris Paikens", "Didzis Gosko"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Barzdins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barzdins et al\\.", "year": 2015}, {"title": "Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project", "author": ["Guntis Barzdins", "Steve Renals", "Didzis Gosko."], "venue": "In: Proceedings of the 10th Language Resources and Evaluation Conference (LREC-2016), (to appear).", "citeRegEx": "Barzdins et al\\.,? 2016", "shortCiteRegEx": "Barzdins et al\\.", "year": 2016}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748\u2013752. Association for Computational Lin-", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "Coarse- tofine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the 43rd Annual Meet- ing on Association for Computational Linguistics, ACL \u201905,", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603:06147", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol-", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "The unreasonable effectiveness of recurrent neural networks", "author": ["Andrej Karpathy"], "venue": "Blog post,", "citeRegEx": "Karpathy.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy.", "year": 2015}, {"title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "arXiv preprint arXiv:1604.00788", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "C4.5: Programs for Machine Learning", "author": ["Ross J. Quinlan"], "venue": null, "citeRegEx": "Quinlan.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan.", "year": 1993}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366\u2013", "citeRegEx": "Wang et al\\.,? 2015a", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "citeRegEx": "Wang et al\\.,? 2015b", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) initially was envisioned as an intermediate representation for semantic machine translation, but has found applications in other NLP fields such as information extraction.", "startOffset": 38, "endOffset": 62}, {"referenceID": 6, "context": "lv For SemEval-2016 Task 8 on Meaning Representation Parsing we took a dual approach: besides developing our own neural AMR parser, we also extended the AMR smatch scoring tool (Cai and Knight, 2013) with a rule-based C6.", "startOffset": 177, "endOffset": 199}, {"referenceID": 14, "context": "0 classifier to guide development of an accuracy-increasing wrapper for the state-of-art AMR parser CAMR (Wang et al., 2015a; 2015b).", "startOffset": 105, "endOffset": 132}, {"referenceID": 3, "context": "0 classifier (Barzdins et al., 2014; 2015), a modification of the legacy C4.", "startOffset": 13, "endOffset": 42}, {"referenceID": 12, "context": "5 classifier (Quinlan, 1993).", "startOffset": 13, "endOffset": 28}, {"referenceID": 9, "context": "Small modifications had to be made to the baseline JAMR (Flanigan et al., 2014) aligner used by CAMR to reliably recognize the \u201cyyyy-mm-dd\u201d date format and to correctly align the date tokens to the graph entries (by default JAMR uses \u201cyymmdd\u201d date format that is ambiguous regarding century and furthermore can be misinterpreted as a number).", "startOffset": 56, "endOffset": 79}, {"referenceID": 1, "context": "We trained a modified TensorFlow seq2seq neural translation model with attention (Abadi et al., 2015; Sutskerev et al., 2014; Bahdanau et al., 2015) to \u201ctranslate\u201d plain English sentences into simplified AMRs.", "startOffset": 81, "endOffset": 148}, {"referenceID": 7, "context": "This explains overall lower accuracy of the neural AMR parser compared to CAMR, which uses external knowledge from wide coverage parsing models of BLLIP parser (Charniak and Johnson, 2005).", "startOffset": 160, "endOffset": 188}, {"referenceID": 10, "context": "Operating seq2seq model on the character-level (Karpathy, 2015; Chung et al., 2016; Luong et al., 2016) rather than standard word-level improved smatch F1 by notable 7%.", "startOffset": 47, "endOffset": 103}, {"referenceID": 8, "context": "Operating seq2seq model on the character-level (Karpathy, 2015; Chung et al., 2016; Luong et al., 2016) rather than standard word-level improved smatch F1 by notable 7%.", "startOffset": 47, "endOffset": 103}], "year": 2016, "abstractText": "Two extensions to the AMR smatch scoring script are presented. The first extension combines the smatch scoring script with the C6.0 rule-based classifier to produce a human-readable report on the error patterns frequency observed in the scored AMR graphs. This first extension results in 4% gain over the state-of-art CAMR baseline parser by adding to it a manually crafted wrapper fixing the identified CAMR parser errors. The second extension combines a per-sentence smatch with an ensemble method for selecting the best AMR graph among the set of AMR graphs for the same sentence. This second modification automatically yields further 0.4% gain when applied to outputs of two nondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level neural translation AMR parser. For AMR parsing task the character-level neural translation attains surprising 7% gain over the carefully optimized word-level neural translation. Overall, we achieve smatch F1=62% on the SemEval-2016 official scoring set and F1=67% on the LDC2015E86 test set.", "creator": "Word"}}}