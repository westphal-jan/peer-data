{"id": "1604.03390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Video Description using Bidirectional Recurrent Neural Networks", "abstract": "although traditionally reserved in the machine translation field, traditional encoder - decoder framework has been recently applied for the generation of graphical and image descriptions. the combination of convolutional and recurrent neural networks in these models often proven to outperform the previous state of the art, obtaining more recognizable video descriptions. in this work we propose pushing further this model by introducing innovative contributions into the encoding stage. first, producing richer image representations between supplying object and location information from convolutional neural networks and second, introducing adaptive synthetic neural networks for capturing both dorsal and backward temporal relationships in the initial frames.", "histories": [["v1", "Tue, 12 Apr 2016 13:09:01 GMT  (1322kb,D)", "http://arxiv.org/abs/1604.03390v1", "8 pages, 3 figures, 1 table, Submitted to International Conference on Artificial Neural Networks (ICANN)"], ["v2", "Mon, 12 Dec 2016 12:28:07 GMT  (1320kb,D)", "http://arxiv.org/abs/1604.03390v2", "8 pages, 3 figures, 1 table, Submitted to International Conference on Artificial Neural Networks (ICANN)"]], "COMMENTS": "8 pages, 3 figures, 1 table, Submitted to International Conference on Artificial Neural Networks (ICANN)", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["\\'alvaro peris", "marc bola\\~nos", "petia radeva", "francisco casacuberta"], "accepted": false, "id": "1604.03390"}, "pdf": {"name": "1604.03390.pdf", "metadata": {"source": "CRF", "title": "Video Description using Bidirectional Recurrent Neural Networks", "authors": ["\u00c1lvaro Peris", "Marc Bola\u00f1os", "Petia Radeva", "Francisco Casacuberta"], "emails": ["lvapeab@prhlt.upv.es", "fcn@prhlt.upv.es", "marc.bolanos@ub.edu", "petia.ivanova@ub.edu"], "sections": [{"heading": null, "text": "Keywords: Video Description, Neural Machine Translation, Birectional Recurrent Neural Networks, LSTM, Convolutional Neural Networks."}, {"heading": "1 Introduction", "text": "Automatic generation of image descriptions is a recent trend in Computer Vision that represents an interesting, but difficult task. This has been possible due to the dramatic advances in Convolutional Neural Network (CNN) models that allowed to outperform the state-of-the-art algorithms in many computer vision problems: object recognition, object detection, activity recognition, etc. Generating descriptions of videos represents an even more challenging task that could lead to multiple applications (e.g. video indexing and retrieval, movie description for multimedia applications or for blind people or human-robot interaction). We can imagine the amount of data generated in YouTube (every day people watch hundreds of millions of hours of video and generate billions of views) and how video description would help to categorize them, provide an efficient retrieval mechanism and serve as a summarization tool for blind people.\nHowever, the problem of video description generation has several properties that make it specially difficult. Besides the significant amount of image information to analyze, videos may have a variable number of images and can be described with sentences of different length. Furthermore, the descriptions\nar X\niv :1\n60 4.\n03 39\n0v 1\n[ cs\n.C V\n] 1\n2 A\npr 2\n01 6\nof videos use to be high-level summaries that not necessarily are expressed in terms of the objects, actions and scenes observed in the images. There are many open research questions in this field requiring deep video understanding. Some of them are how to efficiently extract important elements from the images (e.g. objects, scenes, actions), to define the local (e.g. fine-grained motion) and global spatio-temporal information, determine the salient content worth to describe, and generate the final video description. All these specific questions need the attention of computer vision, machine translation and natural language understanding communities in order to be solved.\nIn this work, we propose to enrich the state-of-the-art architecture using bidirectional neural networks for modeling relationships in two temporal directions. Furthermore, we test the inclusion of supplementary features, which help to detect contextual information from the scene where the video takes place."}, {"heading": "2 Related Work", "text": "Although the problem of video captioning recently appeared thanks to the new learning capabilities offered by Deep Learning techniques, the general pipeline adopted in these works resembles the traditional encoder-decoder methodology used in Machine Translation (MT). The main difference is that, in the encoder step, instead of generating a compact representation of the source language sentence, we generate a representation of the images belonging to the video.\nMT aims to automatically translate text or speech from a source to a target language. Within the last decades, the prevailing approach is the statistical one [5]. The application of connectionist models in the area has drawn much the attention of researchers in the last years. Moreover, a new approach to MT has been recently proposed: the so-called Neural Machine Translation, where the translation process is carried out by a means of a large Recurrent Neural Network (RNN) [11]. These systems rely on the encoder-decoder framework: an encoder RNN produces a compact representation of an input sentence in the source language, and the decoder RNN takes this representation and generates the corresponding target language sentence. Both RNNs usually make use of gated units, such as the popular Long Short-term Memory (LSTM) [4], in order to cope with long-term relationships.\nThe recent reintroduction of Deep Learning in the Computer Vision field through CNNs [6], has allowed to obtain new and richer image representations compared to the traditional hand-crafted ones. These networks have demonstrated to be a powerful tool to extract feature representations for several kinds of computer vision problems like on objects [10] or scenes [18] recognition. Thanks to the CNNs ability to serve as knowledge transfer mechanisms, they have also been usually used as feature extractors.\nThe majority of the works devoted to generate textual descriptions from single images also follow the encoder-decoder architecture. In the encoding stage, they apply a combination of CNN and LSTM for describing the input image. In the decoding stage, an LSTM is in charge of receiving the image information\nand generating, word by word, a final description of the image [15]. The problem of video captioning is similar. Seminal works applied methodologies inspired by classical MT [9]. Nevertheless, more recent works following the encoder-decoder approach, obtained state-of-the-art performances [14,16].\nWe present a new methodology for natural language video description that makes use of deeper structures and a double-way analysis of the input video. We propose to use as a base architecture the one introduced in [16]. On the top of it, our contributions are twofold. First, we produce richer image representations by combining complementary CNNs for detecting objects and contextual information from the input images. Second, we introduce a Bidirectional LSTM (BLSTM) network in the encoding stage, which has the ability to learn forward and backward long-term relationships on the input sequence."}, {"heading": "3 Methodology", "text": "An overview of our proposal is depicted in Fig. 1. We propose an encoder-decoder approach consisting of four stages, using both CNNs and LSTMs for describing images and for modeling their temporal relationship, respectively.\nFirst (blue in the scheme), we apply two state of the art CNN models for extracting complementary features on each of the raw images from the video.\nSecond (red in the scheme), considering we need to describe the actions performed in consecutive frames, we apply a BLSTM for capturing temporal relationships and complementary information by taking a look at the action in a forward and in a backward manner.\nThird (yellow in the scheme), the two output vectors from forward and backward LSTM models of the previous step are concatenated together with the CNN output for each image and are fed to a soft attention model in the decoder. This model decides on which parts of the input video should focus for emitting the next word, considering the description generated so far.\nFourth (green in the scheme), an LSTM network generates the video caption from the representation obtained in previous stages. The variable-length caption is obtained word by word, using a softmax function on the top of this LSTM network.\n3.1 Encoder\nGiven the video description problem, in the encoding stage we need to properly characterize the video for 1) understanding which kind of objects and structures appear in the images, and 2) modeling their relationships and actions along time.\nFor tackling the first part of the problem, several kinds of pretrained CNNs may be used for describing the images, which can be distinguished by the different architectures or by the different datasets used for training. Although an extended comparison and combinations of models could be used for applying this characterization, we propose combining object and context-related information. For this purpose we use a GoogleNet architecture [12] separately trained on two datasets, one for objects (ILSVRC dataset [10]), and the other for scenes (Places 205 [18]).\nThe combination of these two kinds of data can inform about the objects appearing and their surroundings, being ideal for the problem at hand. For a given video, the CNNs generate a sequence Vc of J d-dimensional feature vectors, x1, . . . ,xJ with xj \u2208 Rd for 1 \u2264 j \u2264 J , where J is the number of frames in the video.\nTo solve the second problem, a BLSTM processes the sequence Vc, generating a new sequence Vbi = v1, . . . ,vJ of J vectors. BLSTM networks are composed of two independent LSTM layers namely, forward and backward. Both layers are analogue, but the latter processes the input sequence reversed in time.\nLSTM networks have, in addition to the classical hidden state, a memory state. Let vfj be the forward layer hidden state at the time-step j, and let c f j be its memory state. The hidden state vfj is computed as c f j controlled by an output gate ofj . The current memory state depends on an updated memory state, and on the previous memory state, cfj\u22121, respectively modulated by the forget and input gates, f fj and i f j . The updated memory state c\u0303 f j is obtained by applying a logistic non-linear function to the input and the previous hidden state. Each LSTM gate has associated two weight matrices, accounting for the input and the previous hidden state. Such matrices must be estimated on a training set. Figure 2 shows an illustration of an LSTM unit. The same architecture applies to the backward layer, but dependencies flow from the next time-step to the\nprevious one. Since forward and backward layers are independent, they have different weight matrices to estimate.\nEach feature vector vj computed by the BLSTM results as the concatenation of the forward and backward hidden states: vj = [v f j ;v b j ] \u2208 R2\u00b7D for 1 \u2264 j \u2264 J , being D the size of each forward and backward hidden state. Finally, the encoder combines the sequences Vc and Vbi by concatenating the vectors from the CNN and from the BLSTM, producing a final sequence V of J feature vectors w1, . . . ,wJ , wj = [xj ;vj ] \u2208 Rd+2\u00b7D for 1 \u2264 j \u2264 J .\n3.2 Decoder\nThe decoder is an LSTM network, which acts as a language model, conditioned by the information provided by the encoder. This network is equipped with an attention mechanism [1,16]: a soft alignment model, implemented as a single-layered perceptron, that helps the decoder to know where to look at for generating each output word. Given the sequence V generated by the encoder, at each decoding time-step t the attention mechanism weights the J feature vectors and combines them into a single context vector zt \u2208 Rd+2\u00b7D.\nThe decoder LSTM is defined similarly to the forward layer from the encoder, but it takes into account the previously generated word and the context vector from the attention\nmechanism, in addition to its previous hidden state. The last word representation is provided by a word embedding matrix E \u2208 Rm\u00d7V , being m the size of the word embedding and V the size of the vocabulary. E is estimated together with the rest of the model parameters.\nA probability distribution over the vocabulary of output words is defined from the hidden state ht, by means of a softmax function. This function represents the conditional probability of a word given an input video V and its history (the previously generated words): p(yt|y1, . . . , yt\u22121,V). Following [11], a beam-search method is used to find the caption with highest conditional probability."}, {"heading": "4 Results", "text": "In this section we describe the datasets and metrics used for evaluating and comparing our model to the video captioning state of the art."}, {"heading": "4.1 Dataset", "text": "The Microsoft Research Video Description Corpus (MSVD) [2] is a dataset composed of 1970 open domain clips collected from YouTube and annotated using a crowd sourcing platform. Each video has a variable number of captions, written by different users. We used the splits made by [14,16], separating the dataset in 1200 videos for training, 100 for validation and the remaining 670 for testing. During training, the clips and each of their captions were treated separately, accounting for a total of more than 80, 000 training samples."}, {"heading": "4.2 Evaluation Metrics", "text": "In order to evaluate and compare the results of the different models we used the standardized COCO-Caption evaluation package [3], which provides several metrics for text description comparison. We used three main metrics, all of them presented from 0 (minimum quality) to 100 (maximum quality):\nBLEU [8]: this metric compares the ratio of n-gram structures that are shared between the system hypotheses and the reference sentences.\nMETEOR [7]: it was introduced to solve the lack of the recall component when computing BLEU. It computes the F1 score of precision and recall between hypotheses and references.\nCIDEr [13]: similarly to BLEU, it computes the number of matching ngrams, but penalizes any n-gram frequently found in the whole training set."}, {"heading": "4.3 Experimental Results", "text": "On all the tests we used a batch size of 64, the learning rate was automatically set by the Adadelta [17] method and, as the authors in [16] reported, we applied a frame subsampling, picking only one image every 26 frames for reducing the computational load. The parameters of the network were randomly initialized. An evaluation on the validation set was performed every 1000 updates. The learning process was stopped when the reported error increased after 5 evaluations.\nFor each configuration we run 10 experiments. At each of them, we randomly set the value of the critical model hyperparameters. Such hyperparameters and their tested ranges are m \u2208 [300, 700], |ht| \u2208 [1000, 3000]. When using the BLSTM encoder, we performed an additional selection on |vj | \u2208 [100, 2100].\nFor each configuration, the best model with respect to the BLEU measure on the validation set was selected. In Table 1 we report the results of the best models on the test set. The first row correspond to the result obtained with our system with the object features from [16]. The configurations reported below the horizontal line are our proposals, where Scenes indicates we use scene-related features concatenated to Objects and BLSTM denotes the use of the additional BLSTM encoder."}, {"heading": "5 Discussion and Conclusions", "text": "Analyzing the obtained results, a clear improvement trend can be derived when applying the BLSTM as a temporal inference mechanism. The BLSTM addition when using Objects features allows to improve the result on all metrics, obtaining a benefit of more than 2 BLEU points. Adding scenes-related features also slightly improves the result, although it is not as remarkable as the BLSTM improvement. The combination of Objects+Scenes+BLSTM offers the best CIDEr performance, nevertheless, this result is slightly below the Objects+BLSTM one on the other metrics. This behaviour is probably due to the significant increase on the number of parameters to learn. It should be investigated whether the reduction of the number of parameters by reducing the size of the CNN features, or the use of larger datasets could lead to further improvements.\nIn conclusion, we have presented a new methodology for natural language video description that takes profit from a bidirectional analysis of the input sequence. This architecture has the ability to learn forward and backward longterm relationships on the input images. Additionally, the use of complementary object and scene-related image features has proved to obtain a richer video representation. The improvements have allowed the method to outperform the state-of-the-art results in the problem at hand.\nThese results suggest that deep structures help to transfer the knowledge from the input sequence of frames to the output natural language caption. Hence, the next step to take must delve into the application deeper modeling structures, such as 3D CNNs and multi-layered LSTM networks.\nAcknowledgments. This work was partially founded by TIN2012-38187-C0301, SGR 1219, PrometeoII/2014/030 and by a travel grant by the MIPRCV network. P. Radeva is partially supported by an ICREA Academia2014 grant. We acknowledge NVIDIA for the donation of a GPU used in this work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (arXiv:1409.0473),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "The meteor metric for automatic evaluation of machine translation", "author": ["Alon Lavie", "Michael J Denkowski"], "venue": "Machine translation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Within the last decades, the prevailing approach is the statistical one [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "Moreover, a new approach to MT has been recently proposed: the so-called Neural Machine Translation, where the translation process is carried out by a means of a large Recurrent Neural Network (RNN) [11].", "startOffset": 199, "endOffset": 203}, {"referenceID": 3, "context": "Both RNNs usually make use of gated units, such as the popular Long Short-term Memory (LSTM) [4], in order to cope with long-term relationships.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "The recent reintroduction of Deep Learning in the Computer Vision field through CNNs [6], has allowed to obtain new and richer image representations compared to the traditional hand-crafted ones.", "startOffset": 85, "endOffset": 88}, {"referenceID": 16, "context": "These networks have demonstrated to be a powerful tool to extract feature representations for several kinds of computer vision problems like on objects [10] or scenes [18] recognition.", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "and generating, word by word, a final description of the image [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Seminal works applied methodologies inspired by classical MT [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "Nevertheless, more recent works following the encoder-decoder approach, obtained state-of-the-art performances [14,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 14, "context": "Nevertheless, more recent works following the encoder-decoder approach, obtained state-of-the-art performances [14,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 14, "context": "We propose to use as a base architecture the one introduced in [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "For this purpose we use a GoogleNet architecture [12] separately trained on two datasets, one for objects (ILSVRC dataset [10]), and the other for scenes (Places 205 [18]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "For this purpose we use a GoogleNet architecture [12] separately trained on two datasets, one for objects (ILSVRC dataset [10]), and the other for scenes (Places 205 [18]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "This network is equipped with an attention mechanism [1,16]: a soft alignment model, implemented as a single-layered perceptron, that helps the decoder to know where to look at for generating each output word.", "startOffset": 53, "endOffset": 59}, {"referenceID": 14, "context": "This network is equipped with an attention mechanism [1,16]: a soft alignment model, implemented as a single-layered perceptron, that helps the decoder to know where to look at for generating each output word.", "startOffset": 53, "endOffset": 59}, {"referenceID": 9, "context": "Following [11], a beam-search method is used to find the caption with highest conditional probability.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "The Microsoft Research Video Description Corpus (MSVD) [2] is a dataset composed of 1970 open domain clips collected from YouTube and annotated using a crowd sourcing platform.", "startOffset": 55, "endOffset": 58}, {"referenceID": 12, "context": "We used the splits made by [14,16], separating the dataset in 1200 videos for training, 100 for validation and the remaining 670 for testing.", "startOffset": 27, "endOffset": 34}, {"referenceID": 14, "context": "We used the splits made by [14,16], separating the dataset in 1200 videos for training, 100 for validation and the remaining 670 for testing.", "startOffset": 27, "endOffset": 34}, {"referenceID": 2, "context": "In order to evaluate and compare the results of the different models we used the standardized COCO-Caption evaluation package [3], which provides several metrics for text description comparison.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "BLEU [8]: this metric compares the ratio of n-gram structures that are shared between the system hypotheses and the reference sentences.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "METEOR [7]: it was introduced to solve the lack of the recall component when computing BLEU.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "CIDEr [13]: similarly to BLEU, it computes the number of matching ngrams, but penalizes any n-gram frequently found in the whole training set.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "On all the tests we used a batch size of 64, the learning rate was automatically set by the Adadelta [17] method and, as the authors in [16] reported, we applied a frame subsampling, picking only one image every 26 frames for reducing the computational load.", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "On all the tests we used a batch size of 64, the learning rate was automatically set by the Adadelta [17] method and, as the authors in [16] reported, we applied a frame subsampling, picking only one image every 26 frames for reducing the computational load.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "*Model from [16] only with Object features evaluated on our system.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "The first row correspond to the result obtained with our system with the object features from [16].", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.", "creator": "LaTeX with hyperref package"}}}