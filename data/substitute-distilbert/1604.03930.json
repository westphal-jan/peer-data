{"id": "1604.03930", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "abstract": "this paper considers the problem of canonical - component analysis ( cca ) ( mitchell, 1936 ) and, more broadly, the generalized eigenvector problem and distinct pair of symmetric matrices. these identified two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics ( shi & malik, 2000 ; sheng et al., 2004 ; zhao et al., 2009 ).", "histories": [["v1", "Wed, 13 Apr 2016 19:57:46 GMT  (317kb)", "https://arxiv.org/abs/1604.03930v1", null], ["v2", "Fri, 27 May 2016 18:03:11 GMT  (316kb)", "http://arxiv.org/abs/1604.03930v2", "International Conference on Machine Learning (ICML) 2016"]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["rong ge 0001", "chi jin", "sham m kakade", "praneeth netrapalli", "aaron sidford"], "accepted": true, "id": "1604.03930"}, "pdf": {"name": "1604.03930.pdf", "metadata": {"source": "CRF", "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis", "authors": ["Rong Ge", "Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli", "Aaron Sidford"], "emails": ["rongge@cs.duke.edu", "chijin@cs.berkeley.edu", "sham@cs.washington.edu", "praneeth@microsoft.com", "asid@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n03 93\n0v 2\n[ cs\n.L G\n] 2\n7 M\nWe provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved.\nWe obtain our results by reducing CCA to the top-k generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of O ( zk \u221a \u03ba\n\u03c1 log(1/\u01eb) log (k\u03ba/\u03c1)\n) where z is the total number of nonzero\nentries, \u03ba is the condition number and \u03c1 is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components k up to a log(k) factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets."}, {"heading": "1 Introduction", "text": "Canonical-correlation analysis (CCA) and the generalized eigenvector problem are fundamental problems in scientific computing, data analysis, and statistics (Barnett and Preisendorfer, 1987; Friman et al., 2001).\nThese problems arise naturally in statistical settings. Let X,Y \u2208 Rn\u00d7d denote two large sets of data points, with empirical covariance matrices Sx = 1 nX \u22a4X, Sy = 1 nY \u22a4Y, and Sxy = 1 nX\n\u22a4Y and suppose we wish to find features x,y \u2208 Rd that best encapsulate the similarity or dissimilarity\n\u2217Duke University. Email: rongge@cs.duke.edu \u2020UC Berkeley. Email: chijin@cs.berkeley.edu \u2021University of Washington. Email: sham@cs.washington.edu \u00a7Microsoft Research New England. Email: praneeth@microsoft.com \u00b6Microsoft Research New England. Email: asid@microsoft.com\nof the data sets. CCA is the problem of maximizing the empirical correlation\nmax x\u22a4Sxxx=1 and y\u22a4Syyy=1\nx\u22a4Sxyy (1)\nand thereby extracts common features of the data sets. On the other hand the generalized eigenvalue problems\nmax x 6=0 x\u22a4Sxxx x\u22a4Syyx and max y 6=0 y\u22a4Syyy y\u22a4Sxxy\ncompute features that maximizes discrepancies between the data sets. Both these problems are easily extended to the k-feature case (See Section 3). Algorithms for solving them are commonly used to extract features to compare and contrast large data sets and are used commonly in regression (Kakade and Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al., 2011) and more.\nDespite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms. Both problems can be reduced to performing principle component analysis (PCA), albeit on complicated matrices e.g S \u22121/2 yy S \u22a4 xyS \u22121 xxSxyS \u22121/2 yy for CCA and S \u22121/2 yy SxxS \u22121/2 yy for generalized eigenvector. However applying PCA to these matrices traditionally involves the formation of S \u22121/2 xx and S \u22121/2 yy which is prohibitive for sufficiently large datasets if we only want to estimate top-k eigenspace.\nA natural open question in this area is to what degree can the formation of S \u22121/2 xx and S \u22121/2 yy can be bypassed to obtain efficient scalable algorithms in the case where the number of features k is much smaller than the dimensions of the problem n and d. Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.\nThe central goal of this paper is to answer this question in the affirmative. We present simple globally linearly convergent iterative methods that solve these problems. The running time of these problems scale well as the number of features and conditioning of the problem stay fixed and the size of the datasets grow. Moreover, we implement the method and perform experiments demonstrating that the techniques may be effective for large scale problems.\nSpecializing our results to the single feature case we show how to solve the problems all in time\nO(z \u221a \u03ba\n\u03c1 log 1 \u03c1 log 1 \u01eb ), where \u03ba is the maximum of condition numbers of Sxx and Syy and \u03c1 is the\neigengap of appropriate matrices and mentioned above, and z is the number of nonzero entries in X and Y. To the best of our knowledge this is the first such globally linear convergent algorithm for solving these problems.\nWe achieve our results through a general and versatile framework that allows us to utilize fast linear system solvers in various regimes. We hope that by initiating this theoretical and practical analysis of CCA and the generalized eigenvector problem we can promote further research on the problem and ultimately advance the state-of-the-art for efficient data analysis."}, {"heading": "1.1 Our Approach", "text": "To solve the problems motivated in the previous section we first directly reduce CCA to a generalized eigenvector problem (See Section 5). Consequently, for the majority of the paper we focus on the following:\nDefinition 1 (Top-k Generalized Eigenvector1). Given symmetric matrices A,B where B is positive definite compute w1, \u00b7 \u00b7 \u00b7 ,wk defined for all i \u2208 [k] by\nwi \u2208 argmax w\n\u2223\u2223\u2223w\u22a4Aw \u2223\u2223\u2223 s.t. w \u22a4Bw = 1 and w\u22a4Bwj = 0 \u2200 j \u2208 [i\u2212 1].\nThe generalized eigenvector is equivalent to the problem of computing the PCA of A in the B norm. Consequently, it is the same as computing the top k eigenvectors of largest absolute value of the symmetric matrix M = B\u22121/2AB\u22121/2 and then multiplying by B\u22121/2.\nUnfortunately, as we have discussed, explicitly computing B\u22121/2 is prohibitively expensive when n is large and therefore we wish to avoid forming M explicitly. One natural approach is to develop an iterative methods to approximately apply B\u22121/2 to a vector and then use that method as a subroutine to perform the power method on M. Even if we could perform the error analysis to make this work, such an approach would likely require at least a suboptimal \u2126(log2(1/\u01eb)) iterations to achieve error \u01eb.\nTo bypass these difficulties, we take a closer look at the power method. For some initial vector x, let y = B\u22121/2Mix be the result of i iterations of power method on M followed by multiplying B\u22121/2. Clearly y = (B\u22121A)iB\u22121/2x. Furthermore, since we typically initialize the power method by a random vector and since B is positive definite, if we instead we computed y = (B\u22121A)ix for random x we would likely converge at the same rate as the power method at the cost of just a slightly worse initialization quality.\nConsequently, we can compute our desired eigenvectors by simply alternating between applying A and B\u22121 to a random initial vector. Unfortunately, computing B\u22121 exactly is again outside our computational budget. At best we should only attempt to apply B\u22121 approximately by linear system solvers.\nOne of our main technical contributions is to argue about the effect of inexact solvers in this method. Whereas solving every linear system to target accuracy \u01eb would again require O(log(1/\u01eb)) time per linear system, which leads to a sub-optimal O(log2(1/\u01eb)) overall running time, i.e. sublinear convergence, we instead show how to warm start the linear system solvers and obtain a faster rate. We exploit the fact that as we perform many iterations of power methods, points at time t converge to eigenvectors and therefore we can initialize our linear system solver at time t carefully using our points at time t\u2212 1. Ultimately we show that we only need to make fixed multiplicative progress in solving the linear system in every iteration of the power method, thus the runtime for solving each linear system is independent of \u01eb.\nPutting these pieces together with careful error analysis yields our main result. Our algorithm only requires the ability to apply A to a vector and an approximate linear system solver for B, which in turn can be obtained by just applying B to vectors. Consequently, our framework is versatile, scalable, and easily adaptable to take advantage of faster linear system solvers.\n1We use the term generalized eigenvector to refer to a non-zero vector v such that Av = \u03bbBv for symmetric A and B, not the general notion of eigenvectors for asymmetric matrices."}, {"heading": "1.2 Previous Work", "text": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004). Our analysis in this paper draws on this work extensively and our results should be viewed as the principled application of them to the generalized eigenvector problem.\nThere has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015). To our knowledge, there are no provable guarantees for approximate methods for this problem. Heuristic-based approachs (Witten et al., 2009; Lu and Foster, 2014) compute efficiently, but only give suboptimal result due to coarse approximation. The work in (Ma et al., 2015) provides one natural iterative procedure, where the per iterate computational complexity is low. This work only provides local convergence guarantees and does not provide guarantees of global convergence.\nAlso of note is that many recent algorithms (Ma et al., 2015; Wang et al., 2015) have mini-batch variations, but there\u2019s no guarantees for mini-batch style algorithm for CCA yet. Our algorithm can also be easily extends to a mini-batch version. While we do not explicitly analyze this variation, and we believe our analysis and techniques are helpful for extensions to this setting. We also view this as an important direction for future work.\nWe hope that by establishing the generalized eigenvector problem and providing provable guarantees under moderate regularity assumptions that our results may be further improved and ultimately this may advance the state-of-the-art in practical algorithms for performing data analysis."}, {"heading": "1.3 Our Results", "text": "Our main result in this paper is a linearly convergent algorithm for computing the top generalized eigenvectors (see Definition 1). In order to be able to state our results we introduce some notation. Let \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbd be the eigenvalues of B\u22121A (their existence is guaranteed by Lemma 9 in the appendix). The eigengap \u03c1\ndef = 1\u2212 |\u03bbk+1||\u03bbk| and \u03b3 def = |\u03bb1||\u03bbk| . Let z denote the number of nonzero entries\nin A and B.\nTheorem 2 (Informal version of Theorem 6). Given two matrices A and B \u2208 Rd\u00d7d, there is an algorithm that computes the top-k generalized eigenvectors up to an error \u01eb in time O\u0303( zk \u221a \u03ba(B)\n\u03c1 log 1 \u01eb ),\nwhere \u03ba (B) is the condition number of B and O\u0303 (\u00b7) hides logarithmic terms in d, \u03b3, \u03ba (B) and \u03c1, and nothing else.\nHere is a comparison of our result with previous work.\nTurning to the problem of CCA, cf. (1), the relevant parameters are \u03ba def = max (\u03ba (Sxx) , \u03ba (Syy)) i.e., the maximum of the condition numbers of Sxx and Syy, \u03b3 def = |\u03bb1||\u03bbk | where \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbk are the eigenvalues of S\u22121yy SyxS \u22121 xxSxy in decreasing absolute value. Let z denote the number of nonzeros in X and Y. Our main results are a reduction from CCA to the generalized eigenvector problem.\nTheorem 3 (Informal version of Theorem 7). Given two data matrices X \u2208 Rd1\u00d7n and Y \u2208 Rd2\u00d7n, there is an algorithm that performs top-k CCA up to an error \u01eb in time O\u0303(zk \u221a \u03ba\n\u03c1 log 1 \u01eb ), where\nd = d1 + d2 and O\u0303 (\u00b7) hides logarithmic terms in d, \u03b3, \u03ba and \u03c1, and nothing else.\nTable 2 compares our result with existing results. 2\nWe should note that the actual bounds we obtain are somewhat stronger than the above informal bounds. Some of the terms in logarithm also appear only as additive terms. Finally we also give natural stochastic extensions of our algorithms where the cost of each iteration may be much smaller than the input size. The key idea behind our approach is to use an approximate linear system solver as a black box inside power method on an appropriate matrix. We show that this dependence on a linear system solver is in some sense essential. In Section 6 we show that the generalized eigenvector problem is strictly more general than the problem of solving positive semidefinite linear systems and consequently our dependence on the condition number of B is in some cases optimal.\nSubsequent to the submission of this paper, we learned of the closely related work in (Wang et al., 2016), which presents a number of additional interesting results. We think it is worthwhile to point out that our algorithm only requires black box access to any linear solver. Although the result in Theorem 3 was stated by instantiating the linear system solver by accelerated gradient descent\n2(Ma et al., 2015) only shows local convergence for S-AppGrad. Starting within this radius of convergence requires us to already solve the problem to a high accuracy.\n3This table was inspired by (Wang et al., 2016) in order to facilitate comparison to existing work.\n(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al., 2015; Lin et al., 2015). We summarize the corresponding runtime in Table 3. There \u03ba\u0303 def = max\n( maxi \u2016xi\u20162 \u03c3min(Sxx) , maxi \u2016yi\u2016 2 \u03c3min(Syy) ) ,\nand xi, yi are i-th column of matrix X and Y. Note by definition we always have \u03ba\u0303 \u2265 \u03ba. For generalized eigenvector problem, results of similar flavor as in Table 3 can also be easily derived.\nFinally, we also run experiments to demonstrate the practical effectiveness of our algorithm on both small and large scale datasets."}, {"heading": "1.4 Paper Overview", "text": "In Section 2, we present our notation. In Section 3, we formally define the problems we solve and their relevant parameters. In Section 4, we present our results for the generalized eigenvector problem. In Section 5, we present our results for the CCA problem. In Section 6 we argue that generalized eigenvector computation is as hard as linear system solving and that our dependence on \u03ba(B) is near optimal. In Section 7, we present experimental results of our algorithms on some real world data sets. Due to space limitations, proofs are deferred to the appendix."}, {"heading": "2 Notation", "text": "We use bold capital letters (A,B, \u00b7 \u00b7 \u00b7 ) to denote matrices and bold lowercase letters (u,v, \u00b7 \u00b7 \u00b7 ) for vectors. For symmetric positive semidefinite (PSD) matrix B, we let \u2016u\u2016B def= \u221a u\u22a4Bu denote the B-norm of u and we let \u3008u,v\u3009B def= u\u22a4Bv denotes the inner product of u and v in the B-norm. We say that a matrix W is B-orthonormal if W\u22a4BW = I. We let \u03c3i(A) denotes the ith largest singular value of A, \u03c3min (A) and \u03c3max (A) denote the smallest and largest singular values of A respectively. Similarly we let \u03bbi(A) refers to the i\nth largest eigenvalue of A in magnitude. We let nnz (A) denotes the number of nonzeros in A. We also let \u03ba(B) denote the condition number of B (i.e., the ratio of the largest to smallest eigenvalue)."}, {"heading": "3 Problem Statement", "text": "In this section, we recall the generalized eigenvalue problem, define our error metric, and introduce all relevant parameters. Recall that the generalized eigenvalue problem is to find k vectors wi, i \u2208 [k] such that\nwi \u2208 argmax w\n\u2223\u2223\u2223w\u22a4Aw \u2223\u2223\u2223 s.t. w \u22a4Bw = 1 and w\u22a4Bwj = 0 \u2200 j \u2208 [i\u2212 1].\nUsing stationarity conditions, it can be shown that the vectors wi are given by wi = vi, where vi is an eigenvector of B\n\u22121A with eigenvalue \u03bbi such that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn. Our goal is to recover the top-k eigen space i.e., span{v1, \u00b7 \u00b7 \u00b7 ,vk}. In order to quantify the error in estimating the eigenspace, we use largest principal angle, which is a standard notion of distance between subspaces (Golub and Van Loan, 2012).\nDefinition 4 (Largest principal angle). Let W and V be two k dimensional subspaces, W and V their B-orthonormal basis respectively. The largest principal angle \u03b8 (W,V) in the B-norm is defined to be\n\u03b8 (W,V) def= arccos ( \u03c3min ( V\u22a4BW )) ,\nIntuitively, the largest principal angle corresponds to the largest angle between any vector in the span of W and its projection onto the span of V. In the special case where k = 1, the above definition reduces to our choice in the top-1 setting. Given two matrices W and V, we use \u03b8 (W,V) to denote the largest principle angle between the subspaces spanned by the columns of W and V. We say that W achieves an error of \u01eb if W\u22a4BW = I and sin \u03b8 (W,V) \u2264 \u01eb, where V is the d \u00d7 k matrix whose columns are v1, \u00b7 \u00b7 \u00b7 ,vk. The relevant parameters for us are the eigengap, i.e. the relative difference between kth and (k + 1)th eigenvalues, \u03c1\ndef = 1 \u2212 |\u03bbk+1||\u03bbk| , and \u03ba(B), the condition\nnumber of B."}, {"heading": "4 Our Results", "text": "In this section, we provide our algorithms and results for solving the generalized eigenvector problem. We present our results for the special case of computing the top generalized eigenvector (Section 4.1) followed by the general case of computing the top-k generalized eigenvectors (Section 4.2). However, first we formally define a linear system solver as follows:\nLinear system solver: In each of our main results (Theorems 5 and 6) we assume black box access to an approximate linear system solver. Given a PSD matrix B, a vector b, an initial estimate u0, and an error parameter \u03b4, we require to decrease the error by a multiplicative \u03b4, i.e. output u1 with \u2016u1 \u2212B\u22121b\u20162B \u2264 \u03b4\u2016u0 \u2212B\u22121b\u20162B. We let T (\u03b4) denote the time needed for this operation. Since the error metric \u2016u1 \u2212B\u22121b\u20162B is equivalent to function error on minimizing the convex quadratic f(u)\ndef = 12u \u22a4Bu\u2212u\u22a4b up to constant scaling, an approximate linear system solver is equivalent to an optimization algorithm for f(u). We also specialize our results using Nesterov\u2019s accelerated gradient descent to state our bounds. Stating our results using linear system solver as a blackbox allows the user to choose an efficient solver depending on the structure of B and helps pass any improvements in linear system solvers on to the problem of generalized eigenvectors."}, {"heading": "4.1 Top-1 Setting", "text": "Our algorithm for computing the top generalized eigenvector, GenELin is given in Algorithm 1. The algorithm implements an approximate power method where each iteration consists of approximately multiplying a vector by B\u22121A. In order to do this, GenELin solves a linear system in B and then scales the resulting vector to have unit B-norm. Our main result states that given an oracle for solving the linear systems,4 the number of iterations taken by Algorithm 1 to compute the top eigenvector up to an accuracy of \u01eb is at most 4\u03c1 log 1 \u01eb cos \u03b80 where \u03b80 def = \u03b8 (w0,v1).\nTheorem 5. Recall that the linear system solver takes time T (\u03b4) to reduce the error by a factor \u03b4. Given matrices A and B, GenELin (Algorithm 1) computes a vector wT achieving an error of \u01eb in\n4For example, we could use Nesterov\u2019s accelerated gradient descent, Algorithm 4\nAlgorithm 1 Generalized Eigenvector via Linear System Solver (GenELin)\nInput: T , symmetric matrix A, PSD matrix B. Output: top generalized eigenvector w.\nw\u03030 \u2190 sample uniformly from unit sphere in Rd w0 \u2190 w\u03030/\u2016w\u03030\u2016B for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do\n\u03b2t \u2190 w\u22a4t Awt/w\u22a4t Bwt w\u0303t+1 \u2190 argminw\u2208Rd [12w\u22a4Bw \u2212w\u22a4Awt]\n{Use an optimization subroutine with initialization \u03b2twt }\nwt+1 \u2190 w\u0303t+1/\u2016w\u0303t+1\u2016B end for Return wT .\nT = 2\u03c1 log 1 \u01eb cos \u03b80 iterations, where \u03b80 def = \u03b8 (w0,v1). The running time of the algorithm is at most\nO\n( 1\n\u03c1\n( log 1 cos \u03b80 \u00b7 T ( \u03c12 cos2 \u03b80 16 ) + log 1 \u01eb \u00b7 T ( \u03c12 16 )) + 1 \u03c1 (nnz (A) + nnz (B) + d) log\n1\n\u01eb cos \u03b80\n) .\nFurthermore, if we use Nesterov\u2019s accelerated gradient descent (Algorithm 4) to solve the linear systems in Algorithm 1, the time can be bounded as\nO\n( nnz (B) \u221a \u03ba(B)\n\u03c1\n( log 1\ncos \u03b80 log\n1\n\u03c1 cos \u03b80 + log\n1 \u01eb log 1 \u03c1\n) + 1\n\u03c1 nnz (A) log\n1\n\u01eb cos \u03b80\n) .\nRemarks:\n\u2022 Since GenELin chooses w0 randomly, Lemma 13 tells us that cos \u03b80 \u2265 \u03b6\u221a d\u03ba(B) with probability\ngreater than 1\u2212 \u03b6.\n\u2022 Note that GenELin exploits the sparsity of input matrices since we only need to apply them as operators.\n\u2022 Depending on computational restrictions, we can also use a subset of samples in each iteration of GenELin. In some large scale learning applications using minibatches of data in each iteration helps make the method scalable while still maintaining the quality of performance."}, {"heading": "4.2 Top-k Setting", "text": "In this section, we give an extension of our algorithm and result for computing the top-k generalized eigenvectors. Our algorithm, GenELinK is formally given as Algorithm 2.\nGenELinK is a natural generalization of GenELin from the previous section. Given an initial set of vectors W0, the algorithm proceeds by doing approximate orthogonal iteration. Each iteration involves solving k independent linear systems5 and orthonormalizing the iterates. The following theorem is the main result of our paper which gives runtime bounds for Algorithm 2. As before, we\n5Similarly, as before, we could use Nesterov\u2019s accelerated gradient descent, i.e. Algorithm 4.\nAlgorithm 2 Generalized Eigenvectors via Linear System Solvers-K (GenELinK).\nInput: T , k, symmetric matrix A, PSD matrix B. a subroutine GSB(\u00b7) that performs Gram-Schmidt process, with inner product \u3008\u00b7, \u00b7\u3009B. Output: top k eigen-space W \u2208 Rd\u00d7k. W\u03030 \u2190 random d\u00d7 k matrix with each entry i.i.d from N (0, 1) W0 \u2190 GSB(W\u03030). for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do\n\u0393t \u2190 (W\u22a4t BWt)\u22121(W\u22a4t AWt) W\u0303t+1 \u2190 argminW tr(12W\u22a4BW \u2212W\u22a4AWt)\n{Use an optimization subroutine with initialization Wt\u0393t }\nWt+1 \u2190 GSB(W\u0303t+1) end for Return WT .\nassume access to a blackbox linear system solver and also give a result instantiating the theorem with Nesterov\u2019s accelerated gradient descent algorithm.\nTheorem 6. Suppose the linear system solver takes time T (\u03b4) to reduce the error by a factor \u03b4. Given input matrices A and B, GenELinK computes a d \u00d7 k matrix WT which is an estimate of the top generalized eigenvectors V with an error of \u01eb i.e., W\u22a4TBWT = I and sin \u03b8T \u2264 \u01eb, where \u03b8T def = \u03b8 (WT ,V) in T = 2 \u03c1 log 1 \u01eb cos \u03b80\niterations where \u03b80 = \u03b8 (W0,V). The run time of this algorithm is at most\nO\n( 1\n\u03c1\n( log 1 cos \u03b80 \u00b7 T ( \u03c12 cos4 \u03b80 64k\u03b32 ) + T ( \u03c12 64k\u03b32 ) log 1 \u01eb ) + 1 \u03c1 ( nnz (A) k + nnz (B) k + dk2 ) log\n1\n\u01eb cos \u03b80\n) ,\nwhere \u03b3 def = |\u03bb1||\u03bbk| , |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbk| being the top-k eigenvalues of B \u22121A. Furthermore, if we use Nesterov\u2019s accelerated gradient descent (Algorithm 4) to solve the linear systems in Algorithm 2, the time above can be bounded as\nO\n( nnz (B) k \u221a \u03ba(B)\n\u03c1\n( log 1\ncos \u03b80 log\nk\u03b3\n\u03c1 cos \u03b80 + log\n1 \u01eb log k\u03b3 \u03c1\n) + ( nnz (A) k + dk2 )\n\u03c1 log\n1\n\u01eb cos \u03b80\n) .\nRemarks:\n\u2022 Lemma 13 again tells us that since W0 is chosen to be normalized after choosing uniformly at random from the unit sphere, cos \u03b80 \u2265 \u03b6\u221a\ndk\u03ba(B) with probability greater than 1\u2212 \u03b6.\n\u2022 This result recovers Theorem 5 as a special case, since when k = 1, we also have \u03b3 = |\u03bb1||\u03bb1| = 1."}, {"heading": "5 Application to CCA", "text": "We now outline how the CCA problem can be reduced to computing generalized eigenvectors. The CCA problem is as follows. Given two sets of data points X \u2208 Rn\u00d7d1 and Y \u2208 Rn\u00d7d2 , let\nSx def = X\u22a4X/n, Sy def = Y\u22a4Y/n, and Sxy def = X\u22a4Y/n. We wish to find vectors \u03c61, \u00b7 \u00b7 \u00b7 , \u03c6k and \u03c81, \u00b7 \u00b7 \u00b7 , \u03c8k which are defined recursively as\n(\u03c6i, \u03c8i) \u2208 argmax \u03c6,\u03c8 \u03c6\u22a4Sxy\u03c8\ns.t. \u2016\u03c6\u2016Sx = 1 and \u03c6\u22a4Sx\u03c6j = 0 \u2200 j \u2264 i\u2212 1 \u2016\u03c8\u2016Sy = 1 and \u03c8\u22a4Sy\u03c8j = 0 \u2200 j \u2264 i\u2212 1.\nwhere the values of \u03c6\u22a4i Sxy\u03c8i are called canonical correlations between X and Y. For reduction, we know any stationary point of this optimization problem satisfies Sxy\u03c8i = \u03bbiSx\u03c6i, and Syx\u03c6i = \u00b5iSy\u03c8i, where \u03bbi and \u00b5i are two constants. Combined with the con-\nstraints, we also see that \u03bbi = \u00b5i. This can be written in matrix form as\n( 0 Sxy\nSyx 0 )( \u03c6i \u03c8i ) =\n\u03bbi ( Sx 0 0 Sy )( \u03c6i \u03c8i ) . Suppose the generalized eigenvalues of the above matrices are \u2212\u03bb1 < \u2212\u03bb2 < \u00b7 \u00b7 \u00b7 < \u03bb2 < \u03bb1. The top 2k-dimensional eigen-space of this generalized eigenvalue problem corresponds to the linear subspace spanned by the eigenvectors of \u03bbi and\u2212\u03bbi, which are\n( \u03c6i \u03c8i ) , ( \u2212\u03c6i \u03c8i ) \u2200 i \u2208\n[k]. Once we solve the top-2k generalized eigenvector problem for the matrices\n( 0 Sxy\nSyx 0\n) and\n( Sxx 0 0 Syy ) , we can pick any orthonormal basis that spans the output subspace and choose a random k-dimensional projection of those vectors. The formal algorithm is given in Algorithm 3. Combining this with our results for computing generalized eigenvectors, we obtain the following result.\nAlgorithm 3 CCA via Linear System Solvers (CCALin) Input: T , k, data matrix X \u2208 Rn\u00d7d1 ,Y \u2208 Rn\u00d7d2 Output: top k canonical subspace Wx \u2208 Rd1\u00d7k,Wy \u2208 Rd2\u00d7k.\nSxx \u2190 X\u22a4X/n, Syy \u2190 Y\u22a4Y/n, Sxy \u2190 X\u22a4Y/n. A \u2190 ( 0 Sxy\nS\u22a4xy 0\n) , B \u2190 ( Sxx 0 0 Syy )\n( W\u0304x \u2208 Rd1\u00d72k W\u0304y \u2208 Rd2\u00d72k ) \u2190 GenELinK(A,B). U \u2190 2k \u00d7 k random Gaussian matrix W\u0303x \u2190 W\u0304xU. W\u0303y \u2190 W\u0304yU. Wx = GSSxx(W\u0303x),Wx = GSSyy(W\u0303y) Return Wx,Wy.\nTheorem 7. Suppose the linear system solver takes time T (\u03b4) to reduce the error by a factor \u03b4. Given inputs X and Y, with probability greater than 1 \u2212 \u03b6, then there is some universal constant c, so that Algorithm 3 outputs Wx and Wy such that sin \u03b8(span (\u03c6i; i \u2208 [k]) ,Wx) \u2264 \u01eb, and\nsin \u03b8(span (\u03c8i; i \u2208 [k]) ,Wy) \u2264 \u01eb, in time\nO\n( 1\n\u03c1\n( log d\u03ba \u03b6 \u00b7 T ( c\u03b66\u03c12 d2k5\u03ba2\u03b32 ) + T ( c\u03b62\u03c12 k3\u03b32 ) log 1 \u01eb ) + 1 \u03c1 ( nnz (X,Y) k + dk2 ) log d\u03ba \u03b6\u01eb ) ,\nwhere nnz (X,Y) def = nnz (X) + nnz (Y) and \u03ba def = max (\u03ba (Sxx) , \u03ba (Syy)) and \u03b3 def = \u03bb1\u03bbk . If we use Nesterov\u2019s accelerated gradient descent (Algorithm 4) to solve the linear systems in GenELink, then the total runtime is\nO\n( nnz (X,Y) k \u221a \u03ba\n\u03c1\n( log d\u03ba\n\u03b6 log\nd\u03ba\u03b3\n\u03b6\u03c1 + log\n1 \u01eb log k\u03b3 \u03c1\n) + dk2\n\u03c1 log\nd\u03ba\n\u03b6\u01eb\n) ,\nRemarks:\n\u2022 Note that we depend on the maximum of the condition numbers of Sxx and Syy since the linear systems that arise in GenELinK decompose into two separate linear systems, one in Sxx and the other in Syy.\n\u2022 We can also exploit sparsity in the data matrices X and Y since we only need to apply Sxx,Sxy or Syy only as operators, which can be done by applying X and Y in appropriate order. Exploiting sparsity is crucial for any large scale algorithm since there are many data sets (e.g., URL dataset in our experiments) where dense operations are impractical."}, {"heading": "6 Reduction to Linear System", "text": "Here we show that solving linear systems inB is inherent in solving the top-k generalized eigenvector problem in the worst case and we provide evidence a \u221a \u03ba(B) factor in the running time is essential for a broad class of iterative methods for the problem. Let M be a symmetric positive definite matrix and suppose we wish to solve the linear system Mx = m, i.e. compute x\u2217 with Mx\u2217 = m. If we set A = mm\u22a4 and B = M then\nargmax x\u22a4Bx=1\nx\u22a4Ax = B\u22121m\nm\u22a4B\u22121m\nand consequently computing the top-1 generalized eigenvector yields the solution to the linear system. Therefore, the problem of computing top-k generalized eigenvectors is in general harder than the problem of solving symmetric positive definite linear systems.\nMoreover, it is well known that any method which starts at m and iteratively applies M to linear combinations of the points computed so far must apply M at least \u2126( \u221a \u03ba(B)) in order to halve the error in the standard norm for the problem (Shewchuk, 1994). Consequently, methods that solve the top-1 generalized eigenvector problem by simply applying A and B, which is the same as applying M and taking linear combinations with m, must apply M at least \u2126( \u221a \u03ba(M)) times to achieve small error, unless they exploit more structure of M or the initialization."}, {"heading": "7 Simulations", "text": "In this section, we present our experiment results performing CCA on three benchmark datasets which are summarized in Table 4. We wish to demonstrate two things via these simulations: 1)\nthe behavior of CCALin verifies our theoretical result on relatively small-scale dataset, and 2) scalability of CCALin comparing it with other existing algorithms on a large-scale dataset.\nLet us now specify the error metrics we use in our experiments. The first ones are the principal angles between the estimated subspaces and the true ones. Let Wx and Wy be the estimated subspaces andVx, Vy be the true canonical subspaces. We will use principle angles \u03b8x = \u03b8(Wx,Vx)\nunder Sxx-norm, \u03b8y = \u03b8(Wx,Vx) under Syy-norm and \u03b8B = \u03b8 (( Vx 0 0 Vy ) , ( W\u0304x W\u0304y )) 7, under\nthe ( Sxx 0 0 Syy ) norm. Unfortunately, we cannot compute these error metrics for large-scale datasets since they require knowledge of the true canonical components. Instead we will use Total Correlations Captured (TCC), which is another metric widely used by practitioners, defined to be the sum of canonical correlation between two matrices. Also, Proportion of Correlations Captured is given as\nPCC = TCC(XWx,YWy)/TCC(XVx,YVy)\nFor a fair comparison with other algorithms (which usually call highly optimized matrix inversion subroutines), we use number of FLOPs instead of wall clock time to measure the performance."}, {"heading": "7.1 Small-scale Datasets", "text": "MNIST dataset(LeCun et al., 1998) consists of 60,000 handwritten digits from 0 to 9. Each digit is a image represented by 392 \u00d7 392 real values in [0,1]. Here CCA is performed between left half images and right half images. The data matrix is dense but the dimension is fairly small. Penn Tree Bank (PTB) dataset comes from full Wall Street Journal Part of Penn Tree Bank which consists of 1.17 million tokens and a vocabulary size of 43k(Marcus et al., 1993), which has already been used to successfully learn the word embedding by CCA(Dhillon et al., 2011). Here, the task is to learn correlated components between two consecutive words. We only use the top 10,000 most frequent words. Each row of data matrix X is an indicator vector and hence it is very sparse and X\u22a4X is diagonal.\nSince the input matrices are very ill conditioned, we add some regularization and replace Sxx by Sxx + \u03bbI (and similarly with Syy). In CCALin, we run GenELinK with k = 10 and accelerated gradient descent (Algorithm 4 in the supplementary material) to solve the linear systems. The results are presented in Figure 1 and Figure 2.\nFigure 1 shows a typical run of CCALin from random initialization on both MNIST and PTB dataset. We see although \u03b8x, \u03b8y may be even 90 degree at some point respectively, \u03b8B is always\n6Sparsity is given by (nnz (X) + nnz (Y))/(nd1 + nd2). 7See Algorithm 3 for definition of W\u0304x,W\u0304y\nmonotonically decreasing (as cos \u03b8B monotonically increasing) as predicted by our theory. In the end, as \u03b8B goes to zero, it will push both \u03b8x and \u03b8y go to zero, and PCC go to 1. This demonstrates that our algorithm indeed converges to the true canonical space.\nFurthermore, by a more detailed examination of experimental data in Figure 1, we observe in Figure 2 that sin \u03b8B is indeed linearly convergent as we predicted in the theory. In the meantime, sin \u03b8x and sin \u03b8y may initially converge a bit slower than sin \u03b8B, but in the end they will be upper bounded by sin \u03b8B times a constant factor, thus will eventually converge at a linear rate at least as fast as sin \u03b8B."}, {"heading": "7.2 Large-scale Dataset", "text": "URL Reputation dataset contains 2.4 million URLs and 3.2 million features including both host-based features and lexical based features. Each feature is either real valued or binary. For experiments in this section, we follow the setting of (Ma et al., 2015). We use the first 2 million samples, and run CCA between a subset of host based features and a subset of lexical based features to extract the top 20 components. Although the data matrix X is relatively sparse,\nunlike PTB, it has strong correlations among different coordinates, which makes X\u22a4X much denser (nnz ( X\u22a4X ) /d21 \u2248 10\u22123).\nClassical algorithms are impractical for this dataset on a typical computer, either running out of memory or requiring prohibitive amount of time. Since we cannot estimate the principal angles, we will evaluate TCC performance of CCALin.\nWe compare our algorithm to S-AppGrad (Ma et al., 2015) which is an iterative algorithm and PCA-CCA (Ma et al., 2015), NW-CCA (Witten et al., 2009) and DW-CCA (Lu and Foster, 2014) which are one-shot estimation procedures.\nIn CCALin, we employ GenELinK using stochastic accelerated gradient descent for solving linear systems using minibatches in each of the gradient steps and also leverage sparsity of the data to deal with the large data size. The result is shown in Figure 3. It is clear from the plot that our algorithm takes fewer computations than the other algorithms to achieve the same accuracy."}, {"heading": "8 Conclusion", "text": "In summary, we have provided the first provable globally linearly convergent algorithms for solving canonical correlation analysis and the generalized eigenvector problems. We have shown that for recovering the top k components our algorithms are much faster than traditional methods based on fast matrix multiplication and singular value decomposition when k \u226a n and the condition numbers and eigenvalue gaps of the matrices involved are moderate. Moreover, we have provided empirical evidence that our algorithms may be useful in practice. We hope these results serve as the basis for further improvements in performing large scale data analysis both in theory and in practice."}, {"heading": "A Solving Linear System via Accelerated Gradient Descent", "text": "Algorithm 4 Nesterov\u2019s accelerated gradient descent Input: learning rate \u03b7, factor Q, initial point x0, T . Output: minimizer x\u22c6 of f .\nfor t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do yt+1 \u2190 xt \u2212 (1/\u03b2) \u00b7 \u2207f(xt) xt+1 \u2190 yt+1 + ( \u221a Q\u2212 1)/(\u221aQ+ 1) \u00b7 (yt+1 \u2212 yt) end for Return yT .\nSince we use accelerated gradient descent in our main theorems, for completeness, we put the algorithm and cite its result about iteration complexity here without proof.\nTheorem 8 ((Nesterov, 1983)). Let f be \u03b1-strongly convex and \u03b2-smooth, then accelerated gradient descent with learning rate \u03b7 = 1\u03b2 and Q = \u03b2/\u03b1 satisfies:\nf(xt)\u2212 f(x\u22c6) \u2264 2(f(x0)\u2212 f(x\u22c6)) exp(\u2212 t\u221a Q ) (2)"}, {"heading": "B Proofs of Main Theorem", "text": "In this section we will prove Theorems 5, 6 and 7.\nB.1 Rank-1 Setting\nWe first prove our claim that B\u22121A has an eigenbasis.\nLemma 9. Let (ui, \u03c3i) be the eigenpairs of the symmetric matrix B \u22121/2AB\u22121/2. Then B\u22121/2ui is an eigenvector of B\u22121A with eigenvalue \u03c3i.\nProof. The proof is straightforward.\nB\u22121A ( B\u22121/2ui ) = B\u22121/2 ( B\u22121/2AB\u22121/2ui ) = \u03c3iB \u22121/2ui.\nDenote the eigenpairs of B\u22121A by (\u03bbi,vi), the above lemma further tells us that v\u22a4i Bvj = u\u22a4i uj = \u03b4ij .\nRecall that we defined the angle between w and v1 in the B-norm: \u03b8 (w,v1) = arccos ( |v\u22a41 Bw| ) .\nTo measure the distance from optimality, we use the following potential function for normalized vector w (\u2016w\u2016B = 1):\ntan \u03b8(w,v1) =\n\u221a 1\u2212 |v\u22a41 Bw|2\n|v\u22a41 Bw| . (3)\nLemma 10. Consider any w such that \u2016w\u2016B = 1 and tan \u03b8(w,v1) \u2264 \u01eb. Then, we have:\ncos2 \u03b8(w,v1) = (v \u22a4 1 Bw) 2 \u2265 1\u2212 \u01eb2 and w\u22a4Aw \u2265 \u03bb1(1\u2212 \u01eb2).\nProof. Clearly,\n(v\u22a41 Bw) 2 = cos2 \u03b8(w,v1) =\n1 1 + tan2 \u03b8(w,v1) \u2265 1 1 + \u01eb2 \u2265 1\u2212 \u01eb2,\nproving the first part. For the second part, we have the following:\nw\u22a4Aw = \u2211\ni,j\n(v\u22a4i Bw)(v \u22a4 j Bw)v \u22a4 i Avj =\n\u2211\ni,j\n\u03bbj(v \u22a4 i Bw)(v \u22a4 j Bw)v \u22a4 i Bvj\n= \u2211\ni\n\u03bbi(v \u22a4 i Bw) 2 \u2265 \u03bb1(v\u22a41 Bw)2 \u2265 (1\u2212 \u01eb2)\u03bb1,\nproving the lemma.\nProof of Theorem 5. We will show that the potential function tan \u03b8(wt,v1) decreases geometrically with t. This will directly provides an upper bound for sin \u03b8(wt,v1). For simplicity, through out the proof we will simply denote \u03b8(wt,vi) as \u03b8t.\nRecall the updates in Algorithm 1, suppose at time t, we have wt such that \u2016wt\u2016B = 1. Let us say\nwt+1 = 1 Z (B\u22121Awt + \u03be) (4)\nwhere Z is some normalization factor, and \u03be is the error in solving the least squares. We will first prove the geometric convergence claim assuming\n\u2016\u03be\u2016B \u2264 |\u03bb1| \u2212 |\u03bb2|\n4 min{cos \u03b8t, sin \u03b8t}, (5)\nand then bound the time taken by black-box linear system solver to provide such an accuracy. Since wt can be written as wt = \u2211 i ( w\u22a4t Bvi ) vi, we know B \u22121Awt = \u2211d i=1 \u03bbi ( w\u22a4t Bvi ) vi. Since \u2016wt+1\u2016B = 1 and v\u22a4i Bvj = \u03b4ij , we have\ntan \u03b8t+1 =\n\u221a Z2 \u2212 |v\u22a41 BZwt+1|2\n|v\u22a41 BZwt+1| \u2264\n\u221a\u2211d i=2 ( w\u22a4t Bvi )2 \u03bb2i + \u2016\u03be\u2016B\n| ( w\u22a4t Bv1 ) \u03bb1| \u2212 \u2016\u03be\u2016B\n\u2264\n\u221a 1\u2212 ( w\u22a4t Bv1 )2\n|w\u22a4t Bv1| \u00d7\n|\u03bb2|+ \u2016\u03be\u2016B\u221a 1\u2212(w\u22a4t Bv1) 2\n|\u03bb1| \u2212 \u2016\u03be\u2016B|w\u22a4t Bv1| = tan \u03b8t \u00d7\n|\u03bb2|+ \u2016\u03be\u2016B\u221a 1\u2212(w\u22a4t Bv1) 2\n|\u03bb1| \u2212 \u2016\u03be\u2016B|w\u22a4t Bv1|\nBy definition of \u03b8t, we know cos \u03b8t = |w\u22a4t Bv1| and sin \u03b8t = \u221a 1\u2212 ( w\u22a4t Bv1 )2 giving us\ntan \u03b8t+1 \u2264 tan \u03b8t \u00d7 |\u03bb2|+ \u2016\u03be\u2016Bsin \u03b8t |\u03bb1| \u2212 \u2016\u03be\u2016Bcos \u03b8t .\nSince \u2016\u03be\u2016B \u2264 |\u03bb1|\u2212|\u03bb2|4 min{cos \u03b8t, sin \u03b8t}, we have that\ntan \u03b8t+1 \u2264 |\u03bb1|+ 3|\u03bb2| 3|\u03bb1|+ |\u03bb2| \u00d7 tan \u03b8t.\nLetting \u03b3 = 3|\u03bb1|+|\u03bb2||\u03bb1|+3|\u03bb2| , this shows that G(wt) \u2264 \u03b3 tG(w0). Recalling the definition of eigengap \u03c1 = 1\u2212 |\u03bb2||\u03bb1| , choosing t to be\nt \u2265 2 \u03c1 log\n( 1\n\u01eb cos \u03b80\n) \u2265\nlog ( tan \u03b80\n\u01eb\n)\n( 1 \u03b3 \u2212 1\n) \u2265 log ( tan \u03b80 \u01eb )\nlog (\n1 \u03b3\n) , (6)\nwe are guaranteed that sin \u03b8t \u2264 tan \u03b8t \u2264 \u01eb. This number of iterations 2\u03c1 log ( 1 \u01eb cos \u03b80 ) could be further decompose into two phase: 1) initial phase 2\u03c1 log 1 cos \u03b80 which mainly caused by large initial angle, 2) convergence phase 2\u03c1 log 1 \u01eb which is mainly due to the high accuracy \u01eb we need.\nWe now focus on how to obtain the iterate wt+1 using accelerated gradient descent such that the error \u03be has norm bounded as in (5).\nLet f(w) def = 12w \u22a4Bw \u2212w\u22a4Awt and recall that in each iteration, we use linear system solver to solve the following optimization problem:\nmin w f(w). (7)\nThe minimizer of (7) is B\u22121Awt. Define \u01ebinit and \u01ebdes as initial error and required destination error of linear system solver \u2016w \u2212B\u22121Awt\u20162B. Observe that for any w we have equality,\n\u2016w \u2212B\u22121Awt\u20162B = 2(f(w)\u2212 f(B\u22121Awt)) (8)\nEq.(5) directly poses a condition on \u01ebdes:\n\u01ebdes \u2264 (|\u03bb1| \u2212 |\u03bb2|)2\n16 min{cos2 \u03b8t, sin2 \u03b8t}\nSince we initialize Algorithm 4 with \u03b2twt, where \u03b2t def = w\u22a4t Awt\nw\u22a4t Bwt , the initial error can be bounded\nas follows:\n\u01ebinit = 2(f(\u03b2twt)\u2212 f(B\u22121Awt)) = 2(min\n\u03b2 f(\u03b2wt)\u2212 f(B\u22121Awt)) \u2264 2(f(\u03bb1wt)\u2212 f(B\u22121Awt))\n= \u2016\u03bb1wt \u2212B\u22121Awt\u20162B = \u2211\ni\u22652 (\u03bb1 \u2212 \u03bbi)2\n( w\u22a4t Bvi )2 \u2264 \u03bb21(1\u2212 ( w\u22a4t Bv1 )2 ) = \u03bb21 sin 2 \u03b8t.\nThis means that we wish to decrease the ratio of final to initial error smaller than\n\u01ebdes \u01ebinit \u2264 (|\u03bb1| \u2212 |\u03bb2|) 2 16 min{cos2 \u03b8t, sin2 \u03b8t} \u00d7\n1\n\u03bb21 sin 2 \u03b8t\n= \u03c12\n16 min\n{ 1\ntan2 \u03b8t , 1\n} . (9)\nRecall we defined T (\u03b4) as the time for linear system solver to reduce the error by a factor \u03b4. Therefore, in the initial phase where \u03b8t is large, it would be suffice to solve linear system up to factor \u03b4 = \u03c1 2 cos2 \u03b80\n16 \u2264 \u03c12 16 tan \u03b8t . In convergence phase, where \u03b8t is small, choose \u03b4 = \u03c12\n16 would be sufficient.\nTherefore, adding the computational cost of Algorithm 1 other than by linear system solver, it\u2019s not hard to get the total running time will be bounded by\n2\n\u03c1\n( log 1 cos \u03b80 \u00b7 T ( \u03c12 cos2 \u03b80 16 ) + log 1 \u01eb \u00b7 T ( \u03c12 16 )) + 2 \u03c1 (nnz (A) + nnz (B) + d) log\n1\n\u01eb cos \u03b80 .\nFurthermore, if we run Nesterov\u2019s accelerated gradient descent (Algorithm 4) on function f(w) to solve the linear systems. Since the condition number of the optimization problem (7) is \u03ba(B), by Theorem 8, we know T (\u03b4) = O(nnz (B) \u221a \u03ba(B) log 1\u03b4 ). Substituting this gives runtime:\nO\n( nnz (B) \u221a \u03ba(B)\n\u03c1\n( log 1\ncos \u03b80 log\n1\n\u03c1 cos \u03b80 + log\n1 \u01eb log 1 \u03c1\n) + 1\n\u03c1 nnz (A) log\n1\n\u01eb cos \u03b80\n) .\nwhich finishes the proof.\nB.2 Top-k Setting\nTo prove the convergence of subspace, we need a notion of angle between subspaces. The standard definition the is principal angles.\nDefinition 11 (Principal angles). Let X and Y be subspaces of Rd of dimension at least k. The principal angles 0 \u2264 \u03b8(1) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b8(k) between X and Y with respect to B-based scalar product are defined recursively via:\n\u03b8(i)(X ,Y) = min{arccos( \u3008x,y\u3009B\u2016x\u2016B\u2016y\u2016B ) : x \u2208 X ,y \u2208 Y,x \u22a5B xj ,y \u22a5B yj for all j < i} (xi,yi) \u2208 argmin{arccos( \u3008x,y\u3009B\n\u2016x\u2016B\u2016y\u2016B ) : x \u2208 X ,y \u2208 Y,x \u22a5B xj ,y \u22a5B yj for all j < i}\nFor matrices X and Y, we use \u03b8j(X,Y) to denote the j-th principal angle between their range.\nSince for our interest, we only care the largest principal angle, thus, in the following proof, without ambiguity, for X,Y \u2208 Rd\u00d7k, we use \u03b8(X,Y) to indicate \u03b8(k)(X,Y). Next lemma will tells us this definition of \u03b8(X,Y) to be the largest principal angle is same as what we defined in the main paper Definition 4.\nLemma 12. Let X,Y \u2208 Rd\u00d7k be orthonormal bases (w.r.t B) for subspace X ,Y respectively. Let X\u22a5 be an orthonormal basis for orthogonal complement of X (w.r.t B). Then we have\ncos \u03b8(X ,Y) = \u03c3k(X\u22a4BY), sin \u03b8(X ,Y) = \u2016X\u22a4\u22a5BY\u2016 (10)\nand assuming X\u22a4BY is invertible (\u03b8(X ,Y) < \u03c02 ), we have:\ntan \u03b8(X ,Y) = \u2016X\u22a4\u22a5BY(X\u22a4BY)\u22121\u2016 (11)\nProof. By definition of principal angle, it\u2019s easy to show cos \u03b8(X ,Y) = \u03c3k(X\u22a4BY). The projection operator onto subspace X is XX\u22a4B. It\u2019s also easy to show XX\u22a4B+X\u22a5X\u22a4\u22a5B = I Then, we have:\n(X\u22a4\u22a5BY) \u22a4X\u22a4\u22a5BY = Y \u22a4BX\u22a5X \u22a4 \u22a5BY\n= Y\u22a4B(I\u2212XX\u22a4B)Y = Y\u22a4BY \u2212 (X\u22a4BY)\u22a4(X\u22a4BY) = I\u2212 (X\u22a4BY)\u22a4(X\u22a4BY) (12)\nTherefore: \u2016X\u22a4\u22a5BY\u20162 = 1\u2212 \u03c32k(X\u22a4BY) = 1\u2212 cos2 \u03b8(X ,Y) = sin2 \u03b8(X ,Y) (13)\nSimilarily:\n[X\u22a4\u22a5BY(X \u22a4BY)\u22121]\u22a4X\u22a4\u22a5BY(X \u22a4BY)\u22121\n=[(X\u22a4BY)\u22121]\u22a4[I\u2212 (X\u22a4BY)\u22a4(X\u22a4BY)](X\u22a4BY)\u22121 =[(X\u22a4BY)\u22121]\u22a4(X\u22a4BY)\u22121 \u2212 I (14)\nTherefore:\n\u2016X\u22a4\u22a5BY(X\u22a4BY)\u22121\u20162 = 1\n\u03c32k(X \u22a4BY)\n\u2212 1 = 1 cos2 \u03b8(X ,Y) \u2212 1 = tan 2 \u03b8(X ,Y) (15)\nObviously, \u03b8(X ,Y) is acute, thus sin \u03b8(X ,Y) > 0 and tan \u03b8(X ,Y) > 0, which finishes the proof.\nSimilar to the top one case, for simplicity, we denote \u03b8t def = \u03b8 (Wt,V), where V \u2208 Rd\u00d7k is top k eigen-vector of generalized eigenvalue problem. Now we are ready to prove the theorem. We also denote V\u22a5 \u2208 Rd\u00d7(d\u2212k). Also throughout the proof, for any matrix X, we use notation \u2016X\u2016B \u2261 \u2016B 1 2X\u2016 \u2261 \u221a \u2016X\u22a4BX\u2016 and \u2016X\u2016B,F = \u2016B 1 2X\u2016F = \u221a tr(X\u22a4BX).\nProof of Theorem 5. Let V \u2208 Rd\u00d7k,\u039bV \u2208 Rk\u00d7k be the top k generalized eigen-pairs; and V\u22a5 \u2208 R d\u00d7(d\u2212k),\u039bV\u22a5 \u2208 R(d\u2212k)\u00d7(d\u2212k) be the remaining (d \u2212 k) generalized eigen-pairs (assume all eigenvectors normalized w.r.t. B). Then, we have:\nA = B(V\u039bVV \u22a4 +V\u22a5\u039bV\u22a5V \u22a4 \u22a5)B B = B(VV\u22a4 +V\u22a5V \u22a4 \u22a5)B\nBy approximately solving argminW\u2208Rd\u00d7k tr( 1 2W \u22a4BW\u2212W\u22a4AWt) and Gram-Schmidt process, we have:\nWt+1 = (B \u22121AWt + \u03be)R (16)\nwhere R \u2208 Rk\u00d7k is an invertable matrix generated by Gram-Schmidt process. We will follow the same strategy as in top 1 case, which will first prove the geometric convergence of tan \u03b8t assuming\n\u2016\u03be\u2016B \u2264 |\u03bbk| \u2212 |\u03bbk+1|\n4 min{sin \u03b8t, cos \u03b8t} (17)\nNote here \u03be is a matrix, and \u2016\u03be\u2016B = \u2016B 1 2 \u03be\u2016 = \u221a\n\u2016\u03be\u22a4B\u03be\u2016. Then we will bound the time taken by black-box linear system solver to provide such an accuracy.\nBy definition of tan \u03b8t and linear algebra calculation, we have\ntan \u03b8t+1 = \u2016V\u22a4\u22a5BWt+1(V\u22a4BWt+1)\u22121\u2016 = \u2016V\u22a4\u22a5BW\u0303t+1(V\u22a4BW\u0303t+1)\u22121\u2016 = \u2016(\u039bV\u22a5V\u22a4\u22a5BWt +V\u22a4\u22a5B\u03be)(\u039bVV\u22a4BWt +V\u22a4B\u03be)\u22121\u2016\n\u2264 \u2016(\u039bV\u22a5V \u22a4 \u22a5BWt +V \u22a4 \u22a5B\u03be)(V \u22a4BWt)\u22121\u2016 \u03c3k(\u039bV +V\u22a4B\u03be(V\u22a4BWt)\u22121) \u2264 \u2016\u039bV\u22a5\u2016 tan \u03b8t + \u2016V \u22a4 \u22a5B\u03be(V\n\u22a4BWt)\u22121\u2016 \u03c3k(\u039bV)\u2212 \u2016V\u22a4B\u03be(V\u22a4BWt)\u22121\u2016\n\u2264 \u2016\u039bV\u22a5\u2016 tan \u03b8t + \u2016V \u22a4 \u22a5B\u03be\u2016\u2016V\u22a4BWt)\u22121\u2016\n\u03c3k(\u039bV)\u2212 \u2016V\u22a4B\u03be\u2016\u2016(V\u22a4BWt)\u22121\u2016\n= \u2016\u039bV\u22a5\u2016 tan \u03b8t +\n\u2016V\u22a4 \u22a5 B\u03be\u2016\ncos \u03b8t\n\u03c3k(\u039bV)\u2212 \u2016V \u22a4B\u03be\u2016\ncos \u03b8t\n\u2264 tan \u03b8t |\u03bbk+1|+ \u2016\u03be\u2016Bsin \u03b8t |\u03bbk| \u2212 \u2016\u03be\u2016Bcos \u03b8t\n(18)\nSince \u2016\u03be\u2016B \u2264 |\u03bbk|\u2212|\u03bbk+1|4 min{sin \u03b8t, cos \u03b8t}, we have that:\ntan \u03b8t+1 \u2264 |\u03bbk|+ 3|\u03bbk+1| 3|\u03bbk|+ |\u03bbk+1| tan \u03b8t (19)\n= (1\u2212 2(|\u03bbk| \u2212 |\u03bbk+1|) 3|\u03bbk|+ |\u03bbk+1| ) tan \u03b8t \u2264 exp(\u2212 |\u03bbk| \u2212 |\u03bbk+1| 2|\u03bbk| ) tan \u03b8t (20)\nRecall in this problem \u03c1 = 1\u2212 |\u03bbk+1||\u03bbk| , therefore, we know:\nsin \u03b8t \u2264 tan \u03b8t \u2264 exp(\u2212 \u03c1\n2 \u00b7 t) tan \u03b80 \u2264 exp(\u2212\n\u03c1 2 \u00b7 t) 1 cos \u03b80 (21)\nIf we want sin \u03b8t \u2264 \u01eb, which gives iterations:\nt \u2265 2 \u03c1 log\n1\n\u01eb cos \u03b80 (22)\nLet f(W) = tr(12W \u22a4BW\u2212W\u22a4AWt). For this problem, we can view W as a dk dimensional vector, and use linear system to solve this d, k dimensional problem. Therefore, if we represent W in terms of matrix, the corresponding linear system error is \u2016W \u2212B\u22121AWt\u2016B,F , recall \u2016W\u2016B,F = \u2016B 12W\u2016F = \u221a tr(W\u22a4BW). To satisfy the accuracy requirement, we only need\n\u01ebdes = \u2016\u03be\u20162B,F \u2264 (|\u03bbk| \u2212 |\u03bbk+1|)2\n16 min{sin2 \u03b8t, cos2 \u03b8t} (23)\nRecall we initialize the linear system solver with Wt\u0393t with \u0393t = (W \u22a4 t BWt) \u22121(W\u22a4t AWt), we then have\n\u01ebinit = \u2016Wt\u0393t \u2212B\u22121AWt\u20162B,F = tr[(Wt\u0393t \u2212B\u22121AWt)\u22a4B(Wt\u0393t \u2212B\u22121AWt)] =2[f(Wt\u0393t)\u2212 f(B\u22121AWt)] = 2[argmin\n\u0393\u2208Rk\u00d7k f(Wt\u0393)\u2212 f(B\u22121AWt)] (24)\nLet \u0393\u0302t = (V \u22a4BWt)\u22121\u039bV(V\u22a4BWt), and observe \u2016\u03be\u20162B,F = \u2016B 1 2 \u03be\u20162F = \u2016V\u22a4B\u03be\u20162F + \u2016V\u22a4\u22a5B\u03be\u20162F (Pythagorean theorem under B norm), then we have:\n\u01ebinit =\u2016Wt\u0393t \u2212B\u22121AWt\u20162B,F = 2[argmin \u0393\u2208Rk\u00d7k f(Wt\u0393)\u2212 f(B\u22121AWt)]\n\u22642[f(Wt\u0393\u0302t)\u2212 f(B\u22121AWt)] = \u2016Wt\u0393\u0302t \u2212B\u22121AWt\u20162B,F =\u2016V\u22a4B(Wt\u0393\u0302t \u2212B\u22121AWt)\u20162F + \u2016V\u22a4\u22a5B(Wt\u0393\u0302t \u2212B\u22121AWt)\u20162F =\u2016V\u22a4BWt\u0393\u0302t \u2212 \u039bVV\u22a4BWt\u20162F + \u2016V\u22a4\u22a5BWt\u0393\u0302t \u2212 \u039bV\u22a5V\u22a4\u22a5BWt\u20162F =0 + \u2016V\u22a4\u22a5BWt\u0393\u0302t \u2212 \u039bV\u22a5V\u22a4\u22a5BWt\u20162F \u2264k\u2016V\u22a4\u22a5BWt\u0393\u0302t \u2212 \u039bV\u22a5V\u22a4\u22a5BWt\u20162 \u22642k sin2 \u03b8t(\u2016\u0393\u0302t\u20162 + \u2016\u039bV\u22a5\u20162) \u2264 4k|\u03bb1|2 tan2 \u03b8t (25)\nThe last step is correct since \u2016\u039bV\u22a5\u2016 \u2264 |\u03bb1| and \u2016\u0393\u0302t\u2016 \u2264 \u2016(V\u22a4BWt)\u22121\u2016\u2016\u039bV\u2016\u2016V\u22a4B 1 2 \u2016\u2016B 12Wt\u2016 \u2264 1\ncos \u03b8t |\u03bb1|\nThis means we wish to decrease the ratio of final to initial error smaller than:\n\u01ebdes \u01ebinit \u2264 \u03c1 2 64k\u03b32 min{ 1 cos2 \u03b8t , sin2 \u03b8t cos4 \u03b8t } (26)\nwhere \u03b3 = |\u03bb1||\u03bbk| . Therefore, a two phase analysis of running time depending on \u03b8t is large or small similar to top 1 case would gives the total runtime:\n2\n\u03c1\n( log 1 cos \u03b80 \u00b7 T ( \u03c12 cos4 \u03b80 64k\u03b32 ) + log 1 \u01eb \u00b7 T ( \u03c12 64k\u03b32 )) + 2 \u03c1 ( nnz (A) k + nnz (B) k + dk2 ) log\n1\n\u01eb cos \u03b80 ,\nif we are using the accelerated gradient descent to solve the linear system, we are essentially solve k disjoint optimization problem, with each problem dimension d and condition number \u03ba(B). Directly apply Theorem 8 gives runtime\nO\n( nnz (B) k \u221a \u03ba(B)\n\u03c1\n( log 1\ncos \u03b80 log\nk\u03b3\n\u03c1 cos \u03b80 + log\n1 \u01eb log k\u03b3 \u03c1\n) + ( nnz (A) k + dk2 )\n\u03c1 log\n1\n\u01eb cos \u03b80\n) .\nFinally, since both results Theorem 5 and Theorem 7 are stated in terms of initialization \u03b80, here we will give probablistic guarantee for random initialization.\nLemma 13 (Random Initialization). Let top k eigen-vector be V \u2208 Rd\u00d7k, and the remaining eigenvector be V\u22a5 \u2208 Rd\u00d7(d\u2212k). If we initialize W0 as in Algorithm 2, then With at least probability 1\u2212\u03b7, we have:\ntan \u03b80 = \u2016V\u22a4\u22a5BW0(V\u22a4BW0)\u22121\u2016 \u2264 O( \u221a \u03ba(B)dk\n\u03b7 ) (27)\nProof. Recall W\u0303 is entry-wise sampled from standard Gaussian, and\ntan \u03b80 =\u2016V\u22a4\u22a5BW0(V\u22a4BW0)\u22121\u2016 = \u2016V\u22a4\u22a5BW\u03030(V\u22a4BW\u03030)\u22121\u2016 \u2264 \u2016V\u22a4\u22a5BW\u03030\u2016 \u03c3k(V\u22a4BW\u03030)\n\u2264 \u2016V \u22a4 \u22a5BV\u0303\u22a5\u2016\n\u03c3k(V\u22a4BV\u0303)\n\u2016V\u0303\u22a5 \u22a4 W\u03030\u2016 \u03c3k(V\u0303\u22a4W\u03030) (28)\nWhere V\u0303\u22a5, V\u0303 are the right singular vectors of V\u22a4\u22a5B,V \u22a4B respectively. Then, we have first term:\n\u2016V\u22a4\u22a5BV\u0303\u22a5\u2016 \u03c3k(V\u22a4BV\u0303) = \u2016V\u22a4\u22a5B\u2016 \u03c3k(V\u22a4B) \u2264 \u2016V \u22a4 \u22a5B 1 2\u2016\u2016B 12\u2016 \u03c3k(V\u22a4B 1 2 )\u03c3min(B 1 2 ) = \u03ba(B) 1 2 (29)\nThe last step is true since both V\u22a4\u22a5B 1 2 and V\u22a4B 1 2 are orthonormal matrix.\nFor the second term, we know \u2016V\u0303\u22a5 \u22a4 W\u03030\u2016 \u223c O( \u221a d+ \u221a k) with high probability, and by equation 3.2 in (Rudelson and Vershynin, 2010) we know \u03c3k(V\u0303 \u22a4W\u03030) \u2265 \u03b7\u221ak with probability at least 1\u2212 \u03b7, which finishes the proof.\nB.3 CCA Setting\nSince our approach to CCA directly calls Algorithm 2 for solving generalized eigenvalue problem as subroutine, most of the theoretical property should be clear other than random projection step in Algorithm 3. Here, we give following lemma. The proof of Theorem 7 easily follow from the combination of this lemma and Theorem 6.\nLemma 14. If the ( W\u0304x W\u0304y ) as constructed in Algorithm 3 has angle at most \u03b8 with the true top2k generalized eigenspace of A,B, then with probability 1 \u2212 \u03b6, both Wx, Wy has angle at most O(k2\u03b8/\u03b62) with the true top-k canonical space of X,Y.\nProof. We will prove this for Wy, the proof for Wx follows directly from same strategy.\nRecall B = ( Sxx 0 0 Syy ) . Let \u03a6 \u2208 Rd1\u00d7k be the true top k subspace of X and \u03a8 \u2208 Rd2\u00d7k be the true top k subspace of Y.Then by construction we know the top 2k subspace should be\n1\u221a 2 ( \u03a6 \u2212\u03a6 \u03a8 \u03a8 ) .\nBy properties of principal angle, we know there exists an orthonormal matrix R \u2208 R2k\u00d72k such that\n\u2016 1\u221a 2 B1/2 ( \u03a6 \u2212\u03a6 \u03a8 \u03a8 ) R\u2212B1/2 ( W\u0304x W\u0304y ) \u2016 \u2264 2 sin \u03b8 2 .\nIn particular, if we only look at the last d2 rows, we have\n\u2016 1\u221a 2 S1/2yy\n( \u03a8 \u03a8 ) R\u2212 S1/2yy W\u0304y\u2016 \u2264 2 sin \u03b8\n2 .\nLet U be the random Gaussian projection we used, and let RU = ( U1 U2 ) , we know\nS1/2yy W\u0304yU = 1\u221a 2 S1/2yy\n( \u03a8 \u03a8 )( U1 U2 ) +E\n= 1\u221a 2 S1/2yy \u03a8(U1 +U2) +E,\nwhere E is the error (after multipled by random matrix U), with \u2016E\u2016 \u2264 O(2 \u221a k sin \u03b82) \u2264 O( \u221a k\u03b8).\nLet V = (S 1/2 yy W\u0304yU) \u22a4S1/2yy W\u0304yU, the orthonormalization step gives a matrix Wy that is equivalent (up to rotation) to W\u0304yUV\n\u22121/2. Our goal is to show V\u22121/2 \u2248 ((U1 +U2)\u22a4(U1 +U2))\u22121/2 so we get roughly \u03a8.\nNote that \u03a8\u22a4Syy\u03a8 = I, therefore V = 12 (U1 + U2) \u22a4(U1 + U2) + E\u2032 where the error E\u2032 =\n( 1\u221a 2 S 1/2 yy \u03a8(U1+U2)) \u22a4E+ 1\u221a 2 E\u22a4(S1/2yy \u03a8(U1+U2))+E\u22a4E). We know with high probability \u2016U1+\nU2\u2016 \u2264 O( \u221a k), with probability at least 1 \u2212 \u03b6, \u03c3min(U1 + U2) \u2265 \u2126(\u03b6/ \u221a k). Therefore we know \u03c3min[(U1 +U2) \u22a4(U1 +U2)] \u2265 \u2126(\u03b62/k) and \u2016E\u2032\u2016 \u2264 O(k\u03b8). By matrix perturbation for inverse we\nknow \u2016V\u22121/2 \u2212 \u221a 2((U1 +U2)\n\u22a4(U1 +U2))\u22121/2\u2016 \u2264 O(k2\u03b8/\u03b62). Since (U1 +U2)((U1 +U2)\u22a4(U1 + U2)) \u22121/2 = R\u2032 is an orthonormal matrix, we know there\u2019s some orthonormal matrix R\u2032\u2032 so that:\n\u2016S1/2yy Wy \u2212 S1/2yy \u03a8R\u2032\u2032\u2016 = \u2016S1/2yy W\u0304yUV\u22121/2 \u2212 S1/2yy \u03a8R\u2032\u2016 \u2264\u2016S1/2yy W\u0304yUV\u22121/2 \u2212 \u221a 2S1/2yy W\u0304yU((U1 +U2) \u22a4(U1 +U2)) \u22121/2\u2016\n+ \u2016 \u221a 2S1/2yy W\u0304yU((U1 +U2) \u22a4(U1 +U2)) \u22121/2 \u2212 S1/2yy \u03a8R\u2032\u2016 \u2264 O(k2\u03b8/\u03b62)\nTherefore the angle between the Wy and the truth \u03a8 is bounded by O(k 2\u03b8/\u03b62)."}], "references": [{"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "SIAM Journal on Scientific Computing, 36(5):S111\u2013S131.", "citeRegEx": "Avron et al\\.,? 2014", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Origins and levels of monthly and seasonal forecast skill for united states surface air temperatures determined by canonical correlation analysis", "author": ["T. Barnett", "R. Preisendorfer"], "venue": "Monthly Weather Review, 115(9):1825\u20131850.", "citeRegEx": "Barnett and Preisendorfer,? 1987", "shortCiteRegEx": "Barnett and Preisendorfer", "year": 1987}, {"title": "Prediction of enso episodes using canonical correlation analysis", "author": ["A.G. Barnston", "C.F. Ropelewski"], "venue": "Journal of climate, 5(11):1316\u20131345.", "citeRegEx": "Barnston and Ropelewski,? 1992", "shortCiteRegEx": "Barnston and Ropelewski", "year": 1992}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 129\u2013136. ACM.", "citeRegEx": "Chaudhuri et al\\.,? 2009", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Multi-view learning of word embeddings via cca", "author": ["P. Dhillon", "D.P. Foster", "L.H. Ungar"], "venue": "Advances in Neural Information Processing Systems, pages 199\u2013207.", "citeRegEx": "Dhillon et al\\.,? 2011", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Detection of neural activity in functional mri using canonical correlation analysis", "author": ["O. Friman", "J. Cedefamn", "P. Lundberg", "M. Borga", "H. Knutsson"], "venue": "Magnetic Resonance in Medicine, 45(2):323\u2013330.", "citeRegEx": "Friman et al\\.,? 2001", "shortCiteRegEx": "Friman et al\\.", "year": 2001}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["R. Frostig", "R. Ge", "S.M. Kakade", "A. Sidford"], "venue": "ICML2015.", "citeRegEx": "Frostig et al\\.,? 2015", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "Fast and simple pca via convex optimization", "author": ["D. Garber", "E. Hazan"], "venue": "arXiv preprint arXiv:1509.05647.", "citeRegEx": "Garber and Hazan,? 2015", "shortCiteRegEx": "Garber and Hazan", "year": 2015}, {"title": "Matrix computations, volume 3", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press.", "citeRegEx": "Golub and Loan,? 2012", "shortCiteRegEx": "Golub and Loan", "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "Martinsson", "P.-G.", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013 288.", "citeRegEx": "Halko et al\\.,? 2011", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural computation, 16(12):2639\u20132664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Methods of conjugate gradients for solving linear systems, volume 49", "author": ["M.R. Hestenes", "E. Stiefel"], "venue": "NBS.", "citeRegEx": "Hestenes and Stiefel,? 1952", "shortCiteRegEx": "Hestenes and Stiefel", "year": 1952}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377.", "citeRegEx": "Hotelling,? 1936", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["C. Jin", "S.M. Kakade", "C. Musco", "P. Netrapalli", "A. Sidford"], "venue": "CoRR, abs/1510.08896.", "citeRegEx": "Jin et al\\.,? 2015", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "NIPS2013, pages 315\u2013323.", "citeRegEx": "Johnson and Zhang,? 2013", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "Learning theory, pages 82\u201396. Springer.", "citeRegEx": "Kakade and Foster,? 2007", "shortCiteRegEx": "Kakade and Foster", "year": 2007}, {"title": "Discriminative features via generalized eigenvectors", "author": ["N. Karampatziakis", "P. Mineiro"], "venue": "arXiv preprint arXiv:1310.1934.", "citeRegEx": "Karampatziakis and Mineiro,? 2013", "shortCiteRegEx": "Karampatziakis and Mineiro", "year": 2013}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A universal catalyst for first-order optimization", "author": ["H. Lin", "J. Mairal", "Z. Harchaoui"], "venue": "arXiv preprint arXiv:1506.02186.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "Advances in Neural Information Processing Systems, pages 91\u201399.", "citeRegEx": "Lu and Foster,? 2014", "shortCiteRegEx": "Lu and Foster", "year": 2014}, {"title": "Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis", "author": ["Z. Ma", "Y. Lu", "D.P. Foster"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR Proceedings.", "citeRegEx": "Ma et al\\.,? 2015", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Nonparametric Canonical Correlation Analysis", "author": ["T. Michaeli", "W. Wang", "K. Livescu"], "venue": "CoRR, abs/1511.04839.", "citeRegEx": "Michaeli et al\\.,? 2015", "shortCiteRegEx": "Michaeli et al\\.", "year": 2015}, {"title": "Stronger approximate singular value decomposition via the block lanczos and power methods", "author": ["C. Musco", "C. Musco"], "venue": "arXiv preprint arXiv:1504.05477.", "citeRegEx": "Musco and Musco,? 2015", "shortCiteRegEx": "Musco and Musco", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, volume 27, pages 372\u2013376.", "citeRegEx": "Nesterov,? 1983", "shortCiteRegEx": "Nesterov", "year": 1983}, {"title": "Core-sets for canonical correlation analysis", "author": ["S. Paul"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1887\u20131890. ACM.", "citeRegEx": "Paul,? 2015", "shortCiteRegEx": "Paul", "year": 2015}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications, 31(3):1100\u20131124.", "citeRegEx": "Rokhlin et al\\.,? 2009", "shortCiteRegEx": "Rokhlin et al\\.", "year": 2009}, {"title": "Non-asymptotic theory of random matrices: extreme singular values", "author": ["M. Rudelson", "R. Vershynin"], "venue": "arXiv preprint arXiv:1003.2990.", "citeRegEx": "Rudelson and Vershynin,? 2010", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2010}, {"title": "Conducting and interpreting canonical correlation analysis in personality research: A user-friendly primer", "author": ["A. Sherry", "R.K. Henson"], "venue": "Journal of personality assessment, 84(1):37\u201348.", "citeRegEx": "Sherry and Henson,? 2005", "shortCiteRegEx": "Sherry and Henson", "year": 2005}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": "Technical report, Pittsburgh, PA, USA.", "citeRegEx": "Shewchuk,? 1994", "shortCiteRegEx": "Shewchuk", "year": 1994}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888\u2013905.", "citeRegEx": "Shi and Malik,? 2000", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems", "author": ["D.A. Spielman", "Teng", "S.-H."], "venue": "Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 81\u201390. ACM.", "citeRegEx": "Spielman et al\\.,? 2004", "shortCiteRegEx": "Spielman et al\\.", "year": 2004}, {"title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations", "author": ["W. Wang", "R. Arora", "K. Livescu", "N. Srebro"], "venue": "volume abs/1510.02054. 16", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Large-Scale Approximate Kernel Canonical Correlation Analysis", "author": ["W. Wang", "K. Livescu"], "venue": "CoRR, abs/1511.04773.", "citeRegEx": "Wang and Livescu,? 2015", "shortCiteRegEx": "Wang and Livescu", "year": 2015}, {"title": "Globally convergent stochastic optimization for canonical correlation analysis", "author": ["W. Wang", "J. Wang", "N. Srebro"], "venue": "arXiv preprint arXiv:1604.01870.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": "Biostatistics, page kxp008. 17", "citeRegEx": "Witten et al\\.,? 2009", "shortCiteRegEx": "Witten et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 12, "context": "This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices.", "startOffset": 73, "endOffset": 90}, {"referenceID": 30, "context": "These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "startOffset": 139, "endOffset": 203}, {"referenceID": 10, "context": "These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "startOffset": 139, "endOffset": 203}, {"referenceID": 35, "context": "These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).", "startOffset": 139, "endOffset": 203}, {"referenceID": 1, "context": "Canonical-correlation analysis (CCA) and the generalized eigenvector problem are fundamental problems in scientific computing, data analysis, and statistics (Barnett and Preisendorfer, 1987; Friman et al., 2001).", "startOffset": 157, "endOffset": 211}, {"referenceID": 5, "context": "Canonical-correlation analysis (CCA) and the generalized eigenvector problem are fundamental problems in scientific computing, data analysis, and statistics (Barnett and Preisendorfer, 1987; Friman et al., 2001).", "startOffset": 157, "endOffset": 211}, {"referenceID": 15, "context": "Algorithms for solving them are commonly used to extract features to compare and contrast large data sets and are used commonly in regression (Kakade and Foster, 2007), clustering (Chaudhuri et al.", "startOffset": 142, "endOffset": 167}, {"referenceID": 3, "context": "Algorithms for solving them are commonly used to extract features to compare and contrast large data sets and are used commonly in regression (Kakade and Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al.", "startOffset": 180, "endOffset": 204}, {"referenceID": 16, "context": ", 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al.", "startOffset": 24, "endOffset": 58}, {"referenceID": 4, "context": ", 2009), classification (Karampatziakis and Mineiro, 2013), word embeddings (Dhillon et al., 2011) and more.", "startOffset": 76, "endOffset": 98}, {"referenceID": 1, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 2, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 28, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 16, "context": "Despite the prevalence of these problems and the breadth of research on solving them in practice ((Barnett and Preisendorfer, 1987; Barnston and Ropelewski, 1992; Sherry and Henson, 2005; Karampatziakis and Mineiro, 2013) to name a few), there are relatively few results on obtaining provably efficient algorithms.", "startOffset": 98, "endOffset": 221}, {"referenceID": 0, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 25, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 19, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 20, "context": "Can we develop simple iterative practical methods that solve this problem in close to linear time when k is small and the condition number and eigenvalue gaps are bounded? While there has been recent work on solving these problems using iterative methods (Avron et al., 2014; Paul, 2015; Lu and Foster, 2014; Ma et al., 2015) we are unaware of previous provable global convergence results and more strongly, linearly convergent scalable algorithms.", "startOffset": 255, "endOffset": 325}, {"referenceID": 26, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 9, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 23, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 7, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 13, "context": "While there has been limited previous work on provably solving CCA and generalized eigenvectors, we note that there is an impressive body of literature on performing PCA(Rokhlin et al., 2009; Halko et al., 2011; Musco and Musco, 2015; Garber and Hazan, 2015; Jin et al., 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 169, "endOffset": 276}, {"referenceID": 11, "context": ", 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 56, "endOffset": 125}, {"referenceID": 24, "context": ", 2015) and solving positive semidefinite linear systems(Hestenes and Stiefel, 1952; Nesterov, 1983; Spielman and Teng, 2004).", "startOffset": 56, "endOffset": 125}, {"referenceID": 20, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 32, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 33, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 22, "context": "There has been much recent interest in designing scalable algorithms for CCA(Ma et al., 2015; Wang et al., 2015; Wang and Livescu, 2015; Michaeli et al., 2015).", "startOffset": 76, "endOffset": 159}, {"referenceID": 35, "context": "Heuristic-based approachs (Witten et al., 2009; Lu and Foster, 2014) compute efficiently, but only give suboptimal result due to coarse approximation.", "startOffset": 26, "endOffset": 68}, {"referenceID": 19, "context": "Heuristic-based approachs (Witten et al., 2009; Lu and Foster, 2014) compute efficiently, but only give suboptimal result due to coarse approximation.", "startOffset": 26, "endOffset": 68}, {"referenceID": 20, "context": "The work in (Ma et al., 2015) provides one natural iterative procedure, where the per iterate computational complexity is low.", "startOffset": 12, "endOffset": 29}, {"referenceID": 20, "context": "Also of note is that many recent algorithms (Ma et al., 2015; Wang et al., 2015) have mini-batch variations, but there\u2019s no guarantees for mini-batch style algorithm for CCA yet.", "startOffset": 44, "endOffset": 80}, {"referenceID": 32, "context": "Also of note is that many recent algorithms (Ma et al., 2015; Wang et al., 2015) have mini-batch variations, but there\u2019s no guarantees for mini-batch style algorithm for CCA yet.", "startOffset": 44, "endOffset": 80}, {"referenceID": 20, "context": "S-AppGrad (Ma et al., 2015) \u00d5( \u03c1 log 1 \u01eb )", "startOffset": 10, "endOffset": 27}, {"referenceID": 34, "context": "Subsequent to the submission of this paper, we learned of the closely related work in (Wang et al., 2016), which presents a number of additional interesting results.", "startOffset": 86, "endOffset": 105}, {"referenceID": 20, "context": "(Ma et al., 2015) only shows local convergence for S-AppGrad.", "startOffset": 0, "endOffset": 17}, {"referenceID": 34, "context": "This table was inspired by (Wang et al., 2016) in order to facilitate comparison to existing work.", "startOffset": 27, "endOffset": 46}, {"referenceID": 14, "context": "(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al.", "startOffset": 195, "endOffset": 220}, {"referenceID": 6, "context": "(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al., 2015; Lin et al., 2015).", "startOffset": 258, "endOffset": 298}, {"referenceID": 18, "context": "(AGD), it is immediate to apply Theorem 7 and give the corresponding rates if we instantiate it by other popular algorithms, including gradient descent (GD), stochastic variance reduction (SVRG) (Johnson and Zhang, 2013), and its accelerated version (ASVRG) (Frostig et al., 2015; Lin et al., 2015).", "startOffset": 258, "endOffset": 298}, {"referenceID": 29, "context": "Moreover, it is well known that any method which starts at m and iteratively applies M to linear combinations of the points computed so far must apply M at least \u03a9( \u221a \u03ba(B)) in order to halve the error in the standard norm for the problem (Shewchuk, 1994).", "startOffset": 238, "endOffset": 254}, {"referenceID": 17, "context": "MNIST dataset(LeCun et al., 1998) consists of 60,000 handwritten digits from 0 to 9.", "startOffset": 13, "endOffset": 33}, {"referenceID": 21, "context": "17 million tokens and a vocabulary size of 43k(Marcus et al., 1993), which has already been used to successfully learn the word embedding by CCA(Dhillon et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 4, "context": ", 1993), which has already been used to successfully learn the word embedding by CCA(Dhillon et al., 2011).", "startOffset": 84, "endOffset": 106}, {"referenceID": 20, "context": "For experiments in this section, we follow the setting of (Ma et al., 2015).", "startOffset": 58, "endOffset": 75}, {"referenceID": 20, "context": "We compare our algorithm to S-AppGrad (Ma et al., 2015) which is an iterative algorithm and PCA-CCA (Ma et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 20, "context": ", 2015) which is an iterative algorithm and PCA-CCA (Ma et al., 2015), NW-CCA (Witten et al.", "startOffset": 52, "endOffset": 69}, {"referenceID": 35, "context": ", 2015), NW-CCA (Witten et al., 2009) and DW-CCA (Lu and Foster, 2014) which are one-shot estimation procedures.", "startOffset": 16, "endOffset": 37}, {"referenceID": 19, "context": ", 2009) and DW-CCA (Lu and Foster, 2014) which are one-shot estimation procedures.", "startOffset": 19, "endOffset": 40}], "year": 2016, "abstractText": "This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-k generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of O ( zk \u221a \u03ba \u03c1 log(1/\u01eb) log (k\u03ba/\u03c1) ) where z is the total number of nonzero entries, \u03ba is the condition number and \u03c1 is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components k up to a log(k) factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets.", "creator": "LaTeX with hyperref package"}}}