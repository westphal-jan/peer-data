{"id": "1604.07255", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2016", "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "abstract": "the ability to reuse or transfer knowledge from single task to another in lifelong learning problems, such as minecraft, is one along the major challenges faced in ai. reusing knowledge specific tasks is crucial to solving tasks efficiently involving lower computational prevalence. we provide a detailed learning engine with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action ( also know as options ( sutton et. om. 1999 ) ). the group learns reusable experiences using deep q networks ( mnih et. eng. 2015 ) helps solve tasks in minecraft, a popular video game which avoids an unsolved and high - dimensional lifelong learning problem. these reusable systems, which we refer to as deep skill networks ( dsns ), are then incorporated into our novel hierarchical deep reinforcement learning network ( block - drln ) process. the h - drln is a hierarchical version, regular qnetworks they learns to efficiently solve tasks by reusing knowledge from previously learned dsns. \" h - drln exhibits superior performance and lower learning sample complexity ( by taking advantage of sample similarities ) compared to the regular deep q network ( mnih et. al. 2015 ) automated subdomains task tests. we also show the potential to transfer knowledge between related minecraft tasks without any additional learning.", "histories": [["v1", "Mon, 25 Apr 2016 13:45:50 GMT  (1644kb,D)", "http://arxiv.org/abs/1604.07255v1", null], ["v2", "Thu, 27 Oct 2016 13:18:20 GMT  (1256kb,D)", "http://arxiv.org/abs/1604.07255v2", null], ["v3", "Wed, 30 Nov 2016 17:35:27 GMT  (4065kb,D)", "http://arxiv.org/abs/1604.07255v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["chen tessler", "shahar givony", "tom zahavy", "daniel j mankowitz", "shie mannor"], "accepted": true, "id": "1604.07255"}, "pdf": {"name": "1604.07255.pdf", "metadata": {"source": "CRF", "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "authors": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J. Mankowitz", "Shie Mannor"], "emails": ["{@campus.technion.ac.il", "danielm@tx.technion.ac.il,", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Lifelong learning is defined as the ability to accumulate knowledge across multiple tasks and then reuse or transfer this knowledge in order to solve sub-sequent tasks [Eaton and Ruvolo, 2013]. This is one of the fundamental learning problems in AI [Thrun and Mitchell, 1995; Eaton and Ruvolo, 2013]. There are different ways to performing knowledge transfer across tasks. For example, Ammar et al. (2014) and Ammar et al. (2015) transfer knowledge via a latent basis whereas Brunskill and Li (2014) perform batch optimization across all encountered tasks.\nLifelong learning in real-world domains suffers from the curse of dimensionality. That is, as the state and action spaces increase, it becomes more and more difficult to model and\nsolve new tasks as they are encountered. A challenging, highdimensional domain that incorporates many of the elements found in life-long learning is Minecraft 1. Minecraft is a popular video game whose goal is to build structures, travel on adventures, hunt for food and avoid zombies. An example screenshot from the game is seen in Figure 1. Minecraft is an open research problem as it is impossible to solve the entire game using a single AI technique. Instead, the solution to Minecraft may lie in solving sub-problems, using a divideand-conquer approach, and then providing a synergy between the various solutions. Once an agent learns to solve a subproblem, it has acquired a skill that can then be reused when a similar sub-problem is subsequently encountered.\nMany of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015]. In Minecraft for example, consider building a wooden house as seen in Figure 1. This task can be decomposed into sub-tasks (a.k.a skills) such as chopping trees, sanding the wood, cutting the wood into boards and finally nailing the boards together. Here, the knowledge gained from chopping trees can also be partially reused when cutting the wood into boards. In addition, if the agent receives a new task to build a small city, then the agent can reuse the skills it acquired during the \u2018building a house\u2019 task.\nIn a lifelong learning setting such as Minecraft, learning skills and when to reuse the skills are key to increasing explo-\n1https://minecraft.net/\nar X\niv :1\n60 4.\n07 25\n5v 1\n[ cs\n.A I]\n2 5\nA pr\n2 01\n6\nration, efficiently solving tasks and advancing the capabilities of the Minecraft agent. As mentioned previously, Minecraft and other lifelong learning problems suffer from the curse of dimensionality. Therefore as the dimensionality of the problem increases, it becomes increasingly non-trivial to learn reasonable skills as well as when to reuse these skills.\nReinforcement Learning (RL) provides a generalized approach to skill learning through the options framework [Sutton et al., 1999]. Options are Temporally Extended Actions (TEAs) and are also referred to as skills [da Silva et al., 2012] and macro-actions [Hauskrecht et al., 1998]. Options have been shown both theoretically [Precup and Sutton, 1997; Sutton et al., 1999] and experimentally [Mann and Mannor, 2013] to speed up the convergence rate of RL algorithms. From here on in, we will refer to options as skills.\nRecent work in RL has provided insights into learning reusable skills [Mankowitz et al., 2016a; Mankowitz et al., 2016b], but this has been limited to low dimensional problems. In high-dimensional lifelong learning settings (E.g. Minecraft), learning from visual experiences provides a potential solution to learning reusable skills. With the emergence of Deep Reinforcement Learning, specifically Deep QNetworks (DQNs)[Mnih et al., 2015], RL agents are now equipped with a powerful non-linear function approximator that can learn rich and complex policies. Using these networks the agent learns policies (or skills) from raw image pixels, requiring less domain specific knowledge to solve complicated tasks (E.g Atari video games [Mnih et al., 2015]). While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al., 2015] unless otherwise stated.\nIn our paper, we focus on learning reusable RL skills using Deep Q Networks [Mnih et al., 2015], by solving subproblems in Minecraft. These reusable skills, which we refer to as Deep Skill Networks (DSNs) are then incorporated into our novel Hierarchical Deep Reinforcement Learning (RL) Network (H-DRLN) architecture. The H-DRLN, which is a hierarchical version of the DQN, learns to solve more complicated tasks by reusing knowledge from the pre-learned DSNs. By taking advantage of temporal extension, the H-DRLN learns to solve tasks with lower sample complexity and superior performance compared to vanilla DQNs.\nContributions: (1) A novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. (2) We show the potential to learn reusable Deep Skill Networks (DSNs) and perform knowledge transfer of the learned DSNs to a new task to obtain an optimal solution. (3) Empirical results for learning a H-DRLN in a sub-domain of Minecraft which outperforms the vanilla DQN. (4) We verify empirically the improved convergence guarantees for utilizing reusable DSNs (a.k.a options) within the H-DRLN, compared to the vanilla DQN. (5) The potential to transfer knowledge between related tasks without any additional learning."}, {"heading": "2 Background", "text": "Reinforcement Learning: The goal of an RL agent is to maximize its expected return by learning a policy \u03c0 : S \u2192\n\u2206A which is a mapping from states s \u2208 S to a probability distribution over the action space A. At time t the agent observes a state st \u2208 S, selects an action at \u2208 A, and receives a bounded reward rt \u2208 [0, Rmax] where Rmax is the maximum attainable reward and \u03b3 \u2208 [0, 1] is the discount factor. Following the agents action choice, it transitions to the next state st+1 \u2208 S . We consider infinite horizon problems where the cumulative return at time t is given by Rt = \u2211\u221e t\u2032=t \u03b3\nt\u2032\u2212trt. The action-value function Q\u03c0(s, a) = E[Rt|st = s, at = a, \u03c0] represents the expected return after observing state s and taking an action a under a policy \u03c0. The optimal action-value function obeys a fundamental recursion known as the Bellman equation,\nQ\u2217(st, at) = E [ rt + \u03b3max\na\u2032 Q\u2217(st+1, a\n\u2032) ]\nDeep Q Networks: The DQN algorithm [Mnih et al., 2015] approximates the optimal Q function with a Convolutional Neural Network (CNN) [Krizhevsky et al., 2012], by optimizing the network weights such that the expected Temporal Difference (TD) error of the optimal bellman equation (Equation 1) is minimized:\nEst,at,rt,st+1 \u2016Q\u03b8 (st, at)\u2212 yt\u2016 2 2 , (1)\nwhere\nyt = rt if st+1 is terminalrt + \u03b3max a\u2019 Q\u03b8target ( st+1, a \u2032 ) otherwise .\nNotice that this is an off-line learning algorithm, meaning that the tuples {st,at, rt, st+1, \u03b3} are collected from the agents experience and are stored in the Experience Replay (ER) [Lin, 1993]. The ER is a buffer that stores the agents experiences at each time-step t, for the purpose of ultimately training the DQN parameters to minimize the loss function. When we apply minibatch training updates, we sample tuples of experience at random from the pool of stored samples in the ER. The DQN maintains two separate Q-networks. The current Q-network with parameters \u03b8, and the target Q-network with parameters \u03b8target. The parameters \u03b8target are set to \u03b8 every fixed number of iterations. In order to capture the game dynamics, the DQN represents the state by a sequence of image frames.\nSkills, Options, Macro-actions [Sutton et al., 1999]: A skill \u03c3 is a temporally extended control structure defined by a triple \u03c3 =< I, \u03c0, \u03b2 > where I is the set of states where the skill can be initiated, \u03c0 is the intra-skill policy, which determines how the skill behaves in encountered states, and \u03b2 is the set of termination probabilities determining when a skill will stop execution. \u03b2 is typically either a function of state s or time t.\nSemi-Markov Decision Process (SMDP): Planning with skills can be performed using SMDP theory. For each state in the skill initiation set I , the SMDP model predicts the state in which the skill will terminate and the total reward received\nalong the way. More formally, a SMDP can be defined by a five-tuple < S,\u03a3, P,R, \u03b3 > where S is a set of states, \u03a3 is a set of skills, and P is the transition probability kernel. We assume rewards received at each timestep are bounded by [0, Rmax]. R : S \u00d7 \u03c3 \u2192 [0, Rmax1\u2212\u03b3 ] represents the expected discounted sum of rewards received during the execution of a skill \u03c3 initialized from a state s.\nSkill Policy: A skill policy \u00b5 : S \u2192 \u2206\u03a3 is a mapping from states to a probability distribution over skills \u03a3. The action-value function Q : S \u00d7 \u03a3 \u2192 R represents the longterm value of taking a skill \u03c3 \u2208 \u03a3 from a state s \u2208 S and thereafter always selecting skills according to policy \u00b5 and is defined by Q(s, \u03c3) = E[ \u2211\u221e t=0 \u03b3\ntRt|(s, \u03c3), \u00b5]. We denote the skill reward and transition probability as\nR\u03c3s = E[rt+1 + \u03b3rt+2 + \u00b7 \u00b7 \u00b7+ \u03b3k\u22121rt+k|st = s, \u03c3] ,\nP\u03c3s,s\u2032 = \u221e\u2211 j=0 \u03b3jPr[k = j, st+j = s \u2032|st = s, \u03c3]\nUnder these definitions the optimal skill value function is given by the following equation [Stolle and Precup, ] as:\nQ\u2217\u03a3(s, \u03c3) = E[R\u03c3s + \u03b3kmax \u03c3\u2032\u2208\u03a3 Q\u2217\u03a3(s \u2032, \u03c3\u2032)] . (2)"}, {"heading": "3 Hierarchical Deep RL Network", "text": "The Hierarchical Deep RL Network (H-DRLN) is a new architecture, based on the DQN, that facilitates skill reuse in lifelong learning. In this section, we provide an in-depth description of this network architecture as well as necessary modifications that we implemented in order to convert a vanilla DQN into its hierarchical counterpart.\nH-DRLN architecture: A diagram of this architecture is presented in Figure 2. Here, the outputs of the H-DRLN consist of primitive actions (E.g. Left (L), Right (R) and Forward (F)) as well as skills. The H-DRLN learns a policy that determines when to execute primitive actions and when to reuse pre-learned skills. The pre-learned skills are represented with deep networks and are referred to as Deep Skill Networks (DSNs). They are trained a-priori on various sub-tasks using the vanilla DQN algorithm and the regular Experience Replay (ER) detailed in Section 2.\nIf the H-DRLN chooses to execute a primitive action at at time t, then the action is executed for a single timestep. However, if the H-DRLN chooses to execute a skill \u03c3i (and therefore a DSN as shown in Figure 2), then the DSN executes its policy, \u03c0DSN (s) for a duration of ki timesteps and then gives control back to the H-DRLN.\nThis gives rise to two necessary modifications that we needed to make in order to incorporate skills into the learning procedure and generate a truly hierarchical deep network: (1) Optimize an objective function that incorporates skills; (2) Construct an ER that stores skill experiences.\nSkill Objective Function: As mentioned previously, a HDRLN extends the vanilla DQN architecture to learn control between primitive actions and skills. The H-DRLN loss function has the same structure as Equation 1, however instead of\nminimizing the standard Bellman equation, we minimize the Skill Bellman equation (Equation 2). More specifically, for a skill \u03c3t initiated in state st that has a time duration k, the H-DRLN target function is given by:\nyt =  \u2211k\u22121 j=0 [ \u03b3jrj+t ] if st+k is terminal\u2211k\u22121\nj=0\n[ \u03b3jrj+t ] + \u03b3kmax\n\u03c3\u2019 Q\u03b8target\n( st+k, \u03c3 \u2032 ) else ,\nensuring that temporal extension is preserved. Skill - Experience Replay: We extend the regular ER [Mnih et al., 2015] to incorporate skills and have termed this Skill Experience Replay (S-ER). There are two differences between the standard ER and our S-ER. Firstly, for each sampled skill tuple, we calculate the sum of discounted cumulative rewards generated whilst executing the skill and store this sum in the variable r\u0303. Second, since the skill is executed for k timesteps, we store the transition to state st+k rather than st+1. This yields the skill tuple (st, \u03c3t, r\u0303t, st+k) where \u03c3t is the skill executed at time t."}, {"heading": "4 Experiments", "text": "In order to solve new tasks as they are encountered in a lifelong learning scenario, the agent needs to be able to adapt to new game dynamics as well as reuse skills that it has learned from solving previous tasks. In our experiments, we show (1) the ability of the Minecraft agent to learn a DSN on a Minecraft task termed the one-room domain, shown in Figure 3. We then show (2) the ability of the agent to reuse this DSN to solve a new task, termed the two-room domain shown in Figure 5, by learning a Hierarchical Deep RL Network (H-DRLN) which incorporates this DSN as an action output. Finally, we show (3) the potential to transfer knowledge between related tasks without any additional learning.\nDeep Network Architecture - The deep network architecture used to represent the DSN and H-DRLN is the same as that of the vanilla DQN architecture [Mnih et al., 2015]. The\nH-DRLN however has a different Q-layer with a DSN as an output. State space - As in Mnih et al. (2015), the state space is represented as raw image pixels from the last four image frames which are combined and down-sampled into an 84\u00d784 image which is then vectorized. Actions - The action space for the DSN consists of three actions: (1) Move forward (F), (2) Rotate left by 30\u25e6 (L) and (3) Rotate right by 30\u25e6. The H-DRLN contains the same set of actions as well as the DSN as a fourth action output. Rewards - The agent gets a negative reward that is proportional to its distance to the goal, where the goal is to exit the room. In addition, upon reaching the goal the agent receives a large positive reward. Training - the agent learns in epochs. Each epoch starts from a random location in the domain and terminates after the agent makes 30 (60) steps in the one (two)-room domain. Evaluation - In all of the simulations, we evaluated the agent during training using the current learned architecture every 20k (5k) optimization steps. During evaluation, we averaged the agent\u2019s performance over 500 (1k) steps."}, {"heading": "4.1 Training a DSN", "text": "Our first experiment involved training a DSN in the one room domain (Figure 3). To do so we used the Vanilla DQN parameters that worked on the Atari domain [Mnih et al., 2015] as a starting point and then performed a grid search to find the optimal parameters for learning a DSN for the Minecraft oneroom domain. The best parameter settings that we found include: (1) a higher learning ratio (iterations between emulator states, n-replay = 16), (2) higher learning rate (learning rate = 0.0025) and (3) less exploration (eps endt - 400K). We implemented these modifications, since the standard Minecraft emulator 2 has a slow frame rate (approximately 400 ms per emulator timestep), and these modifications enabled the agent to increase its learning between game states. We also found that a smaller experience replay (replay memory - 100K) provided improved performance, probably due to our task having a relatively short time horizon (approximately 60 timesteps). The rest of the parameters from the Vanilla DQN remained unchanged, since Minecraft and Atari [Mnih et al., 2015] share relatively similar in-game screen resolutions.\n2https://github.com/h2r/burlapcraft\nFigure 4 presents the results of the learned DSN. As seen in the figure, the DSN learned to effectively solve the task - its average reward is increasing (orange) and the agent is able to solve the task with 100% success (blue) after approximately 31 epochs of training."}, {"heading": "4.2 Training a H-DRLN with a DSN", "text": "Using the DSN trained on the one-room domain, we incorporated it into the H-DRLN network architecture as an action output and learned to solve a new Minecraft sub-domain, the two-room domain presented in Figure 5. This domain consists of two-rooms, where the first room is shown in Figure 5a with its corresponding exit (Figure 5b). Note that the exit of the first room is not identical to the exit of the one-room domain. The second room contains a goal (Figure 5c) that is the same as the goal of the one-room domain (Figure 3b).\nSkill Reusability/Knowledge Transfer: We trained the H-DRLN architecture as well as the vanilla DQN on the tworoom domain. The average reward per epoch is shown in Figure 6. We noticed two important observations. (1) The H-DRLN architecture solves the task after a single epoch and generates significantly higher reward compared to the vanilla DQN as seen in the figure. This is because the H-DRLN makes use of knowledge transfer by reusing the DSN trained on the one-room domain to solve the two-room domain. This DSN is able to identify the exit of the first room (which is different from the exit on which the DSN was trained) and navigates the agent to this exit. The DSN is also able to navigate\nthe agent to the exit of the second room and completes the task. The DSN is a temporally extended action as it lasts for multiple timesteps and therefore increases the exploration of the RL agent enabling it to learn to solve the task faster than the vanilla DQN. A video 3 showing the performance of the Minecraft agent using the learned H-DRLN in the two room domain is available online. (2) The vanilla DQN fails to solve the task after 39 epochs. Since the domain contains ambiguous looking walls, the agent tends to get stuck in sub-optimal local minima. The agent completes the task approximately 80% of the time using the H-DRLN whereas it completes the task approximately 5% of the time using the vanilla DQN after 39 epochs.\nTemporal Extension combined with Primitive Actions: Figure 7 shows the action distribution of the agent\u2019s policy during training. We can see that the H-DRLN learns when to make use of temporal extension by reusing the DSN and when to use primitive actions. The DSN runs for 10 timesteps or terminates early if the agent reaches the goal. 10 timesteps is not sufficient to reach the exit of either room and therefore the agent needs to rely on primitive actions, as well as the DSN, in order to solve the given task.\nKnowledge Transfer without Learning: We then decided to evaluate the DSN (which we trained on the oneroom domain) in the two-room domain without performing any additional learning on this network. We found it surprising that the DSN, without any training on the two-room domain, generated a higher reward compared to the vanilla DQN which was specifically trained on the two-room domain for 39 epochs as shown in Figure 8. The DSN performance is not optimal compared to the H-DRLN architecture as seen in the figure but still manages to solve the two-room domain. This is an exciting result as it shows the potential for DSNs to identify and solve related tasks without performing any additional learning."}, {"heading": "5 Discussion", "text": "We have provided the first results for learning Deep Skill Networks (DSNs) in Minecraft, a lifelong learning domain. The\n3 https://www.youtube.com/watch?v= RwjfE4kc6j8\nDSNs are learned using a Minecraft-specific variation of the DQN [Mnih et al., 2015] algorithm. Our Minecraft agent also learns how to reuse these DSNs on new tasks by utilizing our novel Hierarchical Deep RL Network (H-DRLN) architecture. In addition, we show that the H-DRLN provides superior learning performance and faster convergence compared to the vanilla DQN, by making use of temporal extension [Sutton et al., 1999]. Our work can also be interpreted as a form of curriculum learning [Bengio et al., 2009] for RL. Here, we first train the network to solve relatively simple sub-tasks and then use the knowledge it obtained to solve the composite overall task. We also show the potential to perform knowledge transfer between related tasks without any additional learning. We see this work as a building block towards truly general lifelong learning using hierarchical RL and Deep Networks.\nRecently, it has been shown that Deep Networks tend to implicitly capture the hierarchical composition of a given task [Zahavy et al., 2016]. In future work we plan to utilize this implicit hierarchical composition to learn DSNs. In addition, it is possible to distill much of the knowledge from multiple teacher networks into a single student network [Parisotto et al., 2015; Rusu et al., 2015]. We wish to perform a similar technique as well as add auxiliary tasks to train the teacher networks (DSNs) [Suddarth and Kergosien, 1990], ultimately guiding learning in the student network (our H-DRLN)."}], "references": [{"title": "Online multi-task learning for policy gradient methods", "author": ["Haitham B Ammar", "Eric Eaton", "Paul Ruvolo", "Matthew Taylor"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Ammar et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Safe policy search for lifelong reinforcement learning with sublinear regret", "author": ["Haitham Bou Ammar", "Rasul Tutunov", "Eric Eaton"], "venue": "arXiv preprint arXiv:1505.05798,", "citeRegEx": "Ammar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACM Transactions on Intelligent Systems and Technology (TIST)", "author": ["Aijun Bai", "Feng Wu", "Xiaoping Chen. Online planning for large markov decision processes with hierarchical decomposition"], "venue": "6(4):45,", "citeRegEx": "Bai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "R\u00e9mi Munos"], "venue": "arXiv preprint arXiv:1512.04860,", "citeRegEx": "Bellemare et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the 26th annual international conference on machine learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston. Curriculum learning"], "venue": "pages 41\u201348. ACM,", "citeRegEx": "Bengio et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Pac-inspired option discovery in lifelong reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Brunskill and Li. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the Twenty Ninth International Conference on Machine Learning", "author": ["B.C. da Silva", "G.D. Konidaris", "A.G. Barto. Learning parameterized skills"], "venue": "June", "citeRegEx": "da Silva et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ella: An efficient lifelong learning algorithm", "author": ["Eric Eaton", "Paul L Ruvolo"], "venue": "Proceedings of the 30th international conference on machine learning (ICML-13), pages 507\u2013515,", "citeRegEx": "Eaton and Ruvolo. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Hauskrecht et al", "1998] Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive Partitions (ASAP)", "author": ["Daniel J. Mankowitz", "Timothy A. Mann", "Shie Mannor. Adaptive Skills"], "venue": "arXiv preprint arXiv:1602.03351,", "citeRegEx": "Mankowitz et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "author": ["Daniel J. Mankowitz", "Timothy A. Mann", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.03348,", "citeRegEx": "Mankowitz et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Scaling up approximate value iteration with options", "author": ["Mann", "Mannor", "2013] Timothy A. Mann", "Shie Mannor"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2013}, {"title": "et al", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540),", "citeRegEx": "Mnih et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.06342,", "citeRegEx": "Parisotto et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-time models for temporally abstract planning", "author": ["Doina Precup", "Richard S Sutton"], "venue": "Advances in Neural Information Processing Systems 10 (Proceedings of NIPS\u201997),", "citeRegEx": "Precup and Sutton. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Policy distillation", "author": ["Andrei A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "EMCL 2000 Proceedings of the 17th International Conference on Machine Learning", "author": ["Peter Stone", "Manuela Veloso", "Park Ave", "Florham Park. Layered Learning"], "venue": "1810(June):369\u2013381,", "citeRegEx": "Stone et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Adaptive Behavior", "author": ["Peter Stone", "Richard S Sutton", "Gregory Kuhlmann. Reinforcement learning for robocup soccer keepaway"], "venue": "13(3):165\u2013188,", "citeRegEx": "Stone et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "pages 120\u2013129", "author": ["Steven C Suddarth", "YL Kergosien. Rule-injection hints as a means of improving network performance", "learning time. In Neural Networks"], "venue": "Springer,", "citeRegEx": "Suddarth and Kergosien. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence, 112(1), August", "citeRegEx": "Sutton et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Lifelong robot learning", "author": ["Sebastian Thrun", "Tom M Mitchell"], "venue": "Springer,", "citeRegEx": "Thrun and Mitchell. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Van Hasselt et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02658,", "citeRegEx": "Zahavy et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Lifelong learning is defined as the ability to accumulate knowledge across multiple tasks and then reuse or transfer this knowledge in order to solve sub-sequent tasks [Eaton and Ruvolo, 2013].", "startOffset": 168, "endOffset": 192}, {"referenceID": 22, "context": "This is one of the fundamental learning problems in AI [Thrun and Mitchell, 1995; Eaton and Ruvolo, 2013].", "startOffset": 55, "endOffset": 105}, {"referenceID": 7, "context": "This is one of the fundamental learning problems in AI [Thrun and Mitchell, 1995; Eaton and Ruvolo, 2013].", "startOffset": 55, "endOffset": 105}, {"referenceID": 18, "context": "Many of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015].", "startOffset": 133, "endOffset": 191}, {"referenceID": 19, "context": "Many of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015].", "startOffset": 133, "endOffset": 191}, {"referenceID": 2, "context": "Many of the tasks that are encountered by an agent in a lifelong learning setting can be naturally decomposed into skill hierarchies [Stone et al., 2000; Stone et al., 2005; Bai et al., 2015].", "startOffset": 133, "endOffset": 191}, {"referenceID": 21, "context": "Reinforcement Learning (RL) provides a generalized approach to skill learning through the options framework [Sutton et al., 1999].", "startOffset": 108, "endOffset": 129}, {"referenceID": 6, "context": "Options are Temporally Extended Actions (TEAs) and are also referred to as skills [da Silva et al., 2012] and macro-actions [Hauskrecht et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 15, "context": "Options have been shown both theoretically [Precup and Sutton, 1997; Sutton et al., 1999] and experimentally [Mann and Mannor, 2013] to speed up the convergence rate of RL algorithms.", "startOffset": 43, "endOffset": 89}, {"referenceID": 21, "context": "Options have been shown both theoretically [Precup and Sutton, 1997; Sutton et al., 1999] and experimentally [Mann and Mannor, 2013] to speed up the convergence rate of RL algorithms.", "startOffset": 43, "endOffset": 89}, {"referenceID": 10, "context": "Recent work in RL has provided insights into learning reusable skills [Mankowitz et al., 2016a; Mankowitz et al., 2016b], but this has been limited to low dimensional problems.", "startOffset": 70, "endOffset": 120}, {"referenceID": 11, "context": "Recent work in RL has provided insights into learning reusable skills [Mankowitz et al., 2016a; Mankowitz et al., 2016b], but this has been limited to low dimensional problems.", "startOffset": 70, "endOffset": 120}, {"referenceID": 13, "context": "With the emergence of Deep Reinforcement Learning, specifically Deep QNetworks (DQNs)[Mnih et al., 2015], RL agents are now equipped with a powerful non-linear function approximator that can learn rich and complex policies.", "startOffset": 85, "endOffset": 104}, {"referenceID": 13, "context": "g Atari video games [Mnih et al., 2015]).", "startOffset": 20, "endOffset": 39}, {"referenceID": 23, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 17, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 24, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 3, "context": "While different variations of the DQN algorithm exist [Van Hasselt et al., 2015; Schaul et al., 2015; Wang et al., 2015; Bellemare et al., 2015], we will refer to the Vanilla DQN [Mnih et al.", "startOffset": 54, "endOffset": 144}, {"referenceID": 13, "context": ", 2015], we will refer to the Vanilla DQN [Mnih et al., 2015] unless otherwise stated.", "startOffset": 42, "endOffset": 61}, {"referenceID": 13, "context": "In our paper, we focus on learning reusable RL skills using Deep Q Networks [Mnih et al., 2015], by solving subproblems in Minecraft.", "startOffset": 76, "endOffset": 95}, {"referenceID": 13, "context": "Deep Q Networks: The DQN algorithm [Mnih et al., 2015] approximates the optimal Q function with a Convolutional Neural Network (CNN) [Krizhevsky et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 9, "context": ", 2015] approximates the optimal Q function with a Convolutional Neural Network (CNN) [Krizhevsky et al., 2012], by optimizing the network weights such that the expected Temporal Difference (TD) error of the optimal bellman equation (Equation 1) is minimized:", "startOffset": 86, "endOffset": 111}, {"referenceID": 21, "context": "Skills, Options, Macro-actions [Sutton et al., 1999]: A skill \u03c3 is a temporally extended control structure defined by a triple \u03c3 =< I, \u03c0, \u03b2 > where I is the set of states where the skill can be initiated, \u03c0 is the intra-skill policy, which determines how the skill behaves in encountered states, and \u03b2 is the set of termination probabilities determining when a skill will stop execution.", "startOffset": 31, "endOffset": 52}, {"referenceID": 13, "context": "Skill - Experience Replay: We extend the regular ER [Mnih et al., 2015] to incorporate skills and have termed this Skill Experience Replay (S-ER).", "startOffset": 52, "endOffset": 71}, {"referenceID": 13, "context": "Deep Network Architecture - The deep network architecture used to represent the DSN and H-DRLN is the same as that of the vanilla DQN architecture [Mnih et al., 2015].", "startOffset": 147, "endOffset": 166}, {"referenceID": 13, "context": "To do so we used the Vanilla DQN parameters that worked on the Atari domain [Mnih et al., 2015] as a starting point and then performed a grid search to find the optimal parameters for learning a DSN for the Minecraft oneroom domain.", "startOffset": 76, "endOffset": 95}, {"referenceID": 13, "context": "The rest of the parameters from the Vanilla DQN remained unchanged, since Minecraft and Atari [Mnih et al., 2015] share relatively similar in-game screen resolutions.", "startOffset": 94, "endOffset": 113}, {"referenceID": 13, "context": "DSNs are learned using a Minecraft-specific variation of the DQN [Mnih et al., 2015] algorithm.", "startOffset": 65, "endOffset": 84}, {"referenceID": 21, "context": "In addition, we show that the H-DRLN provides superior learning performance and faster convergence compared to the vanilla DQN, by making use of temporal extension [Sutton et al., 1999].", "startOffset": 164, "endOffset": 185}, {"referenceID": 4, "context": "Our work can also be interpreted as a form of curriculum learning [Bengio et al., 2009] for RL.", "startOffset": 66, "endOffset": 87}, {"referenceID": 25, "context": "Recently, it has been shown that Deep Networks tend to implicitly capture the hierarchical composition of a given task [Zahavy et al., 2016].", "startOffset": 119, "endOffset": 140}, {"referenceID": 14, "context": "In addition, it is possible to distill much of the knowledge from multiple teacher networks into a single student network [Parisotto et al., 2015; Rusu et al., 2015].", "startOffset": 122, "endOffset": 165}, {"referenceID": 16, "context": "In addition, it is possible to distill much of the knowledge from multiple teacher networks into a single student network [Parisotto et al., 2015; Rusu et al., 2015].", "startOffset": 122, "endOffset": 165}, {"referenceID": 20, "context": "We wish to perform a similar technique as well as add auxiliary tasks to train the teacher networks (DSNs) [Suddarth and Kergosien, 1990], ultimately guiding learning in the student network (our H-DRLN).", "startOffset": 107, "endOffset": 137}], "year": 2016, "abstractText": "The ability to reuse or transfer knowledge from one task to another in lifelong learning problems, such as Minecraft, is one of the major challenges faced in AI. Reusing knowledge across tasks is crucial to solving tasks efficiently with lower sample complexity. We provide a Reinforcement Learning agent with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action (also know as Options (Sutton et. al. 1999)). The agent learns reusable skills using Deep Q Networks (Mnih et. al. 2015) to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks (DSNs), are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. The H-DRLN is a hierarchical version of Deep QNetworks and learns to efficiently solve tasks by reusing knowledge from previously learned DSNs. The H-DRLN exhibits superior performance and lower learning sample complexity (by taking advantage of temporal extension) compared to the regular Deep Q Network (Mnih et. al. 2015) in subdomains of Minecraft. We also show the potential to transfer knowledge between related Minecraft tasks without any additional learning.", "creator": "LaTeX with hyperref package"}}}