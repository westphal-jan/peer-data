{"id": "1605.01459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach", "abstract": "in order to be effective teammates, robots need to be able to understand high - level human networks to produce, anticipate, and adapt to human motion. we have designed a new approach to enable robots to perceive human group motion in real - time, anticipate future actions, likewise synthesize their own motion accordingly. we explore this within the context of joint action, where humans and others move together synchronously. in this paper, we present an anticipation method which takes high - level group behavior into account. we validate the scenarios within a pilot - robot interaction scenario, where an autonomous mobile robot observes a minimum of human dancers, and then successfully and contingently coordinates its movements to \" execute the dance \". i compared the results of our anticipation solution to move the robot with your system which did not rely on high - level group behavior, and found our method performed better accordingly in terms of more successfully synchronizing the recipient's motion times the next, and also exhibiting easier contingent and fluent motion. these equations suggest that my collective performs actions when it has an understanding of high - level group behavior than when it does act. this work will otherwise enable others in the robotics community too build more fluent and adaptable robots in the future.", "histories": [["v1", "Wed, 4 May 2016 23:48:16 GMT  (1307kb,D)", "http://arxiv.org/abs/1605.01459v1", "11 pages, 7 figures, IEEE Transactions on Robotics 2016 preprint"]], "COMMENTS": "11 pages, 7 figures, IEEE Transactions on Robotics 2016 preprint", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["tariq iqbal", "samantha rack", "laurel d riek"], "accepted": false, "id": "1605.01459"}, "pdf": {"name": "1605.01459.pdf", "metadata": {"source": "CRF", "title": "Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach", "authors": ["Tariq Iqbal", "Samantha Rack", "Laurel D. Riek"], "emails": ["lriek}@nd.edu."], "sections": [{"heading": null, "text": "I. INTRODUCTION As technology advances, autonomous robots are becoming more involved in human society in a variety of roles. Robotic systems have long been involved in assembly lines automating and increasing efficiency of monotonous or dangerous factory procedures [1]. However, as robots leave controlled spaces and begin to work alongside people in teams, many things taken for granted in robotics concerning perception and action do not apply, as people act unpredictably, e.g., they \u201cbreak the rules\u201d when it comes to what a robot can expect a priori. In order for robots to effectively perform their tasks and integrate in Human Social Environments (HSEs), they must be able to comprehend high-level social signals and respond appropriately [2].\nWhile working alongside humans, a robot might encounter people performing various social actions, such as engaging in social activities, or performing synchronous movements [3]. For example, Ros et al. [4] used a humanoid robot to play the role of a dance instructor with children, and Fasola et al. [5] designed a socially assistive robot to engage older adults in physical exercise. Others have used robots to dance and play cooperatively with children in therapeutic settings [6], [7]. Koenemann et al.\nThe authors are from the Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, 46556 USA. e-mail: {tiqbal, srack, lriek}@nd.edu.\ndemonstrated a system which enabled humanoid robots to imitate complex human whole-body motion [8].\nHowever, sometimes it can be difficult for a robot to perceive and understand all of the different types of events involved during these activities to make effective decisions, due to sensor occlusion, unanticipated motion, narrow field of view, etc. On the other hand, if a robot is able to make better sense of its environment and understand highlevel group dynamics, then it can make effective decisions about its actions. If the robot has this understanding of its environment, then its interactions within the team might reach to a higher-level of coordination, resulting in a fluent meshing of actions [9]\u2013[12].\nHuman activity recognition from body movement is an active area of research across many fields [13]\u2013[17]. These activities involve a wide range of behaviors, from gross motor motion (e.g., walking, lifting) to manipulation (e.g., stacking objects). All of these experiments showed impressive results in recognizing activities, either performed by individual or dyad.\nHowever, the focus of most of these methods are to recognize the activity of a single human, rather than to understand the a team\u2019s dynamics and how it might affect behavior. This understanding is critical in humanrobot interaction scenarios, as the \u201cone human, one robot\u201d paradigm is rarely seen in ecological settings [18], [19]. To make informed decisions, robots need to understand this context [10].\nMany disciplines have investigated interaction dynamics within groups, which include sociology, psychology, biology, music and dance [20]\u2013[31]. For example, Nagy et al. [20], [21] investigated collective behavior on animals, and developed automated methods for assessing social dominance and leadership in domestic pigeons. Their investigation explored the effect of social hierarchical structure on dominance and leadership. Their results indicated that dominance and leadership hierarchical structures were independent from each other.\nInspired from bird flocks and fish schools, Leonard et al. [22], [23] investigated how collective group motion emerges when basic animal flocking rules (i.e., cohesive and repulsive element) are applied on a group of human dancers. Using tracked trajectories of head positions of individual dancers, the authors developed a time-varying graph-based method to infer conditions under which certain dancers emerged as the leaders of the group.\nSynchronous motion, or joint action, is a common type of high-level behavior encountered in human-human\nar X\niv :1\n60 5.\n01 45\n9v 1\n[ cs\n.R O\n] 4\nM ay\n2 01\n6\ninteraction. It is a naturally present social interaction that occurs when two or more participants coordinate their actions in space and time to make changes to the environment [32]. Understanding synchronous joint action is important, as it helps to accurately understand the affective behavior of a team, and also provides information regarding the group level cohesiveness [33], [34]. Thus, if a robot has the ability to understand the presence of synchronous joint action in a team, then it can use that information to inform its own actions to enable coordinated movement with the team. It also might learn advanced adaptive coordination techniques the human teams use, such as tempo adaptation or cross-training [1], [35].\nMany approaches have been taken by researchers across different fields to measure the degree of synchronization in continuous time series data, including recurrence analysis [36], correlation [37], and phase difference approaches [33]. Other sets of methods work across categorical time series data, which may define discrete events [38]. However, these event-based methods only consider a single type of event while measuring synchronization. To address this gap, we created an event-based method which can successfully take multiple types of discrete, task-level events into consideration while measuring the degree of synchronization of a system [39].\nRecent work in robotics has focused on developing predictive methods for improving the fluency of a joint interaction between a robot and one or more humans. For example, Hawkins et. al. [40] developed a method that determines an appropriate action for an assistive robot to take when providing parts during an assembly activity. They employ a probabilistic model that considers the presence of variability in the human\u2019s actions. Hoffman et al. [11] proposed an adaptive action selection mechanism for a robot, which could make anticipatory decisions based on confidence of their validity and their relative risks. Through a study, the authors validated the model and presented an improvement in task efficiency when compared to a purely reactive model.\nAdditionally, Pe\u0301rez-D\u2019Arpino et al. [41] proposed a data-driven approach to synthesize anticipatory knowledge of human motion, which they used to predict targets during reaching motions. Unhelkar et al. [42] extended this concept for a human-robot co-navigation task. This model used an \u201chuman turn signals\u201d during walking as anticipatory indicators, in order to predict human motion trajectories. This knowledge was then used for motion planning in simulated dynamic environments.\nWhile this work will improve the ability of robots to have fluent interactions within HSEs, most of these methods are best-suited for dyadic interaction and dexterous manipulation contexts. In contrast, we seek to explore methods for robots that will work robustly in groups, and also for tasks involving gross motion with mobile robots.\nIn our prior work, we explored the problem of automatically modelling and detecting synchronous joint action (SJA) in human teams, using both fixed and mobile sensors. We introduced a new, non-linear dynamical\nmethod which performed more accurately and robustly than existing methods [10], [39].\nIn this paper, we explore how a robot can use these models to synthesize SJA in order to coordinate its movements with a human team. The main contribution of this work is the introduction of a new method for anticipation of robot motion that takes human group dynamics into account. We validated our method within a human-robot interaction scenario, where an autonomous mobile robot observes a team of human dancers, and then successfully and contingently coordinates its movements to \u201cjoin the dance\u201d. We compared the results of our anticipation method with another method that does not rely on high-level group behavior. Our method performed better both in terms of more closely synchronizing the robot\u2019s motion to the team, and exhibiting more contingent and fluent motion.\nThe outline of the paper is as follows. Section II, describes the experimental testbed for studying SJA and the system architecture. Then, Section III provides details of the two anticipation methods. Section IV describes the experimental procedure. Sections V and VI discuss how the data were pre-proessed, and the experimental results. Finally, Section VII discusses the implication of these findings for the robotics community."}, {"heading": "II. SYSTEM ARCHITECTURE AND EXPERIMENTAL TESTBED", "text": "In order to explore how a robot can use human group dynamics to synthesize SJA with a mixed team, we needed an experimental testbed where a robot could perform tasks synchronously with humans. We also required a group activity where each member\u2019s actions would have impact on others\u2019 actions, as well as have impact on the dynamics of the group overall.\nTherefore, we designed a movement task where a team of humans and a robot could coordinate their motion in real-time. Specifically, we explored SJA within the context of synchronous dance. In concert with an experienced dancer, we choreographed a routine to the song Smooth Criminal by Michael Jackson, which is in 4/4 time. The dance is iterative, and performed cyclically in a counterclockwise manner (see Figure 1-A.) There are four iterations in a dance session, corresponding to each of the cardinal directions (North, West, South, and East). Each iteration includes the dancers taking the following steps in order: move forward and backward twice, then, clap, and turn 90-degrees (see Figure 2) [3]."}, {"heading": "A. Data Acquisition Process", "text": "Figure 1-A shows the data acquisition setup. Three human participants and a Turtlebot v.2 robot were arranged in two rows. Four Microsoft Kinect v.2 sensors were positioned approximately three feet above the ground at each of the cardinal directions. Each sensor was connected to a computer (client) to capture and process the depth, infrared, and skeletal data from the Kinect. All four clients and the server ran Windows 8 on an Intel Core i5 processor at 1.70Hz with 12GB of RAM.\nAs we are studying synchronous activity, it was critical all clients and the robot maintained a consistent time reference. Thus, we created a server to manage communication and global time synchronization. Synchronization architecture details can be found in Iqbal et al. [43].\nEach client performed real-time processing of the raw data in order to detect dance events (e.g., move forward, stop, etc), which it sent to the server with a timestamp. When the server received data from the clients, it generated predictions for how the robot should move using one of two anticipation methods, which are described in Section III. The server was also responsible for determining the active client, which refers to which of the four sensors the dancers were facing during a given iteration.\nIn order to allow for offline analysis, the clients also recorded time-synchronized depth, infrared, audio, and skeletal data using at automated interface with Kinect Studio. The server and robot also kept detailed logs of all communication, odometry information, events received from the active client, and information about the dancers."}, {"heading": "B. Client-side data processing", "text": "We extracted five high-level events from the participants\u2019 movements during the dance: start moving forward, stop moving forward, start moving backward, stop moving backward, and clap. The start moving forward event is detected when a participant begins approaching the Kinect, and stop moving forward when they stop moving. Similarly, as a participant moves away from the sensor (backward), that is identified as a start moving backward event, and when they stop, stop moving backward. We also detected participants\u2019 clap events, which occurred at the end of each iteration. See Figure 2.\nTo detect these events from participants\u2019 body movements, clients used the skeletal positions provided by the Kinect. Clients calculated forward and backward motion onsets along the z-axis primarily using the spine base body joint position, as it is the most stable and reliable joint position when participants are in motion.\nHowever, there were times when participants did not move their spine base, but did move their mid-spine, shoulders, or neck, to signal the onset of motion. Therefore, clients also used these positions, again along the z-axis, to detect four additional events: early start moving forward, early stop moving forward, early start moving backward, and early stop moving backward. For these early events, clients calculated joint change positions by comparing the current and previous frame. If at least half of the joint positions changed, then it indicated the participant had started moving. To detect clap events, clients used the x and y coordinates from the 3D skeletal position of the left and right hand and shoulder joints. Claps occurred when the ratio of the distance between the hands and the distance between the shoulder joints was less than a threshold (0.6), and when this ratio value reaches a local minima."}, {"heading": "C. Robot Command Generation and Execution", "text": "After the server determines which movement the robot should make, which it does using an anticipation methods described in III, it sends a movement command to the robot. These commands include: move forward, move backward, stop, and turn. The server translated the clap commands into rotation commands while sending it to the robot, since the robot can\u2019t clap.\nThe robot, which ran the Robot Operating System (ROS) version Hydro on Ubuntu version 12.04, accepted commands from the server, parsed the commands, and used an ROS publisher to send movement commands to the controller. The robot is capable of forward and backward movement, and can rotate on its vertical axis in either direction."}, {"heading": "III. EVENT ANTICIPATION METHODS", "text": "For this work, we created two anticipation methods to move the robot. The first method, synchronization-index based anticipation (SIA), is inspired by our prior SJA detection work [39]. It calculates the synchronicity of the group in real-time, determines who the most synchronous dancer is, and uses that information to move the robot. The second method, event cluster based anticipation (ECA), we created to establish a reasonable comparison anticipation method for SIA that does not rely on group dynamics. ECA is a straightforward method that involves averaging the times participants moved during a previous iteration of the dance. Figure 3 gives a visual comparison of how the two methods work in practice, and they are described textually below."}, {"heading": "A. Synchronization Index Based Anticipation (SIA)", "text": "The SIA method takes a group\u2019s internal dynamics into account when generating robot movements. The main idea is that for a given iteration, the participant who moves the most synchronously with the other dancers is a good model for the robot to follow in order to be well-coordinated with the team. Also, the method will adjust its identification of the most synchronous dancer after each iteration. Figure 3- B explains this method visually.\nThus, to generate future actions for the robot using this method, at the beginning of each iteration we measured the most synchronous person of the group using our the non-linear dynamical method we described in Iqbal and Riek [39]. We will briefly describe the method in Sections III-A1 and III-A2, and then discuss in Section III-A3 how we used the method to assess the most synchronous dancer to inform how the robot should move.\n1) Measuring synchronization of events across two time series: We can express the task-level events associated with each dancer as a time series. Suppose xn and yn are two time series, where n = 1 . . . N . Here, each time series has N samples. Suppose, mx and my are the number of events occuring in time series x and y respectively, and E is the set of all events [39].\nThe events of both series are denoted by ex(i) \u2208 E and ey(j) \u2208 E, where, i = 1 . . .mx, j = 1 . . .my . The event times on both time series are txi and t y j (i = 1 . . .mx, j = 1 . . .my) respectively [39]. In the case of synchronous events in both time series, the same event should appear roughly at the same time, or within a time lag \u00b1\u03c4 [39].\nNow, suppose c\u03c4 (x|y) denotes the number of times a single type of event e \u2208 E appear in time series x shortly after they appear in time series y. Here,\nc\u03c4 (x|y) = mx\u2211 i my\u2211 j J\u03c4ij (1)\nwhere,\nJ\u03c4ij =  1 if 0 < txi \u2212 t y j < \u03c4 1 2 if t x i = t y j\n0 otherwise (2)\nSimilarly, we can calculate c\u03c4 (y|x) denoting the number of times a single type of event e \u2208 E appear in time series y shortly after they appear in time series x.\nNow, Q\u03c4 (e) represents the synchronization of events in two time series, where we are only considering a single type of event e in both time series. From c\u03c4 (x|y) and c\u03c4 (y|x), we can calculate Q\u03c4 (e) as,\nQ\u03c4 (e) = c\u03c4 (x|y) + c\u03c4 (y|x) \u221a mxmy\n(3)\nThe value of Q\u03c4 (e) should be in between 0 and 1 (0 \u2264 Q\u03c4 (e) \u2264 1), as we normalize it by the number of events that appear in both time series. Q\u03c4 (e) = 1 shows that all the events of both time series are fully synchronized, and appeared within a time lag \u00b1\u03c4 on both time series. On the other hand, Q\u03c4 (e) = 0 shows us that the events are asynchronous [39].\nNow, we extend the notion of synchronization of events in two time series for multiple types of events. Suppose we have n types of events {e1, e2, . . . , en} \u2208 E(n), where E(n) is the set of all types of events. First, we calculate Q\u03c4 (ei) for each event type ei \u2208 E(n). While calculating Q\u03c4 (ei), we will not consider any other type of event, except ei [39].\nNow, let mx(ei) be the number of events of type ei occurring in time series x and my(ei) is the number of events of type ei occurring in time series y. To measure synchronization of multiple types of events between two time series, we take the average of Q\u03c4 (ei) for each event type ei, weighted by the number of events of that type. We will call this the synchronization index of that pair [39].\nSo, the overall synchronization of events in time series x and y of that pair is:\n\u2200ei \u2208 E(n) : Qxy\u03c4 = \u2211\n[Q\u03c4 (ei)\u00d7 [mx(ei) +my(ei)]]\u2211 [mx(ei) +my(ei)]\n(4)\nIf all events are synchronous in both time series, then the value of Qxy\u03c4 will be 1. If no synchronous are synchronous, the value of Qxy\u03c4 will be 0 [39].\n2) Measuring the individual synchronization index: We calculated the pairwise synchronization index for each pair. Suppose we have H number of time series. The time series data are represented as s1, s2, . . . , sH . First, we calculate the pairwise event synchronization index for each pair. So, we have the value of Qs1s2\u03c4 , Q s1s3 \u03c4 , . . . , Q s(H\u22121)sH \u03c4 [39].\nWe modified our process slightly from the description in Iqbal and Riek [39]. After calculating the pairwise synchronization index, we built a directed weighted graph from these indices, where each time series is represented by a vertex. However, in [39], after calculating the pairwise synchronization index, an undirected weighted graph was built. In a fully connected situation, the directed and the undirected graph represents the same connectivity.\nSo, if the time series are s1, s2, . . . , sH , then there is a vertex in the graph which will correspond to a time series. We connect a pair of vertices with a weighted edge, based on their synchronization index value. In this case, there will be an incoming and an outgoing edge for each pair of vertices. We will refer to this graph as the group topology graph (GTG) [39].\nThe individual synchronization index (I\u03c4 (si)) depends on both the group composition and the size of the group. We assumed that during this dance performance, each human participant may have some direct or indirect influences on the other human participants of the group [39]. I\u03c4 (si) for a participant is measured as the average of the weight of the outgoing edges to the corresponding vertex in the topology graph. So, the I\u03c4 (si) of series si is:\nI\u03c4 (si) =\n\u2211 j=1,...,H, j 6=iQ\nsisj \u03c4 \u00d7 f(si, sj)\u2211\nj=1,...,H, j 6=i f(si, sj) (5)\nWhere,\nf(si, sj) = { 1 iff edge(si, sj) \u2208 GTG 0 otherwise (6)\n3) Determining the most synchronous dancer and anticipating their next movement: The person with the highest individual synchronization index during an iteration is considered the most synchronous person of the group. This is because a high individual synchronization index indicates close synchronization with the other group members. Thus, let this person be MSP .\nSuppose, during itri, we determine MSP (itri) as the most synchronous dancer of the group. Now, assuming that a similar timing pattern of events will occur during the next iteration (itr(i+1)), if the robot follows the events of the MSP (itri), then the group will become more synchronous.\nWe can describe this concept mathematically. To reach a synchronous state, all events must occur very closely in time, i.e., within a time lag \u00b1\u03c4 . Thus, we want to minimize the difference between event timings for each pair of agents. Now, if \u2206tij represents the time difference of one event between agent i and j, then our goal is:\n\u2200i, j \u2208 H : min ( \u2211 \u2206tij) (7)\nNow for our scenario, as shown in Figure 4, suppose Dancer 2 was the most synchronous person during one iteration (itri) of the dance session, i.e., MSP (itri) was Dancer 2. Now, during itr(i+1), a similar timing pattern holds, and the timing of one particular event of the three dancers and the robot are t1, t2, t3, and tR respectively. To reach a synchronous state, the following is required:\nmin ( \u2211\n\u2206t12 + \u2206t23 + \u2206t1R\n+ \u2206tR3 + \u2206t13 + \u2206tR2) (8)\nAs Dancer 2 is the MSP , from Fig. 4, one can see \u2206t12 + \u2206t23 = \u2206t13, and \u2206t1R + \u2206tR3 = \u2206t13. Thus, Eq. 8 becomes:\nmin ( \u2211 \u2206t13 + \u2206t13 + \u2206t13 + \u2206tR2) (9)\nAs only the term \u2206tR2 depends on the robot\u2019s movement in Equation 9, by minimizing \u2206tR2 we can minimize the equation. Thus, if the robot and the Dancer 2 (in this case, the MSP ) perform the same event at the same time, then \u2206tR2 will become 0, which will minimize Equation 9. This implies that if the robot can perform the events close to the timings of the most synchronous person, then the whole group will reach a more synchronous state.\nThus, for a given iteration, itr \u2208 \u2200 iterations, the server will determine MSP (itri). Then, during the next iteration, itr(i+1), the server will track all movements of MSP (itri). The server then processes these information by utilizing the early detected events (early start moving forward, early stop moving forward, early start moving backward, and early stop moving backward) following the method described in the next paragraph.\nAs we know the timing of the events during the previous iteration of the dance itr(i\u22121), our anticipation method assumed that the similar event will happen more or less at the same time during this iteration itri.\nTherefore, when it was close to the timing of events of MSP (itr(i\u22121)) during itri, and the server received early detected events associated with MSP (itr(i\u22121)), then the server anticipated those events as the indicator of the start of a movement. The server then sent appropriate commands to the robot to perform that movement.\nFor example, suppose Dancer 2 was the most synchronous person during iteration 1, i.e., MSP (itr1) was Dancer 2. Dancer 2 performed a start moving forward event three seconds from the start of itr1. So, during\nitr2, it was assumed that the start moving forward would happen three seconds from the iteration\u2019s start. Thus, if the server received a sufficient number of early start moving forward events around t3, then it notified the robot command generator to generate commands to execute forward movement. This process was similar for all other regular events, excluding the clap event."}, {"heading": "B. Event Cluster-Based Anticipation Method (ECA)", "text": "We created the ECA method to establish a reasonable comparison anticipation method for SIA that does not rely on group dynamics. ECA is theoretically simple, but powerful in nature. As the dance is rhythmic and iterative in nature, the movement events for one iteration are similar to events that happened in the previous iteration.\nThus, we averaged the events timing during one iteration to predict the timing of those same events for the next iteration. Figure 3-A explains this method visually.\nFirst, for one iteration, we presented all the events associated with the dancers by a time series. Thus, this time series represented all the events of that iteration. Then, we clustered all the similar types of events together those happened within a time threshold, . For example, for a single event e, we calculated the timing of the event performed by three human participants, i.e., t(dancer1(itri), e), t(dancer2(itri), e), t(dancer3(itri), e). Here, t represents the timing of an event, and itri represents the iteration i.\nAfter that for each cluster of similar events, we calculated the average time of all the events and used that time as the timing of the event for the next iteration. These events and the times were the predicted events and timing for the next iteration of the dance. Thus, t(robot(itr(i+1)), e) = (t(dancer1(itri), e) + t(dancer2(itri), e)+t(dancer3(itri), e))/3. After the prediction of all the events for next iteration, the method sends a command to the robot command generator module to generate an appropriate movement for the robot."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Pilot Studies", "text": "Before performing the main experiment to compare performance between the two anticipation methods, we performed a series of pilot studies to test the setup of the system, and set various parameters for the two anticipation methods. We conducted two sets of pilot studies, with a total of seven participants (three women, four men). Participants were opportunistically recruited, and compensated with a $5 gift card for participating [3].\nDuring the first set of pilots, a sole participant danced with the robot. Here, we sought to measure two things: how fast the robot received action messages, and how accurately the robot performed with the human participant.\nDuring the second set of pilot studies, a group of three participants danced with the robot. Here, we sought to establish appropriate parameters for the anticipation methods. To acquire these measurements, we recorded events generated from server logs as well as from odometry data\nfrom the robot. We compared the two, noting differences in velocity, distance, and event timings [3].\nResults from the pilot study showed that the robot received messages from the server within a timely manner. We also analyzed the movement patterns of the robot when it coordinated its movements with the humans, and found it to be well-coordinated. Based on these data, we felt confident that the robot was moving synchronously with participants, and continued with the main experiment."}, {"heading": "B. Main experiment", "text": "We recruited a total of nine groups (27 participants in total, 3 persons per group) for our main experiment. 14 participants were women, 13 were men. Their average age was 22.93 years (s.d. = 3.98 years), and the majority were undergraduate and graduate students. Only 3 participants had prior dancing experience, 24 did not. Participants were recruited via mailing list and campus advertisement. Upon scheduling a timeslot, participants were randomly assigned to join a group with two others. Each participant was compensated with an $8 gift card for their time.\nAfter giving informed consent, participants viewed an instructional video of the choreographed dance and the experimenters explained the different movements. The participants then had time to practice the dance movements as a group as many times as they wanted. During this practice session, the robot did not dance with them.\nFollowing the practice session, the group participated in three dance sessions. During the first session, only humans participated in the dance. During the last two sessions, the robot joined the group. In Sessions 2 and 3, the robot moved using either ECA then SIA, or SIA then ECA. (The order was counter-balanced to avoid bias). Participants were blind as to which method was in use.\nDuring the last two sessions, the four clients recorded depth, infrared, and skeletal data of the participants, and the server logged all event and timing data. A single camera mounted on a tripod recorded standard video of the experiment for manual analysis purposes only.\nFollowing the experiment, participants completed a short questionnaire asking them to rate which of the two dance sessions they felt was more synchronous, a measure we have used in prior work [39]. Participants also reported which session they felt they were more synchronous with the rest of the group."}, {"heading": "V. ROBOT DATA PRE-PROCESSING", "text": "The server provided the human movement data logs and the clients raw data during the experiment, as detailed in Sections II and IV-B. However, to conduct a complete comparison between the two anticipation methods, it is also necessary to determine how and when the robot actually moved during the two experimental sessions. To do this, we used the timestamped odometric data from the robot (x and y pose and angular\u2212 z orientation), as well as the server-side robot command logs.\nWe calculated the same events for the robot as for the humans (forward, backward, stop, turn). Based on\nthe changes in two consecutive x or y pose values and the robot\u2019s heading, we calculated whether the robot was moving forward or backward. For example, when the robot faced the first Kinect sensor and moved forward, then the changes in two consecutive pose values would be positive, if moving backward, negative. We detected turn events using changes greater than 0.4 in the z heading value of the Turtlebot\u2019s angular twist message. (Note, turn events are considered equivalent to the humans\u2019 clap events in our analysis.).\nStop events were determined when a difference less than 0.002 was detected between two consecutive poses. These stop events were classified as forward or backward depending on the heading of the robot.\nAfter detecting all events for the robot, we manually checked the data files for any errors. During this process, we determined a 7% missing event rate. These missing events were evenly distributed across both of the anticipation methods. We manually checked the recorded video and odometric logs from the robot, and determined the robot actually moved correctly during the majority of those instances, so manually inserted the missing events into the files. There were a few instances (about 3.7% overall) when the robot did not perform the activity that it was instructed to perform, which was mostly due to network latency. We discarded those data from the analysis."}, {"heading": "VI. DATA ANALYSIS AND RESULTS", "text": "To compare the performance and accuracy of the two anticipation methods, we first measured how synchronously the entire group, including the robot, coordinated their movements during both sessions. We then measured how appropriately timed the robot\u2019s motion was with its human counterparts."}, {"heading": "A. Measuring Synchronization of the Group", "text": "Using the method described in [39] and discussed in Section III-A, we measured the degree of synchronization of the group for each iteration of the dance. First, we created individual time series for each of the dancers and the robot. Events in the time series were start moving forward, stop moving forward, start moving backward, stop moving backward, and clap). Then, we calculated the pairwise synchronization index for each pair using the method described in Section III-A2.\nFrom the pairwise synchronization index, we built a group topology graph (GTG) and calculated the individual synchronization index for each human dancer, as described in Section III-A. As the humans physically stood very close in proximity, we assumed that each of the group members was influenced by all other members of the group across the course of an entire dance session. (Every iteration, participants rotated their position, so a person in the front at itri will end up in the back by itr(i+2).) Thus, in the analysis every human was connected in the graph with all other members of the group, including the robot.\nWhen calculating the robot\u2019s individual synchronization index, we employed slightly different analyses between\nECA and SIA. For ECA, because the robot\u2019s motion was based on the average of all dancers\u2019 motions in the previous iteration, when building the GTG all edges from the robot connected to all other human group members. However, for SIA, at any given itri the robot was only ever following MSP (itr(i\u22121)) in real time. Thus, during itri the robot was only influenced by that person, not by the other group members. Thus, it is logical to take only the pairwise synchronization index between the robot and that person into account while calculating the individual synchronization index of the robot and building the GTG for that iteration. Therefore, we only considered an outgoing edge from the robot to MSP (itr(i\u22121)) in the GTG.\nAfter measuring the individual synchronization index, we calculated the group synchronization index for each group using the method described in [39]. Here, we describe the method very briefly.\nWhile calculating the group synchronization index, both the individual synchronization index as well as the members\u2019 connectivity to the group was taken into consider-\nation. For a given vertex in the GTG, the ratio of the number of outgoing edges connecting to it, and the number of maximum possible edges in a very synchronized condition for that vertex, is called the connectivity value (CV ). Thus we can define CV of series si as:\nCV (si) =\n\u2211 j=1,...,H, j 6=i f(si, sj)\nH \u2212 1 (10)\nThe CV represents how well an individual is synchronized with the rest of the group. First, we calculate each individual\u2019s synchronization index multiplied by their CV . Then, the overall group synchronization index is computed by taking the average of this product [39]. So, the overall group synchronization index, G\u03c4 , is computed by:\nG\u03c4 =\n\u2211 i=1,...,H I\u03c4 (si)\u00d7 CV (si)\nH (11)\nWhile calculating the group synchronization index, we used \u03c4 = 0.25s. This value means we considered two events synchronous when the same types of events in two time series occurred within 0.25 seconds of one another.\nTable I presents the group synchronization indices (GSI) for each group (three humans and one robot), across both anticipation methods (ECA and SIA), and across the four iterations per session). The table also presents the average GSI for each group in the rightmost column. Boldface is used to indicate which of the two methods yielded a higher GSI, and this is indicated for both the per-iteration GSI and the average GSI per group.\nFor 22 out of 36 total individual dance iterations, the SIA method yielded a higher GSI than the ECA method. And in 7 out of 9 trials, the SIA method yielded a higher GSI than the ECA method.1\nUsing a discrete analogue scale, we asked participants to rate on a scale from 1-5 how synchronous they thought the robot was with the other humans during the sessions. Based on their responses, we measured the more synchronous session of that trial, for which 2 out of 3 dancers agreed on their rating. For 7 / 9 trials, this collective rating matched with the more synchronous session of the trials determined by our method (See Table I, last two columns.)"}, {"heading": "B. Measuring Robot Timing Appropriateness", "text": "For both anticipation methods, we aimed to have the robot perform its actions (events) as close as possible in\n1Note, due to a small sample size (n = 36), it would be dubious to run statistical means comparisons, and one should not accept a p-value with certainty [44]. Instead, we agree with Gelman [44] that reliable patterns can be found by averaging, as reported here.\ntime to its human counterparts. Thus, we measured how close the robot\u2019s actual movement was to what the humans were doing at that time.\nThus, as a measure of timing appropriateness of the robot, we calculated the absolute time difference between the time when the robot performed an event, and the ideal timing of that event. As a measure of the ideal timing of an event, we took the average timing of an event performed by the humans. This measure is similar to the absolute offset measure used in [45], however, the timing appropriateness measure used here is within the context of a group.\nFirst, we represented all events associated with the humans during an iteration by a time series. Then, we clustered all the similar types of events together with those that were performed by the dancers within a time threshold, . For example, for a single event e, we calculated the timing of the event performed by three human participants within , i.e., t(dancer1, e), t(dancer2, e), t(dancer2, e). We also calculated the timing of that event performed by the robot, t(robot, e). Then, to calculate the ideal timing for the robot, we take the average of these times of this event performed by the humans. Thus, t(robotideal, e) = (t(dancer1, e) + t(dancer2, e) + t(dancer3, e))/3. Then, we calculated the timing appropriateness (TA) of that event performed by the robot as, TA(e) = |(t(robot, e)\u2212 t(robotideal, e))|. Figure 5 presents an example calculation of TA for event e.\nAfter calculating TA for each event during all the trials, we created two histograms, one for each anticipation method. We used a bin size of 0.1 seconds, starting at 0s and going to 2.5s. Then, we calculated the frequency of the events for which the TA falls within that time span.\nIn Figures 7-A and B, we present histograms representing the timing appropriateness measure, and the cumulative percentage of event frequencies, for the ECA and SIA methods respectively. Figure 7-A (ECA), shows that the robot was able to perform 81.88% of its events within 1.2s, and 90% of its events within 1.6s of the appropriate timing. Figure 7-B (SIA) shows that the robot performed 81.65% of the events within 0.8s, and 90.82% of the events within 1.2s of the appropriate timing.\nFigure 7-C presents the cumulative percentage of events for both methods together. One can find that the robot\nIEEE TRANSACTIONS ON ROBOTICS, VOL 32, ISSUE. 3, JUN 2016 9\n0\n10\n20\n30\n40\n50\n0 0.5 1 1.5 2 2.5\nE ve\nnt F\nre qu\nen cy\nBin Size (in seconds) Frequency Cumulative (%)\n0\n10\n20\n30\n40\n50\n0 0.5 1 1.5 2 2.5\nE ve\nnt F\nre qu\nen cy\nBin Size (in seconds) Frequency Cumulative (%)\n0%\n20%\n40%\n60%\n80%\n100%\n0 0.5 1 1.5 2 2.5\nC um\nul at\niv e\n(% )\nBin Size (in seconds) ECA Cumulative (%) SIA Cumulative (%)\nEv en\nt F re\nqu en\ncy\nBin Size (in seconds)\nC um\nul at\niv e\nPe rc\nen ta\nge\nBin Size (in seconds)\n0%\n20%\n40% 60% 80% 100%\n0 0.5 1 1.5 2 2.5\nC um\nul at iv e (% )\nBin Size (in seconds) ECA Cumulative (%) SIA Cumulative (%)\nBin Size (in seconds)\nECA Cumulative Percentage\n0%\n20% 40% 60% 80% 100%\n0 0.5 1 1.5 2 2.5\nC um ul at iv e (% )\nBin Size (in seconds) CA Cumulative (%) SIA Cumulative (%)SIA Cumulative Percentage Cumulative Distribution\nFig. 7. Event frequency distribution and the cumulative percentage distribution of the timing appropriateness measure for the two anticipation methods. SIA (left) and ECA (right). The rightmost graph shows the distribution of the timing appropriateness measure for both methods.\nperformed the events more appropriately during the SIA method, than compared to the ECA method.\nFor the SIA method, the mean for the timing appropriateness measure was 0.54s (s.d. = 0.59s) (See Figure 6). For the ECA method, the mean timing appropriateness measure was 0.70s (s.d. = 0.50s) (See Figure 6). While these data did not have a normal distribution, as is visible from the graph and a normality test, they did have a sufficient number of means to compare statistically. We conducted a Wicoxon Signed Rank Test, and found that the timing appropriateness values for the ECA method were significantly larger than for the SIA method, z = \u22124.399, p < 0.05, r = \u22120.18. This means that when using the SIA method, the robot moved more appropriately in time than when using the ECA method."}, {"heading": "VII. DISCUSSION AND FUTURE WORK", "text": "The results suggest that the human-robot team was more synchronous using SIA than using the ECA method. Moreover, when SIA was used, the robot was able to perform its actions significantly closer to the appropriate timing of the event. This supports the idea that SIA is wellsuited to provide movement coordination information to a robot during an SJA scenario.\nAdditionally, these results might support the robustness of the SIA method over the ECA method, as the SIA method is more dynamic and adaptable to change within the group. In our study, the SIA method chose the most synchronous dancer in the group, and used that to inform the robot\u2019s actions in real-time. However, relying on a method like ECA would mean that if a dancer was moving asynchronously within the group, the robot\u2019s motion could be adversely affected (as it is following everyone). SIA is robust to handle this phenomenon, as a person who performed asynchronous movements within the group is unlikely to ever be chosen as the most synchronous person.\nThis work shows that taking team dynamics into account can be useful for robots when conducting coordinated activities with teammates. This work can lead others in the robotics community towards further investigating the role of a group on behavior, rather than just focusing on individuals. This has implications not only for human-robot interaction, but also for multi-robot systems research. We are currently exploring the effect of different anticipation methods in multi-human multi-robot scenarios [46], [47].\nOne limitation of this work is how event detection is calculated. In the current setup, a predefined set of human\nactivities were detected by the system to understand the group dynamics. Building on this foundation, our future work will include incorporating human gross motion directly to the synchronization measurement step, instead of using pre-labelled events. Moreover, we are also planning to incorporate a decision module for robots, which will use the perceived knowledge to select the best decision from a set of options, based on the context [19], [48].\nAnother limitation of the current method is how it uses team metrics, and task-related information. For example, the method does not yet incorporate dancer expertise, nor does it factor in the tempo or dynamics of the music. In the future, we plan to incorporate an understanding of these factors. For example, in a team of novice dancers, a robot could perhaps keep a team on tempo.\nIn the future we also seek to explore the use of robotcentric vision and local coordination methods to calculate synchrony. This will enable robots to operate in more dynamic settings, and lessen the need for external sensors. However, incorporating local sensor data will be more challenging as it might be more noisy due to occlusion and local movements. However, we will build on our prior multimodal fusion and others\u2019 robot-centric perception work to overcome this challenge [19], [49].\nWe also will explore incorporating other synchronization methods humans employ, such as adapting to continuous tempo changes, within the SIA algorithm. Models like ADAM (ADaptation and Anticipation Model) have been proposed in the literature to computationally model this behavior in humans by combining adaptation and anticipation during an activity [35], [50]. It may be beneficial for a robot to have this ability both in human-robot and multi-robot teams. This integration might make the SIA algorithm more robust in anticipating, and synthesizing future activities more accurately.\nWe also hope to extend our methods to work beyond SJA activities, such as timed but varied collaborative tasks within industrial settings. A human-robot team working in an industrial setting has specific sequences of activities to perform overtime, some of which might be independent, and might not have to happen synchronously. However, the events do have to happen contingently; so some of our anticipatory methods may be applicable.\nMovement coordination is an important, emerging research area in robotics, neuroscience, biology, and many other fields [13]\u2013[17], [51]. Our work helps enable robots\nto have a better understanding of how to coordinate with the environment. This can be useful both for solving problems in robotics, and perhaps also in fields beyond."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Afzal Hossain, Olivia Choudhury, James Delaney, Cory Hayes, and Michael Gonzales for their assistance."}], "references": [{"title": "Optimization of temporal dynamics for adaptive human-robot interaction in assembly manufacturing,", "author": ["R. Wilcox", "S. Nikolaidis", "J.A. Shah"], "venue": "Proceedings of Robotics: Science and Systems (RSS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The social co-robotics problem space: Six key challenges,", "author": ["L.D. Riek"], "venue": "Proceedings of Robotics: Science, and Systems (RSS), Robotics Challenge and Visions,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Enabling synchronous joint action in human-robot teams,", "author": ["S. Rack", "T. Iqbal", "L.D. Riek"], "venue": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Adaptive human-robot interaction in sensorimotor task instruction: From human to robot dance tutors,", "author": ["R. Ros", "I. Baroni", "Y. Demiris"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A Socially Assistive Robot Exercise Coach for the Elderly,", "author": ["J. Fasola", "M. Mataric"], "venue": "Journal of Human-Robot Interaction,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A dancing robot for rhythmic social interaction,", "author": ["M.P. Michalowski", "S. Sabanovic", "H. Kozima"], "venue": "Proceedings of the ACM/IEEE International Conference on Human-robot interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Understanding a child\u2019s play for robot interaction by sequencing play primitives using hidden markov models,", "author": ["H.W. Park", "A.M. Howard"], "venue": "in Robotics and Automation (ICRA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Real-time imitation of human whole-body motions by humanoids,", "author": ["J. Koenemann", "F. Burget", "M. Bennewitz"], "venue": "in ICRA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Cooperative gestures: effective signaling for humanoid robots,", "author": ["L.D. Riek", "T.-C. Rabinowitch", "P. Bremner", "A.G. Pipe", "M. Fraser", "P. Robinson"], "venue": "Proceedings of the 5th ACM/IEEE International Conference on Human-Robot Interaction,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Joint action perception to enable fluent human-robot teamwork,", "author": ["T. Iqbal", "M.J. Gonzales", "L.D. Riek"], "venue": "Robot and Human Interactive Communication (RO-MAN),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Cost-based anticipatory action selection for human\u2013robot fluency,", "author": ["G. Hoffman", "C. Breazeal"], "venue": "Robotics, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Using spatial and temporal contrast for fluent robot-human handovers,", "author": ["M. Cakmak", "S.S. Srinivasa", "M.K. Lee", "S. Kiesler", "J. Forlizzi"], "venue": "Proceedings of the 6th International conference on Human-robot interaction", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A Self-Training Approach for Visual Tracking and Recognition of Complex Human Activity Patterns,", "author": ["J. Bandouch", "O.C. Jenkins", "M. Beetz"], "venue": "International Journal of Computer Vision, vol. 99,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "4-Dimensional Local Spatio-Temporal Features for Human Activity Recognition,", "author": ["H. Zhang", "L.E. Parker"], "venue": "Intelligent robots and systems (IROS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation,", "author": ["H. Koppula", "A. Saxena"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Berkeley MHAD: A comprehensive Multimodal Human Action Database,", "author": ["F. Ofli", "R. Chaudhry", "G. Kurillo", "R. Vidal", "R. Bajcsy"], "venue": "Applications of Computer Vision (WACV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Experiences with an interactive museum tour-guide robot,", "author": ["W. Burgard", "A.B. Cremers", "D. Fox", "D. H\u00e4hnel", "G. Lakemeyer", "D. Schulz", "W. Steiner", "S. Thrun"], "venue": "Artificial intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Social context perception for mobile robots,", "author": ["A. Nigam", "L. Riek"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Hierarchical group dynamics in pigeon flocks,", "author": ["M. Nagy", "Z. \u00c1kos", "D. Biro", "T. Vicsek"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "In the dance studio: Analysis of human flocking,", "author": ["N.E. Leonard", "G. Young", "K. Hochgraf", "D. Swain", "A. Trippe", "W. Chen", "S. Marshall"], "venue": "American Control Conference (ACC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "In the Dance Studio: An Art and Engineering Exploration of Human Flocking,", "author": ["N.E. Leonard", "G.F. Young", "K. Hochgraf", "D.T. Swain", "A. Trippe", "W. Chen", "K. Fitch", "S. Marshall"], "venue": "Controls and Art,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "In time with the music: the concept of entrainment and its significance for ethnomusicology.", "author": ["M. Clayton", "R. Sager", "U. Will"], "venue": "European meetings in ethnomusicology.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Dynamics of interpersonal coordination,", "author": ["R.C. Schmidt", "M.J. Richardson"], "venue": "Coordination: Neural, behavioral and social dynamics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Nonlinear multivariate analysis of neurophysiological signals,", "author": ["E. Pereda", "R.Q. Quiroga", "J. Bhattacharya"], "venue": "Progress in neurobiology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Role of tempo entrainment in psychophysiological differentiation of happy and sad music?", "author": ["S. Khalfa", "M. Roy", "P. Rainville", "S. Dalla Bella", "I. Peretz"], "venue": "International Journal of Psychophysiology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Learning and synchronising dance movements in south african songs: Cross-cultural motioncapture study,", "author": ["T. Himberg", "M.R. Thompson"], "venue": "Dance Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Movement synchrony and perceived entitativity,", "author": ["D. Lakens"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Synchronization in human musical rhythms and mutually interacting complex systems,", "author": ["H. Hennig"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Joint action: bodies and minds moving together.", "author": ["N. Sebanz", "H. Bekkering", "G. Knoblich"], "venue": "Trends in cognitive sciences,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "Measuring group synchrony: a cluster-phase method for analyzing multivariate movement time-series,", "author": ["M.J. Richardson", "R.L. Garcia", "T.D. Frank", "M. Gergor", "K.L. Marsh"], "venue": "Frontiers in Physiology,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "A System for Real-Time Multimodal Analysis of Nonverbal Affective Social Interaction in User-Centric Media,", "author": ["G. Varni", "G. Volpe", "A. Camurri"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Sensorimotor synchronization with tempo-changing auditory sequences: Modeling temporal adaptation and anticipation,", "author": ["M.M. van der Steen", "N. Jacoby", "M.T. Fairhurst", "P.E. Keller"], "venue": "Brain research,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Recurrence plots for the analysis of complex systems,", "author": ["N. Marwan", "M. Carmenromano", "M. Thiel", "J. Kurths"], "venue": "Physics Reports, vol. 438,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Event synchronization: a simple and fast method to measure synchronicity and time delay patterns,", "author": ["R.Q. Quiroga", "T. Kreuz", "P. Grassberger"], "venue": "Physical review E,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "A Method for Automatic Detection of Psychomotor Entrainment,", "author": ["T. Iqbal", "L.D. Riek"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Anticipating human actions for collaboration in the presence of task and sensor uncertainty,", "author": ["K.P. Hawkins", "S. Bansal", "N.N. Vo", "A.F. Bobick"], "venue": "in Robotics and Automation (ICRA),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Fast target prediction of human reaching motion for cooperative human-robot manipulation tasks using time series classification,", "author": ["C. P\u00e9rez-DArpino", "J. Shah"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Human-robot co-navigation using anticipatory indicators of human walking motion,", "author": ["V.V. Unhelkar", "C. P\u00e9rez-DArpino", "L. Stirling", "J. Shah"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "A Model for Time- Synchronized Sensing and Motion to Support Human-Robot Fluency,", "author": ["T. Iqbal", "M.J. Gonzales", "L.D. Riek"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI), Workshop on Timing in HRI,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Commentary: P values and statistical practice,", "author": ["A. Gelman"], "venue": "Epidemiology, vol. 24,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Synchronization in human-robot Musicianship,", "author": ["G. Hoffman", "G. Weinberg"], "venue": "19th International Symposium in Robot and Human Interactive Communication,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Detecting and Synthesizing Synchronous Joint Action in Human-Robot Teams,", "author": ["T. Iqbal", "L.D. Riek"], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Coordination Dynamics in Multi-human Multi-robot Teams,", "author": ["T. Iqbal", "L.D. Riek"], "venue": "Under Review,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Detecting social context: A method for social event classification using naturalistic multimodal data,", "author": ["M. O\u2019Connor", "L. Riek"], "venue": "Automatic Face and Gesture Recognition (FG),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "First-Person Activity Recognition: What Are They Doing to Me?", "author": ["M.S. Ryoo", "L. Matthies"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "The ADaptation and Anticipation Model (ADAM) of sensorimotor synchronization,", "author": ["M.C.M. van der Steen", "P.E. Keller"], "venue": "Frontiers in Human Neuroscience,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Ant groups optimally amplify the effect of transiently informed individuals,", "author": ["A. Gelblum", "I. Pinkoviezky", "E. Fonio", "A. Ghosh", "N. Gov", "O. Feinerman"], "venue": "Nature communications,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Robotic systems have long been involved in assembly lines automating and increasing efficiency of monotonous or dangerous factory procedures [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "appropriately [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "counter people performing various social actions, such as engaging in social activities, or performing synchronous movements [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "[4] used a humanoid robot to play the role of a dance instructor with children, and Fasola et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] designed a socially assistive robot to engage older adults in physical exercise.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "have used robots to dance and play cooperatively with children in therapeutic settings [6], [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "have used robots to dance and play cooperatively with children in therapeutic settings [6], [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "demonstrated a system which enabled humanoid robots to imitate complex human whole-body motion [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "If the robot has this understanding of its environment, then its interactions within the team might reach to a higher-level of coordination, resulting in a fluent meshing of actions [9]\u2013[12].", "startOffset": 182, "endOffset": 185}, {"referenceID": 11, "context": "If the robot has this understanding of its environment, then its interactions within the team might reach to a higher-level of coordination, resulting in a fluent meshing of actions [9]\u2013[12].", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Human activity recognition from body movement is an active area of research across many fields [13]\u2013[17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "This understanding is critical in humanrobot interaction scenarios, as the \u201cone human, one robot\u201d paradigm is rarely seen in ecological settings [18], [19].", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "This understanding is critical in humanrobot interaction scenarios, as the \u201cone human, one robot\u201d paradigm is rarely seen in ecological settings [18], [19].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "To make informed decisions, robots need to understand this context [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "Many disciplines have investigated interaction dynamics within groups, which include sociology, psychology, biology, music and dance [20]\u2013[31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "Many disciplines have investigated interaction dynamics within groups, which include sociology, psychology, biology, music and dance [20]\u2013[31].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "[20], [21] investigated collective behavior on animals, and developed automated methods for assessing social dominance and leadership in domestic pigeons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22], [23] investigated how collective group motion emerges when basic animal flocking rules (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22], [23] investigated how collective group motion emerges when basic animal flocking rules (i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "that occurs when two or more participants coordinate their actions in space and time to make changes to the environment [32].", "startOffset": 120, "endOffset": 124}, {"referenceID": 29, "context": "Understanding synchronous joint action is important, as it helps to accurately understand the affective behavior of a team, and also provides information regarding the group level cohesiveness [33], [34].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "Understanding synchronous joint action is important, as it helps to accurately understand the affective behavior of a team, and also provides information regarding the group level cohesiveness [33], [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "It also might learn advanced adaptive coordination techniques the human teams use, such as tempo adaptation or cross-training [1], [35].", "startOffset": 126, "endOffset": 129}, {"referenceID": 31, "context": "It also might learn advanced adaptive coordination techniques the human teams use, such as tempo adaptation or cross-training [1], [35].", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "Many approaches have been taken by researchers across different fields to measure the degree of synchronization in continuous time series data, including recurrence analysis [36], correlation [37], and phase difference approaches [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": "Many approaches have been taken by researchers across different fields to measure the degree of synchronization in continuous time series data, including recurrence analysis [36], correlation [37], and phase difference approaches [33].", "startOffset": 230, "endOffset": 234}, {"referenceID": 33, "context": "Other sets of methods work across categorical time series data, which may define discrete events [38].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "To address this gap, we created an event-based method which can successfully take multiple types of discrete, task-level events into consideration while measuring the degree of synchronization of a system [39].", "startOffset": 205, "endOffset": 209}, {"referenceID": 35, "context": "[40] developed a method that determines an appropriate action for an assistive robot to take when providing parts during an assembly activity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed an adaptive action selection mechanism for a robot, which could make anticipatory decisions based on confidence of their validity and their relative risks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[41] proposed a data-driven approach to synthesize anticipatory knowledge of human motion, which they used to predict targets during reaching motions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[42] extended this concept for a human-robot co-navigation task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "method which performed more accurately and robustly than existing methods [10], [39].", "startOffset": 74, "endOffset": 78}, {"referenceID": 34, "context": "method which performed more accurately and robustly than existing methods [10], [39].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "Each iteration includes the dancers taking the following steps in order: move forward and backward twice, then, clap, and turn 90-degrees (see Figure 2) [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 38, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "The first method, synchronization-index based anticipation (SIA), is inspired by our prior SJA detection work [39].", "startOffset": 110, "endOffset": 114}, {"referenceID": 34, "context": "Thus, to generate future actions for the robot using this method, at the beginning of each iteration we measured the most synchronous person of the group using our the non-linear dynamical method we described in Iqbal and Riek [39].", "startOffset": 227, "endOffset": 231}, {"referenceID": 34, "context": "Suppose, mx and my are the number of events occuring in time series x and y respectively, and E is the set of all events [39].", "startOffset": 121, "endOffset": 125}, {"referenceID": 34, "context": "my) respectively [39].", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "In the case of synchronous events in both time series, the same event should appear roughly at the same time, or within a time lag \u00b1\u03c4 [39].", "startOffset": 134, "endOffset": 138}, {"referenceID": 34, "context": "On the other hand, Q\u03c4 (e) = 0 shows us that the events are asynchronous [39].", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "While calculating Q\u03c4 (ei), we will not consider any other type of event, except ei [39].", "startOffset": 83, "endOffset": 87}, {"referenceID": 34, "context": "We will call this the synchronization index of that pair [39].", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "If no synchronous are synchronous, the value of Q \u03c4 will be 0 [39].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": ", Q s(H\u22121)sH \u03c4 [39].", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "We modified our process slightly from the description in Iqbal and Riek [39].", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "However, in [39], after calculating the pairwise synchronization index, an undirected weighted graph was built.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "We will refer to this graph as the group topology graph (GTG) [39].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "We assumed that during this dance performance, each human participant may have some direct or indirect influences on the other human participants of the group [39].", "startOffset": 159, "endOffset": 163}, {"referenceID": 2, "context": "Participants were opportunistically recruited, and compensated with a $5 gift card for participating [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "We compared the two, noting differences in velocity, distance, and event timings [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 34, "context": "Following the experiment, participants completed a short questionnaire asking them to rate which of the two dance sessions they felt was more synchronous, a measure we have used in prior work [39].", "startOffset": 192, "endOffset": 196}, {"referenceID": 34, "context": "Using the method described in [39] and discussed in Section III-A, we measured the degree of synchronization", "startOffset": 30, "endOffset": 34}, {"referenceID": 34, "context": "we calculated the group synchronization index for each group using the method described in [39].", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "Then, the overall group synchronization index is computed by taking the average of this product [39].", "startOffset": 96, "endOffset": 100}, {"referenceID": 39, "context": "1Note, due to a small sample size (n = 36), it would be dubious to run statistical means comparisons, and one should not accept a p-value with certainty [44].", "startOffset": 153, "endOffset": 157}, {"referenceID": 39, "context": "Instead, we agree with Gelman [44] that reliable patterns can be found by averaging, as reported here.", "startOffset": 30, "endOffset": 34}, {"referenceID": 40, "context": "This measure is similar to the absolute offset measure used in [45], however, the timing appropriateness measure used here is within the context of a group.", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "We are currently exploring the effect of different anticipation methods in multi-human multi-robot scenarios [46], [47].", "startOffset": 109, "endOffset": 113}, {"referenceID": 42, "context": "We are currently exploring the effect of different anticipation methods in multi-human multi-robot scenarios [46], [47].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "Moreover, we are also planning to incorporate a decision module for robots, which will use the perceived knowledge to select the best decision from a set of options, based on the context [19], [48].", "startOffset": 187, "endOffset": 191}, {"referenceID": 43, "context": "Moreover, we are also planning to incorporate a decision module for robots, which will use the perceived knowledge to select the best decision from a set of options, based on the context [19], [48].", "startOffset": 193, "endOffset": 197}, {"referenceID": 17, "context": "However, we will build on our prior multimodal fusion and others\u2019 robot-centric perception work to overcome this challenge [19], [49].", "startOffset": 123, "endOffset": 127}, {"referenceID": 44, "context": "However, we will build on our prior multimodal fusion and others\u2019 robot-centric perception work to overcome this challenge [19], [49].", "startOffset": 129, "endOffset": 133}, {"referenceID": 31, "context": "Models like ADAM (ADaptation and Anticipation Model) have been proposed in the literature to computationally model this behavior in humans by combining adaptation and anticipation during an activity [35], [50].", "startOffset": 199, "endOffset": 203}, {"referenceID": 45, "context": "Models like ADAM (ADaptation and Anticipation Model) have been proposed in the literature to computationally model this behavior in humans by combining adaptation and anticipation during an activity [35], [50].", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "Movement coordination is an important, emerging research area in robotics, neuroscience, biology, and many other fields [13]\u2013[17], [51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 46, "context": "Movement coordination is an important, emerging research area in robotics, neuroscience, biology, and many other fields [13]\u2013[17], [51].", "startOffset": 131, "endOffset": 135}], "year": 2016, "abstractText": "In order to be effective teammates, robots need to be able to understand high-level human behavior to recognize, anticipate, and adapt to human motion. We have designed a new approach to enable robots to perceive human group motion in real-time, anticipate future actions, and synthesize their own motion accordingly. We explore this within the context of joint action, where humans and robots move together synchronously. In this paper, we present an anticipation method which takes high-level group behavior into account. We validate the method within a human-robot interaction scenario, where an autonomous mobile robot observes a team of human dancers, and then successfully and contingently coordinates its movements to \u201cjoin the dance\u201d. We compared the results of our anticipation method to move the robot with another method which did not rely on high-level group behavior, and found our method performed better both in terms of more closely synchronizing the robot\u2019s motion to the team, and also exhibiting more contingent and fluent motion. These findings suggest that the robot performs better when it has an understanding of high-level group behavior than when it does not. This work will help enable others in the robotics community to build more fluent and adaptable robots in the future.", "creator": "LaTeX with hyperref package"}}}