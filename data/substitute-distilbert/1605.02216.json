{"id": "1605.02216", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Distributed stochastic optimization for deep learning (thesis)", "abstract": "we study the problem of how to distribute the training of large - scale deep learning models in the parallel computing environment. we propose a new distributed stochastic optimization method like elastic averaging sgd ( easgd ). we analyze the convergence rate of an easgd method in the synchronous scenario and compare exponential stability condition. the constrained admm method in supervised round - robin scheme. an asynchronous and momentum variant of the easgd method is applied directly train complex convolutional neural networks for image classification on the cifar and apache datasets. our approach accelerates the training procedure furthermore achieves better test outputs. it also requires a much smaller amount upon communication before other simpler baseline approaches such as the downpour loop.", "histories": [["v1", "Sat, 7 May 2016 16:55:22 GMT  (4615kb,D)", "http://arxiv.org/abs/1605.02216v1", "This is the author's thesis at under supervision of Yann LeCun. Part of the results are based on the paperarXiv:1412.6651in collaboration with Anna Choromanska and Yann LeCun"]], "COMMENTS": "This is the author's thesis at under supervision of Yann LeCun. Part of the results are based on the paperarXiv:1412.6651in collaboration with Anna Choromanska and Yann LeCun", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sixin zhang"], "accepted": false, "id": "1605.02216"}, "pdf": {"name": "1605.02216.pdf", "metadata": {"source": "CRF", "title": "Distributed stochastic optimization for deep learning", "authors": ["Yann LeCun", "Minjie Wang", "Zhaoguo Wang"], "emails": [], "sections": [{"heading": null, "text": "Distributed stochastic optimization for deep learning\nby\nSixin Zhang\nA dissertation submitted in partial fulfillment\nof the requirements for the degree of\nDoctor of Philosophy\nDepartment of Computer Science\nNew York University\nMay, 2016\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\nYann LeCun\nar X\niv :1\n60 5.\n02 21\n6v 1\n[ cs\n.L G\n] 7\nM ay\n2 01\n6"}, {"heading": "Dedication", "text": ""}, {"heading": "To my parents and grandparents", "text": "ii"}, {"heading": "Acknowledgements", "text": "Almost all the thesis start with a thank you to the thesis advisor, why this is so. Maybe it is the time. The time one spends most with during the PhD. The time the impact from whom will last for long on shaping one\u2019s flavor of the research. My advisor, Yann LeCun, is of no double having a big influence on me. To some extent, the way we interact is like the Elastic Averaging SGD (EASGD) method that we have named together. During my whole PhD, I am very grateful to have the freedom to explore various research subjects. He always has patience to wait, even though sometimes there is no interesting results. It is an experiment. Sometimes, the good result is only one step away, and he has a good feeling to sense that. Sometimes, he is also very strict. Remember when I was preparing the thesis proposal, but was proposing too many directions. Everyone was worried, as the deadline was approaching. To focus and be quick. A lot of the time, the way how we think decides the question we ask and where we go. My advisor\u2019s conceptual way of thinking is still to me a mystery of arts and science.\nI would also like to thank Anna Choromanska for being my collaborator and for helping me at many critical points. When we started to work on the EASGD method, it was not very clear why we gave it a new name. The answer only became sharp when Anna questioned me countlessly why we do what we do. Luckily, we have discovered interesting answers. Chapter 2, Chapter 3 and Chapter 4 of this thesis are based on our joint work.\nChapter 5 is inspired by the discussion with Professor Weinan E, and his students Qianxiao Li and Cheng Tai during my visit to Princeton University. We were trying to use stochastic differential equation to analyze EASGD method. The idea is to get the maximum insight with minimum effort. The results in Chapter 5 share a similar flavor.\nChapter 6 would not come into shape without the helpful discussions with Professor Jinyang Li, and her students Russell Power, Minjie Wang, Zhaoguo Wang, Christopher Mitchell and Yang Zhang. The design and the implementation of the EASGD and EASGD Tree are guided by their valuable experience. Also the numerical results would\niii\nnot be obtained without their feedback, as well as the consistent support of the NYU HPC team, in particular Shenglong Wang.\nI would also like to give special thanks to Professor Rob Fergus, Margaret Wright, and David Sontag for serving in my thesis committee, as well as being my teacher during my PhD studies. Margaret was so eager to read my thesis and asked when do you expect your thesis to converge whenever I have a new version. Rob was so glad to see me when I first came to the CBLL lab, while David is always sharing his knowledge with us during the CBLL talks.\nI also learn a lot from my earlier collaborators, in particular Marco Scoffier, Tom Schaul and Wan Li. Arthur Szlam, Camille Couprie, Clement Farabet, David Eigen, Dilip Krishnan, Joan Bruna, Koray Kavukcuoglu, Matt Zeiler, Olivier Henaff, Pablo Sprechmann, Pierre Sermanet, Ross Goroshin, Xiang Zhang and Y-Lan Boureau are always very mind-refreshing to be around.\nI also appreciate the feedback from many senior researchers on the EASGD method. In particular, Mark Tygert, Samy Bengio, Patrick Combettes, Peter Richtarik, Yoshua Bengio and Yuri Bakhtin.\nThe coach of the NYU running club, Mike Galvan, kicks me up to practice at 6:30am every Monday morning during my last year of the PhD. It gives me a consistency energy to keep running and to finish my PhD.\nAlmost half of my PhD time is spent at the building of Courant Institute. I have learned a lot of mathematics which is still so tremendous to me. The more you write, the less you see. There are a lot of inspiring stories that I would like to write, but maybe I should keep them in mind for the moment.\niv"}, {"heading": "Abstract", "text": "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method.\nWe then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. We also find a surprising connection between the momentum SGD and the EASGD method with a negative moving average rate. A non-convex case is also studied to understand when EASGD can get trapped by a saddle point.\nFinally, we scale up the EASGD method by using a tree structured network topology. We show empirically its advantage and challenge. We also establish a connection between the EASGD and the DOWNPOUR method with the classical Jacobi and the Gauss-Seidel method, thus unifying a class of distributed stochastic optimization methods.\nv"}, {"heading": "Contents", "text": ""}, {"heading": "Dedication ii", "text": ""}, {"heading": "Acknowledgements iv", "text": ""}, {"heading": "Abstract v", "text": ""}, {"heading": "List of Figures viii", "text": ""}, {"heading": "List of Tables xvi", "text": ""}, {"heading": "1 Introduction 1", "text": "1.1 What is the problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Formalizing the problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 An overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Elastic Averaging SGD (EASGD) 9", "text": "2.1 Synchronous EASGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Asynchronous EASGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3 Momentum EASGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14"}, {"heading": "3 Convergence Analysis of EASGD 17", "text": "3.1 Quadratic case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.1.1 One-dimensional case . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.1.2 Generalization to multidimensional case . . . . . . . . . . . . . . . 26\n3.2 Strongly convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.3 Stability of EASGD and ADMM . . . . . . . . . . . . . . . . . . . . . . . 37\nvi"}, {"heading": "4 Performance in Deep Learning 43", "text": "4.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.2 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.3 Further discussion and understanding . . . . . . . . . . . . . . . . . . . . 59\n4.3.1 Comparison of SGD, ASGD, MVASGD and MSGD . . . . . . . . 59 4.3.2 Dependence of the learning rate . . . . . . . . . . . . . . . . . . . . 62 4.3.3 Dependence of the communication period . . . . . . . . . . . . . . 62 4.3.4 The tradeoff between data and parameter communication . . . . . 64 4.3.5 Time speed-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.4 Additional pseudo-codes of the algorithms . . . . . . . . . . . . . . . . . . 67"}, {"heading": "5 The Limit in Speedup 71", "text": "5.1 Additive noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n5.1.1 SGD with mini-batch . . . . . . . . . . . . . . . . . . . . . . . . . 72 5.1.2 Momentum SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.1.3 EASGD and EAMSGD . . . . . . . . . . . . . . . . . . . . . . . . 78\n5.2 Multiplicative noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n5.2.1 SGD with mini-batch . . . . . . . . . . . . . . . . . . . . . . . . . 89 5.2.2 Momentum SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 5.2.3 EASGD and EAMSGD . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.3 A non-convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109"}, {"heading": "6 Scaling up Elastic Averaging SGD 112", "text": "6.1 EASGD Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n6.1.1 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 6.1.2 The result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.2 Unifying EASGD and DOWNPOUR . . . . . . . . . . . . . . . . . . . . . 130"}, {"heading": "7 Conclusion 134", "text": "Bibliography 138\nvii"}, {"heading": "List of Figures", "text": "2.1 The big picture of EASGD. . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1 Theoretical mean squared error (MSE) of the center x\u0303 in the quadratic\ncase, with various choices of the learning rate \u03b7 (horizontal within each block), and the moving rate \u03b2 = p\u03b1 (vertical within each block), the number of processors p = {1, 10, 100, 1000, 10000} (vertical across blocks), and the time steps t = {1, 2, 10, 100,\u221e} (horizontal across blocks). The MSE is plotted in log scale, ranging from 10\u22123 to 103 (from deep blue to red). The dark red (i.e. on the upper-right corners) indicates divergence. . 22\n3.2 The largest absolute eigenvalue of the linear map F = F p3 \u25e6F p2 \u25e6F p1 \u25e6 . . . \u25e6\nF 13 \u25e6F 12 \u25e6F 11 as a function of \u03b7 \u2208 (0, 10\u22122) and \u03c1 \u2208 (0, 10) when p = 3 and p = 8. To simulate the chaotic behavior of the ADMM algorithm, one may pick \u03b7 = 0.001 and \u03c1 = 2.5 and initialize the state s0 either randomly or with \u03bbi0 = 0, x i 0 = x\u03030 = 1000, \u2200i = 1, . . . , p. Figure should be read in color. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.3 Instability of ADMM in the round-robin scheme. Pick p = 3, \u03b7 = 0.001,\n\u03c1 = 2.5 and initialize the state s0 with \u03bb i 0 = 0, x i 0 = x\u03030 = 1000,\u2200i = 1, . . . , p. The x-axis is the time step t, the y-axis is the (one-dimensional) value of the center variable x\u0303t. . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.1 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period \u03c4 = 1 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 49\nviii\n4.2 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period \u03c4 = 4 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 50\n4.3 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period \u03c4 = 16 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 51\n4.4 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period \u03c4 = 64 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 52\n4.5 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 4 for parallel methods on CIFAR with the 7-layer convolutional neural network. . . . . . . . . . 53\n4.6 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 8 for parallel methods on CIFAR with the 7-layer convolutional neural network. . . . . . . . . . 54\n4.7 Training and test loss and the test error for the center variable versus\na wallclock time with the number of local workers p = 16 for parallel methods on CIFAR with the 7-layer convolutional neural network. . . . . 55\n4.8 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 4 on ImageNet with the 11-layer convolutional neural network. . . . . . . . . . . . . . . . . . . 56\n4.9 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 8 on ImageNet with the 11-layer convolutional neural network. . . . . . . . . . . . . . . . . . . 57\n4.10 Convergence of the training and test loss (negative log-likelihood) and\nthe test error (original and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment. . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nix\n4.11 Convergence of the training and test loss (negative log-likelihood) and\nthe test error (original and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment. . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.12 Convergence of the training loss (negative log-likelihood, original) and\nthe test error (zoomed) computed for the center variable as a function of wallclock time for EAMSGD and EASGD run with different values of \u03b7 on the CIFAR experiment. p = 16, \u03c4 = 10. . . . . . . . . . . . . . . . . . 63\n4.13 Convergence of the training loss (negative log-likelihood, original) and\nthe test error (zoomed) computed for the center variable as a function of wallclock time for EASGD and EAMSGD (p = 16, \u03b7 = 0.01, \u03b2 = 0.9, \u03b4 = 0.99) on the CIFAR experiment with various communication period \u03c4 and learning rate decay \u03b3. The learning rate is decreased gradually over time based each local worker\u2019s own clock t with \u03b7t = \u03b7/(1 + \u03b3t) 0.5. . . . . . . . 65\n4.14 The wall clock time needed to achieve the same level of the test error\nthr as a function of the number of local workers p on the CIFAR dataset. From left to right: thr = {21%, 20%, 19%, 18%}. Missing bars denote that the method never achieved specified level of test error. . . . . . . . . . . . 68\n4.15 The wall clock time needed to achieve the same level of the test error thr\nas a function of the number of local workers p on the ImageNet dataset. From left to right: thr = {49%, 47%, 45%, 43%}. Missing bars denote that the method never achieved specified level of test error. . . . . . . . . . . . 69\n5.1 The largest absolute eigenvalue of the matrix M (sp(M)) in Equation 5.6\nas a function of the learning rate \u03b7 \u2208 (0, 2) and the momentum rate \u03b4 \u2208 (\u22121, 1). h = 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 The largest absolute eigenvalue of the matrix M in Equation 5.12 as a\nfunction of the learning rate \u03b7 \u2208 (0, 2) and the moving rate \u03b1 \u2208 (\u22121, 1). h = 1 and \u03b2 = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\nx\n5.3 Three independent simulations of EASGD using the elastic averaging \u03b1 =\n\u03b2/p and the optimal \u03b1 given in Equation 5.17. The x-axis is the time step t in the EASGD updates of Equation 5.9. The y-axis is the squared distance of the center variable to the optimum zero, i.e. x\u03032t . We have chosen h = 1, \u03c3 = 10\u22122, p = 4, \u03b7 = 0.1 and \u03b2 = 0.9. . . . . . . . . . . . . 83\n5.4 The absolute value of the eigenvalues of z1, z2 and z3 in Equation 5.19.\n\u03b7h = 0.1 and \u03b2 = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.5 The absolute value of the eigenvalues of z1, z2 and z3 in Equation 5.19.\n\u03b7h = 1.5 and \u03b2 = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.6 The largest absolute eigenvalue of the matrix Mp in Equation 5.18 as a\nfunction of the learning rate \u03b7 \u2208 (0, 2) and the moving rate \u03b1 \u2208 (\u22121, 1). h = 1 and \u03b2 = 0.9. Note that we computed this spectrum using p = 2, as we have discussed in the text, it is independent of the choice of p for p > 1. 86\n5.7 Three independent simulations of EASGD using the elastic averaging \u03b1 =\n\u03b2/p and the optimal \u03b1 given in Equation 5.17. The x-axis is the time step t in the EASGD updates of Equation 5.9. The y-axis is the squared distance of the center variable to the optimum zero, i.e. x\u03032t . We have chosen h = 1, \u03c3 = 10\u22122, p = 4, \u03b7 = 1.5 and \u03b2 = 0.9. . . . . . . . . . . . . 87\n5.8 The largest absolute eigenvalue of the matrix Mp in Equation 5.20 as a\nfunction of the learning rate \u03b7 \u2208 (0, 2) and the moving rate \u03b1 \u2208 (\u22121, 1). h = 1, \u03b2 = 0.9 and \u03b4 = 0.99. Note that we computed this spectrum again using p = 2, as we have discussed in the text, it is independent of the choice of p for p > 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n5.9 The probability density function of the Gamma distribution \u0393(\u03bb, \u03c9). The\nx-axis and y-axis are both in log-scale to enlarge the singularity at zero and the decay of the tail toward infinity. . . . . . . . . . . . . . . . . . . . 93\n5.10 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the momentum rate \u03b4 \u2208 (\u22121, 1). \u03bb = 0.5, \u03c9 = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\nxi\n5.11 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the momentum rate \u03b4 \u2208 (\u22121, 1). \u03bb = 1, \u03c9 = 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.12 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the momentum rate \u03b4 \u2208 (\u22121, 1). \u03bb = 2, \u03c9 = 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n5.13 The largest absolute eigenvalue of the matrix M in Equation 5.30 as\na function of the momentum rate \u03b4 \u2208 (\u22121, 1) at \u03b7 = \u03bb\u03c9+1 . (\u03bb, \u03c9) \u2208 {(0.5, 0.5), (1, 1), (2, 2)}. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.14 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the input Gamma distribution \u0393(\u03bb, \u03c9) in the range \u03c9 \u2208 (0, 100) and the \u03bb \u2208 (0, 100). (\u03b7, \u03b4) \u2208 {(1, 0), (0.1, 0), (0.1, 0.9)}. . . . . . . . . . . . 100\n5.15 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64]. \u03bb = 0.5, \u03c9 = 0.5, \u03b2 = 0.9, \u03b1 = \u03b2/p. . . . . . . . . . . . . . . . . . . 103\n5.16 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64]. \u03bb = 1, \u03c9 = 1, \u03b2 = 0.9, \u03b1 = \u03b2/p. . . . . . . . . . . . . . . . . . . . . 104\n5.17 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64]. \u03bb = 2, \u03c9 = 2, \u03b2 = 0.9, \u03b1 = \u03b2/p. . . . . . . . . . . . . . . . . . . . . 105\n5.18 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate \u03b7 \u2208 (0, 2) and the number of workers p \u2208 [1, 64]. \u03bb = 10, \u03c9 = 10, \u03b2 = 0.9, \u03b1 = \u03b2/p. The minimal sp(M) = 0.0868 is achieved at p = 29 and \u03b7 = 0.8929. . . . . . . . . . . . . . . . . . . . . . 106\n5.19 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate \u03b7 \u2208 (0, 1) and the moving rate \u03b1 \u2208 (\u22121, 1). \u03bb = 0.5, \u03c9 = 0.5, \u03b2 = 0.9, p = 100. The minimal sp(M) = 0.5024 is achieved at \u03b7 = 0.4343 and \u03b1 = 0.2525. . . . . . . . . . . . . . . . . . . . 108\nxii\n5.20 The smallest eigenvalue of the Hessian matrix H in Equation 5.38 as a\nfunction of the penalty term \u03c1, evaluated at the critical point x = \u221a 1\u2212 \u03c1, y = \u2212\u221a1\u2212 \u03c1, z = 0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.1 The behavior of EASGD Tree in Algorithm 6. . . . . . . . . . . . . . . . . 115 6.2 The two communication schemes of the EASGD Tree. . . . . . . . . . . . 116 6.3 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith \u03c41 = 10 and \u03c42 = 100. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 3, \u03b1 = 0.9/(d+ 1). . . . . . . . . . . . . . . . . 120\n6.4 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith \u03c4u = 8 and \u03c4d = 80. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 3, \u03b1 = 0.9/(d+ 1). . . . . . . . . . . . . . . . . 121\n6.5 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith \u03c41 = 1 and \u03c42 = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 2, \u03b1 = 0.9/(d+ 1). The momentum rate \u03b4 = 0. . 122\n6.6 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith \u03c41 = 1 and \u03c42 = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 3, \u03b1 = 0.9/(d+ 1). The momentum rate \u03b4 = 0.9. 123\nxiii\n6.7 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith \u03c41 = 1 and \u03c42 = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 4, \u03b1 = 0.9/(d+ 1). The momentum rate \u03b4 = 0.99. 124\n6.8 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith \u03c4u = 1 and \u03c4d = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 2, \u03b1 = 0.9/(d+ 1). The momentum rate \u03b4 = 0. . 125\n6.9 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith \u03c4u = 1 and \u03c4d = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 3, \u03b1 = 0.9/(d+ 1). The momentum rate \u03b4 = 0.9. 126\n6.10 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith \u03c4u = 1 and \u03c4d = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, \u03b7 = 5e\u2212 4, \u03b1 = 0.9/(d+ 1). The momentum rate \u03b4 = 0.99. 127\n6.11 EASGD Tree on CIFAR-lowrank using the first and second communica-\ntion scheme with momentum. Training loss and error, test loss and error of the root node versus a wallclock time. The curve e in Figure 6.5, the curve e in Figure 6.6, the curve a in Figure 6.7, the curve b in Figure 6.8, the curve b in Figure 6.9 and the curve d in Figure 6.10 are selected and plotted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nxiv\n6.12 Best test performance of DOWNPOUR (p=16), EASGD (p=16), and\nEASGD Tree (p=256) on CIFAR-lowrank. Training loss and error, test loss and error (of the center variable or the root node) versus a wallclock time. The momentum is not applied (\u03b4 = 0). . . . . . . . . . . . . . . . . 129\nxv"}, {"heading": "List of Tables", "text": "4.1 Learning rates explored for each method shown in Figure 4.1, 4.2, 4.3\nand 4.4 (CIFAR experiment). . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.2 Learning rates explored for each method shown in Figure 4.5, 4.6 and 4.7\n(CIFAR experiment). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3 Learning rates explored for each method shown in Figure 4.8 and 4.9\n(ImageNet experiment). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.4 Approximate computation time, data loading time and parameter com-\nmunication time [sec] for DOWNPOUR (top line for \u03c4 = 1) and EASGD (the time breakdown for EAMSGD is almost identical) (bottom line for \u03c4 = 10). Left time corresponds to CIFAR experiment and right table corresponds to ImageNet experiment. The computation and the communication time may have some overlap, due to the MPI implementation. . . 66\nxvi\nChapter 1"}, {"heading": "Introduction", "text": ""}, {"heading": "1.1 What is the problem", "text": "The subject of this thesis is on how to parallelize the training of large deep learning models that use a form of stochastic gradient descent (SGD) [9].\nThe classical SGD method processes each data point sequentially on a single processor. As an optimization method, it often exhibits fast initial convergence toward the local optimum as compared to the batch gradient method [30]. As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].\nHowever, as the size of the dataset explodes [22], the amount of time taken to go through each data point sequentially becomes prohibitive. To meet this challenge, many distributed stochastic optimization methods have been proposed in literature, e.g. the mini-batch SGD, the asynchronous SGD, and the ADMM -based methods.\nMeanwhile, the scale and the complexity of the deep learning models is also growing to adapt to the growth of the data. There have been attempts to parallelize the training for large-scale deep learning models on thousands of CPUs, including the Google\u2019s Distbelief system [16]. But practical image recognition systems consist of large-scale convolutional\n1\nneural networks trained on a few GPU cards sitting in a single computer [27, 49]. The main challenge is to devise parallel SGD algorithms to train large-scale deep learning models that yield a significant speedup when run on multiple GPU cards across multiple machines. To date, the AlphaGo system is trained using 50 GPUs for a few weeks [52].\nTo solve such large-scale optimization problem, one line of research is to fill-in the gap between the stochastic gradient descent method (SGD) and the batch gradient method [40]. The batch gradient method evaluates the gradient (in a batch) using all the data points while the stochastic gradient descent method uses only a single data point to estimate the gradient of the objective function. The mini-batch SGD, i.e. sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31]. Larger mini-batch size reduces the variance of the stochastic gradient. Consequently, one can use a larger learning rate to gain a faster convergence in training. However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15]. It is even observed that in deep learning problems, using too large mini-batch size may lead to solutions of very poor test accuracy [62].\nAnother possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16]. The idea is similar to the mini-batch SGD, i.e. to distribute the computation of the gradients to the local workers (processors) and collect them by the master (a parameter server), but asynchronously. The advantage of using asynchronous communication is that it allows local workers to communicate with the master at different time intervals, and thus it can significantly reduce the waiting time spent on the synchronization. The tradeoff is that asynchronous behavior results in large communication delay, which can in turn slow down the convergence rate [62].\nThe DOWNPOUR method belongs to the above class of asynchronous SGD methods, and is proposed for training deep learning models [16] . The main ingredient of the DOWNPOUR method is to reduce the (gradient) communication overhead by running\n2\nthe SGD method on each local worker for multiple steps instead of just one single step. This idea resembles the incremental gradient method [6]. The merit is that each local worker can spend more time on the computation than on the communication. The disadvantage, however, is that the method is not very stable when the (gradient) communication period is large. We shall discuss this phenomenon further in Chapter 4.\nThe mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26]. However, conceptually it is still centralized as there is a master server which is needed to collect all the gradients and store the latest model parameter. There\u2019s a class of the distributed optimization methods which is conceptually decentralized. It is based on the idea of consensus averaging [57]. A classical problem is to compute the average value of the local clocks in a sensor network (aka. clock synchronization) [41]. In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].\nThe ADMM (Alternating Direction Method of Multipliers) [11] method can also be used to solve the consensus averaging problem above. The basic idea is to decompose the objective function into several smaller ones so that each one can be solved separately in parallel and then be combined into one solution. In some sense, it is more effective because the dual Lagrangian update is used to close the primal-dual gap associated with the consensus constraints (i.e. to reach the consensus). ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61]. Nevertheless, the consensus can be harder to reach, as the oscillations from the stochastic sampling and the asynchronous behavior need to be absorbed (averaged out) by the whole system (for the constraints to be satisfied).\nIn this thesis, we explore another dimension of such possibility. Unlike the mini-batch SGD and the asynchronous SGD method, we would like to maintain the stochastic nature (oscillation) of the SGD method as the number of workers grows. To accelerate\n3\nthe training of the large-scale deep learning models, in particular under communication constraints, we study instead a weak consensus reaching problem. We use a central variable to average out the noise, but we only maintain a weak coupling (consensus) between the local variables and the central variable. The goal is thus very different to the consensus averaging method and the ADMM method."}, {"heading": "1.2 Formalizing the problem", "text": "Consider minimizing a function F (x) in a parallel computing environment [7] with p \u2208 N workers and a master. In this thesis we focus on the stochastic optimization problem of the following form\nmin x F (x) := E[f(x, \u03be)], (1.1)\nwhere x is the model parameter to be estimated and \u03be is a random variable that follows the probability distribution P over \u2126 such that F (x) = \u222b\n\u2126 f(x, \u03be)P(d\u03be). The optimization\nproblem in Equation 1.1 can be reformulated as follows\nmin x1,...,xp,x\u0303 p\u2211 i=1 E[f(xi, \u03bei)] + \u03c1 2 \u2016xi \u2212 x\u0303\u20162, (1.2)\nwhere each \u03bei follows the same distribution P (thus we assume each worker can sample the entire dataset). In this thesis we refer to xi\u2019s as local variables and we refer to x\u0303 as a center variable.\nThe problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11]. The quadratic penalty term \u03c1 in Equation 1.2 is expected to ensure that local workers will not fall into different attractors that are far away from the center variable.\nWe will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48]. The problem of data communication\n4\nwhen the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work. We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [13].\nWe remark further a connection between the quadratic penalty term in Equation 1.2 and the Moreau-Yosida regularization [33] in convex optimization used to smooth the non-smooth part of the objective function. When using the rectified linear units [35] in deep learning models, the objective function also becomes non-smooth. This suggests that it may indeed be a good idea to use such a quadratic penalty term."}, {"heading": "1.3 An overview", "text": "We now give an overview of the main focus and results of the following chapters.\nChapter 2 introduces the Elastic Averaging SGD (EASGD) method. We first discuss the basic idea and motivate EASGD. We then propose synchronous EASGD and discuss its connection with the classical Jacobi method in the numerical analysis. We further propose an asynchronous extension of EASGD and discuss how the asynchronous EASGD can be thought of as an perturbation of the synchronous EASGD. We end up with combing EASGD with the classical momentum method, notably the Nesterov \u2019s momentum, giving the EAMSGD algorithm.\nChapter 3 provides the convergence analysis of the synchronous EASGD algorithm. The analysis is focused on the convergence of the center variable. We discuss a onedimensional quadratic objective function first. We show that the variance of the center variable tends to zero as the number of workers grows. Meanwhile, the variance of the local variables keeps increasing. We also introduce a double averaging sequence computing the time average of the center variable and show that it is asymptotically optimal. We further extend our analysis to the strongly convex case, and discusses the\n5\ntightness of the bound we have obtained compared to the quadratic case. Finally, we study the stability condition of EASGD and ADMM method in the round-robin scheme. We compute numerically the stability region for the ADMM method and show that it can be unstable when the quadratic penalty term is relatively small. This is in contrast to the stability region of the EASGD method.\nChapter 4 is an empirical study on the performance of various serial and parallel stochastic optimization methods for training deep convolutional neural networks on the CIFAR and ImageNet dataset. We first describe the experimental setup, in particular how we preprocess and sample the dataset by the local workers in parallel. We then compare the performance of EASGD and its momentum variant EAMSGD with the SGD and DOWNPOUR methods (including their averaging and momentum variants). We find that EASGD is very robust when the communication period is large, which is not the case for DOWNPOUR. Furthermore, EAMSGD (the combination of EASGD and Nesterov\u2019s momentum method) achieves the best performance measured both in the wallclock time and in the smallest achievable test error. The smallest achievable test error is further improved as we gradually increase the communication period and the number of processors. We provide further empirical results on the dependency of the learning rate and the communication period. To choose a proper communication period, we also give an explicit analysis in terms of the bandwidth requirement for the data communication and the parameter communication in the ImageNet case.\nChapter 5 studies the limit in speedup of various stochastic optimization methods. We would like to know what would be the limit in theory if we were given an infinite amount of processors. We study first the asymptotic phase of these methods using an additive noise model (a one-dimensional quadratic objective with Gaussian noise). The asymptotic variance can be obtained explicitly, and it can be reduced by either the mini-batch SGD or the EASGD method. However, the SGD method using momentum can strictly increase this asymptotic variance. On the other hand, we seek the optimal momentum rate such that the momentum SGD method converges the fastest. We find that this op-\n6\ntimal momentum rate can either be positive or negative. We ask a similar question for the EASGD method by fixing the moving rate of the center variable and then optimize the moving rate of the local variable. We find that the optimal moving (average) rate of the EASGD method is either zero or negative. The surprising connection between these two results is that they are obtained based on a nearly same proof. We perform a similar moment analysis on the EAMSGD method, and find (but only numerically) that the optimal moving rate can be either positive or negative, depending on the choice of the learning rate.\nWe then move on to study the initial phase of these methods based on a multiplicative noise model. We introduce the Gamma distribution to parametrize the spread the input data distribution. We obtain the optimal learning rate for the mini-batch SGD method, and discuss how fast the optimal rate of convergence varies with the mini-batch size. We find that if the input data distribution has a large spread (to be made precise in Chapter 5), then one can gain more effective speedup by using mini-batch SGD. For the momentum SGD method, we observe that using momentum can slow down the optimal convergence rate, but it can accelerate the convergence when the learning rate is chosen to be sub-optimal. For the EASGD method, we observe a quite different picture to the mini-batch SGD. There is an optimal number of workers that EASGD method will achieve the best convergence rate. We also perform an asymptotic analysis when the number of workers is infinite, and show that the stability region can still be enlarged if the spread of the input data distribution is large.\nWe finally discuss a non-convex case to understand when EASGD can get trapped by a saddle point. This is the phenomenon that we have observed in Chapter 4 when the communication period is too large. We find that if the quadratic penalty term is smaller than a critical value, then the local variables can stay on both sides of a saddle point, and it is a stable configuration. This suggests that EASGD can spend a lot of time in such configuration if the coupling between the master and the workers is too weak.\n7\nChapter 6 attempts to scale up the EASGD method to a larger number of processors. Due to the communication constraints, we propose a tree extension of the EASGD method. The leaf node of the tree performs the gradient descent locally and from time to time performs the elastic averaging with their parent. The root node of the tree tracks the spatial average of the variable of its children, and in turn the average of all the leaf nodes. We perform an empirical study with two different communication schemes. The first scheme exploits the fact that faster communication can be achieved at the bottom layer (between the leaf nodes and their parent) than the upper layers. The second scheme uses a faster upward communication rate and a slower downward communication rate so that the root node can be informed of the latest information from the bottom as quick as possible. We observe that the first scheme gives better training speedup, while the second scheme gives better test accuracy. One difference compared to the asynchronous EASGD experiment in Chapter 4 is that the communication protocol between the tree nodes is fully asynchronous so as to maximize the I/O throughput.\nWe end up the Chapter 6 by establishing a connection between the DONWPOUR and EASGD methods. For clarity, we focus on the synchronous scenario. These two methods can be unified by a same equation once we transform EASGD from the Jacobi form into the Gauss-Seidel form. The difference between the two is the choice of the moving rates. A further stability analysis shows that DONWPOUR has a very singular region for these rates which is separated from EASGD when the number of processors is large.\nThe last chapter concludes the thesis with a reprise of this overview, together with some open questions and directions to follow in the future. We have also made an open source project named mpiT on github to facilitate the communication using MPI under Torch, including our implementation of DOWNPOUR and EASGD.\n8\nChapter 2"}, {"heading": "Elastic Averaging SGD (EASGD)", "text": "In this chapter we introduce the Elastic Averaging SGD method (EASGD) and its variants. EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45]. The basic idea is to let each worker maintain its own local parameter, and the communication and coordination of work among the local workers is based on an elastic force which links the parameters they compute with a center variable stored by the master. The center variable is updated as a moving average where the average is taken in time and also in space over the parameters computed by local workers. The local variables are updated with the SGD-based methods, so as to keep the oscillations during the training process.\nWe discuss first the synchronous EASGD in Section 2.1. Then in Section 2.2 we discuss how to extend the synchronous EASGD to the asynchronous scenario. Finally in Section 2.3 we combine EASGD with two classical momentum methods in the first-order convex optimization literature, i.e. the heavy-ball method and the Nesterov \u2019s momentum method, to accelerate EASGD. The big picture is illustrated in Figure 2.1.\n9\n2.1 Synchronous EASGD\nThe EASGD updates captured in resp. Equation 2.1 and 2.2 are obtained by taking the gradient descent step on the objective in Equation 1.2 with respect to resp. variable xi and x\u0303,\nxit+1 = x i t \u2212 \u03b7(git(xit) + \u03c1(xit \u2212 x\u0303t)) (2.1)\nx\u0303t+1 = x\u0303t + \u03b7 p\u2211 i=1 \u03c1(xit \u2212 x\u0303t), (2.2)\nwhere git(x i t) denotes the stochastic gradient of F with respect to x i evaluated at iteration t, xit and x\u0303t denote respectively the value of variables x i and x\u0303 at iteration t, and \u03b7 is the learning rate.\nThe update rule for the center variable x\u0303 takes the form of moving average where the\n10\naverage is taken over both space and time. Denote \u03b1 = \u03b7\u03c1 and \u03b2 = p\u03b1, then Equation 2.1 and 2.2 become\nxit+1 = x i t \u2212 \u03b7git(xit)\u2212 \u03b1(xit \u2212 x\u0303t) (2.3) x\u0303t+1 = (1\u2212 \u03b2)x\u0303t + \u03b2 ( 1\np p\u2211 i=1 xit\n) . (2.4)\nNote that choosing \u03b2 = p\u03b1 leads to an elastic symmetry in the update rule, i.e. there exists a symmetric force equal to \u03b1(xir \u2212 x\u0303t) between the update of each xi and x\u0303. It gives us a simple and intuitive reason for the the algorithm\u2019s stability over the ADMM method as will be explained in Section 3.3. However, this relation (\u03b2 = p\u03b1) is by no means optimal as we shall see through our analysis in Chapter 5.\nWe interpret our synchronous EASGD as an approximate model for the asynchronous EASGD. Thus in order to minimize the staleness [25] of the difference xit\u2212x\u0303t between the center and the local variable for the asynchronous EASGD (described in Algorithm 1), the update for the master in Equation 2.4 involves xit instead of x i t+1. On the other hand, we can also think of our synchronous EASGD update rules (defined by Equation 2.3 and 2.4) as a Jacobi method [47]. We could have proposed a Gauss-Seidel version of the synchronous EASGD by successively updating the local and center variables with local averaging, local gradient descent, and then the global averaging. This possibility will be made precise in Chapter 6.\nNote also that \u03b1 = \u03b7\u03c1, where the magnitude of \u03c1 (the quadratic penalty term in Equation 1.2) represents the amount of exploration we allow in the model. In particular, small \u03c1 allows for more exploration as it allows xi\u2019s to fluctuate further from the center x\u0303.\nThe distinctive idea of EASGD is to allow the local workers to perform more exploration (small \u03c1) and the master to perform exploitation. This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.\n11\n2.2 Asynchronous EASGD\nWe discussed the synchronous update of EASGD algorithm in the previous section, where the workers update the local variables in parallel such that the ith worker reads the current value of the center variable and use it to update local variable xi using Equation 2.1. All workers share the same global clock. The master has to wait for the xi updates from all p workers before being allowed to update the value of the center variable x\u0303 according to Equation 2.2.\nIn this section we propose its asynchronous variant. The local workers are still responsible for updating the local variables xi\u2019s, whereas the master is updating the center variable x\u0303. Each worker maintains its own clock ti, which starts from 0 and is incremented by 1 after each stochastic gradient update of xi as shown in Algorithm 1. The master performs an update whenever the local workers finished \u03c4 steps of their gradient updates, where we refer to \u03c4 as the communication period. As can be seen in Algorithm 1, whenever \u03c4 divides the local clock of the ith worker, the ith worker communicates with the master and requests the current value of the center variable x\u0303. The worker then waits until the master sends back the requested parameter value, and computes the elastic difference \u03b1(x \u2212 x\u0303) (this entire procedure is captured in step a) in Algorithm 1). The elastic difference is then sent back to the master (step b) in Algorithm 1) who then updates x\u0303.\nNote that the asynchronous behavior described above is partially asynchronous [7]. As in the beginning of each communication period, each worker needs to read the latest parameter from the master (blocking) and then sends the elastic difference back (blocking). Although this only involves local synchronization, one can avoid this synchronization cost by using a fully asynchronous protocol such that no waiting is necessary. The fully asynchronous protocol may however increase the network traffic and the delay in the parameter communication.We shall be more precise on this fully asynchronous protocol when we discuss the EASGD Tree algorithm in Chapter 6 (Section 6.1).\nRecall that we have chosen the Jacobi form in our synchronous EASGD update rules\n12\n(Equation 2.3 and 2.4) as an approximate model for the asynchronous behavior. It suggests another more efficient way to realize the partially asynchronous protocol as follows. At the beginning of each communication period, each local worker sends (nonblocking) its parameter to the master, and the master will send (non-blocking) back the elastic difference once having received that local worker\u2019s parameter. During that period of time, the local worker\u2019s computation can still make progress. At the end of that communication period (i.e. all the \u03c4 gradient updates have completed), each local worker will read (blocking) the elastic difference sent from the master, and then apply it. On the master side, it can either sum the p elastic differences altogether in one step as in the synchronous case or make an update whenever sending an elastic difference.\nThe communication period \u03c4 controls the frequency of the communication between every local worker and the master, and thus the trade-off between exploration and exploitation. We show demonstrate empirically in Chapter 4 (Section 4.3.3) that in deep learning problems, too large or too small communication period can both hurt the performance.\n13\nAlgorithm 1: Asynchronous EASGD: Processing by worker i and the master\nInput: learning rate \u03b7, moving rate \u03b1,\ncommunication period \u03c4 \u2208 N\nInitialize: x\u0303 is initialized randomly, xi = x\u0303,\nti = 0\nRepeat\nx\u2190 xi if (\u03c4 divides ti) then\na) xi \u2190 xi \u2212 \u03b1(x\u2212 x\u0303) b) x\u0303 \u2190 x\u0303 + \u03b1(x\u2212 x\u0303)\nend xi \u2190 xi \u2212 \u03b7gi ti (x) ti \u2190 ti + 1\nUntil forever\n2.3 Momentum EASGD\nThe momentum EASGD (EAMSGD) is a variant of our Algorithm 1 and is captured in Algorithm 2. It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.1 is replaced by the following update\nvit+1 = \u03b4v i t \u2212 \u03b7git(xit + \u03b4vit) (2.5) xit+1 = x i t + v i t+1 \u2212 \u03b7\u03c1(xit \u2212 x\u0303t),\nwhere \u03b4 is the momentum rate. Note that when \u03b4 = 0 we recover the original EASGD algorithm.\n14\nThe idea of momentum is to accelerate the slow components in the gradient descent method. The tradeoff is that we may slow down the components which were originally fast. We shall give an explicit example to illustrate this tradeoff in Chapter 5 (Section 5.2.2).\nIn literature, there\u2019s another well-known momentum variant called heavy-ball method (aka Polyak \u2019s method) [44]. The analysis of its global convergence property is still a very challenging problem in convex optimization literature [21]. If we were to combine it with EASGD, we would have the following update\nvit+1 = \u03b4v i t \u2212 \u03b7git(xit) (2.6) xit+1 = x i t + v i t+1 \u2212 \u03b7\u03c1(xit \u2212 x\u0303t).\nNote that in both cases, we do not add the momentum to the center variable. One reason is that the momentum method has an error accumulation effect [18]. Due to the stochastic noise in the gradient, using momentum can actually result in higher asymptotic variance (see [32] and our discussion in Section 5.1.2). The role of the center variable is indeed to reduce the asymptotic variance.\n15\nAlgorithm 2: Asynchronous EAMSGD: Processing by worker i and the master\nInput: learning rate \u03b7, moving rate \u03b1,\ncommunication period \u03c4 \u2208 N, momentum term \u03b4\nInitialize: x\u0303 is initialized randomly, xi = x\u0303,\nvi = 0, ti = 0\nRepeat\nx\u2190 xi if (\u03c4 divides ti) then\na) xi \u2190 xi \u2212 \u03b1(x\u2212 x\u0303) b) x\u0303 \u2190 x\u0303 + \u03b1(x\u2212 x\u0303)\nend vi \u2190 \u03b4vi \u2212 \u03b7gi ti (x+ \u03b4vi) xi \u2190 xi + vi ti \u2190 ti + 1\nUntil forever\n16\nChapter 3"}, {"heading": "Convergence Analysis of EASGD", "text": "In this chapter, we provide the convergence analysis of the synchronous EASGD algorithm with constant learning rate. The analysis is focused on the convergence of the center variable to the optimum. We discuss one-dimensional quadratic case first (Lemma 3.1.1), then we introduce a double averaging sequence and prove that it is asymptotically optimal. For this, we provide two distinct proofs (one in Lemma 3.1.2 for the one-dimensional case, and the other in Lemma 3.1.3 for the multidimensional case). We extend the analysis to the strongly convex case as stated in Theorem 3.2.1. Finally, we provide stability analysis of the asynchronous EASGD and ADMM methods in the round-robin scheme in Section 3.3."}, {"heading": "3.1 Quadratic case", "text": "Our analysis in the quadratic case extends the analysis of ASGD in [45]. Assume each of the p local workers xit \u2208 Rn observes a noisy gradient at time t \u2265 0 of the linear form given in Equation 3.1.\ngit(x i t) = Ax i t \u2212 b\u2212 \u03beit, i \u2208 {1, . . . , p}, (3.1)\n17\nwhere the matrix A is positive-definite (each eigenvalue is strictly positive) and {\u03beit}\u2019s are i.i.d. random variables, with zero mean and positive-definite covariance matrix \u03a3. Let x\u2217 denote the optimum solution, where x\u2217 = A\u22121b \u2208 Rn."}, {"heading": "3.1.1 One-dimensional case", "text": "In this section we analyze the behavior of the mean squared error (MSE) of the center variable x\u0303t, where this error is denoted as E[\u2016x\u0303t \u2212 x\u2217\u20162], as a function of t, p, \u03b7, \u03b1 and \u03b2, where \u03b2 = p\u03b1. Note that the MSE error can be decomposed as (squared) bias and variance1: E[\u2016x\u0303t \u2212 x\u2217\u20162] = \u2016E[x\u0303t \u2212 x\u2217]\u20162 +V[x\u0303t\u2212x\u2217]. For one-dimensional case (n = 1), we assume A = h > 0 and \u03a3 = \u03c32 > 0. Lemma 3.1.1. Let x\u03030 and {xi0}i=1,...,p be arbitrary constants, then\nE[x\u0303t \u2212 x\u2217] = \u03b3t(x\u03030 \u2212 x\u2217) + \u03b3t \u2212 \u03c6t \u03b3 \u2212 \u03c6 \u03b1u0, (3.2)\nV[x\u0303t \u2212 x\u2217] = p2\u03b12\u03b72 (\u03b3 \u2212 \u03c6)2 ( \u03b32 \u2212 \u03b32t 1\u2212 \u03b32 + \u03c62 \u2212 \u03c62t 1\u2212 \u03c62 \u2212 2 \u03b3\u03c6\u2212 (\u03b3\u03c6)t 1\u2212 \u03b3\u03c6 ) \u03c32 p , (3.3)\nwhere u0 = \u2211p i=1(x i 0 \u2212 x\u2217 \u2212 \u03b11\u2212p\u03b1\u2212\u03c6(x\u03030 \u2212 x\u2217)), a = \u03b7h + (p + 1)\u03b1, c2 = \u03b7hp\u03b1, \u03b3 = 1\u2212 a\u2212 \u221a a2\u22124c2 2 , and \u03c6 = 1\u2212 a+ \u221a a2\u22124c2 2 .\nIt follows from Lemma 3.1.1 that for the center variable to be stable the following has to hold\n\u2212 1 < \u03c6 < \u03b3 < 1. (3.4)\nIt can be verified that \u03c6 and \u03b3 are the two zero-roots of the polynomial in \u03bb: \u03bb2 \u2212 (2\u2212 a)\u03bb+ (1\u2212 a+ c2). Recall that \u03c6 and \u03bb are the functions of \u03b7 and \u03b1. Thus (see proof in Section 3.1.1)\n\u2022 \u03b3 < 1 iff c2 > 0 (i.e. \u03b7 > 0 and \u03b1 > 0).\n\u2022 \u03c6 > \u22121 iff (2\u2212 \u03b7h)(2\u2212 p\u03b1) > 2\u03b1 and (2\u2212 \u03b7h) + (2\u2212 p\u03b1) > \u03b1. 1In our notation, V denotes the variance.\n18\n\u2022 \u03c6 = \u03b3 iff a2 = 4c2 (i.e. \u03b7h = \u03b1 = 0).\nThe proof the above Lemma is based on the diagonalization of the linear gradient map (this map is symmetric due to the relation \u03b2 = p\u03b1). The stability analysis of the asynchronous EASGD algorithm in the round-robin scheme is similar due to this elastic symmetry.\nProof. Substituting the gradient from Equation 3.1 into the update rule used by each local worker in the synchronous EASGD algorithm (Equation 2.3 and 2.4) we obtain\nxit+1 = x i t \u2212 \u03b7(Axit \u2212 b\u2212 \u03beit)\u2212 \u03b1(xit \u2212 x\u0303t), (3.5)\nx\u0303t+1 = x\u0303t + p\u2211 i=1 \u03b1(xit \u2212 x\u0303t), (3.6)\nwhere \u03b7 is the learning rate, and \u03b1 is the moving rate. Recall that \u03b1 = \u03b7\u03c1 and A = h.\nFor the ease of notation we redefine x\u0303t and x i t as follows:\nx\u0303t , x\u0303t \u2212 x\u2217 and xit , xit \u2212 x\u2217.\nWe prove the lemma by explicitly solving the linear equations 3.5 and 3.6. Let xt = (x1t , . . . , x p t , x\u0303t) T . We rewrite the recursive relation captured in Equation 3.5 and 3.6 as simply\nxt+1 = Mxt + bt,\nwhere the drift matrix M is defined as\nM =  1\u2212 \u03b1\u2212 \u03b7h 0 ... 0 \u03b1 0 1\u2212 \u03b1\u2212 \u03b7h 0 ... \u03b1 ... 0 ... 0 ... 0 ... 0 1\u2212 \u03b1\u2212 \u03b7h \u03b1\n\u03b1 \u03b1 ... \u03b1 1\u2212 p\u03b1\n ,\n19\nand the (diffusion) vector bt = (\u03b7\u03be 1 t , . . . , \u03b7\u03be p t , 0) T .\nNote that one of the eigenvalues of matrix M , that we call \u03c6, satisfies (1 \u2212 \u03b1 \u2212 \u03b7h \u2212 \u03c6)(1 \u2212 p\u03b1 \u2212 \u03c6) = p\u03b12. The corresponding eigenvector is (1, 1, . . . , 1,\u2212 p\u03b11\u2212p\u03b1\u2212\u03c6)T . Let\nut be the projection of xt onto this eigenvector. Thus ut = \u2211p i=1(x i t \u2212 \u03b11\u2212p\u03b1\u2212\u03c6 x\u0303t). Let\nfurthermore \u03bet = \u2211p i=1 \u03be i t. Therefore we have\nut+1 = \u03c6ut + \u03b7\u03bet. (3.7)\nBy combining Equation 3.6 and 3.7 as follows\nx\u0303t+1 = x\u0303t + p\u2211 i=1 \u03b1(xit \u2212 x\u0303t) = (1\u2212 p\u03b1)x\u0303t + \u03b1(ut + p\u03b1 1\u2212 p\u03b1\u2212 \u03c6x\u0303t)\n= (1\u2212 p\u03b1+ p\u03b1 2\n1\u2212 p\u03b1\u2212 \u03c6)x\u0303t + \u03b1ut = \u03b3x\u0303t + \u03b1ut,\nwhere the last step results from the following relations: p\u03b1 2\n1\u2212p\u03b1\u2212\u03c6 = 1 \u2212 \u03b1 \u2212 \u03b7h \u2212 \u03c6 and\n\u03c6+ \u03b3 = 1\u2212 \u03b1\u2212 \u03b7h+ 1\u2212 p\u03b1. Thus we obtained\nx\u0303t+1 = \u03b3x\u0303t + \u03b1ut. (3.8)\nBased on Equation 3.7 and 3.8, we can then expand ut and x\u0303t recursively,\nut+1 = \u03c6 t+1u0 + \u03c6 t(\u03b7\u03be0) + . . .+ \u03c6 0(\u03b7\u03bet), (3.9)\nx\u0303t+1 = \u03b3 t+1x\u03030 + \u03b3 t(\u03b1u0) + . . .+ \u03b3 0(\u03b1ut). (3.10)\nSubstituting u0, u1, . . . , ut, each given through Equation 3.9, into Equation 3.10 we obtain\nx\u0303t = \u03b3 tx\u03030 + \u03b3t \u2212 \u03c6t \u03b3 \u2212 \u03c6 \u03b1u0 + \u03b1\u03b7 t\u22121\u2211 l=1 \u03b3t\u2212l \u2212 \u03c6t\u2212l \u03b3 \u2212 \u03c6 \u03bel\u22121. (3.11)\nTo be more specific, the Equation 3.11 is obtained by interchanging the order of sum-\n20\nmation,\nx\u0303t+1 = \u03b3 t+1x\u03030 + t\u2211 i=0 \u03b3t\u2212i(\u03b1ui)\n= \u03b3t+1x\u03030 + t\u2211 i=0 \u03b3t\u2212i(\u03b1(\u03c6iu0 + i\u22121\u2211 l=0 \u03c6i\u22121\u2212l\u03b7\u03bel))\n= \u03b3t+1x\u03030 + t\u2211 i=0 \u03b3t\u2212i\u03c6i(\u03b1u0) + t\u22121\u2211 l=0 t\u2211 i=l+1 \u03b3t\u2212i\u03c6i\u22121\u2212l(\u03b1\u03b7\u03bel) = \u03b3t+1x\u03030 + \u03b3t+1 \u2212 \u03c6t+1\n\u03b3 \u2212 \u03c6 (\u03b1u0) + t\u22121\u2211 l=0 \u03b3t\u2212l \u2212 \u03c6t\u2212l \u03b3 \u2212 \u03c6 (\u03b1\u03b7\u03bel).\nSince the random variables \u03bel are i.i.d, we may sum the variance term by term as follows\nt\u22121\u2211 l=0 ( \u03b3t\u2212l \u2212 \u03c6t\u2212l \u03b3 \u2212 \u03c6 )2 = t\u22121\u2211 l=0 \u03b32(t\u2212l) \u2212 2\u03b3t\u2212l\u03c6t\u2212l + \u03c62(t\u2212l) (\u03b3 \u2212 \u03c6)2\n= 1 (\u03b3 \u2212 \u03c6)2 ( \u03b32 \u2212 \u03b32(t+1) 1\u2212 \u03b32 \u2212 2 \u03b3\u03c6\u2212 (\u03b3\u03c6)t+1 1\u2212 \u03b3\u03c6 + \u03c62 \u2212 \u03c62(t+1) 1\u2212 \u03c62 ) .\n(3.12)\nNote that E[\u03bet] = \u2211p i=1 E[\u03beit] = 0 and V[\u03bet] = \u2211p i=1 V[\u03beit] = p\u03c32. These two facts, the equality in Equation 3.11 and Equation 3.12 can then be used to compute E[x\u0303t] and V[x\u0303t] as given in Equation 3.2 and 3.3 in Lemma 3.1.1."}, {"heading": "Visualizing Lemma 3.1.1", "text": "In Figure 3.1, we illustrate the dependence of MSE on \u03b2, \u03b7 and the number of processors p over time t. We consider the large-noise setting where x\u03030 = x i 0 = 1, h = 1 and \u03c3 = 10. The MSE error is color-coded such that the deep blue color corresponds to the MSE equal to 10\u22123, the green color corresponds to the MSE equal to 1, the red color corresponds to MSE equal to 103 and the dark red color corresponds to the divergence of algorithm EASGD (condition in Equation 3.4 is then violated). The plot shows that we can achieve significant variance reduction by increasing the number of local workers\n21\n22\np. This effect is less sensitive to the choice of \u03b2 and \u03b7 for large p."}, {"heading": "Condition in Equation 3.4", "text": "We are going to show that\n\u2022 \u03b3 < 1 iff c2 > 0 (i.e. \u03b7 > 0 and \u03b2 > 0).\n\u2022 \u03c6 > \u22121 iff (2\u2212 \u03b7h)(2\u2212 \u03b2) > 2\u03b2/p and (2\u2212 \u03b7h) + (2\u2212 \u03b2) > \u03b2/p.\n\u2022 \u03c6 = \u03b3 iff a2 = 4c2 (i.e. \u03b7h = \u03b2 = 0).\nRecall that a = \u03b7h + (p + 1)\u03b1, c2 = \u03b7hp\u03b1, \u03b3 = 1 \u2212 a\u2212 \u221a a2\u22124c2 2 , \u03c6 = 1 \u2212 a+ \u221a a2\u22124c2 2 , and \u03b2 = p\u03b1. We have\n\u2022 \u03b3 < 1\u21d4 a\u2212 \u221a a2\u22124c2 2 > 0\u21d4 a > \u221a a2 \u2212 4c2 \u21d4 a2 > a2 \u2212 4c2 \u21d4 c2 > 0. \u2022 \u03c6 > \u22121\u21d4 2 > a+ \u221a a2\u22124c2 2 \u21d4 4\u2212 a > \u221a a2 \u2212 4c2 \u21d4 4\u2212 a > 0, (4\u2212 a)2 > a2\u2212 4c2 \u21d4\n4\u2212 a > 0, 4\u2212 2a+ c2 > 0\u21d4 4 > \u03b7h+ \u03b2 + \u03b1, 4\u2212 2(\u03b7h+ \u03b2 + \u03b1) + \u03b7h\u03b2 > 0.\n\u2022 \u03c6 = \u03b3 \u21d4 \u221a a2 \u2212 4c2 = 0\u21d4 a2 = 4c2.\nThe next corollary is a consequence of Lemma 3.1.1. As the number of workers p grows, the averaging property of the EASGD can be characterized as follows Corollary 3.1.1. Let the Elastic Averaging relation \u03b2 = p\u03b1 and the condition 3.4 hold, then\nlim p\u2192\u221e lim t\u2192\u221e\npE[(x\u0303t \u2212 x\u2217)2] = \u03b2\u03b7h (2\u2212 \u03b2)(2\u2212 \u03b7h) \u00b7 2\u2212 \u03b2 \u2212 \u03b7h+ \u03b2\u03b7h \u03b2 + \u03b7h\u2212 \u03b2\u03b7h \u00b7 \u03c32 h2 .\nProof. Note that when \u03b2 is fixed, limp\u2192\u221e a = \u03b7h+ \u03b2 and c 2 = \u03b7h\u03b2. Then limp\u2192\u221e \u03c6 = min(1\u2212\u03b2, 1\u2212\u03b7h) and limp\u2192\u221e \u03b3 = max(1\u2212\u03b2, 1\u2212\u03b7h). Also note that using Lemma 3.1.1\n23\nwe obtain\nlim t\u2192\u221e\nE[(x\u0303t \u2212 x\u2217)2] = \u03b22\u03b72 (\u03b3 \u2212 \u03c6)2 ( \u03b32 1\u2212 \u03b32 + \u03c62 1\u2212 \u03c62 \u2212 2\u03b3\u03c6 1\u2212 \u03b3\u03c6 ) \u03c32 p\n= \u03b22\u03b72 (\u03b3 \u2212 \u03c6)2 ( \u03b32(1\u2212 \u03c62)(1\u2212 \u03c6\u03b3) + \u03c62(1\u2212 \u03b32)(1\u2212 \u03c6\u03b3)\u2212 2\u03b3\u03c6(1\u2212 \u03b32)(1\u2212 \u03c62) (1\u2212 \u03b32)(1\u2212 \u03c62)(1\u2212 \u03b3\u03c6) ) \u03c32 p = \u03b22\u03b72 (\u03b3 \u2212 \u03c6)2 ( (\u03b3 \u2212 \u03c6)2(1 + \u03b3\u03c6) (1\u2212 \u03b32)(1\u2212 \u03c62)(1\u2212 \u03b3\u03c6) ) \u03c32 p = \u03b22\u03b72\n(1\u2212 \u03b32)(1\u2212 \u03c62) \u00b7 1 + \u03b3\u03c6 1\u2212 \u03b3\u03c6 \u00b7 \u03c32 p .\nCorollary 3.1.1 is obtained by plugging in the limiting values of \u03c6 and \u03b3.\nThe crucial point of Corollary 3.1.1 is that the MSE in the limit t\u2192\u221e is in the order of 1/p which implies that as the number of processors p grows, the MSE will decrease for the EASGD algorithm. Also note that the smaller the \u03b2 is (recall that \u03b2 = p\u03b1 = p\u03b7\u03c1), the more exploration is allowed (small \u03c1) and simultaneously the smaller the MSE is.\nThe next lemma (Lemma 3.1.2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [45]) {z1, z2, . . . } defined as below\nzt+1 = 1\nt+ 1 t\u2211 k=0 x\u0303k. (3.13)\nLemma 3.1.2 (Weak convergence). If the condition in Equation 3.4 holds, then the normalized double averaging sequence defined in Equation 3.13 converges weakly to the normal distribution with zero mean and variance \u03c32/ph2,\n\u221a t(zt \u2212 x\u2217) \u21c0 N (0, \u03c32\nph2 ), t\u2192\u221e. (3.14)\nProof. As in the proof of Lemma 3.1.1, for the ease of notation we redefine x\u0303t and x i t as follows:\nx\u0303t , x\u0303t \u2212 x\u2217 and xit , xit \u2212 x\u2217.\n24\nAlso recall that {\u03beit}\u2019s are i.i.d. random variables (noise) with zero mean and the same positive definite covariance matrix \u03a3 0. We are interested in the asymptotic behavior of the double averaging sequence {z1, z2, . . . } defined as\nzt+1 = 1\nt+ 1 t\u2211 k=0 x\u0303k. (3.15)\nRecall the Equation 3.11 from the proof of Lemma 3.1.1 (for the convenience it is provided below):\nx\u0303k = \u03b3 kx\u03030 + \u03b1u0 \u03b3k \u2212 \u03c6k \u03b3 \u2212 \u03c6 + \u03b1\u03b7 k\u22121\u2211 l=1 \u03b3k\u2212l \u2212 \u03c6k\u2212l \u03b3 \u2212 \u03c6 \u03bel\u22121,\nwhere \u03bet = \u2211p i=1 \u03be i t. Therefore t\u2211 k=0 x\u0303k = 1\u2212 \u03b3t+1 1\u2212 \u03b3 x\u03030 + \u03b1u0 1 \u03b3 \u2212 \u00b5 ( 1\u2212 \u03b3t+1 1\u2212 \u03b3 \u2212 1\u2212 \u03c6t+1 1\u2212 \u03c6 ) + \u03b1\u03b7 t\u22121\u2211 l=1 t\u2211 k=l+1 \u03b3k\u2212l \u2212 \u03c6k\u2212l \u03b3 \u2212 \u03c6 \u03bel\u22121\n= O(1) + \u03b1\u03b7 t\u22121\u2211 l=1 1 \u03b3 \u2212 \u03c6 ( \u03b3 1\u2212 \u03b3t\u2212l 1\u2212 \u03b3 \u2212 \u03c6 1\u2212 \u03c6t\u2212l 1\u2212 \u03c6 ) \u03bel\u22121\nNote that the only non-vanishing term (in weak convergence) of 1/ \u221a t \u2211t\nk=0 x\u0303k as t\u2192\u221e\nis\n1\u221a t \u03b1\u03b7 t\u22121\u2211 l=1 1 \u03b3 \u2212 \u03c6 ( \u03b3 1\u2212 \u03b3 \u2212 \u03c6 1\u2212 \u03c6 ) \u03bel\u22121. (3.16)\nAlso recall that V[\u03bel\u22121] = p\u03c32 and\n1\n\u03b3 \u2212 \u03c6\n( \u03b3\n1\u2212 \u03b3 \u2212 \u03c6 1\u2212 \u03c6\n) =\n1 (1\u2212 \u03b3)(1\u2212 \u03c6) = 1 \u03b7hp\u03b1 .\nTherefore the expression in Equation 3.16 is asymptotically normal with zero mean and variance \u03c32/ph2.\n25"}, {"heading": "3.1.2 Generalization to multidimensional case", "text": "The asymptotic variance in the Lemma 3.1.2 is optimal with any fixed \u03b7 and \u03b2 for which Equation 3.4 holds. The next lemma (Lemma 3.1.3) extends the result in Lemma 3.1.2 to the multi-dimensional setting. Lemma 3.1.3 (Weak convergence). Let h denotes the largest eigenvalue of A. If (2 \u2212 \u03b7h)(2 \u2212 \u03b2) > 2\u03b2/p, (2 \u2212 \u03b7h) + (2 \u2212 \u03b2) > \u03b2/p, \u03b7 > 0 and \u03b2 > 0, then the normalized double averaging sequence converges weakly to the normal distribution with zero mean and the covariance matrix V = A\u22121\u03a3(A\u22121)T ,\n\u221a tp(zt \u2212 x\u2217) \u21c0 N (0, V ), t\u2192\u221e. (3.17)\nProof. Since A is symmetric, one can use the proof technique of Lemma 3.1.2 to prove Lemma 3.1.3 by diagonalizing the matrix A. This diagonalization essentially generalizes Lemma 3.1.1 to the multidimensional case. We will not go into the details of this proof as we will provide a simpler way to look at the system. As in the proof of Lemma 3.1.1 and Lemma 3.1.2, for the ease of notation we redefine x\u0303t and x i t as follows:\nx\u0303t , x\u0303t \u2212 x\u2217 and xit , xit \u2212 x\u2217.\nLet the spatial average of the local parameters at time t be denoted as yt where yt = 1 p \u2211p i=1 x i t, and let the average noise be denoted as \u03bet, where \u03bet = 1 p \u2211p i=1 \u03be i t. Equations 3.5 and 3.6 can then be reduced to the following\nyt+1 = yt \u2212 \u03b7(Ayt \u2212 \u03bet) + \u03b1(x\u0303t \u2212 yt), (3.18) x\u0303t+1 = x\u0303t + \u03b2(yt \u2212 x\u0303t). (3.19)\nWe focus on the case where the learning rate \u03b7 and the moving rate \u03b1 are kept constant over time2. Recall \u03b2 = p\u03b1 and \u03b1 = \u03b7\u03c1.\n2As a side note, notice that the center parameter x\u0303t is tracking the spatial average yt of the local\n26\nLet\u2019s introduce the block notation Ut = (yt, x\u0303t), \u039et = (\u03b7\u03bet, 0), M = I \u2212 \u03b7L and\nL =  A+ \u03b1\u03b7 I \u2212\u03b1\u03b7 I \u2212\u03b2\u03b7 I \u03b2 \u03b7 I  . From Equations 3.18 and 3.19 it follows that Ut+1 = MUt + \u039et. Note that this linear system has a degenerate noise \u039et which prevents us from directly applying results of [45]. Expanding this recursive relation and summing by parts, we have\nt\u2211 k=0 Uk = M 0U0 +\nM1U0 +M 0\u039e0 + M2U0 +M 1\u039e0 +M 0\u039e1 +\n... M tU0 +M t\u22121\u039e0 + \u00b7 \u00b7 \u00b7+M0\u039et\u22121.\nBy Lemma 3.1.4, \u2016M\u20162 < 1 and thus\nM0 +M1 + \u00b7 \u00b7 \u00b7+M t + \u00b7 \u00b7 \u00b7 = (I \u2212M)\u22121 = \u03b7\u22121L\u22121.\nSince A is invertible, we get\nL\u22121 =  A\u22121 \u03b1\u03b2A\u22121 A\u22121 \u03b7\u03b2 + \u03b1 \u03b2A \u22121  , thus\n1\u221a t t\u2211 k=0 Uk = 1\u221a t U0 + 1\u221a t \u03b7L\u22121 t\u2211 k=1 \u039ek\u22121 \u2212 1\u221a t t\u2211 k=1 Mk+1\u039ek\u22121.\nparameters with a non-symmetric spring in Equation 3.18 and 3.19. To be more precise note that the update on yt+1 contains (x\u0303t\u2212 yt) scaled by \u03b1, whereas the update on x\u0303t+1 contains \u2212(x\u0303t\u2212 yt) scaled by \u03b2. Since \u03b1 = \u03b2/p the impact of the center x\u0303t+1 on the spatial local average yt+1 becomes more negligible as p grows.\n27\nNote that the only non-vanishing term of 1\u221a t \u2211t k=0 Uk is 1\u221a t (\u03b7L)\u22121 \u2211t k=1 \u039ek\u22121, thus in weak convergence we have\n1\u221a t (\u03b7L)\u22121 t\u2211 k=1\n\u039ek\u22121 \u21c0 N ( 0\n0\n ,  V V\nV V ), (3.20) where V = A\u22121\u03a3(A\u22121)T .\nLemma 3.1.4. If the following conditions hold:\n(2\u2212 \u03b7h)(2\u2212 p\u03b1) > 2\u03b1\n(2\u2212 \u03b7h) + (2\u2212 p\u03b1) > \u03b1\n\u03b7 > 0\n\u03b1 > 0\nthen \u2016M\u20162 < 1.\nProof. The eigenvalue \u03bb of M and the (non-zero) eigenvector (y, z) of M satisfy\nM  y z  = \u03bb  y z  . (3.21) Recall that\nM = I \u2212 \u03b7L =  I \u2212 \u03b7A\u2212 \u03b1I \u03b1I \u03b2I I \u2212 \u03b2I  . (3.22) From the Equations 3.21 and 3.22 we obtain\n y \u2212 \u03b7Ay \u2212 \u03b1y + \u03b1z = \u03bby\u03b2y + (1\u2212 \u03b2)z = \u03bbz . (3.23)\n28\nSince (y, z) is assumed to be non-zero, we can write z = \u03b2y/(\u03bb + \u03b2 \u2212 1). Then the Equation 3.23 can be reduced to\n\u03b7Ay = (1\u2212 \u03b1\u2212 \u03bb)y + \u03b1\u03b2 \u03bb+ \u03b2 \u2212 1y. (3.24)\nThus y is the eigenvector of A. Let \u03bbA be the eigenvalue of matrix A such that Ay = \u03bbAy. Thus based on Equation 3.24 it follows that\n\u03b7\u03bbA = (1\u2212 \u03b1\u2212 \u03bb) + \u03b1\u03b2\n\u03bb+ \u03b2 \u2212 1 . (3.25)\nEquation 3.25 is equivalent to\n\u03bb2 \u2212 (2\u2212 a)\u03bb+ (1\u2212 a+ c2) = 0, (3.26)\nwhere a = \u03b7\u03bbA + (p + 1)\u03b1, c 2 = \u03b7\u03bbAp\u03b1. It follows from the condition in Equation 3.4 that \u22121 < \u03bb < 1 iff \u03b7 > 0, \u03b2 > 0, (2\u2212\u03b7\u03bbA)(2\u2212\u03b2) > 2\u03b2/p and (2\u2212\u03b7\u03bbA)+(2\u2212\u03b2) > \u03b2/p. Let h denote the maximum eigenvalue of A and note that 2\u2212\u03b7\u03bbA \u2265 2\u2212\u03b7h. This implies that the condition of our lemma is sufficient.\nAs in Lemma 3.1.2, the asymptotic covariance in the Lemma 3.1.3 is optimal, i.e. meets the Fisher information lower-bound. The fact that this asymptotic covariance matrix V does not contain any term involving \u03c1 is quite remarkable, since the penalty term \u03c1 does have an impact on the condition number of the Hessian in Equation 1.2."}, {"heading": "3.2 Strongly convex case", "text": "We now extend the above proof ideas to analyze the strongly convex case, in which the noisy gradient git(x) = \u2207F (x)\u2212 \u03beit has the regularity that there exists some 0 < \u00b5 \u2264 L, for which \u00b5 \u2016x\u2212 y\u20162 \u2264 \u3008\u2207F (x)\u2212\u2207F (y), x\u2212 y\u3009 \u2264 L \u2016x\u2212 y\u20162 holds uniformly for any x \u2208 Rd, y \u2208 Rd. The noise {\u03beit}\u2019s is assumed to be i.i.d. with zero mean and bounded\n29\nvariance E[ \u2225\u2225\u03beit\u2225\u22252] \u2264 \u03c32.\nTheorem 3.2.1. Let at = E \u2225\u2225\u22251p\u2211pi=1 xit \u2212 x\u2217\u2225\u2225\u22252, bt = 1p\u2211pi=1 E\u2225\u2225xit \u2212 x\u2217\u2225\u22252, ct = E \u2016x\u0303t \u2212 x\u2217\u20162, \u03b31 = 2\u03b7 \u00b5L \u00b5+L and \u03b32 = 2\u03b7L(1\u2212 2 \u221a \u00b5L \u00b5+L ). If 0 \u2264 \u03b7 \u2264 2\u00b5+L(1\u2212 \u03b1), 0 \u2264 \u03b1 < 1 and 0 \u2264 \u03b2 \u2264 1 then  at+1 bt+1\nct+1\n \u2264  1\u2212 \u03b31 \u2212 \u03b32 \u2212 \u03b1 \u03b32 \u03b1 0 1\u2212 \u03b31 \u2212 \u03b1 \u03b1\n\u03b2 0 1\u2212 \u03b2\n  at bt\nct\n+  \u03b72 \u03c3 2 p \u03b72\u03c32\n0\n .\nProof. The idea of the proof is based on the point of view in Lemma 3.1.3, i.e. how close the center variable x\u0303t is to the spatial average of the local variables yt = 1 p \u2211p i=1 x i t. To further simplify the notation, let the noisy gradient be \u2207f it,\u03be = git(xit) = \u2207F (xit) \u2212 \u03beit, and \u2207f it = \u2207F (xit) be its deterministic part. Then EASGD updates can be rewritten as follows,\nxit+1 = x i t \u2212 \u03b7\u2207f it,\u03be \u2212 \u03b1(xit \u2212 x\u0303t), (3.27) x\u0303t+1 = x\u0303t + \u03b2(yt \u2212 x\u0303t). (3.28)\nWe have thus the update for the spatial average,\nyt+1 = yt \u2212 \u03b7 1\np p\u2211 i=1 \u2207f it,\u03be \u2212 \u03b1(yt \u2212 x\u0303t). (3.29)\nThe idea of the proof is to bound the distance \u2016x\u0303t \u2212 x\u2217\u20162 through \u2016yt \u2212 x\u2217\u20162 and 1 p \u2211p i \u2225\u2225xit \u2212 x\u2217\u2225\u22252. We start from the following estimate for the strongly convex function [38],\n\u3008\u2207F (x)\u2212\u2207F (y), x\u2212 y\u3009 \u2265 \u00b5L \u00b5+ L \u2016x\u2212 y\u20162 + 1 \u00b5+ L \u2016\u2207F (x)\u2212\u2207F (y)\u20162 .\n30\nSince \u2207f(x\u2217) = 0, we have\n\u2329 \u2207f it , xit \u2212 x\u2217 \u232a \u2265 \u00b5L \u00b5+ L \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2207f it\u2225\u22252 . (3.30) From Equation 3.27 the following relation holds,\n\u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 = \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u03b72 \u2225\u2225\u2207f it,\u03be\u2225\u22252 + \u03b12 \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 2\u03b7 \u2329 \u2207f it,\u03be, xit \u2212 x\u2217 \u232a \u2212 2\u03b1 \u2329 xit \u2212 x\u0303t, xit \u2212 x\u2217\n\u232a + 2\u03b7\u03b1 \u2329 \u2207f it,\u03be, xit \u2212 x\u0303t \u232a . (3.31)\nBy the cosine rule (2 \u3008a\u2212 b, c\u2212 d\u3009 = \u2016a\u2212 d\u20162\u2212\u2016a\u2212 c\u20162 + \u2016c\u2212 b\u20162\u2212\u2016d\u2212 b\u20162), we have\n2 \u2329 xit \u2212 x\u0303t, xit \u2212 x\u2217 \u232a = \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162 . (3.32)\nBy the Cauchy-Schwarz inequality, we have\n\u2329 \u2207f it , xit \u2212 x\u0303t \u232a \u2264 \u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 . (3.33)\nCombining the above estimates in Equations 3.30, 3.31, 3.32, 3.33, we obtain\n\u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 \u2264 \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u03b72 \u2225\u2225\u2207f it \u2212 \u03beit\u2225\u22252 + \u03b12 \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 2\u03b7 ( \u00b5L\n\u00b5+ L \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2207f it\u2225\u22252 ) + 2\u03b7 \u2329 \u03beit, x i t \u2212 x\u2217 \u232a \u2212 \u03b1\n( \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162 ) + 2\u03b7\u03b1\n\u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 \u2212 2\u03b7\u03b1 \u2329\u03beit, xit \u2212 x\u0303t\u232a . (3.34) Choosing 0 \u2264 \u03b1 < 1, we can have this upper-bound for the terms \u03b12\n\u2225\u2225xit \u2212 x\u0303t\u2225\u22252 \u2212 \u03b1 \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 + 2\u03b7\u03b1 \u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 = \u2212\u03b1(1 \u2212 \u03b1) \u2225\u2225xit \u2212 x\u0303t\u2225\u22252 + 2\u03b7\u03b1 \u2225\u2225\u2207f it\u2225\u2225 \u2225\u2225xit \u2212 x\u0303t\u2225\u2225 \u2264 \u03b72\u03b1 1\u2212\u03b1 \u2225\u2225\u2207f it\u2225\u22252 by applying \u2212ax2 +bx \u2264 b24a with x = \u2225\u2225xit \u2212 x\u0303t\u2225\u2225. Thus we can further bound\n31\nEquation 3.34 with\n\u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 \u2264 (1\u2212 2\u03b7 \u00b5L\u00b5+ L \u2212 \u03b1)\u2225\u2225xit \u2212 x\u2217\u2225\u22252 + (\u03b72 + \u03b72\u03b11\u2212 \u03b1 \u2212 2\u03b7\u00b5+ L)\u2225\u2225\u2207f it\u2225\u22252 \u2212 2\u03b72 \u2329 \u2207f it , \u03beit \u232a + 2\u03b7 \u2329 \u03beit, x i t \u2212 x\u2217 \u232a \u2212 2\u03b7\u03b1 \u2329 \u03beit, x i t \u2212 x\u0303t \u232a (3.35)\n+ \u03b72 \u2225\u2225\u03beit\u2225\u22252 + \u03b1 \u2016x\u0303t \u2212 x\u2217\u20162 (3.36)\nAs in Equation 3.35 and 3.36, the noise \u03beit is zero mean (E\u03beit = 0) and the variance of the noise \u03beit is bounded (E \u2225\u2225\u03beit\u2225\u22252 \u2264 \u03c32), if \u03b7 is chosen small enough such that \u03b72+ \u03b72\u03b11\u2212\u03b1\u2212 2\u03b7\u00b5+L \u2264 0, then\nE \u2225\u2225xit+1 \u2212 x\u2217\u2225\u22252 \u2264 (1\u2212 2\u03b7 \u00b5L\u00b5+ L \u2212 \u03b1)E\u2225\u2225xit \u2212 x\u2217\u2225\u22252 + \u03b72\u03c32 + \u03b1E \u2016x\u0303t \u2212 x\u2217\u20162 .(3.37)\nNow we apply similar idea to estimate \u2016yt \u2212 x\u2217\u20162. From Equation 3.29 the following relation holds,\n\u2016yt+1 \u2212 x\u2217\u20162 = \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it,\u03be \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 \u2329 1\np p\u2211 i=1 \u2207f it,\u03be, yt \u2212 x\u2217 \u232a \u2212 2\u03b1 \u3008yt \u2212 x\u0303t, yt \u2212 x\u2217\u3009\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it,\u03be, yt \u2212 x\u0303t \u232a . (3.38)\nBy \u2329\n1 p \u2211p i=1 ai, 1 p \u2211p j=1 bj \u232a = 1p \u2211p i=1 \u3008ai, bi\u3009 \u2212 1p2 \u2211 i>j \u3008ai \u2212 aj , bi \u2212 bj\u3009, we have\n\u2329 1\np p\u2211 i=1 \u2207f it , yt \u2212 x\u2217 \u232a = 1 p p\u2211 i=1 \u2329 \u2207f it , xit \u2212 x\u2217 \u232a \u2212 1 p2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a . (3.39)\nBy the cosine rule, we have\n2 \u3008yt \u2212 x\u0303t, yt \u2212 x\u2217\u3009 = \u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162 . (3.40)\n32\nDenote \u03bet = 1 p \u2211p i=1 \u03be i t, we can rewrite Equation 3.38 as\n\u2016yt+1 \u2212 x\u2217\u20162 = \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 \u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u2217 \u232a \u2212 2\u03b1 \u3008yt \u2212 x\u0303t, yt \u2212 x\u2217\u3009\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t \u232a . (3.41)\nBy combining the above Equations 3.39, 3.40 with 3.41, we obtain\n\u2016yt+1 \u2212 x\u2217\u20162 = \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 ( 1\np p\u2211 i=1 \u2329 \u2207f it , xit \u2212 x\u2217 \u232a \u2212 1 p2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a ) (3.42)\n+ 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 \u03b1(\u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162)\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t \u232a . (3.43)\nThus it follows from Equation 3.30 and 3.43 that\n\u2016yt+1 \u2212 x\u2217\u20162 \u2264 \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b71 p p\u2211 i=1 ( \u00b5L \u00b5+ L \u2225\u2225xit \u2212 x\u2217\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2207f it\u2225\u22252 ) + 2\u03b7 1\np2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a + 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 \u03b1(\u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162)\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t \u232a . (3.44)\n33\nRecall yt = 1 p \u2211p i=1 x i t, we have the following bias-variance relation,\n1 p p\u2211 i=1 \u2225\u2225xit \u2212 x\u2217\u2225\u22252 = 1p p\u2211 i=1 \u2225\u2225xit \u2212 yt\u2225\u22252 + \u2016yt \u2212 x\u2217\u20162 = 1p2 \u2211 i>j \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 + \u2016yt \u2212 x\u2217\u20162 , 1\np p\u2211 i=1 \u2225\u2225\u2207f it\u2225\u22252 = 1p2 \u2211 i>j \u2225\u2225\u2225\u2207f it \u2212\u2207f jt \u2225\u2225\u22252 + \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2225\u2225\u2225\u2225\u2225 2 . (3.45)\nBy the Cauchy-Schwarz inequality, we have\n\u00b5L\n\u00b5+ L \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 + 1\u00b5+ L \u2225\u2225\u2225\u2207f it \u2212\u2207f jt \u2225\u2225\u22252 \u2265 2 \u221a \u00b5L \u00b5+ L \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a . (3.46)\nCombining the above estimates in Equations 3.44, 3.45, 3.46, we obtain\n\u2016yt+1 \u2212 x\u2217\u20162 \u2264 \u2016yt \u2212 x\u2217\u20162 + \u03b72 \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2212 \u03bet \u2225\u2225\u2225\u2225\u2225 2 + \u03b12 \u2016yt \u2212 x\u0303t\u20162\n\u2212 2\u03b7 ( \u00b5L\n\u00b5+ L \u2016yt \u2212 x\u2217\u20162 +\n1\n\u00b5+ L \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2225\u2225\u2225\u2225\u2225 2)\n+ 2\u03b7 ( 1\u2212 2 \u221a \u00b5L\n\u00b5+ L\n) 1\np2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a + 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 \u03b1(\u2016yt \u2212 x\u2217\u20162 + \u2016yt \u2212 x\u0303t\u20162 \u2212 \u2016x\u0303t \u2212 x\u2217\u20162)\n+ 2\u03b7\u03b1\n\u2329 1\np p\u2211 i=1 \u2207f it \u2212 \u03bet, yt \u2212 x\u0303t \u232a . (3.47)\nSimilarly if 0 \u2264 \u03b1 < 1, we can have this upper-bound for the terms \u03b12 \u2016yt \u2212 x\u0303t\u20162 \u2212 \u03b1 \u2016yt \u2212 x\u0303t\u20162 + 2\u03b7\u03b1 \u2225\u2225\u22251p\u2211pi=1\u2207f it\u2225\u2225\u2225 \u2016yt \u2212 x\u0303t\u2016 \u2264 \u03b72\u03b11\u2212\u03b1 \u2225\u2225\u22251p\u2211pi=1\u2207f it\u2225\u2225\u22252 by applying \u2212ax2 +\n34\nbx \u2264 b24a with x = \u2016yt \u2212 x\u0303t\u2016. Thus we have the following bound for the Equation 3.47\n\u2016yt+1 \u2212 x\u2217\u20162 \u2264 (1\u2212 2\u03b7 \u00b5L \u00b5+ L \u2212 \u03b1) \u2016yt \u2212 x\u2217\u20162 + (\u03b72 +\n\u03b72\u03b1\n1\u2212 \u03b1 \u2212 2\u03b7 \u00b5+ L ) \u2225\u2225\u2225\u2225\u22251p p\u2211 i=1 \u2207f it \u2225\u2225\u2225\u2225\u2225 2\n\u2212 2\u03b72 \u2329 1\np p\u2211 i=1 \u2207f it , \u03bet \u232a + 2\u03b7 \u3008\u03bet, yt \u2212 x\u2217\u3009 \u2212 2\u03b7\u03b1 \u3008\u03bet, yt \u2212 x\u0303t\u3009\n+ 2\u03b7 ( 1\u2212 2 \u221a \u00b5L\n\u00b5+ L\n) 1\np2 \u2211 i>j \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a + \u03b72 \u2016\u03bet\u20162 + \u03b1 \u2016x\u0303t \u2212 x\u2217\u20162 . (3.48)\nSince 2 \u221a \u00b5L \u00b5+L \u2264 1, we need also bound the nonlinear term \u2329 \u2207f it \u2212\u2207f jt , xit \u2212 xjt \u232a \u2264\nL \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252. Recall the bias-variance relation 1p\u2211pi=1 \u2225\u2225xit \u2212 x\u2217\u2225\u22252 = 1p2 \u2211i>j \u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252+ \u2016yt \u2212 x\u2217\u20162. The key observation is that if 1p \u2211p i=1\n\u2225\u2225xit \u2212 x\u2217\u2225\u22252 remains bounded, then larger variance \u2211 i>j\n\u2225\u2225\u2225xit \u2212 xjt\u2225\u2225\u22252 implies smaller bias \u2016yt \u2212 x\u2217\u20162. Thus this nonlinear term can be compensated.\nAgain choose \u03b7 small enough such that \u03b72 + \u03b7 2\u03b1 1\u2212\u03b1 \u2212 2\u03b7 \u00b5+L \u2264 0 and take expectation in Equation 3.48,\nE \u2016yt+1 \u2212 x\u2217\u20162 \u2264 (1\u2212 2\u03b7 \u00b5L \u00b5+ L \u2212 \u03b1)E \u2016yt \u2212 x\u2217\u20162\n+ 2\u03b7L ( 1\u2212 2 \u221a \u00b5L\n\u00b5+ L\n)( 1\np p\u2211 i=1 E \u2225\u2225xit \u2212 x\u2217\u2225\u22252 \u2212 E \u2016yt \u2212 x\u2217\u20162)\n+ \u03b72 \u03c32\np + \u03b1E \u2016x\u0303t \u2212 x\u2217\u20162 . (3.49)\nAs for the center variable in Equation 3.28, we apply simply the convexity of the norm \u2016\u00b7\u20162 to obtain\n\u2016x\u0303t+1 \u2212 x\u2217\u20162 \u2264 (1\u2212 \u03b2) \u2016x\u0303t \u2212 x\u2217\u20162 + \u03b2 \u2016yt \u2212 x\u2217\u20162 . (3.50)\nCombining the estimates from Equations 3.37, 3.49, 3.50, and denote at = E \u2016yt \u2212 x\u2217\u20162,\n35\nbt = 1 p \u2211p i=1 E \u2225\u2225xit \u2212 x\u2217\u2225\u22252, ct = E \u2016x\u0303t \u2212 x\u2217\u20162, \u03b31 = 2\u03b7 \u00b5L\u00b5+L , \u03b32 = 2\u03b7L(1\u2212 2\u221a\u00b5L\u00b5+L ), then  at+1 bt+1\nct+1\n \u2264  1\u2212 \u03b31 \u2212 \u03b32 \u2212 \u03b1 \u03b32 \u03b1 0 1\u2212 \u03b31 \u2212 \u03b1 \u03b1\n\u03b2 0 1\u2212 \u03b2\n  at bt\nct\n+  \u03b72 \u03c3 2 p \u03b72\u03c32\n0\n ,\nas long as 0 \u2264 \u03b2 \u2264 1, 0 \u2264 \u03b1 < 1 and \u03b72 + \u03b72\u03b11\u2212\u03b1 \u2212 2\u03b7 \u00b5+L \u2264 0, i.e. 0 \u2264 \u03b7 \u2264 2\u00b5+L(1\u2212 \u03b1). The above theorem captures the bias-variance tradeoff of the spatial average 1p \u2211p i=1 x i t of the local variables (the at), with respect to the averaged mean squared error of each local variable (the bt). The center variable x\u0303t is tracking 1 p \u2211p i=1 x i t over time (the ct).\nTo get an upper bound on the rate of convergence for x\u0303t, we need to assume the matrix M to be positive, and its spectral norm to be smaller than one. Here\nM =  1\u2212 \u03b31 \u2212 \u03b32 \u2212 \u03b1 \u03b32 \u03b1 0 1\u2212 \u03b31 \u2212 \u03b1 \u03b1\n\u03b2 0 1\u2212 \u03b2\n .\nWe have three eigenvalues of M as follows:\n\u03bb1 = 1\u2212 \u03b1\u2212 \u03b31 \u2212 \u03b32, \u03bb2 = 1 + 1\n2 (\u2212\u03b1\u2212 \u03b2 \u2212 \u03b31 +\n\u221a (\u03b1+ \u03b2 + \u03b31)2 \u2212 4\u03b2\u03b31),\n\u03bb3 = 1 + 1\n2 (\u2212\u03b1\u2212 \u03b2 \u2212 \u03b31 \u2212\n\u221a (\u03b1+ \u03b2 + \u03b31)2 \u2212 4\u03b2\u03b31).\nUnder the conditions of the above theorem (0 \u2264 \u03b7 \u2264 2\u00b5+L(1 \u2212 \u03b1), 0 \u2264 \u03b1 < 1 and 0 \u2264 \u03b2 \u2264 1), we still need to assume \u03bb1 \u2265 0 so that M is positive. Since \u03b31 = 2\u03b7 \u00b5L\u00b5+L \u2265 0 and \u03b32 = 2\u03b7L(1\u2212 2 \u221a \u00b5L \u00b5+L ) \u2265 0, we deduce that \u03bb1 \u2264 1. We can also verify that \u03bb3 \u2264 \u03bb2 \u2264 1. Thus for the stability we only need \u03bb3 \u2265 \u22121.\nFor \u03bb1 > 0, we get the condition 0 < \u03b7 < 1\u2212\u03b1\n2\u00b5L \u00b5+L\n+2L(1\u2212 2 \u221a \u00b5L \u00b5+L ) . For \u03bb3 > \u22121, we have the\n36\ncondition \u03b7 < \u00b5+L\u00b5L (1\u2212 \u03b12\u2212\u03b2 ). When \u00b5 = L, these two conditions mean 0 < \u03b7 < 1\u2212\u03b1L and 0 < \u03b7 < 2L(1 \u2212 \u03b12\u2212\u03b2 ). On the other hand, when \u00b5 = 0, we have 0 < \u03b7 < 1\u2212\u03b12L . In either case, our method operates in the under-damping (no oscillations) region.\nWith the above conditions, we can now ask what is the asymptotic variance of at, bt and ct. By solving the fixed point equation (a\u221e, b\u221e, c\u221e) \u2032 = M(a\u221e, b\u221e, c\u221e) \u2032+(\u03b72 \u03c3 2 p , \u03b7 2\u03c32, 0)\u2032, we obtain\na\u221e = c\u221e = \u03b1/p+ \u03b31/p+ \u03b32 \u03b31(\u03b1+ \u03b31 + \u03b32) \u03b72\u03c32,\nb\u221e = \u03b1/p+ \u03b31 + \u03b32 \u03b31(\u03b1+ \u03b31 + \u03b32) \u03b72\u03c32.\nIf \u00b5 = L, then \u03b32 = 0, we indeed get the asymptotic variance c\u221e of order \u03c3 2/p. This order matches our quadratic case analysis above. However, if \u00b5 << L, then \u03b31 will be close to zero, and we don\u2019t see in this upper bound the benefit of variance reduction by increasing p (number of workers). It would be interesting to find a non-quadratic example such that this can actually happen.\n3.3 Stability of EASGD and ADMM\nIn this section we study the stability of the asynchronous EASGD and ADMM methods in the round-robin scheme [29]. We first state the updates of both algorithms in this setting, and then we study their stability. We will show that in the one-dimensional quadratic case, ADMM algorithm can exhibit chaotic behavior, leading to exponential divergence. The analytic condition for the ADMM algorithm to be stable is still unknown, while for the EASGD algorithm it is very simple.\nIn our setting, the ADMM method [11, 61, 42] involves solving the following minimax\n37\nproblem3,\nmax \u03bb1,...,\u03bbp min x1,...,xp,x\u0303 p\u2211 i=1 F (xi)\u2212 \u03bbi(xi \u2212 x\u0303) + \u03c1 2 \u2016xi \u2212 x\u0303\u20162, (3.51)\nwhere \u03bbi\u2019s are the Lagrangian multipliers. The resulting updates of the ADMM algorithm in the round-robin scheme are given next. Let t \u2265 0 be a global clock. At each t,\nwe linearize the function F (xi) with F (xit)+ \u2329 \u2207F (xit), xi \u2212 xit \u232a + 12\u03b7 \u2225\u2225xi \u2212 xit\u2225\u22252 as in [42]. The updates become\n\u03bbit+1 =  \u03bb i t \u2212 (xit \u2212 x\u0303t) if mod (t, p) = i\u2212 1;\n\u03bbit if mod (t, p) 6= i\u2212 1. (3.52)\nxit+1 =  xit\u2212\u03b7\u2207F (xit)+\u03b7\u03c1(\u03bbit+1+x\u0303t) 1+\u03b7\u03c1 if mod (t, p) = i\u2212 1;\nxit if mod (t, p) 6= i\u2212 1. (3.53)\nx\u0303t+1 = 1\np p\u2211 i=1 (xit+1 \u2212 \u03bbit+1). (3.54)\nEach local variable xi is periodically updated (with period p). First, the Lagrangian multiplier \u03bbi is updated with the dual ascent update as in Equation 3.52. It is followed by the gradient descent update of the local variable as given in Equation 3.53. Then the center variable x\u0303 is updated with the most recent values of all the local variables and Lagrangian multipliers as in Equation 3.54. Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbit \u2190 \u03bbit/\u03c1 in the above updates.\nThe EASGD algorithm in the round-robin scheme is defined similarly and is given below\nxit+1 =  x i t \u2212 \u03b7\u2207F (xit)\u2212 \u03b1(xit \u2212 x\u0303t) if mod (t, p) = i\u2212 1;\nxit if mod (t, p) 6= i\u2212 1. (3.55)\nx\u0303t+1 = x\u0303t + \u2211\ni: mod (t,p)=i\u22121\n\u03b1(xit \u2212 x\u0303t). (3.56)\n3The convergence analysis in [61] is based on the assumption that \u201cAt any master iteration, updates from the workers have the same probability of arriving at the master.\u201d, which is not satisfied in the round-robin scheme.\n38\nAt time t, only the i-th local worker (whose index i\u2212 1 equals t modulo p) is activated, and performs the update in Equations 3.55 which is followed by the master update given in Equation 3.56.\nWe will now focus on the one-dimensional quadratic case without noise, i.e.\nF (x) = x2\n2 , x \u2208 R.\nFor the ADMM algorithm, let the state of the (dynamical) system at time t be st = (\u03bb1t , x 1 t , . . . , \u03bb p t , x p t , x\u0303t) \u2208 R2p+1. The local worker i\u2019s updates in Equations 3.52, 3.53, and 3.54 are composed of three linear maps which can be written as st+1 = (F i 3 \u25e6 F i2 \u25e6 F i1)(st). For simplicity, we will only write them out below for the case when i = 1 and p = 2:\nF 11=  1 \u22121 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n0 0 0 0 1\n , F 12=  1 0 0 0 0 \u03b7\u03c1 1+\u03b7\u03c1 1\u2212\u03b7 1+\u03b7\u03c1 0 0 \u03b7\u03c1 1+\u03b7\u03c1 0 0 1 0 0 0 0 0 1 0\n0 0 0 0 1\n , F 13=  1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n\u2212 1p 1p \u2212 1p 1p 0\n .\nFor each of the p linear maps, it\u2019s possible to find a simple condition such that each map, where the ith map has the form F i3 \u25e6 F i2 \u25e6 F i1, is stable (the absolute value of the eigenvalues of the map are smaller or equal to one). However, when these non-symmetric maps are composed one after another as follows F = F p3 \u25e6 F p2 \u25e6 F p1 \u25e6 . . . \u25e6 F 13 \u25e6 F 12 \u25e6 F 11 , the resulting map F can become unstable! (more precisely, some eigenvalues of the map can sit outside the unit circle in the complex plane).\nWe now present the numerical conditions for which the ADMM algorithm becomes unstable in the round-robin scheme for p = 3 and p = 8, by computing the largest absolute eigenvalue of the map F . Figure 3.2 summarizes the obtained result. We also illustrate this unstable behavior in Figure 3.3\n39\nOn the other hand, the EASGD algorithm involves composing only symmetric linear maps due to the elasticity. Let the state of the (dynamical) system at time t be st = (x1t , . . . , x p t , x\u0303t) \u2208 Rp+1. The activated local worker i\u2019s update in Equation 3.55 and the master update in Equation 3.56 can be written as st+1 = F i(st). In case of p = 2, the map F 1 and F 2 are defined as follows\nF 1=  1\u2212 \u03b7 \u2212 \u03b1 0 \u03b1 0 1 0\n\u03b1 0 1\u2212 \u03b1\n, F 2=  1 0 0 0 1\u2212 \u03b7 \u2212 \u03b1 \u03b1\n0 \u03b1 1\u2212 \u03b1  For the composite map F p \u25e6 . . .\u25e6F 1 to be stable, the condition that needs to be satisfied is actually the same for each i, and is furthermore independent of p (since each linear map F i is symmetric). It essentially involves the stability of the 2\u00d7 2 matrix\n 1\u2212 \u03b7 \u2212 \u03b1 \u03b1 \u03b1 1\u2212 \u03b1  , whose two (real) eigenvalues \u03bb satisfy (1 \u2212 \u03b7 \u2212 \u03b1 \u2212 \u03bb)(1 \u2212 \u03b1 \u2212 \u03bb) = \u03b12. The resulting stability condition (|\u03bb| \u2264 1) is simple and given as\n0 \u2264 \u03b7 \u2264 2, 0 \u2264 \u03b1 \u2264 4\u2212 2\u03b7 4\u2212 \u03b7 .\n40\n41\n42\nChapter 4"}, {"heading": "Performance in Deep Learning", "text": "In this chapter, we compare empirically the performance in deep learning of asynchronous EASGD and EAMSGD with the parallel method DOWNPOUR and the sequential method SGD, as well as their averaging and momentum variants.\nAll the parallel comparator methods are listed below1:\n\u2022 DOWNPOUR [16], the detail and the pseudo-code of the implementation are de-\nscribed in Section 4.4 (Algorithm 3).\n\u2022 Momentum DOWNPOUR (MDOWNPOUR), where the Nesterov\u2019s momentum\nscheme is applied to the master\u2019s update (note it is unclear how to apply it to the local workers or for the case when \u03c4 > 1). The pseudo-code is described in Section 4.4 (Algorithms 4 and 5). \u2022 A method that we call ADOWNPOUR, where we compute the average over time\nof the center variable x\u0303 as follows: zt+1 = (1 \u2212 \u03b1t+1)zt + \u03b1t+1x\u0303t, and \u03b1t+1 = 1t+1 is a moving rate, and z0 = x\u03030. The t denotes the master clock, which is initialized to 0 and incremented every time the center variable x\u0303 is updated. \u2022 A method that we call MVADOWNPOUR, where we compute the moving average 1We have compared asynchronous ADMM [61] with EASGD in our setting as well, the performance is nearly the same. However, ADMM \u2019s momentum variant is not as stable when using large communication period \u03c4 .\n43\nof the center variable x\u0303 as follows: zt+1 = (1 \u2212 \u03b1)zt + \u03b1x\u0303t, and the moving rate \u03b1 was chosen to be constant, and z0 = x\u03030. The t denotes the master clock and is defined in the same way as for the ADOWNPOUR method.\nAll the sequential comparator methods (p = 1) are listed below:\n\u2022 SGD [9] with constant learning rate \u03b7. \u2022 Momentum SGD (MSGD) [55] with constant (Nesterov\u2019s) momentum rate \u03b4. \u2022 ASGD [45] with moving rate \u03b1t+1 = 1t+1 . \u2022 MVASGD [45] with moving rate \u03b1 set to a constant.\nWe perform experiments on two benchmark datasets: CIFAR-10 (we refer to it as CIFAR)2 and ImageNet ILSVRC 2013 (we refer to it as ImageNet)3. We focus on the image classification task with deep convolutional neural networks. We first explain the experimental setup in Section 4.1 and then present the main experimental results in Section 4.2. We present further experimental results in Section 4.3 and discuss the effect of the averaging, the momentum, the learning rate, the communication period, the data and parameter communication tradeoff, and finally the speedup."}, {"heading": "4.1 Experimental setup", "text": "For all our experiments we use a GPU-cluster interconnected with InfiniBand. Each node has 4 Titan GPU processors where each local worker corresponds to one GPU processor. The center variable of the master is stored and updated on the centralized parameter server [16]. Our implementation is available at https://github.com/sixin-zh/mpiT.\nTo describe the architecture of the convolutional neural network, we will first introduce a notation. Let (c, x, y) denotes the size of the input image to each layer, where c is the number of color channels and (x, y) denotes the horizontal and the vertical dimension of\n2Downloaded from http://www.cs.toronto.edu/~kriz/cifar.html. 3Downloaded from http://image-net.org/challenges/LSVRC/2013.\n44\nthe input. Let C denotes the fully-connected convolutional operator and let R denotes the rectified linear non-linearity (relu, c.f. [35]), P denotes the max pooling operator, L denotes the linear operator and D denotes the dropout operator with rate equal to 0.5 and S denotes the the softmax nonlinearity. We use the cross-entropy loss for the classification.\nFor the ImageNet experiment we use the similar approach to [49] with the following 11-layer convolutional neural network: (3, 221, 221) C,R\u2212\u2212\u2212\u2212\u2212\u2192\n(7,7,2,2) (96, 108, 108) P\u2212\u2212\u2212\u2212\u2212\u2192 (3,3,3,3)\n(96, 36, 36) C,R\u2212\u2212\u2212\u2212\u2212\u2192\n(5,5,1,1) (256, 32, 32) P\u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (256, 16, 16) C,R\u2212\u2212\u2212\u2212\u2212\u2192 (3,3,1,1) (384, 14, 14) C,R\u2212\u2212\u2212\u2212\u2212\u2192 (2,2,1,1)\n(384, 13, 13) C,R\u2212\u2212\u2212\u2212\u2212\u2192\n(2,2,1,1) (256, 12, 12) P\u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (256, 6, 6) L,R,D\u2212\u2212\u2212\u2212\u2192 0.5 (4096, 1, 1) L,R,D\u2212\u2212\u2212\u2212\u2192 0.5\n(4096, 1, 1) L,S\u2212\u2212\u2192 (1000, 1, 1).\nFor the CIFAR experiment we use the similar approach to [58] with the following 7-layer convolutional neural network: (3, 28, 28) C,R\u2212\u2212\u2212\u2212\u2212\u2192\n(5,5,1,1) (64, 24, 24) P\u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (64, 12, 12) C,R\u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1)\n(128, 8, 8) P\u2212\u2212\u2212\u2212\u2212\u2192\n(2,2,2,2) (128, 4, 4) C,R\u2212\u2212\u2212\u2212\u2212\u2192 (3,3,1,1) (64, 2, 2) L,R,D\u2212\u2212\u2212\u2212\u2192 0.5 (256, 1, 1) L,S\u2212\u2212\u2192 (10, 1, 1).\nNote that the numbers below the rightarrow of the C and P operator represent the kernel size (first horizontal and then vertical), the stride size (first horizontal and then vertical) and the padding size (if exists, first horizontal and then vertical) on each of the two sides of the image. The number below the rightarrow of the D operator emphasizes the dropout rate 0.5 [54].\nIn our experiments, all the methods we run use the same initial parameter chosen randomly, except that we set all the biases to zero for CIFAR case and to 0.1 for ImageNet case. This parameter is used to initialize the master and all the local workers4. We add l2-regularization \u03bb 2 \u2016x\u2016 2 to the loss function F (x). For ImageNet we use \u03bb = 10\u22125 and for CIFAR we use \u03bb = 10\u22124. We also compute the stochastic gradient using mini-batches of sample size 128.\n4On the contrary, initializing the local workers and the master with different random seeds \u2019traps\u2019 the algorithm in the symmetry breaking phase.\n45"}, {"heading": "Data preprocessing", "text": "For the ImageNet experiment, we re-size each RGB image so that the smallest dimension is 256 pixels. We also re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal flips) of size 3 \u00d7 221 \u00d7 221 pixels and present these to the network in mini-batches of size 128.\nFor the CIFAR experiment, we use the original RGB image of size 3 \u00d7 32 \u00d7 32. As before, we re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal flips) of size 3\u00d7 28\u00d7 28 pixels and present these to the network in mini-batches of size 128.\nThe training and test loss and the test error are only computed from the center patch (3 \u00d7 28 \u00d7 28) for the CIFAR experiment and the center patch (3 \u00d7 221 \u00d7 221) for the ImageNet experiment.\nData prefetching (Sampling the dataset by the local workers in parallel)\nWe will now explain precisely how the dataset is sampled by each local worker as uniformly and efficiently as possible. The general parallel data loading scheme on a single machine is as follows: we use k CPUs, where k = 8, to load the data in parallel. Each data loader reads from the memory-mapped (mmap) file a chunk of c raw images (preprocessing was described in the previous subsection) and their labels (for CIFAR c = 512 and for ImageNet c = 64). For the CIFAR, the mmap file of each data loader contains the entire dataset whereas for ImageNet, each mmap file of each data loader contains different 1/k fractions of the entire dataset. A chunk of data is always sent by one of the data loaders to the first worker who requests the data. The next worker requesting the data from the same data loader will get the next chunk. Each worker requests in total k data chunks from k different data loaders and then process them before asking for new data chunks. Notice that each data loader cycles5 through the\n5Its advantage is observed in [10].\n46\ndata in the mmap file, sending consecutive chunks to the workers in order in which it receives requests from them. When the data loader reaches the end of the mmap file, it selects the address in memory uniformly at random from the interval [0, s], where s = (number of images in the mmap file modulo mini-batch size), and uses this address to start cycling again through the data in the mmap file. After the local worker receives the k data chunks from the data loaders, it shuffles them and divides it into mini-batches of size 128."}, {"heading": "4.2 Experimental results", "text": "For all experiments in this section we use EASGD with \u03b2 = 0.9 and \u03b1 = \u03b2/p, for all momentum-based methods we set the momentum term \u03b4 = 0.99 and finally for MVADOWNPOUR we set the moving rate to \u03b1 = 0.001. We start with the experiment on CIFAR dataset with p = 4 local workers running on a single computing node.\nFor all the methods, we examined the communication periods from the following set \u03c4 = {1, 4, 16, 64}. For each method we examined a wide range of learning rates. The learning rates explored in all experiments are summarized in Table 4.1, 4.2 and 4.3. The CIFAR experiment was run 3 times independently from the same random initialization and for each method we report its best performance measured by the smallest achievable test error.\nFrom the results in Figure 4.1, 4.2, 4.3 and 4.4, we conclude that all DOWNPOUR-based methods achieve their best performance (test error) for small \u03c4 (\u03c4 \u2208 {1, 4}), and become highly unstable for \u03c4 \u2208 {16, 64}. While EAMSGD significantly outperforms comparator methods for all values of \u03c4 by having faster convergence. It also finds better-quality solution measured by the test error and this advantage becomes more significant for \u03c4 \u2208 {16, 64}. Note that the tendency to achieve better test performance with larger \u03c4 is also characteristic for the EASGD algorithm. We remark that if the stochastic gradient is sparse, DOWNPOUR empirically performs well with large communication period [20].\n47\n49\n50\n51\n52\n53\n54\n55\n56\n57\nWe next explore different number of local workers p from the set p = {4, 8, 16} for the CIFAR experiment, and p = {4, 8} for the ImageNet experiment6. For the ImageNet experiment we report the results of one run with the best setting we have found. EASGD and EAMSGD were run with \u03c4 = 10 whereas DOWNPOUR and MDOWNPOUR were run with \u03c4 = 1.\nFor the CIFAR experiment, the results are in Figure 4.5, 4.6 and 4.7. EAMSGD achieves significant accelerations compared to other methods, e.g. the relative speedup for p = 16 (the best comparator method is then MSGD) to achieve the test error 21% equals 11.1. It\u2019s noticeable that the smallest achievable test error by either EASGD or EAMSGD decreases with larger p. This can potentially be explained by the fact that larger p allows for more exploration of the parameter space. In the next section, we discuss further the trade-off between exploration and exploitation as a function of the learning rate (section 4.3.2) and the communication period (section 4.3.3).\nFor the ImageNet experiment, the results are in Figure 4.8 and 4.9. The difficulty in this task is that we need to manually reduce the learning rate, otherwise the training loss will stagnate. Thus our initial learning rate is decreased twice over time, by a factor of 5 and then 2, when we observe that the online predictive loss [12] stagnates. EAMSGD again achieves significant accelerations compared to other methods, e.g. the relative speedup for p = 8 (the best comparator method is then DOWNPOUR) to achieve the test error 49% equals 1.8, and simultaneously it reduces the communication overhead (DOWNPOUR uses communication period \u03c4 = 1 and EAMSGD uses \u03c4 = 10). However, there\u2019s an annealing effect here in the sense that depending on the time the learning rate is reduced, the final test performance can be quite different. This makes the performance comparison difficult to define. In general, this is also a difficulty in comparing the NPhard problem solvers.\n6For the ImageNet experiment, the training loss is measured on a subset of the training data of size 50,000.\n58"}, {"heading": "4.3 Further discussion and understanding", "text": "4.3.1 Comparison of SGD, ASGD, MVASGD and MSGD\nFor comparison we also report the performance of MSGD which outperformed SGD, ASGD and MVASGD on the test dataset. Recall that the way we compare the performance between different methods is based on the smallest achievable test error. Since the test dataset is fixed a prior, we may have the tendency to overfit this test dataset. Indeed, as we shall see in the Figure 4.10. One could use cross-validation to remedy this, we however emphasize that the point here is not to seek the best possible test accuracy, but to see all the possibilities that we can find, i.e. the richness of the dynamics arising from the neural network. We are aware that we could not exhaust all the possibilities.\nFigure 4.10 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment. We observe that the final test performance of ASGD and MSGD are quite close to each other. But ASGD is much faster from the beginning. This explains why we do not see much speedup of the EASGD method (e.g. in Figure 4.1, 4.2, 4.3) and can sometimes be even slower (e.g. in Figure 4.4). It is caused by the sensitivity of the test performance to the choice of the learning rate. We shall discuss this phenomenon further in the next section 4.3.2.\nFigure 4.11 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment. Note that for all CIFAR experiments we always start the averaging for the ADOWNPOUR and ASGD methods from the very beginning of each experiment. But for the ImageNet experiments we start the averaging for the ASGD and MVASGD at the first time when we reduce the learning rate. We have tried to start the averaging from the right beginning, both of the training and test performance are poor and they look very similar to the\n59\n60\n61\nASGD curve in Figure 4.11. The big difference between ASGD and MVASGD is quite striking and worth further study."}, {"heading": "4.3.2 Dependence of the learning rate", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the learning rate. We compare the performance of respectively EAMSGD and EASGD for different learning rates \u03b7 when p = 16 and \u03c4 = 10 on the CIFAR experiment. We observe in Figure 4.12 that higher learning rates \u03b7 lead to better test performance for the EAMSGD algorithm which potentially can be justified by the fact that they sustain higher fluctuations of the local workers. We conjecture that higher fluctuations lead to more exploration and simultaneously they also impose higher regularization. This picture however seems to be opposite for the EASGD algorithm for which larger learning rates hurt the performance of the method and lead to overfitting. Interestingly in this experiment for both EASGD and EAMSGD algorithm, the learning rate for which the best training performance was achieved simultaneously led to the worst test performance."}, {"heading": "4.3.3 Dependence of the communication period", "text": "This section discusses the dependence of the trade-off between exploration and exploitation on the communication period. We observe in Figure 4.13 that EASGD algorithm exhibits very similar convergence behavior when \u03c4 = 1 up to even \u03c4 = 1000 for the CIFAR experiment, whereas EAMSGD can get trapped at a quite high energy level (of the objective) when \u03c4 = 100. This trapping behavior is due to the non-convexity of the objective function. It can be avoided by gradually decreasing the learning rate, i.e. increasing the penalty term \u03c1 (recall \u03b1 = \u03b7\u03c1), as shown in Figure 4.13. In contrast, the EASGD algorithm does not seem to get trapped by any saddle point at all along its trajectory.\n62\n63\nThe performance7 of EASGD being less sensitive to the communication period compared to EAMSGD is another striking observation.\nIt\u2019s also very important to notice the tail behavior of the asynchronous EASGD method, i.e. what would happen if some local worker had finished the gradient updates and stopped the communication with the master. In the EAMSGD case in Figure 4.13, we see that the final training loss and test error can both become worse. This is due to the situation that some of the local workers have stopped earlier than the others, so that the averaging effect on the center variable is diminished."}, {"heading": "4.3.4 The tradeoff between data and parameter communication", "text": "In addition, we report in Table 4.4 the breakdown of the total running time for EASGD when \u03c4 = 10 (the time breakdown for EAMSGD is almost identical) and DOWNPOUR when \u03c4 = 1 into computation time, data loading time and parameter communication time. For the CIFAR experiment the reported time corresponds to processing 400 \u00d7 128 data samples whereas for the ImageNet experiment it corresponds to processing 1024\u00d7 128 data samples. For \u03c4 = 1 and p \u2208 {8, 16} we observe that the communication time accounts for significant portion of the total running time whereas for \u03c4 = 10 the communication time becomes negligible compared to the total running time (recall that based on previous results EASGD and EAMSGD achieve best performance with larger \u03c4 which is ideal in the setting when communication is time-consuming).\nWe shall now examine the data communication cost in detail. Let\u2019s focus on the ImageNet case. Based on the Table 4.4, for each single GPU (p = 1, \u03c4 = 1), it takes around 1248 seconds to process 1024 mini-batches of size 128. This is approximately processing one mini-batch per second. Each mini-batch consists of 128 \u00d7 3 \u00d7 221 \u00d7 221 pixels. If each 7Compared to all earlier results, the experiment in this section is re-run three times with a new random seed and with faster cuDNN package on two Tesla K80 nodes (developer.nvidia.com/cuDNN and github.com/soumith/cudnn.torch). Also to clarify, the random initialization we use is by default in Torch\u2019s implementation. All our methods are implemented in Torch (torch.ch). The Message Passing Interface implementation MVAPICH2 (mvapich.cse.ohio-state.edu) is used for the GPU-CPU communication.\n64\n65\npixel value is represented by one byte, then each mini-batch is around 18 MB. As the whole dataset has around 1,300,000 images, it would take 174 GB. In fact, we have compressed the images as JPEG, so that the whole dataset is only around 36 GB. Thus we gain a compression ratio around 1/5, i.e. we can assume each mini-batch size is 18/5 = 3.6 MB. The required data communication rate is thus 3.6 MB/sec. On the other hand, the DOWNPOUR method with \u03c4 = 1 requires communicating the whole model parameter per mini-batch. As the parameter size is around 233 MB, we need the network bandwidth at least 233 MB/sec per local worker (we have not even accounted for the gradient communication, which can double this cost). The parameter communication cost is thus at least 66 times of the data communication cost. In the ImageNet case, having access to the full dataset by the local workers is indeed a good tradeoff."}, {"heading": "4.3.5 Time speed-up", "text": "In Figure 4.14 and 4.15, we summarize the wall clock time needed to achieve the same level of the test error for all the methods in the CIFAR and ImageNet experiment as a function of the number of local workers p. For the CIFAR (Figure 4.14) we examined the following levels: {21%, 20%, 19%, 18%} and for the ImageNet (Figure 4.15) we examined: {49%, 47%, 45%, 43%}. If some method does not appear on the figure for a given test error level, it indicates that this method never achieved this level. For the CIFAR experiment we observe that from among EASGD, DOWNPOUR and MDOWNPOUR methods, the EASGD method needs less time to achieve a particular level of test error.\n66\nWe observe that with higher p each of these methods does not necessarily need less time to achieve the same level of test error. This seems counter intuitive though recall that the learning rate for the methods is selected based on the smallest achievable test error. For larger p smaller learning rates were selected than for smaller p which explains our results. Meanwhile, the EAMSGD method achieves significant speed-up over other methods for all the test error levels. For the ImageNet experiment we observe that all methods outperform MSGD and furthermore with p = 4 or p = 8 each of these methods requires less time to achieve the same level of test error."}, {"heading": "4.4 Additional pseudo-codes of the algorithms", "text": ""}, {"heading": "DOWNPOUR pseudo-code", "text": "Algorithm 3 captures the pseudo-code of the implementation of DOWNPOUR used in this paper. Similar to the asynchronous behavior of EASGD that we have described in Chapter 2 (Section 2.2), DOWNPOUR method also performs several steps of local gradient updates by each worker i before pushing back the accumulated gradients vi to the center variable. To be more precise, at the beginning of each period, the i-th worker reads a new center variable x\u0303 from the parameter server. Then it performs \u03c4 local SGD steps from the new x\u0303. All the gradients are accumulated (added) and at the end of that period, the total sum vi is pushed (added) back to the parameter server. The center variable is updated by summing the accumulated gradients from any of the local workers. Notice that we do not use any adaptive learning scheme as having been done in [16]."}, {"heading": "MDOWNPOUR pseudo-code", "text": "Algorithms 4 and 5 capture the pseudo-codes of the implementation of momentum DOWNPOUR (MDOWNPOUR) used in this paper. Algorithm 4 describes the behavior of each local worker and Algorithm 5 describes the behavior of the master. Note that\n67\n.\n68\n.\n69\nAlgorithm 3: DOWNPOUR: Processing by worker i and the master\nInput: learning rate \u03b7, communication period \u03c4 \u2208 N Initialize: x\u0303 is initialized randomly, xi = x\u0303, vi = 0, ti = 0 Repeat if (\u03c4 divides ti) then\nx\u0303 \u2190 x\u0303 + vi xi \u2190 x\u0303 vi \u2190 0\nend xi \u2190 xi \u2212 \u03b7gi\nti (xi)\nvi \u2190 vi \u2212 \u03b7gi ti (xi) ti \u2190 ti + 1\nUntil forever\nunlike the DOWNPOUR method, we do not use the communication period \u03c4 . This is because the Nesterov\u2019s momentum is applied to the center variable. Each worker reads an interpolated variable x\u0303 + \u03b4v from the master, and then sends back the stochastic gradient evaluated at that point. In case p = 1, MDOWNPOUR is equivalent to the MSGD method.\nAlgorithm 4: MDOWNPOUR: Processing by worker i\nInitialize: xi = x\u0303 Repeat Receive x\u0303+ \u03b4v from the master: xi \u2190 x\u0303+ \u03b4v Compute gradient gi = gi(xi) Send gi to the master Until forever\nAlgorithm 5: MDOWNPOUR: Processing by the master\nInput: learning rate \u03b7, momentum term \u03b4 Initialize: x\u0303 is initialized randomly, vi = 0, Repeat Receive gi\nv \u2190 \u03b4v \u2212 \u03b7gi x\u0303\u2190 x\u0303+ v Send x\u0303+ \u03b4v\nUntil forever\n70\nChapter 5"}, {"heading": "The Limit in Speedup", "text": "This chapter studies the limitation in speedup of several stochastic optimization methods: the mini-batch SGD, the momentum SGD and the EASGD method. In Section 5.1, we study first the asymptotic phase of these methods using an additive noise model. The continuous-time SDE (stochastic differential equation) approximation of its SGD update is an Ornstein Uhlenbeck process. Then we study the initial phase of these methods using a multiplicative noise model in Section 5.2. The continuous-time SDE approximation of its SGD update is a Geometric Brownian motion. In Section 5.3, we study the stability of the critical points of a simple non-convex problem and discuss when the EASGD method can get trapped by a saddle point."}, {"heading": "5.1 Additive noise", "text": "We (re-)study the simple additive noise model: one-dimensional quadratic objective with Gaussian noise (as in Section 3.1.1). The objective function evaluated at state x \u2208 R is defined to be the average loss of the quadratic form (hx\u2212 \u03be)2, i.e.\nmin x\u2208R\nE[(hx\u2212 \u03be)2]. (5.1)\n71\nHere h > 0 is a scalar, and the expectation is taken over the random variable \u03be, which follows a Gaussian distribution. For simplicity, we assume further that \u03be is zero mean and has a constant variance \u03c32 > 0.\n5.1.1 SGD with mini-batch\nThe update rule for the SGD method for solving the problem in Equation 5.1 is\nxt+1 = xt \u2212 \u03b7(hxt \u2212 \u03bet), (5.2)\nwhere x0 is the initial starting point.\nNotice that in the continuous-time limit, i.e. for small \u03b7, we can approximate the process in Equation 5.2 by an Ornstein Uhlenbeck process [32] as follows,\ndX(t) = \u2212hX(t)dt+\u221a\u03b7\u03c3dB(t).\nIn the discrete-time case, the bias term (the first-order moment Ext) in Equation 5.2 will decrease at a linear rate 1\u2212 \u03b7h, i.e.\nExt+1 = (1\u2212 \u03b7h)Ext.\nThe second-order moment Ex2t changes as\nEx2t+1 = (1\u2212 \u03b7h)2Ex2t + \u03b72\u03c32. (5.3)\nThus the variance will increase as\nVxt+1 = Ex2t+1 \u2212 (Ext+1)2 = (1\u2212 \u03b7h)2Ex2t + \u03b72\u03c32 \u2212 (1\u2212 \u03b7h)2(Ext)2\n= (1\u2212 \u03b7h)2Vxt + \u03b72\u03c32,\n72\nwith Vx0 = 0. As t \u2192 \u221e, we get Vx\u221e = \u03b7 2\n1\u2212(1\u2212\u03b7h)2\u03c3 2. If we use mini-batch of size p,\nthe variance of the noise \u03bet is then reduced by p, and this asymptotic variance becomes\n\u03b72 1\u2212(1\u2212\u03b7h)2 \u03c32 p . The convergence rate of the bias term, 1\u2212 \u03b7h, is however not improved by increasing the mini-batch size p.\n5.1.2 Momentum SGD\nWe now study the update rule for the MSGD (Nesterov\u2019s momentum) method for solving Equation 5.1,\nvt+1 = \u03b4vt \u2212 \u03b7(h(xt + \u03b4vt)\u2212 \u03bet), xt+1 = xt + vt+1, (5.4)\nwhere x0 is the initial starting point and v0 = 0 is the initial velocity set to zero.\nLet \u03b4h = \u03b4(1\u2212 \u03b7h), \u03b7h = \u03b7h, then Equation 5.4 is equivalent to\nvt+1 = \u03b4hvt \u2212 \u03b7hxt + \u03b7\u03bet, xt+1 = xt + vt+1.\nIn case that \u03b4h is chosen independently of h, the update is no different to the heavy ball method [44]. But the momentum term \u03b4h above equals \u03b4(1\u2212 \u03b7h), thus it implicitly depends on h. As we usually choose \u03b4 to be smaller than one, \u03b4h is upper bounded by 1\u2212 \u03b7h. This fact saves us from the variance explosion as \u03b4 tends to one.\nMore precisely, the second-order moment equation can be computed as follows,\nv2t+1 = (\u03b4hvt \u2212 \u03b7hxt + \u03b7\u03bet)2\n= \u03b42hv 2 t + \u03b7 2 hx 2 t + \u03b7 2\u03be2t \u2212 2\u03b4h\u03b7hvtxt + 2\u03b4h\u03b7vt\u03bet \u2212 2\u03b7h\u03b7xt\u03bet x2t+1 = (xt + vt+1) 2 = x2t + 2vt+1xt + v 2 t+1\nvt+1xt = (\u03b4hvt \u2212 \u03b7hxt + \u03b7\u03bet)xt = \u03b4hvtxt \u2212 \u03b7hx2t + \u03b7xt\u03bet vt+1xt+1 = vt+1(xt + vt+1) = vt+1xt + v 2 t+1.\n(5.5)\n73\nNow taking expectation on both sides of the Equation 5.5, we obtain the following recursive relation Ev2t+1 Evt+1xt+1\nEx2t+1\n =  \u03b42h \u22122\u03b4h\u03b7h \u03b72h \u03b42h \u03b4h(1\u2212 2\u03b7h) \u2212\u03b7h(1\u2212 \u03b7h)\n\u03b42h 2\u03b4h(1\u2212 \u03b7h) (1\u2212 \u03b7h)2  \ufe38 \ufe37\ufe37 \ufe38\nM\n Ev2t Evtxt\nEx2t\n+  \u03b72\u03c32 \u03b72\u03c32\n\u03b72\u03c32\n . (5.6)\nTo see the asymptotic behavior, assume v2\u221e = limt\u2192\u221e Ev2t , vx\u221e = limt\u2192\u221e Evtxt, and x2\u221e = limt\u2192\u221e Ex2t . Then by solving v2\u221e vx\u221e\nx2\u221e\n =  \u03b42h \u22122\u03b4h\u03b7h \u03b72h \u03b42h \u03b4h(1\u2212 2\u03b7h) \u2212\u03b7h(1\u2212 \u03b7h)\n\u03b42h 2\u03b4h(1\u2212 \u03b7h) (1\u2212 \u03b7h)2\n  v2\u221e vx\u221e\nx2\u221e\n+  \u03b72\u03c32 \u03b72\u03c32\n\u03b72\u03c32\n ,\nwe obtain\nv2\u221e = 2\n(1\u2212 \u03b4h)(2(1 + \u03b4h)\u2212 \u03b7h) \u03b72\u03c32,\nvx\u221e = 1\n(1\u2212 \u03b4h)(2(1 + \u03b4h)\u2212 \u03b7h) \u03b72\u03c32,\nx2\u221e = 1 + \u03b4h\n\u03b7h(1\u2212 \u03b4h)(2(1 + \u03b4h)\u2212 \u03b7h) \u03b72\u03c32. (5.7)\nFrom Equation 5.7, we see that for the asymptotic variance x2\u221e to be strictly positive, we should assume \u22121 < \u03b4h < 1 and 0 < \u03b7h < 2(1 + \u03b4h). Moreover, compared to the asymptotic variance of SGD (the case that \u03b4 = 0), we can check, for example, that in the region \u03b7h \u2208 (0, 1) and \u03b4h \u2208 (0, 1), the asymptotic variance of MSGD is always larger.\nOn the other hand, the above condition \u22121 < \u03b4h < 1 and 0 < \u03b7h < 2(1 + \u03b4h) is also the condition for the matrix M in Equation 5.6 to remain (strictly) stable, i.e. the largest absolute eigenvalue is (strictly) smaller than one. In fact, we have three eigenvalues for\n74\nthe matrix M as follows,\nz1 = \u03b4h, z2 = (1\u2212\u03b7h)2\u22122\u03b7h\u03b4h+\u03b42h\u2212 \u221a ((1\u2212\u03b7h)2\u22122\u03b7h\u03b4h+\u03b42h)2\u22124\u03b4 2 h\n2 ,\nz3 = (1\u2212\u03b7h)2\u22122\u03b7h\u03b4h+\u03b42h+\n\u221a ((1\u2212\u03b7h)2\u22122\u03b7h\u03b4h+\u03b42h)2\u22124\u03b4 2 h\n2 .\n(5.8)\nLet 2b = z2 + z3 = (1\u2212 \u03b7h)2\u2212 2\u03b7h\u03b4h + \u03b42h and c = z2z3 = \u03b42h. Then z2 and z3 are the two roots (in z) of z2 \u2212 2bz + c = 0. In general, if z2, z3 are real-valued, i.e. b2 \u2212 c > 0, then we have two cases:\n\u2022 b > \u221ac implies z3 = b+ \u221a b2 \u2212 c > \u221ac > z2 = b\u2212 \u221a b2 \u2212 c > 0, \u2022 b < \u2212\u221ac implies z2 = b\u2212 \u221a b2 \u2212 c < \u2212\u221ac < z3 = b+ \u221a b2 \u2212 c < 0.\nOn the other hand, if z2, z3 are not real-valued, i.e. b 2 \u2212 c \u2264 0, then |z3| = |z1| = |z2| = \u221a c. Recall that |z1| = |\u03b4h| = \u221a c. We see thus for the (strict) stability of the matrix M in Equation 5.6, we need \u221a c = |\u03b4h| < 1 and |z3| < 1 because the condition b < \u2212 \u221a c is never satisfied for any real \u03b4h. The condition for |z3| = b+ \u221a b2 \u2212 c < 1 is that 2b\u2212 c < 1, i.e. 0 < \u03b7h < 2(1 + \u03b4h).\nMoreover, as in [32], we can try to minimize |z3| with respect to \u03b4 such that the rate of convergence of the second order moment is maximized for a given \u03b7. We shall prove that the minimal |z3| is achieved at \u03b4h = ( \u221a \u03b7h \u2212 1)2. In fact, it happens when b = \u221a c, i.e. the optimal rate is at the edge where the eigenvalues transit from real-valued to the complex-valued. Notice that b = \u221a c gives us two positive solutions: \u03b4h = ( \u221a \u03b7h \u2212 1)2 and \u03b4h = ( \u221a \u03b7h + 1) 2. Since b = 12 [(\u03b4h \u2212 \u03b7h)2 + 1\u2212 2\u03b7h] is a quadratic function in \u03b4h, the fact that b = \u221a c has two positive solutions means that the quadratic function intersects twice with the line \u03b4h in the first orthant. If \u03b7h \u2265 1/4, the first intersection point is to the left of the minimum of b, and the second intersection point is to the right of the minimum, i.e. ( \u221a \u03b7h \u2212 1)2 \u2264 \u03b7h < ( \u221a \u03b7h + 1) 2. But if 0 < \u03b7h < 1/4, both intersection points will be both to the right of the minimum of b, i.e. \u03b7h < ( \u221a \u03b7h\u2212 1)2 < ( \u221a \u03b7h + 1) 2.\n75\nNevertheless, in either case, we can show b > \u221a c whenever \u03b4h < ( \u221a \u03b7h \u2212 1)2. Thus, in the range \u03b4h < ( \u221a \u03b7h \u2212 1)2, we have z3 > \u221a c > z2 > 0. We thus only need to find the minimum of z3 in the range \u03b4h \u2208 (\u22121, ( \u221a \u03b7h \u2212 1)2], because |z3| = \u03b4h is monotonically increasing in the range 1 > \u03b4h > ( \u221a \u03b7h \u2212 1)2. We now show that the minimal value of z3 is ( \u221a \u03b7h \u2212 1)2 in this range. In fact, if \u03b7h \u2265 1/4, b is monotonically decreasing in this range, thus z3 \u2265 b reaches its minimum at \u03b4h = ( \u221a \u03b7h \u2212 1)2 with z3 = b. If 0 < \u03b7h < 1/4, b firstly decreases for \u03b4h < \u03b7h, then increases for \u03b7h \u2264 \u03b4h \u2264 (1 \u2212 \u221a \u03b7h) 2. We show that z3 is still monotonically decreasing in these two ranges of \u03b4h. We check whether \u2202z3\u2202\u03b4h = (\u03b4h \u2212 \u03b7h)(1 + b\u221a b2\u2212c)\u2212 \u03b4h 1\u221a b2\u2212c < 0 holds. For \u03b4h < \u03b7h, this is equivalent to z3 > \u03b4h \u03b4h\u2212\u03b7h ; for \u03b7h < \u03b4h \u2264 (1\u2212 \u221a \u03b7h) 2, this is equivalent to z3 < \u03b4h \u03b4h\u2212\u03b7h . The former one is true in the range 0 < \u03b4h < \u03b7h, because z3 is always positive. One can check that this is still true in the range \u22121 < \u03b4h < 0. The latter one is equivalent to b+ \u221a b2 \u2212 c < \u03b4h\u03b4h\u2212\u03b7h . Taking square on both sides, we can check that b < \u03b4h\u03b4h\u2212\u03b7h and b 2 \u2212 c < ( \u03b4h\u03b4h\u2212\u03b7h \u2212 b) 2, based on 0 < \u03b4h \u2212 \u03b7h < 1\u2212 2 \u221a \u03b7h < 1.\nThus we conclude that for a fixed \u03b7h such that 0 < \u03b7h < 2(1 + \u03b4h), the minimal |z3| over \u22121 < \u03b4h < 1 is obtained at\n\u03b4h = ( \u221a \u03b7h \u2212 1)2,\nwith the minimal value z3 = b + \u221a b2 \u2212 c = b = \u221ac = \u03b4h = ( \u221a \u03b7h \u2212 1)2. Compared to the rate (1\u2212 \u03b7h)2 in the SGD case (Equation 5.3), MSGD can indeed help if \u03b7h is small enough (usually \u03b7h = \u03b7h is close to the inverse of the condition number of the Hessian in higher dimensional case). To check our above reasoning, we have computed numerically the spectral norm of the matrix M , sp(M), which is given in Figure 5.1. Note that when \u03b7h > 1, we have the optimal momentum rate being negative, i.e. \u03b4 = ( \u221a \u03b7h\u22121)2\n(1\u2212\u03b7h) < 0.\nPerhaps the most special behavior for MSGD is that when the momentum rate \u03b4 gets close to 1, the asymptotic variance of x2\u221e in Equation 5.7 still remains bounded, i.e. x2\u221e = 1+1\u2212\u03b7h\n\u03b72h(2(1+1\u2212\u03b7h)\u2212\u03b7h) \u03b72\u03c32 = 2\u2212\u03b7h4\u22123\u03b7h \u03c32 h2 for \u03b4 = 1 (or \u03b4h = 1 \u2212 \u03b7h). This contrasts to\nthe behavior of the heavy ball method, whose asymptotic variance tends to infinity as\n76\n77\n\u03b4 \u2192 1 [32].\n5.1.3 EASGD and EAMSGD\nWe can now study the moment equation for the EASGD and EAMSGD method. For EASGD, we have the following update rules\nxit+1 = x i t \u2212 \u03b7(hxit \u2212 \u03beit) + \u03b1(x\u0303t \u2212 xit), x\u0303t+1 = x\u0303t + \u03b2( 1 p \u2211p i=1 x i t \u2212 x\u0303t).\n(5.9)\nDenote yt = 1 p \u2211p i=1 x i t and \u03bet = 1 p \u2211p i=1 \u03be i t, then Equation 5.9 can be reduced to\nyt+1 = yt \u2212 \u03b7(hyt \u2212 \u03bet) + \u03b1(x\u0303t \u2212 yt), x\u0303t+1 = x\u0303t + \u03b2(yt \u2212 x\u0303t). (5.10)\nSimilar to MSGD, the second-order moment equation for Equation 5.10 is as follows,\ny2t+1 = ((1\u2212 \u03b7h\u2212 \u03b1)yt + \u03b1x\u0303t + \u03b7\u03bet)2\n= (1\u2212 \u03b7h\u2212 \u03b1)2y2t + \u03b12x\u03032t + \u03b72\u03be2t + 2\u03b1(1\u2212 \u03b7h\u2212 \u03b1)ytx\u0303t + 2\u03b7(1\u2212 \u03b7h\u2212 \u03b1)yt\u03bet + 2\u03b1\u03b7x\u0303t\u03bet,\nyt+1x\u0303t+1 = ((1\u2212 \u03b7h\u2212 \u03b1)yt + \u03b1x\u0303t + \u03b7\u03bet)((1\u2212 \u03b2)x\u0303t + \u03b2yt)\n= (1\u2212 \u03b7h\u2212 \u03b1)\u03b2y2t + (1\u2212 \u03b7h\u2212 \u03b1)(1\u2212 \u03b2)ytx\u0303t + \u03b1\u03b2ytx\u0303t + \u03b1(1\u2212 \u03b2)x\u03032t + \u03b7\u03bet((1\u2212 \u03b2)x\u0303t + \u03b2yt),\nx\u03032t+1 = (1\u2212 \u03b2)2x\u03032t + \u03b22y2t + 2\u03b2(1\u2212 \u03b2)ytx\u0303t.\n(5.11)\n78\nTaking expectation on both sides of the Equation 5.11, we get  Ey2t+1 Eyt+1x\u0303t+1\nEx\u03032t+1\n =  (1\u2212 \u03b7h\u2212 \u03b1)2 2\u03b1(1\u2212 \u03b7h\u2212 \u03b1) \u03b12 (1\u2212 \u03b7h\u2212 \u03b1)\u03b2 (1\u2212 \u03b7h\u2212 \u03b1)(1\u2212 \u03b2) + \u03b1\u03b2 \u03b1(1\u2212 \u03b2)\n\u03b22 2\u03b2(1\u2212 \u03b2) (1\u2212 \u03b2)2  \ufe38 \ufe37\ufe37 \ufe38\nM\n Ey2t Eytx\u0303t\nEx\u03032t\n+  \u03b72 \u03c3 2 p 0\n0\n .\n(5.12)\nSimilar to the analysis of MSGD, we get the following asymptotic variance,\ny2\u221e = limt\u2192\u221e Ey2t = (2\u2212 \u03b2)(1\u2212 \u03b2)\u03b7h + \u03b2(2\u2212 \u03b1\u2212 \u03b2) \u03b7h[(2\u2212 \u03b2)(2\u2212 \u03b7h)\u2212 2\u03b1][\u03b1+ \u03b2 + \u03b7h(1\u2212 \u03b2)] \u03b72\u03c32 p , (5.13)\nyx\u0303\u221e = limt\u2192\u221e Eytx\u0303t = \u03b2((2\u2212 \u03b2)(1\u2212 \u03b7h)\u2212 \u03b1) \u03b7h[(2\u2212 \u03b2)(2\u2212 \u03b7h)\u2212 2\u03b1][\u03b1+ \u03b2 + \u03b7h(1\u2212 \u03b2)] \u03b72\u03c32 p ,\nx\u03032\u221e = lim t\u2192\u221e\nEx\u03032t = \u2212\u03b2(1\u2212 \u03b2)\u03b7h + \u03b2(2\u2212 \u03b1\u2212 \u03b2) \u03b7h[(2\u2212 \u03b2)(2\u2212 \u03b7h)\u2212 2\u03b1][\u03b1+ \u03b2 + \u03b7h(1\u2212 \u03b2)] \u03b72\u03c32 p . (5.14)\nFor the above asymptotic variance to be positive, in particular x2\u221e, we can assume\n\u03b7h > 0,\n\u03b2 > 0,\n(2\u2212 \u03b2)(2\u2212 \u03b7h)\u2212 2\u03b1 > 0, 2\u2212\u03b1\u2212\u03b2\u2212\u03b7h+\u03b2\u03b7h \u03b1+\u03b2+\u03b7h(1\u2212\u03b2) > 0.\n(5.15)\nMoreover, if 0 < \u03b2 < 1, the asymptotic variance of the center variable x\u03032\u221e is strictly smaller than that of the spatial average y2\u221e (by comparing Equation 5.13 and 5.14). Interestingly, if \u03b2 > 1, it becomes strictly bigger.\nNote that it\u2019s still not very clear whether Equation 5.15 is the necessary and sufficient condition for the matrix M (in Equation 5.12) to be (strictly) stable. However, if we look back to the earlier result in Section 3.1.1 about the condition in Equation 3.4, they are nearly the same, except maybe for the last formula. The last formula in Equation 5.15 reads 0 < \u03b1+ \u03b2 + \u03b7h \u2212 \u03b2\u03b7h < 2. The left inequality implies \u03b2\u03b7h < \u03b1+ \u03b2 + \u03b7h. Based on\n79\nthe third formula in Equation 5.15, we have thus \u03b1 + \u03b2 + \u03b7h < 2 + \u03b2\u03b7h 2 < 2 + \u03b1+\u03b2+\u03b7h 2 . This gives us the last formula for the condition in Equation 3.4, which is \u03b1+\u03b2+ \u03b7h < 4. Conversely, assuming \u03b1 > 0 (as assumed in Section 3.1.1 for \u03b2 = p\u03b1), then one can check that \u03b1 + \u03b2 + \u03b7h < 4 implies the last formula in Equation 5.15, which also reads \u03b1\u2212 1 < (\u03b7h \u2212 1)(\u03b2 \u2212 1) < \u03b1+ 1.\nPerhaps the most striking observation is that the optimal \u03b1 such that the convergence rate (of the moment Equation 5.12) is maximized turns out to be negative, given \u03b7h > 0 and \u03b2 > 0 fixed.\nThe three eigenvalues of the matrix M in Equation 5.12 are\nz1= \u2212\u03b1+ (1\u2212 \u03b7h)(1\u2212 \u03b2), z2= b\u2212 \u221a b2 \u2212 c, z3= b+ \u221a b2 \u2212 c,\n(5.16)\nwhere b = 12 [(\u03b1 \u2212 (1 \u2212 \u03b7h \u2212 \u03b2))2 + 1 \u2212 2\u03b2\u03b7h], c = [\u03b1 \u2212 (1 \u2212 \u03b7h)(1 \u2212 \u03b2)]2 = z21 . Denote \u03b1\u2032 = z1 = \u2212\u03b1+ (1\u2212 \u03b7h)(1\u2212 \u03b2), then b = 12 [(\u03b1\u2032\u2212 \u03b7h\u03b2)2 + 1\u2212 2\u03b2\u03b7h] and c = (\u03b1\u2032)2. Using exactly the same analysis and the result from the MSGD case, we get that the minimal |z3| over \u22121 < \u03b1\u2032 < 1 is obtained at\n\u03b1\u2032 = ( \u221a \u03b2\u03b7h \u2212 1)2,\nwhich is equivalent to\n\u03b1 = \u2212( \u221a \u03b2 \u2212\u221a\u03b7h)2. (5.17)\nThe situation that the coupling constant \u03b1 being negative, while \u03b2 being positive suggests a very different perspective to understand EASGD. It seems that our earlier condition \u03b2 = p\u03b1 is unnecessary and is sub-optimal in this case. To check our above results, we have computed numerically the spectral norm of the matrix M for a fixed \u03b2 = 0.9, which is given in Figure 5.2. However, we are in danger this time if we were to simulate\n80\nEASGD using the optimal \u03b1 given in Equation 5.17. In Figure 5.3, we illustrate an unstable behavior in such optimal case. The reason is that our above analysis is based on the reduced Equation 5.10, rather than the original Equation 5.9.\nIn the original Equation 5.9, we have the following form of the drift matrix\nMp =  1\u2212 \u03b1\u2212 \u03b7h 0 ... 0 \u03b1 0 1\u2212 \u03b1\u2212 \u03b7h 0 ... \u03b1 ... 0 ... 0 ... 0 ... 0 1\u2212 \u03b1\u2212 \u03b7h \u03b1\n\u03b2\u2032 \u03b2\u2032 ... \u03b2\u2032 1\u2212 \u03b2\n , (5.18)\nwhose first p rows correspond to the local workers\u2019 updates, and the last row correspond to the master\u2019s update. The p + 1 eigenvalues Mp can be computed recursively as follows: let Ip+1(z) = det(Mp \u2212 z), then we have Ip+1(z) = (1 \u2212 \u03b1 \u2212 \u03b7h \u2212 z)Ip(z) \u2212 \u03b1\u03b2\u2032(1\u2212 \u03b1\u2212 \u03b7h \u2212 z)p\u22121 = \u00b7 \u00b7 \u00b7 = (1\u2212 \u03b1\u2212 \u03b7h \u2212 z)p\u22121[(1\u2212 \u03b2 \u2212 z)(1\u2212 \u03b1\u2212 \u03b7h \u2212 z)\u2212 p\u03b1\u03b2\u2032]. Notice that \u03b2\u2032 = \u03b2/p, thus we have two eigenvalues which do not depend on p, i.e. (1\u2212 \u03b2 \u2212 z)(1\u2212 \u03b1\u2212 \u03b7h\u2212 z)\u2212 \u03b1\u03b2 = 0, and an extra eigenvalue which only shows up for p > 1, i.e. z = 1 \u2212 \u03b1 \u2212 \u03b7h. This extra eigenvalue z = 1 \u2212 \u03b1 \u2212 \u03b7h is completely ignored in our reduced Equation 5.10. Then what is the optimal \u03b1 for the matrix Mp instead? The three eigenvalues of the matrix Mp in Equation 5.18 are\nz1= 1\u2212 \u03b1\u2212 \u03b7h, z2= b\u2212 \u221a b2 \u2212 c, z3= b+ \u221a b2 \u2212 c,\n(5.19)\nwhere b = 12(2\u2212 \u03b2 \u2212 \u03b7h \u2212 \u03b1), c = (1\u2212 \u03b7h)(1\u2212 \u03b2)\u2212 \u03b1.\nGiven \u03b7h > 0 and \u03b2 > 0 fixed, we shall prove that\n\u2022 if \u03b2 > \u03b7h: the optimal \u03b1 = 0. \u2022 if \u03b2 < \u03b7h: the optimal \u03b1 = \u2212( \u221a \u03b2 \u2212\u221a\u03b7h)2.\n81\n82\n83\nThe proof idea is similar to the MSGD case, so we shall be more brief. Let\u2019s focus on the variable c = (1\u2212 \u03b7h)(1\u2212 \u03b2)\u2212 \u03b1 instead of the variable \u03b1 itself. Rewrite Equation 5.19 with z1 = c+\u03b2(1\u2212 \u03b7h), b = 12(c\u2212\u03b2\u03b7h + 1). We find that for b2 > c, we only need c > c2 or c < c1, where c1 = ( \u221a \u03b2\u03b7h \u2212 1)2, and c2 = ( \u221a \u03b2\u03b7h + 1) 2. We also observe that the line z1 as a function of c intersects with the quadratic curve z2 or z3 at c0 = (1\u2212 \u03b7h)(1\u2212 \u03b2) (i.e. at \u03b1 = 0). At c = c0, z1 = 1 \u2212 \u03b7h, z2 = 1 \u2212 12(\u03b2 + \u03b7h) \u2212 12 |\u03b2 \u2212 \u03b7h|, and z3 = 1 \u2212 12(\u03b2 + \u03b7h) + 12 |\u03b2 \u2212 \u03b7h|. If \u03b2 > \u03b7h, then z3 = z1 at c = c0. Otherwise z2 = z1 at c = c0. We can check that the negative optimal \u03b1 = \u2212( \u221a \u03b2 \u2212\u221a\u03b7h)2 is given at c = c1, where z2 and z3 meet and transit from real-valued to complex-valued. Note that z1 is an increasing function of c, and if it intersects with z3, the optimal \u03b1 becomes 0 (rather than being negative). They are illustrated in Figure 5.4 and 5.5 as a function of \u03b1 under the two conditions, i.e. \u03b2 > \u03b7h and \u03b2 < \u03b7h. We also computed numerically the spectral norm of the matrix Mp for a fixed \u03b2 = 0.9, which is given in Figure 5.6. Finally, we show in Figure 5.7 the optimal case of EASGD under the condition \u03b2 < \u03b7h.\nWe have studied the the second-order moment equation 5.12 of the reduced system (Equation 5.10), and the first-order moment equation 5.18 of the original system (Equation 5.9) for the EASGD method. We found that the reduced system can lose critical information about the stability of the original system. However, the eigenvalues are still closely related in the first-order moment matrix Mp and the second-order moment M , i.e. the z2 and z3 in Equation 5.16 and 5.19. Thus for the EAMSGD method, we shall only focus on the first-order moment equation. Its asymptotic variance, which can obtained from the second-order moment equation, is rather complicated and will not be discussed.\nFor EAMSGD, we have the following update rules\nvit+1 = \u03b4v i t \u2212 \u03b7(h(xit + \u03b4vit)\u2212 \u03beit), xit+1 = x i t + v i t+1 + \u03b1(x\u0303t \u2212 xit), x\u0303t+1 = x\u0303t + \u03b2( 1 p \u2211p i=1 x i t \u2212 x\u0303t).\n84\n85\n86\n87\nWe have the following form of the drift matrix for the first-order moment equation,\nMp =  \u03b4h \u2212\u03b7h 0 0 ... ... 0 \u03b4h 1\u2212 \u03b7h \u2212 \u03b1 0 0 ... ... \u03b1 0 0 \u03b4h \u2212\u03b7h ... ... 0 0 0 \u03b4h 1\u2212 \u03b7h \u2212 \u03b1 ... ... \u03b1 ... ... ... ... ... ... ... ... ... ... ... ... ... ...\n0 \u03b2\u2032 0 \u03b2\u2032 ... ... 1\u2212 \u03b2\n , (5.20)\nsuch that [v1t+1, x 1 t+1, v 2 t+1, x 2 t+1, . . . , x\u0303t+1] T = Mp[v 1 t , x 1 t , v 2 t , x 2 t , . . . , x\u0303t] T . Recall that \u03b4h = \u03b4(1 \u2212 \u03b7h), \u03b7h = \u03b7h, and \u03b2\u2032 = \u03b2/p. We can compute the eigenvalues of the above drift matrix Mp recursively, and one can check that they are again independent of the choice of p for p > 1, as in the EASGD case. More precisely, we have\ndet|Mp \u2212 z| = (u(z))p\u22121v(z) = 0, (5.21)\nwhere u(z) = z2 \u2212 (1\u2212 \u03b7\u2212 \u03b1\u2212 \u03b4)z + \u03b4(1\u2212 \u03b1) and v(z) = (\u03b4\u2212 z)(1\u2212 \u03b7\u2212 \u03b1\u2212 z)(1\u2212 \u03b2 \u2212 z) + \u03b7\u03b4(1\u2212 \u03b2 \u2212 z)\u2212 \u03b1\u03b2(\u03b4 \u2212 z).\nThe solution of the Equation 5.21 is quite complicated as it involves a third-order polynomial which is irreducible. It would be difficult to obtain the optimal \u03b4 or \u03b1 as before. So we computed numerically the spectral norm of the matrix Mp in Equation 5.20 for a fixed \u03b2 = 0.9 and \u03b4 = 0.99 (which we used in the experiment in Chapter 4). It is given in Figure 5.8. This result suggests that the optimal \u03b1 increases as the \u03b7 (or \u03b7h as h = 1) decreases. Recall our result of MSGD in Figure 5.1: the optimal \u03b4 increases as \u03b7 decreases. This is consistent with the choice of the learning rate and the momentum rate scheduling in the Nesterov\u2019s optimal methods in literature [28]. We have seen that there\u2019s an intimate connection between MSGD and EASGD when we were studying the optimal momentum rate \u03b4 and the optimal moving rate \u03b1. It suggests that we may also\n88\nfind interesting rate scheduling for EASGD, as well as EAMSGD based on the convex analysis. However, we should be careful as the optimal \u03b1 for EASGD is either zero or negative, while for EAMSGD it can be positive as well."}, {"heading": "5.2 Multiplicative noise", "text": "In this section, we study a multiplicative noise model, which attempts to capture the initial behavior of the stochastic optimization method. It is complementary to the additive noise model, which captures the asymptotic behavior.\nOur starting point is the following linear regression problem\nmin a\u2208R\nE[(v \u2212 au)2],\nwhere the expectation E is taken over the joint distribution of (u, v). If the input data (u, v) satisfies v = a\u2217u, we may reduce the above problem to the following onedimensional case,\nmin x\u2208R\nE[(xu)2]. (5.22)\nAn interesting perspective of this problem is that we can assume that u2 of the input data follows a Gamma distribution \u0393(\u03bb, \u03c9), with mean \u03bb/\u03c9 and variance \u03bb/\u03c92. Note that if u follows a Gaussian distribution with mean zero and variance \u03c32, then \u03bb = 1/2 and \u03c9 = 1/(2\u03c32).\n5.2.1 SGD with mini-batch\nGiven x0 \u2208 R, the mini-batch SGD method for solving the multiplicative noise problem in Equation 5.22 is as follows,\nxt+1 = xt \u2212 \u03b7u2txt, (5.23)\nwhere \u03b7 > 0, and ut is an i.i.d input process.\n89\n90\nAs we assumed that u2t follow a Gamma distribution \u0393(\u03bb, \u03c9), it\u2019s easy to verify that if we use mini-batch of size p, Equation 5.23 becomes\nxt+1 = xt \u2212 \u03b7 1\np p\u2211 i=1 (uit) 2xt, (5.24)\nand this mini-batch 1p \u2211p i=1(u i t) 2 also follows a Gamma distribution \u0393(p\u03bb, p\u03c9), since the mean of the mini-batch is not changed and the variance is divided by p.\nWe also notice that in the continuous-time limit, i.e. for small \u03b7, we can approximate the process in Equation 5.24 by a Geometric Brownian motion [32] as follows,\ndX(t) = \u2212\u03bb \u03c9 X(t)dt+ \u221a \u03b7\n\u221a \u03bb\np\u03c92 X(t)dB(t).\nAn interesting behavior of the geometric Brownian Motion is that its variance can explode (go to infinity), yet the path of the stochastic process still converges almost surely towards zero. In such case, we can observe along the path a few extreme large values. These extreme values may however cause trouble to the stability of the nonlinear dynamics in the training of deep learning models. So our goal here is to control this variance by studying again the second-order moment equation.\nGoing back to the discrete process defined by Equation 5.24. Denote \u03bet = 1 p \u2211p i=1(u i t) 2, we have\nx2t+1 = (1\u2212 \u03b7\u03bet)2x2t . (5.25)\nTaking expectation on both sides of the Equation 5.25, we obtain\nE[x2t+1] = (1\u2212 2\u03b7 p\u03bb\np\u03c9 + \u03b72\np\u03bb(p\u03bb+ 1)\n(p\u03c9)2 )E[x2t ] = (1\u2212 2\u03b7\n\u03bb \u03c9 + \u03b72 \u03bb(p\u03bb+ 1) p\u03c92 )E[x2t ]. (5.26)\nThe rate of convergence defined by the above second-moment equation 5.26 is 1\u22122\u03b7 \u03bb\u03c9 + \u03b72 \u03bb(p\u03bb+1) p\u03c92 . It is monotone decreasing as p increases and will saturate at a limit 1\u22122\u03b7 \u03bb\u03c9 +\n91\n\u03b72 \u03bb 2 \u03c92 = (1 \u2212 \u03b7 \u03bb\u03c9 )2. Given p, we can optimize this convergence rate with respect to the choice of the learning rate \u03b7. This optimal learning rate is given by\n\u03b7p = p\u03c9\np\u03bb+ 1 =\n\u03c9\n\u03bb+ 1/p . (5.27)\nFrom this formula, we can see that as p evolves, \u03b7p will change a lot if \u03bb is much smaller than 1/p. In other words, if \u03bb is very big compared to 1/p, then the change in \u03b7p is not that much as we increases the mini-batch size p. Thus we can expect that the speedup we would gain depends heavily on the distribution of the input data, in particular the shape parameter \u03bb of the Gamma distribution. Suppose for p = 1, the \u03bet follows the Gamma distribution \u0393(\u03bb, \u03c9). Its probability density function is \u03c9 \u03bb\n\u0393(\u03c9)\u03be \u03bb\u22121e\u2212\u03c9\u03be.\nWe illustrate this probability density function in Figure 5.9 for the standard Gaussian case (\u03bb = 1/2, \u03c9 = 1/2) with p = 1, p = 2 and p = 4. We see that when \u03bb is smaller than one, the probability density function has a pole at zero and has also a slower decay (heavier tail) toward infinity. By increasing the mini-batch size p, the probability density function becomes more and more concentrated near its mean. Thus the mini-batch is more effective for such input distribution with very large spread (i.e. small \u03bb).\n5.2.2 Momentum SGD\nGiven x0 \u2208 R and v0 = 0, the MSGD method for solving the multiplicative noise problem in Equation 5.22 is as follows,\nvt+1 = \u03b4vt \u2212 \u03b7\u03bet(xt + \u03b4vt), xt+1 = xt + vt+1, (5.28)\nwhere \u03bet follows a Gamma distribution \u0393(\u03bb, \u03c9).\nDenote \u03b4t = \u03b4(1\u2212\u03b7\u03bet) and \u03b7t = \u03b7\u03bet, the second-order moment equation can be computed\n92\n93\nas follows,\nv2t+1 = (\u03b4(1\u2212 \u03b7\u03bet)vt \u2212 \u03b7\u03betxt)2 = (\u03b4tvt \u2212 \u03b7txt)2\n= \u03b42t v 2 t \u2212 2\u03b7t\u03b4tvtxt + \u03b72t x2t ,\nvt+1xt = (\u03b4tvt \u2212 \u03b7txt)xt = \u03b4tvtxt \u2212 \u03b7tx2t ,\nx2t+1 = (xt + vt+1) 2 = x2t + 2vt+1xt + v 2 t+1\n= x2t + 2(\u03b4tvtxt \u2212 \u03b7tx2t ) + \u03b42t v2t \u2212 2\u03b7t\u03b4tvtxt + \u03b72t x2t , = \u03b42t v 2 t + (1\u2212 2\u03b7t + \u03b72t )x2t + (2\u03b4t \u2212 2\u03b7t\u03b4t)vtxt,\nvt+1xt+1 = vt+1(xt + vt+1) = vt+1xt + v 2 t+1\n= \u03b4tvtxt \u2212 \u03b7tx2t + \u03b42t v2t \u2212 2\u03b7t\u03b4tvtxt + \u03b72t x2t = \u03b42t v 2 t + (\u2212\u03b7t + \u03b72t )x2t + \u03b4t(1\u2212 2\u03b7t)vtxt.\n(5.29)\nTaking expectation on both sides of the Equation 5.29, we obtain the following recursive relation (Ev2t+1,Ex2t+1,Evt+1xt+1)T = M(Ev2t ,Ex2t ,Evtxt)T , where\nM =  \u03b42(1\u2212 2\u03b7u1 + \u03b72u2) \u03b72u2 \u22122\u03b4\u03b7(u1 \u2212 \u03b7u2) \u03b42(1\u2212 2\u03b7u1 + \u03b72u2) 1\u2212 2\u03b7u1 + \u03b72u2 2\u03b4(1\u2212 \u03b7u1)\u2212 2\u03b4\u03b7(u1 \u2212 \u03b7u2)\n\u03b42(1\u2212 2\u03b7u1 + \u03b72u2) \u2212\u03b7u1 + \u03b72u2 \u03b4(1\u2212 \u03b7u1)\u2212 2\u03b4\u03b7(u1 \u2212 \u03b7u2)\n , (5.30)\nu1 = \u03bb \u03c9 and u2 = \u03bb(\u03bb+1) \u03c92 .\nNow we can compare the numerical spectral norm of the matrix M for various choices of the input data distribution parameterized by \u03bb and \u03c9. In Figure 5.10, we illustrate the standard Gaussian case, i.e. uit follows i.i.d. N (0, 1) in Equation 5.24. In Figure 5.11 and 5.12 we illustrate the resulting effect if we use mini-batch of size p = 2 and p = 4. Recall that for \u03b4 = 0, we have the optimal learning rate in Equation 5.27. It achieves nearly the optimal convergence rate (smallest sp(M)) in these Figures. We also observe that the optimal momentum rate is actually 0 at these optimal learning rates. One can check such observation in Figure 5.13. Thus contrary to our earlier MSGD results in the additive noise case (Figure 5.1), the momentum can slow down the optimal convergence\n94\nrate. However, we can yet ask, given \u03b7 and \u03b4 fixed, what is the region of input data distribution \u03bb and \u03c9 such that the momentum helps. We find that the answer is for relatively small slope \u03bb\u03c9 (c.f. Figure 5.14). This phenomenon of acceleration is consistent with our earlier analysis of MSGD in the additive noise case (c.f. Figure 5.1).\n5.2.3 EASGD and EAMSGD\nWe now focus on the EASGD updates for solving the problem in Equation 5.22,\nxit+1 = x i t \u2212 \u03b7\u03beitxit + \u03b1(x\u0303t \u2212 xit), x\u0303t+1 = x\u0303t \u2212 \u03b2p \u2211p i=1(x\u0303t \u2212 xit), (5.31)\nwhere \u03beit follows a Gamma distribution \u0393(\u03bb, \u03c9).\nThe second-order moment equation can be computed as follows,\n(xit+1) 2 = (1\u2212 \u03b1\u2212 \u03b7\u03beit)2(xit)2 + 2\u03b1(1\u2212 \u03b1\u2212 \u03b7\u03beit)x\u0303txit + \u03b12(x\u0303t)2, (x\u0303t+1) 2 = (1\u2212 \u03b2)2(x\u0303t)2 + 2\u03b2(1\u2212 \u03b2)(1p \u2211p i=1 x\u0303tx i t) + \u03b22 p2 \u2211p i=1,j=1 x i tx j t ,\nx\u0303t+1x i t+1 = (1\u2212 \u03b2)(1\u2212 \u03b1\u2212 \u03b7\u03beit)x\u0303txit + \u03b1(1\u2212 \u03b2)(x\u0303t)2 + (1\u2212 \u03b1\u2212 \u03b7\u03beit)\u03b2p \u2211p j=1 x i tx j t , xit+1x j t+1 = (1\u2212 \u03b1\u2212 \u03b7\u03beit)(1\u2212 \u03b1\u2212 \u03b7\u03bejt )xitxjt + \u03b12(x\u0303t)2\n+ \u03b1(1\u2212 \u03b1\u2212 \u03b7\u03beit)x\u0303txit + \u03b1(1\u2212 \u03b1\u2212 \u03b7\u03bejt )x\u0303txjt , i 6= j.\n(5.32)\n95\n96\n97\n98\n99\n100\nTaking expectation on both sides of the Equation 5.32, we get\nE(xit+1)2 = [(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )2 + \u03b72 \u03bb\u03c92 ]E(xit)2 + 2\u03b1(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )E(x\u0303txit) + \u03b12E(x\u0303t)2, E(x\u0303t+1)2 = (1\u2212 \u03b2)2E(x\u0303t)2 + 2\u03b2(1\u2212 \u03b2)1p \u2211p i=1 E(x\u0303txit) + \u03b22 p2 \u2211p i=1,j=1 E(xitx j t ),\nE(x\u0303t+1xit+1) = (1\u2212 \u03b2)(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )E(x\u0303txit) + \u03b1(1\u2212 \u03b2)E(x\u0303t)2\n+ (1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 ) \u03b2 p \u2211p j=1 E(xitx j t ) + \u03b1\u03b2( 1 p \u2211p j=1 E(x\u0303tx j t )),\nE(xit+1x j t+1) = (1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )E(xitx j t ) + \u03b1 2E(x\u0303t)2\n+ \u03b1(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )E(x\u0303txit) + \u03b1(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )E(x\u0303tx j t ), i 6= j.\n(5.33)\nWe can reduce the above system (Equation 5.33) into a closed form by introducing\nat = E[(x\u0303t)2], bt = 1\np p\u2211 i=1 E[(xit)2], ct = 1 p p\u2211 i=1 E[x\u0303txit], dt = 1 p2 p\u2211 i=1 p\u2211 j=1 E[xitx j t ].\nWe get (at+1, bt+1, ct+1, dt+1) T = M(at, bt, ct, dt) T , where\nM =  (1\u2212 \u03b2)2 0 2\u03b2(1\u2212 \u03b2) \u03b22 \u03b12 (1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )2 + \u03b72 \u03bb\u03c92 2\u03b1(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 ) 0 \u03b1(1\u2212 \u03b2) 0 (1\u2212 \u03b2)(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 ) + \u03b1\u03b2 (1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )\u03b2\n\u03b12 \u03b72 \u03bb p\u03c92\n2\u03b1(1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 ) (1\u2212 \u03b1\u2212 \u03b7 \u03bb\u03c9 )2\n .\n(5.34)\nWe will analyze the spectral norm of the matrix M in the limit p goes to infinity. We focus on the question that whether the stability region (i.e. the spectral norm be smaller than one) with respect to the learning rate \u03b7 will be enlarged for large p.\nCase I: \u03b1 = \u03b2/p\nThe characteristic polynomial of M in Equation 5.34 can be written as\nP (z) = [z\u2212(1\u2212\u03b2)2][z\u2212(1\u2212\u03b2)(1\u2212\u03b7 \u03bb \u03c9 )][z\u2212(1\u22122\u03b7 \u03bb \u03c9 +\u03b72 \u03bb(\u03bb+ 1) \u03c92 )][z\u2212(1\u2212\u03b7 \u03bb \u03c9 )2]+Oz( 1 p ),\n101\nwhere Oz( 1 p) is the remaining polynomial in z of order 1 p and higher. The four leading order eigenvalues of P (z) are z1 = (1\u2212\u03b2)2, z2 = (1\u2212\u03b2)(1\u2212\u03b7 \u03bb\u03c9 ), z3 = (1\u22122\u03b7 \u03bb\u03c9+\u03b72 \u03bb(\u03bb+1) \u03c92 ), z4 = (1\u2212 \u03b7 \u03bb\u03c9 )2. We need necessarily \u03b2 \u2208 (0, 2), and in such case the stability condition for \u03b7 is the same as the SGD case (c.f. Equation 5.26), i.e. |1 \u2212 2\u03b7 \u03bb\u03c9 + \u03b72 \u03bb(\u03bb+1) \u03c92 | < 1. Thus the stability region of EASGD remains the same as SGD in the limit p goes to infinity, i.e.\n0 < \u03b7 < 2\u03c9\n\u03bb+ 1 .\nIn the Figure 5.15, 5.16, 5.17 and 5.18, we illustrate the stability region for different \u03bb, \u03c9 and p. Compared with the MSGD method in the Figure 5.13, EASGD improves its optimal convergence rate:\n\u2022 \u03bb = 0.5, \u03c9 = 0.5: sp(M) = 0.5742 at p = 6 and \u03b7 = 0.3814 for EASGD vs.\nsp(M) = 0.6667 for MSGD with \u03b7 = \u03bb\u03c9+1 = 1 3 and \u03b4 = 0.\n\u2022 \u03bb = 1, \u03c9 = 1: sp(M) = 0.4317 at p = 7 and \u03b7 = 0.5225 vs. sp(M) = 0.5 for MSGD\nwith \u03b7 = \u03bb\u03c9+1 = 1 2 and \u03b4 = 0.\n\u2022 \u03bb = 2, \u03c9 = 2: sp(M) = 0.2945 at p = 9 and \u03b7 = 0.6647 vs. sp(M) = 0.3333 for\nMSGD with \u03b7 = \u03bb\u03c9+1 = 2 3 and \u03b4 = 0.\nNote that EASGD \u2019s optimal convergence rate is achieved for some finite p. This is in contrast to the mini-batch SGD method (c.f. Equation 5.27).\nCase II: \u03b1 is independent of \u03b2 and p\nThe characteristic polynomial of M of Equation 5.34 can be written as\nP (z) = (z \u2212 z1)(z \u2212 z2)(z \u2212 z3)(z \u2212 z4) +Oz( 1\np ),\nwhere z1 = (1 \u2212 \u03b2)(1 \u2212 \u03b7 \u03bb\u03c9 ) \u2212 \u03b1, z2 = (1 \u2212 \u03b1)2 \u2212 2(1 \u2212 \u03b1)\u03b7 \u03bb\u03c9 + \u03b72 \u03bb(\u03bb+1) \u03c92 , z3 + z4 = 2 + \u03b12 \u2212 (2\u2212 \u03b2)\u03b2 \u2212 (2\u2212 \u03b7 \u03bb\u03c9 )\u03b7 \u03bb\u03c9 \u2212 2\u03b1(1\u2212 \u03b2 \u2212 \u03b7 \u03bb\u03c9 ), z3z4 = z21 .\n102\n103\n104\n105\n106\nThese four eigenvalues depend on \u03bb, \u03c9, \u03b2, \u03b1, p and \u03b7. It\u2019s not as clear here on how to choose an optimal \u03b1. We can however still find the stability region with respect of \u03b7 in the limit p\u2192\u221e. We ask given \u03bb, \u03c9, \u03b2 and \u03b1, what is the range of \u03b7 such that the four eigenvalues lie on the unit circle of the complex plane. We also observe that the optimal \u03b1 can be positive for large p, in contrast to the additive noise case (c.f. Figure 5.19 and 5.6). Thus to simplify our analysis, we only consider \u03b7 > 0, \u03b1 \u2208 (0, 1) and \u03b2 \u2208 (0, 1). It turns out that in this case,\n\u2022 |z1| < 1: 0 < \u03b7 < 2\u2212\u03b1\u2212\u03b21\u2212\u03b2 \u03c9\u03bb . \u2022 |z2| < 1: 0 < \u03b7 < \u03c9(1\u2212\u03b1)\u03bb+1 + \u03c9 2 \u03bb(\u03bb+1) \u221a \u03bb2 \u03c92 + \u03bb \u03c92 (2\u03b1\u2212 \u03b12). \u2022 |z3| < 1 and |z4| < 1: 0 < \u03b7 < 4\u22122\u03b1\u22122\u03b22\u2212\u03b2 \u03c9\u03bb .\nWe can also check that 2\u2212\u03b1\u2212\u03b21\u2212\u03b2 \u03c9 \u03bb > 4\u22122\u03b1\u22122\u03b2 2\u2212\u03b2 \u03c9 \u03bb , thus we only need to maximize the min0<\u03b1<1{4\u22122\u03b1\u22122\u03b22\u2212\u03b2 \u03c9\u03bb , \u03c9(1\u2212\u03b1) \u03bb+1 + \u03c9 \u03bb(\u03bb+1) \u221a \u03bb2 + \u03bb(2\u03b1\u2212 \u03b12)}. We now prove that the maximum is achieved at \u03b1 = 1 \u2212 \u221a \u03bb. In fact, this is the maximum of the second term, and at that \u03b1, the first term equals 4\u22122\u03b1\u22122\u03b22\u2212\u03b2 \u03c9 \u03bb = 2+2 \u221a \u03bb\u22122\u03b2 2\u2212\u03b2 \u03c9 \u03bb , and the second term equals\n\u221a \u03bb \u03bb+1(\u03c9 + \u03c9 \u03bb ) = \u03c9\u221a \u03bb . The first term being larger than the second term, thus we conclude that the maximal value \u03c9\u221a \u03bb is indeed achieved at \u03b1 = 1\u2212 \u221a \u03bb. Based on the Equation 5.27, the stability region for SGD is 0 < \u03b7 < 2\u03c9\u03bb+1 ; for minibatch SGD with p \u2192 \u221e, it is 0 < \u03b7 < 2\u03c9\u03bb . For EASGD, it is 0 < \u03b7 < \u03c9\u221a\u03bb , with p \u2192 \u221e and \u03b1 = 1 \u2212 \u221a \u03bb. In case \u03bb = 0.5, \u03c9 = 0.5, we can check from Figure 5.19 that \u03b1 = 1\u2212 \u221a 0.5 = 0.2929 achieves the widest range of \u03b7 \u2208 (0, \u221a 0.5) for M to be stable.\nFor the EAMSGD method, it\u2019s much more difficult to analyze the second-order moment equation. It involves a linear system of nine variables: at = E(x\u03032t ), bt = 1p \u2211\ni E(xit)2, ct = 1 p \u2211 i E(x\u0303txit), dt = 1 p2 \u2211 i,j E(xitx j t ), et = 1 pE(x\u0303tv i t), ft = 1 p2 \u2211 i,j E(vitv j t ), gt = 1 pE(v i t) 2, ht = 1 p \u2211 i E(xitvit), kt = 1 p2 \u2211 i,j E(xitv j t ). The linear matrix is quite complicated, so we will not present it here.\n107\n108"}, {"heading": "5.3 A non-convex case", "text": "Here\u2019s an amusing non-convex case which sheds some light on when EASGD will work and when it will not work. Recall our formalization of the problem in Chapter 1, Equation 1.2:\nmin x1,...,xp,x\u0303 p\u2211 i=1 E[f(xi, \u03bei)] + \u03c1 2 \u2016xi \u2212 x\u0303\u20162,\nwhere we refer to xi\u2019s as local variables and we refer to x\u0303 as a center variable.\nIf f(xi, \u03bei) = 14(1\u2212(xi)2)2, which is deterministic (independent of \u03bei) and one-dimensional. We see that f has two minimum xi = 1 and xi = \u22121. It also has a saddle point at xi = 0. We ask how large \u03c1 should be such that the xi\u2019s will have a common minimum. Let\u2019s assume that p = 2, we see that if \u03c1 = 0, then x1 = \u22121 and x2 = 1 can be a stable solution. This is the scenario that the \u2019elasticity\u2019 of EASGD is broken. We have observed a related phenomenon in EAMSGD when the communication period is too large (\u03c4 big) in the Figure 4.13 of Chapter 4. More formally, we have the following objective for p = 2 workers,\nmin x,y,z\u2208R\n1 4 (1\u2212 x2)2 + 1 4 (1\u2212 y2)2 + \u03c1 2 \u2016x\u2212 z\u20162 + \u03c1 2 \u2016y \u2212 z\u20162. (5.35)\nThe partial derivative of our objective in Equation 5.35 with respect to x, y and z is:\n\u2202 \u2202x = (x 2 \u2212 1)x+ \u03c1(x\u2212 z) \u2202 \u2202y = (y\n2 \u2212 1)y + \u03c1(y \u2212 z) \u2202 \u2202z = \u03c1(z \u2212 x) + \u03c1(z \u2212 y).\n(5.36)\nThe question is that what are all the critical points of the objective in Equation 5.35. By setting the derivative to zero in Equation 5.36, they should satisfy z = x+y2 and\n(x2 \u2212 1)x+ \u03c1(x\u2212 x+y2 ) = 0 (y2 \u2212 1)y + \u03c1(y \u2212 x+y2 ) = 0. (5.37)\n109\nObserving three special cases: x = y = z = 1, x = y = z = \u22121 and x = y = z = 0. We can guess that x = \u2212y is also a solution. In fact, they should satisfy z = 0, (x2 \u2212 1)x + \u03c1(x \u2212 0) = 0 and (y2 \u2212 1)y + \u03c1(y \u2212 0) = 0. We have thus either x = 0 or x2 = 1 \u2212 \u03c1 (resp. y = 0 or y2 = 1 \u2212 \u03c1). If \u03c1 < 1, then we indeed find a real critical point x = \u221a 1\u2212 \u03c1, y = \u2212\u221a1\u2212 \u03c1, z = 0. Is this critical point stable? To answer this, we compute the Hessian of our objective in Equation 5.35 with respect to x, y and z:\nH =  3x2 \u2212 1 + \u03c1 0 \u2212\u03c1 0 3y2 \u2212 1 + \u03c1 \u2212\u03c1\n\u2212\u03c1 \u2212\u03c1 2\u03c1.\n . (5.38)\nEvaluating this Hessian at the critical point x = \u221a 1\u2212 \u03c1, y = \u2212\u221a1\u2212 \u03c1, z = 0, we can compute its smallest eigenvalue and see when it is positive definite. Figure 5.20 shows that the smallest eigenvalue is always positive in the range \u03c1 \u2208 (0, 2/3). This suggests that this critical point can indeed be stable, i.e. it is a local optimum introduced by EASGD when the penalty term \u03c1 is small enough.\nFor completeness, we now prove that all the critical points are either of the form x = y or of the form x = \u2212y. In fact, adding the two Equations in 5.37 gives us (x2\u2212 1)x+ (y2\u2212 1)y = 0, i.e. x3 + y3 = x+ y = (x2 \u2212 xy + y2)(x+ y). Subtracting these two Equations gives us x3\u2212x\u2212y3 +y+\u03c1(x\u2212y) = 0, i.e. x3\u2212y3 = (1\u2212\u03c1)(x\u2212y) = (x2 +xy+y2)(x\u2212y). If x 6= y and x 6= \u2212y, then x2 \u2212 xy + y2 = 1\nx2 + xy + y2 = 1\u2212 \u03c1. (5.39)\nAdding and subtracting again the two Equations in 5.39, we obtain x2 + y2 = 1\u2212 \u03c12 , and xy = \u2212\u03c12 . There\u2019s no real solution satisfying these two conditions for any \u03c1 > 0.\n110\n111\nChapter 6"}, {"heading": "Scaling up Elastic Averaging SGD", "text": "This chapter discusses how to scale up the EASGD method to hundreds and thousands of processors. In Section 6.1, we first propose a tree-structured extension of the EASGD method called EASGD Tree. The basic idea and the design principle are discussed in Section 6.1.1, and the numerical results are presented in Section 6.1.2. We present two different communication schemes for the EASGD Tree method. As we had seen the advantage of EAMSGD, we also accelerate EASGD Tree with Nesterov \u2019s momentum method. In Section 6.2, we unify the DONWPOUR method and the EASGD method by considering a Gauss-Seidel reformulation of the EASGD update rules in the synchronous scenario. This unification suggests the possibility of using both DONWPOUR and EASGD under the EASGD Tree. It also suggests that in-between the DONWPOUR and the EASGD method there may be some even better method.\n6.1 EASGD Tree\nThe original motivation of EASGD Tree is to run SGD at multiple time scales, where each scale corresponds to the use of a different learning rate. It naturally gives rise to a hierarchical tree structured organization of the processors. In literature, the tree\n112\nidea has shown up in various contexts. For example, the tree was used to scale up the asynchronous SGD method by aggregating the delayed gradients computed by the intermediate nodes and the leaf nodes [2]. It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62]. The benefit of using a tree is that the number of links to connect a very large number of nodes is minimal. The trade-off is that the connectivity of the whole tree is not robust to the link failure. However, the tree structure has its own charm for its simplicity. The main theoretical challenge is to understand its convergence property in terms of the root of the tree. This still remains open."}, {"heading": "6.1.1 The algorithm", "text": "We start from introducing the EASGD Tree algorithm. There are two important aspects that can guide us to design such an algorithm, one is the computation, the other is the communication.\n\u2022 Do we need intermediate node to perform the gradient computation? We had hoped\nthat each tree node runs SGD in parallel with a smaller and smaller learning rates as the depth of the tree node decreases (from bottom to top). Each intermediate node (i.e. except the leaf node) is expected to average out the noise of its children. The root can thus achieve the smallest variance. One benefit that each node performs its own gradient computation is that it can reduce the bias of the spatial averaging of its children away from the local optimum. The disadvantage is that each node will have the overhead for both computation and communication. In case each tree node uses only one CPU core, it needs to tradeoff the computation speed for the communication throughput. In addition, in the HPC environment (where we run our experiments), we have not observed any visible performance improvement for the intermediate node to perform the gradient computation. It\u2019s also complicated to choose a proper learning rate decay for different levels of the\n113\ntree nodes. On the other hand, if we were to remove the intermediate node\u2019s gradient computation, we can still preserve the multi-scale variance reduction property using the elastic averaging. Moreover, the root node of the tree behaves as the center variable of the EASGD method. Thus in the following we only discuss the EASGD Tree method without intermediate node\u2019s gradient computation.\n\u2022 There are at least two different ways to design the communication protocol. One is\nlocally synchronous, the other is fully asynchronous. We observe that the local synchronization protocol (as in our asynchronous EASGD in Section 2.2) may cause extra waiting time for the parent and the children nodes, but it can give better convergence property (due to a smaller parameter staleness). The fully asynchronous protocol can in theory hide completely this waiting time by allowing each node asynchronously broadcasts its parameter to its neighbours without blocking any computation. In the scale that we simulate the EASGD Tree, we have to use the fully asynchronous protocol as it saves significantly the time which is spent on the chain of waiting. In our HPC environment, the fully asynchronous protocol works well if the network link is stable over time.\n\u2022 The communication cost is different within a machine and across a machine. We\nwould thus structure the tree nodes from bottom-up, i.e. the leaf nodes are allocated on one machine so that their variance can be averaged out the quickest possible. The intermediate nodes including the root of the tree will then have to communicate across the machine boundary (in our HPC environment, there are 20 physical CPU cores per machine). These communication should happen less frequently. We will thus introduce two levels of the communication periods. The first communication period is between the leaf nodes and their parents. The second (smaller) one is between the intermediate nodes. We will also distinguish the push up (toward parent) period and the push down (toward children) period. With this, we can trade-off more push up communication for less push down communication.\n114\nBased on the above discussion, we now present the EASGD Tree in Algorithm 6. Figure 6.1 illustrates this. This generic pseudocode does not distinguish the leaf node, the intermediate node and the root node, so let\u2019s describe their difference first. As in EASGD, each node starts with the same initial parameter x0 and sets its local clock ti to 0. It performs non-blocking read (Irecv) and non-blocking send (Isend) with its parent and children (if any). Every \u03c4up local steps, it sends its parameter to its parent. Every \u03c4down local steps (number of local gradient updates), it sends its parameter to the d children. We denote d to be the degree of the d-ary tree we consider here. Each node also needs to check if there\u2019s any new arrival of the parameter from its parent or its d children. This needs to be interleaved with the gradient computation to achieve high I/O throughput. On receiving a new parameter, a moving average with the moving rate \u03b1 is performed. If it is the leaf node, then a stochastic gradient descent step with learning rate \u03b7 is performed per local step; otherwise it replaces the gradient descent step with a while loop, looping for roughly the same amount of time as a gradient step, just to apply\n115\nthe moving average on any new parameter arrival. Note that for each node, they can use different communication period \u03c4up and \u03c4down. We discuss two such possible schemes below. They are illustrated in Figure 6.2.\nThe first scheme is based on the idea of multi-scale averaging. We would like to have a fast averaging process at the bottom level, and a slower averaging process at the top and intermediate level. Let \u03c41 be the fast communication period between the leaf nodes and their parents. Let \u03c42 be the slow communication period between the intermediate nodes. For the leaf nodes, i.e. without child, we set \u03c4up = \u03c41, \u03c4down = NaN . For their parents (which should sit on the same machine as their children), we set \u03c4up = \u03c42, \u03c4down = \u03c41. For the rest of the intermediate nodes, except for the root node, we set \u03c4up = \u03c4down = \u03c42. Lastly, for the root node, we set \u03c4up = NaN , \u03c4down = \u03c42.\nThe second scheme is to mimic the behavior of EASGD for \u03b2 being large and \u03b1 being small (c.f. Equation 2.3 and 2.4). We use a large \u03c4up to represent large \u03b2, and a small \u03c4down to represent small \u03b1. For simplicity, we set each tree node with the same \u03c4up = \u03c4u and \u03c4down = \u03c4d, except for the leaf node with \u03c4down = NaN and the root node with \u03c4up = NaN .\n116\nThere\u2019s a subtle difference between EASGD and EASGD Tree. As we discussed in Section 2.2, the design of EASGD follows the Jacobi form. In EASGD Tree, we use instead the Gauss-Seidel form to perform the moving average. This is because the time when the parameter will arrive is not predictable in general, so we\u2019d rather perform the update just-in-time. Nevertheless, we should be careful to avoid performing the moving average during the gradient update. Moreover, there\u2019s a deeper connection between the Jacobi/Gauss-Seidel method and EASGD/DOWNPOUR method that we shall discuss in the next Section 6.2. Thus EASGD Tree is somewhere in-between EASGD and DOWNPOUR.\nAlgorithm 6: EASGD Tree Processing by worker i, with its parent p(i) and children {c(i, j)|j = 1, . . . , d}. Input: learning rate \u03b7, moving rate \u03b1, communication period \u03c4up and \u03c4down \u2208 N Initialize: xi = x0, t i = 0\nIrecv xp(i) from parent p(i) Irecv xc(i,j) from children {c(i, j)|j = 1, . . . , d} Repeat if (\u03c4up divides t\ni) then Isend xi to parent p(i)\nend if (\u03c4down divides t\ni) then Isend xi to children {c(i, j)|j = 1, . . . , d}\nend if there is new arrival of xp(i) then\nxi \u2190 xi + \u03b1(xp(i) \u2212 xi) end if there is new arrival of xc(i,j) then\nxi \u2190 xi + \u03b1(xc(i,j) \u2212 xi) end xi \u2190 xi \u2212 \u03b7\u2207f(xi, \u03bei\nti )\nti \u2190 ti + 1 Until forever"}, {"heading": "6.1.2 The result", "text": "In this section, we show empirically the advantage and the challenge to scale up the EASGD Tree to a few hundreds of CPU cores. Since we are limited by the number\n117\nGPUs available, all the results are run on CPUs. The experiment setup is thus different to our earlier results in Section 4.1. We report the results based on CIFAR-10 dataset using a low-rank convolutional neural-network model [56]. The low-rank convolution approximation saves us a lot of CPU computation time, so we can simulate the long term behavior of the algorithm within 12 hours. We do not use any data augmentation as we did in Chapter 4, because it requires extra CPUs to pre-process the input data. Each data point is sampled uniformly random with replacement (different to what we did in Section 4.1). As in [56], we center the input pixel values by the mean and the standard deviation for each of the three channels using the training data. We also add l2-regularization \u03bb 2 \u2016x\u2016 2 to the loss function with \u03bb = 0.0001.\nFor consistency, we describe the detailed model1 using the notation in Section 4.1: (3, 32, 32) C\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\n(1,5,1,1,0,2) (10, 32, 32) C,R\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (5,1,1,1,2,0) (96, 32, 32) P\u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (96, 16, 16) C\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (1,5,1,1,0,2)\n(51, 16, 16) C,R\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\n(5,1,1,1,2,0) (128, 16, 16) P\u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (128, 8, 8) C\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (1,5,1,1,0,2) (51, 8, 8) C,R\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (5,1,1,1,2,0)\n(256, 8, 8) P\u2212\u2212\u2212\u2212\u2212\u2192\n(2,2,2,2) (256, 4, 4) C,R\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (1,1,1,1,0,0) (64, 4, 4) D,L,R\u2212\u2212\u2212\u2212\u2192 0.5 (256, 1, 1) D,L,S\u2212\u2212\u2212\u2212\u2192 (10, 1, 1).\nIn the following experiments, we call CIFAR-lowrank the experiment that is just described. We use a d-ary tree with degree d = 16 and p = 256 leaf nodes running on 16 machines. For simplicity, we set the moving rate \u03b1 at each node to be a constant 0.9/(16 + 1). First we examine the two communication schemes that we have discussed above. Figure 6.3 shows the performance of the first communication scheme using period \u03c41 = 10 and \u03c42 = 100. Figure 6.4 shows the performance of the second communication scheme using \u03c4u = 8 and \u03c4d = 80. We see that the first communication scheme achieves a faster convergence in terms of the training loss. All the six runs are nearly the same at the beginning. However, five runs out of the six have diverged in the middle of the training. We have chosen a quite large learning rate which can trigger this instability. The second communication scheme is more stable. Only one out of the six runs has diverged. It has a slower convergence in terms of the training loss, but it consistently\n1see also in https://github.com/chengtaipu/lowrankcnn/blob/master/cifar/models/baseline_ lowrank_cudnn.lua\n118\nachieves a lower test error (the smallest one is 15.32%, compared to the smallest test error of the first scheme which is 16.86%). Also notice that there are two runs with a slower initial convergence rate. This may be related to the network traffic.\nNext we examine the effect of the momentum when using a mini-batch size of 16. We use mini-batch as we have observed that the momentum can help more when the stochastic gradient variance is smaller (c.f. also Figure 5.14). Moreover, it would be nice to see whether using a relatively small mini-batch size can still hurt the test performance. As in EAMSGD, we can apply Nesterov\u2019s momentum to each of the leaf node during the gradient computation. Compared to the earlier results (without using mini-batch) in Figure 6.3 and 6.4, we increase the total training time from 3 hours to 12 hours. This is because we do not have more CPU resources to parallelize the mini-batch computation. It takes 0.13sec/step in the minibatch case vs. 0.01sec/step in the case without using mini-batch.\nIn Figure 6.5, 6.6 and 6.7, we report the results of the momentum in the first communication scheme. We see that without using momentum (i.e. \u03b4 = 0), the training process is still not very stable as before. Three out of the six runs have diverged. Using momentum \u03b4 = 0.9 allows us to reduce the learning rate by a factor of ten (see also the discussion of Figure 5.14), and gives a much more stable training process. Also they look almost identical, except for the two curves marked in e and f. This difference is due to a longer initialization of all the nodes. Using momentum \u03b4 = 0.99 is also very stable, though the results are more varied.\nIn Figure 6.8, 6.9 and 6.10, we report the results of the momentum in the second communication scheme. Without using momentum is still not as stable. The curve a and c have stopped in the middle without any indication of divergence. But they did have diverged and ended with NaN. Nevertheless, the curve b gives a smallest test error 15.4%. The second communication scheme again leads to better test performance. Using momentum \u03b4 = 0.9 gives a smallest test error 14.91% in curve b. Using momentum \u03b4 = 0.99 also\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\ngives a smallest test error 14.91% in curve d. The most surprising observation is that the curve a has a large peak between hour 8 and 9. We have investigated the log and found that it is correlated with the network traffic.\nWe can now compare all the momentum results by selecting the best test performance curve in previous results. They are summarized in Figure 6.11. It\u2019s very clear that the two communication schemes give very different performance. The first scheme gives better training performance, while the second scheme gives better test performance. The momentum no longer plays a significant role to determine the test performance as in EAMSGD, but it is still helpful. To explain why still remains very open.\nFor completeness, we compare EASGD, DOWNPOUR with p = 16, and the EASGD Tree with p = 256. Figure 6.12 shows the best test performance for each method, selected among a wide range of the learning rates.\n6.2 Unifying EASGD and DOWNPOUR\nIt\u2019s beneficial to write down the updates of the DOWNPOUR method (Algorithm 3 in Section 4.4) in a synchronous scenario. For simplicity, let\u2019s assume that the communication period \u03c4 = 1. The global clock t is shared among the local workers. Every step DOWNPOUR performs the following updates to solve the original Problem 1.1 in Chapter 1:\nxit+1 = x\u0303t \u2212 \u03b7\u2207f(x\u0303t, \u03beit), x\u0303t+1 = x\u0303t + \u2211p i=1(x i t+1 \u2212 x\u0303t).\n(6.1)\nNow compare with the EASGD updates (from Equation 2.4 in Chapter 2):\nxit+1 = x i t \u2212 \u03b7\u2207f(xit, \u03beit)\u2212 \u03b1(xit \u2212 x\u0303t), x\u0303t+1 = x\u0303t + \u03b2 p \u2211p i=1(x i t \u2212 x\u0303t).\n(6.2)\nWe realize that it\u2019s possible to unify these two methods by drawing a parallel between\n130\nthe classical Jacobi and Gauss-Seidel methods. To motivate this connection, let\u2019s look back at the convergence result of EASGD. Lemma 3.1.1 indicates that the divergence can happen if the condition 3.4 is not satisfied. One might wonder why there\u2019s such a stability condition. After all, EASGD performs only the gradient descent and the moving averages. Each of these operations is contractive on its own! A short answer is that EASGD is formalized in the Jacobi form rather than the Gauss-Seidel form in order to capture the delay of the parameter communication. We can transform the EASGD \u2019s Jacobi form in Equation 6.2 to a Gauss-Seidel form as follows:\nxit+1/2 = x i t \u2212 \u03b1(xit \u2212 x\u0303t),\nxit+1 = x i t+1/2 \u2212 \u03b7\u2207f(xit+1/2, \u03beit+1/2), x\u0303t+1 = x\u0303t + \u03b2 p \u2211p i=1(x i t+1 \u2212 x\u0303t).\n(6.3)\nThe first and the third step in Equation 6.3 is contractive if we choose \u03b1 \u2208 (0, 2) and \u03b2 \u2208 (0, 2). Thus the stability condition 3.4 is too much a constraint in this case. Notice that the DOWNPOUR method in Equation 6.1 is equivalent to the EASGD \u2019s GaussSeidel form in Equation 6.3 with \u03b1 = 1 and \u03b2 = p. It is in this sense that DOWNPOUR and EASGD get connected! However, \u03b2 = p can be very large, how can it be stable?\nFor stability analysis, we consider again the quadratic case where \u2207f(x, \u03be) = x. We can reduce the Equation 6.3 by introduing x\u0304t = 1 p \u2211p i=1 x i t as follows:\nx\u0304t+1/2 = x\u0304t \u2212 \u03b1(x\u0304t \u2212 x\u0303t),\nx\u0304t+1 = x\u0304t+1/2 \u2212 \u03b7x\u0304t+1/2, x\u0303t+1 = x\u0303t + \u03b2(x\u0304t+1 \u2212 x\u0303t).\n(6.4)\nThis is a linear system which can be written as (x\u0304t+1, x\u0303t+1) T = M(x\u0304t, x\u0303t) T , where\nM =  1 0 \u03b2 1\u2212 \u03b2   1\u2212 \u03b7 0 0 1   1\u2212 \u03b1 \u03b1 0 1  . (6.5)\n131\nWe evaluate the two eigenvalues z1 and z2 of this matrix M , which should satisfy the equation z2 \u2212 2bz + c = 0. One can check that\nb = 12(2\u2212 \u03b2 \u2212 \u03b7 \u2212 \u03b1(1\u2212 \u03b2)(1\u2212 \u03b7)), c = (1\u2212 \u03b7)(1\u2212 \u03b1)(1\u2212 \u03b2). (6.6)\nWe need a basic lemma to study the stability, i.e. when the absolute value of z1 and z2 are smaller than one. Lemma 6.2.1. Let z1 and z2 be the two roots of the polynomial z 2 \u2212 2bz + c = 0 with real coefficients b \u2208 R and c \u2208 R, then the sufficient and necessary condition for |z1| < 1 and |z2| < 1 is\n\u2212 1 < c < 1, c > 2b\u2212 1, c > \u22122b\u2212 1. (6.7)\nProof. There are two cases to consider. The first one is the real-valued case, i.e. b2 > c. The second one is the imaginary case, i.e. b2 \u2264 c. If b2 \u2264 c, then |z1| = |z2| = c2 < 1 is equivalent to \u22121 < c < 1. If b2 > c then we can suppose z1 = b + \u221a b2 \u2212 c and z2 = b \u2212 \u221a b2 \u2212 c. The condition \u22121 < z2 < z1 < 1 is equivalent to c > 2b \u2212 1, b < 1, c > \u22122b \u2212 1 and b > \u22121. Combining the condition from the two cases gives us the Equation 6.7.\nApplying the Lemma 6.7 to the above case in Equation 6.6, we obtain the stability condition for the reduced linear system 6.4 as below,\n\u22121 < c < 1, 0 < \u03b2\u03b7 < 2(c+ 1),\nwhere c = (1\u2212 \u03b7)(1\u2212 \u03b1)(1\u2212 \u03b2). It\u2019s easy to verify this since 2b = c+ 1\u2212 \u03b2\u03b7.\nWe summarize the obtained stability condition (using Mathematica) in the following theorem. Theorem 6.2.1. Assume \u03b7 > 0, \u03b1 > 0 and \u03b2 > 0, the stability condition for the linear system 6.4 is equivalent to union of the following set of conditions:\n132\n\u2022 0 < \u03b7 < 1, 0 < \u03b2 < 1 and 0 < \u03b1 < c2,\n\u2022 0 < \u03b7 < 1, \u03b2 = 1 and \u03b1 > 0,\n\u2022 0 < \u03b7 < 1, 1 < \u03b2 \u2264 2 and 0 < \u03b1 < c3,\n\u2022 0 < \u03b7 < 1, 2 < \u03b2 < 4/\u03b7 and c2 < \u03b1 < c3,\n\u2022 \u03b7 = 1, 0 < \u03b2 < 2 and \u03b1 > 0,\n\u2022 1 < \u03b7 < 2, 0 < \u03b2 < 1 and 0 < \u03b1 < c3,\n\u2022 1 < \u03b7 < 2, \u03b2 = 1 and \u03b1 > 0,\n\u2022 1 < \u03b7 < 2, 1 < \u03b2 < 2 and 0 < \u03b1 < c2,\n\u2022 2 \u2264 \u03b7 \u2264 4, 0 < \u03b2 < 1 and c2 < \u03b1 < c3,\n\u2022 \u03b7 > 4, 0 < \u03b2 < 4/\u03b7 and c2 < \u03b1 < c3,\nwhere\nc2 = 4\u2212 2\u03b2 \u2212 2\u03b7 + \u03b2\u03b7 2\u2212 2\u03b2 \u2212 2\u03b7 + 2\u03b2\u03b7 , c3 = \u2212\u03b2 \u2212 \u03b7 + \u03b2\u03b7 1\u2212 \u03b2 \u2212 \u03b7 + \u03b2\u03b7 .\nThere is only one condition above which satisfies the DOWNPOUR method when p > 2. This condition is 0 < \u03b7 < 1, 2 < \u03b2 < 4/\u03b7 and c2 < \u03b1 < c3. It means that if \u03b2 = p is very large, we need to use quite small learning rate such that 0 < \u03b7 < 4/p. Moreover, \u03b1 has to stay very close to 1 (i.e. forgetting all the local memory). This is because c2 tends to 1 as \u03b2 = p tends to infinity (assume \u03b2\u03b7 < 4 always holds); and c3 tends to 1 as well. For example, this condition does not hold for \u03b1 = 0.9 and \u03b2 = 16, for any \u03b7 > 0. This is rather surprising: the stability condition for \u03b1 = 1 and \u03b2 = 16 is 0 < \u03b7 < 1/8. However, there\u2019s no \u03b7 > 0 for \u03b1 = 0.9 and \u03b2 = 16 to be stable. This suggest DOWNPOUR is a very peculiar singleton in the class of the methods defined by Equation 6.3. EASGD has much more flexibilities as it operates in the region where \u03b2 \u2208 (0, 2). In practice, we have observed that the case \u03b2 = 1, 0 < \u03b7 < 2, \u03b1 > 0 works well. It is very dangerous to operate in the region of \u03b7 > 2 when \u03b1 is small.\n133\nChapter 7"}, {"heading": "Conclusion", "text": "In this thesis, we focused on the problem of training large-scale deep learning models in a parallel and distributed computing environment.\nOur starting point is a reformulation of the global variable consensus problem into an unconstrained optimization problem using a quadratic penalty between each local variable and the center variable. By reinterpreting the gradient of the quadratic penalty as an averaging process, we introduced the Elastic Averaging SGD (EASGD) method. The EASGD maintains the stochastic nature of the sequential SGD method through a weak coupling between the local variables and the center variable. Each local variable is optimized using an SGD-based method. The averaging process attracts the local variable and the center variable toward each other so that all of them move toward a local optimum. The center variable can slowly track the spatial average of the local variables, thus having a smaller variance. We then discussed how to extend the EASGD method to the asynchronous scenario, and how to accelerate it using the momentum.\nWe proceeded to study the convergence rate the EASGD method in terms of the center variable. In the synchronous scenario, we discussed the variance reduction effect of the EASGD method by increasing the number of local variables, for both quadratic and strongly-convex case. We also introduced a double averaging sequence in the spirit of\n134\nthe Polyak \u2019s averaging and showed that it is indeed asymptotically optimal. We then compared the stability region of the EASGD method with the ADMM method in the round-robin scheme, where we have found quite unusual instability region for the ADMM method.\nHaving studied the (local) convergence property of the EASGD method, we applied its asynchronous extension to train deep learning models. We focused on the CIFAR-10 and ImageNet ILSVRC 2013 dataset for image classification in a supervised learning setting. The (global) convergence behavior of the EASGD method was simulated starting from a random initialization. We found that the EASGD method accelerates the training of the baseline SGD method. Moreover, it is very communication efficient compared to the asynchronous SGD method DOWNPOUR. After combining with Nesterov \u2019s momentum method, we obtained EAMSGD and it achieved even better test accuracy. We also discussed the tradeoff between the data communication and the parameter communication. In our cases, it turned out to save the total bandwidth a lot by using more data communication for less parameter communication (recall EASGD method needs to sample the whole dataset in order to avoid the bias of the stochastic gradient).\nBased on the empirical results we have obtained, we asked what is the limit of the speedup by increasing the number of processors. We studied first an additive noise model to capture the asymptotic behavior of various stochastic optimization method, then we proposed a multiplicative noise model to capture the initial behavior of these methods. For the additive noise case, we first studied the SGD method with mini-batch. We saw that increasing the mini-batch size (i.e. number of processors) can reduce the asymptotic variance, but it is not able to increase the convergence speed. We then studied the momentum SGD in the Nesterov \u2019s form, and showed that faster convergence speed is possible, but that will lead to larger asymptotic variance. We have also generalised the EASGD method by decoupling (de-symmetrize) the elastic averaging between the center variable and the local variables, and found that the EASGD method can behavior like a momentum SGD with a negative moving average rate in some optimal sense (i.e.\n135\nthe center variable pushes the local variables instead of pulling). For the multiplicative noise case, we have found that SGD method with mini-batch can greatly improved the initial convergence speed by choosing the optimal (larger) learning rate. However, the momentum SGD is not as effective in this case. The EASGD method is also slower than SGD with mini-batch, but its stability region is still improved by increasing the number of processors. We showed that given an infinite number of processors, the improvement for stability region of EASGD depends on the spread of the Gamma distribution of the input data. The larger the spread, the more the improvement.\nBack to our starting point, we studied an interesting non-convex case for the EASGD method based on the an unconstrained objective we have introduced. We have discussed the stability of the critical points. We found that when the quadratic penalty is too small (i.e. the coupling between the center variable and the local variable is too weak), the EASGD method can introduce a stable local optimum trapped by a saddle point.\nKnowing the theoretical limitation of the speedup, we scale up the EASGD by using a tree structured network topology. We showed empirically that EASGD Tree can further accelerate the training speed, but no longer in a linear speedup region. This is somehow predicted by our analysis in the multiplicative noise model. The surprising observation is EASGD Tree can still yield better test accuracy when the local variables fluctuate further from the center variable. Moreover, inspired by the design of the EASGD Tree in the asynchronous scenario, we carefully distinguished the difference between the Jacobi form of EASGD with its Gauss-Seidel form, which in turn unified the EASGD and the DOWNPOUR method in the synchronous scenario.\nWe conclude with a few open problems from convex optimization, non-convex optimization, distributed and parallel computing, statistical and deep learning.\n\u2022 The smoothing property of EASGD for non-smooth convex optimization. We have\ndiscussed the connection between the quadratic penalty term in Equation 1.2 and the Moreau-Yosida regularization. Can one expect a good rate of convergence?\n136\n\u2022 The connection between EAMSGD and Katyusha [3]. Katyusha is an accelerated\nvariance reduction method. The speedup of its acceleration is dragged back by a center variable so as to achieve a smaller variance. This resembles the role of the center variable in EAMSGD.\n\u2022 In the simulation of EASGD Tree, we have seen that network congestion can occur.\nIs there a good way to detect when the network becomes slow so as to avoid the potential congestion? This is an adaptive learning rate problem. It is not very simple as we have seen in Chapter 5 that the optimal moving rate can sometime be positive, but sometime be negative.\n\u2022 The convergence analysis of the EASGD Tree is still missing. The basic tool [57]\nbased on the mean-field method (i.e. treating each node the same) seems to be too macroscopic to capture the global (e.g. multi-scale variance reduction) behavior of the EASGD Tree.\n\u2022 The description of the asynchronous behavior using vector-clock [53]. Although we\nhave unified EASGD and DOWNPOUR in the synchronous scenario, it is still not clear how to measure their behavior in the asynchronous scenario and in the realtime workloads. The idea of using vector-clock from the distributed computing is worthing exploring in the future.\n\u2022 How non-convex is the energy landscape? A related question may be what to\ndo when EASGD is trapped by a saddle point. Also why there is a such a big difference between ASGD and MVASGD as we have studied in Figure 4.11?\n\u2022 What is the role of the oscillation in deep learning to achieve better test perfor-\nmance? We have observed that both EAMSGD and EASGD Tree can achieve better test performance when the oscillation of the local variables is sufficiently large. Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?\n137"}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["A. Agarwal", "O. Chapelle", "M. Dud\u0301\u0131k", "J. Langford"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "In NIPS, pages 873\u2013881,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Katyusha: Accelerated variance reduction for faster sgd", "author": ["Z. Allen-Zhu"], "venue": "arXiv preprint arXiv:1603.05953,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Towards an optimal stochastic alternating direction method of multipliers", "author": ["S. Azadi", "S. Sra"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Scaling up machine learning: Parallel and distributed approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A new class of incremental gradient methods for least squares problems", "author": ["D.P. Bertsekas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Parallel and Distributed Computation", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Asynchronous stochastic approximations", "author": ["V. Borkar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Curiously fast convergence of some stochastic gradient descent algorithms. Unpublished open problem offered to the attendance of the SLDS", "author": ["L. Bottou"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M.B. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distributed deep learning using synchronous stochastic gradient descent", "author": ["D. Das", "S. Avancha", "D. Mudigere", "K. Vaidynathan", "S. Sridharan", "D. Kalamkar", "B. Kaul", "P. Dubey"], "venue": "arXiv preprint arXiv:1602.06709,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "First-order methods of smooth convex optimization with inexact oracle", "author": ["O. Devolder", "F. Glineur", "Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "Automatic control, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Distributed deep learning for answer selection", "author": ["M. Feng", "B. Xiang", "B. Zhou"], "venue": "CoRR, abs/1511.01158,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Global convergence of the heavy-ball method for convex optimization", "author": ["E. Ghadimi", "H.R. Feyzmahdavian", "M. Johansson"], "venue": "In Control Conference (ECC), 2015 European,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "The unreasonable effectiveness of data", "author": ["A. Halevy", "P. Norvig", "F. Pereira"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Optimization theory: the finite dimensional case", "author": ["M.R. Hestenes"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1975}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J.K. Kim", "P.B. Gibbons", "G.A. Gibson", "G. Ganger", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Deepspark: Spark-based deep learning supporting asynchronous updates and caffe compatibility", "author": ["H. Kim", "J. Park", "J. Jang", "S. Yoon"], "venue": "arXiv preprint arXiv:1602.08191,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "An optimal method for stochastic composite optimization", "author": ["G. Lan"], "venue": "Mathematical Programming,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Slow learners are fast", "author": ["J. Langford", "A. Smola", "M. Zinkevich"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Dynamics of stochastic gradient algorithms", "author": ["Q. Li", "C. Tai", "W. E"], "venue": "CoRR, abs/1511.06251,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A universal catalyst for first-order optimization", "author": ["H. Lin", "J. Mairal", "Z. Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Sparknet: Training deep networks in spark", "author": ["P. Moritz", "R. Nishihara", "I. Stoica", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1511.06051,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Distributed asynchronous incremental subgradient methods", "author": ["A. Nedi\u0107", "D. Bertsekas", "V. Borkar"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Cooperative distributed multi-agent optimization", "author": ["A. Nedi", "A. Ozdaglar"], "venue": "Convex Optimization in Signal Processing and Communications,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Math. Program.,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Numerical Optimization, Second Edition", "author": ["J. Nocedal", "S. Wright"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2006}, {"title": "Convergence speed in distributed consensus and averaging", "author": ["A. Olshevsky", "J.N. Tsitsiklis"], "venue": "SIAM review,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Gpu asynchronous stochastic gradient descent to speed up neural network training", "author": ["T. Paine", "H. Jin", "J. Yang", "Z. Lin", "T. Huang"], "venue": "arXiv preprint arXiv:1312.6186,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Introduction to optimization", "author": ["B.T. Polyak"], "venue": "Optimization Software New York,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1987}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1992}, {"title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In NIPS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Iterative methods for sparse linear systems", "author": ["Y. Saad"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2003}, {"title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "In Interspeech", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR2014),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation", "author": ["O. Shamir"], "venue": "In NIPS,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Oscillation helps to get division right", "author": ["D.J. Sherratt"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2016}, {"title": "An efficient implementation of vector clocks", "author": ["M. Singhal", "A. Kshemkalyani"], "venue": "Information Processing Letters,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1992}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2013}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["C. Tai", "T. Xiao", "X. Wang", "W. E"], "venue": "In ICLR,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J.N. Tsitsiklis", "D.P. Bertsekas", "M. Athans"], "venue": "American Control Conference,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1984}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "In ICML,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "Distributed average consensus with least-meansquare deviation", "author": ["L. Xiao", "S. Boyd", "S.-J. Kim"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2007}, {"title": "Multi-gpu training of convnets", "author": ["O. Yadan", "K. Adams", "Y. Taigman", "M. Ranzato"], "venue": "In Arxiv,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}, {"title": "Asynchronous distributed admm for consensus optimization", "author": ["R. Zhang", "J. Kwok"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "Staleness-aware async-sgd for distributed deep learning", "author": ["W. Zhang", "S. Gupta", "X. Lian", "J. Liu"], "venue": "arXiv preprint arXiv:1511.05950,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A. Smola", "L. Li"], "venue": "In NIPS,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 2) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 8, "context": "The subject of this thesis is on how to parallelize the training of large deep learning models that use a form of stochastic gradient descent (SGD) [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 29, "context": "As an optimization method, it often exhibits fast initial convergence toward the local optimum as compared to the batch gradient method [30].", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 21, "context": "However, as the size of the dataset explodes [22], the amount of time taken to go through each data point sequentially becomes prohibitive.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "There have been attempts to parallelize the training for large-scale deep learning models on thousands of CPUs, including the Google\u2019s Distbelief system [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "neural networks trained on a few GPU cards sitting in a single computer [27, 49].", "startOffset": 72, "endOffset": 80}, {"referenceID": 48, "context": "neural networks trained on a few GPU cards sitting in a single computer [27, 49].", "startOffset": 72, "endOffset": 80}, {"referenceID": 51, "context": "To date, the AlphaGo system is trained using 50 GPUs for a few weeks [52].", "startOffset": 69, "endOffset": 73}, {"referenceID": 39, "context": "To solve such large-scale optimization problem, one line of research is to fill-in the gap between the stochastic gradient descent method (SGD) and the batch gradient method [40].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 16, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 24, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 30, "context": "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].", "startOffset": 107, "endOffset": 123}, {"referenceID": 24, "context": "However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15].", "startOffset": 210, "endOffset": 218}, {"referenceID": 14, "context": "However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15].", "startOffset": 210, "endOffset": 218}, {"referenceID": 61, "context": "It is even observed that in deep learning problems, using too large mini-batch size may lead to solutions of very poor test accuracy [62].", "startOffset": 133, "endOffset": 137}, {"referenceID": 7, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 28, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 62, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 1, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 45, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 15, "context": "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].", "startOffset": 83, "endOffset": 105}, {"referenceID": 61, "context": "The tradeoff is that asynchronous behavior results in large communication delay, which can in turn slow down the convergence rate [62].", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "The DOWNPOUR method belongs to the above class of asynchronous SGD methods, and is proposed for training deep learning models [16] .", "startOffset": 126, "endOffset": 130}, {"referenceID": 5, "context": "This idea resembles the incremental gradient method [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 33, "context": "The mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26].", "startOffset": 140, "endOffset": 148}, {"referenceID": 25, "context": "The mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26].", "startOffset": 140, "endOffset": 148}, {"referenceID": 56, "context": "It is based on the idea of consensus averaging [57].", "startOffset": 47, "endOffset": 51}, {"referenceID": 40, "context": "clock synchronization) [41].", "startOffset": 23, "endOffset": 27}, {"referenceID": 58, "context": "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 36, "context": "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].", "startOffset": 176, "endOffset": 184}, {"referenceID": 18, "context": "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].", "startOffset": 176, "endOffset": 184}, {"referenceID": 10, "context": "The ADMM (Alternating Direction Method of Multipliers) [11] method can also be used to solve the consensus averaging problem above.", "startOffset": 55, "endOffset": 59}, {"referenceID": 41, "context": "ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61].", "startOffset": 122, "endOffset": 130}, {"referenceID": 60, "context": "ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61].", "startOffset": 122, "endOffset": 130}, {"referenceID": 6, "context": "2 Formalizing the problem Consider minimizing a function F (x) in a parallel computing environment [7] with p \u2208 N workers and a master.", "startOffset": 99, "endOffset": 102}, {"referenceID": 23, "context": "The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11].", "startOffset": 160, "endOffset": 168}, {"referenceID": 10, "context": "The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11].", "startOffset": 160, "endOffset": 168}, {"referenceID": 49, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 15, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 59, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 42, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 47, "context": "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].", "startOffset": 119, "endOffset": 139}, {"referenceID": 6, "context": "when the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work.", "startOffset": 47, "endOffset": 53}, {"referenceID": 4, "context": "when the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work.", "startOffset": 47, "endOffset": 53}, {"referenceID": 12, "context": "We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 32, "context": "2 and the Moreau-Yosida regularization [33] in convex optimization used to smooth the non-smooth part of the objective function.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "When using the rectified linear units [35] in deep learning models, the objective function also becomes non-smooth.", "startOffset": 38, "endOffset": 42}, {"referenceID": 39, "context": "EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45].", "startOffset": 51, "endOffset": 55}, {"referenceID": 44, "context": "EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Thus in order to minimize the staleness [25] of the difference xt\u2212x\u0303t between the center and the local variable for the asynchronous EASGD (described in Algorithm 1), the update for the master in Equation 2.", "startOffset": 40, "endOffset": 44}, {"referenceID": 46, "context": "4) as a Jacobi method [47].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 3, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 7, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 35, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 28, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 1, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 45, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 62, "context": "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.", "startOffset": 69, "endOffset": 98}, {"referenceID": 6, "context": "Note that the asynchronous behavior described above is partially asynchronous [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 38, "context": "It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.", "startOffset": 47, "endOffset": 59}, {"referenceID": 27, "context": "It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.", "startOffset": 47, "endOffset": 59}, {"referenceID": 54, "context": "It is based on the Nesterov \u2019s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.", "startOffset": 47, "endOffset": 59}, {"referenceID": 43, "context": "In literature, there\u2019s another well-known momentum variant called heavy-ball method (aka Polyak \u2019s method) [44].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "The analysis of its global convergence property is still a very challenging problem in convex optimization literature [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "One reason is that the momentum method has an error accumulation effect [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 31, "context": "Due to the stochastic noise in the gradient, using momentum can actually result in higher asymptotic variance (see [32] and our discussion in Section 5.", "startOffset": 115, "endOffset": 119}, {"referenceID": 44, "context": "Our analysis in the quadratic case extends the analysis of ASGD in [45].", "startOffset": 67, "endOffset": 71}, {"referenceID": 44, "context": "2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [45]) {z1, z2, .", "startOffset": 141, "endOffset": 145}, {"referenceID": 44, "context": "Note that this linear system has a degenerate noise \u039et which prevents us from directly applying results of [45].", "startOffset": 107, "endOffset": 111}, {"referenceID": 37, "context": "We start from the following estimate for the strongly convex function [38], \u3008\u2207F (x)\u2212\u2207F (y), x\u2212 y\u3009 \u2265 \u03bcL \u03bc+ L \u2016x\u2212 y\u2016 + 1 \u03bc+ L \u2016\u2207F (x)\u2212\u2207F (y)\u2016 .", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "In this section we study the stability of the asynchronous EASGD and ADMM methods in the round-robin scheme [29].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax", "startOffset": 32, "endOffset": 44}, {"referenceID": 60, "context": "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax", "startOffset": 32, "endOffset": 44}, {"referenceID": 41, "context": "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax", "startOffset": 32, "endOffset": 44}, {"referenceID": 41, "context": "At each t, we linearize the function F (xi) with F (xt)+ \u3008 \u2207F (xt), xi \u2212 xt \u3009 + 1 2\u03b7 \u2225\u2225xi \u2212 xit\u2225\u22252 as in [42].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbt \u2190 \u03bbt/\u03c1 in the above updates.", "startOffset": 89, "endOffset": 101}, {"referenceID": 60, "context": "Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbt \u2190 \u03bbt/\u03c1 in the above updates.", "startOffset": 89, "endOffset": 101}, {"referenceID": 41, "context": "Note that since the step size for the dual ascent update is chosen to be \u03c1 by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be \u03bbt \u2190 \u03bbt/\u03c1 in the above updates.", "startOffset": 89, "endOffset": 101}, {"referenceID": 60, "context": "The convergence analysis in [61] is based on the assumption that \u201cAt any master iteration, updates from the workers have the same probability of arriving at the master.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "All the parallel comparator methods are listed below1: \u2022 DOWNPOUR [16], the detail and the pseudo-code of the implementation are described in Section 4.", "startOffset": 66, "endOffset": 70}, {"referenceID": 60, "context": "\u2022 A method that we call MVADOWNPOUR, where we compute the moving average We have compared asynchronous ADMM [61] with EASGD in our setting as well, the performance is nearly the same.", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "All the sequential comparator methods (p = 1) are listed below: \u2022 SGD [9] with constant learning rate \u03b7.", "startOffset": 70, "endOffset": 73}, {"referenceID": 54, "context": "\u2022 Momentum SGD (MSGD) [55] with constant (Nesterov\u2019s) momentum rate \u03b4.", "startOffset": 22, "endOffset": 26}, {"referenceID": 44, "context": "\u2022 ASGD [45] with moving rate \u03b1t+1 = 1 t+1 .", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "\u2022 MVASGD [45] with moving rate \u03b1 set to a constant.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "The center variable of the master is stored and updated on the centralized parameter server [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 34, "context": "[35]), P denotes the max pooling operator, L denotes the linear operator and D denotes the dropout operator with rate equal to 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "For the ImageNet experiment we use the similar approach to [49] with the following 11-layer convolutional neural network: (3, 221, 221) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (7,7,2,2) (96, 108, 108) P \u2212\u2212\u2212\u2212\u2212\u2192 (3,3,3,3) (96, 36, 36) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1) (256, 32, 32) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (256, 16, 16) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (3,3,1,1) (384, 14, 14) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,1,1) (384, 13, 13) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,1,1) (256, 12, 12) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (256, 6, 6) L,R,D \u2212\u2212\u2212\u2212\u2192 0.", "startOffset": 59, "endOffset": 63}, {"referenceID": 57, "context": "For the CIFAR experiment we use the similar approach to [58] with the following 7-layer convolutional neural network: (3, 28, 28) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1) (64, 24, 24) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (64, 12, 12) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (5,5,1,1) (128, 8, 8) P \u2212\u2212\u2212\u2212\u2212\u2192 (2,2,2,2) (128, 4, 4) C,R \u2212\u2212\u2212\u2212\u2212\u2192 (3,3,1,1) (64, 2, 2) L,R,D \u2212\u2212\u2212\u2212\u2192 0.", "startOffset": 56, "endOffset": 60}, {"referenceID": 53, "context": "5 [54].", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "We also re-scale each pixel value to the interval [0, 1].", "startOffset": 50, "endOffset": 56}, {"referenceID": 0, "context": "As before, we re-scale each pixel value to the interval [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 9, "context": "Notice that each data loader cycles5 through the Its advantage is observed in [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "We remark that if the stochastic gradient is sparse, DOWNPOUR empirically performs well with large communication period [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Thus our initial learning rate is decreased twice over time, by a factor of 5 and then 2, when we observe that the online predictive loss [12] stagnates.", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Notice that we do not use any adaptive learning scheme as having been done in [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "2 by an Ornstein Uhlenbeck process [32] as follows,", "startOffset": 35, "endOffset": 39}, {"referenceID": 43, "context": "In case that \u03b4h is chosen independently of h, the update is no different to the heavy ball method [44].", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "Moreover, as in [32], we can try to minimize |z3| with respect to \u03b4 such that the rate of convergence of the second order moment is maximized for a given \u03b7.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "\u03b4 \u2192 1 [32].", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "This is consistent with the choice of the learning rate and the momentum rate scheduling in the Nesterov\u2019s optimal methods in literature [28].", "startOffset": 137, "endOffset": 141}, {"referenceID": 31, "context": "24 by a Geometric Brownian motion [32] as follows,", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 1) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "34 as a function of the learning rate \u03b7 \u2208 (0, 2) and the number of workers p \u2208 [1, 64].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "For example, the tree was used to scale up the asynchronous SGD method by aggregating the delayed gradients computed by the intermediate nodes and the leaf nodes [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62].", "startOffset": 89, "endOffset": 96}, {"referenceID": 61, "context": "It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62].", "startOffset": 89, "endOffset": 96}, {"referenceID": 55, "context": "We report the results based on CIFAR-10 dataset using a low-rank convolutional neural-network model [56].", "startOffset": 100, "endOffset": 104}, {"referenceID": 55, "context": "As in [56], we center the input pixel values by the mean and the standard deviation for each of the three channels using the training data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "\u2022 The connection between EAMSGD and Katyusha [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 56, "context": "The basic tool [57] based on the mean-field method (i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 52, "context": "\u2022 The description of the asynchronous behavior using vector-clock [53].", "startOffset": 66, "endOffset": 70}, {"referenceID": 53, "context": "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?", "startOffset": 28, "endOffset": 32}, {"referenceID": 57, "context": "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?", "startOffset": 49, "endOffset": 53}, {"referenceID": 50, "context": "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?", "startOffset": 107, "endOffset": 111}], "year": 2016, "abstractText": "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method. We then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. We also find a surprising connection between the momentum SGD and the EASGD method with a negative moving average rate. A non-convex case is also studied to understand when EASGD can get trapped by a saddle point. Finally, we scale up the EASGD method by using a tree structured network topology. We show empirically its advantage and challenge. We also establish a connection between the EASGD and the DOWNPOUR method with the classical Jacobi and the Gauss-Seidel method, thus unifying a class of distributed stochastic optimization methods.", "creator": "LaTeX with hyperref package"}}}