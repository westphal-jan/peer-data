{"id": "1605.02401", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Audio Event Detection using Weakly Labeled Data", "abstract": "acoustic collision detection is essential for content analysis and description of animal recordings. an majority of technical literature on the topic represents the detectors through fully - supervised techniques employing strongly labeled data. however, the labels available for online interactive data are generally weak and do nowhere imply sufficient detail for such methods to be employed. around this stage we propose a framework for identifying acoustic event detectors using only weakly labeled data trapped on a multiple instance learning ( mil ) framework. we first show that audio event detection using weak data can be formulated as an mil problem. we then suggest two frameworks covering solving dual - instance learning, one based on neural networks, and the second on support vector machines. the proposed methods can help in removing the thought consuming and expensive process of manually annotating data to facilitate fully supervised computation. our proposed terms can not only successfully detect events in a recording systems can also provide temporal locations of events in the recording. this is useful as these information were never known in the first place for adequately labeled data.", "histories": [["v1", "Mon, 9 May 2016 02:17:12 GMT  (255kb)", "http://arxiv.org/abs/1605.02401v1", "1st version on arXiv"], ["v2", "Thu, 9 Jun 2016 03:33:13 GMT  (255kb)", "http://arxiv.org/abs/1605.02401v2", "updated version on arXiv"], ["v3", "Wed, 6 Jul 2016 05:46:56 GMT  (256kb)", "http://arxiv.org/abs/1605.02401v3", "ACM Multimedia 2016"]], "COMMENTS": "1st version on arXiv", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.MM", "authors": ["anurag kumar", "bhiksha raj"], "accepted": false, "id": "1605.02401"}, "pdf": {"name": "1605.02401.pdf", "metadata": {"source": "CRF", "title": "Audio Event Detection using Weakly Labeled Data", "authors": ["Anurag Kumar", "Bhiksha Raj"], "emails": ["alnu@andrew.cmu.edu", "bhiksha@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n02 40\n1v 1\n[ cs\n.S D\n] 9\nM ay\n2 01\nKeywords Audio Event Detection, Multiple Instance Learning, Neural Networks, Temporal Localization"}, {"heading": "1. INTRODUCTION", "text": "The amount of consumer-generated multimedia data on the internet has grown almost exponentially in recent times. One popular multimedia upload site, YouTube, reported about a year ago that 300 hours of multimedia recordings are uploaded on it every minute [2]. There are several such sites on the internet today, each of which attract similarly large amounts of data. The recordings are largely unannotated; descriptions if any are limited to simple high-level metadata such as the author, or a brief legend indicating the overall content. Often the legends themselves are cryptic and uninformative to the uninformed, e.g. \u201cMy favorite clip\u201d.\nIn order to be able to organize, categorize, summarize and index these recordings such that they can be retrieved through meaningful queries, one requires analysis of their content. Given the rather spotty nature of the metadata, the description of the content must usually be automatically derived. This naturally requires automatic identification of the objects and events that occur in the recording. Multimedia recordings have both video and audio components. Often, the sounds in the recordings carry information that the video itself may not. Thus, it is necessary not only to de-\ntect the visual objects in the recordings, it is also important to detect the sounds that occur in them.\nAutomatic sound event detection also finds application in other scenarios, such as monitoring traffic for sounds of accidents or impact, surveillance, where one may \u201clisten\u201d for sounds of gunshots [31], screams [25] etc., which might indicate unusual noteworthy activity. It is also useful in cases such as wildlife monitoring [8], context recognition [13] and several health and life style monitoring system.\nIn all cases, the detectors themselves must be \u201ctrained\u201d from examples of the sound to be detected. In general for learning such detectors, one requires annotated data, where the segments of audio containing the desired event are clearly indicated (as well as data in which the events are distinctly not present). We will refer to this type of labeling as strongly labeled data. This is fundamentally limiting, since such well-annotated data are generally scarce.\nMany challenges come out immediately when one attempts to learn detectors from consumer produced videos(audios). Firstly, of course, the recording conditions, styles, and sophistication vary greatly among such recordings, resulting in large within-category variations between different instances of events, making the fundamental learning problem challenging. Much more important however, is the nature of annotations, if any, that they may carry. As mentioned earlier, the vast majority of consumer-produced videos carry little or no content annotation (which is what necessitates the development of automated concept detectors in the first place). Nevertheless, a significant number of these recordings do carry some weak information about their content, in the form of title, tags, etc. By \u201cweak\u201d annotation, we mean that while they may provide information about the presence or absence of particular events in the video, they will not provide additional details such as the number of times these events occur, the precise times in the recording where they occur, the duration of the events. These are the information that are needed to localize the events in the recordings in order to train detectors using conventional methods. Weak labels can be automatically inferred from metadata (tags, titles etc.) associated with recording.\nConventional methods for learning classifiers for event detection generally assume the availability of datasets that contain sufficient information to isolate the segments of the recordings where the target class occurs in order to learn their characteristics. For the other kind of datasets for which only weak labels are present (like those available on internet) most methods rely on manually annotating the available data with the boundaries of events, to facilitate learning\nevent detectors. But creating such annotations is a timeconsuming and expensive procedure. Supervised learning hence becomes a difficult task.\nIn this paper we take the view that it must be possible to learn event detectors from the weakly labeled data itself without requiring the additional effort of detailed annotation. Recordings that have been tagged as containing the event will have regions which are consistent with one another because they contain the event, but are not consistent with any region of recordings that are not similarly tagged. Event-carrying segments can be identified and event detectors trained by keying in on this consistency.\nWe embody this principle into an algorithmic framework which falls under the general rubric ofmultiple-instance learning. Multiple-instance learning is a generalized form of supervised learning in which labels for individual instances are not known; instead labels are available for a collection of instances, or bag as it is usually called within this setup. Although multiple-instance learning approaches have previously been employed for learning event detectors from weakly labeled data, most prior work in this area has focused on visual event recognition. The problem of learning to detect acoustic events from weakly labeled data has received very little attention.\nWe propose a successful framework for acoustic event detection based on the multiple instance learning methodology. In our setup, an audio recording is considered as a bag and segments of the recording are considered as instances within the bag. Besides achieving the goal of learning acoustic event detectors using weakly labeled data; as an added benefit, we are also able to assign temporal locations to occurrences of events in the test data.\nThe rest of the paper is arranged as follows: in Section 2 we give an overview of related work, in Section 3 we formulate the problem and in Section 4 we describe the proposed approach. In Section 5 we describe experimental setup and results and we conclude in Section 6."}, {"heading": "2. RELATED WORK", "text": "The problem of multimedia content analysis has received significant attention in the past decade. Although much of the reported work relates to image- or video-based information extraction, audio-based information extraction has also been explored. These audio based information extraction methods in general involve detection of different kinds of acoustic events such as clapping, cheering, gunshots etc. in audio components of multimedia recordings. A nice survey on the audio event detection challenges, works and state of art can be found in [30]. A brief summary of some audio event detection work is provided here.\nPossibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9]. Audioevent detection has also found its way into consumer devices, particularly for automatic indexing of multimedia recordings of games [16]. The wider setting of detecting sound events in more generic real-world recordings has been restricted in terms of \u201cvocabulary\u201d of sound events considered. This is mainly due to the absence of large vocabulary open source datasets meant for audio event detection research. The authors of [40] model various sound events with Gaussian-mixture models in order to detect them in real-world recordings. The approach followed in [40] is similar to GMM-HMM architecture used for automatic speech recognition. An important simple yet effect approach con-\nverts signals into \u201cbags of audio words\u201d \u2013 population-count histograms \u2013 by clustering feature vectors derived from the signal to a known codebook. Classification is now performed using these bags of words as feature vectors. This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33]. It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few. [18] uses an alternate approach to obtaining bags of words \u2013 sound recordings are first decomposed into sequence of basic sound units called \u201cAcoustic Unit Descriptors\u201d (AUDS), which are themselves learned in an unsupervised manner. Bags of words are then obtained as bags of AUDs. The actual classification may be performed through classifiers such as SVM or random forests [18]. Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].\nIn every one of these cases, the detectors and classifiers that are used to detect the sound events are trained in a supervised manner. Each classifier/detector is trained using several clearly demarcated instances of the type of sound event it must detect, in addition to several \u201cnegative\u201d instances \u2013 instances of audio segments that do not contain the event. Obtaining data of these kinds clearly requires human annotation, and can be time-consuming and expensive process. This clearly presents challenges in terms of training data availability for each event and hence one cannot expect to build detectors for a large vocabulary of sound events.\nOur focus in this paper is to learn detectors from weak annotations \u2013 annotations which only indicate the presence of sound events in recordings, without additional detail about the number of instances, their location in the recording, etc. Such annotations are, in general, easily available and easy to generate if not available.\nIn comparison to supervised techniques, there has been relatively little work on learning to detect events in multimedia using weakly labeled data. Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37]. In the case of audio, however, literature on learning generic sound event detectors from weakly labeled data is almost negligible. Some audio related works include semi-supervised music genre classification are [22] [29]. Two other audio related works are [26] and [8] where authors try to exploit weak labels in bird song and bird species classification.\nIn our work we perform audio event detection (AED) using weakly-labeled data by formulation of it as a multiple instance learning (MIL) problem. MIL is a learning methodology which relies on the labels of an ensemble of instances, rather than labels of individual instances. Several well known learning methods such as Support Vector Machines (SVMs)[4], K-NNs [34] have been modified to learn in this paradigm."}, {"heading": "3. AED USING WEAKLY LABELED DATA", "text": "The problem of audio event detection is that of detecting the occurrences of events (e.g clapping, barking, cheering etc.) in a given audio recording. In order to be able to do so, we require models for these events which can be used to detect their occurrence. Training such models, in\nturn, requires training instances of these sounds. Such instances may be presented either as explicit recordings of these sounds, or as segments from longer audio recordings within which they occur. Our objective is to train the models, instead, using weakly-labeled data which comprises of recordings in which only the presence or absence of these events is identified, without indicating the exact location of the event or even the number of times it has occurred.\nLet R = {Ri : i = 1 to NR} be the collection of audio recordings and E = {Ei : i = 1 to NE} be the set of events for which detection models must be built using R.\nFor each Ri a certain subset of events from E are known to be present (weak-labels). For example, the information might say that Ri contains events E1, E3 and E6. It is also possible that the subset of E present in Ri is empty, meaning that no event from E is present in Ri.\nClearly, to train a detector for an event Ei one cannot simply use all recordings that are marked as containing Ei, since a significant portion of the marked recordings might also contain other events. Moreover, since the start and end times of the occurrences of event Ei in the recordings are not known, it is impossible to extract out the specific segment of the audio that contains the event for further use in supervised learning. Conventional supervised learning is thus not possible. How do we then use these these weak labels for learning detectors for the events? We answer the question in the next Section."}, {"heading": "4. PROPOSED FRAMEWORK", "text": "Our formulation of event detection using weak labels of the kind described above is based onMultiple-Instance Learning (MIL) [10] which is a generalized version of supervised learning in labels are available for a collection of instances. We propose that audio event detection using weak labels is essentially an MIL problem. In the following subsections we first describe the multiple-instance learning framework,Section 4.1, and two popular algorithmic approaches to MIL in Sections 4.3 and 4.2. Subsequently in Sections 4.4, 4.6 and 4.5 we describe how we apply MIL to the problem of learning sound event detectors."}, {"heading": "4.1 Multiple Instance Learning", "text": "The term Multiple Instance Learning was first properly developed by Dietterich et al. in 1997 for drug activity detection [10]. MIL is described in terms of bags; a bag is simply a collection of instances. Labels are attached to the bags, rather than to the individual instances within them. A positive bag is one which has at least one positive instance (an instance from the target class to be classified). A negative bag contains negative instances only. A negative bag is thus pure whereas a positive bag is impure. This generates an asymmetry from a learning perspective as all instances in a negative bag can be uniquely assigned a negative label whereas for a positive bag this cannot be done; an instance in a positive bag may either be positive or negative.\nThus, it is the bag-label pairs, rather than instance-label pairs, which form the training data from which a classifier which classifies individual instances must be learned.\nWe represent the bag-label pairs as (Bi, Yi). Here Bi is the ith bag and contains instances xij where j = 1 to ni and ni is the total number of instances in the bag, i.e. Bi = {xij : j = 1 \u00b7 \u00b7 \u00b7 ni}. Yi \u2208 {\u22121, 1} is the label for bag Bi. Yi = 1 implies a positive bag and Yi = \u22121 implies a negative bag. We might alternatively represent negative labels by 0\nat some place for convenience (in 4.3). Let the total number of bags be indicated by N . One can attempt to infer labels of individual instances xij in the bag from the bag label. The label yij for instances in bag Bi can be stated as:\nYi = 0 \u21d2 yij = 0 \u2200 xij \u2208 Bi, (1)\nYi = 1 \u21d2 yij = 1 for at least one xij \u2208 Bi (2)\nThis relation between Yi and yij can is simply\nYi = max j {yij}. (3)\nThe problem now is to learn a classification model C so that given a new bag B\u0302n it can predict the label Y\u0302n for B\u0302n. Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4]. In this work we work with two frameworks: (1) mi-SVM which modifies support vector machines for the MIL setting and, (2) neural networks for solving MIL problems"}, {"heading": "4.2 MIL for SVM (mi-SVM)", "text": "In [4] Andrews et al. proposed two methods for multipleinstance learning of Support Vector Machines. The first, called mi-SVM, operates at the instance level and maximizes the margin of individual instances from a linear discriminant. The second, referred to as MI-SVM maximizes the margin of bags of instances, rather than individual instances. In this work we use mi-SVM.\nTo understand mi-SVM, we first note that the relation between bag label Yi and instance labels yij can also be represented in the form of linear constraints.\nni \u2211\nj=1\nyij + 1\n2 \u2265 1 \u2200 i s.t Yi = 1, ; yij = \u22121\u2200 i s.t Yi = \u22121\nThe instance labels in mi-SVM are treated as unobserved integer variables subject to the constraints defined above. As in conventional training of SVMs, where we must estimate the parameters of a linear (or kernelized) discriminant function such that the margin of training instances from the discriminant function is maximized, here too we must maximize the margin. However, since the instance labels are unknown, we modify the objective: the goal now is to maximize a soft margin over both the decision function and the hidden integer variables, namely the unknown labels of instances. Optimization of the generalized soft margin is thus\nmin yij ,w,b,\u03be\n1 2 ||w||2 + C \u2211\nij\n\u03beij (4)\nsuch that\n\u2200i, j : yij(\u3008w,xij\u3009+ b) \u2265 1\u2212 \u03beij (5)\n\u03beij \u2265 0 ; yij \u2208 {\u22121, 1} ; yij = \u22121\u2200 i s.t Yi = \u22121 (6) ni \u2211\nj=1\nyij + 1\n2 \u2265 1 \u2200 i s.t Yi = 1 (7)\nIn equation 4 the labels yij of instances belonging to positive bags are unknown integer variables. As a result both, the optimal labeling of these instances as well as the optimal hyperplane (w, b), must be computed. The separating hyperplane must be such that there is at least one pattern from every positive bag in the positive half space and all patterns belonging to negative bags are in the negative half space.\nAlgorithm 1 mi-SVM Algorithm\n1: procedure Learning SVM in MIL setting(Bi, Yi) // Input training bags and labels\n//Initialization Step 2: yij = Yi for all j in Bag Bi 3: repeat 4: compute SVM solution w, b with imputed labels 5: for all bag Bi s.t Yi = 1 do 6: compute fij = (\u3008w,xij\u3009 + b) \u2200 xij in Bi 7: yij = sgn(fij) \u2200 j in bag Bi s.t Yi = 1 8: if ( ni \u2211\nj=1\nyij+1\n2 == 0) then\n9: compute j\u2217 = argmaxj(fij) 10: set yij\u2217=1 11: end if 12: end for 13: until imputed labels no longer change 14: end procedure\nThe above formulation is, however, a difficult integer problem to solve. Andrews et al. [4] proposed an optimization heuristic to solve this integer problem. The main idea behind the heuristic is that for given integer variables i.e. fixed labels, it can be solved exactly through usual quadratic programming. The solution thus is a two step iterative process:\n\u2022 Step 1: Given the integer variables (fixed labels), solve the standard SVM.\n\u2022 Step 2: Given the SVM solution, impute the integer label variables for the positive bags.\nThe instances in positive bags are initialized as positive and are updated as described above. The instances in negative bags are obviously labeled negative and remain so through out the procedure. The two steps are iterated until no changes in labels occur. The overall mi-SVM is shown in Algorithm 1. If it happens during an iteration that all instances in a positive bag are labeled as negative, then the one with the maximum value for discriminant function is assigned a positive label. This process is repeated until no change in imputed labels is observed."}, {"heading": "4.3 MIL of Neural Networks (BP-MIL)", "text": "Neural networks have become increasingly popular for classification tasks [28] [20]. The conventional approach to training neural networks is to provide instance-specific labels for a collection of training instances. Training is performed by updating network weights to minimize the average divergence between the actual network output in response to these training instances and a desired output, typically some representation of their assigned labels [27][35].\nIn the MIL setting, where only bag-level labels are provided for the training data, this procedure must be appropriately modified. In order to do so, we must modify the manner in which the divergence to be minimized is computed, to utilize only bag-level labels. For this, we employ an adaptation of neural networks for multiple instance learning (BP-MIL) proposed in [39].\nLet oij represent the output of the network in response to input xij , the j th instance in Bi, the i th bag of training\ninstances. We define the bag-level divergence for bag Bi as\nEi = 1\n2\n(\nmax 1\u2264j\u2264ni (oij)\u2212 di\n)2\n(8)\nwhere di, the desired output of the network in response to the set of instances from Bi, is simply set to Yi, the label assigned to Bi. Thus for positive bags di = 1, whereas for negative bags di = 0. The central idea behind the baglevel divergence of Equation 8 is to refer to any bag using the instance which produces the maximal output. This was proposed by Dooly et al. in [11] where they showed that irrespective of the number of instances (positive or negative) in a bag, the bag can be fully described by the instance with maximal output.\nThe bag-level divergence of Equation 8 may be understood by noting that the term\u201cmaxj oij\u201d in it effectively represents the bag-level output of the network, and that Equation 8 simply computes the divergence of the bag-level output with respect to the bag-level label of Equation 3.\nThe ideal output of the network in response to any negative instance is 0, whereas for a positive instance it is 1. For negative bags, Equation 8 characterizes the worst-case divergence of all instances in the bag from this ideal output. Minimizing this effectively ensures that the response of the network to all instances from the bag is forced towards 0. In the ideal case, the system will output 0 in response to all inputs in the bag, and the divergence Ei will go to 0.\nFor positive bags, on the other hand, Equation 8 computes the best-case divergence of the instances of the bag from the ideal output of 1. Minimizing this ensures that the response of the network to at least one of the instances from the bag is forced towards 1. In the ideal case, one or more of the inputs in the bag will produce an output of 1 and the divergence Ei will go to zero.\nThe overall divergence on the training set is obtained by summing the divergences of all the bags in the set:\nE =\nN \u2211\ni=1\nEi =\nN \u2211\ni=1\n1\n2\n(\nmax 1\u2264j\u2264ni oij \u2212 di\n)2\n(9)\nThe parameters of the network are trained using conventional backpropagation, with the difference that we now compute gradients of the divergence given in Equation 9, and that entire bags of data must be processed prior to updating network parameters. During training once all instances in a bag have been fed forward through the network, the weight update for the bag is done with respect to the instance in the bag for which the output was maximum. The process is continued until the overall divergence falls below a desired tolerance.\nPrediction using a trained network can now be done instancewise as in done in classical feed-forward neural networks. Bag labels, if required, can be predicted based on the label obtained for the maximal-scoring instance in the bag."}, {"heading": "4.4 MIL for AED using weakly labeled data", "text": "In our setting for training audio-event detectors from weakly labeled data, we only have labels informing us whether a recording contains a given event or not. In order to apply the MIL framework to this scenario, we must first represent the weakly labeled recordings in R in terms of bag-label representations.\nTo convert recording Ri to a bag, it is segmented into a number of short audio segments. The segmentation may or\nmay not be overlap. Let the segments derived from Ri be [IRi1 IRi2 ...IRiK ]. Each of these smaller segments is now treated as an individual instance within the bag Ri. Formally a bag Ri is thus Ri = {IRi1, IRi2, ....IRiK} where K depends upon the duration of recording Ri, the length of the individual segments and the overlap between adjacent segments.\nIf the weak labels for Ri marks and event as being present in Ri, then it will be present in at least one of the instances within it. Hence, Ri will be a positive bag for the instance. On the other hand, if an event is marked as not being present in a recording, then clearly none of the segments(instances) from the recording will be positive for that event, and hence overall that recording is a negative bag for the event. Hence, the weak labels for all the recordings can directly provide bag-label representations needed in MIL. The MIL approaches can now be used to learn a model which can predict the presence or absence of an event in each recording. In fact, it can predict detection of event in each instance which results can be help in transcribing a recording."}, {"heading": "4.5 Temporal Localization of Events", "text": "The MIL frameworks we use in this work learn from baglevel labels, but once learned, can detect presence of events in individual instances. In the context of audio analysis, this implies that not only can we detect predict presence of an event in a test recording (bag) but also in individual segments of the test recording. Formally, if Rx is a test recording, which we may more explicitly represent as Rx(t) where \u201ct\u201d indexes time, the individual segments in the recording, IRx1, IRx2, ....IRxK are given by, IRxk = Rx(t), (k\u2212 1)l \u2032 \u2264 t < (k\u2212 1)l \u2032 + l, where l is the length of the segment in seconds and l \u2032\ndenotes the amount by which segment window is shifted. In the special case of non-overlapping segments l \u2032\n= l. If an event Ei is detected to be present in segment IRxk it means this event can be localized to the time segment (\n(k \u2212 1)l \u2032 , (k \u2212 1) \u2217 l \u2032 + l ) in the recording Rx. Hence, the\nframework can generate information about temporal location of events. Hence, we are able to obtain a complete description of the recording in terms of audio events. This is a unique form of AED learning since the descriptive form of labeling was never present in the training data in the first place."}, {"heading": "4.6 GMM based features for audio segments", "text": "Before we can apply the MIL framework, each segment of audio must first be converted to an appropriate feature representation. First, we use Mel-frequency cepstral coefficient (MFCC) vectors to obtain low-level feature representations of audio segments. However direct characterization of audio as sequences of MFCC vectors tends to be ineffective for the purposes of audio classification [40]; other secondary representations derived from these are required. As described in Section 2, one of simple and yet very successful approach is the Bag of Audio Words feature representation, which quantizes the individual MFCC vectors into a set of codewords, and represents segments of audio as histograms over these codewords.\nHowever, while the bag of words representation has been found to be very effective for classification of long segments of audio, for shorter duration segments such as those in our\ncase, they present several problems. Bag of words representations effectively characterize the distribution of the MFCC vectors in the segment. However, because of the inherent quantization they lose much of the detail of this distribution, which is required for fine-level analysis, such as in the detection of sound events, particularly when the classifiers must be learned from weak labels. While the loss of resolution may be partially resolved by increasing the size of the codebook used for quantization, this can lead to generation of sparse histograms for short audio segments, with large cross-instance variation in the derived features.\n[19] demonstrated that for short audio segments characterization of audio using Gaussian Mixture Model (GMM) can provide robust representations for performance of short audio segments. They suggested Gaussian Mixture based characterization of audio events is a combination of two features: the first, which we represent as ~F , is similar to bag-ofwords characterizations such as [32], and the second, which we represent as ~M is a characterization of the modes of the distribution of vectors in the segment.\nAs a first step to obtaining the ~F and ~M feature vectors for the audio segments, we train a universal Gaussian mixture model (GMM) on MFCC vectors from a large and diverse collection of audio recordings. This background GMM is used to extract the ~F and ~M features. In the following, we will represent this universal GMM as G = {wk, N(x;\u03bbk)}, where wk is the a priori probability or mixture weight of the kth Gaussian in the mixture, N(.) represents a Gaussian, and \u03bbk collectively represents the set of mean and covariance parameters of the kth Gaussian.\n4.6.1 ~F Features For each audio segment we have a sequence ofD-dimensional\nMFCCs vectors denoted by ~xt where t goes from 1 to T . T is the total number of MFCC vectors for the given segment. For each component k of the background GMM we compute\nPr(k|~xt) = wkN(~xt;\u03bbk) G \u2211\nj=1\nwjN(~xt; \u03bbj)\n, (10)\nF (k) = 1\nT\nT \u2211\ni=1\nPr(k|~xt) (11)\nThe ~F feature vectors are ~F = [F (1), F (2), \u00b7 \u00b7 \u00b7 , F (G)]\u22a4. Thus, ~F is G-dimensional vector representing a normalized soft-count histogram distribution of the MFCC vectors in the recording. It captures how the MFCC vectors are distributed across components of G. It is a variant of bag of audio word features where soft assignment is used in place of hard quantization. It can be used as a standalone features as well.\n4.6.2 ~M Features A more detailed characterization can be obtained by ac-\ntually representing the distribution of the feature vectors in the segment. To do so, we train a separate GMM for each audio segment by adapting the universal GMM G to the collection of MFCC vectors in the segment. The means of the universal GMM are adapted to each training segment using the maximum a posteriori (MAP) criterion as described in\n[6]. This is done as follows for kth component of the mixture\nnk =\nT \u2211\nt=1\nPr(k|~xt), (12)\nEk(~x) = 1\nnk\nT \u2211\nt=1\nPr(k|~xt)~xt (13)\nFinally the updated means are computed as\n~\u0302\u00b5k = nk\nnk + r Ek(~x) +\nr\nnk + r ~\u00b5k (14)\nwhere ~\u00b5k is the mean vector of k th Gaussian and r is a relevance factor. The means of all components are then appended to form theG\u00d7D vector ~M as ~M = [ ~\u0302\u00b5\u22a41 , ~\u0302\u00b5 \u22a4 2 , \u00b7 \u00b7 \u00b7 ~\u0302\u00b5 \u22a4 G]\n\u22a4. The above two features are unsupervised methods of characterizing the statistical structure of an audio segment. ~F is the coarser representation, but can however be robustly estimated. ~M is more detailed, though it is also more easily affected by inter-instance variability. Together, they give us a robust representation of both the coarse and fine structure of the signals in short audio segments. In our experiments we have used ~F both in combination with ~M , and as a standalone feature.\nOnce we have a robust set of features for representing events in short audio segments we can extract features for each audio segment (instances) of a recording (bag). Thus a recording(bag) Ri = {IRi1, IRi2, ....IRiK} in feature feature space becomes Ri = {~xRi1; ~xRi2; ....~xRiK} where ~xRij are either the ~F features alone or the concatenated ~F and ~M vectors. The bags are then fed into the the MIL frameworks BP-MIL or mi-SVM for learning event detector models."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "We evaluated the proposed MIL framework on a portion of the TRECVID-MED 2011 database [1]. The videos in this dataset are meant for multimedia event detection task and belong to broad categories such \u201cChanging a vehicle tire\u201d, \u201cAttempting a board trick\u201d, \u201cfeeding an animal\u201d, etc which are not particularly suitable for audio event detection study. Hence we work with more meaningful acoustic events such as clapping, cheering etc. A subset of MED dataset is thus annotated with 10 such events. To be able to compare performance with fully supervised case and to compute performance metrics in case of temporal localization of events, our annotations include actual locations and duration of occurrences of these events. However, only the information regarding the presence or absence of these sounds is used in our MIL based framework.\nA total 457 recordings (bags) are used in the experiments. This is over 22 hours of audio data. From here on we would refer to this set of recordings as the dataset. The length of each recording in the dataset varies from a few seconds to several minutes with an average length of about 2.9 minutes. This implies that the number of instances in each bag also has a wide range.\nIdeally, the length of each segment should be properly set keeping in mind the expected duration of event. However, we observed that segment length decided by heuristics work well. Since the median length of the chosen sound events was less than 1 second, we will report results for all our results on segment length fixed to 1 seconds. The segments are overlapping with 50% overlap. This results in total number of instances well over 150,000. The names of the 10 events\nand the total number of positive bags for each events are given in Table 1. It is worth noting that some of the recordings in the dataset does not contain any of the 10 events and also a recording might be positive bag for more than 1 event. Hence, sum total of numbers in Table 1 is different from the total number of recordings in the dataset. As is clear from our proposed framework our goal is detection i.e presence or absence of audio events in recordings. Hence, the binary classifiers trained for each event has positive bags equal to the number shown in Table 1 and rest of the recording in the dataset are negative bags for that event. The dataset is partitioned into 4 sets. Three of the sets were used to train the models, which were then tested on the fourth set. This was done in all four ways meaning each set becomes a test set. This gives us results on the whole dataset. Hence, all results reported here are on the whole dataset.\nAll recordings were parameterized into sequences of 21- dimensional Mel Frequency Cepstrum Coefficient (MFCC) vectors. MFCC vectors were computed over analysis frames of 20ms, with an overlap of 50%(10ms) between adjacent frames. The ~F and ~M features were derived from these sequences of MFCC vectors. We trained two background GMMs with 64 and 128 Gaussian components respectively. The number of Gaussian component in the features is represented as subscript in the feature such as ~F64, ~M64.\nROC curves are used to analyze the performance. Area Under ROC curve (AUC) [7] is a well known metric used to characterize ROC curves. AUC is used to compare results in different cases. The higher the AUC the better it is. We first show results for detection of events at recording (bag) level using miSVM and BPMIL in Section 5.1 and 5.2 respectively. The instance level results or temporal localization of events are provided in Section 5.3."}, {"heading": "5.1 miSVM Results", "text": "For miSVM framework linear SVMs are used in all experiments. We use LIBLINEAR [14] in our implementation of miSVM framework. The slack parameter C in the SVM formulation is obtained by 4 fold-cross validation over the training set. A comprehensive analysis through comparison of results in different cases is provided. The mean AUC over all events is shown in the last row of each table.\n5.1.1 Comparison with supervised SVM We start by showing comparison of our proposed frame-\nwork with fully supervised AED where strong labels are available. For supervised learning the time stamps in annotations are used to obtain pure examples of each event and then SVM is trained using feature representations of\nthese examples. Table 2 shows this comparison. Comparison is shown for ~F64 features. supSVM refers to supervised SVM in Table 2. As is expected supervised SVMs performs better than miSVM. However, there are several events for which the performance is more or less maintained using weak labels instead of strong labels. Although, the performance of supervised SVMs can be possibly improved by obtaining more strongly labeled examples for each event , Table 2 illustrates that miSVM can achieve fairly decent performance using weak labels only.\n5.1.2 Number of Gaussian Components Table 3 shows AUC results for ~F64 and ~F128 features. It\ncan be noted that there are several events for which increasing the number of Gaussians leads to about 2 \u2212 4% absolute improvement in AUC values. At the same time there are events such as Cheering and Marching Band where this improvement is not observed or goes down as in Children Voices. A drop of about 4% is observed in this case. Fixing cluster size is known to be event specific and this holds for MIL based audio event detection as well.\n5.1.3 Adding ~M Features We now observe the effect of adding ~M features to the\nsystem along with ~F . Table 4 shows comparison of AUC values when only ~F is used and when it is combined with ~M features. The results shown are for 64 Gaussian component. It can be observed that adding ~M features obtained by maximum-a-posteriori adaptation leads to remarkable improvement of results for almost all events. Events on which using ~F only results in very poor performance such as Hammering, Laughing and Marching Band can gain tremen-\ndously by using ~M features. An absolute improvements of 10.3%, 11.4% and 19.4% respectively is observed for these three events. For other events also absolute improvements in range of 2.1%\u2212 14.5% can be noted. The only exception is Engine Noise for which soft-count ~F seems to be better. Although, we are observing improvements in case of miSVM it can actually be classifier dependent.\n5.1.4 Overall Results The overall AUC results across all experiments using miSVM\nis shown in Table 5. This is the best result across different feature representations for audio segments. Corresponding ROC curves in Figure 1. Events such as clanking, children voices, scraping and marching band are easier to detect compared to other events such as drums. The mean AUC over all events is 0.704 which validates the success of our proposed framework."}, {"heading": "5.2 BP-MIL Results", "text": "For the BP-MIL neural network three parameters must be defined, namely, number of hidden layers, the number of nodes in each hidden layer (nno) and learning rate(\u03b7). We used a network with one hidden layer in all experiments. Network is trained for a total of 60 epochs. The learning rate is either fixed at 0.1 throughout training or 0.1 for first 30 epochs and then reduced in each epoch till it reaches 0.01. Larger values of nno are used for larger dimensionality of input features. For ~F64 and ~F128 features 3 different values of nno are used. These are 16, 50 and 100 for ~F64 and 50, 100 and 150 for ~F128. When both ~F and ~M are used, the values of nno used in experiments are 256 and 512. Training neural networks in general require exhaustive tuning of parameters to get good results. Although results presented here shows reasonable success of BP-MIL framework, we believe that better results can be obtained by more aggressive parameter tuning. In fact parameter tuning can give better insight into the BP-MIL framework.\nWe present only the best results across the different settings. Before looking into these results, we make note of the\nfact that for BP-MIL adding ~M features in general leads to poorer performance. Some improvement is observed only for marching band and drums. One possible reason for this might be substantial increase in total number of weight parameters with addition of ~M features. As the network size increases by a considerable amount due to increase in input feature size, a substantial increase in training data size is needed to get good performance. Our data size remains same and hence it might be one of the reasons for poor performance for BP-MIL on addition of M features. Hence, it should be noted that with larger dataset M features can be very useful. ~F as standalone features works well for BPMIL. The overall results for BP-MIL across all experiments is shown in Table 6. The corresponding ROC curves are shown in Figure 2. Events such as Scraping, Clanking and Children Voices are easier to detect in this case as well. Although, the mean AUC remains almost same, one can note that a significant difference with respect to miSVM for several cases."}, {"heading": "5.3 Temporal Localization of Events", "text": "We now show performance of the MIL based framework on temporal localization of the audio events. To evaluate the performance on this task we need the ground truth labels of all instances in all bags. The instances in the bags have been obtained through uniform segmentation in our work. Each instance is a one second window segment of the recording which is moved in an overlapping manner to segment the recording. However, the annotations providing time stamps of events in the recording does not adhere to this uniform segmentation. Thus an event might start and end within a segment and it can also start or end at any point in the segment. Hence, assigning ground truth labels of instances for a valid analysis is not straight forward. We use a simple heuristic to obtain ground truth labels. As described in Section 4.5 each segment represents a specific time duration of the recording. Looking into the actual annotations available, if an event can be marked to be present in atleast 50% of the total length of the segment we call ground truth label of that segment as positive otherwise negative.\nOnce the ground truth labels with respect to an event have been obtained for all instances over all bags, we can analyze the performance in usual fashion. We again present ROC curves and use AUC as metric characterizing these curves. The best AUC values across all experiments for temporal localization using both miSVM and BP-MIL is shown in Table 7. Corresponding ROC curves are shown in Figure 3. The figures in upper row are for miSVM and the figures in bottom row are for BP-MIL. Compared to bag-level results about 5% drop in mean AUC is observed for both cases. For some events such as Hammering and Laughing the performance is poor for both frameworks. For others reasonable performance is obtained. Although, these numbers are not exceptionally higher they are still significant since no temporal information was used during the training stage. Overall, AUC results validate that our proposed framework can work for temporal localization as well."}, {"heading": "6. DISCUSSIONS AND CONCLUSION", "text": "A framework for learning acoustic event detectors from weakly labeled data has been presented in this paper. The learned detectors can both detect and temporally localize events in a recording. We show that we achieve reasonable performance for both tasks. Specifically, events such as Clanking, Scraping, Children Voices are easy to detect using both SVM and neural network approaches. On the other hand events such as Drums, Hammering and Laughing are hard to detect using both methods. Although, specific differentiation can be seen here, a large scale implementation of both methods on a larger number of events might be able to give a better insights for both methods. The overall performance of both methods are almost same. Given the limited amount of training data, a mean AUC of around 0.7 demonstrates the success of the proposed approach. Larger\ndatasets is expected to give better performance for both cases.\nAn important factor in this framework is the representation used to characterize the audio segments, i.e. the instances in the bags. We employed Gaussian mixture based features (~F and ~M). We do not necessarily claim that these are the best features for audio event classification; nonetheless they have been shown effective for AED in short audio segments [19]. Other features may be expected to result in improved overall MIL performance.\nThe success of our method is an important step towards reducing the dependence on strongly labeled data for learning audio event detectors. It shows the pathway to utilize the vast amount of multimedia data available on the web for audio event detection. The weak labels for web multimedia (audio) data can be inferred automatically from associated metadata which can then be directly used in the proposed framework. An interesting and extremely useful product of our proposed framework is the ability to temporally locate events in the recording. This is significant since this information was not present in the original data in the first place. Moreover, the predicted instance level labels can be further used in an active learning framework to improve performance.\nA number of factors still needs to be investigated to create a state-of-art acoustic event detection mechanism in which learning is done using weakly labeled data. We need to consider a large set of events for a more comprehensive view of our framework. Moreover, investigations are required into more effective multiple instance learning methods which can help improve the overall performance and scalability of framework. Multimedia event detection requires detection of higher concepts. In the current work we focused on detecting finer events and their detection can be used for detecting higher concepts. Higher-level concepts in turn may be used to guide MIL. All of these are directions of currently ongoing and future work."}, {"heading": "7. REFERENCES", "text": "[1] Multimedia event detection.\nhttp://www.nist.gov/itl/iad/mig/med11.cfm.\n[2] Youtube statistics. http://www.youtube.com/yt/press/statistics.html.\n[3] E. Amid, A. Mesaros, K. J. Palomaki, J. Laaksonen, and M. Kurimo. Unsupervised feature extraction for multimedia event detection and ranking using audio content. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 5939\u20135943. IEEE, 2014.\n[4] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance learning. Advances in neural information processing systems, 15:561\u2013568, 2002.\n[5] K. Ashraf, B. Elizalde, F. Iandola, M. Moskewicz, J. Bernd, G. Friedland, and K. Keutzer. Audio-based multimedia event detection with DNNs and sparse sampling. In Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pages 611\u2013614. ACM, 2015.\n[6] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier, I. Magrin-Chagnolleau, S. Meignier, T. Merlin, J. Ortega-Garc\u0301\u0131a, D. Petrovska-Delacre\u0301taz, and D. A. Reynolds. A tutorial on text-independent speaker verification. EURASIP journal on applied signal processing, 2004:430\u2013451, 2004.\n[7] A. P. Bradley. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition, 30(7):1145\u20131159, 1997.\n[8] F. Briggs, B. Lakshminarayanan, L. Neal, X. Z. Fern, R. Raich, S. J. Hadley, A. S. Hadley, and M. G. Betts. Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach. The Journal of the Acoustical Society of America, 131(6):4640\u20134650, 2012.\n[9] M. Cristani, M. Bicego, and V. Murino. Audio-visual event recognition in surveillance video sequences. Multimedia, IEEE Transactions on, 9(2):257\u2013267, 2007.\n[10] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pe\u0301rez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89(1):31\u201371, 1997.\n[11] D. R. Dooly, Q. Zhang, S. A. Goldman, and R. A. Amar. Multiple instance learning of real valued data. The Journal of Machine Learning Research, 3:651\u2013678, 2003.\n[12] L. Duan, D. Xu, I. W.-H. Tsang, and J. Luo. Visual event recognition in videos by learning from web data. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(9):1667\u20131680, 2012.\n[13] A. J. Eronen, V. T. Peltonen, J. T. Tuomi, A. P. Klapuri, S. Fagerlund, T. Sorsa, G. Lorho, and J. Huopaniemi. Audio-based context recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 14(1):321\u2013329, 2006.\n[14] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871\u20131874, 2008.\n[15] O. Gencoglu, T. Virtanen, and H. Huttunen.\nRecognition of acoustic events using deep neural networks. In Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European, pages 506\u2013510. IEEE, 2014.\n[16] Q. Huang and S. Cox. Hierarchical language modeling for audio events detection in a sports game. In Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pages 2286\u20132289. IEEE, 2010.\n[17] Z. Kons and O. Toledo-Ronen. Audio event classification using deep neural networks. In INTERSPEECH, pages 1482\u20131486, 2013.\n[18] A. Kumar, P. Dighe, R. Singh, S. Chaudhuri, and B. Raj. Audio event detection from acoustic unit occurrence patterns. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 489\u2013492. IEEE, 2012.\n[19] A. Kumar, R. M. Hegde, R. Singh, and B. Raj. Event detection in short duration audio using gaussian mixture model and random forest classifier. In 21st European Signal Processing Conference 2013 (EUSIPCO 2013), Marrakech, Morocco, Sept. 2013.\n[20] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.\n[21] X. Lu, Y. Tsao, S. Matsuda, and C. Hori. Sparse representation based on a bag of spectral exemplars for acoustic event detection. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6255\u20136259. IEEE, 2014.\n[22] M. I. Mandel and D. P. Ellis. Multiple-instance learning for music information retrieval. In ISMIR 2008: Proceedings of the 9th International Conference of Music Information Retrieval, pages 577\u2013582. Drexel University, 2008.\n[23] O. Maron and T. Lozano-Pe\u0301rez. A framework for multiple-instance learning. Advances in neural information processing systems, pages 570\u2013576, 1998.\n[24] S. Pancoast and M. Akbacak. Bag-of-audio-words approach for multimedia event classification. In Proc. of Interspeech, 2012.\n[25] A. Pikrakis, T. Giannakopoulos, and S. Theodoridis. Gunshot detection in audio streams from movies by means of dynamic programming and bayesian networks. In Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, pages 21\u201324. IEEE, 2008.\n[26] J. F. Ruiz-Mun\u0303oz, M. Orozco-Alzate, and G. Castellanos-Dominguez. Multiple instance learning-based birdsong classification using unsupervised recording segmentation. In Proceedings of the 24th International Conference on Artificial Intelligence, pages 2632\u20132638. AAAI Press, 2015.\n[27] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Cognitive modeling, 5:3, 1988.\n[28] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85\u2013117, 2015.\n[29] Y. Song and C. Zhang. Content-based information fusion for semi-supervised music genre classification. Multimedia, IEEE Transactions on, 10(1):145\u2013152, 2008.\n[30] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange,\nand M. Plumbley. Detection and classification of acoustic scenes and events. Multimedia, IEEE Transactions on, PP, 2015.\n[31] G. Valenzise, L. Gerosa, M. Tagliasacchi, F. Antonacci, and A. Sarti. Scream and gunshot detection and localization for audio-surveillance systems. In Advanced Video and Signal Based Surveillance, 2007. AVSS 2007. IEEE Conference on, pages 21\u201326. IEEE, 2007.\n[32] J. C. Van Gemert, C. J. Veenman, A. W. Smeulders, and J.-M. Geusebroek. Visual word ambiguity. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(7):1271\u20131283, 2010.\n[33] F. Wang, Z. Sun, Y.-G. Jiang, and C.-W. Ngo. Video event detection using motion relativity and feature selection. Multimedia, IEEE Transactions on, 16(5):1303\u20131315, 2014.\n[34] J. Wang and J.-D. Zucker. Solving the multiple-instance problem: A lazy learning approach. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML \u201900, pages 1119\u20131126, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.\n[35] P. J. Werbos. Applications of advances in nonlinear sensitivity analysis. In System modeling and optimization, pages 762\u2013770. Springer, 1982.\n[36] G. Ye, I. Jhuo, D. Liu, Y.-G. Jiang, D. Lee, S.-F. Chang, et al. Joint audio-visual bi-modal codewords for video event detection. In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval, page 39. ACM, 2012.\n[37] T. Zhang, C. Xu, G. Zhu, S. Liu, and H. Lu. A generic framework for event detection in various video domains. In Proceedings of the international conference on Multimedia, pages 103\u2013112. ACM, 2010.\n[38] T. Zhang, C. Xu, G. Zhu, S. Liu, and H. Lu. A generic framework for video annotation via semi-supervised learning. Multimedia, IEEE Transactions on, 14(4):1206\u20131219, 2012.\n[39] Z.-H. Zhou and M.-L. Zhang. Neural networks for multi-instance learning. In Proceedings of the International Conference on Intelligent Information Technology, Beijing, China, pages 455\u2013459, 2002.\n[40] X. Zhuang, X. Zhou, M. A. Hasegawa-Johnson, and T. S. Huang. Real-world acoustic event detection. Pattern Recognition Letters, 31(12):1543\u20131551, 2010."}], "references": [{"title": "Unsupervised feature extraction for multimedia event detection and ranking using audio content", "author": ["E. Amid", "A. Mesaros", "K.J. Palomaki", "J. Laaksonen", "M. Kurimo"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 5939\u20135943. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in neural information processing systems, 15:561\u2013568,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Audio-based multimedia event detection with DNNs and sparse sampling", "author": ["K. Ashraf", "B. Elizalde", "F. Iandola", "M. Moskewicz", "J. Bernd", "G. Friedland", "K. Keutzer"], "venue": "Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pages 611\u2013614. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A tutorial on text-independent speaker verification", "author": ["F. Bimbot", "J.-F. Bonastre", "C. Fredouille", "G. Gravier", "I. Magrin-Chagnolleau", "S. Meignier", "T. Merlin", "J. Ortega-Gar\u0107\u0131a", "D. Petrovska-Delacr\u00e9taz", "D.A. Reynolds"], "venue": "EURASIP journal on applied signal processing, 2004:430\u2013451,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "The use of the area under the roc curve in the evaluation of machine learning algorithms", "author": ["A.P. Bradley"], "venue": "Pattern recognition, 30(7):1145\u20131159,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach", "author": ["F. Briggs", "B. Lakshminarayanan", "L. Neal", "X.Z. Fern", "R. Raich", "S.J. Hadley", "A.S. Hadley", "M.G. Betts"], "venue": "The Journal of the Acoustical Society of America, 131(6):4640\u20134650,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio-visual event recognition in surveillance video sequences", "author": ["M. Cristani", "M. Bicego", "V. Murino"], "venue": "Multimedia, IEEE Transactions on, 9(2):257\u2013267,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence, 89(1):31\u201371,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple instance learning of real valued data", "author": ["D.R. Dooly", "Q. Zhang", "S.A. Goldman", "R.A. Amar"], "venue": "The Journal of Machine Learning Research, 3:651\u2013678,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W.-H. Tsang", "J. Luo"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(9):1667\u20131680,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio-based context recognition", "author": ["A.J. Eronen", "V.T. Peltonen", "J.T. Tuomi", "A.P. Klapuri", "S. Fagerlund", "T. Sorsa", "G. Lorho", "J. Huopaniemi"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 14(1):321\u2013329,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["O. Gencoglu", "T. Virtanen", "H. Huttunen"], "venue": "Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European, pages 506\u2013510. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical language modeling for audio events detection in a sports game", "author": ["Q. Huang", "S. Cox"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pages 2286\u20132289. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Audio event classification using deep neural networks", "author": ["Z. Kons", "O. Toledo-Ronen"], "venue": "INTERSPEECH, pages 1482\u20131486,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio event detection from acoustic unit occurrence patterns", "author": ["A. Kumar", "P. Dighe", "R. Singh", "S. Chaudhuri", "B. Raj"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 489\u2013492. IEEE,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Event detection in short duration audio using gaussian mixture model and random forest classifier", "author": ["A. Kumar", "R.M. Hegde", "R. Singh", "B. Raj"], "venue": "21st European Signal Processing Conference 2013 (EUSIPCO 2013), Marrakech, Morocco, Sept.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse representation based on a bag of spectral exemplars for acoustic event detection", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 6255\u20136259. IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple-instance learning for music information retrieval", "author": ["M.I. Mandel", "D.P. Ellis"], "venue": "ISMIR 2008: Proceedings of the 9th International Conference of Music Information Retrieval, pages 577\u2013582. Drexel University,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-P\u00e9rez"], "venue": "Advances in neural information processing systems, pages 570\u2013576,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Bag-of-audio-words approach for multimedia event classification", "author": ["S. Pancoast", "M. Akbacak"], "venue": "Proc. of Interspeech,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Gunshot detection in audio streams from movies by means of dynamic programming and bayesian networks", "author": ["A. Pikrakis", "T. Giannakopoulos", "S. Theodoridis"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, pages 21\u201324. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple instance learning-based birdsong classification using unsupervised recording segmentation", "author": ["J.F. Ruiz-Mu\u00f1oz", "M. Orozco-Alzate", "G. Castellanos-Dominguez"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, pages 2632\u20132638. AAAI Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling, 5:3,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Content-based information fusion for semi-supervised music genre classification", "author": ["Y. Song", "C. Zhang"], "venue": "Multimedia, IEEE Transactions on, 10(1):145\u2013152,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Detection and classification of acoustic scenes and events. Multimedia", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M. Plumbley"], "venue": "IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Scream and gunshot detection and localization for audio-surveillance systems", "author": ["G. Valenzise", "L. Gerosa", "M. Tagliasacchi", "F. Antonacci", "A. Sarti"], "venue": "Advanced Video and Signal Based Surveillance, 2007. AVSS 2007. IEEE Conference on, pages 21\u201326. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Visual word ambiguity", "author": ["J.C. Van Gemert", "C.J. Veenman", "A.W. Smeulders", "J.-M. Geusebroek"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(7):1271\u20131283,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Video event detection using motion relativity and feature selection", "author": ["F. Wang", "Z. Sun", "Y.-G. Jiang", "C.-W. Ngo"], "venue": "Multimedia, IEEE Transactions on, 16(5):1303\u20131315,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning, ICML \u201900, pages 1119\u20131126, San Francisco, CA, USA,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Applications of advances in nonlinear sensitivity analysis", "author": ["P.J. Werbos"], "venue": "System modeling and optimization, pages 762\u2013770. Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1982}, {"title": "Joint audio-visual bi-modal codewords for video event detection", "author": ["G. Ye", "I. Jhuo", "D. Liu", "Y.-G. Jiang", "D. Lee", "S.-F. Chang"], "venue": "In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "A generic framework for event detection in various video domains", "author": ["T. Zhang", "C. Xu", "G. Zhu", "S. Liu", "H. Lu"], "venue": "Proceedings of the international conference on Multimedia, pages 103\u2013112. ACM,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "A generic framework for video annotation via semi-supervised learning", "author": ["T. Zhang", "C. Xu", "G. Zhu", "S. Liu", "H. Lu"], "venue": "Multimedia, IEEE Transactions on, 14(4):1206\u20131219,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural networks for multi-instance learning", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Proceedings of the International Conference on Intelligent Information Technology, Beijing, China, pages 455\u2013459,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Real-world acoustic event detection", "author": ["X. Zhuang", "X. Zhou", "M.A. Hasegawa-Johnson", "T.S. Huang"], "venue": "Pattern Recognition Letters, 31(12):1543\u20131551,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 28, "context": "Automatic sound event detection also finds application in other scenarios, such as monitoring traffic for sounds of accidents or impact, surveillance, where one may \u201clisten\u201d for sounds of gunshots [31], screams [25] etc.", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "Automatic sound event detection also finds application in other scenarios, such as monitoring traffic for sounds of accidents or impact, surveillance, where one may \u201clisten\u201d for sounds of gunshots [31], screams [25] etc.", "startOffset": 211, "endOffset": 215}, {"referenceID": 5, "context": "It is also useful in cases such as wildlife monitoring [8], context recognition [13] and several health and life style monitoring system.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "It is also useful in cases such as wildlife monitoring [8], context recognition [13] and several health and life style monitoring system.", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "A nice survey on the audio event detection challenges, works and state of art can be found in [30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Possibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9].", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "Possibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Possibly the most popular application so far of soundevent detection has been for surveillance [31] [25] [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "Audioevent detection has also found its way into consumer devices, particularly for automatic indexing of multimedia recordings of games [16].", "startOffset": 137, "endOffset": 141}, {"referenceID": 37, "context": "The authors of [40] model various sound events with Gaussian-mixture models in order to detect them in real-world recordings.", "startOffset": 15, "endOffset": 19}, {"referenceID": 37, "context": "The approach followed in [40] is similar to GMM-HMM architecture used for automatic speech recognition.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "This approach too has been successfully applied to the problems of detecting events in audio [24] as well as for multimodal approaches to event detection [36] and [33].", "startOffset": 163, "endOffset": 167}, {"referenceID": 21, "context": "It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few.", "startOffset": 167, "endOffset": 171}, {"referenceID": 0, "context": "It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few.", "startOffset": 200, "endOffset": 203}, {"referenceID": 18, "context": "It is a general framework for obtaining a fixed length representations for audio clips and can be done on a variety of low level audio features features such as MFCCs [24], autoencoder based features [3] and normalized spectral features [21] to name a few.", "startOffset": 237, "endOffset": 241}, {"referenceID": 15, "context": "[18] uses an alternate approach to obtaining bags of words \u2013 sound recordings are first decomposed into sequence of basic sound units called \u201cAcoustic Unit Descriptors\u201d (AUDS), which are themselves learned in an unsupervised manner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The actual classification may be performed through classifiers such as SVM or random forests [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Deep neural network based approaches have also been proposed for audio event detection [15][17] [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 35, "context": "Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 34, "context": "Even here, the majority of published work focuses on detecting events based on visual content of the data [12] [38] [37].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "Some audio related works include semi-supervised music genre classification are [22] [29].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "Some audio related works include semi-supervised music genre classification are [22] [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "Two other audio related works are [26] and [8] where authors try to exploit weak labels in bird song and bird species classification.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Two other audio related works are [26] and [8] where authors try to exploit weak labels in bird song and bird species classification.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Several well known learning methods such as Support Vector Machines (SVMs)[4], K-NNs [34] have been modified to learn in this paradigm.", "startOffset": 74, "endOffset": 77}, {"referenceID": 31, "context": "Several well known learning methods such as Support Vector Machines (SVMs)[4], K-NNs [34] have been modified to learn in this paradigm.", "startOffset": 85, "endOffset": 89}, {"referenceID": 7, "context": "Our formulation of event detection using weak labels of the kind described above is based onMultiple-Instance Learning (MIL) [10] which is a generalized version of supervised learning in labels are available for a collection of instances.", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "in 1997 for drug activity detection [10].", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 121, "endOffset": 125}, {"referenceID": 31, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "Several methods have been proposed to solve the MIL problem such as Learning Axis-Parallel Concepts [10], Diverse Density[23], Citation-KNN[34], mi-SVM and MI-SVM [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "In [4] Andrews et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "[4] proposed an optimization heuristic to solve this integer problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Neural networks have become increasingly popular for classification tasks [28] [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Neural networks have become increasingly popular for classification tasks [28] [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Training is performed by updating network weights to minimize the average divergence between the actual network output in response to these training instances and a desired output, typically some representation of their assigned labels [27][35].", "startOffset": 236, "endOffset": 240}, {"referenceID": 32, "context": "Training is performed by updating network weights to minimize the average divergence between the actual network output in response to these training instances and a desired output, typically some representation of their assigned labels [27][35].", "startOffset": 240, "endOffset": 244}, {"referenceID": 36, "context": "For this, we employ an adaptation of neural networks for multiple instance learning (BP-MIL) proposed in [39].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "in [11] where they showed that irrespective of the number of instances (positive or negative) in a bag, the bag can be fully described by the instance with maximal output.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "However direct characterization of audio as sequences of MFCC vectors tends to be ineffective for the purposes of audio classification [40]; other secondary representations derived from these are required.", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "[19] demonstrated that for short audio segments characterization of audio using Gaussian Mixture Model (GMM) can provide robust representations for performance of short audio segments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "They suggested Gaussian Mixture based characterization of audio events is a combination of two features: the first, which we represent as ~ F , is similar to bag-ofwords characterizations such as [32], and the second, which we represent as ~ M is a characterization of the modes of the distribution of vectors in the segment.", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Area Under ROC curve (AUC) [7] is a well known metric used to characterize ROC curves.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "We use LIBLINEAR [14] in our implementation of miSVM framework.", "startOffset": 17, "endOffset": 21}], "year": 2017, "abstractText": "Acoustic event detection is essential for content analysis and description of multimedia recordings. The majority of current literature on the topic learns the detectors through fully-supervised techniques employing strongly labeled data. However, the labels available for online multimedia data are generally weak and do not provide sufficient detail for such methods to be employed. In this paper we propose a framework for learning acoustic event detectors using only weakly labeled data based on a Multiple Instance Learning (MIL) framework. We first show that audio event detection using weak data can be formulated as an MIL problem. We then suggest two frameworks for solving multiple-instance learning, one based on neural networks, and the second on support vector machines. The proposed methods can help in removing the time consuming and expensive process of manually annotating data to facilitate fully supervised learning. Our proposed framework can not only successfully detect events in a recording but can also provide temporal locations of events in the recording. This is interesting as these information were never known in the first place for weakly labeled data.", "creator": "LaTeX with hyperref package"}}}