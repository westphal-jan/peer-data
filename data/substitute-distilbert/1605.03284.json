{"id": "1605.03284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "Machine Comprehension Based on Learning to Rank", "abstract": "machine comprehension plays an essential role in nlp and naturally been widely defined with dataset of mctest. since, no dataset is too simple and too small for learning true reasoning abilities. \\ cite { hermann2015teaching } therefore release a large scale news article dataset and propose a deep lstm reader system for artificial comprehension. however, the training process is expensive. we therefore try cognitive - engineered approach with semantics influencing the new algorithms to see how traditional machine learning technique and semantics can help with machine comprehension. simultaneously, our proposed l2r reader system achieves good performance with more and less ambiguous data.", "histories": [["v1", "Wed, 11 May 2016 05:05:05 GMT  (947kb,D)", "http://arxiv.org/abs/1605.03284v1", "9 pages"], ["v2", "Fri, 13 May 2016 01:06:09 GMT  (947kb,D)", "http://arxiv.org/abs/1605.03284v2", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tian tian", "yuezhang li"], "accepted": false, "id": "1605.03284"}, "pdf": {"name": "1605.03284.pdf", "metadata": {"source": "CRF", "title": "Machine Comprehension Based on Learning to Rank", "authors": ["Tian Tian", "Yuezhang Li"], "emails": ["yuezhanl@andrew.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Machine comprehension, as the central goal in NLP, has been explored with a variety of methods. Based on the released dataset MCTest (Richardson et al., 2013), a lexical matching based method is proposed (Smith et al., 2015). It applies linguistic features to tackle this problem. However, this method dives deep into the characteristics of the dataset, and may not generalize well to other datasets. This problem also goes with the discourse relation model introduced in (Narasimhan and Barzilay, 2015) that explore causal, temporal and explanation relationships between two sentences, which does not scale well. Hence, (Wang and McAllester, 2015) introduced a max-margin learning frame that incorporates a feature set of frame semantics, syntax, coreference, and word embeddings. The accuracy\nof 69.94% was achieved in MC500 of MCTest dataset. Meanwhile, (Sachan et al., 2015) applied similar loss function, modeling machine comprehension as textual entailment and solved the problem by constructing latent answer-entailing structure with an accuracy of 67.83%. Although such good accuracy were achieved, the dataset they worked on has some limitations in terms of data size and content. Therefore, our work is based on a much larger news article dataset created by (Hermann et al., 2015).\nDifferent from the MCTest dataset, this news article dataset consists of cloze style questions, i.e. questions generally generated by removing a phrase from a sentence (Taylor, 1953). Since the questions can be formed from a short summary of the document with condensed form of paraphrase, the dataset is suitable for testing machine comprehension (Hermann et al., 2015).\nTo realize machine comprehension, we develop a learning to rank reader (L2R Reader) system by first exploring features on frequency, word distance, syntax and semantics. Then, through learning to rank, we construct a ranking model that can directly pick the answer from the candidate list of answers. Opposed to the deep LSTM (Hermann et al., 2015) that computes the answer based on context information of documents, it is more efficient and does not require much data to reach good performance. Moreover, we incorporate the semantics into the system to improve comprehension ability.\nThis article is organized as follows. Section 2 introduces the task and essential parts of relevant datasets. Section 3 presents related work and distin-\nar X\niv :1\n60 5.\n03 28\n4v 1\n[ cs\n.C L\n] 1\n1 M\nay 2\nguishes our work from them. Section 4 describes our model from aspects of learning to rank algorithms, features and usage of semantic information. Section 5 then evaluates our model from performance, semantics analysis and error analysis. Finally, section 6 summarizes our work and points out contributions."}, {"heading": "2 Task and Datasets", "text": "This section gives a brief introduction of the task and the dataset recently released for this task."}, {"heading": "2.1 Formal Task Description", "text": "This task requires answering a cloze style question based on the understanding of a context document provided with the question. Along with each question and document, it also provides the correct answer to the question and a list of candidate answers. Thus, this can be formalized as follows:\nThe training data consists of tuples (d, q, a, A), where d is a context document for answering the question q, a is the correct answer to question q, A denotes a set of candidate answers to the question and a \u2208 A as defined."}, {"heading": "2.2 Datasets", "text": "The dataset (Hermann et al., 2015) we used in this task were constructed from news article from CNN and Daily Mail websites. The context document of the dataset is from the main body of the news article while the question is formed from one top sentence summarized the news article. Specifically, the question is constructed by replacing the named entity with a placeholder, e.g. \u201c@placeholder and @entity2 welcome son @entity6\u201d is a question defined in the dataset.\nFurthermore, to make context information required for answering question, we use the anonymised version as shown in Figure 2 to eliminate influence of background knowledge. Thus, we must exploit the context to answer the question and these two corpora truly measures the capacity of reading comprehension.\nThe basic statistics of CNN and Daily Mail of dataset are summarized in Table 1."}, {"heading": "3 Related Work", "text": "Machine comprehension generally concentrates on MCTest (Richardson et al., 2013) and due to the limitation of data size, the state of the arts are mainly based on traditional machine learning techniques. For example, (Wang and McAllester, 2015) proposed a max-margin learning framework that combines features on syntax, coreference, frame semantics and word embeddings, which achieves significant improvement on the problem of MCTest question answering. Although recently (Trischler et al., 2016) proposed a parallel-hierarchical model based\non neural network and this method outperforms the previous feature-engineered approaches, it has reasoning limitation that the reasoning can only be achieved by stringing important sentences together. Their experiment proves that MCTest is too simple to learn true reasoning and it is also too small for that goal.\nConsidering the limitations of MCTest dataset, (Hermann et al., 2015) provides a large scale supervised reading comprehension dataset collected from the CNN and Daily Mail websites. This helps with the bottleneck that large dataset is missing on machine comprehension evaluation. With this dataset, (Hermann et al., 2015) propose a deep LSTM reader that achieves an accuracy of 63.8% on CNN and 69.0% on Daily Mail. However, this deep LSTM reader is time consuming for training and no explanation can be found on why it works opposed to traditional approach. Therefore, we propose a traditional machine learning method on this new dataset to investigate what features can help with this task."}, {"heading": "4 Model \u2013 Learning to Rank (L2R) Reader", "text": "Our model consists of learning to rank algorithms, features and semantics. As shown in Figure 1, given a document along with a list of queries that have the placeholder filled with different entities from the list of candidate answers. Then, through a feature extractor, we get several features for each entity (i.e. candidate answer). Combining with correct answer, we employ a learning to rank algorithm (i.e. L2R in Figure 1) to train a ranking model. Based on the ranking model, we generate ranking lists on new unseen dataset that has features extracted using feature extractor; from the rank list, we select the entity with highest ranking score as the predicted answer to the question."}, {"heading": "4.1 Learning to Rank", "text": "Learning to rank is employed in Information Retrieval (IR) and Natural Language Processing (NLP). Generally, it is defined as follows: given a query, the ranking algorithm will generate a list of candidate documents with scores (Hang, 2011). In this task, we select the entity with the highest score as the answer to the question. Here, we introduce three different types of learning to rank algorithms\nthat can help with the task.\nPointwise: In the pointwise approach, the ranking algorithm is transformed into problems including classification and regression to derive a score for every pair of document and query (Hang, 2011). This approach ignores the group structure of ranking and we do not apply it in this task.\nPairwise: In the pairewise approach, the ranking algorithm is transformed into problems of pairewise classification or pairwise regression (Hang, 2011). It also ignores the group structure of ranking. In this project, we mainly focus on the pairewise classification that employs a binary classifier in document pair ranking. We try approaches including RankNet (Burges et al., 2005), RankBoost (Freund et al., 2003), RankSVM (Herbrich et al., 1999), MART and LambdaMART (Burges, 2010)\nListwise: In the listwise approach, the ranking problem is addressed by taking the ranking list as instances in both learning and prediction process (Hang, 2011). It maintains the group structure and we employ the following listwise approaches: ListNet (Cao et al., 2007), AdaRank (Xu and Li, 2007), Coordinate Ascent."}, {"heading": "4.2 Features", "text": "We explore four types of features in this task. We start with the frequency of entity in both document and cloze-style question. Then we try features on word distance with different settings of window size. We further investigate features on syntactics and semantics to see how these affect the model performance."}, {"heading": "4.2.1 Frequency", "text": "Frequency is explored based on one baseline of (Hermann et al., 2015). We simply count the number of entity that is in the candidate answer list appearing in the document and the question, then the count number works as the frequency feature of the entity. If this entity does not show in question or document, we assign a value of 0. The idea behind is that news article usually mentions important entities multiple times and cloze-style question is concerned with such entities."}, {"heading": "4.2.2 Word Distance", "text": "We investigate word distance from three aspects \u2013 word alignment, nBOW, and word mover\u2019s distance (WMD) (Kusner et al., 2015).\nWord Alignment (WA): For the word alignment, we first consider the situation shown in Figure 2. According to Figure 2, we first replace the \u201c@place-\nholder\u201d with one entity in the candidate answer list and search the document to find one matched sentence that containing this entity. Then, we align these two entities and set location index as 0. Starting with this index, words located left will have negative index while words located right will have positive index. With index defined, we align same words of these two sentences and compute the difference of word indexes. As for words without alignment, a penalty is given. Finally, the score capture some word information of question and document sentences.\nNormalized Bag-Of-Words (nBOW): Furthermore, we consider using normalized bag-of-words (nBOW) vectors, d \u2208 Rn, where d denotes the document vector and n is the vocabulary size. To be precise, if word i appears ci times in the document, we let di = ci\u2211n\nj=1 cj\n. Therefore, a sentence is trans-\nformed into a vector and we can do similarity measure accordingly.\nWord Mover\u2019s distance (WMD): The WMD (Kusner et al., 2015) measures the dissimilarity between two text documents using the minimal distance that the embedded words of one document need to move to the embedded words of another document. Here, we apply WMD into two sentences after using Mikolov\u2019s word2vec (Mikolov et al., 2013) to convert words of sentences to embeddings. Different from the two methods above (WD and nBOW), the WMD can move words to semantically similar words, thereby capturing semantic information (see Figure 3). It can capture semantic similarity of two sentences with different unique words. For instance, \u201cThe President greets the press\nin Chicago\u201d and \u201cObama speaks to the medis in Illinoirs\u201d.\nMoreover, we try different window size to extract sentences in the document to optimize our model."}, {"heading": "4.2.3 Syntactic Features", "text": "In this task, we consider syntactic features including part-of-speech (POS) tags and dependency parsing.\nPOS tags: Similar to the word alignment defined in Section 4.2.2, we consider POS tag alignment as one syntactic feature. Specifically, we transform words into POS tags using NLTK1 and employ same technology as word alignment to measure the dissimilarity of two sentences.\nDependency Parsing: If two sentences describe same event, it is likely that they have dependency overlapping (Wang and McAllester, 2015). We thus incorporate dependency parsing to capture such information. To be precise, we use Stanford Parsing2 to get dependencies of a sentence that are shown as several triples like (s, t, arc), e.g. (entity, has, nsubj), where s denotes source word and t is the target word. Then, this dependency-based similarity is evaluated from these three categories: (1) sd = sq and arcd = arcq or td = tq and arcd = arcq; (2) sd = sq andtd = tq; (3) sd = sq, td = tq and arcd = arcq, where d denotes document, q denotes question and sd refers to source word in document sentence while sq represents the target word in question."}, {"heading": "4.2.4 Semantic Features", "text": "In addition to word embeddings applied in WMD in Section 4.2.2. We adopt the SEMAFOR Frame Semantic (FS) parser (Das et al., 2014) to extract some semantic features. Figure 4 gives an example output of the SEMAFOR semantic parser. In this example, five frames are identified. For example,\n1http://www.nltk.org/ 2http://nlp.stanford.edu/software/\nstanford-dependencies.shtml\nThe word \u201csays\u201d is a target, which evokes a semantic frame labeled STATEMENT. Each frame has its own frame elements; e.g., the STATEMENT frame has frame elements of Message and Speaker. Features from these parsers have been shown to be useful for machine comprehension task (Wang and McAllester, 2015). We expect that the document sentence containing the answer will overlap with the question and correct answer in terms of targets, frames evoked and frame elements evoked. Therefore, we design the following features to capture this intuition. To be precise, after parsing a sentence, we get several triples composed of (t, f, e), where t denotes the target, f denotes the frame and e denote a set of elements. Then, the frame semantic based features are derived from the next seven categories: (1) tq = td ; (2) fq = fd; (3) eq = ed; (4) tq = td and fq = fd ; (5)tq = td and eq = ed; (6) fq = fd and eq = ed; (7) tq = td and fq = fd and eq = ed. We count the number of triples satisfying the above requirements from the document sentence and the query sentence to generate seven features."}, {"heading": "4.3 Semantics", "text": "Semantics play a significant role in our model. This section summarizes how our model uses semantics to achieve reading comprehension. In this project, we use semantics in aspects of WMD, Frame Semantic (FS) and coreference. In WMD, we use word embeddings to capture word level semantics and FS helps us to capture sentence level semantics. The coreference is employed to identity chains of mentions within and across sentences for data preprocessing.\nSpecifically, word embeddings (Mikolov et al., 2013) project words into a low-dimensional space and similarity of vectors can capture some word similarity on semantics. For example, the word \u201cParis\u201d is close to \u201cBerlin\u201d rather than \u201cFrance\u201d and vec(\u201cParis\u201d) is closest to vec(\u201cBerlin\u201d) - vec(\u201cGermany\u201d) + vec(\u201cFrance\u201d), where vec(x) de-\nnotes the word embedding of word x. Based on the word embedding given by (Mikolov et al., 2013), the WMD can align semantically similar words together using distance measure, for example, it is much cheaper to transform \u201cIllinois\u201d into \u201cChicago\u201d than \u201cJapan\u201d into \u201cChicago\u201d. Therefore, we incorporate semantics into the model and its performance is improved.\nAs for frame semantics, the semantic similarity that two sentences describe same events but use different words or two sentence have different structures can be captured by frame semantics parsing. For example, two sentence \u201cthe speaker states that he is innocent.\u201d and \u201c\u2018I\u2019m innocent\u2019, he says.\u201d would be parsed to have the same semantic frame of STATEMENT and frame elements of (Message, Speaker), even they use different words and have inverse sentence order.\nCoreference resolution is achieved using Stanford CoreNLP3. We try to resolve the pronoun with the specific description and run the coreference resolution system on each document. As the coreference system will provide a chain of mentions, we take the representative mention to resolve pronoun in the document only, i.e., replacing the pronoun like \u201cit\u201d with the representative mention of its coreference chain."}, {"heading": "5 Evaluation", "text": "We evaluate the L2R reader system by comparing with the baselines of (Hermann et al., 2015). Moreover, we present the system performance with different learning to rank algorithms, based on which, we select RankSVM and LamdaMART as the ranking algorithm for model training. We also evaluate the contribution of single feature to the system performance for final feature decision. Based on the final results, we analyze the effect of incorporating semantics into the system from coreference, word embeddings, and frame semantics. Following by semantic analysis, we conduct error analysis to see how to improve the system in the future work."}, {"heading": "5.1 Experimental Results", "text": "Final Results: Table 5.1 shows the best performance of our model and some baselines. Our L2R\n3http://stanfordnlp.github.io/CoreNLP/\nmodel finally combines all the three kinds of word distance features and frame semantic features described in section 4.2. Our L2R Reader outperforms LSTM-based models proposed in (Hermann et al., 2015) on the CNN dataset and achieves competitive results on the Daily Mail dataset.\nDifferent L2R Algorithms: Table 4 shows the performance of different learning to rank algorithms by using the same features. Model parameters are tuned on the validation set. We found that RankSVM and LambdaMART are two best L2R algorithms on this task. RankSVM performs best on the CNN dataset while LambdaMART performs best on the Daily Mail dataset.\nSingle Feature Performance: We evaluate the performance of each single feature. Table 5 shows the results. We can see that among all single features, word alignment features perform best and frame semantic features perform worst. This can be explained by the nature of the dataset.\nPerformance vs Size of Training Data: One\ngreat advantage of our model is that it only requires a small set of training data compared with neural network based models. Table 6 shows the relationship between our model performance and the number of training data. We can see that our model have a high score given only about 100 training data. Our model performance improves when giving more data, but the speed of improvement gets dramatically slow. Figure 5 shows the trend of the convergence of our model with more data."}, {"heading": "5.2 Semantics Analysis", "text": "We also test the performance of our semantic components, which are co-reference, word embeddings and frame semantics.\nTable 7 shows the results of adding co-reference, deleting word mover\u2019s distance and deleting frame\nsemantics. From the experimental results, we can see that coreference system does not bring improvement to our system, and even harm the performance. This might because the coreference system (Stanford CoNLP) we use in our system cannot have good performance when the sentences are complicated. We can also find from the results that the word embedding and frame semantics components plays a vital role in our system, although the single feature performance of frame semantics is quite low."}, {"heading": "5.3 Error Analysis", "text": "To give insight into our system\u2019s performance and reveal future research directions, we analyze the errors made by our system. We found that many queries require text summarization, event detection, background knowledge and inference. We also found an error on the CNN dataset (refer to Figure 6). We make some detailed analysis as follows.\nFigure 6 shows an example of wrong answer in the gold standard dataset. We can see from the query\nthat the correct answer should be entity0 rather than entity6. Our model successfully get the right answer. The error in the dataset might due to the generation process of the dataset.\nFigure 7 shows an example of requiring high level text summarization. The phrases \u201cwithin 10 minutes\u201d and \u201ckick off\u201d do not appear in the document, but they are high level summarization of the document. Hence, our model lacks the ability of getting the correct answer in such situation.\nFigure 8 shows an example of requiring event detection in filling the cloze question. The word \u201ccollapse\u201d appears several times in the document, but describes different events. Our model fails to capture the difference.\nFigure 9 shows an example of requiring background knowledge and inference in filling the cloze\nquestion. It can be inferred that \u201cpreschool show\u201d is performed by \u201cchildren\u2019s performer\u201d. However, this inference require some background knowledge. Our model cannot perform well in such situation."}, {"heading": "6 Conclusion", "text": "We explored the new cloze style reading comprehension task and designed a learning to rank (L2R) reader system to provide a solution. We incorporate semantics such as word embeddings, frame semantics and coreference resolution into our system and show that they can greatly improve our model performance. We find that our model is poor at high level text summarization, event detection and inference through error analysis. We will investigate into how to solve these kinds of problems by using semantics in the future."}], "references": [{"title": "Learning to rank using gradient descent", "author": ["Burges et al.2005] Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Burges et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Burges et al\\.", "year": 2005}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["Christopher JC Burges"], "venue": null, "citeRegEx": "Burges.,? \\Q2010\\E", "shortCiteRegEx": "Burges.", "year": 2010}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Cao et al.2007] Zhe Cao", "Tao Qin", "Tie-Yan Liu", "MingFeng Tsai", "Hang Li"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Freund et al.2003] Yoav Freund", "Raj Iyer", "Robert E Schapire", "Yoram Singer"], "venue": "The Journal of machine learning research,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "A short introduction to learning to rank", "author": ["LI Hang"], "venue": "IEICE TRANSACTIONS on Information and Systems,", "citeRegEx": "Hang.,? \\Q2011\\E", "shortCiteRegEx": "Hang.", "year": 2011}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["Thore Graepel", "Klaus Obermayer"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Herbrich et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 1999}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "From word embeddings to document distances", "author": ["Kusner et al.2015] Matt Kusner", "Yu Sun", "Nicholas Kolkin", "Kilian Q Weinberger"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Machine comprehension with discourse relations", "author": ["Narasimhan", "Barzilay2015] Karthik Narasimhan", "Regina Barzilay"], "venue": "In 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Avinava Dubey", "Eric P Xing", "Matthew Richardson"], "venue": "In Proceedings of ACL", "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "A strong lexical matching method for the machine comprehension test", "author": ["Smith et al.2015] Ellery Smith", "Nicola Greco", "Matko Bo\u0161njak", "Andreas Vlachos"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "A parallel-hierarchical model for machine comprehension on sparse data", "author": ["Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman"], "venue": "arXiv preprint arXiv:1603.08884", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Adarank: a boosting algorithm for information retrieval", "author": ["Xu", "Li2007] Jun Xu", "Hang Li"], "venue": "In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": "(Hermann et al., 2015) therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Based on the released dataset MCTest (Richardson et al., 2013), a lexical matching based method is proposed (Smith et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 12, "context": ", 2013), a lexical matching based method is proposed (Smith et al., 2015).", "startOffset": 53, "endOffset": 73}, {"referenceID": 11, "context": "Meanwhile, (Sachan et al., 2015) applied similar loss function, modeling machine comprehension as textual entailment and solved the problem by constructing latent answer-entailing structure with an accuracy of 67.", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "Therefore, our work is based on a much larger news article dataset created by (Hermann et al., 2015).", "startOffset": 78, "endOffset": 100}, {"referenceID": 13, "context": "questions generally generated by removing a phrase from a sentence (Taylor, 1953).", "startOffset": 67, "endOffset": 81}, {"referenceID": 6, "context": "Since the questions can be formed from a short summary of the document with condensed form of paraphrase, the dataset is suitable for testing machine comprehension (Hermann et al., 2015).", "startOffset": 164, "endOffset": 186}, {"referenceID": 6, "context": "Opposed to the deep LSTM (Hermann et al., 2015) that computes the answer based on context information of documents, it is more efficient and does not require much data to reach good performance.", "startOffset": 25, "endOffset": 47}, {"referenceID": 6, "context": "The dataset (Hermann et al., 2015) we used in this task were constructed from news article from CNN and Daily Mail websites.", "startOffset": 12, "endOffset": 34}, {"referenceID": 10, "context": "Machine comprehension generally concentrates on MCTest (Richardson et al., 2013) and due to the limitation of data size, the state of the arts are mainly based on traditional machine learning techniques.", "startOffset": 55, "endOffset": 80}, {"referenceID": 14, "context": "Although recently (Trischler et al., 2016) proposed a parallel-hierarchical model based", "startOffset": 18, "endOffset": 42}, {"referenceID": 6, "context": "Considering the limitations of MCTest dataset, (Hermann et al., 2015) provides a large scale supervised reading comprehension dataset collected from the CNN and Daily Mail websites.", "startOffset": 47, "endOffset": 69}, {"referenceID": 6, "context": "With this dataset, (Hermann et al., 2015) propose a deep LSTM reader that achieves an accuracy of 63.", "startOffset": 19, "endOffset": 41}, {"referenceID": 4, "context": "Generally, it is defined as follows: given a query, the ranking algorithm will generate a list of candidate documents with scores (Hang, 2011).", "startOffset": 130, "endOffset": 142}, {"referenceID": 4, "context": "Pointwise: In the pointwise approach, the ranking algorithm is transformed into problems including classification and regression to derive a score for every pair of document and query (Hang, 2011).", "startOffset": 184, "endOffset": 196}, {"referenceID": 4, "context": "Pairwise: In the pairewise approach, the ranking algorithm is transformed into problems of pairewise classification or pairwise regression (Hang, 2011).", "startOffset": 139, "endOffset": 151}, {"referenceID": 0, "context": "We try approaches including RankNet (Burges et al., 2005), RankBoost (Freund et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 3, "context": ", 2005), RankBoost (Freund et al., 2003), RankSVM (Herbrich et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 5, "context": ", 2003), RankSVM (Herbrich et al., 1999), MART and LambdaMART (Burges, 2010)", "startOffset": 17, "endOffset": 40}, {"referenceID": 1, "context": ", 1999), MART and LambdaMART (Burges, 2010)", "startOffset": 29, "endOffset": 43}, {"referenceID": 4, "context": "Listwise: In the listwise approach, the ranking problem is addressed by taking the ranking list as instances in both learning and prediction process (Hang, 2011).", "startOffset": 149, "endOffset": 161}, {"referenceID": 2, "context": "It maintains the group structure and we employ the following listwise approaches: ListNet (Cao et al., 2007), AdaRank (Xu and Li, 2007), Coordinate Ascent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 6, "context": "Frequency is explored based on one baseline of (Hermann et al., 2015).", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "We investigate word distance from three aspects \u2013 word alignment, nBOW, and word mover\u2019s distance (WMD) (Kusner et al., 2015).", "startOffset": 104, "endOffset": 125}, {"referenceID": 7, "context": "Word Mover\u2019s distance (WMD): The WMD (Kusner et al., 2015) measures the dissimilarity between two text documents using the minimal distance that the embedded words of one document need to move to the embedded words of another document.", "startOffset": 37, "endOffset": 58}, {"referenceID": 8, "context": "Here, we apply WMD into two sentences after using Mikolov\u2019s word2vec (Mikolov et al., 2013) to convert words of sentences to embeddings.", "startOffset": 69, "endOffset": 91}, {"referenceID": 8, "context": "Specifically, word embeddings (Mikolov et al., 2013) project words into a low-dimensional space and similarity of vectors can capture some word similarity on semantics.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "Based on the word embedding given by (Mikolov et al., 2013), the WMD can align semantically similar words together using distance measure, for example, it is much cheaper to transform \u201cIllinois\u201d into \u201cChicago\u201d than \u201cJapan\u201d into \u201cChicago\u201d.", "startOffset": 37, "endOffset": 59}, {"referenceID": 6, "context": "We evaluate the L2R reader system by comparing with the baselines of (Hermann et al., 2015).", "startOffset": 69, "endOffset": 91}, {"referenceID": 6, "context": "Our L2R Reader outperforms LSTM-based models proposed in (Hermann et al., 2015) on the CNN dataset and achieves competitive results on the Daily Mail dataset.", "startOffset": 57, "endOffset": 79}], "year": 2017, "abstractText": "Machine comprehension plays an essential role in NLP and has been widely explored with dataset like MCTest. However, this dataset is too simple and too small for learning true reasoning abilities. (Hermann et al., 2015) therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension. However, the training process is expensive. We therefore try feature-engineered approach with semantics on the new dataset to see how traditional machine learning technique and semantics can help with machine comprehension. Meanwhile, our proposed L2R reader system achieves good performance with efficiency and less training data.", "creator": "LaTeX with hyperref package"}}}