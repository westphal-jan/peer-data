{"id": "1605.05950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Interactive Debugging of Knowledge Bases", "abstract": "many ai applications rely on knowledge about a relevant real - world domain that is encoded only means of some logical knowledge base ( kb ). the most essential benefit of logical kbs have the opportunity to perform automatic reasoning to derive relational knowledge or to answer complex queries about the modeled domain. the feasibility of meaningful reasoning requires kbs to meet some minimal quality, such as implicit consistency. without semantic tool assistance, trivial task of resolving inadequate quality criteria in kbs makes mean extremely tough even for domain experts, especially when the problematic kb includes a large number of logical formulas effectively comprises complicated logical formalisms.", "histories": [["v1", "Thu, 19 May 2016 13:40:01 GMT  (2367kb,D)", "http://arxiv.org/abs/1605.05950v1", "Ph.D. Thesis, Alpen-Adria Universit\\\"at Klagenfurt"]], "COMMENTS": "Ph.D. Thesis, Alpen-Adria Universit\\\"at Klagenfurt", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["patrick rodler"], "accepted": false, "id": "1605.05950"}, "pdf": {"name": "1605.05950.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Patrick Rodler"], "emails": [], "sections": [{"heading": null, "text": "Patrick Rodler\nInteractive Debugging of Knowledge Bases\nDISSERTATION\nsubmitted in fulfilment of the requirements for the degree of\nDoktor der Technischen Wissenschaften\nAlpen-Adria-Universit\u00e4t Klagenfurt\nFakult\u00e4t f\u00fcr Technische Wissenschaften\nMentor O.Univ.-Prof. Dipl.-Ing. Dr. Gerhard Friedrich Alpen-Adria-Universit\u00e4t Klagenfurt Institut f\u00fcr Angewandte Informatik\n1st Evaluator O.Univ.-Prof. Dipl.-Ing. Dr. Gerhard Friedrich Alpen-Adria-Universit\u00e4t Klagenfurt Institut f\u00fcr Angewandte Informatik\n2nd Evaluator Univ.-Prof. Dipl.-Ing. Dr.techn. Franz Wotawa"}, {"heading": "TU Graz Institut f\u00fcr Softwaretechnologie", "text": "Klagenfurt, November 2015\nar X\niv :1\n60 5.\n05 95\n0v 1\n[ cs\n.A I]\n1 9\nM ay\n2 01\n6\n\u00a9 Alpen-Adria-Universit\u00e4t Klagenfurt, Studien- und Pr\u00fcfungsabteilung version 2015-02-20\nAffidavit"}, {"heading": "I hereby declare in lieu of an oath that", "text": "- the submitted academic paper is entirely my own work and that no auxiliary materials have been used other than those indicated,\n- I have fully disclosed all assistance received from third parties during the process of writing the paper, including any significant advice from supervisors,\n- any contents taken from the works of third parties or my own works that have been included either literally or in spirit have been appropriately marked and the respective\nsource of the information has been clearly identified with precise bibliographical\nreferences (e.g. in footnotes),\n- to date, I have not submitted this paper to an examining authority either in Austria or abroad and that\n- the digital version of the paper submitted for the purpose of plagiarism assessment is fully consistent with the printed version."}, {"heading": "I am aware that a declaration contrary to the facts will have legal consequences.", "text": "(Signature) (Place, date)\nContents\nList of Figures i\nList of Tables iii\nAbstract v"}, {"heading": "I Prolog 1", "text": ""}, {"heading": "1 Introduction 5", "text": ""}, {"heading": "2 Preliminaries 21", "text": "2.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2 Considered Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.3 Notational Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24"}, {"heading": "3 Knowledge Base Debugging 27", "text": "3.1 Parsimonious Knowledge Base Debugging . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.2 Background Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34"}, {"heading": "4 Diagnosis Computation 37", "text": "4.1 Conflict Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.2 Conflict Sets versus Justifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.3 The Relation between Conflict Sets and Diagnoses . . . . . . . . . . . . . . . . . . . . 41 4.4 Methods for Diagnosis Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.4.1 Computation of a Minimal Conflict Set . . . . . . . . . . . . . . . . . . . . . . 46 4.4.2 Correctness of Conflict Set Computation . . . . . . . . . . . . . . . . . . . . . 49\n4.5 Hitting Set Tree Based Diagnosis Computation . . . . . . . . . . . . . . . . . . . . . . 58 4.5.1 Breadth-First Diagnosis Computation . . . . . . . . . . . . . . . . . . . . . . . 58 4.5.2 Correctness of Breadth-First Diagnosis Computation . . . . . . . . . . . . . . . 62 4.6 Diagnosis Probability Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.6.1 Construction of a Probability Space . . . . . . . . . . . . . . . . . . . . . . . . 67 4.6.2 Using Probabilities for Diagnosis Computation . . . . . . . . . . . . . . . . . . 72 4.6.3 Correctness of Weighted Diagnosis Computation . . . . . . . . . . . . . . . . . 75 4.6.4 Using Probabilities to Compute Minimum Cardinality Diagnoses . . . . . . . . 78 4.7 Non-Interactive Knowledge Base Debugging Algorithm . . . . . . . . . . . . . . . . . . 79\n5 Summary 85\nII Interactive Knowledge Base Debugging 87"}, {"heading": "6 Motivation and Problem Definitions 91", "text": ""}, {"heading": "7 User Interaction 93", "text": "7.1 Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 7.2 Leading Diagnoses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 7.3 Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 7.4 Interpretation of Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 7.5 The Relation between a Query and Its Q-Partition . . . . . . . . . . . . . . . . . . . . . 97 7.6 Existence of Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98"}, {"heading": "8 Query Generation 99", "text": "8.1 Generation of a Pool of Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 8.2 Discussion of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 8.3 Minimization of Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 8.4 Soundness of Query Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 8.5 Complexity of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 8.6 Shortcomings of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 115 8.7 Correctness of Query Pool Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 116"}, {"heading": "9 An Algorithm for Interactive Knowledge Base Debugging 121", "text": "9.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 9.2 Detailed Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n9.2.1 Input Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 9.2.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 9.2.3 Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 9.2.4 Algorithm Walkthrough . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n9.3 Query Selection Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 9.4 Correctness and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136"}, {"heading": "10 Summary 143", "text": "III Iterative Diagnosis Computation 145"}, {"heading": "11 STATICHS: A Static Iterative Diagnosis Computation Algorithm 149", "text": "11.1 Overview and Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 11.2 Algorithm Walkthrough . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 11.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 11.4 Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163"}, {"heading": "12 DYNAMICHS: A Dynamic Iterative Diagnosis Computation Algorithm 171", "text": "12.1 Overview and Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 12.2 Algorithm Walkthrough . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 12.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 12.4 Details and Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n12.4.1 Definitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 12.4.2 The Labeling Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 12.4.3 Impact of Answered Queries on Conflict Sets . . . . . . . . . . . . . . . . . . . 198\n12.4.4 Impact of Answered Queries on Diagnoses . . . . . . . . . . . . . . . . . . . . 200 12.4.5 Redundant Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 12.4.6 Hitting Set Tree Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 12.4.7 De-Facto Non-Redundant Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . 214 12.4.8 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 12.4.9 Soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 12.4.10 Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243"}, {"heading": "13 Discussion of Iterative Diagnosis Computation 249", "text": ""}, {"heading": "IV Two Query Strategies for Efficient Fault Localization in Interactive Ontology Debugging 255", "text": ""}, {"heading": "14 Introduction to the Problem 259", "text": ""}, {"heading": "15 Motivating Examples and Basic Concepts 263", "text": ""}, {"heading": "16 Entropy-Based Query Selection 273", "text": ""}, {"heading": "17 Implementation Details 277", "text": ""}, {"heading": "18 Evaluation 281", "text": ""}, {"heading": "19 Related Work 291", "text": ""}, {"heading": "20 Summary and Conclusions 293", "text": ""}, {"heading": "V Minimizing User Interaction in Ontology Debugging 295", "text": ""}, {"heading": "21 Introduction to the Problem 299", "text": ""}, {"heading": "22 Motivation and Basic Concepts 301", "text": ""}, {"heading": "23 RIO: Risk Optimization for Query Selection 307", "text": ""}, {"heading": "24 Evaluation 311", "text": ""}, {"heading": "25 Related Work 315", "text": ""}, {"heading": "26 Summary and Conclusions 317", "text": ""}, {"heading": "VI A Direct Approach to Sequential Diagnosis of High Cardinality Faults in Knowledge Bases 319", "text": ""}, {"heading": "27 Introduction to the Problem 323", "text": ""}, {"heading": "28 Basic Concepts 325", "text": "29 Interactive Direct Diagnosis of Knowledge Bases 329"}, {"heading": "30 Evaluation 335", "text": ""}, {"heading": "31 Summary and Conclusions 339", "text": ""}, {"heading": "VII Epilog 341", "text": ""}, {"heading": "32 Related Work 345", "text": ""}, {"heading": "33 Summary 349", "text": ""}, {"heading": "34 Future Work Topics 353", "text": "Bibliography 357\nList of Figures\n1.1 The Principle of Non-Interactive KB Debugging . . . . . . . . . . . . . . . . . . . . . . 7 1.2 The Principle of Interactive KB Debugging . . . . . . . . . . . . . . . . . . . . . . . . 12 1.3 Precedence Constraints among the Parts . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.1 Recursion Tree for the Computation of a Minimal Conflict Set . . . . . . . . . . . . . . 50 4.2 Non-Interactive KB Debugging Process without Fault Information . . . . . . . . . . . . 81 4.3 Non-Interactive KB Debugging Process with Fault Information . . . . . . . . . . . . . . 82\n11.1 (Example 11.1) Solving the Problem of Interactive Static KB Debugging . . . . . . . . . 160 11.2 (Example 11.2) Solving the Problem of Interactive Static KB Debugging . . . . . . . . . 161 11.3 (Example 11.2 continued) Solving the Problem of Interactive Static KB Debugging . . . 162\n12.1 (Example 12.1) Solving the Problem of Interactive Dynamic KB Debugging . . . . . . . 186 12.2 (Example 12.1 continued) Solving the Problem of Interactive Dynamic KB Debugging . 187 12.3 (Example 12.2) Solving the Problem of Interactive Dynamic KB Debugging . . . . . . . 192 12.4 (Example 12.2 continued) Solving the Problem of Interactive Dynamic KB Debugging . 193\n15.1 The Search Tree of the Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 271\n18.1 Average Number of Queries Required to Select the Target Diagnosis . . . . . . . . . . . 283 18.2 Example of Prior Fault Probabilities of Syntax Elements Sampled from Extreme, Moderate and Uniform Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284 18.3 Average Time/Query Gain Resulting from the Application of the Extended CKK Partitioning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286 18.4 Average Time Required to Identify the Target Diagnosis Using CKK and Brute Force\nQuery Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n24.1 Average Number of Queries Required by RIO Compared to Other Query Selection Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 24.2 Box-Whisker Plots Illustrating the Performance Discrepancy between Better and Worse Query Selection Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\n29.1 INV-QX Recursion Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332 29.2 Identification of the Target Diagnosis Using INV-HS-TREE and INV-QX . . . . . . . . 332 29.3 Identification of the Target Diagnosis Using HS-TREE and QX . . . . . . . . . . . . . . 333\ni\nList of Tables\n2.1 Symbols and Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n4.1 Propositional Logic Example DPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.2 Description Logic Example DPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.3 Description Logic Example DPI 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.4 Computation of Fault Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n8.1 First-Order Logic Example DPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 8.2 (Example 8.1) Computing Entailments for Query Generation . . . . . . . . . . . . . . . 104 8.3 Queries and Associated Q-Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n9.1 (Example 9.1) Diagnoses Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n11.1 (Example 11.2) Formula Fault Probabilities . . . . . . . . . . . . . . . . . . . . . . . . 157\n12.1 Transition of Node Labels due to Tree Pruning . . . . . . . . . . . . . . . . . . . . . . . 215\n13.1 Comparison: STATICHS versus DYNAMICHS. . . . . . . . . . . . . . . . . . . . . . . . 254\n15.1 Entailments of Ontologies Repaired by Different Diagnoses . . . . . . . . . . . . . . . . 267 15.2 (Example 15.1) Possible Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 15.3 (Example 15.2) Possible Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n16.1 Expected Scores for Minimized Queries (All Axioms Have Equal Fault Probability) . . . 275 16.2 Expected Scores for Minimized Queries (One Axiom Has Greater Fault Probability than the Rest) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 16.3 Probabilities of Diagnoses after Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 276 16.4 Expected Scores for Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n18.1 Diagnosis Results for Several of the Real-World Ontologies . . . . . . . . . . . . . . . . 281 18.2 Minimum, Average and Maximum Time and Calls Required to Compute Leading Most Probable Diagnoses as well as All Diagnoses for Real-World Ontologies . . . . . . . . . 282 18.3 Minimum, Average and Maximum Number of Queries Required by the Entropy-Based and \u201cSplit-In-Half\u201d Query Selection Methods . . . . . . . . . . . . . . . . . . . . . . . 285 18.4 Average Time Required to Compute a Set of Leading Diagnoses and a Query in Each Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286 18.5 Statistics for the Real-World Ontologies Used in the Stress-Tests . . . . . . . . . . . . . 287 18.6 Average Values Measured for Extreme, Moderate and Uniform Distributions in Each of\nthe Good, Average and Bad Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\niii\niv LIST OF TABLES\n22.1 A Set of Queries and Associated Partitions w.r.t. the Initial DPI of the Example Ontology 306\n24.1 Average Time for Debugging Session, between Two Successive Queries and Average Number of Queries Required by Each Strategy . . . . . . . . . . . . . . . . . . . . . . 313 24.2 Percentage Rates Indicating How Often Which Query Selection Strategy Performed Best 313\n30.1 HS-TREE and INV-HS-TREE Applied to Anatomy Benchmark . . . . . . . . . . . . . . 336 30.2 Performance of Sequential Diagnosis Using Direct Computation of Diagnoses . . . . . . 337\nAbstract\nMost artificial intelligence applications rely on knowledge about a relevant real-world domain that is encoded in a knowledge base (KB) by means of some logical knowledge representation language. The most essential benefit of such logical KBs is the opportunity to perform automatic reasoning to derive implicit knowledge or to answer complex queries about the modeled domain. The feasibility of meaningful reasoning requires a KB to meet some minimal quality criteria such as consistency; that is, there must not be any contradictions in the KB. Without adequate tool assistance, the task of resolving such violated quality criteria in a KB can be extremely hard even for domain experts, especially when the problematic KB includes a large number of logical formulas, comprises complicated formalisms, was developed by multiple people or in a distributed fashion or was (partially) generated by means of some automatic systems.\nNon-interactive debugging systems published in research literature often cannot localize all possible faults (incompleteness), suggest the deletion or modification of unnecessarily large parts of the KB (nonminimality), return incorrect solutions which lead to a repaired KB not satisfying the imposed quality requirements (unsoundness) or suffer from poor scalability due to the inherent complexity of the KB debugging problem. Even if a system is complete and sound and considers only minimal solutions, there are generally exponentially many solution candidates to select one from. However, any two repaired KBs obtained from these candidates differ in their semantics in terms of entailments and non-entailments. Selection of just any of these repaired KBs might result in unexpected entailments, the loss of desired entailments or unwanted changes to the KB which in turn might cause unexpected new faults during the further development or application of the repaired KB. Also, manual inspection of a large set of solution candidates can be time-consuming (if not practically infeasible), tedious and error-prone since human beings are normally not capable of fully realizing the semantic consequences of deleting a set of formulas from a KB. Hence there is a need for adequate tools that support a user when facing a faulty KB.\nIn this work, we account for these issues and propose methods for the interactive debugging of KBs which are complete and sound and compute only minimally invasive solutions, i.e. suggest the deletion or modification of just a set-minimal subset of the formulas in the problematic KB. User interaction takes place in the form of queries asked to a person, e.g. a domain expert, about intended and non-intended entailments of the correct KB. To construct a query, only a minimal set of two solution candidates must be available. After the answer to a query is known, the search space for solutions is pruned. Iteration of this process until there is only a single solution candidate left yields a repaired KB which features exactly the semantics desired and expected by the user.\nThe novel contributions of this work are:\n\u2022 Thorough Theoretical Workup of the Topic of Interactive Debugging of Monotonic KBs: We evolve the theory of the topic by first elaborating on the theory of non-interactive KB debugging, revealing crucial shortcomings in the application of non-interactive methods and thereby motivating the development and deployment of interactive approaches in KB debugging. Then, we give some important results that guarantee the feasibility of interactive KB debugging, give some precise definitions of the problems interactive KB debugging aims to solve and present algorithms that provably solve these problems.\nv\nvi ABSTRACT\n\u2022 A Complete Picture of an Interactive Debugging System is Drawn: This is the first work that deals with an entire system of algorithms that are required for the interactive debugging of monotonic KBs, considers and details all algorithms separately, proves their correctness and demonstrates how all these algorithms are orchestrated to make up a full-fledged and provably correct interactive KB debugging system.\n\u2022 Two New Algorithms for the Iterative Computation of Candidate Solutions in the scope of interactive KB debugging are proposed. The first one guarantees constant convergence towards the exact solution of the interactive KB problem by the ascertained reduction of the number of remaining solutions after any query is answered. The second one features powerful search tree pruning techniques and might thus be expected to exhibit a more time- and space-saving behavior than existing algorithms, in particular for growing problem instances.\n\u2022 Suggestion and Extensive Analysis of Different Methods for Selection of the \u201cBest\u201d Query to ask the user next. We compare a greedy \u201csplit-in-half\u201d strategy that proposes queries which eliminate half of the known candidate solutions with a strategy relying on information entropy that chooses the query with highest information gain based on (a user\u2019s) beliefs about faults in the KB. Comprehensive experiments manifest that an average guess of the fault information suffices to reduce the query answering effort for the interacting user, often to a significant extent, by means of the latter strategy compared to the former. Moreover, we demonstrate that both methods clearly outperform a random way of selecting queries.\n\u2022 Presentation of a Reinforcement Learning Query Selection Strategy. Minimal effort for the interacting user can be achieved if both the query selection method is chosen carefully and the provided fault information satisfies some minimum quality requirements. In particular, for deficient fault information and unfavorable strategy for query selection, we observe cases where the overhead in terms of user effort exceeds 2000% (!) in comparison to employing a more favorable query selection strategy. Since, unfortunately, assessment of the fault information is only possible a-posteriori (after the debugging session is finished and the correct solution is known), we devise a learning strategy (RIO) that continuously adapts its behavior depending on the performance achieved and in this vein minimizes the risk of using low-quality fault information. This approach makes interactive debugging practical even in scenarios where reliable fault estimates are difficult to obtain. Evaluations provide evidence that for 100% of the cases in the hardest (from the debugging point of view) class of faulty test KBs, RIO performed at least as good as the best other strategy and in more than 70% of these cases it even manifested superior behavior to the best other strategy. Choosing RIO over other approaches can involve an improvement by the factor of up to 23, meaning that more than 95% of user time and effort might be saved per debugging session.\n\u2022 Provisioning of Mechanisms for Efficiently Dealing with KB Debugging Problems Involving High Cardinality Faults. In the standard interactive debugging approach described in this work, the computation of queries is based on the generation of the set of most probable solution candidates. By this postulation, certain quality guarantees about the output solution can be given. However, we learn that dropping this requirement can bring about substantial savings in terms of time and especially space complexity of interactive debugging, in particular in debugging scenarios where faulty KBs are (partly) generated as a result of the application of automatic systems. In such situations, we propose to base query computation on any set of solution candidates using a \u201cdirect\u201d method for candidate generation. We study the application of this direct method to high cardinality faults in KBs and find out that the number of required queries per debugging session is scarcely affected for cases when the standard approach is also applicable. However, the direct method proves applicable in situations when the standard approach is not (due to time or memory issues) and is still able to locate the correct solution.\nPart I\nProlog\n1\n3 In this part, we first give an introduction in Chapter 1. This includes a motivation why knowledge base debugging is a \u201chot topic\u201d (and even getting hotter as intelligent applications and devices become more and more ubiquitous), an introduction to the non-interactive debugging of knowledge bases and the revealment of decisive shortcomings of this paradigm, e.g. poor scalability and the risk of obtaining solutions of inferior quality. As a solution to the identified issues we then explain how a (group of) user(s) might collaborate with an interactive debugging system to determine high-quality solutions even in scenarios where non-interactive systems fail. Further, we discuss the design and the components of a generic interactive debugger, provide an illustrating example and outline the powerful feature of our system to be able to incorporate background knowledge into the debugging process which can drastically reduce the search space for solutions and disclose faults in the knowledge base that could be missed otherwise. Finally, we provide an enumeration of the contributions of this work and discuss the further organization of this part and of the rest of this work.1\n1Parts of Part I already appeared in [Rod15].\nChapter 1\nIntroduction\nMotivation. Most artificial intelligence applications rely on knowledge that is encoded in a knowledge base (KB) by means of some logical knowledge representation language such as propositional logic (PL) [CL73], Datalog [CGT89], first-order logic (FOL) [CL73], The Web Ontology Language (OWL [PSHH+04], OWL 2 [GHM+08, MPSP09]) or Description Logic (DL) [BCM+07]. Experts in a variety of application domains keep developing KBs of constantly growing size. A concrete example of a repository containing biomedical KBs is the Bioportal2, which comprises vast ontologies with tens or even hundreds of thousands of terms each (e.g. the SNOMED-CT ontology with currently over 395.000 terms). Such KBs however pose a significant challenge for people as well as tools involved in their evolution, maintenance and application.\nAll these activities are based on the most essential benefit of logical KBs, namely the opportunity to perform automatic reasoning to derive implicit knowledge or to answer complex queries about the modeled domain. The feasibility of meaningful reasoning requires a KB to meet the minimum quality criterion consistency, i.e. there must not be any contradictions in the KB. Because any logical formula can be derived from an inconsistent KB. Further on, one might postulate further requirements to be met by a KB. For instance, one might consider faulty a FOL KB entailing \u2200X \u00acp(X) for some predicate symbol p occurring in the KB. Such a KB would be incoherent, i.e. it would violate the requirement coherency (which was originally defined for DL KBs [SHCH07, PSK05]). Additionally, test cases can be specified giving information about desired (positive test cases) and non-desired (negative test cases) entailments a correct KB should feature. This characterization of a KB\u2019s intended semantics is a direct analogon to the field of software debugging, where test cases are exploited as a means to verify the correct semantics of the program code.\nAs KBs are growing in size and complexity, their likeliness of violating one of these criteria increases. Faults in KBs may, for instance, arise because human reasoning is simply overstrained [HBP11, HPS09]. That is, generally a person will not be capable of completely grasping or mentally processing the entire knowledge contained in a (large or complex) KB at once. In fact, a person might fully comprehend some isolated part of a the KB, but might not be able to determine or understand all implications or nonimplications of this isolated part combined with other parts of a KB, i.e. when new logical formulas are added.\nAnother reason for the non-compliance with the mentioned quality criteria imposed on KBs might be that multiple (independently working) editors contribute to the development of the KB [NCLM06] which may lead to contradictory formulas. The OBO Project3 and the NCI Thesaurus4 are examples of\n2http://bioportal.bioontology.org 3http://obo.sourceforge.net 4http://nciterms.nci.nih.gov/ncitbrowser\n5"}, {"heading": "6 CHAPTER 1. INTRODUCTION", "text": "collaborative KB development projects. Employing automatic tools, e.g. [JRG11, NB12, JMSK09], to generate (parts of) KBs can further exacerbate the task of KB quality assurance [Mei11, EFvH+11].\nMoreover, as studies in cognitive psychology [CP71, JL99] attest, humans make systematic errors while formulating or interpreting logical formulas. These observations are confirmed by [RDH+04, RCVB09] which present common faults people make when developing a KB (ontology). Hence, it is essential to devise methods that can efficiently identify and correct faults in a KB.\nNon-Interactive KB Debugging. Given a set of requirements to the KB and sets of test cases, KB debugging methods [SHCH07, KPHS07, FS05, HPS08] can localize a (potential) fault by computing a subset D of the formulas in the KB K called a diagnosis. At least all formulas in a diagnosis must be (adequately) modified or deleted in order to obtain a KB K\u2217 that satisfies all postulated requirements and test cases. Such a KBK\u2217 constitutes the solution to the KB debugging problem. Figure 1.15 outlines such a KB debugging system. The input to the system is a diagnosis problem instance (DPI) defined by\n\u2022 some KB K formulated using some (monotonic) logical language L (every formula in K might be correct or faulty),\n\u2022 (optionally) some KB B (over L) formalizing some background knowledge relevant for the domain modeled by K (such that B and K do not share any formulas; all formulas in B are considered correct)\n\u2022 a set of requirements R to the correct KB,\n\u2022 sets of positive (P ) and negative (N ) test cases (over L) asserting desired semantic properties of the correct KB and\n\u2022 (optionally) some fault information FP, e.g. in terms of fault probabilities of logical formulas in K.\nMoreover, the system requires a sound and complete logical reasoner for deciding consistency (coherency) and calculating logical entailments of a KB formulated over the language L. Some approaches (including the ones presented in this work) use the reasoner as a black-box (e.g. [SFFR12, Hor11]) within the debugging system. That is, the reasoner is called as is and serves as an oracle independent from other computations during the debugging process; that is, the internals of the reasoner are irrelevant for the debugging task. On the other hand, glass-box approaches (e.g. [SHCH07, Hor11, KPSH05]) attempt to exploit internal modifications of the reasoner for debugging purposes; in other words, the sources of problems (e.g. contradictory formulas) in the KB are computed as a direct consequence of reasoning [Hor11]. The advantages of a black-box approach over a glass-box approach are the lower memory consumption and better performance [KPSH05] of the reasoner and the reasoner independence of the debugging method. The latter benefit is essential for the generality of our approaches and their applicability to various knowledge representation formalisms.\nGiven these inputs, the debugging system focuses on (a subset of) all possible fault candidates (usually the set of minimal, i.e. irreducible, diagnoses) and usually outputs the most probable one amongst these if some fault information is provided or the minimum cardinality one, otherwise. Alternatively, a debugging system might also be employed to calculate a predefined number of (most probable or minimum cardinality) minimal diagnoses or to determine all minimal diagnoses computable within a predefined time limit.\n5Thanks to Kostyantyn Shchekotykhin for making available to me parts of this diagram.\n7\nIssues with Non-Interactive KB Debugging Systems. In real-world scenarios, debugging tools often have to cope with large numbers of minimal diagnoses where the trivial application, i.e. deletion, of any minimal diagnosis leads to a (repaired) KB with different semantics in terms of entailed and nonentailed formulas. For example, in [SF10] a sample study of real-world KBs revealed that the number of different minimal diagnoses might exceed thousand by far (1782 minimal diagnoses for a KB with only 1300 formulas). In such situations simple visualization of all these alternative modifications of the ontology is clearly ineffective. Selecting a wrong diagnosis (in terms of its semantics, not in terms of fulfillment of test cases and requirements) can lead to unexpected entailments or non-entailments, lost desired entailments and surprising future faults when the KB is further developed. Manual inspection of a large set of (minimal) diagnoses is time-consuming (if not practically infeasible), error-prone and often computationally infeasible due to the complexity of diagnosis computation.\nMoreover, [Stu08] has put several (non-interactive) debugging systems to the test using a test set of faulty (incoherent OWL) real-world KBs which were partly designed by humans and partly by the application of automatic systems. The result was that most of the investigated systems had serious performance problems, ran out of memory, were not able to locate all the existing faults in the KB (incompleteness), reported parts of a KB as faulty which actually were not faulty (unsoundness), produced only trivial solutions or suggested non-minimal faults (non-minimality). Often, performance problems and incompleteness of non-interactive debugging methods can be traced back to an explosion of the search tree for minimal diagnoses.\nThe Solution: Interactive KB Debugging. In this work we present algorithms for interactive KB debugging. These aim at the gradual reduction of compliant minimal diagnoses by means of user interaction, thereby seeking to prevent the search tree for minimal diagnoses from exploding in size by performing regular pruning operations. \u201cUser\u201d in this case might refer to a single person or multiple persons, usually experts of the particular domain the faulty KB is dealing with such as biology, medicine or chemistry. Throughout an interactive debugging session, the user is asked a set of automatically chosen queries about the domain that should be modeled by a given faulty KB. A query can be created by the system after a set D of a minimum of two minimal diagnoses has been precomputed (we call D the leading diagnoses). Each query is a conjunction (i.e. a set) of logical formulas that are entailed by some correct subset of the formulas in the KB. With regard to one particular query Q, any set of minimal diagnoses for the KB, in particular the set D which has been utilized to generate Q, can be partitioned into three sets, the first one (D+) including all diagnoses in D compliant only with a positive answer to Q, the second (D\u2212) including all diagnoses in D compliant only with a negative answer to Q, and the third (D0) including all"}, {"heading": "8 CHAPTER 1. INTRODUCTION", "text": "diagnoses in D compliant with both answers. A positive answer to Q signalizes that the conjunction of formulas in Q must be entailed by the correct KB wherefore Q is added to the set of positive test cases. Likewise, if the user negates Q, this is an indication that at least one formula in Q must not be entailed by the correct KB. As a consequence, Q is added to the set of negative test cases.\nAssignment of a query Q to either set of test cases results in a new debugging scenario. In this new scenario, all elements of D\u2212 are no longer minimal diagnoses given that Q has been classified as a positive test case. Otherwise, all diagnoses in D+ are invalidated. In this vein, the successive reply to queries generated by the system will lead the user to the single minimal solution diagnosis that perfectly reflects their intended semantics. In other words, after deletion of all formulas in the solution diagnosis from the KB and the addition of the conjunction of all formulas in the specified positive test cases to the KB, the resulting KB meets all requirements and positive as well as negative test cases. In that, the added formulas contained in the positive test cases serve to replace the desired entailments that are broken due to the deletion of the solution diagnosis from the KB.\nThence, in the interactive KB debugging scenario the user is not required to cope with the understanding of which faults (e.g. sources of inconsistency or implications of negative test cases) occur in the faulty initial KB, why they are faults (i.e. why particular entailments are given and others not) and how to repair them. All these tasks are undertaken by the interactive debugging system.\nThe proposed approaches to interactive KB debugging in this work follow the standard model-based diagnosis (MBD) technique [Rei87, dKW87]. MBD has been successfully applied to a great variety of problems in various fields such as robotics [SW05], planning [SW09], debugging of software programs [WSM02], configuration problems [FFJS04], hardware designs [FSW99], constraint satisfaction problems and spreadsheets [ARW12]. Given a description (model) of a system, together with an observation of the system\u2019s behavior which conflicts with the intended behavior of the system, the task of MBD is to find those components of the system (a diagnosis) which, when assumed to be functioning abnormally, provide an explanation of the discrepancy between the intended and the observed system behavior. Translated to the setting of KB debugging, the set of \u201csystem components\u201d comprises the formulas ax i in the given faulty KB K. The \u201csystem description\u201d refers to the statement that the KB K along with the background KB B and the positive test cases p \u2208 P must meet all predefined requirements (e.g. consistency, coherency) and must not logically entail any of the negative test cases n \u2208 N , i.e.\n(i) K \u222a B \u222a \u22c3\np\u2208P p satisfies requirement r for all r \u2208 R and\n(ii) K \u222a B \u222a \u22c3\np\u2208P p 6|= n for all n \u2208 N .\nThe \u201cobservation which conflicts with the intended behavior of the system\u201d corresponds to the finding that (i) or (ii) or both are violated. That is, the \u201csystem description\u201d along with the \u201cobservation\u201d and the assumption that all components are sound yields an inconsistency. An \u201cexplanation for the discrepancy between observed and intended system behavior\u201d (i.e. a diagnosis) is the assumption D that all formulas in a subsetD ofK are faulty (\u201cbehave abnormally\u201d) and all formulas inK\\D are correct (\u201cdo not behave abnormally\u201d) such that the \u201csystem description\u201d along with the \u201cobservation\u201d and the assumption D is consistent. Computation of (minimal) diagnoses is accomplished with the aid of minimal conflict sets, i.e. irreducible sets of formulas in the KB K that preserve the violation of (i) or (ii) or both.\nAn MBD problem can be modeled as an abduction problem [BATJ91], i.e. finding an explanation for a set of data. It was proven in [BATJ91] that the computation of the first explanation (minimal diagnosis) is in P. However, given a set of explanations (minimal diagnoses) it is NP-complete to decide whether there is an additional explanation (minimal diagnosis). Stated differently, the detection of the first explanation can be efficiently accomplished whereas the finding of any further one is intractable (unless P = NP). When seeing the (interactive) KB debugging problem as an abduction problem, one must additionally take into account the costs for reasoning. Because, a call to a logical reasoner is required in order to decide whether or not a set of hypotheses (a subset of the KB) is an explanation\n9 (minimal diagnosis). Incorporating the necessary reasoning costs and assuming consistency a minimal requirement to the correct KB, the finding of the first explanation (minimal diagnosis) is already NPhard even for propositional KBs [SL89] (since propositional satisfiability checking is NP-complete). The worst case complexity for the debugging of KBs formulated over more expressive logics such as OWL 2 (reasoning is 2-NEXPTIME-complete [GHM+08, Kaz08]) will be of course even worse. This seems quite discouraging. However, we have shown in our previous works [RSFF13, SFFR12, SFRF14c] that for many real-world KBs interactive KB debugging is feasible in reasonable time, despite high (or intractable) worst case reasoning costs and the intractable complexity of the abduction (i.e. minimal diagnosis finding) problem as such. Hence, the goal of this work is amongst others to present algorithms that work well in many practical scenarios.\nAssumptions about the Interacting User. About a user u consulting an (interactive) debugging system, we make the following plausible assumptions:\nU1 u is not able to explicitly enumerate a set of logical formulas that express the intended domain that should be modeled in a satisfactory way, i.e. without unwanted entailments or non-fulfilled requirements,\nU2 u is able to answer concrete queries about the intended domain that should be modeled, i.e. u can classify a given logical formula (or a conjunction of logical formulas) as a wanted or unwanted proposition in the intended domain (i.e. an entailment or non-entailment of the correct domain model).\nThe first assumption is obviously justified since otherwise u could have never obtained a faulty KB, i.e. a KB that violates at least one requirement or test case, and there would be no need for u to employ a debugging system.\nRegarding the second assumption, the first thing to be noted is that any KB (i.e. any model of the intended domain) either does entail a certain logical formula ax or it does not entail ax . Second, if u is assumed to bring along enough expertise in that domain, u should be able to gauge the truth of (at least) some formulas about that domain, especially if these formulas constitute logical entailments of parts of the specified knowledge in KB so far. We want to emphasize that u is not required to be capable of answering all possible queries (or formulas) about the respective domain since u might always skip a particular query in our system without any noticeable disadvantages. In such a case, the system keeps generating further queries, one at a time (usually the next-best one according to some quality measure for queries), until u is ready to answer it. As the number of possible queries is usually exponential in the number of minimal diagnoses exploited to compute it, there will be plenty of different \u201csurrogate queries\u201d in most scenarios.\nA Motivating Example. To get a more concrete idea of these assumptions, the reader is invited to think about whether the following first-order KB K is consistent (a similar example is discussed in [HPS09]):\n\u2200X(res(X)\u2194 \u2200Y (writes(X,Y )\u2192 paper(Y ))) (1.1) \u2200X((\u2203Y writes(X,Y ))\u2192 res(X)) (1.2) \u2200X(secr(X)\u2192 gen(X)) (1.3) \u2200X(gen(X)\u2192 \u00acres(X)) (1.4) secr(pam) (1.5)\nIf we assume that the predicate symbols res, secr and gen stand for \u2019researcher\u2019, \u2019secretary\u2019 and \u2019general employee\u2019, respectively, and the constant pam stands for the person Pam, the KB says the following:\n\u2022 Formula 1.1: \u201cSomebody is a researcher if and only if everything they write is a paper.\u201d"}, {"heading": "10 CHAPTER 1. INTRODUCTION", "text": "\u2022 Formula 1.2: \u201cEverybody who writes something is a researcher.\u201d\n\u2022 Formula 1.3: \u201cEach secretary is a general employee.\u201d\n\u2022 Formula 1.4: \u201cNo general employee is a researcher.\u201d\n\u2022 Formula 1.5: \u201cPam is a secretary.\u201d\nThis KB is indeed inconsistent. The reader might agree that it is not very easy to understand why this is the case. The observations made in [HPS09] concerning a slight modification K\u2032 of the KB K extracted from a real-world KB confirm this assumption. Compared to K, the KB K\u2032 included only Formulas 1.1- 1.3 of K, was formulated in DL (cf. Section 2.2), and used the terms A,C, . . . instead of res, paper, . . . . Amongst others, this KB K\u2032 was used as a sample KB in a study where participants had to find out whether a concrete given formula is or is not entailed by a concrete given KB. In the case of the KBK\u2032, the assignment (translated to the terminology in our KBK) was to find out whether \u2200X(secr(X)\u2192 res(X)) is an entailment of formulas 1.1-1.3. Although K\u2032 contains only three formulas, the result was that even participants with many years of experience in DL, among them also DL reasoner developers, did not realize that this is in fact the case (the reason for this entailment to hold is that formulas 1.1-1.3 imply that \u2200X res(X) holds).\nSince \u2200X res(X) is also necessary for the inconsistency of K, this suggests that people might also have severe difficulties in comprehending why K is inconsistent. Once the validity of this entailment is clear, it is relatively straightforward to see that K cannot have any models. For, res(pam) (due to \u2200X res(X)) and \u00acres(pam) (due to formulas 1.3-1.5) are implications of K.\nConsequently, we might also assume that even experienced knowledge engineers (not to mention pure domain experts) could end up with a contradictory KB like K, which substantiates our first assumption (U1) about u. Probably, the intention of those people who specified formulas 1.1-1.3 was not that \u2200X res(X) should be entailed. That is, it might be already a too complex task for many people to (mentally) reason even with such a small KB like this and manually derive implicit knowledge from it.\nHowever, on the other hand, we might well assume u to be able to answer a concrete query about the intended domain they tried to model by K. For instance, one such query could be whether Q1 := {\u2200X res(X)} is a desired entailment of their model (i.e. \u201cshould everybody be a researcher in your intended model of the domain?\u201d). If we assume the (seemingly obvious) case that u negates this query, i.e. asserts that this is an unwanted entailment, then an interactive debugging system (employing a logical reasoner) can derive that at least one of the formulas 1.1 and 1.2 must be faulty. This holds because the only set-minimal explanation in terms of formulas in K for the entailment \u2200X res(X) is given by these two formulas. In other words, the set of formulas {1.1, 1.2} is the only minimal conflict set in K given thatQ1 is a negative test case. Hence, the deletion (or suitable modification) of any of these formulas will break this unwanted entailment.\nBefore it is known that Q1 must not be entailed by the correct KB, given consistency is the only requirement to the KB postulated by u, the complete KB K is a minimal conflict set. That is, after the assignment of a (strategically well-chosen) query to the set of positive or, in this case, negative test cases can already shift the focus of potential modifications or deletions to a subset of only two candidate formulas. We would call these two formulas the remaining minimal diagnoses after an answer to the query Q1 has been submitted.\nInitially, there are five minimal diagnoses, each formula in K is one. The meaning of a diagnosis is that its deletion from K leads to the fulfillment of all requirements and (so-far-)specified positive and negative test cases. As the reader should be easily able to see, the deletion of any formula from K yields a consistent KB; e.g. removing formula 1.5 prohibits the entailment \u00acres(pam) whereas discarding formula 1.2 prohibits the entailment res(pam). The reader should notice that, as soon as the negative test case Q1 is known, removing (only) formula 1.5 does not yield a correct KB since {1.1, 1.2, 1.3, 1.4} still entails Q1 which must not be entailed.\n11\nA second query to u could be, for example, Q2 : {\u2203X((\u2203Y writes(X,Y )) \u2227 \u00acres(X))} (i.e. \u201cis there somebody who writes something, but is no researcher?\u201d). Again, it is reasonable to suppose that u might know whether or not this should hold in their intended domain model. The (seemingly obvious) answer in this case would be positive, e.g. because u intends to model students who write homework, exams, etc., but are no researchers. This positive answer leads to the new positive test case Q2. Adding this positive test case, like a set of new formulas, to the KB K would result in Knew := K \u222a Q2. The debugging system would then figure out that formula 1.2 is the only minimal conflict set in the KBKnew. The reason for this is that the elimination of formula 1.2 breaks the entailmentQ1 (negative test case) and enables the addition of a new desired entailment Q2 (positive test case) without involving the violation of any requirements (consistency). Therefore, formula 1.2 is the only minimal diagnosis that is still compliant with the new knowledge in terms of Q1 = false and Q2 = true obtained.\nIt is important to notice that the solution KB Knew that is returned to the user as a result of the interactive debugging session includes a new logical formula Q2 that can be seen as a repair of the deleted formula 1.2. Since the knowledge after the debugging session is that \u00ac1.2 \u2261 Q2 must be true, this new knowledge is incorporated into the KBKnew. This indicates that the fault in KB was simply that the \u00ac in front of formula 1.2 had been forgotten.\nNotice however that the positive test case Q2 is not added to K as a usual KB formula, but rather as an extension of K that has already been approved by the user. Should the user at some later point in time commit the same fault again (and explicitly specify some formula x equivalent to formula 1.2), then the interactive debugging system, owing to the positive test case Q2, would immediately detect a singleton conflict comprising only formula x. As a consequence, each diagnosis considered during this later debugging session would suggest to delete or modify (at least) x.\nThis scenario should illustrate that, in spite of not being able to specify their domain knowledge in a logically consistent way, the user u might still be able to answer questions about the intended domain, which supports our second assumption made about the user u (the reader might agree that answering Q1 and Q2 is much easier than recognizing the entailment \u2200X res(X) of the KB). In other words, the availability of an (efficient) debugging system could help u debug their KB, without needing to analyze which entailments hold or do not hold, why certain entailments hold or do not hold or why exactly the KB does not meet certain imposed requirements or test cases, by simply answering queries whether a certain entailment should or should not hold. These queries are automatically generated by the system in a way that they focus on the problematic parts of the KB, i.e. the minimal conflict sets, and discriminate between the possible solution candidates, i.e. the minimal diagnoses.\nBenefits of the Usage of Conflict Sets. We want to remark that the usage of minimal conflict sets \u201cnaturally\u201d forces the system to take into consideration only the smallest relevant (faulty) parts of the problematic KB. This is owed to the property of minimal conflict sets to abstract from what all the reasons for a certain entailment or requirements violation are. Instead, only the \u201croot\u201d (subset-minimal) causes for such violations are examined and no computation time is wasted to extract \u201cpurely derived\u201d causes (those which are resolved as a byproduct of fixing all root causes from which it is derived, cf. [Hor11, Kal06]). For example, assuming the debugging scenario involving our example KB consisting only of formulas 1.1-1.4 which is incoherent and a requirements set including coherency. Then, there are two entailments reflecting the incoherency of this KB, first \u2200X \u00acsecr(X) and second \u2200X \u00acgen(X) (these entailments hold due to \u2200X res(X) which follows from formulas 1.1 and 1.2). Of these two, only the second one is a \u201croot\u201d problem; the first one is a \u201cpurely derived\u201d problem. That means, the entailment \u2200X \u00acsecr(X) only holds due to the presence of the entailment \u2200X \u00acgen(X). So, the cause for \u2200X \u00acgen(X) is given by the set of formulas {1.1, 1.2, 1.4} whereas the proper superset {1.1, 1.2, 1.3, 1.4} of this set accounts for the entailment \u2200X \u00acsecr(X). The exploitation of minimal conflict sets (the only minimal conflict set for this KB is {1.1, 1.2, 1.4}) ascertains that such \u201cpurely derived\u201d causes of requirements or test case violations will not be considered at all."}, {"heading": "12 CHAPTER 1. INTRODUCTION", "text": "The Ability to Incorporate Background Knowledge. Another feature of the approaches described in this work is their ability to incorporate relevant additional information in terms of a background knowledge KB B (which is regarded to be correct). B is a (consistent) KB which is usually semantically related with the faulty KB, e.g. B represents knowledge about the domain modeled by K that has already been sufficiently endorsed by domain experts. For instance, a doctor who wants to express their knowledge of dermatology in terms of a KB might resort to an approved background KB that specifies the human anatomy. Taking this background information into account puts the problematic KB into some context with existing knowledge and can thereby help a great deal to restrict the search space for solutions of the (interactive) KB debugging problem. This has also been found in [Stu08]. This useful strategy of prior search space restriction is also exploited in the field of ontology matching6 where automatic systems are employed to generate an alignment, i.e. a set of correspondences between semantically related entities of two different ontologies (KBs). Here, both ontologies are considered correct and diagnoses are only allowed to include elements of the alignment [MST07].\nApplying a strategy like that to our example KB given above, supposing that we know that Pam is not a researcher in the world the KB should model, we might specify the background KB B := {\u00acres(pam)} prior to starting the interactive debugging session. This would immediately reduce the initial set of possible minimal diagnoses from five (i.e. the entire KB) to two (i.e. the first two formulas 1.1 and 1.2). Reason for this is that the entailment \u2200X res(X) of formulas 1.1 and 1.2 already conflicts with the background knowledge \u00acres(pam).\nOutline of an Interactive KB Debugging System. The schema of an interactive debugging system is pictured by Figure 1.2.7 As in the case of a non-interactive debugging system (see above), the system receives as input a diagnosis problem instance (DPI). Further on, a range of additional parameters might be provided to the system. These serve as a means to fine-tune the system\u2019s behavior in various aspects. Hence, we call these inputs tuning parameters. These are (roughly) explained next.\nFirst, some parameters might be specified that take influence on the number of leading diagnoses used for query generation and the necessary computation time invested for leading diagnoses computation. Moreover, some parameter determining the quantity of (pre-)generated queries (of which one is selected to be asked to the user) versus the reaction time (the time it takes the system to compute the next query\n6http://www.ontologymatching.org/ 7Thanks to Kostyantyn Shchekotykhin for making available to me parts of this diagram.\n13\nafter the current one has been answered) of the system can be chosen. A further input argument is a query selection measure constituting a notion of query \u201cgoodness\u201d that is employed to filter out the \u201cbest\u201d query among the set of generated queries. To give the system a criterion specifying when a solution of the interactive KB debugging problem is \u201cgood enough\u201d, the user is allowed to define a fault tolerance parameter \u03c3. The lower this parameter is chosen, the better the (possibly \u201capproximate\u201d) solution that is guaranteed to be found. In case of specifying this parameter to zero, the system will (if feasible) return the \u201cexact\u201d solution of the interactive KB debugging problem. Roughly, the exact solution is given in terms of a solution KB obtained by means of a single solution candidate (minimal diagnosis) that is left after a sufficient number of queries have been answered (and added to the test cases). On the contrary, an approximate solution is represented by a solution KB obtained by means of a solution candidate with sufficiently high probability (where \u201csufficiently high\u201d is determined by \u03c3) at some point where there are still multiple solution candidates available.\nFinally, the user may choose between two different modes (static or dynamic) of determining the leading diagnoses. The static diagnosis computation strategy guarantees a constant \u201cconvergence\u201d towards the exact solution by \u201cfreezing\u201d the set of solution candidates at the very beginning and exploiting answered queries only for the deletion of minimal diagnoses. A possible disadvantage of this approach is the lack of efficient pruning of the used search tree. On the other hand, the dynamic method of calculating leading diagnoses has a primary focus on the preservation of a search tree of small size, thereby aiming at being able to solve diagnosis problem instances which are not solvable by the static approach due to high time and (more critically) space complexity. To this end, more powerful pruning rules are applied in this case which do not permit the algorithm to consider only a fixed set of solution candidates. Rather, the set of minimal diagnoses and minimal conflict sets are generally variable in this case which means that they are subject to change after assignment of an answered query to the test cases.\nLike in the case of a non-interactive debugger, an interactive debugging system requires a sound and complete logical reasoner for deciding consistency (coherency) and calculating logical entailments of a KB formulated over the language L.\nThe workflow in interactive KB debugging illustrated by Figure 1.2 is the following:\n1. A set of leading diagnoses is computed by the diagnosis engine (by means of the fault information, if available) using the logical reasoner and passes it to the query generation module.\n2. The query generation module computes a pool of queries exploiting the set of leading diagnoses and delivers it to the query selection module.\n3. The query selection module filters out the \u201cbest query\u201d (often by means of the fault information, if available) and shows it to the interacting user.\n4. The user submits an answer to the query.\n5. The query along with the given answer is used to formulate a new test case.\n6. This new test case is transferred back to the diagnosis engine and taken into account in prospective iterations. If the stop criterion (as per \u03c3, see above) is not met, another iteration starts at step 1. Otherwise, the solution KB K\u2217 constructed from the currently most probable minimal diagnosis is output.\nContributions of this Work. The contributions of this work are the following:\n\u2022 This work provides a thorough account of the subject and evolves the theory of interactive KB debugging (for monotonic KBs) by presupposing a reader to have only some basic knowledge of logic. Hence, this work addresses newbies as well as people already familiar with related topics. Whereas the comprehensive theoretical considerations might appeal to the more theoretically"}, {"heading": "14 CHAPTER 1. INTRODUCTION", "text": "oriented readers such as researchers, the precise and exhaustive description of all discussed algorithms might be interesting from the implementation point of view and might serve more practically oriented people such as programmers or engineers as an algorithmic cookbook. Further on, the extensive illustration of the way algorithms work by examples might also serve a merely superficially interested reader to just receive a rough impression of how KBs might be interactively debugged.\n\u2022 Except for basics in FOL and PL, this work is self-contained and provides all necessary definitions and proofs to make the topic of interactive KB debugging accessible to the reader.\n\u2022 To the best of our knowledge, this work provides the most comprehensive and detailed introduction to the field of interactive debugging of (monotonic) KBs. Our previous works on the topic [SFFR12, SF10, RSFF13, FS05, SFRF14c] are more application-oriented and thus abstract from some details and omit some of the proofs in favor of comprehensive evaluations of the presented strategies.\n\u2022 This is the first work that gives formal and precise definitions of problems dealt with in interactive KB debugging and introduces methods that provably solve these problems. We believe that precise problem statements are the very basis for all further scientific investigations in a field. Hence, we hope that this work can \u201copen\u201d the important subject of interactive KB debugging to a broader audience of interested researchers. This can lead to further progress and improvements in debugging techniques which we deem essential in the light of the growing number of intelligent applications incorporating KBs of growing size and complexity (keyword: The Semantic Web [BLHL+01]).\n\u2022 An in-depth discussion of query computation including computational complexity considerations together with an accentuation of potential ways of improving these methods is given. The investigated methods for query computation have been used also in [SFFR12, RSFF13, SF10, SFRF14c], but have not been addressed in depth in these works.\n\u2022 We are concerned with the discussion of different ways of exploiting diverse sources of meta information in the KB debugging process from which diagnosis probabilities can be extracted. Our previous works on this topic [SFFR12, RSFF13, SF10, SFRF14c] do not address this matter in a comparable depth.\n\u2022 We give a formal proof of the soundness of an algorithm QX (based on [Jun04]) for the detection of a minimal conflict set in a KB and we show the correctness (completeness, soundness, optimality) of a hitting set tree algorithm HS (based on [Rei87]) for finding minimal diagnoses in a KB in best-first order (i.e. most probable diagnoses first) which uses QX for conflict set computation only on-demand. We are not aware of any other work that comprises such proofs.\n\u2022 We establish the theoretical relationship between the widely-used notions of a conflict set and a justification. The former is i.a. used in [dKW87, Rei87, SFFR12, RSFF13] and the latter i.a. in [HPS08, HPS09, HPS10, Hor11, HBP11, HPS12b, SQJH08, Kal06, MS09, SSZ09, NRG12]. As a consequence, empirical results concerning the one might be translated to the other. For instance, since each minimal conflict set is an subset of a justification and there is an efficient (polynomial) method for computing a minimal conflict set given a superset of a minimal conflict set, a result manifesting the efficiency of justification computation for a set of KBs (e.g. [HPS12a]) implies the efficiency of conflict set computation for the same set of KBs. Moreover, we argue that minimal conflict sets are the better choice for our system since these put the focus of the debugger only on the smallest faulty subsets of the KB whereas justifications are better suited in scenarios where exact explanations for the presence of certain entailments are sought.\n\u2022 Two new algorithms for iterative (leading) diagnosis computation in interactive KB debugging are proposed. One that is guaranteed to reduce the number of remaining solutions after a query is\n15\nanswered and one that features more powerful pruning techniques than our previously published algorithms [SFFR12, RSFF13] (an evaluation that compares the overall efficiency of our previous algorithms with the ones proposed in this work must still be conducted and is part of our future research).\n\u2022 We suggest and extensively analyze different methods for the selection of an \u201coptimal\u201d query to ask the user out of a pool of possible queries. We compare a greedy \u201csplit-in-half\u201d strategy that proposes queries which eliminate half of the leading diagnoses with a strategy relying on information entropy [Sha48] that chooses the query with highest information gain based on some statistic or (a user\u2019s) beliefs about faults in the KB. Comprehensive experiments manifest that only an average guess of the fault information suffices to reduce the query answering effort for the interacting user, often to a significant extent, by means of the latter strategy compared to the former. Moreover, we demonstrate that both methods clearly outperform a random query selection strategy. The latter result witnesses that incorporation of meta (fault) information into the debugging process is in fact reasonable and might relieve the interacting user of a significant proportion of the effort required without taking into account any meta information.\n\u2022 Addressing the issue of choosing the suitable query selection method for some given fault information, we present a reinforcement learning query selection strategy. For, reliance upon a strategy (e.g. information entropy) that fully exploits and gains from the given fault information can speed up the debugging procedure in the normal case, but can also have a negative impact on the performance in the bad case where the actual solution diagnosis is rated as highly improbable. As an alternative, one might prefer to rely on a tool (e.g. \u201csplit-in-half\u201d) which does not consider any fault information at all. In this case, however, possibly well-chosen information cannot be exploited, resulting again in inefficient debugging actions.\nMinimal effort for the interacting user can be achieved if both the query selection method is chosen carefully and the provided fault information satisfies some minimum quality requirements. In particular, for deficient fault information and unfavorable strategy for query selection, we observe cases where the overhead in terms of user effort exceeds 2000% (!) in comparison to employing a more favorable query selection strategy. Since, unfortunately, assessment of the fault information is only possible a-poteriori (after the debugging session is finished and the correct solution is known), we devise a learning strategy (RIO) that continuously adapts its behavior depending on the performance achieved and in this vein minimizes the risk of using low-quality fault information.\nThis approach makes interactive debugging practical even in scenarios where reliable fault estimates are difficult to obtain. Evaluations provide evidence that for 100% of the cases in the hardest (from the debugging point of view) class of faulty test KBs, RIO performed at least as good as the best other strategy and in more than 70% of these cases it even manifested superior behavior to the best other strategy. Choosing RIO over other approaches can involve an improvement by the factor of up to 23, meaning that more than 95% of user time and effort might be saved per debugging session.\n\u2022 We come up with mechanisms for efficiently dealing with KB debugging problems involving high cardinality (minimal) diagnoses. In the standard interactive debugging approach described in the first parts of this work, the computation of queries is based on the generation of the set of most probable (or minimum cardinality) leading diagnoses. By this postulation, certain quality guarantees about the output solution can be given. However, we learn that dropping this requirement can bring about substantial savings in terms of time and especially space complexity of interactive debugging, in particular in debugging scenarios where faulty KBs are (partly) generated as a result of the application of automatic systems, e.g. KB (ontology) learning or matching systems [HSNM11, NB12, JMSK09, RP10, JRGZH12, Mei11]."}, {"heading": "16 CHAPTER 1. INTRODUCTION", "text": "To cope with such situations, we propose to base query computation on any set of leading diagnoses using a \u201cdirect\u201d method for diagnosis generation. Contrary to the standard method that exploits minimal conflict sets, this approach takes advantage of the duality between minimal diagnoses and minimal conflict sets and employs \u201cinverse\u201d algorithms to those used in the standard approach in order to determine minimal diagnoses directly from the DPI without the indirection via conflict sets.\nWe study the application of this direct method to high cardinality faults in KBs and find out that the number of required queries per debugging session is hardly affected for cases when the standard approach is also applicable. However, the direct method proves applicable and able to locate the correct solution diagnosis in situations when the standard approach (albeit one that not yet incorporates the powerful search tree pruning techniques introduced in this work) is not due to time or memory issues.\nOrganization of this Work. This work is subdivided into seven parts. Figure 1.3 illustrates the precedence constraints among the parts. We want to point out that Parts IV-VI correspond to works that have already been published and are thus self-contained, both from the notation and the content point of view. Parts I-III, on the contrary, are constructive and should thence be read in order.\n(Rest of) Part I. In Chapter 2, besides introducing the notation used in this work, we describe the requirements imposed on logical knowledge representation languages L that might be used with our approaches. It should be noted that the postulated properties do not restrict the applications of our approaches very much. For instance, these might be employed to resolve over-constrained constraint satisfaction problems (CSPs) or repair faulty KBs in PL, FOL, DL, Datalog or OWL. Since DL provides the logical underpinning of OWL which has recently received increasing attention due to the extensive research in the field of The Semantic Web [BLHL+01], we will also give a short introduction to DL. For, to underline the flexibility of the presented debugging systems in this work, we will illustrate how they work by means of examples involving PL, FOL as well as DL KBs.\nIn Chapter 3, we first give a formal definition of the KB debugging problem and define a diagnosis problem instance (DPI), the input of a KB debugger, and a solution KB, the output of a KB debugger. Further on, we formally characterize a diagnosis and give the notion of KB validity and what it means for a KB to be faulty. We discuss and prove relationships between these notions and specify properties a DPI must satisfy in order to be solvable by a KB debugger.\nWe motivate why it makes sense to focus on set-minimal diagnoses instead of all diagnoses, i.e. to stick to \u201cThe Principle of Parsimony\u201d [Rei87, BATJ91]. This results in the definition of the problem of\n17\nparsimonious KB debugging. Then, we prove that solving this problem is equivalent to the computation of a minimal diagnosis. Finally, we explain the benefits of using some background KB in (parsimonious) KB debugging.\nIn Chapter 4 we describe methods for diagnosis computation. To this end, we first introduce the notion of a (minimal) conflict set, discuss some properties of conflict sets related to the notion of KB validity and give sufficient and necessary criteria for the existence of non-trivial conflict sets w.r.t. a DPI. Subsequently, we derive the relationship between a conflict set and the notion of a justification (a minimal set of formulas necessary for a particular entailment to hold) which is well-known and frequently used, especially in the fields of DL, OWL and The Semantic Web [HPS08, HPS09, HPS10, Hor11, HBP11, HPS12a]. Concretely, we will demonstrate that a minimal conflict set is a subset of a justification for some negative test case or for some inconsistency (entailment false) or incoherency (entailment \u2200X1, . . . , Xk \u00acp(X1, . . . , Xk) for some predicate symbol p of arity k) of the given KB. Moreover, we will learn that, for the debugging tasks we consider, conflict sets are better suited than justifications.\nHaving deduced all relevant characteristics of (minimal) conflict sets, we proceed to give a description of a method (QX, Algorithm 1) due to [Jun04] which was originally presented as a method for finding preferred explanations (conflicts) in over-constrained CSPs, but can also be employed for an efficient computation of a minimal conflict set w.r.t. a DPI in KB debugging. We discuss and exemplify this algorithm in detail, prove its correctness as a routine for minimal conflict set computation and give complexity results.\nHaving at our disposal a proven sound method for generation of a minimal conflict set, we continue with the delineation of a hitting set tree algorithm similar to the one originally presented in [Rei87] which enables the computation of different minimal conflict sets by means of successive calls to QX, each time given an (adequately) modified DPI. In this manner, a hitting set tree can be constructed (breadth-first) which facilitates the computation of minimal diagnoses (minimum cardinality diagnoses first). We prove the correctness (termination, soundness, completeness, minimum-cardinality-first property) of this hitting set tree algorithm coupled with the QX method which serves to solve the problem of parsimonious KB debugging.\nIn order to be able to incorporate fault information into the diagnoses finding process, we deal with the induction of a probability space over diagnoses in Section 4.6. We discuss several ways of constructing a probability space including different sources of fault information. Hereinafter, we detail how diagnosis probabilities can be determined on the basis of some available fault information and how these can be appropriately updated after new observations (in terms of answered queries) have been made. Furthermore, we outline how fault probabilities can be appropriately incorporated into the hitting set search tree in order to guarantee the discovery of minimal diagnoses in best-first order, i.e. most probable ones first. Then, we prove the correctness (termination, soundness, completeness, best-first property) of this best-first diagnosis finding algorithm for parsimonious KB debugging.\nFinally, we describe a non-interactive KB debugging procedure (Algorithm 3) that relies on this bestfirst diagnosis finding algorithm. Some illustrating examples are provided which at the same time reveal significant shortcomings present in non-interactive KB debugging. This motivates the development of interactive KB debugging algorithms.\nReaders not theoretically inclined or non-interested in the technical details might well skip Sections 4.2, 4.4.2, 4.5.2 and 4.6 in Part I.\nPart II. In Chapter 6, we first discuss how disadvantages of non-interactive KB debugging procedures can be overcome by allowing a user to take part in the debugging process. Then, we define the problem of interactive static KB debugging as well as the problem of interactive dynamic KB debugging which \u201cnaturally\u201d arise from the fact that the DPI in interactive KB debugging is always renewed after a new test case has been specified (a new query has been answered). The former problem searches for a solution KB w.r.t. the DPI given as input such that this solution KB satisfies all test cases added during the debugging"}, {"heading": "18 CHAPTER 1. INTRODUCTION", "text": "session and there is no other such solution KB. The latter problem searches for a solution KB w.r.t. the current DPI (i.e. the input DPI including all new test cases added throughout the debugging session so far) such that there is no other solution KB w.r.t. the current DPI.\nNext, in Chapter 7, the central term of a query is specified which constitutes the medium for user interaction. Queries are generated from a set of leading diagnoses which is characterized thereafter. The set of leading diagnoses is uniquely partitioned into three subsets by each query. The tuple including these subsets is called q-partition. Subsequently, the reader is given some explanations how the q-partition can be interpreted, and how it relates to a query. In fact, we will prove that the notion of a q-partition can serve as a criterion for checking whether a set of logical formulas is a query or not. After that, we will learn that a query exists for any set of (at least two) leading diagnoses which grants that the presented algorithms will definitely be able to come up with a query without the need to impose any restrictions on which (minimal) diagnoses are computed by the diagnosis engine in each iteration.\nChapter 8 shows a method for the generation of (a pool of) set-minimal queries (Algorithm 4) aiming at stressing the interacting user as sparsely as possible, features in-depth discussions of this method\u2019s properties, proves its correctness, provides complexity results and gives some illustrating examples. Further on, drawbacks of this method are pointed out and possible solutions are discussed.\nSubsequently, Chapter 9 deals with the presentation of the central algorithm of this work which implements an interactive KB debugging system (Algorithm 5). First, an overview of the workflow of interactive KB debugging is given, followed by a more comprehensive detailed specification of the algorithm. Some query selection measures are discussed [RSFF13, SFFR12] and optimization versions of the problems of interactive dynamic and static KB debugging are defined where the goal is to obtain the solution to these problems by asking the user a minimal number of queries. Finally, we prove the correctness of the interactive KB debugging algorithm and provide a discussion of its complexity.\nNon-theoretically-oriented readers might well skip Sections 8.2, 8.4, 8.5, 8.7 and 9.4 in Part II. Moreover, for the superficially interested reader, it may suffice to concentrate only on Chapter 6 and Sections 7.1, 7.2 and 9.1 in Part II.\nPart III. Here, we go into detail w.r.t. the two strategies for iterative diagnoses computation introduced in Part II that might be plugged into Algorithm 5 to solve either the interactive static or dynamic KB debugging problem.\nChapter 11 describes the static method and proves its soundness and completeness w.r.t. the computation of minimal diagnoses w.r.t. the DPI given as an input to the interactive KB debugging algorithm and its optimality w.r.t. the discovery of minimal diagnoses in best-first order (most-probable or minimum cardinality diagnoses first). Incorporation of the static method as a routine for leading diagnosis computation into Algorithm 5 provably solves the problem of interactive static KB debugging.\nChapter 12 details the dynamic method and proves its soundness and completeness w.r.t. the computation of minimal diagnoses w.r.t. the current DPI and its optimality w.r.t. the discovery of minimal diagnoses in best-first order (most-probable or minimum cardinality diagnoses first). Employing the dynamic method as a routine for leading diagnosis computation in Algorithm 5 provably solves the problem of interactive dynamic KB debugging.\nThe practically oriented reader or the one that is willing to believe that the presented iterative diagnosis computation techniques in fact work as claimed might skip Sections 11.4 as well as 12.4 in Part III.\nPart IV. In this part, we suggest and extensively analyze different methods for the selection of an \u201coptimal\u201d query (see above). The material dealt with in Part IV is based on the publications [SFFR12, SF10] where the former was published in the journal Web Semantics: Science, Services and Agents on the World Wide Web and the latter in the Proceedings of the 9th International Semantic Web Conference (ISWC 2010).\n19\nPart V. The reinforcement learning query selection strategy (RIO) that makes the presented debugging system robust against the usage of low-quality fault information is presented and thoroughly analyzed in this part which is based on the works [RSFF13, RSFF12, RSFF11, SRF11] published in Web Reasoning and Rule Systems (RR-2013), in the Proceedings of the 7th International Workshop on Ontology Matching (OM-2012), in the Proceedings of the Joint Workshop on Knowledge Evolution and Ontology Dynamics 2011 (EvoDyn2011) and in DX 2011 - 22nd International Workshop on Principles of Diagnosis, respectively.\nPart VI. This part covers the topic of efficiently dealing with KB debugging problems involving high cardinality faults (see above) and relies on material presented in [SFRF14c, SFRF14a, SFRF14b] and published in the Proceedings of the 21st European Conference on Artificial Intelligence (ECAI 2014), in DX 2014 - 25th International Workshop on Principles of Diagnosis and in the Proceedings of the Third International Workshop on Debugging Ontologies and Ontology Mappings (WoDOOM14), respectively.8\nPart VII. To round this work off, we provide a discussion of related work in Chapter 32,9 summarize the contributions of this work in Chapter 33 and deal with our future work topics in Chapter 34.\n8We are glad to report that the publication [SFRF14a] was awarded the Best Paper Award at the DX Workshop that took place in Graz, Austria, in September 2014 (see http://dx-2014.ist.tugraz.at).\n9Note that related work specific to topics addressed in Parts IV-VI is separately treated in these parts.\nChapter 2\nPreliminaries"}, {"heading": "2.1 Assumptions", "text": "The techniques described in this work are applicable for any logical knowledge representation formalism L for which the entailment relation is\n1. monotonic: is given when adding a new logical formula to a KB KL cannot invalidate any entailments of the KB, i.e. KL |= \u03b1L implies that KL \u222a {\u03b2L} |= \u03b1L,\n2. idempotent: is given when adding implicit knowledge explicitly to a KB KL does not yield new entailments of the KB, i.e. KL |= \u03b1L and KL \u222a {\u03b1L} |= \u03b2L implies KL |= \u03b2L and\n3. extensive: is given when each logical formula entails itself, i.e. {\u03b1L} |= \u03b1L for all \u03b1L,\nand for which\n4. reasoning procedures for deciding consistency and calculating logical entailments of a KB are available,\nwhere \u03b1L, \u03b2L are logical formulas and KL is a set { ax (1) L , . . . , ax (n) L } of logical formulas formulated\nover the language L. KL is to be understood as the conjunction \u2227n i=1 ax (i) L . Notice that the elements of a KB are called quite differently in literature. Possible denotations are logical formula (e.g. [KK06]), well-formed formula (e.g. [CL73]), (logical) sentence or axiom (e.g. [RN10]) and axiom (in most of the description logic literature, e.g. [BCM+07]). We will mainly stick to the term formula (sometimes axiom) to refer to the elements of a KB. As the logic will be clear from the context in the sequel, we will omit the index L when referring to formulas or KBs over L throughout the rest of this work."}, {"heading": "2.2 Considered Logics", "text": "To underline the general character of this work, we will illustrate our approaches using example diagnosis problem instances expressed in different logical languages. In this section we give notational remarks concerning these different logics used, namely propositional logic (PL), first-order logic (FOL) as well as description logic (DL). Whereas we assume the reader to be familiar with FOL and PL (a good introduction to PL and FOL can be found in [CL73]), we will give a short introduction to DL.\nRemark 2.1 It is important to notice that the usage of DL as well as FOL examples throughout this work should not suggest that the Properties 1 \u2013 4 stated above are satisfied for any DL or FOL language\n21"}, {"heading": "22 CHAPTER 2. PRELIMINARIES", "text": "L. In fact, it is well-known by the theorems of Church and Turing (cf. [Men09]; the original works are [Chu36, Tur37]) that FOL is not decidable in general, i.e. Property 4 above is not met. Also in the case of DL, which subsumes a range of different logical languages featuring different expressivity and thus different computational complexity of reasoning procedures, there are languages which are undecidable. For instance, a DL language allowing the formalism of equality role-value-maps which facilitates the expression of concepts like \u201cpersons whose co-workers coincide with their relatives\u201d can be proven undecidable [BCM+07, SS89].\nProperty 4 is satisfied, for example, for the DL language SROIQ which is the logical underpinning of OWL 2 [GHM+08]. However, the complexity (2-NEXPTIME-complete [Kaz08]) of logical reasoning is intractable in the worst case for this language which implies the intractability of our methods in the worst case. Nevertheless, other DL languages applied with similar systems as those described in this paper have been showing reasonable performance [SFRF14c, RSFF13, SFFR12]. Also from the theoretical point of view, there are DL languages that allow for efficient reasoning. One example is the OWL 2 EL profile which enables polynomial time reasoning [BBL05]. For this language, the efficient reasoning service ELK has been presented by [KKS14]. For FOL, datalog is an example of a decidable sublanguage where reasoning is efficient [RN10]. Further, restricted sublanguages of FOL can often be translated to some DL language wherefore DL positive results concerning the decidability of reasoning as well as complexity results can be adopted for these restricted FOL languages [BCM+07, chapter 4] [Bor96].\nMoreover, we want to point out that the practical efficiency of our systems depends strongly on the practical performance (which might be by far better than suggested by the worst case reasoning complexities) of the reasoning services called by our algorithms since the reasoning services are used as a black-box (as mentioned in Chapter 1). Possible strategies for improving the reasoning efficiency in the black-box setting are briefly discussed in Chapter 34.\nOntologies and The Semantic Web Ontologies are KBs that formally and explicitly represent common knowledge about a domain in the form of individuals, concepts (set of individuals) and roles (binary relationships between individuals). As, in the last decade, extensive research has been done in the area of The Semantic Web [BLHL+01] making (automatic) ontology development tools and reasoning services more efficient, ontology engineering for the Semantic Web is on the upswing. The Semantic Web aims at the enrichment of unstructured information on the web by semantic meta data which should facilitate the usage of the web as structured database of knowledge of all kinds where computers are able to \u201cunderstand\u201d this structured data, establish relationships between different data sources, combine information from different data sources and (most essentially) derive new (implicit) knowledge from the structured data. At this, ontologies are the key to a common vocabulary used for the semantic meta data. Ontologies are employed to precisely define the meaning of different terms, state relationships between different terms and to introduce new terms by means of already specified ones.\nThe constantly increasing number of people creating ontologies of increasing size (examples were given in Chapter 1) results in more and more (faulty) ontologies which constitute useful application scenarios and test cases for our approaches. For that reason, we also want to use ontology engineering for The Semantic Web as a concrete use case for the presented work. The standard knowledge representation formalism for ontologies is OWL 2 [MPSP09, GHM+08] which relies on DL. A short introduction to DL is given next.\nDescription Logic\nDescription Logic (DL) [BCM+07] is a family of knowledge representation languages with a formal logic-based semantics that are designed to represent knowledge about a domain in form of concept de-"}, {"heading": "2.2. CONSIDERED LOGICS 23", "text": "scriptions. The syntax of a description language L is defined by its signature and a set of constructors. The signature of L corresponds to the union of possibly disjoint setsNC ,NR andNI , whereNC contains all concept names (unary predicates), NR comprises all role names (binary predicates) and NI is the set of all individuals (constants) in L. Each concept and role description can be either atomic or complex. The latter ones are composed using constructors defined in the particular language L. A typical set of DL constructors for complex concepts includes conjunction A u B, disjunction A t B, negation \u00acA, existential \u2203r.A and value \u2200r.A restrictions, where A,B are concept descriptions and r \u2208 NR.\nAxioms are statements of knowledge that must be true in a domain. An ontology K is defined as a tuple (T ,A), where T (TBox) is a set of terminological axioms andA (ABox) a set of assertional axioms. Each TBox axiom is expressed by a general concept inclusion A v B, a form of logical implication, or by a definition A \u2261 B, a kind of logical equivalence, where A and B are concept descriptions or role descriptions. ABox axioms are used to assert properties of individuals in terms of the vocabulary defined in the TBox, e.g. concept A(x) or role r(x, y) assertions, where A is a concept description, r a role description, and x, y \u2208 NI .\nThe semantics of a description language is given in terms of interpretations I = (\u2206I , \u00b7I) consisting of a non-empty domain \u2206I and a function \u00b7I that assigns to every atomic concept A \u2208 NC a set AI \u2286 \u2206I , to every atomic role r \u2208 NR a set rI \u2286 \u2206I \u00d7 \u2206I and to every individual x \u2208 NI some value xI \u2208 \u2206I . The interpretation function is extended to complex concept descriptions by the following inductive definitions:\n>I = \u2206I\n\u22a5I = \u2205 (A uB)I = AI \u2229BI\n(A tB)I = AI \u222aBI\n(\u00acA)I = \u2206I \\AI (\u2203r.A)I = { x \u2208 \u2206I | \u2203y. (x, y) \u2208 rI \u2227 y \u2208 AI } (\u2200r.A)I = { x \u2208 \u2206I | \u2200y. (x, y) \u2208 rI \u2192 y \u2208 AI\n} where > and \u22a5 are predefined concepts; the former is the universal concept and the latter the bottom concept.\nThe semantics of axioms is defined as follows for (1) TBox and (2) ABox axioms: (1) Interpretation I satisfies A v B iff AI \u2286 BI and it satisfies A \u2261 B iff AI = BI . (2) A(x) is satisfied by I iff xI \u2208 AI and r(x, y) is satisfied iff (xI , yI) \u2208 rI . An interpretation I is a model of K = (T ,A) iff it satisfies all TBox axioms in T and all ABox axioms in A. An ontology K is consistent iff it has a model. A concept A (role r) is satisfiable w.r.t K iff there is a model I of K with AI 6= \u2205 (rI 6= \u2205). An ontology K is coherent iff all concepts and roles occurring in K are satisfiable. An axiom \u03b1 is entailed by K iff \u03b1 is true in all models I of K. For a set of axioms X we write K |= X as a shorthand for K |= \u03b1 for all \u03b1 \u2208 X .\nUsually description logic systems provide sound and complete reasoning services to their users. Besides verification of coherency and consistency ofK and satisfiability checking of concepts, reasoner tasks include classification and realization. Classification determines, for each concept name A occurring in K, most specific (general) concepts that subsume (are subsumed by) A. A concept A subsumes (is subsumed by) a concept B iff K |= B v A (K |= A v B). Classification is employed to build a taxonomy of concepts in K. Realization, given an individual name x occurring in K and a given set of concepts in K (usually all concepts in K), computes the most specific concepts A1, . . . , An from the set such that K |= Ai(x) for all i = 1, . . . , n. The most specific concepts are those that are minimal w.r.t. the\n24 CHAPTER 2. PRELIMINARIES\nsubsumption ordering v.\nExample 2.1 The example KB given in the Introduction (Chapter 1) can be equivalently represented in DL (cf. Remark 2.1) as follows:\nRes \u2261 \u2200writes.Paper (2.1) \u2203writes.> v Res (2.2)\nSecr v Gen (2.3) Gen v \u00acRes (2.4) Secr(pam) (2.5)\nwhere Res is the concept symbol with equivalent meaning as the predicate symbol res, the role symbol writes corresponds to the equally named binary predicate, Paper to paper, and so on. Notice that axiom 2.2 states that the domain of writes is Res.\n2.3 Notational Remarks10\nGeneral Notational Conventions. Throughout this work, the nomenclature given by Table 2.1 is used (many of the designators in the table will be explained later in this work). We will mainly refer to an ontology by the term KB.\nIn order to make a clear distinction between scalars and functions, we denote all scalars g by g and all functions g by g(). If an ordered list occurs in a set operation, then this list is interpreted as a (nonordered) set. For example, let L := [1, 3, 4, 2] be an ordered list; then L\u2229{1, 2, 3} yields the set {1, 2, 3}.\nNotational Convention for PL (cf. [RN10]). We use uppercase letters A,B, . . . to denote atoms and the standard logical connectives to build PL formulas from atoms. The operator precedence we use is \u00ac, \u2227, \u2228,\u2192,\u2194, from highest to lowest. Given a PL KBK and a PL formula ax , we call K\u0303 and a\u0303x the signature of K and the signature of ax , respectively. The former comprises all atoms occurring in K and the latter all atoms occurring in ax .\nNotational Convention for FOL (cf. [CGT89]). Variables are denoted by uppercase letters; constants and predicate symbols are denoted by strings beginning with a lowercase letter11. Recalling the example KB given in Chapter 1, X,Y are variables, pam is a constant and res, writes, paper, secr and gen are predicate symbols. FOL formulas are built from the standard logical connectives described for PL above. The operator precedence we use for FOL formulas is the same as stated above12. The precedence of quantifiers \u2200, \u2203 is such that a quantifier outside of any parenthesized expression holds over everything to the right of it; if occurring in a parenthesized expression, a quantifier holds over everything to the right of it within this expression. For example, \u2200Xprof (X)\u2192 \u2203Y secr(Y ) is equivalent to (\u2200X(prof (X)\u2192 (\u2203Y (secr(Y ))))) (i.e. \u201cfor each professor there is at least one secretary\u201d) and not to (\u2200Xprof (X)) \u2192 \u2203Y secr(Y ) (i.e. \u201cif everybody is a professor, then there is at least one secretary\u201d).\nGiven a FOL KB K and a FOL formula ax , we call K\u0303 and a\u0303x the signature of K and the signature of ax , respectively. The former comprises all predicate, function and constant symbols occurring in K and the latter all predicate, function and constant symbols occurring in ax . The signature of the example KB given in Chapter 1 is {res, writes, paper, secr, gen, pam} and the signature of formula 1.2 of this KB is {writes, res}.\n10These conventions apply to Parts I-III and Part VII. Each of the Parts IV-VI is self-contained w.r.t. the used notation. 11We do not use any function symbols throughout this work. 12We do not use equality = in FOL formulas throughout this work."}, {"heading": "2.3. NOTATIONAL REMARKS 25", "text": "Remark 2.2 By analogy with the definition of coherency in DL (see Section 2.2), we call a FOL KB K incoherent iff K |= \u2200X1, . . . , Xk \u00acp(X1, . . . , Xk) for some k-place predicate symbol p in the signature of K where k \u2265 1.\nRemark 2.3 We want to point out that whenever we will speak of entailment computation we address the invocation of a sound reasoning service that is guaranteed to terminate after finite execution time and returns a finite number of entailments for any KB given as input (cf. Remark 2.1). Similarly, when we say that all entailments of a KB are computed, we always refer to a finite set of entailments of certain types output by such a reasoning service. Examples of such entailment types regarding DL are the (a) classification and (b) realization entailments, by which we mean (a) all the subsumption relationships between concept names appearing in the KB, i.e. entailments of the form C1 v C2 for concept names C1, C2 \u2208 K\u0303 and (b) all the concept names instantiated by a given individual for all individuals appearing in the KB, i.e. entailments of the form C(a) for concepts names C \u2208 K\u0303 and individual names a \u2208 K\u0303.\n26 CHAPTER 2. PRELIMINARIES\nChapter 3\nKnowledge Base Debugging\nKB debugging can be seen as a test-driven procedure comparable to test-driven software development and debugging, where test cases are specified to restrict the possible faults until the user detects the actual fault manually or there is only one (highly probable) fault remaining which is in line with the specified test cases. In this chapter, we want to study the theory of (non-interactive) KB debugging, present and discuss mechanisms that can be employed for the debugging of KBs and reveal drawbacks of such systems. In (non-interactive) KB debugging we assume test cases fixed during the debugging procedure. That is, a user might specify a set of test cases offline, run a debugging system and investigate the output solution(s). In case no satisfactory solution has been returned, some additional test cases might be defined offline before the debugger might be invoked again.\nThe inputs to a KB debugging problem can be characterized as follows: Given is a KB K and a KB B (background knowledge), both formulated over some logic L complying with the conditions 1 \u2013 4 given in Chapter 2. All formulas in B are considered to be correct and all formulas in K are considered potentially faulty. K \u222a B does not meet postulated requirements R where {consistency} \u2286 R \u2286 {coherency, consistency} or does not feature desired semantic properties, called test cases.13 Positive test cases (aggregated in the set P ) correspond to desired entailments and negative test cases (N ) represent undesired entailments of the correct (repaired) KB (along with the background KB B). Each test case p \u2208 P and n \u2208 N is a set of logical formulas over L. The meaning of a positive test case p \u2208 P is that the correct KB integrated with B must entail each formula (or the conjunction of formulas) in p, whereas a negative test case n \u2208 N signalizes that some formula (or the conjunction of formulas) in n must not be entailed by the correct KB integrated with B.\nRemark 3.1 In the sequel, we will write K |= X for some set of formulas X to denote that K |= ax for all ax \u2208 X and K 6|= X to state that K 6|= ax for some ax \u2208 X .\nThe described inputs to the KB debugging problem are captured by the notion of a diagnosis problem instance:\nDefinition 3.1 (Diagnosis Problem Instance). Let\n\u2022 K be a KB over L,\n\u2022 P ,N sets including sets of formulas over L,\n\u2022 {consistency} \u2286 R \u2286 {coherency, consistency}, 13We assume consistency a minimal requirement to a solution KB provided by a debugging system, as inconsistency makes a KB\ncompletely useless from the semantic point of view.\n27"}, {"heading": "28 CHAPTER 3. KB DEBUGGING", "text": "\u2022 B be a KB over L such that K \u2229 B = \u2205 and B satisfies all requirements r \u2208 R,\n\u2022 the cardinality of all sets K, B, P , N be finite.\nThen we call the tuple \u3008K,B,P ,N \u3009R a diagnosis problem instance (DPI) over L.14\nNote that, for now, we do not make any assumptions about the contents of the sets K, B, P and N that go beyond Definition 3.1. So, it might be well the case, for example, to specify a DPI according to Definition 3.1 for which there are no solutions or for which only trivial solutions exist. Later on, we will discuss properties a DPI must fulfill to guarantee existence of solutions for it.\nWe define a solution KB for a DPI as follows:\nDefinition 3.2 (Solution KB). Let \u3008K,B,P ,N \u3009R be a DPI. Then a KB K\u2217 is called solution KB w.r.t. \u3008K,B,P ,N \u3009R, written as K\u2217 \u2208 Sol\u3008K,B,P,N \u3009R , iff all the following conditions hold:\n\u2200 r \u2208 R : K\u2217 \u222a B fulfills r (3.1) \u2200 p \u2208 P : K\u2217 \u222a B |= p (3.2) \u2200n \u2208 N : K\u2217 \u222a B 6|= n. (3.3)\nA solution KB K\u2217 w.r.t. a DPI is called maximal, written as K\u2217 \u2208 Solmax\u3008K,B,P,N \u3009R , iff there is no solution KB K\u2032 such that K\u2032 \u2229 K \u2283 K\u2217 \u2229 K.\nNow, the problem of KB debugging can be formalized:\nProblem Definition 3.1 (KB Debugging). Given a DPI \u3008K,B,P ,N \u3009R, find a solution KB w.r.t. \u3008K,B,P ,N \u3009R.\nNote that basically any KBK\u2217 that meets conditions (3.1) - (3.3) is a solution KB in the sense of Definition 3.2. Hence, K\u2217 does not even need to have a non-empty intersection with K. Only the postulation of maximality of a solution KB (as detailed later in Section 3.1) establishes a relationship to the given KB K.\nRemark 3.2 Let K\u2032 := K \u222a B \u222a UP . Then, conditions (3.1) - (3.3) can be reduced to conditions (3.2) and (3.3) if\n\u2022 N := N \u222a {{false}} given R = {consistency} or\n\u2022 N := N \u222a {{\u2200X1, . . . , Xk p(X1, . . . , Xk)\u2192 false} | p is k-place predicate symbol in K\u0303\u2032, k \u2265 1} \u222a {{false}} in case R = {consistency, coherency}.\nThis holds because a KBK is inconsistent iffK |= {false} andK is incoherent iff some predicate symbol in K\u2032 must be false for any instantiation. Notice that the latter must hold for all predicate symbols in K\u2032 and not only in K (see Example 3.1). For PL and DL, the definitions of N are analogous (cf. Chapter 2), but for PL coherency is not defined wherefore only the first bullet is relevant for PL. In what follows we will stick to the more explicit characterization of a solution KB given by Definition 3.2.\n14 In the following we will often call a DPI over L simply a DPI for brevity and since the concrete logic will not be relevant in our theoretical analyses as long as it is compliant with the conditions 1 \u2013 4 given in Chapter 2. Nevertheless we will mean exactly the logic over which a particular DPI is defined when we use the designator L.\n29\nExample 3.1 Let a DL DPI be defined as\nK := {B v C} B := {A v B,C v \u00acA} P := \u2205 N := \u2205 R := {coherency, consistency}\nThen, K\u0303 = {B,C}, but there is some concept A /\u2208 K\u0303, but A \u2208 K\u0303\u2032, which is unsatisfiable w.r.t. K \u222a B. Since we want a solution KB integrated with B to meet the conditions (3.1) - (3.3),K is not a solution KB w.r.t. \u3008K,B,P ,N \u3009R despite the fact that it is perfectly consistent and coherent as an isolated KB.\nWhereas the definition of a solution KB refers to the desired properties of the output of a KB debugging system, the following definition can be seen as a characterization of KBs provided as an input to a KB debugger. If a KB is valid w.r.t. the background knowledge, the requirements and the test cases, then finding a solution KB w.r.t. the DPI is trivial. Otherwise, obtaining a solution KB from it involves modification of the input KB and subsequent addition of suitable formulas. Usually, the KB K part of the DPI given as an input to a debugger is assumed to be invalid w.r.t. this DPI.\nDefinition 3.3 (Valid KB). Let \u3008K,B,P ,N \u3009R be a DPI. Then, we say that a KB K\u2032 is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R iff K\u2032 \u222a B \u222a UP does not violate any r \u2208 R and does not entail any n \u2208 N . A KB is said to be invalid (or faulty) w.r.t. \u3008\u00b7,B,P ,N \u3009R iff it is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.15\nIntuitively, if a KB K is faulty w.r.t. \u3008\u00b7,B,P ,N \u3009R, then there is at least one incorrect formula in K that needs to be corrected or deleted; if a KB K is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, a solution KB can be directly obtained by simply extending K by the set UP of all sentences comprised in positive test cases. Note, however, that K being valid w.r.t. \u3008\u00b7,B,P ,N \u3009R does not necessarily mean that K \u222a B entails any p \u2208 P .\nProposition 3.1. Let \u3008K,B,P ,N \u3009R be a DPI. Then, K\u2032 \u222a UP \u2208 Sol\u3008K,B,P,N \u3009R iff K\u2032 is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nProof. \u201c\u21d2\u201d: If K\u2032 \u222a UP is a solution KB, then K\u2032 \u222a UP \u222a B meets all r \u2208 R as per condition (3.1) and does not entail any n \u2208 N as per condition (3.3). Hence, K\u2032 is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\n\u201c\u21d0\u201d: IfK\u2032 is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, then (K\u2032\u222aUP )\u222aB meets all r \u2208 R, i.e. meets condition (3.1). Moreover, (K\u2032 \u222aUP )\u222aB 6|= n for all n \u2208 N , i.e. (K\u2032 \u222aUP )\u222aB meets condition (3.3). By extensiveness of the used language L, (K\u2032\u222aUP )\u222aB |= p for all p \u2208 P , i.e. condition (3.2) is fulfilled by (K\u2032\u222aUP )\u222aB. Thus, K\u2032 \u222a UP is a solution KB.\nDefinition 3.4 (Extension). Let \u3008K,B,P ,N \u3009R be a DPI over L and K\u2032 \u2286 K. A set of formulas E over L is called an extension w.r.t. K\u2032 and \u3008K,B,P ,N \u3009R, written as E \u2208 EX(K\u2032)\u3008K,B,P,N \u3009R , iff (K \\ K\u2032) \u222a E is a solution KB w.r.t. \u3008K,B,P ,N \u3009R.\nDefinition 3.5 (Diagnosis). Let \u3008K,B,P ,N \u3009R be a DPI. A set of formulas D \u2286 K is called a diagnosis w.r.t. \u3008K,B,P ,N \u3009R, written as D \u2208 aD\u3008K,B,P,N \u3009R , iff there exists some E \u2208 EX(D)\u3008K,B,P,N \u3009R , i.e. (K \\ D) \u222a E is a solution KB w.r.t. \u3008K,B,P ,N \u3009R.\nA diagnosisD w.r.t. \u3008K,B,P ,N \u3009R is minimal, written asD \u2208mD\u3008K,B,P,N \u3009R , iff there is noD\u2032 \u2282 D such that D\u2032 is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. A diagnosis D w.r.t. \u3008K,B,P ,N \u3009R is a minimum cardinality diagnosis w.r.t. \u3008K,B,P ,N \u3009R iff there is no diagnosisD\u2032 w.r.t. \u3008K,B,P ,N \u3009R such that |D\u2032| < |D|.\n15 It would be more precise to call a KB valid w.r.t. the elements B, P , N , R of a DPI. Though, for brevity, we stick to the presented notation where the dot \u00b7 in \u3008\u00b7,B,P ,N \u3009R signalizes the irrelevance of the first element K of a DPI \u3008K,B,P ,N \u3009R for determining validity of a KB K\u2032 w.r.t. this DPI."}, {"heading": "30 CHAPTER 3. KB DEBUGGING", "text": "Proposition 3.2. Let \u3008K,B,P ,N \u3009R be a DPI. Then, D \u2208 aD\u3008K,B,P,N \u3009R iff K \\ D is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nProof. \u201c\u21d2\u201d: If D is a diagnosis w.r.t. \u3008K,B,P , N \u3009R, there is some extension E w.r.t. D and \u3008K,B,P , N \u3009R which implies that (K \\ D) \u222a E is a solution KB w.r.t. \u3008K,B,P ,N \u3009R. Now, assume that K \\ D is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R. By Proposition 3.1, this means that (K \\ D) \u222a UP is not a solution KB. Hence, (K \\ D) \u222a UP \u222a B violates some r \u2208 R or entails some n \u2208 N . As (K \\ D) \u222a E is a solution KB, we have that (K \\ D) \u222a E \u222a B |= p for all p \u2208 P . So, by idempotency of L, (K \\ D) \u222a E \u222a B \u2261 (K \\ D) \u222a E \u222a B \u222a UP \u2287 (K \\ D) \u222a UP \u222a B which violates some r \u2208 R or entails some n \u2208 N . By monotonicity of L, (K\\D)\u222aE \u222aB also violates some r \u2208 R or entails some n \u2208 N whereby (K\\D)\u222aE is not a solution KB which is a contradiction.\n\u201c\u21d0\u201d: IfK\\D is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, then (K\\D)\u222aB\u222aUP does not violate any r \u2208 R and does not entail any n \u2208 N . Since (K\\D)\u222aB\u222aUP also entails each positive test case p \u2208 P by extensiveness of L, we can conclude that (K \\ D) \u222a UP is a solution KB. By Definition 3.4, UP \u2208 EX(D)\u3008K,B,P,N \u3009R and thus D is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nIn other words, D is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R iff (K \\ D) \u222a B meets all requirements, i.e. consistency and/or coherency, as per condition (3.1), does not entail any negative test cases as per condition (3.3), and the positive test cases p \u2208 P can be added to (K \\ D) \u222a B without violating any of the conditions (3.1) or (3.3).\nFrom a given DPI \u3008K,B,P ,N \u3009R, a solution KB K\u2217 can be obtained by a deletion and an expansion step. The deletion step involves the elimination of a diagnosis D \u2286 K from K. Note that, due to monotonicity of L, only deletion (and not expansion) of the KB can effectuate a repair of inconsistencies, incoherencies and unwanted entailments. Note, if K is already valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, then D can be set to \u2205 and the deletion step can be omitted. The expansion step aims at the fulfillment of positive test cases P , i.e. condition (3.2), which is not necessarily the case after the deletion step. In fact, some new logical sentences E \u2208 EX(D)\u3008K,B,P,N \u3009R may need to be added to (K \\D) \u222a B to grant entailment of all positive test cases.\nCorollary 3.1. Let D be a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. Then there is a set of logical sentences E \u2208 EX(D)\u3008K,B,P,N \u3009R over L such that:\n\u2200 r \u2208 R : (K \\ D) \u222a E \u222a B fulfills r \u2200 p \u2208 P : (K \\ D) \u222a E \u222a B |= p \u2200n \u2208 N : (K \\ D) \u222a E \u222a B 6|= n.\nProof. The proposition of the corollary is a direct consequence of Definition 3.2 and Definition 3.5.\nFrom the point of view of a solution KB K\u2217 w.r.t. \u3008K,B,P ,N \u3009R, K \\ K\u2217 is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R and K\u2217 \\ K is one possible extension w.r.t. D and \u3008K,B,P ,N \u3009R.\nProposition 3.3. For each solution KB K\u2217 w.r.t. \u3008K,B,P ,N \u3009R there is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R and an extension E w.r.t. D and \u3008K,B,P ,N \u3009R such that K\u2217 = (K \\ D) \u222a E and E \u2229 D = \u2205.\nProof. LetK\u2217 be a solution KB w.r.t. \u3008K,B,P ,N \u3009R. ThenK\u2217 can be written asK\u2217 = (K\u2229K\u2217)\u222a (K\u2217 \\ K) = (K \\ (K \\ K\u2217)) \u222a (K\u2217 \\ K). Let K \\ K\u2217 =: D and K\u2217 \\ K =: E , then E \u2229 D = \u2205. Further on, D \u2286 K holds and E is a set of logical sentences such thatK\u2217 = (K\\D)\u222aE \u2208 Sol\u3008K,B,P,N \u3009R . Therefore, D \u2208 aD\u3008K,B,P,N \u3009R and E \u2208 EX(D)\u3008K,B,P,N \u3009R .\nCorollary 3.2. The (non-)existence of a diagnosis w.r.t. \u3008K,B,P ,N \u3009R is equivalent to the (non-)existence of a solution KB w.r.t. \u3008K,B,P ,N \u3009R.\n31\nProof. Proposition 3.3 shows that there is a diagnosis for each solution KB. By Definition 3.5, there is also a solution KB for each diagnosis.\nThe next Proposition gives sufficient and necessary criteria for the existence of a solution, i.e. a diagnosis or a solution KB, respectively, for a given DPI.\nProposition 3.4. Let \u3008K,B,P ,N \u3009R be a DPI. Then, a diagnosis D w.r.t. \u3008K,B,P ,N \u3009R exists iff\n\u2022 \u2200 r \u2208 R : B \u222a UP fulfills r and\n\u2022 \u2200n \u2208 N : B \u222a UP 6|= n .\nProof. \u201c\u21d0\u201d: Let us define D := K. Then X := (K \\ D) \u222a B \u222a UP = B \u222a UP . Consequently, X satisfies each r \u2208 R as per condition (3.1), X 6|= n for each n \u2208 N as per condition (3.3), and finally X |= p for each p \u2208 P by extensiveness of L and thus meets condition (3.2). So, X is a solution KB w.r.t. \u3008K,B,P ,N \u3009R wherefore D must be a diagnosis.\n\u201c\u21d2\u201d: Let D \u2286 K be some diagnosis w.r.t. \u3008K,B,P ,N \u3009R. Then, by definition of a diagnosis, there is some solution KB K\u2217 w.r.t. \u3008K,B,P ,N \u3009R. Then K\u2217 \u222a B |= p for all p \u2208 P by condition (3.2), which implies that K\u2217 \u222a B \u222a UP does not feature any new entailments compared to K\u2217 \u222a B by idempotency of L. So, K\u2217 \u222a B \u2261 K\u2217 \u222a B \u222a UP holds. Now, for arbitrary n \u2208 N , since K\u2217 \u222a B 6|= n we have that K\u2217 \u222a B \u222a UP 6|= n , and, by monotonicity of L, that B \u222a UP 6|= n . Analogously, for any r \u2208 R, because K\u2217 \u222a B satisfies r, it must be true that K\u2217 \u222a B \u222a UP satisfies r and, by monotonicity of L, that B \u222a UP satisfies r.\nDefinition 3.6 (Admissible DPI). We call a DPI \u3008K,B,P ,N \u3009R admissible iff there is at least one diagnosis D \u2208 aD\u3008K,B,P,N \u3009R .\nA non-admissible DPI may arise in a situation where a user specifies test cases manually. For this procedure a similar error-proneness as for the user\u2019s formulation of KB formulas can be assumed. And there are lots of pitfalls to escape, as Proposition 3.4 shows. In particular, the specified test cases in P and N must be \u201ccompatible\u201d with each other, i.e. positive test cases must not contradict negative ones. For example, adding p1 := {A v C,E \u2261 B} and p2 := {C v E} to P and n1 := {A v B} to N leads to a contradiction between P and N and consequently to the non-admissibility of a DPI comprising P and N . Furthermore, the background KB B which is considered as correct, must indeed be correct, at least in terms of R; and negative test cases must be specified in a way not to postulate non-entailment of knowledge specified in B. A counterexample is B := {\u2203r.> v A, r(x, y), A v C} and N := {{C(x)}}. And third, the union of positive test cases together with B must be in compliance with R, particularly the formulas in P must not be inconsistent or incoherent. Because the union of positive test cases UP can be viewed as an own KB since all logical sentences occurring in some p \u2208 P must be true in the solution KB. So, in a setting where test cases are specified manually, faults occur as likely in UP as they do in K.\nThe debugging system presented in this work, however, guarantees by automatic test case generation that admissibility of a DPI is satisfied at any time, provided that an admissible DPI is given as an initial input to the debugging system.\nRemark 3.3 In case of a present DPI \u3008K,B,P ,N \u3009R which is non-admissible, the DPI must be properly modified before it can be used with our debugging system. More concretely, the sets B, P as well as N must be prepared in a way that the two conditions in Proposition 3.4 are satisfied. When supposing that B is an already approved and correct KB (which is a reasonable assumption for a KB used as background knowledge during a debugging session), then there are (at least) the following ways to obtain an admissible DPI from a given non-admissible DPI without modifying B.\n(a) One straightforward way to achieve that is the deletion of all manually specified test cases from P and N . After that, both sets are either the empty set (if no automatic test cases, e.g. from former"}, {"heading": "32 CHAPTER 3. KB DEBUGGING", "text": "debugging sessions were included in these sets) or comprise only automatically generated test cases. The former case yields an admissible DPI independently of K by the property of B to not violate any requirements in R (see Definition 3.1). That the latter case implies the admissibility of the DPI is a property of the debugging system described in this work (as we will show later by Corollary 7.3).\n(b) Another way to resolve the non-admissibility of a DPI \u3008K,B,P ,N \u3009R is to first check whether \u3008UP ,B, \u2205,N \u3009R is admissible (verification of Proposition 3.4 by means of a reasoning service). If so, it is clear that B does not conflict with N . Then, a debugger (like the one presented in this work) can be exploited to find an as small as possible subset of the set of all formulas occurring in the positive test cases, the removal of which causes the DPI to become admissible. This would be accomplished by the computation of a minimal diagnosis DP w.r.t. \u3008UP ,B, \u2205,N \u3009R and the usage of the modified admissible DPI \u3008K,B, {UP \\ DP} ,N \u3009R instead of the original one. In this case, only a set-minimal set DP of formulas that were desired entailments of the user are lost. This modification is possible in polynomial time apart from the reasoning costs, i.e. by means of a polynomial number of calls to a reasoner (cf. Chapter 1).\n(c) Otherwise, i.e. if B already conflicts with the negative test cases N , then an algorithm similar to Algorithm 1 (that will be presented in Section 4.4.1) can be employed to determine a maximal subset N \u2032 of N w.r.t. set inclusion such that B will not be in conflict with N \u2032. This approach also requires only a polynomial number of calls to a reasoner (cf. Proposition 4.8). If the resulting modified DPI \u3008K,B,P ,N \u2032\u3009R is not yet admissible, i.e. after adding the positive test cases UP to B there are again conflicts with N \u2032, method (b) must be executed in order to finally obtain an admissible DPI.\nThat is, given a non-admissible DPI, there is a transformation achievable in polynomial time which enables the establishment of admissibility involving a set-minimal number of modifications to the given test cases. Thence, in the rest of this work, we will assume that a DPI given as an input to our algorithms is admissible.\nIn general, there are multiple (minimal) diagnoses for a DPI, i.e. |aD\u3008K,B,P,N \u3009R | \u2265 |mD\u3008K,B,P,N \u3009R | > 1, and there are multiple, in fact infinitely many, extensions E \u2208 EX(D)\u3008K,B,P,N \u3009R for a fixed diagnosis D \u2208 aD\u3008K,B,P,N \u3009R . The task addressed in this work is finding an optimal diagnosis for a given DPI, whereas the identification of an optimal extension w.r.t. that diagnosis and the DPI is not the aim. What we understand by \u201coptimality\u201d of a diagnosis will be addressed in more detail in Part II. Instead, we will content ourselves with finding any extension that enables to formulate a solution KB given a DPI and a diagnosis for that DPI. In fact, the problem of finding a solution KB for a DPI can be reduced to finding a diagnosis for that DPI since a suitable extension can be easily formulated for any diagnosis, as the next proposition shows:\nProposition 3.5. Let \u3008K,B,P ,N \u3009R be a DPI and D \u2208 aD\u3008K,B,P,N \u3009R . Then UP is an extension w.r.t. D and \u3008K,B,P ,N \u3009R.\nProof. Let us assume that there is some D \u2208 aD\u3008K,B,P,N \u3009R and UP is not an extension w.r.t. D and \u3008K,B,P ,N \u3009R. By the definition of a diagnosis, this is equivalent to stating that (K \\ D) \u222a UP is not a solution KB which in turn means that at least one condition (3.1), (3.2) or (3.3) of Definition 3.2 is violated by (K \\ D) \u222a UP . However, the fact that D is a diagnosis implies the existence of some extension E \u2208 EX(D)\u3008K,B,P,N \u3009R that can be added to (K \\ D) to obtain a solution KB. This means that conditions (3.1) and (3.3) must be already valid for (K \\ D), since, by monotonicity of L, addition of logical sentences E can neither solve inconsistencies or incoherencies necessary for fulfillment of condition (3.1) nor invalidate non-desired entailments as per condition (3.3). As a consequence, condition (3.2) must be violated by (K \\ D) \u222a UP . By extensiveness of L it holds that (K \\ D) \u222a UP |= p for all p \u2208 P whereby we obtain that condition (3.2) is fulfilled which yields a contradiction.\nProposition 3.5 claims that the expansion operation, i.e. identifying a concrete extension for a diagnosis, is trivial, at least for our purposes, namely formulating an extension reflecting only evident"}, {"heading": "3.1. PARSIMONIOUS KB DEBUGGING 33", "text": "entailments given by the set of positive test cases P . Consequently, in order to find a solution KB for some DPI, it is sufficient to concentrate on the deletion step, i.e. on the search for diagnoses.\nNote that using UP as a canonical extension when computing diagnoses does not affect the set of identified diagnoses. In other words, exchanging E \u2208 EX(D)\u3008K,B,P,N \u3009R for UP in Definition 3.5 yields an equivalent definition. The following corollary proves this statement and summarizes the relationship between the notions diagnosis, solution KB and valid KB.\nCorollary 3.3. The following statements are equivalent:\n1. D is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R\n2. (K \\ D) \u222a UP is a solution KB w.r.t. \u3008K,B,P ,N \u3009R\n3. (K \\ D) is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nProof. That (1) is equivalent to (2) follows from Definition 3.5 which states that D is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R iff there is some set of sentences E \u2208 EX(D)\u3008K,B,P,N \u3009R such that (K \\ D) \u222a E is a solution KB, and from Proposition 3.5 which proves that UP is an extension w.r.t. any diagnosis D and \u3008K,B,P ,N \u3009R.\nThat (1) is equivalent to (3) follows directly from Proposition 3.2 and the equivalence of (2) and (3) has been shown in Proposition 3.1."}, {"heading": "3.1 Parsimonious Knowledge Base Debugging", "text": "Why are minimal diagnoses interesting? First, the set of minimal diagnoses w.r.t. a DPI captures all the information that explains the unwanted properties, i.e. violation of requirements or test cases, of the DPI. In other words, the minimal diagnoses represent all subset-minimal possibilities to modify a KB in a way it becomes a valid KB w.r.t. the given DPI (e.g. by simply deleting a minimal diagnosis from the KB in the trivial case). By monotonicity of the logic L, each superset of a minimal diagnosis w.r.t. a DPI is a diagnosis w.r.t. this DPI. That is, aD\u3008K,B,P,N \u3009R can be easily reconstructed given mD\u3008K,B,P,N \u3009R . There is however no evidence (in terms of specified requirements and test cases) in a DPI that would justify the selection of a non-minimal diagnosis. That is, if K is a KB and D \u2286 K a minimal diagnosis w.r.t. a DPI including K, K \\ D does not violate any of the postulated properties that must hold for a KB to be valid w.r.t. this DPI. For that reason, there is no evident need to delete or modify any other sentences in K except for the ones in some minimal diagnosis D.\nSecond, usually a setting can be assumed where the author of a KB specifies formulas to the best of their knowledge. Hence, the assumption that a formula is rather correct than faulty, or in other words, that the KB author wants to keep as many formulated sentences as possible in a solution KB obtained from a debugger, is practical.\nThis also motivates the importance of a certain subset of minimal diagnoses, namely minimum cardinality diagnoses, which are the solutions of choice in scenarios where no probabilistic information about the KB authors\u2019 faults is available, e.g. in terms of statistics retrieved from log data of the used IDE (see Section 4.6 for details). In an application where such information is given, minimum cardinality diagnoses might not always be the appropriate choice (for details see Part II). In this case the aim is to find a minimal diagnosis with a maximal probability of including only sentences that are actually faulty (which might not necessarily be a minimum cardinality diagnosis).\nThird, minimality of diagnoses will be a necessary condition to guarantee the possibility of discrimination between different (candidate) diagnoses to formulate a solution KB, as will be seen later in Chapter 7.\nFourth, focusing only on minimal diagnoses rather than all diagnoses can greatly reduce the search space for diagnoses and therefore greatly speed up the debugging procedure (cf. [dKW87])."}, {"heading": "34 CHAPTER 3. KB DEBUGGING", "text": "Projected to the task of KB debugging, namely finding a solution KB w.r.t. a given DPI, this means we are interested in minimal invasiveness, that is making as few formula-deletion-modifications to the input KB K as possible in the course of the performed debugging actions. That is, the actual goal is to find some maximal solution KB K\u2217 for a DPI. Compare with \u201cThe Principle of Parsimony\u201d in [Rei87, p. 7] [BATJ91].\nProblem Definition 3.2 (Parsimonious KB Debugging). Given a DPI \u3008K,B,P ,N \u3009R, the task is to find a maximal solution KB w.r.t. \u3008K,B,P ,N \u3009R.\nThe next proposition shows that this problem can be reduced to finding a minimal diagnosis.\nProposition 3.6. (i) K \\ K\u2217 is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R for each maximal solution KB K\u2217 w.r.t. \u3008K,B,P ,N \u3009R.\n(ii) If D is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R, then (K\\D)\u222aE is a maximal solution KB w.r.t. \u3008K,B,P ,N \u3009R for all extensions E \u2208 EX(D)\u3008K,B,P,N \u3009R .\nProof. Ad (i): Let K\u2217 be an arbitrary maximal solution KB w.r.t. \u3008K,B,P ,N \u3009R. The first observation is that D := K \\ K\u2217 is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R since K\u2217 \\ K \u2208 EX(D)\u3008K,B,P,N \u3009R by the fact that K\u2217 = (K \\ D) \u222a (K\u2217 \\ K) is a solution KB by assumption. Let us assume that there is a diagnosis Dk \u2208 aD\u3008K,B,P,N \u3009R such that D \u2283 Dk. Since Dk is a diagnosis, it holds per Definition 3.5 that there is an extension E \u2208 EX(Dk)\u3008K,B,P,N \u3009R such that K\u2217k := (K \\ Dk) \u222a E is a solution KB. Further on, K\u2229K\u2217k = K\u2229((K\\Dk)\u222aE) = (K\\Dk)\u222a(K\u2229E). SinceK\u2229K\u2217 can be written asK\\(K\\K\u2217) = K\\D which is a strict subset ofK\\Dk which in turn is a subset of (K\\Dk)\u222a(K\u2229E) = K\u2229K\u2217k. Consequently, K\u2229K\u2217 \u2282 K\u2229K\u2217k holds, which is by Definition 3.2 a contradiction to the maximality of the solution KB K\u2217. Thus, D = K \\ K\u2217 is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nAd (ii): Let D be a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R. Then, by Definition 3.5, there is an extension E \u2208 EX(D)\u3008K,B,P,N \u3009R such that K\u2217 := (K \\ D) \u222a E is a solution KB. Let us assume that E \u2229 D 6= \u2205. We can rewrite K\u2217 as K\u2217 = (K \\ D) \u222a (E \u2229 D) \u222a (E \\ D). Since \u2205 \u2282 E \u2229 D \u2286 D, we have that (K \\ D) \u222a (E \u2229 D) \u2283 K \\ D. Thus, there is a D\u2032 := D \\ (E \u2229 D) \u2282 D and an extension E \u2032 \u2208 EX(D\u2032)\u3008K,B,P,N \u3009R such that E \u2032 := E \\ D such that K\u2217 = (K \\ D\u2032) \u222a E \u2032. As K\u2217 is a solution KB, this is a contradiction to the minimality of D. Therefore, (*) E \u2229 D = \u2205 for all E \u2208 EX(D)\u3008K,B,P,N \u3009R must hold.\nLet E be any extension w.r.t.D and \u3008K,B,P ,N \u3009R. Then we can writeK\u2229K\u2217 = K\u2229((K\\D)\u222aE) = (K\\D)\u222a(K\u2229E) and by (*) alsoK\u2229E = ((K\\D)\u222aD)\u2229E = ((K\\D)\u2229E)\u222a(D\u2229E) = (K\\D)\u2229E \u2286 K\\D. Consequently, (**) K \u2229 K\u2217 = K \\ D. Now, assume that there is a solution KB K\u2217k with the property K\u2229K\u2217k \u2283 K\u2229K\u2217. By (**), this implies that K\u2229K\u2217k \u2283 K \\D which means that there is a Dk \u2282 D \u2286 K such that K \u2229 K\u2217k = K \\ Dk \u2286 K\u2217k. Now K\u2217k is a solution KB w.r.t. \u3008K,B,P ,N \u3009R and can be written as K\u2217k = (K\u2217k \u2229 K) \u222a (K\u2217k \\ K) = (K \\ Dk) \u222a (K\u2217k \\ K). By Dk \u2286 K and since there is a set of formulas E := K\u2217k \\ K such that (K \\ Dk) \u222a E \u2208 Sol\u3008K,B,P,N \u3009R we have that E \u2208 EX(Dk)\u3008K,B,P,N \u3009R must hold wherefore Dk is a diagnosis by Definition 3.5. This, however, is a contradiction to the minimality of D. Therefore, K\u2217 = (K \\ D) \u222a E must be a maximal solution KB for any E \u2208 EX(D)\u3008K,B,P,N \u3009R .\nBy claim (i), Proposition 3.6 assures that each maximal solution KB can be found by investigating all minimal diagnoses w.r.t. a DPI. Claim (ii) shows that any solution KB built from a minimal diagnosis is indeed maximal. Thus, finding a suitable minimal diagnosis solves the problem of parsimonious KB debugging completely."}, {"heading": "3.2 Background Knowledge", "text": "The general debugging setting considered in this work envisions the opportunity for the user to specify some background knowledge B, i.e. a set of formulas that are known (or strongly assumed) to be correct"}, {"heading": "3.2. BACKGROUND KNOWLEDGE 35", "text": "in advance. Note that, in order for the debugging procedure to work soundly, before some background knowledge is incorporated into the DPI, it is necessary to verify its conformance with the postulated requirements R (cf. Definition 3.1).We can distinguish between two basic scenarios how background knowledge can be leveraged: (1) We have an initial KB Kinit and we know or want to assume that a subset of formulas in Kinit is correct, i.e. B \u2229 Kinit 6= \u2205, and (2) we have an initial KB Kinit and some background knowledge disjoint from Kinit, i.e. B \u2229 Kinit = \u2205.\nExample use cases for scenario (1) are situations where a user knows that a subset of formulas B in K is definitely sound or wants to restrict the scope of debugging to a particular part of the KB. Concretely, this may occur, for instance, when B is the result, i.e. the finally output solution KB K\u2217, of a former successful debugging session and K is a further development of K\u2217, or in a collaborative setting where many users are involved in the development of K and one of them may want to debug only formulas authored by herself and not touch foreign formulas, which are thus assumed as correct and assigned to B. In (1), Kinit \u2229 B and Kinit \\ B partition the original KB Kinit into a set of correct and a set of possibly incorrect formulas, respectively. The corresponding DPI would thus be \u3008Kinit \\ B,B,P ,N \u3009R for some sets of test cases P and N . Note that this DPI does meet the necessary condition (cf. Definition 3.1) K\u2229B = \u2205 as (Kinit \\ B)\u2229B = \u2205. So, in the debugging session, onlyK := Kinit \\ B is used to search for diagnoses, which can reduce the search space substantially. Though, B is incorporated in the calculations throughout the KB debugging procedure, but no formula in B may take part in a diagnosis. The advantage of this over simply not considering the formulas in B at all is, that the semantics of formulas inB is not lost and can be exploited, e.g., to grant the desired semantic properties also in the context of existing approved knowledge or to facilitate a greater choice of queries to interact with a user, which can be exploited to ask queries with lower cardinality or involving less complex formulas (see Chapter 7 for details on queries).\nIn scenario (2), the corresponding DPI looks like \u3008Kinit,B,P ,N \u3009R for some sets of test cases P and N . An application of this scenario could be the reuse of an existing KB to support an increase of the fault detection rate and thus more sustainable debugging. For example, when formulating a KB Kinit about a domain, a reference KB B in that domain that is thoroughly curated by experts could be leveraged. The use of such a KB B is possible both if Kinit is correct as a standalone KB, i.e. Kinit is already a solution KB for \u3008Kinit, \u2205,P ,N \u3009R, or not. In the first case, Kinit might still contain formulations conflicting with B. In this vein, in both cases, faults may be detected that would have been missed otherwise.\nChapter 4\nDiagnosis Computation\nIn this chapter we describe methods for computing minimal diagnoses w.r.t. a given admissible DPI, provide an in-depth theoretical analysis of these methods including correctness proofs and illustrate the presented algorithms by various examples."}, {"heading": "4.1 Conflict Sets", "text": "The search space for minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R the size of which is in general O(2|K|) (if all subsets of the KB K are investigated) can be reduced to a great extent by exploiting the notion of a conflict set [Rei87, dKW87, SFFR12].\nDefinition 4.1 (Conflict Set). Let \u3008K,B,P ,N \u3009R be a DPI. A set of formulas C \u2286 K is called a conflict set w.r.t. \u3008K,B,P ,N \u3009R, written as C \u2208 aC\u3008K,B,P,N \u3009R , iff C \u222a UP is not a solution KB w.r.t. \u3008K,B,P ,N \u3009R. A conflict set C is minimal, written as C \u2208mC\u3008K,B,P,N \u3009R , iff there is no C\u2032 \u2282 C such that C\u2032 is a conflict set.\nSimply put, a (minimal) conflict set is a (minimal) faulty KB that is a subset of K. That is, a conflict set is one source causing the faultiness of K in the context of B \u222a UP . In other words, a valid KB may not include all the formulas of any conflict set.\nCorollary 4.1. C \u2286 K is a conflict set w.r.t. \u3008K,B,P ,N \u3009R iff C is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nProof. If C is a conflict set w.r.t. \u3008K,B,P ,N \u3009R, then C\u222aUP is not a solution KB, i.e. C\u222aB\u222aUP violates some r \u2208 R, some p \u2208 P or some n \u2208 N . By extensiveness of L, C \u222a B \u222a UP |= p for all p \u2208 P , so C \u222a B \u222a UP must violate some r \u2208 R or entail some n \u2208 N . Thus, by Definition 3.3, C is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nIf C \u2286 K is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, then C\u222aB\u222aUP violates some r \u2208 R or entails some n \u2208 N , wherefore C\u222aUP /\u2208 Sol\u3008K,B,P,N \u3009R . Hence, by Definition 4.1, C is a conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nConsequently, a conflict set C along with the background knowledge B either violates some r \u2208 R, entails some n \u2208 N , or yields to a violation of some r \u2208 R or entailment of some n \u2208 N if all formulas UP comprised by the positive test cases are added to C. Any KB K that is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R is itself a conflict set and includes at least one minimal conflict set.\nProposition 4.1. Let \u3008K,B,P ,N \u3009R be a DPI. Then, K is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R iff K includes at least one minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n37"}, {"heading": "38 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Proof. \u201c\u21d2\u201d: LetK be not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R. ThenK\u222aUP is not a solution KB w.r.t. \u3008K,B,P ,N \u3009R, which means thatK is a conflict set w.r.t. \u3008K,B,P ,N \u3009R by definition 4.1. So, eitherK is a already a minimal conflict set or there must be some subset C \u2282 K which is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n\u201c\u21d0\u201d: Let K include at least one minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then, by Definition 4.1, there is some C \u2286 K such that C \u222a UP is not a solution KB. Hence, by the monotonicity of L, K \u222a UP cannot be a solution KB either. So, by Proposition 3.1, K is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nAs a consequence, a complete and sound method for computing minimal conflict sets w.r.t. a DPI \u3008K,B,P ,N \u3009R can be used to decide validity of K w.r.t. \u3008\u00b7,B,P ,N \u3009R. Moreover, such a method can be used to decide whether a given DPI is admissible, i.e. has solutions. For, if a DPI is admissible and the given KB is invalid w.r.t. this DPI, then there cannot be an empty conflict set. In other words, if the empty KB is a conflict set \u2013 or, equivalently, an empty conflict set exists w.r.t. a DPI \u2013, then the DPI is not admissible.\nProposition 4.2. Let \u3008K,B,P ,N \u3009R be a DPI and K be invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Then, there exists a minimal conflict set C 6= \u2205 w.r.t. \u3008K,B,P ,N \u3009R iff \u3008K,B,P ,N \u3009R is admissible.\nProof. SinceK is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, there must be at least one conflict set w.r.t. \u3008K,B,P ,N \u3009R by Proposition 4.1. Assume that there exists a minimal conflict set C 6= \u2205 w.r.t. \u3008K,B,P ,N \u3009R. This can be true iff \u2205 is not a (minimal) conflict set w.r.t. \u3008K,B,P ,N \u3009R. By Corollary 4.1 and Definition 3.3, this is equivalent to the fact that \u2205 \u222a B \u222a UP \u2261 B \u222a UP does not violate any r \u2208 R and does not entail any n \u2208 N . By Proposition 3.4, this holds iff there exists a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. By Definition 3.6, this is equivalent to \u3008K,B,P ,N \u3009R being admissible.\nThe following proposition provides information about the relationship between (minimal) conflict sets and the background knowledge as well as the positive test cases.\nProposition 4.3. Let \u3008K,B,P ,N \u3009R be a DPI and C a conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then the following holds:\n1. C \u2229 B = \u2205.\n2. If C is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R, then C \u2229 UP = \u2205.\nProof. 1): C \u2229 B = \u2205 holds since C \u2286 K (Definition 4.1) and K \u2229 B = \u2205 (Definition 3.1). 2): Assume that C is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R and C \u2229 UP 6= \u2205. Since C is a conflict set, we have that C \u222a B \u222a UP violates some r \u2208 R or entails some n \u2208 N by Corollary 4.1 and Definition 3.3. Since (C \\ UP ) \u222a B \u222a UP = C \u222a B \u222a UP and (C \\ UP ) \u2282 C, this implies that (C \\ UP ) is a conflict set w.r.t. \u3008K,B,P ,N \u3009R which in turn implies that C /\u2208 mC\u3008K,B,P,N \u3009R which is a contradiction."}, {"heading": "4.2 Conflict Sets versus Justifications", "text": "The notion of a conflict set is closely related to the notion of a justification [HPS08, HPS09, HPS10, Hor11, HBP11, HPS12a] which is frequently adopted in the field of the Semantic Web (cf. Section 2.2) in order to find minimal explanations for particular entailments in DL ontologies. Thus, the paradigm of a justification can be a useful aid in the debugging of faulty ontologies [Kal06]. Note that sometimes justifications are referred to as MinAs (Minimal Axiom Sets) [BP08] or MUPS (Minimal Unsatisfiability Preserving Sub-TBoxes) [SHCH07] where the latter term is mostly used in the context of ontology debugging. The notion of a (minimal) conflict set, on the other hand, has been mainly adopted in the Diagnosis community [Rei87, dKW87, PW03, WSM02, FFJS04]. In this section we want to establish"}, {"heading": "4.2. CONFLICT SETS VERSUS JUSTIFICATIONS 39", "text": "a relationship between these two widely used instruments used for debugging. It will turn out that both terms are strongly related, but in debugging systems like the ones proposed in our work conflict sets are better suited as they automatically focus only on the minimal explanations for faults in a KB.\nFor example, the author of [Kal06] i.a. discusses the use of justifications to aid the debugging of incoherent ontologies, i.e. ontologies that include unsatisfiable concepts (cf. Section 2.2). If there are multiple unsatisfiable concepts, then some of these might be only unsatisfiable due to the unsatisfiability of another concept. Assume, for instance, an incoherent DL KBK := {A < B,B v E u \u00acE}. InK there are two unsatisfiable concepts A and B where A\u2019s unsatisfiability is dependent on B\u2019s unsatisfiability. Using the terminology of [Kal06, Hor11], A would be called a purely derived unsatisfiable concept whereas B would be called a root unsatisfiable concept. Because the (only) justification for the unsatisfiability of A is JA := K whereas the (only) justification for the unsatisfiability of B is JB = {B v E u \u00acE} \u2282 JA. Therefore, [Kal06] proposes to resolve root unsatisfiable concepts first since this might resolve some (purely) derived concepts as well, as in this example. However, finding out whether a concept is root or derived involves the computation of justifications for all unsatisfiable concepts in a KB. On the other hand, reliance on minimal conflict sets would implicate a direct focus on the faultiness (in this example: the incoherency) of the KB and not necessarily on the exact explanations of all unsatisfiable concepts that cause the incoherency. In this vein, no justification for a purely derived concept can be a minimal conflict set. So, the computation of minimal conflict sets involves only the determination of those justifications for faults that must necessarily be resolved. Therefore, for the given example, the only minimal conflict set is JB .\nA justification for a given formula (axiom) relative to a KB is a (subset-)minimal subset of the KB that entails the given formula.\nDefinition 4.2 (Justification for a Formula). [KPHS07] Let K be a KB and \u03b1 a formula, both over L. Then J \u2286 K is called a justification for \u03b1 w.r.t. K, written as J \u2208 Just(\u03b1,K), iff J |= \u03b1 and for all J \u2032 \u2282 J it holds that J \u2032 6|= \u03b1.\nSince we consider test cases which are sets of formulas over L, we generalize the definition of a justification as follows:\nDefinition 4.3 (Justification for a Set of Formulas). Let K, K\u2032 be KBs over L. Then J \u2286 K is called a justification for K\u2032 w.r.t. K, written as J \u2208 Just(K\u2032,K), iff J |= K\u2032 and for all J \u2032 \u2282 J it holds that J \u2032 6|= K\u2032.16\nIn order to express the connection between justifications and conflict sets, we require yet another generalization of this definition. To this end, the following definition characterizes a justification for a set X of KBs relative to a KB K as a (subset-)minimal subset of K such that this subset entails some KB in X .\nDefinition 4.4 (Justification for a Set of Sets of Formulas). Let K be a KB over L and X a set of KBs over L. Then J \u2286 K is called justification for X w.r.t. K, written as J \u2208 Just(X,K), iff J |= K\u2032 for some K\u2032 \u2208 X and for all J \u2032 \u2282 J it holds that J \u2032 6|= K\u2032\u2032 for all K\u2032\u2032 \u2208 X .\nBased on Definition 4.4, the relation between conflict sets and justifications is captured by the following Proposition 4.4. Intuitively, any conflict set w.r.t. \u3008K,B,P ,N \u3009R is the part of a justification for a fault that is relevant for the debugging task, where fault refers to an inconsistency (and/or incoherency) and/or a negative test case entailed by K \u222a B \u222a UP . Since debugging focuses on the deletion of KB formulas only, \u201crelevant\u201d in this context refers to the subset of the justification that does not contain any sentences in B and UP , but solely sentences from K. Importantly, there may be justifications, in general, the relevant subset of which is not a minimal conflict set. The reason why this case can arise in spite of\n16Remember that J |= K\u2032 means that J |= ax for each ax \u2208 K\u2032 (cf. Remark 3.1)."}, {"heading": "40 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "the set-minimality of justifications is that the relevant part of a justification (for some set of sentencesK1, e.g. a negative test case n1 \u2208 N ) may be a superset of the relevant part of another justification (for some other set of sentences K2, e.g. another negative test case n2 \u2208 N ) whereas both justifications are not in a subset-relationship (i.e. contain different sentences from B and/or UP ). This circumstance is illustrated by the following example:\nExample 4.1 Let a DPI \u3008K,B,P ,N \u3009R be defined as\nK := {B v E,E v \u2203r.G} B := {A v B} N := {{A v E} , {B v \u2203r.G}} P := \u2205 R := {consistency}\nWe have thatK\u222aB\u222aUP is consistent and thus no requirement in R is violated. But, the two negative test cases are both entailed byK\u222aB\u222aUP whereforeK is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. The set of justifications for the violation of the first negative test case is Jn1 = {{A v B,B v E}}; for the second one it is Jn2 = {{B v E,E v \u2203r.G}}. The relevant subset of the justification J1 in Jn1 is J1,rel = {B v E} (since {A v B} is in B) whereas the relevant subset of the justification J2 in Jn2 is J2,rel = {B v E, E v \u2203r.G}, i.e. J1,rel \u2282 J2,rel despite that there is no set subset-relationship between J1 and J2. Hence, there are two justifications that explain the invalidity of K w.r.t. \u3008\u00b7,B,P ,N \u3009R, but there is only one minimal conflict set C = J1,rel w.r.t. \u3008K,B,P ,N \u3009R.\nSo, generally, the set of minimal conflict sets w.r.t. a DPI is a subset of the set of justifications for faults in K \u222a B \u222a UP , which is due to the focus on just the parts of justifications that are relevant for the KB debugging task.\nProposition 4.4. Let \u3008K,B,P ,N \u3009R be a DPI. Additionally, let\n(a) X := {{Ai v \u22a5} |Ai \u2208 NC} \u222a {{ri v \u22a5} | ri \u2208 NR} \u222a {{> v \u22a5}} \u222aN if R = {consistency, coherency} and\n(b) X := {{> v \u22a5}} \u222aN if R = {consistency}.17\nThen the following holds:\n1. If C is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R, then there is some J \u2208 Just(X,K \u222a B \u222a UP ) such that (J \u2229 K) \\ UP = C.\n2. For all J \u2208 Just(X,K \u222a B \u222a UP ) it is true that C := (J \u2229 K) \\ UP is a conflict set w.r.t. \u3008K,B,P ,N \u3009R, but not necessarily a minimal one.\nProof. 1): Assume that C \u2208mC\u3008K,B,P,N \u3009R and for all J \u2208 Just(X,K\u222aB \u222aUP ) it holds that (J \u2229K) \\ UP 6= C. There are two cases to distinguish between: (a) there is some sentence in (J \u2229 K) \\ UP that is not in C and (b) there is some sentence in C that is not in (J \u2229 K) \\ UP .\nLet us first assume (a), i.e. for all J \u2208 Just(X,K \u222a B \u222a UP ) it holds that there is some sentence ax in (J \u2229 K) \\ UP that is not in C. Additionally, assume there is a J \u2208 Just(X,K \u222a B \u222a UP ) such that J \u2286 C \u222a B \u222a UP . We can write J as J = S1 \u222a S2 \u222a S3 for S1 := [(J \u2229 K) \\ UP ], S2 := [J \u2229 B] and S3 := [J\u2229UP ]. Since J = S1\u222aS2\u222aS3 \u2286 C\u222aB\u222aUP it must hold in particular that S1 \u2286 C\u222aB\u222aUP and\n17We use DL notation in this proposition since justifications, as argued, are mostly applied to DL KBs. An equivalent formulation of the proposition for FOL or PL is straightforward (cf. Example 2.1 and Remark 3.2). Note that for PL only (b) is relevant since coherency is not defined for PL. Further, recall that NC and NR are defined in Section 2.2."}, {"heading": "4.3. RELATION BETWEEN CONFLICT SETS AND DIAGNOSES 41", "text": "therefore ax \u2208 C \u222a B \u222aUP . However, ax /\u2208 C by assumption, ax /\u2208 B since ax \u2208 K and B \u2229K = \u2205, and ax /\u2208 UP since ax \u2208 S1 and S1\u2229UP = \u2205. This is a contradiction. Hence, for all J \u2208 Just(X,K\u222aB\u222aUP ) it holds that J 6\u2286 C \u222a B \u222a UP . Since X captures all r \u2208 R and n \u2208 N , we can conclude that C is not a conflict set w.r.t. \u3008K,B,P ,N \u3009R which is a contradiction to C \u2208mC\u3008K,B,P,N \u3009R .\nLet us now assume (b), i.e. for all J \u2208 Just(X,K \u222a B \u222a UP ) it holds that there is some sentence ax in C that is not in (J \u2229 K) \\ UP . Since C is a conflict set and since X captures all r \u2208 R and n \u2208 N , we have that C \u222a B \u222a UP |= K\u2032 for some K\u2032 \u2208 X . So, there must be some J0 \u2208 Just(X,K \u222a B \u222a UP ) such that J0 \u2286 C \u222aB \u222aUP . As C \u2208mC\u3008K,B,P,N \u3009R , there cannot be any J \u2208 Just(X,K\u222aB \u222aUP ) with J \u2286 C\u2032 \u222aB \u222aUP for arbitrary C\u2032 \u2282 C. This must hold in particular for J0 which implies that J0 \u2229 C = C which is equivalent to C \u2286 J0. As (1) C \u2286 K (Definition 4.1) and, by Proposition 4.3 and by the fact that C \u2208 mC\u3008K,B,P,N \u3009R , (2) C \u2229 UP = \u2205, we can conclude that C \u2286 (J0 \u2229 K) \\ UP which is a contradiction since there cannot be a ax in C that is not in (J0 \u2229 K) \\ UP .\n2): If J \u2208 Just(X,K\u222aB\u222aUP ), then, by Definition 4.4, J |= K\u2032 for someK\u2032 \u2208 X and J \u2286 K\u222aB\u222aUP . So, [(J \u2229 K) \\ UP ] \u222a B \u222a UP = (J \u2229 K) \u222a B \u222a UP \u2287 J wherefore [(J \u2229 K) \\ UP ] \u222a B \u222a UP |= K\u2032 by monotonicity of L. As K\u2032 \u2208 X and X captures all the reasons why some r \u2208 R or some n \u2208 N may not be fulfilled (cf. the discussion in Chapter 3), we have that [(J \u2229K)\\UP ]\u222aB\u222aUP violates some r \u2208 R or entails some n \u2208 N . This implies that [(J \u2229K) \\UP ] \u222aUP /\u2208 Sol\u3008K,B,P,N \u3009R . Since (J \u2229K) \\UP \u2286 K is also true, (J \u2229 K) \\ UP \u2208 aC\u3008K,B,P,N \u3009R by Definition 4.1.\nTo see that (J \u2229 K) \\ UP /\u2208 mC\u3008K,B,P,N \u3009R holds in general, reconsider Example 4.1 where (J2 \u2229 K) \\ UP = J2 \u2283 C holds for the justification J2 and the minimal conflict set C."}, {"heading": "4.3 The Relation between Conflict Sets and Diagnoses", "text": "A minimal conflict set has the property that deletion of any formula in it yields a set of formulas which is correct in the context of B, P , N and R.\nProposition 4.5. If C is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R, then C\u2032 is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R for each C\u2032 \u2282 C.\nProof. Since C \u2208 mC\u3008K,B,P,N \u3009R , it must hold that C\u2032 /\u2208 aC\u3008K,B,P,N \u3009R . Then, by Corollary 4.1, C\u2032 is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R.\nHence, by deletion of at least one formula from each minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R, a valid KB can be obtained fromK. Thus, a solution KB (K\\D)\u222aUP can be obtained by calculation of a hitting set D of all minimal conflict sets in mC\u3008K,B,P,N \u3009R . The Hitting Set problem is defined as follows:\nDefinition 4.5 (Hitting Set). Let S = {S1, . . . , Sn} be a set of sets. Then, H is called a hitting set of S iff H \u2286 US and H \u2229 Si 6= \u2205 for all i = 1, . . . , n.\nA hitting set H of S is minimal iff there is no hitting set H \u2032 of S such that H \u2032 \u2282 H .\nProposition 4.6. [FS05] A (minimal) diagnosis w.r.t. the DPI \u3008K,B,P ,N \u3009R is a (minimal) hitting set of all minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R.\nNow, we want to contemplate two example DPIs and analyze them regarding the their minimal conflict sets and minimal diagnoses:\nExample 4.2 In this example, we analyze the PL DPI \u3008K,B,P ,N \u3009R given by Table 15.3. There are two minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R, i.e. mC\u3008K,B,P,N \u3009R = {C1, C2} = {\u30081, 2, 5\u3009 , \u30081, 2, 7\u3009}. 18\n18Please notice that we sometimes write i instead of ax i for brevity when it is clear what is meant. We will do so in many other examples as well."}, {"heading": "42 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Why is C1 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We recall Definition 4.1 and argue as follows to deduce the entailment C1 |= n1 where n1 \u2208 N (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 1 : A \u2192 E ax 2 : X \u2228 E \u2192 F \u2227 Y \u2227 Z ax 5 : Y \u2192 \u00acA\nax 1, ax 2, ax 5 : A \u2192 \u00acA \u2261 \u00acA \u2228 \u00acA \u2261 \u00acA n1 \u2208 N : \u00acA\nMinimality of C2 is obvious from this argumentation. i.e. we cannot deduce n1 if any one of the formulas 1, 2 or 5 is omitted, and there is no other fault except for the violation of n1.\nWhy is C2 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We recall Definition 4.1 and argue as follows to deduce the entailment C2 \u222a B |= n1 where n1 \u2208 N (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 1 : A \u2192 E ax 2 : X \u2228 E \u2192 F \u2227 Y \u2227 Z ax 7 : Z \u2192 G\n(G \u2192 \u00acA) \u2208 B : G \u2192 \u00acA ax 1, ax 2, ax 7,B : A \u2192 \u00acA \u2261 \u00acA \u2228 \u00acA \u2261 \u00acA\nn1 \u2208 N : \u00acA\nMinimality of C2 is obvious from this argumentation. i.e. we cannot deduce n1 if any one of the formulas 1, 2 or 7 is omitted, and there is no other fault except for the violation of n1.\nThere are no further minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R. This is fairly easy to see since\n\u2022 K \u222a B \u222aUP = K \u222aB cannot be inconsistent due to the fact that the only negative literal occurring on the righthand side of an implication is \u00acA and A does not occur at the righthand side of any implication in K \u222a B,\n\u2022 there is no other way to deduce n1 than using a superset of the formulas in C1 or C2 and\n\u2022 n1 is the only negative test case in N .\nHence, the set of all minimal diagnoses mD\u3008K,B,P,N \u3009R = {D1,D2,D3} = {[1], [2], [5, 7]} is obtained by computing all minimal hitting sets of mC\u3008K,B,P,N \u3009R = {C1, C2} (cf. Proposition 4.6).\nExample 4.3 In this example, we analyze the DL DPI \u3008K,B,P ,N \u3009R given by Table 4.2. There are four minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R, i.e.\nmC\u3008K,B,P,N \u3009R = {C1, C2, C3, C4} = {\u30081, 2, 5\u3009 , \u30082, 4, 6\u3009 , \u30081, 3, 4\u3009 , \u30081, 5, 6, 8\u3009}\nWhy is C1 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We recall Definition 4.1 and argue as follows to deduce the entailment C1 |= n1 where n1 \u2208 N (left of the colon: the formulas used in the deduction are underlined;"}, {"heading": "4.3. RELATION BETWEEN CONFLICT SETS AND DIAGNOSES 43", "text": "right of the colon: the relevant implications are underlined):\nax 1 : A v B ax 2 : B v G ax 5 : G v K\nax 1, ax 2, ax 5 : A v K n1 \u2208 N : A v K\nMinimality of C1 is follows from this argumentation. i.e. we cannot deduce n1 if any one of the formulas 1, 2 or 5 is omitted, and from the fact that we cannot deduce an incoherency (r2), inconsistency (r1) or the entailment of any other negative test case n \u2208 N for any KB C\u20321 \u222a B \u222a UP for any C\u20321 \u2282 C1.\nWhy is C2 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We recall Definition 4.1 and argue as follows to deduce that C2 \u222a B is incoherent and thus violates the requirement r2 \u2208 R (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nax 2 : B v G ax 6 : G v \u2203r.F\n(1) : ax 2, ax 6 : B v \u2203r.F ax 4 : B v \u2200r.H\n(H v \u00acF ) \u2208 B : H v \u00acF (2) : ax 4,B : B v \u2200r.\u00acF (1) and (2) : B v \u22a5\nr1 \u2208 R : B 6v \u22a5\nSince we cannot deduce an incoherency (r2), inconsistency (r1) or the entailment of any negative test case n \u2208 N for any KB C\u20322 \u222a B \u222a UP for any C\u20322 \u2282 C2, the minimality of C2 follows.\nWhy is C3 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We recall Definition 4.1 and argue as follows to deduce that C3 \u222aB \u222aUP is inconsistent and thus violates the requirement r1 \u2208 R (left of the colon: the formulas used in the deduction are underlined; right of the colon: the relevant implications are underlined):\nA(x) \u2208 B : A(x) ax 1 : A v B\n(1) : ax 1,B : B(x) (2) : p1 \u2208 P : r(x, y)\nax 4 : B v \u2200r.H (3) : (1) and ax 4 : H(y)\n(4) : ax 3 : \u00acH(y) (3) and (4) : E\nNo inconsistency (r1) or incoherency (r2) can be derived and no negative test case n \u2208 N is entailed from any C\u20323 \u222a B \u222a UP for C\u20323 \u2282 C3. Hence, C3 is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nWhy is C4 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We recall Definition 4.1 and argue as follows to deduce the entailment C4 \u222a B |= n2 where n2 \u2208 N (left of the colon: the formulas used in the deduction are"}, {"heading": "44 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "underlined; right of the colon: the relevant implications are underlined):\nax 8 : L v G ax 6 : G v \u2203r.F\n(1) : ax 6, ax 8 : L v \u2203r.F A(x) \u2208 B : A(x)\n(2) : ax 1,B : B(x) (3) : ax 5 : G v K\n(1) and (2) and (3) : L v \u2203r.F, B(x), G v K n1 \u2208 N : L v \u2203r.F, B(x), G v K\nNo inconsistency (r1) or incoherency (r2) can be derived and no negative test case n \u2208 N is entailed from any C\u20324 \u222a B \u222a UP for C\u20324 \u2282 C4. Thus, C4 is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nHence, the set of all minimal diagnoses mD\u3008K,B,P,N \u3009R , obtained by computing all minimal hitting sets of mC\u3008K,B,P,N \u3009R = {C1, C2, C3, C4} (cf. Proposition 4.6), comprises ten minimal diagnoses Di for i = 1, . . . , 10:\nD1 = [1, 2] D2 = [1, 4] D3 = [1, 6] D4 = [2, 3, 5] D5 = [2, 3, 6] D6 = [2, 3, 8] D7 = [2, 4, 6] D8 = [2, 4, 8] D9 = [3, 5, 6] D10 = [4, 5]\nAlthough the DPI \u3008K,B,P ,N \u3009R is very small in size, i.e. number of formulas occurring in it is very small, the reader might agree that it is not trivial on the one hand (1) to realize which subsets of this KB K are (minimal) conflict sets, (2) to see that or why a subset of this KB K along with the background knowledge B and the union of the positive test cases UP is a (minimal) conflict set (cf. [HBP11]), and (3) to assess that there are no further minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R. This example gives a little bit of an impression that tool assistance in the debugging of KBs is inevitable especially for real-world KBs that are huge in size and/or complex in terms of the expressivity of the used logic or in terms of their \u201cdebugging properties\u201d, i.e. large number and/or size of minimal conflict sets and/or minimal diagnoses.\nA means to handle problems (1) and (3) is provided by some method for the computation of a minimal conflict set (e.g. QX given by Algorithm 1 below, see Section 4.4.1) coupled with a hitting set tree algorithm (e.g. HS described by Algorithm 2 below, see Section 4.5) for the systematic computation of different minimal conflict sets, or other mechanisms such as the ALL_JUST_ALG presented in [KPHS07] which computes all justifications for some particular entailment (but, some post-processing of the justifications is necessary to obtain minimal conflict sets, cf. Section 4.2).\nProblem (2) and its complexity for humans has been studied in [HBP11] with a focus on justifications in DL or OWL KBs. Since a minimal conflict set can be regarded as the relevant (i.e. potentially faulty) part of a justification for some undesired entailment (i.e. a violated requirement or test case) as we analyzed in Section 4.2, the cognitive complexity model proposed by [HBP11] applies also to minimal conflict sets. Ways to facilitate the understanding of justifications for humans (that might be successfully applied also to conflict sets) have been addressed in [HPS10, HPS09, HPS08]. Moreover, there is an ontology editing browser SWOOP [KPS+06] equipped with a strikeout feature [Kal06] that highlights parts of justifications that are relevant for the entailment by striking out all irrelevant parts. This is more or less the automation of our analyses of the conflict sets by underlining the relevant parts of the formulas in this example and Example 4.2."}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 45", "text": ""}, {"heading": "4.4 Methods for Diagnosis Computation", "text": "Two common methods employed for the computation of (minimal) diagnoses [SFFR12, RSFF13] are the QuickXPlain algorithm [Jun04] (in short QX) and a hitting set search tree [Rei87, GSW89] (in short HS). Thereby, QX serves as a deterministic method for computing one minimal conflict set w.r.t. a given DPI \u3008K,B,P ,N \u3009R per call. Since a diagnosis is a hitting set of all minimal conflict sets, more than one minimal conflict set is generally required to compute a diagnosis. Due to its determinism, however, QX always computes the same minimal conflict set for the same input DPI. Thus, in order to compute different (or all) minimal conflict sets, the input to QX needs to be varied accordingly. This can be done by means of HS which serves as a search tree to systematically and successively explore all minimal conflict sets w.r.t. an initially given DPI. Note that often not all minimal conflict sets w.r.t. a DPI are necessary to obtain a minimal diagnosis w.r.t. this DPI. This is the case when different minimal conflict sets overlap, i.e. have a non-empty intersection. In the extreme case, when all minimal conflict sets w.r.t. a DPI share some formulas, then the computation of any single minimal conflict set can suffice to obtain a minimal diagnosis, which is actually even a minimum cardinality diagnosis.\nAnother approach for computing a minimal conflict set (or justification) is the \u201cexpand-and-shrink\u201d algorithm presented in [KPHS07]. However, empirical evaluations and a theoretical analysis of the best and worst case complexity of the \u201cexpand-and-shrink\u201d method compared to QX performed in [SFJ08] revealed that the latter is preferable over the former.\nAlso, alternative strategies for the computation of minimal diagnoses have been suggested. One common method is to avoid the indirection of diagnosis computation via minimal conflict sets and use algorithms that determine diagnoses directly [SU06], i.e. without the necessity to compute conflict sets. This approach has been applied for the non-interactive debugging of ontologies [DQPS11] and constraints [FSZ11]. In our previous work, we adopted such a direct technique for the interactive debugging of KBs [SFRF14c]. The reason why we stick to the conflict-based approach in this work is that we want to present best-first algorithms that figure out minimal diagnoses in descending order of their probability. This is not (systematically) realizable with a direct approach."}, {"heading": "46 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": ""}, {"heading": "4.4.1 Computation of a Minimal Conflict Set", "text": "The QX algorithm takes a DPI \u3008Korig,Borig,P ,N \u3009R over some monotonic logic L as input and returns a minimal conflict set C \u2286 Korig w.r.t. \u3008Korig,Borig,P ,N \u3009R as output, if some conflict set exists for the DPI, and \u2019no conflict\u2019 otherwise.\nMonotonic Properties. Basically, QX can be employed to find for an input set X a set-minimal subset Xmin \u2286 X that has a certain property prop for problems of completely different nature such as propositional unsatisfiability or over-constrainedness of constraint satisfaction problems. The only postulated prerequisite for QX to work correctly is that prop is a monotonic property. A property is monotonic if and only if the binary function that returns 1 if the property holds for the input set and 0 otherwise is a monotonic function.\nDefinition 4.6 (Binary Monotonic Function). Let X be a set and f : 2X \u2192 {0, 1} be a binary function defined for all subsets of X . Then, f is monotonic iff\n\u2200X \u2032, X \u2032\u2032 \u2286 X : X \u2032 \u2282 X \u2032\u2032 \u2227 f(X \u2032) = 1 =\u21d2 f(X \u2032\u2032) = 1\nSo, prop is monotonic iff, given that prop holds for some set X \u2032, it follows that prop also holds for any superset X \u2032\u2032 of X \u2032. Note that, by simple logical transformation, an equivalent statement can be derived from Definition 4.6; namely that, given that prop does not hold for some set X \u2032\u2032, it follows that prop does not hold for any subset X \u2032 of X \u2032\u2032 either.\nAs inconsistency and incoherency as well as the entailment of some n \u2208 N over some monotonic language L are clearly monotonic properties, the following proposition holds.\nProposition 4.7. Let \u3008K,B,P ,N \u3009R be a DPI. Then, the invalidity of K\u2032 \u2286 K w.r.t. \u3008\u00b7,B,P ,N \u3009R (as per Definition 3.3) is a monotonic property."}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 47", "text": "By Corollary 4.1, a (minimal) conflict set w.r.t. \u3008K,B,P ,N \u3009R is a (minimal) invalid sub-KB of K w.r.t. \u3008\u00b7,B,P ,N \u3009R. Therefore:\nCorollary 4.2. Let \u3008K,B,P ,N \u3009R be a DPI. Then, being a conflict set w.r.t. \u3008K,B,P ,N \u3009R is a monotonic property.\nThus, QX is applicable for the problem of finding a minimal conflict set w.r.t. a DPI. As we shall see later in Chapter 8, another monotonic property will enable us to apply QX also for the minimization of queries asked to an interacting user in the interactive debugging of KBs.\nHow QX (Algorithm 1) Works. After verifying that the trivial cases, i.e. Korig is already a valid KB w.r.t. \u3008\u00b7,Borig,P ,N \u3009R or Korig = \u2205, are not met, a non-empty minimal conflict set w.r.t. \u3008Korig,Borig, P ,N \u3009R must exist. So, the algorithm enters the recursive procedure QX\u2032(\u2205, \u3008Korig,Borig,P ,N \u3009R). Note that the parameters P ,N ,R of QX\u2032 are used for validity tests (ISKBVALID, line 9) only and are maintained invariant during the entire recursive execution. In case Korig is not a singleton, i.e. it does not hold for sure that Korig is an element of a minimal conflict set w.r.t. \u3008Korig,Borig,P ,N \u3009R, the idea is to apply a divide-and-conquer strategy to reduceKorig into two subproblems and solve one subproblem first, i.e. find a minimal conflict set for this subproblem, and then the second subproblem. The union of the minimal conflict sets found for the subproblems is then a minimal conflict set for the original problem. This division into smaller problems is recursively executed for each subproblem until the trivial case, i.e. the KB of the subproblem that is analyzed includes only one element, occurs. Then this element is an element of a minimal conflict set w.r.t. the original problem.\nSimply put, one can imagine that QX takes Korig, partitions it into K1 and K2 and first considers the DPI with KB K2 and background knowledge B \u222aK1 (line 16). If the latter already includes a conflict set (second condition in line 9), then K2 can be safely discarded and does not need to be further considered. Instead, K1 is further investigated, i.e. the DPI with KB K1,2 and background knowledge B\u222aK1,1 where K1,1 and K2,2 partition K1. Notice that, in this way, |K2| sentences can be dismissed by a single call to ISKBVALID which is the only function in Algorithm 1 that calls a reasoner.\nIf, on the other hand, B\u222aK1 includes no conflict set, K2 is partitioned intoK2,1 andK2,2 and the two DPIs, the first with KB K2,2 and background knowledge B\u222aK1\u222aK2,1 and the second with KB K2,1 and background knowledge B \u222aK1 \u222a C2,2, are recursively analyzed where C2,2 is the result computed for the first DPI.\nThis recursion is executed until encountering a trivial case, i.e. a leaf node of the recursion tree, along each path. Then, the recursion unwinds by building the union of all leaf nodes, i.e. the union of all returned sets for subproblems where a trivial case occurred.\nThe next example illustrates one execution of QX which computes one minimal conflict set:\nExample 4.4 Let us consider the DL example DPI depicted by Table 4.3. We will now demonstrate how a minimal conflict set is computed by Algorithm 1 (see Fig. 4.1). Since K is not the empty set and not a valid KB w.r.t. the DPI (conditions in lines 4 and 2 are false), QX\u2032(\u2205, \u3008K,B,P ,N \u3009R) is called in line 7. This call is illustrated by the root node (node 1\u00a9) of the recursion tree given in Fig. 4.1 (whereas the evaluations made by QX prior to this call are not depicted in the figure). Notice that each node in the tree shows only the values of C, K and B since all other parameters P , N and R are invariant throughout the entire execution of Algorithm 1.\nDue to the fact that C = \u2205 and K includes five formulas and is thus not a singleton, K = {ax 1, . . . , ax 5} is partitioned into K1 = {ax 1, ax 2, ax 3} and K2 = {ax 4, ax 5} and QX\u2032 is recursively called in line 16 with parameters C = K1, K = K2 and B = B \u222a {ax 1, ax 2, ax 3} which is expressed in the figure by a left branch to node 2\u00a9. This call, however, returns \u2205 directly since B \u222a {ax 1, ax 2, ax 3} is already invalid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R because B \u222a {ax 1, ax 2, ax 3} \u222a UP = { A(w), A(v), s(v, w) } \u222a{\nA v B,B v E,B vD u \u00ac\u2203s.C } \u222a{{B(w)}} |= {\u00acC(w)} which is a negative test case, i.e. must not"}, {"heading": "48 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Algorithm 1 QX: Computation of a Minimal Conflict Set Input: a DPI \u3008Korig,Borig,P ,N \u3009R Output: a minimal conflict set w.r.t. \u3008Korig,Borig,P ,N \u3009R\n1: procedure QX(\u3008Korig,Borig,P ,N \u3009R) 2: if ISKBVALID(Korig, (Borig,P ,N ,R)) then 3: return \u2018no conflict\u2019 4: else if Korig = \u2205 then 5: return \u2205 6: else 7: return QX\u2032(\u2205, \u3008Korig,Borig,P ,N \u3009R)\n8: procedure QX\u2032(C, \u3008K,B,P ,N \u3009R) 9: if C 6= \u2205 \u2227 \u00acISKBVALID(B, \u3008\u00b7, \u2205,P ,N \u3009R) then\n10: return \u2205 11: if |K| = 1 then 12: return K 13: k \u2190 SPLIT(|K|) 14: K1 \u2190 GET(K, 1, k) 15: K2 \u2190 GET(K, k + 1, |K|) 16: C2 \u2190 QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R) 17: C1 \u2190 QX\u2032(C2, \u3008K1,B \u222a C2,P ,N \u3009R) 18: return C1 \u222a C2\n19: procedure ISKBVALID(K, \u3008\u00b7,B,P ,N \u3009R) 20: K\u2032 \u2190 K \u222a B \u222a \u22c3 p\u2208P p 21: if \u00acVERIFYREQ(K\u2032,R) then 22: return false 23: for n \u2208 N do 24: if ENTAILS(K\u2032,n) then 25: return false 26: return true\nbe entailed by a solution KB w.r.t. the input DPI (the parts of the formulas relevant for the entailment to hold are underlined). Returning \u2205 in this case means discarding K2 = {ax 4, ax 5}.\nSo, the algorithm opens a right branch from the root to node 3\u00a9 by calling QX\u2032 (line 17) with parameters C = \u2205 (result of left branch), K = K1 = {ax 1, ax 2, ax 3} and B = B. During the execution of this call K1 is partitioned into {ax 1, ax 2} (left branch to node 4\u00a9) and {ax 3} (right branch to node 5\u00a9). In node 4\u00a9, it holds that B\u222a{ax 1, ax 2} can be extended to a solution KB by adding UP , i.e. B\u222a{ax 1, ax 2} is valid. As it is already an established fact since the execution of node 2\u00a9 that B \u222a {ax 1, ax 2, ax 3} is invalid, it must be the case that ax 3 is an element of a minimal conflict set w.r.t. the input DPI (as there is a conflict set w.r.t. the input DPI in {ax 1, ax 2, ax 3}, but there is none in {ax 1, ax 2}). The algorithm accounts for that by checking whether K is a singleton (line 11) in which case it is guaranteed that K is a subset of a minimal conflict set w.r.t. the input DPI. So, node 4\u00a9 returns {ax 3}. This procedure is continued until each path from the root node reaches a node where a trivial case is met. Then the recursion unwinds and, when arrived at the root node, the minimal conflict set \u3008ax 1, ax 3\u3009 is returned.\nThat C := \u3008ax 1, ax 3\u3009 is indeed a conflict set can be recognized easily by the underlinings in the formulas given before. Minimality is given since B \u222a C \u222a UP is neither inconsistent nor incoherent and the deletion of any formula from C breaks the entailment of n1. Hence, QX has returned a sound output."}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 49", "text": "The complexity of Algorithm 1 in terms of the number of calls to the function ISKBVALID, which is the only place in the algorithm where a reasoning service is consulted, is captured by the following proposition.\nProposition 4.8 (Complexity of QX). [Jun04] Let \u3008K,B,P ,N \u3009R be a DPI and the function SPLIT (line 13 of Algorithm 1) be defined as SPLIT(n) = bn2 c where n is a natural number. Then, the worst case number of calls to ISKBVALID during one call to QX(\u3008K,B,P ,N \u3009R) is in O(|C| log |K||C| ) where C is the output of QX(\u3008K,B,P ,N \u3009R).\nFor any other definition of the function SPLIT, the worst case number of ISKBVALID invocations gets larger."}, {"heading": "4.4.2 Correctness of Conflict Set Computation", "text": "This section is dedicated to the proof of correctness of Algorithm 1. First, we show some essential properties of QX by various Lemmata which will finally be exploited to demonstrate the overall soundness of QX.\nThe QX algorithm accepts a DPI \u3008Korig,Borig,P ,N \u3009R over some monotonic language L as input and returns a minimal conflict set C \u2286 Korig w.r.t. \u3008Korig,Borig,P ,N \u3009R as output. First, the algorithm checks whether Korig is a valid KB w.r.t. the input DPI \u3008\u00b7,Borig,P ,N \u3009R (line 2). If so, there is no conflict set for the DPI by Proposition 4.1 and the algorithm returns \u2019no conflict\u2019. Otherwise, the test Korig = \u2205 is performed (line 4). If so, then the negative outcome of the validity test executed in line 2 actually means that one of the two criteria of Proposition 3.4 is violated which, by Definition 3.6, implies that the DPI is not admissible. Invalidity of Korig w.r.t. \u3008\u00b7,Borig,P ,N \u3009R and non-admissiblity of \u3008Korig,Borig,P ,N \u3009R mean that there is only one minimal conflict set C = \u2205 by Proposition 4.2. Thus, \u2205 is returned in line 5.\nLemma 4.1. Let \u3008K,B,P ,N \u3009R be an admissible DPI and K be invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Then, there is a minimal conflict set C \u2283 \u2205 w.r.t. \u3008K,B,P ,N \u3009R.\nProof. The proposition is a direct consequence of Proposition 4.2."}, {"heading": "50 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "and are written in format C,K,B k\u00a9 where k is a counter starting from 1 that indicates when the respective call is made. A recursive call to QX\u2032 (left branch = call in line 16; right branch = call in line 17) is denoted by a normal arrow whereas the return of a set is visualized by a dashed arrow.\nSo, if both initial tests (lines 2 and 4) are negative, then, by Lemma 4.1, there is a non-trivial minimal conflict set w.r.t. \u3008Korig,Borig,P ,N \u3009R wherefore the algorithm enters the recursion by a call to the procedure QX\u2032.\nThe argumentation so far proves the following lemma.\nLemma 4.2.\n\u2022 QX(\u3008K,B,P ,N \u3009R) returns \u2019no conflict\u2019 iff there is no (minimal) conflict w.r.t. \u3008K,B,P ,N \u3009R.\n\u2022 QX(\u3008K,B,P ,N \u3009R) returns \u2205 iff \u2205 is the only (minimal) conflict w.r.t. \u3008K,B,P ,N \u3009R.\n\u2022 QX(\u3008K,B,P ,N \u3009R) returns QX\u2032(\u2205, \u3008K,B,P ,N \u3009R) iff there is some minimal conflict C \u2283 \u2205 w.r.t. \u3008K,B,P ,N \u3009R.\nCorollary 4.3. QX(\u3008K,B,P ,N \u3009R) returns QX\u2032(\u2205, \u3008K,B,P ,N \u3009R) iff \u3008K,B,P ,N \u3009R is an admissible DPI.\nProof. By the third proposition of Lemma 4.2 and Proposition 4.1 we have that QX(\u3008K,B,P ,N \u3009R) returns QX\u2032(\u2205, \u3008K,B,P ,N \u3009R) iff K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. By Proposition 4.2, we can then conclude that QX(\u3008K,B,P ,N \u3009R) returns QX\u2032(\u2205, \u3008K,B,P ,N \u3009R) iff \u3008K,B,P ,N \u3009R is an admissible DPI.\nThe input arguments (at any call) to QX\u2032 are (a) some subset C of the original input KB Korig to QX and (b) a DPI \u3008K,B,P ,N \u3009R where K \u2286 Korig and B \u2287 Borig."}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 51", "text": "The principle of QX\u2032 relies on the following fact.\nLemma 4.3. [Jun04] Let K1,K2 be a partition of K. If C2 is a minimal conflict set w.r.t. \u3008K2,B \u222a K1,P ,N \u3009R and C1 is a minimal conflict set w.r.t. \u3008K1,B\u222aC2,P ,N \u3009R, then C1 \u222aC2 is a minimal conflict set w.r.t. \u3008K1 \u222a K2,B,P ,N \u3009R = \u3008K,B,P ,N \u3009R.\nProof. Since C1 is a minimal conflict set w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R, we have that C1 is invalid w.r.t. \u3008\u00b7,B \u222a C2,P ,N \u3009R. From that we obtain that C1 \u222a C2 must be invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Further on, by the fact that K1,K2 partition K we have that C1 \u2286 K1 \u2286 K since C1 is a minimal conflict set w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R and C2 \u2286 K2 \u2286 K since C2 is a minimal conflict set w.r.t. \u3008K2,B \u222a K1,P ,N \u3009R. Consequently, C1\u222aC2 \u2286 Kmust be true. So, by Corollary 4.1, C1\u222aC2 is a conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nTo show the minimality of C1 \u222a C2, assume that C \u2282 C1 \u222a C2 is a minimal conflict set w.r.t. \u3008K, B,P ,N \u3009R. Due to K1 \u2229 K2 = \u2205 and C1 \u2286 K1 and C2 \u2286 K2, it must hold that C1 \u2229 C2 = \u2205. Thus, (1) C \u2229 C1 \u2282 C1 or (2) C \u2229 C2 \u2282 C2.\nLet us assume (1) holds. Then, C is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R, i.e. C\u222aB\u222aUP = (C\u20321\u222aC2)\u222aB\u222aUP = C\u20321\u222a(B\u222aC2)\u222aUP violates some r \u2208 R or some n \u2208 N where C\u20321 \u2282 C1. This, however, is a contradiction to the minimality of the conflict set C1 w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R.\nNow, let us assume (2) holds. Then, C is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R, i.e. C \u222a B \u222a UP = (C1 \u222a C\u20322) \u222a B \u222a UP violates some r \u2208 R or some n \u2208 N where C\u20322 \u2282 C2. By monotonicity of L and C1 \u2286 K1, this implies C\u20322 \u222a (K1 \u222a B) \u222a UP violates some r \u2208 R or some n \u2208 N , i.e. C\u20322 \u2282 K2 is a conflict set w.r.t. \u3008K2,B \u222a K1,P ,N \u3009R which is a contradiction due to C\u20322 \u2282 C2 and the minimality of the conflict set C2 w.r.t. \u3008K2,B \u222a K1,P ,N \u3009R.\nQX\u2032(C, \u3008K,B,P ,N \u3009R) computes a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R in a divide-and-conquer fashion whereby the argument C is the set of sentences of Korig that has been added to B in the current iteration. That is, in this iteration QX\u2032 will output either (1) \u2205 if the current B (which includes C) already contains a minimal conflict set w.r.t. the original DPI \u3008Korig,Borig,P ,N \u3009R or (2) a minimal conflict set w.r.t. the current DPI \u3008K,B,P ,N \u3009R (i.e. a subset of a minimal conflict set w.r.t. the original DPI) which does not include any sentence from C.\nLemma 4.4.\n1. For each call QX\u2032(C, \u3008K,B,P ,N \u3009R) within Algorithm 1 it holds that C \u2286 B.\n2. If QX\u2032(C, \u3008K,B,P ,N \u3009R) is called in line 16 of Algorithm 1, C 6= \u2205 holds.\n3. If QX\u2032(C, \u3008K,B,P ,N \u3009R) returns \u2205, then there is some non-empty minimal conflict set w.r.t. \u3008C,B\\ C,P ,N \u3009R.\n4. If QX\u2032(C, \u3008K,B,P ,N \u3009R) returns \u2205, then \u2205 is the only minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n5. QX\u2032(C, \u3008K,B,P ,N \u3009R) terminates.\nProof. 1): There are three situations when QX\u2032(C, \u3008K,B,P ,N \u3009R) is called within Algorithm 1, namely in lines 7, 16 and 17. In line 7, C := \u2205 \u2286 B holds. In line 16, C := K1 \u2286 B \u222a K1 =: B holds. In line 17, C := C2 \u2286 B \u222a C2 =: B holds.\n2): In line 16, QX\u2032 is called with C := K1, which is always not the empty set due to the definition of the SPLIT function in line 13 that is used to extract K1 from K.\n3): The first observation is that QX\u2032(C, \u3008K,B,P ,N \u3009R) cannot return \u2205 if C = \u2205 as in this case the first condition in line 9 is not met. Thus, in particular, QX\u2032 cannot return \u2205 if called in line 7.\nSo, \u2205 can be returned by QX\u2032(C, \u3008K,B,P ,N \u3009R) only if it is called (1) in line 16 or (2) in line 17."}, {"heading": "52 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "If QX\u2032(C, \u3008K,B,P ,N \u3009R) returns \u2205, then C 6= \u2205 and B is invalid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R (line 9), i.e. B contains a minimal conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R which is non-empty by Proposition 4.2 since \u3008B, \u2205,P ,N \u3009R is an admissible DPI by admissibility of the input DPI and the invariance of P ,N ,R throughout QX\u2032. Additionally, C \u2286 B holds by the first proposition of this lemma. Now, assume that there is no non-empty (minimal) conflict set w.r.t. \u3008C,B \\ C,P ,N \u3009R. Then, for each minimal conflict set C\u2032 (which we know is non-empty) w.r.t. \u3008B, \u2205,P ,N \u3009R it must hold that C \u2229 C\u2032 = \u2205, i.e. there is already a non-empty minimal conflict set w.r.t. \u3008B \\ C, \u2205,P ,N \u3009R.\nCase (1): Let us assume first that the call to QX\u2032 was made in line 16. Then, before this call to QX\u2032, B was exactly B \\ C. By the second proposition of this lemma, C 6= \u2205 as QX\u2032 was called in line 16. Thus, before the current call to QX\u2032, the algorithm must have already returned \u2205 (both conditions in line 9 are met) in line 10 which is a contradiction to the assumption that QX\u2032(C, \u3008K,B,P ,N \u3009R) was called in line 16.\nCase (2): Now, assume that the call to QX\u2032(C2, \u3008K1,B \u222a C2,P ,N \u3009R) was made in line 17. Then C2 is the result of the call to QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R) in line 16. By the argumentation above, we have that C2 6= \u2205 and there is a non-empty minimal conflict set w.r.t. \u3008B \u222a C2, \u2205,P ,N \u3009R. Moreover, we have that there is a non-empty minimal conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R. However, as QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R) in line 16 did not return \u2205 andK1 6= \u2205 by the second proposition of this lemma, it must hold that B \u222a K1 is valid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R, i.e. there is no (minimal) conflict set w.r.t. \u3008B \u222a K1, \u2205,P ,N \u3009R. By monotonicity of L, this is a contradiction to the fact that there is a non-empty minimal conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R.\n4): Assume QX\u2032(C, \u3008K,B,P ,N \u3009R) returns \u2205 and there is some non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Since \u2205 is returned, both conditions in line 2 must be met, i.e. in particular B must be invalid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R which means that \u3008K,B,P ,N \u3009R is not admissible. By Proposition 4.2, there cannot be a non-empty (minimal) conflict set w.r.t. \u3008K,B,P ,N \u3009R. This yields a contradiction.\n5): QX\u2032(C, \u3008K,B,P ,N \u3009R) either returns \u2205 in line 10 iff the conditions in line 9 are met or otherwise returns K in line 12 iff |K| = 1 or otherwise calls itself recursively in lines 16 and 17. However, for each recursive call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) within QX\u2032(C, \u3008K,B,P ,N \u3009R) it holds that K\u2032 \u2282 K as K\u2032 \u2208 {K1,K2} and K1,K2 \u2282 K due to the definition of the SPLIT function in line 13 that is used to computeK1 andK2 fromK in lines 14 and 15. Hence, each recursive call must finally reach the stopping criterion |K| = 1 and return K if it does not reach the stopping criterion in line 9 before.\nLemma 4.5. Let \u3008K,B,P ,N \u3009R be an admissible DPI. If QX\u2032(C, \u3008K,B,P ,N \u3009R) is called, then at least one of the immediate recursive calls of QX\u2032 in line 16 or line 17 is given an admissible DPI as argument.\nProof. Let us assume that \u3008K,B,P ,N \u3009R is an admissible DPI. Within QX\u2032(C, \u3008K,B, P ,N \u3009R), the immediate recursive call is QX\u2032(K1, \u3008K2,B\u222aK1,P ,N \u3009R) in line 16 and QX\u2032(C2, \u3008K1,B\u222aC2,P ,N \u3009R) in line 17 where K1,K2 is a partition of K and C2 is the result of QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R). If \u3008K2,B \u222a K1,P ,N \u3009R is admissible, then the proposition of the lemma is fulfilled. So, assume that that \u3008K2,B \u222aK1,P ,N \u3009R is not admissible. Due to this non-admissibility, it must hold that B \u222aK1 is invalid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R, so the second condition in line 2 is met. As the call to QX\u2032(K1, \u3008K2,B\u222aK1,P ,N \u3009R) was made in line 16, it must be true by Lemma 4.4, prop. 2 that K1 6= \u2205 wherefore the first condition in line 2 is met as well. Thus, the result of the call of QX\u2032 in line 16 must be \u2205. So, the call of QX\u2032 in line 17 looks like QX\u2032(\u2205, \u3008K1,B,P ,N \u3009R). However, the DPIs \u3008K1,B,P ,N \u3009R and \u3008K,B,P ,N \u3009R are identical except for the first entries, i.e. K1 and K. We know that the latter DPI is admissible. Due to the fact that admissibility of a DPI is defined independently of the KB (the first entry of the DPI tuple), we have that \u3008K1,B,P ,N \u3009R must be admissible. This completes the proof.\nAs long as the algorithm goes downwards in the recursion tree (and has never gone upwards), (1) the invariant that a minimal conflict set exists for each recursive call to QX\u2032 holds, (2) each call to QX\u2032 that"}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 53", "text": "returns, returns a singleton or empty set and (3) the two calls to QX\u2032 immediately before going upwards in the recursion tree for the first time must both return either a singleton or an empty set.\nLemma 4.6 (QX: Downwards Correctness). Let \u3008K,B,P ,N \u3009R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then, the following propositions hold:\n1. Before line 18 has ever been reached during the execution of QX\u2032(C, \u3008K, B,P , N \u3009R), the following holds: If some call to QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) returns a set S, then S = \u2205 or |S| = 1.\n2. Before line 18 has ever been reached during the execution of QX\u2032(C, \u3008K, B,P , N \u3009R), the following holds: If QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) is recursively called, then there is some non-empty minimal conflict set w.r.t. \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R.\n3. Before line 18 has ever been reached during the execution of QX\u2032(C, \u3008K,B,P , N \u3009R), the following holds: If some call to QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) returns a set S, then S is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n4. When line 18 is reached for the first time, each of the calls to QX\u2032 immediately before in lines 16 and 17 must have returned \u2205 or some K with |K| = 1.\nProof. 1): Assume the opposite, i.e. some call to QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) returns a set S with |S| > 1 before line 18 has ever been reached. There are three places where QX\u2032 can return, namely in line 10, in line 12 or in line 18. However, in line 10, only \u2205 and in line 12 only a singleton set can be returned. That is, S must be returned in line 18 which is a contradiction to the assumption that line 18 has not yet been reached.\n2): Induction Base: The first recursive call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) can only occur at line 16 where C\u2032 = K1, K\u2032 = K2 and B\u2032 = B \u222a K1 and K1,K2 is a partition of K as per the definition of the SPLIT and GET functions in lines 13-15. So, K\u2032 \u222a C\u2032 = K and B\u2032 \\ C\u2032 = B. The latter holds since C\u2032 \u2286 K and for each DPI K \u2229 B = \u2205 holds by Definition 3.1. As there is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R we have that there is a non-empty minimal conflict set w.r.t. \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R by the fact that \u3008K,B,P ,N \u3009R = \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R. Thus, the existence of a non-empty minimal conflict set w.r.t. \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R is given during the execution of the first recursive call to QX\u2032.\nInduction Assumption: Now, let us assume that the existence of a non-empty minimal conflict set w.r.t. \u3008K \u222a C,B \\ C,P ,N \u3009R is given during some call QX\u2032(C, \u3008K,B,P , N \u3009R). The goal is now to show that the existence of a non-empty minimal conflict set w.r.t. \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R is given during any recursive call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) that is invoked during execution of QX\u2032(C, \u3008K,B,P ,N \u3009R).\nInduction Step: Now, there are three cases where this recursive call to QX\u2032 can take place, namely (1) in line 16, (2) in line 17 where the result of QX\u2032 in line 16 is C2 = \u2205 and (3) in line 17 where the result of QX\u2032 in line 16 is some C2 with |C2| = 1. The case where some C2 with |C2| > 1 is returned by QX\u2032 in line 16, is impossible due to the assumption that line 18 has not yet been reached and the first proposition of this lemma.\nCase (1): Let us assume that the call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) is made in line 16. Since that call is made within QX\u2032(C, \u3008K,B,P ,N \u3009R), it must hold that some condition in line 2 during QX\u2032(C, \u3008K,B,P , N \u3009R) is violated, as otherwise a return would have taken place in line 10 which is a contradiction to the assumption that QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) is called in line 16.\nLet us first assume that C = \u2205 holds. In this case, the first condition in line 2 is violated and, by the Induction Assumption, it is true that there is a non-empty minimal conflict set w.r.t. the DPI \u3008K \u222a C,B \\ C,P ,N \u3009R which is equal to the DPI \u3008K,B,P ,N \u3009R by C = \u2205. So, an equal argumentation to the one of the Induction Base can be applied to derive that there is a non-empty minimal conflict set w.r.t. \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R."}, {"heading": "54 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "If C 6= \u2205 holds, on the other hand, then the first condition in line 2 is satisfied wherefore the second condition in line 2 must be violated. That is, there is no conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R. As there is a non-empty minimal conflict set w.r.t. \u3008K \u222a C,B \\ C,P ,N \u3009R by the Induction Assumption, C \u2286 B by Lemma 4.4, prop. 1 and |K| \u2265 2 by the fact that there was no return in line 12, there must be a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Again, an equal argumentation to the one of the Induction Base can be applied to derive that there is a non-empty minimal conflict set w.r.t. \u3008K\u2032 \u222a C\u2032,B\u2032 \\ C\u2032,P ,N \u3009R.\nCase (2): Here, we assume that the recursive call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) is made in line 17 and the result of QX\u2032 in line 16 is C2 = \u2205. So, it holds that C\u2032 = C2 = \u2205, K\u2032 = K1 and B\u2032 = B, i.e. the recursive call can be written as QX\u2032(\u2205, \u3008K1,B,P ,N \u3009R). By the fact that QX\u2032(K1, \u3008K2,B\u222aK1,P ,N \u3009R) called in line 16 returned \u2205, both conditions in line 2 during QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R) must have been met. Thus, in particular the existence of a non-empty minimal conflict set w.r.t. \u3008B \u222aK1, \u2205,P ,N \u3009R must be given. Further on, by the Induction Assumption there is a non-empty minimal conflict set w.r.t. \u3008C \u222a K,B \\ C,P ,N \u3009R.\nLet us first assume C = \u2205. In this case \u3008C \u222a K,B \\ C,P ,N \u3009R can be written as \u3008K,B,P ,N \u3009R and it holds that there is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R, i.e. K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. By Proposition 4.2, this implies that \u3008K,B,P ,N \u3009R is admissible. In other words, there is no conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R. Consequently, there must be a non-empty minimal conflict set w.r.t. \u3008K1,B,P ,N \u3009R.\nIf C 6= \u2205, on the other hand, then the second condition in line 2 during QX\u2032(C, \u3008K,B,P ,N \u3009R) must be invalid, i.e. there is no conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R. Consequently, there must be a non-empty minimal conflict set w.r.t. \u3008K1,B,P ,N \u3009R.\nCase (3): Here, we assume that the recursive call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) is made in line 17 and the result of QX\u2032 in line 16 is C2 6= \u2205. As C2 6= \u2205 and line 18 has never been reached by assumption, C2 must have been returned in line 12 of QX\u2032(K1, \u3008K2,B \u222aK1,P ,N \u3009R) (which was called in line 16) wherefore C2 = K2 must hold. So, it holds that C\u2032 = K2, K\u2032 = K1 and B\u2032 = B \u222a K2, i.e. the recursive call can be written as QX\u2032(K2, \u3008K1,B \u222a K2,P ,N \u3009R). By the Induction Assumption, there is a non-empty minimal conflict set w.r.t. \u3008C \u222a K,B \\ C,P ,N \u3009R. Moreover, C \u2286 B by Lemma 4.4, prop. 1 and (*) there is a nonempty minimal conflict set w.r.t. the DPI \u3008K,B,P ,N \u3009R which is equal to the DPI \u3008K1 \u222a K2,B,P ,N \u3009R by the fact that K1,K2 partition K as per the definition of the SPLIT and GET functions in lines 13-15.\nWhat must still be proven, is (*): Let us first assume that C = \u2205 holds. In this case, \u3008C \u222a K,B \\ C,P ,N \u3009R = \u3008K,B,P ,N \u3009R and thus there is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nIf C 6= \u2205, on the other hand, then the second condition in line 2 during QX\u2032(C, \u3008K,B,P ,N \u3009R) must be invalid as otherwise \u2205 would have been returned which is a contradiction to the assumption that the recursive call QX\u2032(C\u2032, \u3008K\u2032,B\u2032,P ,N \u3009R) was invoked in line 17. So, there is no conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R. Consequently, there must be a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R due to C \u2286 B by Lemma 4.4, prop. 1.\n3): Case S 6= \u2205: By S 6= \u2205 and the fact that line 18 has not yet been reached, we obtain by the first proposition of this lemma that |S| = 1 must hold.\nThere are two cases that can trigger QX\u2032(C, \u3008K,B,P ,N \u3009R) to return K with |K| = 1, i.e. case 1 involving C 6= \u2205 and case 2 involving C = \u2205.\nIn case 1, B must be valid w.r.t. \u3008\u00b7, \u2205,P ,N , \u3009R as otherwise \u2205 would be returned in line 10. So, there is no (minimal) conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R.\nAs |K| = 1 by assumption and by the fact that C \u2286 B (holds by Lemma 4.4, prop. 1) and there is some non-empty minimal conflict set w.r.t. \u3008K\u222aC,B\\C,P ,N \u3009R (holds by the second proposition of this lemma), K must include a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Since the only proper subset of K is the empty set, K must be a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nCase 2 can arise only when QX\u2032(C, \u3008K,B,P ,N \u3009R) is called in line 7 or line 17. In line 16 QX\u2032 is called with C 6= \u2205 by Lemma 4.4, prop. 2.\nIn line 7 QX\u2032 is called with C = \u2205 and, by Corollary 4.3, with an admissible DPI \u3008K,B,P ,N \u3009R for"}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 55", "text": "which a non-empty minimal conflict set exists as arguments. By the second proposition of this lemma, there is some non-empty minimal conflict set w.r.t. \u3008K \u222a \u2205,B \\ \u2205,P ,N \u3009R = \u3008K,B,P ,N \u3009R, and, by admissibility of \u3008K,B,P ,N \u3009R, there is no (minimal) conflict set w.r.t. \u3008B, \u2205,P ,N \u3009R. By |K| = 1, K must be a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nA necessary condition for QX\u2032 to be called with C = \u2205 in line 17 is obviously that QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R) called in line 16 returns \u2205. By the Lemma 4.4, prop. 3, there is some non-empty minimal conflict set w.r.t. \u3008K1,B,P ,N \u3009R. In line 17, the call QX\u2032(\u2205, \u3008K1,B,P ,N \u3009R) is made which, by assumption, returns K1 with |K1| = 1. That means K1 is a minimal conflict set w.r.t. \u3008K1,B,P ,N \u3009R.\nCase S = \u2205: Here, both conditions in line 2 must be met, i.e. in particular B is invalid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R which implies thatK is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R and \u3008K,B,P ,N \u3009R is admissible. Therefore, by Proposition 4.2, there is no non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. However, since K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R, there must be a conflict set w.r.t. \u3008K,B,P ,N \u3009R. So, there is only the empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n4): This proposition is an immediate consequence of the first proposition of this lemma.\nLemma 4.7. Let \u3008K,B,P ,N \u3009R be a non-admissible DPI. Then, \u2205 is the only minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R and QX\u2032(C, \u3008K,B,P ,N \u3009R) with C 6= \u2205 returns \u2205 immediately in line 10.\nProof. Since \u3008K,B,P ,N \u3009R is non-admissible, B \u222a UP violates some r \u2208 R or B \u222a UP |= n for some n \u2208 N . Therefore, \u2205 is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R, which, by Corollary 4.1, implies that \u2205 is a (minimal) conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nQX\u2032(C, \u3008K,B,P ,N \u3009R) returns \u2205 in line 10 as both conditions in line 9 are satisfied due to C 6= \u2205 and the non-admissibility of \u3008K,B,P ,N \u3009R.\nLemma 4.8. Let \u3008K,B,P ,N \u3009R be an admissible DPI. Then QX\u2032(C, \u3008K,B,P ,N \u3009R) does not return in line 10.\nProof. By Definition 3.6, B must be valid w.r.t. \u3008\u00b7, \u2205,P ,N \u3009R. Hence, the second condition in line 9 is not satisfied wherefore a return cannot take place in line 10.\nLemma 4.9. Let \u3008K,B,P ,N \u3009R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then the following holds: When QX\u2032(C, \u3008K,B,P ,N \u3009R) reaches line 18 for the first time, C1 \u222a C2 is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nProof. The premises of this lemma are the same as those of Lemma 4.6. By Lemma 4.6, prop. 4 we know that for C2 and C1 that are returned by the the calls to QX\u2032 in lines 16 and 17 |C1| \u2264 1 and |C2| \u2264 1 holds. Moreover, we know by Lemma 4.3 that C1 \u222a C2 is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nWhat remains open is to show that C1 \u222a C2 6= \u2205. To this end, we first assume that C 6= \u2205. Then, by Lemma 4.7, \u3008K,B,P ,N \u3009R must be an admissible DPI since it does not return in line 10, but only in line 18.\nIf, on the other hand, C = \u2205 holds, we can apply Lemma 4.6, prop. 2 to obtain that there is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. This implies that K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Therefore, we can conclude by means of Proposition 4.2 that \u3008K,B,P ,N \u3009R is an admissible DPI.\nThus, in both cases we have that \u3008K,B,P ,N \u3009R is an admissible DPI. Applying Lemma 4.5 yields that at least one recursive call to QX\u2032 in lines 16 and 17 is given an admissible DPI as argument. By Lemma 4.8, this call cannot return in line 10. So, it must return in line 12 by the assumption that line 18 has not yet been reached before, wherefore it must return a set of cardinality 1. This completes the proof.\nAs long as the algorithm goes upwards after going upwards for the first time, a non-empty minimal conflict set is propagated upwards."}, {"heading": "56 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Lemma 4.10 (QX: Upwards Correctness). Let \u3008K,B,P ,N \u3009R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then: After QX\u2032(C, \u3008K,B,P ,N \u3009R) has reached line 18 for the first time, the following holds: As long as line 16 is not reached, each return in line 18 returns a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R.\nProof. The premises of this lemma are the same as those of Lemma 4.6. By Lemma 4.9 we know that a non-empty minimal conflict C set is returned at the first return that is made in line 18. As, by assumption, C is not the result C2 of a prior call to QX\u2032 in line 16, it must be the result C1 of a prior call to QX\u2032 in line 17. Since the premises of Lemma 4.6 are fulfilled, Lemma 4.6 can be applied. Since the call QX\u2032(K1, \u3008K2,B\u222aK1,P ,N \u3009) (that returned C2) in line 16 took place before line 18 was first reached, we have that C2 is a minimal conflict set w.r.t. \u3008K2,B \u222a K1,P ,N \u3009 by Lemma 4.6, prop. 3. By Lemma 4.3, we have that C2 \u222a C is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009. As long as line 16 is not reached, the same argumentation can be used to show that a minimal conflict set is returned in line 18.\nWhen the algorithm goes downwards again after going upwards for the first time, the invariant that that a minimal conflict set exists for each recursive downwards call to QX\u2032 holds.\nLemma 4.11 (QX: Downwards-after-upwards Correctness). Let \u3008K,B,P ,N \u3009R be an admissible DPI and let there be a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then: After QX\u2032(C, \u3008K,B,P , N \u3009R) has reached line 18 for the first time, the following holds: If line 16 is reached for the first time, then, if the DPI \u3008K1,B\u222aC2,P ,N \u3009R which is the argument to the immediate call QX\u2032(C2, \u3008K1,B\u222aC2,P ,N \u3009R) in line 17 is admissible, then there is a non-empty minimal conflict set w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R.\nProof. The premises of this lemma are the same as those of Lemma 4.6. Since line 16 is first reached after line 18 has been reached for the first time, it must hold that QX\u2032(K1, \u3008K2,B \u222a K1,P ,N \u3009R) in line 16 was called before line 18 has been reached. The reason for this to hold is the fact that only returns and no new calls to QX\u2032 can have been made between the first occurrence of line 18 and the next occurrence of line 16.\nTherefore, the result C2 of the call QX\u2032(K1, \u3008K2,B \u222aK1,P ,N \u3009R) in line 16 is a minimal conflict set w.r.t. \u3008K2,B \u222a K1,P ,N \u3009R due to Lemma 4.6, prop. 3. As a consequence, C2 \u222a B \u222a K1 \u222a UP violates some r \u2208 R or some N \u2208 N . As the DPI \u3008K1,B \u222a C2,P ,N \u3009R is admissible by assumption, it holds that C2 \u222a B \u222a UP does not violate any r \u2208 R or N \u2208 N . Hence, K1 must be invalid w.r.t. \u3008\u00b7,B \u222a C2,P ,N \u3009R which implies that there must be a non-empty minimal conflict set S w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R.\nBy applying the argumentation of Lemmas 4.6, 4.10 and 4.11 recursively on the entire recursion tree, we can prove the correctness of QX\u2032.\nLemma 4.12. If QX\u2032(C, \u3008Korig,Borig,P ,N \u3009R) is called in line 7 by Algorithm 1, it returns a non-empty minimal conflict set w.r.t. \u3008Korig,Borig,P ,N \u3009R.\nProof. If QX\u2032(C, \u3008Korig,Borig,P ,N \u3009R) is called in line 7 of Algorithm 1, it must be true, by Lemma 4.2, prop. 4.2 and Corollary 4.3, that \u3008Korig,Borig,P ,N \u3009R is an admissible DPI for which a non-empty minimal conflict set exists. As a consequence, the premises of Lemma 4.6 are met for \u3008Korig,Borig,P ,N \u3009R.\nThere are two cases to consider: Either (a) |Korig| \u2264 1 or (b) |Korig| > 1 for the initial call to QX\u2032(C, \u3008Korig,Borig,P ,N \u3009R) in line 7. In case (a), 0 = |Korig| < 1 cannot hold as there must be a nonempty minimal conflict set C w.r.t. \u3008Korig,Borig,P ,N \u3009R due to Lemma 4.2, prop. 4.2. Since \u2205 \u2282 C \u2286 Korig must hold for C, this would be a contradiction to |Korig| = 0.\nSo, |Korig| = 1 holds in case (a). In this case, QX\u2032 returns Korig immediately in line 12, since C = \u2205 and thus the conditions checked in line 9 cannot be met. In this case, Korig is indeed a nonempty minimal conflict set since for the DPI \u3008Korig,Borig,P ,N \u3009R given as argument there is a non-empty minimal conflict set by Lemma 4.2, prop. 4.2. Therefore \u2205 cannot be a conflict set w.r.t. this DPI whereby Korig is the only possible minimal conflict set due to |Korig| = 1."}, {"heading": "4.4. METHODS FOR DIAGNOSIS COMPUTATION 57", "text": "Case (b): In this case, a direct return can neither take place in line 10 by C = \u2205 nor in line 12 by |Korig| > 1. So, QX\u2032 is called recursively in lines 16 and 17. Since QX\u2032 terminates due to Lemma 4.2, prop. 5, QX\u2032 must reach line 18. The first time some recursive call QX\u2032(C, \u3008K,B,P ,N \u3009R) reaches line 18, it returns a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R due to Lemma 4.9.\nBy Lemma 4.10, as long as line 16 is not reached, i.e. no \u201cleft branch\u201d (call to QX\u2032 in line 16) but only \u201cright branches\u201d (calls to QX\u2032 in line 17) return, a minimal conflict set S is returned for each call to QX\u2032 that \u201cwraps\u201d (is higher in the recursion tree than) the call that was the first to reach line 18. It holds that S 6= \u2205 since S is a union of sets including the non-empty set returned when line 18 was first reached.\nWhen it comes to an execution of line 16, i.e. the left branch returns, then the algorithm will take the right branch by executing line 17, i.e. calling QX\u2032(C2, \u3008K1,B \u222a C2,P ,N \u3009R), and go downwards in the recursion tree.\nNow, there are two cases. First, \u3008K1,B \u222a C2,P ,N \u3009R is non-admissible. Then, by Lemma 4.7, there is only one minimal conflict set w.r.t. \u3008K1,B\u222aC2,P ,N \u3009R, namely \u2205, and QX\u2032(C2, \u3008K1,B\u222aC2,P ,N \u3009R) directly returns \u2205. As also the result C2 of the call to QX\u2032(K1, \u3008K2,B\u222aK1,P ,N \u3009R) immediately before in line 16 is a minimal conflict set w.r.t. \u3008K2,B\u222aK1,P ,N \u3009R, as established above, we can apply Lemma 4.3 to derive that indeed a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R is returned in line 18. Thus, Lemma 4.10 can be further applied to move upwards in the recursion tree until line 16 occurs again.\nSecond, \u3008K1,B \u222a C2,P ,N \u3009R is admissible. Then, by Lemma 4.11, there is a non-empty minimal conflict set w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R. Hence, Lemma 4.6 can be used again for the subtree of the recursion tree rooted at the call QX\u2032(C2, \u3008K1,B \u222a C2,P ,N \u3009R). That is, it can be used to show that each call to QX\u2032 within this subtree returns a minimal conflict set w.r.t. the DPI given as argument as long as the algorithm moves downwards in the tree. Having reached line 18 for the first time, Lemma 4.9 lets us conclude again that a non-empty conflict set w.r.t. the respective argument DPI is actually returned at this place. Subsequently, Lemma 4.10 can be applied to show that each return gives back a minimal conflict set w.r.t. the argument DPI of the respective call, as long as the algorithm moves upwards in the recursion tree.\nWhat is still open is to show that the call QX\u2032(C2, \u3008K1,B \u222a C2,P ,N \u3009R) in line 17 that is made immediately after the algorithm first reached line 16 after moving upwards after reaching line 18 for the first time returns a minimal conflict set w.r.t. \u3008K1,B \u222a C2,P ,N \u3009R, indeed. This holds by the fact that Lemmas 4.6 and 4.10 guarantee that a left branch always returns a minimal conflict set, Lemma 4.11 guarantees that Lemmas 4.6 and 4.10 can be applied after making a single right branch. However, as QX\u2032 terminates the recursion tree is finite and thus the case must arise where the right branch directly returns. In case the DPI \u3008K,B,P ,N \u3009R given as argument for this right branch is non-admissible, the only minimal conflict set \u2205 is returned, as established above. If the DPI \u3008K,B,P ,N \u3009R given as argument for this right branch is admissible, on the other hand, then we have already shown above that there is a non-empty minimal conflict set w.r.t. this DPI. Moreover, |K| = 1 must hold due to the fact that this right branch directly returns (without entering a further recursion). Therefore, K is returned which is actually a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R as K is the only non-empty subset of K.\nProposition 4.9. Let \u3008K,B,P ,N \u3009R be a DPI. Then, QX(\u3008K,B,P ,N \u3009R) terminates and returns \u2022 \u2019no conflict\u2019 iff there is no conflict w.r.t. \u3008K,B,P ,N \u3009R\n(K is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R)\n\u2022 \u2205 iff \u2205 is the only minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R (DPI is non-admissible)\n\u2022 a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R iff there is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R (DPI is admissible and K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R).\nProof. The proposition is a direct consequence of Lemma 4.2 and Lemma 4.12."}, {"heading": "58 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": ""}, {"heading": "4.5 Hitting Set Tree Based Diagnosis Computation", "text": "One way to compute minimal diagnoses from minimal conflict sets is to use a hitting set tree algorithm which was originally proposed by Reiter [Rei87]. In this work we describe methods for non-interactive and interactive diagnosis computation based on the ones used in [FS05, SF10, SFFR12] which are closely related to the original hitting set tree algorithm. Differences of the described non-interactive algorithm to the original one of Reiter are\n1. the usage of different edge weights (probabilities) inducing an order of node generation (uniformcost) different to breadth-first and\n2. the opportunity to specify an execution time threshold t as well as a minimal (nmin) and maximal (nmax) desired number of minimal diagnoses to be computed by the algorithm.\nIn this vein, the algorithm computes at least the nmin most-probable minimal diagnoses w.r.t. the given probabilities and goes on computing further next most-probable minimal diagnoses until either overall computation time reaches the time limit t or nmax diagnoses have been computed.\nSuch a time threshold and an interval of minimal and maximal number of diagnoses is particularly relevant in settings where not all potential minimal faulty sets need to be computed, such as iterative, interactive settings where reaction time is crucial (since a user is waiting to interact with the system). Instead, in such settings only a \u201crepresentative\u201d set of minimal diagnoses is exploited to decide which question to ask a user such that the answer to that question allows the constructed partial tree to be pruned. After pruning, the tree is expanded again to compute another \u201crepresentative\u201d set of minimal diagnoses. Such an interactive KB debugging algorithm will be presented in Part II. The non-interactive version of the KB debugging algorithm is delineated by Algorithm 2 and described next.\nInputs. The algorithm takes as input an admissible DPI \u3008K,B,P ,N \u3009R, some computation timeout t, a desired minimal (nmin) and maximal (nmax) number of minimal diagnoses to be returned, and a function p : K \u2192 (0, 0.5) that assigns to each formula ax \u2208 K a weight that represents the (estimated) likeliness of ax to be faulty and thereby determines the search strategy, e.g. breadth-first or uniform-cost. Within the algorithm, p() is used to impose an order on open nodes that tells the algorithm which node to expand next. Details concerning the function p() will be discussed in Section 4.6 after demonstrating various ways of obtaining information relevant to p() and detailing how p() can be defined by means of such information. Throughout the rest of the current Section 4.5 we assume that p() implies a first-in-first-out sorting of open nodes, i.e. a breadth-first search strategy as described in [Rei87]."}, {"heading": "4.5.1 Breadth-First Diagnosis Computation", "text": "Algorithm Overview and Implementation Remarks. To compute minimal diagnoses w.r.t. \u3008K,B,P , N \u3009R from minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R, the algorithm produces a labeled tree where a nonclosed node is labeled by a minimal conflict set and a closed node is labeled by either valid or closed. From a non-closed node labeled by a minimal conflict set C = {axp, . . . , ax q} there are |C| outgoing edges, each labeled by one ax \u2208 C and each leading to a new node that needs to be labeled. Closed nodes are leaf nodes of the produced tree, i.e. they have no successor nodes, and correspond to non-minimal or duplicate hitting sets (label closed) or to minimal hitting sets (label valid) of all minimal conflict sets w.r.t. the input DPI \u3008K,B,P ,N \u3009R. Conflict sets to label nodes are computed only on-demand for time efficiency after the attempt to reuse an already computed one fails. In case an appropriate order of node labeling (e.g. breadth-first tree construction) is used, the complete tree given when all nodes in the tree are closed contains all minimal diagnoses w.r.t. the DPI \u3008K,B,P ,N \u3009R provided as input. In this complete tree, the set of edge labels on each path from the root node to a node labeled by valid is a minimal diagnosis."}, {"heading": "4.5. HITTING SET TREE BASED DIAGNOSIS COMPUTATION 59", "text": "What Algorithm 2 actually does is building up a pruned HS-tree for a given DPI. So, we next provide formal definitions of a (partial) HS-tree and a (partial) pruned HS-tree based on the definitions given in [Rei87].\nDefinition 4.7 (HS-Tree). Let \u3008K,B,P ,N \u3009R be an admissible DPI. An edge-labeled and node-labeled tree T is called an HS-tree w.r.t. \u3008K,B,P ,N \u3009R iff it is a smallest tree with the following properties:\n1. The root of T is labeled by valid if K is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Otherwise, the root is labeled by a conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n2. If n is a node of T , define H(n) to be the set of edge labels on the path in T from the root node to n. If n is labeled by valid, it has no successor nodes in T . If n is labeled by a conflict set C w.r.t. \u3008K,B,P ,N \u3009R, then for each ax \u2208 C, n has a successor node nax joined to n by an edge labeled by ax . The label for nax is a conflict set C\u2032 w.r.t. \u3008K,B,P ,N \u3009R such that C\u2032 \u2229H(nax ) = \u2205 if such a set C\u2032 exists. Otherwise, nax is labeled by valid.\nT is called a partial HS-tree w.r.t. \u3008K,B,P ,N \u3009R iff T is a HS-tree w.r.t. \u3008K,B,P ,N \u3009R where not all nodes in T are labeled and non-labeled nodes have no successors.\nDefinition 4.8 (Pruned HS-Tree). Let \u3008K,B,P ,N \u3009R be an admissible DPI. An edge-labeled and nodelabeled tree T is called a pruned HS-tree (pHS-tree) w.r.t. \u3008K,B,P ,N \u3009R iff T is the result of constructing an HS-tree w.r.t. \u3008K,B,P ,N \u3009R with due regard to the following rules:\n1. Label nodes in the HS-tree in breadth-first order.\n2. Use only minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R to label nodes in T .\n3. Reusing node labels: If node n is labeled by C and n\u2032 is a node such that H(n\u2032) \u2229 C = \u2205, label n\u2032 by C.\n4. Non-minimality pruning rule: If node n is labeled by valid and node n\u2032 is such thatH(n) \u2286 H(n\u2032), label n\u2032 by closed.\n5. If node n is labeled by closed, it has no successors.\n6. Duplicate pruning rule: If node n is next to be labeled and there is some node n\u2032 such that H(n\u2032) = H(n), then label n by closed.\nT is called a partial pruned HS-tree iff T is a pruned HS-tree where not all nodes in T have been labeled yet and non-labeled nodes have no successors.\nRemark 4.1 Notice that we use a definition of a pruned HS-tree that slightly differs from the definition given in [Rei87] in that we inherently assume that only minimal conflict sets w.r.t. the given DPI are used to label nodes in the tree. Therefore we could omit the last rule in the definition of [Rei87]. Namely, such a situation where some node has been labeled by a subset of the label of another node cannot arise in our definition since no minimal conflict set can be a subset of another different minimal conflict set w.r.t. the same DPI.\nIn general, there are multiple different pHS-trees w.r.t. one and the same DPI [GSW89]. Reason for this is that\n\u2022 the order of adding successor nodes (on the same tree level) to the queue Q and\n\u2022 which of generally multiple minimal conflict sets to (re)use to label a node"}, {"heading": "60 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "is not determined by Definition 4.8.\nBy [Rei87, Theorem 4.8] and Proposition 4.6, the following holds:\nProposition 4.10. Let \u3008K,B,P ,N \u3009R be an admissible DPI and T a pHS-tree w.r.t. \u3008K,B,P ,N \u3009R. Then, {H(n) | n is a node of T labeled by valid} = mD\u3008K,B,P,N \u3009R , i.e. the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nRemark 4.2 A node nd in Algorithm 2 is defined as the set of formulas that label the edges on the path from the root node to nd. In other words, we associate a node n with H(n). In this vein, Algorithm 2 internally does not store a labeled tree, but only \u201crelevant\u201d sets of nodes and conflict sets. That is, it does not store any\n\u2022 non-leaf nodes,\n\u2022 labels of non-leaf nodes, i.e. it does not store which minimal conflict set labels which node,\n\u2022 edges between nodes,\n\u2022 labels of edges and\n\u2022 leaf nodes labeled by closed.\nLet T denote the (partial) pHS-tree produced by Algorithm 2 at some point during its execution (Corollary 4.4 will show that Algorithm 2 using breadth-first search in fact produces a (partial) pHS-tree). Then, Algorithm 2 only stores\n\u2022 a set of nodes Dcalc where each node corresponds to the edge labels along a path in T leading to a leaf node that has been labeled by valid (minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R),\n\u2022 a list of open (non-closed) nodes Q where each node in Q corresponds to the edge labels along a path in T leading from the root node to a leaf node that has been generated, but has not yet been labeled and\n\u2022 the set Ccalc of already computed minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R that have been used to label non-leaf nodes in T .\nWe call \u3008Dcalc,Q,Ccalc\u3009 the relevant data of T . If T is a pHS-tree, then Q is the empty list. This internal representation of the constructed (partial) pHS-tree by its relevant data does not constrain the functionality of the algorithm. This holds as diagnoses are paths from the root, i.e. nodes in the internal representation, and the goal of a (partial) pHS-tree is to determine minimal diagnoses w.r.t. the given DPI. The node labels or edge labels along a certain path and their order along this path is completely irrelevant when it comes to finding a label for the leaf node of this path. Instead, only the set of edge labels is required for the computation of the label for a leaf node. Also, to rule out nodes corresponding to nonminimal diagnoses, it is sufficient to know the set of already found diagnoses Dcalc. No already closed nodes are needed for the correct functionality of Algorithm 2.\nInitialization. First, Algorithm 2 initializes the variable tstart with the current system time (GETTIME), the set of calculated minimal diagnoses Dcalc to the empty set and the ordered queue of open nodes Q to a list including the empty set only (i.e. only the unlabeled root node)."}, {"heading": "4.5. HITTING SET TREE BASED DIAGNOSIS COMPUTATION 61", "text": "The Main Loop. Within the loop (line 5) the algorithm gets the node to be processed next, namely the first node node (GETFIRST, line 6) in the list of open nodes Q ordered by the function pnodes() and removes node from Q (DELETEFIRST, line 7). Note that pnodes() can be directly obtained from p(). As mentioned before, for the moment the reader should simply suppose that pnodes() imposes an order on Q which effectuates a breadth-first labeling of open nodes in the tree. A definition of pnodes() will be given by Definition 4.9 after a motivation and detailed explanation of pnodes() will have been given in Section 4.6.\nComputation of Node Labels. Then, a label is computed for node in line 8. Nodes are labeled by valid, closed or a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R by the procedure LABEL (line 18 ff.). This procedure gets as inputs the DPI \u3008K,B,P ,N \u3009R, the current node node, the set of already computed minimal conflicts (Ccalc) and minimal diagnoses (Dcalc) and the queue Q of open nodes, and it returns an updated set of computed minimal conflicts Ccalc and a label for node. It works as follows:\nA node node is labeled by closed iff (a) there is an already computed minimal diagnosis D in Dcalc that is a subset of this node, i.e. D \u2286 node, which means that node cannot be a minimal diagnosis (nonminimality criterion, lines 19-21) or (b) there is some node nd in the queue of open nodes Q such that node = nd which means that one of the two tree branches with an equal set of edge labels can be closed, i.e. removed from Q (duplicate criterion, lines 22-24).\nIf none of these closed-criteria is met, the algorithm searches for some C in Ccalc, the set of already computed minimal conflict sets, such that C \u2229 node = \u2205 and returns the label C for node (reuse criterion, lines 25-27). This means that the path represented by node cannot be a diagnosis as there is (at least) one minimal conflict set, namely C, that is not hit by node.\nIf the reuse criterion does not apply, a call to QX(\u3008K \\ node,B,P ,N \u3009R) is made (line 28) in order to check whether there is a not-yet-computed minimal conflict set that is not hit by node. Note that the KB K \\ node that is given to QX as part of the argument DPI ensures that only minimal conflict sets C \u2286 K\\node can be computed, i.e. ones that do not share any single formula with node (cf. Section 4.4.1).\nRemark 4.3 A minimal conflict set computed by QX(\u3008K \\ node,B,P ,N \u3009R) is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R indeed since (i) QX(\u3008K \\ node,B,P ,N \u3009R) returning a set C means that C is a minimal conflict set w.r.t. \u3008K \\ node,B,P ,N \u3009R by Proposition 4.9 and (ii) the \u201c\u21d2\u201d direction of Corollary 4.1 implies that C is not valid w.r.t. \u3008\u00b7,B,P ,N \u3009R and (iii) the \u201c\u21d0\u201d direction of Corollary 4.1 lets us conclude that C is a minimal conflict w.r.t. \u3008X,B,P ,N \u3009R where X is any superset of C, in particular X := K.\nQX may then return (a) \u2019no conflict\u2019, i.e. K \\ node is already valid w.r.t. \u3008\u00b7,B,P ,N \u3009R, or (b) a new conflict set L 6= \u2205 such that L /\u2208 Ccalc. Note that the case of the output L = \u2205 of QX cannot arise since (i) the DPI provided as input to the algorithm is assumed to be admissible, (ii) no other DPI for which QX is called can be non-admissible since admissibility is defined only by the sets B,P ,N ,R which remain unmodified throughout the execution of Algorithm 2, and (iii) as per Proposition 4.9, QX returns \u2205 only if the DPI given to it as an argument is non-admissible. Further on, we point out that the conflict set L in case (b) must be a new conflict set since the reuse criterion is always checked before the call to QX and thus must be negative. That is, each C \u2208 Ccalc is hit by node and L is not hit by node wherefore L 6= C must hold for all C \u2208 Ccalc.\nIn each of the described cases, the LABEL procedure returns a tuple including the respective label as explained and the set Ccalc where Ccalc is equal to the input argument Ccalc in all cases except for the case where a new minimal conflict set is computed by QX. In this case, the newly computed conflict set is added to Ccalc (line 32) before the procedure returns.\nProcessing of a Node Label. Back in the main procedure, Ccalc is updated (line 9) and then the label L returned by procedure LABEL is processed as follows:"}, {"heading": "62 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "If L = valid, then there is no minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R that is not hit by (i.e. has an empty intersection with) the current node node. Thus, node is added to the set of calculated minimal diagnoses Dcalc. Minimality of diagnoses added to Dcalc is guaranteed by the pruning rule (lines 19- 21) which eliminates non-minimal nodes (paths) and the way the tree is built level by level by the used breadth-first strategy. In case a uniform-cost variant of tree construction is used, certain properties of the function p() need to be postulated to preserve this minimality guarantee. We discuss these properties in Section 4.6.\nIf, on the other hand, L = closed is the returned label of the procedure LABEL, then there is either a minimal diagnosis in Dcalc that is a subset of the current node node or a duplicate of node is already included in Q. Consequently, node must simply be removed from Q which has already been executed in line 7.\nIn the third case, if a minimal conflict set L is returned in line 8, then L is a label for node meaning that |L| successor nodes of node need to be added to Q in sorted order using the function pnodes() (INSERTSORTED, line 15), as will be explained in more detail in Section 4.6.\nRecap. To summarize, in each iteration, the node node that is the first element of the queue Q is deleted from Q and,\n1. if node is a diagnosis, it is added to the set Dcalc\n2. if there is some diagnosis in Dcalc that is a proper subset of node or node is equal to some other node in Q, no action is performed, i.e. the algorithm deletes node without substitution\n3. if there is some minimal conflict set that node does not hit, then such a conflict set C is computed and for each ax \u2208 C a new node node \u222a {ax} is added to Q.\nWe call each node nd that is added to Q in the latter case a successor of the node node."}, {"heading": "4.5.2 Correctness of Breadth-First Diagnosis Computation", "text": "For the discussion of the output of Algorithm 2 we will exploit the following result saying that Algorithm 2 computes all and only minimal diagnoses, if it executes until the queue of open nodes becomes the empty set.\nProposition 4.11 (Soundness and Completeness of Algorithm 2 using Breadth-First Search). Let \u3008K, B,P ,N \u3009R be an admissible DPI given as input to Algorithm 2. If Algorithm 2 using a breadth-first tree construction strategy terminates due to Q = [], then the algorithm returns exactly the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nProof. This proposition is a consequence of Proposition 4.10 and the following Lemma 4.13 which witnesses that Algorithm 2 using a breadth-first tree construction strategy produces a pHS-tree as per Definition 4.8.\nLemma 4.13. Algorithm 2 with the admissible input DPI \u3008K,B,P ,N \u3009R using a breadth-first tree construction strategy is a procedure for producing a pHS-tree T w.r.t. \u3008K,B,P ,N \u3009R.\nProof. We verify whether all rules given by Definitions 4.7 and 4.8 are satisfied by Algorithm 2.\n\u2022 Definition 4.7, rule 1: The root node \u2205 which is the only element of the initial list Q is labeled by the first call to LABEL for node := \u2205 in line 8. If valid is returned, then QX(\u3008K,B,P ,N \u3009R) must have returned \u2019no conflict\u2019 which is the case if K is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R."}, {"heading": "4.5. HITTING SET TREE BASED DIAGNOSIS COMPUTATION 63", "text": "Otherwise, if valid is not returned by LABEL, then some minimal conflict setLw.r.t. \u3008K,B,P ,N \u3009R must have been returned in line 33. L is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R by Proposition 4.9 and since QX(\u3008K,B,P ,N \u3009R) has not returned \u2019no conflict\u2019 as otherwise valid would have been returned contradicting our assumption and since \u3008K,B,P ,N \u3009R is an admissible DPI by assumption. LABEL cannot have returned earlier in line 21 or line 24, since Dcalc is the empty set and Q the empty list at this time. The former holds since Dcalc is only extended in line 11 which cannot ever have been reached before the first call to LABEL has returned. The latter holds as Q initially contained only \u2205 and as \u2205 was deleted from Q in line 7 before the call to LABEL was made in line 8.\n\u2022 Definition 4.7, rule 2: Suppose a node node is labeled by valid, then it is added to Dcalc in line 11. Since node can only get a label different from closed if it is the only exemplar of this node in Q due to the duplicate criterion (lines 22-24), it must be the case that node /\u2208 Q (line 7) after node has been labeled by valid. Only nodes that get labeled by a conflict set can have successor nodes added to Q in line 15. Only nodes in Q can get a label (cf. lines 6 and 8). For node to be added to Q at some later point in time there must be a proper subset of node that is still in Q as each node newly added to Q is a proper superset of some node in Q (cf. line 15 which is the only position in the algorithm where nodes are added to Q). This is impossible due to the breadth-first tree construction strategy which implies that all nodes of cardinality |node| \u2212 1 have already been labeled (and thus deleted from Q in line 7) when node is being labeled. Hence, if node is labeled by valid, then it has no successors.\nIf node is labeled by some conflict set L, then Algorithm must come to line 15, where a successor node \u222a {e} is added to Q for all e \u2208 L. How node nodee := node \u222a {e} must be labeled is overridden by the rules 3, 4 and 6 of Definition 4.8 (see below).\n\u2022 Definition 4.8, rule 1: This is true by our assumption about p() and pnodes().\n\u2022 Definition 4.8, rule 2: This holds since QX(\u3008K\\node,B,P ,N \u3009R) computes only minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R (cf. Remark 4.3).\n\u2022 Definition 4.8, rule 3: All minimal conflict sets that have been used to label nodes so far are stored in Ccalc. Before a minimal conflict to label node might be computed by a call to QX in line 28, the reuse criterion in lines 25-27 checks whether there is a set C in Ccalc with C \u2229 node. If positive, C is returned as a label for node.\n\u2022 Definition 4.8, rule 4: This is accomplished by the non-minimality criterion in lines 19-21 which checks for existence of a node already labeled by valid which is a subset of the node to be labeled right now. All nodes labeled by valid are stored in Dcalc (cf. lines 10 and 11).\n\u2022 Definition 4.8, rule 5: If some node node is labeled by closed, then no action is performed (cf. line 12). Before each node is labeled in line 8, it is deleted from Q in line 7. That node cannot be inserted into Q at some later point in time follows from the argumentation used above to demonstrate that Definition 4.7, rule 2 is met.\n\u2022 Definition 4.8, rule 6: This is achieved by the duplicate criterion in lines 22-24 where Q is browsed for some node equal to the one that is to be labeled right now. When some node node is next to be labeled, then all duplicates of node must already be in Q as reasoned above in the argumentation to show that Definition 4.7, rule 2 is satisfied. Thus, the criterion must search for duplicates in no other collections than Q. Indeed, only one (i.e. the last non-deleted) exemplar of these duplicates of node in Q can get a label other than closed due to the duplicate criterion which closes duplicates as long as there are any."}, {"heading": "64 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "We conclude that Algorithm 2 is a procedure for constructing a pHS-tree.\nBy Proposition 4.11 and the fact that there is no place in Algorithm 2 where nodes are removed from Dcalc (which implies that only minimal diagnoses can be added to Dcalc), the following corollary is obvious.\nCorollary 4.4. Algorithm 2 with the admissible input DPI \u3008K,B,P ,N \u3009R using a breadth-first tree construction strategy stores by \u3008Dcalc,Q,Ccalc\u3009 the relevant data of\n\u2022 a pHS-tree w.r.t. \u3008K,B,P ,N \u3009R if Algorithm 2 stops due to Q = [],\n\u2022 a partial pHS-tree w.r.t. \u3008K,B,P ,N \u3009R otherwise.\nIf a pHS-tree is computed in breath-first order, minimal diagnoses are generated with increasing cardinality, as the following Corollary 4.5 attests. Consequently, for the generation of all minimum cardinality diagnoses, only the first level of the tree has to be generated, where a node is labeled.\nCorollary 4.5. The following holds for the set D returned by Algorithm 2 using breadth-first search: If D contains some diagnosis of cardinality k, then it includes all diagnoses w.r.t. \u3008K,B,P ,N \u3009R of cardinality lower than k.\nProof. By Proposition 4.11, it is a fact that Algorithm 2 computes all and only minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R. As these are computed in breadth-first order, the first computed diagnoses must be the minimum cardinality ones. To see this, assume that Algorithm 2 returns D which includes one nonminimum cardinality diagnosis D and does not comprise a minimum cardinality diagnosis D\u2032, i.e. |D| > |D\u2032|. By breadth-first search, nodes are labeled in ascending order of their cardinality. And, if the first node of cardinality k is labeled, no more nodes of cardinality k\u22121 can be in Q (cf. proof of Lemma 4.13). So, we have that the pHS-tree obtained by further execution of the algorithm until Q = [] can never label D\u2032 since |D| > |D\u2032| andD has already been labeled. Hence, the algorithm would not returnD\u2032 in its final output D. Since each minimum cardinality diagnosis is a minimal diagnosis, D\u2032 is a minimal diagnosis. Thus, we have a contradiction to the fact that the algorithm computes all minimal diagnoses.\nOutput. The repeat-loop is iterated until the stop criterion (line 16) applies. In case at least nmin minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R exist, there are two cases:\n\u2022 If the finding of the nmin-th minimal diagnosis happens after t\u2032 < t time has passed since the start of Algorithm 2, then the algorithm will continue iterating and terminate only if execution time amounts to at least t time or |D| = nmax at the time line 16 is processed.\n\u2022 Otherwise, if the detection of the nmin-th minimal diagnosis takes place after processing longer than t time, then the algorithm will terminate immediately after having determined the nmin-th minimal diagnosis.\nIn both cases, the output is a set D of minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R such that nmin \u2264 |D| \u2264 nmax and D is the set of best minimal diagnoses as per p(), in this case the set of minimal diagnoses with minimum cardinality since p() is assumed to be specified as to cause a breadth-first tree construction.\nIf fewer than nmin minimal diagnoses exist w.r.t. \u3008K,B,P ,N \u3009R, then Q = [] will be the cause for the algorithm to terminate. In this case, the pHS-tree w.r.t. \u3008K,B,P ,N \u3009R has been built up and all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R are stored in Dcalc. Thus, the output is the set mD\u3008K,B,P,N \u3009R of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R."}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 65", "text": "Termination. The next proposition shows that Algorithm 2 must yield a set of minimal diagnoses after finite time.\nProposition 4.12. Algorithm 2 always terminates.\nProof. This is due to the fact that minimal conflict sets used to label non-leaf nodes are subsets of K and that nodes in Q are subsets of K, which is a finite set by Definition 3.1. Moreover, a node in Q is either deleted without substitution from Q if valid or closed (line 7) or deleted (line 7) and replaced by proper supersets of it (INSERTSORTED in line 15). This means that the cardinality of all nodes in Q is strictly monotonically increasing. Thus each node (path) node is guaranteed to be closed (valid or closed) when node = K as in this case node must hit all possible (minimal) conflict sets Ci w.r.t. \u3008K,B,P ,N \u3009R since Ci \u2286 K holds by Definition 4.1. So, after finite time the queue Q definitely becomes the empty list which is a stop criterion (line 16).\nThe argumentation so far proves the following\nProposition 4.13. Let \u3008K,B,P ,N \u3009R be an admissible DPI, t, nmin, nmax \u2208 N and p : K \u2192 (0, 0.5) defined in a way that Q is always ordered first-in-first-out. For these inputs, Algorithm 2 always terminates and returns a set D of minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R which is\n\u2022 the set of the |D| minimal diagnoses of minimum cardinality w.r.t. \u3008K,B,P ,N \u3009R (i.e. the first |D| elements in mD\u3008K,B,P,N \u3009R if mD\u3008K,B,P,N \u3009R is assumed to be sorted in ascending order by cardinality) such that nmin \u2264 |D| \u2264 nmax, if at least nmin minimal diagnoses exist w.r.t. \u3008K,B,P ,N \u3009R, or\n\u2022 the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R, otherwise."}, {"heading": "4.6 Diagnosis Probability Space", "text": "The induction of a probability space [Dur10] over diagnoses facilitates incorporation of well-established probability theoretic methods into the process of KB debugging; for example, a Bayesian approach [SFFR12, RSFF13, dKW87] for identifying the true diagnosis, i.e. the one which leads to a solution KB with the desired semantics, by repeated measurements (see Part II). Let the true diagnosis be denoted as Dt in the sequel.\nThe Probability Space of All Diagnoses. From the point of view of probability theory, a diagnosis can be viewed as an atomic event in a probability space \u3008\u2126, E , p\u3009 defined as follows:\n\u2022 \u2126 is the sample space consisting of all possible diagnoses w.r.t. a DPI \u3008K,B,P ,N \u3009R, i.e. \u2126 = aD\u3008K,B,P,N \u3009R ,\n\u2022 E is a sigma-algebra on \u2126, in our case the powerset 2\u2126 of \u2126, and \u2022 p is a probability measure assigning a probability to each event in E , i.e. p : E \u2192 [0, 1] such that\u2211 \u03c9\u2208\u2126 p({\u03c9}) = 1 which means \u2211 D\u2208aD\u3008K,B,P,N\u3009R p({D}) = 1.\nSo, p({D}) for D \u2208 aD\u3008K,B,P,N \u3009R can be seen as the probability that D is the true diagnosis, i.e. the probability of the event Dt = D (or Dt \u2208 {D}). Consequently, p({D}) for D \u2208 aD\u3008K,B,P,N \u3009R is the probability distribution of the random variableDt, i.e. the probability distribution of the true diagnosis. In this vein, the probability of a set {Di, . . . ,Dj} \u2208 E is interpreted as the likeliness of this set to comprise the true diagnosis Dt. That is, p({Di, . . . ,Dj}) = p(Dt \u2208 {Di, . . . ,Dj}) = p(Dt = Di \u2228 \u00b7 \u00b7 \u00b7 \u2228 Dt ="}, {"heading": "66 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Algorithm 2 HS: Computation of Minimal Diagnoses Input: an admissible DPI \u3008K,B,P ,N \u3009R, a desired computation timeout t, a desired minimal (nmin) and maximal\n(nmax) number of diagnoses to be returned, a function p : K \u2192 (0, 0.5) Output: a set D which is\n(a) a set of most probable (according to p()) minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R such that nmin \u2264 |D| \u2264 nmax, if at least nmin minimal diagnoses exist w.r.t. \u3008K,B,P ,N \u3009R, or (b) the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R otherwise\n1: procedure HS(\u3008K,B,P ,N \u3009R, t, nmin, nmax, p()) 2: tstart \u2190 GETTIME() 3: Dcalc,Ccalc \u2190 \u2205 4: Q\u2190 [\u2205] 5: repeat 6: node\u2190 GETFIRST(Q) 7: Q\u2190 DELETEFIRST(Q) 8: \u3008L,C\u3009 \u2190 LABEL(\u3008K,B,P ,N \u3009R, node,Ccalc,Dcalc,Q) 9: Ccalc \u2190 C\n10: if L = valid then 11: Dcalc \u2190 Dcalc \u222a {node} 12: else if L = closed then . do nothing 13: else . L must be a minimal conflict set 14: for e \u2208 L do 15: Q\u2190 INSERTSORTED(node \u222a {e} ,Q, pnodes()) 16: until Q = [] \u2228 [|Dcalc| \u2265 nmin \u2227 (|Dcalc| = nmax \u2228 GETTIME()\u2212 tstart > t)] 17: return Dcalc\n18: procedure LABEL(\u3008K,B,P ,N \u3009R, node,Ccalc,Dcalc,Q) 19: for nd \u2208 Dcalc do 20: if node \u2287 nd then . non-minimality 21: return \u3008closed,Ccalc\u3009 22: for nd \u2208 Q do 23: if node = nd then . remove duplicates 24: return \u3008closed,Ccalc\u3009 25: for C \u2208 Ccalc do 26: if C \u2229 node = \u2205 then . reuse C 27: return \u3008C,Ccalc\u3009 28: L\u2190 QX(\u3008K \\ node,B,P ,N \u3009R) 29: if L = \u2019no conflict\u2019 then . node is a diagnosis 30: return \u3008valid,Ccalc\u3009 31: else . L is new minimal conflict set (/\u2208 Ccalc) 32: Ccalc \u2190 Ccalc \u222a {L} 33: return \u3008L,Ccalc\u3009\nDj) = 0.3 means that Dt is an element of {Di, . . . ,Dj} with 30% probability. Note that singletons are often written without curly braces, i.e. p({Di}) is usually written as p(Di); we will also do so in the rest of this work.\nThe elements of the sample space \u2126 of a probability space are often called atomic events because they must be mutually exclusive (i.e. two atomic events cannot \u201chappen\u201d at the same time as an outcome of the fictive experiment a probability space describes) and exhaustive (i.e. for each \u201cexecution\u201d of the experiment the probability space describes one atomic event must \u201chappen\u201d). Since the true diagnosisDt"}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 67", "text": "must be a diagnosis w.r.t. \u3008K,B,P ,N \u3009R and \u2126 by definition comprises all such diagnoses, exhaustiveness is clearly fulfilled. Mutual exclusiveness is a consequence of the fact that each diagnosisD gives complete information about the correctness of each formula axk \u2208 K. In other words, Dt \u2208 {D} is a shorthand for the statement that all ax i \u2208 D are faulty and all ax j \u2208 K \\ D are correct. Thus, any two different diagnoses are mutually exclusive events, i.e. Dt = Di implies Dt 6= Dj for all Dj \u2208 aD such that Di 6= Dj .\nThe probability measure p is completely defined if a probability p(D) for each diagnosis D \u2208 \u2126 is given. Then, by the mutual exclusiveness of events Dt \u2208 {Di} and Dt \u2208 {Dj} for Di 6= Dj , the probability\np(E) = \u2211 D\u2208E p(D) (4.1)\nfor each event E \u2208 E .\nRestricted Probability Spaces of Diagnoses. In many cases, only a restricted set of diagnoses w.r.t. a DPI is considered relevant for the debugging task. That is, the focus is on locating the true diagnosis among a predefined subset of all diagnoses aD\u3008K,B,P,N \u3009R . This involves an adaptation of the probability space, in particular of the set \u2126. For instance, if not the set of all, but only the set of minimal diagnoses mD\u3008K,B,P,N \u3009R w.r.t. \u3008K,B,P ,N \u3009R should be considered by a debugging system \u2013 as motivated in Section 3.1 \u2013 then \u2126 := mD\u3008K,B,P,N \u3009R . The other properties E = 2 \u2126 and \u2211\n\u03c9\u2208\u2126 p({\u03c9}) = 1 remain the same for each restricted probability space, but depend on \u2126. Thus, for example, a probability p(D) for D \u2208mD\u3008K,B,P,N \u3009R \u2286 aD\u3008K,B,P,N \u3009R must be generally defined differently, i.e. assigned a higher value, when \u2126 = mD\u3008K,B,P,N \u3009R instead of \u2126 = aD\u3008K,B,P,N \u3009R . This is due to the condition that all probabilities of atomic events in \u2126 must sum up to 1. In practice, because of the computational complexity of diagnosis computation, the used probability space will usually need to be restricted even further in that \u2126 comprises only a set of \u201cleading diagnoses\u201d which is a subset of all minimal diagnoses w.r.t. a DPI (see Chapter 7)."}, {"heading": "4.6.1 Construction of a Probability Space", "text": "Since a diagnosis constitutes an assumption about the correctness of each formula in the KB, the probability of a diagnosis D (to be the true diagnosis Dt) can be computed by means of fault probabilities of formulas. In other words, computing the probability of the event D = Dt corresponds to computing the probability of the event that exactly all formulas in D are faulty and all other formulas in the KB are correct.\nEstimating Fault Probabilities of Formulas in the KB\nNext we discuss various possibilities of how the probability of an ax \u2208 K might be assessed. To this end, we first make a distinction between situations where some useful empirical data is available or not and then we differentiate between different sorts of such available data and how to take advantage of it.\nEmpirical Data is Accessible. Let us first reflect on how to utilize different empirical data sources in order to compute formula probabilities. Data can be of the following kinds (enumeration may not be complete):\n(a) Regarding formulas: Change logs of formulas in the KB\n(b) Regarding the user: Data about common mistakes of the user who has formulated the KB"}, {"heading": "68 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Ad (a): Prerequisite for the availability of change logs of formulas in the KB is the usage of some KB engineering software with integrated logging or change management. Examples of such KB (ontology) developing environments are Prot\u00e9g\u00e9 [NSD+00], Web Prot\u00e9g\u00e9 [TNNM13], SWOOP [KPS+06], OntoEdit [SEA+02] or KAON2.19 Given a formula ax \u2208 K and its change log, the fault probability p(ax ) of this formula can be estimated by counting the number of modifications accomplished for ax in the change log. The intuition is, the more often ax has been altered, the more uncertain the (set of) author(s) might be about its correctness. This method of probability computation however suffers from a cold-start problem. If a KB is completely newly created, then such information is not available at all. On the other hand, for KBs that are being developed over a long period of time, this method can be assumed to be a rather reliable way of assessing the likeliness of formulas to be faulty.\nAd (b): Clearly, data about common mistakes of a user has to be related to some type of entity that is recurrent and not dependent on a particular KB. Formulas are therefore not suitable and too coarsegrained since one and the same formula will rarely occur in many KBs. More adequate entities to relate a user fault to are predicates (terms) and logical connectives \u2013 these usually (re-)appear in many different KBs. In this way, the extrapolation and reusability of collected personal fault information of a user within one KB and between different KBs is granted.\nOne way of obtaining data about common mistakes of user u on this syntactical level is, for instance, the examination of diagnoses got as a result of past debugging sessions performed on KBs authored by u. Another way is, again, to use the change logs (if available) of formulas in KBs user u has created in the past.\nGiven such a past diagnosis D, we know that all formulas ax \u2208 D that had been written by u have been confirmed to be faulty by a user. So, these formulas could be analyzed for contained predicates (terms) and logical connectives and the probability of being faulty of those syntactical constructs could be raised relative to those constructs that do not occur in formulas inD. At this, the following assumptions could be made:\n\u2022 If a formula has been confirmed to be faulty by the user, then the meaning of all predicates (terms) appearing in this formula is not correct (because in the domain that should be modeled the relationship between the predicates (terms) occurring in the formula stated by the formula must not hold). So, all predicates (terms) in ax get more suspicious of being faulty in general if ax \u2208 D for some past solution diagnosis D.\n\u2022 If a formula including some logical connective is part of some past solution diagnosis, then this type of logical connective gets more suspicious of being faulty in general.\nWhen exploiting change logs of formulas authored by u, the following assumptions could be made:\n\u2022 If a formula has been modified, then a user has changed the meaning of all predicates (terms) appearing in this formula. So, all predicates (terms) in ax get more suspicious of being faulty in general if ax has been edited at least once. The more often it has been altered, the more suspicious the predicates (terms) get.\n\u2022 If some logical connective in a formula is modified, i.e. deleted or added, then this type of logical connective gets more suspicious of being faulty in general.\nThe following example should give an intuition of these assumptions:\nExample 4.5 Imagine the situation where the author of formula ax := \u2200X pet(X) \u2194 animal(X) \u2227 (\u2203Y hasOwner(X,Y ) \u2227 person(Y )) is known to have only vague knowledge about the predicate pet\n19http://kaon2.semanticweb.org/"}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 69", "text": "and to frequently interchange \u2227 and \u2228 when formulating logical formulas. This could be reflected by the assignment of higher fault probability to the predicate pet than to the predicates animal, hasChild and person and by raising the fault probability of \u2227 as well as \u2228 compared to other logical connectives available in the used logic L. Then, formula ax should intuitively have a higher probability of being faulty than, e.g., formula ax \u2032 = \u2200X animal(X) \u2192 \u00acperson(X) since ax \u2032 does not include any of the \u201csuspicious\u201d terms or connectives as ax does.\nA probability of 0.25 of some predicate (term) a occurring inK could then account for the observation made in the logs that, in past debugging sessions (not necessarily related to the current KB K), every fourth formula formulated by user u which includes the term a was modified at least once. Similarly, another term b could be assigned fault probability 0.5 which could reflect that formulas formulated by u including b have been altered twice as often as formulas formulated by u comprising a. Given additionally that a occurred in two formulas formulated by u of past diagnoses whereas b did not occur in any, the probability of a could be increased by some addend or factor to take account of this.\nConcerning some logical connective, say \u2203, the observation that all past diagnosis formulas contained \u2203 and in 80% of formulas formulated by this user including \u2203 the \u2203 connective has been modified at least once, the fault probability of \u2203 might be assigned rather high. In comparison, the probability of some other connective, say \u00ac, occurring in no diagnosis and having been altered only in 10% of the formulas comprising \u00ac, the probability of the \u00ac connective might be estimated rather low.\nA shortcoming of this approach is again a cold-start problem. If a user is new to conceptualizing knowledge in a structured logical manner or at least in the given logical language L, then no such (personalized) past diagnoses or change logs will be available. So, this issue especially concerns beginners who are usually anyhow more prone to errors than expert-users. On the positive side, utilization of such empirical data can yield to fault information that is very well tailored for the user and that can imply a significant reduction of computation time and user effort necessary for debugging of the KB at hand [SFFR12].\nNo Empirical Data is Available. If no data of the kinds (a) and (b) discussed above is available to a debugging system, then we have the following possibilities:\n(c) Common fault patterns\n(d) Subjective self-assessment of a user\n(e) Examination of structural complexity of logical formulas\n(f) Using no probabilities\nAd (c): A common fault pattern [RDH+04, CRV+09, KPSCG06], also called anti-pattern, refers to a set of formulas that either leads to an inconsistency (logical anti-pattern) or corresponds to a potential modeling error that \u2013 alone \u2013 does not lead to a inconsistency or incoherency (non-logical anti-pattern), but still might become a source of inconsistency if merged with other formulas (cf. Section 3.2). Although most of these patterns incorporate more than one formula which makes the individual consideration of a formula in terms of fault probability calculation difficult, an idea to incorporate knowledge about antipatterns to probability estimation of formulas could be to count for each ax \u2208 K in how many different (logical or non-logical) anti-patterns it occurs. The higher this count, the more likely a formula might be involved in a conflict set and thus in the true diagnosis.\nA drawback of this method could be that most of the formulas involved in a KB might not correspond to any formula occurring in an anti-pattern. Thus, one might end up with no probability estimate for most of the formulas in a KB K. Besides that, the information provided by these anti-patterns is not personalized at all and therefore might significantly diverge from the true fault probabilities for a user and lead"}, {"heading": "70 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "to a false bias in the used fault data. This justifies to basically rely on another approach to get a first estimate of a formula\u2019s likeliness of being faulty and use this method only to make adaptations to already established probabilities.\nAd (d): The method of a user\u2019s self-assessment of own fault probabilities supposes a user to be able to specify fault probabilities of predicates (terms), logical connectives or complete formulas by themselves. Since users not always have a clear picture of own strengths and weaknesses, this variant must be regarded with suspicion. Furthermore, in settings where several persons are involved in the engineering of the KB, a reasonable rating of fault probabilities of terms, connectives or formulas authored by other persons might be difficult or impossible for a user.\nAd (e): Here the idea is to examine \u201cgrammatical\u201d (i.e. syntactical) aspects of formulas such as the \u201cnesting depth\u201d of subordinate clauses or the mere \u201clength\u201d of a formula. The underlying assumption can be that higher length and/or deeper nesting means higher complexity and cognitive difficulty in understanding of the formula\u2019s semantics \u2013 as it does in natural language. For instance, it is reasonable to expect formulas like ax 1 := \u2200X a(X) \u2192 (\u2203Y r1(X,Y ) \u2227 (\u2200Z r2(Y, Z) \u2192 b(Z))) to tend to be more error-prone and more likely to be faulty than ax 2 := \u2200X g(X) \u2192 b(X). This intuition is modeled by the maximum nesting depth as well as by the length of ax 1 in comparison to ax 2. Using the analogy to natural language, the maximum nesting depth of a formula could roughly be defined as the maximum number of encapsulated subordinate clauses that cannot be \u201cflattened\u201d occurring in the natural language translation of the formula. For formula ax 1, this would imply a maximum nesting depth of two; for ax 2 it would amount to zero. The reason is that ax 1 stated in natural language would sound \u201cif somebody X is a, then there is somebody Y , who satisfies property r1 with X and for whom anybody, who satisfies property r2 with Y is b\u201d. In this natural language formulation, there are two subordinate clauses, i.e. the clauses beginning with the word \u201cwho\u201d; the first is at nesting depth one and the second at depth two. These subordinate clauses cannot be flattened, i.e. be brought to some lower depth, because the Z is related to the Y which in turn is related to the X . The length of formulas could be defined similarly as in [HPS08] which provides such a definition for DL languages. In this case the length of ax 1 and ax 2 would be four (roughly: four predicates in ax 1) and two (two predicates in ax 2), respectively.\nA disadvantage of such a \u201cgrammatical\u201d approach gets evident when most of the formulas in a KB are rather \u201csimple\u201d, i.e. have a low nesting depth and a short length. In such case this method will give little differentiation between different formulas and should thus be combined with another method of probability estimation in general.\nAd (f): In a situation where all the aforementioned ways of gauging probabilities do not apply or are believed to have a too high risk of introducing a false bias into the debugging system, the solution is to define all formulas to be equally probably faulty. The obvious pro of this is that the system cannot get misled by unreasonable fault probabilities whereas the con is that possibly well-suited probabilistic information cannot be exploited. Moreover, experiments in our previous work [SFFR12] have manifested that fault information of only \u201caverage\u201d quality most often leads to a better performance than no fault information. Apart from that, we have suggested a reinforcement learning \u201cplug-in\u201d to a debugger which could successfully mitigate the negative effect of low-quality fault information and in many cases, in spite of the low-quality fault information, even led to lower resource consumption (user, time) than a debugger without this plug-in using good fault information [RSFF13].\nCollaborative KB Development. In a collaborative development scenario involving several authors, provenance information could be additionally leveraged to refine probability estimates (cf. [KPSCG06]). At this point, user skills could come into play; that is, formulas authored by more experienced authors get a lower overall fault probability as opposed to beginners concerning KB engineering or logic skills or"}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 71", "text": "expertise in the modeled domain. This probability adaptation can also affect syntactical elements in that one and the same predicate (term) or logical connective can get a different probability depending on in which formula it occurs and who authored that formula.\nRemark 4.4 Of course, these assumptions and methods of obtaining fault probabilities of syntactical elements and formulas are only some possible ways of doing so. For example, one might argue that the \u201cauthorship\u201d of a formula is somewhat not clearly defined. What if user u1 has originally written formula ax and then user u2 alters the formula to become ax \u2032? Who is the author of ax \u2032? u1, u2 or both? For whose fault probability computation should the renewed modification of ax \u2032 to ax \u2032\u2032 count? Questions like this one need to be discussed and maybe evaluations using real data need to be accomplished in order to find a practical answer; or perhaps to find out that completely different approaches turn out to be reasonable. This is a topic of our future work.\nRemark 4.5 By the definition of a DPI (Definition 3.1) stating that the KB K must be disjoint with the background knowledge B and the role B has within a DPI, namely to comprise all formulas that are definitely correct, we postulate that no formula ax \u2208 K must have a probability of zero. In a situation when this is not the case, a modified DPI must be used where such formulas have been moved from K to B.\nComputation of Diagnosis Probabilities. In the following, we denote by ax (K) the set of logical connectives and quantifiers occurring in a formula ax (in the KB K) and by a\u0303x (K\u0303) the signature of ax (of K).\nExample 4.6 Considering the DL formula ax := Pet \u2261 Animal u \u2203hasOwner.Person, we have that ax = {\u2261,u,\u2203} and a\u0303x = {Pet,Animal, hasOwner, Person}.\nWe now suppose that either a fault probability p(e) := p(\u201ce is faulty\u201d) of each element e \u2208 K \u222a K\u0303 or the fault probability p(ax ) := p(\u201cax is faulty\u201d) of each formula ax \u2208 K is given. For estimation of these probabilities any (combination) of the methods mentioned above might be employed. In case formula probabilities are given, diagnosis probabilities can be directly computed by Formula 4.3. Otherwise, the following pre-computations must be performed.\nThe fault probability p(ax ) of ax can be calculated as the probability that at least one (occurrence of a) syntactical element in ax is faulty. So, p(ax ) is equal to 1 minus the probability that none of the syntactical elements occurring in ax is faulty. Hence, under the assumption of mutual independence of syntactical faults concerning elements e \u2208 ax \u222a a\u0303x ,\np(ax ) = 1\u2212 \u220f\ne\u2208ax\u222aa\u0303x\n(1\u2212 p(e))n(e) (4.2)\nwhere n(e) is the number of occurrences of syntactical element e in ax . If p(ax ) for all ax \u2208 K is known, the fault probability p(D) of any diagnosisD \u2208 \u2126 \u2286 aD\u3008K,B,P,N \u3009R can be determined as the probability that each formula in D is faulty whereas each formula in K \\ D is correct, i.e. not faulty. Thence,\np(D) = \u220f\naxr\u2208D p(ax r) \u220f axs\u2208K\\D (1\u2212 p(ax s)) (4.3)\nRecall that probabilities of all atomic events in a well-defined probability space must sum up to 1. As not every subset of K is a diagnosis, this is in general not the case. Therefore, diagnosis probabilities"}, {"heading": "72 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "need to be normalized, i.e. each diagnosis probability p(D) must be divided by the sum of all diagnosis probabilities for diagnoses in \u2126. That is, the following adjustment is necessary:\np(D) \u2190 p(D)\u2211 Dk\u2208\u2126 p(Dk)\n(4.4)\nWe want to emphasize that the probability measures p(e) of syntactical elements e and p(ax ) of formulas ax are not required to satisfy any conditions except for p(e) \u2208 (0, 1] and p(ax ) \u2208 (0, 1] for all e \u2208 ax \u222a a\u0303x and all ax \u2208 K (see Remark 4.5 why the intervals (0, 1] are open). In particular, no normalization is needed. The reason for this is that \u201ce is faulty\u201d and \u201cax is faulty\u201d are assumptions about a single logical connective and a single logical formula, respectively. \u201cD is the true diagnosis\u201d, to the contrary, is an assumption about each formula in the KB K. So, the probabilities of two different syntactical elements ei 6= ej are computed on the basis of two different probability spaces, namely \u2126ei = {\u201cei is faulty\u201d, \u201cei is not faulty\u201d} and \u2126ej = {\u201cej is faulty\u201d, \u201cej is not faulty\u201d} which clearly do not depend on each other at all. The same argumentation holds for probabilities of formulas.\nMore Reliable Probabilities through Observations. As we argued before, the basic fault information from which diagnosis probabilities are deduced might be rather vague. A usual way of dealing with scenarios of that kind, is to regard the initial probabilities as a first (a-priori) estimation and to gather additional information, e.g. by making measurements or observations, and exploit this information to adapt the a-priori estimation in order to obtain a more reliable a-posteriori estimation. The more additional information has been accumulated and incorporated, the more realistic is the resulting updated estimation of probabilities.\nA well-known technique enabling computation of a-posteriori probabilities from a-priori probabilities is Bayes\u2019 Theorem. Let p(D) be the a-priori probability of some D \u2208 \u2126 \u2286 aD\u3008K,B,P,N \u3009R and Obs be a new observation. Then, the a-posteriori probability p(D |Obs) of D, i.e. the probability that the true diagnosis Dt = D taking into account the new information Obs, is computed according to Bayes\u2019 Theorem as\np(D |Obs) = p(Obs | D) p(D) p(Obs)\n(4.5)\nwhere p(Obs) is the (a-priori) probability that observation Obs is made and p(Obs | D) is the (a-priori) probability that the observation Obs is made under the assumption that D is the true diagnosis, i.e. Dt = D. That is, the a-priori probability p(D), i.e. the probability that Dt = D without any additional knowledge, must be multiplied by p(Obs | D)/p(Obs) which is often referred to as the support Obs provides for D. If the support is greater than 1, then the a-posteriori probability of D is greater than its a-priori probability, otherwise the a-posteriori probability gets smaller after incorporating the new information Obs. Note that Bayes\u2019 Theorem is only applicable to KB debugging if a suitable class of observations can be defined such that p(Obs) and p(Obs | D) can be computed for observations Obs of this class. As we shall see in Chapter 7, the assignment of test cases to either P or N is one such class of observations. For instance, ti \u2208 P and tj \u2208 N for sets of formulas ti, tj over L are two such observations."}, {"heading": "4.6.2 Using Probabilities for Diagnosis Computation", "text": "If available, formula fault probabilities can be exploited during construction of the pHS-tree (Algorithm 2, Chapter 4) in that most probable instead of minimum cardinality diagnoses are calculated first. To achieve that, breadth-first construction of the tree must be replaced by uniform-cost order of node expansion"}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 73", "text": "by means of the function p() that assigns a fault probability to each formula ax \u2208 K. Thereby, the \u201cprobability\u201d p(nd) of a node nd = {ax s, . . . , ax t} in Algorithm 2 is defined through p(ax ), ax \u2208 K as\np(nd) = \u220f\nax i\u2208nd\np(ax i) \u220f\naxj\u2208K\\nd\n(1\u2212 p(ax j)) (4.6)\nNotice that this formula extends the definition of Formula 4.3 to arbitrary subsets ofK, not only diagnoses. Thus, Formula 4.3 is a special case of Formula 4.6.\nFirst, note that we put \u201cprobability\u201d of a node in quotation marks as, to be concise, each node (path) which is not yet a diagnosis, i.e. needs to be further expanded to become one, has probability zero (of being the true diagnosis Dt). For, a probability space is defined on a set of diagnoses and not on a set of arbitrary subsets nd of the KB. However, we misuse the diagnosis probability space in this case to determine the probability of \u201cpseudo-diagnoses\u201d in order to impose an order on the queue of open nodes in the tree. This will guarantee the finding of the most probable diagnoses first, as we shall see below (Proposition 4.17).\nSecond, note that no normalization, i.e. application of Formula (4.4), is necessary within the scope of the non-interactive Algorithm 2 since the aim here is only the expansion of nodes nd in the order of p(nd) and the return of the most probable identified diagnoses at a certain point in time. For this, the comparison of the probability of one node nd with the probability of another node nd\u2032 suffices. Thus, no other calculations using the properties of a probability space are performed by Algorithm 2. We shall recognize in Chapter 9 that this will not hold for the interactive Algorithm 5 where Formula (4.4) is essential.\nSo, nodes nd are inserted into Q in a way descending order of node probabilities in Q is always maintained. Consequently, nodes with highest fault probability are processed first. This is practical since a user will usually be most interested in seeing those possible faults first that have the highest (estimated) probability to be the actual fault they seek.\nHowever, one needs to be careful when using probabilities as weights in order not to lose the property of Algorithm 2 to compute minimal diagnoses only. To this end, the formula probabilities p(ax ) for all ax \u2208 K must be adapted as\np(ax ) \u2190 c p(ax ) (4.7)\nwhere the factor c is an arbitrary positive real number smaller than 0.5, e.g. c := 0.49/max{ax\u2208K}(p(ax )). This transformation effects that all probabilities p(ax ) become smaller than 50%. In other words, each formula must be more likely to be correct than faulty which in turn means that a minimal diagnosis is more likely than any of its supersets.\nDefinition 4.9. Let p : K \u2192 [0, 1] be some function that assigns to each ax \u2208 K some p(ax ) \u2208 [0, 1]. Then, we denote by pnodes : 2K \u2192 [0, 1] the function that assigns to each node nd \u2286 K some pnodes(nd) \u2208 [0, 1] which is obtained by means of Formula 4.6 and p().\nLemma 4.14. Let nd, nd\u2032 \u2286 K where nd \u2282 nd\u2032 and p : K \u2192 (0, 0.5) a function which assigns to each ax \u2208 K some probability p(ax ) \u2208 (0, 0.5). Then pnodes(nd) > pnodes(nd\u2032) holds.\nProof. According to Formula 4.6 and Definition 4.9 we have that pnodes(nd) = \u220f\nax i\u2208nd\np(ax i) \u220f\naxj\u2208K\\nd\n(1\u2212 p(ax j))\nThen the probability pnodes(nd\u2032) can be computed from pnodes(nd) in that, for each formula ax in nd\u2032 \\ nd \u2286 K \\ nd, we multiply pnodes(nd) by a factor fax := p(ax )/(1 \u2212 p(ax )) because ax \u201cmoves\u201d from K \\ nd to nd. However, fax < 1 holds due to p(ax ) < 0.5 and thus 1\u2212 p(ax ) > 0.5."}, {"heading": "74 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "This result will be a key to proving the completeness, soundness and correctness of Algorithm 2 in the next Section.\nThe next definition characterizes a (partial) weighted pHS-tree, the type of hitting set tree constructed by Algorithm 2 given any function p(ax ) \u2208 (0, 0.5) for all ax \u2208 K as input which is not necessarily specified in a way a breadth-first tree construction is forced.\nDefinition 4.10 (Weighted Pruned HS-Tree). Let \u3008K,B,P ,N \u3009R be an admissible DPI and let w : K \u2192 [0, 1] be a weight function which assigns a weight to each node n \u2286 K with the property that w(n1) > w(n2) if n1 \u2282 n2. An edge-labeled and node-labeled tree T is called a weighted pruned HS-tree (wpHStree) w.r.t. \u3008K,B,P ,N \u3009R and w() iff T is the result of constructing an HS-tree w.r.t. \u3008K,B,P ,N \u3009R with due regard to the following rule\n1. Label open nodes in the HS-tree in order of descending w(),\nand the rules 2 to 6 as per Definition 4.8. T is called a partial weighted pruned HS-tree w.r.t. \u3008K,B,P ,N \u3009R and w() iff T is a weighted pruned HS-tree w.r.t. \u3008K,B,P ,N \u3009R and w() where not all nodes in T have been labeled yet and non-labeled nodes have no successors.\nThen, we have the following relationship between a (partial) pHS-tree and a (partial) wpHS-tree. An explanation why this holds will be given in Section 4.6.4.\nProposition 4.14. A (partial) pHS-tree w.r.t. \u3008K,B,P ,N \u3009R is a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and w() where w() is a weight function which, additionally to the property postulated in Definition 4.10, satisfies w(n1) = w(n2) if |n1| = |n2|.\nIn general, a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and w() is not a (partial) pHS-tree w.r.t. \u3008K,B,P ,N \u3009R.\nLemma 4.15. Algorithm 2 is a procedure for producing a wpHS-tree T w.r.t. \u3008K,B,P ,N \u3009R and pnodes().\nProof. First, the property pnodes(n1) > pnodes(n2) if n1 \u2282 n2 postulated by Definition 4.10 holds by Lemma 4.14 and the fact that the function p given as input to Algorithm 2 satisfies p(ax ) \u2208 (0, 0.5) for all ax \u2208 K. Moreover, the DPI \u3008K,B,P ,N \u3009R provided as an input to Algorithm 2 is admissible, as postulated by Definition 4.10.\nThe compliance with rule 1 of Definition 4.7 as well as with rules 2 to 6 of Definition 4.8 is a simple consequence of Lemma 4.13. In the following we prove that rule 2 of Definition 4.7 and rule 1 of Definition 4.10 are satisfied.\n\u2022 Definition 4.7, rule 2: Suppose a node nd is labeled by valid. Then it is added to Dcalc in line 11. Since nd can only get a label different from closed if it is the only exemplar of this node in Q due to the duplicate criterion (lines 22-24), it must be the case that nd /\u2208 Q (line 7) after nd has been labeled by valid. Only nodes that get labeled by a conflict set can have successor nodes added to Q in line 15. Only nodes in Q can get a label (cf. lines 6 and 8). For nd to be added to Q at some later point in time there must be a proper subset of nd that is still in Q as each node newly added to Q is a proper superset of some node in Q (cf. line 15 which is the only position in the algorithm where nodes are added to Q). This is impossible since Q is ordered descending by pnodes(). Hence, each proper subset of nd must have been ranked before nd in Q and thus must have already been labeled because nd is already labeled by assumption. Hence, if nd is labeled by valid, then it has no successors.\n\u2022 Definition 4.10, rule 1: That nodes are processed and labeled in order of descending pnodes() follows from the fact that new nodes are inserted into Q only in a way that the order of Q by descending pnodes() is maintained (INSERTSORTED in line 15) and by the fact that always the first element of Q is selected to be labeled next (GETFIRST in line 6)."}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 75", "text": "This completes the proof.\nLet the relevant data of a wpHS-tree be defined as for a pHS-tree (cf. Remark 4.2). By the correctness of Lemma 4.15, we have:\nCorollary 4.6. Algorithm 2 stores by \u3008Dcalc,Q,Ccalc\u3009 the relevant data of\n\u2022 a wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes() if Algorithm 2 stops due to Q = [], and\n\u2022 a partial wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes() otherwise."}, {"heading": "4.6.3 Correctness of Weighted Diagnosis Computation", "text": "First, we show the completeness of Algorithm 2 regarding minimal diagnoses, i.e. that it computes all minimal diagnoses w.r.t. the DPI it is given as input.\nLemma 4.16. Only diagnoses w.r.t. \u3008K,B,P ,N \u3009R can be added to Dcalc by Algorithm 2.\nProof. A node nd can be added to Dcalc only in line 11. To reach this line, LABEL must have returned valid for nd. For this to hold, QX(\u3008K \\ nd,B,P ,N \u3009R) must have returned \u2019no conflict\u2019 which implies that nd is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R by Propositions 4.9 and 3.2.\nLemma 4.17. Let T denote a (partial) wpHS-tree produced by Algorithm 2. Further, let Q be the queue of open nodes in T maintained by Algorithm 2 and let nd be some node which occurs only once in Q and which is a proper subset of some minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R. Then:\n(1) The nodes \u2205 = nd1, . . . , ndk along any path from the root node \u2205 to ndk in T satisfy ndi \u2282 ndi+1 and |ndi|+ 1 = |ndi+1| and ndi \u2286 K for 1 \u2264 i \u2264 k.\n(2) If the LABEL function is called for nd, then it yields some minimal conflict set C w.r.t. \u3008K,B,P ,N \u3009R with nd \u2229 C = \u2205.\nProof. (1): In the representation used by Algorithm 2, a node nd in the (partial) wpHS-tree T produced by Algorithm 2 is defined as the set of all edge labels on the path from the root node to nd (see Remark 4.2) and the successor of a node is defined as a node added to Q after nd has been labeled by a minimal conflict set.After the LABEL function for node nd has returned some minimal conflict set L as a label for nd, Algorithm 2 goes to line 15 since L 6= closed and L 6= valid and adds an element nd \u222a {e} to Q for each e \u2208 L. Therefore, it holds that |nd\u222a {e} | = |nd|+ 1 for each successor of nd. Hence, ndi \u2282 ndi+1 and |ndi|+ 1 = |ndi+1| holds for any path of nodes \u2205 = nd1, . . . , ndk in T starting from the root node.\nThe argumentation why each node must be a subset of K is as follows: Suppose node \u222a {e} is added to Q in line 15 which is the only place in Algorithm 2 where nodes are added to Q. So, LABEL must have returned neither valid nor closed for node. Hence, node cannot be a diagnosis w.r.t. \u3008K,B,P ,N \u3009R as otherwise LABEL with argument node must have returned valid in line 30. Due to the fact that node = K is definitely a diagnosis w.r.t. \u3008K,B,P ,N \u3009R as it must hit all minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R which must all be subsets of K (Definition 4.1), node \u2282 K must hold.\n(2): Suppose the LABEL function is called for a node nd \u2208 Q where nd \u2282 D for some minimal diagnosis D.\nFirst, there cannot be any nd\u2032 \u2208 Dcalc with nd\u2032 \u2286 nd since Dcalc includes only diagnoses w.r.t. \u3008K,B,P ,N \u3009R and nd \u2282 D wherefore there would be a diagnosis nd\u2032 \u2282 D, contradiction. Due to the fact that nd is present only once in Q, there cannot be some nd\u2032 = nd in Q. Thus, closed cannot be returned for nd by LABEL.\nBy the facts that a diagnosis must hit all minimal conflict sets (Proposition 4.6) and that nd is a proper subset of a diagnosis, either the criterion checked in line 26 must be true or QX(\u3008K \\ nd,B,P ,N \u3009R)"}, {"heading": "76 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "must return a minimal conflict set L, i.e. L 6= \u2019no conflict\u2019. In both cases, a minimal conflict set is returned by LABEL.\nThere are no other labels that can be returned by LABEL.\nLemma 4.18. Each minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R occurs as a node in Q during the execution of Algorithm 2, if the execution stops due to Q = [].\nProof. For Algorithm 2 it holds that\n(i) if nd is the last exemplar of some node in Q which is a proper subset of some minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R and the LABEL function is called for nd, then it yields some minimal conflict set C w.r.t. \u3008K,B,P ,N \u3009R with nd \u2229 C = \u2205 by Lemma 4.17 and\n(ii) each node nd that has been labeled by some minimal conflict set C is deleted from Q (line 7) whereupon one successor node ndax = nd\u222a{ax} for each element ax \u2208 C is added to Q (INSERTSORTED in line 23) and\n(iii) each minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R is a superset of \u2205 and a subset of K (Definition 3.5) which includes one element of each minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R and includes only elements of minimal conflict sets (Proposition 4.6).\nLet D be some minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R. Then, there is a path of nodes from the root node \u2205 to D in the pHS-tree produced by Algorithm 2, if the execution stops due to Q = [].\nThis holds by the following argumentation: If D = \u2205, then the path is \u3008\u2205\u3009. Now, suppose D \u2283 \u2205. Since D is a minimal diagnosis wherefore no other diagnosis can be equal to \u2205, the root node n0 := \u2205 of the constructed tree must be labeled by some minimal conflict set C1. Then, by (iii), there must be some ax 1 \u2208 C1 that is an element of D. So, we define n1 := {ax 1}. If n1 = D, then the path is \u3008\u2205, n1\u3009. Otherwise, due to D \u2283 n1 and (i), node n1 in the pHS-tree must be labeled by some minimal conflict set C2. Then, by (iii), there must be some ax 2 \u2208 C2 that is an element of D. So, we define n2 := n1 \u222a{ax 2}. If n2 = D, then the path is \u3008\u2205, n1, n2\u3009. Otherwise, due to D \u2283 n2 and (i), node n2 in the pHS-tree must be labeled by some minimal conflict set C3. This reasoning can be continued until nk = D for some k. By (iii), D \u2286 K holds wherefore such k must exist.\nAlgorithm 2 cannot stop executing before nk has been in Q since each node ni labeled by a minimal conflict set Ci+1 involves the addition of |Ci+1| successor nodes to Q by (ii). In particular, the successor node ni \u222a{ax i+1}must be added to Q. As the execution stops due to Q = [], all nodes ni for i \u2264 k must be labeled before termination. Thus, D must be in Q sometime.\nProposition 4.15 (Completeness of Algorithm 2). If Algorithm 2 terminates due to Q = [], then the algorithm returns a set D including all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nProof. Assume some minimal diagnosis D w.r.t. \u3008K,B,P ,N \u3009R where D /\u2208 D after Algorithm 2 has returned due to Q = []. First, each minimal diagnosis will occur in Q throughout the execution of Algorithm 2 because it executes until Q = [] wherefore Lemma 4.18 applies. Any node nd in Q can only be deleted from Q if LABEL is called with the argument node nd (lines 7 and 8). There is no other point in Algorithm 2 where elements are removed from Q. Since at the end Q = [], each minimal diagnosis, in particular D, must be labeled.\nSuppose D is the last exemplar of possibly multiple duplicates of it in Q. Then, the LABEL function cannot return closed for D. This holds, on the one hand, because the duplicate criterion (lines 22-24) only removes possible duplicate nodes from Q, but never the last exemplar of a node in Q. On the other hand, D can never be closed due to the non-minimality criterion (lines 19-21) as Dcalc can only include diagnoses w.r.t. \u3008K,B,P ,N \u3009R by Proposition 4.16. Thus, due to the minimality of D, Dcalc cannot"}, {"heading": "4.6. DIAGNOSIS PROBABILITY SPACE 77", "text": "comprise any diagnosis D\u2032 with D\u2032 \u2286 D, except for some D\u2032 which is equal to D. This would however be a contradiction to the assumption that D /\u2208 D.\nThe reuse criterion (lines 25-27) cannot apply forD either since a minimal diagnosis is a hitting set of all minimal conflict sets (Proposition 4.6) wherefore there cannot be a minimal conflict set in Ccalc which has an empty intersection with D. So, the algorithm will come to line 28 where QX(\u3008K \\ D,B,P ,N \u3009R) will return \u2019no conflict\u2019 (Propositions 4.9 and 3.2). Therefore, D will be labeled by valid and will be added to Dcalc in line 11.\nNext, we show the soundness of Algorithm 2 w.r.t. minimal diagnoses, i.e. that it computes only minimal diagnoses w.r.t. the DPI it is given as input.\nProposition 4.16 (Soundness of Algorithm 2). If an element D is added to the set Dcalc during the execution of Algorithm 2, D is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nProof. Assume that some element nd is added to Dcalc which is not a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. This immediately yields a contradiction due to Lemma 4.16.\nAssume now that some element nd is added to Dcalc which is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R, but not a minimal one. Now, since nd is a non-minimal diagnosis, there is some D \u2282 nd which is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nThen, there are three cases to distinguish: (a) D is in Q and (b) D is in Dcalc and (c) D is neither in Q nor in Dcalc, i.e. the node D has not yet been generated.\nNote that these are all possible cases as D is a minimal diagnosis by assumption. So, D cannot have been ruled out, i.e. labeled by closed, by the non-minimality criterion (lines 19-21) before since only diagnoses can be added to Dcalc as argued in the first paragraph of this proof and there cannot be a diagnosis D\u2032 \u2208 Dcalc such that D\u2032 \u2282 D. The case D\u2032 = D is already considered by case (b). The duplicate criterion (lines 22-24) does not need to be taken into account since it deletes duplicate nodes only.\n(a): To be added to Dcalc, nd must have been the first element of the queue Q by GETFIRST in line 6. Since D \u2208 Q by assumption and since Q is sorted in descending order of node probability (INSERTSORTED in line 15), we conclude that pnodes(D) \u2264 pnodes(nd). However, as pnodes(X) for a node X \u2286 K is defined by means of p(ax ) where p(ax ) \u2208 (0, 0.5) for all ax \u2208 K as per Formula 4.6 (Definition 4.9), Lemma 4.14 applies and establishes the truth of pnodes(S1) > pnodes(S2) if S1 \u2282 S2 for S1, S2 \u2286 K. By D \u2282 nd, this implies pnodes(D) > pnodes(nd), contradiction.\n(b): Assuming case (b), we can derive a contradiction as follows. By the fact that nd is added to Dcalc, it must hold that the LABEL procedure called for nd in line 8 returned valid as part of its output in line 30. However, as D \u2282 nd is already an element of Dcalc by assumption, the LABEL procedure must have already returned in line 21 wherefore it cannot have reached line 30, contradiction.\n(c): Suppose that D has not yet been generated as a node in Q. By Lemma 4.17, the nodes \u2205 = nd1, . . . , ndk along a path from the root node in the pHS-Tree produced by Algorithm 2 satisfy ndi \u2282 ndi+1 and |ndi| + 1 = |ndi+1|. So, by Lemma 4.14, the node probabilities along any path from the root node are strictly monotonically decreasing. Since pnodes(D) > pnodes(nd) holds by the same argumentation as in (a), we have that all nodes on the path from the root node to D have a higher probability than nd. As Q is sorted in descending order of node probability and in each iteration the first element in Q is processed as explained in (a), we infer that D must have already been generated at the time nd is processed, contradiction.\nNext, we argue that Algorithm 2 computes minimal diagnoses in descending order of diagnosis probability according to the parameter p() given as input to the algorithm.\nCorollary 4.7. Let the probability p(D) of a diagnosis D in Algorithm 2 be computed from the given function p(ax ), ax \u2208 K as per Formula 4.3."}, {"heading": "78 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "1. At any point in time during the execution of Algorithm 2, Dcalc comprises the |Dcalc|most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\n2. If Algorithm 2 returns a set D of cardinality n, then D is the set of the n most-probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nProof. (1): By Propositions 4.15 and 4.16, it is a fact that Algorithm 2 computes all and only minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R. What must still be shown is that minimal diagnoses are added to Dcalc in descending order of their probability p() as per Formula 4.3. The probability p(D) of some diagnosis D is equal to pnodes(D) since a each diagnosis is a node and Formula 4.3 is a special case of Formula 4.6 by which the probability pnodes(nd) of a node nd is calculated.\nLet us denote by Dpmax the minimal diagnosis with maximum probability that has not yet been added to Dcalc and by D\u00acpmax an arbitrary minimal diagnosis with non-maximal probability, that is pnodes(D\u00acpmax) < pnodes(Dpmax). So, we need to demonstrate that each node nd \u2282 Dpmax on a path from the root node to nodeDpmax is processed beforeD\u00acpmax is treated. By Lemma 4.17, a path from the root node in the pHS-Tree produced by Algorithm 2 is a set of nodes \u2205 = nd1, . . . , ndk where ndi \u2282 ndi+1 and |ndi| + 1 = |ndi+1|. Further recall that the probability pnodes(X) of a node X \u2286 K in Algorithm 2 is defined as per Formula 4.6. So, by Lemma 4.14, the node probabilities along any path from the root node are strictly monotonically decreasing. Hence, each node nd on a path from the root node to Dpmax has a probability pnodes(nd) > pnodes(Dpmax) > pnodes(D\u00acpmax). By the insertion of new nodes into Q (INSERTSORTED in line 15) in a way descending order of Q as per pnodes() is always maintained, and by the selection of the first element of Q (GETFIRST in line 6) as next node to be processed, each node nd on a path toDpmax must be processed beforeD\u00acpmax is processed. Consequently, minimal diagnoses are added to Dcalc in descending order of their probability p() as per Formula 4.3.\n(2): This proposition follows directly from (1).\nProposition 4.17. Algorithm 2 always terminates and returns a set D of minimal diagnoses w.r.t. \u3008K, B,P , N \u3009R which is\n\u2022 the set of the |D|most probable (w.r.t. p() and Formula 4.3) minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R such that nmin \u2264 |D| \u2264 nmax, if at least nmin minimal diagnoses exist w.r.t. \u3008K,B,P ,N \u3009R, or\n\u2022 the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R, otherwise.\nProof. The proposition is a direct consequence of Propositions 4.12, 4.15 and 4.16 and Corollary 4.7."}, {"heading": "4.6.4 Using Probabilities to Compute Minimum Cardinality Diagnoses", "text": "The function p : K \u2192 (0, 0.5) can be defined in a way that minimum cardinality instead of maximum probability diagnoses are identified first. To this end, p() is specified as a fixpoint function that maps each formula ax \u2208 K to one and the same constant value p(ax ) := c where c is an arbitrary real number such that 0 < c < 0.5, e.g. c := 0.3. That in this setting diagnoses are found in order of ascending cardinality is a simple consequence of Corollary 4.7.\nExample 4.7 Let us now study how such formula and diagnosis probabilities would be constructed for the example DPI depicted by Table 15.3. Let us suppose that the KB K in the DPI was formulated by a single user u for whom the personal fault probabilities of syntactical elements K\u0303 \u222a K given by the first row of Table 4.4 have been extracted from log data of the KB editing software applied by u. Then, the resulting probabilities of formulas ax \u2208 K as per Formula 4.2 are as presented in the rightmost column of Table 4.4. The entries in the table from the second to the last but two column display the number of occurrences of the syntactical element given by the column label in the formula given by the row label. These values are required to compute the formula probabilities listed in the last but one column"}, {"heading": "4.7. NON-INTERACTIVE KB DEBUGGING ALGORITHM 79", "text": "as per Formula 4.2. The final probabilities that can \u201csafely\u201d be incorporated into Algorithm 2 under a guarantee that only minimal diagnoses will be output are shown in the last column. These result from an application of Formula 4.7 to the probabilities given in the last but one column with an adaptation parameter c := 0.49.\nNotice that, for example, p(ax 5) is rather high since the predicates A and Y as well as the connective \u00ac occurring in ax 5 have a comparably high fault probability in relation to syntactical elements appearing in other formulas. Formula ax 3, on the other hand, comprises only two predicates which should be wellunderstood by u and no connectives except for \u2192 which is not problematic for u either. Therefore, its fault probability is rather low."}, {"heading": "4.7 Non-Interactive Knowledge Base Debugging Algorithm", "text": "Algorithm 3 describes the procedure for non-interactive debugging of KBs. The algorithm requires as input all the parameters that are required by Algorithm 2 and an additional parameter auto \u2208 {true, false} indicating either automatic (true) or manual (false) mode. If auto = false , Algorithm 3 calls HS (Algorithm 2) with the parameters as provided. The set of minimal diagnoses D returned by HS is then presented to the user who can select a diagnosis manually after inspecting the diagnoses in D. Alternatively, in case of auto = true , the system calls HS with the parameters as provided, but with nmin = nmax = 1. Hence, only the most probable minimal diagnosis is computed by HS and returned as an output of Algorithm 3 to the user.\nIf a user wants the algorithm to output the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R, then the parameter setting auto = false and nmin = \u221e must be chosen. If, on the other hand, a fixed number n of leading diagnoses should be computed (as long as there are at least n minimal diagnoses for the DPI), then nmin := n =: nmax are the correct parameter settings. Note that in both cases the specification of t has no effect.\nOf course, the user can also apply Algorithm 3 several times with varying parameters t, nmin, nmax and p(). Or they can specify a test case, i.e. add a set of formulas X either to P (if each ax \u2208 X should be entailed by the correct KB) or to N (if the conjunction of all formulas in X must not be implied by the correct KB), and rerun the algorithm with this modified DPI."}, {"heading": "80 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "Algorithm 3 Non-Interactive KB Debugging Input: a tuple \u3008\u3008K,B,P ,N \u3009R, t, nmin, nmax, p(), auto\u3009 consisting of\n\u2022 an admissible DPI \u3008K,B,P ,N \u3009R, \u2022 some computation timeout t, \u2022 a desired minimal (nmin) and maximal (nmax) number of diagnoses to be returned, \u2022 a function p : K \u2192 (0, 0.5) and \u2022 a boolean parameter auto \u2208 {true, false}.\nOutput: a set D which is\n(a) the set of the |D| most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R such that nmin \u2264 |D| \u2264 nmax, if at least nmin minimal diagnoses exist w.r.t. \u3008K,B,P ,N \u3009R, or\n(b) the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R otherwise where \u201cmost-probable\u201d refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p().\n1: if auto = true then 2: D\u2190 HS(\u3008K,B,P ,N \u3009R, t, 1, 1, p()) . see Algorithm 2 3: else 4: D\u2190 HS(\u3008K,B,P ,N \u3009R, t, nmin, nmax, p()) . see Algorithm 2 5: return D\nAnyway, the user must either find the correct diagnosis (if it is an element of the output set D at all) by hand or be convinced that the returned minimum cardinality or respectively maximum probability diagnosis is indeed the one that yields a solution KB with the intended semantics. Moreover, when formulating test cases by hand, a user can be assumed to be as likely to specify something contradictory or faulty as during creation of the KB itself.\nUnsurprisingly, application of Algorithm 3 will often lead to unsatisfying solution ontologies. Remedy for this is provided by Interactive KB Debugging which on the one hand requires higher effort of one (or several) user(s), but on the other hand ensures a high quality solution in terms of its semantics to the problem of Parsimonious KB Debugging (Problem Definition 3.2).\nExample 4.8 Assume a user wants to find a maximal solution KB for the example DPI \u3008K,B,P ,N \u3009R provided by Table 15.3 and that no data giving information about fault probabilities of syntactical constructs or formulas in K is available. Therefore, let p(ax ) := c for some fixed c \u2208 (0, 0.5) (see Section 4.6.2 for an explanation of this choice of c). The non-interactive KB debugging algorithm presented by Algorithm 3 called with \u3008K,B,P ,N \u3009R, the function p(), nmin = \u221e and auto = false as inputs results in the hitting set tree given by the upper picture in Figure 4.2. By nmin =\u221e and auto = false , the user signalizes that inspection of all minimal diagnoses w.r.t. the input DPI is desired. Hence, the (complete) breadth-first pHS-tree as per Algorithm 2 is constructed. So, the output is the set of all minimal diagnoses mD\u3008K,B,P,N \u3009R = {[1], [2], [5, 7]}.\nIn the shown hitting set tree, minimal diagnoses are indicated by nodes labeled by X(D) where D is a name given to this diagnosis. A node closed due to non-minimality is denoted by\u00d7(\u2283D) whereD is some minimal diagnosis that is a subset of the set of edge labels along the path leading from the root node to this node. The label CC means that the minimal conflict set C has been freshly computed by a call to QX. The label CR, on the other hand, means that the minimal conflict set C has been reused from the set of already computed minimal conflict sets. In this example, both minimal conflict sets are computed by QX and no conflict sets are reused. The order of node labeling is indicated by the numbers i\u00a9 starting from 1. Open nodes, i.e. generated nodes that have not yet been labeled, are indicated by a question mark."}, {"heading": "4.7. NON-INTERACTIVE KB DEBUGGING ALGORITHM 81", "text": "In case auto = true was given as an input to the algorithm instead, the partial pHS-tree depicted by the lower picture in Figure 4.2 would be constructed and the output would be D = {D1} = {[1]} containing just the first found and thus most probable minimal diagnosis w.r.t. the input DPI. Note that D1 = [1] and D2 = [2] (which is not computed) have equal probability and whether the one or the other is computed first depends only on the ordering of equally probable (in this case: equal cardinality) nodes in Q. As already mentioned in Section 4.6.2, in this example the most probable diagnosis is equivalent to a minimum cardinality diagnosis since all formula probabilities are equal.\nPlease notice that the internal \u201cflat\u201d representation used by Algorithm 2 which does not store a tree but only the set of open and closed nodes differs from the standard tree representation [Kal06, FS05, SQJH08, Rei87] we use to depict the hitting set tree graphically in Figure 4.2. Whereas within Algorithm 2 a node node stores the set of all the edge labels on the path leading from the root node to node, in the figure we label each node in the tree by the respective label that is computed for this node by the LABEL function, i.e. either by a minimal conflict set, by X or by \u00d7.\nExample 4.9 Recall Example 4.7 which demonstrated how formula fault probabilities are constructed from fault probabilities of syntactical elements for the example DPI depicted by Table 15.3. Now we want to show how the non-interactive KB debugging algorithm given by Algorithm 3 works when these formula probabilities are incorporated.\nSuppose the inputs to the algorithm are the DPI \u3008K,B,P ,N \u3009R, the function p(ax ) for ax \u2208 K displayed by the rightmost column of Table 4.4 and auto = false . Further on, let the user of the debugging algorithm be willing to wait a maximum of one second for an output and let them postulate a minimum of two most probable minimal diagnoses to be returned, e.g. to have at least a second choice if the employed formula probabilities are not perfectly suitable and the most probable diagnosis is not the desired solution. These postulations are expressed by specifying the parameters nmin = 2 and t = 1 (second). Additionally, assume the user expects the provided probabilities to be sufficiently reasonable such that the"}, {"heading": "82 CHAPTER 4. DIAGNOSIS COMPUTATION", "text": "desired diagnosis will be among the best four diagnoses wherefore nmax = 4 is chosen. Moreover, let us imagine that the time for each fresh computation of a minimal conflict plus generation of the (unlabeled) successor nodes of this node is 0.4 seconds and the cost of computing any other label of a node is 0.1 seconds.\nThen the partial wpHS-tree produced by Algorithm 3 initialized in this way is illustrated by Figure 4.3. The used notation is as described in Example 4.8 with one additional attribute. Namely, each edge is not only labeled by one element of the conflict set from which it goes out, but also by a label p \u2208 (0, 1) that is placed near the arrow head of the arrow that expresses the edge. This label p gives the probability as per pnodes() (cf. Definition 4.9) of the (partial) diagnosis that corresponds to the union of the edge labels along the path from the root to and including the edge that is labeled by p. For example, the label 0.06 of the edge directed at the node number 4\u00a9 means that the probability of {2, 5} is 0.06. Further on, open, i.e. generated, but not yet labeled nodes, are designated by a question mark.\nAs outlined by the circled numbers i\u00a9, as a first action the root node is labeled by the newly computed minimal conflict set \u30081, 2, 5\u3009, the computation time of which amounts to 0.4. Then, the tree construction proceeds according to the (partial) diagnosis probabilities according to pnodes() computed from the formula probabilities p(ax ), ax \u2208 K provided by the last column of Table 4.4. Therefore, the most probable edge leading away from the root node is labeled next. This already leads to the finding of the first minimal diagnosis D1 = [2] after overall computation time of 0.5 seconds. Since nmin = 2 diagnoses have not yet been computed and there are still unlabeled open nodes, namely those corresponding to paths {1} and {5}, the algorithm continues the execution by labeling the next best node {5} with a probability of 0.07 \u2013 as opposed to 0.02 for the other open node {1}. Since {5} is neither a superset of an already computed minimal diagnosis nor a duplicate of another open node nor a diagnosis itself, it must be labeled by some minimal conflict set. Because the already established minimal conflict set \u30081, 2, 5\u3009 is not disjoint with {5}, no reuse is possible and QX is called to determine a new minimal conflict set \u30081, 2, 7\u3009 w.r.t. \u3008K,B,P ,N \u3009R. All successor nodes of the newly labeled node 3\u00a9, i.e. the nodes corresponding to the paths {1, 5} , {2, 5} and {5, 7}, are added to the list Q of open nodes such that descending order of probabilities is maintained. The resulting queue is then Q = [{2, 5} , {5, 7} , {1} , {1, 5}]. As a next step, again the first and thus best open node {2, 5} is chosen from Q and labeled by \u00d7(\u2283D1) which means that the corresponding path is closed since it is a superset of an already found minimal diagnosis, namely D1 = [2]. At this point, the overall computation time amounts to 1 second which corresponds to the time limit t. For that reason, the algorithm will go ahead searching for minimal diagnoses only until a minimal number nmin thereof is detected. The node processed next, corresponding to the path {5, 7}, is then determined to be a minimal diagnosis by the LABEL procedure.\nThus, the output of the algorithm after 1.1 seconds execution time is the set of minimal diagnoses D = {[2], [5, 7]} which is a proper subset of all minimal diagnoses D\u3008K,B,P,N \u3009R = {[1], [2], [5, 7]}."}, {"heading": "4.7. NON-INTERACTIVE KB DEBUGGING ALGORITHM 83", "text": "However, if we assume that the user\u2019s intended KB should entail E \u2192 G, for instance, then none of the returned diagnoses can be used to compute a solution KB featuring this entailment when integrated with the background knowledge B. Hence, the true diagnosis Dt would be missed in this case.\nAlso, when computing all minimal diagnoses w.r.t. a DPI \u2013 if this is even possible in a concrete case due to the computational complexity \u2013 and showing them to the user, a user might review just the most probable ones and make a decision on which one to choose only based on these. For instance, [SF10] reported on one DPI where computation of all minimal diagnoses, 1782 in number, is feasible. In such a case it is hard to expect that a user will be willing or will have the time to inspect more than a small fraction of these 1782 diagnoses. The consequence will be a wrong choice of diagnosis in many cases, also because a simple view on a diagnosis will often not lead to the certainty of a user that this one is or is not the desired one. The reason for this is that usually it is too complex for a human brain to perform the necessary mental reasoning to make oneself a picture of the implications of choosing one diagnosis as opposed to another one.\nFor our example DPI, a user getting the output D = mD\u3008K,B,P,N \u3009R = {[1], [2], [5, 7]} with the computed probabilities p([1]) = 12%, p([2]) = 60% and p([5, 7]) = 28% might decide to just inspect the diagnoses that make the most probable 80% fraction of diagnoses. In this case, either [2] or [5, 7] would be selected, which corresponds to a wrong choice in case E \u2192 G should be entailed be the resulting solution KB after integration with the background KB B.\nChapter 5\nSummary\nIn this part, we profoundly introduced the topic of knowledge base debugging. We stated necessary properties of knowledge representation languages to be compatible with our approaches, namely that the entailment relation must be monotonic, idempotent and extensive. We gave precise definitions of the problems of KB debugging and parsimonious KB debugging. Both problems assume a given instance of a diagnosis problem (DPI). The former seeks any solution in line with the given requirements whereas the latter seeks a solution that preserves as much formulas as possible of the given faulty KB, i.e. aims at minimal changes. With the validity of a KB, a solution KB, a diagnosis and a conflict set, we have characterized central notions that will be extensively used throughout this work. We have studied the relationship between all these notions and proved that solving the problem of parsimonious KB debugging is equivalent to finding a minimal diagnosis w.r.t. a given DPI.\nWe established the relationship between conflict sets and justifications, a similar notion that is used concurrently to conflict sets in (prevalently DL, OWL or Semantic Web) literature, and provided evidence that conflict sets are the better choice for the debugging problems addressed here. In particular, conflict sets serve the purpose of reducing the search space for minimal diagnoses \u2013 minimal hitting sets of all minimal conflict sets \u2013 and help a debugging software to focus on the relevant and problematic parts of the faulty KB. A method for the efficient, polynomial time computation of a conflict set was detailed and its correctness was formally proven. Based on this method, we were able to depict a way of computing minimal diagnoses which is based on using a hitting set tree. Such a tree constitutes a systematic way of generating all minimal conflict sets and, in the course of this, also all minimal diagnoses. Depending on the particular situation, the presented algorithm can be configured to compute diagnoses in a predefined order, e.g. most probable diagnoses first or those diagnoses first that are minimally invasive in terms of the changes made to the faulty KB.\nDifferent ways of obtaining and incorporating meta (fault) information into the debugging process were elucidated. Such information, if reasonable, can facilitate and accelerate the debugging process significantly. However, even in the case of the availability of high-quality fault information, we discovered substantial drawbacks of the debugging system presented so far. That is, such a system either chooses automatically a solution (diagnosis) based on the given fault information in a solution space of (generally) exponential size or refers a subset of all solutions, e.g. the most probable solutions, to the user for manual inspection. In the former case, the probability of being presented a solution KB with undesired semantics is very high implying unwanted changes to the faulty KB and unexpected entailments and non-entailments as well as future errors. Such unexpected semantics can be critical or even fatal; one should imagine intelligent medical applications relying on such KBs, for instance. In the latter case, the burden is placed on the user(s) who must mentally anticipate the implications of applying different repairs (using the different submitted diagnoses) to the KB which is practically impossible for human beings both from the\n85"}, {"heading": "86 CHAPTER 5. SUMMARY", "text": "time/effort as well as from the mental perspective. Moreover, it is basically intractable to generate all possible solutions. Hence, it is not even sure that the manually investigated solutions include to correct one (with the postulated semantics).\nThis leads us to the next part which deals with exactly these issues and proposes a solution.\nPart II\nInteractive Knowledge Base Debugging\n87\n89\nThis part is organized as follows:\nIn Chapter 6, we first discuss how disadvantages of non-interactive KB debugging procedures can be overcome by allowing a user to take part in the debugging process. Next, we define the problem of interactive static KB debugging as well as the problem of interactive dynamic KB debugging which \u201cnaturally\u201d arise from the fact that the DPI in interactive KB debugging is always renewed after a new test case has been specified (a new query has been answered). The former problem searches for a solution KB w.r.t. the DPI given as input such that this solution KB satisfies all test cases added during the debugging session and there is no other such solution KB. The latter problem searches for a solution KB w.r.t. the current DPI (i.e. the input DPI including all new test cases added throughout the debugging session so far) such that there is no other solution KB w.r.t. the current DPI.\nNext, in Chapter 7, the central term of a query is specified which constitutes the medium for user interaction. Queries are generated from a set of leading diagnoses which is characterized thereafter. The set of leading diagnoses is uniquely partitioned into three subsets by each query. The tuple including these subsets is called q-partition. Subsequently, the reader is given some explanations how the q-partition can be interpreted, and how it relates to a query. In fact, we will prove that the notion of a q-partition can serve as a criterion for checking whether a set of logical formulas is a query or not. After that, we will learn that a query exists for any set of (at least two) leading diagnoses which grants that the presented algorithms will definitely be able to come up with a query without the need to impose any restrictions on which (minimal) diagnoses are computed by the diagnosis engine in each iteration.\nChapter 8 shows a method for the generation of (a pool of) set-minimal queries (Algorithm 4) aiming at stressing the interacting user as sparsely as possible, features in-depth discussions of this method\u2019s properties, proves its correctness, provides complexity results and gives some illustrating examples. Further on, drawbacks of this method are pointed out and possible solutions are discussed.\nSubsequently, Chapter 9 deals with the presentation of the central algorithm of this work which implements an interactive KB debugging system (Algorithm 5). First, an overview of the workflow of interactive KB debugging is given, followed by a more comprehensive detailed specification of the algorithm. Some query selection measures are discussed [RSFF13, SFFR12] and optimization versions of the problems of interactive dynamic and static KB debugging are defined where the goal is to obtain the solution to these problems by asking the user a minimal number of queries. Finally, we prove the correctness of the interactive KB debugging algorithm and provide a discussion of its complexity.\nNon-theoretically-oriented readers might well skip Sections 8.2, 8.4, 8.5, 8.7 and 9.4 in this part. Moreover, for the superficially interested reader, it may suffice to concentrate only on Chapter 6 and Sections 7.1, 7.2 and 9.1 in this part.20\n20Parts of Part II already appeared in [Rod15].\nChapter 6\nMotivation and Problem Definitions\nSo far, we have learned that the problem of (parsimonious) KB debugging as defined in Problem Definitions 3.1 and 3.2 in Chapter 3 can be solved by investigating minimal diagnoses w.r.t. a given DPI \u3008K,B,P ,N \u3009R. We have seen how minimal diagnoses can be computed, we have introduced a probability space over diagnoses and we have discussed how a-priori probability estimates for diagnoses can be established. Now, assume the situation where a DPI with say 100 minimal diagnoses is given, among which there is one diagnosis D with highest estimated probability p(D) = 10%. By the definitions of a diagnosis and a solution KB (Definitions 3.2 and 3.5), each of the 100 diagnoses can be used to formulate a solution KB w.r.t. the DPI \u3008K,B,P ,N \u3009R. So, should the system output the solution KB (K \\D) \u222aUP obtained fromD as the optimal solution? Will a user be satisfied with a likeliness of 90% of being offered a suboptimal solution? What if the diagnoses probabilities are bad estimates and another diagnosis D\u2032 should actually have a probability of 20%?\nWhy not simply apply Algorithm 3 to show all 100 minimal diagnoses to the user and let them select the preferred one by hand? First, due to the complexity of diagnosis calculation algorithms (cf. Chapter 1), pre-computation of 100 (or, generally, all) minimal diagnoses is usually not tractable within reasonable time. This makes such an approach quite unattractive in an interactive setting. Second, going through large sets of diagnoses can be time-consuming, tedious and error-prone. Third, human beings are normally not capable of (fully) realizing the semantic consequences of deleting a diagnosis from a KB, especially if the KB is large, complex and/or has been created by multiple engineers or automatic systems. Thus, applying a suboptimal diagnosis can result in unexpected entailments or unwanted changes, and thus an incorrect solution KB (incorrect in the sense of the semantics, not in the sense of violating given requirements or test cases), which might cause unexpected new faults and contradictions when augmented by new formulas. Consequently, a solution diagnosis is only acceptable if the user has sufficiently scrutinized and approved its semantic effect to the KB.\nThis leads to the definition of two types of Interactive KB Debugging problems. First, there is the problem of Interactive Dynamic KB Debugging which, given an input DPI, aims at the extension of this DPI by new test cases confirmed by a user such that there is only one minimal diagnosis left w.r.t. the extended DPI. Second, we specify the problem of Interactive Static KB Debugging which, given an input DPI, aims at the formulation of new test cases confirmed by a user such that these new test cases rule out all but one minimal diagnosis w.r.t. the input DPI.\n91"}, {"heading": "92 CHAPTER 6. MOTIVATION AND PROBLEM DEFINITIONS", "text": "Problem Definition 6.1 (Interactive Dynamic KB Debugging). Given a DPI \u3008K,B,P ,N \u3009R, the task is to find a maximal solution KB (K \\ D) \u222a UP\u222aP \u2032 w.r.t. a DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R such that\n\u2022 D is the only minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and\n\u2022 a user has confirmed that each p\u2032 \u2208 P \u2032 is a positive test case and that each n \u2032 \u2208 N \u2032 is a negative test case.\nRemark 6.1 The solution of an Interactive Dynamic KB Debugging problem given the DPI \u3008K,B,P ,N \u3009R solves the problem of KB Debugging (Problem Defnition 3.1) as well as the problem of Parsimonious KB Debugging (Problem Defnition 3.2) for the DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, but in general not for the original DPI \u3008K,B,P ,N \u3009R. This is the reason why we term it \u201cdynamic\u201d, since a solution is found for a version of the initial DPI that has been extended by test cases.\nProblem Definition 6.2 (Interactive Static KB Debugging). Given a DPI \u3008K,B,P ,N \u3009R, the task is to find a maximal solution KB (K \\ D) \u222a UP w.r.t. \u3008K,B,P ,N \u3009R such that\n\u2022 there are sets of positive test cases P \u2032 and negative test cases N \u2032 where a user has confirmed that each p\u2032 \u2208 P \u2032 is a positive test case and that each n \u2032 \u2208 N \u2032 is a negative test case, and\n\u2022 D is the only minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R that satisfies all positive and negative test cases P \u2032 and N \u2032, respectively.\nRemark 6.2 The solution of an Interactive Static KB Debugging problem given the DPI \u3008K,B,P ,N \u3009R constitutes a solution to the problem of KB Debugging (Problem Defnition 3.1) as well as to the problem of Parsimonious KB Debugging (Problem Defnition 3.2) for the original DPI \u3008K,B,P ,N \u3009R, therefore the term \u201cstatic\u201d.\nNow, we give a more formal definition of a true diagnosis (an informal characterization of which was given in Section 4.6). If sufficiently many new test cases are specified and added to a given DPI such that there is only one remaining minimal diagnosis w.r.t. the input DPI (the input DPI extended by the new test cases) left, then this diagnosis is referred to as the true diagnosis w.r.t. Interactive Static (Dynamic) KB Debugging.\nDefinition 6.1 (True Diagnosis). Let Dt be equal to D in Problem Definition 9.2 (9.1). Then Dt is called the true diagnosis w.r.t. Interactive Static KB Debugging (Interactive Dynamic KB Debugging).\nChapter 7\nUser Interaction\nThe idea in interactive KB debugging is to iteratively consult a user asking them to give additional information as regards desired and undesired entailments of the correct KB. Thus, the principle of interactive KB debugging is based on that of Sequential Diagnosis which has been suggested by [dKW87] as an iterative way to localize the faulty components (among an initially large set of possibilities) in malfunctioning digital circuits by performing repeated (most informative) measurements. We have shown in our previous works [SF10, SFFR12] how sequential diagnosis can be applied to KBs (ontologies).\nIn our approach, for the selection of which question (of a pool of possible ones) to ask a user next, an active learning [Set12] approach is applied.21 Active Learning is an iterative supervised machine learning technique in which a learning algorithm is able to interactively query the user to obtain a label for a desired unlabeled instance. In the case of a KB debugging system, an unlabeled instance is a set of logical formulas and the label is whether the conjunction of these formulas should or should not be entailed by the correct KB. Since the learner can choose the instances to be labeled, the number of consultations of an interacting user required to learn a concept (in this case the one solution KB with the desired semantics w.r.t. a given DPI) can often be much lower than the number required in a standard supervised learning setting since the risk that the algorithm must deal with lots of uninformative examples is reduced.\nWe suppose the user of an interactive KB debugger to be a single person or multiple persons, usually experts of the particular domain the faulty KB is dealing with or authors of the faulty KB. Moreover, we assume the interacting user to be able to answer concrete queries about the intended domain that should be modeled. Otherwise put, we suppose that a user can classify a given logical formula (or a conjunction of logical formulas) as a wanted or unwanted proposition in the intended domain, i.e. as an entailment or non-entailment of the correct domain model. We have already argued in Chapter 1 why this assumption is plausible."}, {"heading": "7.1 Queries", "text": "In interactive KB debugging, a set of logical formulas Q is presented to the user who should decide whether to assign Q to the set of positive (P ) or negative (N ) test cases w.r.t. a given DPI \u3008K,B,P ,N \u3009R. In other words, the system asks the user \u201cshould the KB you intend to model entail all formulas in Q?\u201d. In that, Q is generated by the debugging algorithm in a way that any decision of the user\n1. invalidates at least one minimal diagnosis (search space restriction) and\n21Note that the minimal a-posteriori expected entropy of solution candidate probabilities as a means to select the best next measurement as used in [dKW87] is only one of many possible active learning strategies [Set12].\n93"}, {"heading": "94 CHAPTER 7. USER INTERACTION", "text": "2. preserves validity of at least one minimal diagnosis (solution preservation).\nWe call a set of logical formulas Q with these properties a query. Successive classification of queries as entailments (all formulas in Q must be entailed) or non-entailments (at least one formula in Q must not be entailed) of the correct KB enables gradual restriction of the search space for (minimal) diagnoses. Further on, classification of sufficiently many queries guarantees the detection of a single correct solution diagnosis which can be used to determine a solution KB with the correct semantics w.r.t. a given DPI.22\nDefinition 7.1 (Query). Let \u3008K,B,P ,N \u3009R over L and D \u2286 mD\u3008K,B,P,N \u3009R . Then a set of logical formulas Q 6= \u2205 over L is called a query w.r.t. D iff there are diagnoses D,D\u2032 \u2208 D such that D /\u2208 mD\u3008K,B,P\u222a{Q},N \u3009R and D\u2032 /\u2208mD\u3008K,B,P,N\u222a{Q}\u3009R . The set of all queries w.r.t. D and \u3008K,B,P ,N \u3009R is denoted by QD,\u3008K,B,P,N \u3009R .\nRemark 7.1 Although Definition 7.1 only postulates that at least one diagnosis in D is invalidated for whatever answer is given to the query, this implies that, for each answer to the query, there is also a diagnosis that remains valid after adding the corresponding test case to the DPI, as will be shown by Proposition 7.4.\nSo, w.r.t. a set of minimal diagnoses D \u2286 mD\u3008K,B,P,N \u3009R , a query Q is a set of logical formulas that rules out at least one diagnosis in D (and therefore in mD\u3008K,B,P,N \u3009R ) as a candidate to formulate a solution KB, regardless of whether Q is classified as a positive or negative test case."}, {"heading": "7.2 Leading Diagnoses", "text": "Query generation requires a precalculated set of minimal diagnoses D \u2286 mD\u3008K,B,P,N \u3009R that serves as a representative for all minimal diagnoses mD\u3008K,B,P,N \u3009R . As already mentioned, computation of the entire set mD\u3008K,B,P,N \u3009R is generally not tractable within reasonable time. Usually, D is defined as a set of most probable or minimum cardinality diagnoses (cf. Chapter 4). Therefore, D is called the set of leading diagnoses w.r.t. \u3008K,B,P ,N \u3009R [SFFR12].\nThe leading diagnoses D are then exploited to determine a query Q the answering of which enables a discrimination between the diagnoses in mD\u3008K,B,P,N \u3009R . That is, a subset of mD\u3008K,B,P,N \u3009R which is not \u201ccompatible\u201d with the new information obtained by adding the test case Q to P or N is ruled out (see Proposition 7.3 below). For the computation of the subsequent query only a leading diagnoses set Dnew w.r.t. the minimal diagnoses still compliant with the new sets of test cases P \u2032 and N \u2032 is taken into consideration, i.e. Dnew \u2286 D\u3008K,B,P \u2032,N \u2032\u3009R .\nThe number of precomputed leading diagnoses D affects the quality of the obtained query. The higher |D|, the more representative is D w.r.t. mD\u3008K,B,P,N \u3009R , the more options there are to specify a query in a way that a user can easily comprehend and answer it, and the higher is the chance that a query that eliminates a high rate of diagnoses w.r.t. D will also eliminate a high rate of all minimal diagnoses mD\u3008K,B,P,N \u3009R . The selection of a lower |D| on the other hand means better timeliness regarding the interaction with a user, first because fewer leading diagnoses might be computed much faster and second because the search space for an \u201coptimal\u201d query is smaller.23 So, the optimal number of leading diagnoses\n22Correctness of the diagnosis must not be understood as a guarantee that all formulas in the KB which are not in the diagnosis are definitely correct. Instead, correctness must be seen with regard to other diagnoses and with the \u201cPrinciple of Parsimony\u201d in mind (cf. Section 3.1). That is, all other possible diagnoses are ruled out by a present set of test cases wherefore the single remaining diagnosis is the one that is correct (in comparison with all other incorrect ones). And, there is no evidence (at the time the correct diagnosis is found) that any other formulas in the KB might be faulty. This might change however after new formulas are added to the KB.\n23Roughly, a query Q is \u201coptimal\u201d if the number of queries that still need to be answered to identify the desired solution KB after Q is added to the (positive or negative) test cases is minimal. \u201cOptimality\u201d of a query can be captured by quantitative information theoretic measures studied in the field of active learning [Set12] that can be used to estimate the quality of a query beforehand, i.e. before an answer to it is known. See Section 9.3 and [RSFF13, SF10, SFFR12] for details."}, {"heading": "7.3. Q-PARTITIONS 95", "text": "depends on the complexity of the particular DPI considered. One way to determine a suitable |D| can be to first define an interval [nmin, nmax] that must comprise |D| where the upper bound defines the desired number of leading diagnoses and the lower bound the minimally postulated number. Second, the search for minimal diagnoses is run at least as long as it takes to compute nmin diagnoses and at the longest until nmax diagnoses have been found or a timeout t expires that is specified in a manner it enables frequent user interaction. Note that such parameters have already been taken into account in the non-interactive KB debugging Algorithm 2 (see Section 4.7)."}, {"heading": "7.3 Q-Partitions", "text": "Now we introduce the notion of a q-partition, a partition of the leading diagnoses set D induced by a query w.r.t. D. A q-partition will be a helpful instrument in deciding whether a set of logical formulas is a query or not. It will facilitate an estimation of the impact a query answer has in terms of invalidation of minimal diagnoses. And, given fault probabilities, it will enable us to gauge the probability of getting a positive or negative answer to a query.\nFrom now on, given a DPI \u3008K,B,P ,N \u3009R and some minimal diagnosis Di w.r.t. \u3008K,B,P ,N \u3009R, we will use the following abbreviation for the solution KB obtained by deletion of Di along with the given background knowledge B:\nK\u2217i := (K \\ Di) \u222a B \u222a UP (7.1)\nDefinition 7.2 (q-Partition24). Let \u3008K,B,P ,N \u3009R be a DPI over L, D \u2286 mD\u3008K,B,P,N \u3009R . Further, let Q be a set of logical formulas over L and\n\u2022 D+(Q) := {Di \u2208 D | K\u2217i |= Q},\n\u2022 D\u2212(Q) := {Di \u2208 D | \u2203x \u2208 R \u222aN : K\u2217i \u222aQ violates x},\n\u2022 D0(Q) := D \\ (D+j \u222aD \u2212 j ).\nThen \u3008D+(Q),D\u2212(Q),D0(Q)\u3009 is called a q-partition iff Q is a query w.r.t. D and \u3008K,B,P ,N \u3009R.\nRemark 7.2 The set D\u2212(Q) contains exactly those diagnoses Di \u2208 D where K \\ Di is invalid w.r.t. \u3008\u00b7,B,P \u222a {Q} ,N \u3009 (cf. Definition 3.3).\nProposition 7.1. For each query Q w.r.t. some D \u2286 mD\u3008K,B,P,N \u3009R it holds that \u3008D+(Q), D\u2212(Q), D0(Q)\u3009 is a partition of D.\nProof. First, by definition of D0(Q), we have that D+(Q)\u222aD\u2212(Q)\u222aD0(Q) = D, D+(Q)\u2229D0(Q) = \u2205 and D\u2212(Q) \u2229 D0(Q) = \u2205. Second, D+(Q) \u2229 D\u2212(Q) = \u2205 since K\u2217i |= Qj and \u2203x \u2208 R \u222a N : (K\u2217i \u222aQj violates x) imply by idempotency ofL thatK\u2217i violates some x \u2208 R\u222aN which is a contradiction to Di being a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. Thus, each diagnosis in D is an element of exactly one set of D+(Q),D\u2212(Q),D0(Q) which is equivalent to the statement of the proposition.\nRemark 7.3 In fact, Proposition 7.1 holds for any set D \u2286 aD\u3008K,B,P,N \u3009R , i.e. for any subset of all diagnoses w.r.t. \u3008K,B,P ,N \u3009R. This can be easily seen from the proof of Proposition 7.1 which does not require minimality of diagnoses. That is, any set of diagnoses w.r.t. a DPI is partitioned into the three sets D+(Q), D\u2212(Q) and D0(Q) as per Definition 7.2 by a query Q w.r.t. this DPI.\n24In existing literature, e.g. [SFFR12, RSFF13, SF10], a q-partition is often simply referred to as partition. We call it q-partition to emphasize that not each partition of D into three sets is necessarily a q-partition."}, {"heading": "96 CHAPTER 7. USER INTERACTION", "text": "Proposition 7.2. For each query Q w.r.t. some D \u2286 mD\u3008K,B,P,N \u3009R there is one and only one partition \u3008D+(Q),D\u2212(Q),D0(Q)\u3009.\nProof. The existence of a partition D+(Q),D\u2212(Q),D0(Q) follows directly from Proposition 7.1. Assume there are two different partitions \u3008D+1 (Q),D \u2212 1 (Q),D 0 1(Q)\u3009 and \u3008D+2 (Q),D \u2212 2 (Q),D 0 2(Q)\u3009. Then, (a) D+1 (Q) 6= D + 2 (Q) or (b) D \u2212 1 (Q) 6= D \u2212 2 (Q) or (c) D 0 1(Q) 6= D02(Q) must hold. If (a) is true, then there is one diagnosis Di \u2208 D such that K\u2217i |= Q and K\u2217i 6|= Q \u2013 a contradiction. If (b) is true, then there is one diagnosis Di \u2208 D such that K\u2217i \u222aQ violates some x \u2208 R \u222a N and K\u2217i \u222aQ does not violate any y \u2208 R \u222a N \u2013 a contradiction. If (c) is true, then (D+1 (Q) \u222aD \u2212 1 (Q)) 6= (D + 2 (Q) \u222aD \u2212 2 (Q)) which implies that either (a) or (b) must be true.\nDue to the uniqueness of a q-partition \u3008D+(Q),D\u2212(Q),D0(Q)\u3009 for a query Q, we denote this qpartition by P(Q). As a consequence of Definition 7.2 and Proposition 7.2, a query Q is a set of common entailments of KBsK\u2217i , each resulting from the deletion of a single minimal diagnosisDi \u2208 D+(Q) from K.\nCorollary 7.1. For each query Q \u2208 QD,\u3008K,B,P,N \u3009R there is a set of minimal diagnoses D+(Q) \u2286 mD\u3008K,B,P,N \u3009R as defined by Definition 7.2 such that Q \u2286 {e | \u2200Di \u2208 D+(Q) : K\u2217i |= e}."}, {"heading": "7.4 Interpretation of Q-Partitions", "text": "Since K\u2217i corresponds to the solution KB (along with B) obtained under the assumption that Dt = Di, i.e. the true diagnosis (cf. Definition 6.1) corresponds to Di, the sets D+(Q) and D\u2212(Q) can be interpreted as those leading diagnoses that predict the classification of Q as a positive and negative test case, respectively. In other words, if the true diagnosis Dt is in D+(Q), then the true solution KB K\u2217t entails Q by Definition 7.2. Therefore the user will answer Q positively (cf. Definition 6.1). If, conversely,Dt is in D\u2212(Q), then the true solution KBK\u2217t would be invalidated ifQwas answered positively, since K\u2217t \u222a Q = (K \\ Dt) \u222a B \u222a UP\u222a{Q} violates some x \u2208 R \u222a N and thus K \\ Dt is invalid w.r.t. \u3008\u00b7,B,P \u222a {Q} ,N \u3009R, which implies that Dt is not a diagnosis w.r.t. \u3008K,B,P \u222a {Q} ,N \u3009R according to Proposition 3.2. Hence, the user will answer Q negatively (cf. Definition 6.1). Diagnoses in D0(Q) on the other hand neither predict Q \u2208 P nor Q \u2208 N . This means that we do not know how the user will answer a query Q for which the true diagnosis Dt is in D0(Q). In this case, for any answer to Q, the true diagnosis Dt is in the set of minimal diagnoses w.r.t. the new DPI including Q as a test case. To summarize: If the true diagnosis Dt is an element of D+(Q) (D\u2212(Q)), then Q will be answered positively (negatively).\nConversely, this means that a q-partition P(Q) gives a prior indication which leading diagnoses would be invalidated by a user\u2019s answer. Diagnoses in D+(Q) are invalidated by the classification Q \u2208 N , and diagnoses in D\u2212(Q) in case of Q \u2208 P . Diagnoses in D0(Q) can never be invalidated by an answer to Q. Thus, intuitively, queries with D0(Q) = \u2205 are preferable over other queries (as per the information provided by the set of leading diagnoses D) as the number of (definitely) eliminated diagnoses in mD\u3008K,B,P,N \u3009R should be maximized.\nThe following proposition is a direct consequence of Corollary 3.3 and explicates the impact of the addition of a test case to a DPI regarding the set of minimal diagnoses for this DPI.\nProposition 7.3. Let Q be a query w.r.t. D \u2286 mD\u3008K,B,P,N \u3009R and let the answer of a user to Q be u(Q) \u2208 {true, false}.\nIf u(Q) = true , then Di \u2208 mD\u3008K,B,P,N \u3009R is a diagnosis w.r.t. \u3008K,B,P \u222a {Q} ,N \u3009R iff K \\ Di is valid w.r.t. \u3008\u00b7,B,P \u222a {Q} ,N \u3009R."}, {"heading": "7.5. RELATION BETWEEN QUERY AND Q-PARTITION 97", "text": "In other words, both of the following conditions must hold:\n\u2200r \u2208 R : K\u2217i \u222aQ does not violate r \u2200n \u2208 N : K\u2217i \u222aQ 6|= n\nIf u(Q) = false , then Di \u2208 mD\u3008K,B,P,N \u3009R is a diagnosis w.r.t. \u3008K,B,P ,N \u222a {Q}\u3009R iff K \\ Di is valid w.r.t. \u3008\u00b7,B,P ,N \u222a {Q}\u3009R.\nIn other words, both of the following conditions must hold:\n\u2200r \u2208 R : K\u2217i does not violate r \u2200n \u2208 (N \u222a {Q}) : K\u2217i 6|= n\nRemark 7.4 From Proposition 7.3 and Definition 7.2 it is easy to see that at least Di \u2208 D\u2212(Q) \u2282 mD\u3008K,B,P,N \u3009R are eliminated by a positive answer to Q. Namely, D\n\u2212(Q) comprises exactly those diagnoses Di that imply the violation of some r \u2208 R or the entailment of some n \u2208 N if Q is added to K\u2217i . On the other hand, at least Di \u2208 D+(Q) \u2282 mD\u3008K,B,P,N \u3009R are discarded if u(Q) = false as all diagnoses in D+(Q) entail Q which must not be entailed.\nNote that, in general, the addition of a query to the test cases of a DPI causes not only an invalidation of some leading minimal diagnoses in D, but also the elimination of minimal diagnoses that have not even been computed yet. On the other hand, an added test case might also introduce new minimal diagnoses, i.e. ones that were no minimal diagnoses before this test case was added. However, the newly obtained DPI after the addition of any new test case can only exhibit a reduced set of all (i.e. minimal and nonminimal) diagnoses compared with the DPI before the test case was added (we will prove this result by Proposition 12.3)."}, {"heading": "7.5 The Relation between a Query and Its Q-Partition", "text": "The following proposition shows the relationship between a query and its q-partition and provides a criterion that enables to check whether a set of logical formulas is a query w.r.t. some set of leading diagnoses or not.\nProposition 7.4. Let \u3008K,B,P ,N \u3009R be a DPI over L and D \u2286 mD\u3008K,B,P,N \u3009R . Then a set of logical formulas Q 6= \u2205 over L is a query w.r.t. D iff D+(Q) 6= \u2205 and D\u2212(Q) 6= \u2205.\nProof. \u201c\u21d0\u201d: If D+(Q) 6= \u2205 and D\u2212(Q) 6= \u2205 holds, then a non-empty set of diagnoses D\u2212(Q) (D+(Q)) becomes invalid for positive (negative) answer to Q. So, Q is a query.\n\u201c\u21d2\u201d: If Q is a query, then there are diagnoses D,D\u2032 \u2208 D such that D /\u2208 mD\u3008K,B,P\u222a{Q},N \u3009R and D\u2032 /\u2208mD\u3008K,B,P,N\u222a{Q}\u3009R . Consequently,D \u2208 D\\mD\u3008K,B,P\u222a{Q},N \u3009R andD\u2032 \u2208 D\\mD\u3008K,B,P,N\u222a{Q}\u3009R holds. But, as the diagnoses in D \\mD\u3008K,B,P\u222a{Q},N \u3009R are exactly the diagnoses in D that become invalid by the positive answer to Q, we obtain D \u2208 D\u2212(Q). The argumentation for D\u2032 \u2208 D+(Q) is analogous. Hence, D+(Q) 6= \u2205 and D\u2212(Q) 6= \u2205.\nCorollary 7.2. Let D \u2286mD\u3008K,B,P,N \u3009R . Then, for each q-partition P(Q) = \u3008D+(Q),D\u2212(Q),D0(Q)\u3009 w.r.t. D it holds that D+(Q) 6= \u2205 and D\u2212(Q) 6= \u2205.\nProof. Follows from Definition 7.2 which grants the existence of a query for any q-partition and Proposition 7.4 which states that neither D+(Q) nor D\u2212(Q) must be empty sets for any query.\nSo, by Proposition 7.4, a query not only eliminates at least one leading diagnosis, but also leaves at least one leading diagnosis valid. Therefore, an admissible DPI can never get non-admissible by adding a query to the positive or negative test cases."}, {"heading": "98 CHAPTER 7. USER INTERACTION", "text": "Corollary 7.3. Let \u3008K,B,P ,N \u3009R be an admissible DPI, D \u2286mD\u3008K,B,P,N \u3009R andQ \u2208 QD,\u3008K,B,P,N \u3009R . Then \u3008K,B,P \u222a {Q} ,N \u3009R as well as \u3008K,B,P ,N \u222a {Q}\u3009R are admissible DPIs.\nProof. Assume that \u3008K,B,P \u222a {Q} ,N \u3009R is non-admissible. Then there is no valid diagnosis for this DPI. Since \u3008K,B,P ,N \u3009R is an admissible DPI, this means that Q invalidates each diagnosis D \u2208 aD\u3008K,B,P,N \u3009R \u2287 mD\u3008K,B,P,N \u3009R \u2283 D. By Proposition 7.4, this is a contradiction to the fact that Q is a query. The argumentation for \u3008K,B,P ,N \u222a {Q}\u3009R is analogue.\nThis means in particular that a query can never contain a conflict set or result in a violation of some requirement r \u2208 R when added to B \u222a UP (cf. Proposition 3.4)."}, {"heading": "7.6 Existence of Queries", "text": "For any set of at least two leading minimal diagnoses the existence of a query is guaranteed, as the next proposition and corollary show. In particular, this implies that for arbitrary two minimal diagnoses D,D\u2032 w.r.t. a DPI there is a query Q that enables to differentiate between D and D\u2032, i.e. exactly one of these diagnoses is invalidated by each answer to Q.\nProposition 7.5. Let D \u2286 mD\u3008K,B,P,N \u3009R with |D| \u2265 2 and UD be the union of all diagnoses in D. Then\n(I) Q := (UD \\ Di) is a query w.r.t. D for arbitrary Di \u2208 D and\n(II) P(Q) = \u3008{Di} ,D \\ {Di} , \u2205\u3009.\nProof. Ad (I): Assume thatQ is not a query. Then either (1)Q = \u2205 or (2) D+(Q) = \u2205 or (3) D\u2212(Q) = \u2205. In the following we prove that neither (1) nor (2) nor (3) can hold.\n(1): Q = \u2205 means that Di \u2287 UD. Since any diagnosis D in D is a subset of UD, this implies that for each D \u2208 D, D \u2286 Di holds. As |D| \u2265 2 is assumed, there is a Dk 6= Di \u2208 D for which this property holds. This, however, is a contradiction to the minimality of diagnosis Di.\n(2): D+(Q) = \u2205 cannot hold, since (K \\ Di) \u2287 (UD \\ Di) and UD \\ Di |= Q by monotonicity of description logics imply that K\u2217i = (K \\ Di) \u222a B \u222a UP |= Q. Hence, there is at least one diagnosis, namely Di, in D+(Q).\n(3): To prove that D\u2212(Q) 6= \u2205, we must show that there is a diagnosis D \u2208 D such that Y := (K \\ D)\u222aB\u222aUP\u222aQ = (K\\D)\u222aB\u222aUP\u222a(UD\\Di) is incoherent. However, (K\\D)\u222a(UD\\Di) = K\\(D\u2229Di) by distributive and De Morgan laws which yields Y = K \\ (D \u2229 Di) \u222a B \u222a UP . But, D \u2229 Di \u2282 D must hold as D 6\u2286 Di by the subset-minimality of Di whereby D must comprise a formula ax /\u2208 Di. Hence, Y \u2283 (K \\ D) \u222a B \u222a UP is incoherent by subset-minimality of D.\nAd (II): We already know that Di \u2208 D+(Q) by (2). Since D \u2208 D in (3) can be chosen arbitrarily, we obtain that D \u2208 D\u2212(Q) for all diagnoses D \u2208 D \\ {Di}.\nWe immediately obtain a lower bound for the number of queries by Proposition 7.5:\nCorollary 7.4. Let D \u2286 mD\u3008K,B,P,N \u3009R with |D| > 1. Then a lower bound for the number of queries w.r.t. D is |D|.\nRemark 7.5 Notice that the preceding proposition and corollary require a set of minimal diagnoses. This means that subset-minimality of diagnoses is a necessary prerequisite for guaranteeing the possibility of discrimination between diagnoses. In other words, interactive debugging by means of (some or only) non-minimal diagnoses cannot be proven to work correctly (without making any further assumptions).\nChapter 8\nQuery Generation\nIn this chapter we want to describe, discuss and prove the correctness of methods for the generation of queries which takes place in each iteration of an interactive KB debugging algorithm after a set of leading diagnoses has been determined. With Algorithm 4, similar versions of which can be found in [SFFR12, RSFF13], we present a way to compute a pool QP of queries and associated q-partitions w.r.t. a set of leading diagnoses D and a DPI \u3008K,B,P ,N \u3009R. The generation of this pool QP is the first stage of the query computation function used in the interactive debugging algorithm (Algorithm 5) presented below. In a second stage, one particular query that meets certain criteria such as maximum expected information gain is selected from QP (see Section 9.3).\nBefore we give a description of Algorithm 4, let us have a look at some example by which we want to demonstrate the principle how a query w.r.t. some set of leading diagnoses for a DPI can be constructed. This should give the reader a first idea and an intuition of how the presented algorithm works.\nExample 8.1 Consider the example FOL DPI given by Table 15.2. The set of minimal conflict sets mC\u3008K,B,P,N \u3009R = {C1, C2} = {\u30081, 3, 4\u3009 , \u30081, 2, 3, 5\u3009} (like in previous examples, formulas ax i in Table 15.2 are sometimes referred to just by their number i if it is clear from the context what is meant). Let the set of leading diagnoses be the set of all minimal diagnoses, i.e. D = mD\u3008K,B,P,N \u3009R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}. To enable a better understanding of this example, we first analyze why C1 and C2 are minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R.\nWhy is C1 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? In the following we underline the formulas ax i and relevant parts of these formulas used in the derivation of the conflict set. First, there is the background KB B including a1(w) and a1(u). Due to ax 1, by substitution of X by w (written as X/w), we obtain a2(w),m1(w) and m2(w) from a1(w). Likewise, we can derive a2(u),m1(u) and m2(u) from a1(u) by X/u. Substituting X by w in ax 3 yields m1(w)\u2192 \u00aca(w) \u2227 b(w). Thus, we obtain \u00aca(w). A substitution of X by u in ax 4 results in m2(u) \u2192 (\u2200Y s(u, Y ) \u2192 a(Y )) \u2227 d(u). By Y/w, we have m2(u)\u2192 (s(u,w)\u2192 a(w)) \u2227 d(u). Since m2(u) has already been deduced from the background formula a1(u) and s(u,w) is a background formula as well, we can conclude a(w) from ax 4. All in all, we have derived \u00aca(w) and a(w), i.e. an inconsistency, by means of B and C1 (and UP which is the empty set) wherefore C1 is a conflict set w.r.t. \u3008K,B,P ,N \u3009R by Definition 4.1. The minimality of C1 can be easily verified by the way we derived that it is a conflict set; namely, leaving out any of the formulas ax 1, ax 3 or ax 4 does not allow to derive an inconsistency or incoherency (note that the set of negative test cases N is empty).\nWhy is C2 a conflict set w.r.t. \u3008K,B,P ,N \u3009R? We argue as follows to deduce the inconsistency\n99\n100 CHAPTER 8. QUERY GENERATION\nresponsible for C2 to be a conflict set (the relevant implications and used formulas are again underlined):\n(1) : a1(w) \u2208 B : a1(w) (2) : X/w in ax 1 : a1(w) \u2192 a2(w) \u2227m1(w) \u2227m2(w) (3) : X/w in ax 3 : m1(w) \u2192 \u00aca(w) \u2227 b(w)\n(4) : ax 5 and X/w : b(w) \u2192 m3(w) (5) : (1)\u2212 (4) : m3(w)\n(6) : a1(u) \u2208 B : a1(u) (7) : X/u in ax 1 : a1(u) \u2192 a2(u) \u2227m1(u) \u2227m2(u) (8) : X/u in ax 2 : a2(u) \u2192 \u00ac(\u2203Y s(u, Y ) \u2227m3(Y ))\n\u2227 (\u2203Zs(u, Z) \u2227m2(Z)) (9) : (6)\u2212 (8) : \u00ac(\u2203Y s(u, Y ) \u2227m3(Y ))\n(10) : s(u,w) \u2208 B : s(u,w) (11) : (5) and (10) : \u2203Y s(u, Y ) \u2227m3(Y )\n(9) and (11) : E\nMinimality of C2 can again be verified by observing that, given any formula of C2 is left out, no inconsistency or incoherency can be derived.\nNow we show how to construct a query manually. As suggested by Definition 7.2 and Proposition 7.4 and discussed in Section 7.5, an obvious way of generating a query w.r.t. D and \u3008K,B,P ,N \u3009R is via the notion of a q-partition. Definition 7.2 states that Q is a set of common entailments of KBs K\u2217i (Formula 7.1) where Di \u2208 D+(Q), a subset of D. Hence, a first step towards query computation is to choose some non-empty subset S of the leading diagnoses D which we will call the seed for query generation. For our manual construction, let S = {D3,D4} = {[4, 5], [2, 4]}. For each of the diagnoses\n101\nDi in S, we assemble the KB K\u2217i and use a reasoning engine to obtain a set of entailments EDi of K\u2217i . ForD3 we obtainK\u22173 := {1, 2, 3, 4, 5}\\{4, 5}\u222a{6, 7, 8}\u222a{} = {1, 2, 3, 6, 7, 8}. Similarly, we compute K\u22174 = {1, 3, 5, 6, 7, 8}.\nSuppose that the reasoner invoked by the used GETENTAILMENTS function produces only entailments of the type \u2200Xp1(X) \u2192 p2(X) for predicate names p1, p2 and of the type p(a) where p is a predicate name and a is a constant (cf. Remark 2.3). For this purpose, DL and OWL reasoners, respectively, such as Pellet [SPG+07], HermiT [SMH08], FaCT++ [TH06] or KAON225 could be used with their classification and realization reasoning services. The reason why this is possible can be realized after a short analysis of the DPI \u3008K,B,P ,N \u3009R given by Table 15.2. For, this DPI can be translated to DL similarly as demonstrated in Example 2.1. All the mentioned reasoners can deal with the expressivity of the resulting DL language.\nThen, we obtain the sets ED3 and ED4 , i.e. the sets of entailments of K\u22173 and K\u22174 , respectively, as depicted by Table 8.2. The set of common entailments Q, i.e. Q = ED3 \u2229 ED4 is then the set containing all elements in the rows of Table 8.2 that are above the dashed line.\nNotice at this point that the set {a1(w), a1(u), s(u,w)} = B does not need to be computed or, respectively, included in Q since none of these formulas can serve to discriminate between diagnoses (which is the only aim of a query). The simple reason for this is thatK\u2217i for eachDi \u2208 D comprises these formulas and thus each K\u2217i entails these formulas by the extensiveness of FOL (cf. Chapter 2). Since entailed by each potential solution KBK\u2217i , these formulas cannot yield a violation of any requirements or test cases since none of the KBs K\u2217i violates any requirements or test cases (follows from Definitions 3.5 and 3.2).\nContinuing with our query construction, we know by Proposition 7.4 that Q is a query w.r.t. D and \u3008K,B,P ,N \u3009R iff D+(Q) 6= \u2205 and D\u2212(Q) 6= \u2205. Whereas it is trivial that the former condition is met since D+(Q) contains (at least) the two diagnoses D3 and D4 that we used to compute Q (cf. Definition 7.2), we still need to verify whether the latter condition is actually satisfied for Q. To this end, as per Definition 7.2, we must simply find some diagnosis Dj in D \\ S = {D1,D2,D3,D4} \\ {D3,D4} = {D1,D2} such that K\u2217j \u222a Q violates some x \u2208 N \u222a R, i.e. whether some negative test case is entailed or whether this KB is incoherent or inconsistent. So, we start with D1, i.e. we examine (K \\ D1) \u222a B \u222a P \u222aQ = {1, 2, 3, 4, 5} \\ {1} \u222a {6, 7, 8} \u222a {} \u222aQ = {2, 3, 4, 5, 6, 7, 8} \u222aQ.\nAnd, indeed, we are able to prove an inconsistency for this KB. To see that, verify that by X/w in e2 \u2208 Q (see Table 8.2) and a1(w) = ax 6 \u2208 K\u22171 we can derive m1(w) which lets us conclude \u00aca(w) by the substitution of X by w in ax 3 \u2208 K\u22171 . On the other hand, we obtain a(w) by X/u in e3 \u2208 Q, {X/u, Y/w} in ax 4 \u2208 K\u22171 and s(u,w) = ax 8 \u2208 K\u22171 as shown in the explanation for conflict set C1 above. Thus, D1 \u2208 D\u2212(Q).\nThat is, we have just proven thatQ is de facto a query w.r.t. D and \u3008K,B,P ,N \u3009R. And this, although we have not yet assigned each leading diagnosis to the respective set of the q-partition ofQ. In a situation where just any query shall be asked to the user, this would suffice, and the query could be presented to the interacting user.\nHowever, in case a \u201cbest\u201d query according to some criterion shall be determined from a set of different competing queries, usually the computation of the full q-partition of each competing query is required. This is due to the fact that the q-partition provides information about several properties of queries that are considered by common query selection techniques (for details see Section 9.3). So, let us complete the q-partition for our query Q by investigating K\u22172 \u222a Q = {1, 2, 4, 5, 6, 7, 8} \u222a Q. Also in this case we can derive an inconsistency which can be easily realized by reconsidering the argumentation why C2 is a conflict set above and by using e4 \u2208 Q instead of ax 3 /\u2208 K\u22172 \u222aQ. That means, the final q-partition P(Q) for Q is given by \u3008{D3,D4} , {D1,D2} , \u2205\u3009.\nThe next question that arises directly from the proofs that D3,D4 \u2208 D\u2212(Q) is whether there is a (set-minimal) subset Qmin of Q such that Qmin preserves the discrimination properties of Q, i.e. the\n25http://kaon2.semanticweb.org/\n102 CHAPTER 8. QUERY GENERATION\nq-partition P(Qmin) = P(Q). In fact, the answer is yes for the query Q we computed, but also for the majority of other cases. This is a simple consequence of using the reasoning engine as a black-box which suggests a strategy we pursued in our query construction which relies on a precomputation of entailments and a final minimization part. Sticking to this black-box concept however does not allow to use some customized reasoning procedure that pointedly returns a set of common entailments Q for a set of diagnoses S \u2282 D where all formulas in Q are necessary for a requirement or test case violation, respectively, of KBs K\u2217j for diagnoses in D \\ S.\nWhat militates for such a black-box approach is the generality and independence of a particular logic (for which an adequate glass-box reasoner exists), the easier implementation of the debugging system and potential performance issues with a glass-box approach [KPSH05]. For a black-box algorithm to work, only a reasoner implementing a sound and complete inference procedure for the used logic L must be available.\nIn general, there is more than one minimized version of a query that preserves the q-partition. Theoretically, the number of such minimal queries w.r.t. one q-partition can be exponential in the size of the initially computed query that is provided as an input to the minimization procedure. For our query Q, for instance,\nQmin,1 = {a2(u), b(w)} = {e7, e12}, Qmin,2 = {\u2200Xa1(X)\u2192 a2(X), b(w)} = {e1, e12} , Qmin,3 = {\u2200Xa1(X)\u2192 a2(X),\n\u2200Xa1(X)\u2192 m1(X), \u2200Xm1(X)\u2192 b(X)} = {e1, e2, e4} and\nQmin,4 = {\u2200Xa1(X)\u2192 m1(X), \u2200Xa1(X)\u2192 m2(X), \u2200Xm1(X)\u2192 b(X)} = {e2, e3, e4}\nare set-minimal, q-partition preserving subqueries. Namely, each of the sets Qmin,1, Qmin,2 and Qmin,3 together with {2, 5, 6, 7, 8} implies an inconsistency since m3(w) and \u00acm3(w) can be derived and {2, 5, 6, 7, 8} \u2286 K\u22171 and {2, 5, 6, 7, 8} \u2286 K\u22172 . {e2, e3} \u2282 Qmin,4 yields an inconsistency when added to K\u22171 , i.e. a(w) and \u00aca(w) are entailed, and {e4} \u2282 Qmin,4 merged with K\u22172 yields an inconsistency, i.e. the derivation of m3(w) and \u00acm3(w). In order not to overwhelm the user we would of course ask them such a minimized version of a query rather than the full query that contains plenty of irrelevant formulas.\nAn example of a seed S that does not lead to the discovery of a query is S = {D1,D2,D3} since the set of common entailments ED1 \u2229 ED2 \u2229 ED3 = \u2205. Note that this holds when all EDi contain only entailments of the types we specified above. For other types of entailments, i.e. a different specification of the GETENTAILMENTS function, this might no longer hold."}, {"heading": "8.1 Generation of a Pool of Queries", "text": "The main function GETPOOLOFQUERIES of Algorithm 4 gets as inputs an admissible DPI \u3008K,B,P ,N \u3009R over L, a set of leading (minimal) diagnoses D \u2286 mD\u3008K,B,P,N \u3009R such that |D| \u2265 2 and a parameter q \u2208 N \u222a {\u221e} , q \u2265 1 that indicates the number of queries in QD,\u3008K,B,P,N \u3009R the algorithm is supposed to return (where q := \u221e signalizes that a maximum number of queries should be output). The way of generating a pool of queries is guided by Proposition 7.4 which says that a non-empty set Q of formulas over L is a query w.r.t. D and \u3008K,B,P ,N \u3009R if and only if D+(Q) as well as D\u2212(Q) are non-empty sets of diagnoses. That is, the necessary and sufficient criteria for Q to be a query are\n8.1. QUERY POOL GENERATION 103\nAlgorithm 4 Generation of Queries and Q-Partitions Input: an admissible DPI \u3008K,B,P ,N \u3009R, a set of minimal diagnoses D \u2286 mD\u3008K,B,P,N\u3009R such that |D| \u2265 2, a\ndesired number q \u2208 N \u222a {\u221e} , q \u2265 1 of queries w.r.t. \u3008K,B,P ,N \u3009R to be returned Output: a set QP including tuples \u2329 Q, \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a\u232a such that: If q \u2265 |QPmax|, then\n1. there are no two tuples \u3008Q,P(Q)\u3009 , \u3008Q\u2032,P(Q\u2032)\u3009 in QP such that Q = Q\u2032 or P(Q) = P(Q\u2032), and 2. QP includes a tuple \u2329 Q, \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a\u232a only if Q \u2208 QD,\u3008K,B,P,N\u3009R , and 3. QP includes at most one tuple where D+(Q) = Y for each Y \u2282 D, and 4. for each Y \u2282 D for which a query Q w.r.t. D and \u3008K,B,P ,N \u3009R exists such that (a) Q includes only\nentailments computed by the used GETENTAILMENTS function and (b) P(Q) is such that D+(Q) = Y , QP includes a tuple \u3008Q\u2032,P(Q\u2032)\u3009 such that D+(Q\u2032) = Y , and\n5. QP 6= \u2205. If q < |QPmax|, then QP includes q tuples satisfying (1), (2) and (3). (|QPmax| \u2265 0 is the maximum number of tuples \u3008Q,P(Q)\u3009 that can be computed by GETPOOLOFQUERIES by the used GETENTAILMENTS function)\n1: procedure GETPOOLOFQUERIES(\u3008K,B,P ,N \u3009R,D, q) 2: ED \u2190 \u2205 3: for D \u2208 D do 4: ED \u2190 GETENTAILMENTS(D,K,B,P) . EDr is the set of entailments of K\u2217r 5: ED \u2190 ED \u222a {\u3008D, ED\u3009} 6: for \u2205 \u2282 S \u2282 D do 7: isQuery \u2190 false 8: Q\u2190 GETCOMMONENTAILMENTS(S, ED) 9: if Q 6= \u2205 then 10: for Dr \u2208 D \\ S do 11: if Q \u2286 EDr then . Does K\u2217r |= Q ? 12: D+ \u2190 D+ \u222a {Dr} 13: else if \u00acISKBVALID(K\u2217r \u222aQ, \u3008\u00b7, \u2205, \u2205,N \u3009R) then . ISKBVALID (see Algorithm 1) 14: D\u2212 \u2190 D\u2212 \u222a {Dr} 15: isQuery \u2190 true 16: else 17: D0 \u2190 D0 \u222a {Dr} 18: if isQuery \u2227 \u00acINCLQPART(QP, \u2329 D+,D\u2212,D0 \u232a ) then\n19: Q\u2032 \u2190 MINQ(\u2205, Q, \u2205, \u2329 D+,D\u2212,D0 \u232a , \u3008K,B,P ,N \u3009R)\n20: QP\u2190 QP \u222a {\u2329 Q\u2032, \u2329 D+,D\u2212,D0 \u232a\u232a} 21: if |QP| = q then 22: return QP 23: if |QP| = 0 then 24: QP\u2190 ADDTRIVIALQUERIES(D,QP) 25: return QP 26: procedure MINQ(X,Q,QB, \u2329 D+,D\u2212,D0 \u232a , \u3008K,B,P ,N \u3009R)\n27: if X 6= \u2205 \u2227 ISQPARTCONST(QB, \u2329 D+,D\u2212,D0 \u232a , \u3008K,B,P ,N \u3009R) then 28: return \u2205 29: if |Q| = 1 then 30: return Q 31: k \u2190 SPLIT(|Q|) 32: Q1 \u2190 GET(Q, 1, k) 33: Q2 \u2190 GET(Q, k + 1, |Q|) 34: Qmin2 \u2190 MINQ(Q1, Q2, QB \u222aQ1, \u2329 D+,D\u2212,D0 \u232a , \u3008K,B,P ,N \u3009R)\n35: Qmin1 \u2190 MINQ(Qmin2 , Q1, QB \u222aQmin2 , \u2329 D+,D\u2212,D0 \u232a , \u3008K,B,P ,N \u3009R) 36: return Qmin1 \u222aQmin2 37: procedure ISQPARTCONST(Q, \u2329 D+,D\u2212,D0 \u232a , \u3008K,B,P ,N \u3009R) 38: for Dr \u2208 D\u2212 do 39: if ISKBVALID(K\u2217r \u222aQ, \u3008\u00b7, \u2205, \u2205,N \u3009R) then . ISKBVALID (see Algorithm 1) 40: return false 41: for Dr \u2208 D0 do 42: if K\u2217r |= Q then 43: return false 44: return true\n104 CHAPTER 8. QUERY GENERATION\n(CQ1) Q 6= \u2205 and\n(CQ2) D+(Q) 6= \u2205 and\n(CQ3) D\u2212(Q) 6= \u2205.\nNote, since the disjoint sets of diagnoses D+(Q) \u2286 D and D\u2212(Q) \u2286 D must not be empty, |D| \u2265 2 must be postulated in order for any queries to exist w.r.t. D and \u3008K,B,P ,N \u3009R (cf. Corollary 7.4).\nAs a first action (lines 3-5), the algorithm computes a set of entailments EDi for each K\u2217i (cf. Formula 7.1) where Di \u2208 D and stores these entailments along with the respective diagnosis as a tuple \u3008Di, EDi\u3009 in a set ED. This is accomplished by the function GETENTAILMENTS which gets a tuple \u3008X,Y, Z,W \u3009 of arguments where X,Y, Z are sets of formulas over some logic L and W is a set including sets of formulas over L. Then, GETENTAILMENTS computes a finite (cf. Remark 2.3) set of entailments of certain types (cf. Examples 8.1 and 8.6) of the KB (Y \\X) \u222a Z \u222a UW .\nThen, the algorithm runs through all proper non-empty subsets S of the leading diagnoses D and, for each S, it computes the set of common entailments Q of all KBs K\u2217i where Di \u2208 S (function GETCOMMONENTAILMENTS) by means of the precomputed set ED. That is, Q := \u22c2 D\u2208SED. If Q is non-empty, then CQ1 and CQ2 are fulfilled for Q. CQ2 is met since S 6= \u2205 and thus there is a diagnosis Di \u2208 D such that K\u2217i |= Q which implies that D+(Q) 6= \u2205. So, the algorithm proceeds to verify CQ3 (lines 10-17) in that it assigns the remaining diagnoses in D that are not in S to the according sets D+(Q), D\u2212(Q) or D0(Q) as per Definition 7.2. Note that the function ISKBVALID has been speci-\n8.2. DISCUSSION OF QUERY POOL GENERATION 105\nfied in Algorithm 1 on page 48. With the parameters given when called in line 13, ISKBVALID checks whether K\u2217r \u222a Q = (K \\ Dr) \u222a B \u222a UP\u222a{Q} does not violate any requirement in R and does not entail any test case in N . Once the call to this function returns false for one diagnosis Dr \u2208 D \\S, it holds that Dr \u2208 D\u2212(Q) thus CQ3 is definitely met. Therefore, isQuery is set to true in line 15. If, on the other hand, isQuery is not set to true for any diagnosis in D \\ S, then the set D\u2212(Q) = \u2205 and thus Q is not in QD,\u3008K,B,P,N \u3009R .\nSo far, we have proven the following proposition.\nProposition 8.1. Let a DPI \u3008K,B,P ,N \u3009R, a set of diagnoses D \u2286mD\u3008K,B,P,N \u3009R and a natural number q \u2265 1 be the input to the function GETPOOLOFQUERIES. Then, a value stored in variable Q at the time GETPOOLOFQUERIES executes line 18 is a query w.r.t. D and \u3008K,B,P ,N \u3009R iff the variable isQuery stores the value true .\nIf the purpose was only to find queries (and not q-partitions), the algorithm could stop processing for the current Q and go to the next set S, given that isQuery is set to true for some diagnosis. However, as the q-partition provides meaningful information to assess a query, e.g. it gives the number of diagnoses invalidated for each answer or the estimated probability of each answer (cf. Chapter 7), the q-partition is a necessary input to the subsequently called function SELECTBESTQUERY (line 48 in Algorithm 6, see later in Sections 9.2.4 and 9.3) that selects a query from the pool of queries QP. For this reason, the algorithm continues until the computation of the q-partition for Q is complete.\nIn a last step (lines 18-20), given that isQuery is true and there is not yet a query with the same q-partition in QP, the algorithm computes a set-minimal subset Qmin of Q such that the q-partition of Qmin is the same as the one of Q (function MINQ). Finally, the tuple \u2329 Qmin, \u2329 D+,D\u2212,D0 \u232a\u232a including\nthe minimized query Qmin along with its q-partition \u2329 D+,D\u2212,D0 \u232a is added to QP. If |QP| = q, then QP is returned; otherwise, a further iteration for another S is executed. If |QP| = q is not met until all seeds S have been processed, the set QP is checked for emptiness in line 23. If QP = \u2205, then the function ADDTRIVIALQUERIES (line 24) adds |D| \u2265 2 queries as defined byQ in Proposition 7.5 to QP (cf. Corollary 7.4) and then returns QP; otherwise, QP is directly returned.\nRemark 8.1 Notice that lines 23 and 24 in Algorithm 4 aim at ensuring the non-emptiness of the pool of queries QP returned by GETPOOLOFQUERIES for any GETENTAILMENTS function (see Example 8.6 for different specifications of the GETENTAILMENTS function). This is a necessary criterion for the interactive KB debugging system (Algorithm 5) to work in a sound way since it guarantees that the CALCQUERY function (line 16 in Algorithm 5) always returns a query w.r.t. the current set of leading diagnoses D and the given DPI. Note that the |D| queries generated and added to QP by ADDTRIVIALQUERIES can be trivially obtained without the consultation of a reasoning service by extraction of the respective formulas from the KB K, as prescribed by Proposition 7.5."}, {"heading": "8.2 Discussion of Query Pool Generation", "text": "Multiple Equal Q-Partitions. In the general case there is more than one query w.r.t. one and the same q-partition. For that reason alone that a minimized query is a set-minimal subset of an initially computed one where multiple such subsets may exist.\nExample 8.2 An example for such a query resulting in multiple minimized subqueries with identical q-partition can be found in Example 8.1.\nHowever, note that GETPOOLOFQUERIES is designed to compute a pool QP that includes at most one query with one and the same q-partition. The idea behind this is (1) to minimize the calls to the\n106 CHAPTER 8. QUERY GENERATION\nexpensive function MINQ and (2) that two queries with the same q-partition have exactly the same properties w.r.t. common query selection criteria such as maximum expected information gain or maximum worst case invalidation rate of diagnoses after the query answer is known. Such criteria have been shown to often lead to a reduction of debugging effort for the interacting user (cf. [SFFR12, RSFF13]). As the purpose of the computation of the pool of queries QP is to constitute an input to the query selection function that uses exactly such selection measures, the inclusion of only one query with a particular qpartition is reasonable, also (3) to minimize computation time of the query selection function which needs to go through all elements of QP in order to pick the \u201cbest\u201d one in the worst case.\nOn the other hand, regarding the comprehensibility of the query, i.e. the cognitive load on the user when it comes to understanding the meaning of the query, two queries with the same q-partition may well be significantly different. This however is beyond the scope of this work and considered a topic for future research.\nThe following proposition gives evidence that the set QP returned by GETPOOLOFQUERIES is indeed duplicate-free w.r.t. the q-partitions in QP.\nProposition 8.2. Let a DPI \u3008K,B,P ,N \u3009R, a set of diagnoses D \u2286 mD\u3008K,B,P,N \u3009R and q \u2208 N \u222a {\u221e} , q \u2265 1 be the input to the function GETPOOLOFQUERIES. Then, the function GETPOOLOFQUERIES returns a set QP including tuples of the form \u3008Q,P(Q)\u3009 where Q \u2208 QD,\u3008K,B,P,N \u3009R is a query and P(Q) = \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a is the q-partition of Q such that QP does not include any two equal queries and does not include any two equal q-partitions.\nProof. The test of the criterion \u00acINCLQPART tested before the call to MINQ will always return false for the q-partition \u2329 D+,D\u2212,D0 \u232a if \u2329 D+,D\u2212,D0 \u232a is already included in a tuple in QP. Since MINQ is q-partition-preserving, no q-partition that does not occur in a tuple in QP can become equal to some q-partition in QP by a call to MINQ. Therefore, QP cannot include any two equal q-partitions. Since two equal queries have equal q-partitions, any two different q-partitions cannot be q-partitions of equal queries. Thus, QP cannot include any two equal queries either.\nNote that, on account of the q-partition preserving property of MINQ, only such q-partitions are ruled out by the criterion in line 18 that would lead to duplicates at the time they should be added to QP in line 20.\nComputation of Entailments. Generally, the (theoretical) number of entailments of a set of formulas is not finite. However, the entailments (of a certain type) returned by a reasoner are finite. For instance, asked for entailments of {A v B u C}, a reasoner performing the classification reasoning service would give back A v B and A v C, but not entailments like A v B t C or A v C u C u C. That is, when we speak of entailments, then we mean entailments in the practical sense (cf. Remark 2.3), i.e. w.r.t. a reasoning service such as classification for DL KBs which computes all and only subsumptions X v Y such that Y is the most specific concept that subsumes X , or forward-chaining for Datalog KBs which computes all and only atoms that are entailed by the KB.\nExample 8.3 If we recall Example 8.1, we see that the number of computed entailments of K\u22174 and K\u22173 was 19 and 13 respectively, which are rather high numbers in the light of the small KBs, but importantly these numbers are necessarily finite. For, there cannot be more than |Pred|2 entailments of the \u2200Xp1(X)\u2192 p2(X) type and not more than |Pred| |Const| entailments of the p(a) type for a KB whose signature includes the unary predicate symbols Pred and constant symbols Const and does not include any function symbols. In case of KB K\u22173 , for example, the set Pred = {a1, a2,m1,m2,m3, a, b} and Const = {u,w} which means that upper bounds for the number of entailments of the first and second type are 49 and 14, respectively.\n8.2. DISCUSSION OF QUERY POOL GENERATION 107\nFurther, note that the number of existing different q-partitions and which q-partitions there are at all w.r.t. some set of leading diagnoses D and a DPI depends on the function GETENTAILMENTS, i.e. on the set of entailments calculated by it.\nExample 8.4 Recall Example 8.1 where we constructed a queryQw.r.t. the set of all minimal diagnoses for the DPI given by Table 15.2. Assume now that only entailments of the first type, i.e. those of the form \u2200Xp1(X) \u2192 p2(X), and none of the second type p(a) are computed by GETENTAILMENTS and denote the set of entailments of this form of K\u2217i by E\u2032Di . Then, Q \u2032 = E\u2032D3 \u222a E \u2032 D4 = {e1, . . . , e5} (cf. Table 8.2), i.e. a subset of the queryQ computed for a GETENTAILMENTS function producing entailments of both types. The q-partition ofQ\u2032 is the same as the q-partition ofQ, namely \u3008{D3,D4} , {D1,D2} , \u2205\u3009. However, the queries Qmin,1 and Qmin,2 are no longer obtained as minimized versions of Q\u2032, unlike Qmin,3 and Qmin,4 which are subqueries of Q\u2032, too.\nMinimizing the Set D0 in Q-Partitions. Recall that D0 = \u2205 is a desirable property of a q-partition since a query with such q-partition may invalidate any leading diagnosis, depending on the answer to the query (cf. Chapter 7). In other words, no leading diagnosis is guaranteed to be still valid for any answer after the query is added as a test case to the DPI.\nIn general, GETPOOLOFQUERIES computes q-partitions where D0 may be a non-empty set. However, if the GETENTAILMENTS function is specified to compute certain explicit entailments of K, then D0 = \u2205 can be guaranteed.\nDefinition 8.1 (Explicit Entailment). Let K be a KB. Then, \u03b1 is an explicit entailment of K iff \u03b1 \u2208 K.\nNow, if each set of entailments ED computed by GETENTAILMENTS includes all the formulas that occur in some diagnosis in D, but do not occur in D, then GETPOOLOFQUERIES definitely returns a set QP of queries and associated q-partitions where D0(Q) = \u2205 holds for each tuple in QP.\nProposition 8.3. Let \u3008K,B,P ,N \u3009R be a DPI and D \u2286 mD\u3008K,B,P,N \u3009R . If the set ED computed by GETENTAILMENTS meets ED \u2287 UD \\ D for all D \u2208 D, then GETPOOLOFQUERIES computes only queries Q with D0(Q) = \u2205.\nProof. Assume that Q is some query computed by GETPOOLOFQUERIES. As MINQ is a q-partition preserving transformation of Q, we can assume w.l.o.g. that Q is a query computed by GETPOOLOFQUERIES before MINQ is called for Q. We have to show that for an arbitrary diagnosis Di \u2208 D either Di is assigned to D+(Q) or to D\u2212(Q).\nSo, let us assume that there is a diagnosis Dk which is assigned to D0(Q) = D \\ (D+(Q)\u222aD\u2212(Q)) in line 17. Then, Q 6\u2286 EDk and K\u2217k \u222a Q does not violate any x \u2208 R \u222a N must hold, otherwise Dk would have already been assigned to D+(Q) in line 12 or to D\u2212(Q) in line 14. But Q 6\u2286 EDk implies Q 6\u2286 UD \\ Dk since EDk \u2287 UD \\ Dk by precondition. This in turn means that there is some formula ax in Q which is not in UD \\ Dk. Then ax \u2208 Dk must hold, as otherwise for all formulas ax \u2032 \u2208 Q it would hold that ax \u2032 is an entailment ofK\u2217k = (K\\Dk)\u222aB\u222aUP , i.e. an entailment of all formulas inK\u222aB\u222aUP except for those in Dk. However, all entailments of K\u2217k are stored in EDk by the implementation of the function GETENTAILMENTS. Thus Q \u2286 EDk would hold which cannot be the case as shown before. Consequently, we have derived that Q \u2229 Dk 6= \u2205 which means by set-minimality of diagnoses in D, in particular of Dk, that K\u2217k \u222a Q must violate some x \u2208 R \u222a N which is a contradiction to the assumption that Dk \u2208 D0(Q).\nExample 8.5 Let us come back to the example DPI given by Table 15.2. The possibility of a query Q constructed by Algorithm 4 with D0(Q) 6= \u2205 is witnessed by the selection of seed S = {D1} and the assumption that entailments of the two types given in Example 8.1 are produced by GETENTAILMENTS.\n108 CHAPTER 8. QUERY GENERATION\nThe set of entailmentsQ = ED1 = {e4, e14, e15,\u2200Xm2(X)\u2192 d(X)} (for ei cf. Table 8.2). Then,D2 as well asD3 are assigned to D\u2212(Q) as both KBsK\u22173\u222aQ,K\u22174\u222aQ entailm3(w) and \u00acm3(w) wherefore they are both inconsistent and thus violate r1 \u2208 R. However, D4 \u2208 D0(Q) since K\u2217i 6|= \u2200Xm2(X) \u2192 d(X) and hence does not entail Q and since K\u2217i \u222aQ does not violate consistency or coherency (recall that the set of negative test cases is empty in the DPI and thus must not be considered), i.e. does not contain a conflict set.\nApplying Proposition 8.3, we could use a modified GETENTAILMENTS function that returns a minimal set of entailments just that the precondition of the proposition is met, i.e. E\u2032D = UD \\ D for all D \u2208 D. With this function, for the seed S = {D1} we would get Q\u2032 = E\u2032D1 = {2, 3, 4, 5} (again, formulas in Table 15.2 are referred to just by their number). Let us now check whether D0(Q\u2032) is indeed empty. As explicit entailments are stronger than non-explicit ones, we must still have that D2,D3 \u2208 D\u2212(Q\u2032). For D4, we have K\u22174 \u222a Q\u2032 = {1, 3, 5, 6, 7, 8} \u222a {2, 3, 4, 5} = {1, 2, 3, 4, 5, 6, 7, 8} which corresponds to the entire KB plus background knowledge of the given DPI and includes conflict sets C1 = {1, 3, 4} and C2 = {1, 2, 3, 5} wherefore it is inconsistent. Therefore, diagnosis D4 must also be an element of D\u2212(Q\u2032).\nPlease note that making the entailments Q = ED1 computed by the unmodified GETENTAILMENTS function only slightly stronger would already suffice to force inclusion ofD4 in D0(Q). In fact, including ax 4 := \u2200Xm2(X) \u2192 (\u2200Y s(X,Y ) \u2192 a(Y )) \u2227 d(X) in Q instead of \u2200Xm2(X) \u2192 d(X) would make Q non-disjoint with D4 as both comprise ax 4. Consequently, in line with the proof of Proposition 8.3, K\u22174 \u222aQ must include a conflict set ({1, 3, 4}) wherefore D4 \u2208 D\u2212(Q).\nAnother point we want to mention is that empty D0 could also be achieved by making the query slightly weaker. For our concrete query Q = ED1 , this means that leaving out \u2200Xm2(X) \u2192 d(X) would lead to empty D0(Q). However, the difference to the scenario above where we made Q sightly stronger is that D4 would be an element of D+(Q) instead of D\u2212(Q) in this case, i.e. the q-partition would be \u3008{D1,D4} , {D2,D3} , \u2205\u3009.\nA shortcoming of the strategy of making the query weaker is that it can be computationally expensive as perhaps a large number of subsets of Q might need to be considered and tested for fulfillment of D0(Q) = \u2205. Each such test would involve calls to the reasoner which are usually expensive. A second drawback is that no guarantee is given to finally end up with an empty set D0(Q) since weakening of Q might also involve the \u201cshift\u201d of some diagnosis from D\u2212(Q) to D0(Q). On the other hand, the strategy of computing stronger entailments is computationally more resource-saving as (trivially obtained) explicit entailments can be added to make the query stronger. Furthermore, making the query stronger \u2013 in a controlled way, by adding formulas from UD \\UD+(Q) to Q as suggested by Proposition 8.3 \u2013 can never lead to non-empty D0(Q) as Proposition 8.3 substantiates.\n(Non-)Completeness of Query Pool QP. Note that specifying q :=\u221e causes GETPOOLOFQUERIES to run through all S \u2282 D and to compute a maximum number of queries. However, in general, not all theoretically possible queries are computed by GETPOOLOFQUERIES. One trivial reason for this is that only minimized, i.e. set-minimal, queries are contained in the returned set QP.\nBut, also queries Q\u2032 with D+(Q\u2032) = Y \u2282 D will not be included in QP if there is some query Q with D+(Q) = Y such that |D\u2212(Q)| > |D\u2212(Q\u2032)| (and, equivalently, |D0(Q)| < |D0(Q\u2032)|). As we will learn in a moment, both mentioned reasons for the incompleteness of the output of GETPOOLOFQUERIES will even be desirable for reasons of efficiency. That is, the mentioned types of queries that are not taken into account in QP are \u201cnon-preferred\u201d as non-set-minimal queries demand a non-necessary amount of user interaction and the answering of queries Q with a non-necessarily large set D0(Q) involves a worse discrimination between leading minimal diagnoses (and, if these are \u201cgood\u201d representatives of all minimal diagnoses, then of all minimal diagnoses) than other queries Q\u2032 with |D0(Q\u2032)| < |D0(Q)| and D+(Q) = D+(Q\u2032).\n8.2. DISCUSSION OF QUERY POOL GENERATION 109\nStill, GETPOOLOFQUERIES meets a completeness criterion for a subset of all queries QD,\u3008K,B,P,N \u3009R , elements of which cannot be trivially detected to be \u201cnon-preferred\u201d. That is, GETPOOLOFQUERIES is complete w.r.t. the set D+, as the following proposition states. In other words, for each subset X \u2282 D it detects a q-partition with D+ = X , if one exists.\nProposition 8.4. Let a DPI \u3008K,B,P ,N \u3009R, D \u2286 mD\u3008K,B,P,N \u3009R such that |D| \u2265 2 and some q \u2208 N \u222a {\u221e} , q \u2265 1 be the inputs to GETPOOLOFQUERIES and let |QPmax| \u2265 0 be the maximum number of tuples \u3008Q,P(Q)\u3009 that can be computed by GETPOOLOFQUERIES by means of the used GETENTAILMENTS function. Further, let Y be an arbitrary subset of D. If there is some query Q \u2208 QD,\u3008K,B,P,N \u3009R that (1) includes only entailments that are computed by GETENTAILMENTS and (2) has a q-partition such that D+(Q) = Y , then GETPOOLOFQUERIES with parameter q \u2265 |QPmax| returns a set QP including a query Q\u2032 with D+(Q\u2032) = Y . Moreover, this query Q\u2032 is found in the iteration where the seed S = Y .\nProof. Since q \u2265 |QPmax|, GETPOOLOFQUERIES will arrive at a step where it selects the seed S = Y in line 6. Now, let us assume that in this iteration no query Q with D+(Q) = Y is found. Then, either (a) no query is found at all, i.e. CQ1 or CQ2 or CQ3 are violated, or (b) a query Q with D+(Q) 6= Y is found.\n(a): Assume first that CQ1 is violated, i.e. GETCOMMONENTAILMENTS called with argument S returns \u2205. This implies that the KBs K\u2217r for Dr \u2208 Y have no common entailments, if entailments are computed by GETENTAILMENTS. This however means that there cannot be a q-partition with D+ \u2287 Y which is a contradiction to the precondition that there is some query Q \u2208 QD,\u3008K,B,P,N \u3009R that includes only entailments computed by GETENTAILMENTS and has a q-partition such that D+(Q) = Y .\nSecond, assume that CQ2 is violated, i.e. D+(Q) = \u2205. If GETCOMMONENTAILMENTS with argument S returned Q 6= \u2205, then D+(Q) \u2287 S \u2283 \u2205 would hold. Thus, Q = \u2205, i.e. CQ1 is violated. So, as shown before, this leads to a contradiction.\nIn case any of CQ1 or CQ2 is violated, we already derived a contradiction. So, we make the assumption that CQ1 and CQ2 are met. So, finally, let us assume that CQ3 is violated, i.e. that D\u2212(Q) = \u2205. That is, if Q (which must be a non-empty set by CQ1) denotes all common entailments (computable with GETENTAILMENTS) ofK\u2217r forDr \u2208 Y , thenK\u2217i \u222aQ does not violate any x \u2208 R\u222aN for anyDi \u2208 D\\S. Consequently, for all diagnoses Di in D we have that K\u2217i \u222a Q does not violate any x \u2208 R \u222a N . But, as there is, by precondition, a query with D+ = Y , this query must be a subset of all possible common entailments (computable with GETENTAILMENTS) of KBs K\u2217i for diagnoses in Y , i.e. this query must be a subset of Q. But, by monotonicity of L, no K\u2217i \u222aQ\u2032 for a subset Q\u2032 of Q can violate x \u2208 R \u222a N if Q does not. Again, we have a contradiction to the precondition as above.\n(b): Here, a query Q is found with D+(Q) 6= Y and D\u2212(Q) 6= \u2205. Since Q is a query, Q 6= \u2205 must hold. Since the seed S = Y , this means that Q is the set of all common entailments (computable with GETENTAILMENTS) of K\u2217i for Di \u2208 Y , i.e. D+(Q) \u2287 Y . By D+(Q) 6= Y , we conclude that D+(Q) \u2283 Y must be true. The only way of achieving a smaller set D+(Q), namely D+(Q) = Y , is to add some formulas toQ as makingQ smaller can only increase D+(Q). This holds because postulating that, instead of Q, only a subset Q\u2032 of Q must be entailed by K\u2217i , can cause a new KB K\u2217j for diagnosis Dj /\u2208 D+(Q) to entail Q\u2032. However, as Q is the set of all entailments computable with GETENTAILMENTS of KBs K\u2217i for Di \u2208 Y , a superset Q\u2032\u2032 of Q computed by GETENTAILMENTS with D+(Q\u2032\u2032) = Y can never be obtained. Therefore, we have a contradiction to the precondition.\nWe have now proven the following: If there exists a q-partition as described in the proposition, then this q-partition is found in the iteration where the seed S = Y .\nRemark 8.2 Regarding Proposition 8.4, note the following:\n(a) In fact, as one and the same q-partition must occur at most once in QP, GETPOOLOFQUERIES must only keep assigning diagnoses in D \\ S to the respective sets of the q-partition as long as D+ = S. Because for D+ = Z \u2283 S, we know to find a query (if one exists) for the seed S = Z.\n110 CHAPTER 8. QUERY GENERATION\n(b) A statement equivalent to the proposition is: If there is no query (including only entailments computed by the GETENTAILMENTS function) with D+ = Y found for seed S = Y , then such a query and q-partition, respectively, does not exist.\nThe following proposition states that if a q-partition with one and the same set D+ is found twice during the execution of GETPOOLOFQUERIES, then the queries for both q-partitions and thus both qpartitions must be equal. That is, for one set D+, there is at most one tuple in QP.\nProposition 8.5. Let Qi be a query with D+(Qi) = Y in the set QP returned by GETPOOLOFQUERIES and found for seed Si = Y and let Qj be a query with D+(Qj) = Y in the set QP returned by GETPOOLOFQUERIES and found for some seed Sj \u2282 Y . Then Qi = Qj . Proof. Let Q\u2032i, Q \u2032 j be the queries stored in the variable Q in line 18 for seeds Si and Sj , respectively; i.e. the supersets of the queries Qi, Qj before the minimization function MINQ is called for each of them. Q\u2032j \u2286 Q\u2032i holds by the fact that Q\u2032i is the set of all common entailments computable with GETENTAILMENTS of K\u2217r for Dr \u2208 Y and by the fact that Q\u2032j must be a set of common entailments computed by GETENTAILMENTS of exactly these KBs, because of D+(Q\u2032j) = Y and Definition 7.2. Q \u2032 j \u2287 Q\u2032i holds by the fact thatQ\u2032j is computed as intersection of EDr whereDr \u2208 Sj andQ\u2032i is computed as intersection of EDs where Ds \u2208 Si \u2283 Sj . Thus, we can conclude that Q\u2032i = Q\u2032j .\nAs Q\u2032i = Q \u2032 j , also P(Q \u2032 i) = P(Q \u2032 j) must hold for the q-partitions by Proposition 7.2. That the minimized versions Qi, Qj of Q\u2032i, Q \u2032 j output by MINQ are equal, follows from the determinism of the MINQ function, wherefore equal inputs, i.e. (\u2205, Q\u2032i, \u2205,P(Q\u2032i), \u3008K,B,P ,N \u3009R) = (\u2205, Q\u2032j , \u2205,P(Q\u2032j), \u3008K,B,P , N \u3009R), must yield equal outputs.\nRemark 8.3 Proposition 8.5 hints at a possible improvement of Algorithm 4, namely to check in line 6 whether the seed S already occurs as a set D+ in some tuple in QP and only continue the execution for S if this does not hold (not shown in Algorithm 4). In this vein, time and reasoning costs (line 14) can be saved.\nAnother improvement regarding line 6 is to delete all remaining seeds S\u2032 with the property S\u2032 \u2283 S if Q in line 8 is the empty set (not shown in Algorithm 4). Namely, all seeds S\u2032 must also lead to Q = \u2205 since the intersection of ED for D \u2208 S already returned \u2205 wherefore the intersection of ED for D \u2208 S\u2032 must also return \u2205.\nBy now, we know from Proposition 8.5 that, given a query with D+ exists, one and only one qpartition with D+ will be added to QP, but which one?\nW.r.t. one and the same set D+, queries with a set D\u2212 with higher cardinality are preferable over others as the cardinality of D0 should be minimized (cf. Chapter 7). So, preferable queries among those with equal set D+ are those for which D\u2212 is a set-maximal set. Exactly such a query is added to QP for each D+ for which a query exists, as the following proposition shows.\nProposition 8.6. If the set QP returned by GETPOOLOFQUERIES comprises a query Q with D+(Q) = Y , then Q is a query with minimal |D0(Q)| among all queries Q\u2032 with D+(Q\u2032) = Y computable with the function GETENTAILMENTS.\nProof. Assume that GETPOOLOFQUERIES finds a query Q with D+(Q) = Y and |D0(Q)| = k and assume there is a query Q\u2032 (consisting only of entailments computed by function GETENTAILMENTS) with D+(Q\u2032) = Y and with |D0(Q\u2032)| < k. This means that |D\u2212(Q)| < |D\u2212(Q\u2032)|. However, as Q is computed for seed S = Y , Q is a maximal set of entailments computable with GETENTAILMENTS of K\u2217i for Di \u2208 Y . Because Q\u2032 is also a common entailment of K\u2217i for Di \u2208 Y , we have that Q\u2032 \u2286 Q must be true. Since the fact that K\u2217i \u222a Q does not violate any x \u2208 R \u222a N , i.e. the fact that Di /\u2208 D\u2212(Q), implies by monotonicity of L that K\u2217i \u222aQ\u2032 for the subset Q\u2032 of Q cannot violate any x \u2208 R \u222a N either, i.e. Di /\u2208 D\u2212(Q\u2032), we conclude that |D\u2212(Q\u2032)| \u2264 |D\u2212(Q)| must hold. This is a contradiction.\n8.3. QUERY MINIMIZATION 111"}, {"heading": "8.3 Minimization of Queries", "text": "MINQ. The minimization of the query Q by MINQ (see Algorithm 4) while preserving the q-partition aims at simplifying the job of the answering user who only needs to go through a smaller set of logical formulas Qmin in order to come up with an answer to the query. Since the q-partition reflects the properties of a query w.r.t. the invalidation of (leading) diagnoses and two queries have equal such properties, then of course the one that is a subset of the other should be asked.\nThe concept of the function MINQ is similar to the one of QX (Algorithm 1). Like QX, MINQ carries out a divide-and-conquer strategy to find a set-minimal set with a monotonic property. In this case, the monotonic property is not the invalidity of a subset of the KB w.r.t. a DPI (as per Definition 3.3) as it is for the computation of minimal conflict sets using QX, but the property of some Qmin \u2282 Q having the same q-partition as Q. So, the crucial difference between QX and MINQ is the function that checks this monotonic property. For MINQ, this function \u2013 that checks a subset of a query for constant q-partition \u2013 is ISQPARTCONST.\nMINQ \u2013 Input Parameters. MINQ gets five parameters as input. The first three, namelyX,Q andQB, are relevant for the divide-and-conquer execution, whereas the last two, namely the original q-partition\u2329 D+,D\u2212,D0 \u232a of the query (i.e. the parameter Q) that should be minimized, and the DPI \u3008K,B,P ,N \u3009R are both needed as an input to the function ISQPARTCONST. Besides the latter two, another argument QB is passed to this function where QB is a subset of the original query Q. ISQPARTCONST then checks whether the q-partition for the (potential) query QB is equal to the q-partition \u2329 D+,D\u2212,D0\n\u232a of the original query given as argument. The DPI is required as the parameters K,B,P ,N and R are necessary for these checks.\nMINQ \u2013 Testing Sub-Queries for Constant Q-Partition. In particular, ISQPARTCONST tests for each Dr \u2208 D\u2212 whether K\u2217r \u222aQB is valid (w.r.t. \u3008\u00b7, \u2205, \u2205, N\u3009R). If so, this means that Dr /\u2208 D\u2212(QB) and thus that the q-partition of QB is different to the one of Q wherefore false is immediately returned. If true for all Dr \u2208 D\u2212, it is tested for Dr \u2208 D0 whether K\u2217r |= QB. If so, this means that Dr /\u2208 D0(QB) and thus that the q-partition of QB is different to the one of Q wherefore false is immediately returned. If false is not returned for any Dr \u2208 D\u2212 or Dr \u2208 D0, then the conclusion is that QB is a query w.r.t. to D and \u3008K,B,P ,N \u3009R and has the same q-partition as Q wherefore the function returns true .\nNote that, instead of calling a reasoner to answer whether K\u2217r |= QB, the set of precalculated entailments EDr of K\u2217r for each Dr \u2208 D can be given as an argument to MINQ as well as to ISQPARTCONST (not shown in Algorithm 4). In this case an equivalent test is QB \u2286 EDr . Such a strategy is particularly appropriate if reasoning is expensive for the DPI at hand.\nSoundness of ISQPARTCONST is proven by the following lemma.\nLemma 8.1. Let \u3008K,B,P ,N \u3009R be a DPI, D \u2286 mD\u3008K,B,P,N \u3009R , Q \u2208 QD,\u3008K,B,P,N \u3009R with q-partition P(Q) = \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a . Then a non-empty set QB \u2282 Q is a query in QD,\u3008K,B,P,N \u3009R with P(QB) = P(Q) if\n1. \u2200Dr \u2208 D\u2212(Q) : K\u2217r \u222aQB violates some r \u2208 R or entails some n \u2208 N and\n2. \u2200Dr \u2208 D0(Q) : K\u2217r 6|= QB.\nProof. Let Q \u2208 QD,\u3008K,B,P,N \u3009R and QB be an arbitrary proper subset of Q. If criterion 1) of this lemma is met, then we know that each diagnosis in D\u2212(Q) is in D\u2212(QB) as well, i.e. (I): D\u2212(QB) \u2287 D\u2212(Q) holds.\nAssume a minimal diagnosis Dr \u2208 D0(Q). Then, K\u2217r \u222a Q does not violate any r \u2208 R and does not entail any n \u2208 N andK\u2217r does not entailQ. This however implies thatK\u2217r \u222aQB cannot violate any r \u2208 R\n112 CHAPTER 8. QUERY GENERATION\nand cannot entail any n \u2208 N either by monotonicity of L. But it is possible that K\u2217r |= QB. So, validity of criterion 2) of this lemma is sufficient to guarantee that each diagnosis in D0(Q) is in D0(QB) as well, i.e. (II): D0(QB) \u2287 D0(Q) holds.\nAs all diagnoses in D+(Q) entail all formulas in Q by Definition 7.2, all diagnoses in D+(Q) must entailQB as well. Consequently, due to deletion of some formulas fromQ, noDr \u2208 D+(Q) can \u201cmove\u201d to any set D\u2212(QB) or D0(QB). That is, (III): D+(QB) \u2287 D+(Q) must hold.\nSo, the overall conclusion is that, if criterion 1) and 2) are met, then (I), (II) and (III) hold. Assume that some \u2287-relation in i \u2208 {(I), (II), (III)} is a \u2283-relation. This leads to a violation of some j \u2208 {(I), (II), (III)} with j 6= i since \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a and \u2329 D+(QB),D\u2212(QB),D0(QB) \u232a are partitions of D. Therefore, all \u2287-relations must be =-relations and we can derive that P(Q) = P(QB). Moreover, we have that QB must be a query. This is due to the facts that QB is non-empty, Q is a query and the q-partitions of Q and QB are equal. Therefore, D+(QB) = D+(Q) \u2265 1 and D\u2212(QB) = D\u2212(Q) \u2265 1 which lets us conclude by Proposition 7.4 that QB is a query.\nMINQ \u2013 The Divide-and-Conquer Strategy. Intuitively, MINQ partitions the given query Q in two parts Q1 and Q2 and first analyzes Q2 while Q1 is part of QB (line 34). Note that in each iteration QB is the subset of Q that is currently assumed to be part of the sought minimized query (i.e. the one query that will finally be output by MINQ). In other words, analysis of Q2 while Q1 is part of QB means that all irrelevant formulas in Q2 should be located and removed from Q2 resulting in Qmin2 \u2286 Q2. That is, Qmin2 must include only relevant formulas which means that Q min 2 along with QB is a query with an equal q-partition as Q, but the deletion of any further formula from Qmin2 changes the q-partition. After the relevant subset Qmin2 of Q2, i.e. the subset that is part of the minimized query, has been returned, Q1 is removed from QB, Qmin2 is added to QB and Q1 is analyzed for a relevant subset that is part of the minimized query (line 35). This relevant subset, Qmin1 , together with Q min 2 , then builds a set-minimal subset of the input Q that is a query and has a q-partition equal to that of Q. Note that the argument X of MINQ is the subset of Q that has most recently been added to QB.\nFor each call in line 34 or line 35, the input Q to MINQ is recursively analyzed until a trivial case arises, i.e. (a) until Q is identified to be irrelevant for the computed minimized query wherefore \u2205 is returned (lines 27 and 28) or (b) until |Q| = 1 and Q is not irrelevant for the computed minimized query wherefore Q is returned (lines 29 and 30).\nExample 8.6 Let us reconsider the FOL DPI depicted by Table 15.2 on page 270. We recall that sets of minimal conflict sets and minimal diagnoses w.r.t. this DPI were given by mC\u3008K,B,P,N \u3009R = {C1, C2} = {\u30081, 3, 4\u3009 , \u30081, 2, 3, 5\u3009} as well as mD\u3008K,B,P,N \u3009R = {D1,D2,D3,D4} = {[1], [3], [4, 5], [2, 4]}. For this DPI, a set of minimized queries computed by GETPOOLOFQUERIES is presented by Table 8.3. Note that these queries have been produced by different GETENTAILMENTS functions (as indicated by the dashed lines in Table 8.3). That is, Qi for i \u2208 {1, . . . , 5} have been produced by the same GETENTAILMENTS function that is described in Example 8.1. For i \u2208 {6, . . . , 9}, Qi has been computed from a GETENTAILMENTS function that outputs only explicit entailments (cf. Definition 8.1) and Q10 from a GETENTAILMENTS function that returns a finite set of entailments where each entailment is some FOL formula. This could be accomplished, for example, by some resolution-based reasoning procedure [CL73].\nIt is important to realize that the results regarding Algorithm 4 established so far, most of which depend on the particular used GETENTAILMENTS function, must only hold within one part of Table 8.3 (where different parts are separated by the dashed lines). For example, for Q2 and Q9 it holds that D+(Q2) = D\n+(Q9), but D\u2212(Q2) 6= D\u2212(Q9) and D0(Q2) 6= D0(Q9). By application of one and the same GETENTAILMENTS function, this case would be prohibited by Proposition 8.5. Furthermore, by Proposition 8.6, only Q9 would be an element of the query pool QP in this case since D0(Q9) \u2282 D0(Q2).\nMoreover, we want to remark that Q7, Q8 and Q9 can be seen as a proof that Q6 is indeed set-\n8.4. SOUNDNESS OF QUERY MINIMIZATION 113\nminimal. Each Qi, i \u2208 {7, 8, 9} is a result of the removal of a single formula from Q6. And, each such Qi features a q-partition different from the one of Q6. This illustrates quite well the principle of MINQ which performs tests of exactly this kind to verify minimality of a query or detect formulas that might be deleted from it under preservation of the q-partition, respectively.\nAnother essential note is that it is guaranteed that D0(Q6) = \u2205. This holds due to the construction of Q6 as UD \\ D4 = {1, 2, 3, 4, 5} \\ [2, 4] = {1, 3, 5} (recall that we use squared brackets to denote diagnoses in spite of the fact that these are sets, cf. Table 2.1). So, Q6 comprises all formulas occurring in minimal diagnoses except for the ones contained in D4. We have that for any two different minimal diagnoses Di,Dj w.r.t. one and the same DPI it must be true that Di \\ Dj 6= \u2205 as well as Dj \\ Di 6= \u2205 as otherwise one would be necessarily a subset of the other. From this, we can easily derive that K\u2217i \u222a Q6 for i \u2208 {1, . . . , 3}, i.e. for all minimal diagnoses Di w.r.t. this DPI other than D4 which was used to build the query Q6, must comprise a conflict set. This must be valid by the minimality of Di and since by Q6 at least one formula of Di is readded to the KB. Note that a similar argumentation was used in the proof of Proposition 8.3."}, {"heading": "8.4 Soundness of Query Minimization", "text": "The following lemma shows that the function ISQPARTCONST used by MINQ is indeed a monotonic function (cf. Definition 4.6), which is a necessary prerequisite for versions of the QX algorithm to work in a sound way.\nLemma 8.2. Let \u3008K,B,P ,N \u3009R be a DPI, D \u2286 mD\u3008K,B,P,N \u3009R , Q \u2208 QD,\u3008K,B,P,N \u3009R with q-partition P(Q). Further, let f : 2Q \u2192 {0, 1} be a function that maps a subset QB of Q to 1 if QB has q-partition P(QB) = P(Q), to 0 otherwise. Then, f is a monotonic function (as per Definition 4.6).\nProof. Assume a subset Q\u2032 of Q with f(Q\u2032) = 1, i.e. Q\u2032 has q-partition P(Q\u2032) = P(Q). Let Q\u2032 \u2282 Q\u2032\u2032 \u2286 Q and assume that f(Q\u2032\u2032) = 0, i.e. Q\u2032\u2032 has a q-partition P(Q\u2032\u2032) 6= P(Q).\nAs shown in the proof of Lemma 8.1, D+(X1) \u2287 D+(X2) holds for any X1 \u2286 X2. Therefore, we have D+(Q\u2032) \u2287 D+(Q\u2032\u2032) \u2287 D+(Q) and by P(Q\u2032) = P(Q) that D+(Q\u2032) = D+(Q) and thus that all \u2287-relations are =-relations. So, either D\u2212(Q\u2032\u2032) 6= D\u2212(Q) or D0(Q\u2032\u2032) 6= D0(Q) must hold.\nFirst, assume that D\u2212(Q\u2032\u2032) 6= D\u2212(Q). Then, as K\u2217r \u222a Q\u2032\u2032 \u2282 K\u2217r \u222a Q and by monotonicity of L, it can only be the case that for some Dr \u2208 D some x \u2208 R \u222a N that is violated for K\u2217r \u222a Q is not\n114 CHAPTER 8. QUERY GENERATION\nviolated for K\u2217r \u222aQ\u2032\u2032. Hence, D\u2212(Q\u2032\u2032) \u2282 D\u2212(Q) must hold. By a similar argumentation \u2013 without the assumption that D\u2212(Q\u2032) 6= D\u2212(Q\u2032\u2032) holds \u2013 we have that D\u2212(Q\u2032) \u2286 D\u2212(Q\u2032\u2032) and thus, altogether, that D\u2212(Q\u2032) \u2282 D\u2212(Q) must be true. Due to P(Q\u2032) = P(Q) we know that D\u2212(Q\u2032) = D\u2212(Q) which is a contradiction.\nFinally, assume that D0(Q\u2032\u2032) 6= D0(Q). Since K\u2217r \u222a Q does not violate any x \u2208 R \u222a N for Dr \u2208 D0(Q),K\u2217r\u222aQ\u2032\u2032 cannot violate any x \u2208 R\u222aN by monotonicity ofL. As a conclusion, the only possibility for D0(Q\u2032\u2032) 6= D0(Q) is that K\u2217r |= Q\u2032\u2032 for some Dr \u2208 D0(Q), i.e. that Dr \u2208 D+(Q\u2032\u2032) which implies that D0(Q\u2032\u2032) \u2282 D0(Q). By a similar argumentation \u2013 without the assumption that D0(Q\u2032) 6= D0(Q\u2032\u2032) holds \u2013 we have that D0(Q\u2032) \u2286 D0(Q\u2032\u2032) and thus, altogether, that D0(Q\u2032) \u2282 D0(Q) must be true. Due to P(Q\u2032) = P(Q) we know that D0(Q\u2032) = D0(Q) which is a contradiction.\nThis completes the proof for monotonicity of the given function f .\nProposition 8.7 (Correctness of MINQ). Given a query Q \u2208 QD,\u3008K,B,P,N \u3009R as input, MINQ computes a subset Qmin \u2286 Q such that P(Qmin) = P(Q) and there is no Q\u2032 \u2282 Qmin such that P(Q\u2032) = P(Q).\nProof. This proposition is a consequence of the correctness of QX shown by Proposition 4.9, of the correctness of function ISQPARTCONST established by Lemma 8.1 and of the monotonicity of the property tested by the function ISQPARTCONST guaranteed by Lemma 8.2."}, {"heading": "8.5 Complexity of Query Pool Generation", "text": "The complexity of query minimization, i.e. one call to MINQ, in terms of calls to the ISQPARTCONST function is directly obtained from the complexity results for the standard QX algorithm given by Proposition 4.8.\nProposition 8.8 (Complexity of MINQ). Let \u3008K,B,P ,N \u3009R be a DPI, D \u2286 mD\u3008K,B,P,N \u3009R , Q \u2208 QD,\u3008K,B,P,N \u3009R with P(Q) = \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a and the function SPLIT (line 31 of Algorithm 4) be defined as SPLIT(n) = bn2 c where n is a natural number. Then, the worst case number of calls to ISQPARTCONST during one call to MINQ(\u2205, Q, \u2205,P(Q), \u3008K,B,P ,N \u3009R) is in\nO ( |Qmin| log\n|Q| |Qmin| ) where Qmin is the output of MINQ(\u2205, Q, \u2205,P(Q), \u3008K,B,P ,N \u3009R).\nFor any other definition of the function SPLIT, the worst case number of calls to ISQPARTCONST gets larger.\nThe overall complexity of GETPOOLOFQUERIES in terms of calls to functions that call the reasoner, i.e. functions GETENTAILMENTS, ISKBVALID and ISQPARTCONST, is established by the following proposition.\nProposition 8.9 (Complexity of GETPOOLOFQUERIES). Let \u3008K,B,P ,N \u3009R be a DPI, q a natural number and D \u2286 mD\u3008K,B,P,N \u3009R . Then, the worst case number of calls to functions that call a reasoner during one call to GETPOOLOFQUERIES(\u3008K,B,P ,N \u3009R,D, q) is in\nO |D|+ \u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 log \u2223\u2223Q(max)\u2223\u2223\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223  2|D| \nwhere \u2223\u2223Q(max)\u2223\u2223 is the maximum size of a query before minimization, i.e. the size of the set of maximum\ncardinality that is stored in variable Q in line 19 throughout all iterations, and \u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 is the maximum\n8.6. SHORTCOMINGS OF QUERY POOL GENERATION 115\nsize of a minimized query, i.e. the size of the set of maximum cardinality that is stored in variable Q\u2032 in line 19 throughout all iterations.\nProof. During the execution of the for-loop over lines 3-5 the function GETENTAILMENTS is called |D| times. During the execution of the for-loop over lines 6-22 which may be executed at most 2|D|\u22122 times, ISKBVALID is called at most |D| \u2212 1 times since |S| \u2265 1 and S \u2282 D and thus |D \\ S| \u2264 |D| \u2212 1 holds; furthermore, MINQ may be called once, namely if the condition tested by the if-statement in line 18 is true. During one execution of MINQ, by Proposition 8.8, at most\n|Qmin| log |Q| |Qmin|\ncalls to ISQPARTCONST are made where Qmin is the output of the call to MINQ. So, an upper bound of the number of calls to ISQPARTCONST performed by one call to MINQ among all calls to MINQ throughout the execution of GETPOOLOFQUERIES, is\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 log\n\u2223\u2223Q(max)\u2223\u2223\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 where\n\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 is the set of maximum cardinality that is stored in variable Q\u2032 in line 19 throughout all iterations and\n\u2223\u2223Q(max)\u2223\u2223 is the set of maximum cardinality that is stored in variableQ in line 19 throughout all iterations.\nSo, all in all we know that functions that call a reasoner are invoked at most\n|D|+ |D| \u2212 1 + \u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 log \u2223\u2223Q(max)\u2223\u2223\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223  (2|D| \u2212 2) times during the execution of GETPOOLOFQUERIES. Since|D|+ \u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 log \u2223\u2223Q(max)\u2223\u2223\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223  2|D|\nis an upper bound of this number, the proposition holds.\nNote that none of the parameters that affect the complexity of the function GETPOOLOFQUERIES grows with the size of the DPI provided as an input to the interactive KB debugging problem. Merely the costs for reasoning, where a black-box debugging approach has no influence on, are affected by a higher complexity or larger size of the input DPI. Moreover, the size of the most relevant parameter influencing the worst case complexity, namely the exponent |D|, can be specified by the user to any value greater or equal to 2. In other words, minus reasoning time, the generation of a pool of queries is a fixed parameter tractable problem [DF95] in the context of interactive KB debugging."}, {"heading": "8.6 Shortcomings of Query Pool Generation", "text": "First, the exponential time complexity regarding the parameter |D| is a problem arising from the paradigm of computing an optimal query w.r.t. a certain quantitative measure qsm() such as information gain [SFFR12, RSFF13] by calculating a (generally exponentially large) pool QP of queries in a first stage, whereupon\n116 CHAPTER 8. QUERY GENERATION\nqsm(Q) \u2208 R is evaluated for Q \u2208 QP until the one Q\u2217 with optimal qsm(Q\u2217) is found and selected as the query to be asked to the user.\nA key to solving this issue is the use of a different paradigm that does not rely on the computation of the pool QP. Instead, qualitative measures can be derived from quantitative measures that have been used in interactive debugging scenarios [SFFR12, RSFF13, SF10]. These qualitative measures provide a way to estimate the qsm() value of partial q-partitions, i.e. ones where not all leading diagnoses have been assigned to the respective set in the q-partition yet. That way a direct search for a query with (nearly) optimal properties is possible. A similar strategy called CKK has been employed in [SFFR12] for the information gain measure (see Section 9.3). From such a technique we can expect to save a high number of reasoner calls. Because only a usually small subset of q-partitions included in the pool computed by GETPOOLOFQUERIES is required to find a query with desirable properties if the search is implemented by means of a heuristic that involves the exploration of seemingly favorable (potential) queries and (partial) q-partitions, respectively, first. This is a topic of future work.\nAnother shortcoming of GETPOOLOFQUERIES is the extensive use of reasoning services which may be computationally expensive (depending on the given DPI). Instead of computing a set of common entailments Q of a set of KBs K\u2217i first and consulting a reasoner to fill up the (q-)partition for Q in order to test whether Q is a query at all, the idea enabling a significant reduction of reasoner dependence is to compute some kind of canonical query without a reasoner and use simple set comparisons to decide whether the associated partition is a q-partition. Guided by qualitative properties mentioned before, a search for such q-partition with desirable properties can be accomplished without reasoning at all. Also, a set-minimal version of the optimal canonical query can be computed without reasoning aid. Only for the optional enrichment of the identified optimal canonical query by additional entailments and for the subsequent minimization of the enriched query, the reasoner may be employed. This is also a topic of future work.\nAnother aspect that can be improved is that only one minimized version of each query is computed by Algorithm 4. That is, per q-partition P, there might be some set-minimal queries which do not occur in the output set QP. From the point of view of how well a query might be understood by an interacting user, of course not all minimized queries can be assumed equally good in general. Hence, in order to avoid a situation where a potentially best-understood query w.r.t. P is not included in QP, the query minimization process (see Section 8.3) might be adapted to take into account some information about faults the interacting user is prone to. This could be exploited to estimate how well this user might be able to understand and answer a query. For instance, given that the user frequently has problems to apply \u2203 in a correct manner to express what they intend to express, but has never made any mistakes in formulating implications \u2192, then the query Q1 = {\u2200X p(X)\u2192 q(X), r(a)} might be better comprehended than Q2 = {\u2200X\u2203Y s(X,Y )}. One way to achieve the finding of a well-understood query for some q-partition P is to run the query minimization MINQ more than once, each time with a modified input (using a hitting set tree to accomplish this in a systematic manner \u2013 cf. Chapter 4, where an analogue idea is used to compute different minimal conflict sets w.r.t. a DPI). In this way, different set-minimal queries for P can be identified and the process can be stopped when a suitable query is found."}, {"heading": "8.7 Correctness of Query Pool Generation", "text": "The following proposition confirms the correctness of Algorithm 4, i.e. of the function GETPOOLOFQUERIES. Roughly, it states that the output of QP of the function is duplicate-free, i.e. no query or q-partition occurs twice in QP, that QP includes only queries and q-partitions, that tuples in QP are unique w.r.t. the set D+ of a q-partition and that, given q > |QP|, there is no subset Y of D for which a q-partition with D+ = Y exists and for which no q-partition with D+ = Y is an element of QP.\n8.7. CORRECTNESS OF QUERY POOL GENERATION 117\nProposition 8.10. Let a DPI \u3008K,B,P ,N \u3009R, D \u2286 mD\u3008K,B,P,N \u3009R such that |D| \u2265 2 and some q \u2208 N \u222a {\u221e} , q \u2265 1 be the inputs to GETPOOLOFQUERIES and let |QPmax| \u2265 0 be the maximum number of tuples \u3008Q,P(Q)\u3009 that can be computed by GETPOOLOFQUERIES by means of the used GETENTAILMENTS function. If q \u2265 |QPmax| (in particular q =\u221e), then\n1. there are no two tuples \u3008Q,P(Q)\u3009 , \u3008Q\u2032,P(Q\u2032)\u3009 in QP such that Q = Q\u2032 or P(Q) = P(Q\u2032), and 2. QP includes a tuple \u2329 Q, \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a\u232a only if Q \u2208 QD,\u3008K,B,P,N \u3009R , and\n3. QP includes at most one tuple where D+(Q) = Y for each Y \u2282 D, and\n4. for each Y \u2282 D for which a query Q w.r.t. D and \u3008K,B,P ,N \u3009R exists such that\n(a) Q includes only entailments computed by the used GETENTAILMENTS function and\n(b) P(Q) is such that D+(Q) = Y ,\nQP includes a tuple \u3008Q\u2032,P(Q\u2032)\u3009 such that D+(Q\u2032) = Y , and\n5. QP 6= \u2205.\nIf q < |QPmax|, then QP includes q tuples satisfying (1), (2) and (3).\nProof. Statement (1) is a consequence of Proposition 8.2. Statement (2) is an implication of Proposition 8.1 and Proposition 8.7. The former says that only sets Q that are actually queries w.r.t. D and \u3008K,B,P ,N \u3009R can pass line 18. Thus, only queries are passed to MINQ as parameter Q. By the latter which states that MINQ is correct, i.e. outputs a query if the input is a query, statement (2) follows. Statement (3) follows from Proposition 8.5. If q \u2265 |QPmax|, the truth of statement (4) is witnessed by Proposition 8.4. Statement (5) is true by lines 23 and 24 and by Proposition 7.5 as well as Corollary 7.4 and the premise that |D| \u2265 2 which guarantee that the function ADDTRIVIALQUERIES always adds at least |D| \u2265 2 > 0 queries to QP. In case q < |QPmax|, only statements (1), (2) and (3) are satisfied in general (for the same reasons as given above for the case q \u2265 |QPmax|) and QP is returned in line 22 by the definition of |QPmax|. Thence, the condition |QP| = q \u2265 1 tested in line 21 must be valid for QP.\n118 CHAPTER 8. QUERY GENERATION\nAlgorithm 5 Interactive KB Debugging Input: a tuple \u2329 \u3008K,B,P ,N \u3009R, nmin, nmax, t, pK\u0303\u222aK, q, qsm(), \u03c3,mode \u232a consisting of\n\u2022 an admissible DPI \u3008K,B,P ,N \u3009R, \u2022 leading diagnoses computation parameters, natural numbers nmin \u2265 2, nmax, t, \u2022 a function pK\u0303\u222aK : K\u0303 \u222a K \u2192 (0, 1], \u2022 a parameter q \u2208 N \u222a {\u221e} , q \u2265 1 that determines the size of the computed query pool, \u2022 a function qsm(Q) \u2208 R used for query selection that assigns a real number to a query Q to express the\n\u201cgoodness\u201d of Q,\n\u2022 a maximum fault tolerance \u03c3 \u2208 [0, 1] and \u2022 a mode mode \u2208 {static, dynamic} that determines the used method for diagnosis computation.\nOutput: The output depends on mode and \u03c3:\n\u2022 mode = static: a maximal solution KB w.r.t. the input DPI \u3008K,B,P ,N \u3009R which is \u2013 an approximation of the solution to Interactive Static KB Debugging (Problem Def. 6.2) if \u03c3 > 0. \u2013 the (exact) solution to Interactive Static KB Debugging if \u03c3 = 0.\n\u2022 mode = dynamic: a maximal solution KB w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R which is \u2013 an approximation of the solution to Interactive Dynamic KB Debugging (Problem Def. 6.1) if \u03c3 > 0. \u2013 the (exact) solution to Interactive Dynamic KB Debugging if \u03c3 = 0.\n(for a more formal and precise characterization of the output see Proposition 9.1 on page 124)\n1: P \u2032,N \u2032,Ccalc,DX,D\u00d7,Dout,D\u2283, qData\u2190 \u2205 2: Qdup, QA\u2190 [] 3: Q\u2190 [\u2205] 4: answer \u2190 false 5: pK()\u2190 GETFORMULAPROBS(K, pK\u0303\u222aK()) . application of Formulas 4.2 and 4.7 6: while true do 7: if mode = static then . see Algorithm 7 8: \u3008DX,Q,Ccalc,D\u00d7\u3009 \u2190 STATICHS(\u3008K,B,P ,N \u3009R,Q, t, nmin, nmax,\nCcalc,DX,D\u00d7, pK(),P \u2032,N \u2032)\n9: else . see Algorithms 8, 9 and 10 10: \u3008DX,Q,Ccalc,D\u00d7,D\u2283,Qdup\u3009 \u2190 DYNAMICHS(\u3008K,B,P ,N \u3009R,Q,Qdup, t, nmin, nmax,\nCcalc,DX,D\u00d7, pK(),P \u2032,N \u2032,D\u2283)\n11: pD()\u2190 GETPROBDIST(DX, pK(), \u3008K,B,P ,N \u3009R, QA) . see Algorithm 6 12: Dmax \u2190 GETMODE(DX, pD()) 13: if pD(Dmax) \u2265 1\u2212 \u03c3 then . stop criterion 14: return GETSOLKB(Dmax, \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R,P \u2032,mode) . return solution KB 15: else 16: \u3008Q,P(Q)\u3009 \u2190 CALCQUERY(DX, qData, pD(), pK\u0303\u222aK(), qsm(), \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, q) . see Algorithm 6 17: answer \u2190 u(Q) . user interaction 18: QA\u2190 APPEND(\u3008Q, answer\u3009 , QA) 19: Dout \u2190 GETINVALIDDIAGS(P(Q), answer) 20: qData\u2190 UPDATEQDATA(Dout,DX, answer) 21: DX \u2190 DX \\Dout 22: D\u00d7 \u2190 D\u00d7 \u222aDout 23: if answer = true then 24: P \u2032 \u2190 P \u2032 \u222a {Q} 25: else 26: N \u2032 \u2190 N \u2032 \u222a {Q}\n8.7. CORRECTNESS OF QUERY POOL GENERATION 119\nAlgorithm 6 Interactive KB Debugging (continued) 27: procedure GETPROBDIST(DX, pK(), \u3008K,B,P ,N \u3009R, QA) 28: P \u2032\u2032,N \u2032\u2032 \u2190 \u2205 29: pD,prio()\u2190 GETPRIODIAGPROBS(DX, pK(), \u3008K,B,P ,N \u3009R) . application of Formula 4.3 30: for \u3008Q, u(Q)\u3009 \u2208 QA do . run through chronologically sorted query-answer pairs 31: if u(Q) = true then 32: for Dr \u2208 DX do . function GETENTAILMENTS is defined on page 104 33: EDr \u2190 GETENTAILMENTS(Dr,K,B,P \u222a P \u2032\u2032) . EDr is a set of entailments of K\u2217r 34: if Q 6\u2286 EDr then . Dr \u2208 D0(Q) 35: pD,prio(Dr)\u2190 12 pD,prio(Dr) 36: P \u2032\u2032 \u2190 P \u2032\u2032 \u222a {Q} 37: else 38: for Dr \u2208 DX do . ISKBVALID (see Algorithm 1) 39: if ISKBVALID((K \\ Dr) \u222aQ, \u3008\u00b7,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R) then . Dr \u2208 D\n0(Q) 40: pD,prio(Dr)\u2190 12 pD,prio(Dr) 41: N \u2032\u2032 \u2190 N \u2032\u2032 \u222a {Q} 42: sum\u2190 \u2211 Dr\u2208DX pD,prio(Dr) 43: for Dr \u2208 DX do 44: pD,prio(Dr)\u2190 1sum pD,prio(Dr) . normalization 45: return pD,prio()\n46: procedure CALCQUERY(DX, qData, pD(), pK\u0303\u222aK(), qsm(), \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, q) 47: QP\u2190 GETPOOLOFQUERIES(\u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R,DX, q) . see Algorithm 4 48: return SELECTBESTQUERY(QP, qData, pD(), pK\u0303\u222aK(), qsm()) . see Section 9.3\nChapter 9"}, {"heading": "An Algorithm for Interactive", "text": "Knowledge Base Debugging\nIn this chapter we will give a description of an algorithm for interactive KB debugging (Algorithm 5) which implements the entire functionality required by an interactive debugging system. All other algorithms presented so far will be subroutines of Algorithm 5 which are either directly or indirectly called by it. Before we explain and discuss Algorithm 5 in detail, we give the reader a rough and informal overview of the algorithm\u2019s input, output and actions in the following section in order to make the details of the algorithm easier to digest.\nRemark 9.1 Note, in the following, when we speak of the input DPI we refer to the DPI \u3008K,B,P ,N \u3009R that is provided as an input to Algorithm 5, by the current DPI we mean the DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R where P \u2032 and N \u2032, respectively, are all positive and negative test cases added to the input DPI from the start of the algorithm\u2019s execution until the current point in time. Further on, an intermediate (or previous) DPI denotes a DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R which is not the current DPI and where \u2205 \u2286 P \u2032\u2032 \u2286 P \u2032 and \u2205 \u2286 N \u2032\u2032 \u2286 N \u2032. Finally, the last-but-one DPI corresponds to an intermediate DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R where either |P \u2032| = |P \u2032\u2032|+ 1 or |N \u2032| = |N \u2032\u2032|+ 1 is true, but not both."}, {"heading": "9.1 Interactive Debugging Algorithm: Overview", "text": "Input:\nAn admissible DPI and some meta information where the latter consists of\n\u2022 fault probabilities of syntactical elements occurring in the KB,\n\u2022 a minimal and desired number of leading diagnoses,\n\u2022 a desired maximum reaction time (time between two successive queries presented to the user),\n\u2022 a maximum fault tolerance (roughly, the probability of being presented a non-desired solution KB as output),\n\u2022 a measure for query selection (determines which query is the best query within a given set of queries),\n\u2022 a parameter that determines the size of the computed pool of queries in each iteration and\n121\n122 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\n\u2022 a parameter specifying the way the hitting set tree for computation of leading diagnoses is constructed and updated.\nOutput: A solution KB such that the diagnosis used to formulate the solution KB has a probability (w.r.t. the current leading diagnoses) greater than or equal to 1 minus the given maximum fault tolerance.\nProcedure:\n1. Initialization: Compute the fault probability of each formula in the KB by means of the given fault probabilities.\n2. Leading Diagnoses Computation: Use a hitting set tree constructed and updated in a manner as specified in the input coupled with QX to calculate a set of leading diagnoses. In that, the cardinality and computation time of the set of leading diagnoses is determined by the corresponding input parameters specifying minimal and desired number of leading diagnoses and desired reaction time.\n3. Probability Update and Stop Criterion: Use the formula fault probabilities and the new information obtained by already specified test cases (answered queries) to compute updated (posterior) probabilities of the current leading diagnoses. If one diagnosis probability is greater than or equal to 1 minus the maximum fault tolerance, return the solution KB obtained by deletion of this diagnosis from the KB and subsequent addition of the union of all positive test cases.\n4. Query Generation and Selection: Use the set of leading diagnoses (and possibly their fault probabilities) to generate a pool of queries, the size of which depends on the respective parameter provided as input. Given the pool of queries, select the best query according to the given query selection measure.\n5. User Interaction and Incorporation of New Information: Ask the user the selected query and add it to the positive test cases in case of a positive answer and to the negative test cases otherwise.\n6. Hitting Set Tree Update: Update the hitting set tree based on the new information given by the classification of the test case resulting from the query answer. In particular, this involves the deletion of all those minimal diagnoses that conflict with the new test case.\n7. Repeat from Step 2."}, {"heading": "9.2 Interactive Debugging Algorithm: Detailed Description", "text": "To describe the detailed process of Algorithm 5, we first characterize the input arguments, the output and the meaning of the variables used and then provide a step-by-step textual description of the actions taken by the algorithm."}, {"heading": "9.2.1 Input Arguments", "text": "The input parameters of Algorithm 5 are the following:\n\u2022 An admissible DPI \u3008K,B,P ,N \u3009R (cf. Definition 3.6).\n\u2022 Natural numbers nmin \u2265 2, nmax, t for leading diagnoses calculation (see description in Chapter 7 on page 95).\nRemark: The postulation nmin \u2265 2 is necessary in order for the existence of queries w.r.t. any computed set of leading minimal diagnoses D and \u3008K,B,P ,N \u3009R to be guaranteed (see Proposition 7.5).\n9.2. DETAILED ALGORITHM DESCRIPTION 123\n\u2022 A function pK\u0303\u222aK : K\u0303 \u222a K \u2192 (0, 1] that assigns a fault probability pK\u0303\u222aK(e) to each e \u2208 K\u0303 \u222a K reflecting the degree of belief that (one occurrence of) a syntactical element e appearing in K is faulty (see Section 4.6).\nRemarks: Forbidding a probability of zero for syntactical elements assures that no formula in K can have a probability of zero (cf. Remark 4.5).\nRecall from Section 4.6.1 that K\u0303 refers to the signature of K (cf. Chapter 2) and K denotes the set of all logical connectives occurring inK. From probabilities of logical connectives and elements of the signature, probabilities of formulas in K and from those in turn probabilities of diagnoses w.r.t. the DPI can be derived as shown by Formulas 4.2 and 4.3.\nFurther note that in the description of the algorithms in this section, unlike in Section 4.6, we use different denotations for probabilities of syntactical elements (pK\u0303\u222aK), formulas (pK()) and diagnoses (pD()) in order to make a clear distinction between these different functions.\n\u2022 A natural number q \u2265 1 that denotes the number of queries that should be precomputed, i.e. the preferred size of the query pool QP (see Chapter 8), before the \u201cbest\u201d tuple \u3008Q\u2217,P(Q\u2217)\u3009 is selected from QP.\nRemark: In general, higher q implies better quality of the selected query in terms of the query selection measure qsm() (see next bullet point). The chance of locating a good query in a larger set of queries is higher. On the other hand, higher q involves a worse reaction time, i.e. time between two successive queries. The more queries are computed, the more time the function GETPOOLOFQUERIES consumes.\n\u2022 A query selection measure qsm() where qsm : QP \u2192 R is a function that assigns a real-valued number qsm(\u3008Q,P(Q)\u3009) to each tuple in QP, often called the score of \u3008Q,P(Q)\u3009. Remark: qsm() defines what is considered the \u201cbest\u201d query in the set QP, namely the query Q\u2217 in the tuple \u3008Q\u2217,P(Q\u2217)\u3009 with best score among all tuples in the pool QP. Diverse measures that can be used as a qsm() function in this algorithm have been discussed and evaluated within the scope of interactive KB debugging in literature [SFFR12, RSFF13] (for details see Section 9.3).\n\u2022 A maximum fault tolerance \u03c3 that defines the stop criterion of the algorithm. That is, for a current set of leading diagnoses, the stop criterion is satisfied iff the most probable leading diagnosis has an (updated) probability of at least 1\u2212\u03c3 (see below for a precise definition of what \u201cupdated\u201d means). Remark: The smaller \u03c3 is chosen, the higher is the chance that a desired diagnosis is found. Selecting \u03c3 := 0, i.e. admitting zero fault tolerance, is the safest (but also most time-consuming) way to run a debugging session with Algorithm 5, as in this case the session will stop only after all but one diagnosis have been invalidated by test cases.\n\u2022 A mode mode \u2208 {static, dynamic} that determines\n(i) which type of leading diagnoses are computed, i.e. only minimal diagnoses w.r.t. the input DPI (static) or minimal diagnoses w.r.t. the current DPI (dynamic),\n(ii) the hitting set tree pruning strategy after a query has been answered, i.e. conservative pruning (static) or invasive pruning (dynamic),\n(iii) the space and time complexity of diagnosis computation, i.e. not much affected by the asked queries (static) \u2013 tree is almost monotonically growing, but cannot get larger in size than the complete non-interactive hitting set tree (the tree produced by Algorithm 2 with input nmin = \u221e) \u2013 or significantly influenced by the asked queries (dynamic) \u2013 tree may shrink significantly if new test cases do not introduce \u201ccompletely new\u201d minimal conflict sets (that\n124 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nare in no subset-relation with an existing one), or lead to a tree that is significantly larger than the complete non-interactive hitting set tree if many \u201ccompletely new\u201d minimal conflict sets result from the addition of new test cases. For an in-depth discussion and comparison of both strategies the reader may consult Part III."}, {"heading": "9.2.2 Output", "text": "The output of Algorithm 5 can be explained as follows by making a distinction between the two modes of the algorithm specified by input parameter mode:\nProposition 9.1. If mode = static, then Algorithm 5 returns the (exact) solution of the Interactive Static KB Debugging problem (Problem Definition 6.2) if \u03c3 = 0 and an approximate solution of the problem if \u03c3 > 0 where the likeliness of finding the (exact) solution increases with decreasing \u03c3.\nMore concretely, a maximal solution KB K\u2217 = (K \\ Dmax) \u222a UP w.r.t. the input DPI \u3008K,B,P ,N \u3009R is returned such that\n1. Dmax \u2208 D (Dmax is an element of the current set of leading diagnoses)\n2. Dmax = arg maxD\u2208D pD(D) (Dmax is the a-posteriori most probable leading diagnosis)\n3. pD(Dmax) \u2265 1\u2212 \u03c3 (the a-posteriori probability of Dmax exceeds the predefined threshold)\n4. D \u2286mD\u3008K,B,P,N \u3009R\u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R comprises the |D|most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R as per the diagnosis probability measure pD,prio() (the set of leading diagnoses corresponds to the a-priori most probable minimal diagnoses w.r.t. the input DPI that satisfy all specified test cases),\n5. a-priori probability measure pD,prio() is computed from pK\u0303\u222aK() as per\n(a) Formula 4.2 (computation of formula fault probabilities)\n(b) Formula 4.7 (adaptation of formula fault probabilities)\n(c) Formula 4.3 (computation of diagnoses probabilities from formula fault probabilities)\n6. the a-posteriori probability measure pD() is computed from pD,prio() as per Bayes\u2019 Theorem (Formula 4.5, for details see below) taking into account the new information given by the set of all answered queries so far, i.e. the collected sets of positive (P \u2032) and negative (N \u2032) test cases.\nIf mode = dynamic, then Algorithm 5 returns the (exact) solution of the Interactive Dynamic KB Debugging problem (Problem Definition 6.1) if \u03c3 = 0 and an approximate solution of the problem if \u03c3 > 0 where the likeliness of finding the (exact) solution increases with decreasing \u03c3.\nMore concretely, a maximal solution KBK\u2217 = (K\\Dmax)\u222aUP\u222aP \u2032 w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R is returned such that\n1. Dmax \u2208 D (Dmax is an element of the current set of leading diagnoses)\n2. Dmax = arg maxD\u2208D pD(D) (Dmax is the a-posteriori most probable leading diagnosis)\n3. pD(Dmax) \u2265 1\u2212 \u03c3 (the a-posteriori probability of Dmax exceeds the predefined threshold)\n4. D \u2286mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R comprises the |D| most probable minimal diagnoses w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R as per the diagnosis probability measure pD,prio() (the set of leading diagnoses corresponds to the a-priori most probable minimal diagnoses w.r.t. the current DPI),\n9.2. DETAILED ALGORITHM DESCRIPTION 125\n5. the a-priori probability measure pD,prio() is computed from pK\u0303\u222aK() as per\n(a) Formula 4.2 (computation of formula fault probabilities)\n(b) Formula 4.7 (adaptation of formula fault probabilities)\n(c) Formula 4.3 (computation of diagnoses probabilities from formula fault probabilities)\n6. the a-posteriori probability measure pD() is computed from pD,prio() as per Bayes\u2019 Theorem (Formula 4.5, for details see below) taking into account the new information given by the set of all answered queries so far, i.e. the collected sets of positive (P \u2032) and negative (N \u2032) test cases.\nRemark 9.2 We still need to explain what we mean by \u201capproximate solution\u201d of the Interactive Static (Dynamic) KB Debugging problem. Roughly, an approximate solution is one constructed from a diagnosis which is not the only remaining minimal diagnosis. More precisely, an approximate solution of\n\u2022 the Interactive Static KB Debugging problem is a maximal solution KB (K \\ D) \u222a UP such that\n\u2013 D is a minimal diagnosis w.r.t. the input DPI and w.r.t. the current DPI and \u2013 there is some D\u2032 6= D which is a minimal diagnosis w.r.t. the input DPI and w.r.t. the current\nDPI\n\u2022 the Interactive Dynamic KB Debugging problem is a maximal solution KB (K \\D)\u222aUP\u222aP \u2032 such that\n\u2013 D is a minimal diagnosis w.r.t. the current DPI and \u2013 there is some D\u2032 6= D which is a minimal diagnosis w.r.t. the current DPI\nwhere the input DPI is given by \u3008K,B,P ,N \u3009R and the currect DPI by \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R. So, as long as not all but one diagnosis candidate that enables the formulation of a solution KB has been ruled out by the classification of test cases, we speak of an approximate solution. Now, the lower a value for \u03c3 is predefined, the longer Algorithm 5 will usually need to iterate and the more test cases will usually need to be specified until one diagnosis has a probability greater than or equal to 1\u2212 \u03c3. Thence, at the time a diagnosis exceeds the probability 1 \u2212 \u03c3 there will be usually fewer minimal diagnoses left than in case of the selection a higher value for \u03c3. Therefore, the likeliness of picking the (exact) solution will usually be the higher, the lower \u03c3 is.\nRemark 9.3 Note that granting a maximum absolute fault tolerance \u03c3 that is independent of a set of leading diagnoses is generally computationally infeasible due to the high complexity of diagnosis computation (see Chapter 1). Since, for an absolute fault tolerance to hold, all minimal diagnoses w.r.t. the current DPI have to be computed in order to determine their probability and to decide whether the most probable diagnosis has a probability greater than or equal to 1\u2212 \u03c3.\nIn fact, the fault tolerance used by Algorithm 5 which is relative to the set of leading diagnoses, i.e. the (a-priori) most probable minimal diagnoses D w.r.t. a DPI can be interpreted as follows. Under the assumption that the true diagnosis Dt is included in D, the chance that the most probable minimal diagnosis Dmax \u2208 D which satisfies the stop criterion is not equal to Dt is smaller than the predefined threshold \u03c3 (cf. Section 4.6). Thus, under this assumption, the (a-posteriori) probability of being presented a non-desired solution KB as output of Algorithm 5 is smaller than \u03c3.\nThe a-priori diagnoses probability measure pD,prio() refers to the one that is computed directly from the fault information provided as an input to Algorithm 5 whereas the a-posteriori diagnoses probability measure pD() is the one obtained from pD,prio() after incorporating the information given by the new\n126 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\ntest cases specified so far during the debugging session. So, pD,prio() and pD() might differ in terms of the probability order of diagnoses. Incorporation of updated probabilities directly into the hitting set tree algorithms to be used for the determination of leading diagnoses in the order prescribed by an updated probability measure is only possible if there is an additional update operator (besides Bayes\u2019 Theorem for adapting diagnoses probabilities) that can be applied to formula probabilities. For, the latter are exploited in the hitting set tree to assign probability weights to paths that are not yet diagnoses (cf. pnodes() specified by Definition 4.9 and the discussion of Formula 4.6) in order to guide the search for minimal diagnoses in best-first order. Updated diagnosis probabilities are not helpful at all for this purpose. Devising a reasonable mechanism of updating formula probabilities seems to be hard mostly due to the lack of suitable data that might be collected during the debugging session to accomplish that. What would be imaginable during the debugging session is to try to learn something about the fault probability of syntactical elements by examining the positive (all formulas are definitely correct) and singleton negative (the single formula is definitely incorrect) test cases. However, a drawback of such a strategy comes into effect when only syntactically very simple queries are used which is, for instance, the case in Example 8.1 (see the definition of the GETENTAILMENTS function there). From such queries not many useful insights concerning faulty syntactical elements might be gained. On the other hand, such queries are absolutely desirable from the point of view of how well a user might comprehend the formulas asked by the system. Hence, these two aspects seem to contradict each other. Still, it is a topic for future research to attempt to elaborate a solution for that issue.\nA way to achieve that pD() coincides with pD,prio(), at least in case mode = static, is to exclude queries Q with D0(Q) 6= \u2205 (see Remark 9.8). How this might be accomplished is stated by Proposition 8.3. Please notice that ignorance of queries with non-empty D0 does not implicate any disadvantages for interactive debugging. On the contrary, it is even a desirable feature of a debugger and brings along higher computational efficacy of query generation and stronger test cases from the logical point of view (cf. Section 8.2). For the scenario mode = dynamic, it is not possible in general to bypass the probability update by means of such queries (see Remark 9.8)."}, {"heading": "9.2.3 Variables", "text": "The variables used by Algorithm 5 that are not input arguments to the algorithm are the following:\n\u2022 P \u2032,N \u2032 are the sets of positive and negative test cases, respectively, collected during the execution of Algorithm 5 so far. That is, P \u2032 stores all positively answered queries, whereas N \u2032 stores all negatively answered ones.\n\u2022 Ccalc is the set of all conflict sets computed by QX during the execution of Algorithm 5 so far. Remark: In case of static debugging (mode = static), Ccalc includes exclusively minimal conflict sets w.r.t. the input DPI, whereas, in case of dynamic debugging (mode = dynamic), Ccalc may comprise minimal conflict sets w.r.t. the current or any intermediate DPI.\n\u2022 DX is the set of leading diagnoses returned by a call of STATICHS in case of static debugging (mode = static) and by a call of DYNAMICHS in case of dynamic debugging (mode = dynamic).\nRemarks: In case of dynamic debugging, DX \u2286mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R as per the diagnosis probability measure pD,prio() computed from pK\u0303\u222aK() by Formulas 4.2, 4.7, 4.3 and 4.4 (cf. Sections 4.6 and 9.2.2).\nIn case of static debugging, DX \u2286mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R , i.e. DX includes only diagnoses that are minimal diagnoses w.r.t. the input DPI \u3008K,B,P ,N \u3009R as well as w.r.t. the current DPI \u3008K,B,P\u222aP \u2032,N \u222aN \u2032\u3009R. Moreover, DX comprises the most probable minimal diagnoses w.r.t.\n9.2. DETAILED ALGORITHM DESCRIPTION 127\nthe input DPI according to the diagnosis probability measure pD,prio() computed from pK\u0303\u222aK() by Formulas 4.2, 4.7, 4.3 and 4.4 (cf. Sections 4.6 and 9.2.2).\n\u2022 D\u00d7 stores all minimal diagnoses w.r.t. the input DPI that have been invalidated by one of the collected positive and negative test cases P \u2032 and N \u2032, respectively (mode = static). D\u00d7 stores the minimal diagnoses w.r.t. the last-but-one DPI that have been invalidated by the most recently added test case (mode = dynamic).\n\u2022 Dout is the subset of the set of current leading diagnoses DX that has been invalidated by the most recently added test case.\n\u2022 D\u2283 stores all diagnoses that are non-minimal w.r.t. the current DPI, i.e. for each diagnosis nd \u2208 D\u2283 there is some nd\u2032 \u2208 DX such that nd \u2283 nd\u2032 (mode = dynamic). Remark: D\u2283 is solely needed for dynamic and not for static debugging as the latter does not need to store non-minimal diagnoses (cf. rule 4 of Definition 4.8 on page 59). Reason for this is the fact that only minimal diagnoses w.r.t. the input DPI are searched for. On the other hand, in case of dynamic debugging, non-minimal diagnoses might become minimal ones after some new test cases are specified since minimal diagnoses w.r.t. the (changing) current DPI are considered.\n\u2022 qData is an informal variable that comprehends any kind of data that might be taken into account by the query selection measure qsm() and that might need to be adapted after a query has been answered (and diagnoses have been invalidated) in order to take the obtained new information into account. One can imagine qData as a log specific to the particular function qsm() that is used which records data of prior (query answering) iterations executed by the algorithm such as certain performance measures. An example of a qsm() strategy using one such metric, namely the ratio of leading diagnoses invalidated by a test case, can be found in [RSFF13].\n\u2022 QA := [\u3008Q, u(Q)\u3009]Q\u2208P \u2032\u222aN \u2032 where u(Q) \u2208 {true, false} is the chronologically ordered list of queries and user answers collected so far during the execution of Algorithm 5.\n\u2022 Q is the current queue of open nodes in the hitting set tree maintained by Algorithm 5.\n\u2022 The list Qdup roughly stores all duplicate nodes (that is, nodes for each of which there is a node in the hitting set tree that corresponds to an equal set of edge labels) computed so far during the execution of Algorithm 5.\nRemark: The list Qdup is only relevant in case mode = dynamic and not needed if mode = static. The purpose of this set is to enable the \u201creplacement\u201d of pruned nodes which is necessary to guarantee the completeness of DYNAMICHS in terms of not missing any minimal diagnoses (for a detailed explanation, see Chapter 12)."}, {"heading": "9.2.4 Algorithm Walkthrough", "text": "Initialization. In the first 4 lines, variable declarations take place. First, all variables that store sets of conflict sets, diagnoses or test cases, and qData are initialized to the empty set. Further on, Qdup and QA are initialized to an empty list. Finally, the queue Q of open nodes used for the hitting set tree construction by STATICHS (mode = static) or DYNAMICHS (mode = dynamic), respectively, is set to [\u2205] since it initially includes only a non-labeled root node.\nRemark 9.4 The non-labeled root node is denoted by \u2205 since nodes in STATICHS are associated with the set of edge labels along the path in the hitting set tree from the root node to this node (cf. Chapters 4 and 11). Hence, the root node itself corresponds to the empty path which includes no edges.\n128 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nNotice that in case of DYNAMICHS, nodes will be (ordered) lists instead of (non-ordered) sets like in STATICHS (cf. Chapter 12). That is, to be precise, the unlabeled root node in this case corresponds to the empty list []. For the ease of representation of Algorithm 5, only one set Q is initialized to be used with either STATICHS or DYNAMICHS. Thence, by abuse of notation, we associate \u2205 in this case with the empty list [].\nComputing Fault Probabilities of Formulas. Then, GETFORMULAPROBS is called in line 5 with the KB K and the function pK\u0303\u222aK : K\u0303 \u222a K \u2192 (0, 1] as inputs. The function first applies Formula 4.2 to compute probabilities for each formula in K, then applies Formula 4.7 to these probabilities leading to the output pK : K \u2192 (0, 0.5), a function that assigns a value pK(ax ) \u2208 (0, 0.5) to each ax \u2208 K.\nComputing Leading Diagnoses. At this point, all input arguments required by for the hitting set tree construction are instantiated. So, the algorithm enters the while loop in line 6. As a first step within the loop, either STATICHS, if mode = static, or DYNAMICHS, otherwise, is called in order to obtain a tuple including a set of leading diagnoses along with variables that store the \u201cstate\u201d of the (partial) hitting set tree constructed so far and facilitate the reuse of this tree in the next iteration.\nIn concrete terms, STATICHS accepts the arguments \u3008K,B,P ,N \u3009R, Q, t, nmin, nmax, Ccalc, DX, D\u00d7, pK(), P \u2032 and N \u2032 and returns a tuple \u3008D,Q,Ccalc,D\u00d7\u3009 the elements of which are defined as follows:\n\u2022 D is the current set of leading diagnoses such that\n(a) D \u2286mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R that satisfy all test cases P \u2032 and N \u2032 such that (i) nmin \u2264 |D| \u2264 nmax and\n(ii) D \u2283 DX, if such a set D exists; or\n(b) D is equal to the set of all minimal diagnoses mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R , otherwise;\nwhere \u201cmost-probable\u201d refers to the diagnosis probability measure pD,prio() obtained from pK() by application of Formulas 4.3 and 4.4.\n\u2022 Q is the current queue of open nodes of the hitting set tree.\n\u2022 Ccalc \u2286 mC\u3008K,B,P,N \u3009R is the set of all computed minimal conflict sets w.r.t. the input DPI throughout all calls of STATICHS during the execution of Algorithm 5 so far.\n\u2022 D\u00d7 comprises all computed minimal diagnoses throughout all calls of STATICHS during the execution of Algorithm 5 so far where each D \u2208 D\u00d7 has been invalidated by some test case in P \u2032 or N \u2032.\nSimilarly, DYNAMICHS accepts the arguments \u3008K,B,P ,N \u3009R, Q, Qdup, t, nmin, nmax, Ccalc, DX, D\u00d7, pK(), P \u2032, N \u2032 and D\u2283 and returns a tuple \u3008D,Q,Ccalc,D\u00d7,D\u2283,Qdup\u3009 the elements of which are defined as follows:\n\u2022 D is the current set of leading diagnoses such that\n(a) D \u2286 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R such that (i) nmin \u2264 |D| \u2264 nmax and\n9.2. DETAILED ALGORITHM DESCRIPTION 129\n(ii) D \\DX 6= \u2205, if such a set D exists, or\n(b) D is equal to the set of all minimal diagnoses mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R , otherwise,\nwhere \u201cmost-probable\u201d refers to the diagnosis probability measure pD,prio() obtained from pK() by application of Formulas 4.3 and 4.4.\n\u2022 Q is the current queue of open (non-labeled) nodes of the hitting set tree,\n\u2022 Ccalc is a set of conflict sets w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R,\n\u2022 D\u00d7 = \u2205,\n\u2022 D\u2283 is the set of all processed nodes so far throughout the execution of Algorithm 5 that are nonminimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and\n\u2022 Qdup includes all duplicate nodes found so far throughout the execution of Algorithm 5 (for a detailed explanation see Chapter 12 and Algorithm 8).\nRemark 9.5 It is very important to notice that the function pnodes() for p() := pK() as specified by Definition 4.9 on page 73 imposes the same order on a set of minimal diagnoses as the a-priori probability measure pD,prio(). That is pnodes(D) = c \u00b7 pD,prio(D) for all minimal diagnoses D w.r.t. a DPI where c is a constant (which is the same for all diagnoses D). The difference between both functions is that pnodes() is defined for allX \u2286 K whereas pD,prio() is only defined for (leading) minimal diagnosesD \u2286 K. Further on pD,prio() is normalized whereas pnodes() is not which accounts for the (normalization) constant c. The function pnodes() is essential for the best-first construction of the hitting set tree in STATICHS and DYNAMICHS since it allows for the assignment of a \u201cprobability\u201d to non-diagnoses (cf. the discussion of Formula 4.6 on page 73). Since the input argument p() (which is the same for all calls) to STATICHS as well as DYNAMICHS is equal to pK() by lines 8 and 10 in Algorithm 5, the set D returned by STATICHS (DYNAMICHS) is also the set of most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R (\u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) as per the function pnodes() (cf. Proposition 11.1 and Corollary 12.8).\nRemark 9.6 Notice that the return parameter that is relevant for the main purpose of Algorithm 5, namely to compute a query and thereby obtain a new test case classified by the user, is solely the set of leading diagnoses D. The other return parameters serve as a means to store the state of the hitting set tree that is gradually built up by successive calls of STATICHS (if mode = static) and DYNAMICHS (if mode = dynamic), respectively. Whereas Q and Ccalc (and D\u2283 and Qdup in case of DYNAMICHS) are never modified until the next call to STATICHS or DYNAMICHS, the sets DX and D\u00d7 are only changed once, after the subset of invalidated leading diagnoses Dout is known, in lines 21 and 22.\nAt this moment, we do not go into detail regarding the way how leading diagnoses are computed by STATICHS and DYNAMICHS. We simply suppose that both functions act in a manner that the outputs just specified are returned for the given inputs. An in-depth delineation of both functions will be given in Chapters 11 and 12 in Part III. Further note that the return parameter D is stored in variable DX from line 10 on.\nComputing a Probability Distribution of Leading Diagnoses. After the set of leading diagnoses DX has been computed, the variables DX, pK(), \u3008K,B,P ,N \u3009R andQA are used as arguments to the function GETPROBDIST (see Algorithm 6) which computes a probability distribution of the leading diagnoses, i.e. a probability measure pD() for the probability space with sample space \u2126 = DX (cf. Section 4.6). As a first action to achieve this, the (a-priori) probabilities pD,prio(D) for D \u2208 DX are computed from\n130 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nthe (a-priori) probabilities pK(ax ) for formulas ax \u2208 K as per Formula 4.3 (GETPRIODIAGPROBS in line 29). Application of Formula 4.4 is not necessary at this point as probabilities are anyhow normalized at the end of GETPROBDIST (line 44). Notice that the function pK() remains constant, i.e. unmodified, throughout the entire execution of Algorithm 5.\nNow, since a-priori diagnosis probabilities assigned by pD,prio() directly rely upon pK() which in turn is computed directly from the initially given fault probabilities pK\u0303\u222aK(), the probability measure pD,prio() is adapted to yield a-posteriori diagnosis probabilities pD() in order to reflect the new evidence provided by the collected test cases P \u2032 and N \u2032.\nThe a-posteriori probability of a current leading diagnosis D in DX is pD(D |QA) and can be computed by means of Bayes\u2019 Theorem (Formula 4.5) from pD,prio() as follows.\npD(D |QA) = pD,prio(QA | D) pD,prio(D)\npD,prio(QA)\nwhere QA is the chronologically ordered list of queries and user answers collected so far during the execution of Algorithm 5 (see page 127). We point out that pD,prio(QA) is only a normalization factor that is equal for each diagnosis and thus does not need to be explicitly computed. The crucial factor is\npD,prio(QA | D) = pD,prio(\u2200 \u3008Q, u(Q)\u3009 \u2208 QA : Q = u(Q) | D)\nwhich describes the probability of getting exactly the answer u(Q) for each query Q \u2208 P \u2032\u222aN \u2032 under the assumption that D corresponds to the true diagnosis Dt, i.e. Dt = D. In other words, pD,prio(QA | D) is the probability of QA under the assumption that the user answers in a way that u(Q) = true if D \u2208 D+(Q) and u(Q) = false if D \u2208 D\u2212(Q).\nFor a single query Qi, the probability pD,prio(Qi = u(Qi) | D) is defined as (cf. [dKW87])\npD,prio(Qi = u(Qi) | D) =  1, if D \u2208 D+(Qi) 0, if D \u2208 D\u2212(Qi) 1 2 , if D \u2208 D 0(Qi)\n(9.1)\nfor u(Qi) = true and\npD,prio(Qi = u(Qi) | D) =  1, if D \u2208 D\u2212(Qi) 0, if D \u2208 D+(Qi) 1 2 , if D \u2208 D 0(Qi)\n(9.2)\nfor u(Qi) = false where D+(Qi), D\u2212(Qi) and D0(Qi) are computed w.r.t. the DPI \u3008K, B,P \u222aP \u2032\u2032,N \u222a N \u2032\u2032\u3009 where P \u2032\u2032 and N \u2032\u2032, respectively, include all test cases collected prior to Qi, i.e. P \u2032\u2032 \u222a N \u2032\u2032 = {Q1, . . . , Qi\u22121} if queries are numbered chronologically. That is, if D predicted the answer u(Qi) to Qi given by the user, the probability is 1, zero if D predicted the converse answer \u00acu(Qi) and 12 if D did not predict any answer to Qi.\nSo, aside from the normalization factor (see above), pD,prio(Qi = u(Qi) | D) is the factor by which the a-priori probability pD,prio(D) must be multiplied to obtain the a-posteriori probability pD(D) of a diagnosis D after a single query Qi has been answered and added as a test case to the DPI.\nThe intuitive explanation for the update by this factor is that ifD predicted (at least) one answer u(Q) conversely as given by the user, then D is a-posteriori impossible since it has already been invalidated by the addition of test case Q. In case a diagnosis has never predicted the wrong answer, but did not predict any answer for many queries so far, then it is a-posteriori more unlikely than a diagnosis that did predict a correct answer more often. That is, our a-posteriori degree of belief that D is the correct diagnosis is\n9.2. DETAILED ALGORITHM DESCRIPTION 131\nthe higher, the more often D had predicted answers to queries that were later actually given by the user (cf. Section 7.4 for an explanation what we mean by \u201cpredict\u201d).\nThe value of pD,prio(Qi = u(Qi) | D) can be computed by use of QA and the q-partitions P(Q1), . . . , P(Qi\u22121) of the current set of leading diagnoses DX (for which a-posteriori probabilities are to be computed) for all queries Q1, . . . , Qi\u22121 answered before query Qi. Thereby, each P(Qj) where j \u2208 {1, . . . , i\u2212 1} must be computed for a DPI where only Q1, . . . , Qj\u22121 are incorporated as test cases.\nTaking these thoughts into account, GETPROBDIST (Algorithm 6) updates pD,prio(D) for each diagnosis D \u2208 DX in that it runs through all query-answer pairs \u3008Q, u(Q)\u3009 in QA chronologically and for each D \u2208 DX it multiplies pD,prio(D) by 12 if D \u2208 D\n0(Q) as per Formulas 9.1 and 9.2. For each check whether a diagnosis is in D0(Q) in lines 34 and 39 a DPI is used that already incorporates all test cases P \u2032\u2032 and N \u2032\u2032 that have been added chronologically before Q was asked. This is achieved by updating P \u2032\u2032 and N \u2032\u2032 successively (lines 36 and 41). After all elements of QA have been processed, the updated diagnosis probabilities are finally normalized (line 44, cf. Formula 4.4 on page 72) and the resulting function pD,prio() is returned.\nRemark 9.7 Note that the function GETPROBDIST exploits the fact that all diagnoses inDX are leading diagnoses w.r.t. the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R which guarantees that none of these diagnoses has been invalidated by any of the test cases in P \u2032 or in N \u2032 added throughout the execution of Algorithm 5 (cf. Proposition 12.3 given later). Hence, it is clear that each D \u2208 DX must be in D+(Q) \u222aD0(Q) if u(Q) = true and in D\u2212(Q) \u222aD0(Q) if u(Q) = false , and it is only tested whether D /\u2208 D+(Q) in the prior case (line 34) and whetherD /\u2208 D\u2212(Q) in the latter (line 39). It must be further noted that, in case of mode = dynamic, diagnoses in DX are not necessarily minimal diagnoses w.r.t. the intermediate DPIs \u3008K, B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009 that are used for the probability update. However, this is not problematic since any set of (minimal and/or non-minimal) diagnoses is partitioned into the three sets D+(Q), D\u2212(Q) and D0(Q) by a query Q (cf. Remark 7.3) wherefore P(Q) exists for any set DX. Thence, the correctness of GETPROBDIST remains unaffected by the usage of the setting mode = dynamic.\nRemark 9.8 We want to emphasize that an adaptation of pD,prio(D) is only necessary in case D \u2208 D0(Qj) for some query Qj answered so far during the execution of Algorithm 5 as otherwise a multiplication by 1 is required which does not change pD,prio(D).\nFor the case of static debugging (mode = static), an immediate implication of this is the following: The restriction of asking the user only queries Qj w.r.t. a DPI with the property that no minimal diagnosis w.r.t. this DPI can be an element of D0(Qj) makes the probability update for each diagnosis in DX equivalent to a multiplication by 1 and hence obsolete. This must be the case since each diagnosis in DX which is a subset of mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R (see Section 9.2.2) must be a minimal diagnosis w.r.t. each intermediate DPI (which includes a superset of the test cases in the input DPI \u3008K,B,P ,N \u3009R and a subset of the test cases in the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R) as will be substantiated by Proposition 12.5 given later. Consequently, such a scenario implicates that the order of diagnoses computed by STATICHS corresponds to the best-first order also w.r.t. the a-posteriori diagnosis probabilities (cf. Remark 9.3).\nThe approach of only using queries with this property is feasible, e.g. by using a GETENTAILMENTS function in conformity with Proposition 8.3 for the generation of the query pool (GETPOOLOFQUERIES). Such a type of queries is also favorable from the discrimination point of view, as we pointed out in Section 8.2. An improvement of static debugging with this type of queries is to deactivate the probability update, i.e. replace line 11 in Algorithm 5 by line 29 of Algorithm 6. This improvement is not shown in Algorithm 5.\nIn a dynamic debugging session (mode = dynamic), on the contrary, the usage of such queries does not guarantee the triviality of the probability update. For, also if no minimal diagnosis w.r.t. the DPI (for which a query Qj is computed) can be an element of D0(Qj), there may be some non-minimal\n132 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\none which is. For example, for any admissible DPI \u3008K,B,P ,N \u3009R is holds that D := K is a diagnosis (cf. Proposition 3.4 and Definition 3.6), albeit in most cases a non-minimal one. In such a case, (K \\ D) \u222a B \u222a UP which is equal to B \u222a UP cannot entail Qj . Because, were this the case, then all minimal diagnoses Di \u2208 mD\u3008K,B,P,N \u3009R would be elements of D+(Qj) as each K\u2217i \u2287 B \u222a UP and thus each K\u2217i |= Qj by the monotonicity of L. Hence, this would be a contradiction to the fact that Qj is a query w.r.t. \u3008K,B,P ,N \u3009R by Corollary 7.2. On the other hand, (K\\D)\u222aB\u222aUP \u222aQj = B\u222aUP \u222aQj cannot violate any x \u2208 N \u222aR. Since, if this were the case, then addingQj to the positive test cases would lead to a non-admissible DPI \u3008K,B,P \u222a {Qj} ,N \u3009R. By Corollary 7.3, this would be a contradiction to the fact that Qj is a query w.r.t. \u3008K,B,P ,N \u3009R. Thence, D \u2208 D0(Qj) must hold for the assumed non-minimal diagnosis D. From that we conclude that the probability update in dynamic debugging cannot be made obsolete in general by the usage of such a type of queries.\nStop Criterion and Output. The (a-posteriori) probability distribution pD() of leading diagnoses DX is then used in line 12 of Algorithm 5 to compute the mode of this distribution, i.e. the one diagnosis Dmax \u2208 DX with maximum probability according to pD().\nIn the sequel, Dmax is used to check the stop criterion (line 13), namely whether Dmax has a probability greater than or equal to 1 \u2212 \u03c3. If this is the case and mode = static, the function GETSOLKB computes a maximal solution KB w.r.t. the input DPI as (K \\ Dmax) \u222a UP by means of the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, P \u2032 and Dmax. Given that mode = dynamic, GETSOLKB returns a maximal solution KB w.r.t. the current DPI as (K \\ Dmax) \u222a UP\u222aP \u2032 by means of the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R andDmax. This solution KB is then returned as an output of Algorithm 5. If, on the other hand, the stop criterion is not met, the algorithm continues the execution with the computation of another query.\nRemark 9.9 Notice that the returned maximal solution KB (K \\ Dmax) \u222a UP w.r.t. the input DPI in case mode = static can be easily extended to constitute a maximal solution KB w.r.t. the current DPI, namely by extending it by UP \u2032 . Ifmode = dynamic, then the KB output in line 14 is a maximal solution KB w.r.t. the current DPI, but possibly a non-maximal solution KB w.r.t. the input DPI.\nQuery Computation and User Interaction. In line 16, the function CALCQUERY is applied to compute a query and the associated q-partition by means of the leading diagnoses DX, (possibly) the collected data qData, the probability distribution pD() of the leading diagnoses, a query selection function qsm() (which might exploit the function pK\u0303\u222aK()), a parameter q determining the size of the computed query pool and the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nAs a first step within CALCQUERY, the function GETPOOLOFQUERIES computes a query pool QP as detailed in Chapter 8 from DX, q and \u3008K,B,P\u222aP \u2032,N \u222aN \u2032\u3009R. Then, the best tuple \u3008Q,P(Q)\u3009 \u2208 QP according to the function qsm() is searched for and finally returned as the output of CALCQUERY. During the query selection process, the evaluation of the query selection measure qsm(Q) \u2208 R for queries Q where \u3008Q,P(Q)\u3009 \u2208 QP may require qData, the fault probabilities pD() of leading diagnoses as well as the fault probabilities pK\u0303\u222aK() of syntactical elements in K. This depends on which concrete measure qsm() is employed (see Section 9.3 which presents some possible measures).\nAs a next step, the query Q of the best tuple \u3008Q,P(Q)\u3009 \u2208 QP is presented to the interacting user in line 17 which is the only place in Algorithm 5 where user interaction takes place. The user is modeled as a deterministic function u : QD,\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009 \u2192 {true, false} that allocates a positive (true) or negative (false) answer to each query w.r.t. any set of leading diagnoses D for some current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009. The answer u(Q) given by the user is stored in the variable answer.\nRemark 9.10 We want to point out that the algorithm can be easily adapted to allow a user to reject\n9.2. DETAILED ALGORITHM DESCRIPTION 133\nqueries, e.g. if they are not sure how to answer. That is, the user function might be modeled as u : QD,\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009 \u2192 {true, false, unknown} where u(Q) = unknown signifies the rejection of query Q. In this case, an accordingly modified version of Algorithm 5 would calculate an alternative query w.r.t. D and \u3008K, B,P \u222a P \u2032,N \u222a N \u2032\u3009, e.g. the second best one according to the query selection measure qsm() among all tuples in QP (this potential feature is not shown in Algorithm 5). In this vein, a total of |QP| \u2212 1 queries can be dismissed per set of leading diagnoses D.\nWe want to accentuate that the presented interactive algorithm might be easily adapted to cope with queries whose answer is unknown to the user, but a definite assumption for the algorithm to return a correct solution is a user that does not give wrong answers. In other words, the algorithm does not provide inherent mechanisms that allow for the detection of wrong answers or for the debugging of the KB debugging procedure (keyword \u201cgarbage in, garbage out\u201d). So, we suppose the function u() to be deterministic which prohibits the situation that a user might change their mind at a later point in time. Of course, this is still a possible scenario in practice, but in case it arises, a user has to revise, i.e. delete or edit, specified test cases they disagree with by hand before a new debugging session using the modified DPI might be started.\nAnother remark at this place concerns the way a user might choose to answer the query. A \u201cminimal\u201d feedback of a user that we regard as an answer to a query Q is to merely say true , i.e. each formula in Q (or the conjunction of formulas in Q) must be entailed by the correct KB, or false , i.e. at least one formula in Q (or the conjunction of formulas in Q) must not be entailed by the correct KB. The presented algorithm (Algorithm 5) is designed to deal with exactly this kind of an answer. However, imagine a user being presented Q and think of how they might proceed in order to come up with an answer to Q. The first observation is that, in order to respond by true , a user must definitely scrutinize each single formula in Q because otherwise they could never decide for sure whether the conjunction of all formulas in Q is correct. Another observation is that a user might cease to go through the rest of the formulas in case they have already identified one that must not be an entailment of the desired KB. For, in this situation, the overall query Q is already false . This however indicates that at least one formula must be known to be correct or false whatever answer is given to Q. Therefore, we can usually expect a user to be able to give exactly this information, namely one formula in Q that must be incorrect, additionally to answering by false . This extra piece of information can be exploited to achieve better space and time efficiency in the context of diagnosis computation. Proposing more efficient algorithms that exploit this information is a topic for future work.\nIncorporating the New Information. The new information represented by the answer answer to Q is incorporated (lines 18-26) by updating values of all relevant parameters. First, by means of the function APPEND, the tuple consisting of the answered query Q and the corresponding answer answer given by the user is added as a last element to the chronological list of queries and answers QA that is used for the next probability update (line 11).\nThen, the subset Dout of the leading diagnoses DX that gets invalidated after addingQ to the positive or negative test cases of the DPI, respectively, is computed by the function GETINVALIDDIAGS that gets the q-partition P(Q) = \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a of Q and answer as input arguments. Dout then corresponds to the set D\u2212(Q) given that answer is true and to D+(Q) otherwise (cf. Section 7.4). Note that \u2205 \u2282 Dout \u2282 DX holds by Proposition 7.4 and since Q is a query w.r.t. DX (since DX is given as an input to CALCQUERY).\nAs a next step, the data qData is updated. As already pointed out in Section 9.2.3, the form of the variable qData depends on the employed query selection measure qsm() and so do the actions that are performed by UPDATEQDATA.\nIn order to communicate the impact of the answered query to the hitting set tree algorithm (either STATICHS or DYNAMICHS), the set of invalidated leading diagnoses Dout is deleted from the leading\n134 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\ndiagnoses DX and added to D\u00d7. After this update, DX includes all diagnoses that have been computed by the hitting set tree algorithm so far that are minimal diagnoses w.r.t. the current DPI.\nFinally, the new test case Q is added to the new positive test cases P \u2032 if answer is true and to the new negative test cases N \u2032 in case of answer = false ."}, {"heading": "9.3 Query Selection Measures", "text": "In this section, we give a brief introduction to some query selection measures qsm() that have been suggested and evaluated in literature within the scope of KB or ontology debugging [SFFR12, RSFF13]. Such query selection measures, when used as a parameter in an interactive KB debugging algorithm such as the one described by Algorithm 5, aim at solving the following optimization problems. In Interactive Dynamic KB Debugging, the problem is defined as follows:\nProblem Definition 9.1. The task is to solve the problem specified by Problem Definition 6.1 in a way that |P \u2032|+ |N \u2032| is minimal.\nIn Interactive Static KB Debugging, the problem is defined as follows:\nProblem Definition 9.2. The task is to solve the problem specified by Problem Definition 6.2 in a way that |P \u2032|+ |N \u2032| is minimal.\nThat is, these optimization problems aim at the minimization of user effort during interactive KB debugging. In other words, the goal is the minimization of the number of queries required to be asked to a user in order to solve the Interactive Static KB Debugging or the Interactive Dynamic KB Debugging Problem, respectively.\nIn our previous work [SFFR12], we have discussed entropy-based (ENT()) and split-in-half (SPL()) query selection measures.\nEntropy-Based Query Selection. A best query QENT according to ENT() has a maximal information gain among all queriesQwhere \u3008Q,P(Q)\u3009 \u2208 QP. In other words,QENT minimizes the expected entropy of the probability distribution of the leading diagnoses DX after QENT has been added as a test case to the DPI based on the user\u2019s answer u(QENT). As shown in [dKW87], this leads to the definition\nENT(Q) := \u2211\na\u2208{true,false}\np(Q = a) log p(Q = a) + p(D0(Q))\nwhere p() in the case of our algorithm corresponds to the leading diagnoses probability measure pD() computed in line 11 in Algorithm 5 and\np(Q = true) = p(D+(Q)) + 1\n2 p(D0(Q))\np(Q = false) = p(D\u2212(Q)) + 1\n2 p(D0(Q))\n(cf. Section 7.4) where\np(D+(Q)) = \u2211\nD\u2208D+(Q)\np(D)\np(D\u2212(Q)) = \u2211\nD\u2208D\u2212(Q)\np(D)\np(D0(Q)) = \u2211\nD\u2208D0(Q)\np(D)\n9.3. QUERY SELECTION MEASURES 135\nThen, the best query in a pool QP according to qsm() := ENT() is\nQENT = arg min {Q | \u3008Q,P(Q)\u3009\u2208QP} ENT(Q)\nSo, theoretically optimal w.r.t. ENT() is a queryQwhose positive and negative answers are equally likely and for which D0(Q) is the empty set. In other words, the best query has the property that the sum of probabilities of leading diagnoses predicting the positive answer as well as the sum of probabilities of leading diagnoses predicting the negative answer is 50%.\nSplit-In-Half Query Selection. For the selection criterion qsm() := SPL(), on the other hand, the query\nQSPL = arg min {Q | \u3008Q,P(Q)\u3009\u2208QP} SPL(Q)\nis preferred where\nSPL(Q) := \u2223\u2223 |D+(Q)| \u2212 |D\u2212(Q)| \u2223\u2223+ |D0(Q)|\nHence, this measure is optimized by queries Q for which the number of leading diagnoses predicting the positive answer is equal to the number of leading diagnoses predicting the negative answer and for which D0(Q) is the empty set.\nRisk-Optimized Query Selection. For scenarios where a-priori probabilities are vague, we have presented another more complex query selection measure RIO() in [RSFF13] which uses a reinforcement learning strategy to constantly adapt some \u201crisk\u201d parameter that indicates the current amount of trust in the probabilities. Whereas ENT() and SPL() do not rely on qData, this learning strategy does so and requires the invalidation rate or \u201cperformance\u201d, i.e. |Dout||DX| , of the previous iteration for the adaptation of the learning parameter. As long as the invalidation rate is \u201cgood\u201d, the trust in the current (a-posteriori) probabilities \u2013 that strongly depend on the vague a-priori probabilities \u2013 is high, but it is gradually decreased after observing \u201cworse\u201d performance, and so on. High trust in the probabilities means usage of ENT() which can exploit high quality fault information well as demonstrated in the experiments conducted in [SFFR12], whereas low trust involves selection of queries that guarantee a higher worst case invalidation rate, i.e. have similar properties to queries SPL() would select.\nExample 9.1 Let us reconsider the queries and associated q-partitions for the example DPI of Table 15.2 that are depicted by Table 8.3 on page 113. Let us denote by Qi \u227aM Qj that Qi is preferred over Qj and by Qi \u227a M Qj that Qi is equally preferable as Qj if the query selection measure qsm() := M is used. Furthermore, we make the assumption that the probability distribution pD of the (leading) diagnoses DX = {D1, . . . ,D4} is as shown in Table 9.1.\nThen, we make the following observations:\n\u2022 Q6 is the theoretically optimal query w.r.t. ENT() since pD(D+(Q6)) = 0.5, pD(D\u2212(Q6)) = 0.5 and D0(Q6) = \u2205, i.e. the positive and the negative answer have equal probabilities of 50% and thus Q6 the highest theoretically possible information gain of 1 (bit). This can be compared with one toss of a coin where the information gain of tossing the coin and checking whether it is head or tail is highest in a case where the coin is fair. For a coin that shows head with a probability of 0.95, conversely, the information gain of tossing the coin is rather small since we are already quite sure about the result in advance.\n136 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nD \u2208 DX D1 D2 D3 D4 pD(D) 0.15 0.3 0.05 0.5"}, {"heading": "9.4 Interactive Debugging Algorithm: Correctness and Complexity", "text": "First, we prove the correctness of Proposition 9.1 on page 124 by using the results of Sections 11.4 and 12.4.10 which provide evidence for the correctness (soundness, completeness and optimality) of methods STATICHS and DYNAMICHS:\n9.4. ALGORITHM CORRECTNESS AND COMPLEXITY 137\nProof of Proposition 9.1. First, we argue why Algorithm 5 must terminate. The function GETFORMULAPROBS in line 5 terminates since it applies Formulas 4.2 and 4.7 |K| times and |K| is finite by Definition 3.1. If mode = static, then STATICHS terminates due to Proposition 11.1. If mode = dynamic, then DYNAMICHS terminates due to Corollary 12.8. GETPROBDIST terminates since (1) the number of already answered queries |QA| is finite, (2) |DX| is finite since diagnoses are subsets of K and thus there is only a finite number of (minimal) diagnoses w.r.t. any DPI according to Definition 3.1 (since all sets included in the DPI are finite) and (3) reasoning (GETENTAILMENTS and ISKBVALID) is assumed to be decidable for the logic L over which the DPI is formulated as per Chapter 2. Further, GETMODE clearly terminates due to the fact that |DX| is finite and returns the mode Dmax of the diagnoses probability distribution pD() over the diagnoses in DX. Now, if the stop criterion pD(Dmax) \u2265 1 \u2212 \u03c3 is met, then GETSOLKB is called. GETSOLKB simply deletes the given diagnosis Dmax from the given KB K and adds a finite set of formulas to it, and thence terminates.\nIf the stop criterion is not met, then |DX| \u2265 2 must hold as otherwise the single diagnosis D \u2208 DX would necessarily have fulfilled the stop criterion as its probability as per any probability measure over the sample space \u2126 := DX must be equal to 1 and thus greater than or equal to 1\u2212 \u03c3 where \u03c3 \u2265 0.\nDue to |DX| \u2265 2, Proposition 8.10 implies that GETPOOLOFQUERIES (called within CALCQUERY) terminates and yields a non-empty query pool as output. SELECTBESTQUERY (also called within CALCQUERY) terminates as well since it simply selects one query from the pool according to the measure qsm() (cf. Section 9.3). Since we assume the interacting user to answer to a query or to reject it within finite time, u(Q) also terminates. It is clear that APPEND terminates. GETINVALIDDIAGS simply extracts one entry of the given q-partition and thus terminates. Finally, UPDATEQDATA also terminates by assumption (no qsm() must be used for which UPDATEQDATA might not terminate). As a consequence, all functions called in Algorithm 5 terminate. What remains to be proven is that the stop criterion must be met after a finite number of iterations, i.e. after a finite number of test cases have been added to the input DPI.\nIn mode = static the stop criterion must be satisfied after a finite number of iterations due to the following argumentation:\n\u2022 There is a finite set of minimal diagnoses w.r.t. the input DPI \u3008K,B,P ,N \u3009R since each (minimal) diagnosis w.r.t. this DPI is a subset of K according to Definition 3.5 and since |K| is finite by Definition 3.1.\n\u2022 In each iteration, one test case is added either to P \u2032 or N \u2032.\n\u2022 Each test case added to whatever set P \u2032 or N \u2032 invalidates at least one minimal diagnosis w.r.t. the input DPI in the set DX by the definition of a query (Definition 7.1) and since each query is computed w.r.t. the leading diagnoses DX by the correctness of GETPOOLOFQUERIES (cf. Proposition 8.10).\n\u2022 DX contains only minimal diagnoses w.r.t. the input DPI by Proposition 11.1.\n\u2022 Also by Proposition 11.1, no invalidated minimal diagnosis w.r.t. the input DPI can be an element of some subsequent set of leading diagnoses DX.\n\u2022 Therefore, unless the stop criterion is met before due to a sufficiently high probability of one of multiple leading diagnoses as per pD(), Algorithm 5 inmode = staticmust arrive at a point where |DX| = 1 after a finite number of iterations. Note that |DX| = 0 is impossible due to the definition of a query (Definition 7.1) which ensures that each added test case leaves valid at least one minimal diagnosis in DX.\nAlgorithm 5 terminates in mode = dynamic since for any sequence QA of queries that are added to the positive or negative test cases P \u2032 or N \u2032, respectively, there is a finite number kQA such that there is no\n138 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nmore than one minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R for |P \u2032| + |N \u2032| = kQA wherefore the stop criterion must be met. Now, let us assume that the opposite holds. That is, there is a sequence QA\u2217 of queries that are added to the positive or negative test cases P \u2032 or N \u2032, respectively, and for all natural numbers k there is more than one minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R for |P \u2032|+ |N \u2032| = k. Then we argue as follows to derive a contradiction:\n\u2022 There is a finite set of (minimal) diagnoses w.r.t. any DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R obtained from the input DPI by the addition of test cases. This is true since |K| is finite by Definition 3.1 and since each (minimal) diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R is a subset of K according to Definition 3.5.\n\u2022 In each iteration, one test case is added either to P \u2032 or N \u2032.\n\u2022 Each test case added to whatever set P \u2032 or N \u2032 invalidates at least one minimal diagnosis w.r.t. the current DPI in the set DX by the definition of a query (Definition 7.1) and since each query is computed w.r.t. the leading diagnoses DX by the correctness of GETPOOLOFQUERIES (cf. Proposition 8.10).\n\u2022 If DPI denotes the current DPI at the time DYNAMICHS is called, then the set DX returned by DYNAMICHS is a subset of or equal to mDDPI , i.e. DX contains only minimal diagnoses w.r.t. DPI by Corollary 12.8.\n\u2022 Let \u3008DPI0, DPI1, . . . \u3009 denote the sequence of DPIs encountered in the case of adding answered queries as test cases to the input DPI DPI0 as per QA\u2217. Further, let \u3008aD0,aD1, . . . \u3009 be the sequence such that aDi := aDDPIi , i = 0, 1, . . . , i.e. aDi is the set of all diagnoses w.r.t. DPIi. Then aDi \u2283 aDi+1 for all i \u2265 0 due to Corollary 12.4.\n\u2022 As each query added as a test case to DPIi leaves valid at least one (minimal) diagnosis w.r.t. DPIi due to Definition 7.1, we have that aDk \u2283 \u2205 for k = 0, 1, . . . .\n\u2022 Since aDi is finite, there must be some finite number k\u2217 such that |aDk\u2217 | = 1 wherefore |mDk\u2217 | = 1 must also be valid. This is a contradiction.\nThence, Algorithm 5 terminates in any mode mode. Now, we show that propositions (1)-(6) of Proposition 9.1 hold for (i) mode = static and (ii) mode = dynamic.\n(i): First, by the proof so far, we have that Algorithm 5 in mode = static given the input DPI \u3008K,B,P ,N \u3009R terminates. Since the only point where the algorithm can terminate is line 14, GETSOLKB is called with arguments \u3008Dmax, \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R,P \u2032, static\u3009. By the definition of GETSOLKB (see Section 9.2.4), we have that (K \\ Dmax) \u222a UP is returned by the algorithm.\nPropositions (1) and (2) follow from the specification of the GETMODE function which is called with arguments \u3008DX, pD()\u3009. Proposition (3) is true since GETSOLKB can never be reached without pD(Dmax) \u2265 1 \u2212 \u03c3 being fulfilled. DX \u2286 mD\u3008K,B,P,N \u3009R \u2229 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is true due to Proposition 11.1, Remark 9.5 and the fact that DX is obtained as an output of STATICHS. Hence, Proposition (4) holds. Proposition (5) is implied by Remark 9.5 and by the specification of the GETFORMULAPROBS function which computes pK() from pK\u0303\u222aK() as per Formulas 4.2 and 4.7 in line 5. Finally, Proposition (6) is a consequence of the definition of the GETPROBDIST function which accounts for the computation of pD() from pK(), the input DPI, DX and the chronological sequence of all queries and associated answers QA so far. Therefore, Proposition 9.1 is true for mode = static.\n(ii): First, by the proof so far, we have that Algorithm 5 in mode = dynamic given the input DPI \u3008K,B,P ,N \u3009R terminates. Since the only point where the algorithm can terminate is line 14, GETSOLKB is called with arguments \u3008Dmax, \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R,P \u2032, dynamic\u3009. By the definition of GETSOLKB (see Section 9.2.4), we have that (K \\ Dmax) \u222a UP\u222aP \u2032 is returned by the algorithm.\n9.4. ALGORITHM CORRECTNESS AND COMPLEXITY 139\nPropositions (1) and (2) follow from the specification of the GETMODE function which is called with arguments \u3008DX, pD()\u3009. Proposition (3) is true since GETSOLKB can never be reached without pD(Dmax) \u2265 1 \u2212 \u03c3 being fulfilled. DX \u2286 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is true due to Corollary 12.8, Remark 9.5 and the fact that DX is obtained as an output of DYNAMICHS. Hence, Proposition (4) holds. Proposition (5) is implied by Remark 9.5 and by the specification of the GETFORMULAPROBS function which computes pK() from pK\u0303\u222aK() as per Formulas 4.2 and 4.7 in line 5. Finally, Proposition (6) is a consequence of the definition of the GETPROBDIST function which accounts for the computation of pD() from pK(), the input DPI, DX and the chronological sequence of all queries and associated answers QA so far. Therefore, Proposition 9.1 is true for mode = dynamic.\nNext, we show that the solution to Interactive Static KB Debugging is found for \u03c3 = 0 in case mode = static:\n(s1) DX \u2286mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R holds for the output of STATICHS in each iteration by Proposition 11.1. Therefore, DX comprises only minimal diagnoses w.r.t. the input DPI that comply with all specified test cases in P \u2032 and N \u2032.\n(s2) By pK\u0303\u222aK() : K\u0303 \u222a K \u2192 (0, 1] we derive by Formula 4.2 that each formula in K must have a probability greater than zero. Further, by Formula 4.7, no formula in K can have a probability greater than or equal to 0.5 (i.e. in particular a probability of 1 is not possible for a formula). Hence, we have that pK : K \u2192 (0, 0.5) for the measure pK() computed by GETFORMULAPROBS in line 5 in Algorithm 5. Thence, by the definition of pnodes() in STATICHS based on p() := pK() (cf. Definition 4.9 on page 73) due to the fact that pK() is given as an input argument to STATICHS in line 8, we have that no diagnosis can have an (a-priori) probability of zero. Since the function GETPROBDIST might only perform some multiplications of a diagnosis probability by 12 , also the a-posteriori probability of each diagnosis must be greater than zero.\n(s3) Hence, due to \u03c3 = 0, it must be necessarily be true that |DX| = 1 before the algorithm terminates.\n(s4) By Problem Definition 6.2 and the specification of the GETSOLKB function, the output solution KB must be the solution to Interactive Static KB Debugging.\nThat a solution found for \u03c3 > 0 in case mode = static might be an approximate solution to Interactive Static KB Debugging is a direct consequence of the definition of approximate solution given in Remark 9.2.\nFinally, the proof that the solution to Interactive Dynamic KB Debugging is found for \u03c3 = 0 in case mode = dynamic is analogue to the one for mode = static, just\n(d1) DX \u2286 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R holds for the output of DYNAMICHS in each iteration by Corollary 12.8. Therefore, DX comprises only minimal diagnoses w.r.t. the current DPI.\n(d2) By (s2), (s3), Problem Definition 6.1 and the specification of the GETSOLKB function, the output solution KB must be the solution to Interactive Dynamic KB Debugging.\nThat a solution found for \u03c3 > 0 in case mode = dynamic might be an approximate solution to Interactive Dynamic KB Debugging is a direct consequence of the definition of approximate solution given in Remark 9.2.\nThis completes the proof of Proposition 9.1.\nNext, we examine the complexity of Algorithm 5.26 To this end, we denote in the following by expensive operation a call of a (usually) expensive function such as one that internally consults a logical\n26Considerations in the rest of this section rely on the assumption that P 6= NP (cf. http://bit.ly/1lIuNcP).\n140 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nreasoner or another operation such as addition or multiplication that is the most time consuming algorithmic action within a certain part of an algorithm. We analyze Algorithm 5 in terms of the number num of expensive operations that are required during its execution in the worst case. The worst case time required by Algorithm 5 is then the multiplication of the maximal worst case time consumption of any expensive operation throughout the algorithm by num.\nThe next propositions assume |K| as an upper bound of |P \u2032| + |N \u2032|. This is plausible in the light of evaluations performed in e.g. [SFFR12, RSFF13] which substantiate that usually the size of the faulty KB exceeds the number of queries that are necessary to solve the interactive debugging problem by several orders of magnitude.\nWe first investigate the complexity of the function GETPROBDIST which is called once in each iteration of Algorithm 5:\nProposition 9.2. Let |K| be an upper bound of |P \u2032| + |N \u2032|. Then, the function GETPROBDIST in Algorithm 5 requires a number of expensive operations that is linear in |K|.\nProof. The time complexity of GETPROBDIST can be assessed by adding the complexities of (i) GETPRIODIAGPROBS, (ii) the for-loop between line 30 and 41, (iii) the summation in line 42 and (iv) the for-loop in lines 43 and 44. Time complexity of (i) is in O(nmax |K|) since |DX| \u2264 nmax where nmax is a predefined constant and |K| \u2212 1 multiplications must be conducted per diagnosis in DX. (ii) requires |QA| |DX| \u2264 (|P \u2032| + |N \u2032|)nmax \u2264 |K|nmax many calls to functions GETENTAILMENTS and ISKBVALID, respectively, that internally call a logic reasoner. Time requirements of (iii) amount to O(|DX|) = O(nmax) summations. Finally, (iv) involves O(nmax) multiplications.\nThus, we obtain an overall time complexity of O(nmax |K| + nmax |K| + nmax + nmax) = O(|K|) for GETPROBDIST.\nThe next proposition is based on this result and witnesses that Algorithm 5 requires only a quadratic number of expensive operations in the size of the KB K.\nProposition 9.3. Let |K| be an upper bound of |P \u2032| + |N \u2032| and let the function qsm() given as input to Algorithm 5 be such that the time complexity of UPDATEQDATA is in O(|K|). Minus the time consumed by diagnosis computation (by STATICHS in case of mode = static or by DYNAMICHS otherwise), the time complexity in terms of number of required expensive operations of Algorithm 5 is quadratic in |K|.\nProof. Variable instatiation (lines 1-4) and variable update (lines 18-26) is in O(1) where some query selection measure qsm() is supposed to be used, for which the time complexity of UPDATEQDATA is in O(|K|) (this holds for all query selection measures described in Section 9.3). GETFORMULAPROBS called in line 5 runs in O(|K| |axmax|) as Formula 4.2 is applied once to each formula in K for each of which at most |axmax| multiplications are performed where |axmax| is the maximum size of a formula in K in terms of included syntactical elements (multiple occurrences of one and the same symbol are counted multiply). As shown by Proposition 9.2, the complexity of GETPROBDIST called in line 11 is in O(|K|). Execution of GETMODE needs one iteration over all diagnoses in DX in order to determine the one with maximum probability, i.e. it runs in O(|nmax|) = O(1) time since nmax is a constant. Next, GETSOLKB which computes a solution KB from a given diagnosis D works in O(|D|+ |P |+ |P \u2032|) \u2286 O(|K|) since |D| elements need to be deleted from a set of cardinality K which can be accomplished in constant time per element (e.g., using a hashtable) and additionally at most |P |+ |P \u2032| set union operations are required, namely the union of (K \\ D) with UP\u222aP \u2032 where the latter needs |P | + |P \u2032| \u2212 1 set union operations. As |P | is a constant c, O(|D| + |P | + |P \u2032|) \u2286 O(2c |K|) \u2286 O(|K|). In Section 8.5, we have already underlined that GETPOOLOFQUERIES is a fixed parameter tractable problem, i.e. it requires\nO nmax + \u2223\u2223\u2223Q(max)min \u2223\u2223\u2223 log \u2223\u2223Q(max)\u2223\u2223\u2223\u2223\u2223Q(max)min \u2223\u2223\u2223  2nmax  = O(1)\n9.4. ALGORITHM CORRECTNESS AND COMPLEXITY 141\ncalls to a reasoner in the worst case (cf. Proposition 8.9). Similarly, SELECTQUERY involves O(2nmax) comparisons qsm(Qi) < qsm(Qj) for Qi, Qj \u2208 QP since the cardinality of the computed query pool is in O(2nmax). The latter holds due to Proposition 8.10 which substantiates that the calculated query pool includes at most one query Q for which D+(Q) = Y for each Y \u2282 DX. And, an upper bound for the cardinality of DX is the constant nmax. Therefore, the runtime of SELECTQUERY is in O(1), too.\nSince adding up a number of time complexities each of which is at most in O(|K|), we can conclude that the runtime of one iteration of Algorithm 5 minus the time needed for diagnosis computation is also in O(|K|), i.e. linear in |K| in terms of number of expensive operations needed. As there might be a maximum of |K| iterations by the premise that |P \u2032| + |N \u2032| \u2264 |K|, we obtain an overall time complexity \u2013 minus the complexity of diagnoses computation \u2013 of O(|K|2) for Algorithm 5.\nThat is, Algorithm 5 requires only a quadratic number of expensive operations \u201coutside\u201d of the methods STATICHS or DYNAMICHS, respectively, that account for diagnosis computation. That the substantial complexity of Algorithm 5 lies in the computation of diagnoses, is confirmed by the following results.\nThe first result is based on the fact that determining minimal diagnoses w.r.t. a DPI is an MBD problem (cf. page 8) which in turn can be regarded as an abduction problem as defined in [BATJ91]. More precisely, the problem of detecting minimal diagnoses w.r.t. a DPI is a monotonic abduction problem [BATJ91]. Hence, the following proposition holds [BATJ91, Theorem 4.3]:\nProposition 9.4. Let \u3008K,B,P ,N \u3009R be a DPI over L and let ISKBVALID (see Algorithm 1) be a function computable for L in polynomial time w.r.t. the size of \u3008K,B,P ,N \u3009R (cf. the description of the function e in [BATJ91, Section 3.3]). Then, given a set D of minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R such that \u2205 \u2282 D, it is NP-complete to determine whether there is a minimal diagnosis D w.r.t. \u3008K,B,P ,N \u3009R such that D /\u2208 D.\nRemark 9.11 The function ISKBVALID in the case of KB debugging is analogue to the function e used in [BATJ91]. Given the overall data Dall that must be explained by a solution to an abduction problem, the function e computes for a subset H of Hall, the set of all individual hypotheses, the set e(H) = D where D \u2286 Dall is the data explained by H . H is an explanation of the abduction problem iff it is set-minimal and e(H) = Dall [BATJ91].\nIn the case of our KB debugging system, given a DPI \u3008K,B,P ,N \u3009R,Dall corresponds to the set of all requirements in R and all test cases in N violated byK\u222aB\u222aUP . Hall corresponds toK. So, e corresponds to ISKBVALID since ISKBVALID is given some K \\D and \u3008\u00b7,B,P ,N \u3009R (where D corresponds to some H \u2286 Hall) and checks whether (K \\ D) \u222a B \u222a UP does not violate any requirement or test case, i.e. whether e(H) = Dall. Notice that ISKBVALID can easily be slightly modified to return the subset of Dall that is explained by H , i.e. the subset of the initially violated requirements and test cases that are resolved by deletion ofD fromK\u222aB\u222aUP . To this end, the early termination in case of detected invalidity must simply be omitted.\nRemark 9.12 An abduction problem is monotonic [BATJ91] iff for all H,H \u2032 \u2286 Hall it holds that H \u2286 H \u2032 \u2192 e(H) \u2286 e(H \u2032). That parsimonious KB debugging (or the problems given by Problem Definitions 3.2, 6.2, 6.1, 9.2 and 9.1) seen as an abduction problem is indeed monotonic is a simple consequence of the monotonicity of the logic L over which a DPI must be defined (as per the postulations of Chapter 2). For, if (K \\ D\u2032) \u222a B \u222a UP |= x, then also (K \\ D) \u222a B \u222a UP |= x for D \u2286 D\u2032. Modeling requirements r \u2208 R as unwanted entailments of the correct KB (see Remark 3.2), we immediately see that D cannot resolve more unwanted entailments x \u2208 R \u222a N than D\u2032. Thence, parsimonious KB debugging is a monotonic abduction problem.\nUnfortunately, ISKBVALID is not tractable (i.e. computable in polynomial time) for many logics L. In particular, it is already in \u2206P2 = P NP for PL (cf. the polynomial hierarchy defined by [MS72]). This\n142 CHAPTER 9. INTERACTIVE KB DEBUGGING ALGORITHM\nholds since propositional satisfiability checking is NP-complete [Coo71, Kar72] and since ISKBVALID, in order to to check the validity (see Definition 3.3) of a set of PL formulas X w.r.t. some PL DPI \u3008\u00b7,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, requires a polynomial number of calls to a propositional satisfiability checker AlgSAT. For, by the definition of ISKBVALID (see Algorithm 1), one call ofAlgSAT is required for testing whether X \u222a B \u222a UP\u222aP \u2032 is consistent and a maximum of |N | + |N \u2032| further calls are needed to verify whether X \u222a B \u222a UP\u222aP \u2032 \u222a {\u00acn} is consistent for all n \u2208 N \u222a N \u2032, i.e. whether X \u222a B \u222a UP\u222aP \u2032 6|= n for all n \u2208 N \u222a N \u2032 (note that \u00acn refers to the formula \u00acax 1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 \u00acaxk if n := {ax 1, . . . , axk}, cf. page 27). Since we assume |P \u2032| + |N \u2032| \u2264 |K| and since |N | is a constant throughout the execution of Algorithm 5, we have that the number |N |+ |N \u2032|+ 1 \u2264 |N |+ |K|+ 1 of calls to AlgSAT performed by ISKBVALID is bounded by a polynomial in |K|.\nAs a conclusion of this discussion and Proposition 9.4, we have:\nCorollary 9.1. Let \u3008K,B,P ,N \u3009R be a PL DPI given as an input to Algorithm 5. Then, each call of STATICHS or DYNAMICHS within Algorithm 5 must solve (at least) an NP-complete problem by means of an oracle that requires a polynomial number of calls to another NP-complete oracle.\nProof. Both STATICHS and DYNAMICHS must return a set of at least nmin \u2265 2 minimal diagnoses each time they are called (given that nmin minimal diagnoses exist w.r.t. the given DPI) due to the specification of input parameter nmin in Algorithm 5 and the calls of STATICHS and DYNAMICHS in lines 8 and 10, respectively. For the first call, this implies that at least two minimal diagnoses must be found. Hence, Proposition 9.4 applies to the complexity of finding the second minimal diagnosis during the execution of the first call of both STATICHS and DYNAMICHS, just that ISKBVALID does not terminate in polynomial time, but uses a polynomial number of calls to an NP-complete oracle (the propositional satisfiability checker).\nIn each subsequent call of any of the two methods STATICHS and DYNAMICHS, the existing set of leading diagnoses will contain at least one minimal diagnosis w.r.t. the current DPI (since each query leaves valid at least one leading diagnosis, cf. Definition 7.1), and at least one further minimal diagnosis w.r.t. this DPI must be extracted (cf. bullet (aii) in the characterization of the outputs of STATICHS and DYNAMICHS on page 128 ff.). Thus, Proposition 9.4 holds for the computation of the first diagnosis in any subsequent call of any of the two functions, just that ISKBVALID does not terminate in polynomial time, but uses a polynomial number of calls to an NP-complete oracle (the propositional satisfiability checker).\nThe general complexity of ISKBVALID is even worse if DPIs over more expressive logics such as OWL 2 are considered for which one single call of a reasoner invoked by ISKBVALID is already 2- NEXPTIME-complete [GHM+08, Kaz08].\nHowever, in spite of these discouraging theoretical complexity results, debugging techniques similar to the ones discussed in this work have proven to perform reasonably in practice for many real-world KB debugging problems over DL and OWL languages, respectively [SFFR12, RSFF13, SFRF14c] which are more expressive than PL. For instance, we have shown in [SFFR12] that faulty real-world OWL KBs with sizes of up to over 33000 formulas are efficiently interactively debuggable with similar methods as those presented in this work (reaction time of the system, i.e. time between two successive queries: only 1 minute; average query length: not more than 4 formulas; overall number of queries: at most 14). Moreover, we have demonstrated in [RSFF13] that a pair of real-world OWL KBs (the first including over 11000 formulas, the second almost 5000) that has been automatically integrated by diverse ontology matching systems resulting in a faulty aligned KB (see Chapter 32 for details; we also list some matching systems there) can be debugged with absolutely reasonable time and query answering effort for the interacting user. In concrete terms, the RIO debugging strategy proposed in [RSFF13] (which can also be plugged in as a query selection measure into the system described in this work, see Section 9.3) involved an average reaction time of no more than 13 seconds and required an average number of queries to be answered by the user of no more than nine.\nChapter 10\nSummary\nIn this part we dealt with how the process of KB debugging can be designed so as to enable a (group of) user(s) to interact with the debugging software in order to achieve high quality solutions. We defined the problem of interactive static KB debugging as well as the problem of interactive dynamic KB debugging which \u201cnaturally\u201d arise from the fact that the DPI in interactive KB debugging is always renewed after a new test case has been specified (a new query has been answered). The former problem searches for a solution KB w.r.t. the original DPI given as input such that this solution KB satisfies all test cases added during the debugging session and there is no other such solution KB. The latter problem searches for a solution KB w.r.t. the current DPI (i.e. the original DPI including all new test cases added throughout the debugging session so far) such that there is no other solution KB w.r.t. the current DPI.\nWe specified the pivotal notion of a query which constitutes the \u201cinterface\u201d between the debugging system and the interacting user. Queries are sets of logical formulas satisfying the search space restriction as well as the solution preservation property. That is, incorporation of any answer to a particular query into the debugging process leads to a reduction of the search space for solutions on the one hand, but guarantees the existence of at least one remaining solution on the other hand. Queries are generated from a set of leading diagnoses that act as a representative of all (minimal) diagnoses. We established that, for any set of at least two leading diagnoses, a query exists. The unique q-partition of a query constitutes the relationship between a query and the set of leading diagnoses and can be used to decide for a set of logical formulas whether this set is or is not a query. Furthermore, the q-partition can be used to estimate the impact of a query answer on the (distribution of the) set of solutions and thence can be exploited to assess the (expected) quality of different queries which in turn can help to filter out a suitable query among a pool of possible queries.\nIt was also presented how a pool of queries can be generated for a given set of leading diagnoses and a DPI. We showed how to minimize these queries in terms of the included number of logical formulas the aim of which is to strain the user(s) as little as possible when it comes to answering them. Moreover, we pointed out that query generation is a fixed parameter tractable problem due to the fact that the (maximum) number of leading diagnoses can be predefined and therefore constitutes a constant value (which is not growing as the diagnosis problem instance grows). We featured an in-depth discussion of the properties of the query generation algorithm, in the course of which we detected several drawbacks. The gave a hint to potential solutions that we will address in our future work. Additionally, we formally proved the correctness of the query generation method and derived complexity results. All of this was concretized by means of several illustrating examples.\nFinally, we explicated the central algorithm of this work which implements an interactive KB debugging system. First, an overview of the workflow of interactive KB debugging was given, followed by a more comprehensive detailed specification of the algorithm. Some query selection measures (all of\n143\n144 CHAPTER 10. SUMMARY\nwhich are later covered in more depth in Parts IV and V) were discussed and optimization versions of the problems of interactive dynamic and static KB debugging were defined where the goal is to obtain the solution to these problems by asking the user a minimal number of queries. Finally, we formally proved the correctness of the interactive KB debugging algorithm and gave a discussion of its complexity.\nPart III\nIterative Diagnosis Computation\n145\n147\nIn this part we introduce and discuss two methods, STATICHS and DYNAMICHS, which are called in lines 8 and 10 of Algorithm 5, respectively. The former provides a method for solving the Interactive Static KB Debugging Problem (Problem Definition 6.2) whereas the latter aims at solving the Interactive Dynamic KB Debugging Problem (Problem Definition 6.1). Both are methods for iterative diagnosis computation that are employed to compute a set of leading diagnoses in each iteration of the presented interactive KB debugging algorithm (Algorithm 5). Each time a query has been answered by the interacting user and added to the respective set of test cases of the DPI, a subset of the leading diagnoses (and usually also a set of not-yet-computed minimal diagnoses) is invalidated. An iterative diagnosis computation method is then invoked to update the leading diagnoses set taking the new information into account that is given by the recently added test case. That is, the k \u2264 nmax most probable ways of solving the Interactive Static (Dynamic) KB Debugging Problem in the light of the new evidence are extracted by STATICHS (DYNAMICHS) after the search space has been suitably pruned. In this vein, if there is only one solution left, the (exact) solution of Interactive Static (Dynamic) KB Debugging has been found.\nChapter 11 provides an in-depth description of the static method and proves its correctness. Chapter 12 details the dynamic method and demonstrates its correctness. The practically oriented reader or the one that is willing to believe that the presented iterative diagnosis computation techniques in fact work as claimed might skip Sections 11.4 as well as 12.4 in this part.27\n27Parts of Part III already appeared in [Rod15]. However, [Rod15] includes a significantly less detailed presentation of the algorithms and does not give any proofs of correctness.\nChapter 11\nSTATICHS: A Static Iterative Diagnosis Computation Algorithm\nAs the name already suggests, STATICHS (Algorithm 7) is a procedure that solves the problem of Interactive Static KB Debugging defined by Problem Definition 6.2 if used for leading diagnosis computation in Algorithm 5. STATICHS is sound, complete and optimal w.r.t. the set of solutions of the Interactive Static KB Debugging problem (this will be proven in Section 11.4). Optimality refers to the best-first computation of minimal diagnoses regarding a given probability measure."}, {"heading": "11.1 Overview and Intuition", "text": "The STATICHS algorithm is strongly related to the non-interactive hitting set algorithm HS (see Algorithm 2) in that, at any stage during the execution of Algorithm 5, the hitting set tree produced by STATICHS corresponds to some part of the complete (non-interactive) wpHS-tree built-up by Algorithm 2. This is achieved by the strategy to use new test cases only for the invalidation of diagnoses, and not for the computation of conflict sets (and thus diagnoses). That is, all minimal conflict sets are computed w.r.t. the input DPI. Thereby, the introduction of new diagnoses, i.e. ones that are not minimal diagnoses w.r.t. the input DPI, through addition of new test cases to the DPI is prohibited (cf. Proposition 4.6).\nSo, what STATICHS as a subroutine of Algorithm 5 does is gradually building up the standard (noninteractive) wpHS-tree in multiple phases. During each phase some new (not-yet-computed) minimal diagnoses w.r.t. the input DPI are computed, in the order of their probability, most probable ones first. Before such a newly detected minimal diagnosis is added to the set of leading diagnoses (Dcalc \u222aDX), a test is performed that verifies that this new diagnosis is consistent with all test cases added to the input DPI so far. In this vein, all answered queries so far not only serve to eliminate a subset of the set of leading diagnoses at the time when the respective query is answered, but also to eliminate incompatible minimal diagnoses w.r.t. the input DPI that are found at some later point in time. However, in order to be eliminated due to a specified test case, a minimal diagnosis must first be computed. That is, no partial diagnoses can be eliminated due to newly specified test cases.\nBetween each two phases of tree construction, a query computed on the basis of the current set of leading diagnoses is asked to the user (this is accomplished directly in Algorithm 5). After incorporating the user\u2019s answer, some leading diagnoses are eliminated (this is granted by the definition of a query, see Definition 7.1). Moreover, the \u201cstate\u201d of the tree is maintained during the execution of Algorithm 5 until STATICHS is again called in order to calculate further leading diagnoses. The state of the current partial\n149\n150 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\nwpHS-tree is stored by variables\n\u2022 Dcalc \u222a DX \u2013 computed minimal diagnoses w.r.t. the input DPI consistent with all test cases specified so far,\n\u2022 Q \u2013 the list of open, non-labeled nodes,\n\u2022 Ccalc \u2013 minimal conflict sets w.r.t. the input DPI computed so far and\n\u2022 D\u00d7 \u2013 computed minimal diagnoses w.r.t. the input DPI not consistent with all test cases specified so far.\nEach time a tree construction phase, i.e. the computation of new leading diagnoses, is finished, a new diagnosis probability distribution is obtained by the diagnosis probability update as per Bayes\u2019 Theorem described in Section 9.2. Once this distribution involves one highly probable diagnosis (the probability of which exceeds a predefined threshold 1 \u2212 \u03c3) and else just highly improbable ones, the algorithm terminates. The output is a solution KB w.r.t. the input DPI built from this highly probable minimal diagnosis.\nRemark 11.1 In case \u03c3 has a predefined value of zero, the output is the (exact) solution to the problem of Interactive Static KB Debugging for the input DPI. In a scenario where some fault tolerance \u03c3 > 0 is given, the solution KB returned by Algorithm 5 is an approximation of the (exact) solution to Interactive Static KB Debugging for the input DPI where a better approximation can be expected for smaller values of \u03c3 (cf. Remark 9.2). \u201cBetter\u201d in this context refers to the satisfaction of desired semantic properties of the KB returned by Algorithm 5, i.e. desired entailments and desired non-entailments of the KB. The intuition is that the specification of additional test cases T guarantees the output of a KB complying with these test cases, whereas accepting one \u2013 albeit highly probable \u2013 of multiple solution KBs without having incorporated T leaves open the possibility for this KB to not fulfill T .\nHowever, answering queries is effort for an interacting user. Therefore, the approach that involves the \u201cearly\u201d termination of the algorithm after a solution KB has a sufficiently high probability (lower than 1) constitutes a trade-off between exactness of the output and the effort of the user and overall execution time of the interactive KB debugging algorithm, respectively.\nConstant \u201cConvergence\u201d towards the Solution. As said, each added test case is an answered query and thus eliminates at least one minimal diagnosis w.r.t. the input DPI. And, only minimal diagnoses w.r.t. the input DPI are computed by STATICHS. Hence, by the fact that a solution to Interactive Static KB Debugging can only be constructed from a minimal diagnosis w.r.t. the input DPI, it is guaranteed that the number of solutions to Interactive Static KB Debugging is strictly monotonically decreasing throughout the execution of Algorithm 5. That is, the initial number of (all) minimal diagnoses (w.r.t. the input DPI) is \u201cstatic\u201d which means that no \u201cnew\u201d minimal diagnoses can be introduced when the input DPI is extended by new test cases.\nAs a consequence of this, it is reasonable to employ STATICHS in a situation where the (complete) wpHS-tree produced by the standard (non-interactive) algorithm HS is believed to be as compact as to fit into the available system memory. In this case, STATICHS is also guaranteed to not exceed the available memory, even if an exact solution (\u03c3 = 0) is intended.\nUnfortunately, however, it will be generally the case that a complete enumeration of all minimal diagnoses is intractable, especially due to an overwhelming space complexity. In such a case, Algorithm 5 using STATICHS will definitely run out of memory (given that STATICHS is called sufficiently often). The reason is that the space consumption of STATICHS will sooner or later definitely reach the huge extent of the wpHS-tree produced by HS. Nevertheless, STATICHS might be used to (possibly) find some (approximate) solution. This might work in a scenario where the given probabilistic information in terms\n11.2. ALGORITHM WALKTHROUGH 151\nof pK\u0303\u222aK() provided as an input to Algorithm 5 is \u201creasonable\u201d in that the desired diagnosis is assigned a rather high probability and is thus figured out early, before the available memory is exhausted.\nA possible modification of the stop criterion in STATICHS in a way that new leading diagnoses are not computed until a desired number of such is detected or a timeout is reached, but rather until a predefined maximum space is consumed, would not mitigate space complexity issues very much. An explanation for this is that stopping STATICHS on account of no more available memory implies that no further call of STATICHS will be able to execute. That is because, as mentioned before, an added test case can only invalidate already computed diagnoses, no other branches in the wpHS-tree, and each invalidated minimal diagnosis cannot be discarded, but must be stored (in D\u00d7) to avoid the usage of leading diagnoses that are non-minimal w.r.t. the input DPI (cf. lines 21-23 in Algorithm 7).\nPoor Search Tree Pruning. As we explained before, the preservation of a constantly shrinking set of minimal diagnoses comes at the cost of being able to exploit new test cases only partially, i.e. only for the invalidation of already computed minimal diagnoses w.r.t. the input DPI and not for the computation of minimal conflict sets and thus minimal diagnoses. The incorporation of test cases into the DPI that is used to determine minimal conflict sets (line 30 in Algorithm 7) could, on the one hand, lead to new minimal conflict sets that are no minimal conflict sets w.r.t. the input DPI. As a consequence of this, minimal diagnoses might be determined by the algorithm which are no minimal diagnoses w.r.t. the input DPI, but w.r.t. the current DPI. Hence, the soundness of STATICHS w.r.t. the set of solutions of the Interactive Static KB Debugging problem would be violated. Furthermore, such conflict sets could lead to the missing of some minimal diagnoses w.r.t. the input DPI, a violation of the completeness of STATICHS w.r.t. the set of solutions of the Interactive Static KB Debugging problem.\nOn the other hand, the exploitation of new test cases for conflict set generation might give rise to the possibility of pre-pruning of any tree branches, not just branches that already correspond to diagnoses w.r.t. the input DPI. Such a \u201cdynamic\u201d strategy which exploits the new information given by a test case not just partially, but for the invalidation and computation of diagnoses and conflict sets, will be implemented be DYNAMICHS which we will detail in Chapter 12.\nPut another way, in STATICHS only the standard pruning rules for the construction of a wpHS-tree are applicable, namely the deletion of duplicate nodes and the elimination of non-minimal diagnoses (cf. Definition 4.10). Newly defined test cases only facilitate the deletion of tree branches from the leading diagnoses set Dcalc \u222a DX, but not from memory (as invalidated minimal diagnoses must be stored in D\u00d7, as pointed out before).\nTo summarize, STATICHS on the one hand makes sure to only consider relevant solutions of the problem of Interactive Static KB Debugging, but on the other hand suffers from this conservative strategy in that tree pruning cannot be designed very effectively. So, on the positive side, uncontrolled growth of the produced wpHS-tree can be avoided, but, on the negative side, consultation of an interacting user cannot be taken advantage of in terms of reduction of the space complexity of STATICHS compared to the construction of a wpHS-tree by a non-interactive procedure like Algorithm 2."}, {"heading": "11.2 Algorithm Walkthrough", "text": "Input Parameters. When STATICHS (Algorithm 7) is called for the first time in Algorithm 5, the inputs Ccalc, DX, D\u00d7, P \u2032 and N \u2032 correspond to the empty set and Q = [\u2205] (cf. lines 1-4 and 8 in Algorithm 5). Further on, Dcalc is defined to be the empty set at the beginning of each execution of STATICHS. That is, STATICHS starts the construction of the wpHS-tree from an initial tree consisting of a single unlabeled root node \u2205 (\u2208 Q). And, all collections that are later returned by STATICHS, except for Q, are initially empty. Further input arguments are the DPI \u3008K,B,P ,N \u3009R provided as an input to Algorithm 5, the sets of positively (P \u2032) and negatively (N \u2032) answered queries since the start of Algorithm 5, the leading\n152 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\ndiagnosis computation parameters nmin, nmax, t (see the description in Chapter 7 on page 95) and the probability measure p() := pK() that assigns a probability in the interval (0, 0.5) to each formula in K (cf. line 5 in Algorithm 5).\nThe Main Loop. During the repeat-loop, in each iteration the first node node in Q is processed (GETFIRST, line 5). That is, node is deleted from Q (DELETEFIRST, line 6) and the SLABEL function is called given node (i.a.) as a parameter. Notice that elements are added to Q (line 17) in a way that a sorting of Q in descending order according to pnodes() (cf. Definition 4.9) is maintained throughout the execution of STATICHS.\nComputation of a Node Label. The SLABEL function processes node as follows. First, the nonminimality criterion (lines 21-23) is checked. That is, among all nodes in D(\u00d7,X,calc) = D\u00d7\u222aDX\u222aDcalc one is searched which is a subset of node. If such a node nd is found, then node must be a non-minimal diagnosis (nd \u2282 node) or a duplicate diagnosis (nd = node) w.r.t. \u3008K,B,P ,N \u3009R since all sets D\u00d7, DX and Dcalc contain only minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R. In this case, the branch in the wpHS-tree corresponding to node can be dismissed which is taken account of by returning the label closed for node.\nIn case the non-minimality criterion is not satisfied, the duplicate criterion (lines 24-26) is checked next. Here, Q is browsed for a node that is equal to node. If such a one is found, node can be discarded because it suffices to consider only one tree branch among multiple tree branches in the wpHS-tree featuring one and the same set of edge labels. Hence, closed is returned as a label for node. Altogether, this means that only the last processed exemplar of a node corresponding to one and the same set of edge labels is labeled, all others are discarded.\nIf the duplicate criterion is not met, the reuse criterion (lines 27-29) is checked next. That is, Ccalc is browsed for a set C (Ccalc comprises only minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R) such that C and node are disjoint sets. If such a C is detected, then C can be used to label node since the set of edge labels along the path in the wpHS-tree leading from the root node to node does not hit C. In this case, the label C is returned for node by SLABEL.\nGiven that the reuse criterion fails, QX is called given the DPI \u3008K \\ node,B,P ,N \u3009R as an argument (line 30). If the output L is equal to \u2019no conflict\u2019, then we know by Proposition 4.9 that node is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R, wherefore the label valid is returned for node. Otherwise, the output L must be a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R that has an empty set-intersection with node. Since the reuse criterion failed, i.e. there is no set in Ccalc that does not intersect with node, L must be a fresh minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R in the sense that L /\u2208 Ccalc must hold. Therefore the label L is first added to Ccalc and then returned by SLABEL as a label for node.\nProcessing of a Node Label. Back in the main procedure, Ccalc is updated (line 8) and then the label L returned by the SLABEL function is processed as follows. If L = valid, then it is a fact that node is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R, but it is not certain that node also meets all positive test cases P \u2032 and all negative test cases N \u2032 that have been specified and added to \u3008K,B,P ,N \u3009R so far. Thus, according to Proposition 7.3, the validity of the KB K \\ node w.r.t. \u3008\u00b7,B,P \u222a P \u2032,N \u222a N \u2032\u3009R must still be checked (line 10). If successful, node is added to the set Dcalc of calculated minimal diagnoses w.r.t. the input DPI that comply with all answered queries so far. Otherwise, node is added to the set D\u00d7 of minimal diagnoses w.r.t. the input DPI that have been invalidated by some answered query.\nRoughly, the minimality of diagnoses added to Dcalc is assured by the pruning rule (lines 21-23) which eliminates non-minimal nodes and the fact that pnodes() sorts a node nd\u2032 corresponding to a superset of some node nd behind nd in Q.\nIf, on the other hand, L = closed is the label returned by SLABEL, then node must simply be removed from Q which has already been executed in line 6. Thence, no actions are necessary (cf. line 14).\n11.2. ALGORITHM WALKTHROUGH 153\nIn the third case, if a minimal conflict set L is returned by SLABEL, then L is a label for node meaning that |L| successor nodes of node, namely a node node \u222a {e} for all elements e \u2208 L, need to be added to Q in sorted order using the function pnodes() (INSERTSORTED, line 17).\nStop Criterion. The first criterion causing STATICHS to terminate is Q = [] which means that the complete wpHS-tree has been constructed and no further nodes can be labeled. In this case, Dcalc \u222aDX comprises all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R that are compliant with all the specified positive and negative test cases P \u2032 and N \u2032.\nIf the first criterion is not met, then the second criterion is checked. That is, a test is performed which checks whether the number of leading minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R in Dcalc \u222aDX amounts to at least nmin and either |Dcalc \u222a DX| = nmax or more than t time has passed since the start of the execution of STATICHS. In the latter case, nmin \u2264 |Dcalc \u222a DX| < nmax holds. In the former case, |Dcalc \u222aDX| = nmax is satisfied.\nProcessing of the Leading Diagnoses Returned by STATICHS. When a call of STATICHS in Algorithm 5 returns \u3008Dcalc \u222aDX,Q,Ccalc,D\u00d7\u3009, the set Dcalc \u222a DX is stored in the variable DX in Algorithm 5. Between two successive calls of STATICHS in Algorithm 5, only this set DX as well as D\u00d7 are modified. The list Q and the set Ccalc remain unchanged until they are used as input parameters to the next call of STATICHS in Algorithm 5.\nIn case one diagnosis Dmax of the current leading diagnoses in DX has a probability greater or equal 1\u2212\u03c3 as per the probability measure pD() (see Section 9.2), the stop criterion of interactive KB debugging is met and a solution KB w.r.t. \u3008K,B,P ,N \u3009R constructed from the input DPI \u3008K,B,P ,N \u3009R as well as from Dmax is returned to the user. Thereafter, Algorithm 5 terminates and no more calls of STATICHS take place.\nOtherwise, if no leading diagnosis satisfies the stop criterion, a query Q together with its q-partition P(Q) is computed, as was detailed in Chapter 8 and Section 9.2. An answer u(Q) to this query is submitted by the interacting user (line 17 in Algorithm 5). Then u(Q) along with P(Q) is exploited to figure out the subset Dout of DX that does not comply with u(Q). This set Dout is then deleted from DX and added to D\u00d7. Additionally, Q is added to the positive test cases P \u2032 if u(Q) = true and to the negative test cases N \u2032 otherwise. Subsequently, STATICHS is called again given\n\u2022 the updated parameters DX, D\u00d7, P \u2032 and N \u2032 (which are modified within and outside of STATICHS during the execution of Algorithm 5),\n\u2022 the unchanged parameters Q, Ccalc (which are modified only within STATICHS during the execution of Algorithm 5) and\n\u2022 the constant parameters \u3008K,B,P ,N \u3009R, t, nmin, nmax and pK() (which are not modified within or outside of STATICHS during the execution of Algorithm 5).\nThe execution of this next and any subsequent call to STATICHS runs in analogue way as described.\nRemark 11.2 We want to emphasize that queries are computed w.r.t. the current DPI \u3008K,B,P\u222aP \u2032,N \u222a N \u2032\u3009R although STATICHS focuses on solutions to the problem of Interactive Static KB Debugging which involves exclusively minimal diagnoses w.r.t. the input DPI \u3008K,B,P ,N \u3009R. However, a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R that satisfies all positive test cases P \u2032 as well as all negative test cases N \u2032 is also a minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R. And, a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R that does not satisfy all positive test cases P \u2032 as well as all negative test cases N \u2032 is not a minimal diagnosis w.r.t. \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R. These two facts are guaranteed by Proposition 12.5 that will be given on page 201.\nHence, it holds that\n154 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\n\u2022 D is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R that satisfies P \u2032 \u222a {Q} as well as N \u2032 if and only if D is a minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032 \u222a {Q} ,N \u222aN \u2032\u3009R and\n\u2022 D is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R that satisfies P \u2032 as well as N \u2032 \u222a {Q} if and only if D is a minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032 \u222a {Q}\u3009R.\nTherefore, each query constructed during Algorithm 5 with mode = static must be a query w.r.t. the current set of leading diagnoses DX and the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R (cf. Equation 7.1, Definition 7.2 and Proposition 7.3 on pages 95-96).\nAs a consequence of this, no additional test is required in order to ascertain that each diagnosis in the set DX that is given as a parameter to the next call of STATICHS does in fact satisfy all answered queries so far."}, {"heading": "11.3 Illustrating Examples", "text": "In this section we will give two examples of how interactive KB debugging using STATICHS (Algorithm 5 with parameter mode = static) works. The first one will show the similarities and differences between the usage of STATICHS (within Algorithm 5) and HS (within Algorithm 3) since it will depict the application of STATICHS on the same example DPI (see Table 15.3) that was used to show the functionality of HS in examples 4.8 and 4.9. At the same time, the first example will provide evidence that solving the problem of Interactive Static KB Debugging can be more efficient than solving the problem of Interactive Dynamic KB Debugging in terms of the number of query answers required from an interacting user. This will be discussed in more detail in Chapter 13.\nThe second example is supposed to deepen the reader\u2019s understanding of the way STATICHS works. To this end, the example DPI provided by Table 4.2 will be used which constitutes a significantly harder (interactive) debugging task than the DPI investigated in the first example. This example will involve the construction of a relatively large hitting set tree and thereby give a presentiment of the space and time complexity problems caused by the poor tree pruning inherent in the STATICHS algorithm. In addition, this example will draw a reverse image of the first example in that it will stress the advantage of the decision to search for a solution of Interactive Dynamic KB Debugging rather than for a solution of Interactive Static KB Debugging (more on that in Chapter 13).\nExample 11.1 In this example we assume that the author (called user throughout this example) of the (admissible) DPI \u3008K,B,P ,N \u3009R given by Table 15.3 applies Algorithm 5 with mode = static to interactively debug \u3008K,B,P ,N \u3009R. Further, suppose the following user requirements:\nIn order to guarantee a fast reaction time of the system (the time between two successive queries to the user), the user wants each query to be computed from the minimally necessary number of leading diagnoses. Thus, in each iteration exactly two leading diagnoses should be computed by STATICHS (cf. Proposition 7.5). This postulation is reflected by setting nmin = nmax = 2. Notice that the time limit t is irrelevant in this case.\nMoreover, the user desires to get just any query, i.e. they do not demand any particular properties \u2013 such as optimal information gain among a pool of queries \u2013 to be satisfied by a query. This can be ensured by choosing q := 1 (cf. Chapter 8) and qsm() equal to any query selection measure described in Section 9.3.\nThe user is new to KB debugging and has neither an idea of faults they frequently make nor access to any kind of data that would indicate their tendency to certain types of faults. Thence, pK(ax ) := c < 0.5 for all ax \u2208 K, i.e. all formula fault probabilities are specified to be equal (to some constant c). In such a case, if a formula fault probability measure pK() is given as an input to Algorithm 5, then line 5 in Algorithm 5 is omitted. Please notice that this aspect is not shown in Algorithm 5.\n11.3. EXAMPLES 155\nFinally, the user\u2019s intention is to get the (exact) solution to the problem of Interactive Static KB Debugging. This can be taken into account by specifying \u03c3 := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using STATICHS are visualized by Figure 11.1. We use the same notation as in Figures 4.2 and 4.3 which is described in Examples 4.8 and 4.9. The only new notational element here is the =\u21d2 labeled by some designator of a query. That is, X(Di) Qj =\u21d2 X means that Di is still a minimal diagnosis after Qj has been answered and added to the respective set of test cases of the DPI. On the other hand, X(Di) Qj\n=\u21d2 \u00d7 signifies that the minimal diagnosis Di is invalidated through the addition of the answered query Qj to the respective set of test cases of the DPI. Please notice that =\u21d2 does not point at a node of the wpHS-tree. Instead, the label at which =\u21d2 points is to be understood as the new label of the node originally labeled by X(Di) from which the (first of possibly multiple) =\u21d2 goes out. This notation should help to keep track of the evolution of node labels in the wpHS-tree without needing to overload a single node by multiple different successive labels.\nIn the first iteration, i.e. during the execution of the first call of STATICHS during Algorithm 5, the root node (initially the empty set) is labeled by the minimal conflict set \u30081, 2, 5\u3009 w.r.t. \u3008K,B,P ,N \u3009R and three successor nodes, namely {1}, {2} as well as {5}, are added to the queue of open nodes Q. Since all formulas have been assigned an equal fault probability, STATICHS conducts a breadth-first tree construction (as displayed by the numbers i\u00a9 that give the order of node labeling). That is, Q in this case is a first-infirst-out queue. In this vein, first [1] and then [2] are identified as minimal diagnoses w.r.t. the given DPI. Since DX\u222aDcalc = \u2205\u222a{[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of STATICHS causes it to terminate and return \u3008Dcalc \u222aDX,Ccalc,Q,D\u00d7\u3009 = \u3008{[1], [2]} , {\u30081, 2, 5\u3009} , [{5}], \u2205\u3009 (because DX and D\u00d7 are initially empty sets), as shown in the upper right column in Figure 11.1.\nThen, in Algorithm 5, outside of the STATICHS procedure, the first query Q1 = {E \u2192 \u00acA} is computed from the leading diagnoses set {[1], [2]}. The q-partition P(Q1) associated withQ1 is \u3008{[1]} , {[2]}, \u2205\u3009. The user\u2019s answer u(Q1) to Q1 is then false . Thence, the set Dout is calculated from P(Q1) as D+(Q1) = {[1]} (due to negative answer, cf. Remark 7.4), deleted from DX := DX \u222aDcalc to yield DX = {[2]} and added to D\u00d7 to yield D\u00d7 = {[1]}. The set DX corresponds to the set of all already computed minimal diagnoses w.r.t. the input DPI that satisfy all queries answered so far. The set D\u00d7 comprises all already computed minimal diagnoses w.r.t. the input DPI that do not satisfy all queries answered so far. These sets DX and D\u00d7 along with the collections Q and Ccalc which are unmodified outside of STATICHS are used as input arguments for the second call of STATICHS. Notice that, in the figure, the resulting values of operations performed within STATICHS are given in the righthand column above the dashed line whereas values computed outside of STATICHS are given below the dashed line.\nAfter the modifications caused by the addition of the query Q1 to the negative test cases of \u3008K,B,P , N \u3009R have been taken into account in step 4\u00a9, the partial wpHS-tree built in iteration 1 is further constructed in iteration 2 resulting in the tree depicted by the middle picture in the lefthand column of Figure 11.1. Whereas the branches with edge labels {5, 1} and {5, 2} correspond to proper supersets of the minimal diagnoses [1] and [2], respectively, w.r.t. the input DPI \u3008K,B,P ,N \u3009R and are thus closed by the non-minimality criterion tested in the SLABEL function, the branch with edge labels {5, 7} is identified as a minimal diagnosis D3 := [5, 7] w.r.t. \u3008K,B,P ,N \u3009R. However, D3 is not directly added to the set Dcalc. In fact, the validity of the KB K \\ D3 w.r.t. the current DPI \u3008K,B,P ,N \u222a {Q1}\u3009R is tested beforehand. As this test is successful, meaning that D3 \u2208 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P,N\u222a{Q1}\u3009R , D3 can be safely added to Dcalc implying the set of leading diagnoses DX \u222aDcalc = {D2,D3} with cardinality two. Due to nmin = nmax = 2, STATICHS terminates.\nAfter the second query Q2 has been answered negatively involving the dismissal of the leading diagnosisD2, STATICHS ends up with an empty queue Q of open nodes in iteration 3 (see the tree in the lower left column of Figure 11.1). Hence, STATICHS returns a singleton set including the leading diagnosisD3. Now, independently of the specified formula probabilities, pD(D3) = 1 \u2265 1 \u2212 \u03c3 = 1 is satisfied since\n156 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\nthe probability space considered by the probability measure pD() focuses on the sample space \u2126 = {D3} (cf. Sections 4.6 and 9.2). Thus, the stop condition of Algorithm 5 is met wherefore the solution KB Ksol := (K \\ D3) \u222a UP = (K \\ D3) \u222a \u2205 = K \\ D3 is returned to the user. This solution KB Ksol is the (exact) solution to Interactive Static KB Debugging given the DPI \u3008K,B,P ,N \u3009R of Table 15.3 as an input because D3 is the only minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R that conforms with all answered queries Q1 = false and Q2 = false .\nAll in all, the execution of Algorithm 5 in this example performs\n\u2022 2 full QX calls, i.e. calls of QX that actually return a minimal conflict set (there are two minimal conflict sets labeled by C in the picture at the bottom of the lefthand column in Figure 11.1) and\n\u2022 6 validity checks, i.e. calls of QX that return \u2019no conflict\u2019 (one check for each of the three found minimal diagnoses; notice that QX does only perform a single KB validity check by ISKBVALID in case it returns \u2019no conflict\u2019, see Algorithm 1) or calls of ISKBVALID in line 10 in STATICHS (one call for each of the three found minimal diagnoses),\ncomputes\n\u2022 3 minimal diagnoses w.r.t. the input DPI,\n\u2022 2 minimal conflict sets w.r.t. the input DPI and\n\u2022 2 queries and asks the user 2 logical formulas (1 per query)\nand stores\n\u2022 a maximum of 5 nodes (where node refers to the internal representation of a node in STATICHS as a set of edge labels along a path from the root node to a leaf node; there are even more nodes in the sense of tree nodes in the picture at the bottom of the lefthand column in Figure 11.1).\nExample 11.2 Let us now consider the (admissible) DPI \u3008K,B,P ,N \u3009R given by Table 4.2. We assume an expert (called user throughout this example) in the domain Dom modeled by K who wants to find a solution to Interactive Static KB Debugging for the given DPI \u3008K,B,P ,N \u3009R by means of Algorithm 5 with mode = static. Further, we suppose the following requirements:\nThe user wants each query to be computed from three leading diagnoses. Thus, after each iteration of STATICHS, the set DX \u222aDcalc should comprise exactly three elements. This postulation is reflected by setting nmin = nmax = 3. Notice that the time limit t is irrelevant in this case.\nMoreover, as in example 11.1, we assume no demand for queries satisfying special properties which is reflected by choosing q := 1 (cf. Chapter 8) and qsm() equal to any query selection measure described in Section 9.3.\nLet there be several documentations of past debugging sessions (e.g. in terms of formula change logs) involving KBs in the domain Dom of the author auth of K accessible to the user. Further, let the user have extracted term and logical construct probabilities pK\u0303\u222aK(ax ) \u2208 [0, 1] for ax \u2208 K for auth from this data. This function pK\u0303\u222aK : K\u0303 \u222a K \u2192 [0, 1] is then provided as an input to Algorithm 5.\nFinally, the user\u2019s intention is to get the (exact) solution to the problem of Interactive Static KB Debugging. This can be taken into account by specifying \u03c3 := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using STATICHS are visualized by Figures 11.2 as well as 11.3. We use the same notation as in Figures 4.2, 4.3 and 11.1 which is described in Examples 4.8, 4.9 and 11.1.\nAfter the initialization of variables, Algorithm 5 calls the function GETFORMULAPROBS in line 5 which exploits pK\u0303\u222aK() to calculate the function pK() giving the fault probabilities of formulas in K (cf. Sections 4.6.1, 9.2 and Example 4.7). Let the resulting probabilities be as depicted by Table 11.1.\nThen, STATICHS is called for the first time, resulting in the wpHS-tree given in the first picture in\nFigure 11.2. Contrary to Example 11.1, where the tree was built up in breadth-first order, in this example the formula probabilities p() := pK() given by Table 11.1 are used to assign a probability pnodes(n) to each path n in the wpHS-tree starting from the root node (cf. Formula 4.6 and Definition 4.9). In this vein, as outlined by the numbers i\u00a9 indicating when a node is labeled, after the root node has been labeled by C1 := \u30081, 2, 5\u3009, the node corresponding to the outgoing edge of C1 labeled by the formula with the largest fault probability among all formulas in C1 is labeled first. That is, the node {1} with pnodes({1}) = 0.41 (as opposed to the nodes {2} and {5} with 0.25 each) is labeled first. The SLABEL procedure, after checking whether {1} is a non-minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R or a duplicate of some other node in Q (both checks negative), computes another minimal conflict set C2 := \u30082, 4, 6\u3009 such that {1}\u2229C2 = \u2205 (C2 is not hit by the node {1}) to constitute a label for node {1}. The successor nodes {1, 2}, {1, 4} and {1, 6} of {1} are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.\nSince {1, 4} (0.28) as well as {1, 6} (0.27) have a larger probability (as per pnodes()) than the nodes {2} (0.25) and {5} (0.25), Q is given by [{1, 4} , {1, 6} , {2} , {5} , {1, 2}] when it comes to the processing of the next node. Since STATICHS always treats the first node of Q next, it identifies the first minimal diagnosis D1 := [1, 4] w.r.t. \u3008K,B,P ,N \u3009R in step 3\u00a9. In steps 4\u00a9 and 8\u00a9, two further minimal diagnoses D2 := [1, 6] and D3 := [5, 4] are detected. Altogether, the union of DX (initially the empty set) and Dcalc (comprising the three computed diagnoses) now contains 3 = nmin = nmax elements wherefore STATICHS terminates and outputs the tuple \u3008Dcalc \u222aDX,Ccalc,Q,D\u00d7\u3009 where the sets in this tuple are given under the wpHS-tree of iteration 1 in Figure 11.2.\nFrom this set of leading diagnoses DX := DX \u222a Dcalc, the probability measure pD : DX \u2192 [0, 1] is computed by the function GETPROBDIST (cf. Algorithm 6 and Section 9.2). The result is \u3008pD(D1), pD(D2), pD(D3)\u3009 = \u30080.38, 0.37, 0.25\u3009. The mode Dmax := D1 of this probability distribution is then computed by GETMODE. As \u03c3 = 0, pD(Dmax) = 0.38 6\u2265 1 wherefore the stop criterion of Algorithm 5 is not satisfied.\nConsequently, Algorithm 5 proceeds to generate the first queryQ1 = {B v K} (based on the current set of leading diagnoses DX) along with its associated q-partition P(Q1) = \u3008{D1,D2} , {D3} , \u2205\u3009. The diagnosis D1 is in D+(Q1) because K\u22171 = (K \\D1)\u222aB \u222aUP (recall Formula 7.1 for a definition of K\u2217i ) comprises formulas 2, 3, 5, 6, 7, 8 and 9 as well as p1 (cf. Table 4.2) wherefore K\u22171 |= {B v K} = Q1 (due to the set of formulas {2, 3} = {B v G,G v K}). That D2 belongs to D+(Q1) as well follows analogously. On the other hand,D3 \u2208 D\u2212(Q1) must be true sinceK\u22173 \u222aQ1 includes i.a. A v B (formula 1) and B v K (\u2208 Q1) wherefore {A v K} = n1 is an entailment of K\u22173 . Thus, the negative test case n1 is violated.\nThe positive user answer u(Q1) = true is incorporated in that Q1 is appended to the set of positive test cases P yielding P \u222a {Q1} = {{r(x, y)} , {B v K}}. Step 9\u00a9 shows the impact of this test case addition on the set of leading diagnoses, i.e. all diagnoses in the set Dout = D\u2212(Q1) = {D3} (due to positive answer, cf. Remark 7.4) are re-labeled by \u00d7 whereas all other leading diagnoses (D1,D2) are still labeled by X.\nIn the same fashion, further node labelings are conducted in iteration 2 until |DX \u222a Dcalc| = | {D1,D2} \u222a {[2, 1]} | = 3 = nmin = nmax holds again. These actions are displayed by the tree at the bottom of Figure 11.2.\nNotice that, after step 12\u00a9, two nodes corresponding to the same set are elements of the list Q. At\n158 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\nstep 13\u00a9, the duplicate criterion checked by SLABEL comes into play. Since the node {1, 2} (the leftmost branch in the tree) is ranked first in Q (we assume a first-in-first-out ordering of nodes corresponding to equal sets of edge labels in Q), the SLABEL procedure is called given node := {1, 2} as an argument and detects the node {2, 1} (the fourth leftmost branch in the tree) in Q. Hence, node = {1, 2} is closed as a duplicate node which finds expression in the label \u00d7(dup). When {2, 1} (which must have the same probability as {1, 2} due to set-equality) is processed at step 14\u00a9, it is discovered to be a minimal diagnosis (D5) w.r.t. \u3008K,B,P ,N \u3009R.\nMoreover, we want to point out that another minimal diagnosis (D4 = [2, 4, 6]) is found in iteration 2 before D5 is detected. However, D4 is immediately ruled out and added to D\u00d7 (cf. line 13 in STATICHS) due to the fact that K\\D4 is invalid w.r.t. the current DPI \u3008\u00b7,B,P \u222a{Q1} ,N \u3009R (cf. Definition 3.3). The explanation why this holds is as follows:\nBy Definition 3.3,K\\D4 is valid w.r.t. \u3008\u00b7,B,P\u222a{Q1} ,N \u3009R iffK\u22174 = (K\\D4)\u222aB\u222aU(P\u222a{Q1}) (recall Formula 7.1 for a definition of K\u2217i ) does not violate any r \u2208 R = {consistency, coherency} and does not entail any n \u2208 N = {n1,n2} = {{A v K} , {L v \u2203r.F,B(x), G v K}}. Applying the diagnosis D4 to K yields K \\ D4 = {1, 3, 5, 8} which includes in particular formula 1 which is equal to A v B (see Table 4.2). However, there is also the negative test case n1 indicating that A v K must not be entailed by K\u22174 . That is, B v K \u2208 K\u22174 (due to Q1) and A v B \u2208 K\u22174 which implies that K\u22174 |= {A v K} = n1 wherefore K\u22174 is invalid w.r.t. \u3008\u00b7,B,P \u222a {Q1} ,N \u3009R.\nSuch a direct dismissal of a discovered diagnosisDi due to a newly added test case Qj is indicated by k\u00a9X(Di) Qj =\u21d2 k\u00a9\u00d7, i.e. the step number k\u00a9 at the shaft of the =\u21d2 is equal to the step number at the head of =\u21d2. In case of the invalidation of a leading diagnosis (i.e. one that was utilized in the computation of Qj), on the contrary, the step number at the shaft is lower than the step number at the arrow head.\nAs shown at the top of Figure 11.3, the second query Q2 computed from the leading diagnosis set DX\u222aDcalc = {D1,D2,D5} is then answered by u(Q2) = true as well, wherefore the leading diagnoses D2,D5 are ruled out and added to D\u00d7. So, the input argument DX given to the next call of STATICHS in Algorithm 5 consists of the single diagnosis D1.\nIn the third iteration (see the picture given in Figure 11.3), STATICHS again executes in order to complete the leading diagnosis set to contain three elements. However, as we can say in advance, D1 is the only minimal diagnosis w.r.t. the input DPI \u3008K,B,P ,N \u3009R which is also a diagnosis w.r.t. the current DPI \u3008K,B,P \u222a {Q1, Q2} ,N \u3009R. Nevertheless, STATICHS continues expanding the wpHS-tree until it has verified that this is the case (Q = []). This is equivalent to finishing the construction of the noninteractive wpHS-tree that is generated by HS with parameters nmin = nmax = \u221e. We want to stress that the construction of the entire wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and p() := pK() is inevitable in a debugging scenario where the (exact) solution to the Interactive Static KB Debugging problem is sought (the probability w.r.t. pD() of a diagnosis can only be equal to 1 if there is only a single leading diagnosis returned by STATICHS).\nIn fact, there are five further diagnoses D6, . . . ,D10 w.r.t. \u3008K,B,P ,N \u3009R that are detected in iteration 3 and directly dismissed (added to D\u00d7) after the validity check in line 10 of STATICHS. All other tree branches are closed due to the non-minimality (label \u00d7(\u2283Di)) or duplicate criterion (label \u00d7(dup)). Due to \u03c3 = 0 and the associated necessity to grow the wpHS-tree until all leaf nodes are labeled, the final tree (19 labeled leaf nodes) depicted in Figure 11.3 is relatively large in comparison to the small size |K| = 7.\nThis example might already give an idea of the potential explosion of the wpHS-tree produced by STATICHS in case the (exact) solution to the Interactive Static KB Debugging problem is desired. This is why it will usually make sense in practice to specify a fault tolerance \u03c3 > 0 which enables Algorithm 5 with mode = static to escape from the generally intractable complexity of the complete investigation of all minimal diagnoses w.r.t. the input DPI (full construction of the wpHS-tree). However, in this concrete example, allowing a small fault tolerance \u03c3 has no effect either. Actually, \u03c3 \u2265 0.56 is necessary to achieve a premature termination of the tree construction. This holds due to the fact that the probability distributions of leading diagnoses are \u3008pD(D1), pD(D2), pD(D3)\u3009 = \u30080.38, 0.37, 0.25\u3009 (after iteration 1)\n11.3. EXAMPLES 159\nand \u3008pD(D1), pD(D2), pD(D5)\u3009 = \u30080.44, 0.42, 0.14\u3009 (after iteration 2). Now, given say \u03c3 := 0.6, the stop criterion of Algorithm 5 would be met after iteration 2 because pD(Dmax) = pD(D1) = 0.44 \u2265 0.4 = 1 \u2212 0.6 = 1 \u2212 \u03c3. Nate that, in this case, the same (exact) solution would be returned as for the setting \u03c3 := 0. The (significant) difference is just that the final tree in this case has only 14 leaf nodes, of which only 7 are labeled (the labeling of a node is in general significantly more costly than the mere generation of a node). As opposed to this, the full tree comprises 19 labeled nodes. On the other side of the coin, choosing a value of \u03c3 > 0.5, for example, means that \u2013 from the point of view of the knowledge at the time Algorithm 5 terminates \u2013 a solution to Interactive Static KB Debugging is returned by Algorithm 5 which has a higher probability of not being the (exact) solution than of being the (exact) solution.\nAll in all, the execution of Algorithm 5 in this example performs\n\u2022 4 full QX calls, i.e. calls of QX that actually return a minimal conflict set (there are four minimal conflict sets labeled by C in the tree in Figure 11.3) and\n\u2022 20 validity checks, i.e. calls of QX that return \u2019no conflict\u2019 (one check for each of the 10 found minimal diagnoses; notice that QX does only perform a single KB validity check by ISKBVALID in case it returns \u2019no conflict\u2019, see Algorithm 1) or calls of ISKBVALID in line 10 in STATICHS (one call for each of the 10 found minimal diagnoses),\ncomputes\n\u2022 10 minimal diagnoses w.r.t. the input DPI,\n\u2022 4 minimal conflict sets w.r.t. the input DPI and\n\u2022 2 queries and asks the user 2 logical formulas (1 per query)\nand stores\n\u2022 a maximum of 19 nodes (where node refers to the internal representation of a node in STATICHS as a set of edge labels along a path from the root node to a leaf node; there are even more nodes in the sense of tree nodes in the picture in Figure 11.3).\n11.3. EXAMPLES 161\n1 \u00a9 \u30081 ,2 ,5 \u3009C\n2 \u00a9 \u30082 ,4 ,6 \u3009C\n5 \u00a9 \u30081 ,3 ,4 \u3009C\n6 \u00a9 \u30082 ,4 ,6 \u3009R\n? 3 \u00a9 X\n(D 1 )\n4 \u00a9 X\n(D 2 )\n? ?\n7 \u00a9 \u30081 ,5 ,6 ,8 \u3009C\n? 8 \u00a9 X\n(D 3 )\n?\n? ?\n? ?\n1\n0 .4 1 tt\n2 0 .2\n5\n5 0 .2\n5 --\n2\n0 .0\n9\n4\n0 .2\n8\n6\n0 .2\n7\n1\n0 .0\n9\n3 0 .0\n7\n4 0 .1\n8 ++\n2 0 .0\n6\n4 0 .1\n8\n6\n0 .1\n7 **\n1\n0 .0\n6\n5\n0 .0\n4\n6 0 .1\n1\n8\n0 .0\n4 ''\nIt er\nat io\nn 1\n\u232a\nD X \u222a D\nc a lc\n= \u2205 \u222a {D\n1 ,D\n2 ,D\n3 }\n= {[\n1 ,4\n], [1 ,6\n], [5 ,4\n]} ,\nQ =\n[{ 5 ,6 } ,{\n2 ,4 ,6 } ,{\n1 ,2 } ,{\n2 ,1 } ,{\n2 ,3 } ,{\n5 ,2 } ,{\n2 ,4 ,1 } ,{\n2 ,4 ,5 } ,{\n2 ,4 ,8 }]\n, D \u00d7\n= \u2205\nC c a lc\n= {\u3008\n1 ,2 ,5 \u3009, \u30082 ,4 ,6 \u3009, \u30081 ,3 ,4 \u3009, \u30081 ,5 ,6 ,8 \u3009}\n,\n\u3008Q 1 ,P\n(Q 1 )\u3009\n= \u3008{ B v K } ,\u3008 {D\n1 ,D\n2 } ,{ D\n3 } ,\u2205 \u3009\u3009\n, u\n(Q 1 )\n= tr u e\n, D\nX = {D\n1 ,D\n2 },\nD o u t\n= D \u00d7\n= {D\n3 }\n\u232a\n1 \u00a9 \u30081 ,2 ,5 \u3009C\n2 \u00a9 \u30082 ,4 ,6 \u3009C\n5 \u00a9 \u30081 ,3 ,4 \u3009C\n6 \u00a9 \u30082 ,4 ,6 \u3009R\n13 \u00a9 \u00d7\n(d u p )\n3 \u00a9 X\n(D 1 )\n4 \u00a9 X\n(D 2 )\n14 \u00a9 X\n(D 5 )\n? 7 \u00a9 \u30081 ,5 ,6 ,8 \u3009C\n? 8 \u00a9 X\n(D 3 )\n10 \u00a9 \u30081 ,3 ,4 \u3009R\n? ?\n? 11 \u00a9 X (D 4 )\n9 \u00a9 \u00d7\n9 \u00a9 X\n9 \u00a9 X\n? ?\n12 \u00a9 \u00d7\n(\u2283 D\n3 )\n11 \u00a9 \u00d7\n1\n0 .4 1 uu\n2 0 .2\n5\n5 0 .2\n5 --\n2\n0 .0\n9\n4\n0 .2\n8\n6\n0 .2\n7\n1\n0 .0\n9\n3 0 .0\n7\n4 0 .1\n8 ++\n2 0 .0\n6\n4 0 .1\n8\n6\n0 .1 7 ))\n1\n0 .0\n6\n5\n0 .0\n4\n8\n0 .0\n4 ''\n6 0 .1\n1\nQ 1\nQ\n1\nQ\n1\n1\n0 .0\n6 3\n0 .0\n4\n4\n0 .1\n1\nQ 1\nIt er\nat io\nn 2\n\u232a\nFi gu\nre 11\n.2 :(\nE xa\nm pl\ne 11\n.2 )S\nol vi\nng th\ne pr\nob le\nm of\nIn te\nra ct\niv e\nSt at\nic K\nB D\neb ug\ngi ng\n(P ro\nbl em\nD efi\nni tio\nn 6.\n2) fo\nrt he\nex am\npl e\nD PI\ngi ve\nn by\nTa bl\ne 4.\n2 by\nm ea\nns of\nA lg\nor ith\nm 5\nan d\nS TA\nT IC\nH S.\n162 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\nD X \u222a D c a lc = {D 1 ,D 2 } \u222a {D 5 } = { [1 ,4 ],[1 ,6 ],[2 ,1 ]}, Q = [{ 2 ,4 ,6} ,{ 2 ,3} ,{ 5 ,2} ,{ 2 ,4 ,1} ,{ 5 ,6 ,1} ,{ 2 ,4 ,5} ,{ 5 ,6 ,3} ], D \u00d7 = {D 3 ,D 4 } = { [5 ,4 ],[2 ,4 ,6 ]} C c a lc = {\u30081 ,2 ,5\u3009 ,\u30082 ,4 ,6\u3009 ,\u30081 ,3 ,4\u3009 ,\u30081 ,5 ,6 ,8\u3009}, \u3008Q 2 ,P (Q 2 )\u3009 = \u3008{ B v \u2203 r.F } ,\u3008{D 1 } ,{D 2 ,D 5 } ,\u2205\u3009\u3009, u (Q 2 ) = tru e, D X = {D 1 }, D o u t = {D 2 ,D 5 }, D \u00d7 = {D 3 ,D 4 ,D 2 ,D 5 }\n\u232a\n1\u00a9 \u30081,2,5\u3009\nC\n2\u00a9 \u30082 ,4,6\u3009\nC\n5\u00a9 \u30081,3,4\u3009\nC\n6\u00a9 \u30082 ,4,6\u3009\nR\n13 \u00a9 \u00d7 (d u p )\n3\u00a9 X\n(D 1 )\n4\u00a9 X\n(D 2 )\n14 \u00a9 X (D 5 )\n16 \u00a9 \u30081 ,5 ,6,8\u3009\nR 7\u00a9 \u30081 ,5,6,8\u3009\nC\n17 \u00a9 \u30081 ,3 ,4\u3009\nR 8\u00a9 X\n(D 3 )\n10 \u00a9 \u30081,3,4\u3009\nR\n18 \u00a9 \u00d7 (\u2283 D 5 )\n20 \u00a9 \u00d7 (\u2283 D 3 )\n21 \u00a9 X\n(D 6 )\n11 \u00a9 X (D 4 )\n9\u00a9 \u00d7\n9\u00a9 X\n9\u00a9 X\n15 \u00a9 \u00d7\n19 \u00a9 \u00d7 (\u2283 D 2 )\n22 \u00a9 X\n(D 7 )\n12 \u00a9 \u00d7 (\u2283 D 3 )\n28 \u00a9 \u00d7 (\u2283 D 5 )\n24 \u00a9 \u00d7 (\u2283 D 3 )\n25 \u00a9 \u00d7 (\u2283 D 5 )\n26 \u00a9 \u00d7 (d u p )\n23 \u00a9 X\n(D 8 )\n27 \u00a9 X\n(D 9 )\n29 \u00a9 X\n(D 1 0 )\n11 \u00a9 \u00d7\n22 \u00a9 \u00d7\n15 \u00a9 X 15 \u00a9 \u00d7\n21 \u00a9 \u00d7\n23 \u00a9 \u00d7\n27 \u00a9 \u00d7\n29 \u00a9 \u00d7\n(D 1 0 )\n1\n0 .4 1 uu\n2\n0 .2 5\n5\n0 .2 5//\n2\n0 .0 9\n4\n0 .2 8\n6\n0 .2 7\n1\n0 .0 9 uu\n3 0 .0 7\n4 0 .1\n8,,\n2\n0 .0 6 tt\n4\n0 .1 8\n6 0 .1\n7 &&\n1\n0 .0 6 uu\n5 0 .0 4\n8\n0 .0\n4 ))\n6 0 .1 1\nQ 1\nQ 1\nQ 1\n10.0 6 xx 3\n0 .0 4\n4 0 .1\n1 &&\n1\n0 .0 2 xx\n5\n0 .0 2\n6\n0 .0 4\n8\n0 .0 2\n4 0 .0\n4 &&\n10.0 2 xx 3\n0 .0 2\nQ 1\nQ 2\nQ 2\nQ 2\nQ 1\nQ 2\nQ 1\nQ 1\nQ 1 Iteration 3\n\u232a\nD X \u222a D c a lc = {D 1 } \u222a \u2205 = {D 1 }, Q = [],\nC c a lc = {\u30081 ,2 ,5\u3009 ,\u30082 ,4 ,6\u3009 ,\u30081 ,3 ,4\u3009 ,\u30081 ,5 ,6 ,8\u3009},\nD \u00d7 = {D 3 ,D 4 ,D 2 ,D 5 ,D 6 ,D 7 ,D 8 ,D 9 ,D 1 0 } = { [5 ,4 ],[2 ,4 ,6 ],[1 ,6 ],[2 ,1 ],[2 ,4 ,8 ],[5 ,6 ,3 ],[2 ,3 ,6 ],[2 ,3 ,8 ],[5 ,2 ,3 ]}, p D (D 1 ) = 1 \u21d2 return the solution K B (K \\ D 1 )\u222a p 1 (p 1 :cf.Table 4.2) Figure 11.3: (E xam ple 11.2 continued)Solving the problem ofInteractive Static K B D ebugging (Problem D efinition\n6.2)forthe exam ple D PIgiven by Table 4.2 by m eans of\nA lgorithm 5 and S TA T ICH S.\n11.4. ALGORITHM CORRECTNESS 163"}, {"heading": "11.4 Correctness of the Algorithm", "text": "In this section we will demonstrate the correctness of STATICHS. That is, we will prove that STATICHS, given the inputs described in Algorithm 7, yields the outputs enumerated in Algorithm 7. Used in Algorithm 5 to iteratively compute a set of leading diagnoses for query generation, STATICHS in this way serves to solve the problem of Interactive Static KB Debugging approximately (parameter \u03c3 > 0 in Algorithm 5) or exactly (\u03c3 = 0).\nAfter each call to STATICHS during Algorithm 5, the hitting set tree produced by STATICHS is a (partial) wpHS-tree w.r.t. the DPI \u3008K,B,P ,N \u3009R given as an input to Algorithm 5 and pnodes() which can be directly obtained from the function p() given as input to STATICHS. This proposition is made by Lemma 11.3.\nIn order to be able to prove this proposition, we formulate and prove two lemmata, Lemma 11.1 and 11.2. The former, which is given next, shows that this proposition holds for the very first call of STATICHS during the execution of Algorithm 5. The latter assures that this proposition holds for any further call of STATICHS during Algorithm 5 for an adequate set of input parameters to STATICHS. Finally, Lemma 11.3 exploits these results to ascertain that this proposition is satisfied for all calls of STATICHS.\nLemma 11.1. Let the following be the input parameters to the STATICHS function:\n\u2022 \u3008K,B,P ,N \u3009R is the DPI given as input to Algorithm 5,\n\u2022 nmin, nmax, t \u2208 N where nmin \u2265 2,\n\u2022 a function p : K \u2192 (0, 0.5),\n\u2022 Q = [\u2205],\n\u2022 P \u2032 = N \u2032 = D\u00d7 = DX = Ccalc = \u2205.\nThen, STATICHS creates a (partial) wpHS-tree T w.r.t. \u3008K,B,P ,N \u3009R and pnodes() (cf. Definition 4.9) equivalent to one produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R, nmin, nmax, t and p() and returns \u3008D,Q,Ccalc,D\u00d7\u3009 where \u3008D \u222aD\u00d7,Q,Ccalc\u3009 is the relevant data of T .\nProof. Since all input parameters P \u2032, N \u2032, D\u00d7, DX and Ccalc are equal to the empty set, Dcalc = \u2205 and Q includes only the node \u2205, we might regard \u3008Dcalc,Q,Ccalc\u3009 as the initial relevant data of some (partial) wpHS-tree which includes only an unlabeled root node. The root node \u2205 cannot be labeled as otherwise it would be necessarily an element of Dcalc if \u2205 is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R or the set Ccalc would include the conflict set that labels the root node.\nD\u00d7 can never be extended during the execution of STATICHS since line 13 can never be reached. This holds because the test made in line 10 can never be negative. Namely, as P \u2032 = N \u2032 = \u2205, this test actually checks whether K \\ node is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Due to the fact that L = valid has been output as a label for node (line 9) by the SLABEL function called in line 7, it must hold that QX(\u3008K \\ node,B,P ,N \u3009R) yielded \u2019no conflict\u2019. By Proposition 4.9, this implies that K \\ node is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R. Thence, D\u00d7 = \u2205 definitely holds whenever STATICHS terminates.\nMoreover, each node with the label valid is added to Dcalc since line 13 can never be reached. As a consequence, with the given input parameters, the execution of the code between line 2 and line 18 of Algorithm 7 has exactly the same effect as executing the code between line 2 and line 16 of Algorithm 2.\nDX can never be extended as there is no such modification operation at all in STATICHS. Thus, DX = \u2205 holds throughout the execution of STATICHS.\nNow, the SLABEL procedure is equivalent to the LABEL procedure of Algorithm 2, except for the first line of the non-minimality criterion. That is, in STATICHS (line 21) some nd is searched for in D(\u00d7,X,calc) whereas in Algorithm 2 (line 19) such nd is searched in Dcalc. However, we point out that\n164 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\nD(\u00d7,X,calc) in the SLABEL procedure corresponds to the set D\u00d7\u222aDX\u222aDcalc in STATICHS (cf. the call to SLABEL in line 7), where DX = D\u00d7 = \u2205 is an invariant, as argued above. Taking these arguments into account, we have that D(\u00d7,X,calc) in SLABEL in line 21 is equal to Dcalc, just as in Algorithm 2.\nHence, with the given input parameters, we have verified that STATICHS acts equivalently to Algorithm 2. As Algorithm 2 produces a (partial) wpHS-tree T w.r.t. the input DPI \u3008K,B,P ,N \u3009R and pnodes() by Lemma 4.15, we infer that STATICHS also does so.\nAs opposed to Algorithm 2 which returns only Dcalc, STATICHS returns \u3008D,Q,Ccalc,D\u00d7\u3009 where D := Dcalc \u222aDX = Dcalc since DX = \u2205, as argued above. In that, Dcalc, Q and Ccalc correspond exactly to the equally named collections in Algorithm 2 and D\u00d7 = \u2205, as argued above. Therefore, by Corollary 4.6, \u3008D \u222aD\u00d7,Q,Ccalc\u3009 = \u3008Dcalc,Q,Ccalc\u3009 is the relevant data of the (partial) wpHS-tree T w.r.t. \u3008K,B,P ,N \u3009R and pnodes() produced by Algorithm 2.\nThe next lemma manifests that STATICHS, given such parameters that \u3008D\u00d7 \u222aDX,Q,Ccalc\u3009 is the relevant data of a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes(), again yields a (partial) wpHStree w.r.t. \u3008K,B,P ,N \u3009R and pnodes().\nLemma 11.2. Let the following be the input parameters to the STATICHS function:\n\u2022 \u3008K,B,P ,N \u3009R is the DPI given as input to Algorithm 5,\n\u2022 P \u2032 is the set of positive and N \u2032 is the set of negative test cases specified since the start of Algorithm 5 where P \u2032 \u222aN \u2032 \u2283 \u2205,\n\u2022 nmin, nmax, t \u2208 N where nmin \u2265 2,\n\u2022 a function p : K \u2192 (0, 0.5),\n\u2022 D\u00d7 6= \u2205, DX 6= \u2205, Ccalc 6= \u2205 and Q such that \u3008D\u00d7 \u222aDX,Q,Ccalc\u3009 is the relevant data of a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes() produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p().\nThen, STATICHS creates a (partial) wpHS-tree T w.r.t. \u3008K,B,P ,N \u3009R and pnodes() equivalent to one produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p() and returns \u3008D,Q,Ccalc,D\u00d7\u3009 where \u3008D \u222aD\u00d7,Q,Ccalc\u3009 is the relevant data of T .\nProof. Since \u3008D\u00d7 \u222aDX,Q,Ccalc\u3009 is the relevant data of a (partial) wpHS-tree T w.r.t. \u3008K,B,P ,N \u3009R and pnodes() produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p(), it is clear that, if the construction of T is continued by an algorithm working equivalently to Algorithm 2 and using this relevant data, the relevant data of a (partial) wpHS-tree T \u2032 w.r.t. \u3008K,B,P ,N \u3009R and pnodes() will be stored by this algorithm (Corollary 4.6). Therefore, we show that STATICHS is such an algorithm.\nIn Algorithm 2, the set of all already computed minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R is denoted by Dcalc. Nodes labeled by valid are added to Dcalc (line 11) and Dcalc is used in the non-minimality criterion in the LABEL function (line 19). If Algorithm 2 should be used to continue construction of T using the relevant data \u3008D\u00d7 \u222aDX,Q,Ccalc\u3009, the required setting is just to use Dcalc := D\u00d7 \u222aDX and use Q and Ccalc for the equally named variables in Algorithm 2. If then a new node nd labeled by valid were added to Dcalc, we would have that Dcalc := D\u00d7 \u222aDX \u222a {nd}. By Corollary 4.7, this set Dcalc used by Algorithm 2 would at each point in time comprise exactly the |Dcalc| most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R and pnodes().\nIn STATICHS, each node node labeled by valid is added either to Dcalc, which is initially the empty set in STATICHS, or to D\u00d7 (lines 11 and 13), i.e. node is added to Dcalc \u222aD\u00d7. Thus, it is also true to say that node is added to Dcalc \u222aD\u00d7 \u222aDX. So, the first new node nd labeled by valid is added to this set which is then equal to D\u00d7 \u222a DX \u222a {nd}. This set is equal to the set Dcalc that would be used by Algorithm 2 to further construct the (partial) wpHS-tree T .\n11.4. ALGORITHM CORRECTNESS 165\nIn the non-minimality criterion in function SLABEL, D\u00d7,X,calc is used which is equal to the set Dcalc \u222aD\u00d7 \u222aDX in STATICHS (cf. the call to SLABEL in line 7). Hence, Dcalc \u222aD\u00d7 \u222aDX is used and modified in STATICHS in exactly the same way as Dcalc is used and modified in Algorithm 2.\nApart from this, as can be easily verified, the labeling function SLABEL in STATICHS is identical to LABEL in Algorithm 2 and the way Q and Ccalc are used and modified in STATICHS is exactly equivalent to the way these are used and modified in Algorithm 2.\nWhat remains to be shown is that Dcalc \u222a D\u00d7 \u222a DX, as Dcalc in Algorithm 2, always contains all already computed minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R which are the |Dcalc \u222a D\u00d7 \u222a DX| most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nSince D\u00d7 \u222aDX is the first set in the relevant data of a (partial) wpHS-tree T w.r.t. \u3008K,B,P ,N \u3009R and pnodes() produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p(), by Corollaries 4.6 and 4.7, it must be valid that D\u00d7 \u222a DX comprises the |D\u00d7 \u222a DX| most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R. Since Dcalc is initially defined to be the empty set in STATICHS, it is also true to say that D\u00d7 \u222a DX \u222a Dcalc comprises the |D\u00d7 \u222a DX \u222a Dcalc| most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R when STATICHS starts executing. Since, by assumption, the same p() is used by STATICHS as was used for the construction of the (partial) wpHS-tree T so far, the same ordering of Q is used by STATICHS as would be used by Algorithm 2 to further construct the (partial) wpHS-tree T . Therefore, D\u00d7 \u222a DX \u222a Dcalc must indeed comprise the |D\u00d7 \u222a DX \u222a Dcalc| most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R at each point in time.\nThe set D in the tuple \u3008D,Q,Ccalc,D\u00d7\u3009 returned by STATICHS corresponds exactly to Dcalc\u222aDX. So, D \u222aD\u00d7 = D\u00d7 \u222aDX \u222aDcalc.\nTo summarize, STATICHS acts exactly equivalently to Algorithm 2. As a consequence, Corollary 4.6 regarding Algorithm 2 applies to STATICHS as well. This means that the tuple consisting of the set of nodes labeled by valid, i.e. D\u00d7 \u222aDX \u222aDcalc, the list of open nodes Q and the set of minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R in STATICHS store the relevant data of a (partial) wpHS-tree T as it could have been generated by Algorithm 2. This completes the proof.\nLemma 11.3. Any call to STATICHS within Algorithm 5 yields an output \u3008D,Q,Ccalc,D\u00d7\u3009 where\n\u2022 \u3008D \u222aD\u00d7,Q,Ccalc\u3009 is the relevant data of T and\n\u2022 T is a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes() equivalent to one produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p().\nProof. As can be easily verified, the arguments given to STATICHS at the first time it is called throughout the execution of Algorithm 5 correspond exactly to the input parameters to STATICHS assumed in Lemma 11.1 (cf. the variable instantiations in lines 1-4 of Algorithm 5). Thus, by Lemma 11.1, we conclude that the first call to STATICHS during the runtime of Algorithm 5 yields the output \u3008D,Q,Ccalc,D\u00d7\u3009 where \u3008D \u222aD\u00d7,Q,Ccalc\u3009 is the relevant data of T and T is a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes() equivalent to one produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p().\nWhen this first call to STATICHS returns in Algorithm 5, D is renamed to become DX in Algorithm 5 (line 8). Q, Ccalc and D\u00d7 bear unmodified names within Algorithm 5. We point out that Q and Ccalc are not modified anywhere in Algorithm 5. DX and D\u00d7 are modified only in lines 21 and 22. In these lines, a subset Dout of DX is deleted from DX and added to D\u00d7.\nDout must be a subset of DX. This holds, first, because \u3008Q,P(Q)\u3009 is a query Q w.r.t. the leading diagnoses DX and the DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R together with its q-partition P(Q) (CALCQUERY in line 16, cf. Section 9.2). Second, Dout corresponds either to D+(Q) (if the answer u(Q) = false) or to D\u2212(Q) (if the answer u(Q) = true) where both sets must be subsets of the set of leading diagnoses DX by Definition 7.2 (GETINVALIDDIAGS in line 19, cf. Section 9.2).\nHence, DX \u222a D\u00d7 remains unchanged throughout Algorithm 5. By the renaming of D to become DX in Algorithm 5 (see the argumentation above), DX \u222a D\u00d7 is equal to the set D \u222a D\u00d7 where\n166 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\n\u3008D,Q,Ccalc,D\u00d7\u3009 is the output of the first call to STATICHS in Algorithm 5. Therefore, the relevant data \u3008D \u222aD\u00d7,Q,Ccalc\u3009 of T is unmodified until the second call to STATICHS within Algorithm 5 is made.\nSo, we have that the arguments given to STATICHS at the second time it is called throughout the execution of Algorithm 5 correspond exactly to the input parameters to STATICHS assumed in Lemma 11.2. Notice that the probability measure pK() which corresponds to the probability measure p() in STATICHS is never changed throughout the while-loop in Algorithm 5 (cf. Section 9.2).\nThus, by Lemma 11.2, we conclude that the second call to STATICHS during the runtime of Algorithm 5 yields the output \u3008D,Q,Ccalc,D\u00d7\u3009 where \u3008D \u222aD\u00d7,Q,Ccalc\u3009 is the relevant data of T \u2032 and T \u2032 is a (partial) wpHS-tree w.r.t. \u3008K,B,P ,N \u3009R and pnodes() equivalent to one produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p().\nBy means of the same line of argument we used so far and further applications of Lemma 11.2 it can be derived that the proposition of this lemma holds for any call to STATICHS throughout Algorithm 5.\nBy means of the just proven Lemma 11.3, we are now able to show by the next lemma that STATICHS computes minimal diagnoses w.r.t. the DPI \u3008K,B,P ,N \u3009R given as an input to Algorithm 5 in mostprobable-first order. Further on, the next lemma will reveal that only minimal diagnoses w.r.t. the DPI \u3008K,B,P ,N \u3009R are computed by STATICHS which assures the soundness of STATICHS concerning the (input) DPI \u3008K,B,P ,N \u3009R. The soundness of STATICHS as regards the (current) DPI \u3008K,B,P \u222aP \u2032,N \u222a N \u2032\u3009R will be considered in Lemma 11.6 below.\nLemma 11.4. Any call to STATICHS within Algorithm 5 yields an output \u3008D,Q,Ccalc,D\u00d7\u3009 where D \u222aD\u00d7 is the set of |D \u222aD\u00d7| most probable (w.r.t. pnodes()) minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nProof. Let T be the (partial) wpHS-tree T produced by any call to STATICHS within Algorithm 5. Then, by Lemma 11.3,\n\u2022 T is equal to a (partial) wpHS-tree produced by Algorithm 2 with input parameters \u3008K,B,P ,N \u3009R and p() and\n\u2022 the first set Dcalc in the relevant data \u3008Dcalc,Q,Ccalc\u3009 of T produced by Algorithm 2 corresponds to D \u222aD\u00d7.\nSo, by Corollary 4.7, the proposition of this lemma follows.\nMoreover, Lemma 11.3 provides the basis for showing the completeness of STATICHS. That is, Lemma 11.5 will manifest that all minimal diagnoses w.r.t. the DPI \u3008K,B,P ,N \u3009R given as an input to Algorithm 5 will be found by STATICHS given that it keeps executing for a sufficiently long period of time.\nLemma 11.5. Any call to STATICHS within Algorithm 5 where the execution of STATICHS terminates due to Q = [] yields an output \u3008D,Q,Ccalc,D\u00d7\u3009 where D \u222a D\u00d7 is the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R.\nProof. The proposition of this lemma follows from Lemma 11.3 and Proposition 4.15 by an analogue argumentation as in the proof of Lemma 11.4.\nThe following lemma proves that STATICHS is sound w.r.t. the finding of minimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, i.e. the DPI \u3008K,B,P ,N \u3009R given as an input to Algorithm 5 extended by all new positive and negative test cases P \u2032 and N \u2032, respectively, that have been collected so far.\nLemma 11.6. If any call to STATICHS adds an element D to the set Dcalc during the execution of Algorithm 5, D is a minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\n11.4. ALGORITHM CORRECTNESS 167\nProof. By Lemma 11.4 we know that each node node that is added to Dcalc by STATICHS is a minimal diagnosis w.r.t. the input DPI \u3008K,B,P ,N \u3009R. Through the test for validity of K \\ node w.r.t. \u3008\u00b7,B,P \u222a P \u2032,N \u222a N \u2032\u3009R (cf. Definition 3.3) which must be successful before node is added to Dcalc (ISKBVALID in line 10), we have that node must also be a diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R by Proposition 3.2. Since node is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R as argued and due to Proposition 12.4 (see page 200), there cannot be a minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R which is a proper subset of node. Thence, node must be a minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nWe are now in a position to bring to proof that the first set D in the tuple output by any call of STATICHS in Algorithm 5 contains only these minimal diagnoses w.r.t. the (input) DPI \u3008K,B,P ,N \u3009R that are also minimal diagnoses w.r.t. the (current) DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. In other words, this means that the set of leading diagnoses used for query generation in Algorithm 5 consists only of minimal diagnoses w.r.t. the input DPI that are in agreement with the additional information given by all query answers so far.\nLemma 11.7. Any call to STATICHS within Algorithm 5 yields an output \u3008D,Q,Ccalc,D\u00d7\u3009 where D \u2286mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R .\nProof. The output set D of any call to STATICHS during the execution of Algorithm 5 corresponds to the set Dcalc \u222a DX in STATICHS. As per Lemma 11.6, Dcalc includes only minimal diagnoses w.r.t. \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R. By Lemma 11.4, Dcalc includes only minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R. Therefore, we can conclude that Dcalc \u2286 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R . So, we must show that DX \u2286 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R holds when any call to STATICHS during the execution of Algorithm 5 terminates. We will perform an induction proof.\nBase Case: At the first call of STATICHS during the execution of Algorithm 5, the argument DX passed to STATICHS is the empty set. As argued in the proof of Lemma 11.1, DX is never modified throughout STATICHS. Thus, DX = \u2205 \u2286 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R holds for the output of the first call to STATICHS. Therefore, the proposition of this lemma holds for the output of the first call of STATICHS.\nInduction Step: Assume that the proposition of this lemma holds for the last-but-one call to STATICHS during the execution of Algorithm 5 (Induction Hypothesis). Consider the last, i.e. most recent, call to STATICHS during the execution of Algorithm 5.\nFirst, the set DX given as an input argument to STATICHS at the last call of STATICHS is unmodified throughout the entire execution of STATICHS, as already mentioned. Second, DX = D\u2032 \\ Dout \u2286 D\u2032 holds where D\u2032 is the output of the last-but-one call of STATICHS by Algorithm 5 since the only modification to the set D\u2032 (which is denoted by DX in Algorithm 5) during Algorithm 5 is the deletion (line 21) of exactly those diagnoses Dout in D\u2032 that are invalidated by the addition of the most recent test case (GETINVALIDDIAGS in line 19). That is, the input DX to the most recent call to STATICHS includes only diagnoses that comply with the most recently added test case. Call the most recently added test case tc. By the Induction Hypothesis, D\u2032 \u2286 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222a(P \u2032\\{tc}),N\u222a(N \u2032\\{tc})\u3009R . Notice that either tc \u2208 P \u2032 or tc \u2208 N \u2032 holds, but not both. As DX \u2286 D\u2032, it must be true that DX \u2286 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222a(P \u2032\\{tc}),N\u222a(N \u2032\\{tc})\u3009R and DX complies with the test case tc. Hence, we infer that DX \u2286 mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R . Consequently, the proposition of this lemma must hold for each call of STATICHS during the execution of Algorithm 5.\nThe results proven so far in this section facilitate the proof of correctness of STATICHS:\nProposition 11.1 (Correctness of STATICHS). Any call to STATICHS (given the inputs described in Algorithm 7) within Algorithm 5 terminates and yields an output \u3008D,Q,Ccalc,D\u00d7\u3009 where\n(1) it holds for D that\n168 CHAPTER 11. STATIC DIAGNOSIS COMPUTATION ALGORITHM\n(a) D \u2286 mD\u3008K,B,P,N \u3009R \u2229 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R that satisfy all test cases P \u2032 and N \u2032 such that\n(i) nmin \u2264 |D| \u2264 nmax and (ii) D \u2283 DX,\nif such a set D exists, or\n(b) D is equal to the set of all minimal diagnoses mD\u3008K,B,P,N \u3009R \u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R , otherwise,\nwhere \u201cmost-probable\u201d refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p();\n(2) Q is the current queue of open (non-labeled) nodes of the produced (partial) wpHS-tree,\n(3) Ccalc is the set of all minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R computed so far and\n(4) D\u00d7 is the set of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R computed so far where each diagnosis in D\u00d7 does not satisfy all test cases P \u2032 and N \u2032.\nProof. Termination of any call to STATICHS within Algorithm 5 is granted by the fact that each node is a subset of K wherefore 2|K| is a finite upper bound of the overall number of nodes that might be elements of Q during the execution of any call of STATICHS. Moreover, in each iteration of the repeat-loop in STATICHS, one element is removed from Q (line 6) and no once removed element can ever be readded to Q. The latter is satisfied due to the non-minimality criterion (lines 21-23) that deletes all but one nodes set-equal to some set X \u2286 K before the first node set-equal to X is processed and due to the fact that no once labeled nodes, i.e. those nodes that are elements of Dcalc, DX or D\u00d7, are ever added to Q again (because there is no line of code in STATICHS that does so).\nProposition (1): During the execution of Algorithm 5 (and STATICHS), diagnoses are added to D\u00d7 only in line 22. In this line, only and all diagnoses not complying with the most recent test case are added to D\u00d7 (GETINVALIDDIAGS in line 19, cf. Section 9.2). Hence, no diagnosis in D\u00d7 can be in mD\u3008K,B,P,N \u3009R \u2229 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R . Now, by Lemmata 11.4 and 11.7, we deduce that D \u2282 mD\u3008K,B,P,N \u3009R\u2229mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R that satisfy all test cases P \u2032 and N \u2032. If STATICHS does not terminate due to Q = [], properties (a)-(i) and (a)-(ii) of D are direct consequences of the stop criterion in line 18 in STATICHS. Otherwise, we infer by Lemma 11.5 that (b) must be true.\nPropositions (2) and (3) hold by Lemma 11.3 and the definition of relevant data of a (partial) wpHStree (cf. Remark 4.2).\nProposition (4): This proposition follows from the line of argument in the proof of proposition (1) above.\n11.4. ALGORITHM CORRECTNESS 169\nAlgorithm 7 Iterative Construction of a Static Hitting Set Tree Input: a tuple \u3008\u3008K,B,P ,N \u3009R,Q, t, nmin, nmax,Ccalc,DX,D\u00d7, p(),P \u2032,N \u2032\u3009 consisting of\n\u2022 the DPI \u3008K,B,P ,N \u3009R given as input to Algorithm 5, \u2022 the overall sets of positively (P \u2032) and negatively (N \u2032) answered queries added as test cases to \u3008K,B,P ,N \u3009R so far, \u2022 the current queue Q of open (non-labeled) nodes of a (partial) wpHS-tree, \u2022 some desired computation timeout t, \u2022 a desired minimal (nmin \u2265 2) and maximal (nmax) number of minimal diagnoses to be returned, \u2022 the set Ccalc of all minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R computed so far, \u2022 the set DX of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R computed so far that satisfy all test cases P \u2032 and N \u2032, \u2022 the set D\u00d7 of all minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R computed so far that do not satisfy all test cases P \u2032 and N \u2032. \u2022 a function p : K \u2192 (0, 0.5).\nOutput: a tuple \u3008D,Q,Ccalc,D\u00d7\u3009 where \u2022 D is the current set of leading diagnoses such that\n(a) D \u2286mD\u3008K,B,P,N\u3009R \u2229mD\u3008K,B,P\u222aP\u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R that satisfy all test cases P \u2032 and N \u2032 such that (i) nmin \u2264 |D| \u2264 nmax and\n(ii) D \u2283 DX, if such a set D exists, or\n(b) D is equal to the set of all minimal diagnoses mD\u3008K,B,P,N\u3009R \u2229mD\u3008K,B,P\u222aP\u2032,N\u222aN \u2032\u3009R , otherwise, where \u201cmost-probable\u201d refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p();\n\u2022 Q is the current queue of open (non-labeled) nodes of the produced (partial) wpHS-tree, \u2022 Ccalc is the set of all minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R computed so far and \u2022 D\u00d7 comprises those minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R computed so far that do not satisfy all test cases P \u2032 and N \u2032.\n1: procedure STATICHS(\u3008K,B,P ,N \u3009R,Q, t, nmin, nmax,Ccalc,DX,D\u00d7, p(),P \u2032,N \u2032) 2: tstart \u2190 GETTIME() 3: Dcalc \u2190 \u2205 4: repeat 5: node\u2190 GETFIRST(Q) 6: Q\u2190 DELETEFIRST(Q) 7: \u3008L,C\u3009 \u2190 SLABEL(\u3008K,B,P ,N \u3009R, node,Ccalc,D\u00d7 \u222aDX \u222aDcalc,Q) 8: Ccalc \u2190 C 9: if L = valid then . node is minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R 10: if ISKBVALID(K \\ node, \u3008\u00b7,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) then . ISKBVALID (see Algorithm 1) 11: Dcalc \u2190 Dcalc \u222a {node} . node does satisfy all test cases P \u2032 and N \u2032 12: else 13: D\u00d7 \u2190 D\u00d7 \u222a {node} . node does not satisfy all test cases P \u2032 and N \u2032 14: else if L = closed then . do nothing, no need to store non-minimal diagnoses 15: else . L must be a minimal conflict set 16: for e \u2208 L do 17: Q\u2190 INSERTSORTED(node \u222a {e} ,Q, pnodes(), descending) 18: until Q = [] \u2228 [|Dcalc| 6= \u2205 \u2227 |Dcalc \u222aDX| \u2265 nmin \u2227 (|Dcalc \u222aDX| = nmax \u2228 GETTIME()\u2212 tstart > t)] 19: return \u3008Dcalc \u222aDX,Q,Ccalc,D\u00d7\u3009\n20: procedure SLABEL(\u3008K,B,P ,N \u3009R, node,Ccalc,D(\u00d7,X,calc),Q) 21: for nd \u2208 D(\u00d7,X,calc) do 22: if node \u2287 nd then . node is a non-minimal diagnosis 23: return \u3008closed,Ccalc\u3009 24: for nd \u2208 Q do 25: if node = nd then . node is a duplicate node 26: return \u3008closed,Ccalc\u3009 27: for C \u2208 Ccalc do 28: if C \u2229 node = \u2205 then . the minimal conflict set C can be reused to label node 29: return \u3008C,Ccalc\u3009 30: L\u2190 QX(\u3008K \\ node,B,P ,N \u3009R) . see Algorithm 1 (page 48) 31: if L = \u2019no conflict\u2019 then . node is a diagnosis 32: return \u3008valid,Ccalc\u3009 33: else . L is a new minimal conflict set (/\u2208 Ccalc) 34: Ccalc \u2190 Ccalc \u222a {L} 35: return \u3008L,Ccalc\u3009\nChapter 12\nDYNAMICHS: A Dynamic Iterative Diagnosis Computation Algorithm\nAs the name already suggests, DYNAMICHS (Algorithm 8) is a procedure that solves the problem of Interactive Dynamic KB Debugging defined by Problem Definition 6.1 if used for leading diagnosis computation in Algorithm 5. DYNAMICHS is sound, complete and optimal w.r.t. the set of solutions of the Interactive Dynamic KB Debugging problem (this will be proven in Section 12.4.10). Optimality refers to the best-first computation of minimal diagnoses regarding a given probability measure."}, {"heading": "12.1 Overview and Intuition", "text": "Synoptic View of the Algorithm. DYNAMICHS (Algorithm 8) is employed as a subroutine in Algorithm 5 with mode = dynamic to build up a hitting set tree iteratively. That is, each time DYNAMICHS is called in Algorithm 5, it expands the existing tree only to a sufficient extent in order to determine a desired number of new leading diagnoses used for the generation of the next query. Then, the leading diagnoses set is returned.\nOutside of the DYNAMICHS method in Algorithm 5, a new diagnosis probability distribution is obtained by the diagnosis probability update (cf. Section 9.2). Once this distribution involves one diagnosis, the probability of which exceeds a predefined threshold 1\u2212 \u03c3, the algorithm terminates. The output is a solution KB w.r.t. the current DPI built from this highly probable minimal diagnosis.\nRemark 12.1 In case \u03c3 has a predefined value of zero, the output is the (exact) solution to the problem of Interactive Dynamic KB Debugging for the input DPI. In a scenario where some fault tolerance \u03c3 > 0 is given, the solution KB returned by Algorithm 5 is an approximation of the (exact) solution to Interactive Dynamic KB Debugging for the input DPI where a better approximation can be expected for smaller values of \u03c3 (cf. Remark 9.2). \u201cBetter\u201d in this context refers to the satisfaction of desired semantic properties of the KB returned by Algorithm 5, i.e. desired entailments and desired non-entailments of the KB. The intuition is that specification of additional test cases T guarantees the output of a KB complying with these test cases, whereas accepting one \u2013 albeit highly probable \u2013 of multiple solution KBs without having incorporated T leaves open the possibility for this KB to not fulfill T .\nHowever, answering queries is effort for an interacting user. Therefore, the approach that involves the \u201cearly\u201d termination of the algorithm after a solution KB has a sufficiently high probability (lower than 1) constitutes a trade-off between exactness of the output and the effort of the user and overall execution time of the interactive KB debugging algorithm, respectively.\n171\n172 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nIn case there is no highly probable leading diagnosis, a query constructed from the current set of leading diagnoses is asked to the user. The user\u2019s answer is incorporated into the current DPI resulting in a new DPI. Thereafter, DYNAMICHS is invoked again given this new DPI as an argument.\nStorage of the Search Tree. Between each two calls of DYNAMICHS in Algorithm 5, the \u201cstate\u201d of the current hitting set tree is stored by variables\n\u2022 Dcalc \u2013 computed minimal diagnoses w.r.t. the current DPI,\n\u2022 Q \u2013 the list of open, non-labeled nodes,\n\u2022 Ccalc \u2013 (not necessarily minimal) conflict sets w.r.t. the current DPI computed so far,\n\u2022 D\u2283 \u2013 non-minimal diagnoses w.r.t. the current DPI computed so far,\n\u2022 Qdup \u2013 non-labeled duplicate nodes (i.e. nodes corresponding to tree branches with the same set of edge labels as branches that are already present in the tree)\n\u2022 D\u00d7 \u2013 the empty set (is filled up during Algorithm 5 between two calls of DYNAMICHS with diagnoses from Dcalc that have been invalidated by an answered query)\nwhere nodes in the tree again store (among others) the edge labels on the path from the root node to themselves.\nSearch Tree Update. It is immediately apparent from the enumeration given above that, in comparison to STATICHS, additional collections, i.e. D\u2283 as well as Qdup, need to be maintained in order to \u201cremember\u201d the current tree while Algorithm 5 is processing outside of the method DYNAMICHS. The cause for these additional variables is the tree update necessary after each addition of a test case to a DPI. For, each iteration of DYNAMICHS considers a different DPI in terms of the test cases. And, any two different DPIs in general lead to a different hitting set tree and to different sets of minimal diagnoses and conflict sets. Hence, the idea of the tree update is the following: Reuse the partial hitting set tree T (stored by the variables described above) constructed before the new test case was added to the current DPI DPIj and perform suitable modifications to T in order to obtain a tree T \u2032 such that the further expansion of T \u2032 allows to identify all minimal diagnoses w.r.t. the new DPI DPIj+1 resulting from the addition of the new test case to DPIj . In other words, the tree update seeks to establish a tree that is equivalent to one built by execution of DYNAMICHS using the new DPI DPIj+1 starting from an empty tree.\nNode Storage. Notice that, unlike in STATICHS or HS, it is crucial to store nodes not as sets in DYNAMICHS, but as ordered lists of formulas. That is, each node nd stores a list of all the edge labels along the (directed) path in the hitting set tree from the root node to nd where the order of formulas in the list is given by the order of traversing the edge labels along this path. Additionally, DYNAMICHS stores the attribute nd.cs for each node nd which is an ordered list including the node labels, i.e. the conflict sets, along the path from the root node to nd in analogous way. Associating a node with these two lists instead of one set is necessary from the point of view of the tree update. Because this facilitates the differentiation between two nodes corresponding to an equal (partial) diagnosis. For example, there could be some node nd1 that is \u201credundant\u201d after some query Q has been answered, but there is a set-equal node nd2 which is still \u201crelevant\u201d (set-equality refers to equal sets, not lists, of edge labels stored by two nodes). In this case, the algorithm should get rid of nd1 (in order to save time and space) while preserving node nd2 (in order to maintain completeness). Associating set-equal nodes with each other might thus either lead to unnecessary tree expansion steps (if none is deleted) or incompleteness of the algorithm concerning the consideration of all minimal diagnoses (in case both are deleted).\n12.1. OVERVIEW AND INTUITION 173\nAddition of a Test Case Changes Set of Solutions. Unlike the STATICHS algorithm, which is strongly related to the non-interactive hitting set algorithm HS (Algorithm 2) as outlined in Section 11.1, the hitting set tree produced by DYNAMICHS will usually differ significantly from the non-interactive hitting set tree produced by HS. The reason for this is that in DYNAMICHS the initial DPI DPI0 is not fixed (in that conflict sets and diagnoses are calculated only w.r.t. DPI0), but new test cases are also used for the computation of minimal conflict sets (and thus minimal diagnoses) and not only for the invalidation of diagnoses. Hence, every time a query has been answered and a respective test case has been incorporated into the DPI, the minimal conflict sets computed for the old DPI DPIj might not be minimal conflict sets w.r.t. the current DPI DPIj+1 anymore (see Examples 12.1 and 12.2). On the one hand, a minimal conflict set C w.r.t.DPIj might be a non-minimal conflict set w.r.t.DPIj+1 (since there is a new minimal conflict set C\u2032 \u2282 C w.r.t. DPIj+1). On the other hand, there might be also \u201ccompletely new\u201d minimal conflict sets Ck w.r.t. DPIj+1 which are in no set-relationship with any minimal conflict set w.r.t. DPIj .\nDue to this changing set of minimal conflict sets, the set of minimal diagnoses is variable as well (cf. Proposition 4.6). To see this, let D be a minimal diagnosis w.r.t. DPIj . Then D hits all minimal conflict sets Ck in mCDPIj . Now, assume that D comprises (only) the element ax from Ck, but there is a minimal conflict set C\u2032k in mCDPIj+1 such that C\u2032k \u2286 Ck \\ {ax}. In this case, D is not a (minimal) hitting set of all minimal conflict sets in mCDPIj+1 (since D does not hit C\u2032k), i.e. D is not a (minimal) diagnosis w.r.t. DPIj+1. That means, D needs to be extended (by a hitting set of all minimal conflict sets in mCDPIj+1 it does not hit) in order to become a diagnosis w.r.t. DPIj+1. After extending D, both situations might arise, either thatD is a minimal diagnosis w.r.t.DPIj+1 or thatD is a non-minimal diagnosis w.r.t. DPIj+1. When the latter case occurs, DYNAMICHS might often be able to figure out that (the tree branch corresponding to) D is simply redundant (w.r.t. the new DPI DPIj+1) and does not need to be considered during the further expansion of the hitting set tree (which searches for minimal diagnoses w.r.t. DPIj+1 and not w.r.t. DPIj). That is, such redundant tree branches are unnecessary in order to explore all minimal diagnoses w.r.t.DPIj+1 (cf. Sections 12.1 and 12.4.5 for an explanation and precise characterization of redundancy).\nAs a consequence, the nice property of STATICHS that the set of minimal diagnoses that needs to be taken into account given DPIj+1 is a proper subset of the minimal diagnoses set that needed to be considered given DPIj in no longer valid for DYNAMICHS. That is, the set of remaining solution candidates in DYNAMICHS is not guaranteed to \u201cconverge\u201d constantly towards a singleton comprising only one solution. The DPI, the minimal conflict sets as well as the minimal diagnoses are \u201cdynamic\u201d. What holds for both DYNAMICHS and STATICHS is the guarantee that the set of all (i.e. minimal and non-minimal) diagnoses is constantly shrinking, i.e. aDDPIj \u2283 aDDPIj+1 (as well will later prove by Corollary 12.4).\nSearch Tree Pruning. Let T be the hitting set tree produced in the j-th iteration of DYNAMICHS (i.e. T is the tree that was used to search for minimal diagnoses w.r.t. DPIj). Then, after a new test case has been added to DPIj , there are often redundant subtrees in T that can be pruned. The resulting tree T \u2032 can then be used in the (j + 1)-th iteration of DYNAMICHS to identify minimal diagnoses w.r.t. the new DPI DPIj+1. Using T instead of T \u2032 might lead to a significant time and (more severely) space overhead, due to the unnecessary expansion of redundant branches that are known to give no new information at all. Another approach could be to simply discard the entire tree T and start to construct a new one w.r.t. DPIj+1 from scratch. This strategy, however, will usually also suffer from a non-negligible time overhead since most of the tree T can be safely reused in iteration j+1 and only parts of it must be revised. In particular, this strategy would potentially involve many additional calls of QX (which internally calls an expensive reasoner) as, in the worst case (when no pruning is possible), the entire existing tree might be rebuilt.\nAs we shall see in Remark 12.5, Section 12.4 and Examples 12.1 as well as 12.2, the overhead in terms of (expensive) calls to a reasoner (i.e. calls of QX) due to tree pruning (compared to its impact on\n174 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nthe tree) is absolutely reasonable. In fact, only one call of a \u201cfast version\u201d of QX (see Section 12.4.6) might already lead to the deletion of 75% of the tree branches as one can see in the first pruning step in Example 12.2.\nThe evolution of the hitting set tree produced by Algorithm 5 using DYNAMICHS is thus characterized by alternating expansion and pruning phases. Also for very complex problems, in case that expansion phases are \u201cshort enough\u201d such that tree pruning can take place \u201coften enough\u201d, one might be able to keep the hitting set tree \u201csmall enough\u201d to handle it efficiently. The extent of the expansion phase can be steered by the specification of the leading diagnosis parameters nmin, nmax and t (cf. Section 9.2). In the extreme case, these can be defined in a way (nmin = nmax = 2) the algorithm will allow only the computation of a single further minimal diagnosis (in the first expansion phase: two diagnoses) before DYNAMICHS (i.e. the tree expansion phase) terminates and a further pruning phase might take place.\nHowever, it is not automatically warranted that tree pruning is possible after each expansion phase. Similarly, no certainty is given that the transition from DPIj to DPIj+1 just causes the deletion of parts of the tree and no additional expansion of the tree. In fact, this depends on certain properties of the test case that is added after an expansion phase (i.e. properties of the generated query).\nTest Cases Affect Tree Pruning. Some added test case might give rise to some pruning steps as well as it might induce the construction of new subtrees (where \u201cnew\u201d means that these would be no subtress of a hitting set tree w.r.t. the previous DPIDPIj). The latter situation occurs when \u201ccompletely new\u201d minimal conflict sets (see above) are introduced by the addition of a test case. If this is the only impact of a test case, then this test case has only a negative influence on the time and space complexity. In other words, none of the invalidated minimal diagnoses (and no other nodes in the tree) are redundant; but all of them must additionally hit the set of \u201ccompletely new\u201d minimal conflict sets (in order to become diagnoses w.r.t. DPIj+1). Hence, in this case, the transition from DPIj to DPIj+1 results only in monotonic growth of the tree. If possible, such \u201cnegative-impact test cases\u201d must be avoided. On the other hand, one must strive for the usage of \u201cpositive-impact test cases\u201d, i.e. those that only trigger tree pruning, but no tree expansion. Defining and studying properties that constitute such \u201cpositive-impact test cases\u201d and developing specialized algorithms for extracting exactly those types of queries that enable as substantial and effective pruning as possible is a topic of future research.\nAn idea pertinent to this issue could for example be to attempt to extract a query by means of the conflict set C that labels the root node of the tree. More concretely, if any answer to a query yields a new test case that leads to the introduction of a minimal conflict set that is a proper subset of C, then it is for sure that significant pruning can take place (since entire subtrees starting from the root of the tree can be deleted). For instance, the first query Q1 in Example 12.2 features this property. Roughly, the reasons for that are that Q1 is an entailment of a proper subset Csub of C (i.e. Csub is a justification of Q1, cf. Section 4.2) and Q1 is \u201crelevant\u201d for this conflict set C to be a conflict set. In other words, the latter means that Q1 can be used to \u201creplace\u201d the part Csub of C, i.e. (C \\ Csub) \u222aQ1 is invalid w.r.t. the given DPI. That is, addition of Q1 to the positive test cases asserts the correctness of one part of C, namely Csub (cf. Example 12.2), wherefore the other part must be incorrect (because some part of a conflict set must be definitely incorrect). On the other hand, assignment of Q1 to the negative test cases asserts exactly the incorrectness of Csub wherefore the formulas C \\ Csub become obsolete in the minimal conflict set C yielding the new minimal conflict set C\u2032 := Csub. Another desirable property of Q1 is that addition of Q1 to either set of test cases does not imply the origination of any \u201ccompletely new\u201d conflict sets (see above) which result in additional growth of the tree.\nThat is, in its original form (without assuring only the usage of \u201cpositive-impact test cases\u201d), the time and space complexity of DYNAMICHS is a function of the generated queries. There is a potential to perform significant pruning, but also the risk of significant tree growth. In case mostly \u201cpositiveimpact queries\u201d are generated and asked to the user, the performance might be very nice and significantly superior to the one of STATICHS. In the reverse case, the performance might be also worse than the one\n12.2. ALGORITHM WALKTHROUGH 175\nof STATICHS. In the case of STATICHS, there is no chance for significant pruning, but also no chance for a tree growth that goes beyond the size of the non-interactive tree produced by HS.\nIn STATICHS, there are only expansion phases (in case the tree pruning described by Definition 4.8 is considered part of an expansion phase) which means that the tree constructed by STATICHS will constantly grow (apart from the deleted duplicate nodes and non-minimal diagnoses). All the user can do is hope that Algorithm 5 applying STATICHS will not run out of memory (cf. Section 11.1).\nThe idea is now to be able to use DYNAMICHS instead of STATICHS particularly if the latter runs out of memory soon. If the leading diagnosis parameters are specified small enough to prevent the hitting set tree produced during one expansion phase from becoming too large and test cases are not chosen unfavorably, the DYNAMICHS method should be able to outperform STATICHS significantly, as Examples 11.2 and 12.2 suggest."}, {"heading": "12.2 Algorithm Walkthrough", "text": "Input Parameters. When DYNAMICHS (Algorithm 8) is called for the first time in Algorithm 5, the inputs Ccalc, DX, D\u00d7, P \u2032 and N \u2032 correspond to the empty set and Q = [\u2205] (cf. lines 1-4 and 10 in Algorithm 5). Further on, Dcalc is defined to be the empty set at the beginning of each execution of DYNAMICHS. That is, DYNAMICHS starts the construction of the hitting set tree from an initial tree consisting of a single unlabeled root node \u2205 (\u2208 Q). And, all collections that are later returned by DYNAMICHS in line 25, except for Q, are initially empty. Further input arguments are the DPI \u3008K,B,P ,N \u3009R provided as an input to Algorithm 5, the sets of positively (P \u2032) and negatively (N \u2032) answered queries since the start of Algorithm 5 (both sets initially empty), the leading diagnosis computation parameters nmin, nmax, t (see description in Chapter 7 on page 95) and the probability measure p() := pK() that assigns a probability in the interval (0, 0.5) to each formula in K (see line 5 in Algorithm 5).\nTree Update during First Iteration of DYNAMICHS. Before the repeat-loop in DYNAMICHS is entered, the UPDATETREE function is called (line 4), but has no effect. This holds since UPDATETREE first iterates over all elements in D\u00d7, then over all elements in D\u2283 and finally over all elements in DX where D\u00d7 = D\u2283 = DX = \u2205, as pointed out before.\nThe Main Loop. During the repeat-loop, in each iteration the first node node in the queue Q of open (non-labeled) nodes is processed (GETFIRST, line 6). Notice that, anywhere throughout DYNAMICHS, nodes are added to Q in a way that a sorting of Q in descending order according to pnodes() (cf. Definition 4.9) is maintained (cf. INSERTSORTED in lines 17, 68, 77, 80, 100 and 103). Hence, the most probable node (according to pnodes()) is always processed next.\nSo, when node is processed, it is first deleted from Q (DELETEFIRST, line 7). Then a test is performed whether node \u2208 DX, i.e. whether node is already known to be a minimal diagnosis w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. In case this test is positive, node is directly added to Dcalc, the set of leading diagnoses that will be output by the current call of DYNAMICHS. Otherwise, the DLABEL function is called given node (i.a.) as a parameter (line 11).\nComputation of a Node Label. The DLABEL function processes node as follows. First, the nonminimality criterion (lines 27-29) is checked. That is, among all nodes in Dcalc, one is searched which is a proper subset of node. If such a node nd is found, then node must be a non-minimal diagnosis w.r.t. the current DPI since, anytime throughout the execution of DYNAMICHS, Dcalc contains only minimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R (this will be proven later by Proposition 12.9). In this case, unlike in STATICHS, the branch in the hitting set tree corresponding to node cannot be simply discarded, but needs to be still stored (in the set D\u2283). It is necessary to store non-minimal diagnoses as\n176 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nthese might become minimal diagnoses w.r.t. the new DPI obtained after the subsequent addition of a new test case to the current DPI (cf. Proposition 12.5).\nIn case the non-minimality criterion is not satisfied, the reuse criterion (lines 30-40) is checked next. That is, the set Ccalc containing (not necessarily minimal) conflict sets w.r.t. the current DPI is browsed for a set C such that C and node are disjoint sets. If such a set C is found, there must be some set X \u2286 C which is a minimal conflict set w.r.t. the current DPI. This minimal conflict set X can then be used to label node since the set of edge labels along the path in the tree leading from the root node to node does not hit X (because it does not hit C).\nThe minimality of C is verified by a call of QX(\u3008C,B,P \u222a P \u2032,N \u222a N \u2032\u3009R) that yields X , a minimal conflict set w.r.t. the current DPI (cf. Proposition 4.9; notice that X must be a non-empty set due to Proposition 12.2, for details see Section 12.4). In case X \u2282 C (line 33), before X is returned as a label for node, the following tree pruning steps are performed:\n\u2022 All the conflict sets Ci used as node labels in the hitting set tree or in duplicate tree branches so far (i.e. Ci \u2208 nd.cs for a node nd \u2208 Q\u222aD\u2283\u222aQdup) such thatX \u2282 Ci are replaced byX (PRUNEQDUP and PRUNE in lines 36-38),\n\u2022 any subtree is pruned if its root node is linked to a node now labeled byX (replacing some Ci \u2283 X) by an edge with label ax where ax is in Ci \\X (PRUNEQDUP and PRUNE in lines 36-38) and\n\u2022 for each pruned node nd, if there is a non-pruned node in Qdup suited to construct a node nd\u2032 that can replace nd, nd\u2032 is added to the collection of nodes from which nd was deleted (PRUNEQDUP and PRUNE in lines 36-38),\n\u2022 all the conflict sets Ci \u2208 Ccalc that are proper supersets of X are deleted from Ccalc and X is added to Ccalc (ADDSETDELSUPSETS in line 39).\nOtherwise, C (= X) is directly returned by DLABEL without performing any tree pruning because the reused conflict set C is (still) a minimal conflict set w.r.t. the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R (notice that each element of Ccalc was added to Ccalc as a minimal conflict set w.r.t. some DPI \u3008K,B,P\u222aP \u2032\u2032,N\u222a N \u2032\u2032\u3009R where P \u2032\u2032 \u2286 P \u2032 and N \u2032\u2032 \u2286 N \u2032 during the execution of this or a previous call of DYNAMICHS). For an in-depth explanation of the pruning functions PRUNE and PRUNEQDUP the reader is kindly referred to Section 12.4.6.\nRemark 12.2 During the execution of the first call of DYNAMICHS in Algorithm 5, no tree pruning can take place (neither within the scope of DLABEL nor anywhere else) since all elements of Ccalc (initially the empty set) must be minimal conflict sets w.r.t. the input DPI which is at the same time the current DPI. Pruning of the hitting set tree is only possible in case some non-leaf nodes of the tree are labeled by conflict sets that are not minimal w.r.t. the current DPI.\nGiven that the reuse criterion fails, QX is called given the current DPI \u3008K\\node,B,P\u222aP \u2032,N \u222aN \u2032\u3009R as an argument (line 41). If the output L is equal to \u2019no conflict\u2019, then we know by Proposition 4.9 that node is a diagnosis w.r.t. the current DPI, wherefore the label valid is returned for node. Otherwise, the output L must be a minimal conflict set w.r.t. \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R that has an empty set-intersection with node. Since the reuse criterion failed, i.e. there is no set in Ccalc that does not intersect with node, L must be a fresh minimal conflict set w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R in the sense that L /\u2208 Ccalc must hold. Therefore the label L is first added to Ccalc and then returned by DLABEL as a label for node.\nRemark 12.3 Please notice that this call of QX to label a node is one of the key differences between STATICHS and DYNAMICHS. Whereas the former uses QX exclusively for the computation of minimal conflict sets w.r.t. the (static) input DPI exploiting just the initial sets of positive and negative test cases P and N , respectively, the latter employs QX to compute minimal conflict sets w.r.t. the (dynamic)\n12.2. ALGORITHM WALKTHROUGH 177\ncurrent DPI which includes all new test cases (P \u2032 and N \u2032) resulting from answered queries in the ongoing interactive debugging session so far.\nProcessing of a Node Label. Back in the main procedure, the label L returned by the DLABEL function is processed as follows. If L = valid, then it is a fact that node is a minimal diagnosis w.r.t. the current DPI (cf. Proposition 12.9 in Section 12.4.9) wherefore node is added to the set Dcalc. Otherwise, if nonmin is the returned label for node, node is added to the set D\u2283 of non-minimal diagnoses w.r.t. the current DPI. Otherwise, i.e. if L /\u2208 {valid, nonmin}, then L must be a minimal conflict set w.r.t. the current DPI (see the description of node label computation above). In this case, |L| successor nodes of node are generated (lines 18 and 19). For each logical formula e \u2208 L, a new node is computed from node (and node.cs) as nodee := ADD(node, e) and nodee.cs := ADD(node.cs, L) which means that e is appended to the end of the list node and L is appended to the end of the list node.cs.\nIf there is already a node nd \u2208 Q such that nd = nodee (line 20), where \u2019=\u2019 applied to these lists means that the list nd interpreted as a set is equal to the list nodee interpreted as a set (cf. Section 12.4.1 for an explication of this notation), then there is already a branch in the existing tree which includes the same set of edge labels as the new node nodee. Note that the tree branch corresponding to nd will differ from the one corresponding to nodee in terms of the order of edge labels or (the order of) the node labels visited when traversed starting from the root node. As it makes no sense to expand two branches with equal sets of edge labels in a hitting set tree (cf. rule 6 in Definition 4.8) for time and space complexity reasons and the fact that the sought diagnoses are sets \u2013 and not lists \u2013 of edge labels in the tree, such a duplicate node nodee is stored in the separate list Qdup. This list Qdup is always kept sorted by ascending node-cardinality (INSERTSORTED in line 21).\nThe purpose of storing and not deleting such nodes is the possibility that the now \u201cactive\u201d branch nd might be pruned after the addition of some test case whereas nodee might be unaffected by that pruning step. In this case, nodee, given it meets certain properties (see Section 12.4 for details), can be reactivated and incorporated into the tree in order to replace nd. Had nodee just been discarded instead of being stored, the completeness of Algorithm 5 with mode = dynamic would be violated in general. That is, we would not have any guarantee that all minimal diagnoses w.r.t. the current DPI are actually explored by the algorithm.\nOtherwise, if there is no node in Q that is set-equal to nodee, then nodee is added to the k-th position in Q (INSERTSORTED in line 23) if there are (exactly) k \u2212 1 nodes in Q that have a probability as per pnodes() that is greater than or equal to pnodes(nodee).\nStop Criterion. The repeat-loop of DYNAMICHS is executed until the stop criterion in line 24 is satisfied. The first criterion causing DYNAMICHS to terminate is Q = [] which means that the complete hitting set tree has been constructed and no further nodes can be labeled. In this case, Dcalc comprises all minimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R (cf. Proposition 12.8).\nIf the first criterion is not met, then the second criterion is checked. That is, a test is performed which checks first whether there is at least one new diagnosis w.r.t. the current DPI in Dcalc which was not returned by the last-but-one call of DYNAMICHS (i.e. which is not an element of DX). Notice that this criterion or Q = [] will be definitely met after finite execution time of DYNAMICHS since either new nodes in Q will be processed (and labeled) until there is some new diagnosis w.r.t. the current DPI identified or the Q will become empty.\nAdditionally, the second criterion involves a test that checks whether the cardinality of Dcalc amounts to at least nmin and either |Dcalc| = nmax or more than t time has passed since the start of the execution of DYNAMICHS. In the latter case, nmin \u2264 |Dcalc| < nmax holds. In the former case, |Dcalc| = nmax is satisfied.\n178 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nProcessing of the Leading Diagnoses Returned by DYNAMICHS. When a call of DYNAMICHS in Algorithm 5 returns \u3008Dcalc,Q,Ccalc,D\u00d7,D\u2283,Qdup\u3009, the set Dcalc is stored in the variable DX in Algorithm 5. Between two successive calls of DYNAMICHS in Algorithm 5, only this set DX as well as D\u00d7 are modified. The collections Q, Ccalc, D\u2283 as well as Qdup remain unchanged until they are used as input parameters when it comes to the next call of DYNAMICHS in Algorithm 5.\nIn case one diagnosis Dmax of the current leading diagnoses in DX has a probability greater than or equal to 1\u2212 \u03c3 as per the probability measure pD() (see Section 9.2), the stop criterion of interactive KB debugging is met and the solution KB (K\\Dmax)\u222aUP\u222aP \u2032 w.r.t. the current DPI \u3008K,B,P\u222aP \u2032,N \u222aN \u2032\u3009R is returned to the user (GETSOLKB in line 14, cf. Section 9.2). Thereafter, Algorithm 5 terminates and no more calls of DYNAMICHS take place.\nOtherwise, if no leading diagnosis satisfies the stop criterion, a query Q together with its q-partition P(Q) is computed as has been detailed in Chapter 8 and Section 9.2. An answer u(Q) to this query is submitted by the interacting user (line 17 in Algorithm 5). Then u(Q) along with P(Q) is exploited to figure out the subset Dout of DX that does not comply with u(Q). This set Dout is then deleted from DX and added to D\u00d7. Additionally, Q is added to the positive test cases P \u2032 if u(Q) = true and to the negative test cases N \u2032 otherwise. Subsequently, DYNAMICHS is called again given\n\u2022 the updated parameters DX, D\u00d7, P \u2032 and N \u2032 (which are modified within and outside of DYNAMICHS during the execution of Algorithm 5),\n\u2022 the unchanged parameters Q, Ccalc, D\u2283 and Qdup (which are modified only within DYNAMICHS during the execution of Algorithm 5) and\n\u2022 the constant parameters \u3008K,B,P ,N \u3009R, t, nmin, nmax and pK() (which are not modified within or outside of DYNAMICHS during the execution of Algorithm 5).\nThe execution of this next and any subsequent call to DYNAMICHS runs in analogue way as described so far, except for the effect of the UPDATETREE function called at the very beginning of each execution of DYNAMICHS (recall that the execution of UPDATETREE had no effect during the first execution of DYNAMICHS). We shall now explicate how this function works in all other executions of DYNAMICHS, except for the first one.\nTree Update. Between line 48 and line 69, UPDATETREE goes through all nodes nd \u2208 D\u00d7 (recall that D\u00d7 includes exactly these diagnoses that have been ruled out by the most recently answered query) and first performs the Quick Redundancy Check (QRC, lines 50-54) for nd. If the QRC is not successful, it additionally performs the Complete Redundancy Check (CRC, lines 56-60) for nd.\nThe QRC (for details see Lemma 12.6) aims at identifying whether nd is redundant and can be pruned, i.e. it attempts to find a witness of redundancy of nd. Informally, a redundant node in (redundant subtree of) the tree is a node (subtree) such that the further expansion of the current tree without this node (subtree) still yields to the detection of all minimal diagnoses w.r.t. the current DPI. A witness of redundancy of nd is a minimal conflict set C\u2032 w.r.t. the current DPI such that a superset C \u2283 C\u2032 was used as a node label on the tree path nd represents (that is, there is some i \u2264 |nd.cs| such that C is the i-th element of nd.cs, i.e. C = nd.cs[i]) and the label (nd[i]) of the outgoing edge of C on the path represented by nd is an element not in C\u2032 (that is, an element in C \\ C\u2032). Formal and precise characterizations of redundancy of nodes and the witness of redundancy of a node are given by Definition 12.4 in Section 12.4.5.\nTo this end, the QRC involves the call of QX(\u3008Und.cs \\ nd,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) which returns X . If X is a set (and not \u2019no conflict\u2019), then X is a minimal conflict set w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R (as Und.cs \\ nd \u2286 K, cf. Proposition 4.9). To check if X is in fact a witness of redundancy of nd, X \u2282 C (line 52) is tested for all C \u2208 nd.cs. If such a C is located, X is a witness of redundancy of nd and the QRC is successful (expressed by quickRC \u2190 true in line 53). In this case, the execution is resumed at line 61.\n12.2. ALGORITHM WALKTHROUGH 179\nThe QRC bears its name due to the fact that it requires at most one call of QX (which internally performs expensive calls to a reasoner). Moreover, it passes to QX a (DPI including a) KB of a size that is generally significantly smaller than |K| where |K| is roughly the size of the KB used in the (more expensive) calls of QX made in the DLABEL function. Hence, the QRC will be usually very fast (cf. Proposition 4.8).\nOtherwise, since the negative outcome of the QRC (which is sound, but not complete w.r.t. the finding of a witness of redundancy of nd) does not imply the non-existence of a witness of redundancy of nd, the CRC (for details see Lemma 12.7) must be performed. As the name already suggests, the CRC is sound and complete and will therefore be positive and yield a witness of redundancy if and only if there is some. The CRC involves multiple calls of QX(\u3008nd.cs[i] \\ {nd[i]} ,B,P \u222a P \u2032,N \u222aN \u2032\u3009R), one for each conflict set nd.cs[i] in nd.cs. It is straightforward from the characterization of a witness of redundancy given before that, given the CRC returns a set X , X is a witness of redundancy of nd.\nIf nd is non-redundant, there cannot be any witness of redundancy of nd. Hence, the complete and sound method CRC will not find such a one. Therefore, quickRC = false and completeRC = false must hold in line 61. In this case, the for-loop in line 48 continues with the next node in D\u00d7.\nOn the other hand, if nd is redundant, due to the completeness of CRC, either quickRC = true or completeRC = true must hold when it comes to the execution of the if-statement in line 61. At this point, it is guaranteed that the variable X stores a witness of redundancy of nd.\nThe CRC, contrary to the QRC, generally requires multiple (at most |nd|) calls of QX (which internally performs expensive calls to a reasoner). But, like the QRC, it passes to QX a (DPI including a) KB of a size that is generally significantly smaller than |K|. Furthermore, at most one call of QX will involve more than one call of ISKBVALID (see Algorithm 1), i.e. the function that calls the reasoner. This must be true since CRC only requires an additional call of QX if a witness of redundancy has not yet been found. And, each call of QX that does not find a witness of redundancy of nd returns \u2019no conflict\u2019 which necessitates only a single invocation of ISKBVALID. Hence, each execution of the CRC will be very fast in general as well (cf. Proposition 4.8).\nWhat comes next is the pruning of all redundant nodes in the tree for which X is a witness of redundancy. Essentially, the same pruning steps are performed here as in the reuse criterion described in \u2019Computation of a node label\u2019 above. A detailed discussion of the pruning functions PRUNE as well as PRUNEQDUP can be found in Section 12.4.6.\nNotice that a redundant node is guaranteed to be a redundant node in any further iteration of DYNAMICHS (using a new current DPI that incorporates new test cases). We will prove this by Lemma 12.4 in Section 12.4.5. So, nodes pruned by PRUNE or PRUNEQDUP can be deleted for good and do not need to be stored any longer. Moreover, it should be noted that only redundant nodes are pruned at any pruning step in DYNAMICHS. For, as long as a node in DYNAMICHS is not known to be redundant, some successor node of this node might be a minimal diagnosis w.r.t. the current DPI. Thus, the deletion of such a node could perhaps prevent the algorithm from finding a particular minimal diagnosis which would implicate the algorithm\u2019s incompleteness.\nRemark 12.4 Since the removal of a node from a collection S \u2208 {D\u00d7,Q,Qdup,D\u2283} within the scope of PRUNE or PRUNEQDUP can be followed by the re-addition to S of a suitable duplicate node constructed from a node stored in Qdup (see Section 12.4.6 for a precise explanation of node replacements), D\u00d7 might be changed both in that nodes are deleted from it and added to it during the for-loop (line 48). Therefore, the \u2019for nd \u2208 D\u00d7\u2019-statement must be read as \u2019if nd is a node in the current set D\u00d7 which has not yet been processed\u2019. For a better code readability, we abstained from using a programmatically precise representation of this issue in Algorithm 9.\nDue to the soundness and completeness of QRC paired with CRC concerning the identification of a witness of redundancy for a given node and the accomplished pruning of (at least) all nodes in D\u00d7 for which a witness of redundancy has been extracted, all nodes that are in D\u00d7 when the algorithm reaches\n180 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nline 67 are non-redundant nodes. Consequently, there is no evidence to exclude the remaining nodes in D\u00d7 from the further search for minimal diagnoses. For this reason, each of these nodes is reinserted into Q by INSERTSORTED in line 68 such that the sorting of Q in descending order of pnodes() is maintained. Then these nodes are deleted from D\u00d7. Thus, D\u00d7 = \u2205 holds after each execution of UPDATETREE.\nSo, in DYNAMICHS, unlike in STATICHS, diagnoses (and nodes in general) are not ruled out due to the fact that they contradict an answered query, but only if they are (found to be) redundant. Nevertheless, a diagnosis that contradicts an answered query is a \u201chot candidate\u201d for finding some witness of redundancy. For that reason, UPDATETREE searches for witnesses of redundancy (only) by means of D\u00d7 which includes the most \u201csuspicious\u201d nodes. Namely, it comprises those nodes that were minimal diagnoses w.r.t. the last-but-one DPI, but have been invalidated by the most recently answered query. The two possible reasons for a diagnosis nd to be invalidated are its redundancy as defined above or that it does not hit a new minimal conflict set (which is not a subset of one in nd.cs) that has been introduced by the addition of the test case resulting from the user\u2019s query answer. Thus, it is likely to detect witnesses of redundancy by investigating nodes in D\u00d7, as the QRC and the CRC do. Throughout the pruning steps performed in lines 62-65, witnesses of redundancy extracted from nodes in D\u00d7 are exploited to remove redundant nodes in the other collections Qdup, D\u2283 and Q as well.\nRemark 12.5 It should be noted that the collections Q as well as D\u2283 are not necessarily cleaned from all redundant nodes after all pruning steps in UPDATETREE are finished. At this point, all those redundant nodes are still elements of these collections for which no witness of redundancy was found (there might exist one, though) throughout the redundancy checks (QRC and CRC) performed.\nAssuring the non-existence of redundant nodes in Q and D\u2283 might involve extensive usage of the (expensive) reasoner. In the worst case, one call of QX for each non-leaf node along each path from the root node to a leaf node labeled by nonmin or to a leaf node that has no label would be necessary. However, the number of these non-leaf nodes is generally exponential in the maximum length of such a path in the tree. In comparison, the number of calls of QX for investigating all nodes in D\u00d7 by QRC and CRC is polynomial (linear) in the maximum length of a tree path labeled by \u00d7. For, the number of QX-calls cannot get larger than (nmax \u2212 1)(|ndmax| + 1) where the constant nmax is the maximum number of desired leading diagnoses predefined by the user and |ndmax| is the maximum cardinality of some nd \u2208 D\u00d7. This holds since |D\u00d7| \u2264 nmax \u2212 1 (cf. Corollary 7.3) and QRC requires at most one and CRC at most |ndmax| QX-calls.\nOther than that, the chance of locating new witnesses of redundancy by means of investigating nodes in Q and D\u2283 can be assumed to be smaller than for nodes in D\u00d7 since there is no indication or evidence that these nodes might be redundant. So, cleaning Q and D\u2283 from all redundant nodes might be significant effort with negligible impact. Therefore, DYNAMICHS is designed to focus the search for witnesses of redundancy only on the \u201csuspicious nodes\u201d in D\u00d7.\nAs mentioned above, when the execution arrives at line 70, only nodes that are definitely redundant (because they were deleted due to some witness of redundancy) have been deleted from the sets Q, D\u00d7, D\u2283 and Qdup.\nIn lines 70-78, each node nd \u2208 D\u2283 which has not been deleted throughout the pruning operations in line 65 is processed as follows: If there is no minimal diagnosis D \u2208 DX such that nd \u2283 D, then nd is removed from D\u2283 and reinserted into Q (lines 77 and 78) in a way the sorting of Q in descending order according to pnodes() is maintained (INSERTSORTED). This re-insertion is plausible since there is no more evidence of nd (which is a non-minimal diagnosis w.r.t. the last-but-one DPI) being a non-minimal diagnosis w.r.t. the current DPI (non-minimal diagnoses might become minimal diagnoses by the addition of test cases, cf. Section 12.4.3 and Proposition 12.5).\nOtherwise, nd remains an element of the set of non-minimal diagnoses D\u2283 w.r.t. the current DPI as DX comprises exclusively minimal diagnoses w.r.t. the current DPI and one of these is a proper subset of nd.\n12.3. EXAMPLES 181\nIn lines 79-80, all elements in DX, each of which is a minimal diagnosis w.r.t. the current DPI, are added to Q in a way the sorting of Q in descending order according to pnodes() is maintained.\nRemark 12.6 Please notice that the elements of DX, although they are known to be minimal diagnoses w.r.t. the current DPI, are not directly added to the set of found leading diagnoses Dcalc w.r.t. the current DPI, but to Q. The reason for this is that there might be (not-yet-found) minimal diagnoses w.r.t. the current DPI (nodes in Q or successor nodes thereof) which were not minimal diagnoses w.r.t. the lastbut-one DPI (and thus are no elements of DX) that have a higher probability as per pnodes() than elements of DX. For instance, such diagnoses might have been added to Q from the set D\u2283 in line 77.\nIn this way, since always the first (and most probable) node in Q is processed next, a guarantee is given that Dcalc always comprises the |Dcalc| most probable minimal diagnoses w.r.t. the current DPI as per pnodes(). The knowledge of the validity of minimal diagnoses in DX w.r.t. the current DPI is however not forgotten, but exploited in line 12 (i.e. no call of DLABEL and QX is necessary for a node in DX to be added to Dcalc), as elucidated in \u2019The main loop\u2019 above."}, {"heading": "12.3 Illustrating Examples", "text": "In this section we will give two examples of how interactive KB debugging using DYNAMICHS (Algorithm 5 with parametermode = dynamic) works. The first one will show the similarities and differences between the usage of DYNAMICHS (within Algorithm 5) and HS (within Algorithm 3) since it will depict the application of STATICHS on the same example DPI (see Table 15.3) that was used to show the functionality of HS in examples 4.8 and 4.9. At the same time, the first example will provide evidence that solving the problem of Interactive Dynamic KB Debugging can be less efficient than solving the problem of Interactive Static KB Debugging in terms of the number of query answers required from an interacting user. This will be discussed in more detail in Chapter 13.\nThe second example is supposed to deepen the reader\u2019s understanding of the way DYNAMICHS works. To this end, the example DPI provided by Table 4.2 will be used which constitutes a significantly harder (interactive) debugging task than the DPI investigated in the first example. This example will involve the construction of a relatively large hitting set tree in the first iteration of DYNAMICHS (which behaves very similarly to STATICHS as well as HS and constructs the same wpHS-tree as these methods), but will then show the power of the tree pruning that can be exploited in Interactive Dynamic KB Debugging in that the tree will shrink rapidly after the addition of test cases. Hence, this example will emphasize the advantage of the decision to search for a solution of Interactive Dynamic KB Debugging rather than for a solution of Interactive Static KB Debugging (more on that in Chapter 13).\nNotice that, in the following examples, whenever some tuple or list occurs in an expression using set operators, it is interpreted as a set.\nExample 12.1 In this example we assume that the author (called user throughout this example) of the (admissible) DPI \u3008K,B,P ,N \u3009R given by Table 15.3 applies Algorithm 5 with mode = dynamic to interactively debug \u3008K,B,P ,N \u3009R. Further, the same scenario and parameter settings as in Example 11.1 are supposed. That is, nmin = nmax = 2 (notice that the time limit t is irrelevant in this case), q := 1 (cf. Chapter 8), qsm() is equal to any query selection measure described in Section 9.3, pK(ax ) := c < 0.5 for all ax \u2208 K, i.e. all formula fault probabilities are specified to be equal (to some constant c) and \u03c3 := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using DYNAMICHS are visualized by Figures 12.1 and 12.2. We use the same notation as in Figures 4.2, 4.3, 11.1, 11.2 and 11.3 which is described in Examples 4.8, 4.9, 11.1 and 11.2.\nIn the first iteration, i.e. during the execution of the first call of DYNAMICHS during Algorithm 5, the root node (initially the empty set) is labeled by the minimal conflict set \u30081, 2, 5\u3009 w.r.t. \u3008K,B,P ,N \u3009R and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.cs = nd2.cs =\n182 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nnd3.cs = [\u30081, 2, 5\u3009], are added to the queue of open nodes Q. Since all formulas have been assigned an equal fault probability, DYNAMICHS conducts a breadth-first tree construction (as displayed by the numbers i\u00a9 that give the order of node labeling). That is, Q in this case is a first-in-first-out queue. In this vein, first [1] and then [2] are identified as minimal diagnoses w.r.t. the given DPI.\nSince Dcalc = {[1], [2]} has a cardinality of nmin = nmax = 2, the stop criterion of DYNAMICHS causes it to terminate and return \u3008Dcalc,Q,Ccalc,Q,D\u00d7,D\u2283,Qdup\u3009 = \u3008 {[1], [2]}, [[5]], {\u30081, 2, 5\u3009}, \u2205, \u2205, []\u3009, as shown in the upper right column in Figure 12.1.\nThen, in Algorithm 5, outside of the DYNAMICHS procedure, the first query Q1 = {E \u2192 \u00acA} is computed from the leading diagnoses set {[1], [2]}. The q-partition P(Q1) associated with Q1 is \u3008{[1]} , {[2]} , \u2205\u3009. The user\u2019s answer u(Q1) to Q1 is then false . Thence, the set Dout is calculated from P(Q1) as D+(Q1) = {[1]} (due to negative answer, cf. Remark 7.4), deleted from DX := DX \u222aDcalc to yield DX = {[2]} and added to D\u00d7 to yield D\u00d7 = {[1]}. Now, the set DX corresponds to the set of all computed (i.e. added to Dcalc) minimal diagnoses w.r.t. the last-but-one DPI \u3008K,B,P ,N \u3009R that are minimal diagnoses w.r.t. current DPI \u3008K,B,P ,N \u222a {Q1}\u3009R, i.e. that satisfy the most recently answered queryQ1. The set D\u00d7 comprises all computed (i.e. added to Dcalc) minimal diagnoses w.r.t. the last-butone DPI \u3008K,B,P ,N \u3009R that are not minimal diagnoses w.r.t. current DPI \u3008K,B,P ,N \u222a{Q1}\u3009R, i.e. that do not satisfy the most recently answered query Q1.\nThese sets DX and D\u00d7 along with the collections Q, Qdup, D\u2283 and Ccalc which are unmodified outside of DYNAMICHS are used as input arguments for the second call of DYNAMICHS. Notice that, in Figures 12.1 and 12.2, the resulting values of operations performed within DYNAMICHS are given in the righthand column above the dashed line whereas values computed outside of DYNAMICHS are given below the dashed line.\nThe execution of the second call of DYNAMICHS starts with a call of the UPDATETREE function. The purpose of this function is to transform the hitting set tree T that was constructed by the first call of DYNAMICHS into an updated hitting set tree T \u2032. Whereas the tree T was used to locate minimal diagnoses w.r.t. the last-but-one DPI \u3008K,B,P ,N \u3009R, the modified tree T \u2032 should serve to generate minimal diagnoses w.r.t. the current DPI \u3008K,B,P ,N \u222a {Q1}\u3009R. The parameters DX, D\u00d7, Q, Qdup, D\u2283 and Ccalc that represent the tree T (given at the top of the lefthand column in Figure 12.1), where DX \u222aD\u00d7 is equal to the set Dcalc produced by the first call of DYNAMICHS, are i.a. given as input arguments to the UPDATETREE function.\nAs a first step within UPDATETREE, a redundancy check is performed for each diagnosis in D\u00d7. In this case D\u00d7 = {D1} since D1 is the only minimal diagnosis that has been ruled out by the most recently added negative test case Q1. The purpose of the redundancy check is to figure out whether D1 is redundant w.r.t. the current DPI and must be pruned or whether it might be extended to become a minimal diagnosis w.r.t. the current DPI.\nFirst, the Quick Redundancy Check (QRC) QX(\u3008{2, 5} ,B,P ,N \u222a {Q1}\u3009R) = \u30082, 5\u3009 (line 50 in DYNAMICHS) is executed for D1 which detects (line 52 in DYNAMICHS) that D1 (and possibly some further nodes) is redundant and can be pruned. This holds since the minimal conflict set \u30081, 2, 5\u3009 w.r.t. the last-but-one DPI \u3008K,B,P ,N \u3009R is not a minimal conflict set w.r.t. the current DPI \u3008K,B,P ,N \u222a{Q1}\u3009R because \u30082, 5\u3009 returned by QX is already a minimal conflict set w.r.t. the current DPI (cf. Proposition 4.9). We call the minimal conflict set \u30082, 5\u3009 a witness of redundancy for D1. Hence, all branches in the hitting set tree starting from the outgoing edge of \u30081, 2, 5\u3009 labeled by 1 can be safely deleted from all collections representing the new tree T \u2032 (warranted that all minimal diagnoses w.r.t. the current DPI can still be generated from the pruned tree T \u2032).\nPlease notice that the QRC involves only a single call of QX using a KB of a size (here: 2) that is generally significantly smaller than |K| (here: 7) which is roughly the size of the KB used in calls of QX made in the DLABEL function. Hence, the QRC will be usually very fast.\nAn illustration why \u30082, 5\u3009 \u201creplaces\u201d \u30081, 2, 5\u3009 as a minimal conflict set w.r.t. the current DPI can be given as follows: First, \u30081, 2, 5\u3009 is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R as it is a set-minimal subset\n12.3. EXAMPLES 183\nof K that entails {\u00acA} = n1 \u2208 N , there is no other negative test case in N except for n1 and there is no proper subset C\u2032 of \u30081, 2, 5\u3009 where C\u2032 \u222a B \u222a UP violates any r \u2208 R (see example 4.2 for a detailed explanation). Second, formula 2 implies in particular E \u2192 Y which, along with formula 5 (Y \u2192 \u00acA), yields E \u2192 \u00acA. As the negative answer to Q1 is equivalent to postulating that {E \u2192 \u00acA} must not be entailed by the KB desired by the user, we have that \u30082, 5\u3009 is a conflict set w.r.t. \u3008K,B,P ,N \u222a {Q1}\u3009R. As neither {2} nor {5} is a invalid KB w.r.t. \u3008\u00b7,B,P ,N \u222a {Q1}\u3009R (cf. Corollary 4.1 and Definition 4.1), we have that \u30082, 5\u3009 is a minimal conflict set w.r.t. \u3008K,B,P ,N \u222a {Q1}\u3009R.\nBecause the QRC has been successful, yielding some witness of redundancy of D1, the Complete Redundancy Check (CRC) is no more necessary and the collections Qdup, Q, D\u00d7 as well as D\u2283 are processed by the PRUNE and PRUNEQDUP functions, respectively, which involve the removal of all nodes in these collections that are redundant due to the witness \u30082, 5\u3009. In other words, all nodes are eliminated which correspond to a path in the tree that includes a node label Cold \u2283 \u30082, 5\u3009 and the label e of the outgoing edge of Cold on this path is an element of Cold \\ \u30082, 5\u3009. Moreover, all the supersets of \u30082, 5\u3009 in Ccalc (here, only \u30081, 2, 5\u3009) are replaced by \u30082, 5\u3009 since they are not minimal conflict sets anymore (ADDSETDELSUPSETS).\nThe pruning of nodes is expressed by dashed arrows in the pictures labeled by \u2019Updated Tree\u2019 in Figures 12.1 and 12.2 where the location of cutting a branch is marked by a crossline at the shaft of a dashed arrow. Furthermore, the elements of \u201cold\u201d minimal conflict sets that are no more elements of known (i.e. already computed) current minimal conflict sets are crossed out. As shown by the picture \u2019Updated Tree\u2019 in the righthand column of Figure 12.1, D1 is the only removed node during the pruning steps using the witness of redundancy \u30082, 5\u3009.\nSince D\u2283 = \u2205, UPDATETREE directly jumps to the last three lines where all elements of DX are readded to Q in sorted order (but at the same time remain elements of DX). In the figure, this is displayed by the Q1\n=\u21d2 pointing to a question mark (which stands for an open node) instead of a checkmark as in the case of the STATICHS algorithm. Notice that, although it is a fact that all elements of DX are minimal diagnoses w.r.t. the current DPI, this step is necessary in order to make sure the set Dcalc returned by any call of DYNAMICHS actually comprises the |Dcalc| most probable minimal diagnoses w.r.t. the current DPI. For, there might be, for instance, some node that is a non-minimal diagnosis w.r.t. the last-but-one DPI (and is thus not an element of DX), but becomes a minimal diagnosis w.r.t. the current DPI and has a higher probability than some node in DX. Additionally, we want to point out that no calls of the DLABEL procedure are needed for diagnoses in DX as we know their label must be valid. This is reflected by the test in line 8 in DYNAMICHS.\nIn the figure, all the updated collections D\u2283, Ccalc, Q as well as Qdup, after being processed by UPDATETREE are shown at the bottom of fields labeled by UPDATETREE. We want to remark that D\u00d7 is always the empty set at the end of the execution of UPDATETREE since each node in D\u00d7 gets either pruned or is reinserted into Q as an open node. These updated collections represent the new pruned hitting set tree that can be further constructed in order to detect all and only minimal diagnoses w.r.t. the current DPI \u3008K,B,P ,N \u222a{Q1}\u3009R. Note that the actions carried out by UPDATETREE take place between steps 4\u00a9 and 5\u00a9.\nThe expansion of this tree during the repeat-loop in DYNAMICHS is depicted by the picture named \u2019Iteration 2\u2019 in Figure 12.1. Namely, first (step 5\u00a9) the node [2] is directly labeled by valid (line 8) since it is a known minimal diagnosis w.r.t. the current DPI (as explained before). In the sixth step, [5] is labeled by the minimal conflict set \u30081, 2, 7\u3009 w.r.t. the current DPI and three further nodes ([5, 1], [5, 2] and [5, 7], all with nd.cs = [\u30082, 5\u3009 , \u30081, 2, 7\u3009]) are generated as successor nodes of [5] and are added to Q. Now, [5, 1] (first-in-first-out) is the foremost node in Q and is thus processed next and found to be a minimal diagnosis w.r.t. the current DPI. Therefore, DYNAMICHS terminates and returns i.a. the new set of leading diagnoses Dcalc = {[2], [5, 1]}.\nPlease notice the difference here to Example 11.1 where the node {5, 1} never became part of Q in STATICHS due to the existence of a minimal diagnosis [1] w.r.t. the input DPI \u3008K,B,P ,N \u3009R which is\n184 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\na proper subset of this node (and due to the fact that STATICHS must only consider minimal diagnoses w.r.t. the input DPI). In the current example, this node can only become relevant w.r.t. the current DPI if all (known) diagnoses (here, only [1]) that are proper subsets of it have already been pruned. It should now be clear to the reader why non-minimal nodes cannot be deleted for good as in STATICHS and why the set D\u2283 is necessary in DYNAMICHS.\nThis leading diagnosis [5, 1] is also the reason why the second query Q2 = {E \u2192 G} is different from the second query (Y \u2192 \u00acA) calculated in Example 11.1.\nThe execution of the algorithm continues in an analogue manner as explained so far. In the following, we just want to explain some interesting aspects in the rest of its execution:\n\u2022 After the query Q3 = {Y \u2192 \u00acA} (the same query as the second query in Example 11.1) is answered negatively and Q3 is added to N \u2032 yielding the current DPI \u3008K,B,P ,N \u222a {Q1, Q2, Q3}\u3009R, the UPDATETREE function not only prunes [2] = D2 \u2208 D\u00d7 and adds [5, 7] = D4 \u2208 DX to Q as we delineated above for the first query Q1, but adds [5, 2] \u2208 D\u2283 to Q as well. The reason for that is the deletion of the minimal diagnosis [2] w.r.t. the last-but-one DPI \u3008K,B,P ,N \u222a {Q1, Q2}\u3009R wherefore the last evidence for the non-minimality of node [5, 2] has been deleted. Hence, the status of [5, 2] as a non-minimal diagnosis is no more justified wherefore it must be added to the queue to preserve the completeness of the algorithm w.r.t. the finding of all minimal diagnoses w.r.t. the current DPI. And, indeed, [5, 2] is identified as minimal diagnosis (D5) in iteration 4.\n\u2022 For each element of D\u00d7 during each execution of UPDATETREE throughout the execution of Algorithm 5, the Quick Redundancy Check (QRC) is successful. That is, each witness of redundancy used for pruning throughout the entire runtime of the algorithm could be determined very fast. Namely, as it is easy to see from line 50 in DYNAMICHS, the KB used in the call of QX in the QRC for some node nd has a size in O((|nd| \u2212 1)|Cmax|) where Cmax is the minimal conflict set of maximum cardinality in Ccalc. In most of the cases, |nd| |K| as well as |Cmax| |K| will hold. The (usually more expensive) Complete Redundancy Check (CRC), which requires O(|nd|) calls to QX with a KB of size O(|Cmax| \u2212 1), is thus never employed.\n\u2022 In this example, the same minimal diagnosis [5, 7] is used to compute the finally returned solution KB as in Example 11.1. The only difference between both outputs is that the KB (K \\ [5, 7]) \u222aQ4 returned by DYNAMICHS in this example contains the new positive test case Q4 \u2208 P \u2032. The output by STATICHS in Example 11.1 does not contain any newly specified positive test case in P \u2032 (cf. Remark 9.9), just the union of the \u201coriginal\u201d positive test cases in P (apart from that, there is not even a newly specified positive test case in Example 11.1).\n\u2022 In spite of finding the same solution diagnosis, STATICHS requires fewer queries than DYNAMICHS. Notably, DYNAMICHS even needs a proper superset of the queries asked by STATICHS (Q1, Q2 in Example 11.1 are equal to Q1, Q3 in our current example) in this case. Such a proposition however cannot be made in general since the queries formulated by STATICHS generally differ from those formulated by DYNAMICHS. In this vein, it might just as well be the case that it takes DYNAMICHS fewer queries to finish than it takes STATICHS, due to its advantages in tree pruning.\nAll in all, the execution of Algorithm 5 in this example performs\n\u2022 2 full QX calls, i.e. calls of QX using the KBK\\node for a node node that actually return a minimal conflict set (there are two minimal conflict sets labeled by C in Figures 12.1 and 12.2 which do not result from QRC, CRC or the minimality test of a conflict set in line 32 of DYNAMICHS),\n\u2022 4 fast QX calls, i.e. executions of QX within the scope of the QRC (one call of QX each for the QRC of D1, D3, D2 and D5),\n12.3. EXAMPLES 185\n\u2022 5 validity checks, i.e. calls of QX that return \u2019no conflict\u2019 (one check for each of the five found minimal diagnoses where the identification of diagnoses D2 at step 5\u00a9, D2 at step 9\u00a9, D4 at step\n14\u00a9 and D4 at step 16\u00a9 does not require any call to a reasoning service by means of DX, see line 8 in DYNAMICHS; notice that QX does only perform a single KB validity check by ISKBVALID in case it returns \u2019no conflict\u2019, see Algorithm 1) and\n\u2022 4 tree update processes involving 4 pruned nodes (1 per tree update),\ncomputes\n\u2022 5 minimal diagnoses (D1, D2, D4 w.r.t. the input DPI and D3 and D5 w.r.t. some DPI resulting from the input DPI by addition of new test cases),\n\u2022 6 minimal conflict sets (\u30081, 2, 5\u3009 as well as \u30081, 2, 7\u3009 w.r.t. the input DPI and the subsets thereof \u30082, 5\u3009, \u30082, 7\u3009, \u30085\u3009 and \u30087\u3009 w.r.t. some DPI resulting from the input DPI by addition of new test cases) and\n\u2022 4 queries and asks the user 4 logical formulas (1 per query)\nand stores\n\u2022 a maximum of 4 nodes (where node refers to the internal representation of a node nd in DYNAMICHS as a list of edge labels (nd) and a list of node labels (nd.cs) along a path from the root node to a leaf node).\n188 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nExample 12.2 Let us now consider the (admissible) DPI \u3008K,B,P ,N \u3009R given by Table 4.2. We assume an expert (called user throughout this example) in the domain Dom modeled by K who wants to find a solution to Interactive Dynamic KB Debugging for the given DPI \u3008K,B,P ,N \u3009R by means of Algorithm 5 with mode = dynamic. Further, the same scenario and parameter settings as in Example 11.2 are supposed. That is, nmin = nmax = 3 (notice that the time limit t is irrelevant in this case), q := 1 (cf. Chapter 8), qsm() is equal to any query selection measure described in Section 9.3, pK\u0303\u222aK : K\u0303\u222aK \u2192 [0, 1] is given such that pK(ax ) for ax \u2208 K resulting from the application of GETAXIOMSPROBS is as given by Table 11.1 and \u03c3 := 0.\nThe tree constructed and parameters computed and used by Algorithm 5 using DYNAMICHS are visualized by Figures 12.3 and 12.4. We use the same notation as in Figures 4.2, 4.3, 11.1, 11.2, 11.3, 12.1 and 12.2 which is described in Examples 4.8, 4.9, 11.1, 11.2 and 12.1.\nAfter the initialization of variables, Algorithm 5 calls the function GETFORMULAPROBS in line 5 which exploits pK\u0303\u222aK() to calculate the function pK() giving the fault probabilities of formulas in K (cf. Sections 4.6.1, 9.2 and Example 4.7).\nThen, DYNAMICHS is called for the first time, resulting in the hitting set tree given in the first picture in Figure 12.3. As outlined by the numbers i\u00a9 indicating at which point in time a node is labeled, the root node (initially the empty set) is labeled first by C1 := \u30081, 2, 5\u3009 and three successor nodes, namely nd1 := [1], nd2 := [2] as well as nd3 := [5] with nd1.cs = nd2.cs = nd3.cs = [\u30081, 2, 5\u3009], are added to the queue of open nodes Q. Contrary to Example 12.1, where the tree was built up in breadth-first order, in this example the formula probabilities p() := pK() given by Table 11.1 are used to assign a probability pnodes(n) to each path n in the tree starting from the root node (cf. Formula 4.6 and Definition 4.9). In this vein, the node corresponding to the outgoing edge of C1 labeled by the formula with the largest fault probability among all formulas in C1 is processed next. That is, the node [1] with pnodes([1]) = 0.41 (as opposed to the nodes [2] and [5] with 0.25 each) is labeled next. The DLABEL procedure, after checking whether [1] is a non-minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R (check is negative), computes another minimal conflict set C2 := \u30082, 4, 6\u3009 such that [1] \u2229 C2 = \u2205 (C2 is not hit by the node [1]) to constitute a label for node [1]. The successor nodes [1, 2], [1, 4] and [1, 6] of [1] are generated and added to the list Q in a way that the sorting of Q in descending order of pnodes() is maintained.\nSince [1, 4] (0.28) as well as [1, 6] (0.27) have a larger probability (as per pnodes()) than the nodes [2] (0.25) and [5] (0.25), Q is given by [[1, 4], [1, 6], [2], [5], [1, 2]] when it comes to the processing of the next node. Since DYNAMICHS always treats the first node of Q next, it identifies the first minimal diagnoses D1 := [1, 4] andD2 := [1, 6] w.r.t. \u3008K,B,P ,N \u3009R at steps 3\u00a9 and 4\u00a9, respectively. At step 5\u00a9, when node [2] is processed, a minimal conflict set C3 := \u30081, 3, 4\u3009 is computed and set as a label for [2], giving rise to the generation of three further nodes [2, 1], [2, 3] and [2, 4], all with ndi.cs = [\u30081, 2, 5\u3009 , \u30081, 3, 4\u3009].\nHowever, notice that not all of these new nodes are added to Q, contrary to STATICHS (cf. Example 11.2). For, there is already a node [1, 2] corresponding to the set {1, 2} in Q. Due to the test performed in line 20, this duplicate node [2, 1] is assigned to the list Qdup which is expressed in the figure by dup. Since diagnoses are sets, not lists, [1, 2, ax 1, . . . , axk] and [2, 1, ax 1, . . . , axk] constitute one and the same diagnosis and it is irrelevant whether the one or the other is found. Hence, the nodes [1, 2] and [2, 1] are regarded as duplicates. Nevertheless, ndi := [2, 1] (with ndi.cs = [\u30081, 2, 5\u3009 , \u30081, 3, 4\u3009]) must not be completely deleted as it might be the case that (some successor node of) ndj := [1, 2] (with ndj .cs = [\u30081, 2, 5\u3009 , \u30082, 4, 6\u3009]) becomes redundant due to the eventual addition of some test case. For example, in case the reason for the redundancy of ndj is given (only) by a witness of redundancy that is a subset of \u30082, 4, 6\u3009, ndj is pruned and replaced by the node ndi which is still non-redundant.\nThence, only [2, 3] and [2, 4] are added to Q as successor nodes of the processed node [2]. Next, the minimal conflict set C2 = \u30082, 4, 6\u3009 is reused (lines 30-40 in DLABEL) as a label for node [5] with pnodes([5]) = 0.25 and the three new nodes [5, 2], [5, 4] as well as [5, 6] are generated and assigned to Q at step 7\u00a9. Then, the fourth minimal conflict set C4 := \u30081, 5, 6, 8\u3009 is computed to label the node [2, 4] with pnodes([2, 4]) = 0.18 and the four new nodes [2, 4, 1], [2, 4, 5], [2, 4, 6] as well as [2, 4, 8] are generated\n12.3. EXAMPLES 189\nand assigned to Q st step 8\u00a9. At step 9\u00a9, the third minimal diagnosis D3 := [5, 4] w.r.t. \u3008K,B,P ,N \u3009R is eventually found and added to Dcalc which now has reached a cardinality of 3 = nmin = nmax wherefore DYNAMICHS stops and returns i.a. the set of leading diagnoses Dcalc = {[1, 4], [1, 6], [5, 4]}. The returned values are given in the lefthand column in Figure 12.3.\nAs in Example 11.2, where a debugging session for the same DPI using STATICHS is presented, the first query Q1 is computed as {B v K} and answered by true by the user. The assignment of Q1 to the positive test cases of the DPI \u3008K,B,P ,N \u3009R brings the opportunity to perform some significant pruning actions (within the function UPDATETREE called at the beginning of the second call of DYNAMICHS). These are shown in the tree with the caption \u2019Updated Tree\u2019 and in the righthand column in Figure 12.3.\nAs a first step within UPDATETREE, a redundancy check is performed for each diagnosis in D\u00d7. In this case D\u00d7 = {D3} = {[5, 4]} since D3 is the only minimal diagnosis that has been ruled out by the most recently added positive test case Q1. The purpose of the redundancy check is to figure out whether D3 is redundant w.r.t. the current DPI and must be pruned or whether it might be extended to become a minimal diagnosis w.r.t. the current DPI.\nFirst, the Quick Redundancy Check (QRC) QX(\u3008{1, 2, 6} ,B,P \u222a {Q1} ,N \u3009) = \u30081\u3009 (line 50 in DYNAMICHS) is executed for D3 where the KB {1, 2, 6} used in this call of QX is obtained by deletion of node := D3 from the union of all conflict sets (the elements of node.cs) along the path that corresponds to D3, i.e. {1, 2, 6} = (\u30081, 2, 5\u3009 \u222a \u30082, 4, 6\u3009) \\ [5, 4]. By means of the QRC it is figured out (line 52 in DYNAMICHS) that D3 (and possibly some further nodes) is redundant and can be pruned. This holds since the minimal conflict set \u30081, 2, 5\u3009 w.r.t. the last-but-one DPI \u3008K,B,P ,N \u3009R is not a minimal conflict set w.r.t. the current DPI \u3008K,B,P\u222a{Q1} ,N \u3009R because \u30081\u3009 returned by QX is already a minimal conflict set w.r.t. the current DPI (cf. Proposition 4.9). We call this minimal conflict set \u30081\u3009 a witness of redundancy for D3. Hence, all branches in the hitting set tree starting from an outgoing edge of \u30081, 2, 5\u3009 labeled by 2 or by 5 can be safely deleted from all collections storing nodes in DYNAMICHS.\nAn illustration why \u30081\u3009 \u201creplaces\u201d \u30081, 2, 5\u3009 as a minimal conflict set w.r.t. the current DPI can be given as follows: First, \u30081, 2, 5\u3009 is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R as it is a set-minimal subset of K that entails {A v K} = n1 \u2208 N and there is no proper subset C\u2032 of \u30081, 2, 5\u3009 where C\u2032 \u222a B \u222a UP violates any r \u2208 R or entails any n \u2208 N (see example 4.3 for a detailed explanation). Second, considering the current DPI \u3008K,B,P \u222a {Q1} ,N \u3009R, we have that \u30081, 2, 5\u3009 \u222a B \u222a UP\u222a{Q1} |= n1, too. However, {2, 5} = {B v G,G v K} |= {B v K} = Q1 implies that B \u222a UP\u222a{Q1} \u2287 Q1 can replace the subset {2, 5} of the conflict set \u30081, 2, 5\u3009. For, formula 1 (A v B) along with Q1 (B v K) already entails n1. Further, B \u222a UP\u222a{Q1} cannot violate any negative test case ni \u2208 N or requirement rj \u2208 R by the admissibility of the input DPI \u3008K,B,P ,N \u3009R, the fact that Q1 is a query, Corollary 7.3, Definition 3.6 and Proposition 3.4. Thus, by Definition 4.1, \u30081\u3009 is in fact a minimal conflict set w.r.t. the current DPI \u3008K,B,P \u222a {Q1} ,N \u3009R.\nNow, the first nice thing at this point is that \u30081\u3009 is not only a witness of redundancy of nodes nd where \u30081, 2, 5\u3009 \u2208 nd.cs, but of each nd (in the tree or in the set Qdup of duplicate nodes) where nd.cs contains a conflict set that is a proper superset of \u30081\u3009. That is, \u30081\u3009 also replaces \u30081, 3, 4\u3009 as well as \u30081, 5, 6, 8\u3009. This implicates that two outgoing edges (those labeled by 2 or 5) of \u30081, 2, 5\u3009, two outgoing edges (those labeled by 3 or 4) of \u30081, 3, 4\u3009 and three outgoing edges (those labeled by 5, 6 or 8) of \u30081, 5, 6, 8\u3009 can be pruned.\nThe second nice thing that has an even more significant bearing on tree pruning than the first thing is that \u30081\u3009 is a witness of redundancy of the conflict set that labels the root node. That is, pruning can take place at the very top of the tree and two of three subtrees rooted at successor nodes of the root node can be pruned. That is, for instance, within the rightmost subtree of the root node in the picture with caption \u2019Updated Tree\u2019 in Figure 12.3 no pruning is possible at all since the conflict set \u30082, 4, 6\u3009 labels the root node of this subtree and \u30081\u3009 is not a subset of \u30082, 4, 6\u3009. However, this subtree is still redundant since it is connected with the root node by a \u201credundant\u201d edge labeled by 5. As a consequence, we can observe the pruning of a total of 9 nodes (of altogether 12 nodes in the tree) in only one execution of UPDATETREE.\n190 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nNow, to receive an impression of the power of tree pruning in DYNAMICHS, the reader is invited to compare the trees used in iterations 2 and 3 in the current example (the bottom left pictures in Figure 12.3 and Figure 12.4) with the trees used in iterations 2 and 3 in Example 11.2 (the bottom picture in Figure 11.2 and the picture in Figure 11.3) which deals with the debugging of the same DPI (just by means of STATICHS instead of DYNAMICHS), uses the same sets of leading diagnoses in each iteration, thus the same queries, and of course the same user (that gives the same answers in both examples).\nAfter all diagnoses of DX are added to Q as a final action within UPDATETREE, the repeat-loop of the second iteration of DYNAMICHS is entered. Here, the minimal diagnoses D1 (pnodes(D1) = 0.28, step 11\u00a9), D2 (0.27, 12\u00a9) and D4 (0.09, 13\u00a9) are found and assigned to the empty set Dcalc before DYNAMICHS terminates again. Notice that only one call of the DLABEL procedure is required in the second iteration (for node [1, 2]) due to the test in line 8 of DYNAMICHS which is positive forD1 andD2 (sinceD1,D2 \u2208 DX).\nOnce the second query Q2 = {B v \u2203r.F} is added to the positive test cases resulting in the DPI \u3008K,B,P \u222a {Q1, Q2} ,N \u3009R, the UPDATETREE function causes the pruning of two further nodes (D2 = [1, 6] and D4 = [1, 2]) leading to the continuance of only a single node (D1 = [1, 4]) in the memory of DYNAMICHS (see the picture with caption \u2019Updated Tree\u2019 in Figure 12.4). The reason for this is that Q2 can \u201creplace\u201d the part {2, 6} = {B v G,G v \u2203r.F} (which entails Q2) of the minimal conflict set \u30082, 4, 6\u3009 w.r.t. the last-but-one DPI \u3008K,B,P \u222a {Q1} ,N \u3009R such that \u30082, 4, 6\u3009 \\ {2, 6} = \u30084\u3009 is already a minimal conflict set w.r.t. the current DPI \u3008K,B,P \u222a {Q1, Q2} ,N \u3009R (cf. the analysis of the minimal conflict set C2 = \u30082, 4, 6\u3009 in Example 4.3).\nSince, by now, all minimal conflict sets \u30081, 2, 5\u3009, \u30082, 4, 6\u3009, \u30081, 5, 6, 8\u3009 as well as \u30081, 3, 4\u3009w.r.t. the input DPI \u3008K,B,P ,N \u3009R have \u201cshrunk\u201d as much as to constitute only two different set-minimal sets \u30081\u3009 and \u30084\u3009, it is clear by Proposition 4.6 that there can be only a single minimal diagnosis [1, 4] w.r.t. the current DPI \u3008K,B,P \u222a {Q1, Q2} ,N \u3009R. Therefore, the third iteration of DYNAMICHS terminates due to Q = [] and returns the singleton set Dcalc = {[1, 4]}. Consequently, the probability pD([1, 4]) = 1 wherefore Algorithm 5 also stops executing and returns (K \\ [1, 4]) \u222a p1 \u222a Q1 \u222a Q2 as the (exact) solution to the Interactive Dynamic KB Debugging problem for the DPI \u3008K,B,P ,N \u3009R.\nThe advantage of DYNAMICHS in this example over STATICHS in Example 11.2 in iterations 2 and 3 is that the pruning of nodes lets the algorithm automatically focus on the still relevant (i.e. non-redundant) parts of the tree. STATICHS, on the other hand, is doomed to spend most of the execution time for investigating nodes that turn out to be already invalidated by some specified test case(s). As already mentioned in Example 11.2, the inability of STATICHS to \u201cearly-prune\u201d incomplete branches of the tree is especially unfavorable in the last iteration of STATICHS in case \u03c3 = 0 since all irrelevant minimal diagnoses w.r.t. the input DPI must first be computed before they can be ruled out.\nThis immense upside of DYNAMICHS over STATICHS (see the analysis in the end of Example 11.2) also finds expression in the quantitative analysis of this example given next. All in all, the execution of Algorithm 5 in this example performs\n\u2022 4 full QX calls, i.e. calls of QX using the KBK\\node for a node node that actually return a minimal conflict set (there are four minimal conflict sets labeled by C in Figures 12.3 and 12.4 which do not result from QRC, CRC or the minimality test of a conflict set in line 32 of DYNAMICHS),\n\u2022 2 fast QX calls, i.e. executions of QX within the scope of the QRC (one call of QX each for the QRC of D3 and D2),\n\u2022 4 validity checks, i.e. calls of QX that return \u2019no conflict\u2019 (one check for each of the four found minimal diagnoses where the identification of diagnosesD1 at step 11\u00a9,D2 at step 12\u00a9 andD1 at step 15\u00a9 does not require any call to a reasoning service by means of DX, see line 8 in DYNAMICHS;\nnotice that QX does only perform a single KB validity check by ISKBVALID in case it returns \u2019no conflict\u2019, see Algorithm 1) and\n12.3. EXAMPLES 191\n\u2022 2 tree update processes involving 11 pruned nodes (9 nodes during the first update between steps 10\u00a9 and 11\u00a9 and 2 nodes during the second between steps 14\u00a9 and 15\u00a9),\ncomputes\n\u2022 4 minimal diagnoses (D1, D2, D3 and D4, all w.r.t. the input DPI),\n\u2022 6 minimal conflict sets (\u30081, 2, 5\u3009, \u30082, 4, 6\u3009, \u30081, 3, 4\u3009 and \u30081, 5, 6, 8\u3009 w.r.t. the input DPI and the subsets thereof \u30081\u3009 and \u30084\u3009 w.r.t. some DPI resulting from the input DPI by addition of new test cases) and\n\u2022 2 queries and asks the user 2 logical formulas (1 per query)\nand stores\n\u2022 a maximum of 12 nodes (where node refers to the internal representation of a node nd in DYNAMICHS as a list of edge labels (nd) and a list of node labels (nd.cs) along a path from the root node to a leaf node).\nFinally, we want to emphasize that, in all executions of UPDATETREE throughout this example, the usually very efficient QRC was successful right off and the usually more time-consuming CRC was never required.\n192 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\n1\u00a9\u30081, 2, 5\u3009C\n2\u00a9\u30082, 4, 6\u3009C 5\u00a9\u30081, 3, 4\u3009C 7\u00a9\u30082, 4, 6\u3009R\n? 3\u00a9X(D1) 4\u00a9X(D2) 6\u00a9 dup ? 8\u00a9\u30081, 5, 6, 8\u3009C ? 9\u00a9X(D3) ?\n? ? ? ?\n1\n0.41 rr\n2 0.25 5 0.25 ,,\n2\n0.09ww 4 0.28 6 0.27''\n1\n0.09ww 3 0.07 4 0.18''\n2\n0.06ww 4 0.18 6 0.17''\n1\n0.06ww\n5\n0.04\n6\n0.11\n8\n0.04\nIteration 1\n\u232a\nDcalc = {D1,D2,D3} = {[1, 4], [1, 6], [5, 4]}\nQ = [[5, 6], [2, 4, 6], [1, 2], [2, 3], [5, 2],\n[2, 4, 1], [2, 4, 5], [2, 4, 8]]\nCcalc = {\u30081, 2, 5\u3009 , \u30082, 4, 6\u3009 , \u30081, 3, 4\u3009 , \u30081, 5, 6, 8\u3009}\nD\u2283 = \u2205\nQdup = [[2, 1]]\n\u3008Q1,P(Q1)\u3009 = \u3008{B v K} , \u3008{D1,D2} , {D3} , \u2205\u3009\u3009\nu(Q1) = true\nDX = {D1,D2}, D\u00d7 = {D3}\n\u232a\nUPDATETREE:\nQRC (D3): QX(\u3008{1, 2, 6} ,B,P \u222a {Q1} ,N \u3009) = \u30081\u3009\n\u21d2 PRUNEQDUP/PRUNE: \u30081, 2, 5\u3009 \u2192 \u30081\u3009, \u30081, 3, 4\u3009 \u2192 \u30081\u3009\n\u2022 prune all subtrees starting from nodes \u30081, 2, 5\u3009\nby an outgoing edge with label 2 or 5\n\u2022 prune all subtrees starting from nodes \u30081, 3, 4\u3009\nby an outgoing edge with label 3 or 4\n\u2022 replace by \u30081\u3009 all node labels in the tree\nthat are proper supersets of \u30081\u3009\n\u21d2 D\u2283 = \u2205, Ccalc = {\u30081\u3009 , \u30082, 4, 6\u3009},\nQ = [[1, 4], [1, 6], [1, 2]], Qdup = []\n\u232a\n1\u00a9 \u2329 1, A2, A5 \u232aC 2\u00a9\u30082, 4, 6\u3009C 5\u00a9 \u2329 1, A3, A4 \u232aC 7\u00a9\u30082, 4, 6\u3009R\n? 3\u00a9X(D1) 4\u00a9X(D2) 6\u00a9 dup ? 8\u00a9 \u2329 1, A5, A6, A8 \u232aC ? 9\u00a9X(D3) ?\n10\u00a9? 10\u00a9? 10\u00a9\u00d7\n? ? ? ?\n1\n0.41 rr\n_ 2 0.25\n5 0.25 ,,\n2\n0.09ww 4 0.28 6 0.27''\n1 0.09ww _ 3 0.07 4 0.18'' 2 0.06ww 4 0.18 6 0.17''\nQ1 Q1\nQ1 1\n0.06ww\n?\n5\n0.04\n_\n6\n0.11\n8\n0.04\nUpdated Tree\n\u232a\n1\u00a9\u30081\u3009C\n2\u00a9\u30082, 4, 6\u3009C\n13\u00a9X(D4) 3\u00a9X(D1) 4\u00a9X(D2)\n11\u00a9X(D1) 12\u00a9X(D2)\n1\n0.41ss\n2\n0.09 ww 4 0.28 6 0.27''\nQ1 Q1\nIteration 2\n\u232a Dcalc = {D1,D2,D4} = {[1, 4], [1, 6], [1, 2]} Q = [] Ccalc = {\u30081\u3009 , \u30082, 4, 6\u3009}\nD\u2283 = \u2205\nQdup = []\n\u3008Q2,P(Q2)\u3009 = \u3008{B v \u2203r.F} , \u3008{D1} , {D2,D4} , \u2205\u3009\u3009\nu(Q2) = true\nDX = {D1}, D\u00d7 = {D2,D4}\n\u232a\nFigure 12.3: (Example 12.2) Solving the problem of Interactive Dynamic KB Debugging (Problem Definition 6.1) for the example DPI given by Table 4.2 by means of Algorithm 5 and DYNAMICHS.\n12.3. EXAMPLES 193\nUPDATETREE:\nQRC (D2): QX(\u3008{2, 4} ,B,P \u222a {Q1, Q2} ,N \u3009) = \u30084\u3009\n\u21d2 PRUNE: \u30082, 4, 6\u3009 \u2192 \u30084\u3009\n\u21d2 D\u2283 = \u2205, Ccalc = {\u30081\u3009 , \u30084\u3009}\nQ = [[1, 4]], Qdup = []\n\u232a 1\u00a9\u30081\u3009C 2\u00a9 \u2329 A2, 4, A6\n\u232aC 13\u00a9X(D4) 3\u00a9X(D1) 4\u00a9X(D2)\n11\u00a9X(D1) 12\u00a9X(D2)14\u00a9\u00d7\n14\u00a9? 14\u00a9\u00d7\n1\n0.41ss\n/ 2\n0.09 ww 4 0.28 6 0.27''\nQ1 Q1 Q2\nQ2 Q2\nUpdated Tree\n\u232a\n1\u00a9\u30081\u3009C\n2\u00a9\u30084\u3009C\n3\u00a9X(D1)\n11\u00a9X(D1)\n15\u00a9X(D1)\n1\n0.41ss\n4 0.28\nQ1\nQ2\nIteration 3\n\u232a Dcalc = {D1} = {[1, 4]} Q = [] Ccalc = {\u30081\u3009 , \u30084\u3009}\nD\u2283 = \u2205\nQdup = []\npD(D1) = 1\n\u21d2 return the solution KB (K \\ D1) \u222a p1 \u222aQ1 \u222aQ2 (p1: cf. Table 4.2)\nFigure 12.4: (Example 12.2 continued) Solving the problem of Interactive Dynamic KB Debugging (Problem Definition 6.1) for the example DPI given by Table 4.2 by means of Algorithm 5 and DYNAMICHS.\n194 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM"}, {"heading": "12.4 Algorithm Details and Correctness", "text": "In this section we will discuss DYNAMICHS in a detailed way and give proofs of its completeness and soundness. To this end, we first give some definitions and some hints regarding the notation used in this section."}, {"heading": "12.4.1 Definitions and Notation", "text": "The DYNAMICHS algorithm will require a different storage of nodes than STATICHS and Algorithm 2 since it will not interpret different branches with the same set of edge labels in the hitting set tree to be equivalent. So, DYNAMICHS, as opposed to STATICHS and Algorithm 2, will not discard any branch that is a duplicate branch in terms of its edge labels. Instead, a set storing these duplicate branches will be consulted each time a branch is found to be \u201credundant\u201d and thus needs to be pruned. This strategy enables the substitution of a \u201credundant\u201d branch by a \u201cnon-redundant\u201d branch featuring an equal set of edge labels.\nThat is why a node nd in (the hitting set tree produced by) DYNAMICHS corresponds to the ordered list of edge labels visited when traversing a path from the root node to some leaf node. As an attribute of nd, nd.cs corresponds to the ordered list of node labels visited when traversing a path from the root node to some leaf node.\nDefinition 12.1. Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Let further P \u2032\u20321 , . . . ,P \u2032\u2032 k and N \u2032\u2032 1 , . . . ,N \u2032\u2032 k such that P \u2032\u2032j \u2286 P \u2032 and N \u2032\u2032j \u2286 N \u2032 for j \u2208 {1, . . . , k}. Then we define in DYNAMICHS\n\u2022 a node nd = [ax 1, . . . , axk] to be an (ordered) list of elements ax j \u2208 K\nwhere each node nd stores as an attribute\n\u2022 the (ordered) list nd.cs = [C1, . . . , Ck] such that Cj is a minimal conflict set w.r.t. \u3008K,B,P\u222aP \u2032\u2032j ,N\u222a N \u2032\u2032j \u3009R and ax j \u2208 Cj for all j \u2208 {1, . . . , k} corresponding to the set of node labels on the path from the root node to nd.\nFurther, nd[i] refers to the i-th element in nd, i.e. to ax i, and nd.cs[i] refers to the i-th element in nd.cs, i.e. to Ci. Notice that conflict sets nd.cs[i] itself are (non-ordered) sets.\nMoreover, we define\n\u2022 |nd| and |nd.cs| to denote the number of elements in the lists nd and nd.cs,\n\u2022 nd[i..k] := [nd[i], . . . , nd[k]] for i \u2264 k and |nd| \u2265 k,\n\u2022 nd.cs[i..k] := [nd.cs[i], . . . , nd.cs[k]] for i \u2264 k and |nd.cs| \u2265 k,\n\u2022 nodes nd and nd[i..k] appearing on the left or right side of expressions using the following set operators to be considered as (non-ordered) sets: \u2283,\u2287,\u2282,\u2286,=, \\\nWe call\n\u2022 nd[1..k] where (k < |nd|) k \u2264 |nd| a (proper) subnode of nd and\n\u2022 nd\u2032\u2032 a successor (node) of nd\u2032 iff nd\u2032 is a proper subnode of nd\u2032\u2032.\n\u2022 nd the same node as nd\u2032 iff\n\u2013 |nd| = |nd\u2032| and\n12.4. ALGORITHM DETAILS AND CORRECTNESS 195\n\u2013 nd[i] = nd\u2032[i] for i \u2208 {1, . . . , |nd|} and \u2013 nd.cs[i] = nd\u2032.cs[i] for i \u2208 {1, . . . , |nd|}.\nExample 12.3 For instance, in line 20 of Algorithm 8, the test nodee \u2208 Q checks whether there is some set nd in Q such that nodee and nd interpreted as sets are equal. That is, nodee := {1, 3, 2} is equal to nd := {2, 1, 3} although the order of formulas is different and the ordered sets of conflict sets nodee.cs and nd.cs might be different as well. Another example of this interpretation of nodes as sets can be found in line 50 where Und.cs \\ nd refers to the set difference of the union of all sets in nd.cs and the set nd. If, e.g. Und.cs := {1, 2, 3, 4} and nd := {4, 2}, the result of this set difference is {1, 3} or, equivalently, {3, 1}.\nOn the other hand, if the operator is not one of those listed above, then node is interpreted as an ordered set. For example, consider line 19 where the ADD operator is used to append a logical formula e to the end of the ordered set of formulas node. Suppose, e.g. node := [3, 1, 2] and e := 4, then the result is [3, 1, 2, 4] which is not equal to [1, 2, 3, 4].\nThe following definition characterizes alternative paths in a hitting set tree produced by DYNAMICHS, i.e. different paths leading to the same (leaf) node in the tree.\nDefinition 12.2. Let nd and nd\u2032 be nodes in DYNAMICHS such that\n\u2022 |nd\u2032| \u2264 |nd|,\n\u2022 nd\u2032 = nd[1..|nd\u2032|] and\n\u2022 there is some j \u2208 { 1, . . . , |nd\u2032| } with the property that nd\u2032[j] 6= nd[j] or nd\u2032.cs[j] 6= nd.cs[j].\nFurther, let ADD(L1, L2) be the function that outputs the list [a1, . . . , an, b1, . . . , bm] given two lists L1 := [a1, . . . , an] and L2 := [b1, . . . , bm].\nThen we call\n\u2022 nd\u2032 an alternative subnode of nd,\n\u2022 nd\u2032 a proper alternative subnode of nd if |nd\u2032| < |nd| and\n\u2022 node where\n\u2013 node := ADD(nd\u2032, nd[|nd\u2032|+ 1..|nd|]) and \u2013 node.cs := ADD(nd\u2032.cs, nd.cs[|nd\u2032.cs|+ 1..|nd.cs|])\nan alternative equal node of nd.\n\u2022 In a context where nd\u2032 is relevant, we call node the alternative equal node of nd constructed from nd\u2032.\nRegarded as a set, an alternative equal node node of some node nd is equal to nd. There is just at least one difference between node and nd with regard to the order of elements in nd as opposed to the order of elements in node or with regard to the (order of) elements in nd.cs as opposed to the (order of) elements in node.cs.\nExample 12.4 Let nd := [1, 2, 3, 4] with nd.cs := [\u30081, 2, 3\u3009 , \u30082, 6\u3009 , \u30083, 6, 7\u3009 , \u30084, 5\u3009]. Then, nd1 := [2, 1] with nd1.cs := [\u30081, 2, 3\u3009 , \u30081, 4\u3009] as well as nd2 := [3, 2, 1] with nd2.cs := [\u30081, 2, 3\u3009 , \u30082, 6\u3009 , \u30081, 4\u3009] are alternative subnodes of nd. To see that nd1 is an alternative subnode of nd, observe that the setequality between nd1 = [2, 1] and nd[1..|nd1|] = [1, 2] holds and 2 = nd1[j] 6= nd[j] = 1 for j := 1\n196 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nholds. Similarly, for nd2, we have that the set equality between [1, 2, 3] and [3, 2, 1] holds and the elements on the j-th position for, e.g. j := 1, are different, i.e. 1 6= 3.\nThese alternative subnodes of nd can be used to construct the following alternative equal nodes of nd: The one obtained from nd1 is node1 := [2, 1, 3, 4] with node1.cs := [\u30081, 2, 3\u3009 , \u30081, 4\u3009 , \u30083, 6, 7\u3009 , \u30084, 5\u3009] and the one obtained from nd2 is node2 := [3, 2, 1, 4] with node1.cs := [\u30081, 2, 3\u3009 , \u30082, 6\u3009 , \u30081, 4\u3009 , \u30084, 5\u3009].\nThe following definition introduces the terminology that will be used throughout this section to refer to nodes in DYNAMICHS with certain properties.\nDefinition 12.3. In DYNAMICHS, a node nd with nd.cs is called\n\u2022 generated iff it is built in lines 18 and 19,\n\u2022 processed iff lines 6-15 have been executed for node := nd,\n\u2022 pruned iff\n\u2013 it is found to be redundant in line 91 and no node nd\u2032\u2032 = nd is added to S\u2032 in line 100 or \u2013 it is found to be redundant in line 112 and no node nd\u2032\u2032 = nd is added to Dupnew in line 121\n\u2022 replaced iff it is found to be redundant in line 91 and some node ndrep = nd is added to S\u2032 in line 100\n\u2022 combined-replaced iff it is found to be redundant in line 112 and some node ndcomb,rep = nd is added to Dupnew in line 121\nat any point in time during the execution of DYNAMICHS at any call to DYNAMICHS during the execution of Algorithm 5.\nThe node ndrep is referred to as replacement node (of nd) and the node ndcomb,rep is referred to as combined replacement node (of nd)."}, {"heading": "12.4.2 The Labeling Function in DYNAMICHS", "text": "The following two lemmata provide an analysis of the DLABEL function and characterize the output given by this function independently of when it is called during the execution of Algorithm 5.\nThe first one analyzes the case where DLABEL returns valid or nonmin which means that the node for which DLABEL was called is a diagnosis or a non-minimal diagnosis w.r.t. the current DPI, respectively. Further on, it states that only diagnoses w.r.t. the current DPI can be stored in the set Dcalc and only diagnoses for whose non-minimality there is evidence in terms of a diagnosis in Dcalc can be labeled by nonmin.\nLemma 12.1. Let the DLABEL procedure be called at any point in time during the execution of DYNAMICHS given i.a. some node node, some DPI \u3008K,B,P ,N \u3009R, some set of positive test cases P \u2032 and some set of negative test cases N \u2032 as argument. Then the following holds:\n(1) If DLABEL returns valid, node is a diagnosis w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\n(2) During this execution of DYNAMICHS, Dcalc comprises only diagnoses w.r.t. the current DPI \u3008K,B, P \u222a P \u2032,N \u222aN \u2032\u3009R.\n(3) If DLABEL returns nonmin, node is a non-minimal diagnosis w.r.t. the current DPI \u3008K, B, P \u222a P \u2032,N \u222aN \u2032\u3009R.\n(4) At the time the label nonmin is returned for node, there is some diagnosis D\u2032 w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R such that D\u2032 \u2208 Dcalc and node \u2283 D\u2032.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 197\nProof. (1): Assume that DLABEL returns valid for node. Then, by Proposition 4.9, Remark 4.3, Corollary 3.3, Corollary 7.3 and the fact that the DPI \u3008K,B,P ,N \u3009R used in DYNAMICHS as an input to DLABEL is the same DPI as the admissible one given as an input to Algorithm 5, node must be a diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R. This proves proposition (1).\n(2): This is a direct conclusion from proposition (1) and the facts that nodes labeled by valid are added to the set Dcalc in line 13, at the beginning of the execution of DYNAMICHS, Dcalc = \u2205 holds (line 3) and Dcalc is modified only in line 13 throughout DYNAMICHS.\n(3): At the beginning of the execution of DYNAMICHS, Dcalc = \u2205 (line 3) and Dcalc is modified only in line 13 throughout DYNAMICHS. In line 13, exactly those nodes are added to Dcalc for which the DLABEL function returns valid. By the correctness of proposition (1), only diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R can be added to Dcalc.\nNow, assume DLABEL returns nonmin for node. Then, due to the fact that Dcalc can only comprise diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and node \u2283 D\u2032 for some D\u2032 \u2208 Dcalc by line 27, node must be a non-minimal diagnosis w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\n(4): This is a direct consequence of proposition (3).\nThe following lemma states that the set Ccalc given as an input to DLABEL must include only minimal conflict sets, each w.r.t. the current DPI or some DPI including only a subset of the test cases the current DPI comprises. Moreover, it provides evidence that, in case DLABEL returns a set, this set is a minimal conflict set w.r.t. the current DPI which is not hit by the node given as input to DLABEL.\nLemma 12.2. Let the DLABEL procedure be called at any point in time during the execution of DYNAMICHS given i.a. some node node, a set of sets Ccalc, some DPI \u3008K,B,P ,N \u3009R, some set of positive test cases P \u2032 and some set of negative test cases N \u2032 as argument. Then,\n(1) each element in Ccalc is a minimal conflict set w.r.t. some DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R where P \u2032\u2032 \u2286 P \u2032 and N \u2032\u2032 \u2286 N \u2032 and\n(2) if DLABEL returns a set L, then this set L is a minimal conflict set w.r.t. the current DPI \u3008K, B, P \u222a P \u2032,N \u222aN \u2032\u3009R and node \u2229 L = \u2205.\nProof. (1): At the first call to DYNAMICHS, Ccalc = \u2205 is given as an input argument to DYNAMICHS (lines 1 and 10 in Algorithm 5). The only places throughout DYNAMICHS where Ccalc is modified are lines 39, 45 and 66. However, modifications to Ccalc in lines 39 and 66 can only take place in case there is already some element in Ccalc. That is, the first element must be added to Ccalc in line 45.\nIn line 45, only minimal conflict sets w.r.t. some DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R are added to Ccalc where P \u2032\u2032 \u2286 P \u2032 and N \u2032\u2032 \u2286 N \u2032 since the call to DLABEL might have taken place during some prior execution of DYNAMICHS during the execution of Algorithm 5. In order to reach line 45, QX called with the DPI \u3008K \\ node,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R as argument must not return \u2019no conflict\u2019 (line 41). That is, a minimal conflict set L 6= \u2205 w.r.t. \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R is computed in line 41 by Propostition 4.9, Remark 4.3, Corollary 7.3 and the fact that the DPI \u3008K,B,P ,N \u3009R used in DYNAMICHS as an input to DLABEL is the same DPI as the admissible one given as an input to Algorithm 5.\nIn lines 39 and 66, the following is true: (*) Only minimal conflict sets that are proper subsets of elements already in Ccalc can be added to Ccalc. In the case of line 39, (*) is true due to the following reasons: In order to reach line 39, QX(\u3008C,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R) = X 6= C must hold for some element C \u2208 Ccalc. Since Ccalc is never changed in Algorithm 5 between two calls to DYNAMICHS, Ccalc comprises only conflict sets w.r.t. the current DPI or previous DPIs (including fewer test cases than the current one). Moreover, a minimal conflict set C can only shrink after the addition of new test cases to the DPI for which it was computed by Proposition 12.1. Hence, the newly added element X must be a proper subset of the existing element C in Ccalc. That X is a minimal conflict set w.r.t. the DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R follows from QX(\u3008C,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R) = X , Propostition 4.9,\n198 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nRemark 4.3, Corollary 7.3 and the fact that the DPI \u3008K,B,P ,N \u3009R used in DYNAMICHS as an input to DLABEL is the same DPI as the admissible one given as an input to Algorithm 5.\nIn the case of line 66, (*) is true due to the following reasons: Due to Lemmata 12.6 and 12.7, quickPC = true or completePC = true can only hold if X is a witness of redundancy of nd. By Definition 12.4, a witness of redundancy is a conflict set w.r.t. the current DPI which is a proper subset of some conflict set that has been used as a label in nd.cs. However, each label in nd.cs must be an element of Ccalc due to lines 30, 45 and 19.\n(2): That, in case DLABEL returns a set L, it returns a minimal conflict set w.r.t. the current DPI is a consequence from the inference in the proof of proposition (1). We still need to show that L\u2229 node = \u2205.\nIf DLABEL returns in line 46, we can derive from the fact that L is the output of the call QX(\u3008K\\ node, B,P \u222a P \u2032,N \u222a N \u2032\u3009R), Proposition 4.9 and Definition 4.1 that L \u2286 K \\ node which implies that L \u2229 node = \u2205.\nIf DLABEL returns in line 34 or line 40, then the return can be executed only if the check C\u2229node = \u2205 is true in line 31. By the argumentation in the proof of proposition (1), for the returned set L it must hold that L \u2286 C. Hence, L \u2229 node = \u2205 is satisfied.\nAs a simple conclusion from Lemma 12.2, we have that the argumentX passed to the PRUNE function called within DLABEL is a minimal conflict set w.r.t. the current DPI:\nCorollary 12.1. Assume the execution of some call to DYNAMICHS during the execution of Algorithm 5 using the current DPI DPI . Anytime PRUNE is called within DLABEL, the input X given to it is a minimal conflict set w.r.t. DPI .\nProof. Assume the execution of some call to DYNAMICHS during the execution of Algorithm 5 using the current DPI DPI . Then, Lemma 12.2 says that the set X returned in line 40 is a minimal conflict set w.r.t. DPI . Since X is not modified by any of the functions PRUNE and ADDSETDELSUPSETS, we obtain the proposition of this corollary.\nFrom this we derive that the inputX passed to PRUNEQDUP called within DLABEL must be a minimal conflict set w.r.t. the current DPI:\nCorollary 12.2. Assume the execution of some call to DYNAMICHS during the execution of Algorithm 5 using the current DPI DPI . Anytime PRUNEQDUP is called within DLABEL, the input X given to it is a minimal conflict set w.r.t. DPI .\nProof. This corollary is a direct consequence of Corollary 12.1 and the fact that the argument X given to PRUNEQDUP is the same argument X that is given to PRUNEQDUP (none of these functions modifies X)."}, {"heading": "12.4.3 Impact of Answered Queries on Conflict Sets", "text": "After one call to DYNAMICHS in Algorithm 5 returns, the set Dcalc (called DX in Algorithm 5) returned by DYNAMICHS is used as a set of leading diagnoses w.r.t. the current DPI in order to compute a query. After the answered query is incorporated into the DPI, a new call to DYNAMICHS for this new current DPI is made.\nAs we have learned from Lemmata 12.1 and 12.2, the new call to DYNAMICHS considers only minimal diagnoses and minimal conflict sets w.r.t. the new current DPI. Therefore, the next proposition investigates the impact of the addition of the answered query as a new test case on the set of minimal conflict sets w.r.t. the new current DPI. Concretely, it claims that the transition from a DPI to a new DPI extended by a test case does change the set of minimal conflict sets, that each (minimal) conflict set remains a (not necessarily minimal) conflict set and that minimal conflict sets cannot grow in size.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 199\nIt is however important to notice that some \u201cnew\u201d minimal conflict set might emerge in the course of this DPI-transition which is not in a subset-relationship with any existing minimal conflict set.\nProposition 12.1. Let D be a set of minimal diagnoses w.r.t. \u3008K,B,P ,N \u3009R and Q \u2208 QD,\u3008K,B,P,N \u3009R . Further, let either P \u2032 = P \u222a {Q} or N \u2032 = N \u222a {Q}. Then it holds that\n(1) mC\u3008K,B,P,N \u3009R 6= mC\u3008K,B,P \u2032,N \u2032\u3009R ,\n(2) each conflict set w.r.t. \u3008K,B,P ,N \u3009R is a conflict set w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R,\n(3) each minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R is a conflict set w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R,\n(4) there are no C \u2208mC\u3008K,B,P,N \u3009R and C\u2032 \u2208mC\u3008K,B,P \u2032,N \u2032\u3009R such that C \u2282 C\u2032,\n(5) if there is a subset-relationship between C \u2208mC\u3008K,B,P,N \u3009R and C\u2032 \u2208mC\u3008K,B,P \u2032,N \u2032\u3009R , then C\u2032 = C or C\u2032 \u2282 C.\nProof. (1): Assume the opposite, namely that mC\u3008K,B,P,N \u3009R = mC\u3008K,B,P \u2032,N \u2032\u3009R . Then, by Proposition 4.6, mD\u3008K,B,P,N \u3009R = mD\u3008K,B,P \u2032,N \u2032\u3009R must be true. This however is a contradiction to Definition 7.1 and the fact that Q is a query.\n(2): Let C be a conflict set w.r.t. \u3008K,B,P ,N \u3009R. Then C \u222a B \u222a UP violates some x \u2208 R \u222a N . If P \u2032 = P \u222a {Q} holds, then, by monotonicity of L, C \u222a B \u222a UP\u222a{Q} violates some x \u2208 R \u222aN , i.e. C is a conflict set w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R. Otherwise, if N \u2032 = N \u222a {Q} is given, then C \u222a B \u222aUP violates some x \u2208 R \u222aN \u2282 R \u222aN \u2032, i.e. C is a conflict set w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R.\n(3): This is a direct consequence of (2), since each minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R is a conflict set w.r.t. \u3008K,B,P ,N \u3009R.\n(4): Since, by (3), each minimal conflict set w.r.t. \u3008K, B, P ,N \u3009R is also a conflict set w.r.t. \u3008K, B,P \u2032,N \u2032\u3009R, there cannot be a minimal conflict set C\u2032 w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R which is a proper superset of a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R as this would imply non-minimality of C\u2032 w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R.\n(5): This proposition is a direct consequence of (4).\nGiven the existence of some non-empty minimal conflict set w.r.t. an admissible DPI DPI , the extension of the test cases of DPI by a query yields a new DPI DPI \u2032 for which all minimal conflict sets are non-empty:\nProposition 12.2. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that \u3008K,B,P ,N \u3009R is admissible and P \u2032 \u2287 P and N \u2032 \u2287 N and |P \u2032 \u222aN \u2032| = |P \u222aN |+ 1. Let further C 6= \u2205 be a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R and Q \u2208 (P \u2032 \u222a N \u2032) \\ (P \u222a N ) be a query w.r.t. some D \u2286 mD\u3008K,B,P,N \u3009R and \u3008K,B,P ,N \u3009R. Then, for each minimal conflict set C\u2032 w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R it holds that C\u2032 6= \u2205.\nProof. Assume there is some minimal conflict set C\u2032 w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R such that C\u2032 = \u2205. This implies that there cannot be a minimal conflict set C\u2032\u2032 w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R which is not the empty set because C\u2032 would be a proper subset of C\u2032\u2032, which would be a contradiction to the minimality of C\u2032\u2032.\nDue to Corollary 7.3 and the fact that a query Q w.r.t. some D \u2286mD\u3008K,B,P,N \u3009R and \u3008K,B,P ,N \u3009R is added to \u3008K,B,P ,N \u3009R in order to obtain \u3008K,B,P \u2032,N \u2032\u3009R, we have that \u3008K,B,P \u2032,N \u2032\u3009R must be admissible.\nBy Corollary 3.3, K cannot be valid w.r.t. \u3008\u00b7,B,P ,N \u3009R since \u2205 cannot be a diagnosis w.r.t. \u3008K, B,P ,N \u3009R by Proposition 4.6 and the fact that C is a non-empty minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R. From this we can infer that K cannot be valid w.r.t. \u3008\u00b7,B,P \u2032,N \u2032\u3009R as P \u2032 \u2287 P and N \u2032 \u2287 N .\nNow, by Proposition 4.2, there must be some minimal conflict set w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R which is not the empty set, contradiction.\n200 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM"}, {"heading": "12.4.4 Impact of Answered Queries on Diagnoses", "text": "Next, we analyze what influence answered queries that are added as new test cases to the current DPI have on the (minimal) diagnoses w.r.t. this DPI. The first lemma assures that each DPI constructed during the execution of Algorithm 5 must be admissible as a consequence of the postulated admissibility of the DPI given as an initial input to Algorithm 5.\nLemma 12.3. Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Then, the DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R is admissible.\nProof. The admissibility of \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R follows from the fact that \u3008K,B,P ,N \u3009R is the (coercively) admissible input DPI of Algorithm 5, Corollary 7.3 which reveals that admissibility of a DPI is preserved under the addition of a query to the test cases of the DPI and the fact that P \u2032 as well as N \u2032 are sets of queries. The latter holds because CALCQUERY (Algorithm 5, line 16) computes only queries and the only place where P \u2032 and N \u2032 are modified is lines 24-26 where only sets returned by CALCQUERY are added to P \u2032 and N \u2032.\nThe next proposition confirms the restrictive character of test cases. That is, any extension of a current DPI through the addition of a test case cannot lead to a set of (all) diagnoses w.r.t. the new DPI that is a superset of the set of (all) diagnoses w.r.t. the current DPI. We want to point out that this is not necessarily true for the set of minimal diagnoses.\nProposition 12.3. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that P \u2032 \u2287 P and N \u2032 \u2287 N . Then, each diagnosis w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R is also a diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nProof. Let D\u2032 \u2208 aD\u3008K,B,P \u2032,N \u2032\u3009R . Then, by Corollary 3.3 and Definition 3.2, (K \\ D\u2032) \u222a B \u222a UP \u2032 does not violate any x \u2208 R \u222a N \u2032. Since however formulas, in particular those in UP \u2032\\P , that are added to a KB cannot invalidate any (unwanted) entailments, in particular those in N \u2032, and cannot resolve any inconsistencies or incoherencies by the monotonicity of L, we can conclude that (K\\D\u2032)\u222aB \u222aUP does not violate any x \u2208 R \u222a N \u2032 either. Since N \u2032 \u2287 N , non-violation of any test case in N \u2032 implies non violation of any test case in N also. Consequently, (K\\D\u2032)\u222aB\u222aUP does not violate any x \u2208 R\u222aN and entails all p \u2208 P (due to UP ) whereforeD\u2032 \u2208 aD\u3008K,B,P,N \u3009R due to Corollary 3.3 and Definition 3.2.\nAs a consequence of this, each minimal diagnosis w.r.t. the new DPI is a diagnosis w.r.t. the current DPI, i.e. either a minimal or a non-minimal diagnosis w.r.t. the current DPI.\nCorollary 12.3. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that P \u2032 \u2287 P and N \u2032 \u2287 N . Then, each minimal diagnosis w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R is also a diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nProof. Since Proposition 12.3 holds for all diagnoses w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R, it also holds for all minimal diagnoses w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R since each minimal diagnosis is a diagnosis.\nAdding a test case to a DPI cannot make minimal diagnoses shrink:\nProposition 12.4. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that P \u2032 \u2287 P and N \u2032 \u2287 N and let D \u2208mD\u3008K,B,P,N \u3009R . Then, for all D\u2032 \u2208mD\u3008K,B,P \u2032,N \u2032\u3009R , it holds that D\u2032 6\u2282 D. Proof. Let D \u2208 mD\u3008K,B,P,N \u3009R and let D\u2032 \u2208 mD\u3008K,B,P \u2032,N \u2032\u3009R such that P \u2032 \u2287 P ,N \u2032 \u2287 N and suppose D\u2032 \u2282 D. By Proposition 12.3, D\u2032 must be a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. By D\u2032 \u2282 D, this is a contradiction to the premise that D \u2208mD\u3008K,B,P,N \u3009R , i.e. that D is minimal.\nIn fact, it even holds that each \u201cnew\u201d minimal diagnosis (which is not a minimal diagnosis w.r.t. the current DPI) resulting from the addition of a test case to the current DPI must be a proper superset of some minimal diagnosis w.r.t. the current DPI. In other words, a minimal diagnosis w.r.t. the new DPI is either a minimal diagnosis w.r.t. the current DPI or a proper superset of some minimal diagnosis w.r.t. the current DPI.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 201\nProposition 12.5. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that P \u2032 \u2287 P and N \u2032 \u2287 N and let D\u2032 \u2208 mD\u3008K,B,P \u2032,N \u2032\u3009R and D\u2032 /\u2208 mD\u3008K,B,P,N \u3009R . Then, there is some D \u2208 mD\u3008K,B,P,N \u3009R such that D \u2282 D\u2032.\nProof. By Corollary 12.3, we know that D\u2032 \u2208mD\u3008K,B,P \u2032,N \u2032\u3009R is a diagnosis w.r.t. \u3008K,B,P ,N \u3009R. If D\u2032 is already a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R, then the proposition holds. Otherwise, there must be some D \u2282 D\u2032 such that D is a minimal diagnosis w.r.t. \u3008K,B,P ,N \u3009R.\nAddition of a query to whatever test case set of a DPI DPI implies that the set of all diagnoses w.r.t. the new DPI is a proper subset of all diagnoses w.r.t. DPI:\nCorollary 12.4. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that\n\u2022 P \u2032 \u2287 P and N \u2032 \u2287 N ,\n\u2022 |P \u2032| = |P |+ 1 or |N \u2032| = |N |+ 1, but not both, and\n\u2022 (P \u2032 \u222a N \u2032) \\ (P \u222a N ) = {Q} where Q is a query w.r.t. some set D \u2286 mD\u3008K,B,P,N \u3009R and \u3008K,B,P ,N \u3009R.\nThen, aD\u3008K,B,P \u2032,N \u2032\u3009R \u2282 aD\u3008K,B,P,N \u3009R holds.\nProof. By Proposition 12.3 we have that aD\u3008K,B,P \u2032,N \u2032\u3009R \u2286 aD\u3008K,B,P,N \u3009R . Since \u3008K,B,P \u2032,N \u2032\u3009R results from \u3008K,B,P ,N \u3009R by the addition of the query Q w.r.t. some set D and \u3008K,B,P ,N \u3009R to either P or N , we conclude by Definition 7.1 that at least one minimal diagnosis D w.r.t. \u3008K,B,P ,N \u3009R in D is not a minimal diagnosis w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R. Assume, D is a non-minimal diagnosis w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R. In this case, there must be some D\u2032 \u2282 D such that D\u2032 is a minimal diagnosis w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R. This is a contradiction to Proposition 12.4. Consequently,D /\u2208 aD\u3008K,B,P \u2032,N \u2032\u3009R . Hence, D \u2208 aD\u3008K,B,P,N \u3009R \\aD\u3008K,B,P \u2032,N \u2032\u3009R . By aD\u3008K,B,P \u2032,N \u2032\u3009R \u2286 aD\u3008K,B,P,N \u3009R , the proposition of the corollary follows."}, {"heading": "12.4.5 Redundant Nodes in DYNAMICHS", "text": "The following result constitutes the basis for the definition of a redundant node we give in the next section. It is already stated in [Rei87], but without a proof. It testifies that the set of all minimal hitting sets of a collection F of sets remains steady if elements that are not set-minimal sets in F are deleted from F . By Proposition 4.6, the same must hold for the set of all minimal diagnoses of the collection of all minimal conflict sets w.r.t. some DPI DPI . That is, considering only minimal hitting sets of minimal conflict sets w.r.t. DPI is sufficient for completeness of a hitting set tree algorithm concerning the finding of all minimal diagnoses w.r.t. DPI .\nHowever, we proved by Proposition 12.1 that existing conflict sets will tend to shrink gradually through the specification of new test cases. This implicates that more and more nodes ndi stored by DYNAMICHS will have the property that ndi.cs will include non-minimal conflict sets w.r.t. the current DPI which constitutes the first of two criteria that are together sufficient for a safe pruning of ndi. By safe pruning we mean the deletion of a node without eliminating any minimal diagnoses w.r.t. the current DPI.\nProposition 12.6. If F is a collection of sets, and if S \u2208 F and S\u2032 \u2208 F such that S \u2282 S\u2032, then Fsub := F \\ {S\u2032} has the same minimal hitting sets as F .\nProof. Let D be a minimal hitting set of Fsub, then D is a hitting set of F since D \u2229 S 6= \u2205 holds which implies by S \u2282 S\u2032 that D \u2229 S\u2032 6= \u2205. Assume that D is a non-minimal hitting set of F , i.e. that a subset D\u2032 \u2282 D is a hitting set of F . Then, however, by minimality of D w.r.t. Fsub we have that not all sets in\n202 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nFsub are hit by D\u2032 and thus, by Fsub \u2282 F , that not all sets in F can be hit by D\u2032, contradiction. Thus, each minimal hitting set of Fsub is also a minimal hitting set of F .\nLet D be a minimal hitting set of F , then D is clearly a hitting set of Fsub \u2282 F . Suppose that D is a non-minimal hitting set of Fsub, i.e. that a proper subset of D is a hitting set of Fsub. Let D\u2032 \u2282 D be a subset-minimal such subset ofD. That is,D\u2032 is a minimal hitting set of Fsub. SinceD is a minimal hitting set of F , D\u2032 is not a (minimal) hitting set of F , but a minimal hitting set of Fsub. This is a contradiction to the already proven fact that any minimal hitting set of Fsub is also a minimal hitting set of F .\nAssume the first criterion for a safe pruning of a node ndi, namely the existence of some non-minimal conflict set w.r.t. the current DPI in ndi.cs, is met. Then, we have not yet any evidence that ndi is obsolete since for each of the non-minimal conflict sets in ndi.cs there must be one (or multiple) proper subset(s) which is a minimal conflict set w.r.t. the current DPI. Let C\u00acmin be one particular non-minimal conflict set in ndi.cs and let C be the particular proper subset of C\u00acmin that is the first \u201cwitness\u201d found by DYNAMICHS which documents the non-minimality of C\u00acmin. Then C\u00acmin can be split into two disjoint parts, namely C and the set of formulas C that C\u00acmin does not share with C.\nNow, the second criterion for a safe pruning of ndi is about whether ndi hits C. If so, then ndi is not a (partial) hitting set of only minimal conflict sets w.r.t. the current DPI. Put another way, this means that, under the assumption that a wpHS-tree was constructed using only the \u201cstatic\u201d current DPI, then the label C\u00acmin would have never been produced and hence the node ndi could have never been generated. Eventually, by the considerations made in Sections 4.6.3 and 11.4, we know that such a static hitting set tree algorithm is complete although not taking into account nodes like ndi.\nThese thoughts motivate the following definition of a redundant node28.\nDefinition 12.4. Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Further, let nd be a node in DYNAMICHS. Then we call nd a redundant node w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R iff there is\n\u2022 some r \u2208 {1, . . . , |nd|} and\n\u2022 some minimal conflict set C w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R\nsuch that\n\u2022 C \u2282 nd.cs[r] and\n\u2022 nd[r] \u2208 nd.cs[r] \\ C.\nMoreover, C is called a witness of redundancy of nd.\nA node node in DYNAMICHS can be only redundant w.r.t. a DPI DPI if node.cs comprises some non-minimal conflict set w.r.t. DPI:\nCorollary 12.5. Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Further, let nd be a node in DYNAMICHS such that nd[i] \u2208 mC\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R for all i \u2208 {1, . . . , |nd|}. Then nd is not a redundant node w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nProof. Since nd.cs comprises only minimal conflict sets w.r.t. \u3008K,B,P ,N \u3009R, there cannot be any C \u2208 mC\u3008K,B,P,N \u3009R such that C \u2282 nd.cs[i] for some i.\nA node that is redundant w.r.t. some DPI DPI remains redundant w.r.t. any DPI \u2032 that includes a superset of the test cases DPI includes:\n28We adopt the term \u201credundant\u201d from [Rei87] where is was informally used in the same context.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 203\nLemma 12.4. Let \u3008K,B,P ,N \u3009R and \u3008K,B,P \u2032,N \u2032\u3009R be two DPIs such that P \u2032 \u2287 P and N \u2032 \u2287 N . Further, let nd be a redundant node w.r.t. \u3008K,B,P ,N \u3009R. Then, nd is a redundant node w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R.\nProof. By Proposition 12.1, if \u3008K,B,P \u2032,N \u2032\u3009R results from the addition of a single new positive or negative test case to \u3008K,B,P ,N \u3009R, there cannot be any minimal conflict set w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R that is a proper superset of a minimal conflict w.r.t. \u3008K,B,P ,N \u3009R. By Definition 12.4, we can derive that any redundant node w.r.t. \u3008K,B,P ,N \u3009R must be a redundant node w.r.t. \u3008K,B,P \u2032,N \u2032\u3009R. The proposition of this lemma is a consequence of further applications of Proposition 12.1.\nThis implies that a redundant node that is deleted during the execution of DYNAMICHS using the current DPIDPI cannot become non-redundant throughout the entire remaining execution of the interactive debugging session, i.e. the execution of Algorithm 5. Reason for this is that the sets of test cases in a DPI can only be extended and not reduced in the course of debugging.\nRemark 12.7 Note that this has consequences on the way how \u201cmind-changes\u201d of a user might be handled by the interactive algorithm. It implies that the current state of DYNAMICHS (stored in the output variables of DYNAMICHS) cannot be exploited in case a user decides to discard some already answered query or to switch the already submitted answer of some query, resulting in some modified DPI DPI \u2032. In such a situation a new construction of a hitting set tree by DYNAMICHS using the DPI DPI \u2032 is indicated. Otherwise, some already pruned redundant node w.r.t. DPI might become a relevant node for DPI \u2032 which would lead to a violation of the postulated completeness of DYNAMICHS w.r.t. each current DPI, in this case the DPI DPI \u2032.\nThe following result is straightforward and claims that each successor node of a redundant node ndi w.r.t. DPI is a redundant node w.r.t. DPI . So, if r is the minimal value such that both criteria of Definition 12.4 hold for ndi, all successor nodes of the subnode ndi[1..r] of ndi can be deleted. In other words, the entire subtree (of the hitting set tree produced by DYNAMICHS) rooted at an outgoing edge e of a non-minimal conflict set where e is labeled by an element ax which is not an element of a given witness of redundancy is obsolete.\nLemma 12.5. Let \u3008K,B,P ,N \u3009R be a DPI, nd be a redundant node w.r.t. \u3008K,B,P ,N \u3009R and nd\u2032 be a successor node of nd. Then, nd\u2032 is a redundant node w.r.t. \u3008K,B,P ,N \u3009R.\nProof. The proposition of this lemma is a direct consequence of Definition 12.4."}, {"heading": "12.4.6 Hitting Set Tree Pruning in DYNAMICHS", "text": "The main pruning operations performed by DYNAMICHS take place in the scope of the UPDATETREE function which is called right at the beginning of the execution of each call to DYNAMICHS. Assume a call to DYNAMICHS during Algorithm 5 given i.a. the DPI \u3008K,B,P ,N \u3009R and the test cases P \u2032 and N \u2032 as arguments and suppose the last-but-one call to DYNAMICHS was given P \u2032\u2032 and N \u2032\u2032 as arguments. The job of UPDATETREE is to restore the parameters that store the state of DYNAMICHS (for DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222a N \u2032\u2032\u3009R) in a way that they include at least all nodes that would be included by the respective parameters produced by a call to DYNAMICHS for the static DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nRoughly speaking, this involves the following actions:\n\u2022 Pruning: That is, only nodes that are definitely redundant w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R are deleted. A node is definitely redundant if a witness of redundancy of it is known.\n\u2022 Replacement: A deleted redundant node is replaced by an alternative equal node of it which is non-redundant w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, if there is such a one. Alternative equal nodes are constructed from the list of duplicate nodes Qdup.\n204 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\n\u2022 Rearrangement: the reassignation of nodes to Q that \u201csurvived\u201d all pruning steps or were introduced in the course of a replacement step and for which no evidence w.r.t. \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R is given that it should be assigned to any other set.\nMore concretely, UPDATETREE has the following effect on the collections Q, D\u00d7, D\u2283, Qdup which are, together with DX, the only node-storing collections of DYNAMICHS at the beginning of the execution of each call to DYNAMICHS:\n(a) If nd is in Qdup, then nd is removed from Qdup only if there is a known witness of redundancy of nd w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. If there is an alternative equal replacement node nd\u2032 of nd which is constructable from some node in Qdup, then nd\u2032 is added to Qdup.\n(b) If nd is in Q, then nd is removed from Q only if there is a known witness of redundancy of nd w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. If there is an alternative equal replacement node nd\u2032 of nd which is constructable from some node in Qdup, then nd\u2032 is added to Q.\n(c) If nd is in D\u00d7 and there is no known witness of redundancy of nd w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, then nd is added to Q.\n(d) If nd is in D\u00d7 and nd is redundant w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, then, if there is some alternative equal replacement node nd\u2032 of nd which is constructable from some node in Qdup, then nd\u2032 is added to Q.\n(e) If nd is in D\u2283, there is no known witness of redundancy of nd w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R and there is no known minimal diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R which is a proper subset of nd, then nd is added to Q.\n(f) All nodes nd in DX are added to Q.\nSome comments: Step (a) is conducted by PRUNEQDUP before PRUNE is called, for each witness of redundancy X of some node detected during the execution of UPDATETREE. PRUNE is the function that prunes or replaces nodes that are elements of any other collection than Qdup, i.e. Q, D\u00d7 or D\u2283, and for whichX is a witness of redundancy. In this vein, the PRUNE function just needs to perform a test whether there is any node in Qdup that enables the construction of a replacement node of a deleted node. No check for redundancy of nodes in Qdup is necessary at this stage since Qdup has already been processed and cleaned from all redundant nodes w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nUnder the assumption that the deletion of a node redundant w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R is safe in terms of completeness of DYNAMICHS as to finding all minimal diagnoses w.r.t. \u3008K,B,P\u222aP \u2032,N \u222aN \u2032\u3009R (which we will prove throughout this section), UPDATETREE acts safely. That is, deletion actions are performed just on the basis of given evidence in the form of a witness of redundancy. However, it must be accentuated that this does not necessarily imply the pruning or replacement of all redundant nodes w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. This is quite desired as guaranteeing complete pruning might be very costly concerning execution time since it would involve the precomputation of all not-yet-computed minimal conflict sets w.r.t. the current DPI at once. In the bad case, since these computations would take place online, i.e. between two successive queries shown to the user, this would be anything but beneficial for an interactive algorithm whose usability and usefulness depends greatly on its timeliness. Apart from that, a single newly added test case can be expected to lead to the introduction of only a small number of minimal conflict sets w.r.t. the current DPI that are no minimal conflict sets w.r.t. the last-but-one DPI.\nWhich nodes are pruned throughout UPDATETREE depends on which witnesses of redundancy are found, i.e. which minimal conflict sets are computed. The UPDATETREE function is implemented to search targeted for witnesses of redundancy of stored nodes. That is, instead of just computing any minimal conflict set w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, it focuses on the set of nodes D\u00d7 which includes\n12.4. ALGORITHM DETAILS AND CORRECTNESS 205\nthe subset of all minimal diagnoses Dcalc computed in the last-but-one iteration of DYNAMICHS w.r.t. the last-but-one DPI \u3008K,B,P \u222aP \u2032\u2032,N \u222aN \u2032\u2032\u3009R, which are no diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. Note that we will prove later in this section that Dcalc, and thus D\u00d7 and DX which are subsets thereof, will indeed comprise only minimal diagnoses. So, UPDATETREE looks for witnesses of redundancy by means of exactly these minimal diagnoses that have been invalidated through the addition of the most recent answered query to the test cases of the DPI. Each diagnosis nd w.r.t. the last-but-one DPI can be invalidated only because it does not hit some minimal conflict set w.r.t. the current DPI and not because it is a non-minimal hitting set of all minimal conflict sets w.r.t. the current DPI. This can be directly inferred from Proposition 12.4 which manifests that minimal diagnoses cannot shrink by the addition of a new test case i.e. there cannot be any minimal diagnosis w.r.t. the current DPI which is a proper subset of nd.\nNow, two cases can be identified for a minimal conflict set C w.r.t. the current DPI that is not hit by nd:\nC1: C is not in a subset-relationship with any minimal conflict set in nd.cs. That is, C is definitely not a witness of redundancy of nd.\nC2: C is in a subset-relationship with some minimal conflict set in nd.cs. That is, C satisfies the first criterion of a witness of redundancy of nd (cf. Definition 12.4). Thence, C might be a witness of redundancy of nd.\nNow, the idea is to try to figure out very fast some C for a node nd \u2208 D\u00d7 such that C is a witness of redundancy of nd. This idea is implemented in the so-called Quick Redundancy Check (QRC) which\n\u2022 calls QX just once given the DPI \u3008Und.cs \\ nd,B,P ,N \u3009R with the usually very small KB Und.cs \\ nd \u2286 K in order to calculate just one minimal conflict set C w.r.t. the current DPI\n\u2022 and then verifies whether C is a witness of redundancy of nd by conducting at most |nd| subsetrelationship checks.\nThe following lemma confirms that QRC (lines 50-54 in Algorithm 9), if successful, indeed computes a witness of redundancy of nd and thus gives evidence that nd is redundant w.r.t. the current DPI.\nLemma 12.6 (Quick Redundancy Check \u2013 QRC). Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Further, let nd be some node in DYNAMICHS. Then the following holds:\nIf QX(\u3008Und.cs \\ nd,B,P \u222a P \u2032,N \u222a N \u2032\u3009R) returns a set C such that C \u2282 nd.cs[i] for some i \u2208 {1, . . . , |nd.cs|}, then\n\u2022 nd is a redundant node w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and\n\u2022 C is a witness of redundancy of nd.\nProof. First, Und.cs \\ nd includes all elements in the union of all conflict sets in nd.cs except for the elements occurring in nd. So, if QX(\u3008Und.cs \\ nd,B,P \u222a P \u2032,N \u222a N \u2032\u3009R) returns a set C, then C is a minimal conflict set w.r.t. \u3008Und.cs \\ nd,B,P \u222a P \u2032,N \u222aN \u2032\u3009R by Proposition 4.9.\nBy Definition 4.1, C \u2286 Und.cs \\ nd holds wherefore C \u2229 nd = \u2205. By K \u2287 Und.cs \\ nd and Remark 4.3, C is a minimal conflict set w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nIf C \u2282 nd.cs[i] for some i \u2208 {1, . . . , |nd.cs|}, then we have that C is a minimal conflict set w.r.t. \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R which is a proper subset of nd.cs[i]. Since C \u2229nd = \u2205 implies that nd[i] /\u2208 C for all i \u2208 {1, . . . , |nd|}, we conclude that nd[i] \u2208 nd.cs[i] \\ C. Now, by Definition 12.4, nd is a redundant node w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and C is a witness of redundancy of nd.\n206 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nRemark 12.8 Please notice that the opposite direction does not necessarily hold. That is, if the node nd is redundant w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, QX(\u3008Und.cs \\ nd,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) might return\n\u2022 some C which is not a subset of any conflict set in nd.cs or\n\u2022 \u2019no conflict\u2019.\nAs an illustration of that remark, we give the following example:\nExample 12.5 For instance, assume a node nd = [1, 2] with nd.cs = [\u30081, 2, 3\u3009 , \u30082, 4, 5\u3009] and that \u30082, 3\u3009 is a minimal conflict set w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R wherefore nd is redundant by Definition 12.4. Then Und.cs \\ nd = {3, 4, 5}.\nSuppose that \u30083, 5\u3009 is a minimal conflict set w.r.t. the current DPI as well. So, in this case, QX(\u3008{3, 4, 5}, B,P \u222aP \u2032,N \u222aN \u2032\u3009R) might return \u30083, 5\u3009. However, \u30083, 5\u3009 is neither a subset of \u30081, 2, 3\u3009 nor a subset of \u30082, 4, 5\u3009 wherefore \u30083, 5\u3009 is no witness of redundancy of nd.\nOn the other hand, if we suppose that \u30082, 3\u3009 and \u30082, 4, 5\u3009 are the only minimal conflict sets w.r.t. the current DPI that are subsets of Und.cs = {1, 2, 3, 4, 5}, then \u2019no conflict\u2019 is the output of the call to QX. This holds since nd[2] = 2 is an element of both \u30082, 3\u3009 and \u30082, 4, 5\u3009 and hence not an element of Und.cs \\ nd = {3, 4, 5}. Therefore, neither \u30082, 3\u3009 nor \u30082, 4, 5\u3009 is returned by QX since QX(\u3008{3, 4, 5} ,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) can only return a set that is a subset of {3, 4, 5} by Proposition 4.9 and Definition 4.1.\nIn both cases of the previous example, an existing witness of redundancy of nd is not detected by QRC. In this situation, i.e. when QRC is negative, a Complete Redundancy Check (CRC) is performed which involves QX investigating all the DPIs \u3008nd.cs[i] \\ nd[i],B,P \u222a P \u2032,N \u222aN \u2032\u3009R for i \u2208 {1, . . . , |nd|} separately. CRC, as substantiated by the following lemma, does find a witness of redundancy if the node nd is redundant w.r.t. the current DPI; and, if CRC does not find a witness of redundancy w.r.t. the current DPI, then nd is non-redundant w.r.t. the current DPI.\nLemma 12.7 (Complete Redundancy Check \u2013 CRC). Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Further, let nd be some node in DYNAMICHS. Then, the following holds:\n(1) nd is redundant w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R iff there is some i \u2208 {1, . . . , |nd|} such that QX(\u3008nd.cs[i] \\ {nd[i]} ,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) = X where X 6= \u2019no conflict\u2019.\n(2) If there is some i \u2208 {1, . . . , |nd|} such that QX(\u3008nd.cs[i] \\ {nd[i]} ,B,P \u222a P \u2032,N \u222a N \u2032\u3009R) = X where X 6= \u2019no conflict\u2019, then X is a witness of redundancy of nd.\nProof. (1): \u201c\u21d0\u201d: Assume there is some i \u2208 {1, . . . , |nd|} such that QX(\u3008nd.cs[i]\\{nd[i]} ,B,P\u222aP \u2032,N\u222a N \u2032\u3009R) = X where X 6= \u2019no conflict\u2019. Then, by Proposition 4.9, we have that X is a minimal conflict set w.r.t. \u3008nd.cs[i] \\ {nd[i]} ,B,P \u222a P \u2032,N \u222a N \u2032\u3009R such that X \u2286 nd.cs[i] \\ {nd[i]}. By Definition 4.1, X is a minimal conflict set w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. Hence, we can conclude that nd[i] /\u2208 X . By Definition 12.1 and since nd is a node in DYNAMICHS, it holds that nd[i] \u2208 nd.cs[i]. As a consequence, nd[i] \u2208 nd.cs[i] \\X holds. By Definition 12.4, nd is redundant w.r.t. \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R (and X is a witness of redundancy of nd).\n\u201c\u21d2\u201d: Suppose nd is a redundant node w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. Then, by Definition 12.4, there must be some r \u2208 {1, . . . , |nd|} and some minimal conflict set X w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R such that (i) X \u2282 nd.cs[r] and (ii) nd[r] \u2208 nd.cs[r] \\ X . By (ii), nd[r] /\u2208 X . By Definition 12.1 and the fact that nd is a node in DYNAMICHS, we obtain that nd[r] \u2208 nd.cs[r] must be true. Hence, by (i), we derive that X \u2286 nd.cs[r] \\ {nd[r]}. By Proposition 4.9, QX given some DPI DPI outputs a minimal conflict set w.r.t. DPI iff there is a minimal conflict set w.r.t. DPI . Therefore and since QX(\u3008nd.cs[i] \\ {nd[i]} ,B,P \u222aP \u2032,N \u222aN \u2032\u3009R) is called for each i \u2208 {1, . . . , |nd|}, it must also be called\n12.4. ALGORITHM DETAILS AND CORRECTNESS 207\nfor i := r since r \u2208 {1, . . . , |nd|}. So, some minimal conflict set X \u2032, and not \u2019no conflict\u2019, must be returned by QX(\u3008nd.cs[r] \\ {nd[r]} ,B,P \u222a P \u2032,N \u222a N \u2032\u3009R) since there is at least one minimal conflict set w.r.t. \u3008nd.cs[r] \\ {nd[r]} ,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, namely X .\n(2): This proposition follows directly from (1) (\u201c\u21d0\u201d).\nAt the point where some witness of redundancy X of some node nd \u2208 D\u00d7 is found by QRC or CRC in UPDATETREE, the next steps (lines 62-65) involve the pruning of Qdup, Q, D\u00d7 and D\u2283. As already mentioned, Qdup is the first collection to be cleaned from redundant nodes (w.r.t. the witness X) in PRUNEQDUP in order to constitute an input to the PRUNE function that does not include any redundant nodes (w.r.t. the witnessX) and can be used \u201cblindly\u201d to construct replacement nodes of redundant nodes (w.r.t. the witness X) deleted from Q, D\u00d7 or D\u2283.\nBefore any pruning steps have ever been executed during the execution of Algorithm 5, Qdup comprises all generated nodes nddup for which, at generation time, there was one node nd \u2208 Q such that nddup = nd. That means, nddup is stored in Qdup in order to be available as an alternative equal node of nd or as an alternative subnode of some successor of nd in case nd is found to be redundant w.r.t. some current DPI.\nIf some node nddup in Qdup is found to be redundant w.r.t. the current DPI, there might be other nodes in Qdup from which a non-redundant alternative equal node nd\u2032dup of nddup w.r.t. the current DPI can be constructed. By Definition 12.3, we call such a node nd\u2032dup a combined replacement node of nddup. The name stems from the fact that nd\u2032dup is generated as a combination of existing nodes in Qdup. Combining two nodes nd1, nd2 \u2208 Qdup such that nd1 is a proper alternative subnode of nd2 yields nd3 with nd2 = nd3. nd3 is constructed in that the first (redundant) part of nd2 (and nd2.cs) is replaced by the (non-redundant) part nd1 (and nd1.cs).\nSuch a combination is \u201clegitimate\u201d since it gives a node nd3 that would have been constructed if all duplicate nodes would have been added to Q and processed regularly instead of being added to Qdup. The strategy to store duplicate nodes (where \u201cduplicate\u201d refers to the set a node represents) in a separate collection Qdup as soon as they are found is part of the space-saving policy the DYNAMICHS algorithm pursues. For, in general, this prevents the algorithm to generate and store exponentially many nodes corresponding to equal sets. Since diagnoses are sets and not lists like nodes, it suffices to find only one node corresponding to a diagnosis. Only if some active node (one that is not in Qdup) becomes redundant, some other set-equal node, if available, is constructed from the stored duplicate nodes. This idea is very similar to the way pruning is handled in the directed acyclic graph described in [GSW89].\nThe idea of node combination is formalized by the following definition.\nDefinition 12.5. Let S be a collection of nodes in DYNAMICHS and let Si be the set of nodes of cardinality i in S. Further, let the set Comb1(S) := S1 and let Combi(S) comprise\n\u2022 all nodes in Si and\n\u2022 all nodes nd such that nd is an alternative equal node of some node in Si constructed from some node in \u22c3i\u22121 j=1 Combj(S).\nThen, Comb(S) := \u22c3\u221e\ni=1 Combi(S) is called the set of combined nodes of S and a node in Combi(S) is called a combined node of cardinality i in S.\nFurther, let node be a node in DYNAMICHS and X be a minimal conflict set w.r.t. the current DPI. Then,\n\u2022 Combnode(S) := {nd | nd \u2208 Comb(S), nd = node} is the set of combined equal nodes of nd of S and\n\u2022 Combnode,X(S) \u2286 Combnode(S) is the set of combined equal nodes of nd of S for which X is not a witness of redundancy.\n208 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nThe following corollary summarizes some simple consequences of Definition 12.5.\nCorollary 12.6. Let S be a set of nodes in DYNAMICHS and let Si be the set of nodes of cardinality i in S. Then:\n(1) Combi(S) = \u2205 iff Si = \u2205.\n(2) Combi(S) includes only nodes of cardinality i.\n(3) Combnode(S) = \u2205 iff there is no node nd \u2208 S such that nd = node.\n(4) If nd \u2208 Combi(S) and nd /\u2208 S, then\n\u2022 there is some nd\u2032 \u2208 Combj(S) for some j \u2208 {1, . . . , i\u2212 1} and \u2022 some nd\u2032\u2032 \u2208 Si\nsuch that\n\u2022 nd\u2032 is an alternative subnode of nd\u2032\u2032 and \u2022 nd = ADD(nd\u2032, nd\u2032\u2032[j + 1..i]) and \u2022 nd.cs = ADD(nd\u2032.cs, nd\u2032\u2032.cs[j + 1..i]).\nThe example we give next illustrates Definition 12.5.\nExample 12.6 Recall the nodes nd, nd1, nd2, node1 and node2 of Example 12.4 and let nd3 := [1, 2, 6, 4] with nd3.cs := [\u30081, 2, 3\u3009 , \u30082, 6\u3009 , \u30083, 6, 7\u3009 , \u30084, 5\u3009] and S := {nd, nd1, nd2, nd3}. Then,\nS1 = \u2205 S2 = {nd1} S3 = {nd2} S4 = {nd, nd3} Si = \u2205 \u2200i > 4\nComb1(S) = \u2205 Comb2(S) = {nd1} Comb3(S) = {nd2} Comb4(S) = {nd, node1, node2, nd3, nd4} Combi(S) = \u2205 \u2200i > 4 Combnd(S) = {nd, node1, node2}\nCombnode1(S) = Combnode2(S) = Combnd(S)\nCombnd3(S) = {nd3, nd4} Combnd4(S) = Combnd3(S)\nwhere\nnd4 := [2, 1, 6, 4]\nnd4.cs := [\u30081, 2, 3\u3009 , \u30081, 4\u3009 , \u30083, 6, 7\u3009 , \u30084, 5\u3009]\nis the alternative equal node of nd3 constructed from nd1.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 209\nThe PRUNEQDUP function is always called given the current list Qdup which is anytime sorted in ascending order by node cardinality. This holds by lines 21, 121 and 124 which are the only places where nodes are added to Qdup throughout DYNAMICHS and where nodes are inserted into Qdup such that the order by node cardinality is preserved. Now, the next lemma substantiates that PRUNEQDUP, given some minimal conflict set X w.r.t. the current DPI, updates Qdup in a way that all redundant nodes w.r.t. the witness X are deleted, each deleted node is replaced by one non-redundant combined replacement node w.r.t. the witness X if such a one is constructable (cf. Definition 12.5), and for each remaining node nd, i.e. nd is a non-deleted node or a combined replacement node of some deleted node, each superset of X in nd.cs is replaced by X .\nThis leads to a new list Qdup returned by PRUNEQDUP which includes only non-redundant nodes w.r.t. the witness X . Furthermore, the new list Qdup contains a node corresponding to each set (path) S for which there was a corresponding node in the old list Qdup if there would be a non-redundant (w.r.t. X) node corresponding to S in a hitting set tree equal to the one produced by DYNAMICHS except that all duplicate nodes corresponding to equal sets (paths) would be regularly processed and expanded.\nLemma 12.8. Let \u3008K,B,P ,N \u3009R be a DPI and let the input parameters to the PRUNEQDUP function be:\n\u2022 X is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R,\n\u2022 Dup is a set of nodes sorted ascending by node cardinality.\nThen, PRUNEQDUP returns Dupnew where Dupnew includes\n(1) all nodes in Dup for which X is not a witness of redundancy,\n(2) at least one node in Combnd,X(Dup) for each node nd \u2208 Dup for which X is a witness of redundancy, if Combnd,X(Dup) 6= \u2205 and\n(3) only nodes nd such that there is no r \u2208 {1, . . . , |nd|} for which nd.cs[r] \u2283 X .\nProof. The function PRUNEQDUP walks through all nodes ndi in the set Dup. If X is not a witness of redundancy of ndi, tested in lines 111 and 112 exactly as prescribed by Definition 12.4, then k = 0 must hold in line 116 by lines 109-115. Thus, line 124 is executed and ndi added to Dupnew. Since no nodes are removed from Dupnew throughout PRUNEQDUP, proposition (1) is valid.\nOtherwise, i.e. if X is a witness of redundancy of ndi, then line 113 must have been executed at least once before line 116 is reached. This implies that k > 0 must hold in line 116. At this point, k stores the maximum position in (the list) ndi at which the redundancy criterion of lines 111 and 112 is satisfied. So, in line 117, nodes in Dupnew are tested successively until some ndj \u2208 Dupnew meets |ndj| \u2265 k and ndi[1..|ndj|] = ndj. This means that the subnode ndi[1..|ndj|] of ndi can be replaced by ndj (and ndi.cs[1..|ndj|] by ndj.cs) to yield an alternative equal node ndinew of ndi (lines 119 and 120).\nWe still have to show that X cannot be a witness of redundancy of ndinew. For this to hold it is sufficient that X is not a witness of redundancy of ndj by |ndj| \u2265 k. So, we must verify that Dupnew can comprise only nodes of which X is not a witness of redundancy. We prove this by induction.\nSince Dupnew is initialized to be the empty set when the function PRUNEQDUP starts executing, we just need to investigate which nodes are added to Dupnew within PRUNEQDUP. Addition of nodes to Dupnew happens at lines 121 and 124.\nBase case: When line 121 executed for the first time during the execution of PRUNEQDUP, Dupnew can only comprise nodes which have been added to it in line 124. By the argumentation used to prove proposition (1) of this lemma, it holds thatX is not a witness of redundancy of any node added toDupnew in line 124. Thus, there cannot be a witness of redundancy of the very first node added to Dupnew in line 121.\nInduction step: Let us assume that Dupnew comprises only nodes such that X is not a witness of redundancy of any of them. Further, suppose that ndinew is added to Dupnew when line 121 is executed\n210 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nfor the k-th time where k > 1. Then, by the same line of argument as in the base case, we can conclude that X is not a witness of redundancy of ndinew.\nEach node ndinew added to Dupnew in line 121 is an element of Combndi,X(Dup). Namely, ndj satisfies the criterion in line 118 and thus ndinew is an element ofCombndi(Dup) by Definition 12.5. And, as shown before, X is not a witness of redundancy of ndinew, wherefore ndinew \u2208 Combndi,X(Dup) by the definition of Combndi,X(Dup) (Definition 12.5).\nThence, if Combndi,X(Dup) 6= \u2205, there must be at least one node nd added to Dupnew such that nd \u2208 Combndi,X(Dup) andX is not a witness of redundancy of nd. Consequently, proposition (2) holds.\nProposition (3): First, observe that each node in Dup is definitely processed as ndi by the for-loop in line 107 and the fact that there is no criterion that can cause a preliminary break of this for-loop. Each time the first part of the redundancy check (line 111) is successful for ndi, we know that some conflict set ndi.cs[m] is non-minimal w.r.t. \u3008K,B,P ,N \u3009R. If the second part of the redundancy check (line 112) is negative, then ndi[m] \u2208 X , wherefore there is \u2013 at least so far \u2013 no evidence that ndi is redundant w.r.t. \u3008K,B,P ,N \u3009R. In this case, ndi might later be inserted to Dupnew (in case X is not a witness of redundancy of ndi) and hence the set ndi.cs[m] is replaced by the minimal conflict set X w.r.t. \u3008K,B,P ,N \u3009R in line 115. If the second part of the redundancy check in line 112 is positive, then it is guaranteed that ndi is either combined-replaced or pruned. This holds due to lines 116-122 and since k > 0 must be true due to line 113. That a combined replacement node that might be found for some redundant ndi throughout lines 116-122 meets proposition (3) can be shown by induction in a very similar way as proposition (2) was shown.\nThe following corollary is a direct consequence of Lemma 12.8 and states that the updated list Qdup (if interpreted as a set) is a subset of the set of combined nodes of the old list Qdup. In other words, no nodes corresponding to sets (paths) that are not represented by a node in the old list Qdup can be introduced throughout PRUNEQDUP. The introduction of such nodes corresponding to \u201cnew\u201d sets (paths) can only take place in line 21 where newly generated nodes are added to Qdup.\nCorollary 12.7. Given the same preconditions as in Lemma 12.8, PRUNEQDUP returns Dupnew where Dupnew \u2286 Comb(Dup).\nThe following result provides sufficient and necessary criteria for a node nd to be a combined node of Qdup. Roughly, these criteria involve the existence of a sequence of nodes nd1, . . . , ndk \u2208 Qdup where each node in this sequence is a proper alternative subnode of the next node and nd is constructed from this sequence of nodes in that nd is an alternative equal node of ndk constructed from nd\u2032k\u22121. nd \u2032 k\u22121 in turn is an alternative equal node of ndk\u22121 constructed from nd\u2032k\u22122, and so on. Finally, nd \u2032 2 is an alternative equal node of nd2 constructed from nd1 and nd1 \u2208 Qdup.\nLemma 12.9. Let nd be a node in DYNAMICHS. Then, nd \u2208 Comb(Qdup) iff there are nodes nd1, . . . , ndk \u2208 Qdup for k \u2265 1 such that\n(1) |nd1| < \u00b7 \u00b7 \u00b7 < |ndk| = |nd|,\n(2) it holds that\nnd[i1] = nd1[i1] for i1 \u2208 {1, . . . , |nd1|} nd[i2] = nd2[i2] for i2 \u2208 {|nd1|+ 1, . . . , |nd2|}\n. . .\nnd[ik] = ndk[ik] for ik \u2208 {|ndk\u22121|+ 1, . . . , |ndk|}\nand\n(3) ndi is an alternative subnode of ndi+1 for i \u2208 {1, . . . , k \u2212 1}.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 211\nProof. \u201c\u21d2\u201d: Suppose nd \u2208 Comb(Qdup) and that |nd| = i. Then, there are two cases, either nd \u2208 Qdup or nd /\u2208 Qdup.\nIn the former case, we can define nd1 as nd and the proposition of the lemma holds. In the latter case, by proposition 2 of Corollary 12.6, Definition 12.5 and |nd| = i, it holds that nd \u2208 Combi(Qdup). By proposition 4 of Corollary 12.6 and the fact that nd \u2208 Combi(Qdup), there is some nd\u2032 \u2208 Combj(Qdup) for some j \u2208 {1, . . . , i\u2212 1} and some nd\u2032\u2032 \u2208 Qdup with |nd\u2032\u2032| = i such that nd = ADD(nd\u2032, nd\u2032\u2032[j + 1..i]) and nd.cs = ADD(nd\u2032.cs, nd\u2032\u2032.cs[j + 1..i]). Moreover, proposition 4 of Corollary 12.6 states that nd\u2032 is an alternative subnode of nd\u2032\u2032.\nSo, set ndk to nd\u2032\u2032 and ndk\u22121 to nd\u2032. Then, we obtain that |ndk\u22121| < |ndk| by j < i, that ndk\u22121 is an alternative subnode of ndk and that nd[ik] = ndk[ik] for ik \u2208 {|ndk\u22121|+ 1, . . . , |ndk|} must be true. That is, propositions (1), (2) and (3) hold for ndk and ndk\u22121.\nNow, again, there are two cases for ndk\u22121, i.e. either ndk\u22121 \u2208 Qdup or ndk\u22121 /\u2208 Qdup. In the former case, we can define nd1 as ndk\u22121 and the proposition of the lemma holds. In the latter case, the same argumentation as for nd can be applied to show the existence of some ndk\u22122 that meets propositions (1), (2) and (3). Due to the fact that the cardinality of ndk\u2212i\u22121 is strictly smaller than the cardinality of ndk\u2212i for all i and the fact that Comb1(Qdup) = Qdup, the case ndk\u2212m \u2208 Qdup must finally arise for some m.\n\u201c\u21d0\u201d: Suppose there are nodes nd1, . . . , ndk \u2208 Qdup such that propositions (1)-(3) are satisfied. Let k = 1. Then, by propositions (1) and (2) of this lemma, we have that nd is the same node as nd1. Since nd1 \u2208 Qdup and by Definition 12.5, we have that nd \u2208 Comb(Qdup). So, the lemma holds for k = 1.\nNow, assume that the lemma holds for k = m for some natural number m. That is, assume that there is a node nd \u2208 Comb(Qdup) if there are nodes nd1, . . . , ndm \u2208 Qdup such that |nd1| < \u00b7 \u00b7 \u00b7 < |ndm| = |nd|,\nnd[i1] = nd1[i1] for i1 \u2208 {1, . . . , |nd1|} nd[i2] = nd2[i2] for i2 \u2208 {|nd1|+ 1, . . . , |nd2|}\n. . .\nnd[im] = ndm[im] for im \u2208 {|ndm\u22121|+ 1, . . . , |ndm|}\nand ndi is an alternative subnode of ndi+1 for i \u2208 {1, . . . ,m\u2212 1}. Let now k = m + 1. That is, assume that there are nodes nd1, . . . , ndm+1 \u2208 Qdup such that |nd1| < \u00b7 \u00b7 \u00b7 < |ndm+1| = |nd|,\nnd\u2032[i1] = nd1[i1] for i1 \u2208 {1, . . . , |nd1|} nd\u2032[i2] = nd2[i2] for i2 \u2208 {|nd1|+ 1, . . . , |nd2|}\n. . .\nnd\u2032[im+1] = ndm+1[im+1] for im+1 \u2208 {|ndm|+ 1, . . . , |ndm+1|}\nand ndi is an alternative subnode of ndi+1 for i \u2208 {1, . . . ,m}. What we need to show is that nd\u2032 \u2208 Comb(Qdup).\nIf nd\u2032 \u2208 Qdup, then, by Definition 12.5, the lemma is true. So suppose nd\u2032 /\u2208 Qdup. By the definition of an alternative subnode (Definition 12.2), nodesub \u2286 node in case nodesub is an alternative subnode of node. So, because ndi is an alternative subnode of ndi+1 and |ndi| < |ndi+1| for i \u2208 {1, . . . ,m}, we have that ndi \u2282 ndi+1 for i \u2208 {1, . . . ,m}. Consequently, ndi \u2286 nd\u2032 for i \u2208 {1, . . . ,m+ 1} and ndi \u2286 nd for i \u2208 {1, . . . ,m} must hold. Due to |ndm| = |nd| we obtain the set-equality between ndm and nd. This result along with ndm \u2286 nd\u2032 and |ndm| < |ndm+1| = |nd\u2032|\n212 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nimplies that nd \u2282 nd\u2032. However, since\nndx[i1] = nd1[i1] for i1 \u2208 {1, . . . , |nd1|} ndx[i2] = nd2[i2] for i2 \u2208 {|nd1|+ 1, . . . , |nd2|}\n. . .\nndx[im] = ndm[im] for im \u2208 {|ndm\u22121|+ 1, . . . , |ndm|}\nis met for ndx being the same node as nd as well as for ndx being the same node as nd\u2032, we can conclude that nd[i] = nd\u2032[i] for i \u2208 {1, . . . , |nd|}.\nMoreover, we have that nd\u2032[im+1] = ndm+1[im+1] for im+1 \u2208 { |nd|+ 1, . . . , |nd\u2032| } since |nd| = |ndm| and |nd\u2032| = |ndm+1|. Since nd\u2032 /\u2208 Qdup and ndm+1 \u2208 Qdup by assumption, we have that ndm+1 which is set-equal to nd\u2032 (as argued before) must be an alternative equal node of nd\u2032. That is, there must be some j such that ndm+1[j] 6= nd\u2032[j] or ndm+1.cs[j] 6= nd\u2032.cs[j] for some j \u2208 {1, . . . , |nd|}. Hence, ndm+1[j] 6= nd[j] or ndm+1.cs[j] 6= nd.cs[j] wherefore nd must be an alternative subnode of ndm+1. Because nd \u2208 Comb(Qdup) and ndm+1 \u2208 Qdup, we infer by Definition 12.5 that nd\u2032 \u2208 Comb(Qdup).\nThe PRUNE function (lines 63-65) is called given a collection S \u2208 {Q,D\u00d7,D\u2283}, a minimal conflict set X w.r.t. the current DPI and Qdup which has already been updated and cleaned from redundant nodes (w.r.t. the witness X) by the PRUNEQDUP function. So, let nddup \u2208 Qdup be a (not necessarily proper) alternative subnode of some node node that is stored in S. AssumeX is a witness of redundancy of node. By Lemma 12.8 and since nddup \u2208 Qdup, X cannot be a witness of redundancy of nddup. Further, let r \u2208 {1, . . . , |node|} be the highest number such that X \u2282 node.cs[r] and node[r] \u2208 node.cs[r] \\ X . Now, in case r \u2264 |nddup| holds, nddup (and nddup.cs) can be used to replace the first |nddup| elements of node (and node.cs). The result is an alternative equal node of node which is non-redundant w.r.t. the current DPI and which can be added to S after deletion of node as a representative of the set (path) node has represented.\nNow, the next lemma substantiates that PRUNE updates S in a way that all redundant nodes w.r.t. the witness X are deleted, each deleted node is replaced by one non-redundant replacement node w.r.t. the witness X if such a one is constructable from Qdup and for each remaining node nd, i.e. nd is a nondeleted node or a replacement node of some deleted node, each superset of X in nd.cs is replaced by X .\nThis leads to a new set S returned by PRUNE which includes only non-redundant nodes w.r.t. the witnessX . Furthermore, the new set S contains a node corresponding to each set (path) Y for which there was a corresponding node in the old set S if there would be a non-redundant (w.r.t.X) node corresponding to Y in a hitting set tree equal to the one produced by DYNAMICHS except that all duplicate nodes corresponding to equal sets (paths) would be regularly processed and expanded.\nLemma 12.10. Let \u3008K,B,P ,N \u3009R be a DPI and let the following be the input parameters to the PRUNE function:\n\u2022 X is a minimal conflict set w.r.t. \u3008K,B,P ,N \u3009R,\n\u2022 S is a set of nodes in DYNAMICHS,\n\u2022 Dup is a set of nodes where\n\u2013 X is not a witness of redundancy of any node in Dup and\n\u2013 for each nd \u2208 S there might be some nd\u2032 \u2208 Dup such that nd\u2032 is an alternative subnode of nd and\n12.4. ALGORITHM DETAILS AND CORRECTNESS 213\n\u2013 for each node nd \u2208 Dup there is no r \u2208 {1, . . . , |nd|} for which nd.cs[r] \u2283 X .\n\u2022 pnodes is as defined by Definition 4.9.\nThen, PRUNE returns S\u2032 where the following holds:\n(1) S\u2032 is a set such that S \\ S\u2032 includes exactly these nodes in S for which X is a witness of redundancy and S \u2229 S\u2032 includes exactly these nodes in S for which X is not a witness of redundancy.\n(2) Each element nd \u2208 S\u2032 \\S is an alternative equal node of some node in S \\S\u2032 constructed from some node in Dup such that X is not a witness of redundancy of nd.\n(3) Let nd \u2208 S \\ S\u2032 and Altnd denote the set of all alternative equal nodes of nd, each of which can be constructed from some node in Dup and for each of which X is not a witness of redundancy. Then there is some nd\u2032 \u2208 Altnd such that nd\u2032 \u2208 S\u2032 \\ S.\n(4) S\u2032 includes only nodes nd such that there is no r \u2208 {1, . . . , |nd|} for which nd.cs[r] \u2283 X .\nProof. The PRUNE procedure runs through all nodes nd \u2208 S and for each nd runs through all sets in nd.cs (lines 87 and 89). Lines 90 and 91 perform a check whether X is a witness of redundancy of nd, implementing exactly the criteria given by Definition 12.4. If the check is not successful for any i \u2208 {1, . . . , |nd|}, i.e. X is not a witness of redundancy of nd, then k = 0 must hold when line 95 is reached. Hence, nd is added to S\u2032 in line 103 in this case. As only nodes different from nd can be added to S\u2032 in line 100 and as there are no other ways nodes might be added to S\u2032, we have that S \\ S\u2032 includes exactly these nodes in S for which X is a witness of redundancy and S \u2229S\u2032 includes exactly these nodes in S for which X is not a witness of redundancy. So, proposition (1) is true.\nThe truth of proposition (2) can be derived as follows: By the proof of proposition (1), line 100 is the only place where nodes that are not elements of S are added to S\u2032. Hence, each node in S\u2032 \\ S must be added to S\u2032 in line 100. Thus, only nodes nodenew := ADD(node, nd[|node| + 1..|nd|]) with nodenew.cs := ADD(node.cs, nd.cs[|node| + 1..|nd|]) constructed exactly as per Definition 12.2 in lines 98 and 99 where nd \u2208 S can be added to S\u2032.\nNow, we still have to show that node is an alternative subnode of nd. From the precondition that X is not a witness of redundancy of any node in Dup, X cannot be a witness of redundancy of node. Moreover, |node| \u2265 k must hold as line 97 has been passed. So, we have that X must be a witness of redundancy for nd[1..|node|] since k > 0 (line 95) and by the way k is constructed (lines 88-92). Hence, there must be some j \u2208 {1, . . . , |node|} with the property that node[j] 6= nd[j] or node.cs[j] 6= nd.cs[j] wherefore node is indeed an alternative subnode of nd. Thus, nodenew is an alternative equal node of nd by Definition 12.2.\nThat nd \u2208 S \\ S\u2032 must be true can be explained as follows. By the argumentation to prove proposition (1) and (2) so far, we know that only nodes can be added to S\u2032 in line 100 and line 103 for which X is not a witness of redundancy. Moreover, we have shown that line 100 can only be reached for some node nd \u2208 S for which X is a witness of redundancy. Consequently, nd /\u2208 S\u2032 must hold.\nThat X is not a witness of redundancy of nodenew can be derived as follows: From the precondition that X is not a witness of redundancy of any node in Dup, X cannot be a witness of redundancy of nodenew[1..|node|] with nodenew.cs[1..|node|] since nodenew[j] = node[j] and nodenew.cs[j] = node.cs[j] for all j \u2208 {1, . . . , |node|}. k is the maximum index such that X \u2282 nd.cs[k] and nd[k] \u2208 nd.cs[k]\\X by lines 88-92. Since |node| \u2265 k,X cannot be a witness of redundancy of nodenew[|node|+ 1..|nd|] with nodenew.cs[|node|+ 1..|nd|] either since nodenew[j] = nd[j] and nodenew.cs[j] = nd.cs[j] for all j \u2208 {|node|+ 1, . . . , |nd|}. Therefore, X cannot be a witness of redundancy of nodenew.\nProposition (3): As already argued, for each node nd \u2208 S \\ S\u2032, line 96 must be reached. Then, in line 96, all nodes inDup are investigated in order to find an alternative subnode of nd. So, if there is such a one, then it must be found.\n214 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nProposition (4): For a node nd that is added to S\u2032 in line 103, the for-loop in line 89 must have been executed. Since, as already shown, line 92 cannot be executed for a node that is added to S\u2032 in line 103, line 94 must have been executed for all i \u2208 {1, . . . , |nd|}. Hence, proposition (4) holds for all nodes inserted into S\u2032 in line 103.\nFor nodes\nnodenew := ADD(node, nd[|node|+ 1..|nd|]) nodenew.cs := ADD(node.cs, nd.cs[|node|+ 1..|nd|])\ninserted into S\u2032 in line 100, proposition (4) follows from the precondition that Dup includes only nodes n such that there is no r \u2208 {1, . . . , |n|} for which n.cs[r] \u2283 X , from the fact that node \u2208 Dup and the fact that line 94 must have been executed for all indices i > k."}, {"heading": "12.4.7 De-Facto Non-Redundant Nodes in DYNAMICHS", "text": "The following definition introduces a notion that is of rather theoretical use for the proof of completeness of DYNAMICHS we will give later. The definition assumes a fixed DPI and characterizes as active sublabel of a particular conflict set nd.cs[r] in nd.cs the subset of nd.cs[r] that \u201csurvives\u201d all the pruning steps, i.e. PRUNEQDUP and PRUNE calls, during all executions of DYNAMICHS up to the one with a current DPI DPI . Notice that the shape of the active sublabel can never be known in advance as we do not know which witnesses of redundancy might be found. This makes up the theoretical nature of this definition. However, we will be able to show that no active sublabel of a node can be the empty set under certain preconditions that are met for DYNAMICHS.\nDefinition 12.6. Let\n\u2022 nd be a node in DYNAMICHS,\n\u2022 r \u2208 {1, . . . , |nd|} fixed,\n\u2022 DPI1, . . . , DPIn be a sequence of DPIs where DPIj includes a proper subset of the test cases DPIj+1 includes for j \u2208 {1, . . . , n\u2212 1},\n\u2022 DPIn is equal to DPI or includes a proper subset of the test cases DPI includes,\n\u2022 C1, . . . , Cn be the chronological sequence of all sets X given as an argument to PRUNE and PRUNEQDUP during all executions of DYNAMICHS up to and including the one with current DPI DPI where\n\u2013 each Ci is a minimal conflict set w.r.t. DPIi for i \u2208 {1, . . . , n} \u2013 Ck \u2283 Ck+1 for k \u2208 {1, . . . , n\u2212 1}, \u2013 nd.cs[r] \u2283 C1.\nThen, we call Cn the active sublabel of nd.cs[r] w.r.t. DPI .\nThe next definition of a de-facto non-redundant node is based on Definition 12.6. A de-facto nonredundant node w.r.t. DPI includes at each position an element that hits the active sublabel w.r.t. DPI at this position. Again, this definition is of theoretical rather than practical use, but crucial for the proof of completeness of DYNAMICHS. In fact, we will be able to show that for each minimal diagnosis w.r.t. DPI there must be \u2013 anytime during any execution of DYNAMICHS with a current DPI including a subset of the test cases in DPI \u2013 a de-facto non-redundant node corresponding to a subset of this diagnosis. In further consequence, this will allow us to derive the algorithm\u2019s completeness concerning the detection of all minimal diagnoses w.r.t. DPI .\n12.4. ALGORITHM DETAILS AND CORRECTNESS 215\nDefinition 12.7. We call a node nd in DYNAMICHS de-facto non-redundant w.r.t. DPI iff nd[r] is an element of an active sublabel w.r.t. DPI for all r \u2208 {1, . . . , |nd|}.\nA de-facto non-redundant node w.r.t. a DPI DPI \u201csurvives\u201d all pruning steps at least until the execution of DYNAMICHS with current DPI DPI:\nProposition 12.7. Let nd be a node which is de-facto non-redundant w.r.t. DPI . Then, nd cannot be pruned or replaced during any execution of DYNAMICHS up to and including the one with current DPI DPI .\nProof. By Definitions 12.6 and 12.7, PRUNE and PRUNEQDUP cannot be called given a witness of redundancy of nd during any execution of DYNAMICHS up to and including the one with current DPI DPI . By Lemmata 12.8 and 12.10, only nodes can be pruned or replaced for which the input set X given to PRUNE and PRUNEQDUP is a witness of redundancy.\nExample 12.7 Let K = {1, . . . , 10} be the KB of the (admissible) input DPI DPI0 to Algorithm 5 and let nd := [1, 2, 3, 4] with nd.cs := [\u30081, 5, 7\u3009 , \u30082, 4, 6\u3009 , \u30083, 6, 7\u3009 , \u30084, 5\u3009] be a node stored by DYNAMICHS during the execution of some call to DYNAMICHS during Algorithm 5. Moreover, let DPI be a fixed DPI constructed during the execution Algorithm 5 that includes a (not necessarily proper) superset of the test cases in DPI0. Assume that the chronological sequence of all inputs X to PRUNE and PRUNEQDUP throughout all executions of DYNAMICHS up to and including the one with current DPI DPI during Algorithm 5 and after nd has been generated is given by \u30081, 6\u3009 , \u30083, 7\u3009 , \u30081, 3, 8\u3009 , \u30082\u3009 , \u30084\u3009 , \u30081, 5\u3009.\nThen nd.cs undergoes the transition depicted by Table 12.1 induced by this sequence of X arguments to PRUNE/PRUNEQDUP. We can observe in Table 12.1 that each proper superset of some argument X of\nPRUNE/PRUNEQDUP that occurs in nd.cs is replaced byX (cf. Lemmata 12.8 and 12.10). This is the case, for instance, for X = \u30083, 7\u3009 in the second row of the table which replaces nd.cs[3] = \u30083, 6, 7\u3009. Similar situations can be found in rows 4-6. No changes to nd.cs are triggered for X = \u30081, 6\u3009 or X = \u30081, 3, 8\u3009 in rows 1 and 3, respectively, because at this stage nd.cs does not include any superset of X .\nWe learn from the last row of the table that nd is de-facto non-redundant w.r.t. DPI . This holds, first, since we considered the chronological sequence of all inputs X to PRUNE and PRUNEQDUP throughout all executions of DYNAMICHS up to and including the one with current DPI DPI during Algorithm 5. Second, we have that\nnd[1] = 1 \u2208 \u30081, 5\u3009 = nd.cs\u2032[1] nd[2] = 2 \u2208 \u30082\u3009 = nd.cs\u2032[2] nd[3] = 3 \u2208 \u30083, 7\u3009 = nd.cs\u2032[3] nd[4] = 4 \u2208 \u30084\u3009 = nd.cs\u2032[4]\n216 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nwhere nd.cs\u2032 is the value of nd.cs given by the last row of the table which is the \u201ccurrent\u201d value of nd.cs during the execution of DYNAMICHS with current DPI DPI . By Definition 12.6, nd.cs\u2032[i] is the active sublabel of nd.cs[i] w.r.t. DPI for i \u2208 {1, . . . , 4}. That is, for example, \u30083, 7\u3009 is the active sublabel of nd.cs[3]. As we realized that each element of nd is an element of an active sublabel w.r.t.DPI , we obtain the de-facto non-redundancy of nd w.r.t. DPI as per Definition 12.7.\nNotice that the sole definition of redundancy of a node w.r.t.DPI (Definition 12.4) does not perfectly serve our purposes as it does not take into account the order in which new conflict sets emerge and are used for pruning.\nFor instance, consider nd.cs[2] = \u30082, 4, 6\u3009which includes 2 as well as 4. Both values \u30082\u3009 and \u30084\u3009 ofX in rows 4 and 5 of Table 12.1 must be conflict sets w.r.t.DPI by Proposition 12.1, which says that conflict sets cannot grow after the addition of a test case to a DPI, and the fact that each X must be a minimal conflict set w.r.t. some DPI including a subset of the test cases in DPI . In fact, by Proposition 12.2 and the admissibility of DPI0, \u30082\u3009 and \u30084\u3009 are even minimal conflict sets w.r.t. DPI . Thus, application of Definition 12.4 yields that nd is redundant w.r.t. DPI because \u30084\u3009 \u2282 \u30082, 4, 6\u3009 and nd[2] = 2 \u2208 \u30082, 4, 6\u3009 \\ \u30084\u3009 (cf. Definition 12.4). However, bearing in mind that \u30082\u3009 was known to the algorithm before \u30084\u3009, or, \u30082\u3009 was used for pruning before \u30084\u3009, we have that the set nd.cs[2], after being modified by PRUNE or PRUNEQDUP, is not redundant w.r.t. DPI . This is true since the new set nd.cs[2] = \u30082\u3009 which is not a superset of \u30084\u3009.\nSo, to summarize, a node is (theoretically) redundant w.r.t. DPI as per Definition 12.4 iff there is a minimal conflict set w.r.t. DPI which is a witness of redundancy of this node. As however the example above has shown, whether a node is found to be redundant or not depends on the order of conflict sets used for pruning. This fact is also mentioned in [GSW89]. And, a (theoretically) redundant node w.r.t. DPI does not necessarily need to be discovered by DYNAMICHS and might be modified by PRUNE or PRUNEQDUP in a way that it becomes non-redundant w.r.t. DPI .\nOn the other hand, the definition of de-facto non-redundancy w.r.t. DPI (Definition 12.7) incorporates exactly these thoughts and declares only nodes as de-facto non-redundant w.r.t. DPI which are actually not found to be redundant w.r.t. DPI .\nThe criteria for a node nd to be a combined node of Qdup given by Lemma 12.9 will facilitate the proof of the next lemma. This lemma states that a combined node of Qdup which is non-redundant w.r.t. some DPI \u3008K,B,P\u222aP \u2032,N \u222aN \u2032\u3009R cannot be pruned during DYNAMICHS given i.a. the DPI \u3008K,B,P ,N \u3009R and sets of positively and negatively answered queries P \u2032\u2032 and N \u2032\u2032 as input where P \u2032\u2032 \u2286 P \u2032 and N \u2032\u2032 \u2286 N \u2032. This result will constitute an essential prerequisite for the proof of completeness of DYNAMICHS.\nLemma 12.11. Let nd \u2208 Comb(Qdup) be some node that is de-facto non-redundant w.r.t. the DPI DPI and letDPI \u2032 be some DPI that is either equal toDPI or includes only a subset of the test cases ofDPI . Then, throughout any execution of DYNAMICHS using the current DPIDPI \u2032, nd \u2208 Comb(Qdup) holds.\nProof. First, we show that there cannot be a minimal conflict set C w.r.t. DPI \u2032 such that PRUNEQDUP is called with X := C and there is some q \u2208 {1, . . . , |nd|} with the property that C \u2282 nd.cs[q] and nd[q] \u2208 nd.cs[q] \\ C.\nSo, assume that PRUNEQDUP is called with X := C and there is some C w.r.t. DPI \u2032 such that there is some q \u2208 {1, . . . , |nd|} with the property that C \u2282 nd.cs[q] and nd[q] \u2208 nd.cs[q] \\ C. Let now C1, . . . , Cn be the (arbitrary actual) chronological sequence of all sets X given as an argument to PRUNE and PRUNEQDUP during all executions of DYNAMICHS up to and including the one with current DPI DPI where\n\u2022 nd.cs[q] \u2283 C1,\n\u2022 each Ci is a minimal conflict set w.r.t. DPIi for i \u2208 {1, . . . , n}\n\u2022 Ck \u2283 Ck+1 for k \u2208 {1, . . . , n\u2212 1},\n12.4. ALGORITHM DETAILS AND CORRECTNESS 217\n\u2022 DPIj includes a proper subset of the test cases DPIj+1 includes for j \u2208 {1, . . . , n\u2212 1},\n\u2022 DPIn is equal to DPI or includes a proper subset of the test cases DPI includes.\nThen, Cn is the active sublabel of nd.cs[q] w.r.t. DPI . Since C \u2282 nd.cs[q] and X := C is an argument of PRUNEQDUP during DPI \u2032, we have that C must be equal to some set Cj in the sequence C1, . . . , Cn. By Definition 12.7 and the de-facto non-redundancy of nd w.r.t. DPI , nd.cs[q] \u2208 Cn must hold. By Cn \u2286 Cj = C, we finally obtain nd.cs[q] \u2208 C, which is a contradiction to nd[q] \u2208 nd.cs[q] \\ C.\nLemma 12.9 and nd \u2208 Comb(Qdup) guarantee the existence of nodes nd1, . . . , ndk \u2208 Qdup for k \u2265 1 such that\n(1) |nd1| < \u00b7 \u00b7 \u00b7 < |ndk| = |nd|,\n(2) it holds that\nnd[i1] = nd1[i1] for i1 \u2208 {1, . . . , |nd1|} nd[i2] = nd2[i2] for i2 \u2208 {|nd1|+ 1, . . . , |nd2|}\n. . .\nnd[ik] = ndk[ik] for ik \u2208 {|ndk\u22121|+ 1, . . . , |ndk|}\nand\n(3) ndi is an alternative subnode of ndi+1 for i \u2208 {1, . . . , k \u2212 1}.\nSo, let us assume that nd /\u2208 Comb(Qdup) at some point in time during the execution of DYNAMICHS using the current DPI DPI \u2032. That is, some node ndj for some j \u2208 {1, . . . , k} must have been deleted from Qdup. Nodes can only be deleted from Qdup in the scope of the function PRUNEQDUP. By Lemma 12.8 and Corollary 12.2, only nodes for which X is a witness of redundancy can be deleted from Qdup by the function PRUNEQDUP where X is the minimal conflict set given to PRUNEQDUP.\nThus, assume that ndj for some j \u2208 {1, . . . , k} is the first node among nd1, . . . , ndk \u2208 Qdup deleted from Qdup by PRUNEQDUP given the minimal conflict set X w.r.t. DPI \u2032 as an argument. Then, as X must be a witness of redundancy of ndj , we have that there is some m \u2208 {1, . . . , |ndj |} such that X \u2282 ndj .cs[m] and ndj [m] \u2208 ndj .cs[m] \\X .\nSince Lemma 12.9 holds also for j \u2264 k and ndj is the first node among nd1, . . . , ndk \u2208 Qdup deleted from Qdup, we deduce that there is some node node \u2208 Comb(Qdup) such that |node| = |ndj | and node[r] = nd[r] for r \u2208 {1, . . . , |ndj |} where |ndj | \u2264 |nd|. As pointed out before, there cannot be any q \u2208 {1, . . . , |nd|} such that X \u2282 nd.cs[q] and nd[q] \u2208 nd.cs[q] \\X . This, however, is a contradiction that there is some m \u2208 {1, . . . , |ndj |} such that X \u2282 ndj .cs[m] and ndj [m] \u2208 ndj .cs[m] \\X .\nHence, none of the nodes nd1, . . . , ndk \u2208 Qdup can be deleted throughout the execution of DYNAMICHS using the current DPI DPI \u2032. Consequently, by Lemma 12.9, nd \u2208 Comb(Qdup) must be preserved.\nThe finding of the next lemma is that a node nd in DYNAMICHS cannot be processed before all nodes that are set-equal to nd or proper subsets of nd have been generated.\nLemma 12.12. Let GenNodes be the set of all nodes generated throughout the execution of all calls to DYNAMICHS during the execution of Algorithm 5. Then, a node nd cannot be processed before each node nd\u2032 \u2208 GenNodes where nd\u2032 \u2286 nd is generated.\nProof. Let nd\u2032 \u2208 GenNodes such that nd\u2032 \u2286 nd. Assume that nd is processed, but nd\u2032 has not yet been generated. In order to be processed, nd must be an element of Q. By the fact that nd\u2032 \u2208 GenNodes, nd\u2032 must be generated at some point in time. In order for nd\u2032 to be generated, some node nd\u2032\u2032 with nd\u2032\u2032 \u2282 nd\u2032 must be an element of Q. This follows from\n218 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\n\u2022 the fact that each generated node is a superset of some node in Q (cf. lines 6, 18 and 23 and Definition 12.3),\n\u2022 the fact that Q can only be modified by (a) deleting from Q some node and adding a set of successor nodes of it to Q (lines 6, 7 and 23) or by (b) deleting from Q some node and possibly adding to Q a replacement node of it in the function PRUNE and\n\u2022 the fact that for any replacement node ndrep of nd it holds that ndrep = nd.\nBy Lemma 4.14, each node which is a proper subset of another node has a higher probability as per pnodes(). Since nd is processed before nd\u2032 is generated and nodes in Q are processed in descending order of pnodes() (lines 23 and 6), pnodes(nd) > pnodes(nd\u2032\u2032) where nd\u2032\u2032 \u2282 nd\u2032 \u2286 nd, contradiction.\nThe purpose of the following definition is to refer to a node that results from another node nd by several replacements conducted by PRUNE as a node in a transitive replaces-relation with nd. This will simplify the notation used in the following two lemmata.\nDefinition 12.8. Let ndi \u223cRep ndj iff ndi is a replacement node of ndj computed so far by PRUNE at any time during the execution of any call to DYNAMICHS during the execution of Algorithm 5. Further, let the set Rep := {\u3008ndi, ndj\u3009 | ndi \u223cRep ndj}. Then we say that nd1 is in a transitive replaces-relation with ndk iff there is a sequence of nodes nd1, nd2, . . . , ndk\u22121, ndk such that \u3008ndi, ndi+1\u3009 \u2208 Rep for all i \u2208 {1, . . . , k \u2212 1}."}, {"heading": "12.4.8 Completeness of DYNAMICHS", "text": "Lemmata 12.13 and 12.14 constitute the key results towards proving the completeness of DYNAMICHS in terms of finding the complete set of minimal diagnoses w.r.t. any current DPI DPI in case the execution of DYNAMICHS with current DPI DPI terminates on account of Q = []. In other words, if there are no more open nodes in the hitting set tree constructed by DYNAMICHS with current DPI DPI , all minimal diagnoses w.r.t. DPI have been labeled by valid and are thus elements of the set Dcalc.\nThe completeness proof (Lemma 12.8) will be a proof by induction where Lemma 12.13 will serve to derive the base case of the induction, whereas Lemma 12.14 will be exploited to establish the induction step.\nLemma 12.13 assumes an arbitrary fixed \u201ccurrent\u201d DPI DPI such that DYNAMICHS with this \u201ccurrent\u201d DPI DPI returns due to Q = []. Further on, it assumes an arbitrary minimal diagnosis D w.r.t. DPI and a de-facto non-redundant node nd w.r.t. DPI which is a proper subset of D generated anytime throughout all executions of DYNAMICHS during the execution of Algorithm 5 up to the one with the current DPI DPI .\nGiven these preconditions, the lemma establishes the existence of a node ndsuc that corresponds to a superset of nd and to a subset of D, includes one element more than the set nd and is generated anytime throughout all executions of DYNAMICHS during the execution of Algorithm 5 up to the one with the current DPI DPI . Moreover, it states that the node nd\u2032suc set-equal to this generated node that is an element of Q cannot be pruned. However, it might be replaced. In case there is only one potential replacement node of nd\u2032suc constructable from (the combined nodes of) Qdup, this replacement node is de-facto non-redundant w.r.t. DPI . Any node nd\u2032suc,rep in a transitive replaces relation with nd \u2032 suc cannot be pruned either. It might again be replaced. In case there is only one potential replacement node of nd\u2032suc,rep constructable from (the combined nodes of) Qdup, this replacement node is de-facto non-redundant w.r.t. DPI .\nFiguratively, with respect to the hitting set tree constructed by DYNAMICHS, this lemma predicates the following: Let the hitting set tree produced by DYNAMICHS be completely constructed for an arbitrary DPI DPI . In case there is any tree branch whose edge labels correspond to a part of the minimal\n12.4. ALGORITHM DETAILS AND CORRECTNESS 219\ndiagnosisD w.r.t. DPI and which is known to be definitely not pruned during this tree construction, then this branch must be extended by one edge labeled by an element of D and this extended path is known to be definitely not pruned during this tree construction.\nNotice that during this tree construction, in practice, we will generally never be able to say that a concrete branch corresponding to a partial minimal diagnosis will definitely not be pruned. For, this depends on the answers to queries submitted by the interacting user. Nevertheless, for the proof of completeness of DYNAMICHS, it suffices to just know that there is any such branch in the tree.\nLemma 12.13. Assume the execution of DYNAMICHS with the current DPI DPI and assume that the execution stops due to Q = []. Let\n\u2022 GenNodes be the set of all nodes generated throughout the execution of all calls to DYNAMICHS during the execution of Algorithm 5,\n\u2022 D be some minimal diagnosis w.r.t. DPI ,\n\u2022 nd \u2208 GenNodes such that nd is de-facto non-redundant w.r.t. DPI and nd \u2282 D and\nThen there are nodes ndsuc and nd \u2032 suc such that the following holds:\n(1) nd \u2282 ndsuc \u2286 D.\n(2) |ndsuc| = |nd|+ 1.\n(3) ndsuc \u2208 GenNodes.\n(4) nd\u2032suc = ndsuc is an element of Q immediately after ndsuc has been generated.\n(5) If PRUNE is called given a witness of redundancy of nd\u2032suc, then some replacement node of nd \u2032 suc\nis found. If only one replacement node of nd\u2032suc is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\n(6) Let nd\u2032suc,rep be in a transitive replaces-relation with nd \u2032 suc. If PRUNE is called given a witness of\nredundancy of nd\u2032suc,rep, then some replacement node of nd \u2032 suc,rep is found. If only one replacement node of nd\u2032suc,rep is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\nProof. Now, since nd \u2208 GenNodes, we know that nd must be generated at some point in time during the execution of any call to DYNAMICHS during the execution of Algorithm 5. As the execution Excurr of the call to DYNAMICHS using DPI is assumed to terminate due to Q = [] and no more nodes can be generated after Q = [] (each generated node is constructed by extending a node in Q), nd must be generated the latest during Excurr.\nSo, let us consider exactly the point in time when nd is generated. Since this point in time might not arise during the execution Excurr of DYNAMICHS, but during some execution Exprev taking place before Excurr which uses some \u201ccurrent\u201d DPI which includes fewer test cases than the current DPIDPI of Excurr, we call the \u201ccurrent\u201d DPI in Exprev in the following DPIprev . That is, DPIprev might be equal to DPI or comprise a subset of the test cases DPI includes.\nFirst, we observe that immediately after nd has been generated, there is some node nd\u2032 \u2208 Q such that nd\u2032 = nd. If nd\u2032 is not the same node as nd, then nd \u2208 Qdup. This follows from lines 20-23.\nSecond, we have that nd\u2032 \u2208 Q cannot be pruned before it is processed. In case nd\u2032 is the same node as nd, this follows from Proposition 12.7 and the precondition that nd is de-facto non-redundant w.r.t. DPI . Notice that in this case nd \u2208 Q cannot even be replaced (also by Proposition 12.7).\nOtherwise, if nd\u2032 is not the same node as nd, we argue as follows: Assume that nd\u2032 is redundant w.r.t. DPIprev and that the PRUNE function is called with arguments Q, Qdup and some minimal conflict set\n220 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nX w.r.t. DPIprev which is a witness of redundancy of nd\u2032. Then, since nd is de-facto non-redundant w.r.t. DPI , since DPIprev includes a subset of the test cases DPI comprises and by Proposition 12.7, nd cannot have been deleted from Qdup during any pruning step. Thence, by Lemma 12.10, nd (or some other node set-equal to nd\u2032 for which X is not a witness of redundancy) must be constructed and added to Q in lines 96-101 during the execution of the PRUNE function.\nThat is, before any node set-equal to nd is processed, any number of calls to PRUNE with arguments Q, Qdup and some minimal conflict set X w.r.t. any DPI DPIprev imply that Q includes some node that is set-equal to nd. Let us denote by node the node set-equal to nd that is finally processed.\nThere must be some execution of DYNAMICHS with some DPI (which might be equal to DPI or include a subset of the test cases in DPI) during which node is processed. This holds as the execution of DYNAMICHS with DPI is assumed to stop because of Q = [], since not all nodes set-equal to node can be pruned, as just argued before, and because the only alternative way, except for pruning, to achieve the deletion of a node from Q (line 7) is to process it. Let DPIprev now be the \u201ccurrent\u201d DPI of the execution of DYNAMICHS during which node is processed. Further, we denote the DPI considered by the immediate subsequent execution of DYNAMICHS by DPIprev+1, and so on.\nWhen node is processed, it is either\n\u2022 (a) labeled by a set (DLABEL returns in line 40, 46 or 34) or\n\u2022 (b) not labeled by a set (DLABEL returns in line 29 or 43).\nCase (b): In this case, DLABEL returns either\n\u2022 (i) nonmin or\n\u2022 (ii) valid.\nCase (i): By Lemma 12.1, node must be a non-minimal diagnosis w.r.t. DPIprev . By line 15, node is then added to the set D\u2283. D\u2283 is never modified throughout Algorithm 5 and is given as an input argument to each subsequent call to DYNAMICHS by line 10 in Algorithm 5. During the execution of some subsequent call to DYNAMICHS using the DPI DPIprev+i for i \u2265 1, the set D\u2283 might be modified by the UPDATETREE function (line 65 and lines 70-78) or in the DLABEL function (line 38) called for DPIprev+i. Because node = nd and nd is de-facto non-redundant w.r.t. DPI , we infer by the same argumentation as used above that node \u2208 D\u2283 cannot be pruned, i.e. node considered as a set cannot be deleted from D\u2283 in line 65 or line 38. The truth of this is supported by Corollary 12.1 and Lemmata 12.6 and 12.7 which say that PRUNE can only be called given some minimal conflict set X w.r.t. DPIprev+i. So, after any number of calls to PRUNE, we have that either node \u2208 D\u2283 or, otherwise, there is some node in D\u2283 which is set-equal to node and which is in a transitive replaces-relation with node. We keep calling this (possibly replacement) node node in the following.\nBy Lemma 12.1, at the time node was processed, there must be some diagnosis D\u2032 w.r.t. DPIprev such that D\u2032 \u2208 Dcalc and node \u2283 D\u2032. Additionally, by Lemma 12.1, the set Dcalc computed during DYNAMICHS for some \u201ccurrent\u201d DPI DPIj comprises only diagnoses w.r.t. DPIj . Now, we have node \u2282 D since nd \u2282 D and node = nd, and D\u2032 \u2282 node. That is, D\u2032 \u2282 D. By the precondition that D is a minimal diagnosis w.r.t. DPI , D\u2032 cannot be a diagnosis w.r.t. DPI . Thus, there cannot be any such D\u2032 in Dcalc computed during DYNAMICHS for DPI .\nAll nodes in Dcalc returned by some call to DYNAMICHS using DPI DPI1 that are no diagnoses w.r.t. DPI2, the extension of DPI1 by a new query added as a positive or negative test case, are added to the set D\u00d7 (and not to DX) in line 22 of Algorithm 5 and are thus no elements of the set DX given as an argument to DYNAMICHS at the next call to DYNAMICHS. The elements of DX given as an argument to\n12.4. ALGORITHM DETAILS AND CORRECTNESS 221\nDYNAMICHS at the next call to DYNAMICHS using DPI2 are definitely added to Q again in lines 79-80 as DX is not modified elsewhere in DYNAMICHS before lines 79-80 are reached.\nTherefore, we need to differentiate between two cases: Either\n\u2022 (x1) D\u2032 \u2208 D\u00d7 never holds for the input argument D\u00d7 to any call to DYNAMICHS or\n\u2022 (x2) D\u2032 \u2208 D\u00d7 holds at least once for the input argument D\u00d7 to some call to DYNAMICHS.\nCase (x1): Since D\u2032 \u2208 Dcalc holds after the execution of DYNAMICHS using DPIprev stops, we have that D\u2032 \u2208 DX must hold for the argument DX given to DYNAMICHS using DPIprev+1. After UPDATETREE returns during DYNAMICHS using DPIprev+1, D\u2032 \u2208 Q holds as argued. Subsequently, D\u2032 might be added again to Dcalc and then to DX again in line 21 of Algorithm 5 and to Q again in line 80 during DYNAMICHS using DPIprev+2, and so forth. But, when a test case is added to some DPI DPIprev+i in Algorithm 5 that invalidates the diagnosisD\u2032 (yielding the DPIDPIprev+i+1),D\u2032 /\u2208 Dcalc is assumed to hold (otherwise it would be an element of D\u00d7 against our assumption). Such a test case must be added sometime as argued above. By Proposition 12.3, D\u2032 cannot be a (minimal) diagnosis w.r.t. any DPI including a superset of the test cases inDPIprev+i+1 either. Notice that the caseD\u2032 /\u2208 Dcalc can emerge in spite of the fact thatD\u2032 is a minimal diagnosis w.r.t. DPIprev+i because there may be minimal diagnoses w.r.t. DPIprev+i that have a higher probability as per pnodes() than D\u2032. For DPIprev+i+1 and all DPIs including more test cases than DPIprev+i+1, D\u2032 cannot be added to Dcalc anymore due to Lemma 12.1 since only diagnoses w.r.t. the currently used DPI can be added to Dcalc.\nCase (x2): Here, D\u2032 \u2208 D\u00d7 holds at least once for the input argument D\u00d7 to some call to DYNAMICHS using the DPI DPIprev+i. Then, DYNAMICHS using the DPI DPIprev+i\u22121 must have returned a set Dcalc including D\u2032 as otherwise D\u2032 cannot be added to D\u00d7. Hence, D\u2032 must be a diagnosis w.r.t. DPIprev+i\u22121 by Lemma 12.1. Since D\u2032 is added to D\u00d7, it cannot be a diagnosis w.r.t. DPIprev+i. This must hold\n\u2022 by Remark 7.4,\n\u2022 since the set added to D\u00d7 in Algorithm 5 is exactly the set Dout returned by GETINVALIDDIAGS in line 19 of Algorithm 5 and\n\u2022 Dout = D+(Q) in case the user answer u(Q) to the query Q w.r.t. Dcalc and DPIprev+i\u22121 is false and Dout = D\u2212(Q) otherwise (notice that Dcalc is called DX in Algorithm 5).\nSo, by Proposition 12.3, D\u2032 cannot be a (minimal) diagnosis w.r.t. any DPI including more test cases than DPIprev+i either.\nEach element in D\u00d7 is processed by the UPDATETREE function (lines 48-69) called for the DPI DPIprev+i. In lines 48-69, each node ndx in D\u00d7 can only be pruned or either ndx or a node in a transitive replaces-relation with ndx is added to Q in line 68. Dcalc is not modified by UPDATETREE and Dcalc = \u2205 holds at the beginning of the execution of each call to DYNAMICHS. (A node set-equal to) D\u2032 cannot ever be readded to Dcalc by Lemma 12.1 and since D\u2032 is not a diagnosis w.r.t any DPI including more test cases than DPIprev+i. Hence, D\u2032 \u2208 Dcalc can never hold for any DPI including more test cases than DPIprev+i.\nHence, there must be some DPI DPIprev+k such that DX given as input to the DYNAMICHS-call for DPIprev+k does not include any diagnosis D\u2032 \u2282 node. So, during the execution of the call to DYNAMICHS using DPIDPIprev+k, node must be deleted from D\u2283 and be reinserted into Q by lines 70- 78 in UPDATETREE which is called at the beginning of the execution of DYNAMICHS at any call to DYNAMICHS. This must hold since all nodes ndx in D\u2283 that have not yet been pruned and for which there is no diagnosis in DX which is a proper subset of ndx, are added to Q throughout lines 70-78.\n222 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nAs shown, both criteria are met for node during the execution of the call to DYNAMICHS using DPI DPIprev+k.\nCase (ii): By Lemma 12.1, we know that node is a diagnosis w.r.t. DPIprev and that node is added to Dcalc. Since node \u2282 D and D is a minimal diagnosis w.r.t. DPI , we obtain, by the same argumentation as in (i), that there must be some DPI DPIprev+k such that DX given as input to the DYNAMICHS-call for DPIprev+k does not include node.\nIf node /\u2208 D\u00d7, then it cannot ever be added to Dcalc again, as argued in case (i). Otherwise, during the execution of UPDATETREE which is called at the beginning of the execution of each call to DYNAMICHS, D\u00d7 is modified in lines 48-69.\nNow, we differentiate between two cases, namely node is either\n\u2022 (\u00acr) non-redundant w.r.t. DPI or\n\u2022 (r) redundant w.r.t. DPI .\nCase (\u00acr): Due to the non-redundancy of node w.r.t. DPI , Lemma 12.4, Lemma 12.10 and Corollary 12.1, node cannot be replaced or pruned throughout lines 48-66. Thus, node is reinserted into Q in line 68.\nCase (r): Since node is redundant w.r.t. DPI , it may or may not be redundant w.r.t.DPIprev+k+1. So, during the UPDATETREE function called in DYNAMICHS for DPIprev+k+1, there may or may not be some call to PRUNE given some X as argument which is a witness of redundancy of node. In the latter case, node will not be replaced or pruned during any PRUNE execution and will be reinserted into Q in line 68. In the former case, node might be replaced, but it cannot be pruned due to the same reasoning as given above in case (i). So, either node or some node in a transitive replaces-relation with node must be in D\u00d7 at the time line 67 is reached. This node is then added to Q in line 68.\nNow, both cases (i) and (ii) identified for case (b) lead to the reinsertion of node or some node setequal to node into Q. Notice that this node has the same properties as node before one of the cases (i) or (ii) emerged. That is, if PRUNE is called given a witness of redundancy of node, then a replacement node of node is found. And, if only one replacement node of node is found, this replacement node is de-facto non-redundant w.r.t. DPI .\nIf node is the same node as nd, this holds since there cannot be a witness of redundancy of nd due to the de-facto non-redundancy of nd w.r.t. DPI and Proposition 12.7. Otherwise, this holds by Lemma 12.10 and since node = nd and nd \u2208 Qdup must hold due the de-facto non-redundancy of nd w.r.t. DPI and Proposition 12.7. So, we call this reinserted node again node.\nFurthermore, node can be neither labeled by valid nor by nonmin during the execution of DYNAMICHS for DPI . This holds by Lemma 12.1 and since node can be neither a diagnosis nor a nonminimal diagnosis w.r.t.DPI due to node \u2282 D and the fact thatD is a minimal diagnosis w.r.t.DPI . As a consequence of this and the assumption that the DYNAMICHS-call for DPI terminates due to Q = [], case (a) must arise at some point in time for node during some execution of DYNAMICHS for some (previous) DPI not-necessarily equal to DPI .\nCase (a): In this case, by Lemma 12.2, DLABEL returns a minimal conflict set L w.r.t. DPIprev as a label for node where L has the property that L \u2229 node = \u2205.\nIt must hold that L 6= \u2205. Otherwise, by Proposition 4.2, either\n\u2022 (v1) K is valid w.r.t. \u3008\u00b7,B,Pprev,Nprev\u3009R where DPIprev = \u3008K,B,Pprev,Nprev\u3009R or\n\u2022 (v2) DPIprev is non-admissible.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 223\nIn the former case (v1), we know by Corollary 3.3 that the only (minimal) diagnosis w.r.t. DPIprev is \u2205. If DPIprev is equal to DPI , this is a contradiction to the existence of some minimal diagnosis w.r.t. DPI , namely D, which is not the empty set. D \u2283 \u2205 must hold since, by precondition, there is a node nd such that nd \u2282 D and since \u2205 \u2286 nd.\nOtherwise, if DPIprev includes a proper subset of the test cases DPI includes, DPI can never be a current DPI during any execution of DYNAMICHS during the same execution of Algorithm 5 during which there is an execution of DYNAMICHS using DPIprev as a current DPI. This holds as there must be at least two diagnoses in DX (which is the set Dcalc returned by DYNAMICHS for DPIprev) in line 13 of Algorithm 5 in order for DYNAMICHS to be called again with an extended DPI. For, in case there is only one diagnosis, i.e. \u2205, then the probability of this diagnosis is 1 which is greater or equal 1 \u2212 \u03c3 for any choice of \u03c3 due to \u03c3 \u2265 0. Consequently, Algorithm 5 would return in line 14. This is a contradiction to the assumption that there is an execution of DYNAMICHS using DPI as a current DPI.\nIn the latter case (v2), we can infer by Corollary 7.3, which states that adding queries as test cases to an admissible DPI can never yield a non-admissible DPI, that the DPI given as an input to Algorithm 5 must be non-admissible, contradiction.\nThence,L 6= \u2205 and DYNAMICHS will execute lines 17-23 and generate one node nodee := ADD(node, e) with nodee.cs := ADD(node.cs, L) for each e \u2208 L (cf. Definition 12.2 for an explanation of the function ADD).\nNow, we have that there must be some non-empty active sublabel of L = nodee.cs[r] w.r.t. DPI where r := |nodee| by Definition 12.6. This holds by the following argumentation:\nThe first observation is that nodee.cs[r] cannot be reduced twice during one and the same execution of DYNAMICHS using one and the same DPI DPIprev+j which results from DPIprev by addition of test cases. For, by Corollaries 12.1 and 12.2 and Lemmata 12.6 and 12.7, PRUNE as well as PRUNEQDUP can only be called given some minimal conflict set X w.r.t. DPIprev+j . By Lemmata 12.10 and 12.8, all nodes ndx that are in the set returned by PRUNE and PRUNEQDUP, respectively, have the property that there are no proper supersets of X in ndx.cs. Moreover, there are no proper subsets of X in ndx.cs. Because each ndx.cs[m] for m \u2208 {1, . . . , |ndx.cs|} must be a minimal conflict set w.r.t. some DPI equal to DPIprev+j or including a subset of the test cases in DPIprev+j . Otherwise, ndx could not be a node during the execution of DYNAMICHS where DPIprev+j is the current DPI. By Proposition 12.1, there cannot be any m \u2208 {1, . . . , |ndx.cs|} such that ndx.cs[m] \u2282 X as X is a minimal conflict set w.r.t. DPIprev+j . As two minimal conflict sets w.r.t. DPIprev+j can never be in a proper subset-relationship with one another, L = nodee.cs[r] can be modified at most once by PRUNE or PRUNEQDUP for the DPI DPIprev+j .\nSecond, by Proposition 12.1, each minimal conflict set w.r.t. DPIprev is a conflict set w.r.t. any DPI DPIprev+j that results from DPIprev by addition of test cases, that is, in particular, w.r.t. DPI . So, there must be some minimal conflict set Cj w.r.t. each DPIprev+j such that Cj \u2286 L and there cannot be any minimal conflict set w.r.t. DPIprev+j that is a proper superset of L.\nThird, we have that L 6= \u2205, L is a minimal conflict set w.r.t. DPIprev , and DPIprev+j includes a superset of the test cases in DPIprev . Thus, by Proposition 12.2, each minimal conflict set w.r.t. DPIprev+j must be non-empty. In particular, this implies that all minimal conflict sets w.r.t. DPI that are subsets of L must be non-empty.\nBy these three observations, the criteria of Definition 12.6 can be applied to analyze the active subnode of nodee.cs[r] w.r.t. DPI . That is, if C1, . . . , Cn is the (arbitrary actual) chronological sequence of all sets X given as an argument to PRUNE and PRUNEQDUP during all executions of DYNAMICHS from the one with current DPI DPIprev up to and including the one with current DPI DPI where\n\u2022 nodee.cs[r] \u2283 C1,\n\u2022 each Ci is a minimal conflict set w.r.t. DPIi for i \u2208 {1, . . . , n}\n\u2022 Ck \u2283 Ck+1 for k \u2208 {1, . . . , n\u2212 1},\n224 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\n\u2022 DPIj includes a proper subset of the test cases DPIj+1 includes for j \u2208 {1, . . . , n\u2212 1},\n\u2022 DPIn is equal to DPI or includes a proper subset of the test cases DPI includes and\n\u2022 DPIprev includes a proper subset of the test cases DPI1 includes,\nthen Cn is the active sublabel of nodee.cs[r] w.r.t. DPI . However, as argued before, the minimal conflict set Cn w.r.t. DPIn cannot be the empty set. As a consequence, we obtain that there must be a non-empty active sublabel of nodee.cs[r] w.r.t. DPI .\nBy Propositions 12.1 and 12.2, there is a non-empty minimal conflict set C\u2032 w.r.t. DPI such that C\u2032 \u2286 Cn. Due to Cn \u2282 \u00b7 \u00b7 \u00b7 \u2282 C1 \u2282 nodee.cs[r] = L we conclude that Cn \u2282 L. Therefore, \u2205 \u2282 C\u2032 \u2282 L holds.\nBy Proposition 4.6, each minimal diagnosis w.r.t. DPI is a minimal hitting set of all minimal conflict sets w.r.t. DPI . Thence, we have that C\u2032 \u2229 D 6= \u2205. So, by C\u2032 \u2282 L, we have that \u2205 \u2282 C\u2032 \u2229 D \u2286 L \u2229 D \u2286 L. Consequently, we define ndsuc := nodex = ADD(node, x) with ndsuc.cs := nodex.cs = ADD(node.cs, L) for some x \u2208 C\u2032 \u2229 D \u2286 L. Then, ndsuc \u2286 D because node \u2282 D and x \u2208 D. It is clear from the inference so far that nd \u2282 ndsuc, |ndsuc| = |nd| + 1 and ndsuc \u2208 GenNodes. This shows the truth of propositions (1)-(3).\nProposition (4) must hold by lines 20-23. Now we argue why propositions (5) and (6) must hold. Assume that nd\u2032suc \u2208 Q is redundant w.r.t. some DPI DPI \u2032\u2032prev which is equal to DPI or includes a subset of the test cases in DPI . Then, there must be some minimal conflict set C\u2032\u2032 w.r.t.DPI \u2032\u2032prev which is a witness of redundancy of nd \u2032 suc. Suppose that PRUNE is called given X := C\u2032\u2032 as an argument. Now, we have to distinguish two cases: Either\n\u2022 (q1) ndsuc was added to Q after it was generated or\n\u2022 (q2) ndsuc was added to Qdup after it was generated\n(there are no other possibilities, see lines 17-23). For each of these two cases, there are two more cases to discriminate between:\n\u2022 (c1) C\u2032\u2032 \u2282 nd\u2032suc.cs[|nd \u2032 suc|] and nd \u2032 suc[|nd \u2032 suc|] \u2208 nd \u2032 suc.cs[|nd \u2032 suc|] \\ C\u2032\u2032 or\n\u2022 (c2) C\u2032\u2032 \u2282 nd\u2032suc.cs[j] and nd \u2032 suc[j] \u2208 nd \u2032 suc.cs[j] \\ C\u2032\u2032 for some j \u2208 { 1, . . . , |nd\u2032suc| \u2212 1 } .\nCase (q1): Here, we have that nd\u2032suc is the same node as ndsuc since ndsuc was added to Q after generation and no node replacement can have taken place because nd\u2032suc is defined as the node setequal to ndsuc that is an element of Q immediately after ndsuc has been generated. And, only one node corresponding to one and the same set can be in Q at the same time.\nCase (c1): We have that C\u2032\u2032 must be equal to some minimal conflict set Cj in the sequence C1, . . . , Cn. This must be truesince, first, DPI \u2032\u2032prev is equal to DPI or includes a subset of the test cases in DPI and DPIprev includes a proper subset of the test cases in DPI \u2032\u2032prev .\nTo understand why the latter must hold, recall that DPIprev is the DPI of the call to DYNAMICHS where ndsuc was generated and the minimal conflict set L was computed. By assumption, however, there is some minimal conflict set w.r.t. DPI \u2032\u2032prev , namely C\u2032\u2032, such that C\u2032\u2032 \u2282 nd \u2032 suc.cs[|nd \u2032 suc|] = L. Hence, it cannot be truethat both L and C\u2032\u2032 are minimal conflict sets w.r.t. the same DPI. Otherwise, we would have a contradiction to the minimality of L. By Proposition 12.1, which states that minimal conflict sets cannot grow by the addition of new test cases to the DPI, we obtain the claimed fact that DPIprev includes a proper subset of the test cases in DPI \u2032\u2032prev .\n12.4. ALGORITHM DETAILS AND CORRECTNESS 225\nSecond, the sequence C1, . . . , Cn comprises all setsX given as an argument to PRUNE and PRUNEQDUP during all executions of DYNAMICHS from the one with current DPI DPIprev up to and including the one with current DPI DPI where nd\u2032suc.cs[|nd \u2032 suc|] \u2283 C1 \u2283 \u00b7 \u00b7 \u00b7 \u2283 Cn holds. Reason for this to be valid is the fact that nd\u2032suc is the same node as ndsuc in the currently considered case (q1). Now, recall that C\u2032 is a minimal conflict set w.r.t. DPI such that x \u2208 C\u2032 \u2229 D \u2282 L. Further, by nd\u2032suc = nodex, we have that nd \u2032 suc[|nd \u2032 suc|] = x. Due to C\u2032 \u2286 Cn and Cn \u2286 Cj , we have that C\u2032 \u2286 Cj . Therefore, we can infer by C\u2032\u2032 = Cj that C\u2032 \u2286 C\u2032\u2032 is true . Now, x \u2208 C\u2032 implies that x \u2208 C\u2032\u2032 wherefore x /\u2208 nd\u2032suc.cs[|nd \u2032 suc|] \\ C\u2032\u2032. By x = nd \u2032 suc[|nd \u2032 suc|], this is a contradiction to the assumption of case (c1). Hence, case (c2) must arise.\nCase (c2): We have that nd\u2032suc[1..|nd \u2032 suc| \u2212 1] is the same node as node since nd \u2032 suc = nodex. Then, there are two cases: Either\n\u2022 (s1) node is the same node as nd or\n\u2022 (s2) node is not the same node as nd.\nCase (s1): If node is the same node as nd, then node is de-facto non-redundant w.r.t. DPI since nd is de-facto non-redundant w.r.t. DPI by precondition. Moreover, x is an element of the active sublabel of nd\u2032suc[|nd \u2032 suc|] w.r.t. DPI , as specified before. Thus, by Definition 12.7, nd \u2032 suc is de-facto non-redundant w.r.t. DPI . Hence, PRUNE cannot be given an argument C\u2032\u2032 which is a witness of redundancy of nd\u2032suc where C\u2032\u2032 is a minimal conflict set w.r.t. DPI \u2032\u2032prev . This holds due to\n\u2022 the fact that DPI \u2032\u2032prev comprises a (not necessarily proper) subset of the test cases in DPI ,\n\u2022 Proposition 12.7 which states that a de-facto non-redundant node w.r.t. DPI cannot be pruned or replaced during any execution of DYNAMICHS with a current DPI that includes a (not necessarily proper) subset of the test cases in DPI and\n\u2022 Lemma 12.10 which says that nd\u2032suc would be replaced or pruned in case that PRUNE is called given a witness of redundancy of nd\u2032suc.\nSo, we have derived a contradiction to the assumption that PRUNE is called given a minimal conflict set X := C\u2032\u2032 w.r.t. DPI \u2032\u2032prev which is a witness of redundancy of nd \u2032 suc. Hence, case (s2) must be true .\nCase (s2): If node is not the same node as nd, then node may or may not be de-facto non-redundant w.r.t. DPI . In the former case, the same argumentation as in case (s1) applies and yields a contradiction. In the latter case, we know that C\u2032\u2032 \u2282 nd\u2032suc.cs[j] as well as nd \u2032 suc[j] \u2208 nd \u2032 suc.cs[j] \\ C\u2032\u2032 must be truefor\nsome j \u2208 { 1, . . . , |nd\u2032suc| \u2212 1 } . So, by Lemma 12.10, nd\u2032suc is not an element of the returned list Q \u2032 of the call to PRUNE given the arguments Q (which includes nd\u2032suc), X := C\u2032\u2032 and Qdup. However, at least one replacement node of nd\u2032suc must be found by PRUNE. This must hold by the following reasoning: First, nd \u2208 Qdup must hold at the time this call to PRUNE is made. This is satisfied since\n\u2022 the entire (current) list Qdup is browsed for an alternative subnode of nd\u2032suc,\n\u2022 nd \u2208 Qdup holds at some point in time during the execution of DYNAMICHS with the current DPI DPIprev due to the fact that node is not the same node as nd and the argumentation at the beginning of this proof,\n\u2022 DPIprev includes a subset of the test cases in DPI \u2032\u2032prev ,\n226 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\n\u2022 DPI \u2032\u2032prev includes a subset of the test cases in DPI ,\n\u2022 Proposition 12.7 states that a de-facto non-redundant node w.r.t.DPI cannot be pruned or replaced during the execution of DYNAMICHS with a current DPI that includes a subset of the test cases in DPI ,\n\u2022 nodes can only be deleted from Qdup by being pruned and\n\u2022 nd is de-facto non-redundant w.r.t. DPI .\nSecond, by line 21 and PRUNEQDUP, which are the only places in DYNAMICHS where Qdup is modified, Qdup is sorted in ascending order by node cardinality at any time during the execution of any call to DYNAMICHS.\nThird, in order to construct a replacement node of nd\u2032suc, PRUNE first determines the maximal k such that C\u2032\u2032 \u2282 nd\u2032suc.cs[k] and nd \u2032 suc[k] \u2208 nd \u2032 suc.cs[k] \\ C\u2032\u2032. As case (c1) was proven to be false, we conclude that k \u2264 |nd\u2032suc| \u2212 1 must hold. Then, in line 96, an alternative subnode of nd \u2032 suc\n\u2022 which has cardinality k + z where z \u2265 0 is minimal and\n\u2022 from which a replacement node of nd\u2032suc can be constructed\nis searched for in Qdup. To see this, observe that elements in Qdup \u2013 which is sorted in ascending order of node cardinality, as argued \u2013 are visited in order starting from the lowest cardinality node (line 96).\nFourth, nd \u2208 Qdup is an alternative equal node of node. Since nd\u2032suc = nodex, we have that nd is an alternative subnode of nd\u2032suc such that k \u2264 |nd \u2032 suc| \u2212 1 = |nd|.\nThus, we have that one replacement node of nd\u2032suc is definitely found by PRUNE. And, in case there is only one replacement node of nd\u2032suc constructable during PRUNE, then this replacement node is given by nd\u2032suc,new := ADD(nd, x) with nd \u2032 suc,new.cs := ADD(nd.cs, L). By the de-facto non-redundancy of nd and since x is specified as an element of the active sublabel of nd\u2032suc.cs[|nd \u2032 suc|] w.r.t. DPI (see above), we obtain by Definition 12.7 that nd\u2032suc,new is a de-facto non-redundant node w.r.t.DPI . Thence, proposition (5) is true .\nDue to |nd| = |node| = |nd\u2032suc| \u2212 1, the alternative subnode of nd \u2032 suc actually found by PRUNE cannot have a cardinality greater than |nd\u2032suc| \u2212 1. So, let ndalt be the found alternative subnode of nd\u2032suc. Since |ndalt| \u2264 |nd \u2032 suc|\u22121, we obtain that the replacement node nd \u2032 suc,new,1 of nd \u2032 suc constructed from ndalt must meet nd\u2032suc,new,1[|nd \u2032 suc|] = nd \u2032 suc[|nd \u2032 suc|] = x as well as nd \u2032 suc,new,1.cs[|nd \u2032 suc|] = nd\u2032suc.cs[|nd \u2032 suc|] = L. That is, the first |nd| = |node| = |nd \u2032 suc| \u2212 1 positions of nd \u2032 suc,new,1 as a set correspond to a node in a transitive replaces-relation with nd. Therefore, the same line of argument as used for nd\u2032suc can be applied to any node nd \u2032 suc,rep in a transitive replaces-relation with nd\u2032suc. That is, the following must be valid for any node nd \u2032 suc,rep in a transitive replaces-relation with nd\u2032suc:\n\u2022 nd\u2032suc,rep[|nd \u2032 suc|] = x and nd \u2032 suc,rep.cs[|nd \u2032 suc|] = L.\n\u2022 If PRUNE is called given a witness of redundancy of nd\u2032suc,rep, then some replacement node of nd\u2032suc,rep is found. And, if only one replacement node of nd \u2032 suc,rep is constructable, then this\nreplacement node is de-facto non-redundant w.r.t. DPI .\nAfter once a replacement node of nd\u2032suc or of some node in a transitive replaces-relation with nd \u2032 suc is found which is de-facto non-redundant w.r.t. DPI , this replacement node cannot be replaced or pruned by Proposition 12.7. Therefore, by Lemma 12.10, no witness of redundancy of this replacement node can exist w.r.t. any DPI including a (not necessarily proper) subset of the test cases in DPI . Thence, proposition (6) is true .\n12.4. ALGORITHM DETAILS AND CORRECTNESS 227\nCase (q2): Here, we have that nd\u2032suc is not the same node as ndsuc. This must be valid as nd \u2032 suc is defined as the node set-equal to ndsuc that is an element of Q immediately after ndsuc was generated and ndsuc is assumed to be added to Qdup after being generated.\nNow, independently of whether (c1) or (c2) occurs, the following holds: If PRUNE is called given a witness of redundancy C\u2032\u2032 of nd\u2032suc w.r.t. DPI \u2032\u2032prev , then a replacement node of nd \u2032 suc is found. And, if only one replacement node of nd\u2032suc is constructable, then this replacement node is de-facto nonredundant w.r.t. DPI .\nTo understand why this must hold, first recall that ndsuc is a successor of node, i.e. ndsuc[1..|ndsuc|\u2212 1] is the same node as node. Furthermore, node is the node set-equal to nd that is processed. That is, node is either the same node as nd or it is in a transitive replaces-relation with nd. Then, the same two cases (s1) and (s2) can be distinguished as in case (q1)(c2) where (s1) leads to a contradiction. So, case (s2) must be true . That is, node is not the same node as nd. Hence, by the argumentation in case (q1)(c2)(s2), nd \u2208 Qdup must hold during the execution of any call to DYNAMICHS with a current DPI that comprises a (not necessarily proper) superset of the test cases in DPIprev \u2013 which is the current DPI at the time nd is generated \u2013 and a (not necessarily proper) subset of the test cases in DPI . In particular, this implies that nd \u2208 Qdup at the time PRUNE is called given the witness of redundancy C\u2032\u2032 of nd\u2032suc w.r.t. DPI \u2032\u2032prev as an argument.\nBy assumption, ndsuc has been added to Qdup after being generated. Now, suppose PRUNEQDUP is called given a witness of redundancy C\u2032 of ndsuc \u2208 Qdup w.r.t. some DPIDPI \u2032prev as an argument. Then DPI \u2032prev must comprise a (not necessarily proper) superset of the test cases in DPIprev . This can be concluded from Lemma 12.12 which implies that ndsuc cannot have been generated during an execution of DYNAMICHS with a current DPI including a proper subset of the test cases in DPIprev . Hence, the argumentation before implicates that nd \u2208 Qdup at the time PRUNEQDUP is called given the witness of redundancy C\u2032 of ndsuc w.r.t. DPI \u2032prev as an argument.\nThus, ndsuc cannot be pruned on account of Lemma 12.8 which says that a node can only be pruned from Qdup if the set Combndsuc(Qdup) of combined equal nodes of ndsuc of Qdup (cf. Definition 12.5) is the empty set.\nHowever, Combndsuc(Qdup) 6= \u2205 must be valid. Because we demonstrated that\n\u2022 nd \u2208 Qdup,\n\u2022 ndsuc \u2208 Qdup,\n\u2022 ndsuc is the same node as nodex = ADD(node, x) with ndsuc.cs being equal to nodex.cs = ADD(node.cs, L),\n\u2022 nd = node and\n\u2022 x is specified as an element of the active sublabel of ndsuc.cs[|ndsuc|] w.r.t. DPI (see above) wherefore x /\u2208 ndsuc.cs[|ndsuc|] \\ C\u2032.\nTherefore,\nndcomb := ADD(nd, ndsuc[|nd|+ 1..|ndsuc|]) = ADD(nd, x) ndcomb.cs := ADD(nd.cs, ndsuc.cs[|nd|+ 1..|ndsuc|]) = ADD(nd.cs, L)\nis a combined equal node of ndsuc of Qdup, i.e. ndcomb \u2208 Combndsuc(Qdup). The node ndcomb is defacto non-redundant w.r.t. DPI as nd is de-facto non-redundant w.r.t. DPI and since x is an element of the active sublabel of ndsuc.cs[|ndsuc|] w.r.t. DPI .\nBy Definition 12.5, any combined equal node of ndsuc must share the element at the |ndsuc|-th position with ndsuc and ndsuc.cs, respectively. Hence, the first |ndsuc|\u22121 elements of a combined equal node\n228 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nof ndsuc are set-equal to the first |ndsuc| \u2212 1 elements of ndsuc. So, there exists a combined equal node, namely ndcomb, of any (redundant) node that results from ndsuc by a set of combined replacements.\nBy Lemma 12.11, the fact that ndcomb \u2208 Combndsuc(Qdup) \u2286 Comb(Qdup) at some point in time during the execution of DYNAMICHS with current DPI DPI \u2032prev and the de-facto non-redundancy of ndcomb w.r.t. DPI , we conclude that, during any execution of DYNAMICHS with a current DPI that includes a (not necessarily proper) superset of the test cases in DPI \u2032prev and includes a (not necessarily proper) subset of the test cases in DPI , ndcomb \u2208 Comb(Qdup) must hold. Because DPI \u2032prev is an arbitrary DPI that comprises a (not necessarily proper) superset of the test cases in DPIprev , we derive that ndcomb \u2208 Comb(Qdup) must be trueparticularly during the execution of DYNAMICHS with the current DPI DPI \u2032\u2032prev .\nIf C\u2032\u2032 is a witness of redundancy of ndsuc \u2208 Qdup, then the updated list Qdup returned by PRUNEQDUP must include a combined replacement node of ndsuc, either ndcomb or some other node. Otherwise, i.e. if C\u2032\u2032 is not a witness of redundancy of ndsuc \u2208 Qdup, the updated list Qdup returned by PRUNEQDUP must include ndsuc.\nPRUNE is always called immediately after PRUNEQDUP and thus uses the updated list Qdup which comprises a node set-equal to ndsuc and thus set-equal to nd\u2032suc. Consequently, we have that one replacement node of nd\u2032suc is definitely found by PRUNE. And, in case there is only one replacement node of nd\u2032suc constructable during PRUNE, this replacement node is given by ndcomb. Thence, proposition (5) is true .\nIndependently of which replacement node of nd\u2032suc is actually found by PRUNE, a set-equality between this replacement node and ndcomb will hold. This is truesince ndcomb = nd\u2032suc and since each replacement node, by definition, is set-equal to the node it replaces. Consequently, this set-equality holds for any node in a transitive replaces-relation with nd\u2032suc. So, we have that one replacement node of any node nd\u2032suc,rep in a transitive replaces-relation with nd \u2032 suc is definitely found by PRUNE. And, in case there is only one replacement node of nd\u2032suc,rep constructable during PRUNE, this replacement node is given by ndcomb which is de-facto non-redundant w.r.t. DPI .\nThat ndcomb, after it has been used as a replacement node of nd\u2032suc or of some node in a transitive replaces-relation with nd\u2032suc, cannot be pruned or replaced, follows from Proposition 12.7 and the fact that ndcomb is de-facto non-redundant w.r.t.DPI . Therefore, by Lemma 12.10, no witness of redundancy of ndcomb can exist w.r.t. any DPI including a (not necessarily proper) subset of the test cases in DPI . Thence, proposition (6) is true .\nThe next result, Lemma 12.14, assumes an arbitrary fixed \u201ccurrent\u201d DPIDPI such that DYNAMICHS with this \u201ccurrent\u201d DPIDPI returns due to Q = []. Further on, it assumes an arbitrary minimal diagnosis D w.r.t. DPI and a node nd which is a proper subset of D such that nd is an element of Q anytime throughout all executions of DYNAMICHS during the execution of Algorithm 5 up to the one with the current DPI DPI . Additionally, nd cannot be pruned. It might be replaced; and in case there is only one potential replacement node of nd constructable from (the combined nodes of) Qdup, this replacement node is de-facto non-redundant w.r.t. DPI . Any node nd\u2032 in a transitive replaces relation with nd cannot be pruned either. It might again be replaced. In case there is only one potential replacement node of nd\u2032 constructable from (the combined nodes of) Qdup, this replacement node is de-facto non-redundant w.r.t. DPI .\nGiven these preconditions, the lemma establishes the existence of a node ndsuc that corresponds to a superset of nd and to a subset of D, includes one element more than the set nd and is generated anytime throughout all executions of DYNAMICHS during the execution of Algorithm 5 up to the one with the current DPI DPI . Moreover, it states that the node nd\u2032suc set-equal to this generated node that is an element of Q cannot be pruned. However, it might be replaced. In case there is only one potential replacement node of nd\u2032suc constructable from (the combined nodes of) Qdup, this replacement node is de-facto non-redundant w.r.t. DPI . Any node nd\u2032suc,rep in a transitive replaces relation with nd \u2032 suc\n12.4. ALGORITHM DETAILS AND CORRECTNESS 229\ncannot be pruned either. It might again be replaced. In case there is only one potential replacement node of nd\u2032suc,rep constructable from (the combined nodes of) Qdup, this replacement node is de-facto non-redundant w.r.t. DPI .\nPictured, with respect to the hitting set tree constructed by DYNAMICHS, this lemma purports the following: Let the hitting set tree produced by DYNAMICHS be completely constructed for an arbitrary DPI DPI . In case there is any tree branch whose edge labels correspond to a part of the minimal diagnosisD w.r.t. DPI and which is known to be definitely not pruned during this tree construction, then this branch must be extended by one edge labeled by an element of D and this extended path is known to be definitely not pruned during this tree construction.\nLemma 12.14. Assume the execution of DYNAMICHS with the current DPI DPI and assume that the execution stops due to Q = []. Let\n\u2022 GenNodes be the set of all nodes generated throughout the execution of all calls to DYNAMICHS during the execution of Algorithm 5,\n\u2022 D be some minimal diagnosis w.r.t. DPI ,\n\u2022 DPI \u2032prev be a DPI which is either equal to DPI or includes fewer test cases than DPI and which is the current DPI during any particular call to DYNAMICHS,\n\u2022 nd be some node such that the following holds:\n\u2013 nd \u2282 D. \u2013 There is some execution of DYNAMICHS with current DPI DPI \u2032prev during which it holds at\nsome point in time that nd \u2208 Q. \u2013 If PRUNE is called given a witness of redundancy of nd, then some replacement node of nd\nis found. If only one replacement node of nd is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\n\u2013 Let nd\u2032 be in a transitive replaces-relation with nd. If PRUNE is called given a witness of redundancy of nd\u2032, then some replacement node of nd\u2032 is found. If only one replacement node of nd\u2032 is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\nThen there are nodes ndsuc and nd \u2032 suc such that the following holds:\n(1) nd \u2282 ndsuc \u2286 D.\n(2) |ndsuc| = |nd|+ 1.\n(3) ndsuc \u2208 GenNodes.\n(4) nd\u2032suc = ndsuc is an element of Q immediately after ndsuc has been generated.\n(5) If PRUNE is called given a witness of redundancy of nd\u2032suc, then some replacement node of nd \u2032 suc\nis found. If only one replacement node of nd\u2032suc is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\n(6) Let nd\u2032suc,rep be in a transitive replaces-relation with nd \u2032 suc. If PRUNE is called given a witness of\nredundancy of nd\u2032suc,rep, then some replacement node of nd \u2032 suc,rep is found. If only one replacement node of nd\u2032suc,rep is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\n230 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nProof. Since nd \u2208 Q holds at some point in time during the execution of some call to DYNAMICHS with current DPI DPI \u2032prev and since the execution of DYNAMICHS with DPI terminates due to Q = [], we have that some node set-equal to nd must be processed. This must be satisfied because nodes can only be deleted from Q in that they are processed or pruned, and nd cannot be pruned from Q. For, by precondition, if PRUNE is called given a witness of redundancy of nd, then a replacement node of nd is found. And, if only one replacement node ndrep of nd is found, ndrep is de-facto non-redundant w.r.t. DPI .\nNow, let nd1 be a replacement node of nd found by PRUNE called with some witness of redundancy of nd. Then, by precondition, what holds for nd also holds for nd1. That is, if PRUNE is called given a witness of redundancy of nd1, then a replacement node of nd1 is found. And, if only one replacement node nd1,rep of nd1 is found, nd1,rep is de-facto non-redundant w.r.t. DPI .\nThe same holds for any ndi which is in a transitive replaces-relation with nd. So, anytime PRUNE is called for a node set-equal to nd, at least one replacement node is found by PRUNE. And, in case ndi is de-facto non-redundant w.r.t. DPI \u2013 which must be the case sooner or later for some node in a transitive replaces-relation with nd, by the given preconditions \u2013 then, by Proposition 12.7, ndi cannot be pruned or replaced.\nHence, let us denote by node the node set-equal to nd that is finally processed. Let DPIprev now be the \u201ccurrent\u201d DPI of the execution of DYNAMICHS during which node is processed. Further, we denote the DPI of the immediate subsequent execution of DYNAMICHS by DPIprev+1, and so on.\nSince node is processed, it is either\n\u2022 (s) labeled by a set (DLABEL returns in line 40, 46 or 34) or\n\u2022 (\u00acs) not labeled by a set (DLABEL returns in line 29 or 43).\nCase (\u00acs): In this case, DLABEL returns\n\u2022 (i) nonmin or\n\u2022 (ii) valid.\nCase (i): By Lemma 12.1, node must be a non-minimal diagnosis w.r.t. DPIprev. By line 15, node is then added to the set D\u2283. D\u2283 is never modified throughout Algorithm 5 and is given as an input argument to each subsequent call to DYNAMICHS by line 10 in Algorithm 5. During the execution of some subsequent call to DYNAMICHS using the DPI DPIprev+i for i \u2265 1, the set D\u2283 might be modified by the PRUNE function called during UPDATETREE (line 65 and lines 70-78) or during DLABEL (line 38).\nRecall that node is either the same node as nd or in a transitive replaces-relation with nd. Hence, by the argumentation given before, we have that, if PRUNE is called given a witness of redundancy of node, then there is a replacement node of node found by PRUNE. And, if there is only one replacement node of node found by PRUNE, then this replacement node is de-facto non-redundant w.r.t. DPI .\nTherefore, node \u2208 D\u2283 cannot be pruned, i.e. node considered as a set cannot be deleted from D\u2283 in line 65 or line 38. So, after any number of calls to PRUNE, we have that either node \u2208 D\u2283 or, otherwise, there is some node in D\u2283 which is set-equal to node and which is in a transitive replaces-relation with node. We keep calling this (possibly replacement) node node in the following.\nBy Lemma 12.1, at the time node was processed, there must be some diagnosis D\u2032 w.r.t. DPIprev such that D\u2032 \u2208 Dcalc and node \u2283 D\u2032. Additionally, by Lemma 12.1, the set Dcalc computed during DYNAMICHS for some \u201ccurrent\u201d DPI DPIj comprises only diagnoses w.r.t. DPIj . Now, we have node \u2282 D since nd \u2282 D and node = nd, and D\u2032 \u2282 node. That is, D\u2032 \u2282 D. By the precondition that D is a minimal diagnosis w.r.t. DPI , D\u2032 cannot be a diagnosis w.r.t. DPI . Thus, there cannot be any such D\u2032 in Dcalc computed during DYNAMICHS for DPI .\n12.4. ALGORITHM DETAILS AND CORRECTNESS 231\nAll nodes in Dcalc returned by some call to DYNAMICHS using DPI DPI1 that are no diagnoses w.r.t. DPI2, the extension of DPI1 by a new query added as a positive or negative test case, are added to the set D\u00d7 (and not to DX) in line 22 of Algorithm 5 and are thus no elements of the set DX given as an argument to DYNAMICHS at the next call to DYNAMICHS. The elements of DX given as an argument to DYNAMICHS at the next call to DYNAMICHS using DPI2 are definitely added to Q again in lines 79-80 as DX is not modified elsewhere in DYNAMICHS before lines 79-80 are reached.\nTherefore, we need to differentiate between two cases: Either\n\u2022 (x1) D\u2032 \u2208 D\u00d7 never holds for the input argument D\u00d7 to any call to DYNAMICHS or\n\u2022 (x2) D\u2032 \u2208 D\u00d7 holds at least once for the input argument D\u00d7 to some call to DYNAMICHS.\nCase (x1): Since D\u2032 \u2208 Dcalc holds after the execution of DYNAMICHS using DPIprev stops, we have that D\u2032 \u2208 DX must hold for the argument DX given to DYNAMICHS using DPIprev+1. After UPDATETREE returns during DYNAMICHS using DPIprev+1, D\u2032 \u2208 Q holds as argued. Subsequently, D\u2032 might be added again to Dcalc and then to DX again in line 21 of Algorithm 5 and to Q again in line 80 during DYNAMICHS using DPIprev+2, and so forth. But, when a test case is added to some DPI DPIprev+i in Algorithm 5 that invalidates the diagnosis D\u2032 (yielding the DPI DPIprev+i+1), D\u2032 /\u2208 Dcalc is assumed to hold (otherwise it would be an element of D\u00d7 against our assumption). Such a test case must be added sometime as argued above. By Proposition 12.3, D\u2032 cannot be a (minimal) diagnosis w.r.t. any DPI including more test cases than DPIprev+i+1 either. Notice that the case D\u2032 /\u2208 Dcalc can emerge in spite of the fact that D\u2032 is a minimal diagnosis w.r.t. DPIprev+i because there may be minimal diagnoses w.r.t. DPIprev+i that have a higher probability than D\u2032. For DPIprev+i+1 and all DPIs including more test cases than DPIprev+i+1, D\u2032 cannot be added to Dcalc anymore due to Lemma 12.1 which claims that only diagnoses w.r.t. the currently used DPI can be added to Dcalc.\nCase (x2): Here, D\u2032 \u2208 D\u00d7 holds at least once for the input argument D\u00d7 to some call to DYNAMICHS using the DPI DPIprev+i. Then, DYNAMICHS using the DPI DPIprev+i\u22121 must have returned a set Dcalc including D\u2032 as otherwise D\u2032 cannot be added to D\u00d7. Hence, D\u2032 must be a diagnosis w.r.t. DPIprev+i\u22121 by Lemma 12.1. Since D\u2032 is added to D\u00d7, it cannot be a diagnosis w.r.t. DPIprev+i. This must hold\n\u2022 by Remark 7.4,\n\u2022 since the set added to D\u00d7 in Algorithm 5 is exactly the set Dout returned by GETINVALIDDIAGS in line 19 of Algorithm 5 and\n\u2022 Dout = D+(Q) in case the user answer u(Q) to the query Q w.r.t. Dcalc and DPIprev+i\u22121 is false and Dout = D\u2212(Q) otherwise (notice that Dcalc is referred to as DX in Algorithm 5).\nSo, by Proposition 12.3, D\u2032 cannot be a (minimal) diagnosis w.r.t. any DPI including more test cases than DPIprev+i either.\nEach element in D\u00d7 is processed by the UPDATETREE function (lines 48-69) called for the DPI DPIprev+i. In lines 48-69, each node ndx in D\u00d7 can only be pruned or either ndx or a node in a transitive replaces-relation with ndx is added to Q in line 68. Dcalc is not modified by UPDATETREE and Dcalc = \u2205 holds at the beginning of the execution of each call to DYNAMICHS. (A node set-equal to) D\u2032 cannot ever be readded to Dcalc by Lemma 12.1 and since D\u2032 is not a diagnosis w.r.t any DPI including more test cases than DPIprev+i. Hence, D\u2032 \u2208 Dcalc can never hold for any DPI including more test cases than DPIprev+i.\nHence, there must be some DPI DPIprev+k such that DX given as input to the DYNAMICHS-call for DPIprev+k does not include any diagnosis D\u2032 \u2282 node. So, during the execution of the call to\n232 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nDYNAMICHS using DPIDPIprev+k, node must be deleted from D\u2283 and be reinserted into Q by lines 70- 78 in UPDATETREE which is called at the beginning of the execution of DYNAMICHS at any call to DYNAMICHS. This must hold since all nodes ndx in D\u2283 that have not yet been pruned and for which there is no diagnosis in DX which is a proper subset of ndx, are added to Q throughout lines 70-78. As shown, both criteria are met for node during the execution of the call to DYNAMICHS using DPI DPIprev+k.\nCase (ii): By Lemma 12.1, we know that node is a diagnosis w.r.t. DPIprev and that node is added to Dcalc. Since node \u2282 D and D is a minimal diagnosis w.r.t. DPI , we obtain, by the same argumentation as in (i), that there must be some DPI DPIprev+k such that DX given as input to the DYNAMICHS-call for DPIprev+k does not include node.\nIf node /\u2208 D\u00d7, then it cannot ever be added to Dcalc again, as argued in case (i). Otherwise, during the execution of UPDATETREE which is called at the beginning of the execution of each call to DYNAMICHS, D\u00d7 is modified in lines 48-69.\nNow, we differentiate between two cases, namely node is either\n\u2022 (\u00acr) non-redundant w.r.t. DPI or\n\u2022 (r) redundant w.r.t. DPI .\nCase (\u00acr): Due to the non-redundancy of node w.r.t. DPI , Lemma 12.4, Lemma 12.10 and Corollary 12.1, node cannot be replaced or pruned throughout lines 48-66. Thus, node is reinserted into Q in line 68.\nCase (r): Since node is redundant w.r.t. DPI , it may or may not be redundant w.r.t.DPIprev+k+1. So, during the UPDATETREE function called in DYNAMICHS for DPIprev+k+1, there may or may not be some call to PRUNE given some X as argument which is a witness of redundancy of node. In the latter case, node will not be replaced or pruned during any PRUNE execution and will be reinserted into Q in line 68. In the former case, node might be replaced, but it cannot be pruned due to the same reasoning as given in the second paragraph of case (i). So, either node or some node in a transitive replaces-relation with node must be in D\u00d7 at the time line 67 is reached. This node is then added to Q in line 68.\nNow, both cases (i) and (ii) identified for case (\u00acs) lead to the reinsertion of node or some node in a transitive replaces-relation with node \u2013 which is thus set-equal to nd \u2013 into Q. Notice that this node has the same properties as node before one of the cases (i) or (ii) emerged (by analogue reasoning as conducted above). That is, if PRUNE is called given a witness of redundancy of node, then a replacement node of node is found. And, if only one replacement node of node is found, this replacement node is de-facto non-redundant w.r.t. DPI . So, we call this reinserted node again node.\nFurthermore, node can be neither labeled by valid nor by nonmin during the execution of DYNAMICHS for DPI . This holds by Lemma 12.1 and since node can be neither a diagnosis nor a nonminimal diagnosis w.r.t.DPI due to node \u2282 D and the fact thatD is a minimal diagnosis w.r.t.DPI . As a consequence of this and the assumption that the DYNAMICHS-call for DPI terminates due to Q = [], case (s) must arise at some point in time for node during some execution of DYNAMICHS for some (previous) DPI not-necessarily equal to DPI .\nCase (s): In this case, by Lemma 12.2, DLABEL returns a minimal conflict set L w.r.t. DPIprev as a label for node where L has the property that L \u2229 node = \u2205.\nIt must hold that L 6= \u2205. Otherwise, by Proposition 4.2, either\n\u2022 (v1) K is valid w.r.t. \u3008\u00b7,B,Pprev,Nprev\u3009R where DPIprev = \u3008K,B,Pprev,Nprev\u3009R or\n12.4. ALGORITHM DETAILS AND CORRECTNESS 233\n\u2022 (v2) DPIprev is non-admissible.\nIn the former case (v1), we know by Corollary 3.3 that the only (minimal) diagnosis w.r.t. DPIprev is \u2205. If DPIprev is equal to DPI , this is a contradiction to the existence of some minimal diagnosis w.r.t. DPI , namely D, which is not the empty set. D \u2283 \u2205 must hold since, by precondition, there is a node nd such that nd \u2282 D and since \u2205 \u2286 nd.\nOtherwise, if DPIprev includes a proper subset of the test cases DPI includes, DPI can never be a current DPI during any execution of DYNAMICHS during the same execution of Algorithm 5 during which there is an execution of DYNAMICHS where DPIprev is the current DPI. This holds as there must be at least two diagnoses in DX in line 13 of Algorithm 5 in order for DYNAMICHS to be called again with a DPI including a proper superset of the test cases inDPIprev (notice that, in Algorithm 5, the name of the set Dcalc returned by DYNAMICHS for DPIprev is DX). For, in case there is only one diagnosis, i.e. \u2205, then the probability of this diagnosis is 1 which is greater or equal 1\u2212 \u03c3 for any choice of \u03c3 due to \u03c3 \u2265 0. Consequently, Algorithm 5 would return in line 14. This is a contradiction to the assumption that there is an execution of DYNAMICHS where DPI is the current DPI.\nIn the latter case (v2), we can infer by Corollary 7.3, which states that adding queries as test cases to an admissible DPI can never yield a non-admissible DPI, that the DPI given as an input to Algorithm 5 must be non-admissible, contradiction.\nThence,L 6= \u2205 and DYNAMICHS will execute lines 17-23 and generate one node nodee := ADD(node, e) with nodee.cs := ADD(node.cs, L) for each e \u2208 L (cf. Definition 12.2 for an explanation of the function ADD).\nNow, we have that there must be some non-empty active sublabel of L = nodee.cs[r] w.r.t. DPI where r := |nodee| by Definition 12.6. Definition 12.6 is applicable by the following argumentation:\nThe first observation is that nodee.cs[r] cannot be reduced twice during one and the same execution of DYNAMICHS using one and the same DPI DPIprev+j which results from DPIprev by addition of test cases. For, by Corollaries 12.1 and 12.2 and Lemmata 12.6 and 12.7, PRUNE as well as PRUNEQDUP can only be called given some minimal conflict set X w.r.t. DPIprev+j . By Lemmata 12.10 and 12.8, all nodes ndx that are in the set returned by PRUNE and PRUNEQDUP, respectively, have the property that there are no proper supersets of X in ndx.cs. Moreover, there are no proper subsets of X in ndx.cs. Because each ndx.cs[m] for m \u2208 {1, . . . , |ndx.cs|} must be a minimal conflict set w.r.t. some DPI equal to DPIprev+j or including a subset of the test cases in DPIprev+j . Otherwise, ndx could not be a node during the execution of DYNAMICHS where DPIprev+j is the current DPI. By Proposition 12.1, there cannot be any m \u2208 {1, . . . , |ndx.cs|} such that ndx.cs[m] \u2282 X as X is a minimal conflict set w.r.t. DPIprev+j . As two minimal conflict sets w.r.t. DPIprev+j can never be in a proper subset-relationship with one another, L = nodee.cs[r] can be modified at most once by PRUNE or PRUNEQDUP for the DPI DPIprev+j .\nSecond, by Proposition 12.1, each minimal conflict set w.r.t. DPIprev is a conflict set w.r.t. any DPI DPIprev+j that results from DPIprev by addition of test cases; that is, in particular, w.r.t. DPI . So, there must be some minimal conflict set Cj w.r.t. each DPIprev+j such that Cj \u2286 L and there cannot be any minimal conflict set w.r.t. DPIprev+j that is a proper superset of L.\nThird, we have that L 6= \u2205, L is a minimal conflict set w.r.t. DPIprev , and DPIprev+j includes a superset of the test cases in DPIprev . Thus, by Proposition 12.2, each minimal conflict set w.r.t. DPIprev+j must be non-empty. In particular, Proposition 12.2 implies that all minimal conflict sets w.r.t. DPI that are subsets of L must be non-empty.\nBy these three observations, the criteria of Definition 12.6 can be applied to analyze the active subnode of nodee.cs[r] w.r.t. DPI . That is, if C1, . . . , Cn is the (arbitrary actual) chronological sequence of all sets X given as an argument to PRUNE and PRUNEQDUP during all executions of DYNAMICHS from the one with current DPI DPIprev up to and including the one with current DPI DPI where\n\u2022 nodee.cs[r] \u2283 C1,\n234 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\n\u2022 each Ci is a minimal conflict set w.r.t. DPIi for i \u2208 {1, . . . , n}\n\u2022 Ck \u2283 Ck+1 for k \u2208 {1, . . . , n\u2212 1},\n\u2022 DPIj includes a proper subset of the test cases DPIj+1 includes for j \u2208 {1, . . . , n\u2212 1},\n\u2022 DPIn is equal to DPI or includes a proper subset of the test cases DPI includes and\n\u2022 DPIprev includes a proper subset of the test cases DPI1 includes,\nthen Cn is the active sublabel of nodee.cs[r] w.r.t. DPI . However, as argued before, the minimal conflict set Cn w.r.t. DPIn cannot be the empty set. As a consequence, we obtain that there must be a non-empty active sublabel of nodee.cs[r] w.r.t. DPI .\nBy Propositions 12.1 and 12.2, there is a non-empty minimal conflict set C\u2032 w.r.t. DPI such that C\u2032 \u2286 Cn. Due to Cn \u2282 \u00b7 \u00b7 \u00b7 \u2282 C1 \u2282 nodee.cs[r] = L we conclude that Cn \u2282 L. Therefore, \u2205 \u2282 C\u2032 \u2282 L holds.\nBy Proposition 4.6, each minimal diagnosis w.r.t. DPI is a minimal hitting set of all minimal conflict sets w.r.t. DPI . Thence, we have that C\u2032 \u2229 D 6= \u2205. So, by C\u2032 \u2282 L, we have that \u2205 \u2282 C\u2032 \u2229 D \u2286 L \u2229 D \u2286 L. Consequently, we define ndsuc := nodex = ADD(node, x) with ndsuc.cs := nodex.cs = ADD(node.cs, L) for some x \u2208 C\u2032 \u2229 D \u2286 L. Then, ndsuc \u2286 D because node \u2282 D and x \u2208 D. It is clear from the inference so far that nd \u2282 ndsuc, |ndsuc| = |nd| + 1 and ndsuc \u2208 GenNodes. This shows the truth of propositions (1)-(3).\nProposition (4) must hold by lines 20-23. Now we argue why propositions (5) and (6) must hold. Assume that nd\u2032suc \u2208 Q is redundant w.r.t. some DPI DPI \u2032\u2032prev which is equal to DPI or includes fewer test cases than DPI . Then, there must be some minimal conflict set C\u2032\u2032 w.r.t. DPI \u2032\u2032prev which is a witness of redundancy of nd \u2032 suc. Suppose that PRUNE is called given X := C\u2032\u2032 as an argument. Now, we have to distinguish two cases: Either\n\u2022 (q1) ndsuc was added to Q after it was generated or\n\u2022 (q2) ndsuc was added to Qdup after it was generated\n(there are no other possibilities, see lines 17-23). For each of these two cases, there are two more cases to discriminate between:\n\u2022 (c1) C\u2032\u2032 \u2282 nd\u2032suc.cs[|nd \u2032 suc|] and nd \u2032 suc[|nd \u2032 suc|] \u2208 nd \u2032 suc.cs[|nd \u2032 suc|] \\ C\u2032\u2032 or\n\u2022 (c2) C\u2032\u2032 \u2282 nd\u2032suc.cs[j] and nd \u2032 suc[j] \u2208 nd \u2032 suc.cs[j] \\ C\u2032\u2032 for some j \u2208 { 1, . . . , |nd\u2032suc| \u2212 1 } .\nCase (q1): Here, we have that nd\u2032suc is the same node as ndsuc since ndsuc was added to Q after generation and no node replacement can have taken place because nd\u2032suc is defined as the node setequal to ndsuc that is an element of Q immediately after ndsuc has been generated. And, only one node corresponding to one and the same set can be in Q at the same time.\nCase (c1): We have that C\u2032\u2032 must be equal to some minimal conflict set Cj in the sequence C1, . . . , Cn. This must be truesince, first, DPI \u2032\u2032prev is equal to DPI or includes a subset of the test cases in DPI and DPIprev includes a proper subset of the test cases in DPI \u2032\u2032prev .\nTo understand why the latter must hold, recall that DPIprev is the DPI of the call to DYNAMICHS where ndsuc was generated and the minimal conflict set L was computed. By assumption, however, there is some minimal conflict set w.r.t. DPI \u2032\u2032prev, namely C\u2032\u2032, such that C\u2032\u2032 \u2282 nd \u2032 suc.cs[|nd \u2032 suc|] = L. Hence, it cannot be truethat both L and C\u2032\u2032 are minimal conflict sets w.r.t. the same DPI. Otherwise, we would have\n12.4. ALGORITHM DETAILS AND CORRECTNESS 235\na contradiction to the minimality of L. By Proposition 12.1, which states that minimal conflict sets cannot grow by the addition of new test cases to the DPI, we obtain the claimed fact that DPIprev includes a proper subset of the test cases in DPI \u2032\u2032prev .\nSecond, the sequence C1, . . . , Cn comprises all setsX given as an argument to PRUNE and PRUNEQDUP during all executions of DYNAMICHS from the one with current DPI DPIprev up to and including the one with current DPI DPI where L = nd\u2032suc.cs[|nd \u2032 suc|] \u2283 C1 \u2283 \u00b7 \u00b7 \u00b7 \u2283 Cn holds. Reason for this to be valid is the fact that nd\u2032suc is the same node as ndsuc in the currently considered case (q1). Now, recall C\u2032 is a minimal conflict set w.r.t. DPI such that x \u2208 C\u2032 \u2229 D \u2282 L. Further, by nd\u2032suc = nodex, we have that nd\u2032suc[|nd \u2032 suc|] = x. Since C\u2032 \u2286 Cn, we have that C\u2032 \u2286 Cj must hold due to Cn \u2286 Cj . Therefore, we can infer by C\u2032\u2032 = Cj that C\u2032 \u2286 C\u2032\u2032 is true . Now, x \u2208 C\u2032 implies that x \u2208 C\u2032\u2032 wherefore x /\u2208 nd\u2032suc.cs[|nd \u2032 suc|] \\ C\u2032\u2032. By x = nd \u2032 suc[|nd \u2032 suc|], this is a contradiction to the assumption of case (c1). Hence, case (c2) must arise.\nCase (c2): We have that nd\u2032suc[1..|nd \u2032 suc|\u22121] must be redundant w.r.t.DPI \u2032\u2032prev . The subnode nd \u2032 suc[1.. |nd\u2032suc| \u2212 1] of nd \u2032 suc is the same node as node by nd \u2032 suc = nodex. So, suppose PRUNE is called with arguments Q (which inlcudes nd\u2032suc), X := C\u2032\u2032 and Qdup during the execution of DYNAMICHS with current DPI DPI \u2032\u2032prev .\nRecall that node is the node set-equal to nd that is processed. That is, node is either the same node as nd or it is in a transitive replaces-relation with nd. Therefore, by the preconditions of this lemma, the following holds: If PRUNE is called given a witness of redundancy of node, then a replacement node of node is found. And, if only one replacement node noderep of node is found, then noderep is de-facto non-redundant w.r.t. DPI .\nSo, at the time PRUNE might be called given a witness of redundancy of node, Comb(Qdup) must include a (non-necessarily proper) alternative subnode noderep,sub of node from which the de-facto nonredundant node noderep w.r.t. DPI can be constructed as\nnoderep := ADD(noderep,sub, node[|noderep,sub|+ 1..|node|]) noderep.cs := ADD(noderep,sub.cs, node.cs[|noderep,sub|+ 1..|node|])\nThis holds due to\n\u2022 Corollary 12.7, which says that each call to PRUNEQDUP returns the list Qdup, a subset of Comb(Qdup),\n\u2022 the fact that PRUNEQDUP is always called immediately before PRUNE is called and\n\u2022 the fact that PRUNE searches for alternative subnodes for the construction of a replacement node of a redundant node exactly in the output set of PRUNEQDUP.\nBy Definition 12.7, this is implies that noderep,sub must be de-facto non-redundant w.r.t. DPI as otherwise the de-facto non-redundancy w.r.t. DPI could not hold for noderep.\nConsequently, by Lemma 12.11, noderep,sub \u2208 Comb(Qdup) must always be satisfied during any execution of DYNAMICHS using a DPI that is equal to DPI or includes a subset of the test cases in DPI . Hence, in particular, this must hold for the DPI DPI \u2032\u2032prev .\nBy line 21 and PRUNEQDUP, which are the only places in DYNAMICHS where Qdup is modified, Qdup is sorted in ascending order by node cardinality at any time during the execution of any call to DYNAMICHS.\nIn order to construct a replacement node of nd\u2032suc, PRUNE first determines the maximal k such that C\u2032\u2032 \u2282 nd\u2032suc.cs[k] and nd \u2032 suc[k] \u2208 nd \u2032 suc.cs[k] \\ C\u2032\u2032. As case (c1) was proven to be false, we conclude that k \u2264 |nd\u2032suc| \u2212 1 must hold. Due to the fact that nd \u2032 suc[1..|nd \u2032 suc| \u2212 1] is the same node as node, as reasoned above, and the fact that a de-facto non-redundant alternative equal node noderep (see above)\n236 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nof node can be constructed from noderep,sub \u2208 Comb(Qdup), we obtain that k \u2264 |noderep,sub|. This holds because the truth of both node.cs[m] \u2283 C\u2032\u2032 and node[m] \u2208 node.cs[m] \\ C\u2032\u2032 for some m \u2208 {|noderep,sub|+ 1, . . . , |node|}would be a contradiction to the de-facto non-redundancy of noderep w.r.t. DPI .\nThen, in line 96, an alternative subnode of nd\u2032suc\n\u2022 which has cardinality k + z where z \u2265 0 is minimal and\n\u2022 from which a replacement node of nd\u2032suc can be constructed\nis searched for in Qdup. To see this, observe that elements in Qdup \u2013 which is sorted in ascending order of node cardinality, as argued \u2013 are visited in order starting from the lowest cardinality node (line 96).\nHowever, there is an alternative subnode noderep,sub of node such that k \u2264 |noderep,sub| \u2264 |node| = |nd\u2032suc| \u2212 1 and noderep,sub is an element of the argument Qdup given to PRUNE, as shown above. As nd\u2032suc is the same node as nodex, node is a subnode of nd \u2032 suc. Therefore, noderep,sub is an alternative subnode of nd\u2032suc. Thus, we have that one replacement node of nd\u2032suc is definitely found by PRUNE. And, in case there is only one replacement node of nd\u2032suc constructable during PRUNE, then this replacement node is given by nd\u2032suc,new := ADD(noderep,sub, nodex[|noderep,sub| + 1..|nodex|]) = ADD(noderep, x) with nd\u2032suc,new.cs := ADD(noderep,sub.cs, nodex.cs[|noderep,sub| + 1..|nodex|]) = ADD(noderep.cs, L). As it is straightforward from the deductions above, nd\u2032suc,new is de-facto non-redundant w.r.t. DPI . Thence, proposition (5) is true .\nDue to |noderep,sub| \u2264 |node| = |nd\u2032suc| \u2212 1, the alternative subnode of nd \u2032 suc actually found by PRUNE cannot have a cardinality greater than |nd\u2032suc|\u22121. So, let ndalt be the found alternative subnode of nd\u2032suc. Since |ndalt| \u2264 |nd \u2032 suc|\u22121, we obtain that the replacement node nd \u2032 suc,new,1 of nd \u2032 suc constructed from ndalt must meet nd\u2032suc,new,1[|nd \u2032 suc|] = nd \u2032 suc[|nd \u2032 suc|] = x as well as nd \u2032 suc,new,1.cs[|nd \u2032 suc|] = nd\u2032suc.cs[|nd \u2032 suc|] = L. That is, the first |node| = |nd \u2032 suc| \u2212 1 positions as a set correspond to a node in a transitive replaces-relation with nd. Now, we have the following precondition of this lemma: Let nd\u2032 be in a transitive replaces-relation with nd. If PRUNE is called given a witness of redundancy of nd\u2032, then some replacement node of nd\u2032 is found. If only one replacement node of nd\u2032 is found, then this replacement node is de-facto non-redundant w.r.t. DPI .\nTherefore, the same line of argument as used for nd\u2032suc can be applied to any node nd \u2032 suc,rep in a transitive replaces-relation with nd\u2032suc. That is, the following must be valid for any node nd \u2032 suc,rep in a transitive replaces-relation with nd\u2032suc:\n\u2022 nd\u2032suc,rep[|nd \u2032 suc|] = x and nd \u2032 suc,rep.cs[|nd \u2032 suc|] = L.\n\u2022 If PRUNE is called given a witness of redundancy of nd\u2032suc,rep, then some replacement node of nd\u2032suc,rep is found. And, if only one replacement node of nd \u2032 suc,rep is constructable, then this\nreplacement node is de-facto non-redundant w.r.t. DPI .\nAfter once a replacement node of nd\u2032suc or of some node in a transitive replaces-relation with nd \u2032 suc is found which is de-facto non-redundant w.r.t. DPI , this replacement node cannot be replaced or pruned by Proposition 12.7. Therefore, by Lemma 12.10, no witness of redundancy of this replacement node can exist w.r.t. any DPI including a (not necessarily proper) subset of the test cases in DPI . Thence, proposition (6) is true .\nCase (q2): Here, we have that nd\u2032suc is not the same node as ndsuc. This must be valid as nd \u2032 suc is defined as the node set-equal to ndsuc that is an element of Q immediately after ndsuc was generated and ndsuc is assumed to be added to Qdup after being generated.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 237\nNow, independently of whether (c1) or (c2) occurs, the following holds: If PRUNE is called given a witness of redundancy of nd\u2032suc, then a replacement node of nd \u2032 suc is found. And, if only one replacement node of nd\u2032suc is constructable, then this replacement node is de-facto non-redundant w.r.t. DPI . To understand why this must hold, first recall that ndsuc is a successor of node, i.e. ndsuc[1..|ndsuc|\u2212 1] is the same node as node. Furthermore, node is the node set-equal to nd that is processed. That is, node is either the same node as nd or it is in a transitive replaces-relation with nd.\nTherefore, by the preconditions of this lemma, the following holds: If PRUNE is called given a witness of redundancy of node, then a replacement node of node is found. And, if only one replacement node noderep of node is constructable, then noderep is de-facto non-redundant w.r.t. DPI .\nAs argued in case (q1)(c2), Comb(Qdup) must include a subnode noderep,sub of noderep that is defacto non-redundant w.r.t. DPI and from which noderep is constructed. This must be satisfied during any execution of DYNAMICHS using a DPI that is equal to DPI or includes a subset of the test cases in DPI . Hence, in particular, this must hold for the DPI DPI \u2032\u2032prev .\nSince ndsuc has been added to Qdup by assumption, it might be found to be redundant w.r.t. some DPI (either equal to DPI or including a subset of the test cases in DPI) during some execution of PRUNEQDUP. If so, ndsuc cannot be pruned on account of Lemma 12.8 which says that a node can only be pruned from Qdup if the set Combndsuc(Qdup) of combined equal nodes of ndsuc of Qdup (cf. Definition 12.5) is the empty set.\nHowever, Combndsuc(Qdup) 6= \u2205 must be valid. Because we demonstrated that\n\u2022 noderep,sub \u2208 Comb(Qdup),\n\u2022 ndsuc \u2208 Qdup,\n\u2022 ndsuc is the same node as nodex = ADD(node, x) with ndsuc.cs being equal to nodex.cs = ADD(node.cs, L) and\n\u2022 x /\u2208 ndsuc.cs[|ndsuc|] \\ C\u2032\u2032 (see case (q1)(c1)) wherefore C\u2032\u2032 must be a witness of redundancy of node.\nTherefore, ndcomb := ADD(noderep,sub, nodex[|noderep,sub| + 1..|nodex|]) = ADD(noderep, x) with ndcomb.cs := ADD(noderep,sub.cs, nodex.cs[|noderep,sub| + 1..|nodex|]) = ADD(noderep.cs, L) is a combined equal node of ndsuc of Qdup, i.e. ndcomb \u2208 Combndsuc(Qdup). As argued in case (q1)(c2), this node ndcomb (denoted by nd\u2032suc,new in case (q1)(c2)) is de-facto non-redundant w.r.t. DPI .\nBecause PRUNE is called immediately after PRUNEQDUP and thus uses the updated list Qdup which comprises ndcomb and because ndcomb = ndsuc = nd\u2032suc, we have that one replacement node of nd \u2032 suc is definitely found by PRUNE. And, in case there is only one replacement node of nd\u2032suc constructable during PRUNE, this replacement node is given by ndcomb. Thence, proposition (5) is true .\nBy Proposition 12.7, the fact that ndcomb \u2208 Combndsuc(Qdup) \u2286 Comb(Qdup) at some point in time during the execution of DYNAMICHS with current DPI DPI \u2032\u2032prev and the de-facto non-redundancy of ndcomb w.r.t. DPI , we conclude that, during any execution of DYNAMICHS with a current DPI that includes a (not necessarily proper) superset of the test cases in DPI \u2032\u2032prev and includes a (not necessarily proper) subset of the test cases in DPI , ndcomb \u2208 Comb(Qdup) must hold. Further on, ndcomb = nd\u2032suc is true .\nHence, independently of which replacement node of nd\u2032suc is actually found by PRUNE, a set-equality between this replacement node and ndcomb will hold. This is truesince each replacement node, by definition, is set-equal to the node it replaces. Consequently, this set-equality holds for any node in a transitive replaces-relation with nd\u2032suc. So, we have that one replacement node of any node nd \u2032 suc,rep in a transitive replaces-relation with nd\u2032suc is definitely found by PRUNE. And, in case there is only one replacement node of nd\u2032suc,rep constructable during PRUNE, this replacement node is given by ndcomb which is defacto non-redundant w.r.t. DPI .\n238 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nThat ndcomb, after it has been used as a replacement node of nd\u2032suc or of some node in a transitive replaces-relation with nd\u2032suc, cannot be pruned or replaced, follows from Proposition 12.7 and the fact that ndcomb is de-facto non-redundant w.r.t.DPI . Therefore, by Lemma 12.10, no witness of redundancy of ndcomb can exist w.r.t. any DPI including a (not necessarily proper) subset of the test cases in DPI . Thence, proposition (6) is true .\nIn the following we prove the completeness of DYNAMICHS. Given an arbitrary minimal diagnosis D w.r.t. to an arbitrary fixed DPI DPI , Proposition 12.8 testifies that there must be some node set-equal to D that is processed during the execution of DYNAMICHS with current DPI DPI in case this execution terminates by reason of Q = []. Second, the proposition demonstrates that the set Dcalc returned by this execution of DYNAMICHS comprises all minimal diagnoses w.r.t. DPI . Additionally, the proposition shows that, at any point in time during the execution of Algorithm 5, some node that corresponds to a subset of D must be stored by DYNAMICHS.\nIn terms of the hitting set tree produced by DYNAMICHS, the proposition states that, after all branches in the tree have been closed or pruned, there is a closed branch labeled by valid for each minimal diagnosis w.r.t. DPI . And, for any minimal diagnosis D w.r.t. DPI , at any time during the tree construction, there is some branch that corresponds to a part of D.\nThis proposition will be proven by deriving the existence of a de-facto non-redundant node ndD w.r.t. DPI for any minimal diagnosis D w.r.t. DPI such that ndD \u2286 D. In case ndD = D, we will deduce directly that the proposition must be true . Otherwise, i.e. if ndD \u2282 D, then Lemmata 12.13 and 12.14 will be exploited.\nProposition 12.8 (Completeness of DYNAMICHS). Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS and assume that DYNAMICHS terminates due to Q = []. Let further DPI := \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R and D be some minimal diagnosis w.r.t. DPI . Then the following holds:\n(1) At some point in time during the execution of DYNAMICHS with current DPI DPI , there is a node nd such that nd = D and nd is processed.\n(2) The execution of DYNAMICHS with current DPIDPI returns a set Dcalc that comprises all minimal diagnoses w.r.t. DPI .\n(3) Let DPI \u2032 be an arbitrary DPI that includes a (not necessarily proper) subset of the test cases in DPI . Then, at any point in time during the execution of DYNAMICHS with current DPIDPI \u2032, there is some node nd\u2032 such that nd\u2032 \u2286 D and nd\u2032 is an element of one of the collections Q,Dcalc,DX,D\u00d7 or D\u2283.\nProof. Let GenNodes be the set of all nodes generated throughout the execution of all calls to DYNAMICHS during the execution of Algorithm 5.\nAssume first that D = \u2205. This means that DPI must be the input DPI of Algorithm 5. Assume the opposite.\nA query is only generated and added as a new test case to the DPI in lines 16 and 24 or 26 of Algorithm 5 if there are at least two diagnoses in the set Dcalc (called DX in Algorithm 5) returned by DYNAMICHS. Otherwise, line 16 cannot be reached since there must be exactly one diagnosis in DX when it comes to the execution of line 13 wherefore the probability of this diagnosis must be equal to 1 which is greater or equal to 1 \u2212 \u03c3 for any choice of \u03c3 (recall that \u03c3 is positive). Please notice that DX = \u2205 cannot hold in line 13 since this would imply the non-admissibility of the input DPI given to Algorithm 5 by Corollary 7.3 and Definition 3.6. By precondition, however, the DPI provided as an input to Algorithm 5 must be admissible.\nNow, since DPI is assumed to be not equal to the input DPI of Algorithm 5, we have, by the argumentation given, that there must have been at least two diagnoses w.r.t. the input DPI.\n12.4. ALGORITHM DETAILS AND CORRECTNESS 239\nLet us first assume that K is valid w.r.t. \u3008\u00b7,B,P ,N \u3009R where \u3008K,B,P ,N \u3009R is the input DPI. Then, by Corollary 3.3, \u2205 is a diagnosis w.r.t. the input DPI. Obviously, it must be a minimal diagnosis and the only minimal diagnosis w.r.t. the input DPI, contradiction.\nSecond, suppose that K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R. By Proposition 4.6 which says that a diagnosis w.r.t. some DPI is a hitting set of all minimal conflict sets w.r.t. this DPI, we conclude that there must be at least one minimal conflict set C w.r.t. the input DPI. Now, by Proposition 12.1, there must be a minimal conflict set C\u2032 w.r.t. DPI such that C\u2032 \u2286 C. By Proposition 4.2, the fact that K is invalid w.r.t. \u3008\u00b7,B,P ,N \u3009R, the fact that the input DPI is admissible and Corollary 7.3 which states that the addition of queries as test cases cannot make an admissible DPI non-admissible, we obtain that \u2205 \u2282 C\u2032. By Proposition 4.6, this is a contradiction to D = \u2205 and the fact that D is a diagnosis w.r.t. DPI .\nSo, DPI is the input DPI. Hence, the first call to DYNAMICHS throughout the execution of Algorithm 5 considers this DPI. During the execution of the first call to DYNAMICHS, Q = [\u2205] holds by lines 3 and 10 of Algorithm 5. The function UPDATETREE has no effect during the execution of the first call to DYNAMICHS in Algorithm 5. That is, in particular, it does not modify Q. For, UPDATETREE first iterates over all elements in D\u00d7, then over all elements in D\u2283 and finally over all elements in DX where D\u00d7 = D\u2283 = DX = \u2205 by lines 1 and 10 in Algorithm 5. Hence, Q = [\u2205] holds when DYNAMICHS reaches line 6 wherefore \u2205 is processed.\nNow, assume D 6= \u2205. In this case, the root node must be labeled by some minimal conflict set L w.r.t. the DPI given as input to Algorithm 5. To see this, suppose the opposite, i.e. that the root node is labeled by (i) nonmin or (ii) valid.\nCase (i): This leads to a contradiction. For, Dcalc = \u2205 holds at the beginning of each execution of DYNAMICHS (line 3). The root node \u2205must be the first node that is processed throughout all executions of DYNAMICHS during the execution of Algorithm 5 since it holds for each other node node that node \u2283 \u2205. Thus, the non-minimality criterion (lines 27-29) cannot be satisfied because Dcalc = \u2205 must hold in line 27 when DLABEL is executed for the root node. Hence, the label nonmin is impossible for the node \u2205.\nCase (ii): By Lemma 12.1, we can deduce that \u2205 is a diagnosis w.r.t. the input DPI. The fact that there cannot be any diagnosis w.r.t. the input DPI which is a proper subset of \u2205 implies that \u2205 is a minimal diagnosis w.r.t. the input DPI. By the reasoning applied before (in the case D = \u2205), we obtain that DPI is equal to the input DPI and that \u2205 is the only minimal diagnosis w.r.t. DPI . This is a contradiction to the existence of a minimal diagnosis w.r.t. DPI , namely D, which is non-empty.\nConsequently, the root node must be labeled by some minimal conflict set L w.r.t. the input DPI. Hence, DYNAMICHS will execute lines 17-23 and generate one node nodee := ADD(\u2205, e) = [e] with nodee.cs := ADD(\u2205, L) = [L] for each e \u2208 L (cf. Definition 12.2 for an explanation of the function ADD). This means that nodee \u2208 GenNodes for each e \u2208 L. As L is a set and thus comprises only one exemplar of each element, there cannot be a set-equal node node\u2032e of nodee in Q at the time nodee is generated. So, each nodee must be added to Q in line 23.\nBy Proposition 12.1, there must be some minimal conflict set C w.r.t. DPI such that C \u2286 L. Since D is a diagnosis w.r.t. DPI , we have that C \u2229 D 6= \u2205 by Proposition 4.6. Thence, L \u2229 D 6= \u2205 must be true . Therefore, in particular, L 6= \u2205 must hold.\nAssume that |D| = 1. This implies by Proposition 4.6 that each minimal conflict set w.r.t. DPI includes x. Further, there is some x \u2208 L such thatD = {x} = nodex. By Corollary 12.1 and Lemmata 12.6 and 12.7, PRUNE is only called given some minimal conflict set X w.r.t. the current DPI DPIprev as argument. As DYNAMICHS using DPI is assumed to terminate due to Q = [], DPIprev must be equal to DPI or include only a subset of the test cases DPI includes. By Proposition 12.1, it must hold for X that it is equal to or a superset of some minimal conflict set w.r.t. DPI . Hence x \u2208 X must hold wherefore X cannot be a witness of redundancy of nodex. So, nodex can never be pruned and must be finally processed as DPI terminates due to Q = [] and nodes can only be deleted from Q by being pruned or processed. So far, we have established the truth of the lemma for |D| \u2264 1.\n240 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nNow, suppose |D| \u2265 2. In the following, we argue that there must be some node nodey \u2282 D for some y \u2208 L which is de-facto non-redundant w.r.t. DPI .\nAs DYNAMICHS using DPI is assumed to terminate due to Q = [], each node nodee for e \u2208 L must have been generated (and L must have been computed) during DYNAMICHS with some current DPI DPIprev which is equal toDPI or includes only a subset of the test casesDPI includes. LetDPIprev+i be any DPI which includes a proper superset of the test cases DPIprev includes and is either equal to DPI or comprises a subset of the test cases DPI comprises. Then, Proposition 12.1 manifests that there must be some minimal conflict set Ci w.r.t. DPIprev+i such that Ci \u2286 L. Since we proved above that L 6= \u2205 must hold, we deduce by Proposition 12.2 that Ci 6= \u2205 must be valid.\nFrom Corollaries 12.1, 12.2 and Lemmata 12.6 and 12.7 we infer that PRUNE as well as PRUNEQDUP are always called with a minimal conflict set X w.r.t. the current DPI given as an argument. Lemma 12.8 and the fact that PRUNE is always called immediately after PRUNEQDUP given the argument Qdup which is the output list of PRUNEQDUP, we have that the list Qdup includes only nodes nd such that there is no r \u2208 {1, . . . , |nd|} for which nd.cs[r] \u2283 X . As a consequence of this, we have by Lemma 12.10 that for all nodes nd in the collection S\u2032 returned by PRUNE there is no r \u2208 {1, . . . , |nd|} for which nd.cs[r] \u2283 X .\nThence, the first time PRUNE is called with someX1 \u2282 L,X1 is a minimal conflict set w.r.t. some DPI DPIprev+i. Thus, as argued, X1 \u2283 \u2205 must hold. So, after PRUNE has finished executing, for each node node in its output set there will be no r \u2208 {1, . . . , |node|} such that node.cs[r] \u2283 X1. For any further minimal conflict set X2 w.r.t. some DPIprev+i+k for which PRUNE is called, we have that X2 \u2283 \u2205 and for each node node in its output set there will be no r \u2208 {1, . . . , |node|} such that node.cs[r] \u2283 X2, and so on.\nFor L, in particular, there is some (possibly empty) sequence of minimal conflict sets X1, . . . , Xn w.r.t. DPIs DPIprev+i1 , . . . , DPIprev+in (ij < ij+1 for j \u2208 {1, . . . , n\u2212 1}) such that L \u2283 X1 and Xi \u2283 Xi+1 for i \u2208 {1, . . . , n} where this sequence includes all such conflict sets which restrict a conflict set used to label nodes that was initially given by L. SinceXn is a minimal conflict set w.r.t.DPIprev+in which is equal to DPI or includes only a subset of the test cases DPI includes, we have that there must be some minimal conflict set C w.r.t. DPI such that C \u2286 Xn, as already argued. As D must hit C by Proposition 4.6, we obtain that D \u2229Xn 6= \u2205.\nSo, by the inference given, there must be some y \u2208 L such that y \u2208 X1 \u2229 \u00b7 \u00b7 \u00b7 \u2229Xn and y \u2208 D. That is, nodey \u2282 D.\nSince |nodee| = 1 and nodee.cs[1] = L for all e \u2208 L, in particular for e = y, we obtain by Definitions 12.6 and 12.7 that nodey is de-facto non-redundant w.r.t. DPI .\nSo, the preconditions of Lemma 12.13 are met for nodey . As a consequence, there must be a node nd\u2032suc such that |nd \u2032 suc| = |nodey|+1, nd \u2032 suc \u2286 D, nd \u2032 suc is an element of Q immediately after nodey has been processed and nd\u2032suc satisfies the postulations to the node nd in the preconditions of Lemma 12.14. Hence, if nd\u2032suc \u2282 D, there must be a node nd \u2032\u2032 suc such that |nd \u2032\u2032 suc| = |nd \u2032 suc| + 1, nd \u2032\u2032 suc \u2286 D, nd \u2032\u2032 suc is an element of Q immediately after a node set-equal to nd\u2032suc has been processed and nd \u2032\u2032 suc satisfies the postulations to the node nd in the preconditions of Lemma 12.14. This reasoning by means of Lemma 12.14 can be further applied to finally derive that some node nd = Dmust be generated and some node nd\u2032 set-equal to nd must be an element of Q. By Lemma 12.14, either nd\u2032 or a node set-equal to nd\u2032 which is in a transitive replaces-relation with nd\u2032 must finally be processed. Reason for this is that nd\u2032 \u2208 Q cannot be pruned, but can only be replaced, and each replacement node is set-equal to nd\u2032 and thus to D. Moreover, the execution of DYNAMICHS with current DPI DPI terminates due to Q = [] wherefore each node in Q must be either pruned or processed as these are the only two ways nodes might be eliminated from Q.\nIf some node nd = D is processed during an execution of DYNAMICHS with current DPI some DPI DPI \u2032 that includes a proper subset of the test cases in DPI , then DLABEL cannot return a set L. This holds by Lemma 12.2 and Proposition 12.1. The former says that nd\u2229L = \u2205 and L is a minimal conflict set w.r.t. DPI \u2032. The latter asserts that each conflict set w.r.t. DPI is a conflict set w.r.t. DPI . Moreover,\n12.4. ALGORITHM DETAILS AND CORRECTNESS 241\nwe can deduce that L 6= \u2205must hold if a set L is returned by DLABEL by a similar argumentation as used in the proof of Lemma 12.14. That is, by Proposition 4.6, we have that D cannot be a diagnosis w.r.t. DPI , contradiction.\nHence, DLABEL must return nonmin or valid for nd. In the former case, it would be added to D\u2283, in the latter to Dcalc. Similarly as done in the proof of Lemma 12.14, we can show that nd must be reinserted into Q the latest during the execution of DYNAMICHS with current DPI DPI and, in particular, nd must be an element of Q when the repeat-loop during the execution of DYNAMICHS with current DPI DPI is entered. Thus, nd must be (again) processed during the execution of DYNAMICHS with current DPI DPI . This proves proposition (1).\nProposition (2): At the beginning of each execution of DYNAMICHS, it holds that Dcalc = \u2205. This is truein particular for the execution of DYNAMICHS with current DPI DPI . Now, proposition (1) reveals that, for each diagnosis D w.r.t. DPI , at some point in time during the execution of DYNAMICHS with current DPI DPI , there is a node nd such that nd = D and nd is processed. When nd is processed, the DLABEL function is called for nd. The DLABEL function might return (a) a set L, (b) nonmin or (c) valid. There are no other possible return values of DLABEL.\nCase (a): By Lemma 12.2, L must be a minimal conflict set w.r.t. DPI such that nd \u2229 L = \u2205. According to Proposition 4.6, it must hold for D that D \u2229 L 6= \u2205 since D is a minimal diagnosis w.r.t. DPI . Since D = nd, we obtain a contradiction.\nCase (b): By Lemma 12.1, Dcalc can comprise only diagnoses w.r.t. DPI . By line 27, this yields that there is a diagnosis w.r.t. DPI that is a proper subset of nd. This however is a contradiction to the set-equality of nd with the minimal diagnosis D w.r.t. DPI .\nConsequently, case (c) must arise. This implies that nd is added to Dcalc in line 13. Proposition (3) is a direct consequence of the reasoning in this proof and in the proofs of Lem-\nmata 12.13 and 12.14."}, {"heading": "12.4.9 Soundness of DYNAMICHS", "text": "Having established the completeness of each call to DYNAMICHS concerning the minimal diagnoses w.r.t. the current DPI DPI at this call, we are now able to prove the soundness of each call to DYNAMICHS. That is, we will demonstrate that only minimal diagnoses w.r.t. DPI can be added to the set Dcalc during DYNAMICHS with the current DPI DPI . Necessary condition for the proof of the following proposition is the completeness of DYNAMICHS, i.e. Proposition 12.8.\nProposition 12.9 (Soundness of DYNAMICHS). Let \u3008K,B,P ,N \u3009R be the DPI and P \u2032 and N \u2032 the sets of positively and negatively answered queries given as an input to DYNAMICHS. Let further DPI := \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R. Then, the following holds:\n(1) At any point in time during the execution of DYNAMICHS with current DPIDPI , each node in Dcalc is a minimal diagnosis w.r.t. DPI .\n(2) At any point in time during the execution of DYNAMICHS with current DPI DPI , Dcalc comprises the |Dcalc| most-probable minimal diagnoses w.r.t. DPI .\nProof. Proposition (1): At the beginning of any execution of DYNAMICHS, the set Dcalc is the empty set (line 3). So, it suffices to show that only minimal diagnoses w.r.t. DPI can be added to Dcalc during the execution of DYNAMICHS with the current DPI DPI .\nA node node can be added to Dcalc exclusively in line 13. In order for this line to be reached, by the criterion that is checked in line 12, node must be processed and labeled by valid. By Lemma 12.1, if node gets labeled by valid, then it is a diagnosis w.r.t. DPI .\nSo, assume that node is added to Dcalc where node is a non-minimal diagnosis w.r.t. DPI . Since node must have been processed and labeled by valid, the DLABEL function must have been executed\n242 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\ngiven node as an argument and must have returned in line 43. Hence, there can be no node nd \u2208 Dcalc such that nd \u2282 node holds, as otherwise DLABEL would have already returned in line 29.\nHowever, since node is a non-minimal diagnosis w.r.t. DPI there must be some minimal diagnosis D w.r.t. DPI such that D \u2282 node. Moreover, by Proposition 12.8, at any point in time before D is added to Dcalc, there must be some node nd such that nd \u2286 D and nd is an element of one of the collections (a) Dcalc, (b) DX, (c) D\u00d7, (d) D\u2283 or (e) Q. So, let us consider these cases in sequence.\nCase (a): First, nd \u2286 D and D \u2282 node implies that nd \u2282 node must be valid. As mentioned above, there can be no node in Dcalc which is a proper subset of node, contradiction.\nCase (b): In this case, nd must be also an element of Q since all nodes in DX are inserted into Q during UPDATETREE which is executed before the repeat-loop is entered, i.e. before it can come to the assumed addition of node to Dcalc which can only take place within the repeat-loop. So, in fact case (e) applies here.\nCase (c): As can be easily seen from lines 67-69 in UPDATETREE, D\u00d7 must be the empty set at the time node might be added to Dcalc by analogue argumentation as in case (b), contradiction.\nCase (d): By lines 70-78 in UPDATETREE and the fact that UPDATETREE must have been executed before the assumed addition of node to Dcalc can take place as argued in case (b), we have that there must be some node ndsub \u2208 DX such that ndsub \u2282 nd. Otherwise, nd would have been deleted from D\u2283 in line 78. By nd \u2282 node as per case (a), we deduce that ndsub \u2282 node. Due to nd \u2286 D, it must be truethat ndsub \u2286 D. Thus, we have derived that case case (b) holds for the node ndsub. By the deductions in case (b) above, we eventually know that case (e) must hold.\nThence, assumption of cases (a) and (c) is contradictory. Cases (b) and (d) imply the truth of case (e). Therefore, case (e) must occur.\nCase (e): Due to the facts that all nodes are inserted into Q in a manner that descending order of nodes in Q by pnodes() is maintained (cf. lines 23, 100 and 103) and always the first node in Q is processed next (cf. line 6), we conclude that pnodes(nd) \u2264 pnodes(node) must be valid. However, due to nd \u2286 D \u2282 node we have that nd \u2282 node. Now, by Lemma 4.14, pnodes(n) > pnodes(n\u2032) holds for any two nodes n and n\u2032 such that n \u2282 n\u2032. Therefore, pnodes(nd) > pnodes(node), contradiction.\nProposition (2): By proposition (1), each node added to Dcalc must be a minimal diagnosis w.r.t. DPI .\nAssume any point in time t during the execution of DYNAMICHS with the current DPI DPI . Then, |Dcalc| = m \u2265 0 must hold. We use induction by m to prove proposition (2).\nBase Case: Suppose that m = 0 and some minimal diagnosis D w.r.t. DPI is added to Dcalc where D is not the most probable minimal diagnosis w.r.t. DPI . This implies that D is processed and that D has the highest probability as per pnodes() among all nodes that are elements of Q at time t, as argued in the proof of proposition (1).\nLet us denote by D1 the most probable minimal diagnosis w.r.t. DPI . That is, pnodes(D1) > pnodes(D) holds.\nThen, by Proposition 12.8, at any point in time during the execution of DYNAMICHS with the current DPI DPI , there must be some node nd1 such that nd1 \u2286 D1 and nd1 is an element of one of the collections (a) Dcalc, (b) DX, (c) D\u00d7, (d) D\u2283 or (e) Q.\nCase (a) can be ruled out due to the assumption that Dcalc = \u2205. Cases (b)-(d) can be treated analogously as above in the proof of proposition (1). Hence, case (e) must hold.\nThat is, nd1 \u2208 Q at time t and nd1 is equal to or a subset of D1. As pnodes(nd1) \u2265 pnodes(D1) > pnodes(D) holds by Lemma 4.14, we can infer that D has not the highest probability as per pnodes() among all nodes that are elements of Q at time t, contradiction.\nInductive Step: Now, let m > 0 and assume that the m most probable minimal diagnoses w.r.t. DPI are already elements of Dcalc. Suppose further that some minimal diagnosis D w.r.t. DPI is added to Dcalc where D is not the (m+ 1)-th most probable minimal diagnosis w.r.t. DPI . This implies that D is processed and that D has the highest probability as per pnodes() among all nodes that are elements of Q\n12.4. ALGORITHM DETAILS AND CORRECTNESS 243\nat time t. Let us denote by Dm+1 the (m + 1)-th most probable minimal diagnosis w.r.t. DPI . That is, pnodes(Dm+1) > pnodes(D) holds since the m most probable minimal diagnoses w.r.t. DPI are already elements of Q.\nThen, by Proposition 12.8, at any point in time during the execution of DYNAMICHS with the current DPI DPI , there must be some node ndm+1 such that ndm+1 \u2286 Dm+1 and ndm+1 is an element of one of the collections (a) Dcalc, (b) DX, (c) D\u00d7, (d) D\u2283 or (e) Q.\nCase (a) can be ruled out due to proposition (1) which affirms that only minimal diagnoses w.r.t.DPI can be elements of Dcalc. As Dm+1 is not an element of Dcalc per assumption, a node ndm+1 = Dm+1 cannot be an element of Dcalc. Furthermore, by the fact that Dm+1 is a minimal diagnosis w.r.t. DPI , any node ndm+1 \u2282 Dm+1 cannot be a (minimal) diagnosis w.r.t. DPI and thus cannot be an element of Dcalc. Cases (b)-(d) can be treated analogously as above in the proof of proposition (1). Hence, case (e) must hold.\nThat is, ndm+1 \u2208 Q at time t and ndm+1 is equal to or a subset of Dm+1. As pnodes(ndm+1) \u2265 pnodes(Dm+1) > pnodes(D) holds by Lemma 4.14, we can infer that D has not the highest probability as per pnodes() among all nodes that are elements of Q at time t, contradiction."}, {"heading": "12.4.10 Correctness of DYNAMICHS", "text": "Now, we are able to prove that DYNAMICHS terminates and yields an output complying with the assertions given in Algorithm 8:\nCorollary 12.8. Any call to DYNAMICHS (given the inputs described in Algorithm 8) within Algorithm 5 terminates and yields an output \u3008Dcalc,Q,Ccalc,D\u00d7,D\u2283,Qdup\u3009 where\n(1) Dcalc is the current set of leading diagnoses such that\n(a) Dcalc \u2286 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R such that\n(i) nmin \u2264 |Dcalc| \u2264 nmax and (ii) Dcalc \\DX 6= \u2205,\nif such a set Dcalc exists; or\n(b) Dcalc is equal to the set of all minimal diagnoses mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R , otherwise;\nwhere \u201cmost-probable\u201d refers to the probability measure pnodes() given by Definition 4.9 and obtained from the function p() given as an input argument to DYNAMICHS.\n(2) Q is the current queue of open (non-labeled) nodes of the produced hitting set tree,\n(3) Ccalc is a set of conflict sets w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R,\n(4) D\u00d7 = \u2205,\n(5) D\u2283 is the set of all processed nodes so far throughout the execution of Algorithm 5 that are nonminimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and\n(6) Qdup includes a node set-equal to X for a set X \u2286 K iff\n\u2022 nd = X is a generated node that is de-facto non-redundant w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, such that, at generation time of nd, there was a node set-equal to X in Q or \u2022 there is a de-facto non-redundant node nd\u2032 = X w.r.t. the current DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032\u3009R\nwhich is a combined equal node of some generated node nd\u2032\u2032 that has been added to Qdup.\n244 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nProof. First, we prove that any call to DYNAMICHS within Algorithm 5 terminates. To this end, assume that a call to DYNAMICHS executes infinitely. That is, Q = [] must not be satisfied at any time during the execution of DYNAMICHS due to the stop criterion of DYNAMICHS in line 24.\nHowever, the overall number of nodes that might be elements of Q during the processing of the repeat-loop of any call to DYNAMICHS is finite. This is satisfied since each node nd in DYNAMICHS is a list corresponding to a subset of K and each element of the list nd.cs is a subset of K as well. For, a node can never correspond to a proper superset of K by Proposition 4.9 which says that QX(\u3008K \\ D, B,P \u222a P \u2032,N \u222a N \u2032\u3009R) returns \u2019no conflict\u2019 in case K \\ D is valid w.r.t. \u3008\u00b7,B,P \u222a P \u2032,N \u222aN \u2032\u3009R which is equivalent to D being a diagnosis w.r.t. \u3008K \\ D,B,P \u222a P \u2032,N \u222aN \u2032\u3009R by Corollary 3.3. Now, the DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R is admissible which follows from the admissibility of the input DPI \u3008K,B,P ,N \u3009R and Corollary 7.3. That D := K must be a diagnosis w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R is a direct consequence of the admissibility of \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and Definition 3.6. Therefore DLABEL must return valid for each node the latest when the node becomes set-equal to K. A node that was assigned the label valid and added to Dcalc can never be processed again during this execution of DYNAMICHS wherefore no successors of such a node can be added to Q. The same holds for some node that is labeled by nonmin and added to D\u2283.\nThence, the assumption that Q 6= [] forever implies that there is (at least) one node node that is never removed from Q.\nBy Lemma 12.12, each node that is a subset of or set-equal to a once processed node nd must have been generated before nd is processed. That is, after a node is processed, it is guaranteed that no proper subsets of it can ever be processed and no subsets of it can ever be added to Q. After a node nd is processed and is not labeled by valid or nonmin, nd is not an element of Q anymore (cf. line 7) and Q comprises a set of successor nodes of nd where each such node corresponds to a proper superset of nd (cf. line 23). Consequently, a node in Q that is processed can either be deleted whereupon no successor thereof is added to Q (in case of pruning or labeling a node by valid or nonmin) or be deleted whereupon proper supersets of it are added to Q (in case of labeling a node by a conflict set).\nA (combined) replacement of a node involves the substitution of this node by another node set-equal to it. However, there can be only finitely many possibilities to construct a replacement or combined replacement node of some node since Comb(Qdup) \u2287 Qdup also includes only nodes, i.e. finitely many elements. Therefore, each node in Q can be replaced only finitely many times.\nSince in each iteration of the repeat-loop in DYNAMICHS one node is processed, the cardinality of the nodes that are elements of Q is strictly monotonically increasing.\nAs node is supposed to be never processed, we have that in each iteration of the repeat-loop, one of the other nodes in Q must by processed. By the given argumentation, we know that after finitely many iterations, Q = [node] must be given (since all other nodes must be already pruned or labeled). Hence, node will be processed in the next iteration as GETFIRST in line 6 must catch node, contradiction.\nProposition (1): This proposition is a direct consequence of Proposition 12.9-(2) and the stop criterion of DYNAMICHS in line 24.\nProposition (2) is clear. Proposition (3) follows from Lemma 12.2 which asserts that each element of Ccalc is a minimal conflict set w.r.t. some DPI \u3008K,B,P \u222a P \u2032\u2032,N \u222aN \u2032\u2032\u3009R where P \u2032\u2032 \u2286 P \u2032 and N \u2032\u2032 \u2286 N \u2032. By Proposition 12.1, we obtain that each element of Ccalc is a conflict set w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nProposition (4): This proposition is true since UPDATETREE is called at the beginning of each execution of DYNAMICHS and all elements in D\u00d7 that have not been deleted from D\u00d7 before are deleted in lines 67-69. After UPDATETREE has finished processing, there is no other place in DYNAMICHS where nodes can be added to D\u00d7. Hence, D\u00d7 = \u2205 must hold when DYNAMICHS terminates.\nProposition (5): The elements of D\u2283 after UPDATETREE at the beginning of the execution of DYNAMICHS has returned must be non-minimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R by lines 70-78 and the fact that DX comprises only diagnoses w.r.t. the current DPI. The latter holds by\n12.4. ALGORITHM DETAILS AND CORRECTNESS 245\nlines 19 and 21 of Algorithm 5 where only diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R are added to DX. That only non-minimal diagnoses w.r.t. the current DPI can be added to D\u2283 during the execution of the repeat-loop is a simple implication of Lemma 12.1-(4).\nProposition (6) is a consequence of lines 20-21, the definition of de-facto non-redundancy (Definition 12.7) and Lemma 12.8.\n246 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nAlgorithm 8 Iterative Construction of a Dynamic Hitting Set Tree Input: a tuple \u3008\u3008K,B,P ,N \u3009R,Q,Qdup, t, nmin, nmax,Ccalc,DX,D\u00d7, p(),P \u2032,N \u2032,D\u2283\u3009 consisting of\n\u2022 the DPI \u3008K,B,P ,N \u3009R given as input to Algorithm 5, \u2022 the overall sets of positively (P \u2032) and negatively (N \u2032) answered queries added as test cases to \u3008K,B,P ,N \u3009R so far, \u2022 a queue Q of open (non-labeled) nodes, \u2022 some computation timeout t, \u2022 a desired minimal (nmin \u2265 2) and maximal (nmax) number of minimal diagnoses to be returned, \u2022 a set Ccalc of conflict sets w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, \u2022 a set DX of minimal diagnoses w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, \u2022 a set D\u00d7 of minimal diagnoses w.r.t. the last-but-one DPI that are invalidated by the most recently added test case, \u2022 a function p : K \u2192 (0, 0.5), \u2022 a set D\u2283 of non-minimal diagnoses w.r.t. the last-but-one DPI and \u2022 a set Qdup of stored (duplicate) nodes nd that can be used when it comes to constructing a replacement node of a pruned\nnode nd\u2032 \u2287 nd after tree pruning. Output: a tuple \u3008Dcalc,Q,Ccalc,D\u00d7,D\u2283\u3009 where\n\u2022 Dcalc is the current set of leading diagnoses such that (a) Dcalc \u2286 mD\u3008K,B,P\u222aP\u2032,N\u222aN \u2032\u3009R is the set of most probable minimal diagnoses w.r.t. \u3008K,B,P \u222a P\n\u2032,N \u222a N \u2032\u3009R such that (i) nmin \u2264 |Dcalc| \u2264 nmax and (ii) Dcalc \\DX 6= \u2205, if such a set Dcalc exists, or\n(b) Dcalc is equal to the set of all minimal diagnoses mD\u3008K,B,P\u222aP\u2032,N\u222aN \u2032\u3009R , otherwise,\nwhere \u201cmost-probable\u201d refers to the probability measure pnodes() (cf. Definition 4.9) obtained from the given function p();\n\u2022 Q is the current queue of open (non-labeled) nodes of the hitting set tree, \u2022 Ccalc is a set of conflict sets w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R, \u2022 D\u00d7 = \u2205, \u2022 D\u2283 is the set of all processed nodes so far throughout the execution of Algorithm 5 that are non-minimal diagnoses w.r.t. the\ncurrent DPI \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R and \u2022 Qdup includes a node set-equal to X for a set X \u2286 K iff\n\u2013 nd = X is a generated node that is de-facto non-redundant w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R, such that, at generation time of nd, there was a node set-equal to X in Q or \u2013 there is a de-facto non-redundant node nd\u2032 = X w.r.t. the current DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R which is a combined equal node of some generated node nd\u2032\u2032 that has been added to Qdup.\n1: procedure DYNAMICHS(\u3008K,B,P ,N \u3009R,Q,Qdup, t, nmin, nmax,Ccalc,DX,D\u00d7, p(),P \u2032,N \u2032,D\u2283) 2: tstart \u2190 GETTIME() 3: Dcalc \u2190 \u2205 4: \u3008Q,D\u00d7,D\u2283,Ccalc,Qdup\u3009 \u2190 UPDATETREE(\u3008K,B,P ,N \u3009R,D\u00d7,Q,Qdup,D\u2283,DX,Ccalc, p(),P \u2032,N \u2032) 5: repeat . UPDATETREE (see Algorithm 9) 6: node\u2190 GETFIRST(Q) . node is processed 7: Q\u2190 DELETEFIRST(Q) 8: if node \u2208 DX then .DX includes only minimal diagnoses w.r.t. current DPI 9: L\u2190 valid 10: else 11: \u3008L,Ccalc,Qdup\u3009 \u2190 DLABEL(\u3008K,B,P ,N \u3009R, node,Ccalc,Dcalc,Q,Qdup, p(),P \u2032,N \u2032) 12: if L = valid then . DLABEL (see Algorithm 9) 13: Dcalc \u2190 Dcalc \u222a {node} . node is a minimal diagnosis w.r.t. current DPI 14: else if L = nonmin then 15: D\u2283 \u2190 D\u2283 \u222a {node} . node is a non-minimal diagnosis w.r.t. current DPI 16: else 17: for e \u2208 L do . L is a minimal conflict set w.r.t. current DPI 18: nodee \u2190 ADD(node, e) . nodee is generated 19: nodee.cs\u2190 ADD(node.cs, L) 20: if nodee \u2208 Q then . nodee is a (set-equal) duplicate of a node in Q 21: Qdup \u2190 INSERTSORTED(nodee,Qdup, cardinality, ascending) 22: else 23: Q\u2190 INSERTSORTED(nodee,Q, pnodes(), descending) 24: until Q = [] \u2228 [Dcalc \\DX 6= \u2205 \u2227 |Dcalc| \u2265 nmin \u2227 (|Dcalc| = nmax \u2228 GETTIME()\u2212 tstart > t)] 25: return \u3008Dcalc,Q,Ccalc,D\u00d7,D\u2283,Qdup\u3009\n12.4. ALGORITHM DETAILS AND CORRECTNESS 247\nAlgorithm 9 Iterative Construction of a Dynamic Hitting Set Tree (continued) 26: procedure DLABEL(\u3008K,B,P ,N \u3009R, node,Ccalc,Dcalc,Q,Qdup, p(),P \u2032,N \u2032) . DLABEL (see page 196) 27: for nd \u2208 Dcalc do 28: if node \u2283 nd then . node is a non-minimal diagnosis 29: return \u3008nonmin,Ccalc,Qdup\u3009 30: for C \u2208 Ccalc do .Ccalc includes only conflict sets w.r.t. current DPI 31: if C \u2229 node = \u2205 then . reuse (a subset of) C to label node 32: X \u2190 QX(\u3008C,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) . Algorithm 1 (page 48) to test if C is minimal w.r.t. current DPI 33: if X = C then 34: return \u3008C,Ccalc,Qdup\u3009 35: else . X \u2282 C 36: Qdup \u2190 PRUNEQDUP(X,Qdup) . PRUNEQDUP (see Algorithm 10) 37: Q\u2190 PRUNE(X,Q,Qdup, pnodes()) . PRUNE (see Algorithm 10) 38: D\u2283 \u2190 PRUNE(X,D\u2283,Qdup, \u2205) 39: Ccalc \u2190 ADDSETDELSUPSETS(X,Ccalc) . add X to Ccalc and delete all its supersets from Ccalc 40: return \u3008X,Ccalc,Qdup\u3009 41: L\u2190 QX(\u3008K \\ node,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) . Algorithm 1 (page 48) to test if node is a diagnosis 42: if L = \u2019no conflict\u2019 then . node is a diagnosis 43: return \u3008valid,Ccalc,Qdup\u3009 44: else . L is a new minimal conflict set (/\u2208 Ccalc) 45: Ccalc \u2190 Ccalc \u222a {L} 46: return \u3008L,Ccalc,Qdup\u3009\n47: procedure UPDATETREE(\u3008K,B,P ,N \u3009R,D\u00d7,Q,Qdup,D\u2283,DX,Ccalc, p(),P \u2032,N \u2032) 48: for nd \u2208 D\u00d7 do 49: quickRC, completeRC \u2190 false 50: X \u2190 QX(\u3008Und.cs \\ nd,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) . QRC begin 51: for C \u2208 nd.cs do 52: if X \u2282 C then . QRC (see page 205) 53: quickRC \u2190 true 54: break . QRC end 55: if quickRC = false then . CRC begin 56: for i\u2190 1, . . . , |nd| do 57: X \u2190 QX(\u3008nd.cs[i] \\ {nd[i]} ,B,P \u222a P \u2032,N \u222aN \u2032\u3009R) . CRC (see page 206) 58: if X 6= \u2019no conflict\u2019 then 59: completeRC \u2190 true 60: break . CRC end 61: if quickRC = true \u2228 completeRC = true then . condition true iff nd redundant w.r.t. current DPI 62: Qdup \u2190 PRUNEQDUP(X,Qdup) . PRUNEQDUP (see Algorithm 10) 63: Q\u2190 PRUNE(X,Q,Qdup, pnodes()) . PRUNE (see Algorithm 10) 64: D\u00d7 \u2190 PRUNE(X,D\u00d7,Qdup, \u2205) 65: D\u2283 \u2190 PRUNE(X,D\u2283,Qdup, \u2205) 66: Ccalc \u2190 ADDSETDELSUPSETS(X,Ccalc) . add X to Ccalc and delete all its supersets from Ccalc 67: for nd \u2208 D\u00d7 do . add all (non-pruned) nodes in D\u00d7 to Q 68: Q\u2190 INSERTSORTED(nd,Q, pnodes(), descending) 69: D\u00d7 \u2190 D\u00d7 \\ {nd} 70: for nd \u2208 D\u2283 do . update D\u2283: add all nodes to Q which are not proper supersets of a diagnosis in DX 71: nonmin\u2190 false 72: for nd\u2032 \u2208 DX do 73: if nd \u2283 nd\u2032 then 74: nonmin\u2190 true 75: break 76: if nonmin = false then 77: Q\u2190 INSERTSORTED(nd,Q, pnodes(), descending) 78: D\u2283 \u2190 D\u2283 \\ {nd} 79: for D \u2208 DX do . reinsert known minimal diagnoses to Q to find diagnoses in order of descending pnodes() 80: Q\u2190 INSERTSORTED(D,Q, pnodes(), descending) 81: return \u3008Q,D\u00d7,D\u2283,Ccalc,Qdup\u3009\n248 CHAPTER 12. DYNAMIC DIAGNOSIS COMPUTATION ALGORITHM\nAlgorithm 10 Iterative Construction of a Dynamic Hitting Set Tree (continued) 82: procedure PRUNE(X,S,Dup, sort_measure) . PRUNE (see page 212) 83: if S is a list then 84: S\u2032 \u2190 [] 85: else 86: S\u2032 \u2190 \u2205 87: for nd \u2208 S do 88: k \u2190 0 89: for i = 1 to |nd.cs| do 90: if nd.cs[i] \u2283 X then . check first redundancy criterion (Definition 12.4 on page 202) 91: if nd[i] \u2208 nd.cs[i] \\X then . check second redundancy criterion (Definition 12.4 on page 202) 92: k \u2190 i 93: else 94: nd.cs[i]\u2190 X . replace each superset of X in nd.cs by X 95: if k > 0 then . nd is redundant 96: for node\u2190 Dup[1], . . . , Dup[|Dup|] do 97: if |node| \u2265 k \u2227 nd[1..|node|] = node then 98: ndnew \u2190 ADD(node, nd[|node|+ 1..|nd|]) . construct replacement node ndnew of nd 99: ndnew.cs\u2190 ADD(node.cs, nd.cs[|node|+ 1..|nd|]) 100: S\u2032 \u2190 INSERTSORTED(ndnew, S\u2032, sort_measure, descending) 101: break 102: else . X is not a witness of redundancy of nd 103: S\u2032 \u2190 INSERTSORTED(nd, S\u2032, sort_measure, descending) 104: return S\u2032\n105: procedure PRUNEQDUP(X,Dup) . PRUNEQDUP (see page 209) 106: Dupnew \u2190 [] 107: for i\u2190 1 to |Dup| do 108: ndi\u2190 Dup[i] 109: k \u2190 0 110: for m\u2190 1 to |ndi.cs| do 111: if ndi.cs[m] \u2283 X then . check first redundancy criterion (Definition 12.4 on page 202) 112: if ndi[m] \u2208 ndi.cs[m] \\X then . check second redundancy criterion (Definition 12.4 on page 202) 113: k \u2190 m 114: else 115: ndi.cs[m]\u2190 X . replace each superset of X in ndi.cs by X 116: if k > 0 then . ndi is redundant 117: for ndj \u2208 Dupnew do 118: if |ndj| \u2265 k \u2227 ndi[1..|ndj|] = ndj then 119: ndinew \u2190 ADD(ndj, ndi[|ndj|+ 1..|ndi|]) . construct combined replacement node ndinew of ndi 120: ndinew.cs\u2190 ADD(ndj.cs, ndi.cs[|ndj|+ 1..|ndi|]) 121: Dupnew \u2190 INSERTSORTED(ndinew, Dupnew, cardinality, ascending) 122: break 123: else . X is not a witness of redundancy of ndi 124: Dupnew \u2190 INSERTSORTED(ndi, Dupnew, cardinality, ascending) 125: return Dupnew\nChapter 13\nDiscussion of Iterative Diagnosis Computation\nIn this chapter we want to summarize properties of and differences between STATICHS and DYNAMICHS that we already pointed out in previous sections and, additionally, we want to shed light on some further interesting aspects of these iterative diagnosis computation methods in the scope of interactive KB debugging (Algorithm 5). Table 13.1 provides an overview of what we did discuss or will discuss below.\nFirst Segment of Table 13.1 \u2013 Addressed Problem and Properties w.r.t. Solutions. The first row of the table has been proven by Proposition 9.1 on page 124. Results given by the second up to the fourth row of the table are substantiated by Proposition 11.1 (STATICHS) and Corollary 12.8 (DYNAMICHS). We have discussed in Section 11.1 that Algorithm 5 with mode = static can artificially fix the search space for possible solutions initially. This is an inherent property of the Interactive Static KB Debugging Problem which the algorithm aims to solve in static mode. For, a minimal diagnosis w.r.t. the input DPI which satisfies all answered queries added as test cases throughout the debugging session must be detected (see left column of category \u201cdiagnoses\u201d in Table 13.1). Hence, the solution space is given by |mDinputDPI |. \u201cInitially fixed search space\u201d in this case means that, given the fault tolerance \u03c3 = 0, Algorithm 5 in static mode must compute all minimal diagnoses w.r.t. the input DPI, i.e. the entire set mDinputDPI . In case of dynamic mode, on the other hand, the solution space (i.e. minimal diagnoses w.r.t. the current DPI, see right column of Table 13.1 in category \u201cdiagnoses\u201d) that needs to be explored by Algorithm 5 for a given value of zero for \u03c3 is not known in advance. It rather depends on which test cases are specified or, respectively, which queries the user is asked. In case of the usage of mainly \u201cpositive-impact queries\u201d, the search space might have significantly smaller cardinality than mDinputDPI whereas it might grow significantly beyond the cardinality of mDinputDPI in a scenario where many unfavorable \u201cnegativeimpact queries\u201d are generated (cf. Section 12.1). The maximum theoretically possible cardinality of the search space for DYNAMICHS is given by |aDinputDPI | due to Corollary 12.4.\nSecond Segment of Table 13.1 \u2013 Impact of New Test Cases and Computation Focus. The properties given in the category \u201ccomputes\u201d in Table 13.1 are confirmed by Proposition 11.1 (STATICHS) and Corollary 12.8 (DYNAMICHS). Hence, other than DYNAMICHS which analyzes the current DPI in terms of minimal conflict sets and diagnoses in each iteration, STATICHS must only consider minimal conflict sets w.r.t. the input DPI (see categories \u201cdiagnoses\u201d and \u201cconflict sets\u201d in Table 13.1). This is sufficient for the exploration of all minimal diagnoses w.r.t. the input DPI by Proposition 4.6. In this vein, new test cases in static KB debugging are not taken into account in the computation of minimal\n249\n250 CHAPTER 13. DISCUSSION OF ITERATIVE DIAGNOSIS COMPUTATION\nconflict sets. Instead, new test cases are just exploited to invalidate already computed minimal diagnoses w.r.t. the input DPI. Thus, test cases specified during static KB debugging are treated somewhat inferior to test cases already present in the input DPI. Because, the newly gained information given by these test cases is not utilized to reveal new faults in the KB or to lay the focus on just the now relevant parts of existing faults, but only for the purpose of constraining the search space for minimal diagnoses w.r.t. the input DPI \u3008K,B,P ,N \u3009R. We might thus call test cases added during the execution of Algorithm 5 with mode = static pure differentiation test cases (see category \u201cpurpose of test cases\u201d in Table 13.1).\nOf course, seen from the point of view of a current DPI, i.e. the input DPI extended by differentiation test cases, STATICHS does not guarantee completeness w.r.t. this current DPI, but only w.r.t. the initial one. This however does not mean that, after the (exact) solution K\u2217 := (K \\ D) \u222a UP of the Interactive Static KB Debugging problem has been localized by means of STATICHS, the differentiation test cases (P \u2032 and N \u2032) cannot be simply added to the DPI. In this case, K\u2217 is still a maximal solution KB w.r.t. the extended input DPI \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R. In other words, there is no conflict set (and thus no diagnosis) w.r.t. \u3008K\\D,B,P \u222aP \u2032,N \u222aN \u2032\u3009R andK\\D is valid w.r.t. \u3008\u00b7,B,P \u222aP \u2032,N \u222aN \u2032\u3009R. However, in spite of using the (exact) solution KB of the Interactive Static KB Debugging problem, it is not ensured that this solution is the optimal one w.r.t. the extended DPI, i.e. of the Interactive Dynamic KB Debugging problem. This is because user interaction is just exploited to the extent that the best solution w.r.t. the input DPI is crystallized out. It is not used to have the solution verified by the user in the light of the extended DPI.\nOn the other hand, test cases assigned throughout dynamic KB debugging by means of Algorithm 5 with mode = dynamic are treated equally as test cases already given in the input DPI. They are used to prune the search space and to pinpoint new faults that arise from added test cases resulting from answered queries. The dynamic algorithm assists the user in filtering out a solution and verifying in a thorough manner that this solution is the desired one w.r.t. the extended DPI, among all existing solutions w.r.t. the extended DPI. Due to these aspects we might regard Algorithm 5 with mode mode = dynamic as the standard method for Interactive KB Debugging.\nIn Sections 11.1, 12.1, 12.4.3 and 12.4.4 we have thoroughly investigated the impact of new test cases (answered queries) added to the DPI on the set of minimal (all) diagnoses and the set of minimal conflict sets considered by the respective method STATICHS or DYNAMICHS. For the former, we have shown that (for arbitrary iteration i of Algorithm 5) mDi \u2283 mDi+1 and aDi \u2283 aDi+1 where mDi and aDi denote the set of all minimal diagnoses and the set of all diagnoses, respectively, that are relevant (for the DPI considered) during iteration i. That is, the set of minimal as well as the set of all diagnoses (w.r.t. the input DPI) is reduced to a proper subset after a new test case has been added. For the latter, (for arbitrary iteration i of Algorithm 5) we have argued that generally mDi 6\u2283 mDi+1, but still aDi \u2283 aDi+1, where mDi and aDi are defined as above. That is, not only might some minimal diagnoses (w.r.t. the last-but-one DPI) be invalidated, but also some new ones (w.r.t. the current DPI) might originate from the incorporation of the information given by a query answer.\nConcerning minimal conflict sets, the set of all (or: relevant) minimal conflict sets does not change throughout a debugging session by means of STATICHS, i.e. mCi = mCi+1 (for arbitrary iteration i of Algorithm 5) where mCi is the set of minimal conflict sets relevant (for the DPI considered) during iteration i. This holds since the minimal conflict sets w.r.t. the input DPI are artificially fixed (see above). On the contrary, the assignment of a new test case using DYNAMICHS involves the reduction of some minimal conflict sets (w.r.t. the last-but-one DPI) to smaller subset conflict sets (w.r.t. the current DPI) and/or the introduction of some \u201ccompletely new\u201d minimal conflict sets (which are in no subset-relation with existing ones, cf. Section 12.1). These results are summarized by the categories \u201cset of all X upon addition of a test case\u201d in Table 13.1.\nThird Segment of Table 13.1 \u2013 Hitting Set Tree Construction, Pruning and Complexity. Regarding the constructed hitting set tree, we have explained that STATICHS builds a wpHS-tree (see Definition 4.10\n251\non page 74 and the argumentation in Section 11.4) just as the HS method which is employed for diagnosis computation in the presented non-interactive KB debugging scenario (Algorithm 3). The main differences between Algorithm 5 in static mode and Algorithm 3 are, first, that the former constructs the wpHS-tree step-by-step in multiple phases. Between each two phases a query is generated and presented to the user. The latter, by contrast, finishes the tree construction (to the extent as prescribed by the given parameters nmin, nmax and t, see Section 4.7) before a single most probable automatically selected solution or a set of solutions is displayed to the user. Second, the tree constructed by the interactive static algorithm exhibits a different labeling of leaf nodes than the one built up be the non-interactive algorithm. In the former, some leaf nodes might be labeled by \u00d7 indicating that the path to this node is a minimal diagnosis w.r.t. the input DPI, but one which is not in accordance with all answered queries. Notice that such invalidated diagnoses cannot be simply deleted in favor of memory savings, but must be stored in order for the nonminimality criterion (lines 21-23) to function properly which is necessary to preserve the property of STATICHS to compute only minimal diagnoses (cf. Lemma 11.7). In the non-interactive wpHS-tree, on the other hand, all minimal diagnoses w.r.t. the input DPI are labeled by X.\nWhat the interactive static and the non-interactive tree have in common is the usage of only minimal conflict sets w.r.t. the input DPI as labels of internal (i.e. non-leaf) nodes and the adherence to the \u201cstandard\u201d pruning rules [Rei87] as per Definition 4.8 on page 59, i.e. the immediate deletion of non-minimal and duplicate tree paths. Except for the standard pruning actions that take place during tree expansion, no separate pruning phases are performed by STATICHS. The reason for this is the fixation of the minimal conflict sets, i.e. the consideration of only minimal conflict sets w.r.t. the input DPI. Incorporation of new minimal conflict sets resulting from answered queries would generally negate completeness of STATICHS w.r.t. the exploration of all minimal diagnoses w.r.t. the input DPI. Integration of new conflict sets that are subsets of existing ones, however, is the key to more substantial pruning actions carried out by DYNAMICHS.\nDue to the more or less equivalent construction of both the tree built up by STATICHS and the one constructed by the HS method in the non-interactive algorithm, it is straightforward to recognize that the worst case time and space complexity of both tree computations (without taking into the account other actions performed by the interactive algorithm like probability updates and query generations) are equal. By worst case complexity we refer to the complexity of the search for the (exact) solution of the Interactive Static KB Debugging Problem on the one hand and the complexity of enumerating all minimal diagnoses w.r.t. the input DPI on the other hand. In particular, the complexity of tree construction in static KB debugging is independent of given parameters such as the ones for leading diagnoses computation (nmin, nmax and t) and of the test cases that are classified positively or negatively, respectively, during the debugging session.\nTo sum up, due to the artificial fixation of the solution set, there is no possibility of tree pruning in static KB debugging except for the standard pruning rules and hence no way to escape the generally immense worst case complexity for diagnosis search in case \u03c3 = 0.\nThe hitting set tree constructed by DYNAMICHS, on the other hand, might differ significantly from the wpHS-tree produced by the non-interactive algorithm. First, it uses minimal conflict sets w.r.t. the current DPI to label internal nodes in the tree during each expansion stage. Since minimal conflict sets can only \u201cshrink\u201d and not \u201cgrow\u201d due to the integration of test cases into a DPI as stated by Proposition 12.1, the finding that by now a subset of a former minimal conflict set (w.r.t. some previous DPI) is already a minimal conflict set (w.r.t. the current DPI) gives rise to very powerful ways of tree pruning, as we detailed in Section 12.4.6 and illustrated by Example 12.2. In this vein, the evolution of the tree produced by DYNAMICHS can be characterized by alternating expansion and pruning stages. A pruning stage takes place after a test case has been added to the last-but-one DPI in order to modify the tree Ti used to search for minimal diagnoses w.r.t. the last-but-one DPI to obtain a tree Ti+1 that enables the discovery of all minimal diagnoses w.r.t. the current DPI. Concretely, both pre-pruning as well as post-pruning is possible during a pruning phase. Pre-pruning refers to the deletion of tree paths ending in an open leaf node, i.e.\n252 CHAPTER 13. DISCUSSION OF ITERATIVE DIAGNOSIS COMPUTATION\npaths corresponding to partial diagnoses, and post-pruning refers to the deletion of tree paths ending in a closed node, i.e. paths corresponding to (minimal or non-minimal) diagnoses. Both pre- and post-pruning are not possible in STATICHS. The ability for significant tree pruning comes at the cost of not being able to exploit the standard pruning rules as STATICHS does. For, non-minimal diagnoses and duplicate tree paths must be stored to guarantee the proper working of tree pruning and in further consequence the completeness of minimal diagnoses search for each current DPI (see Section 12.4).\nAs we pointed out in Section 12.1, the test cases specified during the dynamic debugging session and the defined leading diagnoses computation parameters nmin, nmax and t might have a material influence on the extent of possible tree pruning on the one hand and the extent of undesired tree growth on the other. Thence, worst case time and space complexity of the tree generation by means of DYNAMICHS cannot be initially (at least theoretically) quantified as in the case of STATICHS. Consequently, significant savings as well as a substantial overhead compared to STATICHS are possible. Careful \u201ccontrol\u201d of certain properties of asked queries (added test cases) might help to keep considerable unwanted tree growth within bounds, as we touched upon in Section 12.1 and will elaborate on in future work.\nNevertheless, we want to mention a shortcoming of STATICHS compared to DYNAMICHS. Namely, for \u03c3 = 0, STATICHS must enumerate all minimal diagnoses w.r.t. the input DPI (otherwise no diagnosis can have a probability of 1, see the proof of Proposition 9.1 in Section 9.4) whereas DYNAMICHS might be able to obtain some extended DPI (by the addition of test cases) soon for which only one minimal diagnosis exists. This might require the computation of only a small fraction of the number of |mDinputDPI | minimal diagnoses that STATICHS must determine and therefore might be substantially more time and space saving than figuring out all minimal diagnoses w.r.t. some DPI. This is quite well illustrated by Examples 11.2 and 12.2.\nFourth Segment of Table 13.1 \u2013 Query Generation and Bias. We explained in Remark 11.2 on page 153 that queries in STATICHS are computed w.r.t. the current DPI albeit only minimal diagnoses w.r.t. the input DPI (which are at the same time minimal diagnoses w.r.t. the current DPI, cf. bullet (a) on page 128) are considered and calculated by Algorithm 5 with mode = static. In the case of dynamic debugging it is clear that queries are computed w.r.t. the current DPI since only minimal diagnoses w.r.t. the current DPI are taken into account.\nAnother important property of an interactive KB debugging algorithm is whether it is biased or unbiased. Intuitively, we call an interactive KB debugging algorithm biased w.r.t. some current DPI DPI encountered during its execution iff there might be a minimal diagnosis D w.r.t. DPI such that D might be definitely invalidated independently of the answers a user gives. In other words, an interactive KB debugging algorithm is unbiased iff for each minimal diagnosis D w.r.t. DPI there is a set QAD including query answer-pairs such that the addition of the positive queries inQAD to the positive test cases ofDPI and the addition of the negative queries inQAD to the negative test cases ofDPI yields an extended DPI DPI \u2032 such that D is the only minimal diagnosis w.r.t. DPI \u2032. This means that unbiasedness implies that any solution w.r.t. any encountered current DPI during the debugging session might be found as the finally remaining (exact) solution diagnosis. So, all solutions are treated equitably by an unbiased algorithm and only the user may decide by their given answers which solutions are and which are not ruled out.\nMore formally, we define unbiasedness of an interactive KB debugging algorithm as follows:\nDefinition 13.1. Let \u3008K,B,P ,N \u3009R be the input DPI given to an algorithm AlgX that solves the Interactive X Debugging Problem for X \u2208 {static, dynamic}. Let P \u2032 \u2287 \u2205 and N \u2032 \u2287 \u2205 be the sets of test cases specified so far during the execution of AlgX and let D \u2286 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R be the current set of leading diagnoses. Then, we call AlgX biased w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R iff there is a diagnosis D \u2208 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R and a query Q \u2208 QD,\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R such that D /\u2208mD\u3008K,B,P\u222aP \u2032\u222a{Q},N\u222aN \u2032\u3009R and D /\u2208mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u222a{Q}\u3009R .\nFurther, we call AlgX unbiased iff there cannot be any sets of test cases P \u2032 \u2287 \u2205 and N \u2032 \u2287 \u2205 during\n253\nany execution of AlgX such that AlgX is biased w.r.t. \u3008K,B,P \u222a P \u2032,N \u222aN \u2032\u3009R.\nRemark 13.1 It is important to notice the difference between completeness (which has already been established for Algorithm 5 using any of the methods STATICHS or DYNAMICHS, see Lemma 11.5 and Proposition 12.8) and unbiasedness of an algorithm. Completeness refers to the guarantee that the algorithm explores all minimal diagnoses w.r.t. any DPI DPI . However, it does not say anything about what might happen after a new test case Q is added to DPI . Although it does state that all minimal diagnoses w.r.t. the new DPI DPI \u2032 are explored, it leaves us unclear about what effect the addition of the queryQ to the test cases might have had on the minimal diagnoses. So, there might be a minimal diagnosis w.r.t. DPI that would have been ruled out by both answers to Q thereby violating unbiasedness, but not completeness. To sum up, completeness gives us guarantees about what happens during the diagnosis computation phase whereas unbiasedness gives us guarantees about what happens during the transition from one DPI to a new DPI.\nIn the following, we show that Algorithm 5 in both static and dynamic mode is unbiased.\nProposition 13.1. Assume the execution of Algorithm 5 with mode \u2208 {static, dynamic} given the input DPI \u3008K,B,P ,N \u3009R. Further, let D := Dcalc be the set of minimal diagnoses w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R returned by a call of DYNAMICHS in case of mode = dynamic and D := Dcalc \u222aDX be the set of minimal diagnoses w.r.t. \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R returned by a call of STATICHS in case of mode = static. Moreover, let D \u2208mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R .\nThen, no query Q w.r.t. D and \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R can be computed by Algorithm 5 such that D /\u2208mD\u3008K,B,P\u222aP \u2032\u222a{Q},N\u222aN \u2032\u3009R and D /\u2208mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u222a{Q}\u3009R .\nProof. Let us consider the q-partition P(Q) = \u2329 D+(Q),D\u2212(Q),D0(Q) \u232a of the query Q that is computed by Algorithm 5 for the set of leading diagnoses D. By Proposition 7.1, we have that D+(Q) \u222a D\u2212(Q) \u222aD0(Q) = D and D+(Q), D\u2212(Q) and D0(Q) are pairwise disjoint sets, i.e. the sets D+(Q), D\u2212(Q) and D0(Q) constitute a partition of the set D. Let us now assume that each diagnosis in mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is assigned to its respective set in P(Q) as per Definition 7.2 yielding the tuple \u2329 D+m(Q),D \u2212 m(Q),D 0 m(Q) \u232a where D+m(Q)\u222aD\u2212m(Q)\u222aD0m(Q) = mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R . Then, by analogue argumentation as in the proof of Proposition 7.1, we obtain that D+m(Q), D \u2212 m(Q) and D 0 m(Q)\nare pairwise disjoint sets. That is, \u2329 D+m(Q),D \u2212 m(Q),D 0 m(Q) \u232a is the (extended) q-partition of Q w.r.t. the leading diagnoses set mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R . By Remark 7.4, we have that Dpos := D+m(Q) \u222a D0m(Q) are minimal diagnoses w.r.t. the DPI \u3008K,B,P \u222a P \u2032 \u222a {Q} ,N \u222a N \u2032\u3009R (positive answer u(Q)) and Dneg := D\u2212m(Q) \u222aD0m(Q) are minimal diagnoses w.r.t. the DPI \u3008K,B,P \u222aP \u2032,N \u222aN \u2032 \u222a {Q}\u3009R (negative answer u(Q)). Since Dpos \u222aDneg \u2287 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R , we have that each diagnosis in mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R is either in Dpos or in Dneg (or in both). Hence, for each diagnosis D \u2208 mD\u3008K,B,P\u222aP \u2032,N\u222aN \u2032\u3009R there is some answer u(Q) \u2208 {true, false} to the query Q such that D is a diagnosis w.r.t. the DPI resulting from \u3008K,B,P \u222a P \u2032,N \u222a N \u2032\u3009R by addition of the new test caseQ to the respective set (P \u222aP \u2032 for positive and N \u222aN \u2032 for negative answer). Consequently, the claimed proposition holds.\nCorollary 13.1. Algorithm 5 with mode \u2208 {static, dynamic} is unbiased for any given input DPI \u3008K,B,P ,N \u3009R.\n254 CHAPTER 13. DISCUSSION OF ITERATIVE DIAGNOSIS COMPUTATION\nPart IV\nTwo Query Strategies for Efficient Fault Localization in Interactive\nOntology Debugging\n255\n257\nIn this part, we suggest and extensively analyze different methods for the selection of an \u201coptimal\u201d query. The material dealt with in Part IV is based on the publications [SFFR12, SF10] where the former was published in the journal Web Semantics: Science, Services and Agents on the World Wide Web and the latter in the Proceedings of the 9th International Semantic Web Conference (ISWC 2010).\nChapter 14\nIntroduction to the Problem\nOntology acquisition and maintenance are important prerequisites for the successful application of semantic systems in areas such as the Semantic Web. However, as state of the art ontology extraction methods cannot automatically acquire ontologies in a complete and error-free fashion, users of such systems must formulate and correct logical descriptions on their own. In most of the cases these users are domain experts who have little or no experience in expressing knowledge in representation languages like OWL 2 DL [GHM+08]. Studies in cognitive psychology, e.g. [CP71, JL99], indicate that humans make systematic errors while formulating or interpreting logical descriptions, with the results presented in [RDH+04, RCVB09] confirming that these observations also apply to ontology development. Moreover, the problem gets even more if an ontology is developed by a group of users, such as OBO Foundry29 or NCI Thesaurus30, is based on a set of imported third-party ontologies, etc. In this case inconsistencies might appear if some user does not understand or accept the context in which shared ontological descriptions are used. Therefore, identification of erroneous ontological definitions is a difficult and time-consuming task.\nSeveral ontology debugging methods [SHCH07, KPHS07, FS05, HPS08] were proposed to simplify ontology development and maintenance. Usually the main aim of debugging is to obtain a consistent and, optionally, coherent ontology. These basic requirements can be extended with additional ones, such as test cases [FS05], which must be fulfilled by the target ontology Ot. Any ontology that does not fulfill the requirements is faulty regardless of how it was created. For instance, an ontology might be created by an expert specializing descriptions of the imported ontologies (top-down) or by an inductive learning algorithm from a set of examples (bottom-up).\nNote that even if all requirements are completely specified, many logically equivalent target ontologies might exist. They may differ in aspects such as the complexity of consistency checks, size or readability. However, selecting between logically equivalent theories based on such measures is out of the scope of this work. Furthermore, although target ontologies may evolve as requirements change over time, we assume that the target ontology remains stable throughout a debugging session.\nGiven an set of requirements (e.g. formulated by a user) and a faulty ontology, the task of an ontology debugger is to identify the set of alternative diagnoses, where each diagnosis corresponds to a set of possibly faulty axioms. More concretely, a diagnosisD is a subset of an ontologyO such that one should remove (change) all the axioms of a diagnosis from the ontology (i.e. O \\ D) in order to formulate an ontology O\u2032 that fulfills all the given requirements. Only if the set of requirements is complete the only possible ontology O\u2032 corresponds to the target ontology Ot. In the following we refer to the removal of a diagnosis from the ontology as a trivial application of a diagnosis. Moreover, in practical applica-\n29http://www.obofoundry.org 30http://ncit.nci.nih.gov\n259\n260 CHAPTER 14. INTRODUCTION TO THE PROBLEM\ntions it might be inefficient to consider all possible diagnoses. Therefore, modern ontology debugging approaches focus on the computation of minimal diagnoses. A set of axioms Di is a minimal diagnosis iff there is no proper subset D\u2032i \u2282 Di which is a diagnosis. Thus, minimal diagnoses constitute minimal required changes to the ontology.\nApplication of diagnosis methods can be problematic in the cases for which many alternative minimal diagnoses exist for a given set of test cases and requirements. A sample study of real-world incoherent ontologies, which were used in [KPHS07], showed that hundreds or even thousands of minimal diagnoses may exist. In the case of the Transportation ontology the diagnosis method was able to identify 1782 minimal diagnoses 31. In such situations a simple visualization of all alternative sets of modifications to the ontology is ineffective. Thus an efficient debugging method should be able to discriminate between the diagnoses in order to select the target diagnosisDt. Trivial application ofDt to the ontologyO allows a user to extend (O \\ Dt) with a set of additional axioms EX and, thus, to formulate the target ontology Ot, i.e. Ot = (O \\ Dt) \u222a EX .\nOne possible solution to the diagnosis discrimination problem would be to order the set of diagnoses by various preference criteria. For instance, Kalyanpur et al. [KPSCG06] suggest a measure to rank the axioms of a diagnosis depending on their structure, usage in test cases, provenance, and impact in terms of entailments. Only the top ranking diagnoses are then presented to the user. Of course this set of diagnoses will contain the target diagnosis only in cases where the faulty ontology, the given requirements and test cases provide sufficient data to the appropriate heuristic. However, it is difficult to identify which information, e.g. test cases, is really required to identify the target diagnosis. That is, a user does not know a priori which and how many tests should be provided to the debugger to ensure that it will return the target diagnosis.\nIn this part we present an approach for the acquisition of additional information by generating a sequence of queries, the answers of which can be used to reduce the set of diagnoses and ultimately identify the target diagnosis. These queries should be answered by an oracle such as a user or an information extraction system. In order to construct queries we exploit the property that different ontologies resulting from trivial applications of different diagnoses entail unequal sets of axioms. Consequently, we can differentiate between diagnoses by asking the oracle if the target ontology should entail a set of logical sentences or not. These entailed logical sentences can be generated by the classification and realization services provided in description logic reasoning systems [SPG+07, HM01, MSH09]. In particular, the classification process computes a subsumption hierarchy (sometimes also called \u201cinheritance hierarchy\u201d of parents and children) for each concept description mentioned in a TBox. For each individual mentioned in an ABox, the realization computes all the concept names of which the individual is an instance [SPG+07].\nWe propose two methods for selecting the next query of the set of possible queries: The first method employs a greedy approach that selects queries which try to cut the number of diagnoses in half. The second method exploits the fact that some diagnoses are more likely than others because of typical user errors [RDH+04, RCVB09]. Beliefs for an error to occur in a given part of a knowledge base, represented as a probability, can be used to estimate the change in entropy of the set of diagnoses if a particular query is answered. In our evaluation the fault probabilities of axioms are estimated by the type and number of the logical operators employed. For example, roughly speaking, the greater the number of logical operators and the more complex these operators are, the greater the fault probability of an axiom. For assigning prior fault probabilities to diagnoses we employ the fault probabilities of axioms. Of course other methods for guessing prior fault probabilities, e.g. based on context of concept descriptions, measures suggested in the previous work [KPSCG06], etc., can be easily integrated in our framework. Given a set of diagnoses and their probabilities the method selects a query which minimizes the expected entropy of a set of diagnoses after an oracle answers a query, i.e. maximizes the information gain. An oracle should answer such queries until a diagnosis is identified whose probability is significantly higher than those of all other\n31In Chapter 18, we will give a detailed characterization of these ontologies.\n261\ndiagnoses. This diagnosis is most likely to be the target diagnosis. In the first evaluation scenario we compare the performance of both methods in terms of the number of queries needed to identify the target diagnosis. The evaluation is performed using generated examples as well as real-world ontologies presented in Tables 18.1 and 18.5. In the first case we alter a consistent and coherent ontology with additional axioms to generate conflicts that result in a predefined number of diagnoses of a required length. Each faulty ontology is then analyzed by the debugging algorithm using entropy, greedy and \u201crandom\u201d strategies, where the latter selects queries at random. The evaluation results show that in some cases the entropy-based approach is almost 60% better than the greedy one whereas both approaches clearly outperformed the random strategy.\nIn the second evaluation scenario we investigate the robustness of the entropy-based strategy with respect to variations in the prior fault probabilities. We analyze the performance of entropy-based and greedy strategies on real-world ontologies by simulating different types of prior fault probability distributions as well as the \u201cquality\u201d of these probabilities that might occur in practice. In particular, we identify the cases where all prior fault probabilities are (1) equal, (2) \u201cmoderately\u201d varied or (3) \u201cextremely\u201d varied. Regarding the \u201cquality\u201d of the probabilities we investigate cases where the guesses based on the prior diagnosis probabilities are good, average or bad. The results show that the entropy method outperforms \u201csplit-in-half\u201d in almost all of the cases, namely when the target diagnosis is located in the more likely two thirds of the minimal diagnoses. In some situations the entropy-based approach achieves even twice the performance of the greedy one. Only in cases where the initial guess of the prior probabilities is very vague (the bad case), and the number of queries needed to identify the target diagnosis is low, \u201csplit-inhalf\u201d may save on average one query. However, if the number of queries increases, the performance of the entropy-based query selection increases compared to the \u201csplit-in-half\u201d strategy. We observed that if the number of queries is greater than 10, the entropy-based method is preferable even if the initial guess of the prior probabilities is bad. This is due to the effect that the initial bad guesses are improved by the Bayes-update of the diagnoses probabilities as well as an ability of the entropy-based method to stop in the cases when a probability of some diagnosis is above an acceptance threshold predefined by the user. Consequently, entropy-based query selection is robust enough to handle different prior fault probability distributions.\nAdditional experiments performed on big real-world ontologies demonstrate the scalability of the suggested approach. In our experiments we were able to identify the target diagnosis in an ontology with over 33000 axioms using entropy-based query selection in only 190 seconds using an average of five queries.\nThe remainder of Part IV is organized as follows: Chapter 15 presents two introductory examples as well as the basic concepts. The details of the entropy-based query selection method are given in Chapter 16. Chapter 17 describes the implementation of the approach and is followed by evaluation results in Chapter 18. An overview of related work is given in Chapter 19 and conclusions are drawn in Chapter 20.\nChapter 15\nMotivating Examples and Basic Concepts\nWe begin by presenting the fundamentals of ontology diagnosis and then show how queries and answers can be generated and employed to differentiate between sets of diagnoses.\nDescription Logics Since the underlying knowledge representation method of ontologies in the Semantic Web is based on description logics, we start by briefly introducing the main concepts, employing the usual definitions as in [Bor96, Baa03]. A knowledge base is comprised of two components, namely a TBox (denoted by T ) and a ABox (A). The TBox defines the terminology whereas the ABox contains assertions about named individuals in terms of the vocabulary defined in the TBox. The vocabulary consists of concepts, denoting sets of individuals, and roles, denoting binary relationships between individuals. These concepts and roles may be either atomic or complex, the latter being obtained by employing description operators. The language of descriptions is defined recursively by starting from a schema S = (CN ,RN , IN ) of disjoint sets of names for concepts, roles, and individuals. Typical operators for the construction of complex descriptions are CtD (disjunction), CuD (conjunction), \u00acC (negation), \u2200R.C (concept value restriction), and \u2203R.C(concept exists restriction), where C and D are elements of CN and R \u2208 RN .\nKnowledge bases are defined by a finite set of logical sentences. Sentences regarding the TBox are called terminological axioms whereas sentences regarding the ABox are called assertional axioms. Terminological axioms are expressed by C v D (Generalized Concept Inclusion) which corresponds to the logical implication. Let a, b \u2208 IN be individual names. C(a) and R(a, b) are thus assertional axioms.\nConcepts (rsp. roles) can be regarded as unary (rsp. binary) predicates. Roughly speaking description logics can be seen as fragments of first-order predicate logic (without considering transitive closure or special fixpoint semantics). These fragments are specifically designed to ensure decidability or favorable computational costs.\nThe semantics of description terms are usually given using an interpretation I = \u3008\u2206I , (\u00b7)I\u3009, where \u2206I is a domain (non-empty universe) of values, and (\u00b7)I is a function that maps every concept description to a subset of \u2206I , and every role name to a subset of \u2206I \u00d7\u2206I . The mapping also associates a value in \u2206I with every individual name in IN . An interpretation I is a model of a knowledge base iff it satisfies all terminological axioms and assertional axioms. A knowledge base is satisfiable iff a model exists. A concept description C is coherent (satisfiable) w.r.t. a TBox T , if a model I of T exists such that CI 6= \u2205.\n263\n264 CHAPTER 15. MOTIVATING EXAMPLES AND BASIC CONCEPTS\nA TBox is incoherent iff an incoherent concept description exists.\nDiagnosis of Ontologies\nExample 15.1 Consider a simple ontology O with the terminology T :\nax 1 : A v B ax 2 : B v C ax 3 : C v D ax 4 : D v R\nand assertions A : {A(w),\u00acR(w), A(v)}. Assume that the user explicitly states that the three assertional axioms should be considered as correct, i.e. these axioms are added to a background theory B. The introduction of a background theory ensures that the diagnosis method focuses purely on the potentially faulty axioms.\nFurthermore, assume that the user requires the currently inconsistent ontologyO\u222aB to be consistent. The only irreducible set of non-background axioms (minimal conflict set) that preserves the inconsistency is CS : \u3008ax 1, ax 2, ax 3, ax 4\u3009. That is, one has to modify or remove the axioms of at least one of the following diagnoses D1 : [ax 1] D2 : [ax 2] D3 : [ax 3] D4 : [ax 4] to restore the consistency of the ontology. However, it is unclear which of the ontologies Oi = O \\ Di obtained by application of diagnoses from the set D : {D1, . . . ,D4} is the target one.\nDefinition 15.1. A target ontology Ot is a set of logical sentences characterized by a set of background axioms B, a set of sets of logical sentences P that must be entailed by Ot and the set of sets of logical sentences N that must not be entailed by Ot.\nA target ontology Ot must fulfill the following necessary requirements:\n\u2022 Ot must be satisfiable (optionally coherent)\n\u2022 B \u2286 Ot\n\u2022 Ot |= p \u2200p \u2208 P\n\u2022 Ot 6|= n \u2200n \u2208 N\nGiven B, P, and N , an ontologyO is faulty iffO does not fulfill all the necessary requirements of the target ontology.\nNote that the approach presented in this work can be used with any knowledge representation language for which there exists a sound and complete procedure to decide whether O |= ax and the entailment operator |= is extensive, monotone and idempotent. For instance, these requirements are fulfilled by all subsets of OWL 2 which are interpreted under OWL Direct Semantics.\nDefinition 15.1 allows a user to identify the target diagnosis Dt by providing sufficient information about the target ontology in the sets B, P and N . For instance, if in Example 15.1 the user provides the information that Ot |= {B(w)} and Ot 6|= {C(w)}, the debugger will return only one diagnosis, namely D2. Application of this diagnosis results in a consistent ontology O2 = O \\D2 that \u2013 integrated with the background knowledge B \u2013 entails {B(w)} because of ax 1 and the assertion A(w). In addition, O2 \u222a B does not entail {C(w)} since O2 \u222a B \u222a {\u00acC(w)} is consistent and, moreover, {\u00acR(w), ax 4, ax 3} |= {\u00acC(w)}. All other ontologies Oi = (O \\ Di) obtained by the application of the diagnoses D1,D3 and D4 do not fulfill the given requirements, since O1 \u222a B \u222a {B(w)} is inconsistent and therefore any consistent extension ofO1\u222aB cannot entail {B(w)}. As bothO3\u222aB andO4\u222aB entail {C(w)},O2\u222aB corresponds to the target ontology Ot.\n265\nDefinition 15.2. Let \u3008O,B, P,N\u3009 be a diagnosis problem instance, where O is an ontology, B a background theory, P a set of sets of logical sentences which must be entailed by the target ontology Ot, and N a set of sets of logical sentences which must not be entailed by Ot.\nA set of axioms D \u2286 O is a diagnosis iff the set of axioms O \\ D can be extended by a logical description EX such that:\n1. (O \\ D) \u222a B \u222a EX is consistent (and coherent if required)\n2. (O \\ D) \u222a B \u222a EX |= p \u2200p \u2208 P\n3. (O \\ D) \u222a B \u222a EX 6|= n \u2200n \u2208 N\nA diagnosis Di defines a partition of the ontology O where each axiom ax j \u2208 Di is a candidate for changes by the user and each axiom axk \u2208 O\\Di is correct. IfDt is the set of axioms ofO to be changed (i.e. Dt is the target diagnosis) then the target ontology Ot is (O \\ Dt) \u222a B \u222a EX for some EX defined by the user.\nIn the following we assume the background theory B together with the sets of logical sentences in the sets P and N always allow formulation of the target ontology. Moreover, a diagnosis exists iff a target ontology exists.\nProposition 15.1. A diagnosis D for a diagnosis problem instance \u3008O,B, P,N\u3009 exists iff\nB \u222a \u22c3 p\u2208P p\nis consistent (coherent) and \u2200n \u2208 N : B \u222a \u22c3 p\u2208P p 6|= n\nThe set of all diagnoses is complete in the sense that at least one diagnosis exists where the ontology resulting from the trivial application of a diagnosis is a subset of the target ontology:\nProposition 15.2. Let D 6= \u2205 be the set of all diagnoses for a diagnosis problem instance \u3008O,B, P,N\u3009 and Ot the target ontology. Then a diagnosis Dt \u2208 D exists s.t. (O \\ Dt) \u2286 Ot.\nThe set of all diagnoses can be characterized by the set of minimal diagnoses.\nDefinition 15.3. A diagnosis D for a diagnosis problem instance \u3008O,B, P,N\u3009 is a minimal diagnosis iff there is no D\u2032 \u2282 D such that D\u2032 is a diagnosis.\nProposition 15.3. Let \u3008O,B, P,N\u3009 be a diagnosis problem instance. For every diagnosis D there is a minimal diagnosis D\u2032 s.t. D\u2032 \u2286 D.\nDefinition 15.4. A diagnosis D for a diagnosis problem instance \u3008O,B, P,N\u3009 is a minimum cardinality diagnosis iff there is no diagnosis D\u2032 such that |D\u2032| < |D|.\nTo summarize, a diagnosis describes which axioms are candidates for modification. Despite the fact that multiple diagnoses may exist, some are more preferable than others. E.g. minimal diagnoses require minimal changes, i.e. axioms are not considered for modification unless there is a reason. Minimal cardinality diagnoses require changing a minimal number of axioms. The actual type of error contained in an axiom is irrelevant as the concept of diagnosis defined here does not make any assumptions about errors themselves. There can, however, be instances where an ontology is faulty and the empty diagnosis is the only minimal diagnosis, e.g. if some axioms are missing and nothing must be changed.\nThe extension EX plays an important role in the ontology repair process, suggesting axioms that should be added to the ontology. For instance, in Example 15.1 the user requires that the target ontology\n266 CHAPTER 15. MOTIVATING EXAMPLES AND BASIC CONCEPTS\nmust not entail {B(w)} but has to entail {B(v)}, that is N = {{B(w)}} and P = {{B(v)}}. Because, the example ontology O is inconsistent some sentences must be changed. The consistent ontology O1 = O \\ D1 (along with the background axioms B) neither entails {B(v)} nor {B(w)} (in particular O1 \u222a B |= {\u00acB(w)}). Consequently, O1 has to be extended with a set EX of logical sentences in order to entail {B(v)}. This set of logical sentences can be approximated with EX = {B(v)}. O1 \u222a B \u222a EX is satisfiable, entails {B(v)} but does not entail {B(w)}. All other ontologies Oi = O \\ Di, i = 2, 3, 4 (integrated with B) are consistent but entail {B(w), B(v)} and must be rejected because of the monotonic semantics of description logic. That is, there is no such extension EX that (Oi \u222aB\u222aEX) 6|= {B(w)}. Therefore, the diagnosisD1 is the minimum cardinality diagnosis which allows the formulation of the target ontology. Note that formulation of the complete extension is impossible, since our diagnosis approach deals with changes to existing axioms and does not learn new axioms.\nThe following corollary characterizes diagnoses without employing the true extension EX to formulate the target ontology. The idea is to use the sentences which must be entailed by the target ontology to approximate EX as shown above.\nCorollary 15.1. Given a diagnosis problem instance \u3008O,B, P,N\u3009, a set of axioms D \u2286 O is a diagnosis iff\n(O \\ D) \u222a B \u222a \u22c3 p\u2208P p (Condition 1)\nis satisfiable (coherent) and \u2200n \u2208 N : (O \\ D) \u222a B \u222a \u22c3 p\u2208P p 6|= n (Condition 2)\nProof sketch: (\u21d2) LetD \u2286 O be a diagnosis for \u3008O,B, P,N\u3009. Since there is anEX s.t. (O\\D)\u222aB\u222aEX is satisfiable (coherent) and (O\\D)\u222aB\u222aEX |= p for all p \u2208 P , it follows that (O\\D)\u222aB\u222aEX\u222a \u22c3 p\u2208P p\nis satisfiable (coherent) and therefore (O \\ D) \u222a B \u222a \u22c3\np\u2208P p is satisfiable (coherent). Consequently, the first condition of the corollary is fulfilled. Since (O \\ D) \u222a B \u222a EX |= p for all p \u2208 P and (O \\ D) \u222a B \u222a EX 6|= n for all n \u2208 N it follows that (O \\ D) \u222a B \u222a EX \u222a \u22c3 p\u2208P p 6|= n for all n \u2208 N .\nConsequently, (O \\ D) \u222a B \u222a \u22c3\np\u2208P p 6|= n for all n \u2208 N and the second condition of the corollary is fulfilled.\n(\u21d0) Let D \u2286 O and \u3008O,B, P,N\u3009 be a diagnosis problem instance. Without limiting generality let EX = P . By Condition 1 of the corollary (O \\ D) \u222a B \u222a \u22c3 p\u2208P p is satisfiable (coherent). Therefore, for EX = P the sentences (O \\ D) \u222a B \u222a EX are satisfiable (coherent), i.e. the first condition for a diagnosis is fulfilled and these sentences entail p for all p \u2208 P which corresponds to the second condition a diagnosis must fulfill. Furthermore, by Condition 2 of the corollary (O \\ D) \u222a B \u222a EX 6|= n for all n \u2208 N holds and therefore the third condition for a diagnosis is fulfilled. Consequently, D \u2286 O is a diagnosis for \u3008O,B, P,N\u3009.\nConflict sets, which are the parts of the ontology that preserve the inconsistency/incoherency, are usually employed to constrain the search space during computation of diagnoses.\nDefinition 15.5. Given a diagnosis problem instance \u3008O,B, P,N\u3009, a set of axioms CS \u2286 O is a conflict set iff CS \u222a B \u222a \u22c3 p\u2208P p is inconsistent (incoherent) or n \u2208 N exists s.t. CS \u222a B \u222a \u22c3 p\u2208P p |= n.\nDefinition 15.6. A conflict set CS for an instance \u3008O,B, P,N\u3009 is minimal iff there is no CS\u2032 \u2282 CS such that CS\u2032 is a conflict set.\nA set of minimal conflict sets can be used to compute the set of minimal diagnoses as shown in [Rei87]. The idea is that each diagnosis must include at least one element of each minimal conflict set.\nProposition 15.4. D is a minimal diagnosis for the diagnosis problem instance \u3008O,B, P,N\u3009 iff D is a minimal hitting set for the set of all minimal conflict sets of \u3008O,B, P,N\u3009.\nGiven a set of sets S, a set H is a hitting set of S iff H \u2229 Si 6= \u2205 for all Si \u2208 S and H \u2286 \u22c3\nSi\u2208S Si. Most modern ontology diagnosis methods [SHCH07, KPHS07, FS05, HPS08] are implemented according to Proposition 28.2 and differ only in details, such as how and when (minimal) conflict sets are computed, the order in which hitting sets are generated, etc.\nDifferentiating between Diagnoses\nThe diagnosis method usually generates a set of diagnoses for a given diagnosis problem instance. Thus, in Example 15.1 an ontology debugger returns a set of four minimal diagnoses {D1, . . . ,D4}. As explained in the previous section, additional information, i.e. sets of sets of logical sentences P and N , can be used by the debugger to reduce the set of diagnoses. However, in the general case the user does not know which sets P and N to provide to the debugger such that the target diagnosis will be identified. Therefore, the debugger should be able to identify sets of logical sentences on its own and only ask the user or some other oracle, whether these sentences must or must not be entailed by the target ontology. To generate these sentences the debugger can apply each of the diagnoses in D = {D1, . . . ,Dn} and obtain a set of ontologies Oi = O \\ Di , i = 1, . . . , n that fulfill the user requirements. For each ontology Oi a description logic reasoner can generate a set of entailments such as entailed subsumptions provided by the classification service and sets of class assertions provided by the realization service. These entailments can be used to discriminate between the diagnoses, as different ontologies entail different sets of sentences due to extensivity of the entailment relation. Note that in the examples provided in this section we consider only two types of entailments, namely subsumption and class assertion. In general, the approach presented in this work is not limited to these types and can use all of the entailment types supported by a reasoner.\nFor instance, in Example 15.1 for each ontology Oi = (O \\ Di) , i = 1 . . . 4 (integrated with B) the realization service of a reasoner returns the set of class assertions presented in Table 15.1. Without any additional information the debugger cannot decide which of these sentences must be entailed by the target ontology. To obtain this information the diagnosis method must query an oracle that can specify whether the target ontology entails some set of sentences or not. E.g. the debugger could ask an oracle if {D(w)} is entailed by the target ontology (Ot |= {D(w)}). If the answer is yes, then {D(w)} is added to P and D4 is considered as the target diagnosis. All other diagnoses are rejected because (O\\Di)\u222aB\u222a{D(w)} for i = 1, 2, 3 is inconsistent. If the answer is no, then {D(w)} is added to N and D4 is rejected as (O \\D4) \u222a B |= {D(w)} and we have to ask the oracle another question. In the following we consider a query Q as a set of logical sentences such that Ot |= Q holds iff Ot |= qi for all qi \u2208 Q.\nProperty 1. Given a diagnosis problem instance \u3008O,B, P,N\u3009, a set of diagnoses D, a set of logical sentences Q representing the query (Ot |= Q) and an oracle able to evaluate the query:\nIf the oracle answers yes then every diagnosis Di \u2208 D is a diagnosis for P \u222a {Q} iff both conditions\n268 CHAPTER 15. MOTIVATING EXAMPLES AND BASIC CONCEPTS\nhold:\n(O \\ Di) \u222a B \u222a \u22c3 p\u2208P p \u222aQ is consistent (coherent)\n\u2200n \u2208 N : (O \\ Di) \u222a B \u222a \u22c3 p\u2208P p \u222aQ 6|= n\nIf the oracle answers no then every diagnosis Di \u2208 D is a diagnosis for N \u222a {Q} iff both conditions hold:\n(O \\ Di) \u222a B \u222a \u22c3 p\u2208P p is consistent (coherent)\n\u2200n \u2208 (N \u222a {Q}) : (O \\ Di) \u222a B \u222a \u22c3 p\u2208P p 6|= n\nIn particular, a query partitions the set of diagnoses D into three disjoint subsets.\nDefinition 15.7. For a query Q, each diagnosis Di \u2208 D of a diagnosis problem instance \u3008O,B, P,N\u3009 can be assigned to one of the three sets DP, DN or D\u2205 where\n\u2022 Di \u2208 DP iff it holds that (O \\ Di) \u222a B \u222a \u22c3 p\u2208P p |= Q\n\u2022 Di \u2208 DN iff it holds that (O \\ Di) \u222a B \u222a \u22c3 p\u2208P p \u222aQ\nis inconsistent (incoherent). \u2022 Di \u2208 D\u2205 iff Di \u2208 D \\ ( DP \u222aDN ) Given a diagnosis problem instance we say that the diagnoses in DP predict a positive answer (yes) as a result of the query Q, diagnoses in DN predict a negative answer (no), and diagnoses in D\u2205 do not make any predictions.\nProperty 2. Given a diagnosis problem instance \u3008O,B, P,N\u3009, a set of diagnoses D, a query Q and an oracle:\nIf the oracle answers yes then the set of rejected diagnoses is DN and the set of remaining diagnoses is DP \u222aD\u2205.\nIf the oracle answers no then the set of rejected diagnoses is DP and the set of remaining diagnoses is DN \u222aD\u2205.\nConsequently, given a query Q either DP or DN is eliminated but D\u2205 always remains after the query is answered. For generating queries we have to investigate for which subsets DP,DN \u2286 D a query exists that can differentiate between these sets. A straight forward approach is to investigate all possible subsets of D. In our evaluation we show that this is feasible if we limit the number n of minimal diagnoses to be considered during query generation and selection. E.g. for n = 9, the algorithm has to verify 512 possible partitions in the worst case.\nGiven a set of diagnoses D for the ontology O, a set P of sets of sentences that must be entailed by the target ontology Ot and a set of background axioms B, the set of partitions PR for which a query exists can be computed as follows:\n269\n1. Generate the power set P (D), PR\u2190 \u2205\n2. Assign an element of P (D) to the set DPi and generate a set of common entailments Ei of all ontologies (O \\ Dj) \u222a B \u222a \u22c3 p\u2208P p, where Dj \u2208 DPi\n3. If Ei = \u2205, then reject the current element DPi , i.e. set P (D) \u2190 P (D) \\ {DPi } and goto Step 2. Otherwise set Qi \u2190 Ei.\n4. Use Definition 15.7 and the queryQi to classify the diagnosesDk \u2208 D\\DPi into the sets DPi , DNi and D\u2205i . The generated partition is added to the set of partitions PR\u2190 PR\u222a{ \u2329 Qi,D P i ,D N i ,D \u2205 i \u232a }\nand set P (D)\u2190 P (D) \\ {DPi }. If P (D) 6= \u2205 then go to Step 2.\nIn Example 15.1 the set of diagnoses D of the ontology O contains 4 elements. Therefore, the power set P (D) includes 15 elements {{D1}, {D2} , . . . , {D1,D2,D3,D4}}, assuming we omit the element corresponding to \u2205 as it does not contain any diagnoses to be evaluated. Moreover, assume that P and N are empty. In each iteration an element of P (D) is assigned to the set DPi . For instance, the algorithm assigns DP1 = {D1,D2}. In this case the set of common entailments is empty as (O \\ D1) \u222a B has no entailed sentences (see Table 15.1). Therefore, the set {D1,D2} is rejected and removed from P (D). Assume that in the next iteration the algorithm selects DP2 = {D2,D3}. In this case the set of common entailments E2 = {B(w)} is not empty and so Q2 = {B(w)}. The remaining diagnoses D1 and D4 are classified according to Definition 15.7. That is, the algorithm selects the first diagnosis D1 and verifies whether (O \\ D1) \u222a B |= {B(w)}. Given the negative answer of the reasoner, the algorithm checks if (O \\ D1) \u222a B \u222a {B(w)} is inconsistent. Since the condition is satisfied the diagnosis D1 is added to the set DN2 . The second diagnosis D4 is added to the set DP2 as it satisfies the first requirement (O \\ D4) \u222a B |= {B(w)}. The resulting partition \u3008{B(w)}, {D2,D3,D4}, {D1}, \u2205\u3009 is added to the set PR.\nHowever, a query need not include all of the entailed sentences. If a query Q partitions the set of diagnoses into DP, DN and D\u2205 and an (irreducible) subset Q\u2032 \u2282 Q exists which preserves the partition then it is sufficient to query Q\u2032. In our example, Q2 : {B(w), C(w)} can be reduced to its subset Q\u20322 : {C(w)}. If there are multiple irreducible subsets that preserve the partition then we select one of them.\nAll of the queries and their corresponding partitions generated in Example 15.1 are presented in Table 15.2. Given these queries the debugger has to decide which one should be asked first in order to minimize the number of queries to be answered. A popular query selection heuristic (called \u201csplit-inhalf\u201d) prefers queries which allow half of the diagnoses to be removed from the set D regardless of the answer of an oracle.\nUsing the data presented in Table 15.2, the \u201csplit-in-half\u201d heuristic determines that asking the oracle if (Ot |= {C(w)}) is the best query (i.e. the reduced query Q2), as two diagnoses from the set D are removed regardless of the answer. Assuming that D1 is the target diagnosis, then an oracle will answer no to our question (i.e. Ot 6|= {C(w)}). Based on this feedback, the diagnoses D3 and D4 are removed according to Property 2. Given the updated set of diagnoses D and P = {{C(w)}} the partitioning algorithm returns the only partition \u3008{B(w)} , {D2} , {D1} , \u2205\u3009. The heuristic then selects the query {B(w)}, which is also answered with no by the oracle. Consequently, D1 is identified as the only remaining minimal diagnosis.\nIn general, if n is the number of diagnoses and we can split the set of diagnoses in half with each query, then the minimum number of queries is log2n. Note that this minimum number of queries can only be achieved when all minimal diagnoses are considered at once, which is intractable even for relatively small values of n.\nHowever, in case probabilities of diagnoses are known we can reduce the number of queries by utilizing two effects:\n1. We can exploit diagnoses probabilities to assess the likelihood of each answer and the expected value of the information contained in the set of diagnoses after an answer is given.\n2. Even if multiple diagnoses remain, further query generation may not be required if one diagnosis is highly probable and all other remaining diagnoses are highly improbable.\nExample 15.2 Consider an ontology O with the terminology T :\nax 1 : A1 v A2 uM1 uM2 ax 4 : M2 v \u2200s.A uD ax 2 : A2 v \u00ac\u2203s.M3 u \u2203s.M2 ax 5 : M3 \u2261 B t C ax 3 : M1 v \u00acA uB\nand the background theory containing the assertions A : {A1(w), A1(u), s(u,w)}. The ontology along with the background theory is inconsistent and the set of minimal conflict sets CS = {\u3008ax 1, ax 3, ax 4\u3009 , \u3008ax 1, ax 2, ax 3, ax 5\u3009}. To restore consistency, the user should modify all axioms of at least one minimal diagnosis:\nD1 : [ax 1] D3 : [ax 4, ax 5] D2 : [ax 3] D4 : [ax 4, ax 2]\nFollowing the same approach as in Example 15.1, we compute a set of possible queries and corresponding partitions using the algorithm presented above. A set of possible irreducible queries for Example 15.2 and their partitions are presented in Table 15.3. These queries partition the set of diagnoses D in a way that makes the application of myopic strategies, such as \u201csplit-in-half\u201d, inefficient. A greedy algorithm based on such a heuristic would first select the first query Q1, since there is no query that cuts the set of diagnoses in half. If D4 is the target diagnosis then Q1 will be answered with yes by an oracle (see Figure 15.1). In the next iteration the algorithm would also choose a suboptimal query, the first untried query Q2, since there is no partition that divides the diagnoses D1, D2, and D4 into two groups of equal size. Once again, the oracle answers yes, and the algorithm identifies query Q4 to differentiate between D1 and D4.\n271\nHowever, in real-world settings the assumption that all axioms fail with the same probability is rarely the case. For example, Roussey et al. [RCVB09] present a list of \u201canti-patterns\u201d where an anti-pattern is a set of axioms, such as {C1 v \u2200R.C2, C1 v \u2200R.C3, C2 \u2261 \u00acC3} that corresponds to a minimal conflict set. The study performed by [RCVB09] shows that such conflict sets often occur in practice due to frequent misuse of certain language constructs like quantification or disjointness. Such studies are ideal sources for estimating prior fault probabilities. However, this is beyond the scope of our work presented in this part.\nOur approach for computing the prior fault probabilities of axioms is inspired by [RDH+04] and considers the syntax of a knowledge representation language, such as restrictions, conjunction, negation, etc. For instance, if a user frequently changes the universal to the existential quantifier and vice versa in order to restore coherency, then we can assume that axioms including such restrictions are more likely to fail than the other ones. In [RDH+04] the authors report that in most cases inconsistent ontologies are created because users (a) mix up \u2200r.S and \u2203r.S, (b) mix up \u00ac\u2203r.S and \u2203r.\u00acS, (c) mix up t and u, (d) wrongly assume that classes are disjoint by default or overuse disjointness, or (e) wrongly apply negation. Observing that misuses of quantifiers are more likely than other failure patterns one might find that the axioms ax 2 and ax 4 are more likely to be faulty than ax 3 (because of the use of quantifiers), whereas ax 3 is more likely to be faulty than ax 5 and ax 1 (because of the use of negation).\nDetailed justifications of diagnoses probabilities are given in the next section. However, let us assume some probability distribution of the faults according to the observations presented above such that: (a) the diagnosis D2 is the most probable one, i.e. single fault diagnosis of an axiom containing a negation; (b) although D4 is a double fault diagnosis, it follows D2 closely as its axioms contain quantifiers; (c) D1 and D3 are significantly less probable than D4 because conjunction/disjunction in ax 1 and ax 5 have a significantly lower fault probability than negation in ax 3. Taking this information into account asking query Q1 is essentially useless because it is highly probable that the target diagnosis is either D2 or D4 and, therefore, it is highly probable that the oracle will respond with yes. Instead, asking Q3 is more informative because regardless of the answer we can exclude one of the highly probable diagnoses, i.e. either D2 or D4. If the oracle responds to Q3 with no then D2 is the only remaining diagnosis. However, if the oracle responds with yes, diagnoses D4, D3, and D1 remain, where D4 is significantly more probable compared to diagnoses D3 and D1. If the difference between the probabilities of the diagnoses is high enough such thatD4 can be accepted as the target diagnosis, no additional questions are required. Obviously this strategy can lead to a substantial reduction in the number of queries compared to myopic approaches as we demonstrate in our evaluation.\nNote that in real-world application scenarios failure patterns and their probabilities can be discovered by analyzing the debugging actions of a user in an ontology editor, like Prot\u00e9g\u00e9. Learning of fault probabilities can be used to \u201cpersonalize\u201d the query selection algorithm to prefer user-specific faults.\n272 CHAPTER 15. MOTIVATING EXAMPLES AND BASIC CONCEPTS\nHowever, as our evaluation shows, even a rough estimate of the probabilities is capable of outperforming the \u201csplit-in-half\u201d heuristic.\nChapter 16\nEntropy-Based Query Selection\nTo select the best query we exploit a-priori failure probabilities of each axiom derived from the syntax of description logics or some other knowledge representation language, such as OWL. That is, the user is able to specify own beliefs in terms of the probability of syntax element such as \u2200, \u2203, u, etc. being erroneous; alternatively, the debugger can compute these probabilities by analyzing the frequency of various syntax elements in the target diagnoses of different debugging sessions. If no failure information is available then the debugger can initialize all of the probabilities with some small value. Compared to statistically well-founded probabilities, the latter approach provides a suboptimal but useful diagnosis discrimination process, as discussed in the evaluation.\nGiven the failure probabilities of all syntax elements se \u2208 S of a knowledge representation language used in O, we can compute the failure probability of an axiom ax i \u2208 O\np(ax i) = p(Fse1 \u222a Fse2 \u222a \u00b7 \u00b7 \u00b7 \u222a Fsen)\nwhere Fse1 . . . Fsen represent the events that the occurrence of a syntax element sej in ax i is faulty. E.g. for ax2 of Example 15.2 p(ax2) = p(Fv \u222a F\u00ac \u222a F\u2203 \u222a Fu \u222a F\u2203). Assuming that each occurrence of a syntax element fails independently, i.e. an erroneous usage of a syntax element sek makes it neither more nor less probable that an occurrence of syntax element sej is faulty, the failure probability of an axiom is computed as:\np(ax i) = 1\u2212 \u220f se\u2208S (1\u2212 Fse)c(se) (16.1)\nwhere c(sej) returns number of occurrences of the syntax element sej in an axiom ax i. If among other failure probabilities the user states that p(Fv) = 0.001, p(F\u00ac) = 0.01, p(F\u2203) = 0.05 and p(Fu) = 0.001 then p(ax 2) = p(Fv \u222a F\u00ac \u222a F\u2203 \u222a Fu \u222a F\u2203) = 0.108.\nGiven the failure probabilities p(ax i) of axioms, the diagnosis algorithm first calculates the a-priori probability p(Dj) that Dj is the target diagnosis. Since all axioms fail independently, this probability can be computed as [dKW87]:\np(Dj) = \u220f\naxn \u2208Dj\np(axn) \u220f\naxm \u2208O\\Dj\n1\u2212 p(axm) (16.2)\nThe prior probabilities for diagnoses are then used to initialize an iterative algorithm that includes two main steps: (a) the selection of the best query and (b) updating the diagnoses probabilities given query feedback.\n273\n274 CHAPTER 16. ENTROPY-BASED QUERY SELECTION\nAccording to information theory the best query is the one that, given the answer of an oracle, minimizes the expected entropy of the set of diagnoses [dKW87]. Let p(Qi = yes) be the probability that query Qi is answered with yes and p(Qi = no) be the probability for the answer no. Furthermore, let p(Dj |Qi = yes) be the probability of diagnosis Dj after the oracle answers yes and p(Dj |Qi = no) be the probability after the oracle answers no. The expected entropy after querying Qi is:\nHe(Qi) = \u2211\nv\u2208{yes,no} p(Qi = v) \u2211 Dj\u2208D \u2212p(Dj |Qi = v) log2 p(Dj |Qi = v)\nBased on a one-step-look-ahead information theoretic measure, the query which minimizes the expected entropy is considered best. This formula can be simplified to the following score function [dKW87] which we use to evaluate all available queries and select the one with the minimum score to maximize information gain:\nsc(Qi) = \u2211\nv\u2208{yes,no}\n[ p(Qi = v) log2p(Qi = v) ] + p(D\u2205i ) + 1 (16.3)\nwhere v \u2208 {yes, no} is a feedback of an oracle and D\u2205i is the set of diagnoses which do not make any predictions for the query Qi. The probability of the set of diagnoses p(D\u2205i ) as well as of any other set of diagnoses Di like DPi and D N i is computed as:\np(Di) = \u2211 Dj\u2208Di p(Dj)\nbecause by Definition 28.2, each diagnosis uniquely partitions all of the axioms of an ontology O into two sets, correct and faulty, and thus all diagnoses are mutually exclusive events.\nSince, for a query Qi, the set of diagnoses D can be partitioned into the sets DPi , D N i and D \u2205 i , the\nprobability that an oracle will answer a query Qi with either yes or no can be computed as:\np(Qi = yes) = p(D P i ) + p(D \u2205 i )/2\np(Qi = no) = p(D N i ) + p(D \u2205 i )/2\n(16.4)\nClearly this assumes that for each diagnosis of D\u2205i both outcomes are equally likely and thus the probability that the set of diagnoses D\u2205i predicts either Qi = yes or Qi = no is p(D \u2205 i )/2.\nFollowing feedback v for a query Qs, i.e. Qs = v, the probabilities of the diagnoses must be updated to take the new information into account. The update is made using Bayes\u2019 rule for each Dj \u2208 D:\np(Dj |Qs = v) = p(Qs = v|Dj)p(Dj)\np(Qs = v) (16.5)\nwhere the denominator p(Qs = v) is known from the query selection step (Equation 16.4) and p(Dj) is either a prior probability (Equation 16.2) or is a probability calculated using Equation 16.5 after a previous iteration of the debugging algorithm. We assign p(Qs = v|Dj) as follows:\np(Qs = v|Dj) =  1, if Dj predicted Qs = v; 0, if Dj is rejected by Qs = v; 1 2 , if Dj \u2208 D \u2205 s\nExample 16.1 (Example 15.1 continued) Suppose that the debugger is not provided with any information about possible failures and therefore assumes that all syntax elements fail with the same probability\n0.01 and therefore p(ax i) = 0.01 for all ax i \u2208 O. Using Equation 16.2 we can calculate probabilities for each diagnosis. For instance, D1 suggests that only one axiom ax 1 should be modified by the user. Hence, we can calculate the probability of diagnosisD1 as p(D1) = p(ax 1)(1\u2212p(ax 2))(1\u2212p(ax 3))(1\u2212 p(ax 4)) = 0.0097. All other minimal diagnoses have the same probability, since every other minimal diagnosis suggests the modification of one axiom. To simplify the discussion we only consider minimal diagnoses for query selection. Therefore, the prior probabilities of the diagnoses can be normalized to p(Dj) = p(Dj)/ \u2211 Dj\u2208D p(Dj) and are equal to 0.25.\nGiven the prior probabilities of the diagnoses and a set of queries (see Table 15.2) we evaluate the score function (Equation 16.3) for each query. E.g. for the first query Q1 : {B(w)} the probability p(D\u2205) = 0 and the probabilities of both the positive and negative outcomes are: p(Q1 = 1) = p(D2) + p(D3) + p(D4) = 0.75 and p(Q1 = 0) = p(D1) = 0.25. Therefore the query score is sc(Q1) = 0.1887.\nThe scores computed during the initial stage (see Table 16.1) suggest thatQ2 is the best query. Taking into account thatD1 is the target diagnosis the oracle answers no to the query. The additional information obtained from the answer is then used to update the probabilities of diagnoses using the Equation 16.5. Since D1 and D2 predicted this answer, their probabilities are updated, p(D1) = p(D2) = 1/p(Q2 = 1) = 0.5. The probabilities of diagnoses D3 and D4 which are rejected by the oracle\u2019s answer are also updated, p(D3) = p(D4) = 0.\nIn the next iteration the algorithm recomputes the scores using the updated probabilities. The results show that Q1 is the best query. The other two queries Q2 and Q3 are irrelevant since no information will be gained if they are asked. Given the oracle\u2019s negative feedback to Q1, we update the probabilities p(D1) = 1 and p(D2) = 0. In this case the target diagnosis D1 was identified using the same number of steps as the \u201csplit-in-half\u201d heuristic.\nHowever, if the user specifies that the first axiom is more likely to fail, e.g. p(ax 1) = 0.025, then Q1 : {B(w)} will be selected first (see Table 16.2). The recalculation of the probabilities given the negative outcome Q1 = 0 sets p(D1) = 1 and p(D2) = p(D3) = p(D4) = 0. Therefore the debugger identifies the target diagnosis in only one step.\nExample 16.2 (Example 15.2 continued) Suppose that in ax 4 the user specified \u2200s.A instead of \u2203s.A and \u00ac\u2203s.M3 instead of \u2203s.\u00acM3 in ax 2. Therefore D4 is the target diagnosis. Moreover, assume that the debugger is provided with observations of three types of faults: (1) conjunction/disjunction occurs with probability p1 = 0.001, (2) negation p2 = 0.01, and (3) restrictions p3 = 0.05. Using Equation 16.1 we can calculate the probability of the axioms containing an error: p(ax 1) = 0.0019, p(ax 2) = 0.1074, p(ax 3) = 0.012, p(ax 4) = 0.051, and p(ax 5) = 0.001. These probabilities are exploited to calculate the prior probabilities of the diagnoses (see Table 16.3) and to initialize the query selection process. To\nsimplify matters we focus on the set of minimal diagnoses. In the first iteration the algorithm determines that Q3 is the best query and asks the oracle whether Ot |= {M1 v B} is true or not (see Table 16.4). The obtained information is then used to recalculate the probabilities of the diagnoses and to compute the next best subsequent query, i.e. Q4, and so on. The query process stops after the third query, since D4 is the only diagnosis that has the probability p(D4) > 0.\nGiven the feedback of the oracle Q4 = yes for the second query, the updated probabilities of the diagnoses show that the target diagnosis has a probability of p(D4) = 0.9918 whereas p(D3) is only 0.0082. In order to reduce the number of queries a user can specify a threshold, e.g. \u03c3 = 0.95. If the absolute difference in probabilities of two most probable diagnoses is greater than this threshold, the query process stops and returns the most probable diagnosis. Therefore, in this example the debugger based on the entropy query selection requires less queries than the \u201csplit-in-half\u201d heuristic. Note that already after the first answer Q3 = yes the most probable diagnosis D4 is three times more likely than the second most probable diagnosis D1. Given such a great difference we could suggest to stop the query process after the first answer if the user would set \u03c3 = 0.65.\nChapter 17\nImplementation Details\nThe iterative ontology debugger (Algorithm 11) takes a faulty ontologyO as input. Optionally, a user can provide a set of axioms B that are known to be correct as well as a set P of axioms that must be entailed by the target ontology and a set N of axioms that must not. If these sets are not given, the corresponding input arguments are initialized with \u2205. Moreover, the algorithm takes a set FP of fault probabilities for axioms ax i \u2208 O, which can be computed as described in Chapter 16 by exploiting knowledge about typical user errors. Alternatively, if no estimates of such probabilities are available, all probability values can be initialized using a small constant. We show the results of such a strategy in our evaluation section. The two other arguments \u03c3 and n are used to improve the performance of the algorithm. \u03c3 specifies the diagnosis acceptance threshold, i.e. the minimum difference in probabilities between the most likely and second-most likely diagnoses. The parameter n defines the maximum number of most probable diagnoses that should be considered by the algorithm during each iteration. A further performance gain in Algorithm 11 can be achieved if we approximate the set of the nmost probable diagnoses with the set of the nmost probable minimal diagnoses, i.e. we neglect non-minimal diagnoses. We call this set of at most nmost probable minimal diagnoses the leading diagnoses. Note, under the reasonable assumption that the fault probability of each axiom p(ax i) is less than 0.5, for every non-minimal diagnosis ND a minimal diagnosisD \u2282 ND exists which from Equation 16.2 is more probable thanND. Consequently the query selection algorithm presented here operates on the set of minimal diagnoses instead of all diagnoses (i.e. non-minimal diagnoses are excluded). However, the algorithm can be adapted with moderate effort to also consider non-minimal diagnoses.\nWe use the approach proposed by Friedrich et al. [FS05] to compute diagnoses and employ the combination of two algorithms, QUICKXPLAIN [Jun04] and HS-TREE [Rei87]. In a standard implementation the latter is a breadth-first search algorithm that takes an ontology O, sets P and N , and the maximum number of most probable minimal diagnoses n as an input. The algorithm generates minimal hitting sets using minimal conflict sets, which are computed on-demand. This is motivated by the fact that in some circumstances a subset of all minimal conflict sets is sufficient for generating a subset of all required minimal diagnoses. For instance, in Example 15.2 the user wants to compute only n = 2 leading minimal diagnoses and a minimal conflict search algorithm returns CS1. In this case HS-TREE identifies two required minimal diagnoses D1 and D2 and avoiding the computation of the minimal conflict set CS2. Of course, in the worst case, when all minimal diagnoses have to be computed the algorithm should compute all minimal conflict sets. In addition, the HS-TREE generation reuses minimal conflict sets in order to avoid unnecessary computations. Thus, in the real-world scenarios we evaluated (see Table 18.1), less than 10 minimal conflict sets were contained in the faulty ontologies having at most 13 elements while the maximal cardinality of minimal diagnoses was observed to be at most 9. Therefore, space limitations were not a problem for the breadth-first generation. However, for scenarios involving diagnoses of greater\n277\n278 CHAPTER 17. IMPLEMENTATION DETAILS\nAlgorithm 11 ONTODEBUGGING(O,B, P,N, FP, n, \u03c3) Input: ontology O, set B of background axioms, set P of sets of logical sentences to be entailed, set N of sets\nof logical sentences not to be entailed, set FP of fault probabilities for axioms, maximum number n of most probable minimal diagnoses, acceptance threshold \u03c3\nOutput: a diagnosis D\n1: DP \u2190 \u2205 2: QH \u2190 \u2205 3: T \u2190 \u3008\u2205, \u2205, \u2205, \u2205\u3009 4: while BELOWTHRESHOLD(DP, \u03c3) \u2227 GETSCORE(T ) 6= 1 do 5: DP \u2190 HS-TREE(O,B, P,N, FP,QH, n) 6: T \u2190 SELECTQUERY(DP,O,B, P ) 7: Q\u2190 GETQUERY(T ) 8: if Q = \u2205 then 9: exit loop\n10: if GETANSWER(Ot |= Q) then 11: P \u2190 P \u222a {Q} 12: else 13: N \u2190 N \u222a {Q} 14: QH \u2190 QH \u222a {T} 15: return MOSTPROBABLEDIAGNOSIS(DP )\ncardinalities iterative-deepening strategies could be applied. In our implementation of HS-TREE we use the uniform-cost search strategy. Given additional information in terms of axiom fault probabilities FP , the algorithm expands a leaf node in a search-tree if it is an element of the path corresponding to the maximum probability hitting set of minimal conflict sets computed so far. The probability of each minimal hitting set can be computed using Equation 16.2. Consequently, the algorithm computes a set of diagnoses ordered by their probability starting from the most probable one. HS-TREE terminates if either the n most probable minimal diagnoses are identified or no further minimal diagnoses can be found. Thus the algorithm computes at most nminimal diagnoses regardless of the number of all minimal diagnoses.\nHS-TREE uses QUICKXPLAIN to compute required minimal conflicts. This algorithm, given a set of axioms AX and a set of correct axioms B returns a minimal conflict set CS \u2286 AX , or \u2205 if axioms AX \u222a B are consistent. In the worst case, to compute a minimal conflict QUICKXPLAIN performs 2k(log(s/k) + 1) consistency checks, where k is the size of the generated minimal conflict set and s is the number of axioms in the ontology. In the best case only log(s/k) + 2k are performed [Jun04]. Importantly, the size of the ontology is contained in the log function. Therefore, the time needed for consistency checks in our test ontologies remained below 0.2 seconds, even for real world knowledge bases with thousands of axioms. The maximum time to compute a minimal conflict was observed in the Sweet-JPL ontology and took approx. 5 seconds (see Table 18.2).\nIn order to take past answers into account the HS-TREE updates the prior probabilities of the diagnoses by evaluating Equation 16.5. All required data is stored in the query history QH as well as in the sets P and N . When complete, HS-TREE returns a set of tuples of the form \u3008Di, p(Di)\u3009 where Di is contained in the set of the n most probable minimal diagnoses (leading diagnoses) and p(Di) is its probability calculated using Equation 16.2 and Equation 16.5.\nIn the query-selection phase Algorithm 11 calls SELECTQUERY function (Algorithm 12) to generate a tuple T = \u2329 Q,DP,DN,D\u2205 \u232a , where Q is the minimum score query (Equation 16.3) and DP,DN and D\u2205 the sets of diagnoses constituting the partition. The generation algorithm carries out a depthfirst search, removing the top element of the set D and calling itself recursively to generate all possible\n279\nAlgorithm 12 SELECTQUERY(DP,O,B, P ) Input: set DP of tuples \u3008Di, p(Di)\u3009, ontology O, set of background axioms B, set P of sets of logical sentences\nthat must be entailed by the target ontology Output: a tuple \u2329 Q,DP,DN,D\u2205 \u232a 1: D\u2190 GETDIAGNOSES(DP ) 2: T \u2190 GENERATE(\u2205,D,O,B, P,DP ) 3: return MINIMIZEQUERY(T )\n4: procedure GENERATE(DP, D,O,B, P,DP ) returns a tuple \u2329 Q,DP,DN,D\u2205 \u232a 5: if D = \u2205 then 6: D\u2190 GETDIAGNOSES(DP ) 7: return CREATEQUERY(DP,O,B, P,D) 8: D \u2190 POP(D) 9: left \u2190 GENERATE(DP, D,O,B, P,DP )\n10: right \u2190 GENERATE(DP \u222a {D} , D,O,B, P,DP ) 11: if GETSCORE(left , DP ) < GETSCORE(right , DP ) then 12: return left 13: else 14: return right\nsubsets of the leading diagnoses. The set of leading diagnoses D is extracted from the set of tuples DP by the GETDIAGNOSES function. In each leaf node of the search tree the GENERATE function calls CREATEQUERY creates a query given a set of diagnoses DP by computing common entailments and partitioning the set of diagnoses D \\ DP, as described in Section 15. If a query for the set DP does not exist (i.e. there are no common entailments) or DP = \u2205 then CREATEQUERY returns an empty tuple T = \u3008\u2205, \u2205, \u2205, \u2205\u3009. In all inner nodes of the tree the algorithm selects a tuple that corresponds to a query with the minimum score as found using the GETSCORE function. This function may implement the entropybased measure (Equation 16.3), \u201csplit-in-half\u201d or any other preference criteria. Given an empty tuple T = \u3008\u2205, \u2205, \u2205, \u2205\u3009 the function returns the highest possible score of a used measure. In general, CREATEQUERY is called 2n times, where we set n = 9 in our evaluation. Furthermore, for each leading diagnosis not in DP, CREATEQUERY has to check if the associated query is entailed. If a query is not entailed, a consistency check has to be performed. Entailments are determined by classification/realization and a subset check of the generated sentences. Common entailments are computed by exploiting the intersection of entailments for each diagnosis contained in DP. Note that the entailments for each leading diagnosis are computed just once and reused in for subsequent calls of CREATEQUERY.\nIn the function MINIMIZEQUERY, the query Q of the resulting tuple \u2329 Q,DP,DN,D\u2205 \u232a is iteratively reduced by applying QUICKXPLAIN such that sets DP, DN and D\u2205 are preserved. This is implemented by replacing the consistency checks performed by QUICKXPLAIN with checks that ensure that the reduction of the query preserves the partition. In order to check if a partition is preserved, a consistency/entailment check is performed for each element in DN and D\u2205. Elements of DP need not be checked because these elements entail the query and therefore any reduction. In the worst case n(2k log(s/k) + 2k) consistency checks have to be performed in MINIMIZEQUERY where k is the length of the minimized query. Entailments of leading diagnoses are reused.\nAlgorithm 11 invokes the function GETQUERY to obtain the query from the tuple stored in T and calls GETANSWER to query the oracle. Depending on the answer, Algorithm 11 extends either the set P or the set N and thus excludes diagnoses not compliant with the query answer from the results of HS-TREE in further iterations. Note, the algorithm can be easily adapted to allow the oracle to reject a query if the\n280 CHAPTER 17. IMPLEMENTATION DETAILS\nanswer is unknown. In this case the algorithm proceeds with the next best query (w.r.t. the GETSCORE function) until no further queries are available.\nAlgorithm 11 stops if the difference in the probabilities of the top two diagnoses is greater than the acceptance threshold \u03c3 or if no query can be used to differentiate between the remaining diagnoses (i.e. the score of the minimum score query equals to the maximum score of the used measure). The most probable diagnosis is then returned to the user. If it is impossible to differentiate between a number of highly probable minimal diagnoses, the algorithm returns a set that includes all of them. Moreover, in the first case (termination due to \u03c3), the algorithm can continue if the user is not satisfied with the returned diagnosis and at least one further query exists.\nAdditional performance improvements can be achieved by using greedy strategies in Algorithm 12. The idea is to guide the search such that a leaf node of the left-most branch of a search tree contains a set of diagnoses DP that might result in a tuple \u2329 Q,DP,DN,D\u2205 \u232a with a low-score query. This method is based on the property of Equation 16.3 that sc(Q) = 0 if\u2211 Di\u2208DP p(Di) = \u2211 Dj\u2208DN p(Dj) = 0.5 and p(D\u2205) = 0\nConsequently, the query selection problem can be presented as a two-way number partitioning problem: given a set of numbers, divide them into two sets such that the difference between the sums of the numbers in each set is as small as possible. The Complete Karmarkar-Karp (CKK) algorithm [Kor98], which is one of the best algorithms developed for the two-way partitioning problem, corresponds to an extension of the Algorithm 12 with a set differencing heuristic [KKLO86]. The algorithm stops if the optimal solution to the two-way partitioning problem is found or if there are no further subsets to be investigated. In the latter case the best found solution is returned.\nThe main drawback of applying CKK to the query selection process is that none of the pruning techniques can be used. Also even if the algorithm finds an optimal solution to the two-way partitioning problem there just might be no query for a found set of diagnoses DP. Moreover, since the algorithm is complete it still has to investigate all subsets of the set of diagnoses in order to find the minimum score query. To avoid this exhaustive search we extended CKK with an additional termination criterion: the search stops if a query is found with a score below some predefined threshold \u03b3. In our evaluation section we demonstrate substantial savings by applying the CKK partitioning algorithm.\nTo sum up, the proposed method depends on the efficiency of the classification/realization system and consistency/coherency checks given a particular ontology. The number of calls to a reasoning system can be reduced by decreasing the number of leading diagnoses n. However, the more leading diagnoses provide the more data for generating the next best query. Consequently, by varying the number of leading diagnoses it is possible to balance runtime with the number of queries needed to isolate the target diagnosis.32\n32The source code as well as precompiled binaries can be downloaded from http://rmbd.googlecode.com. The package also includes a Prot\u00e9g\u00e9-plugin implementing the methods as described.\nChapter 18\nEvaluation\nWe evaluated our approach using the real-world ontologies presented in Table 18.1 with the aim of demonstrating its applicability real-world settings. In addition, we employed generated examples to perform controlled experiments where the number of minimal diagnoses and their cardinality could be varied to make the identification of the target diagnosis more difficult. Finally, we carried out a set of tests using randomly modified large real-world ontologies to provide some insights on the scalability of the suggested debugging method.\nFor the first test we created a generator which takes a consistent and coherent ontology, a set of fault patterns together with their probabilities, the minimum number of minimum cardinality diagnosesm, and the required cardinality |Dt| of these minimum cardinality diagnoses as inputs. We also assumed that the target diagnosis has cardinality |Dt|. The output of the generator is an alteration of the input ontology for which at least the given number of minimum cardinality diagnoses with the required cardinality exist. Furthermore, to introduce inconsistencies (incoherencies), the generator applies fault patterns randomly to the input ontology depending on their probabilities.\nIn this experiment we took five fault patterns from a case study reported by Rector et al. [RDH+04] and assigned fault probabilities according to their observations of typical user errors. Thus we assumed that in cases (a) and (b) (see Section 15), where an axiom includes some roles (i.e. property assertions), axiom descriptions are faulty with a probability of 0.025, in cases (c) and (d) 0.01 and in case (e) 0.001. In each iteration, the generator randomly selected an axiom to be altered and applied a fault pattern. Following this, another axiom was selected using the concept taxonomy and altered correspondingly to introduce an inconsistency (incoherency). The fault patterns were randomly selected in each step using\n281\n282 CHAPTER 18. EVALUATION\nthe probabilities provided above. For instance, given the description of a randomly selected concept A and the fault pattern \u201cmisuse of negation\u201d, we added the construct u\u00acX to the description of A, where X is a new concept name. Next, we randomly selected concepts B and S such that S v A and S v B and added uX to the description of B. During the generation process, we applied the HS-TREE algorithm after each introduction of an incoherency/inconsistency to control two parameters: the minimum number of minimal cardinality diagnoses in the ontology and their cardinality. The generator continues to introduce incoherences/inconsistencies until the specified parameter values are reached. For instance, if the minimum number of minimum cardinality diagnoses is equal to m = 6 and their cardinality is |Dt| = 4, then the generated ontology will include at least 6 diagnoses of cardinality 4 and possibly some additional number of minimal diagnoses of higher cardinalities.\nThe resulting faulty ontology as well as the fault patterns and their probabilities were inputs for the ontology debugger. The acceptance threshold \u03c3 was set to 0.95 and the number of most probable minimal diagnoses n was set to 9. In addition, one of the minimal diagnoses with the required cardinality was randomly selected as the target diagnosis. Note, the target ontology is not equal to the original ontology, but rather a corrected version of the altered one in which the faulty axioms were repaired by replacing them with their original (correct) versions according to the target diagnosis. The tests were performed\n283\nusing the ontologies bike2 to bike9, bcs3, galen and galen2 from Racer\u2019s benchmark suite33. The average results of the evaluation performed on each test ontology (presented in Figure 18.1) show that the entropy-based approach outperforms the \u201csplit-in-half\u201d heuristic as well as the random query selection strategy by more than 50% for the |Dt| = 2 case due to its ability to estimate the probabilities of diagnoses and to stop once the target diagnosis crossed the acceptance threshold. On average the algorithm required 8 seconds to generate a query. In addition, Figure 18.1 shows that the number of queries required increases as the cardinality of the target diagnosis increases, regardless of the method. Despite this, the entropy-based approach remains better than the \u201csplit-in-half\u201d method for diagnoses with increasing cardinality. The approach did however require more queries to discriminate between high cardinality diagnoses because in such cases more minimal conflicts were generated. Consequently, the debugger should consider more minimal diagnoses in order to identify the target one.\nFor the next test we selected seven real-world ontologies described in Tables 18.1 and 18.234. Performance of both the entropy-based and \u201csplit-in-half\u201d selection strategies was evaluated using a variety of different prior fault probabilities to investigate under which conditions the entropy-based method should be preferred.\nIn our experiments we distinguished between three different distributions of prior fault probabilities: extreme, moderate and uniform (see Figure 18.2 for an example). The extreme distribution simulates a situation in which very high failure probabilities are assigned to a small number of syntax elements. That is, the provider of the estimates is quite sure that exactly these elements are causing a fault. For instance, it may be well known that a user has problems formulating restrictions in OWL whereas all other elements, such as subsumption and conjunction, are well understood. In the case of a moderate distribution the estimates provide a slight bias towards some syntax elements. This distribution has the same motivation as the extreme one, however, in this case the probability estimator is less sure about the sources of possible errors in axioms. Both extreme and moderate distributions correspond to the exponential distribution with \u03bb = 1.75 and \u03bb = 0.5 respectively. The uniform distribution models the situation where no prior fault probabilities are provided and the system assigns equal probabilities to all syntax elements found in a faulty ontology. Of course the prior probabilities of diagnoses may not reflect the actual situation. Therefore, for each of the three distributions we differentiate between good, average and bad cases. In the good case the estimates of the prior fault probabilities are correct and the\n33Available at http://www.racer-systems.com/products/download/benchmark.phtml 34All experiments were performed on a PC with Core2 Duo (E8400), 3 Ghz with 8 Gb RAM, running Windows 7 and Java 6.\n284 CHAPTER 18. EVALUATION\n0,5 4,4 4,31369251 0 0,5 0 0,5 0,11590998 1 0,30326533 0,22727273 0,44629124 0,10345921 2 0,18393972 0,45454545 0,39835173 0,09234588 3 0,11156508 0,68181818 0,35556178 0,08242631 4 0,06766764 0,90909091 0,31736821 0,07357228 5 0,0410425 1,13636364 0,2832773 0,06566933 6 0,02489353 1,36363636 0,25284835 0,05861529 7 0,01509869 1,59090909 0,22568801 0,05231898 8 0,00915782 1,81818182 0,20144516 0,04669901 9 0,0055545 2,04545455 0,17980642 0,04168272"}, {"heading": "10 0,00336897 2,27272727 0,16049206 0,03720526", "text": ""}, {"heading": "11 0,00204339 2,5 0,1432524 0,03320876", "text": ""}, {"heading": "12 0,00123938 2,72727273 0,12786458 0,02964156", "text": ""}, {"heading": "13 0,00075172 2,95454545 0,11412968 0,02645754", "text": ""}, {"heading": "14 0,00045594 3,18181818 0,10187015 0,02361553", "text": ""}, {"heading": "15 0,00027654 3,40909091 0,09092751 0,02107881", "text": ""}, {"heading": "16 0,00016773 3,63636364 0,08116031 0,01881458", "text": ""}, {"heading": "17 0,00010173 3,86363636 0,07244227 0,01679356", "text": ""}, {"heading": "18 6,1705E\u201005 4,09090909 0,0646607 0,01498964", "text": ""}, {"heading": "19 3,7426E\u201005 4,31818182 0,057715 0,01337949", "text": ""}, {"heading": "20 2,27E\u201005 4,54545455 0,0515154 0,0119423", "text": ""}, {"heading": "21 1,3768E\u201005 4,77272727 0,04598174 0,01065949", "text": "22 8,3509E\u201006 5 0,0410425 0,00951447\ntarget diagnosis is assigned a high probability. The average case corresponds to the situation when the target diagnosis is neither favored nor penalized by the priors. In the bad case the prior distribution is unreasonable and disfavors the target diagnosis by assigning it a low probability.\nWe executed 30 tests for each of the combinations of the distributions and cases with an acceptance threshold \u03c3 = 0.85 and a required number of most probable minimal diagnoses n = 9. Each iteration started with the generation of a set of prior fault probabilities of syntax elements by sampling from a selected distribution (extreme, moderate or uniform). Given the priors we computed the set of all minimal diagnoses D of a given ontology and selected the target one according to the chosen case (good, average or bad). In the good case the prior probabilities favor the target diagnosis and, therefore, it should be selected from the diagnoses with high probability. The set of diagnoses was ordered according to their probabilities and the algorithm iterated through the set starting from the most probable element. In the first iteration the most probable minimal diagnosisD1 is added to the setG. In next iteration j a diagnosis Dj was added to the set G if \u2211 i\u2264j p(Di) \u2264 1 3 and to the set A if \u2211 i\u2264j p(Di) \u2264 2 3 . The obtained set G contained all most probable diagnoses which we considered as good. All diagnoses in the set A \\ G were classified as average and the remaining diagnoses D \\A as bad. Depending on the selected case we randomly selected one of the diagnoses as the target from the appropriate set.\nThe results of the evaluation presented in Table 18.3 show that the entropy-based query selection approach clearly outperforms \u201csplit-in-half\u201d in good and average cases for the three probability distributions. The average time required by the debugger to perform such basic operations as consistency checking, computation of minimal conflicts and diagnoses is presented in Table 18.4. The results indicate that on average at most 7 seconds required to compute up to 9 minimal diagnoses and a query. Moreover, the number of axioms in a query remains reasonable in most of the cases stays bounds, i.e. between 1 and 4 axioms per query.\nIn the uniform case better results were observed since the diagnoses have different cardinality and structure, i.e. they include different syntax elements. Consequently, even if equal probabilities for all syntax elements (uniform distribution) are given, the probabilities of diagnoses are different. Axioms with a greater number of syntax elements receive a higher fault probability. Also, diagnoses with a smaller cardinality in many cases receive a higher probability. This information provides enough bias to favor the entropy-based method.\nIn the bad case, where the target diagnosis received a low probability and no information regarding the\n285\n286 CHAPTER 18. EVALUATION\nOntology Good Average Bad DT QT QL DT QT QL DT QT QL Chemical 459.33 117.67 3 461.33 121 3.34 256.67 75.67 2.19 Koala 88.33 1308.33 3.47 92 1568.67 3.90 56.33 869.33 2.36 Sweet-JPL 2387.33 691.67 1.48 2272 926 1.61 2103 1240.33 1.57 miniTabmis 481.33 2764.33 3.27 398.33 2892 2.53 238.67 3223 1.76 University 189.33 822.67 3.91 145 903.33 2.82 113 872 2.11 Economy 2953.33 6927 3.06 3239 8789 3.80 3083 8424.67 1.58 Transportation 6577.33 9426.33 2.37 7080.67 10135.33 2.29 7186.67 9599.67 1.64\n0 0 0 max\u00a0win\u00a0Q 0,15 0,03 0,19 max\u00a0loss\u00a0Q 0,37 0,14 0,38\nmax\u00a0win\u00a0T 32% 34% 37%\nmax\u00a0loss\u00a0T 33% 38% 35%\nAvg \u20100,11 \u20100,03 \u20100,07\nprior fault probabilities was given, we observed that the performance of the entropy-method improved as more queries were posed. In particular, in the University ontology the performance is essentially similar (7.27 vs. 7.37) whereas in the Economy and Transportation ontology the entropy-based method can save and average of two queries.\n\u201cSplit-in-half\u201d appears to be particularly inefficient in all good, average and bad cases when applied to ontologies with a large number of minimal diagnoses, such as Economy and Transportation. The main problem is that no stop criteria can be used with the greedy method as it is unable to provide any ordering on the set of diagnoses. Instead, the method continues until no further queries can be generated, i.e. only one minimal diagnosis exists or there are no discriminating queries. Conversely, the entropy-based method is able to improve its probability estimates using Bayes-updates as more queries are answered and to exploit the differences in the probabilities in order to decide when to stop.\nThe most significant gains are achieved for ontologies with many minimal diagnoses and for the average and good cases, e.g. the target diagnosis is within the first or second third of the minimal diagnoses ranked by their prior probability. In these cases the entropy-based method can save up to 60% of the queries.\n287\nTherefore, we can conclude that even rough estimates of the prior fault probabilities are sufficient, provided that the target diagnosis is not significantly penalized. Even if no fault probabilities are available and there are many minimal diagnoses, the entropy-based method is advantageous. The differences between probabilities of individual syntax elements appears not to influence the results of the query selection process and affect only the number of outliers, i.e. cases in which the diagnosis approach required either few or many queries compared to the average.\nAnother interesting observation is that often both methods eliminated more than n diagnoses in one iteration. For instance, in the case of the Transportation ontology both methods were able to remove hundreds of minimal diagnoses with a small number of queries. This behavior appears to stem from relations between the diagnoses. That is, the addition of a query to either P or N allows the method to remove not only the diagnoses in sets DP or DN, but also some unobserved diagnoses that were not in any of the sets of n leading diagnoses computed by HS-TREE. Given the sets P and N , HS-TREE automatically invalidates all diagnoses which do not fulfill the requirements (see Definition 28.2).\nThe extended CKK method presented in Chapter 17 was evaluated in the same settings as the complete Algorithm 12 with acceptance threshold \u03b3 = 0.1. The obtained results presented in Figure 18.3 show that the extended CKK method decreases the length of a debugging session by at least 60% while requiring\n288 CHAPTER 18. EVALUATION\n289\non average 0.1 queries more than Algorithm 12. In some cases (mostly for the uniform distribution) the debugger using CKK search required even fewer queries than Algorithm 12 because of the inherent uncertainty of the domain. The plot of the average time required by Algorithm 12 and CKK to identify the target diagnosis presented in Figure 18.4 shows that the application of the latter can reduce runtime significantly.\nIn the last experiment we tried to simulate an expert developing large real-world ontologies35 as described in Table 18.5. Often in such settings an expert makes small changes to the ontology and then runs the reasoner to verify that the changes are valid, i.e. the ontology is consistent and its entailments are correct. To simulate this scenario we used the generator described in the first experiment to introduce 1 to 3 random changes that would make the ontology incoherent. Then, for each modified ontology, we performed 15 tests using the fault distributions as in the second test. The results obtained by the entropy-based query selection method using CKK for query computation are presented in Table 18.6. These results show that the method can be used for analysis of large ontologies with over 33000 axioms while requiring a user to wait for only a minute to compute the next query.\n35The ontologies taken from TONES repository http://owl.cs.manchester.ac.uk/repository\nChapter 19\nRelated Work\nDespite the range of ontology diagnosis methods available (see [SHCH07, KPHS07, FS05]), to the best of our knowledge no interactive ontology debugging methods, such as our \u201csplit-in-half\u201d or entropy-based methods, have been proposed so far. The idea of ranking of diagnoses and proposing a target diagnosis is presented in [KPSCG06]. This method uses a number of measures such as: (a) the frequency with which an axiom appears in conflict sets, (b) impact on an ontology in terms of its \u201clost\u201d entailments when an axiom is modified or removed, (c) ranking of test cases, (d) provenance information about axioms, and (e) syntactic relevance. For each axiom in a conflict set, these measures are evaluated and combined to produce a rank value. These ranks are then used by a modified HS-TREE algorithm to identify diagnoses with a minimal rank. However, the method fails when a target diagnosis cannot be determined reliably with the given a-priori knowledge. In our work required information is acquired until the target diagnosis can be identified with confidence. In general, the work of [KPSCG06] can be combined with the ideas presented in our work as axiom ranks can be taken into account together with other observations for calculating the prior probabilities of the diagnoses.\nThe idea of selecting the next best query based on the expected entropy was exploited in the generation of decisions trees in [Qui86] and further refined for selecting measurements in the model-based diagnosis of circuits in [dKW87]. We extend these methods to query selection in the domain of ontology debugging.\nIn the area of debugging logic programs, Shapiro [Sha83] developed debugging methods based on query answering. Roughly speaking, Shapiro\u2019s method aims to detect one fault at a time by querying an oracle about the intended behavior of a Prolog program at hand. In our terminology, for each answer that must not be entailed this diagnosis approach generates one conflict at a time by exploiting the proof tree of a Prolog program. The method then identifies a query that splits the conflict in half. Our approach can deal with multiple diagnoses and conflicts simultaneously which can be exploited by query generation strategies such as \u201csplit-in-half\u201d and entropy-based methods. Whereas the \u201csplit-in-half\u201d strategy splits the set of diagnoses in half, Shapiros\u2019s method focuses on one conflict. Furthermore, the exploitation of failure probabilities is not considered in [Sha83]. However, Shapiro\u2019s method includes the learning of new clauses in order to cover not entailed answers. Interleaving discrimination of diagnoses and learning of descriptions is currently not considered in our approach because of their additional computational costs.\nFrom a general point of view Shapiro\u2019s method can be seen as a prominent example of inductive logic programming (ILP) including systems such as [MB88, Mug95]. In particular, [Mug95] proposes inverse entailments combined with general to specific search through a refinement graph with the goal of generating a theory (hypothesis) which covers the examples and fulfills additional properties. Compared to ILP, the focus of our work lies on the theory revision. However, our knowledge representation languages are variants of description logics and not logic programs. Moreover, our method aims to discover axioms\n291\n292 CHAPTER 19. RELATED WORK\nwhich must be changed while minimizing user interaction. Preferences of theory changes are expressed by probabilities which are updated through Bayes\u2019 rule. Other preferences based on plausible extensions of the theory were not considered, again because of their computational costs.\nAlthough model-based diagnosis has also been applied to logic programs [CFD93], constraint knowledge bases [FFJS04] and hardware descriptions [FSW99], none of these approaches propose a query generation method to discriminate between diagnoses.\nChapter 20\nSummary and Conclusions\nIn this part we presented an approach to the interactive debugging of ontologies. This approach is applicable to any knowledge representation language with monotonic semantics. We showed that the axioms generated by classification and realization reasoning services can be exploited to generate queries which differentiate between diagnoses. For selecting the best next query we proposed two strategies: The \u201csplit-in-half\u201d strategy prefers queries which allow eliminating a half of the leading diagnoses. The entropy-based strategy employs information theoretic concepts to exploit knowledge about the likelihood of axioms to be faulty. Based on the probability of an axiom containing an error we predict the (expected) information gain produced by a query result, enabling us to select the best subsequent query according to a one-step-lookahead entropy-based scoring function. We described the implementation of an interactive debugging algorithm and compared the entropy-based method with the \u201csplit-in-half\u201d strategy. Our experiments showed a significant reduction in the number of queries required to identify the target diagnosis when the entropy-based method is applied. Depending on the quality of the given prior fault probabilities the required number of queries could be reduced by up to 60%.\nIn order to evaluate the robustness of the entropy-based method we experimented with different prior fault probability distributions as well as different qualities of the prior probabilities. Furthermore, we investigated cases where knowledge about failure probabilities is missing or inaccurate. In case such knowledge is unavailable, the entropy-based methods ranks the diagnoses based on the number of syntax elements contained in an axiom and the number of axioms in a diagnosis. Given that this is a reasonable guess (i.e. the target diagnosis is not at the lower end of the diagnoses ranked by their prior probabilities), the entropy-based method outperformed \u201csplit-in-half\u201d. Moreover, even if the initial guess is not reasonable, the entropy-based method improves the accuracy of the probabilities as more questions are asked. Furthermore, the applicability of the approach to real-world ontologies containing thousands of axioms was demonstrated by an extensive set of evaluations which are publicly available.\n293\nPart V\nMinimizing User Interaction in Ontology Debugging\n295\n297\nA reinforcement learning query selection strategy (RIO) that makes the presented debugging system robust against the usage of low-quality fault information is presented and thoroughly analyzed in this part which is based on the publications [RSFF13, RSFF12, RSFF11, SRF11] published in Web Reasoning and Rule Systems (RR-2013), in the Proceedings of the 7th International Workshop on Ontology Matching (OM-2012), in the Proceedings of the Joint Workshop on Knowledge Evolution and Ontology Dynamics 2011 (EvoDyn2011) and in DX 2011 - 22nd International Workshop on Principles of Diagnosis, respectively.\nChapter 21\nIntroduction to the Problem\nThe foundation for widespread adoption of Semantic Web technologies is a broad community of ontology developers which is not restricted to experienced knowledge engineers. Instead, domain experts from diverse fields should be able to create ontologies incorporating their knowledge as autonomously as possible. The resulting ontologies are required to fulfill some minimal quality criteria, usually consistency, coherency and no undesired entailments, in order to grant successful deployment. However, the correct formulation of logical descriptions in ontologies is an error-prone task which accounts for a need for assistance in ontology development in terms of ontology debugging tools. Usually, such tools [SHCH07, KPHS07, FS05, HPS08] use model-based diagnosis [Rei87] to identify sets of faulty axioms, called diagnoses, that need to be modified or deleted in order to meet the imposed quality requirements. The major challenge inherent in the debugging task is often a substantial number of alternative diagnoses.\nIn [SFFR12] this issue is tackled by letting the user take action during the debugging session by answering queries about entailments and non-entailments of the desired ontology. These answers pose constraints to the validity of diagnoses and thus help to sort out incompliant diagnoses step-by-step. In addition, a Bayesian approach is used to continuously readjust the fault probabilities by means of the additional information given by the user. The user effort in this interactive debugging procedure is strongly affected by the quality of the initially provided meta information, i.e. prior knowledge about fault probabilities of a user w.r.t. particular logical operators. To get this under control, the selection of queries shown to the user can be varied correspondingly. To this end, two essential paradigms for choosing the next \u201cbest\u201d query have been proposed, split-in-half and entropy-based.\nIn order to opt for the optimal strategy, however, the quality of the meta information, i.e. good or bad (which means high or low probability of the correct solution), must be known in advance. This would, however, implicate the pre-knowledge of the initially unknown solution. Entropy-based methods can make optimal profit from exploiting properly adjusted initial fault probabilities (high potential), whereas they can completely fail in the case of weak prior information (high risk). The split-in-half technique, on the other hand, manifests constant behavior independently of the probabilities given (no risk), but lacks the ability to leverage appropriate fault information (no potential). This matter of fact is witnessed by the evaluation we conducted, which shows that an unsuitable combination of meta information and query selection strategy can result in a substantial increase of more than 2000% w.r.t. number of queries to a user. So, there is a need to either (1) guarantee a sufficiently suited choice of prior fault information, or (2) to manage the \u201crisk\u201d of unsuitable method selection. The task of (1) might not be a severe problem in a debugging scenario involving a faulty ontology developed by a single expert, since the meta information might be extracted from the logs of previous sessions, if available, or specified by the expert based on their experience w.r.t. own faults. However, realization of task (1) is a major issue in scenarios involving\n299\n300 CHAPTER 21. INTRODUCTION TO THE PROBLEM\nautomatized systems producing (parts of) ontologies, e.g. ontology alignment and ontology learning, or numerous users collaborating in modeling an ontology, where the choice of reasonable meta information is rather unclear. Therefore, we focus on accomplishing task (2).\nThe contribution of this part is a new RIsk Optimization reinforcement learning method (RIO), which allows to minimize user interaction throughout a debugging session on average compared to existing strategies, for any quality of meta information (high potential at low risk). By virtue of its learning capability, our approach is optimally suited for debugging ontologies where only vague or no meta information is available. A learning parameter is constantly adapted based on the information gathered so far. On the one hand, our method takes advantage of the given meta information as long as good performance is achieved. On the other hand, it gradually gets more independent of meta information if suboptimal behavior is measured.\nExperiments on two datasets of faulty real-world ontologies show the feasibility, efficiency and scalability of RIO. The evaluation will indicate that, on average, RIO is the best choice of strategy for both good and bad meta information with savings as to user interaction of up to 80%.\nThe problem specification, basic concepts and a motivating example are provided in Chapter 22. Chapter 23 explains the suggested approach and gives implementation details. Evaluation results are described in Chapter 24. Related work is discussed in Chapter 25. Chapter 26 concludes.\nChapter 22\nMotivation and Basic Concepts\nFirst we provide an informal introduction to ontology debugging, particularly addressing readers unfamiliar with the topic. Later we introduce precise formalizations. We assume the reader to be familiar with description logics [BCM+07].\nOntology debugging deals with the following problem: Given is an ontology O which does not meet postulated requirements R, e.g. R = {coherency, consistency}. O is a set of axioms formulated in some monotonic knowledge representation language, e.g. OWL DL. The task is to find a subset of axioms in O, called diagnosis, that needs to be altered or eliminated from the ontology in order to meet the given requirements. The presented approach to ontology debugging does not rely upon a specific knowledge representation formalism, it solely presumes that it is logic-based and monotonic. Additionally, the existence of sound and complete procedures for deciding logical consistency and for calculating logical entailments is assumed. These procedures are used as a black box. For OWL DL, e.g., both functionalities are provided by a standard DL-reasoner.\nA diagnosis is a hypothesis about the state of each axiom in O of being either correct or faulty. Generally, there are many diagnoses for one and the same faulty ontologyO. The problem is then to figure out the single diagnosis, called target diagnosis D\u2217, that complies with the knowledge to be modeled by the intended ontology. In interactive ontology debugging we assume a user, e.g. the author of the faulty ontology or a domain expert, interacting with an ontology debugging system by answering queries about entailments of the desired ontology, called the target ontologyO\u2217. The target ontology can be understood as O minus the axioms of D\u2217 plus a set of axioms needed to preserve the desired entailments, called positive test cases. Note that the user is not expected to know O\u2217 explicitly (in which case there would be no need to consult an ontology debugger), but implicitly in that they are able to answer queries about O\u2217.\nA query is a set of axioms and the user is asked whether the conjunction of these axioms is entailed by O\u2217. Every positively (negatively) answered query constitutes a positive (negative) test case fulfilled by O\u2217. The set of positive (entailed) and negative (non-entailed) test cases is denoted by P and N , respectively. So, P and N are sets of sets of axioms, which can be, but do not need to be, initially empty. Test cases can be seen as constraints O\u2217 must satisfy and are therefore used to gradually reduce the search space for valid diagnoses. Roughly, the overall procedure consists of (1) computing a predefined number of diagnoses, (2) gathering additional information by querying the user, (3) incorporating this information to prune the search space for diagnoses, and so forth, until a stopping criterion is fulfilled, e.g. one diagnosis D\u2217 has overwhelming probability.\nThe general debugging setting we consider also envisions the opportunity for the user to specify some background knowledge B, i.e. a set of axioms that are known to be correct. B is then incorporated in the calculations throughout the ontology debugging procedure, but no axiom in B may take part in a\n301\n302 CHAPTER 22. MOTIVATION AND BASIC CONCEPTS\ndiagnosis. For example, in case the user knows that a subset of axioms in O is definitely sound, all axioms in this subset are added to B before initiating the debugging session. The advantage of this over simply not considering the axioms in B at all is, that the semantics of axioms in B is not lost and can be exploited, e.g., in query generation. B and O \\ B partition the original ontology into a set of correct and possibly incorrect axioms, respectively. In the debugging session, only O := O \\ B is used to search for diagnoses. This can reduce the search space for diagnoses substantially. Another application of background knowledge could be the reuse of an existing ontology to support successful debugging. For example, when formulating an ontology about medical terms, a thoroughly curated reference ontology B could be leveraged to find own formulations contradicting the correct ones in B, which would not be found without integration of B into the debugging procedure.\nMore formally, ontology debugging can be defined in terms of a diagnosis problem instance, for which we search for solutions, i.e. diagnoses, that enable to formulate the target ontology:\nDefinition 22.1 (Diagnosis Problem Instance, Target Ontology). Let O = T \u222a A be an ontology with terminological axioms T and assertional axioms A, B a set of axioms which are assumed to be correct (background knowledge), R a set of requirements to O, P and N respectively a set of positive and negative test cases, where each test case p \u2208 P and n \u2208 N is a set of axioms. Then we call the tuple \u3008O,B,P ,N \u3009R a diagnosis problem instance (DPI). An ontology O\u2217 is called target ontology w.r.t. \u3008O,B,P ,N \u3009R iff all the following conditions hold:\n\u2200 r \u2208 R : O\u2217 \u222a B fulfills r \u2200 p \u2208 P : O\u2217 \u222a B |= p \u2200n \u2208 N : O\u2217 \u222a B 6|= n.\nDefinition 22.2 (Diagnosis). We call D \u2286 O a diagnosis w.r.t. a DPI \u3008O,B,P ,N \u3009R iff (O \\ D) \u222a ( \u22c3\np\u2208P p) is a target ontology w.r.t. \u3008O,B,P ,N \u3009R. A diagnosis D w.r.t. a DPI is minimal iff there is no D\u2032 \u2282 D such that D\u2032 is a diagnosis w.r.t. this DPI. The set of minimal diagnoses w.r.t. a DPI is denoted by mD.\nNote that a diagnosis D gives complete information about the correctness of each axiom axk \u2208 O, i.e. all axi \u2208 D are assumed to be faulty and all axj \u2208 O \\ D are assumed to be correct.\nExample 22.1 Consider O := T \u222a A with terminological axioms T := O1 \u222a O2 \u222aM12:\nO1 ax 1 : PhD v Researcher ax 2 : Researcher v DeptEmployee\nO2 ax 3 : PhDStudent v Student ax 4 : Student v \u00acDeptMember\nM12 ax 5 : PhDStudent v PhD ax 6 : DeptEmployee v DeptMember\nand an assertional axiom A = {PhDStudent(s)}, where M12 is an automatically generated set of axioms serving as semantic links between O1 and O2. The given ontology O is inconsistent since it describes s as both a DeptMember and not.\nLet us assume that the assertion PhDStudent(s) is considered as correct and is thus added to the background theory, i.e. B := A, and that no test cases are initially specified, i.e. the sets P and N are empty. For the resulting DPI \u3008T ,A, \u2205, \u2205\u3009{coherence} the set of minimal diagnoses mD = {D1 : [ax 1],D2 : [ax 2],D3 : [ax 3],D4 : [ax 4],D5 : [ax 5],D6 : [ax 6]}. mD can be computed by a diagnosis algorithm such as the one presented in [FS05].\n303\nWith six minimal diagnoses for only six ontology axioms, this example already gives an idea that in many cases |mD| can get very large. Note that generally the computation of all minimal diagnoses w.r.t. a given DPI is not feasible within reasonable time due to the complexity of the underlying algorithms. Therefore, in practice, especially in an interactive scenario where reaction time is essential, a set of leading diagnoses D \u2286mD is considered as a representative for mD.36 Concerning the optimal number of leading diagnoses, a trade-off between representativeness and complexity of associated computations w.r.t. D needs to be found.\nWithout any prior knowledge in terms of diagnosis fault probabilities or specified test cases, each diagnosis in D is equally likely to be the target diagnosis D\u2217. In other words, for each D \u2208 D w.r.t. the DPI \u3008T ,A, \u2205, \u2205\u3009{coherence}, the ontology (O \\ D) \u222a ( \u22c3 p\u2208P p) meets all the conditions defining a target ontology. However, besides postulating coherence the user might want the target ontology to entail that s is a student as well as a researcher, i.e. O\u2217 |= t1 where t1 := {Researcher(s), Student(s)}. Formulating t1 as a positive test case yields the DPI \u3008T ,A, {t1}, \u2205\u3009{coherence}, for which only diagnoses D2,D4,D6 \u2208 D are valid and enable to formulate a correspondingO\u2217. All other diagnoses in D are ruled out by the fact that t1 \u2208 P , which means they have a probability of zero of being the target diagnosis. If t1 \u2208 N , in contrast, this would imply that D2,D4,D6 had to be rejected.\nSo, it depends on the test cases specified by a user which diagnosis will finally be identified as target diagnosis. Also, the order in which test cases are specified, is crucial. For instance, consider the test cases t1 := {PhD(s)} and t2 := {Student(s)}. If t1 \u2208 P is specified before t2 \u2208 N , then t1 \u2208 P is redundant, since the only diagnosis agreeing with t2 \u2208 N is D3 which preserves also the entailment t1 in the resulting target ontology O\u2217 = (O \\ D3) \u222a \u2205 without explicating it as a positive test case.\nSince it is by no means trivial to get the right \u2013 in the sense of most informative \u2013 test cases formulated in the proper order such that the number of test cases necessary to detect the target diagnosis is minimized, interactive debugging systems offer the functionality to automatize selection of test cases. The benefit is that the user can just concentrate on \u201canswering\u201d the provided test cases which means assigning them to either P or N . We call such automatically generated test cases queries. The theoretical foundation for the application of queries is the fact that O \\ Di and O \\ Dj for Di 6= Dj \u2208 D entail different sets of axioms.\nDefinition 22.3 (Query, Partition). Let D be a set of minimal diagnoses w.r.t. a DPI \u3008O,B,P ,N \u3009R and O\u2217i := (O \\ Di) \u222a B \u222a ( \u22c3 p\u2208P p) for Di \u2208 D. Then a set of axioms Xj 6= \u2205 is called a query w.r.t. D iff D+j := {Di \u2208 D | O\u2217i |= Xj} 6= \u2205 and D \u2212 j := {Di \u2208 D | \u2203x \u2208 N \u222a R : O\u2217i \u222aXj violates x} 6= \u2205. The (unique) partition of a query Xj is denoted by \u3008D+j ,D \u2212 j ,D 0 j \u3009 where D0j = D\\ (D + j \u222aD \u2212 j ). XD terms a set of queries and associated partitions w.r.t. D in which one and the same partition of D occurs at most once and only if there is an associated query for this partition.\nNote that, in general, there can be nq queries for a particular partition of D where nq can be zero or some positive integer. We are interested in (1) only those partitions for each of which nq \u2265 1 and (2) only one query for each such partition. The set XD includes elements such that (1) and (2) holds. XD for a given set of minimal diagnoses D w.r.t. a DPI can be generated as shown in Algorithm 13. In each iteration, given a set of diagnoses D+k \u2282 D, common entailments37 Xk := { e | \u2200Di \u2208 D+k : O\u2217i |= e\n} are computed (GETENTAILMENTS) and used to classify the remaining diagnoses in D \\ D+k to obtain the partition \u3008D+k ,D \u2212 k ,D 0 k\u3009 associated with Xk. Then, if the partition \u3008D + k ,D \u2212 k ,D 0 k\u3009 does not already occur in XD (INCLUDESPARTITION), the query Xk is minimized [SFFR12] (MINIMIZEQUERY) such that its partition is preserved, yielding a query X \u2032k \u2286 Xk such that any X \u2032\u2032k \u2282 X \u2032k is not a query or has\n36So, we will speak of D instead of mD throughout this work. Note that the restriction to a subset of mD does not necessarily have implications on the completeness of the associated ontology debugging algorithm. E.g., the algorithm can be iterative and recompute new diagnoses on demand and nevertheless guarantee completeness (as the algorithm presented in this work).\n37Note, when we speak of entailments throughout this work, we address (only) the finite set of entailments computed by the classification and realization services of a DL-reasoner.\n304 CHAPTER 22. MOTIVATION AND BASIC CONCEPTS\nAlgorithm 13 Generation of Queries and Partitions Input: DPI \u3008O,B,P ,N \u3009R, set of minimal diagnoses D w.r.t. \u3008O,B,P ,N \u3009R Output: a set of queries and associated partitions XD\n1: XD \u2190 \u2205 2: for D+k \u2282 D do 3: Xk \u2190 GETENTAILMENTS(O,B,P ,D+k ) 4: if Xk 6= \u2205 then 5: for Dr \u2208 D \\D+k do 6: if O\u2217r |= Xk then 7: D+k \u2190 D + k \u222a {Dr} 8: else if REQVIOLATED(O\u2217r \u222aXk) then 9: D\u2212k \u2190 D \u2212 k \u222a {Dr}\n10: else 11: D0k \u2190 D0k \u222a {Dr} 12: if \u00acINCLUDESPARTITION(XD, \u2329 D+k ,D \u2212 k ,D 0 k \u232a ) then\n13: XD \u2190 XD \u222a MINIMIZEQUERY( \u2329 Xk, \u2329 D+k ,D \u2212 k ,D 0 k \u232a\u232a ) 14: return XD\nnot the same partition. Finally, X \u2032k is added to XD together with its partition \u3008D + k ,D \u2212 k ,D 0 k\u3009. Function REQVIOLATED(arg) returns true if arg violates some requirement in R or entails some negative test case in N .\nAsking the user a query Xj means asking them (O\u2217 |= Xj?). Let the answering of queries by a user be modeled as function u : XD \u2192 {t, f}. If uj := u(Xj) = t, then P \u2190 P \u222a {Xj} and D\u2190 D \\D\u2212j . Otherwise, N \u2190 N \u222a {Xj} and D \u2190 D \\D+j . Prospectively, according to Definition 22.2, only those diagnoses are considered in the set D that comply with the new DPI obtained by the addition of a test case. This allows us to formalize the problem we address in this work:\nProblem Definition 22.1 (Query Selection). Given D w.r.t. a DPI \u3008O,B,P ,N \u3009R, a stopping criterion stop : D \u2192 {t, f} and a user u, find a next query Xj \u2208 XD such that (1) (Xj , . . . , Xq) is a query sequence of minimal length and (2) there exists a D\u2217 \u2208 D w.r.t. \u3008O,B,P \u2032,N \u2032\u3009R such that stop(D\u2217) = t, where P \u2032 := P \u222a {Xi |Xi \u2208 {Xj , . . . , Xq}, ui = t} and N \u2032 := N \u222a {Xi |Xi \u2208 {Xj , . . . , Xq}, ui = f}.\nTwo strategies for selecting the \u201cbest\u201d next query have been proposed [SFFR12]:\nSplit-In-Half Strategy (SPL) selects the query Xj which minimizes the following scoring function: scsplit(Xj) := \u2223\u2223|D+j | \u2212 |D\u2212j |\u2223\u2223+ |D0j |\nSo, SPL prefers queries which eliminate half of the diagnoses independently of the query outcome.\nEntropy-Based Strategy (ENT) uses information about prior probabilities pt for the user to make a mistake when using a syntactical construct of type t \u2208 CT (L), where CT (L) is the set of constructors available in the used knowledge representation language L, e.g. {\u2200,\u2203,v,\u00ac,t,u} \u2282 CT (OWL DL). These fault probabilities pt are assumed to be independent and used to calculate fault probabilities of axioms axk as\np(axk) = 1\u2212 \u220f\nt\u2208CT (1\u2212 pt)n(t)\n305\nwhere n(t) is the number of occurrences of construct type t in axk. The probabilities of axioms can in turn be used to determine fault probabilities of diagnoses Di \u2208 D as\np(Di) = \u220f\naxr\u2208Di\np(ax r) \u220f\naxs\u2208O\\Di\n(1\u2212 p(ax s)). (22.1)\nENT selects the query Xj \u2208 XD with highest expected information gain, i.e. that minimizes the following scoring function [SFFR12]:\nscent(Xj) = \u2211\na\u2208{t,f}\np(uj = a) log2 p(uj = a) + p(D 0 j ) + 1\nwhere\np(uj = t) = \u2211 Dr\u2208D+j p(Dr) + 1 2 p(D0j )\nand\np(D0j ) = \u2211 Dr\u2208D0j p(Dr)\nThe answer uj = a is used to update probabilities p(Dk) for Dk \u2208 D according to the Bayesian formula, yielding p(Dk|uj = a).\nThe result of the evaluation in [SFFR12] shows that ENT reveals better performance than SPL in most of the cases. However, SPL proved to be the best strategy in situations when misleading prior information is provided, i.e. the target diagnosis D\u2217 has low probability. So, one can regard ENT as a high risk strategy with high potential to perform well, depending on the priorly unknown quality of the given fault information. SPL, in contrast, can be seen as a no-risk strategy without any potential to leverage good meta information. Therefore, selection of the proper combination of prior probabilities {pt | t \u2208 CT (L)} and query selection strategy is crucial for successful diagnosis discrimination and minimization of user interaction.\nExample 22.2 (Example 22.1 continued) To illustrate this, let a user who wants to debug our example ontology O set p(ax i) := 0.001 for axi(i=1,...,4) and p(ax 5) := 0.1, p(ax 6) := 0.15, e.g. because the user doubts the correctness of ax 5, ax 6 while being quite sure that axi(i=1,...,4) are correct. Assume that D2 corresponds to the target diagnosis D\u2217, i.e. the settings provided by the user are inept. Application of ENT starts with computation of prior fault probabilities of diagnoses p(D1) = p(D2) = p(D3) = p(D4) = 0.003, p(D5) = 0.393, p(D6) = 0.591 (Formula 22.1). Then (O\u2217 |= X1?) with X1 := {DeptEmployee(s), Student(s)}, will be identified as the optimal query since it has the minimal score scent(X1) = 0.02 (see Table 22.1 for queries and partitions w.r.t. the example ontology). However, since the unfavorable answer u1 = f is given, this query eliminates only two of six diagnoses D4 and D6. The Bayesian probability update then yields p(D2) = p(D3) = p(D4) = 0.01 and p(D5) = 0.97. As next query X2 with scent(X2) = 0.811 is selected and answered unfavorably (u2 = t) as well which results in the elimination of only one of four diagnoses D5. By querying X3 (scent(X3) = 0.082, u3 = t) and X4 (sc(X4) = 0, u4 = t), the further execution of this procedure finally leads to the target diagnosis D2. So, application of ENT requires four queries to find D\u2217. If SPL is used instead, only three queries are required. The algorithm can select one of the two queries X5 or X9 because each eliminates half of all diagnoses in any case. Let the strategy select X5 which is answered positively (u5 = t). As successive queries, X6 (u6 = f ) and X1 (u1 = f ) are selected, which leads to the revelation of D\u2217 = D2.\n306 CHAPTER 22. MOTIVATION AND BASIC CONCEPTS\nThis scenario demonstrates that the no-risk strategy SPL (three queries) is more suitable than ENT (four queries) for fault probabilities which disfavor the target diagnosis. Let us suppose, on the other hand, that probabilities are assigned more reasonably in our example, e.g. D\u2217 = D6. Then it will take ENT only two queries (X1, X6) to find D\u2217 while SPL will still require three queries, e.g. (X5, X1, X6).\nThis example indicates that, unless the target diagnosis is known in advance, one can never be sure to select the best strategy from SPL and ENT. In Chapte 23 we present a learning query selection algorithm that combines the benefits of both SPL and ENT. It adapts the way of selecting the next query depending on the elimination rate (like SPL) and on information gain (like ENT). Thereby its performance approaches the performance of the better of both SPL and ENT.\nChapter 23\nRIO: Risk Optimization for Query Selection\nThe proposed Risk Optimization Algorithm (RIO) extends ENT strategy with a dynamic learning procedure that learns by reinforcement how to select the next query. Its behavior is determined by the achieved performance in terms of diagnosis elimination rate w.r.t. the set of leading diagnoses D. Good performance causes similar behavior to ENT, whereas aggravation of performance leads to a gradual neglect of the given meta information, and thus to a behavior akin to SPL. Like ENT, RIO continually improves the prior fault probabilities based on new knowledge obtained through queries to a user.\nRIO learns a \u201ccautiousness\u201d parameter c whose admissible values are captured by the user-defined interval [c, c]. The relationship between c and queries is as follows:\nDefinition 23.1 (Cautiousness of a Query). We define the cautiousness cq(Xi) of a query Xi \u2208 XD as follows:\ncq(Xi) := min\n{ |D+i |, |D \u2212 i | }\n|D| \u2208\n0, \u230a |D| 2 \u230b |D|  =: [cq, cq] A query Xi is called braver than query Xj iff cq(Xi) < cq(Xj). Otherwise Xi is called more cautious than Xj . A query with maximum cautiousness cq is called no-risk query.\nDefinition 23.2 (Elimination Rate). Given a query Xi and the corresponding answer ui \u2208 {t, f}, the elimination rate\ne(Xi, ui) = |D\u2212i | |D|\nif ui = t\nand\ne(Xi, ui) = |D+i | |D|\nif ui = f\nThe answer ui to a query Xi is called favorable iff it maximizes the elimination rate e(Xi, ui). Otherwise ui is called unfavorable. The minimal or worst case elimination rate minui\u2208{t,f}(e(Xi, ui)) of Xi is denoted by ewc(Xi).\n307\n308 CHAPTER 23. RIO: RISK OPTIMIZATION FOR QUERY SELECTION\nSo, the cautiousness cq(Xi) of a query Xi is exactly the worst case elimination rate, i.e. cq(Xi) = ewc(Xi) = e(Xi, ui) given that ui is the unfavorable query result. Intuitively, parameter c characterizes the minimum proportion of diagnoses in D which should be eliminated by the successive query.\nDefinition 23.3 (High-Risk Query). Given a query Xi and cautiousness c, Xi is called a high-risk query iff cq(Xi) < c, i.e. the cautiousness of the query is lower than the algorithm\u2019s current cautiousness value c. Otherwise,Xi is called non-high-risk query. By NHRc(XD) \u2286 XD we denote the set of non-high-risk queries w.r.t. c. For given cautiousness c, the set of queries XD can be partitioned in high-risk queries and non-high-risk queries.\nExample 23.1 (Example 22.2 continued) Let the user specify c := 0.3 for the set D with |D| = 6. Given these settings, X1 := {DeptEmployee(s), Student(s)} is a non-high-risk query since its partition \u3008D+1 ,D \u2212 1 ,D 0 1\u3009 = \u3008{D4,D6} , {D1,D2,D3,D5} , \u2205\u3009 and thus its cautiousness cq(X1) = 2/6 \u2265 0.3 = c. The query X2 := {PhD(s)} with the partition \u3008{D1,D2,D3,D4,D6} , {D5} , \u2205\u3009 is a high-risk query because cq(X2) = 1/6 < 0.3 = c andX3 := {Researcher(s), Student(s)} with \u3008{D2,D4,D6}, {D1,D3,D5}, \u2205\u3009 is a no-risk query due to cq(X3) = 3/6 = cq .\nGiven a user\u2019s answer us to a query Xs, the cautiousness c is updated depending on the elimination rate e(Xs, us) by c \u2190 c + cadj where the cautiousness adjustment factor cadj := 2 (c \u2212 c)adj . The scaling factor 2 (c\u2212c) regulates the extent of the cautiousness adjustment depending on the interval length c\u2212 c. More crucial is the factor adj that indicates the sign and magnitude of the cautiousness adjustment:\nadj :=\n\u230a |D| 2 \u2212 \u03b5 \u230b |D| \u2212 e(Xs, us)\nwhere \u03b5 \u2208 (0, 12 ) is a constant which prevents the algorithm from getting stuck in a no-risk strategy for even |D|. E.g., given c = 0.5 and \u03b5 = 0, the elimination rate of a no-risk query e(Xs, us) = 12 resulting always in adj = 0. The value of \u03b5 can be set to an arbitrary real number, e.g. \u03b5 := 14 . If c + cadj is outside the user-defined cautiousness interval [c, c], it is set to c if c < c and to c if c > c. Positive cadj is a penalty telling the algorithm to get more cautious, whereas negative cadj is a bonus resulting in a braver behavior of the algorithm. Note, for the user-defined interval [c, c] \u2286 [cq, cq] must hold. c\u2212 cq and cq\u2212 c represent the minimal desired difference in performance to a high-risk (ENT) and no-risk (SPL) query selection, respectively. By expressing trust (disbelief) in the prior fault probabilities through specification of lower (higher) values for c and/or c, the user can take influence on the behavior of RIO.\nExample 23.2 (Example 23.1 continued) Assume p(ax i) := 0.001 for axi(i=1,...,4) and p(ax 5) := 0.1, p(ax 6) := 0.15 and the user rather disbelieves these fault probabilities and thus sets c = 0.4, c = 0 and c = 0.5. In this case RIO selects a no-risk query X3 just as SPL would do. Given u3 = t and |D| = 6, the algorithm computes the elimination rate e(X3, t) = 0.5 and adjusts the cautiousness by cadj = \u22120.17 which yields c = 0.23. This allows RIO to select a higher-risk query in the next iteration, whereupon the target diagnosis D\u2217 = D2 is found after asking three queries. In the same situation, ENT (starting with high-risk query X1) would require four queries.\nRIO, described in Algorithm 14, starts with the computation of minimal diagnoses. GETDIAGNOSES function implements a combination of HS-Tree and QuickXPlain algorithms [SFFR12]. Using uniformcost search, the algorithm extends the set of leading diagnoses D with a maximum number of most probable minimal diagnoses such that |D| \u2264 n.\nThen the GETPROBABILITIES function calculates the fault probabilities p(Di) for each diagnosis Di of the set of leading diagnoses D using Formula (22.1). Next it adjusts the probabilities as per the Bayesian theorem taking into account all previous query answers which are stored in P and N . Finally, the resulting probabilities padj(Di) are normalized. Based on the set of leading diagnoses D,\n309\nAlgorithm 14 Risk Optimization Algorithm (RIO) Input: DPI \u3008O,B,P ,N \u3009R, fault probabilities of diagnoses DP , cautiousness C = (c, c, c), number of leading\ndiagnoses n to be considered, acceptance threshold \u03c3 Output: a minimal diagnosis D w.r.t. \u3008O,B,P ,N \u3009R\n1: D\u2190 \u2205 2: repeat 3: D\u2190 GETDIAGNOSES(D, n,O,B,P ,N ) 4: DP \u2190 GETPROBABILITIES(DP,D,P ,N ) 5: XD \u2190 GENERATEQUERIES(O,B,P ,D) 6: Xs \u2190 GETMINSCOREQUERY(DP,XD) 7: if GETQUERYCAUTIOUSNESS(Xs,D) < c then 8: Xs \u2190 GETALTERNATIVEQUERY(c,XD, DP,D) 9: if GETANSWER(Xs) = yes then\n10: P \u2190 P \u222a {Xs} 11: else 12: N \u2190 N \u222a {Xs} 13: c\u2190 UPDATECAUTIOUSNESS(D,P ,N , Xs, c, c, c) 14: until ABOVETHRESHOLD(DP, \u03c3) \u2228 ELIMINATIONRATE(Xs) = 0 15: return MOSTPROBABLEDIAG(D, DP )\nGENERATEQUERIES generates queries according to Algorithm 13. GETMINSCOREQUERY determines the best query Xsc \u2208 XD according to scent:\nXsc = arg min Xk\u2208XD (scent(Xk))\nIf Xsc is a non-high-risk query, i.e. c \u2264 cq(Xsc) (determined by GETQUERYCAUTIOUSNESS), Xsc is selected. In this case, Xsc is the query with best information gain in XD and moreover guarantees the required elimination rate specified by c.\nOtherwise, GETALTERNATIVEQUERY selects the query Xalt \u2208 XD (Xalt 6= Xsc) which has minimal score scent among all least cautious non-high-risk queries Lc. That is,\nXalt = arg min Xk\u2208Lc (scent(Xk))\nwhere\nLc := {Xr \u2208 NHRc(XD) | \u2200Xt \u2208 NHRc(XD) : cq(Xr) \u2264 cq(Xt)}\nIf there is no such query Xalt \u2208 XD, then Xsc is selected. Given the user\u2019s answer us, the selected query Xs \u2208 {Xsc ,Xalt} is added to P or N accordingly (see Chapter 22). In the last step of the main loop the algorithm updates the cautiousness value c (function UPDATECAUTIOUSNESS) as described above.\nBefore the next query selection iteration starts, a stop condition test is performed. The algorithm evaluates whether the most probable diagnosis is at least \u03c3% more likely than the second most probable diagnosis (ABOVETHRESHOLD) or none of the leading diagnoses has been eliminated by the previous query, i.e. GETELIMINATIONRATE returns zero for Xs. If a stop condition is met, the presently most likely diagnosis is returned (MOSTPROBABLEDIAG).\nChapter 24\nEvaluation\nGoals. This evaluation should demonstrate that (1) there is a significant discrepancy between existing strategies SPL and ENT concerning user effort where the winner depends on the quality of meta information, (2) RIO exhibits superior average behavior compared to ENT and SPL w.r.t. the amount of user interaction required, irrespective of the quality of specified fault information, (3) RIO scales well and (4) its reaction time is well suited for an interactive debugging approach.\nProvenance of Test Data. As data source for the evaluation we used faulty real-world ontologies produced by automatic ontology matching systems (cf. Example 22.1). Matching of two ontologies Oi and Oj is understood as detection of correspondences between elements of these ontologies [SE13]:\nDefinition 24.1 (Ontology matching). Let Q(O) \u2286 S(O) denote the set of matchable elements in an ontology O, where S(O) denotes the signature of O. An ontology matching operation determines an alignmentMij , which is a set of correspondences between matched ontologies Oi and Oj . Each correspondence is a 4-tuple \u3008xi, xj , r, v\u3009, such that xi \u2208 Q(Oi), xj \u2208 Q(Oj), r is a semantic relation and v \u2208 [0, 1] is a confidence value. We call OiMj := Oi \u222a \u03c6(Mij) \u222a Oj the aligned ontology for Oi and Oj where \u03c6 maps each correspondence to an axiom.\nLet in the followingQ(O) be the restriction to atomic concepts and roles in S(O), r \u2208 {v,w,\u2261} and \u03c6 the natural alignment semantics [MS09] that maps correspondences one-to-one to axioms of the form xi r xj . We evaluate RIO using aligned ontologies by the following reasons: (1) Matching results often cause inconsistency/incoherence of ontologies. (2) The (fault) structure of different ontologies obtained through matching generally varies due to different authors and matching systems involved in the genesis of these ontologies. (3) For the same reasons, it is hard to estimate the quality of fault probabilities, i.e. it is unclear which of the existing query selection strategies to chose for best performance. (4) Available reference mappings can be used as correct solutions of the debugging procedure.\nTest Datasets. We used two datasets D1 and D2: Each faulty aligned ontologyOiMj in D1 is the result of applying one of four ontology matching systems to a set of six independently created ontologies in the domain of conference organization. For a given pair of ontologies Oi 6= Oj , each system produced an alignmentMij . The average size of OiMj per matching system was between 312 and 377 axioms. D1 is a superset of the dataset used in [Stu08] for which all debugging systems under evaluation manifested correctness or scalability problems. D2, used to assess the scalability of RIO, is the set of ontologies from the ANATOMY track in the Ontology Alignment Evaluation Initiative38 (OAEI) 2011.5 [SE13], which\n38http://oaei.ontologymatching.org\n311\n312 CHAPTER 24. EVALUATION\ncomprises two input ontologies O1 (11545 axioms) and O2 (4838 axioms). The size of the aligned ontologies generated by results of seven different matching systems was between 17530 and 17844 axioms. 39\nReference Solutions. For the dataset D1, based on a manually produced reference alignment Rij \u2286 Mij for ontologies Oi,Oj (cf. [MST08]), we were able to fix a target diagnosis D\u2217 := \u03c6(Mij \\ Rij) for each incoherent OiMj . In cases where D\u2217 represented a non-minimal diagnosis, it was randomly redefined as a minimal diagnosis D\u2217 \u2282 \u03c6(Mij \\ Rij). In case of D2, given the ontologies O1 and O2, the output M12 of a matching system, and the correct reference alignment R12, we fixed D\u2217 as follows: We carried out (prior to the actual experiment) a debugging session with DPI \u3008\u03c6(M12 \\ R12), O1 \u222a O2 \u222a \u03c6(M12 \u2229R12), \u2205, \u2205\u3009{coherence} and randomly chose one of the identified diagnoses as D\u2217.\nTest Settings. We conducted 4 experiments EXP-i (i = 1, . . . , 4), the first two with dataset D1 and the other two with D2. In experiments 1 and 3 we simulated good fault probabilities by setting p(axk) := 0.001 for axk \u2208 Oi \u222a Oj and p(axm) := 1 \u2212 vm for axm \u2208 Mij , where vm is the confidence of the correspondence underlying axm. Unreasonable fault information was used in experiments 2 and 4. In EXP-4 the following probabilities were defined: p(axk) := 0.01 for axk \u2208 Oi \u222a Oj and p(axm) := 0.001 for axm \u2208 Mij . In EXP-2, in contrast, we used probability settings of EXP-1, but altered the target diagnosis D\u2217 in that we precomputed (before the actual experiment started) the 30 most probable minimal diagnoses, and from these we selected the diagnosis with the highest number of axioms axk \u2208 OiMj \\ \u03c6(Mij) as D\u2217.\nThroughout all four experiments, we set |D| := 9 (which proved to be a good trade-off between computation effort and representativeness of the leading diagnoses), \u03c3 := 85% and as input parameters for RIO we set c := 0.25 and [c, c] := [cmin, cmax] = [0, 49 ]. To let tests constitute the highest challenge for the evaluated methods, the initial DPI was specified as \u3008OiMj , \u2205, \u2205, \u2205\u3009{coherence}, i.e. the entire search space was explored without adding parts ofOiMj to B, althoughD\u2217 was always a subset of the alignment Mij only. In practice, given such prior knowledge, the search space could be severely restricted and debugging greatly accelerated. All tests were executed on a Core-i7 (3930K) 3.2Ghz, 32GB RAM with Ubuntu Server 11.04 and Java 6 installed.40\nMetrics. Each experiment involved a debugging session of ENT, SPL as well as RIO for each ontology in the respective dataset. In each debugging run we measured the number of required queries (q) until D\u2217 was identified, the overall debugging time (debug) assuming that queries are answered instantaneously and the reaction time (react), i.e. the average time between two successive queries. The queries generated in the tests were answered by an automatic oracle by means of the target ontology OiMj \\ D\u2217.\nObservations. The difference w.r.t. the number of queries per test run between the better and the worse strategy in {SPL,ENT} was absolutely significant, with a maximum of 2300% in EXP-4 and averages of 190% to 1145% throughout all four experiments (Figure 24.2). Moreover, results show that varying quality of fault probabilities in {EXP-1,EXP-3} compared to {EXP-2,EXP-4} clearly affected the performance of ENT and SPL (see first two rows in Figure 24.2). This perfectly motivates the application of RIO.\nResults of both experimental sessions, \u3008EXP-1,EXP-2\u3009 and \u3008EXP-3,EXP-4\u3009, are summarized in Figures 24.1(a) and 24.1(b), respectively. The figures show the (average) number of queries asked by RIO\n39Source ontologies, produced alignments by each matcher, and reference alignments were downloaded from http://bit.ly/Zffkow (D1) and http://bit.ly/Koh1NB as well as http://bit.ly/MU5Ca9 (D2).\n40See http://code.google.com/p/rmbd/wiki for code and details.\n313\nand the (average) differences to the number of queries needed by the per-session better and worse strategy in {SPL,ENT}, respectively. The results illustrate clearly that the average performance achieved by RIO was always substantially closer to the better than to the worse strategy. In both EXP-1 and EXP-2, throughout 74% of 27 debugging sessions, RIO worked as efficiently as the best strategy (Figure 24.2). In 26% of the cases in EXP-2, RIO even outperformed both other strategies; in these cases, RIO could save more than 20% of user interaction on average compared to the best other strategy. In one scenario in EXP-1, it took ENT 31 and SPL 13 queries to finish, whereas RIO required only 6 queries, which amounts to an improvement of more than 80% and 53%, respectively. In \u3008EXP-3,EXP-4\u3009, the savings achieved by RIO were even more substantial. RIO manifested superior behavior to both other strategies in 29% and 71% of cases, respectively. Not less remarkable, in 100% of the tests in EXP-3 and EXP-4, RIO was at least as efficient as the best other strategy. Recalling Figure 24.2, this means that RIO can\n314 CHAPTER 24. EVALUATION\nPrinted by Mathematica for Students\navoid query overheads of over 2000%. Table 24.1, which provides average values for q, react and debug per strategy, demonstrates that RIO is the best choice in all experiments w.r.t. q. Consequently, RIO is suitable for both good and poor meta information.\nAs to time aspects, RIO manifested good performance, too. Since times consumed in \u3008EXP-1,EXP-2\u3009 are almost negligible, consider the more meaningful results obtained in \u3008EXP-3,EXP-4\u3009. While the best reaction time in both experiments was achieved by SPL, we can clearly see that SPL was significantly inferior to both ENT and RIO concerning q and debug. RIO revealed the best debugging time in EXP-4, and needed only 2.2% more time than the best strategy (ENT) in EXP-3. However, if we assume the user being capable of reading and answering a query in, e.g., 30 sec on average, which is already quite fast, then the overall time savings of RIO compared to ENT in EXP-3 would already account for 5%. Doing the same thought experiment for EXP-4, RIO would save 25% (w.r.t. ENT) and 50% (w.r.t. SPL) of debugging time on average. All in all, the measured times confirm that RIO is well suited for interactive debugging.\nChapter 25\nRelated Work\nA similar interactive technique was presented in [NRG12], where a user is successively asked single ontology axioms in order to obtain a partition of a given ontology into a set of desired and a set of undesired consequences. However, given an inconsistent/incoherent ontology, this technique starts from an empty set of desired consequences aiming at adding to this set only axioms which preserve coherence, whereas our approach starts from the complete ontology aiming at finding a minimal set of axioms responsible for the violation of pre-specified requirements.\nAn approach for alignment debugging was proposed in [Mei11]. This work describes approximate algorithms for computing a \u201clocal optimal diagnosis\u201d and complete methods to discover a \u201cglobal optimal diagnosis\u201d. Optimality in this context refers to the maximum sum of confidences in the resulting coherent alignment. In contrast to our framework, diagnoses are determined automatically without support for user interaction. Instead, techniques for manual revision of the alignment as a procedure independent from debugging are demonstrated.\n315\nChapter 26\nSummary and Conclusions\nWe have shown problems of state-of-the-art interactive ontology debugging strategies w.r.t. the usage of unreliable meta information. To tackle this issue, we proposed a learning strategy which combines the benefits of existing approaches, i.e. high potential and low risk. Depending on the performance of the diagnosis discrimination actions, the trust in the a-priori information is adapted. Tested under various conditions, our algorithm revealed good scalability and reaction time as well as superior average performance to two common approaches in the field in all tested cases w.r.t. required user interaction. Highest achieved savings amounted to more than 80% and user interaction overheads resulting from the wrong choice of strategy of up to 2300% could be saved. In the hardest test cases, the new strategy was not only on average, but in 100% of the test cases at least as good as the best other strategy.\n317\nPart VI"}, {"heading": "A Direct Approach to Sequential", "text": "Diagnosis of High Cardinality Faults in\nKnowledge Bases\n319\n321\nIn this part we cover the topic of efficiently dealing with KB debugging problems involving high cardinality faults. This part relies on material [SFRF14c, SFRF14a, SFRF14b] published in the Proceedings of the 21st European Conference on Artificial Intelligence (ECAI 2014), in DX 2014 - 25th International Workshop on Principles of Diagnosis and in the Proceedings of the Third International Workshop on Debugging Ontologies and Ontology Mappings (WoDOOM14), respectively.41\n41We are glad to report that the publication [SFRF14a] was awarded the Best Paper Award at the DX Workshop that took place in Graz, Austria in September 2014 (see http://dx-2014.ist.tugraz.at).\nChapter 27\nIntroduction to the Problem\nModel-based diagnosis (MBD) [Rei87] is a general method which can be used to find errors in hardware, software, knowledge-bases (KBs), orchestrated web-services, configurations, etc. In particular, ontology (KB) debugging tools [KPHS07, FS05, HPS08] can localize a (potential) fault by finding sets of axioms D \u2286 K called diagnoses for the KB K. Diagnoses are generated using minimal conflict sets, i.e. irreducible sets of axioms CS \u2286 K that violate some requirements, by using a consistency checker (black-box approach). At least all axioms of a minimal diagnosis must be modified or deleted in order to formulate a fault-free knowledge-base K\u2217. A knowledge-base K is faulty if some requirements, such as consistency of K, presence or absence of specific entailments, are violated.\nSequential MBD methods [dKW87] applied to KB debugging acquire additional information in order to discriminate between diagnoses [SFFR12]. Generated queries are answered by some oracle providing additional observations about the entailments of a valid KB. As various applications show, the standard methods work very satisfactorily for cases where the number of faults (minimal conflict sets) is low (single digit number), consistency checking is fast (single digit number of seconds), and sufficient possibilities for observations are available.\nHowever, there are situations when KBs comprise a large number of faults. For example, in ontology matching scenarios two KBs with several thousands of axioms are merged into a single one. High quality matchers (e.g. [JRG11]) require the diagnosis of such substantially extended KBs, but could not apply standard diagnosis methods because of the large number of minimal diagnoses and their high cardinality. E.g. there are cases when the minimum cardinality of diagnoses is greater than 20.\nIn order to deal with hard diagnosis instances, we propose to relax the requirement for sequential diagnosis to compute a set of preferred minimal diagnoses, such as a set of most probable diagnoses. Instead, we compute just some set of minimal diagnoses which can be used for query generation. This allows to use direct computation of diagnoses [SU06] without computing conflict sets. The direct approach was applied for non-interactive diagnosis of ontologies [DQPS11, BKP12] and constraints [FSZ11]. A recent approach [SKFP12] does not generate the standard HS-TREE, but still depends on the minimization of conflict sets, i.e. |D| minimized conflicts have to be discovered. Consequently, if |D| m, substantially more consistency checks are required, where |D| is the cardinality of the minimal diagnosis and m is the number of minimal diagnoses required for query generation.\nSince we are replacing the set of most probable diagnoses by just a set of minimal diagnoses, some important practical questions have to be addressed. (1) Is a substantial number of additional queries needed, (2) is this approach able to locate the faults, and (3) how efficient is this approach?\nIn order to answer these questions we have exploited the most difficult diagnosis problems of the ontology alignment competition [EFvH+11]. Our evaluation shows that sequential diagnosis by direct diagnosis generation needs approximately the same number of queries (\u00b11) in order to identify the faults.\n323\n324 CHAPTER 27. INTRODUCTION TO THE PROBLEM\nThis evaluation was carried out for cases where the standard sequential diagnosis method was applicable. Furthermore, the evaluation shows that our proposed method is able to locate faults in all cases correctly, particularly in those cases where debugging sessions by means of the standard method are not successful (due to overwhelming time or space consumption). Moreover, for the hardest cases (i.e., more than 4 minutes overall debugging time), the additional computation costs introduced by the direct method apart from the costs needed for theorem proving are less than 50%, i.e. reasoning costs amount to more than two thirds of overall computation time.\nThe rest of Part VI is organized as follows: Chapter 28 gives a brief introduction to the main notions of sequential KB diagnosis. The details of the suggested algorithms are presented in Chapter 29. In Chapter 30 we provide evaluation results whereupon Chapter 31 gives a conclusion.\nChapter 28\nBasic Concepts\nIn the following we present (1) the fundamental concepts regarding the diagnosis of KBs and (2) the interactive localization of axioms which must be changed.\nDiagnosis of KBs. Given a knowledge-baseK which is a set of logical sentences (axioms), the user can specify particular requirements during the knowledge-engineering process. The most basic requirement is satisfiability, i.e. a logical model exists. A further frequently employed requirement is coherence. Coherence requires that there exists a model s.t. the interpretation of every unary predicate is non-empty. In other words, if we add \u2203Y a(Y ) to K for every unary predicate a, then the resulting KB must be satisfiable. In addition, as it is common practice in software engineering, the knowledge-engineer (user for short) may specify test cases. Test cases are axioms which must (not) be entailed by a valid KB.\nDefinition 28.1. Given a set of axioms P (called positive test cases) and a set of axioms N (called negative test cases), a knowledge-base K\u2217 is valid iff it fulfills the following requirements:\n1. K\u2217 is satisfiable (and coherent if required)\n2. K\u2217 |= p \u2200p \u2208 P\n3. K\u2217 6|= n \u2200n \u2208 N\nLet us assume that there is a non-valid KB K, then a set of axioms D \u2286 K must be removed and possibly some axioms EX must be added by the user s.t. an updated K\u2217 becomes valid, i.e. K\u2217 := (K \\D) \u222aEX . The goal of diagnosis is to provide information to the users which are the sets of axioms D (which is called a diagnosis) that must be changed. In order to prevent unnecessary changes,D is often required to be subset-minimal, i.e. the set should be as small as possible. Furthermore, we allow the user to define a set of axioms B (called the background theory) which must not be changed (i.e. the correct axioms). More formally:\nDefinition 28.2. Given a diagnosis problem instance (DPI) specified by \u3008K,B, P,N\u3009 where\n\u2022 K is a knowledge-base,\n\u2022 B a background theory,\n\u2022 P a set of axioms which must be implied by a valid knowledge-base K\u2217 and\n\u2022 N a set of axioms, each of which must not be implied by K\u2217\n325\n326 CHAPTER 28. BASIC CONCEPTS\nD \u2286 K is a diagnosis w.r.t. \u3008K,B, P,N\u3009 iff K \\D can be extended by a set of logical sentences EX such that:\n1. (K \\ D) \u222a B \u222a EX is consistent\n2. (K \\ D) \u222a B \u222a EX |= p for all p \u2208 P\n3. (K \\ D) \u222a B \u222a EX 6|= n for all n \u2208 N\nD is a minimal diagnosis iff there is no D\u2032 \u2282 D such that D\u2032 is a diagnosis. D is a minimum cardinality diagnosis iff there is no diagnosis D\u2032 such that |D\u2032| < |D|.42\nThe following proposition of [SFFR12] characterizes diagnoses by replacing EX with the positive test cases.\nCorollary 28.1. Given a DPI \u3008K,B, P,N\u3009, a set of axioms D \u2286 K is a diagnosis w.r.t. \u3008K,B, P,N\u3009 iff\n(K \\ D) \u222a B \u222a { \u2227 p\u2208P p}\nis satisfiable (coherent) and\n\u2200n \u2208 N : (K \\ D) \u222a B \u222a { \u2227 p\u2208P p} 6|= n\nHereafter we assume that a diagnosis always exists. Proposition 28.1. A diagnosisD w.r.t. a DPI \u3008K,B, P,N\u3009 exists iff B\u222a{ \u2227\np\u2208P p} is consistent (coherent) and \u2200n \u2208 N : B \u222a { \u2227 p\u2208P p} 6|= n\nFor the computation of diagnoses conflict sets are usually employed to constrain the search space. A conflict set is the part of the KB that preserves the inconsistency/incoherency.\nDefinition 28.3. Given a DPI \u3008K,B, P,N\u3009, a set of axiomsCS \u2286 K is a conflict set w.r.t. \u3008K,B, P,N\u3009 iff CS\u222aB\u222a{ \u2227 p\u2208P p} is inconsistent (incoherent) or there is an n \u2208 N such that CS\u222aB\u222a{ \u2227 p\u2208P p} |= n. CS is minimal iff there is no CS\u2032 \u2282 CS such that CS\u2032 is a conflict set.43\nMinimal conflict sets can be used to compute the set of minimal diagnoses as it is shown in [Rei87]. The idea is that each diagnosis must include at least one element of each minimal conflict set.44\nProposition 28.2. D is a (minimal) diagnosis w.r.t. the DPI \u3008K,B, P,N\u3009 iff D is a (minimal) hitting set for the set of all minimal conflict sets w.r.t. \u3008K,B, P,N\u3009.\nFor the generation of a minimal conflict set, diagnosis systems use a divide-and-conquer method (e.g. QUICKXPLAIN [Jun04], for short QX), which we discussed in Sections 4.4.1 and 4.4.2. In the worst case, QX requires O(|CS| log( |K||CS| )) calls to the reasoner, where CS is the returned minimal conflict set.\nThe computation of minimal diagnoses in KB debugging systems is implemented using Reiter\u2019s Hitting Set HS-TREE algorithm [Rei87] (cf. Algorithm 2 in Chapter 4). The algorithm constructs a directed\n42If clear from the context, we will often callD simply a diagnosis without explicitly stating the DPI w.r.t. which it is a diagnosis in the rest of Part VI.\n43If clear from the context, we will often call CS simply a conflict set without explicitly stating the DPI w.r.t. which it is a conflict set in the rest of Part VI.\n44In the rest of Part VI, we consider only minimal conflict sets to avoid the issues concerning the pruning rule [Rei87] described in [GSW89].\n327\ntree from the root to the leaves, where each non-leave node is labeled with a minimal conflict set and leave nodes are labeled by X (no conflicts) or \u00d7 (pruned).\nEach (X) node corresponds to a minimal diagnosis. The minimality of the diagnoses is guaranteed by the minimality of conflict sets used for labeling the nodes, the pruning rule and the breadth-first strategy of the tree generation. Moreover, because of the breadth-first strategy the minimal diagnoses are generated in increasing order of their cardinality. Under the assumption that diagnoses with lower cardinality are more probable than those with higher cardinality, HS-TREE generates most probable minimal diagnoses first.\nDiagnoses Discrimination. For many real-world DPIs, a diagnosis system can return a large number of (minimal) diagnoses. Each minimal diagnosis corresponds to a different set of axioms in the given KB K. All the axioms of any minimal diagnosis might be deleted from K or changed accordingly in order to formulate a valid K\u2217. The user may extend the test cases P and N such that diagnoses are eliminated, thus identifying exactly the correct minimal diagnosis. For discriminating between minimal diagnoses we assume that the user knows some of the sentences a valid K\u2217 must (not) entail, that is the user serves as an oracle.\nProperty 3. Given a DPI \u3008K,B, P,N\u3009, a set of diagnoses D w.r.t. \u3008K,B, P,N\u3009, and a logical sentence Q representing the oracle query K\u2217 |= Q . If the oracle gives the answer yes then Di \u2208 D is a diagnosis w.r.t. \u3008K,B, P \u222a {Q}, N\u3009 iff both conditions hold:\n(K \\ Di) \u222a B \u222a { \u2227 p\u2208P p} \u222a {Q} is consistent\n\u2200n \u2208 N : (K \\ Di) \u222a B \u222a { \u2227 p\u2208P p} \u222a {Q} 6|= n\nIf the oracle gives the answer no then Di \u2208 D is a diagnosis w.r.t. \u3008K,B, P,N \u222a {Q}\u3009 iff both conditions hold:\n(K \\ Di) \u222a B \u222a { \u2227 p\u2208P p} is consistent\n\u2200n \u2208 (N \u222a {Q}) : (K \\ Di) \u222a B \u222a { \u2227 p\u2208P p} 6|= n\nHowever, many different queries might exist for some set of diagnoses |D| \u2265 2, in the extreme case exponentially many (in |D|). To select the best query, the authors in [SFFR12] suggest two query selection strategies: SPLIT-IN-HALF (SPL) and ENTROPY (ENT). The first strategy is a greedy approach preferring queries which allow to remove half of the diagnoses in D, for both answers to the query. The second is an information-theoretic measure, which estimates the information gain for both outcomes of each query and returns the query that maximizes the expected information gain. The prior fault probabilities required for evaluating the ENT measure can be obtained from statistics of previous diagnosis sessions. For instance, if the user has problems to apply \u201c\u2203\u201d, then the diagnosis logs are likely to contain more repairs of axioms including this quantifier. Consequently, the prior fault probabilities of axioms including \u201c\u2203\u201d should be higher. Given the fault probabilities of axioms, one can calculate prior fault probabilities of diagnoses as well as evaluate ENT (see [SFFR12] for more details). The queries for both strategies are constructed by exploiting so called classification and realization services provided by description logic reasoners. Given a KB K and interpreting unary predicates as classes (rsp. concepts), the classification generates the inheritance (subsumption) tree, i.e. the entailments K |= \u2200X p(X) \u2192 q(X), if p is a subclass of q. Realization computes, for each individual name t occurring in a KBK, a set of most specific classes p s.t. K |= p(t) (see [BCM+07] for details).\n328 CHAPTER 28. BASIC CONCEPTS\nDue to the number of diagnoses and the complexity of diagnosis computation, not all diagnoses are exploited for generating queries but a set of minimal diagnoses of size less or equal to some (small) predefined number m [SFFR12]. We call this set the leading diagnoses and denote it by D from now on. This set comprises the (most probable) minimal diagnoses which represent the set of all diagnoses.\nThe sequential KB debugging process can be sketched as follows. As input a DPI and some meta information, such as prior fault estimates F , query selection strategy sQ (SPL or ENT) and stop criterion \u03c3, are given. As output a minimal diagnosis is returned that has a posterior probability of at least 1\u2212\u03c3. For sufficiently small \u03c3 this means that the returned diagnosis is highly probable whereas all other minimal diagnoses are highly improbable.\n1. Using QX and HS-TREE, compute a set of leading diagnoses D of cardinality min(m, a), where a is the number of all minimal diagnoses w.r.t. the DPI and m is the number of leading diagnoses predefined by a user.\n2. Use the prior fault probabilities F and the already specified test cases to compute (posterior) probabilities of diagnoses in D by the Bayesian Rule (cf. [SFFR12]).\n3. If some diagnosis D \u2208 D has a probability greater than or equal to 1\u2212 \u03c3 or the user accepts D as the axioms to be changed then stop and return D.\n4. Use D to generate a set of queries and select the best query Q according to sQ.\n5. Ask the user K\u2217 |= Q and, depending on the answer, add Q either to P or to N .\n6. Remove elements from D violating the newly acquired test case.\n7. Repeat at Step 1.\nChapter 29\nInteractive Direct Diagnosis of Knowledge Bases\nThe novelty of our approach is the interactivity combined with the direct calculation of diagnoses. To this end we will utilize an \u201cinverse\u201d version of the QX algorithm [Jun04] called INV-QX and an associated \u201cinverse\u201d version of HS-TREE termed INV-HS-TREE.\nThis combination of algorithms was first used in [FSZ11]. However, we introduced two modifications: (i) a depth-first search strategy instead of breadth-first and (ii) a new pruning rule which moves axioms from K to B instead of just removing them from K, since not adding them to B might result in losing some of the minimal diagnoses.\nINV-QX \u2013 Key Idea. INV-QX relies on the monotonic semantics of the used knowledge representation language. The algorithm takes a DPI \u3008K,B, P,N\u3009 and a ranking heuristic \u227a as input and outputs either one minimal diagnosis or \u2019no diagnosis exists\u2019. The ranking heuristic assigns a fault probability to each axiom in K, if this information is available; otherwise every axiom has the same rank.\nThe main idea behind Algorithm 15 is to start with the set D0 = \u2205 and extend it until a subset of axioms D \u2286 K is found such that D is a minimal diagnosis with respect to Definition 28.2. In the first steps (lines 1-3), Algorithm 15 defines a (potentially) faulty set of axioms K\u2032 and a set B\u2032 of axioms assumed to be correct and sorts K\u2032 w.r.t. the ranking heuristic (SORT). Next, INV-QX verifies whether a diagnosis exists for the input data (line 4), i.e. if the conditions given by Proposition 28.1 are met. This is accomplished by a call to the VERIFY function (defined in line 18 ff.) which requires a reasoner that implements consistency checking (ISCONSISTENT) and allows to decide whether a set of axioms K\u2032 entails some axiom n or not (ENTAILS). Concretely, VERIFY tests for given arguments B (set of correct axioms), D (potential minimal diagnosis), K (potentially faulty set of axioms), N (negative test cases) whether the set D is a minimal diagnosis or not according to Corollary 28.1. In case no diagnosis exists, the algorithm returns \u2019no diagnosis exists\u2019, otherwise it calls the function FINDDIAG in line 6.\nFINDDIAG (line 7) is the main function of the algorithm which takes six arguments as input. The values of the arguments B, K and N remain constant during the recursion and are required only for the verification of requirements, i.e. calls to the VERIFY function. The values of D (potential diagnosis), \u2206 (axioms most recently added toD) andK\u2206 (part of the original knowledge base that is currently analyzed for the inclusion of axioms that are elements of the sought minimal diagnosis) on the other hand change throughout the recursive calls of FINDDIAG. The two latter sets are obtained by recurrently partitioning the set K\u2206 (SPLIT and GETELEMENTS in lines 12-14). In most of the implementations SPLIT is specified so as to return k = b|K\u2206|/2c which causes the splitting of K\u2206 into partitions of equal cardinality (this results in the best worst case time complexity [Jun04]). The algorithm pursues this to divide-and-conquer\n329\n330 CHAPTER 29. INTERACTIVE DIRECT KB DIAGNOSIS\nAlgorithm 15 INV-QX(K,B, P,N,\u227a) Input: faulty set of axioms K, set of background axioms B, set of positive test cases P , set of negative test cases N ,\nranking heuristic \u227a Output: a minimal diagnosis D or \u2019no diagnosis exists\u2019\n1: K\u2032 \u2190 K \\ B 2: B\u2032 \u2190 B \u222a P 3: K\u2032 \u2190 SORT(K\u2032,\u227a) 4: if \u00acVERIFY(B\u2032, \u2205, \u2205, N) then 5: return \u2019no diagnosis exists\u2019 6: return FINDDIAG(B\u2032, \u2205,K\u2032,K\u2032,K\u2032, N)\n7: procedure FINDDIAG(B,D,\u2206,K\u2206,K, N ) returns a minimal diagnosis 8: if \u2206 6= \u2205 \u2227 VERIFY(B,D,K, N) then 9: return \u2205\n10: if |K\u2206| = 1 then 11: return K\u2206 12: k \u2190 SPLIT(|K\u2206|) 13: K1 \u2190 GETELEMENTS(K\u2206, 1, k) 14: K2 \u2190 GETELEMENTS(K\u2206, k + 1, |K\u2206|) 15: D2 \u2190 FINDDIAG(B,D \u222aK1,K1,K2,K, N) 16: D1 \u2190 FINDDIAG(B,D \u222aD2,D2,K1,K, N) 17: return D1 \u222a D2\n18: procedure VERIFY(B,D,K, N ) returns true or false 19: K\u2032 \u2190 (K \\ D) \u222a B 20: if \u00acISCONSISTENT(K\u2032) then 21: return false 22: for n \u2208 N do 23: if ENTAILS(K\u2032, n) then 24: return false 25: return true\nstrategy (lines 15 and 16) until it identifies that the set D is a diagnosis (line 8). In further iterations the algorithm minimizes this diagnosis by splitting it into sub-diagnoses of the form D = D\u2032 \u222a K\u2206, where K\u2206 contains only one axiom. In case D is a diagnosis and D\u2032 is not, the algorithm decides that K\u2206 is a subset of the sought minimal diagnosis. Just as the original QX algorithm, INV-QX always terminates and it returns a minimal diagnosis for a given DPI (provided there exists one).\nINV-QX requires O(|D| log( |K||D| )) calls to a reasoner to find a minimal diagnosis D. Moreover, in opposite to SAT or CSP methods, e.g. [NPQW13], INV-QX can be used to compute diagnoses in cases when satisfiability checking is beyond NP. For instance, reasoning for most of the KBs used in Chapter 30 is EXPTIME-complete.\nINV-QX is a deterministic algorithm and returns one and the same minimal diagnosis if applied twice to one and the same DPI. In order to obtain a different next diagnosis, the DPI used as input for INV-QX must be modified accordingly. To this end, we employ the INV-HS-TREE algorithm.\nINV-HS-TREE \u2013 Construction. The algorithm is inverse to the HS-TREE algorithm in the sense that nodes are now labeled by minimal diagnoses (instead of minimal conflict sets) and a path from the root to an open node is a partial conflict set (instead of a partial diagnosis). The algorithm constructs a directed tree from the root to the leaves, where each node nd is labeled either with a minimal diagnosis\n331\nD or \u00d7 (pruned) which indicates that the node is closed. For each s \u2208 D there is an outgoing edge labeled by s. Let H(nd) be the set of edge labels on the path from the root to the node nd. Initially the algorithm generates an empty root node and adds it to a LIFO-queue, thereby implementing a depth-first search strategy. Until the required number m of minimal diagnoses is reached or the queue is empty, the algorithm removes the first node nd from the queue and labels nd by applying the following steps:\n1. (reuse): D \u2208 D if D \u2229H(nd) = \u2205, add for each s \u2208 D a node to the LIFO-queue, or\n2. (pruned): \u00d7 if INV-QX(K \\ H(nd),B \u222a H(nd), P,N) = \u2019no-diagnosis-exists\u2019, (according to Proposition 28.1), or\n3. (compute): D if INV-QX(K \\H(nd),B \u222aH(nd), P,N) = D; add D to D and add for each s \u2208 D a node to the LIFO-queue.\nReuse of known diagnoses in Step 1 and the addition ofH(nd) to the background theory B in Steps 2 and 3 allows the algorithm to force INV-QX to search for a minimal diagnosis that is different to all already computed minimal diagnoses in D. So, if neither Step 1 nor Step 2 are applicable, INV-HS-TREE calls INV-QX which is guaranteed to compute a new minimal diagnosis D which is then added to the set D.\nINV-HS-TREE \u2013 Update Procedure for Interactivity. Since paths in INV-HS-TREE are (1) irrelevant and need not be maintained, and (2) only a small (linear) number of nodes/paths is in memory due to the application of a depth-first search, the update procedure after a query Q has been answered involves a reconstruction of the tree. In particular, by answering Q, m \u2212 k of (maximally) m leading diagnoses are invalidated and deleted from memory. The k still valid minimal diagnoses are used to build a new tree. To this end, the root is labeled by any of these k minimal diagnoses and a tree is constructed as described above where the k diagnoses are incorporated for the reuse check. Note that the recalculation of a diagnosis that has been invalidated by a query is impossible as in subsequent iterations a new DPI is considered which includes the answered query as a test case.\nINV-HS-TREE \u2013 Comparison to HS-TREE. Since INV-QX(K,B \u222a H(nd), P,N) = \u2019no diagnosis exists\u2019 means H(nd) is a conflict set w.r.t. the current DPI \u3008K,B, P,N\u3009, in INV-HS-TREE any path that is a conflict set is automatically closed. This makes a pruning rule similar to the one in HS-TREE which closes a node nd given an alternative path H(nd\u2032) to a closed node nd\u2032 with H(nd\u2032) \u2286 H(nd) obsolete. So, INV-HS-TREE benefits from the fact that minimality of diagnoses is independent of path-minimality, and thereby might save time for comparison of exponentially many paths over HS-TREE.\nAnother great advantage of INV-HS-TREE over HS-TREE is that it can be constructed using a spacesaving depth-first strategy. The reason for this is again that minimality of paths (conflict sets) is irrelevant in INV-HS-TREE whereas in HS-TREE minimality of paths (diagnoses) is essential. In an implementation where successors of a node are generated one at a time in INV-HS-TREE, the space complexity of the entire tree construction is linear and amounts to O(2m) = O(m) where m is the predefined maximum number of leading diagnoses. This holds as k < m still valid diagnoses from the previous iteration are in memory, plus a path in the tree can comprise a maximum of m nodes corresponding to different (reused or new) diagnoses before the search is stopped (|D| = m). No conflict sets are stored.\nFor HS-TREE, by contrast, the worst-case space complexity is exponential, i.e. O(|CSmax|d) where |CSmax| is the size of the minimal conflict set with maximum cardinality (among all minimal conflict sets w.r.t. the given DPI) and d is the tree depth were m minimal diagnoses have been generated.\nThe crucial disadvantage of INV-HS-TREE compared to HS-TREE is that the former cannot guarantee the computation of diagnoses in a special order, e.g. minimum cardinality or maximum fault probability first.\n332 CHAPTER 29. INTERACTIVE DIRECT KB DIAGNOSIS\nExample 29.1 Consider a DPI with the following knowledge base K:\nax 1 : \u2200X c(X)\u2192 a(X) ax 4 : \u2200X b(X)\u2192 c(X) ax 2 : \u2200X c(X)\u2192 e(X) ax 5 : \u2200X b(X)\u2192 \u00acd(X) ax 3 : \u2200X a(X)\u2192 \u00ac(c(X) \u2228 \u00acb(X))\nthe background knowledge B = {a(v), b(w), c(s)}, one positive P = {d(v)} and one negative N = {e(w)} test case.\nLet us first show how a minimal diagnosis is computed by INV-QX (see Figure 29.1). The algorithm starts with an empty diagnosis D = \u2205 and K\u2206 containing all axioms of K 1 . VERIFY called in line 8 returns false since (B \u222a P ) \u222a (K \\ \u2205) is inconsistent. Since moreover |K\u2206| 6= 1 (line 10), the algorithm splits K\u2206 into {ax 1, ax 2} and {ax 3, ax 4, ax 5} (lines 12-14) and passes the sub-problem (line 15) to the next level of recursion 2 . Since the setD = {ax 1, ax 2} is not a diagnosis, i.e. the KB (B\u222aP )\u222a(K\\D) is inconsistent and |K\u2206| = | {ax 3, ax 4, ax 5} | 6= 1, the problem in K\u2206 is split one more time (lines 12-14). On the second level of recursion 3 the set D is a diagnosis, yet not a minimal one. The function VERIFY returns true and the algorithm starts to analyze the found diagnosis. Therefore, it verifies whether the last extension of the set D is a subset of a minimal diagnosis 4 . Since the extension includes only one axiom ax 3 and the extended set {ax 1, ax 2} is not a diagnosis, the algorithm concludes that ax 3 must be an element of the a minimal diagnosis. The leftmost branch of the recursion tree terminates and returns\n333\nXX\n\u3008ax 2, ax 4\u3009C \u3008ax 2, ax 4\u3009R\n\u3008ax 3, ax 5\u3009C \u3008ax 3, ax 5\u3009R\n\u3008ax 1, ax 3\u3009C\nax4 $$ ax2 zz\nax1zz ax3 $$\nax2 ax4 $$\n\u232a Minimal diagnoses:D1 = [ax 3, ax 4] D2 = [ax 2, ax 3]\nQuery: K\u2217 |= c(w) Answer: no\n\u232a\nIteration 1\nX\n\u00d7\n\u00d7\nX\n\u00d7\n\u3008ax 3, ax 5\u3009R\n\u3008ax 2, ax 4\u3009R\n\u3008ax 3, ax 4\u3009C\n\u3008ax 3, ax 5\u3009C\n\u3008ax 2, ax 4\u3009C\n\u3008ax 1, ax 3\u3009C\nax5\n$$ ax3 ax5 $$ax3zz\nax4\n$$ ax2 ax4 $$ax2zz\nax3 $$ ax1 zz \u232a\nMinimal diagnoses: D1 = [ax 3, ax 4] D3 = [ax 1, ax 4, ax 5]\nQuery: K\u2217 |= \u2200X a(X)\u2192 c(X) Answer: yes\nNo further minimal diagnoses, return D = {[ax 4, ax 3]}\nIteration 2\nFigure 29.3: Identification of the target diagnosis [ax4, ax3] using HS-TREE and QX computing conflicts on-demand. All computed node labels are denoted with C and all reused with R.\n{ax 3}. This axiom is added to the set D and the algorithm starts investigating whether the two axioms {ax 1, ax 2} also belong to a minimal diagnosis 5 . First, it tests the set {ax 3, ax 1} 6 , which is not a diagnosis, and in the next iteration it identifies {ax 3, ax 2} as a minimal diagnosis in node 7 which is the final output of INV-QX.\nIn general, for the sample DPI there are three minimal diagnoses {D1 : [ax 2, ax 3], D2 : [ax 3, ax 4], D3 : [ax 1, ax 4, ax 5]} and four minimal conflict sets {CS1 : \u3008ax 1, ax 3\u3009 , CS2 : \u3008ax 2, ax 4\u3009 , CS3 : \u3008ax 3, ax 5\u3009 , CS4 : \u3008ax 3, ax 4\u3009}.\nNow we show how INV-HS-TREE can be applied to find the (correct) diagnosis that allows the formulation of a valid KB (with the desired semantics in terms of entailments and non-entailments). Assume that the number of leading diagnoses required for query generation is set tom = 2. Applied to the sample DPI, INV-HS-TREE computes a minimal diagnosis D1 := [ax 2, ax 3] = INV-QX(K,B, P,N) to label the root node, see Figure 29.2. Next, it generates one successor node that is linked with the root by an edge labeled with ax 2. For this node INV-QX(K \\ {ax 2} ,B \u222a {ax 2} , P,N) yields a minimal diagnosis D2 := [ax 3, ax 4] disjoint with {ax 2}. Now |D| = 2 and a query is generated and answered as in Figure 29.2. Adding c(w) to the negative test cases invalidates D1 since (K \\ D1) \u222a B \u222a P |= c(w). In the course of the update, D1 is deleted and D2 used as the root of a new tree. An edge labeled with ax3 is created and diagnosis D3 := [ax 1, ax 4, ax 5] is generated. After the answer to the second query is added to the positive test cases, D3 is invalidated and all outgoing edge labels ax 3, ax 4 of the root D2 of the new tree are conflict sets for the current DPI \u3008K,B, {d(v),\u2200X a(X)\u2192 c(X)} , {e(w), c(w)}\u3009, i.e. all leaf nodes are labeled by \u00d7 and the tree construction is complete. So, D2 is returned as its probability is 1.\nFinally, let us compare the performance of HS-TREE [Rei87] with the one of INV-HS-TREE. Applied to our sample DPI, the standard interactive diagnosis process using HS-TREE first calls QX [Jun04]\n334 CHAPTER 29. INTERACTIVE DIRECT KB DIAGNOSIS\nwhich returns a minimal conflict set \u3008ax 1, ax 3\u3009 (Figure 29.3). This minimal conflict set is used to label the root node of the HS-TREE. By reuse (R) of already computed minimal conflict sets or further calls (C) to QX (if there is no conflict set to reuse) the algorithm extends the HS-TREE until m = 2 leading minimal diagnoses D := {D1,D2} for the DPI are computed. To discriminate between diagnoses in D, the query K\u2217 |= c(w) is computed. Given the answer no, D2 is invalidated which is reflected by the closing of the corresponding node in the tree (label \u00d7). The second iteration considers the new DPI \u3008K,B, {d(v)} , {e(w), c(w)}\u3009 and involves further expansion of (open nodes in) the tree under consideration of the pruning rule until the size of leading diagnoses D is 2, i.e. {D1,D3}. After the positive answer to the second query and closing of the invalidated diagnosis D3, the recalculation of D (not shown in Figure 29.3) yields no further minimal diagnoses. So, the algorithm terminates and returns D1. As we can see, HS-TREE comprises a lot of intermediate nodes in comparison to INV-HS-TREE. That leads to a dramatic difference in memory consumption between these two approaches.\nChapter 30\nEvaluation\nWe evaluated our approach DIR (based on INV-QX and INV-HS-TREE) versus the standard technique STD [SFFR12] (based on QX and HS-TREE) using a set of KBs created by automatic matching systems. Given two knowledge bases Ki and Kj , a matching system outputs an alignment Mij which is a set of correspondences between semantically related entities of Ki and Kj . Let Q(K) denote the set of all elements ofK for which correspondences can be produced, i.e. names of predicates. Each correspondence is a tuple \u3008xi, xj , r, v\u3009, where xi \u2208 Q(Ki), xj \u2208 Q(Kj) and xi, xj have the same arity, r \u2208 {\u2190,\u2194,\u2192} is a logical operator and v \u2208 [0, 1] is a confidence value. The latter expresses the probability of a correspondence to be correct. Let X be a vector of distinct logical variables with a length equal to the arity of xi, then each \u3008xi, xj , r, v\u3009 \u2208Mij is translated to the axiom \u2200X xi(X) r xj(X). Let K(Mij) denote the set of axioms resulting from such a translation for the alignment Mij . Then the result of the matching process is an aligned KB Kij = Ki \u222a K(Mij) \u222a Kj .\nThe KBs considered in this section were created by ontology matching systems participating in the Ontology Alignment Evaluation Initiative (OAEI) 2011 [EFvH+11]. Each matching experiment in the framework of OAEI represents a scenario in which a user obtains an alignment Mij by means of some (semi)automatic tool for two real-world ontologies Ki and Kj . The latter are KBs expressed by the Web Ontology Language (OWL) [GHM+08] whose semantics is compatible with the SROIQ description logic (DL). This DL is a decidable fragment of first-order logic for which a number of effective reasoning methods exist [BCM+07]. Note that, SROIQ is a member of a broad family of DL knowledge representation languages. All DL KBs considered in this evaluation are expressible in SROIQ.\nThe goal of the first experiment was to compare the performance of STD and DIR on a set of large, but diagnostically uncomplicated KBs, generated for the Anatomy experiment of OAEI.45 In this experiment the matching systems had to find correspondences between two KBs describing the human and the mouse anatomy. K1 (Human) andK2 (Mouse) include 11545 and 4838 axioms, respectively, whereas the size of the alignmentM12 produced by different matchers varies between 1147 and 1461 correspondences. Seven matching systems produced a classifiable but incoherent output. One system generated a classifiable and coherent aligned KB. However, this system employes a built-in heuristic diagnosis engine which does not guarantee to produce minimal diagnoses. That is, some axioms are removed without reason. Four systems produced KBs which could not be processed by current reasoning systems (e.g. HermiT) since these KBs could not be classified within 2 hours.\nFor testing the performance of our system we have to define the correct output of sequential diagnosis which we call the target diagnosis Dt. We assume that the only available knowledge is Mij together with Ki and Kj . In order to measure the performance of the matching systems the organizers of OAEI\n45All KBs and source code of programs used in the evaluation can be downloaded from http://code.google.com/p/rmbd/wiki/ DirectDiagnosis. The tests were performed on Core i7, 64GB RAM running Ubuntu, Java 7 and HermiT as DL reasoner.\n335\n336 CHAPTER 30. EVALUATION\nprovided a golden standard alignmentMt considered as correct. Nevertheless, we cannot assume thatMt is explicitly available since the matching system would have used this information. W.r.t. the knowledge available, any minimal diagnosis w.r.t. the DPI \u3008K(Mij),Ki \u222a Kj , \u2205, \u2205\u3009 (i.e. K(Mij) is the KB and Ki \u222a Kj used as background theory) can be selected as Dt. However, for every alignment we selected a minimal diagnosis as target diagnosis Dt which is outside the golden standard. By this procedure we mimic cases where additional information can be acquired such that no correspondence of the golden standard is removed in order to establish coherence. We stress that this setting is unfavorable for diagnosis since providing more information by exploiting the golden standard would reduce the number of queries to ask. Consequently, we limit the knowledge to Kij and use Kij \\ Dt to answer the queries.\nIn particular, the selection of a target diagnosis Dt for each Kij output by a matching system was done in two steps: (i) compute the set of all minimal diagnoses AD w.r.t. the correspondences which are not in the golden standard, i.e.K(Mij \\Mt), and use Ki\u222aKj \u222aK(Mij \u2229Mt) as background theory. The set of test cases are empty. I.e. the DPI is \u3008K(Mij \\Mt),Ki \u222a Kj \u222a K(Mij \u2229Mt), \u2205, \u2205\u3009. (ii) select Dt randomly from AD. The prior fault probabilities of axioms ax \u2208 K(Mij) expressing correspondences were set to 1\u2212 vax where vax is the confidence value provided by the matcher.\nThe tests were performed for the mentioned seven incoherent alignments where the input DPI is \u3008K(Mij),Ki \u222a Kj , \u2205, \u2205\u3009 and the output is a minimal diagnosis. We tested DIR and STD with both query selection strategies SPLIT-IN-HALF (SPL) and ENTROPY (ENT) in order to evaluate the quality of fault probabilities based on confidence values. Moreover, for generating a query, the number of leading diagnoses was limited to m = 9.\nThe results of the first experiment are presented in Table 30.1. DIR computed Dt within 36 sec. on average and slightly outperformed STD which required 36.7 sec. The number of asked queries was equal for both methods in all but two cases resulting from KBs produced by the MapSSS system. For these KBs, DIR required one query more using ENT and one query less using SPL. In general, the results obtained for the Anatomy case show that DIR and STD have similar performance in both runtime and number of queries. Both DIR and STD identified the target diagnosis. Moreover, the confidence values\n337\nprovided by the matching systems appeared to be a good estimate for fault probabilities. Thus, in many cases ENT was able to find Dt using one query only, whereas SPL used 4 queries on average.\nIn the first experiment, the identification of the target diagnosis by sequential STD required the computation of 19 minimal conflicts on average. Moreover, the average size of a minimum cardinality diagnosis over all KBs in this experiment was 7. In the second experiment (see below), where STD is not applicable, the cardinality of the target diagnosis is significantly higher.\nThe second experiment was performed on KBs of the OAEI Conference benchmark which turned out to be problematic for STD. For these KBs we observed that the minimum cardinality diagnoses comprise 18 elements on average. In 11 of the 13 KBs of the second experiment (see Table 30.2), STD was unable to find any diagnosis within 2 hours. In the other two cases STD succeeded to find one minimal diagnosis for csa-conference-ekaw and nine for ldoa-conference-confof. However, DIR even succeeded to find 30 minimal diagnoses for each KB within time acceptable for interactive diagnosis settings. Moreover, on average DIR was able to find 1 minimal diagnosis in 8.9 sec., 9 minimal diagnoses in 40.83 sec. and 30 minimal diagnoses in 107.61 sec. (see Column 2 of Table 30.2). This result shows that DIR is a stable and practically applicable method even in cases where a knowledge base comprises high-cardinality faults.\n338 CHAPTER 30. EVALUATION\nIn the Conference experiment, we first selected the target diagnosis Dt for each Kij just as it was done in the described Anatomy case. Next, we evaluated the performance of sequential DIR using both query selection methods. The results of the experiment presented in Table 30.2 show that DIR found Dt for each KB. On average DIR solved the problems more efficiently using ENT than SPL because also in the Conference case the confidence values provided a reasonable estimation of axiom fault probabilities. Only in three cases ENT required more queries than SPL.\nMoreover, the experiments show that the efficiency of debugging methods depends highly on the runtime of the underlying reasoner. For instance, in the hardest case consistency checking took 93.4% of the total time whereas all other operations \u2013 including construction of the search tree, generation and selection of queries \u2013 took only 6.6% of time. Consequently, sequential DIR requires only a small fraction of computation effort. Runtime improvements can be achieved by advances in reasoning algorithms or the reduction of the number of consistency checks. Currently, in order to generate a query, DIR requires O(m \u2217 |D| log( |K||D| )) checks to find m leading diagnoses.\nA further source for improvements can be observed for the ldoa-ekaw-iasted ontology where both methods asked the same number of queries. In this case, a sequential diagnosis session using ENT query selection method required only half of the consistency checks SPL did. However, an average consistency check made in the session using ENT took almost twice as long as an average consistency check using SPL. The analysis of this ontology showed that there is a small subset of axioms (called \u201chot spot\u201d in [GPS12]) which made reasoning considerably harder. As practice shows, they can be resolved by suitable queries. This can be observed in the ldoa-ekaw-iasted case where SPL acquired appropriate test cases early and thereby foundDt faster. Therefore, research and application of methods allowing fast identification of such hot spots might result in a significant improvement of diagnosis runtime.\nChapter 31\nSummary and Conclusions\nIn this part, we presented a sequential diagnosis method for faulty KBs which is based on the direct computation of minimal diagnoses. We were able to reduce the number of consistency checks by avoiding the computation of minimized conflict sets and by computing just some set of minimal diagnoses instead of a set of most probable diagnoses or a set of minimum cardinality diagnoses. The presented evaluation results in Chapter 30 indicate that the performance of the suggested sequential diagnosis system is either comparable with or outperforms the existing approach in terms of runtime and required number of queries in case a KB includes a large number of faults. The scalability of the algorithms was demonstrated on a set of large KBs including thousands of axioms.\n339\nPart VII\nEpilog\n341\n343\nIn this part we provide a discussion of related work in Chapter 32,46 summarize the contributions of this work in Chapter 33 and deal with our future work topics in Chapter 34.\n46Note that related work specific to topics addressed in Parts IV-VI is separately treated in these parts.\nChapter 32\nRelated Work\nTo the best of our knowledge no interactive KB debugging methods that ask a user automatically selected queries have been proposed to repair faulty (monotonic) KBs so far (except for our own previous works [SF10, SFFR12, RSFF13, SFRF14c]).\nNon-interactive debugging methods for KBs (ontologies) are introduced in [SHCH07, KPHS07, FS05]. Ranking of diagnoses and proposing a \u201cbest\u201d diagnosis is presented in [KPSCG06]. This method uses a number of measures such as (a) the frequency with which a formula appears in conflict sets, (b) the impact on the KB in terms of its \u201clost\u201d entailments when some formula is modified or removed, (c) provenance information about the formula and (d) syntactic relevance of a formula. All these measures are evaluated for each formula in a conflict set. The scores are then combined in a rank value which is associated with the corresponding formula. These ranks are then used by a modified hitting set tree algorithm that identifies diagnoses with a minimal rank. In this work no query generation and selection strategy is proposed if the intended diagnosis cannot be determined reliably with the given a-priori knowledge. In our work additional information is acquired until the minimal diagnosis with the intended semantics can be identified with confidence. In general, the work of [KPSCG06] can be combined with the approaches presented in our work as ranks of logical formulas can be taken into account together with other observations for calculating the prior probabilities of minimal diagnoses (see Section 4.6.1).\nThe idea of selecting the next query based on certain query selection measures was exploited in the generation of decisions trees [Qui86] and for selecting measurements in the model-based diagnosis of circuits [dKW87] (in both works, the minimal expected entropy measure was used). We extended these methods to query selection in the domain of KB debugging [SF10] and devised further query selection measures [SFFR12, RSFF13].\nAn approach for the debugging of faulty aligned KBs (ontologies) was proposed by [Mei11]. An aligned KB is the union of two KBs K1 and K2 and an alignment A1,2 (which is properly formatted as a set of logical formulas, cf. Definition 18 in [Mei11]). A1,2 is a set of correspondences (each with an associated automatically computed confidence value) produced by an automatic system (an ontology matcher) given K1 and K2 as inputs where each correspondence represents a (possible) semantic relationship between a term occurring in the first and a term occurring in the second input KB. The goal of a debugging system for faulty aligned KBs is usually the determination of a subset of the alignment A\u20321,2 \u2282 A1,2 such that the aligned KB using A\u20321,2 is not faulty. In terms of our approaches, this corresponds to the setting K := A1,2 and B := K1 \u222a K2. We have already shown in [RSFF12, SFRF12] that our systems can also be applied for fault localization in aligned KBs. The work of [Mei11] describes approximate algorithms for computing a \u201clocal optimal diagnosis\u201d and complete methods to discover a \u201cglobal optimal diagnosis\u201d. Optimality in this context refers to the maximum sum of confidences in the resulting repaired alignment A\u20321,2. In contrast to our framework, diagnoses are determined automatically\n345\n346 CHAPTER 32. RELATED WORK\nwithout support for user interaction. Instead, [Mei11] demonstrates techniques for the manual revision of the alignment as a procedure independent from debugging. Another difference to our approach is the way of detecting sources of faults. We rely on a divide-and-conquer algorithm [Jun04] for the identification of a minimal conflict set C \u2286 A1,2 (in [Mei11] C is called a MIPS, cf. [FS05, SHCH07]). In the worst case the method we use exhibits only O(|C| \u2217 log(|A1,2|/|C|)) calls of some function that performs a check for faults in a KB and internally uses a reasoner (in our case ISKBVALID, see Algorithm 1). The \u201cshrink\u201d strategy applied in [Mei11] (which is similar to the \u201cexpand-and-shrink\u201d method used in [KPHS07]), on the other hand, requires a worst case number of O(|A1,2|) calls to such a function. Empirical evaluations and a theoretical analysis of the best and worst case complexity of the \u201cexpand-and-shrink\u201d method compared to the divide-and-conquer method performed in [SFJ08] revealed that the latter is preferable over the former. It should be noted that a similar divide-and-conquer method as used in our work could most probably also be plugged into the system in [Mei11] instead of the \u201cshrink\u201d method.\nThere are some ontology matchers which incorporate alignment repair features: CODI [HSNM11], YAM++ [NB12], ASMOV [JMSK09] and KOSIMap [RP10], for instance, employ logic-based techniques to search for a set of predefined \u201canti-patterns\u201d which must not occur in the aligned ontology, either to avoid inconsistencies or incoherencies or to eliminate unwanted or redundant entailments. In case such a pattern is revealed, it is resolved by eliminating from the alignment some correspondences responsible for its occurrence. All the techniques incorporated in these matchers are distinct from the presented approaches in that they implement incomplete or approximate methods of alignment repair, i.e. not all alternative solutions to the alignment debugging problem are taken into account. As a consequence of this, on the one hand, the final alignment produced by these systems may still trigger faults in the aligned KB. On the other hand, a suboptimal solution may be found, e.g. in terms of the user-intended semantics w.r.t. the aligned ontology or other criteria such as alignment confidence or cardinality.\nAnother ontology matcher, LogMap 2 [JRGZH12], provides integrated debugging features and the opportunity for a user to interact during this process. However, the system is not really comparable with ours since it is very specialized and dedicated to the goal of producing a fault-free alignment. Concretely, there are at least two differences to our approach. First, LogMap 2 uses incomplete reasoning mechanisms in order to speed up the matching process. Hence, the output is not guaranteed to be fault-free. Second, the option for user interaction aims in fact at the revision of a set of correspondences, i.e. the sequential assessing of single correspondences as \u2019faulty\u2019 or \u2019correct\u2019. Our approach, on the contrary, asks the user queries (i.e. entailments of non-faulty parts of the KB).\nAn interactive technique similar to our approaches was presented in [NRG12], where a user is successively asked single KB formulas (ontology axioms) in order to obtain a partition of a given ontology into a set of desired or correct and a set of undesired or incorrect formulas. Whereas our strategies aim at finding a parsimonious solution involving minimal change to the given faulty KB in order to repair it, the method proposed in [NRG12] pursues a (potentially) more invasive approach to KB quality assurance, namely a (reasoner-supported) exhaustive manual inspection of (parts of) a KB. Given an inconsistent/incoherent KB, this technique starts from an empty set of desired formulas aiming at adding to this set only correct formulas of the KB which preserve consistency and coherency. Our approach, on the other hand, works its way forward the other way round in that it starts from the complete KB aiming at finding a minimal set of formulas to be deleted or modified which are responsible for the violation of the pre-specified requirements. Another difference of our approach compared to the one suggested in [NRG12] is the type of queries asked to the user and the way these are selected. Our method allows for the generation of queries which are not explicit formulas in the KB, but implicit consequences of nonfaulty parts of the KB. Besides, the set of selectable queries in our approach differs from one iteration to the next due to the changing set of leading diagnoses whereas queries (i.e. KB formulas) in [NRG12] are known in advance and the challenge is to figure out the best ordering of formulas to be assessed by the user. Whereas we apply mostly information theoretic measures (e.g. the minimal expected entropy in the set of leading diagnoses after a query has been answered), the authors in [NRG12] employ \u201cimpact\n347\nmeasures\u201d which, roughly speaking, indicate the number of automatically classifiable formulas in case of positive and, respectively, negative classification of a query (i.e. a particular formula).\nChapter 33\nSummary\nIn this work we motivated why appropriate tool assistance is a must when it comes to repairing faulty KBs. For, KBs that do not satisfy some minimal quality criteria such as logical consistency can make artificial intelligence applications relying on the domain knowledge modeled by this KB completely useless. In such a case, no meaningful reasoning or answering of queries about the domain is possible.\nNon-interactive debugging systems published in research literature often cannot localize all possible faults (incompleteness), suggest the deletion or modification of unnecessarily large parts of the KB (nonminimality), return incorrect solutions which lead to a repaired KB not satisfying the imposed quality requirements (unsoundness) or suffer from poor scalability due to the inherent complexity of the KB debugging problem [Stu08]. Even if a system is complete and sound and considers only minimal solutions, there are generally exponentially many solution candidates to select one from. However, any two repaired KBs obtained from these candidates differ in their semantics in terms of entailments and non-entailments. Selection of just any of these repaired KBs might result in unexpected entailments, the loss of desired entailments or unwanted changes to the KB which in turn might cause unexpected new faults during the further development or application of the repaired KB. Also, manual inspection of a large set of solution candidates can be time-consuming (if not practically infeasible), tedious and error-prone since human beings are normally not capable of fully realizing the semantic consequences of deleting a set of formulas from a KB.\nTo account for this issue, we evolved a comprehensive theory on which provably complete, sound and optimal (in terms of given probability information) interactive KB debugging systems can be built which suggest only minimal changes to repair a present KB. Interaction with a user is realized by asking the user queries. That is, a conjunction of logical formulas must be classified either as an intended or a non-intended entailment of the correct KB. To construct a query, only a minimal set of two solution candidates must be available. After the answer to a query is known, the search space for solutions is pruned. Iteration of this process until there is only a single solution candidate left yields a (repaired) solution KB which features exactly the semantics desired and expected by the user.\nWe presented algorithms for the computation of minimal conflict sets, i.e. irreducible faulty subsets of the KB, and for the computation of minimal diagnoses, i.e. irreducible sets of KB formulas that must be properly modified or deleted in order to repair the KB. We combined these algorithms with methods that derive probabilities of diagnoses from meta information about faults (e.g. the outcome of a statistical analysis) to constitute a non-interactive debugging system for monotonic KBs which computes minimal diagnoses in best-first order. Building on the idea of this non-interactive method, we devised a complete and sound best-first algorithm for the interactive debugging of monotonic KBs that allows a user to take part in the debugging process in order to figure out the best solution.\nIn order to integrate the new information collected by successive consultations of the user, the diag-\n349\n350 CHAPTER 33. SUMMARY\nnoses computation in an interactive system must be regularly stopped. That is, there must be alternating phases, on the one hand for the further exploration of the solution space in order to gain new evidence for query generation and on the other hand for user interaction. To this end, we proposed two new strategies for the iterative computation of minimal diagnoses that exactly serve this purpose. The first strategy, STATICHS, takes advantage of an artificial fixation of the solution set which guarantees the monotonic reduction of the solution space independently of the asked queries, the given answers or other parameters of the algorithm. In this vein, the complexity of this algorithm is initially known and the maximum overhead compared to the non-interactive algorithm is polynomially bound.47 On the downside, STATICHS cannot optimally exploit the information given by the answered queries and thus cannot employ powerful methods that enable a more efficient pruning of the solution search space.\nSuch powerful methods can be incorporated by the second suggested strategy, DYNAMICHS, the performance of which can be orders of magnitude better than the (initially fixed) performance of STATICHS in the best case. That is, the ability to fully incorporate the information gained from user interaction might lead to a modified problem instance for which only a single (best) solution exists with only a small fraction of the time, space and user effort needed by STATICHS. Moreover, the (exact) solution located by means of an interactive debugging session applying DYNAMICHS is generally a better (verified) solution than the (exact) solution found by use of STATICHS. However, the complexity of DYNAMICHS depends to a great degree on which queries are generated and which input parameters are chosen and the worst case complexity is not initially bound as in case of STATICHS. In the design of DYNAMICHS we put a particular emphasis on memory saving behavior which is manifested, for instance, by the manner how duplicate search tree paths are handled.\nFor selecting the best subsequent query in interactive debugging we first proposed and exhaustively analyzed two strategies: The \u201csplit-in-half\u201d strategy prefers queries which allow eliminating a half of the leading diagnoses. The entropy-based strategy employs information theoretic concepts to exploit knowledge about the likelihood of formulas to be faulty. Based on the probability of a formula containing an error we can predict the (expected) information gain produced by a query result, enabling us to select the best subsequent query according to a one-step-lookahead entropy-based scoring function.\nIn comprehensive experiments using real-world KBs we compared the entropy-based method with the \u201csplit-in-half\u201d strategy and witnessed a significant reduction in the number of queries required to identify the correct diagnosis when the entropy-based method is applied. Depending on the quality of the given prior fault probabilities, the required number of queries could be reduced by up to 60%. In order to evaluate the robustness of the entropy-based method we experimented with different prior fault probability distributions as well as different qualities of the prior probabilities. Furthermore, we investigated cases where knowledge about fault probabilities is missing or inaccurate. In case such knowledge is unavailable, the entropy-based methods ranks the diagnoses based on the number of syntax elements contained in a formula and the number of formulas in a diagnosis. Given that this is a reasonable guess (i.e. the sought diagnosis is not at the lower end of the diagnoses ranked by their prior probabilities), the entropy-based method outperformed \u201csplit-in-half\u201d. Moreover, even if the initial guess is not reasonable, the entropy-based method improves the accuracy of the probabilities as more questions are asked. Furthermore, the applicability of the approach to real-world KBs containing thousands of formulas was demonstrated by an extensive set of evaluations.\nWe showed that unconditional reliance upon the entropy-based method might still be problematic in the presence of fault information that is considerably uncertain. For, the entropy-based strategy fully exploits and gains from the given fault information. In this vein, it proved to speed up the debugging procedure in the normal case. However, we found out in experiments that it might also have a negative impact on the performance in the bad case where the actual solution diagnosis is rated as highly improb-\n47This holds under the reasonable assumption that, in practice, a debugging session will involve only a polynomial number of queries to an interacting user. Recall that a user can abort the debugging session at any time and select the currently most probable diagnosis as their solution to the debugging problem.\n351\nable. As an alternative, one might prefer to rely on a tool (e.g. \u201csplit-in-half\u201d) which does not consider any fault information at all. In this case, however, possibly well-chosen information cannot be exploited, resulting again in inefficient debugging actions.\nMinimal effort for the interacting user can be achieved if both the query selection method is chosen carefully and the provided fault information satisfies some minimum quality requirements. In particular, for deficient fault information and unfavorable strategy for query selection, we reported on cases where the overhead in terms of user effort exceeds 2000% (!) in comparison to employing a more favorable query selection strategy. Unfortunately, assessment of the fault information is only possible a-poteriori (after the debugging session is finished and the correct solution is known). To tackle this issue, we proposed a reinforcement learning strategy (RIO) which combines the benefits of the entropy-based and the \u201c\u2019split-in-half\u2019 approaches, i.e. high potential (to perform well) and low risk (to perform badly). RIO continuously adapts its behavior depending on the performance achieved and in this vein minimizes the risk of integrating low-quality fault information into the debugging process.\nThe RIO approach makes interactive debugging practical even in scenarios where reliable fault estimates are difficult to obtain. Tested under various conditions, the RIO algorithm revealed good scalability and reaction time as well as superior average performance to both the entropy-based as well as the \u201csplitin-half\u201d strategy in all tested cases w.r.t. required amount of user interaction. Highest achieved savings of RIO as against the best other strategy amounted to more than 80%. Further on, the performed evaluations provided evidence that for 100% of the cases in the hardest (from the debugging point of view) class of faulty test KBs, RIO performed at least as good as the best other strategy and in more than 70% of these cases it even manifested superior behavior to the best other strategy. Choosing RIO over other approaches can involve an improvement by the factor of up to 23, meaning that more than 95% of user time and effort might be saved per debugging session.\nMoreover, we came up with mechanisms for efficiently dealing with KB debugging problems involving high cardinality faults. In the standard interactive debugging approach described in the first parts of this work, the computation of queries is based on the generation of the set of most probable (or minimum cardinality) leading diagnoses. By this postulation, certain quality guarantees about the output solution can be given. However, we learned that dropping this requirement can bring about substantial savings in terms of time and especially space complexity of interactive debugging, in particular in debugging scenarios where faulty KBs are (partly) generated as a result of the application of automatic systems, e.g. KB (ontology) learning or matching systems.\nTo cope with such situations, we proposed to base query computation on any set of leading diagnoses using a \u201cdirect\u201d method for diagnosis generation. Contrary to the standard method that exploits minimal conflict sets, this approach takes advantage of the duality between minimal diagnoses and minimal conflict sets and employs \u201cinverse\u201d algorithms to those used in the standard approach in order to determine minimal diagnoses directly from the DPI without the indirection via conflict sets.\nWe studied the application of this direct method to high cardinality faults in KBs and noticed that the number of required queries per debugging session is hardly affected for cases when the standard approach is also applicable. However, the direct method proved applicable and able to locate the correct solution diagnosis also in situations when the standard approach (albeit one that not yet incorporates the powerful search tree pruning techniques introduced in this work) is not due to time or memory issues.\nWe want to point out that this work is unique in that it provides an in-depth theoretical workup of the topic of interactive KB debugging which (to the best of our knowledge) cannot be found in such a detailed fashion in other works. Furthermore, this is the first work that gives precise definitions of the problems addressed in interactive KB debugging. Additionally, it is unique in that it features (new) algorithms that provably solve these interactive KB debugging problems. To account for a tradeoff between solution quality and execution time, these algorithms are equipped with a feature to compute approximate solutions where the goodness of the approximation can be steered by the user. Another unique characteristic of this work is that it deals with an entire system of algorithms that are required for the interactive debug-\n352 CHAPTER 33. SUMMARY\nging of monotonic KBs, considers and details all algorithms separately, analyzes their complexity, proves their correctness and demonstrates how all these algorithms are orchestrated to make up a full-fledged and provably correct interactive KB debugging system.\nChapter 34\nFuture Work Topics\nThis work has given rise to several questions we will elaborate on in our future work:\nQuery Generation and Selection. Our discussions of the presented query generation methods have revealed some drawbacks (cf. Chapter 8). Albeit being a fixed-parameter tractable problem as argued, the exponential time complexity regarding the number of leading diagnoses |D| in case an optimal query is desired is clearly an aspect that should be improved. This high complexity arises from the paradigm of computing an optimal query w.r.t. some measure qsm() by calculating a (generally exponentially large) pool QP of queries in a first stage, whereupon the best query in QP according to qsm() is filtered out in a second stage.\nA key to solving this issue is the use of a different paradigm that does not rely on the computation of the pool QP. Instead, qualitative measures can be derived from quantitative measures that have been used in interactive debugging scenarios [SFFR12, RSFF13, SF10]. These qualitative measures provide a way to estimate the qsm() value of partial q-partitions, i.e. ones where not all leading diagnoses have been assigned to the respective set in the q-partition yet. In this way a direct search for a query with (nearly) optimal properties is possible. A similar strategy called CKK has been employed in [SFFR12] for the information gain measure qsm() := ENT() (see Section 9.3). From such a technique we can expect to save a high number of reasoner calls. Because usually only a small subset of q-partitions included in a query pool (of exponential cardinality) is required to find a query with desirable properties if the search is implemented by means of a heuristic that involves the exploration of seemingly favorable (potential) queries and (partial) q-partitions, respectively, first.\nAnother shortcoming of the paradigm of query pool generation and subsequent selection of the best query is the extensive use of reasoning services which may be computationally expensive (depending on the given DPI). Instead of computing a set of common entailments Q of a set of KBs K\u2217i first and consulting a reasoner to fill up the (q-)partition for Q in order to test whether Q is a query at all (see Chapter 8), the idea enabling a significant reduction of reasoner dependence is to compute some kind of canonical query without a reasoner and use simple set comparisons to decide whether the associated partition is a q-partition. Guided by qualitative properties mentioned before, a search for such q-partition with desirable properties can be accomplished without reasoning at all. Also, a set-minimal version of the optimal canonical query can be computed without reasoning aid. Only for the optional enrichment of the identified optimal canonical query by additional entailments and for the subsequent minimization of the enriched query, the reasoner may be employed. We will present strategies accounting for these ideas in the near future.\nAnother aspect that can be improved is that only one minimized version of each query is computed by Algorithm 4. That is, per q-partition P, there might be some set-minimal queries which do not occur\n353\n354 CHAPTER 34. FUTURE WORK TOPICS\nin the output set QP. From the point of view of how well a query might be understood by an interacting user, of course not all minimized queries can be assumed equally good in general. For instance, consider the minimized queries Q4 and Q10 in Table 8.3 on page 113. Both are equally good regarding their qpartitions (just the sets D+ and D\u2212 are commuted), but most people will probably agree that Q4 is much easier to comprehend from the logical point of view and thus much easier to answer.\nHence, in order to avoid a situation where a potentially best-understood query w.r.t. P is not included in QP, the query minimization process (see Section 8.3) might be adapted to take into account some information about faults the interacting user is prone to. This could be exploited to estimate how well this user might be able to understand and answer a query. For instance, given that the user frequently has problems to apply \u2203 in a correct manner to express what they intend to express, but has never made any mistakes in formulating implications\u2192, then the query Q1 = {\u2200X p(X)\u2192 q(X), r(a)} might be better comprehended than Q2 = {\u2200X\u2203Y s(X,Y )}. One way to achieve the finding of a well-understood query for some q-partition P is to run the query minimization MINQ more than once, each time with a modified input (using a hitting set tree to accomplish this in a systematic manner \u2013 cf. Chapter 4, where an analogue idea is used to compute different minimal conflict sets w.r.t. a DPI). In this way, different set-minimal queries for P can be identified and the process can be stopped when a suitable query is found.\nIn order to come up with such a strategy, however, one must first gain insight into how well a user might understand certain logical formalisms and what properties make a query easy to comprehend from the logical perspective. It is planned to gather corresponding data about different users in the scope of a user study and to utilize the results to achieve a model of \u201cquery hardness\u201d (by sticking to a similar overall methodology as used in [HBP11]) in order to come up with strategies for the determination of minimal queries that are easily understood. Note that such a model could also act as a guide how to specify the initial fault probabilities of syntactical elements that are used to obtain diagnoses probabilities (see Section 4.6).\nIncorporating A-Posteriori Probabilities into Diagnosis Search. As we discussed in Remark 9.3 on page 125, the a-priori (pD,prio()) and the a-posteriori (pD()) diagnoses probabilities might not only differ in terms of the probability values assigned to different diagnoses, but also in terms of the probability order of diagnoses. Incorporation of updated probabilities directly into the hitting set tree algorithms to be used for the determination of leading diagnoses in the order prescribed by an updated probability measure is only possible if there is an additional update operator (besides Bayes\u2019 Theorem for adapting diagnoses probabilities) that can be applied to formula probabilities. For, the latter are exploited in the hitting set tree to assign probability weights to paths that are not yet diagnoses (cf. pnodes() specified by Definition 4.9 and the discussion of Formula 4.6) in order to guide the search for minimal diagnoses in best-first order. Updated diagnosis probabilities are not helpful at all for this purpose. Devising a reasonable mechanism of updating formula probabilities seems to be hard mostly due to the lack of suitable data that might be collected during the debugging session to accomplish that. What would be imaginable during the debugging session is to try to learn something about the fault probability of syntactical elements by examining the positive (all formulas are definitely correct) and singleton negative (the single formula is definitely incorrect) test cases. However, a drawback of such a strategy comes into effect when only syntactically very simple queries are used which is, for instance, the case in Example 8.1 (see the definition of the GETENTAILMENTS function there). From such queries not many useful insights concerning faulty syntactical elements might be gained. On the other hand, such queries are absolutely desirable from the point of view of how well a user might comprehend the formulas asked by the system. Hence, these two aspects seem to contradict each other. Still, it is a topic for future research to attempt to elaborate a solution for that issue.\nFacilitation of More Informative User Answers. The debugging system described in this work is designed to get along with just a \u201cminimal\u201d feedback of a user regarding an asked query. That is, we\n355\nassume the user\u2019 answer to a query Q to be merely true , i.e. each formula in Q (or the conjunction of formulas in Q) must be entailed by the correct KB, or false , i.e. at least one formula in Q (or the conjunction of formulas in Q) must not be entailed by the correct KB. However, imagine a user being presented Q and think of how they might proceed in order to come up with an answer to Q. The first observation is that, in order to respond by true , a user must definitely scrutinize each single formula in Q because otherwise they could never decide for sure whether the conjunction of all formulas in Q is correct. Another observation is that a user might cease to go through the rest of the formulas in case they have already identified one that must not be an entailment of the desired KB. For, in this situation, the overall query Q is already false . This however indicates that at least one formula must be known to be correct or false whatever answer is given to Q. Therefore, we can usually expect a user to be able to give exactly this information, namely one formula in Q that must be incorrect, additionally to answering by false . This extra piece of information can be exploited to achieve better space and time efficiency in the context of diagnosis computation since knowing which formula must definitely not be entailed gives more information that just a set of formulas of which we know that at least one among those is not entailed. Apart from that, there might be other pieces of additional information a user might be easily able to give additionally to the \u201cminimal\u201d feedback we assume in this work. Proposing more efficient algorithms that exploit such tapes of additional information is on our future work agenda.\nUsage of \u201cPositive-Impact\u201d Queries in Combination with DYNAMICHS. As we discussed in Section 12.1 in the context of Algorithm 5 in dynamic mode, an added test case might give rise to some pruning steps as well as it might induce the construction of new subtrees (where \u201cnew\u201d means that these would be no subtress of a hitting set tree w.r.t. the DPI not including this test case). The latter situation occurs when \u201ccompletely new\u201d minimal conflict sets (those that are in no subset-relationship with existing ones) are introduced by the addition of a test case. If this is the only impact of a test case, then this test case has only a negative influence on the time and space complexity of Algorithm 5 using DYNAMICHS. In other words, none of the invalidated minimal diagnoses (and no other nodes in the tree) are redundant, but all of them must additionally hit the set of \u201ccompletely new\u201d minimal conflict sets (in order to become diagnoses w.r.t. new DPI). Hence, in this case, the transition from one DPI to another including this test case results only in monotonic growth of the tree. If possible, such \u201cnegative-impact test cases\u201d must be avoided. On the other hand, one must strive for the usage of \u201cpositive-impact test cases\u201d, i.e. those that only trigger tree pruning, but no tree expansion. Defining and studying properties that constitute such \u201cpositive-impact test cases\u201d and \u201cnegative-impact test cases\u201d, respectively, and developing specialized algorithms for extracting exactly those types of queries that enable as substantial and effective pruning as possible in the context of DYNAMICHS is part of our already ongoing research. Note that a rough intuition of which properties make out a \u201cpositive-impact test case\u201d is illustrated on the basis of an example in Section 12.1.\nFinding the Right Expert to Answer a Query in a Collaborative KB Development Setting. As we mentioned in Chapter 1, there are collaborative KB development projects such as the OBO Project48 and the NCI Thesaurus49, where many different people contribute to the specification of their knowledge in large KBs. In such a setting, it may be hard to decide who is the person that has the highest chance of being able to answer a concrete query correctly. The idea in such a scenario could be to use a combination of different measures such as educational level (e.g. professor versus PhD student) or hierarchy of contributors (e.g. senior user versus regular user), statistical information about past faults of a contributor (e.g., how many of the formulas originally authored by a person have been corrected by other persons of higher educational level) or provenance information regarding terms occurring in the query (who has\n48http://obo.sourceforge.net 49http://nciterms.nci.nih.gov/ncitbrowser\n356 CHAPTER 34. FUTURE WORK TOPICS\nauthored most of the formulas in which these terms occur?) in order to learn an \u201cexpert model\u201d and use it to devise some kind of recommender system [JZFF10] that suggests which person to ask a particular query.\nOnce established, such an expert model together with provenance information of KB formulas and other types of information discussed in Section 4.6.1 could also be exploited when it comes to the definition of the fault information provided as input to our debugging system. An example of a system which enables the remote collaborative development of KBs (ontologies) and also provides logs of interesting usage data such as formula change logs and provenance information is Web Prot\u00e9g\u00e9 [TNNM13].\nStudying the Performance of the Newly Proposed Iterative Diagnosis Computation Mechanisms. We will conduct extensive experiments using faulty real-world KBs in order to assess the impact of the usage of the powerful search tree pruning techniques of the DYNAMICHS method or the guaranteed \u201cconvergence\u201d towards the correct solution diagnosis of the STATICHS in comparison to interactive debugging algorithms used in our previous works [SFFR12, RSFF13, SF10, SFRF14c].\nMethods for Query Selection without Computation of Diagnoses. We are also working on \u201cconflictbased debugging\u201d methods that do not rely on the computation of leading diagnoses for query generation. Instead, queries might be generated directly from (minimal) conflict sets. Such methods might be used together with a boolean hitting set search tree (which was originally proposed by [JL02] and optimized by [PQ12]) where the tree is regularly pruned using test cases such that tree branching is mostly or completely suppressed. In this manner, the tree remains small in size and all in all computes only a single diagnosis, i.e. the one consistent with all answered queries. Such an approach could be very space saving. Nevertheless, it is unclear whether the number of required queries and/or the computation time might increase. Implementing such an approach and answering these open questions is a topic on our future work agenda.\nEmploying Advanced Reasoning Techniques to Increase Debugging Efficiency. To cope with application contexts where reasoning is the main obstacle for efficient debugging, a plan for future work is to integrate advances reasoning techniques into our system.\nFor example, a modular combination of reasoners [RGH12] might be adopted. In such a system there are two sound reasoners are combined where one (R1, e.g. HermiT [SMH08]) is complete for the full logic L (e.g. OWL 2 [GHM+08]) and the other one (R2, e.g. ELK [KKS14]) is complete for only a fragment L\u2032 \u2282 L (e.g. the OWL 2 EL profile [GHM+08]), but L\u2032 can be handles much more efficiently by R2. The system in [RGH12] could be used to assign the bulk of the workload on R2 while relying on R1 only if necessary.\nAnother interesting approach might be to employ techniques introduced in [GPS12] for detecting so-called \u201chot spots\u201d in KBs which, when deleted from the KB, lead to much more efficient reasoning. Since reasoning in our approaches is mostly applied to fractions of the faulty KB, we could possibly benefit from such an approach. For instance, queries are entailments of a set of different non-faulty fractions K\u2217i = (K \\ Di) \u222a B \u222a UP of the original KB. Now, given that a hot spot H is included, say in B \u222a UP , then we might well delete H from this subset of K\u2217i and might still obtain meaningful queries. The reason is that H does not include any formulas in UD (where D is the set of leading diagnoses) which are essential for query computation from the diagnosis discrimination point of view. Formulas in B \u222a UP , on the other hand, are included in all non-faulty fractions K\u2217i and thus do not directly serve the discrimination between diagnoses. Since UD might be much smaller in size than B \u222a UP in many scenarios (due to a usually small number of leading diagnoses in D), there might be a high chance for hot spots to be located in B \u222a UP rather than in UD."}], "references": [{"title": "Constraint-based Debugging of Spreadsheets", "author": ["Rui Abreu", "Andr\u00e9 Riboira", "Franz Wotawa"], "venue": "In CIbSE,", "citeRegEx": "Abreu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abreu et al\\.", "year": 2012}, {"title": "Appendix: Description Logic Terminology", "author": ["Franz Baader"], "venue": "Description Logic Handbook,", "citeRegEx": "Baader.,? \\Q2003\\E", "shortCiteRegEx": "Baader.", "year": 2003}, {"title": "The computational complexity of abduction", "author": ["Tom Bylander", "Dean Allemang", "Michael Tanner", "John Josephson"], "venue": "Artificial Intelligence,", "citeRegEx": "Bylander et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Bylander et al\\.", "year": 1991}, {"title": "Pushing the EL envelope", "author": ["Franz Baader", "Sebastian Brandt", "Carsten Lutz"], "venue": "In IJCAI,", "citeRegEx": "Baader et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2005}, {"title": "The Description Logic Handbook: Theory, Implementation, and Applications", "author": ["Franz Baader", "Diego Calvanese", "Deborah L. McGuinness", "Daniele Nardi", "Peter F. PatelSchneider", "editors"], "venue": null, "citeRegEx": "Baader et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2007}, {"title": "Context-dependent views to axioms and consequences of Semantic Web ontologies", "author": ["Franz Baader", "Martin Knechtel", "Rafael Penaloza"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Baader et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2012}, {"title": "On the relative expressiveness of description logics and predicate logics", "author": ["Alex Borgida"], "venue": "Artificial Intelligence,", "citeRegEx": "Borgida.,? \\Q1996\\E", "shortCiteRegEx": "Borgida.", "year": 1996}, {"title": "Axiom Pinpointing in General Tableaux", "author": ["Franz Baader", "R. Penaloza"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Baader and Penaloza.,? \\Q2008\\E", "shortCiteRegEx": "Baader and Penaloza.", "year": 2008}, {"title": "Model-Based Diagnosis Meets Error Diagnosis in Logic Programs", "author": ["Luca Console", "Gerhard Friedrich", "Daniele Theseider Dupre"], "venue": "In IJCAI,", "citeRegEx": "Console et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Console et al\\.", "year": 1993}, {"title": "What you always wanted to know about Datalog (and never dared to ask)", "author": ["Stefano Ceri", "Georg Gottlob", "Letizia Tanca"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Ceri et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Ceri et al\\.", "year": 1989}, {"title": "An unsolvable problem of elementary number theory", "author": ["Alonzo Church"], "venue": "American Journal of Mathematics,", "citeRegEx": "Church.,? \\Q1936\\E", "shortCiteRegEx": "Church.", "year": 1936}, {"title": "Symbolic Logic and Mechanical Theorem Proving", "author": ["Chin-Liang Chang", "Richard Char-Tung Lee"], "venue": null, "citeRegEx": "Chang and Lee.,? \\Q1973\\E", "shortCiteRegEx": "Chang and Lee.", "year": 1973}, {"title": "The complexity of theorem-proving procedures", "author": ["Stephen A. Cook"], "venue": "In Proceedings of the third annual ACM symposium on Theory of computing,", "citeRegEx": "Cook.,? \\Q1971\\E", "shortCiteRegEx": "Cook.", "year": 1971}, {"title": "Sources of error in syllogistic reasoning", "author": ["John Ceraso", "Angela Provitera"], "venue": "Cognitive Psychology,", "citeRegEx": "Ceraso and Provitera.,? \\Q1971\\E", "shortCiteRegEx": "Ceraso and Provitera.", "year": 1971}, {"title": "Patternbased OWL Ontology Debugging Guidelines", "author": ["Oscar Corcho", "Catherine Roussey", "Vilches Bl\u00e1zquez", "Luis Manuel", "Ivan P\u00e9rez"], "venue": "Workshop on Ontology Patterns (WOP 2009), collocated with the 8th International Semantic Web Conference (ISWC", "citeRegEx": "Corcho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Corcho et al\\.", "year": 2009}, {"title": "Fixed-parameter tractability and completeness I: Basic results", "author": ["Rod G. Downey", "Michael R. Fellows"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Downey and Fellows.,? \\Q1995\\E", "shortCiteRegEx": "Downey and Fellows.", "year": 1995}, {"title": "Diagnosing multiple faults", "author": ["Johan de Kleer", "Brian C. Williams"], "venue": "Artificial Intelligence,", "citeRegEx": "Kleer and Williams.,? \\Q1987\\E", "shortCiteRegEx": "Kleer and Williams.", "year": 1987}, {"title": "A Decomposition-Based Approach to OWL DL Ontology Diagnosis", "author": ["Jianfeng Du", "Guilin Qi", "Jeff Z. Pan", "Yi-Dong Shen"], "venue": "In Proceedings of 23rd IEEE International Conference on Tools with Artificial Intelligence,", "citeRegEx": "Du et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Du et al\\.", "year": 2011}, {"title": "Probability: Theory and Examples, Fourth Edition", "author": ["Rick Durrett"], "venue": null, "citeRegEx": "Durrett.,? \\Q2010\\E", "shortCiteRegEx": "Durrett.", "year": 2010}, {"title": "Final results of the Ontology Alignment Evaluation Initiative", "author": ["J\u00e9r\u00f4me Euzenat", "Alfio Ferrara", "Willem Robert van Hage", "Laura Hollink", "Christian Meilicke", "Andriy Nikolov", "Dominique Ritze", "Fran\u00e7ois Scharffe", "Pavel Shvaiko", "Heiner Stuckenschmidt", "Ondrej Sv\u00e1b-Zamazal", "C\u00e1ssia Trojahn dos Santos"], "venue": "In Proceedings of the 6th International Workshop on Ontology Matching,", "citeRegEx": "Euzenat et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Euzenat et al\\.", "year": 2011}, {"title": "Consistency-based diagnosis of configuration knowledge bases", "author": ["Alexander Felfernig", "Gerhard Friedrich", "Dietmar Jannach", "Markus Stumptner"], "venue": "Artificial Intelligence,", "citeRegEx": "Felfernig et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Felfernig et al\\.", "year": 2004}, {"title": "A General Diagnosis Method for Ontologies", "author": ["Gerhard Friedrich", "Kostyantyn Shchekotykhin"], "venue": "Proceedings of the 4th International Semantic Web Conference (ISWC", "citeRegEx": "Friedrich and Shchekotykhin.,? \\Q2005\\E", "shortCiteRegEx": "Friedrich and Shchekotykhin.", "year": 2005}, {"title": "Model-based diagnosis of hardware", "author": ["Gerhard Friedrich", "Markus Stumptner", "Franz Wotawa"], "venue": "designs. Artif. Intell.,", "citeRegEx": "Friedrich et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Friedrich et al\\.", "year": 1999}, {"title": "An efficient diagnosis algorithm for inconsistent constraint sets", "author": ["Alexander Felfernig", "Monika Schubert", "Christoph Zehentner"], "venue": "Artificial Intelligence for Engineering Design, Analysis and Manufacturing,", "citeRegEx": "Felfernig et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Felfernig et al\\.", "year": 2011}, {"title": "OWL 2: The next step for OWL", "author": ["Bernardo Cuenca Grau", "Ian Horrocks", "Boris Motik", "Bijan Parsia", "Peter F. Patel-Schneider", "Ulrike Sattler"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Grau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grau et al\\.", "year": 2008}, {"title": "Performance Heterogeneity and Approximate Reasoning in Description Logic Ontologies", "author": ["Rafael Goncalves", "Bijan Parsia", "Ulrike Sattler"], "venue": "In Proceedings of 11th International Semantic Web Conference (ISWC", "citeRegEx": "Goncalves et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goncalves et al\\.", "year": 2012}, {"title": "A correction to the algorithm in Reiter\u2019s theory of diagnosis", "author": ["Russell Greiner", "Barbara A. Smith", "Ralph W. Wilkerson"], "venue": "Artificial Intelligence,", "citeRegEx": "Greiner et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Greiner et al\\.", "year": 1989}, {"title": "The cognitive complexity of OWL justifications", "author": ["Matthew Horridge", "Samantha Bail", "Bijan Parsia"], "venue": "In Proceedings of the 10th International Semantic Web Conference (ISWC", "citeRegEx": "Horridge et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Horridge et al\\.", "year": 2011}, {"title": "RACER System Description", "author": ["Volker Haarslev", "Ralf M\u00fcller"], "venue": "1st International Joint Conference on Automated Reasoning,", "citeRegEx": "Haarslev and M\u00fcller.,? \\Q2001\\E", "shortCiteRegEx": "Haarslev and M\u00fcller.", "year": 2001}, {"title": "Justification based Explanation in Ontologies", "author": ["Matthew Horridge"], "venue": "PhD thesis, University of Manchester,", "citeRegEx": "Horridge.,? \\Q2011\\E", "shortCiteRegEx": "Horridge.", "year": 2011}, {"title": "Laconic and Precise Justifications in OWL", "author": ["Matthew Horridge", "Bijan Parsia", "Ulrike Sattler"], "venue": "Proceedings of the 7th International Semantic Web Conference (ISWC 2008),", "citeRegEx": "Horridge et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Horridge et al\\.", "year": 2008}, {"title": "Lemmas for Justifications in OWL", "author": ["Matthew Horridge", "Bijan Parsia", "Ulrike Sattler"], "venue": "In Proceedings of the 22nd Workshop of Description Logics DL2009. CEUR Workshop Proceedings,", "citeRegEx": "Horridge et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Horridge et al\\.", "year": 2009}, {"title": "Justification Oriented Proofs in OWL", "author": ["Matthew Horridge", "Bijan Parsia", "Ulrike Sattler"], "venue": "In Proceedings of the 9th International Semantic Web Conference (ISWC", "citeRegEx": "Horridge et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Horridge et al\\.", "year": 2010}, {"title": "Extracting justifications from BioPortal ontologies", "author": ["Matthew Horridge", "Bijan Parsia", "Ulrike Sattler"], "venue": "In Proceedings of the 11th International Semantic Web Conference (ISWC", "citeRegEx": "Horridge et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Horridge et al\\.", "year": 2012}, {"title": "Justification Masking in Ontologies", "author": ["Matthew Horridge", "Bijan Parsia", "Ulrike Sattler"], "venue": "In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning,", "citeRegEx": "Horridge et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Horridge et al\\.", "year": 2012}, {"title": "CODI: Combinatorial Optimization for Data Integration - Results for OAEI", "author": ["Jakob Huber", "Timo Sztyler", "Jan Noessner", "Christian Meilicke"], "venue": "In Proceedings of the 6th International Workshop on Ontology Matching,", "citeRegEx": "Huber et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huber et al\\.", "year": 2011}, {"title": "Computing the minimal hitting sets with binary HS-tree", "author": ["Yun-fei Jiang", "Li Lin"], "venue": "Journal of software,", "citeRegEx": "Jiang and Lin.,? \\Q2002\\E", "shortCiteRegEx": "Jiang and Lin.", "year": 2002}, {"title": "Ontology Matching with Semantic Verification", "author": ["Yves R. Jean-Mary", "E. Patrick Shironoshita", "Mansur R. Kabuka"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Jean.Mary et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jean.Mary et al\\.", "year": 2009}, {"title": "Logmap: Logic-based and scalable ontology matching", "author": ["Ernesto Jim\u00e9nez-Ruiz", "Bernardo Cuenca Grau"], "venue": "In Proceedings of the 10th International Semantic Web Conference (ISWC", "citeRegEx": "Jim\u00e9nez.Ruiz and Grau.,? \\Q2011\\E", "shortCiteRegEx": "Jim\u00e9nez.Ruiz and Grau.", "year": 2011}, {"title": "Large-scale interactive ontology matching: Algorithms and implementation", "author": ["Ernesto Jim\u00e9nez-Ruiz", "Bernardo Cuenca Grau", "Yujiao Zhou", "Ian Horrocks"], "venue": "In Proceedings of 20th European Conference on Artificial Intelligence", "citeRegEx": "Jim\u00e9nez.Ruiz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jim\u00e9nez.Ruiz et al\\.", "year": 2012}, {"title": "QUICKXPLAIN: Preferred Explanations and Relaxations for OverConstrained Problems", "author": ["Ulrich Junker"], "venue": "Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence,", "citeRegEx": "Junker.,? \\Q2004\\E", "shortCiteRegEx": "Junker.", "year": 2004}, {"title": "Recommender Systems: An Introduction", "author": ["Dietmar Jannach", "Markus Zanker", "Alexander Felfernig", "Gerhard Friedrich"], "venue": null, "citeRegEx": "Jannach et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jannach et al\\.", "year": 2010}, {"title": "Debugging and Repair of OWL Ontologies", "author": ["Aditya Kalyanpur"], "venue": "PhD thesis,", "citeRegEx": "Kalyanpur.,? \\Q2006\\E", "shortCiteRegEx": "Kalyanpur.", "year": 2006}, {"title": "Reducibility among combinatorial problems", "author": ["Richard M. Karp"], "venue": "Complexity of Computer Computations,", "citeRegEx": "Karp.,? \\Q1972\\E", "shortCiteRegEx": "Karp.", "year": 1972}, {"title": "SRIQ and SROIQ are harder than SHOIQ", "author": ["Yevgeny Kazakov"], "venue": "In Proceedings of the 21st Workshop of Description Logics DL2008,", "citeRegEx": "Kazakov.,? \\Q2008\\E", "shortCiteRegEx": "Kazakov.", "year": 2008}, {"title": "Logik f\u00fcr Informatiker", "author": ["Martin Kreuzer", "Stefan K\u00fchling"], "venue": "Pearson Studium, Mu\u0308nchen, Germany,", "citeRegEx": "Kreuzer and K\u00fchling.,? \\Q2006\\E", "shortCiteRegEx": "Kreuzer and K\u00fchling.", "year": 2006}, {"title": "Probabilistic analysis of optimum partitioning", "author": ["Narendra Karmarkar", "Richard M. Karp", "George S. Lueker", "Andrew M. Odlyzko"], "venue": "Journal of Applied Probability,", "citeRegEx": "Karmarkar et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Karmarkar et al\\.", "year": 1986}, {"title": "Siman\u010d\u00edk. The incredible ELK", "author": ["Yevgeny Kazakov", "Markus Kr\u00f6tzsch", "Franti\u0161ek"], "venue": "Journal of automated reasoning,", "citeRegEx": "Kazakov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazakov et al\\.", "year": 2014}, {"title": "A complete anytime algorithm for number partitioning", "author": ["Richard E. Korf"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf.,? \\Q1998\\E", "shortCiteRegEx": "Korf.", "year": 1998}, {"title": "Swoop: A Web Ontology Editing Browser", "author": ["Aditya Kalyanpur", "Bijan Parsia", "Evren Sirin", "Bernardo Cuenca Grau", "James Hendler"], "venue": "J. Web Sem.,", "citeRegEx": "Kalyanpur et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kalyanpur et al\\.", "year": 2006}, {"title": "Repairing Unsatisfiable Concepts in OWL Ontologies", "author": ["Aditya Kalyanpur", "Bijan Parsia", "Evren Sirin", "Bernardo Cuenca Grau"], "venue": "The Semantic Web: Research and Applications, 3rd European Semantic Web Conference, ESWC 2006,", "citeRegEx": "Kalyanpur et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kalyanpur et al\\.", "year": 2006}, {"title": "Debugging Unsatisfiable Classes in OWL Ontologies", "author": ["Aditya Kalyanpur", "Bijan Parsia", "Evren Sirin", "James Hendler"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Kalyanpur et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kalyanpur et al\\.", "year": 2005}, {"title": "Machine Invention of First-order Predicates by Inverting Resolution", "author": ["Stephen Muggleton", "Wray L. Buntine"], "venue": "Proceedings of the 5th International Conference on Machine Learning", "citeRegEx": "Muggleton and Buntine.,? \\Q1988\\E", "shortCiteRegEx": "Muggleton and Buntine.", "year": 1988}, {"title": "Alignment Incoherence in Ontology Matching", "author": ["Christian Meilicke"], "venue": "PhD thesis, Universita\u0308t Mannheim,", "citeRegEx": "Meilicke.,? \\Q2011\\E", "shortCiteRegEx": "Meilicke.", "year": 2011}, {"title": "Introduction to Mathematical Logic, Fifth Edition", "author": ["Elliott Mendelson"], "venue": "CRC Press,", "citeRegEx": "Mendelson.,? \\Q2009\\E", "shortCiteRegEx": "Mendelson.", "year": 2009}, {"title": "OWL 2 Web Ontology Language Structural Specification and Functional-Style Syntax", "author": ["Boris Motik", "Peter F. Patel-Schneider", "Bijan Parsia"], "venue": "W3C recommendation,", "citeRegEx": "Motik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Motik et al\\.", "year": 2009}, {"title": "The equivalence problem for regular expressions with squaring requires exponential space", "author": ["Albert R. Meyer", "Larry J. Stockmeyer"], "venue": "In 13th Annual Symposium on Switching and Automata Theory,", "citeRegEx": "Meyer and Stockmeyer.,? \\Q1972\\E", "shortCiteRegEx": "Meyer and Stockmeyer.", "year": 1972}, {"title": "An Efficient Method for Computing Alignment Diagnoses", "author": ["Christian Meilicke", "Heiner Stuckenschmidt"], "venue": "In Proceedings of the 3rd International Conference on Web Reasoning and Rule Systems,", "citeRegEx": "Meilicke and Stuckenschmidt.,? \\Q2009\\E", "shortCiteRegEx": "Meilicke and Stuckenschmidt.", "year": 2009}, {"title": "Hypertableau Reasoning for Description Logics", "author": ["Boris Motik", "Rob Shearer", "Ian Horrocks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Motik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Motik et al\\.", "year": 2009}, {"title": "Repairing Ontology Mappings", "author": ["Christian Meilicke", "Heiner Stuckenschmidt", "Andrei Tamilin"], "venue": "Proceedings of the 22nd National Conference on Artificial intelligence -", "citeRegEx": "Meilicke et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meilicke et al\\.", "year": 2007}, {"title": "Reasoning Support for Mapping Revision", "author": ["Christian Meilicke", "Heiner Stuckenschmidt", "Andrei Tamilin"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Meilicke et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meilicke et al\\.", "year": 2008}, {"title": "Inverse entailment and Progol 1 Introduction", "author": ["Stephen Muggleton"], "venue": "New Generation Computing, Special issue on Inductive Logic Programming,", "citeRegEx": "Muggleton.,? \\Q1995\\E", "shortCiteRegEx": "Muggleton.", "year": 1995}, {"title": "YAM++ - A combination of graph matching and machine learning approach to ontology alignment task", "author": ["Duyhoa Ngo", "Zohra Bellahsene"], "venue": "Journal of Web Semantics - The Semantic Web Challenge 2011 Special Issue,", "citeRegEx": "Ngo and Bellahsene.,? \\Q2012\\E", "shortCiteRegEx": "Ngo and Bellahsene.", "year": 2012}, {"title": "A framework for ontology evolution in collaborative environments", "author": ["Natalya F. Noy", "A. Chugh", "W. Liu", "Mark A. Musen"], "venue": "In Proceedings of the 5th International Semantic Web Conference", "citeRegEx": "Noy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Noy et al\\.", "year": 2006}, {"title": "The route to success: A performance comparison of diagnosis algorithms", "author": ["Iulia Nica", "Ingo Pill", "Thomas Quaritsch", "Franz Wotawa"], "venue": "In Proceedings of the Twenty-Third international Joint Conference on Artificial Intelligence,", "citeRegEx": "Nica et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nica et al\\.", "year": 2013}, {"title": "Interactive Ontology Revision", "author": ["Nadeschda Nikitina", "Sebastian Rudolph", "Birte Glimm"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Nikitina et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nikitina et al\\.", "year": 2012}, {"title": "Creating Semantic Web Contents with Prot\u00e9g\u00e9-2000", "author": ["Natalya F. Noy", "Michael Sintek", "Stefan Decker", "Monica Crub\u00e9zy", "Ray W. Fergerson", "Mark A. Musen"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Noy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Noy et al\\.", "year": 2000}, {"title": "Optimizations for the Boolean Approach to Computing Minimal Hitting Sets", "author": ["Ingo Pill", "Thomas Quaritsch"], "venue": "In Proceedings of the 20th European Conference on Artificial Intelligence,", "citeRegEx": "Pill and Quaritsch.,? \\Q2012\\E", "shortCiteRegEx": "Pill and Quaritsch.", "year": 2012}, {"title": "OWL Web Ontology Language Semantics and Abstract Syntax", "author": ["Peter F. Patel-Schneider", "Patrick Hayes", "Ian Horrocks"], "venue": "W3C recommendation,", "citeRegEx": "Patel.Schneider et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Patel.Schneider et al\\.", "year": 2004}, {"title": "Debugging OWL ontologies", "author": ["Bijan Parsia", "Evren Sirin", "Aditya Kalyanpur"], "venue": "Proceedings of the 14th international conference on World Wide Web,", "citeRegEx": "Parsia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Parsia et al\\.", "year": 2005}, {"title": "Model-Based Diagnosis or Reasoning from First Principles", "author": ["Bernhard Peischl", "Franz Wotawa"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Peischl and Wotawa.,? \\Q2003\\E", "shortCiteRegEx": "Peischl and Wotawa.", "year": 2003}, {"title": "Induction of Decision Trees", "author": ["John Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan.", "year": 1986}, {"title": "Vilches-Bl\u00e1zquez. A catalogue of OWL ontology antipatterns", "author": ["Catherine Roussey", "Oscar Corcho", "Luis Manuel"], "venue": "In International Conference On Knowledge Capture,", "citeRegEx": "Roussey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Roussey et al\\.", "year": 2009}, {"title": "OWL Pizzas: Practical Experience of Teaching OWL-DL: Common Errors & Common Patterns", "author": ["Alan Rector", "Nick Drummond", "Matthew Horridge", "Jeremy Rogers", "Holger Knublauch", "Robert Stevens", "Hai Wang", "Chris Wroe"], "venue": "Engineering Knowledge in the Age of the SemanticWeb 14th International Conference,", "citeRegEx": "Rector et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rector et al\\.", "year": 2004}, {"title": "A Theory of Diagnosis from First Principles", "author": ["Raymond Reiter"], "venue": "Artificial Intelligence,", "citeRegEx": "Reiter.,? \\Q1987\\E", "shortCiteRegEx": "Reiter.", "year": 1987}, {"title": "MORe: Modular combination of OWL reasoners for ontology classification", "author": ["Ana Armas Romero", "Bernardo Cuenca Grau", "Ian Horrocks"], "venue": "In Proceedings of the 11th International Semantic Web Conference", "citeRegEx": "Romero et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2012}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Stuart J. Russell", "Peter Norvig"], "venue": "Pearson Education,", "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "A Theory of Interactive Debugging of Knowledge Bases in Monotonic Logics", "author": ["Patrick Rodler"], "venue": "Master\u2019s thesis, Alpen-Adria Universita\u0308t Klagenfurt,", "citeRegEx": "Rodler.,? \\Q2015\\E", "shortCiteRegEx": "Rodler.", "year": 2015}, {"title": "KOSIMap: Use of Description Logic Reasoning to Align Heterogeneous Ontologies", "author": ["Quentin Reul", "Jeff Z. Pan"], "venue": "Proceedings of the 23rd International Workshop on Description Logics DL2010,", "citeRegEx": "Reul and Pan.,? \\Q2010\\E", "shortCiteRegEx": "Reul and Pan.", "year": 2010}, {"title": "Balancing Brave and Cautious Query Strategies in Ontology Debugging", "author": ["Patrick Rodler", "Kostyantyn Shchekotykhin", "Philipp Fleiss", "Gerhard Friedrich"], "venue": "Proceedings of the Joint Workshop on Knowledge Evolution and Ontology Dynamics 2011 (EvoDyn2011),", "citeRegEx": "Rodler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rodler et al\\.", "year": 2011}, {"title": "RIO: Minimizing User Interaction in Debugging of Aligned Ontologies", "author": ["Patrick Rodler", "Kostyantyn Shchekotykhin", "Philipp Fleiss", "Gerhard Friedrich"], "venue": "In Proceedings of the 7th International Workshop on Ontology Matching (OM-2012),", "citeRegEx": "Rodler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rodler et al\\.", "year": 2012}, {"title": "RIO: Minimizing User Interaction in Ontology Debugging", "author": ["Patrick Rodler", "Kostyantyn Shchekotykhin", "Philipp Fleiss", "Gerhard Friedrich"], "venue": "Web Reasoning and Rule Systems,", "citeRegEx": "Rodler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rodler et al\\.", "year": 2013}, {"title": "Ontology matching: State of the art and future challenges", "author": ["Pavel Shvaiko", "J\u00e9r\u00f4me Euzenat"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Shvaiko and Euzenat.,? \\Q2013\\E", "shortCiteRegEx": "Shvaiko and Euzenat.", "year": 2013}, {"title": "OntoEdit: Collaborative Ontology Development for the Semantic Web", "author": ["York Sure", "Michael Erdmann", "Juergen Angele", "Steffen Staab", "Rudi Studer", "Dirk Wenke"], "venue": "In Proceedings of the 1st International Semantic Web Conference (ISWC", "citeRegEx": "Sure et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sure et al\\.", "year": 2002}, {"title": "Query strategy for sequential ontology debugging", "author": ["Kostyantyn Shchekotykhin", "Gerhard Friedrich"], "venue": "Proceedings of the 9th International Semantic Web Conference (ISWC", "citeRegEx": "Shchekotykhin and Friedrich.,? \\Q2010\\E", "shortCiteRegEx": "Shchekotykhin and Friedrich.", "year": 2010}, {"title": "Interactive Ontology Debugging: Two Query Strategies for Efficient Fault Localization", "author": ["Kostyantyn Shchekotykhin", "Gerhard Friedrich", "Philipp Fleiss", "Patrick Rodler"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2012}, {"title": "On Computing Minimal Conflicts for Ontology Debugging", "author": ["Kostyantyn Shchekotykhin", "Gerhard Friedrich", "Dietmar Jannach"], "venue": "In MBS 2008 - Workshop on Model-Based Systems,", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2008}, {"title": "Direct computation of diagnoses for ontology alignment", "author": ["Kostyantyn Shchekotykhin", "Philipp Fleiss", "Patrick Rodler", "Gerhard Friedrich"], "venue": "Proceedings of the 7th International Workshop on Ontology Matching", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2012}, {"title": "A direct approach to sequential diagnosis of high cardinality faults in knowledge bases", "author": ["Kostyantyn Shchekotykhin", "Gerhard Friedrich", "Patrick Rodler", "Philipp Fleiss"], "venue": "In DX 2014 - 25th International Workshop on Principles of Diagnosis (DX 2014),", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2014}, {"title": "Interactive Ontology Debugging using Direct Diagnosis", "author": ["Kostyantyn Shchekotykhin", "Gerhard Friedrich", "Patrick Rodler", "Philipp Fleiss"], "venue": "Proceedings of the Third International Workshop on Debugging Ontologies and Ontology Mappings (WoDOOM14). CEUR Workshop Proceedings,", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2014}, {"title": "Sequential diagnosis of high cardinality faults in knowledge-bases by direct diagnosis generation", "author": ["Kostyantyn Shchekotykhin", "Gerhard Friedrich", "Patrick Rodler", "Philipp Fleiss"], "venue": "In Proceedings of the 21st European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2014}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon.,? \\Q1948\\E", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Algorithmic Program Debugging", "author": ["Ehud Shapiro"], "venue": null, "citeRegEx": "Shapiro.,? \\Q1983\\E", "shortCiteRegEx": "Shapiro.", "year": 1983}, {"title": "Debugging Incoherent Terminologies", "author": ["Stefan Schlobach", "Zhisheng Huang", "Ronald Cornet", "Frank Harmelen"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "Schlobach et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schlobach et al\\.", "year": 2007}, {"title": "Exploring the Duality in Conflict-Directed Model-Based Diagnosis", "author": ["Roni Stern", "Meir Kalech", "Alexander Feldman", "Gregory Provan"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence Exploring,", "citeRegEx": "Stern et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2012}, {"title": "Abductive and default reasoning: A computational core", "author": ["Bart Selman", "Hector Levesque"], "venue": "Proceedings of the 8th National Conference on Artificial Intelligence,", "citeRegEx": "Selman and Levesque.,? \\Q1989\\E", "shortCiteRegEx": "Selman and Levesque.", "year": 1989}, {"title": "HermiT : A Highly-Efficient OWL Reasoner", "author": ["Rob Shearer", "Boris Motik", "Ian Horrocks"], "venue": "In Proc. of the 5th Int. Workshop on OWL: Experiences and Directions (OWLED 2008 EU),", "citeRegEx": "Shearer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shearer et al\\.", "year": 2008}, {"title": "Pellet: A practical OWL-DL reasoner", "author": ["Evren Sirin", "Bijan Parsia", "Bernardo Cuenca Grau", "Aditya Kalyanpur", "Y Katz"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Sirin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sirin et al\\.", "year": 2007}, {"title": "A Modularization-Based Approach to Finding All Justifications for OWL DL Entailments", "author": ["Boontawee Suntisrivaraporn", "Guilin Qi", "Qiu Ji", "Peter Haase"], "venue": "In Proceedings of the 7th International Semantic Web Conference (ISWC", "citeRegEx": "Suntisrivaraporn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Suntisrivaraporn et al\\.", "year": 2008}, {"title": "Balancing brave and cautious query strategies in ontology debugging", "author": ["Kostyantyn Shchekotykhin", "Patrick Rodler", "Gerhard Friedrich"], "venue": "In 22nd International Workshop on Principles of Diagnosis (DX", "citeRegEx": "Shchekotykhin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shchekotykhin et al\\.", "year": 2011}, {"title": "Subsumption in KL-ONE is undecidable", "author": ["Manfred Schmidt-Schau\u00df"], "venue": "In Proceedings of the 1st International Conference on Principles of Knowledge Representation and Reasoning,", "citeRegEx": "Schmidt.Schau\u00df.,? \\Q1989\\E", "shortCiteRegEx": "Schmidt.Schau\u00df.", "year": 1989}, {"title": "Which Kind of Module Should I Extract", "author": ["Ulrike Sattler", "Thomas Schneider", "Michael Zakharyaschev"], "venue": "Proceedings of the 22nd International Workshop on Description Logics,", "citeRegEx": "Sattler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sattler et al\\.", "year": 2009}, {"title": "Debugging OWL Ontologies - A Reality Check", "author": ["Heiner Stuckenschmidt"], "venue": "Proceedings of the 6th International Workshop on Evaluation of Ontology-based Tools and the Semantic Web Service Challenge (EON),", "citeRegEx": "Stuckenschmidt.,? \\Q2008\\E", "shortCiteRegEx": "Stuckenschmidt.", "year": 2008}, {"title": "Enumerating Minimally Revised Specifications Using Dualization", "author": ["Ken Satoh", "Takeaki Uno"], "venue": "New Frontiers in Artificial Intelligence,", "citeRegEx": "Satoh and Uno.,? \\Q2006\\E", "shortCiteRegEx": "Satoh and Uno.", "year": 2006}, {"title": "Detecting and locating faults in the control software of autonomous mobile robots", "author": ["Gerald Steinbauer", "Franz Wotawa"], "venue": "In IJCAI International Joint Conference on Artificial Intelligence,", "citeRegEx": "Steinbauer and Wotawa.,? \\Q2005\\E", "shortCiteRegEx": "Steinbauer and Wotawa.", "year": 2005}, {"title": "Robust Plan Execution Using Model-Based Reasoning", "author": ["Gerald Steinbauer", "Franz Wotawa"], "venue": "Advanced Robotics,", "citeRegEx": "Steinbauer and Wotawa.,? \\Q2009\\E", "shortCiteRegEx": "Steinbauer and Wotawa.", "year": 2009}, {"title": "FaCT++ description logic reasoner: System description", "author": ["Dmitry Tsarkov", "Ian Horrocks"], "venue": "Proc. of the Int. Joint Conf. on Automated Reasoning (IJCAR", "citeRegEx": "Tsarkov and Horrocks.,? \\Q2006\\E", "shortCiteRegEx": "Tsarkov and Horrocks.", "year": 2006}, {"title": "WebProt\u00e9g\u00e9: A Collaborative Ontology Editor and Knowledge Acquisition Tool for the Web", "author": ["Tania Tudorache", "Csongor Nyulas", "Natalya F. Noy", "Mark A. Musen"], "venue": "Semantic Web,", "citeRegEx": "Tudorache et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tudorache et al\\.", "year": 2013}, {"title": "On Computable Numbers, with an Application to the Entscheidungsproblem", "author": ["Alan Mathison Turing"], "venue": "Proceedings of the London Mathematical Society,", "citeRegEx": "Turing.,? \\Q1937\\E", "shortCiteRegEx": "Turing.", "year": 1937}, {"title": "Model-Based Debugging or How to Diagnose Programs Automatically", "author": ["Franz Wotawa", "Markus Stumptner", "Wolfgang Mayer"], "venue": "Developments in Applied Artificial Intelligence,", "citeRegEx": "Wotawa et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wotawa et al\\.", "year": 2002}], "referenceMentions": [], "year": 2016, "abstractText": "v", "creator": "LaTeX with hyperref package"}}}