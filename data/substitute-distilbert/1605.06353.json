{"id": "1605.06353", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction", "abstract": "through this work, we calculate parameter reduction towards the m $ ^ 2 $ metric, the standard metric for automatic grammar error correction ( gec ) tasks. after implementing m $ ^ 2 $ as a scorer in the moses tuning framework, collaborators investigate interactions of dense and sparse features, different optimizers, and various strategies for approximately 2010 - 2014 shared task. patients notice erratic correlation when optimizing sparse feature weights with m $ ^ 2 $ and offer partial solutions. to say surprise, we find that a bare - bones phrase - based smt application with size - specific parameter - tuning outperforms all previously published results for the conll - 5000 target set by a large margin ( 46. 37 % m $ ^ 2 $ over previously 40. 56 %, by a neural encoder - decoder set ) while carefully trained on similarly same data. our newly introduced dense and sparse features overlap that sample, and we improve the state - of - the - art to 49. 49 % m $ ^ 2 $.", "histories": [["v1", "Fri, 20 May 2016 13:43:56 GMT  (141kb,D)", "http://arxiv.org/abs/1605.06353v1", null], ["v2", "Wed, 5 Oct 2016 08:42:23 GMT  (156kb,D)", "http://arxiv.org/abs/1605.06353v2", "Accepted for publication at EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "roman grundkiewicz"], "accepted": true, "id": "1605.06353"}, "pdf": {"name": "1605.06353.pdf", "metadata": {"source": "META", "title": "Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction", "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz", "Adam Mickiewicz"], "emails": ["junczys@amu.edu.pl", "romang@amu.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "Statistical machine translation (SMT), especially the phrase-based variant, is well established in the field of automatic grammatical error correction (GEC) and systems that are either pure SMT or incorporate SMT as system components occupied top positions in GEC shared tasks for different languages.\nWith the recent paradigm shift in machine translation towards neural translation models, neural encoder-decoder models are expected to appear in the field of GEC as well, and first published results (Xie et al., 2016) already mark the new state-of-theart for GEC. As it is the case in classical bilingual\nmachine translation research, these models should be compared against strong SMT baselines. In this paper we attempt to provide these baselines.\nDuring our first experiments, we find \u2014 to our surprise \u2014 that a bare-bones phrase-based system outperforms the best published results on the CoNLL-2014 test set by a significant margin only due to a task-specific parameter tuning scheme when being trained on the same data as these previous systems. When we further investigate the influence of well-known SMT-specific features and introduce new features adapted to the problem of GEC, our final systems outperform the best reported results by 9% M2, moving the state-of-the-art results for the CoNLL-2014 test set from 40.56% M2 to 49.49%.\nThe paper is organized as follows: section 2 describes previous work, especially the CoNLL-2014 shared tasks on GEC and relevant follow-up papers. Our main contributions are presented in sections 3 and 4 where we investigate the interaction of parameter tuning towards the M2 metric with task-specific dense and sparse features. Especially tuning for sparse features is more challenging than initially expected, but it seems that we found optimizer hyperparameters that make sparse feature weight tuning with M2 feasible. Section 5 reports on the effects of adding a web-scale n-gram language model to our models, as it has been done in previous work."}, {"heading": "2 Previous Work", "text": ""}, {"heading": "2.1 The CoNLL-2014 Shared Task", "text": "While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task\nar X\niv :1\n60 5.\n06 35\n3v 1\n[ cs\n.C L\n] 2\n0 M\nay 2\n(Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected. Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well.\nThe goal of the CoNLL-2014 shared task was to evaluate algorithms and systems for automatically correcting grammatical errors in essays written by second language learners of English. Grammatical errors of 28 types were targeted. Participating teams were given training data with manually annotated corrections of grammatical errors and were allowed to use additional publicly available data.\nThe corrected system outputs were evaluated blindly using the MaxMatch (M2) metric (Dahlmeier and Ng, 2012). Thirteen system submissions took part in the shared task. Among the top-three positioned systems, two submissions \u2014 CAMB (Felice et al., 2014) and AMU (JunczysDowmunt and Grundkiewicz, 2014) \u2014 were partially or fully based on SMT. The second system, CUUI (Rozovskaya et al., 2014), was a classifierbased approach, another popular paradigm in GEC."}, {"heading": "2.2 Aftermath", "text": "Shortly after the shared task, Susanto et al. (2014) published a work on GEC systems combinations. They combined the output from a classificationbased system and a SMT-based system using MEMT (Heafield and Lavie, 2010), reporting new state-of-the-art results for the CoNLL-2014 test set.\nXie et al. (2016) present a neural networkbased approach to GEC. Their method relies on a character-level encoder-decoder recurrent neural network with an attention mechanism. They use data from the public Lang-8 corpus and combine their model with an n-gram language model trained on web-scale Common Crawl data. Adding synthesized erroneous data, they achieve the best published results for the CoNLL-2014 test set so far.\nIn Figure 1 we give a graphical overview of the published results for the CoNLL-2014 test set in comparison to the results we will discuss in this work. Positions marked with (r) use only data restricted data set which corresponds to the data set used by Susanto et al. (2014). Positions with (u)\nmake use of web-scale data, this corresponds to the data used in Xie et al. (2016). We marked the participants of the CoNLL-2014 shared task as unrestricted as some participants made use of Common Crawl data or Google n-grams."}, {"heading": "3 Dense feature optimization", "text": "Moses comes with tools that can tune parameter vectors according to different MT tuning metrics. Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al., 2002). BLEU was never designed for grammatical error correction; we find that directly optimizing for M2 works far better."}, {"heading": "3.1 Tuning towards M2", "text": "The M2 metric (Dahlmeier and Ng, 2012) is an FScore, based on the edits extracted from a Levenshtein distance matrix. For the CoNLL-2014 shared task, the \u03b2-parameter was set to 0.5, putting two times more weight on precision than on recall.\nJunczys-Dowmunt and Grundkiewicz (2014) have shown that tuning with BLEU is counterproductive in a settings where M2 is the evaluation metric. For inherently weak systems this can result in all correction attempts to be disabled, MERT then learns to disallow all changes since they lower the similarity to the reference as determined by BLEU. Systems with better training data, can be tuned with BLEU without suffering this \u201cdisabling\u201d effect, but will reach non-optimal performance. However, Susanto et al. (2014) tune the feature weights of their two SMT-based systems with BLEU on the CoNLL2013 test set and report state-of-the-art results.\nW re-implemented the M2 metric in C++ and added it as a scorer to the Moses parameter optimization framework. Due to this integration we can now tune parameter weights with MERT, PRO or Batch Mira. The inclusion of the latter two enables us to experiment with sparse features.\nBased on Clark et al. (2011) concerning the effects of optimizer instability, we report results averaged over five tuning runs. Additionally, we compute parameter weight vector centroids as suggested by Cettolo et al. (2011). They showed that parameter vector centroids averaged over several tuning runs yield similar to or better than average results and reduce variance. We generally confirm this for M2-based tuning."}, {"heading": "3.2 Dense features", "text": "The standard features in SMT have been chosen to help guiding the translation process. In a GEC setting the most natural units seem to be minimal edit operations that can be either counted or modeled in context with varying degrees of generalization. That way, the decoder can be informed on several levels of abstraction how the output differs from the input.1 In this section we implement several features that try to capture these operation in isolation and in context."}, {"heading": "3.2.1 Stateless features", "text": "Our stateless features are computed during translation option generation before decoding, modeling relations between source and target phrases. They are meant to extend the standard SMT-specific\n1We believe this is important information that currently has not yet been mastered in neural encoder-decoder approaches.\nMLE-based phrase and word translation probabilities with meaningful phrase-level information about the correction process.\nLevenshtein distance. Junczys-Dowmunt and Grundkiewicz (2014) use word-based Levenshtein distance between source and target phrases as a translation model feature, Felice et al. (2014) independently experiment with a character-based version.\nEdit operation counts. We further refine Levenshtein distance feature with edit operation counts. Based on the Levenshtein distance matrix, the numbers of deletions, insertions, and substitutions that transform the source phrase into the target phrase are computed, the sum of these counts is equal to the original Levenshtein distance(see Table 1 for examples)."}, {"heading": "3.2.2 Stateful features", "text": "Contrary to stateless features, stateful features can look at translation hypotheses outside their own span and take advantage of the constructed target context. The most typical stateful features are language models. In this section, we discuss LM-like features over edit operations.\nOperation Sequence Model. Durrani et al. (2013) introduce Operation Sequence Models in Moses. These models are Markov translation models that in our setting can be interpreted as Markov edition models. Translations between identical words are matches, translations that have different words on source and target sides are substitutions; insertions and deletions are interpreted in the same way as for SMT. Gaps, jumps, and other operations typical for OSMs do not appear as we disabled reordering.\nWord-class language model. The monolingual Wikipedia data has been used create a 9-gram wordclass language model with 200 word-classes produced by word2vec (Mikolov et al., 2013). This features allows to capture possible long distance dependencies and semantical aspects."}, {"heading": "3.3 Training and Test Data", "text": "The training data provided in both shared tasks is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013). NUCLE consists of 1,414 essays written by Singaporean students who are nonnative speakers of English. The essays cover a wide range of topics, such as environmental pollution, health care, etc. The grammatical errors in these essays have been hand-corrected by professional English teachers and annotated with one of the 28 predefined error type.\nAnother 50 essays, collected and annotated similarly as NUCLE, were used in both CoNLL GEC shared tasks as blind test data. The CoNLL-2014 test set has been annotated by two human annotators, the CoNLL-2013 by one annotator. Many participants of CoNLL-2014 shared task used the test set from 2013 as development set for their systems.\nAs mentioned before, in order to make our results comparable to previous work, we report main results using similar training data as Susanto et al. (2014). We refer to this setting that as the \u201cresticted-data setting\u201d (r). Parallel data for translation model training is adapted from the above mentioned NUCLE corpus and the publicly available Lang-8 corpus (Mizumoto et al., 2012). Uncorrected sentences serve as source language data, corrected counterparts as target language data. For language modeling, the tar-\nget language sentences of both parallel resources are used, additionally we extract all text from the English version of Wikipedia. Table 2 lists all data sources and sizes.\nPhrase-based SMT makes it ease to scale up in terms of training data, especially in the case of ngram language models. The CoNLL shared tasks did not impose any restrictions concerning training data (apart from having to be freely available) on its participants. As mentioned above, to demonstrate the ease of data integration we propose an \u201cunrestricted setting\u201d (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of the shared task submissions, and later in Xie et al. (2016). We use Common Crawl data made-available by Buck et al. (2014)."}, {"heading": "3.4 Experiments", "text": "Our system is based on the phrase-based part of the statistical machine translation system Moses (Koehn et al., 2007). Only plain text data is used for language model and translation model training. External linguistic knowledge is introduced during parameter tuning as the tuning metric relies on the error annotation present in NUCLE. The translation model is built with the standard Moses training script, word-alignment models are produced with MGIZA++ (Gao and Vogel, 2008), we restrict the word alignment training to 5 iterations of Model 1 and 5 iterations of the HMM-Model. No reordering models are used, the distortion limit is set to 0, effectively prohibiting any reordering. All systems use one 5-gram language model that has been estimated from the target side of the parallel data available for translation model training. Another 5-gram language model trained on Wikipedia in the restricted setting or on Common Crawl data in the unrestricted case. We combine the two language models as features in the log-linear model of Moses.\nSystems are retuned when new features of any type are added. We first successfully reproduce results from Susanto et al. (2014) for BLEU-based tuning on the CoNLL-2013 test set as the development set (Fig. 2a) using similar training data. Repeated tuning places the scores reported by Susanto et al. (2014) for their SMT-ML combinations (37.90 \u2013 39.39) within the range of possible values for a purely Moses-based system without any spe-\ncific features (35.19 \u2013 38.38) or with just the Levenshtein distance features (37.46 \u2013 40.52). Since Susanto et al. (2014) do not report results for multiple tuning steps, the extend of influence of optimizer instability on their experiments remains unclear. Even with BLEU-based tuning, we can see significant improvements when replacing Levenshtein distance with the finer-grained edit operations, and another performance jump with additional stateful features. The value range of the different tuning runs for the last feature set includes the currently bestperforming system (Xie et al. (2016) with 40.56%), but the result for the averaged centroid are inferior.\nTuning directly with M2 (Fig. 2b) and averaging weights across five iterations, yields between 40.66% M2 for a vanilla Moses system and 42.32% for a system with all described dense features. Results seen to be more stable. Averaging weight vectors across runs to produce the final vector seems like a fair bet. Performance with the averaged weight vectors is either similar to or better than the average number for five runs.\nTo emphasize: bare-bones Moses without any specialized features and with restricted data rivals the best reported systems on the CoNLL-2014 test set (40.56%) trained on unrestricted data. This is achieved alone due to M2-based tuning. Better tun-\ning sets (next section), task-specific features, and more data (section 5) only increase that performance advantage."}, {"heading": "3.5 Larger development sets", "text": "No less important than choosing the correct tuning metric is a good choice of the development set. Among MT researches, there is a number of more or less well known truths about suitable development sets for translation-focused settings: usually they consist of between 2000 and 3000 sentences, they should be a good representation of the testing data, sparse features require more sentences or more references, etc. Until now, we followed the seemingly obvious approach from Susanto et al. (2014) to tune on the CoNLL-2013 test set. The CoNLL-2013 test set consists of 1380 sentences, which might be barely enough for a translation-task, and it is unclear how to quantify it in the context of grammar correction. Furthermore, calculating the error rate in this set reveals that only 14.97% of the tokens are part of an erroneous fragment, for the rest, input and reference data are identical. Intuitively, this seems to be very little significant data for tuning an SMT system.\nWe therefore decide to take advantage of the entire NUCLE data as a development set which so far has only been used as translation model train-\ning data. NUCLE consist of more than 57,000 sentences, however, the error rate is significantly lower than in the previous development set, only 6.23%. We adapt the error rate by greedily removing sentences from NUCLE until an error of ca. 15% is reached, 23381 sentences and most error annotations remain. We further divide the data into four folds. Each folds serves as development set for parameter tuning, while the three remaining parts are treated as translation model training data. The full Lang-8 data is concatenated with is NUCLE training set, and four models are trained. Tuning is then performed four times and the resulting four parameter weight vectors are averaged into a single weight vector across folds. We repeat this procedure again five times which results in 20 separate tuning steps. Results on the CoNLL-2014 test set are obtained using the full translation model with a parameter vector average across five runs. The CoNLL-2013 test set is not being used for tuning and can serve as a second test set.\nAs can be seen in Fig. 2c, this procedure significantly improves performance, also for the barebones set-up (41.63%). The lower variance between iterations is an effect of averaging across folds.\nIt turns out that what was meant to be a strong baseline, is actually among the strongest systems reported for this task, outperformed only by the further improvements over this baseline presented in this work."}, {"heading": "4 Sparse Features", "text": "We saw that introducing finer-grained edit operations improved performance. The natural evolution of that idea are features that describe specific cor-\nrection operations with and without context. This can be accomplished with sparse features, but tuning sparse features according to the M2 metric poses unexpected problems."}, {"heading": "4.1 Optimizing for M2 with PRO and Mira", "text": "The MERT tool included in Moses cannot handle parameter tuning with sparse feature weights and one of the other optimizers available in Moses has to be used. We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3).\nExperiments with Mira hyper-parameters allowed to counter these effects. We first change the background BLEU approximation method in Batch Mira to use model-best hypotheses (--model-bg) which seems to produce more satisfactory results. Inspecting the tuning process, however, reveals problems with this setting, too. Figure 3 reveals how instable the tuning process with Mira is across iterations. The best result is reached after only three iterations. In a setting with sparse features this would\nresult in only a small set of weighted sparse features. After consulting with one of the authors of BatchMira, we set the background corpus decay rate to 0.001 (-D 0.001), resulting in a sentence-level approximation of M2. Mira\u2019s behavior seems to stabilize across iterations. At this point it is not quite clear why this is required. While PRO\u2019s behavior is more sane during tuning, results on the test sets are subpar. It seems that no comparable hyperparameter settings exist for PRO."}, {"heading": "4.2 Sparse edit operations", "text": "Our sparse edit operations are again based on the Levenshtein distance matrix and count specific edits that are annotated with the source and target tokens that took part in the edit. For the following erroneous/corrected sentence pair Err: Then a new problem comes out . Cor: Hence , a new problem surfaces .\nwe generate sparse features that model contextless edits (matches are omitted):\nsubst(Then,Hence)=1 insert(,)=1 subst(comes, surfaces)=1 del(out)=1\nand sparse features with one-sided left or right or two-sided context: <s>_subst(Then,Hence)=1 subst(Then,Hence)_a=1 Hence_insert(,)=1 insert(,)_a=1 problem_subst(comes, surfaces)=1 subst(comes, surfaces)_out=1 comes_del(out)=1 del(out)_.=1 <s>_subst(Then,Hence)_a=1 Hence_insert(,)_a=1 problem_subst(comes, surfaces)_out=1 comes_del(out)_.=1\nAll sparse feature types are added on-top of our best dense-features system. When using sparse features with context, the contextless features are included. All the presented features are stateless, the\ncontext annotation comes from the erroneous source sentence, not from the corrected target sentence. We further investigate different source factors, elements taking part in the edit operation or appearing in the context can either be word forms (factor 0) or word classes (factor 1). As before for dense features we average sparse feature weights across folds and multiple tuning runs.\nFigure 4 summarizes the results for our sparse feature experiments. On both test sets we can see significant improvements when including editbased sparse features, the performance increases even more when source context is added. The CoNLL-2013 test set contains annotations from only one annotator and is strongly biased towards high precision which might explain the greater instability. It appears that sparse features with context where surface forms and word-classes are mixed allow for the best fine-tuning."}, {"heading": "5 Adding a web-scale language model", "text": "Until now we restricted our experiments to data used by Susanto et al. (2014). However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl. Xie et al. (2016) reach the best published result for the task (before this work) by integrating a similar n-gram language model with their neural approach.\nTable 4 summarizes the best results reported in this paper for the CoNLL-2014 test set (column 2014) before and after adding the Common Crawl n-gram language model. The vanilla Moses baseline with the Common Crawl model can be seen as a new simple baseline for unrestricted settings and is ahead of any previously published result. The combination of sparse features and web-scale monolingual data marks our best result, outperforming previously published results by 9% M2 (a relative improvement of 16%) using similar training data. While our sparse features cause a respectable gain when used with the smaller language model, the web-scale language model seems to cancel out part of the effect.\nBryant and Ng (2015) extended the CoNLL-2014 test set with additional annotations from two to ten annotators. We report results for this valuable re-\nsource (column 2014-10) as well.2 According to the Bryant and Ng (2015), human annotators seem to reach on average 72.58% M2 which can be seen as an upperbound for the task. In this work, we made a large step towards this upperbound."}, {"heading": "6 Conclusions", "text": "Despite the fact that statistical machine translation approaches are among the most popular methods in automatic grammatical error correction, few papers that report results for the CoNLL-2014 test set seem to have fully exploited its full potential. An important aspect, when training SMT systems, that one needs to tune model parameters towards the task evaluation metric, seems to have been underexplored.\nWe have shown that a bare-bones SMT system actually outperforms the best reported results for any paradigm in GEC if correct parameter tuning is performed. With this tuning mechanism available, taskspecific features have been explored that bring further significant improvements, putting phrase-based SMT ahead of other approaches by a large margin. None of the explored features require complicated pipelines or reranking mechanisms. Instead they are a natural part of the log-linear model in phrasebased SMT. It is therefore quite easy to reproduce our results and the presented systems may serve as new baselines for automatic grammatical error correction.\n2See Bryant and Ng (2015) for a re-assessment of the CoNLL-2014 systems with this extended test set."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Colin Cherry for his help with Batch Mira hyper-parameters and Kenneth Heafield for many helpful comments and discussions. This work was partially funded by the Polish National Science Centre (Grant No. 2014/15/N/ST6/02330)."}], "references": [{"title": "Correcting ESL errors using phrasal SMT techniques", "author": ["Chris Brockett", "William B. Dolan", "Michael Gamon."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational", "citeRegEx": "Brockett et al\\.,? 2006", "shortCiteRegEx": "Brockett et al\\.", "year": 2006}, {"title": "How far are we from fully automatic high quality grammatical error correction", "author": ["Christopher Bryant", "Hwee Tou Ng"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Bryant and Ng.,? \\Q2015\\E", "shortCiteRegEx": "Bryant and Ng.", "year": 2015}, {"title": "N-gram counts and language models from the Common Crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference, pages 3579\u2013 3584, Reykjav\u0131\u0301k, Iceland.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Methods for smoothing the optimizer instability in SMT", "author": ["Mauro Cettolo", "Nicola Bertoldi", "Marcello Federico."], "venue": "MT Summit XIII: the Thirteenth Machine Translation Summit, pages 32\u201339.", "citeRegEx": "Cettolo et al\\.,? 2011", "shortCiteRegEx": "Cettolo et al\\.", "year": 2011}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Better evaluation for grammatical error correction", "author": ["Daniel Dahlmeier", "Hwee Tou Ng."], "venue": "Proceedings of the 2012 Conference of the North American Chapter", "citeRegEx": "Dahlmeier and Ng.,? 2012", "shortCiteRegEx": "Dahlmeier and Ng.", "year": 2012}, {"title": "Building a large annotated corpus of learner english: The NUS Corpus of Learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22\u2013", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "Can markov models over minimal translation units help phrase-based smt? In ACL (2), pages 399\u2013405", "author": ["Nadir Durrani", "Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn."], "venue": "The Association for Computer Linguistics.", "citeRegEx": "Durrani et al\\.,? 2013", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Mariano Felice", "Zheng Yuan", "\u00d8istein E. Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Lan-", "citeRegEx": "Felice et al\\.,? 2014", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Parallel implementations of word alignment tool", "author": ["Qin Gao", "Stephan Vogel."], "venue": "Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49\u201357. ACL.", "citeRegEx": "Gao and Vogel.,? 2008", "shortCiteRegEx": "Gao and Vogel.", "year": 2008}, {"title": "Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme", "author": ["Kenneth Heafield", "Alon Lavie."], "venue": "The Prague Bulletin of Mathematical Linguistics, 93:27\u201336.", "citeRegEx": "Heafield and Lavie.,? 2010", "shortCiteRegEx": "Heafield and Lavie.", "year": 2010}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 1352\u20131362, Stroudsburg, USA. Association for Computational Linguistics.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "The amu system in the conll-2014 shared task: Grammatical error", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2014", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The effect of learner corpus size in grammatical error correction of ESL writings", "author": ["Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yu Matsumoto."], "venue": "Proceedings of COLING 2012, pages 863\u2013872.", "citeRegEx": "Mizumoto et al\\.,? 2012", "shortCiteRegEx": "Mizumoto et al\\.", "year": 2012}, {"title": "The CoNLL2013 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task, pages 1\u2013", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natu-", "citeRegEx": "Ng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL \u201903, pages 160\u2013167, Stroudsburg, USA. Association for Compu-", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311\u2013318, Stroudsburg, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The Illinois-Columbia system in the CoNLL-2014 shared task", "author": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash."], "venue": "CoNLL2014, pages 34\u201342.", "citeRegEx": "Rozovskaya et al\\.,? 2014", "shortCiteRegEx": "Rozovskaya et al\\.", "year": 2014}, {"title": "System combination for grammatical error correction", "author": ["Hendy Raymond Susanto", "Peter Phandi", "Tou Hwee Ng."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951\u2013962. Association for Computa-", "citeRegEx": "Susanto et al\\.,? 2014", "shortCiteRegEx": "Susanto et al\\.", "year": 2014}, {"title": "Neural language correction with character-based attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "CoRR, abs/1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "NAIST at 2013 CoNLL grammatical error correction shared task", "author": ["Ippei Yoshimoto", "Tomoya Kose", "Kensuke Mitsuzawa", "Keisuke Sakaguchi", "Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Yuji Matsumoto."], "venue": "Proceedings of the 17th", "citeRegEx": "Yoshimoto et al\\.,? 2013", "shortCiteRegEx": "Yoshimoto et al\\.", "year": 2013}, {"title": "Constrained grammatical error correction using statistical machine translation", "author": ["Zheng Yuan", "Mariano Felice."], "venue": "Proceedings of the 17th Conference on Computational Natural Language Learning: Shared Task, pages 52\u201361, Sofia, Bulgaria. Association for", "citeRegEx": "Yuan and Felice.,? 2013", "shortCiteRegEx": "Yuan and Felice.", "year": 2013}], "referenceMentions": [{"referenceID": 22, "context": "With the recent paradigm shift in machine translation towards neural translation models, neural encoder-decoder models are expected to appear in the field of GEC as well, and first published results (Xie et al., 2016) already mark the new state-of-theart for GEC.", "startOffset": 199, "endOffset": 217}, {"referenceID": 0, "context": "While machine translation has been used for GEC in works as early as Brockett et al. (2006), we start our discussion with the CoNLL-2014 shared task ar X iv :1 60 5.", "startOffset": 69, "endOffset": 92}, {"referenceID": 17, "context": "(Ng et al., 2014) where for the first time an unrestricted set of errors had to be fully corrected.", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "Previous work, most notably during the CoNLL sharedtask 2013 (Ng et al., 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al.", "startOffset": 61, "endOffset": 78}, {"referenceID": 23, "context": ", 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well.", "startOffset": 93, "endOffset": 140}, {"referenceID": 24, "context": ", 2013), concentrated only on five selected errors types, but machine translation approaches (Yoshimoto et al., 2013; Yuan and Felice, 2013) were used as well.", "startOffset": 93, "endOffset": 140}, {"referenceID": 6, "context": "The corrected system outputs were evaluated blindly using the MaxMatch (M2) metric (Dahlmeier and Ng, 2012).", "startOffset": 83, "endOffset": 107}, {"referenceID": 9, "context": "Among the top-three positioned systems, two submissions \u2014 CAMB (Felice et al., 2014) and AMU (JunczysDowmunt and Grundkiewicz, 2014) \u2014 were partially or fully based on SMT.", "startOffset": 63, "endOffset": 84}, {"referenceID": 20, "context": "The second system, CUUI (Rozovskaya et al., 2014), was a classifierbased approach, another popular paradigm in GEC.", "startOffset": 24, "endOffset": 49}, {"referenceID": 11, "context": "They combined the output from a classificationbased system and a SMT-based system using MEMT (Heafield and Lavie, 2010), reporting new state-of-the-art results for the CoNLL-2014 test set.", "startOffset": 93, "endOffset": 119}, {"referenceID": 20, "context": "Shortly after the shared task, Susanto et al. (2014) published a work on GEC systems combinations.", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "Positions marked with (r) use only data restricted data set which corresponds to the data set used by Susanto et al. (2014). Positions with (u) Co NL L 20 14 (u)", "startOffset": 102, "endOffset": 124}, {"referenceID": 22, "context": "make use of web-scale data, this corresponds to the data used in Xie et al. (2016). We marked the participants of the CoNLL-2014 shared task as unrestricted as some participants made use of Common Crawl data or Google n-grams.", "startOffset": 65, "endOffset": 83}, {"referenceID": 18, "context": "Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al.", "startOffset": 73, "endOffset": 84}, {"referenceID": 19, "context": "Prior work used Moses with default settings: minimum error rate training (Och, 2003) towards BLEU (Papineni et al., 2002).", "startOffset": 98, "endOffset": 121}, {"referenceID": 6, "context": "The M2 metric (Dahlmeier and Ng, 2012) is an FScore, based on the edits extracted from a Levenshtein distance matrix.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": "Based on Clark et al. (2011) concerning the effects of optimizer instability, we report results averaged over five tuning runs.", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": "Additionally, we compute parameter weight vector centroids as suggested by Cettolo et al. (2011). They showed that parameter vector centroids averaged over several tuning runs yield similar to or better than average results and reduce variance.", "startOffset": 75, "endOffset": 97}, {"referenceID": 12, "context": "Junczys-Dowmunt and Grundkiewicz (2014) use word-based Levenshtein distance between source and target phrases as a translation model feature, Felice et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 9, "context": "Junczys-Dowmunt and Grundkiewicz (2014) use word-based Levenshtein distance between source and target phrases as a translation model feature, Felice et al. (2014) independently experiment with a character-based version.", "startOffset": 142, "endOffset": 163}, {"referenceID": 8, "context": "Durrani et al. (2013) introduce Operation Sequence Models in Moses.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "The monolingual Wikipedia data has been used create a 9-gram wordclass language model with 200 word-classes produced by word2vec (Mikolov et al., 2013).", "startOffset": 129, "endOffset": 151}, {"referenceID": 7, "context": "The training data provided in both shared tasks is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013).", "startOffset": 93, "endOffset": 117}, {"referenceID": 15, "context": "Parallel data for translation model training is adapted from the above mentioned NUCLE corpus and the publicly available Lang-8 corpus (Mizumoto et al., 2012).", "startOffset": 135, "endOffset": 158}, {"referenceID": 20, "context": "As mentioned before, in order to make our results comparable to previous work, we report main results using similar training data as Susanto et al. (2014). We refer to this setting that as the \u201cresticted-data setting\u201d (r).", "startOffset": 133, "endOffset": 155}, {"referenceID": 12, "context": "As mentioned above, to demonstrate the ease of data integration we propose an \u201cunrestricted setting\u201d (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of the shared task submissions, and later in Xie et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 12, "context": "As mentioned above, to demonstrate the ease of data integration we propose an \u201cunrestricted setting\u201d (u) based on the data used in Junczys-Dowmunt and Grundkiewicz (2014), one of the shared task submissions, and later in Xie et al. (2016). We use Common Crawl data made-available by Buck et al.", "startOffset": 131, "endOffset": 239}, {"referenceID": 2, "context": "We use Common Crawl data made-available by Buck et al. (2014).", "startOffset": 43, "endOffset": 62}, {"referenceID": 10, "context": "The translation model is built with the standard Moses training script, word-alignment models are produced with MGIZA++ (Gao and Vogel, 2008), we restrict the word alignment training to 5 iterations of Model 1 and 5 iterations of the HMM-Model.", "startOffset": 120, "endOffset": 141}, {"referenceID": 21, "context": "We first successfully reproduce results from Susanto et al. (2014) for BLEU-based tuning on the CoNLL-2013 test set as the development set (Fig.", "startOffset": 45, "endOffset": 67}, {"referenceID": 21, "context": "We first successfully reproduce results from Susanto et al. (2014) for BLEU-based tuning on the CoNLL-2013 test set as the development set (Fig. 2a) using similar training data. Repeated tuning places the scores reported by Susanto et al. (2014) for their SMT-ML combinations (37.", "startOffset": 45, "endOffset": 246}, {"referenceID": 21, "context": "Since Susanto et al. (2014) do not report results for multiple tuning steps, the extend of influence of optimizer instability on their experiments remains unclear.", "startOffset": 6, "endOffset": 28}, {"referenceID": 21, "context": "Since Susanto et al. (2014) do not report results for multiple tuning steps, the extend of influence of optimizer instability on their experiments remains unclear. Even with BLEU-based tuning, we can see significant improvements when replacing Levenshtein distance with the finer-grained edit operations, and another performance jump with additional stateful features. The value range of the different tuning runs for the last feature set includes the currently bestperforming system (Xie et al. (2016) with 40.", "startOffset": 6, "endOffset": 503}, {"referenceID": 21, "context": "Until now, we followed the seemingly obvious approach from Susanto et al. (2014) to tune on the CoNLL-2013 test set.", "startOffset": 59, "endOffset": 81}, {"referenceID": 12, "context": "We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3).", "startOffset": 37, "endOffset": 60}, {"referenceID": 4, "context": "We first experimented with both, PRO (Hopkins and May, 2011) and Batch Mira (Cherry and Foster, 2012), for the dense features only, and found PRO and Batch Mira with standard settings to either severely underperform in comparison to MERT or to suffer from instability with regard to different test sets (Table 3).", "startOffset": 76, "endOffset": 101}, {"referenceID": 20, "context": "Until now we restricted our experiments to data used by Susanto et al. (2014). However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl.", "startOffset": 56, "endOffset": 78}, {"referenceID": 13, "context": "However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl.", "startOffset": 96, "endOffset": 136}, {"referenceID": 13, "context": "However, systems from the CoNLL-2014 were free to use any publicly available data, for instance Junczys-Dowmunt and Grundkiewicz (2014) make use of an n-gram language model trained from Common Crawl. Xie et al. (2016) reach the best published result for the task (before this work) by integrating a similar n-gram language model with their neural approach.", "startOffset": 96, "endOffset": 218}, {"referenceID": 1, "context": "2 According to the Bryant and Ng (2015), human annotators seem to reach on average 72.", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "See Bryant and Ng (2015) for a re-assessment of the CoNLL-2014 systems with this extended test set.", "startOffset": 4, "endOffset": 25}], "year": 2017, "abstractText": "In this work, we study parameter tuning towards the M2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M2 and offer partial solutions. To our surprise, we find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M2 over previously 40.56%, by a neural encoder-decoder model) while being trained on the same data. Our newly introduced dense and sparse features widen that gap, and we improve the state-ofthe-art to 49.49% M2.", "creator": "LaTeX with hyperref package"}}}