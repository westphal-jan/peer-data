{"id": "1605.06560", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Functional Hashing for Compressing Neural Networks", "abstract": "as the entropy of deep neural networks ( dnns ) trend to grow through absorb the increasing sizes of data, security and energy consumption estimates had required more and more attentions for industrial applications, especially on mobile devices. this paper presents a conceptual structure based on functional hashing to compress dnns, namely persistence. for each reconstruction amongst a deep net, funhashnn uses multiple low - cost hash functions to fetch values in the compression space, and then employs a small reconstructed network to recover that entry. the reconstruction network is plugged into possibly whole cache and trained jointly. funhashnn includes the recently proposed hashednets as usual degenerated case, and resultant from larger value capacity - less reconstruction loss. we further discuss extensions with dual space hashing and multi - hops. on several experimental datasets, funhashnn demonstrates high compression ratios with little loss on prediction constraints.", "histories": [["v1", "Fri, 20 May 2016 23:44:19 GMT  (1995kb,D)", "http://arxiv.org/abs/1605.06560v1", "submitted to NIPS 2016"]], "COMMENTS": "submitted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["lei shi", "shikun feng", "zhifanzhu"], "accepted": false, "id": "1605.06560"}, "pdf": {"name": "1605.06560.pdf", "metadata": {"source": "CRF", "title": "Functional Hashing for Compressing Neural Networks", "authors": ["Lei Shi", "Shikun Feng", "Zhifan Zhu"], "emails": ["zhuzhifan}@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13]. As the sizes of data mount up, people usually have to increase the number of parameters in DNNs so as to absorb the vast volume of supervision. High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].\nOn the other hand the memory and energy consumption is usually, if not always, constrained in industrial applications [21, 35]. For instance, for commercial search engines (e.g., Google and Baidu) and recommendation systems (e.g., NetFlix and YouTube), the ratio between the increased model size and the improved performance should be considered given limited online resources. Compressing the model size becomes more important for applications on mobile and embedded devices [15, 21]. Having DNNs running on mobile apps owns many great features such as better privacy, less network bandwidth and real time processing. However, the energy consumption of battery-constrained mobile devices is usually dominated by memory access, which would be greatly saved if a DNN model can fit in on-chip storage rather than DRAM storage (c.f. [15, 16] for details).\nA recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35]. With different intuitions, there are mainly two types of DNN compression methods, which could be used in conjunction for better parameter savings. The first type tries to revise the training target into more informative supervision using dark knowledge. In specific, Hinton et al. [18] suggested to train a large network ahead, and distill a much smaller model on a combination of the original labels and the soft-output by the large net. The second type observes the redundancy existence\nar X\niv :1\n60 5.\n06 56\n0v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\nin network weights [7, 11], and exploits techniques to constrain or reduce the number of free-parameters in DNNs during learning. This paper focuses on the latter type.\nTo constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions. Moreover [15, 16] proposed a simple-yet-effective pruning-retraining iteration during training, followed by quantization and fine-tuning. Chen et al. [7] proposed HashedNets to efficiently implement parameter sharing prior to learning, and showed notable compression with much less loss of accuracy than low-rank decomposition. More precisely, prior to training, a hash function is used to randomly group (virtual) weights into a small number of buckets, so that all weights mapped into one hash bucket directly share a same value. HashedNets was further deliberated in frequency domain for compressing convolutional neural networks in [6].\nIn applications, we observe HashedNets compresses model sizes greatly at marginal loss of accuracy for some situations, whereas also significantly loses accuracy for others. After revisiting its mechanism, we conjecture this instability comes from at least three factors. First, hashing and training are disjoint in a two-phase manner, i.e., once inappropriate collisions exist, there may be no much optimization room left for training. Second, one single hash function is used to fetch a single value in the compression space, whose collision risk is larger than multiple hashes [4]. Third, parameter sharing within a buckets implicitly uses identity mapping from the hashed value to the virtual entry.\nThis paper proposes an approach to relieve this instability, still in a two-phase style for preserving efficiency. Specifically, we use multiple hash functions [4] to map per virtual entry into multiple values in compression space. Then an additional network plays in a mapping function role from these hashed values to the virtual entry before hashing, which can be also regarded as \u201creconstructing\u201d the virtual entry from its multiple hashed values. Plugged into and jointly trained within the original network, the reconstruction network is of a comparably ignorable size, i.e., at low memory cost.\nThis functional hashing structure includes HashedNets as a degenerated special case, and facilitates less value collisions and better value reconstruction. Shortly denoted as FunHashNN, our approach could be further extended with dual space hashing and multi-hops. Since it imposes no restriction on other network design choices (e.g. dropout and weight sparsification), FunHashNN can be considered as a standard tool for DNN compression. Experiments on several datasets demonstrate promisingly larger reduction of model sizes and/or less loss on prediction accuracy, compared with HashedNets."}, {"heading": "2 Background", "text": "Notations. Throughout this paper we express scalars in regular (A or b), vectors in bold (x), and matrices in capital bold (X). Furthermore, we use xi to represent the i-th dimension of vector x, and use Xij to represent the (i, j)-th entry of matrix X. Occasionally, [x]i is also used to represent the i-th dimension of vector x for specification clarity . Notation E[\u00b7] stands for the expectation operator.\nFeed Forward Neural Networks. We define the forward propagation of the `-th layer as\na`+1i = f(z `+1 i ), with z `+1 i = b `+1 i + d`\u2211 j=1 V `ija ` j , for \u2200i \u2208 [1, d`+1]. (1)\nFor each `-th layer, d` is the output dimensionality, b` is the bias vector, and V` is the (virtual) weight matrix in the `-th layer. Vectors z`,a` \u2208 Rd` denote the units before and after the activation function f(\u00b7). Typical choices of f(\u00b7) include rectified linear unit (ReLU) [29], sigmoid and tanh [2].\nFeature Hashing has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection [32, 34]. Briefly, it maps an input vector x \u2208 Rn to a much smaller feature space via \u03c6 : Rn \u2192 RK with\nK n. Following the definition in [34], the mapping \u03c6 is a composite of two approximate uniform hash functions h : N \u2192 {1, . . . ,K} and \u03be : N \u2192 {\u22121,+1}. The j-th element of \u03c6(x) is defined as:\n[\u03c6(x)]j = \u2211\ni:h(i)=j\n\u03be(i)xi. (2)\nAs shown in [34], a key property is its inner product preservation, which we quote and restate below.\nLemma [Inner Product Preservation of Original Feature Hashing] With the hash defined by Eq. (2), the hash kernel is unbiased, i.e., E\u03c6[\u03c6(x)>\u03c6(y)] = x>y. Moreover, the variance is varx,y = 1K \u2211 i 6=j ( x2i y 2 j + xiyixjyj ) , and thus varx,y = O( 1K ) if ||x||2 = ||y||2 = const.\nHashedNets in [7]. As illustrated in Figure 1(a), HashedNets randomly maps network weights into a smaller number of groups prior to learning, and the weights in a same group share a same value thereafter. A naive implementation could be trivially achieved by maintaining a secondary matrix that records the group assignment, at the expense of additional memory cost however. HashedNets instead adopts a hash function that requires no storage cost with the model. Assume there is a finite memory budge K` per layer to represent V`, with K` (d` + 1)d`+1. We only need to store a weight vector w` \u2208 RK` , and assign V `ij an element in w` indexed by a hash function h`(i, j), namely\nV `ij = \u03be `(i, j) \u00b7 w`h`(i,j), (3)\nwhere hash function h`(i, j) outputs an integer within [1,K`]. Another independent hash function \u03be`(i, j) : (d`+1 \u00d7 d`)\u2192 \u00b11 outputs a sign factor, aiming to reduce the bias due to hash collisions [34]. The resulting matrix V` is virtual, since d` could be increased without increasing the actual number of parameters in w` once the compression space size K` is determined and fixed.\nSubstituting Eq. (3) into Eq. (1), we have z`+1i = b `+1 i + \u2211d` j=1 \u03be `(i, j)w`h`(i,j)a ` j . During training, w` is updated by back propagating the gradient via z`+1 (and the virtual V`). Besides, the activation function f(\u00b7) in Eq. (1) was kept as ReLU in [7] to further relieve the hash collision effect through a sparse feature space. In both [7] and this paper, the open source xxHash1 is adopted as an approximately uniform hash implementation with low cost.\n1http://cyan4973.github.io/xxHash/"}, {"heading": "3 Functional Hashing for Neural Network Compression", "text": ""}, {"heading": "3.1 Structure Formulation", "text": "For clarity, we will focus on a single layer throughout and drop the super-script `. Still, vector w \u2208 RK denotes parameters in the compression space. The key difference between FunHashNN and HashedNets [7] lies in (i) how to employ hash functions, and (ii) how to map from w to V:\n\u2022 Instead of adopting one pair of hash function (h, \u03be) in Eq. (3), we use a set of multiple pairs of independent random hash functions. Let\u2019s say there are U pairs of mappings {hu, \u03beu}Uu=1, each hu(i, j) outputs an integer within [1,K], and each \u03beu(i, j) selects a sign factor. \u2022 Eq. (3) of HashedNets employs an identity mapping between one element in V and\none hashed value, i.e., Vij = \u03be(i, j)wh(i,j). In contrast, we use a multivariate function g(\u00b7) to describe the mapping from multiple hashed values {\u03beu(i, j)whu(i,j)}Uu=1 to Vij . Specifically,\nVij = g ([ \u03be1(i, j)wh1(i,j), . . . , \u03beU (i, j)whU (i,j) ] ; \u03b1 ) . (4)\nTherein, \u03b1 is referred to as the parameters in g(\u00b7). Note that the input \u03beu(i, j)whu(i,j) is order sensitive from u = 1 to U . We choose g(\u00b7) to be a multi-layer feed forward neural network, and other multivariate functions may be considered as alternatives.\nAs a whole, Figure 1(b) illustrates our FunHashNN structure, which can be easily plugged in any matrices of DNNs. Note that \u03b1 in the reconstruction network g(\u00b7) is of a much smaller size compared to w. For instance, a setting with U = 4 and a 1-layer g(\u00b7;\u03b1) of \u03b1 \u2208 R4 performs already well enough in experiments. In other words, Eq. (4) just uses an ignorable amount of additional memory to describe a functional w-to-V mapping, whose properties will be further explained in the sequel."}, {"heading": "3.2 Training Procedure", "text": "The parameters in need of updating include w in the compression and \u03b1 in g(\u00b7). Training FunHashNN is equivalent to training a standard neural network, except that we need to forward/backward-propagate values related to w through g(\u00b7) and the virtual matrix V.\nForward Propagation. Substituting Eq. (4) into Eq. (1), we still omit the super-script ` and get\nzi = bi + d\u2211 j=1 ajVij = bi + d\u2211 j=1 aj \u00b7 g ([ \u03be1(i, j)wh1(i,j), . . . , \u03beU (i, j)whU (i,j) ] ; \u03b1 ) . (5)\nBackward Propagation. Denote L as the final loss function, e.g., cross entropy or squared loss, and suppose \u03b4i = \u2202L\u2202zi is already available by back-propagation from layers above. The derivatives of L with respect to w and \u03b1 are computed by\n\u2202L \u2202w = \u2211 i \u2211 j aj\u03b4i \u2202Vij \u2202w , \u2202L \u2202\u03b1 = \u2211 i \u2211 j aj\u03b4i \u2202Vij \u2202\u03b1 , (6)\nwhere, since we choose g(\u00b7) as a multilayer neural network, derivatives \u2202Vij\u2202w and \u2202Vij \u2202\u03b1 can be calculated through the small network g(\u00b7) in a standard back-propagation manner.\nComplexity. Concerning time and memory cost, FunHashNN roughly has the same complexity as HashedNets, since the small network g(\u00b7) is quite light-weighted. One key variable factor is the way to implement multiple hash functions. On one hand, if they are calculated online, then FunHashNN requires little additional time if tackling them in parallel. On the other, if they are pre-computed and stored in dicts to avoid hashing time cost, the multiple hash functions of FunHashNN demand more storage space. In application, we suggest to pre-compute hashes during offline training for speedup, and to compute hashes in parallel during online prediction for saving memory under limited budget."}, {"heading": "3.3 Property Analysis", "text": "In this part, we try to depict the properties of our FunHashNN from several aspects to help understanding it, especially in comparison with HashedNets [7].\nValue Collision. It should be noted, both HashedNets and FunHashNN conduct hashing prior to training, i.e., in a two-phase manner. Consequently, it would be unsatisfactory if hashing collisions happen among important values. For instance in natural language processing tasks, one may observe wired results if there are many hashing collisions among embeddings (which form a matrix) of frequent words, especially when they are not related at all. In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5]. In intuition, when we have multiple hash functions, the items colliding in one function are hashed differently by other hash functions.\nValue Reconstruction. In both HashedNets and FunHashNN, the hashing trick can be viewed as a reconstruction of the original parameter V from w \u2208 RK . In this sense, the approach with a lower reconstruction error is preferred2. Then we have at least the following two observations:\n\u2022 The maximum number of possible distinct values output by hashing intuitively explains the modelling capability [32]. For HashedNets, considering the sign hashing function \u03be(\u00b7), we have at most 2K possible distinct values of Eq. (3) to represent elements in V. In contrast, since there are multiple ordered hashed inputs, FunHashNN has at most (2K)U possible distinct values of Eq. (4). Note that the memory size K is the same for both. \u2022 The reconstruction error may be difficult to analyzed directly, since the hashing mechanism is trained jointly within the whole network. However, we observe g ([ \u03be1(i, j)wh1(i,j), . . . , \u03beU (i, j)whU (i,j) ] ; \u03b1 ) degenerates to g(\u03be1(i, j)wh1(i,j))\nif we assign zeros to all entries in \u03b1 unrelated to the 1st input dimension. Since g(\u03be1(i, j)wh1(i,j)) depends only on one single pair of hash functions, it is conceptually equivalent to HashedNets. Consequently, including HashedNets as a special case, FunHashNN with freely adjustable \u03b1 is able to reach a lower reconstruction error to fit the final accuracy better.\nFeature Hashing. In line with previous work [32, 34], we compare HashedNets and FunHashNN in terms of feature hashing. For specification clarity, we drop the sign hashing functions \u03be(\u00b7) below for both methods, the analysis with which is straightforward by replacing K hereafter with 2K.\n\u2022 For HashedNets, one first defines a hash mapping function \u03c6(1)i (a), whose k-th element is [\n\u03c6 (1) i (a) ] k , \u2211 j:h(i,j)=k aj , for k = 1, . . . ,K. (7)\nThus zi by HashedNets can be computed as the inner product (details c.f. Section 4.3 in [7])\nzi = w >\u03c6 (1) i (a). (8)\n\u2022 For FunHashNN, we first define a hash mapping function \u03c6(2)i (a). Different from a K-dim output in Eq. (7), it is of a much larger size KU , with (\u2211U u=1 kuK (u\u22121) ) -th\nelement as[ \u03c6\n(2) i (a) ]\u2211U u=1 kuK (u\u22121) , \u2211 j:h1(i,j)=k1 h2(i,j)=k2\n... hU (i,j)=kU\naj , for \u2200u, ku = 1, . . . ,K. (9)\n2One might argue that there exists redundancy in V, whereas here we could imagine V is already structured and filled by values with least redundancy.\nSecond, we define vector g\u03b1(w) still of length KU , whose (\u2211U u=1 kuK (u\u22121) ) -th entry is\n[g\u03b1(w)]\u2211U u=1 kuK (u\u22121) , g (wk1 , wk2 , . . . , wkU ; \u03b1) , for \u2200u, ku = 1, . . . ,K. (10)\nThus zi by FunHashNN can be computed as the following inner product\nzi = g\u03b1(w) > \u03c6 (2) i (a). (11)\nThe difference between Eq. (8) and Eq. (11) further explains the above discussion about \u201cthe maximum number of possible distinct values\u201d."}, {"heading": "3.4 Extensions", "text": "Hashing on Dual Space. If considering a linear model f(x;\u03b8) = \u03b8>x, one can not only deliver analysis like Bayesian or hashing on input feature space of x, but also do similarly on the dual space of \u03b8 [3]. We now revisit the \u201creconstruction\u201d network g(xij ;\u03b1) in Eq. (4), where vector xij concatenates the hashed values \u03beu(i, j)whu(i,j) for u = 1, . . . , U . What we did in Eq. (4) is in fact hashing (i, j) through w to get the input feature of g(\u00b7). In analogy, we can also hash (i, j) to fetch parameters of g(\u00b7), namely we have a new \u201creconstruction\u201d network in the following form:\nVij = g(xij ;\u03b1ij), with [xij ]u = \u03beu(i, j)whu(i,j) and [\u03b1ij ]r = \u03be \u2032 r(i, j)w \u2032 h\u2032r(i,j) , (12)\nwhere {\u03be\u2032r(\u00b7), h\u2032r(\u00b7)} are additional multiple pairs of hash functions applied on \u03b1, and w\u2032 is an additional vector in the compression space of \u03b1. The size of \u03b1ij remains the same as previous. Using this trick, the maximum number of possible distinct values of V further increases exponentially, so that FunHashNN has more potential ability to fit the prediction well. We denote FunHashNN with dual space hashing shortly as FunHashNN-D, and illustrate its structure in Figure 1(c).\nMulti-hops. We conjecture that FunHashNN could be used in a multi-hops structure, by imagining w in the compression space plays a virtual role similar to V. Specifically, we can build another level of hash functions { \u03be (1) u (\u00b7), h(1)u (\u00b7) } and compression space w(1).\nThereafter, each entry in w is hashed into multiple values in w(1) via { \u03be (1) u (\u00b7), h(1)u (\u00b7) } . Then\nanother reconstruction network g(1)(\u00b7) is used to learn the mapping from the hashed values in w(1) to the corresponding entry in w.\nThis procedure can be implemented recursively. If there are in total M -hops, what we need to save in fact just includes a (possibly much more smaller) vector w(M) at the final hop, a series of M small reconstruction networks {g(m)(\u00b7)}Mm=1, and a series of hashing functions. In contrast, the multi-hops version of HashedNets is equivalent to just adjusting the compression ratio, or say the size K."}, {"heading": "4 Related Work", "text": "Recent studies have confirmed the redundancy existence in the parameters of deep neural networks. Denil et al. [11] decomposed a matrix in a fully-connected layers as the product of two low-rank matrices, so that the number of parameters decreases linearly as the latent dimensionality decreases. More structured decompositions Fastfood [25] and Deep Fried [35] were proposed not only to reduce the number of parameters, but also to speed up matrix multiplications. More recently, Han et al. [15, 16] proposed to iterate pruning-retraining during training DNNs, and used quantization and fine-tuning as a post-processing step. Huffman coding and hardware implementation were also considered. In order to mostly keep accuracy, the authors suggested multiple rounds of pruning-retraining. That is, for little accuracy loss, we have to prune slowly enough and thus suffer from increased training time. Again, the most related work to ours is HashedNets [7], which was then extended in [6] to random hashing in frequency domain for compressing convolutional neural networks. Either HashedNets or FunHashNN could be combined in conjunction with other techniques for better compression.\nExtensive studies have been made on constructing and analyzing multiple hash functions, which have shown better performances over one single hash function [4]. One multi-hashing algorithm, d-random scheme [1], uses only one hash table but d hash functions, pretty similar to our settings. One choice alternative to d-random is the d-left algorithm proposed in [5], used for improving IP lookups. Hashing algorithms for natural language processing are also studied in [14]. Papers [32, 34] investigated feature hashing (a.k.a. the hashing trick), providing useful bounds and feasible results."}, {"heading": "5 Experiments", "text": "We conduct extensive experiments to evaluate FunHashNN on DNN compression. Codes for fully reproducibility will be open source soon after necessary polishment."}, {"heading": "5.1 Environment Descriptions", "text": "Datasets. Three benchmark datasets [24] are considered here, including (1) the original MNIST hand-written digit dataset, (2) dataset BG-IMG as a variant to MNIST, and (3) binary image classification dataset CONVEX. For all datasets, we use prespecified training and testing splits. In particular, the original MNIST dataset has #train=60,000 and #test=10,000, while the remaining both have #train=12,000 and #test=50,000. Moreover, collected from a commercial search engine, a large scale dataset with billions of samples is used to learn DNNs for pairwise semantic ranking. We randomly split out 20% samples from the training data to form the validation set.\nMethods and Settings. In [7], the authors compared HashedNets against several DNN compression approaches, and showed HashedNets performs consistently the best, including the low-rank decomposition [11]. Under the same settings, we compare FunHashNN with HashedNets3 and a standard neural network without compression. All activation functions are chosen as ReLU.\nThe settings of FunHashNN are tested in two scenarios. First, we will fix to use FunHashNN in Figure 1(b) without extensions, and then compare the effects of compression by FunHashNN and HashedNets. Second, we compare different configurations of FunHashNN itself, including the number U of seeds, the layer of reconstruction network g(\u00b7), and extension with the dual space hashing. Hidden layers within g(\u00b7) keep using tanh as activation functions. Results by the multi-hops extension of FunHashNN will be included in another ongoing paper for systematic comparisons."}, {"heading": "5.2 Varying Compression Ratio", "text": "To test robustness, we vary the compression ratio with (1) a fixed virtual network size (i.e., the size of V` in each layer), and then with (2) a fixed memory size (i.e., the size of w` in each layer). Three-layer (1 hidden layer) and five-layer (3 hidden layers) networks are investigated. In experiments, we vary the compression ratio geometrically within {1, 12 , 1 4 , . . . , 1 64}. For FunHashNN, this comparison sticked to use 4 hash functions, 3-layer g(\u00b7), and without dual space hashing.\nWith Virtual Network Size Fixed. The hidden layer for 3-layer nets initializes at 1000 units, and for 5-layer nets starts at 100 units per layer. As the compression ratio ranges from 1 to 1/64 with a fixed virtual network size, the memory decreases and it becomes increasingly difficult to preserve the classification accuracy. The testing errors are shown in Figure 2, where standard neural networks with equivalent parameter sizes are included in comparison. FunHashNN shows robustly effective compression against the compression ratios, and persistently produces better prediction accuracy than HashedNets. It should be noted, even when the compression ratio equals to one, FunHashNN with the reconstruction network structure is still not equivalent to HashedNets and performs better.\nWith Memory Storage Fixed. We change to vary the compression ratio from 1 to 1/64 with a fixed memory storage size, i.e., the size of the virtual network increases while the\n3HashedNets code downloaded from http://www.cse.wustl.edu/\u223cwenlinchen/project/HashedNets/index.html\nnumber of free parameters remains unchanged. In this sense, we\u2019d better call it expansion instead of compression. Both 3-layer and 5-layer nets initialize at 50 units per hidden layer. The testing errors in this scenario are shown in Figure 3. At all compression (expansion) ratios on each dataset, FunHashNN performs better than or at least comparably well compared to HashedNets."}, {"heading": "5.3 Varying Configurations of FunHashNN", "text": "On 3-layer nets with compression ratio 1/8, we vary the configuration dimensions of FunHashNN, including the number of hash functions (U), the structure of layers of the reconstruction network g(\u00b7), and whether dual space hashing is turned on. Since it is impossible to enumerate all probable choices, U is restricted to vary in {2, 4, 8, 16}. The structure of g(\u00b7) is chosen from 2 \u223c 4 layers, with U \u00d7 1, U \u00d7 0.5U \u00d7 1, U \u00d7 U \u00d7 0.5U \u00d7 1 layerwise widths, respectively. We denote Ux-Gy as x hash functions and y layers of g(\u00b7), and a suffix -D indicates the dual space hashing.\nTable 1 shows the performances of FunHashNN with different configurations on MNIST. The observations are summarized below. First, the series from index (0) to (1.x) fixes a 3-layer g(\u00b7) and varies the number of hash functions. As listed, more hash functions do not ensure\na better accuracy, and instead U4-G3 performs the best, perhaps because too many hash functions potentially brings too many partial collisions. Second, the series from (0) to (2.x) fixes the number of hash functions and varies the layer number in g(\u00b7), where three layers performs the best mainly due to its strongest representability. Third, indices (3.x) show further improved accuracies using dual space hashing."}, {"heading": "5.4 Pairwise Semantic Ranking", "text": "Finally, we evaluate the performance of FunHashNN on semantic learning-to-rank DNNs. The data is collected from logs of a commercial search engine, with per clicked query-url being a positive sample and per non-clicked being a negative sample. There are totally around 45B samples. We adopt a deep convolutional structured semantic model similar to [19, 31], which is of a siamese structure to describe the semantic similarity between a query and a url title. The network is trained to optimize the cross entropy for each pair of positive and negative samples per query.\nThe performance is evaluated by correct-to-wrong pairwise ranking ratio on testing set. In Figure 4, we plot the performance by a baseline network as training proceeds, compared to FunHashNN and HashNet both with 1/4 compression ratio. With U = 4 hash functions, FunHashNN performs better than HashedNets throughout the training epochs, and even comparable to the full network baseline which requires 4 times of memory storage. The deterioration of HashedNets probably comes from many inappropriate collisions on word embeddings, especially for words of high frequencies."}, {"heading": "6 Conclusion and Future Work", "text": "This paper presents a novel approach FunHashNN for neural network compression. Briefly, after adopting multiple low-cost hash functions to fetch values in compression space, FunHashNN employs a small reconstruction network to recover each entry in an matrix of the original network. The reconstruction network is plugged into the whole network and learned jointly. The recently proposed HashedNets [7] is shown as a degenerated special case of FunHashNN. Extensions of FunHashNN with dual space hashing and multi-hops are also discussed. On several datasets, FunHashNN demonstrates promisingly high compression ratios with little loss on prediction accuracy.\nAs future work, we plan to further systematically analyze the properties and bounds of FunHashNN and its extensions. More industrial applications are also expected, especially on mobile devices. This paper focuses on the fully-connected layer in DNNs, and the compression performance on other structures (such as convolutional layers) is also planned to be studied. As a simple and effective approach, FunHashNN is expected to be a standard tool for DNN compression."}], "references": [{"title": "Balanced allocations", "author": ["Y. Azar", "A. Broder", "A. Karlin", "E. Upfal"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Oxford University Press, Inc.,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Multilevel adaptive hashing", "author": ["A. Broder", "A. Karlin"], "venue": "SODA,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Using multiple hash functions to improve IP lookups", "author": ["A. Broder", "M. Mitzenmacher"], "venue": "INFOCOM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Compressing convolutional neural networks", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "ICML,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "An improved data stream summary: The Count-Min sketch and its application", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "J. Algorithms, 55:29\u201338,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "8-bit approximations for parallelism in deep learning", "author": ["T. Dettmers"], "venue": "ICLR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Domain adaptation for large scale sentiment classification: a deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Sketch algorithms for estimating point queries in NLP", "author": ["A. Goyal", "H.I. Daume", "G. Cormode"], "venue": "EMNLP/CoNLL,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "ICLR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. rahman Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "CIKM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning to hash with multiple representations", "author": ["Y. Kang", "S. Kim", "S. Choi"], "venue": "IEEE ICDM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "ICLR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical evaluations of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Q.V. Le", "T. Sarlos", "A.J. Smola"], "venue": "ICML,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv:1312.4400,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Diversity networks", "author": ["Z. Mariet", "S. Sra"], "venue": "ICLR,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y.-L. Boureau", "Y. LeCun"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "CIKM,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "JMLR, 10:2615\u20132637,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to hash for indexing big data \u2013 a survey", "author": ["J. Wang", "W. Liu", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of IEEE, 104(1):34\u201357,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "ICML,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "In ICCV,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "FunHashNN includes the recently proposed HashedNets [7] as a degenerated case, and benefits from larger value capacity and less reconstruction loss.", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 180, "endOffset": 183}, {"referenceID": 12, "context": "Deep Neural networks (DNNs) have been receiving ubiquitous success in wide applications, ranging from computer vision [22], to speech recognition [17], natural language processing [8], and domain adaptation [13].", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].", "startOffset": 212, "endOffset": 224}, {"referenceID": 11, "context": "High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].", "startOffset": 212, "endOffset": 224}, {"referenceID": 26, "context": "High performance computing techniques are investigated to speed up DNN training, concerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc [10, 12, 27].", "startOffset": 212, "endOffset": 224}, {"referenceID": 20, "context": "On the other hand the memory and energy consumption is usually, if not always, constrained in industrial applications [21, 35].", "startOffset": 118, "endOffset": 126}, {"referenceID": 34, "context": "On the other hand the memory and energy consumption is usually, if not always, constrained in industrial applications [21, 35].", "startOffset": 118, "endOffset": 126}, {"referenceID": 14, "context": "Compressing the model size becomes more important for applications on mobile and embedded devices [15, 21].", "startOffset": 98, "endOffset": 106}, {"referenceID": 20, "context": "Compressing the model size becomes more important for applications on mobile and embedded devices [15, 21].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "[15, 16] for details).", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[15, 16] for details).", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "A recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35].", "startOffset": 136, "endOffset": 148}, {"referenceID": 20, "context": "A recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35].", "startOffset": 136, "endOffset": 148}, {"referenceID": 34, "context": "A recent trend of studies are thus motivated to focus on compressing the size of DNNs while mostly keeping their predictive performance [15, 21, 35].", "startOffset": 136, "endOffset": 148}, {"referenceID": 17, "context": "[18] suggested to train a large network ahead, and distill a much smaller model on a combination of the original labels and the soft-output by the large net.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "in network weights [7, 11], and exploits techniques to constrain or reduce the number of free-parameters in DNNs during learning.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "in network weights [7, 11], and exploits techniques to constrain or reduce the number of free-parameters in DNNs during learning.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "To constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 24, "context": "To constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 34, "context": "To constrain the network redundancy, efforts [11, 25, 35] formulated an original weight matrix into either low-rank or fast-food decompositions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 14, "context": "Moreover [15, 16] proposed a simple-yet-effective pruning-retraining iteration during training, followed by quantization and fine-tuning.", "startOffset": 9, "endOffset": 17}, {"referenceID": 15, "context": "Moreover [15, 16] proposed a simple-yet-effective pruning-retraining iteration during training, followed by quantization and fine-tuning.", "startOffset": 9, "endOffset": 17}, {"referenceID": 6, "context": "[7] proposed HashedNets to efficiently implement parameter sharing prior to learning, and showed notable compression with much less loss of accuracy than low-rank decomposition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "HashedNets was further deliberated in frequency domain for compressing convolutional neural networks in [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "Second, one single hash function is used to fetch a single value in the compression space, whose collision risk is larger than multiple hashes [4].", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "Specifically, we use multiple hash functions [4] to map per virtual entry into multiple values in compression space.", "startOffset": 45, "endOffset": 48}, {"referenceID": 28, "context": "Typical choices of f(\u00b7) include rectified linear unit (ReLU) [29], sigmoid and tanh [2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "Typical choices of f(\u00b7) include rectified linear unit (ReLU) [29], sigmoid and tanh [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 31, "context": "Feature Hashing has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection [32, 34].", "startOffset": 161, "endOffset": 169}, {"referenceID": 33, "context": "Feature Hashing has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection [32, 34].", "startOffset": 161, "endOffset": 169}, {"referenceID": 33, "context": "Following the definition in [34], the mapping \u03c6 is a composite of two approximate uniform hash functions h : N \u2192 {1, .", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "As shown in [34], a key property is its inner product preservation, which we quote and restate below.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "HashedNets in [7].", "startOffset": 14, "endOffset": 17}, {"referenceID": 33, "context": "Another independent hash function \u03be(i, j) : (d \u00d7 d)\u2192 \u00b11 outputs a sign factor, aiming to reduce the bias due to hash collisions [34].", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "(1) was kept as ReLU in [7] to further relieve the hash collision effect through a sparse feature space.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "In both [7] and this paper, the open source xxHash1 is adopted as an approximately uniform hash implementation with low cost.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "(a) HashedNets [7].", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The key difference between FunHashNN and HashedNets [7] lies in (i) how to employ hash functions, and (ii) how to map from w to V:", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "In this part, we try to depict the properties of our FunHashNN from several aspects to help understanding it, especially in comparison with HashedNets [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5].", "startOffset": 96, "endOffset": 105}, {"referenceID": 3, "context": "In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5].", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "In the literature, multiple hash functions are known to perform better than one single function [1, 4, 5].", "startOffset": 96, "endOffset": 105}, {"referenceID": 31, "context": "\u2022 The maximum number of possible distinct values output by hashing intuitively explains the modelling capability [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 31, "context": "In line with previous work [32, 34], we compare HashedNets and FunHashNN in terms of feature hashing.", "startOffset": 27, "endOffset": 35}, {"referenceID": 33, "context": "In line with previous work [32, 34], we compare HashedNets and FunHashNN in terms of feature hashing.", "startOffset": 27, "endOffset": 35}, {"referenceID": 6, "context": "3 in [7]) zi = w >\u03c6 (1) i (a).", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "If considering a linear model f(x;\u03b8) = \u03b8>x, one can not only deliver analysis like Bayesian or hashing on input feature space of x, but also do similarly on the dual space of \u03b8 [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 10, "context": "[11] decomposed a matrix in a fully-connected layers as the product of two low-rank matrices, so that the number of parameters decreases linearly as the latent dimensionality decreases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "More structured decompositions Fastfood [25] and Deep Fried [35] were proposed not only to reduce the number of parameters, but also to speed up matrix multiplications.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "More structured decompositions Fastfood [25] and Deep Fried [35] were proposed not only to reduce the number of parameters, but also to speed up matrix multiplications.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "[15, 16] proposed to iterate pruning-retraining during training DNNs, and used quantization and fine-tuning as a post-processing step.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[15, 16] proposed to iterate pruning-retraining during training DNNs, and used quantization and fine-tuning as a post-processing step.", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "Again, the most related work to ours is HashedNets [7], which was then extended in [6] to random hashing in frequency domain for compressing convolutional neural networks.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Again, the most related work to ours is HashedNets [7], which was then extended in [6] to random hashing in frequency domain for compressing convolutional neural networks.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "Extensive studies have been made on constructing and analyzing multiple hash functions, which have shown better performances over one single hash function [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "One multi-hashing algorithm, d-random scheme [1], uses only one hash table but d hash functions, pretty similar to our settings.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "One choice alternative to d-random is the d-left algorithm proposed in [5], used for improving IP lookups.", "startOffset": 71, "endOffset": 74}, {"referenceID": 13, "context": "Hashing algorithms for natural language processing are also studied in [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "Papers [32, 34] investigated feature hashing (a.", "startOffset": 7, "endOffset": 15}, {"referenceID": 33, "context": "Papers [32, 34] investigated feature hashing (a.", "startOffset": 7, "endOffset": 15}, {"referenceID": 23, "context": "Three benchmark datasets [24] are considered here, including (1) the original MNIST hand-written digit dataset, (2) dataset BG-IMG as a variant to MNIST, and (3) binary image classification dataset CONVEX.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "In [7], the authors compared HashedNets against several DNN compression approaches, and showed HashedNets performs consistently the best, including the low-rank decomposition [11].", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [7], the authors compared HashedNets against several DNN compression approaches, and showed HashedNets performs consistently the best, including the low-rank decomposition [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "We adopt a deep convolutional structured semantic model similar to [19, 31], which is of a siamese structure to describe the semantic similarity between a query and a url title.", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "We adopt a deep convolutional structured semantic model similar to [19, 31], which is of a siamese structure to describe the semantic similarity between a query and a url title.", "startOffset": 67, "endOffset": 75}, {"referenceID": 6, "context": "The recently proposed HashedNets [7] is shown as a degenerated special case of FunHashNN.", "startOffset": 33, "endOffset": 36}], "year": 2016, "abstractText": "As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple low-cost hash functions to fetch values in the compression space, and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets [7] as a degenerated case, and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets, FunHashNN demonstrates high compression ratios with little loss on prediction accuracy.", "creator": "LaTeX with hyperref package"}}}