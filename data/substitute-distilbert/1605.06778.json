{"id": "1605.06778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2016", "title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit", "abstract": "we introduce openxbow, our open - source toolkit for the generation of bag - rank - words ( bow ) representations from multimodal input. in the bow principle, word histograms were first used as features in document classification, limiting the idea misses and can easily be adapted to, e. g., acoustic or visual low - level descriptors, incorporating a prior step of vector quantisation. hierarchical tagged toolkit supports morphological numeric input features and text object input gives computed subbags to a tensor bag. it provides a variety of extensions and options. to our knowledge, openxbow is the first unicode available toolkit for the generation of crossmodal bags - of - words. accessibility capabilities of the tags will resolved in two sample scenarios : time - continuous speech - based emotion representations and sentiment analysis in tweets where improved results over other feature representation forms were observed.", "histories": [["v1", "Sun, 22 May 2016 12:14:55 GMT  (176kb,D)", "http://arxiv.org/abs/1605.06778v1", "9 pages, 1 figure, pre-print"]], "COMMENTS": "9 pages, 1 figure, pre-print", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.IR", "authors": ["maximilian schmitt", "bj\\\"orn w schuller"], "accepted": false, "id": "1605.06778"}, "pdf": {"name": "1605.06778.pdf", "metadata": {"source": "CRF", "title": "openXBOW \u2013 Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit", "authors": ["Maximilian Schmitt", "Bj\u00f6rn W. Schuller"], "emails": ["maximilian.schmitt@uni-passau.de", "bjoern.schuller@uni-passau.de"], "sections": [{"heading": null, "text": "Keywords: bag-of-words, multimodal signal processing, histogram feature representations"}, {"heading": "1. Introduction", "text": "The bag-of-words (BoW) principle is a common practice in natural language processing (NLP) (Wo\u0308llmer et al., 2012; Weninger et al., 2013). In this method, word histograms are generated, i. e., within a text document, the frequencies of words from a dictionary are counted. The resulting word-frequency vector is then input to a classifier, such as na\u0308\u0131ve Bayes or a support vector machine (SVM), i. e., machine learning schemes which are known to cope well with possibly irrelevant features and large, yet sparse feature vectors.\nOne major drawback of the BoW approach is that the order of the words in a document, which often implies important information, is not taken into account. In order to overcome this problem, n-grams have been employed, where sequences of n words or characters (ncharacter-grams) are counted instead of single words (Wallach, 2006; Schuller et al., 2009).\nBoW has been adopted by the visual community, where it is known under the name bag-of-visual-words (BoVW) (Fei-Fei and Perona, 2005; Sivic et al., 2005). Instead of lexical words, local image features are extracted from an image and then their general distribution is modelled by a histogram according to a learnt codebook, which substitutes the dictionary from BoW.\nIn recent years, the principle has also been employed successfully in the field of audio classification, where it is known under the term bag-of-audio-words (BoAW). Acoustic lowlevel descriptors (LLDs), such as mel-frequency cepstral coefficients (MFCC), are extracted\n\u2217. B.W. Schuller is also with the Department of Computing, Imperial College London, UK.\nar X\niv :1\n60 5.\n06 77\n8v 1\n[ cs\n.C V\n] 2\n2 M\nay 2\n01 6\nfrom the audio signal, then, the LLD vectors from single frames are quantised according to a codebook (Pancoast and Akbacak, 2012). This codebook can be the result of either k-means clustering (Pokorny et al., 2015) or random sampling of LLD vectors (Rawat et al., 2013). Other approaches employ expectation maximisation (EM) clustering, which leads to a soft vector quantisation step (Grzeszick et al., 2015). A histogram finally describes the distribution of the codebook vectors over the whole audio signal or one segment. Major applications of BoAW are acoustic event detection and multimedia event detection (Liu et al., 2010; Pancoast and Akbacak, 2012; Rawat et al., 2013; Plinge et al., 2014; Lim et al., 2015) but they have also been successfully used in music information retrieval (Riley et al., 2008) and emotion recognition from speech (Pokorny et al., 2015).\nIn this contribution, we introduce the first open-source toolkit for the generation of BoW representations across modalities, thus named \u2018openXBOW\u2019 (\u201copen cross-BoW\u201d). The motivation behind openXBOW is to ease the generation of a fused BoW-based representation from different modalities. These modalities can be the audio or the visual domain, providing numeric LLDs, and written documents or transcriptions of speech, providing text. However, in principle arbitrary other types of modalities \u201cx\u201d can be thought off, such as stemming from physiological measurement or feature streams as used in brain computing, etc. As a placeholder for arbitrary modalities, we thus introduce the notion of bag-of-x-words or BoXW for short. In case of multimodal or \u2018crossmodal\u2019 usage, the output of the toolkit is a concatenated feature vector consisting of histogram representations per modality or combinations of these. Multimodal BoW have already been employed, e. g., for depression monitoring in (Joshi et al., 2013), exploiting both the audio and the video domain.\nopenXBOW provides a multitude of options, e. g., different modes of vector quantisation, codebook generation, term frequency weighting, and methods known from natural language processing to process the textual features. To the knowledge of the authors, such a toolkit has not been published, so far, whereas there are already some libraries implementing BoVW, such as DBoW2 (Ga\u0301lvez-Lo\u0301pez and Tardos, 2012).\nIn the next section, we give an overview of the openXBOW tool, its structure and its options. In section 3, we give results from two exemplary applications of the tool. We conclude and give an outlook on future developments in openXBOW in the final section 4."}, {"heading": "2. Overview", "text": "openXBOW is implemented in Java and can thus be used on any platform. It has been published on GitHub as a public repository1, including both the source code and a compiled jar file for those who do not have a Java Development Kit installed. The software and the source code can be freely used by the research community for non-commercial purposes.\nopenXBOW supports three different file formats for input of LLDs and text and the output of the BoW feature vector:\n\u2022 ARFF (Attribute-Relation File Format), used in the machine learning software Weka (Hall et al., 2009)\n\u2022 CSV (Comma separated values), with separator semicolon (;)\n1. https://github.com/openXBOW/openXBOW\n\u2022 LIBSVM file format, used in LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) (only output)\nThe input is processed by openXBOW in the way shown in figure 1. First, there is an optional preprocessing stage, where input LLDs with a low activity can be excluded from further processing. This is especially relevant in the field of speech recognition, where LLDs at instants of time without voice activity should not be considered as they describe only the background noise.\nIn case feature types with (significantly) different ranges of values are combined in one BoW, normalisation or standardisation of the LLDs is essential. Both options are available, the corresponding parameters are stored in the codebook file, so that they can be applied in the same way to a given test file (online approach).\nAll options with a corresponding help text are displayed in the command line window if openXBOW is started without any arguments or the argument (-h), e. g.,\njava -jar openXBOW.jar -h\nAll configurations are made in the command line call.\nFor codebook generation, there are four different methods available at the time; random sampling means that the codebook vectors are picked randomly from the input LLDs, whereas random sampling++ favours far-off vectors as proposed in (Arthur and Vassilvitskii, 2007) as an improved initialisation step for kmeans, called in that case kmeans++. For the latter two methods, up to 500 updates of cluster centroids and cluster assignments are executed after initialisation.\nIn case of nominal class labels, the codebook generation can also be done in a supervised manner, learning a codebook from all LLDs in one class separately, first, and then concatenating these codebooks to form a super-codebook (Grzeszick et al., 2015).\nIn case of large LLD vectors, split vector quantisation (SVQ) can be suitable, as proposed in (Pokorny et al., 2015), where subvectors are first quantised and then the indexes of the quantised subvectors are processed in the usual scheme. However, it is also possible to split the input LLDs manually into well-defined subvectors in order to generate different codebooks for different feature types.\nA way of doing soft vector quantisation without EM-clustering, is applying Gaussian encoding to the term frequencies (TF), i. e., the number of occurrences of each word from the\ncodebook (Pancoast and Akbacak, 2014). The TF is then weighted in each word assignment by the distance to the word in the codebook. This can be especially useful in combination with multiple assignments, where not only the closest word from the codebook is considered but a certain number Na of closest words.\nFor the text domain, standard processing techniques are available, such as stopping, n-grams, and n-character-grams.\nFinally, the BoW representations of different domains are fused into one feature vector. For post-processing, logarithmic TF-weighting (TFlog = lg(TF+1)), and inverse documentfrequency (IDF) weighting (TFIDF = TF \u00b7 lg NDF , N: Number of instances, DF: Number of instances where the word is present) can be applied (Riley et al., 2008).\nThe resulting histograms can then be normalised, a step which is essential if the input instances to be classified have different sizes or if (voice or any other modality\u2019s) activity detection is used and so different segments of the input signal have different numbers of assigned words."}, {"heading": "3. Experiments", "text": "In this section, the usage of openXBOW is exemplified in two scenarios: time-continuous processing of speech input for emotion recognition, and BoW processing of tweets.\nIn fact, there are comparably few suitable multimodal databases with acoustic, visual, and text input available. In this introductory paper, we exemplify the principle on separate tasks for audio and text words to showcase the principle. However, results of openXBOW with multimodal input will be shown in our future efforts."}, {"heading": "3.1 Time- and value-continuous emotion recognition from speech", "text": "Emotion recognition from speech has been conducted on the RECOLA (Remote Collaborative and Affective Interactions) corpus (Ringeval et al., 2013). In this database, 46 French participants have been recorded in dyadic remote collaboration for 5 minutes, each. During their collaboration, their face, speech and some physiological measures have been recorded. In our present example, we focus on the audio domain.\nA gold standard in terms of arousal and valence, generated from the annotations by six different persons, was used as a target in our experiments. The corpus was split into three partitions, a training partition (16 subjects) in order to learn the codebook and train the classifier, a validation partition to optimise the parameters of openXBOW and support vector regression (SVR) with a linear kernel and a test partition to prove the universality of the learnt regressor. The input LLDs (MFCCs 1\u201312 and log-energy) were extracted with our toolkit openSMILE (Eyben et al., 2013).\nTo learn a codebook and generate a BoAW representation (BoAW arousal train.arff) from the training file LLD train.csv, the following options have been applied:\n-i LLD_train.csv -o BoAW_arousal_train.arff -l arousal_train.csv -t 8.0 0.8 -standardizeInput -size 1000 -c random++ -B codebook.txt -a 20 -log\nThe option -t 8.0 0.8 means that the sequence of input LLDs is segmented into windows of 8.0 seconds width and the hop size between two successive windows is 0.8 seconds. Labels\nfor arousal in the file arousal train.csv must be available for exactly those instants in time (see option -h for information on the labels file format).\nAfter standardisation, a codebook of 1 000 words is generated by random sampling++ and stored in the file codebook.txt together with information on the parameters of standardisation (mean, and standard deviation per LLD). Each LLD is then assigned to the 20 closest (-a 20) words in the codebook. Finally, the number TFs are compressed applying logarithmic TF-weighting. Also, this information is stored in the codebook files, as it must be used in the same way with the validation and test instances. The order of the options does not have any effect.\nAfter the optimum configuration of BoAW has been found (let us assume that it is the one described above), the BoAW can be generated from the validation (and also test) files (valid) in the following way:\n-i LLD_valid.csv -o BoAW_arousal_valid.arff -l arousal_valid.csv -t 8.0 0.04 -b codebook.txt -a 20\nThe codebook is loaded with the command -b. Please note that, the file codebook.txt implies also the standardisation and the log-TF-weighting. The hop size is chosen differently for the validation partition; it is only 0.04 seconds (-t 8.0 0.04) which is not an issue if the corresponding labels are included in the labels file (arousal valid.csv).\nTable 1 shows the results of BoAW for emotion recognition compared to the baseline of the AVEC 2016 challenge at ACM Multimedia 2016 (Valstar et al., 2016), which is also carried out on RECOLA under the same conditions. However, the number of recordings in the respective training, validation and test set in the AVEC 2016 challenge were smaller than the ones we used.\nThe optimum window size for the prediction of valence is 10.0 seconds, compared to 8.0 seconds for arousal. All labels have been shifted to the front in time in order to compensate the natural delay between the shown emotion of the subject and the reaction of the annotator. A shift of 4.0 seconds prove to be an optimum. The complexity of SVR has been optimised for each configuration. As a metric for evaluation, the concordance correlation coefficient (CCC) is used, which takes also scaling of the outputs into account, compared to the linear correlation coefficient. Note that, the CCC is also used as competition measure in the AVEC 2016 competition.\nIt can be clearly seen that, our approach significantly outperforms the baseline on the validation and test sets, except for the performance for arousal on the validation partition, where a similar result is achieved."}, {"heading": "3.2 Twitter sentiment analysis", "text": "Our second experiment shows obtainable performances on written text on another welldefined task. A large corpus of short messages from the social network Twitter2, known as \u2018tweets\u2019, in English language is provided by Thinknook3. This dataset has been collected by the University of Michigan and Niek Sanders. It includes 1 578 627 tweets with a 2-class annotation of either positive or negative sentiment. An accuracy of 75 % is reported as state-of-the-art.\nIn order to train a dictionary and create a BoW representation from the training set senti-train.csv, the following command line arguments can be used:\n-i senti-train.csv -attributes ncr0 -o BoW.arff -B dictionary.txt -minTermFreq 1000 -maxTermFreq 30000 -nGram 2 -log -idf\nThe option -attributes is needed if the data structure of the input is not the standard defined in the openXBOW help text. It specifies each input feature in the input file, i. e., each column in an input CSV file. n means that, the corresponding column (the first column here) specifies the name (or an index) as a unique ID this line belongs to. All lines in the input with the same name are put into one bag later on. The second column in the Twitter dataset (c) specifies the class label. The third column (r, remove) can be discarded as it does not contain any relevant information, the last column of the input contains the text to be classified. The digit 0 always specifies text input, the digits 1 to 9 specify numeric input. A separate codebook is generated for every digit if at least one feature with a digit is present. More information about the input format can be found in the help text.\n-minTermFreq and -maxTermFreq implement stopping as known in NLP. Thereby, either very rare words or very common words (such as \u2018and\u2019, or \u2018or\u2019) can be excluded from the dictionary as/if they are not likely to carry meaningful information. 2-grams (-nGram 2) and log-TF-IDF weighting (-log -idf) are also used in this sample call.\nTo apply the same model to the test set, the following line is used:\n-i senti-test.csv -attributes ncr0 -o BoW.arff -b dictionary.txt -nGram 2\nNote that, the TF-weighting is stored in the dictionary and is thus not needed; the option -nGram 2 must, however, be repeated.\nIn our experiments, the best results have been achieved without using (higher order) n-grams, i. e., simply employing uni-grams or each word by itself, respectively. We split the whole corpus into two partitions, the first 1 000 000 instances form the training set, the remaining 578 627 instances form the test set. Support vector machine with linear kernel was again found to be suitable to handle the high-dimensional BoW vector.\nA minimum term frequency of 500 and a maximum term frequency of 100 000, leading to a dictionary size of 1 875 terms was found suitable. With an SVM complexity of 0.1,\n2. http://twitter.com 3. http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\na weighted accuracy (WA) of 77.28 % and an unweighted accuracy (UA) of 77.29 % have been achieved. These results appear to match or outperform a little bit the state-of-the-art. Even better performance might be achieved with more sophisticated approaches, such as long short-term memory recurrent neural networks (Schuller et al., 2015)."}, {"heading": "4. Conclusions and outlook", "text": "We introduced our novel openXBOW toolkit \u2013 a first of its kind \u2013 for the generation of BoW representations from multimodal symbolic (including text), but also numeric (such as audio or video feature streams) information representations. In two (monomodal) examples we showed the potential of the toolkit and the underlying BoXW principle by outperforming the state-of-the-art on a well-defined modern speech-based emotion recognition competition task. The full potential is, however, likely to be revealed once targeting actual crossmodal tasks. Likewise, it stands to reason that in the future, the performance can be improved using input from the visual domain, the physiological domain, and the transcribed speech on the emotion recognition task considered.\nWe have also shown that our toolkit already provides state-of-the-art results in text classification.\nFuture work on openXBOW will include further soft vector quantisation techniques such as using EM clustering or non-negative matrix factorisation-based soft clustering and methods taking the order of the crossmodal words into account, such as temporal augmentation (Grzeszick et al., 2015) and n-grams for numeric features (Pancoast and Akbacak, 2013). Another goal is to add a graphical user interface for configuration in order to make the work more convenient.\nThe public repository4 will be regularly updated. The authors kindly request for feedback."}, {"heading": "Acknowledgments", "text": "This work has been supported by the European Union\u2019s Horizon 2020 Programme under grant agreement No. 645094 (IA SEWA) and the European Community\u2019s Seventh Framework Programme through the ERC Starting Grant No. 338164 (iHEARu)."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In Proc. of the 18th annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Recent developments in openSMILE, the munich open-source multimedia feature extractor", "author": ["F. Eyben", "F. Weninger", "F. Gro\u00df", "B. Schuller"], "venue": "In Proc. of the 21st ACM International Conference on Multimedia,", "citeRegEx": "Eyben et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eyben et al\\.", "year": 2013}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Fei.Fei and Perona.,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei and Perona.", "year": 2005}, {"title": "Bags of binary words for fast place recognition in image sequences. Robotics", "author": ["D. G\u00e1lvez-L\u00f3pez", "J.D. Tardos"], "venue": "IEEE Transactions on,", "citeRegEx": "G\u00e1lvez.L\u00f3pez and Tardos.,? \\Q2012\\E", "shortCiteRegEx": "G\u00e1lvez.L\u00f3pez and Tardos.", "year": 2012}, {"title": "Temporal acoustic words for online acoustic event detection", "author": ["R. Grzeszick", "A. Plinge", "G.A. Fink"], "venue": "In Proc. 37th German Conf. Pattern Recognition, Aachen,", "citeRegEx": "Grzeszick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grzeszick et al\\.", "year": 2015}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Multimodal assistive technologies for depression diagnosis and monitoring", "author": ["J. Joshi", "R. Goecke", "S. Alghowinem", "A. Dhall", "M. Wagner", "J. Epps", "G. Parker", "M. Breakspear"], "venue": "Journal on MultiModal User Interfaces,", "citeRegEx": "Joshi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2013}, {"title": "Robust sound event classification using lbp-hog based bag-of-audio-words feature representation", "author": ["H. Lim", "M.J. Kim", "H. Kim"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Lim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2015}, {"title": "Coherent bag-of audio words model for efficient large-scale video copy detection", "author": ["Y. Liu", "W.-L. Zhao", "C.-W. Ngo", "C.-S. Xu", "H.-Q. Lu"], "venue": "In Proc. of the ACM International Conference on Image and Video Retrieval,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Bag-of-audio-words approach for multimedia event classification", "author": ["S. Pancoast", "M. Akbacak"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Pancoast and Akbacak.,? \\Q2012\\E", "shortCiteRegEx": "Pancoast and Akbacak.", "year": 2012}, {"title": "N-gram extension for bag-of-audio-words", "author": ["S. Pancoast", "M. Akbacak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Pancoast and Akbacak.,? \\Q2013\\E", "shortCiteRegEx": "Pancoast and Akbacak.", "year": 2013}, {"title": "Softening quantization in bag-of-audio-words", "author": ["S. Pancoast", "M. Akbacak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Pancoast and Akbacak.,? \\Q2014\\E", "shortCiteRegEx": "Pancoast and Akbacak.", "year": 2014}, {"title": "A bag-of-features approach to acoustic event detection", "author": ["A. Plinge", "R. Grzeszick", "G.A. Fink"], "venue": "In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Plinge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plinge et al\\.", "year": 2014}, {"title": "Detection of negative emotions in speech signals using bags-of-audio-words", "author": ["F. Pokorny", "F. Graf", "F. Pernkopf", "B. Schuller"], "venue": "In Proc. 1st International Workshop", "citeRegEx": "Pokorny et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pokorny et al\\.", "year": 2015}, {"title": "Robust audiocodebooks for large-scale event detection in consumer videos", "author": ["S. Rawat", "P.F. Schulam", "S. Burger", "D. Ding", "Y. Wang", "F. Metze"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Rawat et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rawat et al\\.", "year": 2013}, {"title": "A text retrieval approach to content-based audio hashing", "author": ["M. Riley", "E. Heinen", "J. Ghosh"], "venue": "In ISMIR 2008, 9th International Conference on Music Information Retrieval,", "citeRegEx": "Riley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Riley et al\\.", "year": 2008}, {"title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions", "author": ["F. Ringeval", "A. Sonderegger", "J. Sauer", "D. Lalanne"], "venue": "In Face and Gestures", "citeRegEx": "Ringeval et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ringeval et al\\.", "year": 2013}, {"title": "The Godfather\u201d vs. \u201cChaos\u201d: Comparing Linguistic Analysis based on Online Knowledge Sources and Bags-of-N-Grams for Movie Review Valence Estimation", "author": ["B. Schuller", "J. Schenk", "G. Rigoll", "T. Knaup"], "venue": "In Proceedings 10th International Conference on Document Analysis and Recognition,", "citeRegEx": "Schuller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schuller et al\\.", "year": 2009}, {"title": "Sentiment Analysis and Opinion Mining: On Optimal Parameters and Performances", "author": ["B. Schuller", "A.E.-D. Mousa", "V. Vasileios"], "venue": "WIREs Data Mining and Knowledge Discovery,", "citeRegEx": "Schuller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schuller et al\\.", "year": 2015}, {"title": "Discovering object categories in image collections", "author": ["J. Sivic", "B.C. Russell", "A.A. Efros", "A. Zisserman", "W.T. Freeman"], "venue": null, "citeRegEx": "Sivic et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2005}, {"title": "Avec 2016-depression, mood, and emotion recognition workshop and challenge", "author": ["M. Valstar", "J. Gratch", "B. Schuller", "F. Ringeval", "D. Lalanne", "M.T. Torres", "S. Scherer", "G. Stratou", "R. Cowie", "M. Pantic"], "venue": "arXiv preprint arXiv:1605.01600,", "citeRegEx": "Valstar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Valstar et al\\.", "year": 2016}, {"title": "Topic modeling: Beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Wallach.,? \\Q2006\\E", "shortCiteRegEx": "Wallach.", "year": 2006}, {"title": "Words that Fascinate the Listener: Predicting Affective Ratings of On-Line Lectures", "author": ["F. Weninger", "P. Staudt", "B. Schuller"], "venue": "International Journal of Distance Education Technologies, Special Issue on Emotional Intelligence for Online Learning,", "citeRegEx": "Weninger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weninger et al\\.", "year": 2013}, {"title": "Fully Automatic Audiovisual Emotion Recognition \u2013 Voice, Words, and the Face", "author": ["M. W\u00f6llmer", "M. Kaiser", "F. Eyben", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "Proceedings of Speech Communication; 10. ITG Symposium,", "citeRegEx": "W\u00f6llmer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "W\u00f6llmer et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "The bag-of-words (BoW) principle is a common practice in natural language processing (NLP) (W\u00f6llmer et al., 2012; Weninger et al., 2013).", "startOffset": 91, "endOffset": 136}, {"referenceID": 24, "context": "The bag-of-words (BoW) principle is a common practice in natural language processing (NLP) (W\u00f6llmer et al., 2012; Weninger et al., 2013).", "startOffset": 91, "endOffset": 136}, {"referenceID": 23, "context": "In order to overcome this problem, n-grams have been employed, where sequences of n words or characters (ncharacter-grams) are counted instead of single words (Wallach, 2006; Schuller et al., 2009).", "startOffset": 159, "endOffset": 197}, {"referenceID": 19, "context": "In order to overcome this problem, n-grams have been employed, where sequences of n words or characters (ncharacter-grams) are counted instead of single words (Wallach, 2006; Schuller et al., 2009).", "startOffset": 159, "endOffset": 197}, {"referenceID": 4, "context": "BoW has been adopted by the visual community, where it is known under the name bag-of-visual-words (BoVW) (Fei-Fei and Perona, 2005; Sivic et al., 2005).", "startOffset": 106, "endOffset": 152}, {"referenceID": 21, "context": "BoW has been adopted by the visual community, where it is known under the name bag-of-visual-words (BoVW) (Fei-Fei and Perona, 2005; Sivic et al., 2005).", "startOffset": 106, "endOffset": 152}, {"referenceID": 11, "context": "from the audio signal, then, the LLD vectors from single frames are quantised according to a codebook (Pancoast and Akbacak, 2012).", "startOffset": 102, "endOffset": 130}, {"referenceID": 15, "context": "This codebook can be the result of either k-means clustering (Pokorny et al., 2015) or random sampling of LLD vectors (Rawat et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 16, "context": ", 2015) or random sampling of LLD vectors (Rawat et al., 2013).", "startOffset": 42, "endOffset": 62}, {"referenceID": 6, "context": "Other approaches employ expectation maximisation (EM) clustering, which leads to a soft vector quantisation step (Grzeszick et al., 2015).", "startOffset": 113, "endOffset": 137}, {"referenceID": 10, "context": "Major applications of BoAW are acoustic event detection and multimedia event detection (Liu et al., 2010; Pancoast and Akbacak, 2012; Rawat et al., 2013; Plinge et al., 2014; Lim et al., 2015) but they have also been successfully used in music information retrieval (Riley et al.", "startOffset": 87, "endOffset": 192}, {"referenceID": 11, "context": "Major applications of BoAW are acoustic event detection and multimedia event detection (Liu et al., 2010; Pancoast and Akbacak, 2012; Rawat et al., 2013; Plinge et al., 2014; Lim et al., 2015) but they have also been successfully used in music information retrieval (Riley et al.", "startOffset": 87, "endOffset": 192}, {"referenceID": 16, "context": "Major applications of BoAW are acoustic event detection and multimedia event detection (Liu et al., 2010; Pancoast and Akbacak, 2012; Rawat et al., 2013; Plinge et al., 2014; Lim et al., 2015) but they have also been successfully used in music information retrieval (Riley et al.", "startOffset": 87, "endOffset": 192}, {"referenceID": 14, "context": "Major applications of BoAW are acoustic event detection and multimedia event detection (Liu et al., 2010; Pancoast and Akbacak, 2012; Rawat et al., 2013; Plinge et al., 2014; Lim et al., 2015) but they have also been successfully used in music information retrieval (Riley et al.", "startOffset": 87, "endOffset": 192}, {"referenceID": 9, "context": "Major applications of BoAW are acoustic event detection and multimedia event detection (Liu et al., 2010; Pancoast and Akbacak, 2012; Rawat et al., 2013; Plinge et al., 2014; Lim et al., 2015) but they have also been successfully used in music information retrieval (Riley et al.", "startOffset": 87, "endOffset": 192}, {"referenceID": 17, "context": ", 2015) but they have also been successfully used in music information retrieval (Riley et al., 2008) and emotion recognition from speech (Pokorny et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 15, "context": ", 2008) and emotion recognition from speech (Pokorny et al., 2015).", "startOffset": 44, "endOffset": 66}, {"referenceID": 8, "context": ", for depression monitoring in (Joshi et al., 2013), exploiting both the audio and the video domain.", "startOffset": 31, "endOffset": 51}, {"referenceID": 5, "context": "To the knowledge of the authors, such a toolkit has not been published, so far, whereas there are already some libraries implementing BoVW, such as DBoW2 (G\u00e1lvez-L\u00f3pez and Tardos, 2012).", "startOffset": 154, "endOffset": 185}, {"referenceID": 7, "context": "\u2022 ARFF (Attribute-Relation File Format), used in the machine learning software Weka (Hall et al., 2009)", "startOffset": 84, "endOffset": 103}, {"referenceID": 1, "context": "\u2022 LIBSVM file format, used in LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 3, "context": "\u2022 LIBSVM file format, used in LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) (only output)", "startOffset": 73, "endOffset": 91}, {"referenceID": 0, "context": "For codebook generation, there are four different methods available at the time; random sampling means that the codebook vectors are picked randomly from the input LLDs, whereas random sampling++ favours far-off vectors as proposed in (Arthur and Vassilvitskii, 2007) as an improved initialisation step for kmeans, called in that case kmeans++.", "startOffset": 235, "endOffset": 267}, {"referenceID": 6, "context": "In case of nominal class labels, the codebook generation can also be done in a supervised manner, learning a codebook from all LLDs in one class separately, first, and then concatenating these codebooks to form a super-codebook (Grzeszick et al., 2015).", "startOffset": 228, "endOffset": 252}, {"referenceID": 15, "context": "In case of large LLD vectors, split vector quantisation (SVQ) can be suitable, as proposed in (Pokorny et al., 2015), where subvectors are first quantised and then the indexes of the quantised subvectors are processed in the usual scheme.", "startOffset": 94, "endOffset": 116}, {"referenceID": 13, "context": "codebook (Pancoast and Akbacak, 2014).", "startOffset": 9, "endOffset": 37}, {"referenceID": 17, "context": "For post-processing, logarithmic TF-weighting (TFlog = lg(TF+1)), and inverse documentfrequency (IDF) weighting (TFIDF = TF \u00b7 lg N DF , N: Number of instances, DF: Number of instances where the word is present) can be applied (Riley et al., 2008).", "startOffset": 226, "endOffset": 246}, {"referenceID": 18, "context": "Emotion recognition from speech has been conducted on the RECOLA (Remote Collaborative and Affective Interactions) corpus (Ringeval et al., 2013).", "startOffset": 122, "endOffset": 145}, {"referenceID": 2, "context": "The input LLDs (MFCCs 1\u201312 and log-energy) were extracted with our toolkit openSMILE (Eyben et al., 2013).", "startOffset": 85, "endOffset": 105}, {"referenceID": 22, "context": "Table 1 shows the results of BoAW for emotion recognition compared to the baseline of the AVEC 2016 challenge at ACM Multimedia 2016 (Valstar et al., 2016), which is also carried out on RECOLA under the same conditions.", "startOffset": 133, "endOffset": 155}, {"referenceID": 20, "context": "Even better performance might be achieved with more sophisticated approaches, such as long short-term memory recurrent neural networks (Schuller et al., 2015).", "startOffset": 135, "endOffset": 158}, {"referenceID": 6, "context": "Future work on openXBOW will include further soft vector quantisation techniques such as using EM clustering or non-negative matrix factorisation-based soft clustering and methods taking the order of the crossmodal words into account, such as temporal augmentation (Grzeszick et al., 2015) and n-grams for numeric features (Pancoast and Akbacak, 2013).", "startOffset": 265, "endOffset": 289}, {"referenceID": 12, "context": ", 2015) and n-grams for numeric features (Pancoast and Akbacak, 2013).", "startOffset": 41, "endOffset": 69}], "year": 2016, "abstractText": "We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e. g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed.", "creator": "LaTeX with hyperref package"}}}