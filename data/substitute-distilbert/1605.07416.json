{"id": "1605.07416", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Refined Lower Bounds for Adversarial Bandits", "abstract": "we provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. the new results show such recent upper bounds that assign ( to ) hold with high - probability or ( b ) depend on the true lossof the best arm & ( c ) depend within the quadratic variation of the losses, are close to tight. besides this we prove two impossibility results. first, the existence containing a single arm that is optimal in every round cannot improve the regret in the smooth case. final, simultaneous regret cannot scale with the effective component of the wins. besides contrast, both results are possible in the full - information setting.", "histories": [["v1", "Tue, 24 May 2016 12:36:47 GMT  (22kb)", "http://arxiv.org/abs/1605.07416v1", null], ["v2", "Mon, 27 Feb 2017 13:48:10 GMT  (23kb)", "http://arxiv.org/abs/1605.07416v2", null]], "reviews": [], "SUBJECTS": "math.ST cs.LG stat.ML stat.TH", "authors": ["s\u00e9bastien gerchinovitz", "tor lattimore"], "accepted": true, "id": "1605.07416"}, "pdf": {"name": "1605.07416.pdf", "metadata": {"source": "CRF", "title": "Refined Lower Bounds for Adversarial Bandits", "authors": ["S\u00e9bastien Gerchinovitz"], "emails": ["sebastien.gerchinovitz@math.univ-toulouse.fr", "tor.lattimore@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n07 41\n6v 1\n[ m\nat h.\nST ]\n2 4\nM ay"}, {"heading": "1 Introduction", "text": "We consider the standard K-armed adversarial bandit problem, which is a game played over T rounds between a learner and an adversary. In every round t \u2208 {1, . . . , T } the learner chooses a probability distribution pt = (pi,t)16i6K over {1, . . . ,K}. The adversary then chooses a loss vector \u2113t = (\u2113i,t)16i6K \u2208 [0, 1]K , which may depend on pt. Finally the learner samples an action from pt denoted by It \u2208 {1, . . . ,K} and observes her own loss \u2113It,t. The learner would like to minimise her regret, which is the difference between cumulative loss suffered and the loss suffered by the optimal action in hindsight.\nRT (\u21131:T ) =\nT\u2211\nt=1\n\u2113It,t \u2212 min 16i6K\nT\u2211\nt=1\n\u2113i,t ,\nwhere \u21131:T \u2208 [0, 1]TK is the set of losses chosen by the adversary. A famous strategy is called Exp3, which satisfies E[RT (\u21131:T )] = O( \u221a\nKT log(K))) where the expectation is taken over the randomness in the algorithm and the choices of the adversary [Auer et al., 2002]. There is also a lower bound showing that for every learner there is an adversary for which the expected regret is E[RT (\u21131:T )] = \u2126( \u221a KT ) [Auer et al., 1995]. If the losses are chosen ahead of time, then the adver-\nsary is called oblivious, and in this case there exists a learner for which E[RT (\u21131:T )] = O( \u221a KT ) [Audibert and Bubeck, 2009]. One might think that this is the end of the story, but it is not so. While the worst-case expected regret is one quantity of interest, there are many situations where a refined regret guarantee is more informative. Recent research on adversarial bandits has primarily focussed on these issues, especially the questions of obtaining regret guarantees that hold with high probability as well as stronger guarantees when the losses are \u201cnice\u201d in some sense. While there are now a wide range of strategies with upper bounds that depend on various quantities, the literature is missing lower bounds for many cases, some of which we now provide.\nWe focus on three classes of lower bound, which are described in detail below. The first addresses the optimal regret achievable with high probability, where we show there is little room for improvement over existing strategies. Our other results concern lower bounds that depend on some kind of regularity in the losses (\u201cnice\u201d data). Specifically we prove lower bounds that replace T in the regret bound with the loss of the best action (called first-order bounds) and also with the quadratic variation of the losses (called second-order bounds).\nHigh probability bounds Existing strategies Exp3.P [Auer et al., 2002] and Exp-IX [Neu, 2015] are tuned with a confidence parameter \u03b4 \u2208 (0, 1) and satisfy for some universal constant c > 0\nP\n(\nRT (\u21131:T ) > c \u221a KT log(K/\u03b4) ) 6 \u03b4 . (1)\nAn alternative tuning of Exp-IX or Exp3.P [Bubeck and Cesa-Bianchi, 2012] leads to a single algorithm for which\n\u2200\u03b4 \u2208 (0, 1) P ( RT (\u21131:T ) > c \u221a KT ( \u221a log(K) + log(1/\u03b4) \u221a\nlog(K)\n))\n6 \u03b4 . (2)\nThe difference is that in (1) the algorithm depends on \u03b4 while in (2) it does not. The cost of not knowing \u03b4 is that the log(1/\u03b4) moves outside the square root. In Section 2 we prove two lower bounds showing that there is little room for improvement in either (1) or (2).\nFirst-order bounds An improvement over the worst-case regret bound of O( \u221a TK) is the socalled improvement for small losses. Specifically, there exist strategies (eg., FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al. [2006], Rakhlin and Sridharan [2013]) such that for all \u21131:T \u2208 [0, 1]KT\nE[RT (\u21131:T )] 6 O (\u221a L\u2217TK log(K) +K log(KT ) )\n, with L\u2217T = min 16i6K\nT\u2211\nt=1\n\u2113i,t , (3)\nwhere the expectation is with respect to the internal randomisation of the algorithm (the losses are fixed). This result improves on the O( \u221a KT ) bounds since L\u2217T 6 T is always guaranteed and sometimes L\u2217T is much smaller than T . In order to evaluate the optimality of this bound, we first rewrite it in terms of the small-loss balls B\u03b1,T defined for all \u03b1 \u2208 [0, 1] and T > 1 by\nB\u03b1,T , { \u21131:T \u2208 [0, 1]KT : L\u2217T T 6 \u03b1 } . (4)\nCorollary 1. The first-order regret bound (3) of Neu [2015] is equivalent to:\n\u2200\u03b1 \u2208 [0, 1], sup \u21131:T\u2208B\u03b1,T\nE[RT (\u21131:T )] 6 O (\u221a \u03b1TK log(K) +K log(KT ) ) .\nThe proof is straightforward. Our main contribution in Section 3 is a lower bound of the order of\u221a \u03b1TK for all \u03b1 \u2208 \u2126(log(T )/T ). This minimax lower bound shows that we cannot hope for a better bound than (3) (up to log factors) if we only know the value of L\u2217T .\nSecond-order bounds Another type of improved regret bound was derived by Hazan and Kale [2011b] and involves a second-order quantity called the quadratic variation.\nQT =\nT\u2211\nt=1\n\u2016\u2113t \u2212 \u00b5T \u201622 6 TK\n4 , (5)\nwhere \u00b5T = 1T \u2211T\nt=1 \u2113t is the mean of all loss vectors. (In other words, QT /T is the sum of the empirical variances of all the K arms). Hazan and Kale [2011b] addressed the general online linear optimisation setting. In the particular case of adversarial K-armed bandits with an oblivious adversary (as is the case here), they showed that there exists an efficient algorithm such that for some absolute constant c > 0 and for all T > 2\n\u2200\u21131:T \u2208 [0, 1]KT , E[RT (\u21131:T )] 6 c ( K2 \u221a QT logT +K 1.5 log2 T +K2.5 logT ) . (6)\nAs before we can rewrite the regret bound (6) in terms of the small-variation balls V\u03b1,T defined for all \u03b1 \u2208 [0, 1/4] and T > 1 by\nV\u03b1,T , {\n\u21131:T \u2208 [0, 1]KT : QT TK 6 \u03b1\n}\n. (7)\nCorollary 2. The second-order regret bound (6) of Hazan and Kale [2011b] is equivalent to:\n\u2200\u03b1 \u2208 [0, 1/4], sup \u21131:T\u2208V\u03b1,T\nE[RT (\u21131:T )] 6 c ( K2 \u221a \u03b1TK logT +K3/2 log2 T +K5/2 log T ) .\nThe proof is straightforward because the losses are deterministic and fixed in advance by an oblivious adversary. In Section 4 we provide a lower bound of order \u221a \u03b1TK that holds whenever \u03b1 = \u2126(log(T )/T ). This minimax lower bound shows that we cannot hope for a bound better than (7) by more than a factor of K2 \u221a logT if we only know the value of QT . Closing the gap is left as an open question.\nTwo impossibility results in the bandit setting We also show in Section 4 that, in contrast to the full-information setting, regret bounds involving the cumulative variance of the algorithm as in [Cesa-Bianchi et al., 2007] cannot be obtained in the bandit setting. More precisely, we prove that two consequences that hold true in the full-information case, namely: (i) a regret bound proportional to the effective range of the losses and (ii) a bounded regret if one arm performs best at all rounds, must fail in the worst case for every bandit algorithm.\nAdditional notation and key tools Before the theorems we develop some additional notation and describe the generic ideas in the proofs. For 1 6 i 6 K let Ni(t) be the number of times action i has been chosen after round t. All our lower bounds are derived by analysing the regret incurred by strategies when facing randomised adversaries that choose the losses for all actions from the same joint distribution in every round (sometimes independently for each action and sometimes not). Ber(\u03b1) denotes the Bernoulli distribution with parameter \u03b1 \u2208 [0, 1]. If P and Q are measures on the same probability space, then KL(P,Q) is the KL-divergence between them. For a < b we define clip[a,b](x) = min {b,max {a, x}} and for x, y \u2208 R we let x \u2228 y = max{x, y}. Our main tools throughout the analysis are the following information-theoretic lemmas. The first bounds the KL divergence between the laws of the observed losses/actions for two distributions on the losses.\nLemma 1. Fix a randomised bandit algorithm and two probability distributions Q1 and Q2 on [0, 1]K . Assume the loss vectors \u21131, . . . , \u2113T \u2208 [0, 1]K are drawn i.i.d. from either Q1 or Q2, and denote by Qj the joint probability distribution on all sources of randomness when Qj is used (formally, Qj = Pint \u2297 (Q\u2297Tj ), where Pint is the probability distribution used by the algorithm for its internal randomisation). Let t > 1. Denote by ht = (Is, \u2113Is,s)16s6t\u22121 the history available at the beginning of round t, by Q(ht,It)j the law of (ht, It) under Qj , and by Qj,i the ith marginal distribution of Qj . Then,\nKL (\nQ (ht,It) 1 ,Q (ht,It) 2\n)\n=\nK\u2211\ni=1\nEQ1 [ Ni(t\u2212 1) ] KL ( Q1,i, Q2,i ) .\nResults of roughly this form are well known and the proof follows immediately from the chain rule for the relative entropy and the independence of the loss vectors across time (see [Auer et al., 2002] or Appendix A). One difference is that the losses need not be independent across the arms, which we heavily exploit in our proofs by using correlated losses. The second key lemma is an alternative to Pinsker\u2019s inequality that proves useful when the Kullback-Leibler divergence is larger than 2. It has previously been used for bandit lower bounds (in the stochastic setting) by Bubeck et al. [2013].\nLemma 2 (Lemma 2.6 in Tsybakov 2008). Let P and Q be two probability distributions on the same measurable space. Then, for every measurable subset A (whose complement we denote by Ac),\nP (A) +Q(Ac) > 1\n2 exp\n( \u2212KL(P,Q) ) ."}, {"heading": "2 Zero-Order High Probability Lower Bounds", "text": "We prove two new high-probability lower bounds on the regret of any bandit algorithm. The first shows that no strategy can enjoy smaller regret than \u2126( \u221a\nKT log(1/\u03b4)) with probability at least 1\u2212 \u03b4. Upper bounds of this form have been shown for various algorithms including Exp.3P [Auer et al., 2002] and Exp3-IX [Neu, 2015]. Although this result is not very surprising, we are not aware of any existing work on this problem and the proof is less straightforward than one might expect. An added\nbenefit of our result is that the loss sequences producing large regret have two special properties. First, the optimal arm is the same in every round and second the range of the losses in each round is O( \u221a K log(1/\u03b4)/T ). These properties will be useful in subsequent analysis. In the second lower bound we show that any algorithm for which E[RT (\u21131:T )] = O( \u221a KT ) must\nnecessarily suffer a high probability regret of at least \u2126( \u221a KT log(1/\u03b4)) for some sequence \u21131:T . The important difference relative to the previous result is that strategies with log(1/\u03b4) appearing inside the square root depend on a specific value of \u03b4, which must be known in advance. Theorem 1. Suppose K > 2 and \u03b4 \u2208 (0, 1/4) and T > 32(K \u2212 1) log(2/\u03b4), then there exists a sequence of losses \u21131:T \u2208 [0, 1]KT such that\nP\n(\nRT (\u21131:T ) > 1\n27\n\u221a (K \u2212 1)T log(1/(4\u03b4)) ) > \u03b4/2 ,\nwhere the probability is taken with respect to the randomness in the algorithm. Furthermore \u21131:T can be chosen in such a way that there exists an i such that for all t it holds that \u2113i,t = minj \u2113j,t and maxj,k{\u2113j,t \u2212 \u2113k,t} 6 \u221a (K \u2212 1) log(1/(4\u03b4))/T/(4 \u221a log 2).\nTheorem 2. Suppose K > 2, T > 1, and there exists a strategy and constant C > 0 such that for any \u21131:T \u2208 [0, 1]KT it holds that E[RT (\u21131:T )] 6 C \u221a\n(K \u2212 1)T . Let \u03b4 \u2208 (0, 1/4) satisfy \u221a\n(K \u2212 1)/T log(1/(4\u03b4)) 6 C and T > 32 log(2/\u03b4). Then there exists \u21131:T \u2208 [0, 1]KT for which\nP\n(\nRT (\u21131:T ) >\n\u221a\n(K \u2212 1)T log(1/(4\u03b4)) 203C\n)\n> \u03b4/2 ,\nwhere the probability is taken with respect to the randomness in the algorithm.\nCorollary 3. If p \u2208 (0, 1) and C > 0, then there does not exist a strategy such that for all T , K , \u21131:T \u2208 [0, 1]KT and \u03b4 \u2208 (0, 1) the regret is bounded by P ( RT (\u21131:T ) > C \u221a (K \u2212 1)T logp(1/\u03b4) ) 6 \u03b4.\nThe corollary follows easily by integrating the assumed high-probability bound and applying Theorem 2 for sufficiently large T and small \u03b4. The proof may be found in Appendix E.\nProof of Theorems 1 and 2 Both proofs rely on a carefully selected choice of correlated stochastic losses described below. Let Z1, Z2, . . . , ZT be a sequence of i.i.d. Gaussian random variables with mean 1/2 and variance \u03c32 = 1/(32 log(2)). Let \u2206 \u2208 [0, 1/30] be a constant that will be chosen differently in each proof and define K random loss sequences \u211311:T , . . . , \u2113 K 1:T where\n\u2113ji,t =\n \n clip[0,1](Zt \u2212\u2206) if i = 1 clip[0,1](Zt \u2212 2\u2206) if i = j 6= 1 clip[0,1](Zt) otherwise .\nFor 1 6 j 6 K let Qj be the measure on \u21131:T \u2208 [0, 1]KT and I1, . . . , IT when \u2113i,t = \u2113ji,t for all 1 6 i 6 K and 1 6 t 6 T . Informally, Qj is the measure on the sequence of loss vectors and actions when the learner interacts with the losses sampled from the jth environment defined above. Lemma 3. Let \u03b4 \u2208 (0, 1) and suppose \u2206 6 1/30 and T > 32 log(2/\u03b4). Then Qi ( RT (\u2113 i 1:T ) > \u2206T/4 ) > Qi (Ni(T ) 6 T/2)\u2212 \u03b4/2 and EQi [RT (\u2113i1:T )] > 7\u2206EQi [T \u2212Ni(T )]/8.\nThe proof may be found in Appendix D.\nProof of Theorem 1. First we choose the value of \u2206 that determines the gaps in the losses by \u2206 = \u221a\n\u03c32(K \u2212 1) log(1/(4\u03b4))/(2T ) 6 1/30. By the pigeonhole principle there exists an i > 1 for which EQ1 [Ni(T )] 6 T/(K\u22121). Therefore by Lemmas 2 and 1, and the fact that the KL divergence between clipped Gaussian distributions is always smaller than without clipping (see Lemma 7 in Appendix B),\nQ1 (N1(T ) 6 T/2) +Qi (N1(T ) > T/2) > 1\n2 exp\n( \u2212KL (\nQ (hT ,IT ) 1 ,Q (hT ,IT ) i\n))\n> 1\n2 exp\n(\n\u2212EQ1 [Ni(T )](2\u2206) 2\n2\u03c32\n)\n> 1\n2 exp\n(\n\u2212 2T\u2206 2\n\u03c32(K \u2212 1)\n)\n= 2\u03b4 .\nBut by Lemma 3\nmax k\u2208{1,i}\nQk ( RT (\u2113 k 1:T ) > T\u2206/4 ) > max {Q1 (N1(T ) 6 T/2) , Qi (Ni(T ) 6 T/2)} \u2212 \u03b4/2\n> 1\n2 (Q1 (N1(T ) 6 T/2) +Qi (N1(T ) > T/2))\u2212 \u03b4/2 > \u03b4/2 .\nTherefore there exists an i \u2208 {1, . . . ,K} such that\nQi\n(\nRT (\u2113 i 1:T ) >\n\u221a\n\u03c32T (K \u2212 1) 32 log\n( 1\n4\u03b4\n))\n= Qi ( RT (\u2113 i 1:T ) > T\u2206/4 ) > \u03b4/2 .\nThe result is completed by substituting the value of \u03c32 = 1/(32 log(2)) and by noting that maxj,k{\u2113j,t \u2212 \u2113k,t} 6 2\u2206 6 \u221a (K \u2212 1) log(1/(4\u03b4))/T/(4 \u221a log 2) Qi-almost surely.\nProof of Theorem 2. By the assumption on \u03b4 we have \u2206 = 7\u03c3 2\n16C \u221a K\u22121 T log ( 1 4\u03b4 ) 6 1/30. Suppose\nfor all i > 1 that\nEQ1 [Ni(T )] > \u03c32\n2\u22062 log\n( 1\n4\u03b4\n)\n. (8)\nThen by the assumption in the theorem statement and the second part of Lemma 3 we have\nC \u221a (K \u2212 1)T > EQ1 [RT (\u211311:T )] > 7\u2206\n8 EQ1\n[ K\u2211\ni=2\nNi(T )\n]\n> 7\u03c32(K \u2212 1)\n16\u2206 log\n1\n4\u03b4 = C\n\u221a\n(K \u2212 1)T ,\nwhich is a contradiction. Therefore there exists an i > 1 for which Eq. (8) does not hold. Then by the same argument as the previous proof it follows that\nmax k\u2208{1,i} Qk\n(\nRT (\u2113 k 1:T ) >\n7\u03c32\n4 \u00b7 16C \u221a (K \u2212 1)T log 1 4\u03b4\n)\n= max k\u2208{1,i}\nQk ( RT (\u2113 k 1:T ) > T\u2206/4 ) > \u03b4/2 .\nThe result is completed by substituting the value of \u03c32 = 1/(32 log(2))."}, {"heading": "3 First-Order Lower Bound", "text": "First-order upper bounds provide improvement over minimax bounds when the loss of the optimal action is small. Recall from Corollary 1 that first-order bounds can be rewritten in terms of the smallloss balls B\u03b1,T defined in (4). Theorem 3 below provides a new lower bound of order \u221a L\u2217TK, which matches the best existing upper bounds up to logarithmic factors. As is standard for minimax results this does not imply a lower bound on every loss sequence \u21131:T . Instead it shows that we cannot hope for a better bound if we only know the value of L\u2217T . Theorem 3. Let K > 2, T > K \u2228 118, and \u03b1 \u2208 [(c log(32T ) \u2228 (K/2))/T, 1/2], where c = 64/9. Then for any randomised bandit algorithm sup\u21131:T\u2208B\u03b1,T E[RT (\u21131:T )] > \u221a \u03b1TK/27, where the expectation is taken with respect to the internal randomisation of the algorithm.\nOur proof is inspired by that of Auer et al. [2002, Theorem 5.1]. The key difference is that we take Bernoulli distributions with parameter close to \u03b1 instead of 1/2. This way the best cumulative loss L\u2217T is ensured to be concentrated around \u03b1T , and the regret lower bound \u221a \u03b1TK \u2248 \u221a\n\u03b1(1 \u2212 \u03b1)TK can be seen to involve the variance \u03b1(1 \u2212 \u03b1)T of the binomial distribution with parameters \u03b1 and T .\nFirst we state the stochastic construction of the losses and prove a general lemma that allows us to prove Theorem 3 and will also be useful in Section 4 to a derive a lower bound in terms of the quadratic variation. Let \u03b5 \u2208 [0, 1 \u2212 \u03b1] be fixed and define K probability distributions (Qj)Kj=1 on [0, 1]KT such that under Qj the following hold:\n\u2022 All random losses \u2113i,t for 1 6 i 6 K and 1 6 t 6 T are independent. \u2022 \u2113i,t is sampled from a Bernoulli distribution with parameter\u03b1+\u03b5 if i 6= j, or with parameter \u03b1 if i = j.\nLemma 4. Let \u03b1 \u2208 (0, 1), K > 2, and T > K/(4(1\u2212\u03b1)). Consider the probability distributionsQj on [0, 1]KT defined above with \u03b5 = (1/2) \u221a\n\u03b1(1\u2212 \u03b1)K/T , and set Q\u0304 = 1K \u2211K\nj=1 Qj . Then for any randomised bandit algorithm E[RT (\u21131:T )] > \u221a\n\u03b1(1 \u2212 \u03b1)TK/8, where the expectation is with respect to both the internal randomisation of the algorithm and the random loss sequence \u21131:T which is drawn from Q\u0304.\nThe assumption T > K/(4(1\u2212 \u03b1)) above ensures that \u03b5 6 1\u2212 \u03b1, so that the Qj are well defined.\nProof of Lemma 4. We lower bound the regret by the pseudo-regret for each distribution Qj :\nEQj\n[ T\u2211\nt=1\n\u2113It,t \u2212 min 16i6K\nT\u2211\nt=1\n\u2113i,t\n]\n> EQj\n[ T\u2211\nt=1\n\u2113It,t\n]\n\u2212 min 16i6K EQj\n[ T\u2211\nt=1\n\u2113i,t\n]\n=\nT\u2211\nt=1\nEQj [ \u03b1+ \u03b5\u2212 \u03b51{It=j} ] \u2212 T\u03b1 = T\u03b5\n(\n1\u2212 1 T\nT\u2211\nt=1\nQj(It = j)\n)\n, (9)\nwhere the first equality follows because EQj [\u2113It,t] = EQj [EQj [\u2113It,t|\u21131:t\u22121, It]] = EQj [\u03b1 + \u03b5 \u2212 \u03b51{It=j}] since under Qj , the conditional distribution of \u2113t given (\u21131:t\u22121, It) is simply \u2297Ki=1B(\u03b1 + \u03b5 \u2212 \u03b51{i=j}). To bound (9) from below, note that by Pinsker\u2019s inequality we have for all t \u2208 {1, . . . , T } and j \u2208 {1, . . . ,K}, Qj(It = j) 6 Q0(It = j) + (KL(QIt0 ,QItj )/2)1/2, where Q0 = Ber(\u03b1)\u2297KT is the joint probability distribution that makes all the \u2113i,t i.i.d. Ber(\u03b1), and Q It 0 and Q It j denote the laws of It under Q0 and Qj respectively. Plugging the last inequality above into (9), averaging over j = 1, . . . ,K and using the concavity of the square root yields\nEQ\u0304\n[ T\u2211\nt=1\n\u2113It,t \u2212 min 16i6K\nT\u2211\nt=1\n\u2113i,t\n]\n> T\u03b5\n\n1\u2212 1 K \u2212\n\u221a \u221a \u221a \u221a 1\n2T\nT\u2211\nt=1\n1\nK\nK\u2211\nj=1\nKL ( Q\nIt 0 ,Q It j\n)\n\n , (10)\nwhere we recall that Q\u0304 = 1K \u2211K\nj=1 Qj . The rest of the proof is devoted to upper-bounding KL(QIt0 ,Q It j ). Denote by ht = (Is, \u2113Is,s)16s6t\u22121 the history available at the beginning of round t. From Lemma 1\nKL (\nQ It 0 ,Q It j\n) 6 KL (\nQ (ht,It) 0 ,Q (ht,It) j\n)\n= EQ0 [ Nj(t\u2212 1) ] KL ( B(\u03b1+ \u03b5),B(\u03b1) )\n6 EQ0 [ Nj(t\u2212 1)\n] \u03b52\n\u03b1(1\u2212 \u03b1) , (11)\nwhere the last inequality follows by upper bounding the KL divergence by the \u03c72 divergence (see Appendix B). Averaging (11) over j \u2208 {1, . . . ,K} and t \u2208 {1, . . . , T } and noting that \u2211Tt=1(t \u2212 1) 6 T 2/2 we get\n1\nT\nT\u2211\nt=1\n1\nK\nK\u2211\nj=1\nKL ( Q\nIt 0 ,Q It j\n) 6 1\nT\nT\u2211\nt=1\n(t\u2212 1)\u03b52 K\u03b1(1\u2212 \u03b1) 6\nT\u03b52\n2K\u03b1(1\u2212 \u03b1) .\nPlugging the above inequality into (10) and using the definition of \u03b5 = (1/2) \u221a \u03b1(1\u2212 \u03b1)K/T yields\nEQ\u0304\n[ T\u2211\nt=1\n\u2113It,t \u2212 min 16i6K\nT\u2211\nt=1\n\u2113i,t\n]\n> T\u03b5\n(\n1\u2212 1 K \u2212 1 4\n)\n> 1\n8\n\u221a\n\u03b1(1\u2212 \u03b1)TK .\nProof of Theorem 3. We show that there exists a loss sequence \u21131:T \u2208 [0, 1]KT such that L\u2217T 6 \u03b1T and E[RT (\u21131:T )] > (1/27) \u221a \u03b1TK. Lemma 4 above provides such kind of lower bound, but without the guarantee on L\u2217T . For this purpose we will use Lemma 4 with a smaller value of \u03b1 (namely, \u03b1/2) and combine it with Bernstein\u2019s inequality to prove that L\u2217T 6 T\u03b1 with high probability.\nPart 1: Applying Lemma 4 with \u03b1/2 (note that T > K > K/(4(1\u2212\u03b1)) by assumption on T and \u03b1) and noting that maxj EQj [RT (\u21131:T )] > EQ\u0304[RT (\u21131:T )] we get that for some j \u2208 {1, . . . ,K} the probability distribution Qj defined with \u03b5 = (1/2) \u221a (\u03b1/2)(1\u2212 \u03b1/2)K/T satisfies\nEQj [RT (\u21131:T )] > 1\n8\n\u221a \u03b1\n2\n(\n1\u2212 \u03b1 2\n)\nTK > 1\n32\n\u221a 6\u03b1TK (12)\nsince \u03b1 6 1/2 by assumption.\nPart 2: Next we prove that Qj(L \u2217 T > T\u03b1) 6\n1\n32T . (13)\nTo this end, first note that L\u2217T 6 \u2211T\nt=1 \u2113j,t. Second, note that under Qj , the \u2113j,t, t > 1, are i.i.d. Ber(\u03b1/2). We can thus use Bernstein\u2019s inequality: applying Theorem 2.10 (and a remark on p.38) of Boucheron et al. [2013] with Xt = \u2113j,t \u2212 \u03b1/2 6 1 = b, with v = T (\u03b1/2)(1 \u2212 \u03b1/2), and with c = b/3 = 1/3), we get that, for all \u03b4 \u2208 (0, 1), with Qj-probability at least 1\u2212 \u03b4,\nL\u2217T 6\nT\u2211\nt=1\n\u2113j,t 6 T\u03b1\n2 +\n\u221a\n2T \u03b1\n2\n(\n1\u2212 \u03b1 2\n)\nlog 1\n\u03b4 +\n1 3 log 1 \u03b4\n6 T\u03b1\n2 +\n(\n1 + 1\n3\n)\u221a\nT\u03b1 log 1\n\u03b4 6\nT\u03b1\n2 +\nT\u03b1\n2 = T\u03b1 , (14)\nwhere the second last inequality is true whenever T\u03b1 > log(1/\u03b4) and that last is true whenever T\u03b1 > (8/3)2 log(1/\u03b4) = c log(1/\u03b4). By assumption on \u03b1, these two conditions are satisfied for \u03b4 = 1/(32T ), which concludes the proof of (13).\nConclusion: We show by contradiction that there exists a loss sequence \u21131:T \u2208 [0, 1]KT such that L\u2217T 6 \u03b1T and\nE[RT (\u21131:T )] > 1\n64\n\u221a 6\u03b1TK , (15)\nwhere the expectation is with respect to the internal randomisation of the algorithm. Imagine for a second that (15) were false for every loss sequence \u21131:T \u2208 [0, 1]KT satisfying L\u2217T 6 \u03b1T . Then we would have 1{L\u2217\nT 6\u03b1T}EQj [RT (\u21131:T )|\u21131:T ] 6 (1/64)\n\u221a 6\u03b1TK almost surely (since the internal\nsource of randomness of the bandit algorithm is independent of \u21131:T ). Therefore by the tower rule for the first expectation on the r.h.s. below, we would get\nEQj [RT (\u21131:T )] = EQj\n[\nRT (\u21131:T )1{L\u2217 T 6\u03b1T}\n]\n+ EQj\n[\nRT (\u21131:T )1{L\u2217 T >\u03b1T}\n]\n6 1\n64\n\u221a 6\u03b1TK + T \u00b7Qj(L\u2217T > T\u03b1) 6 1\n64\n\u221a 6\u03b1TK + 1\n32 <\n1\n32\n\u221a 6\u03b1TK (16)\nwhere (16) follows from (13) and by noting that 1/32 < (1/64) \u221a 6\u03b1TK since \u03b1 > K/(2T ) > 4/(6T ) > 4/(6TK). Comparing (16) and (12) we get a contradiction, which proves that there exists a loss sequence \u21131:T \u2208 [0, 1]KT satisfying both L\u2217T 6 \u03b1T and (15). We conclude the proof by noting that \u221a 6/64 > 1/27. Finally, the condition T > K \u2228 118 is sufficient to make the interval[\n(c log(32T ) \u2228 (K/2))/(T ), 12 ] non empty."}, {"heading": "4 Second-Order Lower Bounds", "text": "We start by giving a lower bound on the regret in terms of the quadratic variation that is close to existing upper bounds except in the dependence on the number of arms. Afterwards we prove that bandit strategies cannot adapt to losses that lie in a small range or the existence of an action that is always optimal.\nLower bound in terms of quadratic variation We prove a lower bound of \u2126( \u221a \u03b1TK) over any small-variation ball V\u03b1,T (as defined by (7)) for all \u03b1 = \u2126(log(T )/T ). This minimax lower bound matches the upper bound of Corollary 2 up to a multiplicative factor of K2 \u221a\nlog(T ). Closing this gap is left as an open question, but we conjecture that the upper bound is loose (see also the COLT open problem by Hazan and Kale [2011a]). Theorem 4. Let K > 2, T > (32K) \u2228 601, and \u03b1 \u2208 [(2c1 log(T ) \u2228 8K)/T, 1/4], where c1 = (4/9)2(3 \u221a 5 + 1)2 6 12. Then for any randomised bandit algorithm,\nsup\u21131:T\u2208V\u03b1,T E[RT (\u21131:T )] > \u221a \u03b1TK/25, where the expectation is taken with respect to the internal randomisation of the algorithm.\nThe proof is very similar to that of Theorem 3; it also follows from Lemma 4 and Bernstein\u2019s inequality. It is postponed to Appendix C.\nImpossibility results In the full-information setting (where the entire loss vector is observed after each round) Cesa-Bianchi et al. [2007, Theorem 6] designed a carefully tuned exponential weighting algorithm for which the regret depends on the variation of the algorithm and the range of the losses:\n\u2200\u21131:T \u2208 RKT , E[RT (\u21131:T )] 6 4 \u221a VT logK + 4ET logK + 6ET , (17)\nwhere the expectation is taken with respect to the internal randomisation of the algorithm and ET = max16t6T max16i,j6K |\u2113i,t \u2212 \u2113j,t| denotes the effective range of the losses and VT = \u2211T t=1 VarIt\u223cpt(\u2113It,t) denotes the cumulative variance of the algorithm (in each round t the expert\u2019s action It is drawn at random from the weight vector pt). The bound in (17) is not closedform because VT depends on the algorithm, but has several interesting consequences:\n1. If for all t the losses \u2113i,t lie in an unknown interval [at, at + \u03c1] with a small width \u03c1 > 0, then VarIt\u223cpt(\u2113It,t) 6 \u03c1 2/4, so that VT 6 T\u03c12/4. Hence\nE[RT (\u21131:T )] 6 2\u03c1 \u221a T logK + 4\u03c1 logK + 6\u03c1 .\nTherefore, though the algorithm by Cesa-Bianchi et al. [2007, Section 4.2] does not use the prior knowledge of at or \u03c1, it is able to incur a regret that scales linearly in the effective range \u03c1. 2. If all the losses \u2113i,t are nonnegative, then by Corollary 3 of [Cesa-Bianchi et al., 2007] the second-order bound (17) implies the first-order bound\nE[RT (\u21131:T )] 6 4\n\u221a\nL\u2217T\n(\nMT \u2212 L\u2217T T\n)\nlogK + 39MT max{1, logK} , (18)\nwhere MT = max16t6T max16i6K \u2113i,t . 3. If there exists an arm i\u2217 that is optimal at every round t (i.e., \u2113i\u2217,t = mini \u2113i,t for all t > 1),\nthen a translation-invariant algorithm with regret guarantees as in (18) above suffers a bounded regret. This is the case for the algorithm1 of Cesa-Bianchi et al. [2007, Section 4.1] tuned with E = 1 (if all losses lie in [0, 1]). Then by the translation invariance of the algorithm all losses \u2113i,t appearing in the regret bound can be replaced with the translated losses \u2113i,t \u2212 \u2113i\u2217,t > 0, so that a bound of the same form as (18) implies a regret bound of O(logK). 4. Assume that the loss vectors \u2113t are i.i.d. with a unique optimal arm in expectation (i.e., there exists i\u2217 such that E[\u2113i\u2217,1] < E[\u2113i,1] for all i 6= i\u2217). Then using the Hoeffding-Azuma inequality we can show that the algorithm of Cesa-Bianchi et al. [2007, Section 4.2] has with high probability a bounded cumulative variance VT , and therefore (by (17)) incurs a bounded regret, in the same spirit as in de Rooij et al. [2014], Gaillard et al. [2014].\nWe already know that point 2 has a counterpart in the bandit setting. If one is prepared to ignore logarithmic terms, then point 4 also has an analogue in the bandit setting due to the existence of logarithmic regret guarantees for stochastic bandits [Lai and Robbins, 1985]. The following corollaries show that in the bandit setting it is not possible to design algorithms to exploit the range of the losses or the existence of an arm that is always optimal. We use Theorem 1 as a general tool but the bounds can be improved to \u221a TK/30 by analysing the expected regret directly (similar to Lemma 4). Corollary 4. Let K > 2, T > 32(K \u2212 1) log(14) and \u03c1 > 0.22 \u221a\n(K \u2212 1)/T . Then for any randomised bandit algorithm, sup\u21131,...,\u2113T\u2208C\u03c1 E[RT (\u21131:T )] > \u221a\nT (K \u2212 1)/504, where the expectation is with respect to the randomness in the algorithm, and C\u03c1 , { x \u2208 [0, 1]K : maxi,j |xi \u2212 xj | 6 \u03c1 } . Corollary 5. Let K > 2 and T > 32(K \u2212 1) log(14). Then, for any randomised bandit algorithm, there is a loss sequence \u21131:T \u2208 [0, 1]KT such that there exists an arm i\u2217 that is optimal at every round t (i.e., \u2113i\u2217,t = mini \u2113i,t for all t > 1), but E[RT (\u21131:T )] > \u221a\nT (K \u2212 1)/504, where the expectation is with respect to the randomness in the algorithm.\nProof of Corollaries 4 and 5. Both results follow from Theorem 1 by choosing \u03b4 = 0.15. Therefore there exists an \u21131:T such that P{RT (\u21131:T ) > \u221a\n(K \u2212 1)T log(1/(4 \u00b7 0.15)/27} > 0.15/2, which implies (since RT (\u21131:T ) > 0 here) that E[RT (\u21131:T )] > \u221a\n(K \u2212 1)T/504. Finally note that \u21131:T \u2208 C\u03c1 since \u03c1 > \u221a (K \u2212 1) log(1/(4\u03b4))/T/(4 \u221a log 2) and there exists an i such that \u2113i,t 6 \u2113j,t for all j and t. 1The fully automatic algorithm of Cesa-Bianchi et al. [2007, Section 4.2] is not translation-invariant because the estimated ranges Et only take values on a dyadic grid."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Aur\u00e9lien Garivier and \u00c9milie Kaufmann for insightful discussions. The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project ALICIA)."}, {"heading": "A Proof of Lemma 1", "text": "The proof is well known (eg., Auer et al. [2002]). We only write it for the convenience of the reader. Recall that ht = ( Is, \u2113Is,s ) 16s6t\u22121 . Next we write QX|Yj the law of X conditionally on Y underQj . By the chain rule for the Kullback-Leibler divergence, note that\nKL (\nQ (ht,It) 1 ,Q (ht,It) 2\n)\n=\nt\u22121\u2211\ns=1\nEQ1\n[ KL (\nQ Is|hs 1 ,Q Is|hs 2\n) +KL (\nQ \u2113Is,s|(hs,Is) 1 ,Q \u2113Is,s|(hs,Is) 2\n)]\n+ EQ1\n[ KL (\nQ It|ht 1 ,Q It|ht 2\n)]\n. (19)\nNote that Q1(Is = i|hs) = pi,s = Q2(Is = i|hs) for all s (by definition of a randomised algorithm with weight vector ps at time s), so that the first and third KL terms equal zero. As for the second one, we can check that Q\u2113Is,s|(hs,Is)j = Qj,Is (the Is-th marginal of Qj). Combining all these remarks with (19), we get\nKL (\nQ (ht,It) 1 ,Q (ht,It) 2\n)\n=\nt\u22121\u2211\ns=1\nEQ1 [ KL(Q1,Is , Q2,Is) ] =\nt\u22121\u2211\ns=1\nEQ1\n[ K\u2211\ni=1\n1{Is=i} KL(Q1,i, Q2,i)\n]\n=\nK\u2211\ni=1\nEQ1 [ Ni(t\u2212 1) ] KL ( Q1,i, Q2,i ) ,\nwhich concludes the proof.\nB Inequalities from Information Theory\nWe first recall below a well-known data-processing inequality that follows from conditional Jensen\u2019s inequality. A variant of it in the discrete setting can be found, e.g., in Cover and Thomas [2006]. The main message is that transforming the data at hand can only reduce the ability to distinguish between two probability distributions.\nLemma 5 (Contraction of entropy). Let P and Q be two probability distributions on the same measurable space (\u2126,F), and let X be any random variable on (\u2126,F). Denote by PX and QX the law of X under P and Q respectively. Then,\nKL ( PX ,QX ) 6 KL(P,Q) .\nNext we recall an inequality between the Kullback-Leibler divergence and the chi-squared divergence. In the particular case of Bernoulli distributions with parameters p, q \u2208 [0, 1], these divergences are given respectively by2\nkl(p, q) = p log p q + (1\u2212 p) log 1\u2212 p 1\u2212 q and \u03c7 2(p, q) = (p\u2212 q)2 q(1\u2212 q) .\nLemma 6 (Consequence of Lemma 2.7 in Tsybakov 2008). Let p, q \u2208 [0, 1]. Then\nkl(p, q) 6 \u03c72(p, q) .\nThe final lemma is a straightforward corollary of Lemma 5 and the KL divergence between two Gaussians.\nLemma 7. For a 6 b and define clip[a,b](x) = max {a,min {x, b}}. Let Z be normally distributed with mean 1/2 and variance \u03c32 > 0. Define X = clip[0,1](Z) and Y = clip[0,1](Z \u2212 \u03b5) for \u03b5 \u2208 R. Then KL(PX ,PY ) 6 \u03b52/(2\u03c32).\n2We use the usual conventions: 0 log 0 = 0/0 = 0 and a/0 = +\u221e for all a > 0."}, {"heading": "C Proof of Theorem 4", "text": "The proof follows the same lines as that of Theorem 3. In the sequel we show that there exists a loss sequence \u21131:T \u2208 [0, 1]KT such that QT 6 \u03b1TK and E[RT (\u21131:T )] > (1/25) \u221a \u03b1TK. As in the proof of Theorem 3, we use Lemma 4 with \u03b1/2 and combine it with Bernstein\u2019s inequality to prove that QT 6 \u03b1TK with high probability.\nPart 1: Applying Lemma 4 with \u03b1/2 (note that T > 32K > K/(4(1 \u2212 \u03b1)) by assumption on T and \u03b1) and noting that maxj EQj [RT (\u21131:T )] > EQ\u0304[RT (\u21131:T )] we get that for some j \u2208 {1, . . . ,K} the probability distribution Qj defined with \u03b5 = (1/2) \u221a (\u03b1/2)(1\u2212 \u03b1/2)K/T satisfies\nEQj [RT (\u21131:T )] > 1\n8\n\u221a \u03b1\n2\n(\n1\u2212 \u03b1 2\n)\nTK > 1\n32\n\u221a 7\u03b1TK (20)\nsince \u03b1 6 1/4 by assumption.\nPart 2: Next we prove that\nQj(QT > \u03b1TK) 6 1\n32T . (21)\nTo this end recall that \u00b5T = 1T \u2211T t=1 \u2113t and\nQT =\nT\u2211\nt=1\n\u2016\u2113t \u2212 \u00b5T \u201622 = K\u2211\ni=1\nT\u2211\nt=1\n(\u2113i,t \u2212 \u00b5i,T )2\n\ufe38 \ufe37\ufe37 \ufe38\n=:vi,T\n.\nNoting that \u2113i,t \u2208 {0, 1} almost surely, we have vi,T = T\u00b5i,T (1\u2212 \u00b5i,T ) 6 T\u00b5i,T = \u2211T t=1 \u2113i,t. Recall that under Qj , the \u2113i,t, t > 1, are i.i.d. Ber ( \u03b1ji ) where \u03b1 j i = (\u03b1/2) + \u03b51{i6=j} (we used Lemma 4 with \u03b1/2). We now apply Bernstein\u2019s inequality exactly as after (13): combined with a union bound, it yields that, for all \u03b4 \u2208 (0, 1), with Qj-probability at least 1 \u2212 \u03b4, for all i \u2208 {1, . . . ,K},\nT\u2211\nt=1\n\u2113i,t 6 T\u03b1 j i +\n\u221a\n2T\u03b1ji\n( 1\u2212 \u03b1ji ) log K\n\u03b4 +\n1 3 log K \u03b4\n6 T (\u03b1\n2 + \u03b5\n)\n+\n\u221a\n2T (\u03b1\n2 + \u03b5\n)\nlog K\n\u03b4 +\n1 3 log K \u03b4 . (22)\nNow note that, by definition of \u03b5 = (1/2) \u221a\n(\u03b1/2)(1\u2212 \u03b1/2)K/T and by the assumption T > 8K/\u03b1,\n\u03b1 2 + \u03b5 6 \u03b1 2 + 1 2\n\u221a\n\u03b1K 2T 6 5\u03b1 8 .\nSubstituting the last upper bound in (22) and using the assumption T\u03b1 > 4 log(K/\u03b4) (that we check later) to obtain log(K/\u03b4) 6 (1/2) \u221a T\u03b1 log(K/\u03b4) we get\nT\u2211\nt=1\n\u2113i,t 6 5T\u03b1\n8 +\n(\u221a 5\n2 +\n1\n6\n)\u221a\nT\u03b1 log K\n\u03b4 6\n5T\u03b1\n8 +\n3T\u03b1\n8 = T\u03b1 , (23)\nwhere the last inequality is true whenever T\u03b1 > c1 log(K/\u03b4) with c1 = (4/9)2(3 \u221a 5 + 1)2. By the assumption \u03b1 > 2c1 log(T )/T > c1 log(32TK)/T (since T > 32K), the condition T\u03b1 > c1 log(K/\u03b4) is satisfied for \u03b4 = 1/(32T ) (as well as the weaker condition T\u03b1 > 4 log(K/\u03b4) mentioned above). We conclude the proof of (21) via QT = \u2211K i=1 vi,T 6 \u2211K i=1 \u2211T t=1 \u2113i,t 6 \u03b1TK by (23).\nConclusion: We show by contradiction that there exists a loss sequence \u21131:T \u2208 [0, 1]KT such that QT 6 \u03b1TK and\nE[RT (\u21131:T )] > 1\n64\n\u221a 7\u03b1TK , (24)\nwhere the expectation is with respect to the internal randomisation of the algorithm. Imagine for a second that (24) were false for every loss sequence \u21131:T \u2208 [0, 1]KT satisfying QT 6 \u03b1TK . Then we would have 1{QT6\u03b1TK}EQj [RT (\u21131:T )|\u21131:T ] 6 (1/64) \u221a 7\u03b1TK almost surely (since the internal source of randomness of the bandit algorithm is independent of \u21131:T ). Therefore, using the tower rule for the first expectation on the r.h.s. below, we would get\nEQj [RT (\u21131:T )] = EQj [ RT (\u21131:T )1{QT6\u03b1TK} ] + EQj [ RT (\u21131:T )1{QT>\u03b1TK} ]\n6 1\n64\n\u221a 7\u03b1TK + T \u00b7Qj(QT > \u03b1TK)\n6 1\n64\n\u221a 7\u03b1TK + 1\n32 <\n1\n32\n\u221a 7\u03b1TK (25)\nwhere (25) follows from (21) and by noting that 1/32 < (1/64) \u221a 7\u03b1TK since \u03b1 > 8K/T > 4/(7TK). Comparing (25) and (20) we get a contradiction, which proves that there exists a loss sequence \u21131:T \u2208 [0, 1]KT satisfying both QT 6 \u03b1TK and (24). We conclude the proof by noting that \u221a 7/64 > 1/25.\nNota: the assumption T > (32K) \u2228 601 is sufficient to make the interval [ 2c1 log(T )\u2228(8K)\nT , 1 4\n]\nnon empty."}, {"heading": "D Proof of Lemma 3", "text": "First we use the definition of the losses to bound\nRT (\u2113 i 1:T ) =\nT\u2211\nt=1\n(\u2113It,t \u2212 \u2113i,t) > \u2206 T\u2211\nt=1\n1{Zt\u2208[2\u2206,1\u22122\u2206] and It 6=i} .\nLet Wt = 1{Zt\u2208[2\u2206,1\u22122\u2206]}, which forms an i.i.d. Bernoulli sequence with\nQi (Wt = 0) 6 exp\n(\n\u2212 (1/2\u2212 2\u2206) 2\n2\u03c32\n)\n= p 6 1/8 ,\nwhere the inequality follows by standard tail bounds on the Gaussian integral [Boucheron et al., 2013, Exercise 2.7]. Therefore by Hoeffding\u2019s bound\nQi\n( T\u2211\nt=1\nWt 6 3T\n4\n)\n= Qi\n( T\u2211\nt=1\nWt \u2212 TEQi [W1] 6 3T\n4 \u2212 (1\u2212 p)T\n)\n6 exp (\u2212T/32) 6 \u03b4/2 .\nThe first part of the statement follows from the union bound and because if Ni(T ) 6 T/2 and \u2211T\nt=1 Wt > 3T/4, then\n\u2206\nT\u2211\nt=1\n1{Zt\u2208[2\u2206,1\u22122\u2206] and It 6=i} > \u2206T/4 .\nFor the second part we use below the tower rule (conditioning on It and the history ht up to t \u2212 1) and the fact that Wt is independent of It given ht but also independent of ht to get that\nEQi [RT (\u2113 i 1:T )] > \u2206\nT\u2211\nt=1\nQi (Wt = 1 and It 6= i)\n> \u2206\nT\u2211\nt=1\nQi (Wt = 1)Qi (It 6= i) > 7\u2206\n8 EQi [T \u2212Ni(T )] ,\nwhich completes the proof."}, {"heading": "E Proof of Corollary 3", "text": "Suppose on the contrary that such a strategy exists. Then\nE[RT (\u21131:T )] 6\n\u222b \u221e\n0\nP (RT (\u21131:T ) > x) dx\n6\n\u222b \u221e\n0\nexp\n \u2212 (\nx\nC \u221a (K \u2212 1)T\n) 1 p\n\n dx 6 C \u221a (K \u2212 1)T .\nBy the assumption in the corollary we have\n\u03b4 > P ( RT (\u21131:T ) > C \u221a (K \u2212 1)T logp(1/\u03b4) )\n= P\n(\nRT (\u21131:T ) >\n\u221a\n(K \u2212 1)T log(1/(16\u03b4)) 203C \u00b7 203C 2 logp(1/\u03b4) log(1/(16\u03b4))\n)\n,\nwhich leads to a contradiction by choosing \u03b4 sufficiently small and T sufficiently large and applying Theorem 2 to show that there exists an \u21131:T \u2208 [0, 1]KT for which\nP\n(\nRT (\u21131:T ) >\n\u221a\n(K \u2212 1)T log(1/(16\u03b4)) 203C\n)\n> 2\u03b4 ."}], "references": [{"title": "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring", "author": ["C. Allenberg", "P. Auer", "L. Gy\u00f6rfi", "G. Ottucs\u00e1k"], "venue": "In Proceedings of ALT\u20192006,", "citeRegEx": "Allenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Allenberg et al\\.", "year": 2006}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J. Audibert", "S. Bubeck"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "The nonstochastic multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Concentration inequalities: a nonasymptotic theory of independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S. Bubeck", "V. Perchet", "P. Rigollet"], "venue": "In Proceedings of The 26th Conference on Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Mach. Learn.,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience [John Wiley & Sons], second edition,", "citeRegEx": "Cover and Thomas.,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2006}, {"title": "Follow the leader if you can, hedge if you must", "author": ["S. de Rooij", "T. van Erven", "P.D. Gr\u00fcnwald", "W.M. Koolen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Rooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooij et al\\.", "year": 2014}, {"title": "A second-order bound with excess losses", "author": ["P. Gaillard", "G. Stoltz", "T. van Erven"], "venue": "In Proceedings of the 27th Conference on Learning Theory (COLT\u201914),", "citeRegEx": "Gaillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaillard et al\\.", "year": 2014}, {"title": "A simple multi-armed bandit algorithm with optimal variation-bounded regret", "author": ["E. Hazan", "S. Kale"], "venue": "In Proceedings of the 24th Conference on Learning Theory,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Better algorithms for benign bandits", "author": ["E. Hazan", "S. Kale"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Adv. in Appl. Math.,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "First-order regret bounds for combinatorial semi-bandits", "author": ["G. Neu"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pages 1360\u20131375,", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Online learning with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In Proceedings of the 26th Conference on Learning Theory,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Incomplete information and internal regret in prediction of individual sequences", "author": ["G. Stoltz"], "venue": "PhD thesis, Paris-Sud XI University,", "citeRegEx": "Stoltz.,? \\Q2005\\E", "shortCiteRegEx": "Stoltz.", "year": 2005}, {"title": "Introduction to nonparametric estimation", "author": ["A. Tsybakov"], "venue": "Springer Science & Business Media,", "citeRegEx": "Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "A famous strategy is called Exp3, which satisfies E[RT (l1:T )] = O( \u221a KT log(K))) where the expectation is taken over the randomness in the algorithm and the choices of the adversary [Auer et al., 2002].", "startOffset": 184, "endOffset": 203}, {"referenceID": 2, "context": "There is also a lower bound showing that for every learner there is an adversary for which the expected regret is E[RT (l1:T )] = \u03a9( \u221a KT ) [Auer et al., 1995].", "startOffset": 140, "endOffset": 159}, {"referenceID": 1, "context": "If the losses are chosen ahead of time, then the adversary is called oblivious, and in this case there exists a learner for which E[RT (l1:T )] = O( \u221a KT ) [Audibert and Bubeck, 2009].", "startOffset": 156, "endOffset": 183}, {"referenceID": 3, "context": "P [Auer et al., 2002] and Exp-IX [Neu, 2015] are tuned with a confidence parameter \u03b4 \u2208 (0, 1) and satisfy for some universal constant c > 0", "startOffset": 2, "endOffset": 21}, {"referenceID": 14, "context": ", 2002] and Exp-IX [Neu, 2015] are tuned with a confidence parameter \u03b4 \u2208 (0, 1) and satisfy for some universal constant c > 0", "startOffset": 19, "endOffset": 30}, {"referenceID": 5, "context": "P [Bubeck and Cesa-Bianchi, 2012] leads to a single algorithm for which", "startOffset": 2, "endOffset": 33}, {"referenceID": 13, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al.", "startOffset": 14, "endOffset": 25}, {"referenceID": 13, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al.", "startOffset": 14, "endOffset": 63}, {"referenceID": 0, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al. [2006], Rakhlin and Sridharan [2013]) such that for all l1:T \u2208 [0, 1] E[RT (l1:T )] 6 O (\u221a LTK log(K) +K log(KT ) ) , with LT = min 16i6K T \u2211", "startOffset": 64, "endOffset": 88}, {"referenceID": 0, "context": ", FPL-TRIX by Neu [2015] with earlier results by Stoltz [2005], Allenberg et al. [2006], Rakhlin and Sridharan [2013]) such that for all l1:T \u2208 [0, 1] E[RT (l1:T )] 6 O (\u221a LTK log(K) +K log(KT ) ) , with LT = min 16i6K T \u2211", "startOffset": 64, "endOffset": 118}, {"referenceID": 14, "context": "The first-order regret bound (3) of Neu [2015] is equivalent to: \u2200\u03b1 \u2208 [0, 1], sup l1:T\u2208B\u03b1,T E[RT (l1:T )] 6 O (\u221a \u03b1TK log(K) +K log(KT ) ) .", "startOffset": 36, "endOffset": 47}, {"referenceID": 11, "context": "Second-order bounds Another type of improved regret bound was derived by Hazan and Kale [2011b] and involves a second-order quantity called the quadratic variation.", "startOffset": 73, "endOffset": 96}, {"referenceID": 11, "context": "Hazan and Kale [2011b] addressed the general online linear optimisation setting.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "The second-order regret bound (6) of Hazan and Kale [2011b] is equivalent to: \u2200\u03b1 \u2208 [0, 1/4], sup l1:T\u2208V\u03b1,T E[RT (l1:T )] 6 c ( K \u221a \u03b1TK logT +K log T +K log T ) .", "startOffset": 37, "endOffset": 60}, {"referenceID": 7, "context": "Two impossibility results in the bandit setting We also show in Section 4 that, in contrast to the full-information setting, regret bounds involving the cumulative variance of the algorithm as in [Cesa-Bianchi et al., 2007] cannot be obtained in the bandit setting.", "startOffset": 196, "endOffset": 223}, {"referenceID": 3, "context": "Results of roughly this form are well known and the proof follows immediately from the chain rule for the relative entropy and the independence of the loss vectors across time (see [Auer et al., 2002] or Appendix A).", "startOffset": 181, "endOffset": 200}, {"referenceID": 2, "context": "Results of roughly this form are well known and the proof follows immediately from the chain rule for the relative entropy and the independence of the loss vectors across time (see [Auer et al., 2002] or Appendix A). One difference is that the losses need not be independent across the arms, which we heavily exploit in our proofs by using correlated losses. The second key lemma is an alternative to Pinsker\u2019s inequality that proves useful when the Kullback-Leibler divergence is larger than 2. It has previously been used for bandit lower bounds (in the stochastic setting) by Bubeck et al. [2013]. Lemma 2 (Lemma 2.", "startOffset": 182, "endOffset": 600}, {"referenceID": 3, "context": "3P [Auer et al., 2002] and Exp3-IX [Neu, 2015].", "startOffset": 3, "endOffset": 22}, {"referenceID": 14, "context": ", 2002] and Exp3-IX [Neu, 2015].", "startOffset": 20, "endOffset": 31}, {"referenceID": 4, "context": "38) of Boucheron et al. [2013] with Xt = lj,t \u2212 \u03b1/2 6 1 = b, with v = T (\u03b1/2)(1 \u2212 \u03b1/2), and with c = b/3 = 1/3), we get that, for all \u03b4 \u2208 (0, 1), with Qj-probability at least 1\u2212 \u03b4, LT 6 T \u2211", "startOffset": 7, "endOffset": 31}, {"referenceID": 11, "context": "Closing this gap is left as an open question, but we conjecture that the upper bound is loose (see also the COLT open problem by Hazan and Kale [2011a]).", "startOffset": 129, "endOffset": 152}, {"referenceID": 7, "context": "If all the losses li,t are nonnegative, then by Corollary 3 of [Cesa-Bianchi et al., 2007] the second-order bound (17) implies the first-order bound", "startOffset": 63, "endOffset": 90}, {"referenceID": 13, "context": "If one is prepared to ignore logarithmic terms, then point 4 also has an analogue in the bandit setting due to the existence of logarithmic regret guarantees for stochastic bandits [Lai and Robbins, 1985].", "startOffset": 181, "endOffset": 204}, {"referenceID": 7, "context": "This is the case for the algorithm1 of Cesa-Bianchi et al. [2007, Section 4.1] tuned with E = 1 (if all losses lie in [0, 1]). Then by the translation invariance of the algorithm all losses li,t appearing in the regret bound can be replaced with the translated losses li,t \u2212 li\u2217,t > 0, so that a bound of the same form as (18) implies a regret bound of O(logK). 4. Assume that the loss vectors lt are i.i.d. with a unique optimal arm in expectation (i.e., there exists i such that E[li\u2217,1] < E[li,1] for all i 6= i). Then using the Hoeffding-Azuma inequality we can show that the algorithm of Cesa-Bianchi et al. [2007, Section 4.2] has with high probability a bounded cumulative variance VT , and therefore (by (17)) incurs a bounded regret, in the same spirit as in de Rooij et al. [2014], Gaillard et al.", "startOffset": 39, "endOffset": 791}, {"referenceID": 7, "context": "This is the case for the algorithm1 of Cesa-Bianchi et al. [2007, Section 4.1] tuned with E = 1 (if all losses lie in [0, 1]). Then by the translation invariance of the algorithm all losses li,t appearing in the regret bound can be replaced with the translated losses li,t \u2212 li\u2217,t > 0, so that a bound of the same form as (18) implies a regret bound of O(logK). 4. Assume that the loss vectors lt are i.i.d. with a unique optimal arm in expectation (i.e., there exists i such that E[li\u2217,1] < E[li,1] for all i 6= i). Then using the Hoeffding-Azuma inequality we can show that the algorithm of Cesa-Bianchi et al. [2007, Section 4.2] has with high probability a bounded cumulative variance VT , and therefore (by (17)) incurs a bounded regret, in the same spirit as in de Rooij et al. [2014], Gaillard et al. [2014]. We already know that point 2 has a counterpart in the bandit setting.", "startOffset": 39, "endOffset": 815}], "year": 2017, "abstractText": "We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.", "creator": "LaTeX with hyperref package"}}}