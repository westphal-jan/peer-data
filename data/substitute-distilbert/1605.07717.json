{"id": "1605.07717", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Deep Structured Energy Based Models for Anomaly Detection", "abstract": "in one view, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. we propose deep structured energy transfer models ( dsebms ), where the wave function is the output of a deterministic deep neural network with structure. we suggest novel model architectures to integrate ebms with broader types of data such as static data, sequential values, and spatial data, and apply complicated sample architectures to adapt to the data structure. our training algorithm is built upon the recent development of score matching \\ cite { sm }, which connects an ebm with a sampled sample, remove the need for complicated sampling method. statistically sound a criterion can be derived for detecting detection purpose from the perspective of the dynamic landscape of the associated distribution. we investigate two relevant criteria, performing anomaly detection : the energy score and the reconstruction error. extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing candidates.", "histories": [["v1", "Wed, 25 May 2016 03:40:18 GMT  (8782kb,D)", "http://arxiv.org/abs/1605.07717v1", "To appear in ICML 2016"], ["v2", "Thu, 16 Jun 2016 02:36:10 GMT  (8782kb,D)", "http://arxiv.org/abs/1605.07717v2", "To appear in ICML 2016"]], "COMMENTS": "To appear in ICML 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shuangfei zhai", "yu cheng", "weining lu", "zhongfei zhang"], "accepted": true, "id": "1605.07717"}, "pdf": {"name": "1605.07717.pdf", "metadata": {"source": "META", "title": "Deep Structured Energy Based Models for Anomaly Detection", "authors": ["Shuangfei Zhai", "Yu Cheng", "Weining Lu", "Zhongfei (Mark) Zhang"], "emails": ["SZHAI2@BINGHAMTON.EDU", "CHENGYU@US.IBM.COM", "LUWN14@MAILS.TSINGHUA.EDU.CN", "ZHONGFEI@CS.BINGHAMTON.EDU"], "sections": [{"heading": "1. Introduction", "text": "Anomaly detection (also called novelty or outlier detection) is to identify patterns that do not conform to the expected normal patterns (Chandola et al., 2009). Existing methods for outlier detection either construct a profile for normal data examples and then identify the examples not conform-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\ning to the normal profile as outliers, or explicitly isolate outliers based on statistical or geometric measures of abnormality. A variety of methods can be found in the survey (Zimek et al., 2012). Anomaly detection is to correctly characterize data distribution in nature, so the normality of the data characteristic can be characterized as a distribution and any future data can be benchmarked against the normality. Apparently, the statistical power and accuracy of the anomaly detection methods depend on the capacity of the model that is used to characterize the data distribution.\nOur work is inspired by the extraordinary capacity of deep models which are able to capture the complex distributions in real-world applications. Recent empirical and theoretical studies indicate that deep architectures are able to achieve better generalization ability compared to the shallow counterparts on challenging recognition tasks (Bengio, 2009). The key ingredient to the success of deep learning is its ability to learn multiple levels of representations with increasing abstraction. For example, it is shown that properly regularized autoencoders (Vincent et al., 2010; Rifai et al., 2011) are able to effectively characterize the data distribution and learn useful representations, which are not achieved by shallow methods such as PCA or K-Means.\nHowever, deep models have not been systematically studied and developed for anomaly detection. Central questions critical to this development include: 1) how to effectively model the data generating distribution with a deep model? 2) how to generalize a model to a range of data structures such as static data, sequential data, and spatial data? 3) how to develop computationally efficient training algorithms to make them scalable? and 4) how to derive statistically sound decision criteria for anomaly detection purpose?\nTo answer these questions in a systematic manner, in this work, we propose deep structured energy based models (DSEBMs). Our approach falls into the category of en-\nar X\niv :1\n60 5.\n07 71\n7v 1\n[ cs\n.L G\n] 2\n5 M\nergy based models (EMBs) (LeCun et al., 2006), which is a powerful tool for density estimation. An EBM works by coming up with a specific parameterization of the negative log probability, which is called energy, and then computing the density with a proper normalization. In this work, we focus on deep energy based models (Ngiam et al., 2011), where the energy function is composed of a deep neural network. Moreover, we investigate various model architectures as to accommodate different data structures. For example, for data with static vector inputs, standard feed forward neural networks can be applied. However, for sequential data, such as audio sequence, recurrent neural networks (RNNs) are known to be better choices. Likewise, convolutional neural networks (CNNs) are significantly more efficient at modeling spatial structures (Krizhevsky et al., 2012), such as on images. Our model thus allows the energy function to be composed of deep neural networks with designated structures (fully connected, recurrent or convolutional), significantly extending the application of EBMs to a wide spectrum of data structures.\nDespite its powerful expressive ability, the training of EBMs remains fairly complicated compared to the training of a deterministic deep neural network, as the former requires carefully designed sampling algorithm to deal with its intrinsic intractability. The need of efficient training algorithms is even more severe with DSEBMs, given its complicated structured parameterization. To this end, we adopt the score matching method (Hyva\u0308rinen, 2005) as the training algorithm, instead of the default maximum likelihood estimation (MLE). Similarly to (Vincent, 2011), we are able to train a DSEBM in the same way as that of a deep denoising autoencoder (DAE) Vincent et al. (2010), which only requires standard stochastic gradient descent (SGD). This significantly simplifies the training procedure and allows us to efficiently train a DSEBM on large datasets.\nIn order to perform the actual anomaly detection with a trained DSEBM, we investigate two decision criteria, the energy score and the reconstruction error. We show that the two criteria are closely connected from the view of the energy landscape, and evaluate their effectiveness under different scenarios. We perform extensive evaluations on several benchmarks varying from static, sequential to spatial data. We show that our method consistently matches or outperforms the competing algorithms."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Energy Based Models (EBMs)", "text": "EBMs are a family of probabilistic models that can be used to build probability density functions. An EBM parameter-\nizes a density function for input x \u2208 Rd in the form:\np(x; \u03b8) = e\u2212E(x;\u03b8)\nZ(\u03b8) , (1)\nwhere E(x; \u03b8) is the energy (negative log probability) associated with instance x; Z(\u03b8) = \u222b x e\u2212E(x;\u03b8)dx is the partition function to ensure that the density function integrates to probability 1; \u03b8 are the model parameters to be learned. The nice property of EBM is that one is free to parameterize the energy in any sensible way, giving it much flexibility and expressive power. Learning is conducted by assigning lower energy (hence higher probability) to observed instances and vice versa. However, directly applying MLE is impossible due to the intractability of the partition fuction Z(\u03b8), and thus one usually needs to resort to MCMC methods and approximate the integration with the summation over samples from a Markov chain."}, {"heading": "2.2. Restricted Boltzmann Machine (RBM)", "text": "RBM (Hinton, 2010) is one of the most well know examples of EBM. For continuous input data, the energy function of an RBM takes the form:\nE(x; \u03b8) = 1\n2 \u2016x\u2212 b\u2032\u201622 \u2212 K\u2211 j=1 g(WTj x+ bj), (2)\nwhere W \u2208 Rd\u00d7K (with Wj being the jth column), b \u2208 RK (with bj as the jth element) and b\u2032 \u2208 Rd are the parameters to learn; g(x) is the soft plus function log(1+ex). Multiple RBMs can be trained and stacked on top of each other to formulate a deep RBM, which makes it useful for initializing multi-layer neural networks. Although efficient training algorithms, such as contrastive divergence, are proposed to make RBM scalable, it is still considerably more difficult to train than a deterministic neural network (Hinton, 2010)."}, {"heading": "2.3. Denoising Autoencoders and Score Matching", "text": "Autoencoders are unsupervised models that learn to reconstruct the input. A typical form of autoencoder minimizes the following objective function:\nN\u2211 i=1 \u2016xi \u2212 f(xi; \u03b8)\u201622 (3)\nwhere f(\u00b7; \u03b8) is the reconstruction function that maps Rd \u2192 Rd which is usually composed of an encoder followed by a decoder with symmetrical architecture and shared parameters. One particularly interesting variant of autoencoders is DAEs (Vincent et al., 2010), which learn to construct the inputs given their randomly corrupted versions:\nN\u2211 i=1 E \u2016xi \u2212 f(xi + ; \u03b8)\u201622, (4)\nwhere \u223c N (0, \u03c32I) is an isotropic Gaussian noise. DAEs are easy to train with standard stochastic gradient descent (SGD) and perform significantly better than unregularized autoencoders.\nWhile RBM and DAEs are typically considered as two alternative unsupervised deep models, it is recently shown they are closely related to each other. In particular, (Vincent, 2011) shows that training an RBM with score matching (SM) (Hyva\u0308rinen, 2005) is equivalent to a one-layer DAE. SM is an alternative method to MLE, which is especially suitable for estimating non-normalized density functions such as EBM. Instead of trying to directly maximize the probability of training instances, SM minimizes the following objective function:\nJ(\u03b8) = 1\n2 \u222b x px(x)\u2016\u03c8(x; \u03b8)\u2212 \u03c8x(x)\u201622dx, (5)\nwhere px(x) is the true data distribution which is unknown; \u03c8(x; \u03b8) = \u2207x log p(x; \u03b8) = \u2212\u2207xE(x; \u03b8) and \u03c8x(x; \u03b8) = \u2207x log px(x) are the score function of the model and the true density function, respectively. In words, J(\u03b8) measures the expected distance of the scores between the model density and the true density. (Vincent, 2011) shows that by approximating the px(x) with the Parzen window density 1 N \u2211N i=1N (xi, \u03c32I), minimizing Equation 5 yields an objective function in the same form as that of an autoencoder in Equation 4, with a reconstruction function defined as:\nf(x; \u03b8) = x\u2212\u2207xE(x; \u03b8). (6)\nSubstituting E(x; \u03b8) with Equation 2 yields a typical onelayer DAE (up to a constant factor):\nf(x; \u03b8) =W\u03c3(WTx+ b) + b\u2032, (7)\nwhere \u03c3(x) is the sigmoid function 11+e\u2212x .\nEquation 6 plays a key role in this work, as it allows one to efficiently train an arbitrary EBM in a similar way to that of a DAE, as long as the energy function is differentiable w.r.t. x and \u03b8. As will be demonstrated below, this makes training a deep EBM with various underlying structures in an end-to-end fashion, without the need of resorting to sophisticated sampling procedures (Ngiam et al., 2011) or layer wise pretraining (Hinton, 2010)."}, {"heading": "3. Deep Structured EBMs", "text": "Deep architectures allow one to model complicated patterns efficiently, which makes it especially suitable for high dimensional data. On the other hand, it is often necessary to adapt the architecture of a deep model to the structure of data. For example, recurrent neural networks (RNNs) have been shown to work very well at modeling sequential data;\nconvolutional neural networks (CNNs) are very effective at modeling data with spatial structure. In this work, we extend EBMs further to deep structured EBMs (DSEBMs), where we allow the underlying deep neural network encoding the energy function to take architectures varying from fully connected, recurrent, and convolutional. This generalizes EBMs (LeCun et al., 2006) as well as deep EBMs (Ngiam et al., 2011) as it makes our model applicable to a much wider spectrum of applications, including static data, sequential data and spatial data. Moreover, we show that by deriving the proper reconstruction functions with Equation 6, DSEBMs can be easily trained with SGD, regardless of the type of the underlying architecture. In the following, we will elaborate the formulation of DSEBMs for the three cases."}, {"heading": "3.1. Fully Connected EBM", "text": "This case is conceptually the same as the deep EBMs proposed in (Ngiam et al., 2011). Without loss of generality, we express the energy function of an L-layer fully connected EBM as:\nE(x; \u03b8) = 1\n2 \u2016x\u2212 b\u2032\u201622 \u2212 KL\u2211 j=1 hL,j\ns.t. hl = g(W T l hl\u22121 + b1), l \u2208 [1, L]\n(8)\nwhere Wl \u2208 RKl\u22121\u00d7Kl , bl \u2208 RKl are the parameters for the lth layer; Kl is the dimensionality of the lth layer. The 0th layer is defined as the input itself; and thus we have K0 = d, h0 = x. We have explicitly included the term \u2016x \u2212 b\u2032\u201622 which acts as a prior, punishing the probability of the inputs that are far away from b\u2032 \u2208 Rd.\nFollowing the chain rule of gradient computation, one can derive the reconstruction function as follows:\nf(x; \u03b8) = x\u2212\u2207xE(x; \u03b8) = h\u20320 + b\u2032\nh\u2032l\u22121 = \u2202hl \u2202hl\u22121 h\u2032l = \u03c3(W T l hl\u22121 + bl) \u00b7 (Wlh\u2032l), for l \u2208 [1, L\u2212 1],\n(9)\nwhere h\u2032L = 1 with 1 \u2208 RKL denoting a column vector of all ones; \u00b7 is the element-wise product between vectors. One can then plug in the resulting f(x; \u03b8) into Equation 4 and train it as a regular L-layer DAE."}, {"heading": "3.2. Recurrent EBMs", "text": "Our formulation of recurrent EBMs is similar to (Boulanger-Lewandowski et al., 2012), where an EBM is built at each time step, with parameters determined by an underlying RNN. Formally, given a sequence of length T x = [x1, ..., xT ], xt \u2208 Rd, we factorize the joint probability as p(x) = \u220fT t=1 p(x t|x1,...,t\u22121) with the chain rule of\nprobability. For each time step t, p(xt|x1,...,t\u22121) is modeled as an EBM with energy E(xt|\u03b8t). In contrast to the conventional formulation of EBM, the parameters of the EMB \u03b8t is a function of the inputs from all the previous time steps x1,...,t\u22121. A natural choice of \u03b8t is to let it be the output of an RNN. As a concrete example, consider the energy function at each step follows that in Equation 2, by replacingW, b, b\u2032 withW t, bt, b\u2032t. Directly letting the RNN to update all the parameters at each step requires a large RNN with lots of parameters. As a remedy, (BoulangerLewandowski et al., 2012) proposes to fix W t = W for all time steps, only letting bt and b\u2032t to be updated by the RNN:\nht = g(Whhh t\u22121 +Whxx t + bh) bt =Wbhh t + b, b\u2032t =Wb\u2032hh t + b\u2032, (10)\nwhere Whh \u2208 RKrnn\u00d7Krnn ,Whx \u2208 RKrnn\u00d7d, bh \u2208 RKrnn are parameters of the RNN; Wbh \u2208 RKebm\u00d7Krnn , b \u2208 RKebm ,Wb\u2032h \u2208 Rd\u00d7Krnn , b\u2032 \u2208 Rd are the weights with which to transform the hidden state of the RNN to the adaptive biases.\nTraining the recurrent EBM with score matching is similar to that of a fully connected EBM. To see this, we now have p(x) = e \u2212 \u2211T t=1 E(x\nt;\u03b8t)\u220fT t=1 Z t , where Zt = \u2211 x e \u2212E(x;\u03b8t)\nis the partition function for the tth step. Plugging p(x) into Equation 5, we have \u03c8(x) = \u2207x log p(x) \u2248 \u2212[\u2207Tx1E(x1; \u03b81), ...,\u2207TxTE(x\nT ; \u03b8T )]T . In the last step, we have made a simplification by omitting the gradient term of \u2207xjE(xi; \u03b8i), for j < i 1. Accordingly, Equation 6 is modified to f(x) = x \u2212 [\u2207Tx1E(x1; \u03b81), ...,\u2207TxTE(x\nT ; \u03b8T )]T . One is then able to train the recurrent EBM by plugging f(x) into Equation 4 and perform the standard SGD. Note that in this case the standard backpropagation is replaced with backpropagation through time, due to the need of updating the RNN parameters."}, {"heading": "3.3. Convolutional EBMs", "text": "Previously, the combination of CNN and RBM have been proposed in (Lee et al., 2009), where several layers of RBMs are alternately convolved with an image then stacked on top of each other. In this paper, we take a significantly different approach by directly building deep EBMs with convolution operators (with optional pooling layers or fully connected layers), simply by replacing hL in Equation 8 with the output of a CNN. Using a deterministic deep convolutional EBM allows one to directly train the model end-to-end with score matching, thus significantly simplifies the training procedure compared with (Lee et al., 2009). Formally, consider the input of the (l \u2212 1)th layer\n1While one can also choose to use the full gradient, we find this simplification works well in practice, yielding an objective in a much more succinct form.\nhl\u22121 \u2208 RKl\u22121\u00d7dl\u22121\u00d7dl\u22121 is a dl\u22121\u00d7dl\u22121 image withKl\u22121 channels. We define hl as the output of a convolution layer:\nhl,j = g( Kl\u22121\u2211 k=1 W\u0303l,j,k \u2217 hl\u22121,k + bl,j), j \u2208 [1,Kl]. (11)\nHere W \u2208 RKl\u00d7Kl\u22121\u00d7dw,l\u00d7dw,l are the Kl convolutional filters of size dw,l \u00d7 dw,l; bl \u2208 RKl is the bias for each filter. We denote the tilde operator (A\u0303) as flipping a matrix A horizontally and vertically; \u2217 is the \u201dvalid\u201d convolution operator, where convolution is only conducted where the input and the filter fully overlap. dl = dl\u22121 \u2212 dw,l + 1 is thus the size of the output image following the valid convolution. In order to compute the reconstruction function following equation 9, we modify the recurrence equation from h\u2032l to h \u2032 l\u22121 for a convolutional operator as:\nh\u2032l\u22121,k = Kl\u2211 j=1 [\u03c3( Kl\u22121\u2211 k=1 W\u0303l,j,k\u2217hl\u22121,k+bl,j)\u00b7(Wl,j,k h\u2032l,j)]. (12) Here we have denoted as the \u201dfull\u201d convolution operator, where convolution is conducted whenever the input and the filter overlap by at least one position. Besides the convolution layer, we can also use a max pooling layer which typically follows a convolution layer. Denote dp \u00d7 dp as the pooling window size, passing hl\u22121 through a max pooling layer gives an output as:\nhl,k,p,q, xl,k,p,q, yl,k,p,q\n= max (p\u22121)dp+1\u2264i\u2264pdp,(q\u22121)dp+1\u2264j\u2264qdp\nhl\u22121,k,i,j . (13)\nHere the max operation returns the maximum value as the first term and the corresponding maximum coordinates as the second and third. Rewriting the recurrence equation corresponding to Equation 8 yields:\nh\u2032l\u22121,k,xl,k,p,q,yl,k,p,q = h \u2032 l,k,p,q, p, q \u2208 [1, dl], (14)\nwhere the unassigned entries of h\u2032l\u22121 are all set as zero.\nThe derivation above shows that one is able to compute the reconstruction function f(x) for a deep convolutional EBM consisting of convolution, max pooling and fully connected layers. Other types of layers such as mean pooling can also be managed in a similar way, which is omitted due to the space limit."}, {"heading": "4. Deep Structured EBMs for Anomaly Detection", "text": "Performing anomaly detection given a trained EBM naturally corresponds to identifying data points that are assigned low probability by the model. With a trained DSEBM, we can then select samples that are assigned\nprobability lower than some pre-chosen threshold pth as outliers. Although computing the exact probability according to Equation 1 is intractable, one can immediately recognize the following logic:\np(x; \u03b8) < pth \u21d2 log p(x; \u03b8) < log pth \u21d2 E(x; \u03b8) > log pth + logZ(\u03b8)\u21d2 E(x; \u03b8) > Eth. (15)\nHere we have used the fact that Z(\u03b8) is a constant that does not depend on x; hence selecting samples with probability lower than pth is equivalent to selecting those with energy higher than a corresponding energy threshold Eth. 2\nMoreover, motivated by the connection of EBMs and DAEs, we further investigate another decision criteria which is based on the reconstruction error. In the early work of (Williams et al., 2002), autoencoders (which they call replicator neural networks) have been applied to anomaly detection. However, their adoption autoencoders are unregularized, and thus could not be interpreted as a density model. The authors then propose to use the reconstruction error as the decision criterion, selecting those with high reconstruction error as outliers. On the other hand, with a DSEBM trained with score matching, we are able to derive the corresponding reconstruction error as \u2016x \u2212 f(x; \u03b8)\u201622 = \u2016\u2207xE(x; \u03b8)\u201622. The corresponding decision rule is thus \u2016\u2207xE(x; \u03b8)\u201622 > Errorth, with some threshold Errorth. 3 In other words, examples with high reconstruction errors correspond to examples whose energy has large gradient norms. This view makes reconstruction error a sensible criterion as on the energy surface inliers usually sit close to local minimums, where the gradient should be close to zero. However, using the reconstruction error might produce false positive examples (outliers that are classified as inliers), as if a sample sits close to a local maximum on the energy surface, the gradient of its energy will also be small. We demonstrate the two criteria in Figure 4 with a 1D example. As we see, for x1 and x3, both energy and reconstruction error produces the correct prediction. However, x2 is a false positive example under the reconstruction error criterion, which energy correctly recognizes as an outlier. However, note that in a high dimensional input space, the probability that an outlier resides around a local maximum grows exponentially small w.r.t. the dimensionality. As a result, the reconstruction error still serves as a reasonable criterion."}, {"heading": "5. Experimental Evaluation", "text": "In this section, we evaluate the proposed anomaly detection framework, where our two proposed anomaly detection cri2For a recurrent DSEBM, this decision rule is modified as\u2211T t=1E(x\nt; \u03b8t) > Eth 3For a recurrent DSEBM, this decision rule is modified as\u2211T t\u22121 \u2016\u2207xtE(x t; \u03b8t)\u201622 > Errorth\nteria using energy and reconstruction error are abbreviated as DSEBM-e and DSEBM-r, respectively. Our experiments consist of three types of data: static data, sequential data (e.g., audio) and spatial data (e.g., image), where we apply fully connected EBM, recurrent EBM and convolutional EBM, respectively. The specifications of benchmark datasets used are summarized in Table 1. To demonstrate the effectiveness of DSEBM, we compare our approach with several well-established baseline methods that are publicly available. Ground truth labels are available in all data sets; and we report precision (mean precision), recall (mean recall), and F1 score (mean F1) for the anomaly detection results achieved by all methods. Below we detail the baselines, explain the experimental methodology, and discuss the results."}, {"heading": "5.1. Static Data", "text": "There benchmark datasets are used in this study: KDD99 10 percent, Thyroid and Usenet from the UCI repository (Lichman, 2013). The training and test sets are split by 1:1 and only normal samples are used for training the model. We compare DSEBMs (with 2-layer fully connected energy function) with a variety of competing methods, including two reconstruction-based outlier detection methods, PCA and Kernel PCA, two density-based methods Kernel Density Estimator (KDE) (Parzen, 1962) and Robust Kernel Density Estimator (RKDE) (Kim & Scott, 2012), along with the traditional one-class learning method One-Class SVM (OC-SVM) (Scho\u0308lkopf et al., 2001). We also include the method proposed in (Williams et al., 2002), named AutoEncoder Outlier Detection (AEOD) as one baseline.\nThe results are shown in Table 2. We see that, overall, DSEBM-e and DSEBM-r achieve comparable or better performances compared with the best baselines. On Thyroid, the performances of DSEBMs are slightly worse than OCSVM and RKDE. We speculate that this is most likely caused by the low dimensionality of the dataset (10), where kernel based methods (which OC-SVM and RKDE are) are very effective. However, on Usenet which has a much higher dimensionality (659), DSEBM-e achieves the best result, measured by recall and F1. This is consistent with our intuition, as on high-dimensional datasets, deep models are more effective and necessary to resolve the underlying complication. On KDD99, DSEBM-e also achieves the best F1."}, {"heading": "5.2. Sequential Data", "text": "For this task, we use three sequential datasets: (1) CUAVE which contains audio-visual data of ten spoken digits (zero to nine); (2) NATOPS which contains 24 classes of bodyand-hand gestures used by the US Navy in aircraft handling aboard aircraft carriers; (3) FITNESS which contains users\u2019 daily fitness behaviors collected from health care devices, including diet, sleep and exercise information. According to the BMI change, the users are categorized into two groups \u201dlosing weight\u201d and \u201dgaining weight\u201d. For a single category, the outlier samples are simulated with a proportion 0.1 \u2264 \u03c1 \u2264 0.4 from other categories. The datasets are split into training and test by 2:1, where 2/3 of the normal samples are used for training split. We compare DSEBM-r and DSEBM-e with three static baselines,\nKernel PCA, RKDE and OC-SVM. Also, we include two sequential methods: 1) HMMs, where the model is trained with the normal training sequences, and the posterior probability p(y|x) of each test sequence is computed as the normalized negative log-likelihood; 2) OCCRF (Song et al., 2013), where the model learns from a one-class dataset and captures the temporal dependence structure using conditional random fields (CRFs). Table 3 (with \u03c1 = 0.3) shows the performances of all the methods. We see that DSEBMe achieves the highest mean precision and mean F1 score for most cases while DSEBM-r achieves the second best, both beating the competing methods with a margin. HMM shows high precision rates but low recall rates, resulting in a low F1 score. OCCRF is the second best performing method, following our two DSEBM variants, due to its ability to capture the temporal information with CRFs. On FITNESS, DSEBMs improves over 4%, 6% and 5% on mean precision, mean recall and mean F1 over the other baselines. Similar trends can be seen in Figure 3 (with \u03c1 varying from 0.1 to 0.4). All these results demonstrate DSEBMs\u2019 ability to benefit from the rich temporal (with large Avg(t)) information, thanks to the underlying RNN."}, {"heading": "5.3. Spatial Data", "text": "We use three public image datasets: Caltech-101, MNIST and CIFAR-10 for this sub task. On Caltech-101, we choose 11 object categories as inliers, each of which contains at least 100 images, and sample outlier images with a proportion 0.1 \u2264 \u03c1 \u2264 0.4 from the other categories. On MNIST and CIFAR-10, we use images from a single\ncategory as inliers, and sample images from the other categories with a proportion 0.1 \u2264 \u03c1 \u2264 0.4. Each dataset is split into a training and testing set with a ratio of 2:1. We compare DSEBMs (with one convolutional layer + one pooling layer + one fully connected layer) with several baseline methods including: High-dimensional Robust PCA (HR-PCA), Kernel PCA (KPCA), Robust Kernel Density Estimator (RKDE), One-Class SVM (OC-SVM) and Unsupervised One-Class Learning (UOCL) (Liu et al., 2014). All the results are shown in Table 4 with \u03c1 = 0.3. We see that DSEBM-e is the best performing method overall in terms of mean recall and mean F1, with particularly large margins on large datasets (MNIST and CIFAR-10). Measured by F1, DSEBM-e improves 3.5% and 2.3% over the best-performing baselines. Figure 3 with \u03c1 varying from 0.1 to 0.4 also demonstrates consistent results."}, {"heading": "5.4. Energy VS. Reconstruction Error", "text": "In terms of the two decision criteria of DSEBM, we observe that DSEBM-e consistently outperforms DSEBM-r on all the benchmarks except for the Thyroid dataset. This verifies our conjecture that the energy score is a more accurate decision criterion than reconstruction error. In addition, to gain further insight on the behavior of the two criteria, we demonstrate seven outliers selected from the Caltech-101 benchmark in Figure 4. For each image, the energy scores are displayed at the second row in red, followed by the reconstruction error displayed in green and the correct inlier class. Interestingly, all the seven outliers are visually similar to the inlier class and have small reconstruction errors (compared with the threshold). However, we are able to successfully identify all of them with energy (which are higher than the energy threshold)."}, {"heading": "6. Related Work", "text": "There has been a large body of work concentrating on anomaly detection (Chandola et al., 2009), noticeably: (1) the reconstruction based methods such as PCA and Kernel PCA, Robust PCA and Robust Kernel PCA; (2) the probability density based methods, including parametric estimators (Zimek et al., 2012) and nonparametric estimators such as the kernel density estimator (KDE) and the more recent robust kernel density estimator (RKDE); (3) methods of learning a compact data model such that as many as possible normal samples are enclosed inside, for example, oneclass SVM and SVDD. Graham et al. proposed a method based on autoencoder (Williams et al., 2002). However, all the methods above are static in nature which does not assume the structure of data. Two types of data are extensively studied in sequential anomaly detection: sequential time series data and event data. Sun et al. proposes a technique that uses Probabilistic Suffix Trees (PST) to find the nearest neighbors for a given sequence to detect sequential anomalies in protein sequences (Sun et al., 2006). Song et al. presents a one class conditional random fields method for general sequential anomaly detection tasks (Song et al., 2013). Our model is significantly different from the above mentioned methods, where our use of RNN encoded EBM gives us much modeling power and statistical soundness at the same time. Among the few approaches designed for spatial data, (Nam & Sugiyama, 2015) proposes to use\nCNNs in least-squares direct density-ratio estimation, and demonstrated its usefulness in inlier-based outlier detection of images. Despite the usage similar use of CNNs, our work takes a very different path by directly modeling the density. Methodology-wise, there is also a recent surge of training EBMs with score matching (Vincent, 2011; Swersky et al., 2011; Kingma & Cun, 2010). However, most of them are constrained to shallow models, thus limiting their application to relatively simple tasks."}, {"heading": "7. Conclusion", "text": "We proposed training deep structured energy based models for the anomaly detection problem and extended EBMs to deep architectures with three types of structures: fully connected, recurrent and convolutional. To significantly simplify the training procedure, score matching is proposed in stead of MLE as the training algorithm. In addition, we have investigated the proper usage of DSEBMs for the purpose of anomaly detection, in particular focusing on two decision criteria: energy score and reconstruction error. Systematic experiments are conducted on three types of datasets: static, sequential and spatial, demonstrating that DSEBMs consistently match or outperform the stateof-the-art anomaly detection algorithms. To be best of our knowledge, this is the first work that extensively evaluates deep structured models to the anomaly detection problem."}], "references": [{"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1206.6392,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Anomaly detection: A survey", "author": ["Chandola", "Varun", "Banerjee", "Arindam", "Kumar", "Vipin"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["Fei-Fei", "Li", "Fergus", "Rob", "Perona", "Pietro"], "venue": "Comput. Vis. Image Underst.,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Hinton and Geoffrey.,? \\Q2010\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2010}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2005}, {"title": "Robust kernel density estimation", "author": ["Kim", "JooSeuk", "Scott", "Clayton D"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Regularized estimation of image statistics by score matching", "author": ["Kingma", "Diederik P", "Cun", "Yann L"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kingma et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2010}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Lecun", "Yann", "Bottou", "Lon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Unsupervised one-class learning for automatic outlier removal", "author": ["Liu", "Wei", "Hua", "Gang", "Smith", "John R"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Direct density ratio estimation with convolutional neural networks with application in outlier detection", "author": ["Nam", "Hyun Ha", "Sugiyama", "Masashi"], "venue": "IEICE Transactions,", "citeRegEx": "Nam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2015}, {"title": "Learning deep energy models", "author": ["Ngiam", "Jiquan", "Chen", "Zhenghao", "Koh", "Pang W", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "On estimation of a probability density function and mode", "author": ["Parzen", "Emanuel"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Parzen and Emanuel.,? \\Q1962\\E", "shortCiteRegEx": "Parzen and Emanuel.", "year": 1962}, {"title": "Cuave: A new audio-visual database for multimodal human-computer interface research", "author": ["E.K. Patterson", "S. Gurbuz", "Z. Tufekci", "J.N. Gowdy"], "venue": null, "citeRegEx": "Patterson et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2017}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["Sch\u00f6lkopf", "Bernhard", "Platt", "John C", "Shawe-Taylor", "Smola", "Alex J", "Williamson", "Robert C"], "venue": "Neural Comput.,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "One-class conditional random fields for sequential anomaly detection", "author": ["Song", "Yale", "Wen", "Zhen", "Lin", "Ching-Yung", "Davis", "Randall"], "venue": "In Proceedings of the TwentyThird International Joint Conference on Artificial Intelligence,", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Mining for Outliers in Sequential Databases", "author": ["Sun", "Pei", "Chawla", "Sanjay", "Arunasalam", "Bavani"], "venue": "SDM. SIAM,", "citeRegEx": "Sun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2006}, {"title": "On autoencoders and score matching for energy based models", "author": ["Swersky", "Kevin", "Buchman", "David", "Freitas", "Nando D", "Marlin", "Benjamin M"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "A connection between score matching and denoising autoencoders", "author": ["Vincent", "Pascal"], "venue": "Neural computation,", "citeRegEx": "Vincent and Pascal.,? \\Q2011\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2011}, {"title": "A comparative study of rnn for outlier detection in data mining", "author": ["Williams", "Graham J", "Baxter", "Rohan A", "He", "Hongxing", "Hawkins", "Simon", "Gu", "Lifang"], "venue": "In ICDM,", "citeRegEx": "Williams et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2002}, {"title": "A survey on unsupervised outlier detection in highdimensional numerical data", "author": ["Zimek", "Arthur", "Schubert", "Erich", "Kriegel", "Hans-Peter"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Zimek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zimek et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Anomaly detection (also called novelty or outlier detection) is to identify patterns that do not conform to the expected normal patterns (Chandola et al., 2009).", "startOffset": 137, "endOffset": 160}, {"referenceID": 22, "context": "A variety of methods can be found in the survey (Zimek et al., 2012).", "startOffset": 48, "endOffset": 68}, {"referenceID": 15, "context": "For example, it is shown that properly regularized autoencoders (Vincent et al., 2010; Rifai et al., 2011) are able to effectively characterize the data distribution and learn useful representations, which are not achieved by shallow methods such as PCA or K-Means.", "startOffset": 64, "endOffset": 106}, {"referenceID": 12, "context": "In this work, we focus on deep energy based models (Ngiam et al., 2011), where the energy function is composed of a deep neural network.", "startOffset": 51, "endOffset": 71}, {"referenceID": 8, "context": "Likewise, convolutional neural networks (CNNs) are significantly more efficient at modeling spatial structures (Krizhevsky et al., 2012), such as on images.", "startOffset": 111, "endOffset": 136}, {"referenceID": 12, "context": "As will be demonstrated below, this makes training a deep EBM with various underlying structures in an end-to-end fashion, without the need of resorting to sophisticated sampling procedures (Ngiam et al., 2011) or layer wise pretraining (Hinton, 2010).", "startOffset": 190, "endOffset": 210}, {"referenceID": 12, "context": ", 2006) as well as deep EBMs (Ngiam et al., 2011) as it makes our model applicable to a much wider spectrum of applications, including static data, sequential data and spatial data.", "startOffset": 29, "endOffset": 49}, {"referenceID": 12, "context": "This case is conceptually the same as the deep EBMs proposed in (Ngiam et al., 2011).", "startOffset": 64, "endOffset": 84}, {"referenceID": 0, "context": "Our formulation of recurrent EBMs is similar to (Boulanger-Lewandowski et al., 2012), where an EBM is built at each time step, with parameters determined by an underlying RNN.", "startOffset": 48, "endOffset": 84}, {"referenceID": 21, "context": "In the early work of (Williams et al., 2002), autoencoders (which they call replicator neural networks) have been applied to anomaly detection.", "startOffset": 21, "endOffset": 44}, {"referenceID": 16, "context": "We compare DSEBMs (with 2-layer fully connected energy function) with a variety of competing methods, including two reconstruction-based outlier detection methods, PCA and Kernel PCA, two density-based methods Kernel Density Estimator (KDE) (Parzen, 1962) and Robust Kernel Density Estimator (RKDE) (Kim & Scott, 2012), along with the traditional one-class learning method One-Class SVM (OC-SVM) (Sch\u00f6lkopf et al., 2001).", "startOffset": 396, "endOffset": 420}, {"referenceID": 21, "context": "We also include the method proposed in (Williams et al., 2002), named AutoEncoder Outlier Detection (AEOD) as one baseline.", "startOffset": 39, "endOffset": 62}, {"referenceID": 2, "context": "4 Caltech-101 (Fei-Fei et al., 2007) 101 300 \u00d7 200 9,146 NA 0.", "startOffset": 14, "endOffset": 36}, {"referenceID": 9, "context": "4 MNIST (Lecun et al., 1998) 10 28 \u00d7 28 70,000 NA 0.", "startOffset": 8, "endOffset": 28}, {"referenceID": 17, "context": "Also, we include two sequential methods: 1) HMMs, where the model is trained with the normal training sequences, and the posterior probability p(y|x) of each test sequence is computed as the normalized negative log-likelihood; 2) OCCRF (Song et al., 2013), where the model learns from a one-class dataset and captures the temporal dependence structure using conditional random fields (CRFs).", "startOffset": 236, "endOffset": 255}, {"referenceID": 10, "context": "We compare DSEBMs (with one convolutional layer + one pooling layer + one fully connected layer) with several baseline methods including: High-dimensional Robust PCA (HR-PCA), Kernel PCA (KPCA), Robust Kernel Density Estimator (RKDE), One-Class SVM (OC-SVM) and Unsupervised One-Class Learning (UOCL) (Liu et al., 2014).", "startOffset": 301, "endOffset": 319}, {"referenceID": 1, "context": "There has been a large body of work concentrating on anomaly detection (Chandola et al., 2009), noticeably: (1) the reconstruction based methods such as PCA and Kernel PCA, Robust PCA and Robust Kernel PCA; (2) the probability density based methods, including parametric estimators (Zimek et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 22, "context": ", 2009), noticeably: (1) the reconstruction based methods such as PCA and Kernel PCA, Robust PCA and Robust Kernel PCA; (2) the probability density based methods, including parametric estimators (Zimek et al., 2012) and nonparametric estimators such as the kernel density estimator (KDE) and the more recent robust kernel density estimator (RKDE); (3) methods of learning a compact data model such that as many as possible normal samples are enclosed inside, for example, oneclass SVM and SVDD.", "startOffset": 195, "endOffset": 215}, {"referenceID": 21, "context": "proposed a method based on autoencoder (Williams et al., 2002).", "startOffset": 39, "endOffset": 62}, {"referenceID": 18, "context": "proposes a technique that uses Probabilistic Suffix Trees (PST) to find the nearest neighbors for a given sequence to detect sequential anomalies in protein sequences (Sun et al., 2006).", "startOffset": 167, "endOffset": 185}, {"referenceID": 17, "context": "presents a one class conditional random fields method for general sequential anomaly detection tasks (Song et al., 2013).", "startOffset": 101, "endOffset": 120}, {"referenceID": 19, "context": "Methodology-wise, there is also a recent surge of training EBMs with score matching (Vincent, 2011; Swersky et al., 2011; Kingma & Cun, 2010).", "startOffset": 84, "endOffset": 141}], "year": 2017, "abstractText": "In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyv\u00e4rinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.", "creator": "LaTeX with hyperref package"}}}