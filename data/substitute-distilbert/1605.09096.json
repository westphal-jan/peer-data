{"id": "1605.09096", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "abstract": "understanding how words change their meanings before time is key to models of language and character evolution, no historical data on meaning is unknown, making definitions hard to develop truth test. word embeddings show promise as a diachronic tool, but have not been carefully interpreted. we develop a precise methodology for quantifying semantic tendencies by evaluating word embeddings ( ppmi, svd, word2vec ) against known historical changes. we then use this methodology to reveal statistical laws of semantic evolution. using six historical regions among four languages and two centuries, readers propose two quantitative laws of semantic phenomena : ( i ) the law having conformity - - - the equation of semantic change scales with an important power - law of phonetic frequency ; ( ii ) the law of innovation - - - property of frequency, words that are more polysemous have higher rates of semantic change.", "histories": [["v1", "Mon, 30 May 2016 03:54:18 GMT  (379kb,D)", "http://arxiv.org/abs/1605.09096v1", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016 (this http URL) under working-title \"Diachronic word embeddings reveal laws of semantic change\""], ["v2", "Tue, 7 Jun 2016 20:24:30 GMT  (629kb,D)", "http://arxiv.org/abs/1605.09096v2", "Accepted for publication at the Association for Computational Linguistics (ACL), 2016 (this http URL)"], ["v3", "Mon, 8 Aug 2016 09:36:39 GMT  (441kb,D)", "http://arxiv.org/abs/1605.09096v3", "Association for Computational Linguistics (ACL), 2016 (this http URL)"], ["v4", "Mon, 26 Sep 2016 23:35:05 GMT  (441kb,D)", "http://arxiv.org/abs/1605.09096v4", "Association for Computational Linguistics (ACL), 2016"]], "COMMENTS": "Accepted for publication at the Association for Computational Linguistics (ACL), 2016 (this http URL) under working-title \"Diachronic word embeddings reveal laws of semantic change\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["william l hamilton", "jure leskovec", "dan jurafsky"], "accepted": true, "id": "1605.09096"}, "pdf": {"name": "1605.09096.pdf", "metadata": {"source": "CRF", "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "authors": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky"], "emails": ["wleif@stanford.edu", "jure@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Shifts in word meaning exhibit systematic regularities (Bre\u0301al, 1897; Ullmann, 1962). The rate of semantic change, for example, is higher in some words than others (Blank, 1999) \u2014 compare the stable semantic history of cat (from ProtoGermanic kattuz, \u201ccat\u201d) to the varied meanings of English cast: \u201cto mould\u201d, \u201ca collection of actors\u2019, \u201ca hardened bandage\u201d, etc. (all from Old Norse kasta, \u201cto throw\u201d, Simpson et al., 1989).\nVarious hypotheses have been offered about such regularities in semantic change, such as an increasing subjectification of meaning, or the grammaticalization of inferences (e.g., Geeraerts, 1997; Blank, 1999; Traugott and Dasher, 2001).\nBut many core questions about semantic change remain unanswered. One is the role of frequency. Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007). What is the role of word frequency in meaning change?\nAnother unanswered question is the relationship between semantic change and polysemy. Words gain senses over time as they semantically drift (Bre\u0301al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al., 2006) and rates of L2 learning (Crossley et al., 2010). But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015). Furthermore, polysemy is strongly correlated with frequency\u2014high frequency words have more senses (Zipf, 1945; I\u0307lgen and Karaoglan, 2007)\u2014so understanding how polysemy relates to semantic change requires controling for word frequency.\nAnswering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Bre\u0301al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001). One promising avenue is the use of distributional semantics, in which words are embedded in vector spaces according to their co-occurrence relationships (Bullinaria and Levy, 2007; Turney and Pantel, 2010), and the embeddings of words\n1We use \u2018polysemy\u2019 here to refer to related senses as well as rarer cases of accidental homonymy.\nar X\niv :1\n60 5.\n09 09\n6v 1\n[ cs\n.C L\n] 3\n0 M\nay 2\n01 6\nare then compared across time-periods. This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al., 2014) as well as to test a few specific hypotheses, such as whether English synonyms tend to change meaning in similar ways (Xu and Kemp, 2015). However, these works employ widely different embedding approaches and test their approaches only on English.\nIn this work, we develop a robust methodology for quantifying semantic change using embeddings by comparing state-of-the-art approaches (PPMI, SVD, word2vec) on novel benchmarks.\nWe then apply this methodology in a large-scale cross-linguistic analysis using 6 corpora spanning 200 years and 4 languages (English, German, French, and Chinese). Based on this analysis, we propose two statistical laws relating frequency and polysemy to semantic change: \u2022 The law of conformity: Rates of semantic\nchange scale with a negative power of word frequency. \u2022 The law of innovation: After controlling for\nfrequency, polysemous words have significantly higher rates of semantic change."}, {"heading": "2 Diachronic embedding methods", "text": "The following sections outline how we construct diachronic (historical) word embeddings, by first constructing embeddings in each time-period and then aligning them over time, and the metrics that we use to quantify semantic change.2\n2 github.com/williamleif/historical-embeddings"}, {"heading": "2.1 Embedding algorithms", "text": "We use three methods to construct word embeddings within each time-period: PPMI, SVD, and SGNS (i.e., word2vec).3 These distributional methods represent each word wi by a vector wi that captures information about its co-occurrence statistics. These methods operationalize the \u2018distributional hypothesis\u2019 that word semantics are implicit in co-occurrence relationships (Harris, 1954; Firth, 1957). The semantic similarity/distance between two words is approximated by the cosine similarity/distance between their vectors (Turney and Pantel, 2010)."}, {"heading": "2.1.1 PPMI", "text": "In the PPMI representations, the vector embedding for word wi \u2208 V contains the positive point-wise mutual information (PPMI) values betweenwi and a large set of pre-specified \u2018context\u2019 words. The word vectors correspond to the rows of the matrix MPPMI \u2208 R|V|\u00d7|VC | with entries given by\nMPPMIi,j = max\n{ log ( p\u0302(wi, cj)\np\u0302(w)p\u0302(cj)\n) \u2212 \u03b1, 0 } ,\n(1) where cj \u2208 VC is a context word and \u03b1 > 0 is a negative prior, which provides a smoothing bias (Levy et al., 2015). The p\u0302 correspond to the smoothed empirical probabilities of word (co-)occurrences within fixed-size sliding windows of text. Clipping the PPMI values above zero ensures they remain finite and has been shown to dramatically improve results (Bullinaria and Levy, 2007; Levy et al., 2015); intuitively, this clipping ensures that the representations emphasize positive word-word correlations over negative ones.\n3Synchronic applications of these three methods are reviewed in detail in Levy et al. (2015)."}, {"heading": "2.1.2 SVD", "text": "SVD embeddings correspond to low-dimensional approximations of the PPMI embeddings learned via singular value decomposition (Levy et al., 2015). The vector embedding for word wi is given by\nwSVDi = (U\u03a3 \u03b3)i , (2)\nwhere MPPMI = U\u03a3V> is the truncated singular value decomposition of MPPMI and \u03b3 \u2208 [0, 1] is an eigenvalue weighting parameter. Setting \u03b3 < 1 has been shown to dramatically improve embedding qualities (Turney and Pantel, 2010; Bullinaria and Levy, 2012). This SVD approach can be viewed as a generalization of Latent Semantic Analysis (Landauer and Dumais, 1997), where the term-document matrix is replaced with MPPMI. Compared to PPMI, SVD representations can be more robust, as the dimensionality reduction acts as a form of regularization."}, {"heading": "2.1.3 Skip-gram with negative sampling", "text": "SGNS \u2018neural\u2019 embeddings are optimized to predict co-occurrence relationships using an approximate objective known as \u2018skip-gram with negative sampling\u2019 (Mikolov et al., 2013). In SGNS, each word wi is represented by two dense, lowdimensional vectors: a word vector (wSGNSi ) and context vector (cSGNSi ). These embeddings are optimized via stochastic gradient descent so that\np\u0302(ci|wi) \u221d exp(wSGNSi \u00b7 cSGNSj ), (3)\nwhere p(ci|wi) is the empirical probability of seeing context word ci within a fixed-length window of text, given that this window contains wi. The SGNS optimization avoids computing the normalizing constant in (3) by randomly drawing \u2018negative\u2019 context words, cn, for each target word and ensuring that exp(wSGNSi \u00b7cSGNSn ) is small for these examples.\nSGNS has the benefit of allowing incremental initialization during learning, where the embed-\ndings for time t are initialized with the embeddings from time t \u2212 \u2206 (Kim et al., 2014). We employ this trick here, though we found that it had a negligible impact on our results."}, {"heading": "2.2 Datasets, pre-processing, and hyperparameters", "text": "We trained models on the 6 datasets described in Table 1, taken from Google N-Grams (Lin et al., 2012) and the COHA corpus (Davies, 2010). The Google N-Gram datasets are extremely large (comprising\u22486% of all books ever published), but they also contain many corpus artifacts due, e.g., to shifting sampling biases over time (Pechenick et al., 2015). In contrast, the COHA corpus was carefully selected to be genre-balanced and representative of American English over the last 200 years, though as a result it is two orders of magnitude smaller. The COHA corpus also contains pre-extracted word lemmas, which we used to validate that our results hold at both the lemma and raw token levels. All the datasets were aggregated to the granularity of decades.4\nWe follow the recommendations of Levy et al. (2015) in setting the hyperparameters for the embedding methods, though preliminary experiments were used to tune key settings. For all methods, we used symmetric context windows of size 4 (on each side). For SGNS and SVD, we use embeddings of size 300. See Appendix A for further implementation and pre-processing details."}, {"heading": "2.3 Aligning historical embeddings", "text": "In order to compare word vectors from different time-periods we must ensure that the vectors are aligned to the same coordinate axes. Explicit PPMI vectors are naturally aligned, as each column simply corresponds to a context word. Low-dimensional embeddings will not be naturally aligned due to the non-unique nature of the\n4The 2000s decade of the Google data was discarded due to shifts in the sampling methodology (Michel et al., 2011).\nSVD and the stochastic nature of SGNS. In particular, both these methods may result in arbitrary orthogonal transformations, which do not affect pairwise cosine-similarities within-years but will preclude comparison of the same word across time.5\nWe use orthogonal Procrustes to align the learned low-dimensional embeddings. Defining W(t) \u2208 Rd\u00d7|V| as the matrix of word embeddings learned at year t, we align across time-periods while preserving cosine similarities by optimizing:\nR(t) = arg min Q>Q=I \u2016W(t)Q\u2212W(t+1)\u2016F , (4)\nwith R(t) \u2208 Rd\u00d7d. The solution corresponds to the best rotational alignment and can be obtained efficiently using an application of SVD (Scho\u0308nemann, 1966)."}, {"heading": "2.4 Time-series from historical embeddings", "text": "Diachronic word embeddings can be used in two ways to quantify semantic change: (i) we can measure changes in pair-wise word similarities over time, or (ii) we can measure how an individual word\u2019s embedding shifts over time.\nPair-wise similarity time-series Measuring how the cosine-similarity between pairs of words changes over time allows us to test hypotheses about specific linguistic or cultural shifts in a controlled manner. We quantify shifts by computing the similarity time-series\ns(t)(wi, wj) = cos-sim(w (t) i ,w (t) j ) (5)\nbetween two words wi and wj over a time-period (t, ..., t + \u2206). We then measure the Spearman correlation (\u03c1) of this series against time, which allows us to assess the magnitude and significance of pairwise similarity shifts; since the Spearman correlation is non-parametric, this measure essentially detects whether the similarity series increased/decreased over time in a significant manner, regardless of the \u2018shape\u2019 of this curve.6\nMeasuring semantic displacement After aligning the embeddings for individual timeperiods, we can use the aligned word vectors to\n5Previous work circumvented this problem by either avoiding low-dimensional embeddings (e.g., Gulordava and Baroni, 2011; Jatowt and Duh, 2014) or by performing heuristic local alignments per word (Kulkarni et al., 2014).\n6Other metrics or change-point detection approaches, e.g. mean shifts (Kulkarni et al., 2014) could also be used.\ncompute the semantic displacement that a word has undergone during a certain time-period. In particular, we can directly compute the cosinedistance between a word\u2019s representation for different time-periods, i.e. cos-dist(wt,wt+\u2206), as a measure of semantic change. We can also use this measure to quantify \u2018rates\u2019 of semantic change for different words by looking at the displacement between consecutive time-points."}, {"heading": "3 Comparison of different approaches", "text": "We compare the different distributional approaches on a set of benchmarks designed to test their scientific utility. We evaluate both their synchronic accuracy (i.e., ability to capture word similarity within individual time-periods) and their diachronic validity (i.e., ability to quantify semantic changes over time)."}, {"heading": "3.1 Synchronic Accuracy", "text": "We evaluated the synchronic (within-time-period) accuracy of the methods using a standard modern benchmark and the 1990s portion of the ENGALL data. On Bruni et al. (2012)\u2019s MEN similarity task of matching human judgments of word similarities, SVD performed best (\u03c1 = 0.739), followed by PPMI (\u03c1 = 0.687) and SGNS (\u03c1 = 0.649). These results echo the findings of Levy et al. (2015), where they found SVD to perform best on similarity tasks while SGNS performed best on analogy tasks (which are not the focus of this work)."}, {"heading": "3.2 Diachronic Validity", "text": "We evaluate the diachronic validity of the methods on two historical semantic tasks.\nDetecting known shifts. First, we tested whether the methods capture known historical shifts in meaning. The goal in this task is for the methods to correctly capture whether pairs of words moved closer or further apart in semantic space during a pre-determined time-period. We use a set of independently attested shifts as an evaluation set (Table 2). For comparison, we evaluated the methods on both the large (but messy) ENGALL data and the smaller (but clean) COHA data. On this task, all the methods performed almost perfectly in terms of capturing the correct directionality of the shifts (i.e., the pairwise similarity series have the correct sign on their Spearman correlation with time), but there were\nsome differences in whether the methods deemed the shifts statistically significant at the p < 0.05 level.7 Overall, SGNS performed the best on the full English data, but its performance dropped significantly on the smaller COHA dataset, where SVD performed best. PPMI was noticeably worse than the other two approaches (Table 3).\nDiscovering shifts from data. We tested whether the methods discover reasonable shifts by examining the top-10 words that changed the most from the 1900s to the 1990s according to the semantic displacement metric introduced in Section 2.4 (limiting our analysis to words with relative frequencies above 10\u22125 in both decades). We used the ENGFIC data as the most-changed list for ENGALL was dominated by scientific terms due to changes in the corpus sample.\nTable 4 shows the top-10 words discovered by each method. SGNS performed by far the best on this task, with 70% of its top-10 list corresponding to genuine semantic shifts, followed by 40% for SVD, and 10% for PPMI. However, a large portion of the discovered words for PPMI (and less so SVD) correspond to borderline cases, e.g. know,\n7All subsequent significance tests are at p < 0.05.\nthat have not necessarily shifted significantly in meaning but that occur in different contexts due to global genre/discourse shifts. The poor quality of the nearest neighbors generated by the PPMI algorithm\u2014which are skewed by PPMI\u2019s sensitivity to rare events\u2014also made it difficult to assess the quality of its discovered shifts. SVD was the most sensitive to corpus artifacts (e.g., co-occurrences due to cover pages and advertisements), but it still captured a number of genuine semantic shifts.\nTable 5 details representative example shifts in English, French, and German. Chinese lacks sufficient historical data for this task, as only years 1950-1999 are usable; however, we do still see some significant changes for Chinese in this short time-period, such as \u75c5\u6bd2 (\u201cvirus\u201d) moving closer to\u7535\u8111 (\u201ccomputer\u201d, \u03c1 = 0.89)."}, {"heading": "3.3 Methodological recommendations", "text": "PPMI is clearly worse than the other two methods; it performs poorly on all the benchmark tasks, is extremely sensitive to rare events, and is prone to false discoveries from global genre shifts. Between SVD and SGNS the results are somewhat equivocal, as both perform best on two out of the four tasks (synchronic accuracy, ENGALL detection, COHA detection, discovery). Overall, SVD performs best on the synchronic accuracy task and has higher average accuracy on the \u2018detection\u2019 task, while SGNS performs best on the \u2018discovery\u2019 task. These results suggest that both these methods are reasonable choices for studies of semantic change but that they each have their own tradeoffs: SVD is more sensitive, as it performs well on detection tasks even when using a small dataset, but this sensitivity also results in false discoveries due to corpus artifacts. In contrast, SGNS\nis robust to corpus artifacts in the discovery task, but it is not sensitive enough to perform well on the detection task with a small dataset. Qualitatively, we found SGNS to be most useful for discovering new shifts and visualizing changes (e.g., Figure 1), while SVD was most effective for detecting subtle shifts in usage."}, {"heading": "4 Statistical laws of semantic change", "text": "We now show how diachronic embeddings can be used in a large-scale cross-linguistic analysis to reveal statistical laws that relate frequency and polysemy to semantic change. In particular, we analyze how a word\u2019s rate of semantic change,\n\u2206(t)(wi) = cos-dist(w (t) i ,w (t+1) i ) (6)\ndepends on its frequency, f (t)(wi) and a measure of its polysemy, d(t)(wi) (defined in Section 4.4)."}, {"heading": "4.1 Setup", "text": "We present results using SVD embeddings (though analogous results were found to hold with SGNS). Using all four languages and all four conditions for English (ENGALL, ENGFIC, and COHA with and without lemmatization), we performed regression analysis on rates of semantic change, \u2206(t)(wi); thus, we examined one data-point per word for each pair of consecutive\ndecades and analyzed how a word\u2019s frequency and polysemy at time t correlate with its degree of semantic displacement over the next decade. To ensure the robustness of our results, we analyzed only the top-10000 non\u2013stop words by average historical frequency (lower-frequency words tend to lack sufficient co-occurrence data across years) and we discarded proper nouns (changes in proper noun usage are primarily driven by nonlinguistic factors, e.g. historical events, Traugott and Dasher, 2001). We also log-transformed the semantic displacement scores and normalized the scores to have zero mean and unit variance; we denote these normalized scores by \u2206\u0303(t)(wi).\nWe performed our analysis using a linear mixed model with random intercepts per word and fixed effects per decade; i.e., we fit \u03b2f , \u03b2d, and \u03b2t s.t.\n\u2206\u0303(t)(wi) = \u03b2f log ( f (t)(wi) ) +\u03b2d log ( d(t)(wi) ) + \u03b2t + zwi + (t) wi \u2200wi \u2208 V, t \u2208 {t0, ..., tn}, (7)\nwhere zwi \u223c N (0, \u03c3wi) is the random intercept for word wi and (t) wi \u2208 N (0, \u03c3) is an error term. \u03b2f , \u03b2d and \u03b2t correspond to the fixed effects for frequency, polysemy and the decade t, respectively8. Intuitively, this model estimates the effects\n8Note that time is treated as a categorical variable, as each decade has its own fixed effect.\nof frequency and polysemy on semantic change, while controlling for temporal trends and correcting for the fact that measurements on same word will be correlated across time. We fit (7) using the standard restricted maximum likelihood algorithm (McCulloch and Neuhaus, 2001; Appendix C)."}, {"heading": "4.2 Overview of results", "text": "We find that, across languages, rates of semantic change obey a scaling relation of the form\n\u2206(wi) \u221d f(wi)\u03b2f \u00d7 d(wi)\u03b2d , (8)\nwith \u03b2f < 0 and \u03b2d > 0. This finding implies that frequent words change at slower rates while polysemous words change faster, and that both these relations scale as power laws."}, {"heading": "4.3 Law of conformity: Frequently used words change at slower rates", "text": "Using the model in equation (7), we found that the logarithm of a word\u2019s frequency, log(f(wi)), has a significant and substantial negative effect on rates of semantic change in all settings (Figures 2a and 3a). Given the use of log-transforms in preprocessing the data this implies rates of semantic change are proportional to a negative power (\u03b2f ) of frequency, i.e.\n\u2206(wi) \u221d f(wi)\u03b2f , (9)\nwith \u03b2f \u2208 [\u22121.4,\u22120.3] across languages/datasets. The relatively large range of values for \u03b2f is driven by the fact that the COHA datasets are outliers due to their relatively small sample sizes (Figure 3)."}, {"heading": "4.4 Law of innovation: Polysemous words change at faster rates", "text": "We quantify the polysemy of a word by measuring the local clustering coefficient of its co-occurrence network (Watts and Strogatz, 1998)9, i.e.\nd(wi) = \u2212 \u2211\nci,cj\u2208NPPMI(wi) I {PPMI(ci, cj) > 0} |NPPMI(wi)|(|NPPMI(wi)| \u2212 1) ,\nwhere NPPMI(wi) = {wj : PPMI(wi, wj) > 0}. According to this metric, a word will have a high clustering coefficient (and thus a low polysemy score) if the words that it cooccurs with also tend to cooccur with each other. This metric and its relatives are often used in word-sense discrimination and correlate with, e.g., number of senses in WordNet (Dorow and Widdows, 2003; Ferret, 2004); however, it does have a slight bias towards rating contextually-diverse discourse function words (e.g., also) as highly polysemous. Table 6 gives examples of the least and most polysemous words in the ENGFIC data, according to\n9Since raw co-occurrence networks are relatively dense and noisy, we compute the local clustering coefficients on the network induced by the PPMI metric.\nthis score. As expected, this measure has significant intrinsic positive correlation with frequency (0.45 < r < 0.8, p < 0.05, across datasets)\u2014i.e., frequent words tend to be used in a greater diversity of contexts \u2014 so we interpret the effect of this measure only after controlling for frequency; this control is naturally captured in equation (7).\nAfter fitting the model in equation (7), we found that the logarithm of the polysemy score exhibits a strong positive effect on rates of semantic change, throughout all four languages (Figure 3b). As with frequency, the relation takes the form of a power law\n\u2206(wi) \u221d d(wi)\u03b2d , (10)\nwith a language/corpus dependent scaling constant in \u03b2d \u2208 [0.2, 0.8]. Note that this relationship is a complete reversal from what one would expect according to d(wi)\u2019s positive correlation with frequency. Figure 2b shows the relationship of polysemy with rates of semantic change in the ENGALL data after regressing out effect of frequency (using the method of Graham, 2003)."}, {"heading": "5 Discussion", "text": "We show how distributional methods can reveal statistical laws of semantic change and offer a robust methodology for future work in this area.\nThe two statistical laws we propose have strong implications for future work in historical semantics. The law of conformity\u2014frequent words change more slowly\u2014clarifies frequency\u2019s role in semantic change. Future studies of semantic change must account for frequency\u2019s conforming effect: when examining the interaction between some linguistic process and semantic change, the law of conformity should serve as a null model in which the interaction is driven primarily by underlying frequency effects. The\nlaw of innovation\u2014polysemous words change more quickly\u2014quantifies the central role polysemy plays in semantic change, an issue that has concerned linguists for more than 100 years (Bre\u0301al, 1897). Previous works argued that semantic change leads to polysemy (Wilkins, 1993; Hopper and Traugott, 2003), but our results suggest that polysemy may actually play a causal role in inducing semantic change. Overall, these two factors explain between 48% and 88% of the variance10 in rates of semantic change (across conditions). This remarkable degree of explanatory power indicates that frequency and polysemy are perhaps the two most crucial linguistic factors that explain rates of semantic change over time.\nThese empirical statistical laws also lend themselves to various causal mechanisms. The law of conformity might be a consequence of learning: perhaps people are more likely to use rare words mistakenly in novel ways, a mechanism formalizable by Bayesian models of word learning and corresponding to the biological notion of genetic drift (Reali and Griffiths, 2010). Or perhaps a sociocultural conformity bias makes people less likely to accept novel innovations of common words, a mechanism analogous to the biological process of purifying selection (Boyd and Richerson, 1988; Pagel et al., 2007). Moreover, such mechanisms may also be partially responsible for the law of innovation. Highly polysemous words tend to have more rare senses (Kilgarriff, 2004), and rare senses may be unstable by the law of conformity. While our results cannot confirm such causal links, they nonetheless highlight a new role for frequency and polysemy in language change and the importance of distributional models in historical research.\n10Marginal R2 (Nakagawa and Schielzeth, 2013)."}, {"heading": "A Hyperparameter and pre-processing details", "text": "For all datasets, words were lowercased and stripped of punctuation. For the Google datasets we built models using the top-100000 words by their average frequency over the entire historical time-periods, and we used the top-50000 for COHA. During model learning we also discarded all words within a year that occurred below a certain threshold (500 for the Google data, 100 for the COHA data).\nFor all methods, we used the hyperparameters recommended in Levy et al. (2015). For the context word distributions in all methods, we used context distribution smoothing with a smoothing parameter of 0.75. Note that for SGNS this corresponds to smoothing the unigram negative sampling distribution. For both, SGNS and PPMI, we set the negative sample prior \u03b1 = log(5), while we set this value to \u03b1 = 0 for SVD, as this improved results. When using SGNS on the Google data, we also subsampled, with words being random removed with probability pr(wi) = 1 \u2212 \u221a 10\u22125\nf(wi) , as\nrecommended by Levy et al. (2015) and Mikolov et al. (2013). Furthermore, to improve the computational efficiency of SGNS (which works with text streams and not co-occurrence counts), we downsampled the larger years in the Google NGram data to have at most 109 tokens. No such subsampling was performed on the COHA data.\nFor all methods, we defined the context set to simply be the same vocabulary as the target words, as is standard in most word vector applications (Levy et al., 2015). However, we found that the PPMI method benefited substantially from larger contexts (similar results were found in Bullinaria and Levy, 2007), so we did not remove any lowfrequency words per year from the context for that method. The other embedding approaches did not appear to benefit from the inclusion of these lowfrequency terms, so they were dropped for computational efficiency.\nFor SGNS, we used the implementation provided in Levy et al. (2015). The implementations for PPMI and SVD are released with the code package associated with this work.\nB Visualization algorithm\nTo visualize semantic change for a word wi in two dimensions we employed the following procedure, which relies on the t-SNE embedding method (Van der Maaten and Hinton, 2008) as a subroutine:\n1. Find the union of the word wi\u2019s k nearest neighbors over all necessary time-points.\n2. Compute the t-SNE embedding of these words on the most recent (i.e., the modern) time-point.\n3. For each of the previous time-points, hold all embeddings fixed, except for the target word\u2019s (i.e., the embedding for wi), and optimize a new t-SNE embedding only for the target word. We found that initializing the embedding for the target word to be the centroid of its k\u2032-nearest neighbors in a timepoint was highly effective.\nThus, in this procedure the background words are always shown in their \u201cmodern\u201d positions, which makes sense given that these are the current meanings of these words. This approximation is necessary, since in reality all words are moving. Code for this procedure is included in the paper\u2019s release."}, {"heading": "C Regression analysis details", "text": "In addition to the pre-processing mentioned in the main text, we also normalized the contextual diversity scores d(wi) within years by subtracting the yearly median. This was necessary because there was substantial changes in the median contextual diversity scores over years due to changes in corpus sample sizes etc. Data points corresponding to words that occurred less than 1000 times during a time-period were also discarded, as these points lack sufficient data to robustly estimate change rates (this threshold only came into effect on the COHA data, however). We removed stop words and proper nouns by (i) removing all stop-words from the available lists in Python\u2019s NLTK package (Bird et al., 2009) and (ii) restricting our analysis to words with part-of-speech (POS) tags corresponding to four main linguistic categories (common nouns, verbs, adverbs, and adjectives), using the POS sources in Table 1.\nWhen analyzing the effects of frequency and contextual diversity, the model contained fixed effects for these features and for time along with random effects for word identity. We opted not to control for POS tags in the presented results, as contextual diversity is co-linear with these tags (e.g., adverbs are more contextual diverse than nouns), and the goal was to demonstrate the main effect of contextual diversity across all word types. That said, the effect of contextual diversity remained strong and significantly positive in all datasets even after controlling for POS tags.\nTo fit the linear mixed models, we used the Python statsmodels package with restricted maximum likelihood estimation (REML) (Seabold and Perktold, 2010). All mentioned significance scores were computed according to Wald\u2019s z-tests, though these results agreed with Bonferroni corrected likelihood ratio tests on the eng-all data.\nThe visualizations in Figure 2 were computed on the eng-all data and correspond to bootstrapped locally-linear kernel regressions with bandwidths selected via the AIC Hurvitch criteria (Li and Racine, 2007)."}], "references": [{"title": "Contextual diversity, not word frequency, determines word-naming and lexical decision times", "author": ["James S. Adelman", "Gordon D.A. Brown", "Jos\u00e9 F. Quesada."], "venue": "Psychological Science, 17(9):814\u2013823.", "citeRegEx": "Adelman et al\\.,? 2006", "shortCiteRegEx": "Adelman et al\\.", "year": 2006}, {"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, Inc.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Why do new meanings occur? A cognitive typology of the motivations for lexical semantic change", "author": ["Andreas Blank."], "venue": "Peter Koch and Andreas Blank, editors, Historical Semantics and Cognition. Walter de Gruyter, Berlin, Germany.", "citeRegEx": "Blank.,? 1999", "shortCiteRegEx": "Blank.", "year": 1999}, {"title": "Culture and the Evolutionary Process", "author": ["Robert Boyd", "Peter J Richerson."], "venue": "University of Chicago Press, Chicago, IL.", "citeRegEx": "Boyd and Richerson.,? 1988", "shortCiteRegEx": "Boyd and Richerson.", "year": 1988}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136\u2013145. Asso-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Essai de S\u00e9mantique: Science des significations", "author": ["Michel Br\u00e9al."], "venue": "Hachette, Paris, France.", "citeRegEx": "Br\u00e9al.,? 1897", "shortCiteRegEx": "Br\u00e9al.", "year": 1897}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behav. Res. Methods, 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: stop-lists, stemming, and SVD", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior Research Methods, 44(3):890\u2013907, September.", "citeRegEx": "Bullinaria and Levy.,? 2012", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "Frequency of Use And the Organization of Language", "author": ["J.L. Bybee."], "venue": "Oxford University Press, New York City, NY.", "citeRegEx": "Bybee.,? 2007", "shortCiteRegEx": "Bybee.", "year": 2007}, {"title": "The development of polysemy and frequency use in english second language speakers", "author": ["Scott Crossley", "Tom Salsbury", "Danielle McNamara."], "venue": "Language Learning, 60(3):573\u2013605.", "citeRegEx": "Crossley et al\\.,? 2010", "shortCiteRegEx": "Crossley et al\\.", "year": 2010}, {"title": "The Corpus of Historical American English: 400 million words, 1810-2009", "author": ["Mark Davies"], "venue": null, "citeRegEx": "Davies.,? \\Q2010\\E", "shortCiteRegEx": "Davies.", "year": 2010}, {"title": "Discovering corpus-specific word senses", "author": ["Beate Dorow", "Dominic Widdows."], "venue": "Proc. 10th EACL Conf., pages 79\u201382. Association for Computational Linguistics.", "citeRegEx": "Dorow and Widdows.,? 2003", "shortCiteRegEx": "Dorow and Widdows.", "year": 2003}, {"title": "Discovering word senses from a network of lexical cooccurrences", "author": ["Olivier Ferret."], "venue": "Proc. CoLing 2004, page 1326. Association for Computational Linguistics.", "citeRegEx": "Ferret.,? 2004", "shortCiteRegEx": "Ferret.", "year": 2004}, {"title": "A Synopsis of Linguistic Theory, 1930-1955", "author": ["J.R. Firth."], "venue": "Studies in Linguistic Analysis. Special volume of the Philological Society. Basil Blackwell, Oxford, UK.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Diachronic Prototype Semantics: A Contribution to Historical Lexicology", "author": ["Dirk Geeraerts."], "venue": "Clarendon Press, Oxford, UK.", "citeRegEx": "Geeraerts.,? 1997", "shortCiteRegEx": "Geeraerts.", "year": 1997}, {"title": "Confronting multicollinearity in ecological multiple regression", "author": ["Michael H Graham."], "venue": "Ecology, 84(11):2809\u20132815.", "citeRegEx": "Graham.,? 2003", "shortCiteRegEx": "Graham.", "year": 2003}, {"title": "A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus", "author": ["Kristina Gulordava", "Marco Baroni."], "venue": "Proc. GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, pages", "citeRegEx": "Gulordava and Baroni.,? 2011", "shortCiteRegEx": "Gulordava and Baroni.", "year": 2011}, {"title": "Distributional structure", "author": ["Zellig S. Harris."], "venue": "Word, 10:146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Grammaticalization", "author": ["Paul J Hopper", "Elizabeth Closs Traugott."], "venue": "Cambridge University Press, Cambridge, UK.", "citeRegEx": "Hopper and Traugott.,? 2003", "shortCiteRegEx": "Hopper and Traugott.", "year": 2003}, {"title": "A framework for analyzing semantic change of words across time", "author": ["Adam Jatowt", "Kevin Duh."], "venue": "Proc. 14th ACM/IEEE-CS Conf. on Digital Libraries, pages 229\u2013238. IEEE Press.", "citeRegEx": "Jatowt and Duh.,? 2014", "shortCiteRegEx": "Jatowt and Duh.", "year": 2014}, {"title": "Principles and Methods for Historical Linguistics", "author": ["R. Jeffers", "Ilse Lehiste."], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Jeffers and Lehiste.,? 1979", "shortCiteRegEx": "Jeffers and Lehiste.", "year": 1979}, {"title": "How dominant is the commonest sense of a word? In Text, Speech and Dialogue, pages 103\u2013111", "author": ["Adam Kilgarriff."], "venue": "Springer.", "citeRegEx": "Kilgarriff.,? 2004", "shortCiteRegEx": "Kilgarriff.", "year": 2004}, {"title": "Temporal analysis of language through neural language models", "author": ["Yoon Kim", "Yi-I. Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov."], "venue": "arXiv preprint arXiv:1405.3515.", "citeRegEx": "Kim et al\\.,? 2014", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Statistically significant detection of linguistic change", "author": ["Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proc. 24th WWW Conf., pages 625\u2013635. International World Wide Web Conferences Steering Committee.", "citeRegEx": "Kulkarni et al\\.,? 2014", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2014}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais."], "venue": "Psychol. Rev., 104(2):211.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Trans. Assoc. Comput. Ling., 3.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Nonparametric econometrics: theory and practice", "author": ["Qi Li", "Jeffrey Scott Racine."], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Li and Racine.,? 2007", "shortCiteRegEx": "Li and Racine.", "year": 2007}, {"title": "Quantifying the evolutionary dynamics of language", "author": ["Erez Lieberman", "Jean-Baptiste Michel", "Joe Jackson", "Tina Tang", "Martin A. Nowak."], "venue": "Nature, 449(7163):713\u2013716.", "citeRegEx": "Lieberman et al\\.,? 2007", "shortCiteRegEx": "Lieberman et al\\.", "year": 2007}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Yuri Lin", "Jean-Baptiste Michel", "Erez Lieberman Aiden", "Jon Orwant", "Will Brockman", "Slav Petrov."], "venue": "Proceedings of the ACL 2012 system demonstrations, pages 169\u2013174. Association for", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Generalized linear mixed models", "author": ["Charles E McCulloch", "John M Neuhaus."], "venue": "WileyInterscience, Hoboken, NJ.", "citeRegEx": "McCulloch and Neuhaus.,? 2001", "shortCiteRegEx": "McCulloch and Neuhaus.", "year": 2001}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A general and simple method for obtaining R 2 from generalized linear mixed-effects models", "author": ["Shinichi Nakagawa", "Holger Schielzeth."], "venue": "Methods Ecol. Evol., 4(2):133\u2013142.", "citeRegEx": "Nakagawa and Schielzeth.,? 2013", "shortCiteRegEx": "Nakagawa and Schielzeth.", "year": 2013}, {"title": "Frequency of word-use predicts rates of lexical evolution throughout Indo-European history", "author": ["Mark Pagel", "Quentin D. Atkinson", "Andrew Meade."], "venue": "Nature, 449(7163):717\u2013720.", "citeRegEx": "Pagel et al\\.,? 2007", "shortCiteRegEx": "Pagel et al\\.", "year": 2007}, {"title": "Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution", "author": ["Eitan Adam Pechenick", "Christopher M. Danforth", "Peter Sheridan Dodds."], "venue": "PLoS ONE, 10(10).", "citeRegEx": "Pechenick et al\\.,? 2015", "shortCiteRegEx": "Pechenick et al\\.", "year": 2015}, {"title": "Words as alleles: connecting language evolution with Bayesian learners to models of genetic drift", "author": ["F. Reali", "T.L. Griffiths."], "venue": "Proc. R. Soc. B, 277(1680):429\u2013436.", "citeRegEx": "Reali and Griffiths.,? 2010", "shortCiteRegEx": "Reali and Griffiths.", "year": 2010}, {"title": "Tracing semantic change with latent semantic analysis", "author": ["Eyal Sagi", "Stefan Kaufmann", "Brady Clark."], "venue": "Kathryn Allan and Justyna A. Robinson, editors, Current Methods in Historical Semantics, page 161. De Gruyter Mouton, Berlin, Germany.", "citeRegEx": "Sagi et al\\.,? 2011", "shortCiteRegEx": "Sagi et al\\.", "year": 2011}, {"title": "The Lefff 2 syntactic lexicon for French: architecture, acquisition, use", "author": ["Beno\u02c6it Sagot", "Lionel Cl\u00e9ment", "Eric de La Clergerie", "Pierre Boullier."], "venue": "LREC 06, pages 1\u20134.", "citeRegEx": "Sagot et al\\.,? 2006", "shortCiteRegEx": "Sagot et al\\.", "year": 2006}, {"title": "Adding manual constraints and lexical look-up to a Brilltagger for German", "author": ["Gerold Schneider", "Martin Volk."], "venue": "Proceedings of the ESSLLI98 Workshop on Recent Advances in Corpus Annotation, Saarbr\u00fccken.", "citeRegEx": "Schneider and Volk.,? 1998", "shortCiteRegEx": "Schneider and Volk.", "year": 1998}, {"title": "A generalized solution of the orthogonal Procrustes problem", "author": ["Peter H Sch\u00f6nemann."], "venue": "Psychometrika, 31(1):1\u201310.", "citeRegEx": "Sch\u00f6nemann.,? 1966", "shortCiteRegEx": "Sch\u00f6nemann.", "year": 1966}, {"title": "Statsmodels: Econometric and statistical modeling with python", "author": ["J.S. Seabold", "J. Perktold."], "venue": "Proceedings of the 9th Python in Science Conference.", "citeRegEx": "Seabold and Perktold.,? 2010", "shortCiteRegEx": "Seabold and Perktold.", "year": 2010}, {"title": "Regularity in Semantic Change", "author": ["Elizabeth Closs Traugott", "Richard B Dasher."], "venue": "Cambridge University Press, Cambridge, UK.", "citeRegEx": "Traugott and Dasher.,? 2001", "shortCiteRegEx": "Traugott and Dasher.", "year": 2001}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Intell. Res., 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Semantics: An Introduction to the Science of Meaning", "author": ["S. Ullmann."], "venue": "Barnes & Noble, New York City, NY.", "citeRegEx": "Ullmann.,? 1962", "shortCiteRegEx": "Ullmann.", "year": 1962}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(2579-2605):85.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Collective dynamics of \u2018small-world\u2019networks", "author": ["Duncan J Watts", "Steven H Strogatz."], "venue": "Nature, 393(6684):440\u2013442.", "citeRegEx": "Watts and Strogatz.,? 1998", "shortCiteRegEx": "Watts and Strogatz.", "year": 1998}, {"title": "Understanding semantic change of words over centuries", "author": ["Derry Tanti Wijaya", "Reyyan Yeniterzi."], "venue": "Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web, pages 35\u201340. ACM.", "citeRegEx": "Wijaya and Yeniterzi.,? 2011", "shortCiteRegEx": "Wijaya and Yeniterzi.", "year": 2011}, {"title": "From part to person: Natural tendencies of semantic change and the search for cognates", "author": ["David P Wilkins."], "venue": "Cognitive Anthropology Research Group at the Max Planck Institute for Psycholinguistics.", "citeRegEx": "Wilkins.,? 1993", "shortCiteRegEx": "Wilkins.", "year": 1993}, {"title": "Cognitive Factors Motivating The Evolution Of Word Meanings: Evidence From Corpora, Behavioral Data And Encyclopedic Network Structure", "author": ["B. Winter", "Graham Thompson", "Matthias Urban."], "venue": "Evolution of Language: Proceedings of", "citeRegEx": "Winter et al\\.,? 2014", "shortCiteRegEx": "Winter et al\\.", "year": 2014}, {"title": "A computational evaluation of two laws of semantic change", "author": ["Yang Xu", "Charles Kemp."], "venue": "Proc. 37th Annu. Conf. Cogn. Sci. Soc.", "citeRegEx": "Xu and Kemp.,? 2015", "shortCiteRegEx": "Xu and Kemp.", "year": 2015}, {"title": "Historical Semantic Chaining and Efficient Communication: The Case of Container Names", "author": ["Yang Xu", "Terry Regier", "Barbara C. Malt."], "venue": "Cogn. Sci.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer."], "venue": "Natural language engineering, 11(02):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "The meaning-frequency relationship of words", "author": ["George Kingsley Zipf."], "venue": "The Journal of General Psychology, 33(2):251\u2013256.", "citeRegEx": "Zipf.,? 1945", "shortCiteRegEx": "Zipf.", "year": 1945}, {"title": "For the context word distributions in all methods, we used context distribution smoothing with a smoothing parameter of 0.75", "author": ["Levy"], "venue": null, "citeRegEx": "Levy,? \\Q2015\\E", "shortCiteRegEx": "Levy", "year": 2015}, {"title": "Furthermore, to improve the computational efficiency of SGNS (which works with text streams and not co-occurrence counts)", "author": ["Levy"], "venue": "Mikolov et al", "citeRegEx": "Levy,? \\Q2013\\E", "shortCiteRegEx": "Levy", "year": 2013}, {"title": "The implementations for PPMI and SVD are released with the code package", "author": ["Levy"], "venue": "For SGNS,", "citeRegEx": "Levy,? \\Q2015\\E", "shortCiteRegEx": "Levy", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Shifts in word meaning exhibit systematic regularities (Br\u00e9al, 1897; Ullmann, 1962).", "startOffset": 55, "endOffset": 83}, {"referenceID": 42, "context": "Shifts in word meaning exhibit systematic regularities (Br\u00e9al, 1897; Ullmann, 1962).", "startOffset": 55, "endOffset": 83}, {"referenceID": 2, "context": "The rate of semantic change, for example, is higher in some words than others (Blank, 1999) \u2014 compare the stable semantic history of cat (from ProtoGermanic kattuz, \u201ccat\u201d) to the varied meanings of English cast: \u201cto mould\u201d, \u201ca collection of actors\u2019, \u201ca hardened bandage\u201d, etc.", "startOffset": 78, "endOffset": 91}, {"referenceID": 2, "context": "Various hypotheses have been offered about such regularities in semantic change, such as an increasing subjectification of meaning, or the grammaticalization of inferences (e.g., Geeraerts, 1997; Blank, 1999; Traugott and Dasher, 2001).", "startOffset": 172, "endOffset": 235}, {"referenceID": 40, "context": "Various hypotheses have been offered about such regularities in semantic change, such as an increasing subjectification of meaning, or the grammaticalization of inferences (e.g., Geeraerts, 1997; Blank, 1999; Traugott and Dasher, 2001).", "startOffset": 172, "endOffset": 235}, {"referenceID": 8, "context": "Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007).", "startOffset": 258, "endOffset": 315}, {"referenceID": 32, "context": "Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007).", "startOffset": 258, "endOffset": 315}, {"referenceID": 27, "context": "Frequency plays a key role in other linguistic changes, associated sometimes with faster change\u2014sound changes like lenition occur in more frequent words\u2014and sometimes with slower change\u2014high frequency words are more resistant to morphological regularization (Bybee, 2007; Pagel et al., 2007; Lieberman et al., 2007).", "startOffset": 258, "endOffset": 315}, {"referenceID": 5, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al.", "startOffset": 55, "endOffset": 110}, {"referenceID": 46, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al.", "startOffset": 55, "endOffset": 110}, {"referenceID": 18, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al.", "startOffset": 55, "endOffset": 110}, {"referenceID": 0, "context": "Words gain senses over time as they semantically drift (Br\u00e9al, 1897; Wilkins, 1993; Hopper and Traugott, 2003), and polysemous words1 occur in more diverse contexts, affecting lexical access speed (Adelman et al., 2006) and rates of L2 learning (Crossley et al.", "startOffset": 197, "endOffset": 219}, {"referenceID": 9, "context": ", 2006) and rates of L2 learning (Crossley et al., 2010).", "startOffset": 33, "endOffset": 56}, {"referenceID": 14, "context": "But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015).", "startOffset": 122, "endOffset": 177}, {"referenceID": 47, "context": "But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015).", "startOffset": 122, "endOffset": 177}, {"referenceID": 49, "context": "But we don\u2019t know whether the diverse contextual use of polysemous words makes them more or less likely to undergo change (Geeraerts, 1997; Winter et al., 2014; Xu et al., 2015).", "startOffset": 122, "endOffset": 177}, {"referenceID": 51, "context": "Furthermore, polysemy is strongly correlated with frequency\u2014high frequency words have more senses (Zipf, 1945; \u0130lgen and Karaoglan, 2007)\u2014so understanding how polysemy relates to semantic change requires controling for word frequency.", "startOffset": 98, "endOffset": 137}, {"referenceID": 5, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 42, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 2, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 18, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 40, "context": "Answering these questions requires new methods that can go beyond the case-studies of a few words (often followed over widely different timeperiods) that are our most common diachronic data (Br\u00e9al, 1897; Ullmann, 1962; Blank, 1999; Hopper and Traugott, 2003; Traugott and Dasher, 2001).", "startOffset": 190, "endOffset": 285}, {"referenceID": 6, "context": "One promising avenue is the use of distributional semantics, in which words are embedded in vector spaces according to their co-occurrence relationships (Bullinaria and Levy, 2007; Turney and Pantel, 2010), and the embeddings of words", "startOffset": 153, "endOffset": 205}, {"referenceID": 41, "context": "One promising avenue is the use of distributional semantics, in which words are embedded in vector spaces according to their co-occurrence relationships (Bullinaria and Levy, 2007; Turney and Pantel, 2010), and the embeddings of words", "startOffset": 153, "endOffset": 205}, {"referenceID": 35, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 45, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 16, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 19, "context": "This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al.", "startOffset": 81, "endOffset": 178}, {"referenceID": 23, "context": ", 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al., 2014) as well as to test a few specific hypotheses, such as whether English synonyms tend to change meaning in similar ways (Xu and Kemp, 2015).", "startOffset": 151, "endOffset": 174}, {"referenceID": 48, "context": ", 2014) as well as to test a few specific hypotheses, such as whether English synonyms tend to change meaning in similar ways (Xu and Kemp, 2015).", "startOffset": 126, "endOffset": 145}, {"referenceID": 17, "context": "These methods operationalize the \u2018distributional hypothesis\u2019 that word semantics are implicit in co-occurrence relationships (Harris, 1954; Firth, 1957).", "startOffset": 125, "endOffset": 152}, {"referenceID": 13, "context": "These methods operationalize the \u2018distributional hypothesis\u2019 that word semantics are implicit in co-occurrence relationships (Harris, 1954; Firth, 1957).", "startOffset": 125, "endOffset": 152}, {"referenceID": 41, "context": "The semantic similarity/distance between two words is approximated by the cosine similarity/distance between their vectors (Turney and Pantel, 2010).", "startOffset": 123, "endOffset": 148}, {"referenceID": 25, "context": "(1) where cj \u2208 VC is a context word and \u03b1 > 0 is a negative prior, which provides a smoothing bias (Levy et al., 2015).", "startOffset": 99, "endOffset": 118}, {"referenceID": 6, "context": "Clipping the PPMI values above zero ensures they remain finite and has been shown to dramatically improve results (Bullinaria and Levy, 2007; Levy et al., 2015); intuitively, this clipping ensures that the representations emphasize positive word-word correlations over negative ones.", "startOffset": 114, "endOffset": 160}, {"referenceID": 25, "context": "Clipping the PPMI values above zero ensures they remain finite and has been shown to dramatically improve results (Bullinaria and Levy, 2007; Levy et al., 2015); intuitively, this clipping ensures that the representations emphasize positive word-word correlations over negative ones.", "startOffset": 114, "endOffset": 160}, {"referenceID": 25, "context": "Synchronic applications of these three methods are reviewed in detail in Levy et al. (2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 10, "context": "5\u00d7 10 1800-1999 (Davies, 2010) ENGFIC English Fiction from Google books 7.", "startOffset": 16, "endOffset": 30}, {"referenceID": 10, "context": "5\u00d7 10 1800-1999 (Davies, 2010) COHA English Genre-balanced sample 4.", "startOffset": 16, "endOffset": 30}, {"referenceID": 10, "context": "1\u00d7 10 1810-2009 (Davies, 2010) FREALL French Google books (all genres) 1.", "startOffset": 16, "endOffset": 30}, {"referenceID": 36, "context": "9\u00d7 10 1800-1999 (Sagot et al., 2006) GERALL German Google books (all genres) 4.", "startOffset": 16, "endOffset": 36}, {"referenceID": 37, "context": "3\u00d7 10 1800-1999 (Schneider and Volk, 1998) CHIALL Chinese Google books (all genres) 6.", "startOffset": 16, "endOffset": 42}, {"referenceID": 50, "context": "0\u00d7 10 1950-1999 (Xue et al., 2005)", "startOffset": 16, "endOffset": 34}, {"referenceID": 25, "context": "SVD embeddings correspond to low-dimensional approximations of the PPMI embeddings learned via singular value decomposition (Levy et al., 2015).", "startOffset": 124, "endOffset": 143}, {"referenceID": 41, "context": "Setting \u03b3 < 1 has been shown to dramatically improve embedding qualities (Turney and Pantel, 2010; Bullinaria and Levy, 2012).", "startOffset": 73, "endOffset": 125}, {"referenceID": 7, "context": "Setting \u03b3 < 1 has been shown to dramatically improve embedding qualities (Turney and Pantel, 2010; Bullinaria and Levy, 2012).", "startOffset": 73, "endOffset": 125}, {"referenceID": 24, "context": "This SVD approach can be viewed as a generalization of Latent Semantic Analysis (Landauer and Dumais, 1997), where the term-document matrix is replaced with MPPMI.", "startOffset": 80, "endOffset": 107}, {"referenceID": 30, "context": "SGNS \u2018neural\u2019 embeddings are optimized to predict co-occurrence relationships using an approximate objective known as \u2018skip-gram with negative sampling\u2019 (Mikolov et al., 2013).", "startOffset": 153, "endOffset": 175}, {"referenceID": 22, "context": "SGNS has the benefit of allowing incremental initialization during learning, where the embeddings for time t are initialized with the embeddings from time t \u2212 \u2206 (Kim et al., 2014).", "startOffset": 161, "endOffset": 179}, {"referenceID": 28, "context": "We trained models on the 6 datasets described in Table 1, taken from Google N-Grams (Lin et al., 2012) and the COHA corpus (Davies, 2010).", "startOffset": 84, "endOffset": 102}, {"referenceID": 10, "context": ", 2012) and the COHA corpus (Davies, 2010).", "startOffset": 28, "endOffset": 42}, {"referenceID": 33, "context": ", to shifting sampling biases over time (Pechenick et al., 2015).", "startOffset": 40, "endOffset": 64}, {"referenceID": 25, "context": "We follow the recommendations of Levy et al. (2015) in setting the hyperparameters for the embedding methods, though preliminary experiments were used to tune key settings.", "startOffset": 33, "endOffset": 52}, {"referenceID": 38, "context": "The solution corresponds to the best rotational alignment and can be obtained efficiently using an application of SVD (Sch\u00f6nemann, 1966).", "startOffset": 118, "endOffset": 136}, {"referenceID": 19, "context": "Previous work circumvented this problem by either avoiding low-dimensional embeddings (e.g., Gulordava and Baroni, 2011; Jatowt and Duh, 2014) or by performing heuristic local alignments per word (Kulkarni et al.", "startOffset": 86, "endOffset": 142}, {"referenceID": 23, "context": ", Gulordava and Baroni, 2011; Jatowt and Duh, 2014) or by performing heuristic local alignments per word (Kulkarni et al., 2014).", "startOffset": 105, "endOffset": 128}, {"referenceID": 23, "context": "mean shifts (Kulkarni et al., 2014) could also be used.", "startOffset": 12, "endOffset": 35}, {"referenceID": 4, "context": "On Bruni et al. (2012)\u2019s MEN similarity task of matching human judgments of word similarities, SVD performed best (\u03c1 = 0.", "startOffset": 3, "endOffset": 23}, {"referenceID": 4, "context": "On Bruni et al. (2012)\u2019s MEN similarity task of matching human judgments of word similarities, SVD performed best (\u03c1 = 0.739), followed by PPMI (\u03c1 = 0.687) and SGNS (\u03c1 = 0.649). These results echo the findings of Levy et al. (2015), where they found SVD to perform best on similarity tasks while SGNS performed best on analogy tasks (which are not the focus of this work).", "startOffset": 3, "endOffset": 232}, {"referenceID": 23, "context": "gay homosexual, lesbian happy, showy ca 1920 (Kulkarni et al., 2014) fatal illness, lethal fate, inevitable <1800 (Jatowt and Duh, 2014) awful disgusting, mess impressive, majestic <1800 (Simpson et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 19, "context": ", 2014) fatal illness, lethal fate, inevitable <1800 (Jatowt and Duh, 2014) awful disgusting, mess impressive, majestic <1800 (Simpson et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 45, "context": ", 1989) nice pleasant, lovely refined, dainty ca 1900 (Wijaya and Yeniterzi, 2011) broadcast transmit, radio scatter, seed ca 1920 (Jeffers and Lehiste, 1979) monitor display, screen \u2014 ca 1930 (Simpson et al.", "startOffset": 54, "endOffset": 82}, {"referenceID": 20, "context": ", 1989) nice pleasant, lovely refined, dainty ca 1900 (Wijaya and Yeniterzi, 2011) broadcast transmit, radio scatter, seed ca 1920 (Jeffers and Lehiste, 1979) monitor display, screen \u2014 ca 1930 (Simpson et al.", "startOffset": 131, "endOffset": 158}, {"referenceID": 23, "context": ", 1989) record tape, album \u2014 ca 1920 (Kulkarni et al., 2014) guy fellow, man \u2014 ca 1850 (Wijaya and Yeniterzi, 2011) call phone, message \u2014 ca 1890 (Simpson et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 45, "context": ", 2014) guy fellow, man \u2014 ca 1850 (Wijaya and Yeniterzi, 2011) call phone, message \u2014 ca 1890 (Simpson et al.", "startOffset": 34, "endOffset": 62}, {"referenceID": 26, "context": "The trendlines show 95% CIs from bootstrapped kernel regressions on the ENGALL data (Li and Racine, 2007).", "startOffset": 84, "endOffset": 105}, {"referenceID": 44, "context": "We quantify the polysemy of a word by measuring the local clustering coefficient of its co-occurrence network (Watts and Strogatz, 1998)9, i.", "startOffset": 110, "endOffset": 136}, {"referenceID": 11, "context": ", number of senses in WordNet (Dorow and Widdows, 2003; Ferret, 2004); however, it does have a slight bias towards rating contextually-diverse discourse function words (e.", "startOffset": 30, "endOffset": 69}, {"referenceID": 12, "context": ", number of senses in WordNet (Dorow and Widdows, 2003; Ferret, 2004); however, it does have a slight bias towards rating contextually-diverse discourse function words (e.", "startOffset": 30, "endOffset": 69}, {"referenceID": 5, "context": "The law of innovation\u2014polysemous words change more quickly\u2014quantifies the central role polysemy plays in semantic change, an issue that has concerned linguists for more than 100 years (Br\u00e9al, 1897).", "startOffset": 184, "endOffset": 197}, {"referenceID": 46, "context": "Previous works argued that semantic change leads to polysemy (Wilkins, 1993; Hopper and Traugott, 2003), but our results suggest that polysemy may actually play a causal role in inducing semantic change.", "startOffset": 61, "endOffset": 103}, {"referenceID": 18, "context": "Previous works argued that semantic change leads to polysemy (Wilkins, 1993; Hopper and Traugott, 2003), but our results suggest that polysemy may actually play a causal role in inducing semantic change.", "startOffset": 61, "endOffset": 103}, {"referenceID": 34, "context": "The law of conformity might be a consequence of learning: perhaps people are more likely to use rare words mistakenly in novel ways, a mechanism formalizable by Bayesian models of word learning and corresponding to the biological notion of genetic drift (Reali and Griffiths, 2010).", "startOffset": 254, "endOffset": 281}, {"referenceID": 3, "context": "Or perhaps a sociocultural conformity bias makes people less likely to accept novel innovations of common words, a mechanism analogous to the biological process of purifying selection (Boyd and Richerson, 1988; Pagel et al., 2007).", "startOffset": 184, "endOffset": 230}, {"referenceID": 32, "context": "Or perhaps a sociocultural conformity bias makes people less likely to accept novel innovations of common words, a mechanism analogous to the biological process of purifying selection (Boyd and Richerson, 1988; Pagel et al., 2007).", "startOffset": 184, "endOffset": 230}, {"referenceID": 21, "context": "Highly polysemous words tend to have more rare senses (Kilgarriff, 2004), and rare senses may be unstable by the law of conformity.", "startOffset": 54, "endOffset": 72}, {"referenceID": 31, "context": "Marginal R (Nakagawa and Schielzeth, 2013).", "startOffset": 11, "endOffset": 42}], "year": 2017, "abstractText": "Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity\u2014the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation\u2014independent of frequency, words that are more polysemous have higher rates of semantic change.", "creator": "TeX"}}}