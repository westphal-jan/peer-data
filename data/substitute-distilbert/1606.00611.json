{"id": "1606.00611", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks", "abstract": "in visual interaction tasks, supervised learning shows excellent performance. on the up hand, kernel learning utilizes cheap unlabeled data and not pretend to solve the same tasks more strongly. we show thus the recursive autoconvolutional function, adopted from physics, boosts existing unsupervised methods to learn more powerful filters. we use traditionally well established multilayer convolutional network and train filters layer - wise. to build a stronger classifier, we design a thicker light committee of different models. the total number of trainable parameters is not greatly reduced by using better filters in higher layers. we evaluate our networks on the mnist, cifar - 10 and stl - 10 benchmarks when leverage several state of the art results among other unsupervised methods.", "histories": [["v1", "Thu, 2 Jun 2016 10:37:46 GMT  (449kb,D)", "http://arxiv.org/abs/1606.00611v1", null], ["v2", "Sun, 26 Mar 2017 18:31:05 GMT  (321kb,D)", "http://arxiv.org/abs/1606.00611v2", "8 pages, accepted to International Joint Conference on Neural Networks (IJCNN 2017)"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["boris knyazev", "erhardt barth", "thomas martinetz"], "accepted": false, "id": "1606.00611"}, "pdf": {"name": "1606.00611.pdf", "metadata": {"source": "CRF", "title": "Autoconvolution for Unsupervised Feature Learning", "authors": ["Boris Knyazev", "Erhardt Barth"], "emails": ["bknyazev@bmstu.ru", "barth@inb.uni-luebeck.de", "martinetz@inb.uni-luebeck.de", "borknyaz@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Large-scale visual tasks can now be solved with big deep neural networks, if thousands of labeled samples are available and if training time is not an issue. Efficient GPU implementations of standard computational blocks make training and following usage feasible.\nA major drawback of supervised neural networks is that they heavily rely on labeled data. It is true that in real applications it does not really matter which methods are used to achieve the desired outcome. But in some cases, labeling can be an expensive process. On the other side, natural data are full of abstract features unrelated to object classes. Unsupervised learning exploits abundant amounts of these cheap unlabeled data and can help to solve the same tasks more efficiently. In general, unsupervised learning is important to move towards artificial intelligence [1]. Another drawback of supervised methods and some recent unsupervised developments [2, 3, 4] is their excessive use of data augmentation. In this regard, unsupervised methods are shown to be able to learn these and more complex transformations, e.g., by \"watching\" videos [5].\nIn this work, we learn a visual representation model for image classification free from the aforementioned pitfalls. As a result, our work can potentially be easily applied to any natural data (e.g., audio). We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12]. These methods are particularly suitable for the tasks with only few labeled training samples, such as STL-10 [7], as well as variants of MNIST [13] and CIFAR-10 [14] with smaller training sets. In addition, on full variants of these datasets we demonstrate that unsupervised learning is steadily becoming comparable with and even outperforms its supervised counterpart, including some convolutional neural networks (CNNs) trained by back-\n\u2217Alternative e-mail: borknyaz@gmail.com\nar X\niv :1\n60 6.\n00 61\n1v 1\n[ cs\n.C V\n] 2\npropagation on thousands of labeled samples [15, 16]. This way, we provide further evidence that unsupervised learning is suitable for building efficient visual representations.\nThe main contribution of this work is adaptation of the recursive autoconvolution operator. Concretely, we demonstrate that this operator can be used to encourage existing clustering methods (e.g., k-means) or other learning methods (e.g., independent component analysis (ICA)) to train more powerful filters (Fig. 1) that resemble the ones learned by CNNs (Sections 3 and 4.1). Our second novelty is a method for building a committee of support vector machines (SVM) trained on several subsets of data projected by principal component analysis (PCA) (Section 4.6). Finally, we substantially reduce the total number of learned filters in higher layers of the network without loss of classification accuracy (Sections 4.4 and 5.3). In the end, we report several state of the art results among unsupervised methods while keeping computational cost relatively low (Section 5.3)."}, {"heading": "2 Related Work", "text": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].\nHowever, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used. The only exceptions are [4, 2], in which image transformations are used.\nIn these works, learning filters with clustering methods, such as k-means, is a standard approach [7, 2, 12, 4, 11]. For this reason, and to make comparison of our results easier, k-means is also adopted in this work as a default method. Moreover, clustering methods can learn overcomplete dictionaries without additional modifications, such as done for ICA in [10]. Nevertheless, since ICA [20] is also a common practice to learn filters, we conduct a couple of simple experiments with this method to probe our novel idea more thoroughly. In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.\nRecently, this framework was successfully transfered to unsupervised learning in [11, 4]. In their works, as well as in our work, the forward pass is mostly kept standard, while methods to learn stronger (in terms of classification) filters are developing. For instance, in [11], k-means is enhanced by introducing convolutional clustering. Convolutional extension of clustering and coding methods is one of the ways to reduce redundancy in filters and improve classification, e.g., convolutional sparse coding [21]. In this work, we suggest another concept of making filters more powerful, namely, by recursive autoconvolution applied to image patches before unsupervised learning.\nAutoconvolution (or self-convolution) and its properties seem to be first analyzed in physics (in spectroscopy [22]) and later in function optimization [23] as the problem of deautoconvolution arose. This operator also appeared in visual tasks to extract invariant patterns [24]. But, to the best of our knowledge, its recursive version, used as a pillar in this work, was first suggested in [25] for parametric description of images and temporal sequences. By contrast, we use this operator to extract patterns, which are then treated as convolution kernels in a multilayer CNN, which we refer to as an AutoCNN."}, {"heading": "3 Autoconvolution", "text": "We first describe the routine for processing arbitrary discrete data based on autoconvolution. It is convenient to consider autoconvolution in the frequency domain. According to the convolution theorem, for N -dimensional discrete signals X and Y, such as images (N = 2): F(X \u2217 Y) = kF(X) \u25e6 F(Y), where F - the N -dimensional forward discrete Fourier transform (DFT), \u25e6 - pointwise matrix product, k - a normalizing coefficient (which will be ignored further, since we apply normalization afterwards). Hence, autoconvolution is defined as\nX \u2217X = F\u22121(F(X)2), (1)\nwhere F\u22121 - the N -dimensional inverse DFT. Because of squared frequencies, phase information is mixed and the inverse operation becomes ill-posed. A lot of work has been devoted to that problem, e.g., [23]. In this work, we focus only on the forward operator.\nTo extract patterns from X, it is necessary to make sure that mean(X) = 0 and std(X) > 0 before computing (1). Also, to compute linear autoconvolution, X is first zero-padded (however, for such images as in MNIST, this step might be optional). We also tried to work with autocorrelation defined as X ?X = F\u22121(F(X) \u25e6 conj[F(X)])), but in our experience filters extracted with autoconvolution are more powerful."}, {"heading": "3.1 Recursive Autoconvolution", "text": "We adopt the recursive autoconvoution operator, proposed earlier in [25] by simple extension of (1):\nXn = Xn\u22121 \u2217Xn\u22121 = F\u22121(F(Xn\u22121)2), (2)\nwhere n = [0, nmax] - an index of the recursive iteration (or autoconvoution order). If n = 0, X0 equals the input, i.e. a raw image patch. For n = 1 expression (2) becomes equal (1). In our work, we limit nmax = 4 as higher orders do not lead to better classification results.\nIn [25], image patterns extracted using this operator were used for parametric description of images. In this work, we use extracted patterns as convolution kernels, i.e. filters, because we noticed that applying (2) with n > 1 to images provides sparse wavelet like patterns, which are usually learned by a CNN in the first layer (see [16], Fig. 3 or [5], Fig. 6) or by other unsupervised learning methods, e.g. ICA [see 10, Fig. 1] or sparse coding [see 6, Fig. 2].\nOne of the issues with recursive autoconvoution is that the size of X is doubled after each iteration n due to zero-padding. To deal with that, we simply take the central part of the result or resize (subsample) it to its original size after each iteration (Fig. 1 (a),(b), where the second option is picked). We randomly choose one of these options to make the set of patches richer.\nAccording to our statistics of extracted patches, autoconvolution order n is inversely proportional to the joint spatial \u03c3xy and frequency \u03c3uv resolution, i.e. n \u223c 1/(\u03c3xy\u03c3uv), where \u03c3xy = \u03c3x\u03c3y =\u221a D1D2 and D1, D2 - are eigenvalues of the weighted covariance matrix of X in the spatial domain; analogously for \u03c3uv. Therefore, to cover a wider range of spatio-frequency properties, patches extracted with several orders are combined into one global set.\nWe are now ready to describe the architecture of a multilayer convolutional network (AutoCNN), which we used for image classification."}, {"heading": "4 Autoconvolutional Multilayer Architecture", "text": ""}, {"heading": "4.1 Learning Filters", "text": "The baseline method for unsupervised filter learning is chosen to be k-means as in [11, 4]. Specifically, for some layer l of an AutoCNN and for some training sample we have input X(l)in \u2208 Ral\u00d7al\u00d7Kl\u22121 , where Kl\u22121 - the number of filters (or channels) in the previous layer. To learn filters for this layer l, we take random patches X(l) \u2208 Rsl\u00d7sl\u00d7dl from a subset of training samples and apply n-order recursive autoconvolution (2), where dl - the depth of filters, which is \u2264 Kl\u22121 due to grouping described below in Section 4.4. Only squared inputs, patches and filters are considered for simplicity. To learn more powerful filters, we take results of several orders (e.g., in case n = [0, 3] we have 4\npatches instead of 1) and combine them into one global set of autoconvolutional patches. Note that in case n = 0, we extract more patches to make the total number of input data points for k-means about the same as for combinations of orders. We also experience with each of the orders independently to determine which orders contribute the most during classification (Fig. 1 (c)-(f)). In this global set, all patches are first scaled to have values in the range [0,1] , then they are ZCA-whitened as in [14, 7, 9, 11, 4]. For this set, k-means clustering (or another method, e.g., ICA) is applied, which produces a set of Kl data points (a dictionary) D(l) \u2208 Rsl\u00d7sl\u00d7dl\u00d7Kl . These data points are first l2-normalized and then used as convolution kernels (filters) for layer l."}, {"heading": "4.2 Preprocessing and Convolution", "text": "For image preprocessing we use standard techniques like ZCA-whitening and standardization 2 with few modifications. First, only colored datasets and only in the first layer are whitened, because for such clean grayscale images as in MNIST it is not reasonable. Second, in contrast to [11] or [7], where global or local normalization techniques are used, we use batch standardization (i.e. the entire batch is treated as a single vector) with a typical batch size of 125 to divide the dataset into equal batches. Additionally, since for higher layers we have groups of connected feature maps (as described below in Section 4.4), we standardize groups independently before convolutions. After preprocessing, inputs are convolved with filters with zero-padding, so that sizes of inputs and responses are equal."}, {"heading": "4.3 Response Rectification, Normalization and Pooling", "text": "We experiment with two popular rectifiers which are applied to filter responses: max(0,x) (or ReLU), which is used by default (unless otherwise specified), and absolute values |x|. Between layers we use local contrast normalization (LCN), as in [4]. Then, simple max-pooling over squared (of size ml) disjoint spatial regions within a feature map is used. In addition, nonlinear \"rootsift\" normalization (sign(x) \u221a |x|/\u2016x\u20162) applied after pooling turned out to be beneficial for colored datasets. This normalization as well as LCN are used only to report final classification results."}, {"heading": "4.4 Selecting Connections between Layers", "text": "In CNNs trained with backpropagation, typically depth dl of filters equals the number of filters in the previous layer, i.e. dl = Kl\u22121. Alternatively, connections between feature maps and filters can also be sparse [13] or learned along with the network weights [11].\nFor unsupervised learning methods, such as k-means, it is hard to learn discriminative filters, if dl is too high (e.g., 64 in [15]). Therefore, in this work, we follow the practice, established in unsupervised CNNs [11, 4], to divide feature maps into ng relatively small groups, so that filters D(l) \u2208 Rsl\u00d7sl\u00d7dl\u00d7Kl are learned independently for each group, where dl Kl\u22121. Convolutions are concatenated for all groups, so that responses of size al/ml \u00d7 al/ml \u00d7 ngKl are forward passed to the next layer. For ng = 1 this is a typical CNN architecture. Our contribution is that for l > 1, instead of treating groups independently, we learn filters for all groups altogether as described in more detail in Section 5.3.\nContrary to [11, 4], where feature groups are formed randomly or learned in a supervised fashion, we use the approach from [9]. Specifically, from all Kl feature maps random ng maps are first chosen, for each of them the closest (dl \u2212 1) features are then found. The similarity is computed as correlation between squared vectors S(xj ,xk) = corr(x2j ,x 2 k) for some whitened feature maps xj and xk. To improve connections, we repeat this heuristic procedure several times and select few variants of connections with the smallest total distance S. From these variants, we select the one with the maximum number of unique feature maps connected to the next layer. This way, we try to find a compromise between diversity of filters and their connectivity to each other, which is necessary to form invariant groups [10]. Our connection scheme is, in fact, incomplete. That is, only part of feature maps is propagated to higher layers, even if ngdl > Kl\u22121. But as we apply a multidictionary approach similarly to [11, 12], this is not an issue, because eventually features from all layers are concatenated and fed to a classifier.\n2Along this work, vector x is considered standardized if its mean(x) = 0 and std(x) = 1."}, {"heading": "4.5 Dimension Reduction and Classification", "text": "In [11], fully connected layers with dropout are trained on top of the unsupervised features. In this work, we first apply principal component analysis (PCA) together with whitening and then train a classifier on the projected data. For particularly large features (>30k and up to 300k in our experiments) random PCA [26] turned out to be extremely useful.\nFor classification, we use an SVM with the RBF kernel with a one-vs-one multiclass variant (unless otherwise specified). Feature vectors are standardized before classification. In all experiments, the SVM regularization constant was C = 16. The width of the RBF kernel was chosen to be \u03b3 = 1/pj , where pj is the dimensionality of the projected feature vector, the input of the SVM (see also the next section). For better final classification results for CIFAR-10 and STL-10, we also apply a one-vs-all method (we use a very efficient GPU implementation from [27])."}, {"heading": "4.6 SVM Committee", "text": "A committee of models tends to give better classification results [4]. In this work, we build a committee of J models with just one set of filters, so that the models in the committee are determined only by their SVM. The filters, the PCA matrix and other parameters are fixed for all models. After the images were processed with a learned AutoCNN, PCA is performed once, and pj , j = 1, ..., J first principal components are chosen. Then for each j, an SVM is trained on the features projected on the pj first principal components. After all iterations j are complete, the SVM scores within the committee are averaged."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "We evaluate our method on several image classification benchmarks: MNIST [13], CIFAR-10 [14] and STL-10 [7]. To demonstrate that unsupervised learning is particularly effective for datasets with few training samples, such as STL-10, which has only 100 images per class, we test our method on smaller versions of MNIST and CIFAR-10, namely MNIST (100), MNIST (300) and CIFAR-10 (400) with just 100, 300 and 400 images per class respectively, using the same experimental protocol as in previous works, e.g. [11, 3]: average results, and their standard deviations on the test set using 10 random subsets (folds) from the training set, are reported. For STL-10 these folds are predefined.\nWhile in previous works all labeled training samples are typically used as unlabeled data during unsupervised learning, we found that it is enough to use at most 10k samples to learn filters and 20k - to perform PCA in all experiments, including STL-10, which contains 100k unlabeled samples.\nWe train models with one and two layers according to the proposed architecture (Section 4), which turned out to be sufficient to confirm effectiveness of our method. All reported classification results are average errors for MNIST and accuracies for all other datasets in percent, unless otherwise stated."}, {"heading": "5.2 Model Parameters Validation", "text": "For model validation 3 random sets of 10k training and 10k test samples are drawn from the original training sets in case of MNIST and CIFAR-10, and 1k training and 1k test samples in case of STL-10. Average cross-validation results using the baseline model for 3 folds and among 3 different PCA dimensionalities (pj = 50, 100, 150) are reported. The baseline model uses ReLU and max-pooling of size (m1) 4 for MNIST, 8 for CIFAR-10 and 16 for STL-10.\nFirst, for different filter sizes (s1) we observe that autoconvolution (2) greatly improves classification, if neither filters nor input images are whitened (Fig. 2 (a)-(c)). We also show that in case of CIFAR-10 and STL-10 (Fig. 2 (b),(c)) whitening applied both to input images and filters considerably enhances results, which are shown in single markers for n = 0 and for several n \u2265 0 only for the best s1. Next, we conducted tests with whitening applied only to filters. It turned out to be an optimal scenario only for MNIST, and the results in this case are shown only for this dataset, similarly in single markers (Fig. 2 (a)). For CIFAR-10 and STL-10 this scenario is still more beneficial than completely without whitening, and the advantage of n \u2265 0 over n = 0 in this case is much more noticeable than in case of whitening both. The number of filters in these experiments is fixed to K1 = 96. Overall, the best\nperformance with recursive autoconvolution is superior than without it for a wide range of filter sizes, so, evidently, better filter size or preprocessing are not the reasons for our accuracy increase.\nGiven the obtained results (Fig. 2 (a)-(c)), for the next experiments we apply whitening both to images and filters for CIFAR-10 and STL-10 and whitening only to filters for MNIST. For CIFAR-10 and MNIST we set s1 = 13 and for STL-10 s1 = 29 px. Using these parameters, we estimated that the effect of applying recursive autoconvolution scales well with the number of filters (Fig. 2 (d)), although for STL-10 we observe some noise, probably, because we used too few folds and a rather small number of k-means iterations.\nNevertheless, our results without whitening and with whitening applied only to filters are very relevant in practice, because in some tasks whitening of inputs is difficult to be performed or not applicable [10], e.g., in case of large inputs we would need to compute a huge covariance matrix or in case of MNIST whitening is harmful.\nBy further cross-validation we tuned other model parameters. In detail, an error is decreased for MNIST by using |x| instead of ReLU (from 1.17% to 0.98%). In its turn, for CIFAR-10 and STL-10 a parametric rectifier max(0.25,min(25,x)), \"rootsift\" normalization (see Section 4.3) and larger max-pooling (m1 = 20 for STL-10 in case of one layer) altogether yield 0.5-1.5%. For all datasets, a committee of J = 4\u2212 20 SVM models (introduced in Section 4.6) in the ranges pj = [50, 400] for MNIST and pj = [30, 1500] for colored datasets gains another 0.05-0.10% and 1-3% respectively (Table 1). Using these tuning parameters we report our test results in Tables 1 and 2. Additionally to these parameters, in Table 3 results for CIFAR-10 and STL-10 are achieved with LCN, |x| for the second layer and a one-vs-all SVM, which together give about 1.5-2%."}, {"heading": "5.3 Multilayer Performance", "text": "For a multilayer AutoCNN we determine if applying recursive autoconvolution is reasonable for higher layers (Table 1). It turned out, that for all datasets the results are consistently better with n \u2265 0 either for the first or second layer and, in particular, for both layers. For all cases in these experiments all model settings are kept the same. The architectures in our experiments have short notations. For instance, \"128c13-2p\u219232g-4ch-64c9-6p\" denotes a two layer network with 128 filters of size s1 = 13 and max-pooling of size m1 = 2 in the first layer, and 64 filters of size s2 = 9 and depth d2 = 4 with m2 = 6 in the second layer, whereas feature maps of the first layer are connected into ng = 32 groups. Thus, in these experiments (Table 1), for MNIST, CIFAR-10 and STL-10 we design small (given that unsupervised models tend to be much larger) two layer networks: 128c13-2p\u219232g-2ch-64c9-2p, 128c13-2p\u219232g-2ch-64c11-4p and 96c29-4p\u219224g-2ch-64c17-6p respectively. In all experiments with 2 layer networks, for layer 1 we use combinations n = [1, 3] for MNIST and n = [0, 4] for others, while for layer 2 we found n = [2, 3] to be optimal.\nWe next investigate connections from the first to the second layer. In previous works [11, 4], filters of higher layers are distinct for each group (learned independently), i.e. the total number of filters in layer 2 is defined as K2ng, as described in Section 4.4. We discovered that using the same filters for all groups has no negative effect on classification accuracy, while it speeds up learning and\nsignificantly reduces the number of trainable parameters (Table 1). In this case, patches from all groups are concatenated before clustering and the total number of filters in layer 2 becomes equal K2, i.e. the same filters are shared between all groups. To exploit features from both layers, the multidictionary approach is employed as in [11, 12]. First layer features are obtained according to a single layer architecture with a larger pooling size, so that sizes of the 1 and 2 layer feature maps are equal. Features of both layers are then combined into a large vector and passed to PCA and SVMs.\nFinally, we evaluate our method on the test datasets (Tables 2 and 3, where the number of filters in dictionaries or convolutional layers, i.e. with filters larger than 1 \u00d7 1, is specified if known, the best results within a group of methods are in bold, parameters for STL-10 in parentheses). For MNIST, CIFAR-10 and STL-10 we increase the networks to 192c13-2p\u219232g-3ch-64c9-2p, 1024c13-2p\u2192128g-4ch-160c11-4p and 1024c29-4p\u2192192g-4ch-160c17-8p respectively. However, at the same time, we prune the first layer features so that only the filters connected to layer 2 are used (in Tables 2 and 3, an approximate number of filters in layer 1 is indicated). For the full tests on MNIST and CIFAR-10 we report the average for 10 independent tests (as done, for instance, in [19]) and, in addition, the results of averaging the SVM scores of all J \u00d7 10 models (denoted as \"comb\").\nNotably, in most of the previous works, the results are very good either for simple grayscale images (MNIST) or more complex colored datasets (CIFAR-10, STL-10), or for smaller or larger datasets only. The only exception seems to be Ladder Networks [19], which are much larger and deeper than our models and use a supervised cost. Also, in [28] better results are achieved for MNIST with a single layer, but in our experience it can be quite easy to fine tune to such a simple task. Besides, we report the average for 10 tests, and for some runs our error was close to their results. For STL-10 comparable results are obtained in [12, 28], however our model has several times fewer features compared to [12], while in [28] the most important model component is handcrafted SIFT features, whereas we learn features from data. Our single layer model is especially effective for CIFAR-10, for which we obtain an accuracy better than in all previous multilayer unsupervised models (without data augmentation) and even better than a two layer CNN [16]. Our two layer network with a smaller total number of filters outperforms our single layer (except for the full tests on MNIST), which suggests the importance of depth in our case in the same way as in supervised CNNs. It is also superior than many other supervised and unsupervised models, including a large 3 layer CNN in [3] (in case of full data), which relies on excessive data augmentation and a 3 layer supervised CNN [15] based on an advanced pooling scheme. Other previous works, showing higher accuracies, are either fully\nor semi-supervised. In this work, we show competitive results both for simple and more complex datasets, as well as both for smaller and larger ones using the same model with few tuning parameters.\nTo further advocate for our method, we checked if recursive autoconvolution is able to improve other learning methods. For this purpose, we learned filters with ICA [20] on patches with n = 0 and n \u2265 0 (AutoCNN-ICA) using the same procedure as with k-means (see Section 4.1). For CIFAR-10 we use larger filters since we were not able to learn overcomplete dictionaries. The gap between these two results is pronounced (Table 3), which strongly confirms the effectiveness of our method.\nNote that the numbers of model filters presented in Tables 2 and 3 do not always reflect the total number of trainable parameters (Nmodel) nor the total computational cost. While our 2 layer model has seemingly many more filters than some CNNs, its maximum number of filter channels (dl) is only 4, while in CNNs dl = Kl\u22121 (i.e. 64-512). Thus, for convolutional layers of our large model for CIFAR-10 Nmodel = 400\u00d7 13\u00d7 13\u00d7 3 + 160\u00d7 11\u00d7 11\u00d7 4 \u2248 280k, while in [15] it is \u2248 200k, which is smaller only because of smaller filters. Most other CNNs, presented in Tables 2 and 3, have many more parameters. Training our large two layer network on CIFAR-10 takes about 100 minutes on NVIDIA GTX 980 Ti in a Matlab implementation. Our smaller two layer network for MNIST is trained in about 25 minutes, while large single layer models can be trained in about 15 minutes on CIFAR-10 and in just 3-4 minutes on MNIST (including an SVM committee in all cases)."}, {"heading": "6 Conclusion", "text": "The importance of unsupervised learning in visual tasks is increasing and development is driven by the necessity to better exploit massive amounts of unlabeled data. We propose a novel unsupervised feature learning method and report superior results in several image classification tasks among the works not relying on data augmentation or supervised fine tuning. We adopt recursive autoconvolution and demonstrate its great utility for unsupervised learning methods, such as k-means and ICA. We argue that it can be integrated into other learning methods, including recently devised convolutional clustering, to boost their performance since recursive autoconvolution reveals complex image patterns. Furthermore, we significantly reduce the total number of trainable parameters by using shared filters and propose a simple method to build a committee of SVM models. As a result, the proposed autoconvolutional network performs better than most of the unsupervised, and several supervised, models in various classification tasks with only few but also with thousands of labeled samples."}, {"heading": "Acknowledgments", "text": "This work is jointly supported by the German Academic Exchange Service (DAAD) and the Ministry of Education and Science of the Russian Federation (project number 3708)."}], "references": [{"title": "Direct modeling of complex invariances for visual object features", "author": ["Ka Y Hui"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Committees of deep feedforward networks trained with few data", "author": ["Bogdan Miclut"], "venue": "In Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Simple method for high-performance digit recognition based on sparse coding", "author": ["Kai Labusch", "Erhardt Barth", "Thomas Martinetz"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1985}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y Ng", "Honglak Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y Ng"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Convolutional clustering for unsupervised learning", "author": ["Aysegul Dundar", "Jonghoon Jin", "Eugenio Culurciello"], "venue": "arXiv preprint arXiv:1511.06241v2,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Stable and efficient representation learning with nonnegativity constraints", "author": ["Tsung-Han Lin", "HT Kung"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Convolutional kernel networks", "author": ["Julien Mairal", "Piotr Koniusz", "Zaid Harchaoui", "Cordelia Schmid"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S Huang"], "venue": "arXiv preprint arXiv:1412.6597,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Stacked what-where auto-encoders", "author": ["Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Semi-supervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Fast and robust fixed-point algorithms for independent component analysis", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Fast convolutional sparse coding", "author": ["Hilton Bristow", "Anders Eriksson", "Simon Lucey"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "The inversion of autoconvolution integrals", "author": ["V Dose", "Th Fauster", "H-J Gossmann"], "venue": "Journal of Computational Physics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1981}, {"title": "On autoconvolution and regularization", "author": ["Rudolf Gorenflo", "Bernd Hofmann"], "venue": "Inverse Problems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "Multi-scale auto-convolution for affine invariant pattern recognition", "author": ["Janne Heikkil\u00e4"], "venue": "In Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Convolutional sparse coding for static and dynamic images analysis", "author": ["BA Knyazev", "VM Chernenkiy"], "venue": "Science & Education of Bauman MSTU/Nauka i Obrazovanie of Bauman MSTU,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "A GPU-tailored approach for training kernelized SVMs", "author": ["Andrew Cotter", "Nathan Srebro", "Joseph Keshet"], "venue": "In SIGKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Unsupervised feature learning with C-SVDDNet", "author": ["Dong Wang", "Xiaoyang Tan"], "venue": "arXiv preprint arXiv:1412.7259,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Winner-take-all autoencoders", "author": ["Alireza Makhzani", "Brendan J Frey"], "venue": "In NIPS, pages 2773\u20132781,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters", "author": ["John G Daugman"], "venue": "JOSA A,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1985}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["Andrea Vedaldi", "Brian Fulkerson"], "venue": "In Proceedings of the 18th ACM international conference on Multimedia,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "MatConvNet: Convolutional neural networks for matlab", "author": ["Andrea Vedaldi", "Karel Lenc"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Another drawback of supervised methods and some recent unsupervised developments [2, 3, 4] is their excessive use of data augmentation.", "startOffset": 81, "endOffset": 90}, {"referenceID": 1, "context": "Another drawback of supervised methods and some recent unsupervised developments [2, 3, 4] is their excessive use of data augmentation.", "startOffset": 81, "endOffset": 90}, {"referenceID": 2, "context": "Another drawback of supervised methods and some recent unsupervised developments [2, 3, 4] is their excessive use of data augmentation.", "startOffset": 81, "endOffset": 90}, {"referenceID": 3, "context": ", by \"watching\" videos [5].", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 5, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 6, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 7, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 8, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 9, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 2, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 10, "context": "We improve on the previous works in which network filters (or weights) are learned layer-wise without label information [6, 7, 8, 9, 10, 11, 4, 12].", "startOffset": 120, "endOffset": 147}, {"referenceID": 5, "context": "These methods are particularly suitable for the tasks with only few labeled training samples, such as STL-10 [7], as well as variants of MNIST [13] and CIFAR-10 [14] with smaller training sets.", "startOffset": 109, "endOffset": 112}, {"referenceID": 11, "context": "These methods are particularly suitable for the tasks with only few labeled training samples, such as STL-10 [7], as well as variants of MNIST [13] and CIFAR-10 [14] with smaller training sets.", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "These methods are particularly suitable for the tasks with only few labeled training samples, such as STL-10 [7], as well as variants of MNIST [13] and CIFAR-10 [14] with smaller training sets.", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "propagation on thousands of labeled samples [15, 16].", "startOffset": 44, "endOffset": 52}, {"referenceID": 14, "context": "propagation on thousands of labeled samples [15, 16].", "startOffset": 44, "endOffset": 52}, {"referenceID": 15, "context": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].", "startOffset": 137, "endOffset": 145}, {"referenceID": 17, "context": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].", "startOffset": 137, "endOffset": 145}, {"referenceID": 1, "context": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].", "startOffset": 205, "endOffset": 212}, {"referenceID": 12, "context": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].", "startOffset": 205, "endOffset": 212}, {"referenceID": 3, "context": "Unsupervised learning is used quite often as an additional regularizer in the form of weights initialization [17] or reconstruction cost [18, 19], or as an independent visual model trained on still images [3, 14] or image sequences [5].", "startOffset": 232, "endOffset": 235}, {"referenceID": 4, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 5, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 6, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 7, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 8, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 0, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 10, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 78, "endOffset": 101}, {"referenceID": 9, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 122, "endOffset": 129}, {"referenceID": 2, "context": "However, to a larger extent, our method is related to another series of works [6, 7, 8, 9, 10, 2, 12] and, in particular, [11, 4], in which filters (or some basis) are learned layer-wise and, contrary to the methods above, neither backpropagation, fine tuning nor data augmentation is used.", "startOffset": 122, "endOffset": 129}, {"referenceID": 2, "context": "The only exceptions are [4, 2], in which image transformations are used.", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "The only exceptions are [4, 2], in which image transformations are used.", "startOffset": 24, "endOffset": 30}, {"referenceID": 5, "context": "In these works, learning filters with clustering methods, such as k-means, is a standard approach [7, 2, 12, 4, 11].", "startOffset": 98, "endOffset": 115}, {"referenceID": 0, "context": "In these works, learning filters with clustering methods, such as k-means, is a standard approach [7, 2, 12, 4, 11].", "startOffset": 98, "endOffset": 115}, {"referenceID": 10, "context": "In these works, learning filters with clustering methods, such as k-means, is a standard approach [7, 2, 12, 4, 11].", "startOffset": 98, "endOffset": 115}, {"referenceID": 2, "context": "In these works, learning filters with clustering methods, such as k-means, is a standard approach [7, 2, 12, 4, 11].", "startOffset": 98, "endOffset": 115}, {"referenceID": 9, "context": "In these works, learning filters with clustering methods, such as k-means, is a standard approach [7, 2, 12, 4, 11].", "startOffset": 98, "endOffset": 115}, {"referenceID": 8, "context": "Moreover, clustering methods can learn overcomplete dictionaries without additional modifications, such as done for ICA in [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "Nevertheless, since ICA [20] is also a common practice to learn filters, we conduct a couple of simple experiments with this method to probe our novel idea more thoroughly.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.", "startOffset": 38, "endOffset": 54}, {"referenceID": 5, "context": "In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.", "startOffset": 38, "endOffset": 54}, {"referenceID": 6, "context": "In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.", "startOffset": 38, "endOffset": 54}, {"referenceID": 7, "context": "In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.", "startOffset": 38, "endOffset": 54}, {"referenceID": 10, "context": "In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.", "startOffset": 38, "endOffset": 54}, {"referenceID": 11, "context": "In contrast to various coding schemes [6, 7, 8, 9, 12], popular in unsupervised learning, our forward pass is built upon a well established supervised method - a multilayer convolutional network [13] and its rectifying and pooling blocks.", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "Recently, this framework was successfully transfered to unsupervised learning in [11, 4].", "startOffset": 81, "endOffset": 88}, {"referenceID": 2, "context": "Recently, this framework was successfully transfered to unsupervised learning in [11, 4].", "startOffset": 81, "endOffset": 88}, {"referenceID": 9, "context": "For instance, in [11], k-means is enhanced by introducing convolutional clustering.", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": ", convolutional sparse coding [21].", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "Autoconvolution (or self-convolution) and its properties seem to be first analyzed in physics (in spectroscopy [22]) and later in function optimization [23] as the problem of deautoconvolution arose.", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "Autoconvolution (or self-convolution) and its properties seem to be first analyzed in physics (in spectroscopy [22]) and later in function optimization [23] as the problem of deautoconvolution arose.", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "This operator also appeared in visual tasks to extract invariant patterns [24].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "But, to the best of our knowledge, its recursive version, used as a pillar in this work, was first suggested in [25] for parametric description of images and temporal sequences.", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": ", [23].", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "We adopt the recursive autoconvoution operator, proposed earlier in [25] by simple extension of (1):", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "In [25], image patterns extracted using this operator were used for parametric description of images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "filters, because we noticed that applying (2) with n > 1 to images provides sparse wavelet like patterns, which are usually learned by a CNN in the first layer (see [16], Fig.", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "3 or [5], Fig.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "The baseline method for unsupervised filter learning is chosen to be k-means as in [11, 4].", "startOffset": 83, "endOffset": 90}, {"referenceID": 2, "context": "The baseline method for unsupervised filter learning is chosen to be k-means as in [11, 4].", "startOffset": 83, "endOffset": 90}, {"referenceID": 1, "context": ", in case n = [0, 3] we have 4", "startOffset": 14, "endOffset": 20}, {"referenceID": 12, "context": "In this global set, all patches are first scaled to have values in the range [0,1] , then they are ZCA-whitened as in [14, 7, 9, 11, 4].", "startOffset": 118, "endOffset": 135}, {"referenceID": 5, "context": "In this global set, all patches are first scaled to have values in the range [0,1] , then they are ZCA-whitened as in [14, 7, 9, 11, 4].", "startOffset": 118, "endOffset": 135}, {"referenceID": 7, "context": "In this global set, all patches are first scaled to have values in the range [0,1] , then they are ZCA-whitened as in [14, 7, 9, 11, 4].", "startOffset": 118, "endOffset": 135}, {"referenceID": 9, "context": "In this global set, all patches are first scaled to have values in the range [0,1] , then they are ZCA-whitened as in [14, 7, 9, 11, 4].", "startOffset": 118, "endOffset": 135}, {"referenceID": 2, "context": "In this global set, all patches are first scaled to have values in the range [0,1] , then they are ZCA-whitened as in [14, 7, 9, 11, 4].", "startOffset": 118, "endOffset": 135}, {"referenceID": 9, "context": "Second, in contrast to [11] or [7], where global or local normalization techniques are used, we use batch standardization (i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Second, in contrast to [11] or [7], where global or local normalization techniques are used, we use batch standardization (i.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "Between layers we use local contrast normalization (LCN), as in [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "Alternatively, connections between feature maps and filters can also be sparse [13] or learned along with the network weights [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "Alternatively, connections between feature maps and filters can also be sparse [13] or learned along with the network weights [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": ", 64 in [15]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "Therefore, in this work, we follow the practice, established in unsupervised CNNs [11, 4], to divide feature maps into ng relatively small groups, so that filters D \u2208 Rsl\u00d7sl\u00d7dl\u00d7Kl are learned independently for each group, where dl Kl\u22121.", "startOffset": 82, "endOffset": 89}, {"referenceID": 2, "context": "Therefore, in this work, we follow the practice, established in unsupervised CNNs [11, 4], to divide feature maps into ng relatively small groups, so that filters D \u2208 Rsl\u00d7sl\u00d7dl\u00d7Kl are learned independently for each group, where dl Kl\u22121.", "startOffset": 82, "endOffset": 89}, {"referenceID": 9, "context": "Contrary to [11, 4], where feature groups are formed randomly or learned in a supervised fashion, we use the approach from [9].", "startOffset": 12, "endOffset": 19}, {"referenceID": 2, "context": "Contrary to [11, 4], where feature groups are formed randomly or learned in a supervised fashion, we use the approach from [9].", "startOffset": 12, "endOffset": 19}, {"referenceID": 7, "context": "Contrary to [11, 4], where feature groups are formed randomly or learned in a supervised fashion, we use the approach from [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "This way, we try to find a compromise between diversity of filters and their connectivity to each other, which is necessary to form invariant groups [10].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "But as we apply a multidictionary approach similarly to [11, 12], this is not an issue, because eventually features from all layers are concatenated and fed to a classifier.", "startOffset": 56, "endOffset": 64}, {"referenceID": 10, "context": "But as we apply a multidictionary approach similarly to [11, 12], this is not an issue, because eventually features from all layers are concatenated and fed to a classifier.", "startOffset": 56, "endOffset": 64}, {"referenceID": 9, "context": "In [11], fully connected layers with dropout are trained on top of the unsupervised features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "For particularly large features (>30k and up to 300k in our experiments) random PCA [26] turned out to be extremely useful.", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "For better final classification results for CIFAR-10 and STL-10, we also apply a one-vs-all method (we use a very efficient GPU implementation from [27]).", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "A committee of models tends to give better classification results [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "We evaluate our method on several image classification benchmarks: MNIST [13], CIFAR-10 [14] and STL-10 [7].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "We evaluate our method on several image classification benchmarks: MNIST [13], CIFAR-10 [14] and STL-10 [7].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "We evaluate our method on several image classification benchmarks: MNIST [13], CIFAR-10 [14] and STL-10 [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": "[11, 3]: average results, and their standard deviations on the test set using 10 random subsets (folds) from the training set, are reported.", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[11, 3]: average results, and their standard deviations on the test set using 10 random subsets (folds) from the training set, are reported.", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "raw (n = 0); n = 1; n = 2; n = 3; n = [1, 3]; n = [0, 3]", "startOffset": 38, "endOffset": 44}, {"referenceID": 1, "context": "raw (n = 0); n = 1; n = 2; n = 3; n = [1, 3]; n = [0, 3]", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "Nevertheless, our results without whitening and with whitening applied only to filters are very relevant in practice, because in some tasks whitening of inputs is difficult to be performed or not applicable [10], e.", "startOffset": 207, "endOffset": 211}, {"referenceID": 28, "context": "6) in the ranges pj = [50, 400] for MNIST and pj = [30, 1500] for colored datasets gains another 0.", "startOffset": 51, "endOffset": 61}, {"referenceID": 1, "context": "In all experiments with 2 layer networks, for layer 1 we use combinations n = [1, 3] for MNIST and n = [0, 4] for others, while for layer 2 we found n = [2, 3] to be optimal.", "startOffset": 78, "endOffset": 84}, {"referenceID": 2, "context": "In all experiments with 2 layer networks, for layer 1 we use combinations n = [1, 3] for MNIST and n = [0, 4] for others, while for layer 2 we found n = [2, 3] to be optimal.", "startOffset": 103, "endOffset": 109}, {"referenceID": 0, "context": "In all experiments with 2 layer networks, for layer 1 we use combinations n = [1, 3] for MNIST and n = [0, 4] for others, while for layer 2 we found n = [2, 3] to be optimal.", "startOffset": 153, "endOffset": 159}, {"referenceID": 1, "context": "In all experiments with 2 layer networks, for layer 1 we use combinations n = [1, 3] for MNIST and n = [0, 4] for others, while for layer 2 we found n = [2, 3] to be optimal.", "startOffset": 153, "endOffset": 159}, {"referenceID": 9, "context": "In previous works [11, 4], filters of higher layers are distinct for each group (learned independently), i.", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "In previous works [11, 4], filters of higher layers are distinct for each group (learned independently), i.", "startOffset": 18, "endOffset": 25}, {"referenceID": 9, "context": "To exploit features from both layers, the multidictionary approach is employed as in [11, 12].", "startOffset": 85, "endOffset": 93}, {"referenceID": 10, "context": "To exploit features from both layers, the multidictionary approach is employed as in [11, 12].", "startOffset": 85, "endOffset": 93}, {"referenceID": 17, "context": "For the full tests on MNIST and CIFAR-10 we report the average for 10 independent tests (as done, for instance, in [19]) and, in addition, the results of averaging the SVM scores of all J \u00d7 10 models (denoted as \"comb\").", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "Model MNIST (100) MNIST (comb) 1-2 Layer, unsupervised, no data augmentation Sparse coding (169) [6] \u2212 0.", "startOffset": 97, "endOffset": 100}, {"referenceID": 26, "context": "59 C-SVDDNet (400+multiscale+SIFT) [28] \u2212 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "35 CONV-WTA (128-2048) [29] 1.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Supervised and semi-supervised state of the art Ladder Network (full cost)[19] 0.", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "02 Kernel CNN (12-400) [16] 2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "(96-1536) [11] 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "5 Stochastic pooling (64-64-64) [15] \u223c 4.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "The only exception seems to be Ladder Networks [19], which are much larger and deeper than our models and use a supervised cost.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "Also, in [28] better results are achieved for MNIST with a single layer, but in our experience it can be quite easy to fine tune to such a simple task.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "For STL-10 comparable results are obtained in [12, 28], however our model has several times fewer features compared to [12], while in [28] the most important model component is handcrafted SIFT features, whereas we learn features from data.", "startOffset": 46, "endOffset": 54}, {"referenceID": 26, "context": "For STL-10 comparable results are obtained in [12, 28], however our model has several times fewer features compared to [12], while in [28] the most important model component is handcrafted SIFT features, whereas we learn features from data.", "startOffset": 46, "endOffset": 54}, {"referenceID": 10, "context": "For STL-10 comparable results are obtained in [12, 28], however our model has several times fewer features compared to [12], while in [28] the most important model component is handcrafted SIFT features, whereas we learn features from data.", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "For STL-10 comparable results are obtained in [12, 28], however our model has several times fewer features compared to [12], while in [28] the most important model component is handcrafted SIFT features, whereas we learn features from data.", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "Our single layer model is especially effective for CIFAR-10, for which we obtain an accuracy better than in all previous multilayer unsupervised models (without data augmentation) and even better than a two layer CNN [16].", "startOffset": 217, "endOffset": 221}, {"referenceID": 1, "context": "It is also superior than many other supervised and unsupervised models, including a large 3 layer CNN in [3] (in case of full data), which relies on excessive data augmentation and a 3 layer supervised CNN [15] based on an advanced pooling scheme.", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "It is also superior than many other supervised and unsupervised models, including a large 3 layer CNN in [3] (in case of full data), which relies on excessive data augmentation and a 3 layer supervised CNN [15] based on an advanced pooling scheme.", "startOffset": 206, "endOffset": 210}, {"referenceID": 6, "context": "Model CIFAR-10 (400) CIFAR-10 (comb) STL-10 1-3 Layer, unsupervised, no data augmentation Sparse coding/OMP (1600/6000) [8] 66.", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "8 [9] 81.", "startOffset": 2, "endOffset": 5}, {"referenceID": 10, "context": "8 NOMP-20 (3200-6400-6400) [12] 72.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "6 C-SVDDNet (500+multiscale+SIFT) [28] \u2212 \u2212 68.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "(96-1536) [11] \u2212 \u2212 65.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "4 CONV-WTA (256-1024-4096) [29] \u2212 80.", "startOffset": 27, "endOffset": 31}, {"referenceID": 2, "context": "Supervised, semi-supervised or with data augmentation state of the art Committee of networks (300-5625) [4] \u2212 \u2212 68.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "55 View-invariant k-means (3 layers, 6400) [2] 72.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "7 Exemplar-CNN (92-256-512) [3] 77.", "startOffset": 28, "endOffset": 31}, {"referenceID": 16, "context": "3 SWWAE (8-10 layers) [18] \u2212 92.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "33 Ladder Network (\u0393-model, 7 layers) [19] 79.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "47 \u2212 \u2212 Kernel CNN (12-800+50-800) [16] \u2212 82.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "32 Stochastic pooling (64-64-64) [15] \u223c 65 84.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "For this purpose, we learned filters with ICA [20] on patches with n = 0 and n \u2265 0 (AutoCNN-ICA) using the same procedure as with k-means (see Section 4.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Thus, for convolutional layers of our large model for CIFAR-10 Nmodel = 400\u00d7 13\u00d7 13\u00d7 3 + 160\u00d7 11\u00d7 11\u00d7 4 \u2248 280k, while in [15] it is \u2248 200k, which is smaller only because of smaller filters.", "startOffset": 121, "endOffset": 125}], "year": 2016, "abstractText": "In visual recognition tasks, supervised learning shows excellent performance. On the other hand, unsupervised learning exploits cheap unlabeled data and can help to solve the same tasks more efficiently. We show that the recursive autoconvolutional operator, adopted from physics, boosts existing unsupervised methods to learn more powerful filters. We use a well established multilayer convolutional network and train filters layer-wise. To build a stronger classifier, we design a very light committee of SVM models. The total number of trainable parameters is also greatly reduced by using shared filters in higher layers. We evaluate our networks on the MNIST, CIFAR-10 and STL-10 benchmarks and report several state of the art results among other unsupervised methods.", "creator": "LaTeX with hyperref package"}}}