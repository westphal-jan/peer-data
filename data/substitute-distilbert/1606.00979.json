{"id": "1606.00979", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information", "abstract": "with the rapid growth of knowledge bases ( kbs ) on the web, how to combine full advantage of algorithms becomes visibly important. knowledge base - based question answering ( kb - a ) is one of the most promising approaches to access the substantial resources. meantime, as the neural network - based ( nn - based ) methods develop, nn - based kb - qa has already achieved impressive implementation. however, previous work did not put emphasis surrounding question representation, and the question is converted into a fixed vector regardless of its candidate answers. this simple representation strategy is unable to express the proper information of the recipient. hence, we present a neural attention - based model to represent the questions dynamically according to structurally different focuses of various candidate answer aspects. \" addition, we leverage the missing knowledge at the underlying kb, geared at integrating the internal kb matrix into dynamic representation of the answers. and it also alleviates the out loud conflict ( oov ) problem, which helps the attention relation to represent the question more precisely. the experimental results on webquestions imply the effectiveness concerning the proposed approach.", "histories": [["v1", "Fri, 3 Jun 2016 06:40:14 GMT  (297kb,D)", "http://arxiv.org/abs/1606.00979v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL cs.NE", "authors": ["yuanzhe zhang", "kang liu", "shizhu he", "guoliang ji", "zhanyi liu", "hua wu", "jun zhao"], "accepted": false, "id": "1606.00979"}, "pdf": {"name": "1606.00979.pdf", "metadata": {"source": "CRF", "title": "Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information", "authors": ["Yuanzhe Zhang", "Kang Liu", "Shizhu He", "Guoliang Ji", "Zhanyi Liu", "Hua Wu", "Jun Zhao"], "emails": ["jzhao}@nlpr.ia.ac.cn", "hua}@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "As the amount of the knowledge bases (KBs) grows, people are paying more attention to seeking effective methods for accessing these precious intellectual resources. There are several tailor-made languages designed for querying KBs, such as SPARQL [PrudHommeaux et al., 2008]. However, to handle such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the vocabularies of the KBs. By contrast, knowledge base-based question answering (KB-QA) [Unger et al., 2014], which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years.\nThe goal of KB-QA is to automatically return answers from the KB given natural language questions. There are\ntwo mainstream research directions for this task, i.e., semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al., 2014b; Bordes et al., 2014a; Dong et al., 2015; Bordes et al., 2015] methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods are more like to search answers from the KB based on the information conveyed in the questions. Here, ranking techniques are often adopted to make correct selections from candidate answers. In general, IR-based methods are easier and more flexible to implement. [Dong et al., 2015; Bordes et al., 2015] have proven that IRbased methods could acquire competitive performance compared with SP-based methods through the experiments conducted over Freebase [Bollacker et al., 2008].\nRecently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task [Bordes et al., 2014b]. They belong to IRbased methods. Different from previous methods, NN-based methods represent both the questions and the answers as semantic vectors. Then the complex process of KB-QA could be converted into a similarity matching process between an input question and its candidate answers in a semantic space. The candidates with the highest similarity score will be considered as the final answers. Because they are adaptive and robust, NN-based methods have attracted more and more attention, and this paper also focus on using neural networks to answer questions over knowledge base.\nIn NN-based methods, the crucial step is to compute the similarity score between a question and a candidate answer, where the key is to learn their representations. Previous methods put more emphasis on learning representations of the answer end. For example, [Bordes et al., 2014a] considers the importance of the subgraph of the candidate answers. [Dong et al., 2015] makes use of the context and the type of the answers. By contrast, the representation methods of the question end are oligotrophic. Existing approaches often represent a question into a single vector using a simple bag-of-words (BOW) model [Bordes et al., 2014b; Bordes et al., 2014a], whereas its relatedness to the answer end is neglected. We argue that a question should be repre-\nar X\niv :1\n60 6.\n00 97\n9v 1\n[ cs\n.I R\n] 3\nJ un\n2 01\n6\nsented differently according to the different focuses of various answer aspects1.\nTake question \u201cWho is the president of France?\u201d and one of its candidate answers \u201cFrancois Hollande\u201d as an example. When dealing with the answer entity Francois Holland, \u201cpresident\u201d and \u201cFrance\u201d in the question is more focused, and the question representation should bias towards the two words. While facing the answer type /business/board member, \u201cWho\u201d should be the most prominent word. Obviously, this is an attention mechanism, which reflects how the focus of answer aspects could influence the representation of the question.\nWhen learning the representations of the questions, we should make proper use of each word in the question according to different attention of each aspect of the candidate answer, instead of simply compressing them into a fixed vector. We believe that such kind of representations are more expressive. [Dong et al., 2015] represents questions using three CNNs with different parameters when dealing with different answer aspects including the answer path, the answer context and the answer type. We think simply selecting three independent CNNs is mechanical and inflexible. Thus, we go one step further, and propose an attentionbased neural network to perform question answering over KB. Different to [Dong et al., 2015], we represent the question differently according to different answer resources, not allowing them sharing the same network as [Dong et al., 2015] does. For instance, /business/board member and /location/country are both answer types, but the question representation will be different according to their different attention in our method.\nOn the other hand, we notice that the representations of the KB resources (entities and relations) are also limited in previous work. To be specific, they are often learned barely on the QA training data, which results in two limitations. 1) The deficiency of the global information of the KB. The previous methods merely utilize the answer-related part in the KB, i.e., answer path and answer context [Bordes et al., 2014a; Dong et al., 2015], to learn the representations of KB resources. The global information of the KB is completely ignored. For example, if question-answer pair (q, a) appears in the training data, and the global KB information implies us that a\u2032 is similar to a 2, denoted by (a \u223c a\u2032), then (q, a\u2032) is more probable to be right. However, current QA training mechanism cannot guarantee (a \u223c a\u2032) could be learned. 2) The problem of out of vocabulary (OOV). Due to the limited coverage of the training data, the OOV problem is common while testing, and many answer entities in testing candidate set have never been seen before. In this scenario, the representation of such unseen KB resources could not be learned precisely. The attention of these resources become the same because they shared the same OOV embedding, and this will do harm to the proposed attention model. To tackle these two problems, we additionally incorporates KB itself as train-\n1An answer aspect could be the answer entity itself, the answer type, the answer context, etc.\n2The complete KB is able to offer this kind of information, e.g., a and a\u2032 share massive context.\ning data for training embeddings besides original questionanswer pairs. In this way, the global structure of the whole knowledge could be captured, and the OOV problem could be alleviated naturally.\nIn summary, the contributions of this paper are as follows. 1) We present a novel attention-based NN model tailored to\nthe KB-QA task, which considers the influence of the answer aspects for representing questions.\n2) We leverage the global KB information, aiming at representing the answers more precisely. It also alleviates the OOV problem.\n3) The experimental results on the open dataset WEBQUESTIONS demonstrate the effectiveness of the proposed approach."}, {"heading": "2 Overview", "text": "The goal of the KB-QA task could be formulated as follows. Given a natural language question q, return an entity set A as answers. The architecture of our proposed KBQA system is shown in Figure 1, which illustrates the basic flow of our approach. First, we identify the topic entity of the question, and generate candidate answers from Freebase. Then, the candidate answers are represented with regard to their four aspects. Next, an attention-based neural network is employed to represent the question under the influence of the candidate answer aspects. Finally, the similarity score between the question and each corresponding candidate answer is calculated, and the candidates with the highest score will be selected as the final answers3.\nWe utilize Freebase [Bollacker et al., 2008] as our knowledge base. It now has more than 3 billion facts, and is used as the supporting KB for many QA tasks. In Freebase, the facts are represented by subject-property-object triples (s,p,o). For clarity, we call each basic element a resource, which could be either an entity or a relation. For example, (/m/0f8l9c, location.country.capital, /m/05qtj)4 describe the fact that the capital of France is Paris, /m/0f8l9c and /m/05qtj are entities denoting France and Paris respectively, and location.country.capital is a relation.\n3We also adopt a margin strategy to obtain multiple answers for a question and this will be explained in the next section.\n4Note that the Freebase prefixes are omitted for neatness."}, {"heading": "3 Our Approach", "text": ""}, {"heading": "3.1 Candidate Generation", "text": "The candidate answers should be all the entities of Freebase ideally, but in practice, this is time consuming and not really necessary. For each question q, we can use Freebase API [Bollacker et al., 2008] to identify a topic entity, which could be simply understood as the main entity of the question. For example, France is the topic entity of question \u201cWho is the president of France?\u201d. Freebase API method is able to resolve as many as 86% questions if we use the top1 result [Yao and Van Durme, 2014]. After getting the topic entity, we collect all the entities directly connected to it and the ones connected with 2-hop5. These entities constitute a candidate set Cq ."}, {"heading": "3.2 The Proposed Neural Attention Model", "text": "We present an attention-based neural network, which represents the question dynamically according to different answer aspects. Concretely, each aspect of the answer pays different attention to the question and thus decides how the question is represented. The extent of the attention is used as the weight of each word in the question. Figure 2 is the architecture of our model. We will illustrate how the system works as follows. LSTM\nFirst of all, we have to obtain the representation of each word in the question. These representations retain all the information of the question, and could serve the following steps. Suppose question q is expressed as q = (x1, x2, ..., xn), where xi denotes the ith word. As shown in Figure 2, we first look up a word embedding matrix Ew \u2208 Rd\u00d7vw to get the word embeddings, which is randomly initialized, and updated during the training process. Here, d means the dimension of the embeddings and vw denotes the vocabulary size of natural language words. Then, the embeddings are fed into a long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] networks. LSTM has been proven to be effective in many natural language processing (NLP) tasks such as machine translation [Sutskever et al., 2014] and dependency parsing [Dyer et al., 2015], and it is adept in harnessing long sentences. Note that if we use unidirectional LSTM, the outcome of a specific word contains only the information of the words before it, whereas the words after it is not taken into account. To avoid this, we employ bidirectional LSTM as [Bahdanau et al., 2015] does, which consists of both forward and backward networks. The forward LSTM handles the question from left to right, and the backward LSTM processes in the reverse order. Thus, we could acquire two hidden state sequences, one from the forward one (~h1,~h2, ...,~hn) and the other from the backward one ( \u2190 h1, \u2190 h2, ..., \u2190 hn). We concatenate the forward hidden state and the backward hidden state of each word, resulting in hj = [~hj ; \u2190 hj ]. The hidden unit of forward and backward LSTM is d2 , so the concatenated vector is of dimension d. In this way, we obtain the representation of each word in the question.\n5For example, (/m/0f8l9c, governing officials, government position held.office holder, /m/02qg4z) is a 2-hop connection.\nAnswer aspect representation In the answer end, we directly use the embedding for each answer aspect through the KB embedding matrix Ek \u2208 Rd\u00d7vk . Here, vk means the vocabulary size of the KB resources. The embedding matrix is randomly initialized and learned during training, and could be further enhanced with the help of the global information as described in Section 3.3. Concretely, we employ four kinds of answer aspects, namely, answer entity ae, answer relation ar, answer type at and answer context6 ac. Their embeddings are denoted as ee, er, et and ec respectively. It is worth noting that the answer context consists of multiple KB resources, and we denote it as (c1, c2, ..., cn). We first acquire their KB embeddings (ec1, ec2, ..., ecn) through Ek, then calculate an average em-\nbedding by ec = 1n n\u2211\ni=1\neci.\nAttention model The most crucial part of the proposed approach is the attention mechanism. Based on our assumption, each answer aspect should have different attention towards the same question. The extent of attention can be measured by the relatedness between each word representation hj and an answer aspect embedding ei. We propose the following formulas to calculate the weights.\n\u03b1ij = exp(wij) n\u2211 k=1 exp(wik) (1)\nwij =W T (tanh[hj ; ei]) + b (2)\nHere, \u03b1ij denotes the attention weight of the jth word in the question, in terms of answer aspect ei, where ei \u2208 {ee, er, et, ec}. n is the length of the question. W \u2208 R2d\u00d71 is\n6Here, the entities that directly connected to the answer entity is regarded as the answer context.\nan intermediate matrix and b is an offset value. Both of them are randomly initialized and updated during training. Subsequently, the attention weights are employed to calculate a weighted sum of the words, resulting in a semantic vector that represent the question, according to the specific answer aspect ei.\nqi = n\u2211 j=1 \u03b1ijhj (3)\nBy now, the similarity score of question q and this particular candidate answer a could be defined as follows.\nS(q, a) = \u2211\nei\u2208{ee,er,et,ec}\nqi \u00b7 ei (4)\nThe proposed attention model could also be intuitively interpreted as a re-reading mechanism [Hermann et al., 2015]. Our aim is to select correct answers from a candidate set. When we consider a candidate answer, suppose we first look at its type, and we will re-read the question to find out which part of the question should be more focused (handling attention). Then we go to next aspect and re-read the question again, until the all the aspects are utilized. We believe that this mechanism is beneficial for the system to better understand the question with the help of the answer aspects, and leads to a performance promotion. Training\nWe first construct the training data. Since we have question-answer pairs (q, a) as supervision data, candidate set Cq of question q can be divided into two subsets, namely, correct answer set Pq and wrong answer setNq . For each correct answer a \u2208 Pq , we randomly select k wrong answers a\u2032 \u2208 Nq as negative examples. For some topic entities, there may be not enough wrong answers to acquire k wrong answers. Under this circumstance, we extend Nq from other randomly selected candidate set Cq\u2032 . With the generated training data, we are able to make use of pairwise training.\nThe training loss is given as follows.\nLq,a,a\u2032 = [\u03b3 + S(q, a \u2032)\u2212 S(q, a)]+ (5)\nWhere \u03b3 is a positive real number that ensure a margin between positive and negative examples. And [z]+ means max(0, z). The intuition of this training strategy is to guarantee the score of positive question-answer pairs be higher than negative ones with a margin.\nThe objective function is as follows.\nmin \u2211 q 1 |Pq| \u2211 a\u2208Pq \u2211 a\u2032\u2208Nq Lq,a,a\u2032 (6)\nWe adopt stochastic gradient descent (SGD) to implement the learning process, mini-batches are utilized. Inference\nIn testing stage, we straightforwardly take advantage of the candidate answer set Cq of the question. We have to calculate S(q, a) for each a \u2208 Cq , and find out the maximum value Smax.\nSmax = argmax a\u2208Cq\n{S(q, a)} (7)\nIt is worth noting that many questions have more than one answer, so it is improper to set Smax as the final answer. Instead, we make use of the margin \u03b3 in the loss function, if the score of an candidate answer is within the margin compared with smax, we put it in the final answer set.\nA = {a\u0302|Smax \u2212 S(q, a\u0302) < \u03b3} (8)"}, {"heading": "3.3 Combining Global Knowledge Information", "text": "In this section, we elaborate how the global information of the KB could be leveraged. As stated before, we try to take into account the complete structural information of the KB. To this end, we adopt TransE model [Bordes et al., 2013] to represent the KB, and integrate the representations into the QA training process.\nIn TransE model, the entities and relations are represented by low dimensional embeddings. The basic idea is that the relations are regarded as translations in the embedding space. Here, for consistency, we denote each fact as (s, p, o), and use boldface (s, p, o) to denote their embeddings. The embedding of the tail entity o should be close to the embedding of head entity s plus the embedding of relation p, i.e., (s + p \u2248 o). The energy of a triple (s, p, o) is equal to d(s+p, o) for some dissimilarity d, defined as \u2016s + p \u2212 o\u201622. To learn the embeddings, TransE minimizes the following loss function. Lk = \u2211\n(s,p,o)\u2208S \u2211 (s\u2032,p,o\u2032)\u2208S\u2032 [\u03b3k + d(s+ p, o)\u2212 d(s\u2032 + p, o\u2032)]+\n(9) Where S is the set of KB facts and S\u2032 is the corrupted facts, which is composed of positive facts with either the head or tail replace by a random entity. The loss function favors lower values of the energy for positive facts than for negative facts.\nIn our implementation, we filter out the completely unrelated facts to save time. To be more specific, we first collect all the topic entities of all the questions as initial set. Then, we expand the set by adding direct connected and 2-hop entities. Finally, all the facts in which these entities appeared form the positive set. The negative facts are randomly corrupted ones. This a compromise solution due to the large scale of Freebase.\nTo combine the global information to our training process, we adopts a multi-task training strategy. Specifically, we perform our KB-QA training and TransE training in turn. After each epoch of KB-QA training, 100 epochs of TransE training is conducted, and the embeddings of the KB resources are shared and updated during both training processes. The proposed training process ensures that the global KB information act as additional supervision, and the interconnections among the resources are fully considered. In addition, as more KB resources are involved, the OOV problem will be relieved, which is able to bring additional benefits to the attention model."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "To evaluate the proposed method, we select WEBQUESTIONS [Berant et al., 2013] dataset that includes 3,778\nquestion-answer pairs for training and 2,032 for testing. The questions are collected from Google Suggest API, and the answers are labeled manually by Amazon MTurk. All the answers are from Freebase. We use three-quarter (2,833) of the training data as training set, and the remaining quarter as validate set. F1 score computed by the script provided by [Berant et al., 2013] is select as the evaluation metric"}, {"heading": "4.2 Settings", "text": "For KB-QA training, we use mini-batch stochastic gradient descent to minimize the pairwise training loss. The minibatch size is set to 50. The learning rate is set to 0.01. Both the word embedding matrix Ew and KB embedding matrix Ev are normalized after each epoch. The embedding size d = 128, and the hidden unit size is 64. Margin \u03b3 is set to 0.6. Negative example number k = 500. The TransE training process defines the embeddings dimension to 128, and the mini-batch size is also 50. \u03b3k is set to 1. All these hyperparameters of the proposed network is determined according to the performance on the validate set."}, {"heading": "4.3 Results", "text": "The Effectiveness of the proposed approach\nTo demonstrate the effectiveness of the proposed approach, we compare our method with previous NN-based methods. Table 1 shows the results on WEBQUESTIONS test set. The methods listed in the table all employ neural network for KBQA. [Bordes et al., 2014b] applies BOW method to obtain a single vector for both questions and answers. [Bordes et al., 2014a] further improves their work by proposing the concept of subgraph embeddings. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by BOW strategy. [Yang et al., 2014] follows the SP-based manner, but uses embeddings to map entities and relations into KB resources, then the question can be converted into logical forms. They jointly consider the two mapping process. [Dong et al., 2015] uses three columns of CNNs to represent questions corresponding to three aspects of the answers, namely the answer context, the answer path and the answer type. [Bordes et al., 2015] puts KB-QA into the memory networks [Sukhbaatar et al., 2015] framework, and achieves the state-of-the-art performance. ours represents the proposed approach.\nFrom the results, we can observe that ours achieves the best performance on WEBQUESTIONS. Here [Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015] all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically rep-\nresent the questions. Also note that [Bordes et al., 2015] uses additional training data such as Reverb [Fader et al., 2011] and their original dataset SimpleQuestions. [Dong et al., 2015] employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspect in the question representation. Besides, the global KB information is leveraged. So, we believe that the results faithfully show that the proposed approach is more effective than the other competitive methods. It is worth noting that [Yih et al., 2015] achieves an F1 of 52.5, much higher than other methods. Their staged system is able to address more questions with constraints and aggregations. However, their approach applies numbers of manually designed rules and features, which come from the observations on the training set questions. These particular manual efforts reduce the adaptability of their approach. Model Analysis\nIn this part, we further discuss the impacts of the components of our model. Table 2 indicates the effectiveness of different parts in the model.\nLSTM employs unidirectional LSTM, and uses the last hidden state as the question representation. Bi LSTM adopts a bidirectional LSTM. If we use (~h1,~h2, ...,~hn) to denote the forward LSTM, and use ( \u2190 h1, \u2190 h2, ..., \u2190 hn) to indicate backward LSTM, then the final presentation of the question is [~hn; \u2190 h1]. Bi LSTM+ATT is the bidirectional LSTM with neural attention (four answer aspects are used). Bi LSTM+GKI denote the bidirectional LSTM model with global KB information (GKI). Bi LSTMS+ATT+GKI is the same as ours, which is the bidirectional LSTM model with both attention model and global KB information.\nFrom the results, we could observe the followings. 1) Bi LSTM+ATT dramatically improves the F1 score by 2.7% compared with Bi LSTM. Similarly, Bi LSTM+ATT+GKI significantly outperforms Bi LSTM+ GKI by 2.2%. They straightforwardly prove that the proposed attention model is effective.\n2) Bi LSTM+GKI performs better than Bi LSTM, and achieves a 1.5% improvement. Similarly, Bi LSTM+ATT+GKI improves Bi LSTM+ATT by 1%. The results indicate that the proposed training strategy successfully leverages the global information of the underlying KB.\n3) Bi LSTM+ATT+GKI achieves the best performance as we expected, and improves the original Bi LSTM dramatically by 3.7%. This directly shows the power of the attention model and the global KB information.\nTo clearly demonstrate the effectiveness of the attention\nmechanism in our approach, we present the attention weights of a question in the form of heat maps as shown in Figure 3.\nFrom this example we can observe that our methods is able to capture the attention properly. It is instructive to figure out the attention part of the question when dealing with different answer aspects. The heat map will help us understand which parts are most useful for selecting correct answers. For instance, from Figure 3, we can see that location.country is paying great attention to \u201cWhere\u201d, indicating that \u201cWhere\u201d is much more important than the other parts in the question when dealing with this type. In other words, the other parts are not that crucial since \u2018Where\u201d is strongly implying that the question is asking about a location."}, {"heading": "4.4 Error Analysis", "text": "We randomly sample 100 imperfectly answered questions and categorize the errors into two main classes as follows. Wrong attention\nIn some occasions (17 in 100 questions, 17%), we find the generated attention weights unreasonable. For instance, for question \u201cWhat are the songs that Justin Bieber wrote?\u201d, answer type /music/composition pays the most attention on \u201cWhat\u201d rather than \u201csongs\u201d. We think this is due to the bias of the training data, and we believe these errors could be solved by introducing more instructive training data in the future. Complex questions and label errors\nAnother challenging problem is the complex questions (34%). For example, \u201cWhen was the last time Knicks won the championship?\u201d is actually to ask the last championship, but the predicted answers give all the championships. This is due to that the model cannot learn what does \u201clast\u201d mean in the training process. In addition, the label mistakes also influence the evaluation (3%) . For example, \u201cWhat college did John Nash teach at?\u201d. The labeled answer is Princeton University, but Massachusetts Institute of Technology should also be an answer, and the proposed method is able to answer it correctly.\nOther errors include topic entity generation error and the multiple answers error (giving more answers than expected). We guess these errors are caused by the simplest implementations of the related steps in our method, and we will not explain them in detail due to space limitation."}, {"heading": "5 Related Work", "text": ""}, {"heading": "5.1 Neural Network-based KB-QA", "text": "[Bordes et al., 2014b] first applies NN-based method to solve KB-QA problem. The questions and KB triples are represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method is employed to obtain a single vector for both the questions and the answers. Pairwise training is utilized, and the negative examples are randomly selected from the KB facts. They also present a training data generation method, i.e., using KB facts to and some heuristics rules to generate natural language questions.\n[Bordes et al., 2014a] further improves their work by proposing the concept of subgraph embeddings. The key idea is to involve as much as information in the answer end. Besides the answer triple, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by BOW strategy.\n[Yih et al., 2014] focuses on single-relation questions. The KB-QA task is divided into two parts, i.e., finding the entity mention-entity mapping and then mapping the remaining relation pattern to the KB relation. They train two CNN models to perform the mapping processes. [Yang et al., 2014] handles entity and relation mapping as joint procedures. Strictly speaking, these two methods follow the SP-based manner, but they take advantage of neural networks to obtain intermediate mapping results.\nThe most similar work to ours is [Dong et al., 2015]. They consider the different aspects of answers, using three columns of CNNs to represent questions respectively. The difference is that our approach uses attention mechanism for each unique answer aspect, so the question representation is not fixed to only three types. Moreover, we utilize the global KB information."}, {"heading": "5.2 Attention-based Model", "text": "[Bahdanau et al., 2015] first applies attention model in NLP. They improve the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning alignment and translation. They argue that representing the source sentence by a fixed vector is unreasonable, and propose a soft-align method, which could be understood as the attention mechanism.\n[Luong et al., 2015] is also tackling machine translation task. They propose two attentions models, i.e., a global model and a local model. The latter further indicates a small scope to attend, and achieves better results.\n[Rush et al., 2015] implements sentence-level summarization task. They utilize local attention-based model that generate each word of the summary conditioned on the input sentence.\nOur approach differs from previous work in that we are using attentions to help represent question dynamically, not generating current word from vocabulary as before."}, {"heading": "6 Conclusion", "text": "In this paper, we focus on the KB-QA task. First, we consider the impacts of the different answers and their aspects when\nrepresenting the question, and propose a novel attentionbased model for KB-QA. Specifically, the attention of the answer aspect for each word in the question is used. This kind of dynamic representation is more precise and flexible. Second, we leverage the global KB information, which could take full advantage of the complete KB, and also could alleviate the OOV problem. The extensive experiments demonstrate that the proposed approach could achieve better performance compared with other state-of-the-art NN-based methods."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of ICLR,", "citeRegEx": "Bahdanau et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang. Semantic parsing on freebase from question-answer pairs"], "venue": "pages 1533\u20131544,", "citeRegEx": "Berant et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "Proceedings of SIGMOD, pages 1247\u20131250. ACM,", "citeRegEx": "Bollacker et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Advances in Neural Information Processing Systems 26", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795.", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of EMNLP", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston. Question answering with subgraph embeddings"], "venue": "pages 615\u2013620,", "citeRegEx": "Bordes et al.. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 165\u2013180", "author": ["Antoine Bordes", "Jason Weston", "Nicolas Usunier. Open question answering with weakly supervised embedding models. In Machine Learning", "Knowledge Discovery in Databases"], "venue": "Springer,", "citeRegEx": "Bordes et al.. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "Proceedings of ICLR,", "citeRegEx": "Bordes et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of ACL", "author": ["Qingqing Cai", "Alexander Yates. Large-scale semantic parsing via schema matching", "lexicon extension"], "venue": "pages 423\u2013433,", "citeRegEx": "Cai and Yates. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of ACL and IJCNLP", "author": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu. Question answering over freebase with multicolumn convolutional neural networks"], "venue": "pages 260\u2013269,", "citeRegEx": "Dong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": "arXiv preprint arXiv:1505.08075,", "citeRegEx": "Dyer et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 1535\u20131545", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni. Identifying relations for open information extraction. In Proceedings of EMNLP"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Fader et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom. Teaching machines to read", "comprehend"], "venue": "pages 1684\u20131692,", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Proceedings of EMNLP", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer. Scaling semantic parsers with on-the-fly ontology matching"], "venue": "pages 1545\u20131556,", "citeRegEx": "Kwiatkowski et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of EMNLP", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning. Effective approaches to attentionbased neural machine translation"], "venue": "pages 1412\u20131421,", "citeRegEx": "Luong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Eric PrudHommeaux", "Andy Seaborne"], "venue": "Sparql query language for rdf. W3C recommendation, 15,", "citeRegEx": "PrudHommeaux et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "pages 379\u2013389,", "citeRegEx": "Rush et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "End-to-end memory networks. In Advances in Neural Information Processing Systems, pages 2431\u20132439,", "citeRegEx": "Sukhbaatar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Reasoning on the Web in the Big Data Era", "author": ["Christina Unger", "Andr\u00e9 Freitas", "Philipp Cimiano. An introduction to question answering over linked data. In Reasoning Web"], "venue": "pages 100\u2013140.", "citeRegEx": "Unger et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of EMNLP", "author": ["Min-Chul Yang", "Nan Duan", "Ming Zhou", "Hae-Chang Rim. Joint relational embeddings for knowledge-based question answering"], "venue": "pages 645\u2013650,", "citeRegEx": "Yang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Xuchen Yao", "Benjamin Van Durme"], "venue": "Proceedings of ACL, pages 956\u2013966,", "citeRegEx": "Yao and Van Durme. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of ACL", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek. Semantic parsing for single-relation question answering"], "venue": "pages 643\u2013648,", "citeRegEx": "Yih et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "Proceedings of ACL and IJCNLP, pages 1321\u2013 1331,", "citeRegEx": "Yih et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S Zettlemoyer", "Michael Collins"], "venue": "Proceedings of UAI, pages 658\u2013666,", "citeRegEx": "Zettlemoyer and Collins. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "In Proceedings of ACLIJCNLP", "author": ["Luke S Zettlemoyer", "Michael Collins. Learning context-dependent mappings from sentences to logical form"], "venue": "pages 976\u2013984,", "citeRegEx": "Zettlemoyer and Collins. 2009", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "There are several tailor-made languages designed for querying KBs, such as SPARQL [PrudHommeaux et al., 2008].", "startOffset": 82, "endOffset": 109}, {"referenceID": 19, "context": "By contrast, knowledge base-based question answering (KB-QA) [Unger et al., 2014], which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years.", "startOffset": 61, "endOffset": 81}, {"referenceID": 24, "context": ", semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al.", "startOffset": 36, "endOffset": 184}, {"referenceID": 25, "context": ", semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al.", "startOffset": 36, "endOffset": 184}, {"referenceID": 13, "context": ", semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al.", "startOffset": 36, "endOffset": 184}, {"referenceID": 7, "context": ", semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al.", "startOffset": 36, "endOffset": 184}, {"referenceID": 1, "context": ", semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al.", "startOffset": 36, "endOffset": 184}, {"referenceID": 23, "context": ", semantic parsing-based (SP-based) [Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al.", "startOffset": 36, "endOffset": 184}, {"referenceID": 21, "context": ", 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al., 2014b; Bordes et al., 2014a; Dong et al., 2015; Bordes et al., 2015] methods.", "startOffset": 50, "endOffset": 159}, {"referenceID": 5, "context": ", 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al., 2014b; Bordes et al., 2014a; Dong et al., 2015; Bordes et al., 2015] methods.", "startOffset": 50, "endOffset": 159}, {"referenceID": 4, "context": ", 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al., 2014b; Bordes et al., 2014a; Dong et al., 2015; Bordes et al., 2015] methods.", "startOffset": 50, "endOffset": 159}, {"referenceID": 8, "context": ", 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al., 2014b; Bordes et al., 2014a; Dong et al., 2015; Bordes et al., 2015] methods.", "startOffset": 50, "endOffset": 159}, {"referenceID": 6, "context": ", 2015] and information retrieve-based (IR-based) [Yao and Van Durme, 2014; Bordes et al., 2014b; Bordes et al., 2014a; Dong et al., 2015; Bordes et al., 2015] methods.", "startOffset": 50, "endOffset": 159}, {"referenceID": 8, "context": "[Dong et al., 2015; Bordes et al., 2015] have proven that IRbased methods could acquire competitive performance compared with SP-based methods through the experiments conducted over Freebase [Bollacker et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 6, "context": "[Dong et al., 2015; Bordes et al., 2015] have proven that IRbased methods could acquire competitive performance compared with SP-based methods through the experiments conducted over Freebase [Bollacker et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 2, "context": ", 2015] have proven that IRbased methods could acquire competitive performance compared with SP-based methods through the experiments conducted over Freebase [Bollacker et al., 2008].", "startOffset": 158, "endOffset": 182}, {"referenceID": 5, "context": "Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task [Bordes et al., 2014b].", "startOffset": 125, "endOffset": 147}, {"referenceID": 4, "context": "For example, [Bordes et al., 2014a] considers the importance of the subgraph of the candidate answers.", "startOffset": 13, "endOffset": 35}, {"referenceID": 8, "context": "[Dong et al., 2015] makes use of the context and the type of the answers.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Existing approaches often represent a question into a single vector using a simple bag-of-words (BOW) model [Bordes et al., 2014b; Bordes et al., 2014a], whereas its relatedness to the answer end is neglected.", "startOffset": 108, "endOffset": 152}, {"referenceID": 4, "context": "Existing approaches often represent a question into a single vector using a simple bag-of-words (BOW) model [Bordes et al., 2014b; Bordes et al., 2014a], whereas its relatedness to the answer end is neglected.", "startOffset": 108, "endOffset": 152}, {"referenceID": 8, "context": "[Dong et al., 2015] represents questions using three CNNs with different parameters when dealing with different answer aspects including the answer path, the answer context and the answer type.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Different to [Dong et al., 2015], we represent the question differently according to different answer resources, not allowing them sharing the same network as [Dong et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 8, "context": ", 2015], we represent the question differently according to different answer resources, not allowing them sharing the same network as [Dong et al., 2015] does.", "startOffset": 134, "endOffset": 153}, {"referenceID": 4, "context": ", answer path and answer context [Bordes et al., 2014a; Dong et al., 2015], to learn the representations of KB resources.", "startOffset": 33, "endOffset": 74}, {"referenceID": 8, "context": ", answer path and answer context [Bordes et al., 2014a; Dong et al., 2015], to learn the representations of KB resources.", "startOffset": 33, "endOffset": 74}, {"referenceID": 2, "context": "We utilize Freebase [Bollacker et al., 2008] as our knowledge base.", "startOffset": 20, "endOffset": 44}, {"referenceID": 2, "context": "For each question q, we can use Freebase API [Bollacker et al., 2008] to identify a topic entity, which could be simply understood as the main entity of the question.", "startOffset": 45, "endOffset": 69}, {"referenceID": 21, "context": "Freebase API method is able to resolve as many as 86% questions if we use the top1 result [Yao and Van Durme, 2014].", "startOffset": 90, "endOffset": 115}, {"referenceID": 12, "context": "Then, the embeddings are fed into a long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997] networks.", "startOffset": 66, "endOffset": 100}, {"referenceID": 18, "context": "LSTM has been proven to be effective in many natural language processing (NLP) tasks such as machine translation [Sutskever et al., 2014] and dependency parsing [Dyer et al.", "startOffset": 113, "endOffset": 137}, {"referenceID": 9, "context": ", 2014] and dependency parsing [Dyer et al., 2015], and it is adept in harnessing long sentences.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "To avoid this, we employ bidirectional LSTM as [Bahdanau et al., 2015] does, which consists of both forward and backward networks.", "startOffset": 47, "endOffset": 70}, {"referenceID": 11, "context": "The proposed attention model could also be intuitively interpreted as a re-reading mechanism [Hermann et al., 2015].", "startOffset": 93, "endOffset": 115}, {"referenceID": 3, "context": "To this end, we adopt TransE model [Bordes et al., 2013] to represent the KB, and integrate the representations into the QA training process.", "startOffset": 35, "endOffset": 56}, {"referenceID": 1, "context": "To evaluate the proposed method, we select WEBQUESTIONS [Berant et al., 2013] dataset that includes 3,778", "startOffset": 56, "endOffset": 77}, {"referenceID": 1, "context": "F1 score computed by the script provided by [Berant et al., 2013] is select as the evaluation metric", "startOffset": 44, "endOffset": 65}, {"referenceID": 5, "context": "[Bordes et al., 2014b] applies BOW method to obtain a single vector for both questions and answers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "[Bordes et al., 2014a] further improves their work by proposing the concept of subgraph embeddings.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "[Yang et al., 2014] follows the SP-based manner, but uses embeddings to map entities and relations into KB resources, then the question can be converted into logical forms.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "[Dong et al., 2015] uses three columns of CNNs to represent questions corresponding to three aspects of the answers, namely the answer context, the answer path and the answer type.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "[Bordes et al., 2015] puts KB-QA into the memory networks [Sukhbaatar et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": ", 2015] puts KB-QA into the memory networks [Sukhbaatar et al., 2015] framework, and achieves the state-of-the-art performance.", "startOffset": 44, "endOffset": 69}, {"referenceID": 5, "context": "Here [Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015] all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions.", "startOffset": 5, "endOffset": 70}, {"referenceID": 4, "context": "Here [Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015] all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions.", "startOffset": 5, "endOffset": 70}, {"referenceID": 6, "context": "Here [Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015] all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions.", "startOffset": 5, "endOffset": 70}, {"referenceID": 6, "context": "Also note that [Bordes et al., 2015] uses additional training data such as Reverb [Fader et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 10, "context": ", 2015] uses additional training data such as Reverb [Fader et al., 2011] and their original dataset SimpleQuestions.", "startOffset": 53, "endOffset": 73}, {"referenceID": 8, "context": "[Dong et al., 2015] employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspect in the question representation.", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "It is worth noting that [Yih et al., 2015] achieves an F1 of 52.", "startOffset": 24, "endOffset": 42}, {"referenceID": 5, "context": "[Bordes et al., 2014b] first applies NN-based method to solve KB-QA problem.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "[Bordes et al., 2014a] further improves their work by proposing the concept of subgraph embeddings.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "[Yih et al., 2014] focuses on single-relation questions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "[Yang et al., 2014] handles entity and relation mapping as joint procedures.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "The most similar work to ours is [Dong et al., 2015].", "startOffset": 33, "endOffset": 52}, {"referenceID": 0, "context": "[Bahdanau et al., 2015] first applies attention model in NLP.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "[Luong et al., 2015] is also tackling machine translation task.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "[Rush et al., 2015] implements sentence-level summarization task.", "startOffset": 0, "endOffset": 19}], "year": 2016, "abstractText": "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge basebased question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural networkbased (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. And it also alleviates the out of vocabulary (OOV) problem, which helps the attention model to represent the question more precisely. The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}