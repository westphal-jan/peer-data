{"id": "1606.01545", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Neural Net Models for Open-Domain Discourse Coherence", "abstract": "discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. yet existing models of coherence focus on individual aspects of experience ( lexical overlap, rhetorical variability, personality differentiation ) and are trained on narrow domains. we introduce algorithms therefore capture three kinds on interactions by users to distinguish coherent from everyday discourse from massive amounts of open - domain training infrastructure. we propose two models, one discriminative and one generative, both using lstms as the backbone. the discriminative model represents windows of sentences from original speaker - generated articles as coherent examples and windows generated by randomly replacing sentences as incoherent examples. the generative model is a \\ sts whenever each describes the probability of generating a sentence given its contexts. our models achieve state - of - the - art performance on multiple coherence evaluations. qualitative analysis suggests that such analytic model captures many technologies illustrating coherence including lexical, temporal, causal, and entity - based coherence.", "histories": [["v1", "Sun, 5 Jun 2016 18:29:45 GMT  (209kb,D)", "http://arxiv.org/abs/1606.01545v1", null], ["v2", "Sun, 29 Jan 2017 00:21:43 GMT  (502kb,D)", "http://arxiv.org/abs/1606.01545v2", null], ["v3", "Sun, 24 Sep 2017 01:38:11 GMT  (492kb,D)", "http://arxiv.org/abs/1606.01545v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "dan jurafsky"], "accepted": true, "id": "1606.01545"}, "pdf": {"name": "1606.01545.pdf", "metadata": {"source": "CRF", "title": "Neural Net Models for Open-Domain Discourse Coherence", "authors": ["Jiwei Li", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even applications like psychiatric diagnosis (Elveva\u030ag et al., 2007; Bedi et al., 2015).\nVarious frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan,\n1System, code and datasets available upon publication.\n1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences. Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts. Entity grid models (Barzilay and Lapata, 2008) model the referential coherence of entities moving in and out of focus across a text. None of the models fully captures the rich semantic, discourse, and inferential links between coherent text units. Furthermore, previous work has been difficult to scale up and apply in open domains.\nWe propose to capture many of these aspects of discourse coherence (e.g., lexical, causal, entity focus) in neural net frameworks. We present two models: a discriminative model that induces coherence in an unsupervised manner from large datasets of real world texts by treating human generated texts as coherent examples and texts with random sentence replacements as negative examples; and a generative model that uses sequence-to-sequence models (Sutskever et al., 2014) (SEQ2SEQ) to model the likelihood of generating a sentence based on its context.\nWe evaluate the models on two text-ordering datasets, one from the literature (Barzilay and Lapata, 2008), and a new larger open-domain one. The discriminative model achieves state-of-the-art performance on the domain specific dataset presented in Barzilay and Lapata (2008), pushing the state-ofthe-art result to 96% accuracy, significantly outperforming all previous models. The generative model obtains the best result on a large open-domain setting, including on the difficult task of reconstructing paragraph order, and qualitative evaluation suggests that it captures multiple types of coherence.\nar X\niv :1\n60 6.\n01 54\n5v 1\n[ cs\n.C L\n] 5\nJ un\n2 01\n6"}, {"heading": "2 Related Work", "text": "There are many frameworks for discourse coherence:\nLexical Coherence Coherence is strongly cued by words: words linked by identity, synonymy or other lexical relations forming chains across discourse segments (Halliday and Hasan, 1976). Early models used tools like thesauri (Morris and Hirst, 1991). Later work used Latent Semantic Analysis (LSA) embeddings (Foltz et al., 1998; Foltz, 2007), representing sentences with LSA vectors and measuring coherence with the cosine similarity of adjacent sentences, with the goal of capturing more subtle lexical relations that might not be available in thesauri.\nStructured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).\nEntity Grid Models Many recent coherence models are based instead on centering theory, a model of which entity is in focus at a point in the discourse, and how smoothly that focus shifts from sentence to sentence depending, e.g., on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004). The most influential such model is the entity grid model of Barzilay and Lapata (2008), in which sentences are represented by a vector of coreferent discourse entities along with their grammatical roles. Probabilities of entity transitions between adjacent sentences are concatenated to document vector representation, used as the input to machine learning classifiers. Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011), and entity graphs (Guinaudeau and Strube, 2013).\nNeural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014).\nOur proposed discriminative model extends the coherence model of Li and Hovy (2014), a neural classifier trained on small domain-specific datasets (earthquake and accidents) using negative sampling at the sentence level. The algorithm we present significantly outperforms the classifier of Li and Hovy (2014).\nThe proposed generative model uses a SEQ2SEQ backbone to generate a sentence from its contexts. SEQ2SEQ models have been successfully applied to a variety of NLP tasks including machine translation (Sutskever et al., 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al., 2015). Our idea of predicting the current sentence based on the previous one is similar to skip-thought models (Kiros et al., 2015) that build an LSTM encoder-decoder model by predicting tokens in neighboring sentences. We use the mutual dependency between the two consecutive sequences to measure coherence. This idea of modeling the mutual dependency between two sequences for neural generation has been explored by Li et al. (2015) for dialogue generation.\nThe two models we propose can also be viewed as the a kind of generalization of the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to the sentence level. The generative model that predicts the next sentence based on the previous sentence is comparable to the skip-gram algorithm\u2019s predicting the next word given its context using a softmax function. The discriminative model is comparable to the negative sampling strategy (Goldberg and Levy, 2014) which has been widely used in training word embeddings."}, {"heading": "3 Models", "text": "In this section, we describe the two models for coherence modeling, which are respectively suitable for different scenarios in real world applications."}, {"heading": "3.1 The Discriminative Model", "text": "Notations Let C denote a sequence of coherent texts taken from original articles generated by humans. C is comprised of a sequence of sentences C = {sn\u2212L, ..., sn\u22121, sn, sn+1, ..., sn+L} where L denotes the half size of the context window. Each sentence s is comprised of a sequence of words s = {w1, w2, ...}. Each word w is associated with\na K dimensional vector hw and each sentence is as well associated with a K dimensional vector xs.\nThe model we propose is demonstrated in Figure 1. We treat cliques taken from the original articles as coherent positive examples and cliques with random replacements of center sentence sn as negative ones. Each clique C is thus associated with a binary variable yC indicating whether it is from original human generated articles or from random replacements.\nEach clique C is associated with a (2L+ 1)\u00d7K dimensional vector by concatenating the representations of its constituent sentences2. The sentence representation is obtained from LSTMs. For word compositions, we use the representation output from the last time step to represent the entire sentence. By concatenating representations of its constituent sentences, we obtain a (2L + 1) \u00d7 K dimensional vector for C. We then map the (2L+ 1)\u00d7K representation to a K dimensional vector using nonlinear composition:\nL(C) = tanh(W \u00b7 [xsn\u2212L , ..., xsn , ..., xsn+L ]) (1)\nwhere W \u2208 RK\u00d7(2L+1)K . To model negative incoherent examples, we resort to noise contrastive estimation (Gutmann and Hyva\u0308rinen, 2010). Let p(yC = 1|C) denote the possibility that clique C is coherent. Correspondingly, p(yC = 0|C) = 1\u2212 p(yC = 1|C)\n2To classify first/last sentences and include their context, we would need special beginning/ending sentence vectors; for simplicity, we treat all elements in beginning/ending sentence vectors as zero.\ndenotes the probability that the clique with sentence randomly sampled and incoherent. The probability that the current pair is coherent, i.e., p(yC = 1|C) is given by:\nlog p(yC = 1|C) = log 1\n1 + exp(\u2212UT \u00b7 L(C)) (2)\nwhere U \u2208 R1\u00d7K . For a given C, let C \u2032 denote the collection of negative cliques generated by replacing the middle sentence sN . The loss function is then:\nLoss = log p(yC = 1|C) + \u2211 C\u2032 log p(yC\u2032 = 0|C \u2032)\n(3) The proposed model can be viewed as an extension of Li and Hovy\u2019s (2014) model but is practical at large scale3.\nTraining Word vectors and LSTM parameters are randomly initialized from the uniform distribution [-0.1,0.1]. Since the model does not require softmax for word prediction, we keep a relative large vocabulary of the top 200,000 most frequent words. We adopt stochastic gradient decent with min-batch size 128 and clip the gradients if the norm of gradient vectors exceed 5. We set the number of negative examples to 10, 5 of which are sampled from the same document, and the rest from random documents. We use a dropout rate of 0.2 in training and employ no\n3Li and Hovy\u2019s (2014) recursive neural model operates on parse trees, which does not support batched computation and is therefore hard to scale up.\nregularizers. We run 7 epochs with initial learning rate of 1.0. After 4 iterations, we begin halving the learning rate after each epoch."}, {"heading": "3.2 The Generative Model", "text": "In a coherent context, a machine should be able to guess the next utterance given the previous one. We therefore propose measuring the degree of coherence using the likelihood of observing a sentence given its context.\nGiven two consecutive sentences [si, si+1], we measure the coherence by combining the likelihood of generating si given si+1 and generating si+1 given si:\nL(si, si+1) = 1\n2 [log p(si|si+1) + log p(si+1|si)]\n(4) Eq.4 measures the mutual dependency between the two consecutive sentences. Both p(si|si+1) and p(si+1|si) can be computed using SEQ2SEQ models (Sutskever et al., 2014). SEQ2SEQ models define a distribution over outputs y and sequentially predict tokens using a softmax function:\np(y|x) = nT\u220f t=1 exp(f(ht\u22121, eyt))\u2211 w\u2032 exp(f(ht\u22121, ew\u2032))\nwhere f(ht\u22121, eyt) denotes the activation function between ht\u22121 and ewt , where ht\u22121 is the representation output from the LSTM at time t\u2212 1. Each sentence concludes with a special end-of-sentence symbol EOS. Commonly, the input and output each use different LSTMs with separate sets of compositional parameters to capture different compositional patterns. During decoding, the algorithm terminates when an EOS token is predicted.\nWe separately train two models: p(si+1|si) that predicts the next sentence based on the previous one from the original passages and and p(si|si+1) that predicts the previous sentence given the next sentence. p(si|si+1) can be trained in the similar way as p(si+1|si) with sources and targets swapped. To avoid the model favoring shorter sequences, the log likelihood is divided by the length of the sequence.\nTraining We adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decoding, each of which consists of a different set of parameters. Each LSTM layer consists of 1,000\nhidden neurons, and the dimensionality of word embeddings is set to 1,000. We keep a vocabulary of the top 50,000 most frequent words. Other training details are given below, broadly aligned with Sutskever et al. (2014): LSTM parameters and embeddings are initialized from a uniform distribution in [-0.1, 0.1]; Stochastic gradient decent is implemented using a fixed learning rate of 0.1; Batch size is set to 128; Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. We run 8 epochs in total and start halving the learning rate after 5 epochs."}, {"heading": "4 Experimental Results", "text": "In this section, we describe experimental results. We first evaluate the proposed model on the task of sentence ordering using two datasets, a standard domain-specific dataset (Barzilay and Lapata, 2008) and a newly constructed open-domain dataset from Wikipedia. Next we propose the task of paragraph reconstruction that reconstruct an original paragraph from its constituent sentences whose order has been permuted."}, {"heading": "4.1 Sentence Ordering, Domain-specific Data", "text": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011). The corpus consists of 200 articles each from two domains: NTSB airplane accident reports (V=4758, 10.6 sentences/document) and AP earthquake reports (V=3287, 11.5 sentences/document), split into training and testing. For each document, pairs of permutations are generated4. Each pair contains the original document order and a random permutation of the sentences from the same document.\nWe use reduced versions of both our models to 4Permutations downloaded from people.csail.mit.\nedu/regina/coherence/CLsubmission/.\nallow fair comparison with baselines. We therefore do not use the massive Wikipedia training set, holding the datasize constant and training only on the earthquake/traffic training set. For the discriminative model, we generate noise negative examples from random replacements in the training set, with the only difference that random replacements only come from the same document. We use 300 dimensional embeddings borrowed from GLOVE (Pennington et al., 2014) to initialize word embeddings. Word embeddings are kept fixed during training and we update LSTM parameters using AdaGrad (Duchi et al., 2011). For the generative model, due to the small size of the dataset, we train a one layer LSTM SEQ2SEQ model with word dimensionality and number of hidden neurons set to 100.\nWe report performances from the following widely used baselines in coherence literature.\n(1) Entity Grid Model: The grid model (Barzilay and Lapata, 2008) represents the sentence as a column of a grid of features and applies machine learning methods (e.g., SVM) to identify the coherent transitions based on entity features. Results are directly taken from Barzilay and Lapata\u2019s (2008) paper.\n(2) HMM: A hidden-markov model described in Louis and Nenkova (2012) models the cluster transition probability in the coherent texts. Results are from their paper.\n(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013).\n(4) Li and Hovy (2014): A recursive neural model computes sentence representations based on parse trees. Negative sampling is used to construct negative incoherent examples. Representations of neighboring sentences are concatenated and fed into a neural classification, outputting whether a clique of sequences is coherent or not. Results are from their paper\n(5) Foltz et al. (1998) computes the semantic relatedness of two text units as the cosine similarity between their LSA vectors. The coherence of a discourse is the average of the cosine of adjacent sentences. We used this intuition, but with more modern embedding models: (1) 300-dimensional Glove word vectors (Pennington et al., 2014), embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al., 2003) with 300 topics, trained on the Wikipedia dataset using Gibbs sampling. We compute coherence as the average cosine between adjacent sentences. Since embeddings are pre-trained, these models do not make use of training data.\nResults are reported in Table 2. The proposed discriminative model significantly outperforms the model presented in Li and Hovy (2014) as well as all non-neural baselines. It achieves roughly 100% accuracy on the earthquake dataset and 93% on the accident dataset, marking a significant advancement in the benchmark. The generative model does not perform competitively on this dataset. This is due to the small size of the dataset, leading the generative model to overfit.\nThe simple LSA method of calculating cosine similarity between adjacent sentences, adopted from Foltz et al. (1998), does not yield competitive results, con-\nfirming that while simple centroids of word embeddings may do a good job of modeling lexical coherence, lexical coherence is only one component of discourse coherence."}, {"heading": "4.2 Evaluating Ordering on Open-domain", "text": "Since the dataset presented in Barzilay and Lapata (2008) is quite domain-specific, we propose testing coherence with a much larger, open-domain dataset: Wikipedia. We created a test set by randomly selecting 984 paragraphs from Wikipedia dump 2014, each paragraph consisting of at least 16 sentences. The training set is the 80 million sentences. We ensure that there is no overlap between the training set and the test set. Based on this dataset, we define the following tasks for evaluation:"}, {"heading": "4.2.1 Binary Permutation Classification", "text": "We adopt the same strategy as in Barzilay and Lapata (2008), in which we generate permutations for the original Wikipedia paragraphs. We follow the protocols described in the subsection above to compare the degree of coherence between the original texts and their permutations. Each pair whose original paragraph\u2019s score is higher than its permutation is treated as being correctly classified, else incorrectly classified. Models are evaluated using accuracy.\nBaselines Our baselines consist of the Glove and LDA updates of the lexical coherence baselines (Foltz et al., 1998). We also implement the Entity Grid Model (Barzilay and Lapata, 2008) using the Wikipedia training set. For each noun in a sentence, we extract its syntactic role (subject, object or other). We use a wikipedia dump parsed using the Fanse Parser (Tratz and Hovy, 2011). Subjects and objects are extracted based on nsubj and dobj relations in the dependency trees. (Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).5 We also employ the uni-directional\n5Our implementation of the Entity Grid Model is built upon public available code at https://github.com/karins/ CoherenceFramework.\nbaseline in which the coherence score is computed using only p(si+1|si), i.e., predicting the next sentence given the previous one.\nResults Figure 3 presents results on the binary classification task. Once again, purely lexical methods (Foltz et al., 1998) do not yield compelling results. Contrary to the findings on the domain specific dataset in the previous subsection, the discriminative model does not yield compelling results, performing only slightly better than the entity grid model. We believe the poor performance is due to the sentencelevel negative sampling used by the discriminative model. Due to the huge semantic space in the opendomain setting, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don\u2019t cover the space of possible meanings. By contrast the dataset in Barzilay and Lapata (2008) is very domain-specific, and the semantic space is thus relatively small. By treating all other sentences in the document as negative, the discriminative strategy\u2019s negative samples form a much larger proportion of the semantic space, leading to good performance.\nThe proposed generative model performs significantly better than all other baselines. Compared with the dataset in Barzilay and Lapata (2008), overfitting is not an issue here due to the great amount of training data. In line with our expectation the bi-directional model which models the bidirectional dependency between the two consecutive sentences outperforms the uni-directional model which only handles the case of predicting the next sentence."}, {"heading": "4.2.2 Paragraph Reconstruction", "text": "The accuracy of our models on the binary task of detecting the original sentence ordering is very high, on both the prior small task and our large opendomain version. We therefore believe it is time for\nthe community to move to a more difficult task for measuring coherence.\nWe suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003). More formally, given a set of permuted sentences s1, s2, ..., sN (N the number of sentences in the original document), our goal is return the original (presumably most coherent) ordering of s.\nBecause the discriminative model calculates the coherence of a sentence given the known previous and following sentences, it cannot be applied to this task since we don\u2019t know the surrounding context. Hence, we only use the generative model. We explore the following two settings:\n(1) The first sentence of a paragraph is given: for each step, we compute the coherence score of placing each remaining candidate sentence to the right of the partially constructed document. We use beam search with beam size 10.6\n(2) No clue is given: we employed the graph based method described in Lapata (2003). We first construct a graph where the each vertex denotes a sentence and the edge weight u\u2192 v denotes the coherence score of sentence v coming after u. Note that weight values for u \u2192 v and v \u2192 u are different. We initialize the vertex list V using all vertexes in the graph. Similar to Lapata (2003), we employ a greedy search model. The greedy algorithm first picks the edge u \u2192 v with the highest coherence score, and deletes all the outgoing edges from vertex u and all incoming edges to vertex v. u, v are removed from the vertex list V . Next, for each time step, let vleft and vright respectively denote the left-most\n6Not guaranteed to find the optimal solution since this generation task is known to be NP-complete (0).\nand right-most node in the partially constructed paragraph. The greedy model chooses whether to expand the paragraph to the left and to the right by comparing maxv\u2032\u2208V S(v\u2032, vleft) with maxv\u2032\u2208V S(vright, v\u2032), where the former denotes the maximal coherence score of placing a remaining sentence to the left of the partially constructed paragraph and the latter denotes the maximal score of appending a sentence to the right of the paragraph. The newly selected node is added to the paragraph and removed from the vertex list V . We repeat this process until V is empty.\nWe use the Entity Grid model as a baseline for both the settings. LSA-style cosine similarity based lexical methods are symmetric regarding the next sentence and the previous sentence. We therefore can not tell which sentence should come first. We thus only use it as a baseline in the first-sentence-beinggiven setting.\nEvaluating the absolute positions of sentences would be too harsh, penalizing orderings that maintain relative position between sentences through which local coherence can be manifested. We therefore use Kendall\u2019s Tau (Lapata, 2003; Lapata, 2006), a metric of rank correlation for evaluation. Kendall\u2019s \u03c4 is computed based on the number of inversions in the rankings as follows:\n\u03c4 = 1\u2212 2# of inversions N \u00d7 (N \u2212 1)\n(5)\nwhere N denotes the number of sentences in the original document and inversions denote the number of interchanges of consecutive elements needed to reconstruct the original document. Kendall\u2019s \u03c4 can be efficiently computed by counting the number of intersections of lines when aligning the original document and the generated document. We refer the readers to Lapata (2003) for more details.\nResults are reported in Figure 4. The generative model outputs both the Entity Grid model and the lexical model by a large margin. In line with our expectations, better scores are observed in the firstsentence-given setting than the no-clue-given setting, We again observe a performance boost from the bidirectional model over the uni-directional model."}, {"heading": "4.3 Qualitative Analysis", "text": "To investigate which kinds of coherence the model is capable of handling, we examine some relevant examples, annotated with the (log-likelihood) coherence\nscore from the generative model. Each of the examples above/below was chosen in advance, before we trained our model, hence were not \u201ccherry-picked\u201d.\nCase 1: Lexical Coherence Pinochet was arrested. His arrest was unexpected. -4.25 Pinochet was arrested. His death was unexpected. -4.68 Mary ate some apples. She likes apples. -5.66 Mary ate some apples. She likes pears. -6.16 Mary ate some apples. She likes Paris. -6.72\nThe model can handle lexical coherence, correctly favoring the 1st over the 2nd, and the 3rd over the 4th examples.\nCase 2: Temporal Order Washington was unanimously elected president in the first two national elections. He oversaw the creation of a strong, well-financed national government. -3.48 Washington oversaw the creation of a strong, wellfinanced national government. He was unanimously elected president in the first two national elections. -4.52\nCase 3: Causal Relationship Bret enjoys video games; therefore, he sometimes is late to appointments. -7.59 Bret sometimes is late to appointments; therefore, he enjoys video games. -7.64\nThe model also does well at the much more complex task of dealing with temporal and causal relationships. From its training the model is exposed to the general preference of natural text for temporal order, and even for the more subtle causal links.\nCase 4: Centering/Referential Coherence Mary ate some apples. She likes apples. -5.66 She ate some apples. Marry likes apples. -7.64\nThe model can handle simple cases of referential coherence. Example3: -3.72 John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could finally buy a piano. He arrived just as the store was closing for the day. Example4: -4.55 John went to his favorite music store to buy a piano. It was a store John had frequented for many years\nHe was excited that he could finally buy a piano.. It was closing just as John arrived.\nIn these examples from Miltsakaki and Kukich (2004), the model successfully captures the fact that the second text is less coherent due to rough shifts. The model, in mapping sentences to semantic vector space successfully captures a representation of entity focus and its subtle syntactic cues."}, {"heading": "5 Conclusion", "text": "We investigate the problem of discourse coherence, treating natural texts as coherent and permutations as non-coherent, and training large neural models that achieve state of the art performance on coherence, including on large open-domain test sets. The performance and our qualitative analysis suggest that the distributed sentence representations built by the model capture some of the implicit linguistic components of coherence. Our model outperforms LSA baselines, suggesting it models lexical coherence well, and seems to capture semantic coherence like temporal and causal relations, which prior models like LSA and grid-based models are not designed to capture. The model also outperforms grid-based models, suggesting it may do well at capturing coherence based on entity focus across a discourse.\nSEQ2SEQ models have achieved recent success in many generation tasks. The fact that our generative model does well at the open-domain tasks, including paragraph reconstruction despite its known difficulty (0), suggests that SEQ2SEQ models can also play an important role for modeling discourse coherence."}, {"heading": "6 Acknowledgement", "text": "We would like to thank Kelvin Guu, Percy Liang, Chris Manning, Sida Wang, Ziang Xie and other members of the Stanford NLP groups for insightful comments and suggestions. Jiwei Li is supported by the Facebook Fellowship, to which we gratefully acknowledge. This work partially supported by NSF Award IIS-1514268. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF or Facebook."}], "references": [{"title": "Computing locally coherent discourses", "author": ["Ernst Althaus", "Nikiforos Karamanis", "Alexander Koller."], "venue": "Proceedings of ACL 2004.", "citeRegEx": "Althaus et al\\.,? 2004", "shortCiteRegEx": "Althaus et al\\.", "year": 2004}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics, 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Sentence fusion for multidocument news summarization", "author": ["Regina Barzilay", "Kathleen R McKeown."], "venue": "Computational Linguistics, 31(3):297\u2013328.", "citeRegEx": "Barzilay and McKeown.,? 2005", "shortCiteRegEx": "Barzilay and McKeown.", "year": 2005}, {"title": "Automated analysis of free speech predicts psychosis onset in high-risk youths", "author": ["Gillinder Bedi", "Facundo Carrillo", "Guillermo A Cecchi", "Diego Fern\u00e1ndez Slezak", "Mariano Sigman", "Nat\u00e1lia B Mota", "Sidarta Ribeiro", "Daniel C Javitt", "Mauro Copelli", "Cheryl M Corcoran"], "venue": null, "citeRegEx": "Bedi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bedi et al\\.", "year": 2015}, {"title": "Better document-level sentiment analysis from rst discourse parsing", "author": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein."], "venue": "arXiv preprint arXiv:1509.01599.", "citeRegEx": "Bhatia et al\\.,? 2015", "shortCiteRegEx": "Bhatia et al\\.", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Using entity-based features to model coherence in student essays", "author": ["Jill Burstein", "Joel Tetreault", "Slava Andreyev."], "venue": "Human language technologies: The 2010 annual conference of the North American chapter of the Association for Computational Linguistics, pages", "citeRegEx": "Burstein et al\\.,? 2010", "shortCiteRegEx": "Burstein et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Extending the entity grid with entity-specific features", "author": ["Micha Eisner", "Eugene Charniak."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 125\u2013129. Association for", "citeRegEx": "Eisner and Charniak.,? 2011", "shortCiteRegEx": "Eisner and Charniak.", "year": 2011}, {"title": "Coreferenceinspired coherence modeling", "author": ["Micha Elsner", "Eugene Charniak."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 41\u201344. Association for Computa-", "citeRegEx": "Elsner and Charniak.,? 2008", "shortCiteRegEx": "Elsner and Charniak.", "year": 2008}, {"title": "A unified local and global model for discourse coherence", "author": ["Micha Elsner", "Joseph L Austerweil", "Eugene Charniak."], "venue": "HLT-NAACL, pages 436\u2013443.", "citeRegEx": "Elsner et al\\.,? 2007", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Quantifying incoherence in speech: An automated methodology and novel application to schizophrenia", "author": ["Brita Elvev\u00e5g", "Peter W Foltz", "Daniel R Weinberger", "Terry E Goldberg."], "venue": "Schizophrenia research, 93(1):304\u2013316.", "citeRegEx": "Elvev\u00e5g et al\\.,? 2007", "shortCiteRegEx": "Elvev\u00e5g et al\\.", "year": 2007}, {"title": "Extending the entity-based coherence model with multiple ranks", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315\u2013324. Association for Computational", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["Peter W Foltz", "Walter Kintsch", "Thomas K Landauer."], "venue": "Discourse processes, 25(23):285\u2013307.", "citeRegEx": "Foltz et al\\.,? 1998", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Discourse coherence and lsa", "author": ["Peter W Foltz."], "venue": "Handbook of latent semantic analysis, pages 167\u2013184.", "citeRegEx": "Foltz.,? 2007", "shortCiteRegEx": "Foltz.", "year": 2007}, {"title": "word2vec explained: Deriving mikolov et al.\u2019s negativesampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["Barbara J Grosz", "Scott Weinstein", "Aravind K Joshi."], "venue": "Computational linguistics, 21(2):203\u2013225.", "citeRegEx": "Grosz et al\\.,? 1995", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Graphbased local coherence modeling", "author": ["Camille Guinaudeau", "Michael Strube."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93\u2013103.", "citeRegEx": "Guinaudeau and Strube.,? 2013", "shortCiteRegEx": "Guinaudeau and Strube.", "year": 2013}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 297\u2013304.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Cohesion in English", "author": ["M.A.K. Halliday", "Ruqaiya Hasan."], "venue": "Longman.", "citeRegEx": "Halliday and Hasan.,? 1976", "shortCiteRegEx": "Halliday and Hasan.", "year": 1976}, {"title": "Planning coherent multisentential text", "author": ["Eduard H Hovy."], "venue": "Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 163\u2013 169. Association for Computational Linguistics.", "citeRegEx": "Hovy.,? 1988", "shortCiteRegEx": "Hovy.", "year": 1988}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13\u201324.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3276\u2013 3284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["Mirella Lapata."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 545\u2013552. Association for Computational Linguistics.", "citeRegEx": "Lapata.,? 2003", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Automatic evaluation of information ordering: Kendall\u2019s tau", "author": ["Mirella Lapata."], "venue": "Computational Linguistics, 32(4):471\u2013484.", "citeRegEx": "Lapata.,? 2006", "shortCiteRegEx": "Lapata.", "year": 2006}, {"title": "Discourse relations and defeasible knowledge", "author": ["Alex Lascarides", "Nicholas Asher."], "venue": "Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 55\u201362. Association for Computational Linguistics.", "citeRegEx": "Lascarides and Asher.,? 1991", "shortCiteRegEx": "Lascarides and Asher.", "year": 1991}, {"title": "A model of coherence based on distributed sentence representation", "author": ["Jiwei Li", "Eduard Hovy."], "venue": "Proceedings of Empirical Methods in Natural Language Processing.", "citeRegEx": "Li and Hovy.,? 2014", "shortCiteRegEx": "Li and Hovy.", "year": 2014}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061\u20132069.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Automatically evaluating text coherence using discourse relations", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 997\u20131006. As-", "citeRegEx": "Lin et al\\.,? 2011", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "A coherence model based on syntactic patterns", "author": ["Annie Louis", "Ani Nenkova."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1157\u20131168. Association for", "citeRegEx": "Louis and Nenkova.,? 2012", "shortCiteRegEx": "Louis and Nenkova.", "year": 2012}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C Mann", "Sandra A Thompson."], "venue": "Text, 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "From local to global coherence: A bottom-up approach to text planning", "author": ["Daniel Marcu."], "venue": "AAAI/IAAI, pages 629\u2013635. Citeseer.", "citeRegEx": "Marcu.,? 1997", "shortCiteRegEx": "Marcu.", "year": 1997}, {"title": "Coh-metrix: Capturing linguistic features of cohesion", "author": ["Danielle S. McNamara", "Max M. Louwerse", "Philip M. McCarthy", "Arthur C. Graesser."], "venue": "Discourse Processes, 47(4):292\u2013330.", "citeRegEx": "McNamara et al\\.,? 2010", "shortCiteRegEx": "McNamara et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluation of text coherence for electronic essay scoring systems", "author": ["Eleni Miltsakaki", "Karen Kukich."], "venue": "Natural Language Engineering, 10(01):25\u201355.", "citeRegEx": "Miltsakaki and Kukich.,? 2004", "shortCiteRegEx": "Miltsakaki and Kukich.", "year": 2004}, {"title": "Planning text for advisory dialogues", "author": ["Johanna D Moore", "Cecile L Paris."], "venue": "Proceedings of the 27th annual meeting on Association for Computational Linguis-", "citeRegEx": "Moore and Paris.,? 1989", "shortCiteRegEx": "Moore and Paris.", "year": 1989}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text", "author": ["J. Morris", "G. Hirst."], "venue": "Computational Linguistics, 17(1):21\u201348.", "citeRegEx": "Morris and Hirst.,? 1991", "shortCiteRegEx": "Morris and Hirst.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Centering: A parametric theory and its instantiations", "author": ["Massimo Poesio", "Rosemary Stevenson", "Barbara Di Eugenio", "Janet Hitzeman."], "venue": "Computational linguistics, 30(3):309\u2013363.", "citeRegEx": "Poesio et al\\.,? 2004", "shortCiteRegEx": "Poesio et al\\.", "year": 2004}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Functional centering: Grounding referential coherence in information structure", "author": ["Michael Strube", "Udo Hahn."], "venue": "Computational linguistics, 25(3):309\u2013344.", "citeRegEx": "Strube and Hahn.,? 1999", "shortCiteRegEx": "Strube and Hahn.", "year": 1999}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A fast, accurate, non-projective, semantically-enriched parser", "author": ["Stephen Tratz", "Eduard Hovy."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1257\u20131268. Association for Computational Linguistics.", "citeRegEx": "Tratz and Hovy.,? 2011", "shortCiteRegEx": "Tratz and Hovy.", "year": 2011}, {"title": "Evaluating discourse-based answer extraction for why-question answering", "author": ["Suzan Verberne", "Lou Boves", "Nelleke Oostdijk", "PeterArno Coppen."], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in informa-", "citeRegEx": "Verberne et al\\.,? 2007", "shortCiteRegEx": "Verberne et al\\.", "year": 2007}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Centering theory in discourse", "author": ["Marilyn A Walker", "Aravind Krishna Joshi", "Ellen Friedman Prince."], "venue": "Oxford University Press.", "citeRegEx": "Walker et al\\.,? 1998", "shortCiteRegEx": "Walker et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al.", "startOffset": 144, "endOffset": 172}, {"referenceID": 20, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al.", "startOffset": 188, "endOffset": 213}, {"referenceID": 32, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al.", "startOffset": 188, "endOffset": 213}, {"referenceID": 45, "context": "Modeling the discourse coherence of a text (the way parts of a text are linked into a coherent whole) is essential for tasks like summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even applications like psychiatric diagnosis (Elvev\u00e5g et al.", "startOffset": 233, "endOffset": 256}, {"referenceID": 11, "context": ", 2007), and even applications like psychiatric diagnosis (Elvev\u00e5g et al., 2007; Bedi et al., 2015).", "startOffset": 58, "endOffset": 99}, {"referenceID": 3, "context": ", 2007), and even applications like psychiatric diagnosis (Elvev\u00e5g et al., 2007; Bedi et al., 2015).", "startOffset": 58, "endOffset": 99}, {"referenceID": 13, "context": "Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences.", "startOffset": 34, "endOffset": 90}, {"referenceID": 14, "context": "Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences.", "startOffset": 34, "endOffset": 90}, {"referenceID": 33, "context": "Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) generalize lexical cohesion via LSA embeddings of sentences.", "startOffset": 34, "endOffset": 90}, {"referenceID": 31, "context": "Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts.", "startOffset": 27, "endOffset": 80}, {"referenceID": 25, "context": "Relational models like RST (Mann and Thompson, 1988; Lascarides and Asher, 1991) define relations that hierarchically structure texts.", "startOffset": 27, "endOffset": 80}, {"referenceID": 1, "context": "Entity grid models (Barzilay and Lapata, 2008) model the referential coherence of entities moving in and out of focus across a text.", "startOffset": 19, "endOffset": 46}, {"referenceID": 43, "context": "ments as negative examples; and a generative model that uses sequence-to-sequence models (Sutskever et al., 2014) (SEQ2SEQ) to model the likelihood of generating a sentence based on its context.", "startOffset": 89, "endOffset": 113}, {"referenceID": 1, "context": "We evaluate the models on two text-ordering datasets, one from the literature (Barzilay and Lapata, 2008), and a new larger open-domain one.", "startOffset": 78, "endOffset": 105}, {"referenceID": 1, "context": "We evaluate the models on two text-ordering datasets, one from the literature (Barzilay and Lapata, 2008), and a new larger open-domain one. The discriminative model achieves state-of-the-art performance on the domain specific dataset presented in Barzilay and Lapata (2008), pushing the state-ofthe-art result to 96% accuracy, significantly outperforming all previous models.", "startOffset": 79, "endOffset": 275}, {"referenceID": 19, "context": "Lexical Coherence Coherence is strongly cued by words: words linked by identity, synonymy or other lexical relations forming chains across discourse segments (Halliday and Hasan, 1976).", "startOffset": 158, "endOffset": 184}, {"referenceID": 38, "context": "Early models used tools like thesauri (Morris and Hirst, 1991).", "startOffset": 38, "endOffset": 62}, {"referenceID": 13, "context": "Later work used Latent Semantic Analysis (LSA) embeddings (Foltz et al., 1998; Foltz, 2007), representing sentences with LSA vectors and measuring coherence with the cosine similarity of adjacent sentences, with the goal of capturing more subtle lexical relations that might not be available in thesauri.", "startOffset": 58, "endOffset": 91}, {"referenceID": 14, "context": "Later work used Latent Semantic Analysis (LSA) embeddings (Foltz et al., 1998; Foltz, 2007), representing sentences with LSA vectors and measuring coherence with the cosine similarity of adjacent sentences, with the goal of capturing more subtle lexical relations that might not be available in thesauri.", "startOffset": 58, "endOffset": 91}, {"referenceID": 31, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 100, "endOffset": 125}, {"referenceID": 25, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 225, "endOffset": 253}, {"referenceID": 20, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 372, "endOffset": 407}, {"referenceID": 37, "context": "Structured Discourse Relations Early work used discourse relations like Rhetorical Structure Theory (Mann and Thompson, 1988), a manually defined set of discourse relations between clauses, or Discourse Representation Theory (Lascarides and Asher, 1991)) a formal semantic model of discourse contexts, coreference and scope, to create coherent paragraphs in text planning (Hovy, 1988; Moore and Paris, 1989).", "startOffset": 372, "endOffset": 407}, {"referenceID": 16, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 47, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 42, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 40, "context": ", on the syntactic positions in which entities appear (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004).", "startOffset": 54, "endOffset": 139}, {"referenceID": 9, "context": "Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 8, "context": "Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al.", "startOffset": 99, "endOffset": 126}, {"referenceID": 29, "context": "Entity grid models have been extended with coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011), and entity graphs (Guinaudeau and Strube, 2013).", "startOffset": 148, "endOffset": 166}, {"referenceID": 17, "context": ", 2011), and entity graphs (Guinaudeau and Strube, 2013).", "startOffset": 27, "endOffset": 56}, {"referenceID": 1, "context": "The most influential such model is the entity grid model of Barzilay and Lapata (2008), in which sentences are represented by a vector of coreferent discourse entities along with their grammatical roles.", "startOffset": 60, "endOffset": 87}, {"referenceID": 21, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al.", "startOffset": 102, "endOffset": 148}, {"referenceID": 4, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al.", "startOffset": 102, "endOffset": 148}, {"referenceID": 27, "context": ", 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014).", "startOffset": 98, "endOffset": 140}, {"referenceID": 21, "context": ", 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014).", "startOffset": 98, "endOffset": 140}, {"referenceID": 4, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014). Our proposed discriminative model extends the coherence model of Li and Hovy (2014), a neural classifier trained on small domain-specific datasets (earthquake and accidents) using negative sampling at the sentence level.", "startOffset": 128, "endOffset": 367}, {"referenceID": 4, "context": "Neural Net Models Recent work focuses instead on representing sentences as dense, real-valued vectors (Ji and Eisenstein, 2014; Bhatia et al., 2015), such as by learning sentence representations as part of supervised RST discourse parsing (Li et al., 2014; Ji and Eisenstein, 2014). Our proposed discriminative model extends the coherence model of Li and Hovy (2014), a neural classifier trained on small domain-specific datasets (earthquake and accidents) using negative sampling at the sentence level. The algorithm we present significantly outperforms the classifier of Li and Hovy (2014).", "startOffset": 128, "endOffset": 592}, {"referenceID": 43, "context": "SEQ2SEQ models have been successfully applied to a variety of NLP tasks including machine translation (Sutskever et al., 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 46, "context": ", 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 41, "context": ", 2014), dialogue generation (Vinyals and Le, 2015), and abstractive summarization (Rush et al., 2015).", "startOffset": 83, "endOffset": 102}, {"referenceID": 22, "context": "Our idea of predicting the current sentence based on the previous one is similar to skip-thought models (Kiros et al., 2015) that build an LSTM encoder-decoder model by predicting tokens in neighboring sentences.", "startOffset": 104, "endOffset": 124}, {"referenceID": 22, "context": "Our idea of predicting the current sentence based on the previous one is similar to skip-thought models (Kiros et al., 2015) that build an LSTM encoder-decoder model by predicting tokens in neighboring sentences. We use the mutual dependency between the two consecutive sequences to measure coherence. This idea of modeling the mutual dependency between two sequences for neural generation has been explored by Li et al. (2015) for dialogue generation.", "startOffset": 105, "endOffset": 428}, {"referenceID": 34, "context": "The two models we propose can also be viewed as the a kind of generalization of the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to the sentence level.", "startOffset": 100, "endOffset": 146}, {"referenceID": 35, "context": "The two models we propose can also be viewed as the a kind of generalization of the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to the sentence level.", "startOffset": 100, "endOffset": 146}, {"referenceID": 18, "context": "To model negative incoherent examples, we resort to noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010).", "startOffset": 81, "endOffset": 110}, {"referenceID": 20, "context": "The proposed model can be viewed as an extension of Li and Hovy\u2019s (2014) model but is practical at large scale3.", "startOffset": 59, "endOffset": 73}, {"referenceID": 20, "context": "Li and Hovy\u2019s (2014) recursive neural model operates on parse trees, which does not support batched computation and is therefore hard to scale up.", "startOffset": 7, "endOffset": 21}, {"referenceID": 43, "context": "Both p(si|si+1) and p(si+1|si) can be computed using SEQ2SEQ models (Sutskever et al., 2014).", "startOffset": 68, "endOffset": 92}, {"referenceID": 43, "context": "Other training details are given below, broadly aligned with Sutskever et al. (2014): LSTM parameters and embeddings are initialized from a uniform distribution in [-0.", "startOffset": 61, "endOffset": 85}, {"referenceID": 1, "context": "We first evaluate the proposed model on the task of sentence ordering using two datasets, a standard domain-specific dataset (Barzilay and Lapata, 2008) and a newly constructed open-domain dataset from Wikipedia.", "startOffset": 125, "endOffset": 152}, {"referenceID": 1, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 30, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 10, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 29, "context": "Dataset We first evaluate the proposed algorithms on a dataset widely adopted in sentence ordering and predicate on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011).", "startOffset": 214, "endOffset": 305}, {"referenceID": 27, "context": "842 Recursive Neural Models (Li et al. 2014) 0.", "startOffset": 28, "endOffset": 44}, {"referenceID": 1, "context": "920 Entity Grid Model (Barzilay and Lapata, 2008) 0.", "startOffset": 22, "endOffset": 49}, {"referenceID": 30, "context": "888 HMM (Louis and Nenkova, 2012) 0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 30, "context": "880 HMM+Entity (Louis and Nenkova, 2012) 0.", "startOffset": 15, "endOffset": 40}, {"referenceID": 30, "context": "876 HMM+Content (Louis and Nenkova, 2012) 0.", "startOffset": 16, "endOffset": 41}, {"referenceID": 17, "context": "847 Graph (Guinaudeau and Strube, 2013) 0.", "startOffset": 10, "endOffset": 39}, {"referenceID": 1, "context": "920 Entity Grid Model (Barzilay and Lapata, 2008) 0.904 0.872 0.888 HMM (Louis and Nenkova, 2012) 0.822 0.938 0.880 HMM+Entity (Louis and Nenkova, 2012) 0.842 0.911 0.876 HMM+Content (Louis and Nenkova, 2012) 0.742 0.953 0.847 Graph (Guinaudeau and Strube, 2013) 0.846 0.635 0.740 Foltz et al. (1998)-Glove 0.", "startOffset": 23, "endOffset": 301}, {"referenceID": 1, "context": "920 Entity Grid Model (Barzilay and Lapata, 2008) 0.904 0.872 0.888 HMM (Louis and Nenkova, 2012) 0.822 0.938 0.880 HMM+Entity (Louis and Nenkova, 2012) 0.842 0.911 0.876 HMM+Content (Louis and Nenkova, 2012) 0.742 0.953 0.847 Graph (Guinaudeau and Strube, 2013) 0.846 0.635 0.740 Foltz et al. (1998)-Glove 0.705 0.682 0.693 Foltz et al. (1998)-LDA 0.", "startOffset": 23, "endOffset": 345}, {"referenceID": 13, "context": "Baseline numbers from prior work (except for Foltz et al. (1998)) are reprinted from the best performance reported in those papers.", "startOffset": 45, "endOffset": 65}, {"referenceID": 39, "context": "We use 300 dimensional embeddings borrowed from GLOVE (Pennington et al., 2014) to initialize word embeddings.", "startOffset": 54, "endOffset": 79}, {"referenceID": 7, "context": "Word embeddings are kept fixed during training and we update LSTM parameters using AdaGrad (Duchi et al., 2011).", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "(1) Entity Grid Model: The grid model (Barzilay and Lapata, 2008) represents the sentence as a column of a grid of features and applies machine learning methods (e.", "startOffset": 38, "endOffset": 65}, {"referenceID": 1, "context": "(1) Entity Grid Model: The grid model (Barzilay and Lapata, 2008) represents the sentence as a column of a grid of features and applies machine learning methods (e.g., SVM) to identify the coherent transitions based on entity features. Results are directly taken from Barzilay and Lapata\u2019s (2008) paper.", "startOffset": 39, "endOffset": 297}, {"referenceID": 30, "context": "(2) HMM: A hidden-markov model described in Louis and Nenkova (2012) models the cluster transition probability in the coherent texts.", "startOffset": 44, "endOffset": 69}, {"referenceID": 17, "context": "(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013).", "startOffset": 200, "endOffset": 229}, {"referenceID": 17, "context": "(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013).", "startOffset": 26, "endOffset": 55}, {"referenceID": 17, "context": "(3) Graph Based Approach: Guinaudeau and Strube (2013) extended the entity grid model to a graph representing the text that embeds entity transition information needed for local coherence computation (Guinaudeau and Strube, 2013). (4) Li and Hovy (2014): A recursive neural model computes sentence representations based on parse trees.", "startOffset": 26, "endOffset": 254}, {"referenceID": 39, "context": "We used this intuition, but with more modern embedding models: (1) 300-dimensional Glove word vectors (Pennington et al., 2014), embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 5, "context": ", 2014), embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al., 2003) with 300 topics, trained on the Wikipedia dataset using Gibbs sampling.", "startOffset": 132, "endOffset": 151}, {"referenceID": 12, "context": "(5) Foltz et al. (1998) computes the semantic relatedness of two text units as the cosine similarity between their LSA vectors.", "startOffset": 4, "endOffset": 24}, {"referenceID": 20, "context": "The proposed discriminative model significantly outperforms the model presented in Li and Hovy (2014) as well as all non-neural baselines.", "startOffset": 90, "endOffset": 102}, {"referenceID": 13, "context": "The simple LSA method of calculating cosine similarity between adjacent sentences, adopted from Foltz et al. (1998), does not yield competitive results, con-", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "Since the dataset presented in Barzilay and Lapata (2008) is quite domain-specific, we propose testing coherence with a much larger, open-domain dataset: Wikipedia.", "startOffset": 31, "endOffset": 58}, {"referenceID": 1, "context": "We adopt the same strategy as in Barzilay and Lapata (2008), in which we generate permutations for the original Wikipedia paragraphs.", "startOffset": 33, "endOffset": 60}, {"referenceID": 13, "context": "Baselines Our baselines consist of the Glove and LDA updates of the lexical coherence baselines (Foltz et al., 1998).", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "We also implement the Entity Grid Model (Barzilay and Lapata, 2008) using the Wikipedia training set.", "startOffset": 40, "endOffset": 67}, {"referenceID": 44, "context": "We use a wikipedia dump parsed using the Fanse Parser (Tratz and Hovy, 2011).", "startOffset": 54, "endOffset": 76}, {"referenceID": 1, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 12, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).", "startOffset": 331, "endOffset": 403}, {"referenceID": 6, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).", "startOffset": 331, "endOffset": 403}, {"referenceID": 1, "context": "(Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution over 80 million Wikipedia sentences, we follow other researchers in using Barzilay and Lapata\u2019s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).", "startOffset": 331, "endOffset": 403}, {"referenceID": 13, "context": "686 Foltz et al. (1998)-Glove 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 13, "context": "686 Foltz et al. (1998)-Glove 0.597 Foltz et al. (1998)-LDA 0.", "startOffset": 4, "endOffset": 56}, {"referenceID": 13, "context": "Once again, purely lexical methods (Foltz et al., 1998) do not yield compelling results.", "startOffset": 35, "endOffset": 55}, {"referenceID": 1, "context": "By contrast the dataset in Barzilay and Lapata (2008) is very domain-specific, and the semantic space is thus relatively small.", "startOffset": 27, "endOffset": 54}, {"referenceID": 1, "context": "Compared with the dataset in Barzilay and Lapata (2008), overfitting is not an issue here due to the great amount of training data.", "startOffset": 29, "endOffset": 56}, {"referenceID": 23, "context": "We suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003).", "startOffset": 152, "endOffset": 166}, {"referenceID": 23, "context": "(2) No clue is given: we employed the graph based method described in Lapata (2003). We first construct a graph where the each vertex denotes a sentence and the edge weight u\u2192 v denotes the coherence score of sentence v coming after u.", "startOffset": 70, "endOffset": 84}, {"referenceID": 23, "context": "(2) No clue is given: we employed the graph based method described in Lapata (2003). We first construct a graph where the each vertex denotes a sentence and the edge weight u\u2192 v denotes the coherence score of sentence v coming after u. Note that weight values for u \u2192 v and v \u2192 u are different. We initialize the vertex list V using all vertexes in the graph. Similar to Lapata (2003), we employ a greedy search model.", "startOffset": 70, "endOffset": 385}, {"referenceID": 23, "context": "We therefore use Kendall\u2019s Tau (Lapata, 2003; Lapata, 2006), a metric of rank correlation for evaluation.", "startOffset": 31, "endOffset": 59}, {"referenceID": 24, "context": "We therefore use Kendall\u2019s Tau (Lapata, 2003; Lapata, 2006), a metric of rank correlation for evaluation.", "startOffset": 31, "endOffset": 59}, {"referenceID": 23, "context": "We refer the readers to Lapata (2003) for more details.", "startOffset": 24, "endOffset": 38}, {"referenceID": 36, "context": "In these examples from Miltsakaki and Kukich (2004), the model successfully captures the fact that the second text is less coherent due to rough shifts.", "startOffset": 23, "endOffset": 52}], "year": 2016, "abstractText": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) and are trained on narrow domains. We introduce algorithms that capture diverse kinds of coherence by learning to distinguish coherent from incoherent discourse from vast amounts of opendomain training data. We propose two models, one discriminative and one generative, both using LSTMs as the backbone. The discriminative model treats windows of sentences from original human-generated articles as coherent examples and windows generated by randomly replacing sentences as incoherent examples. The generative model is a SEQ2SEQ model that estimates the probability of generating a sentence given its contexts. Our models achieve state-of-the-art performance on multiple coherence evaluations. Qualitative analysis suggests that our generative model captures many aspects of coherence including lexical, temporal, causal, and entity-based coherence.1", "creator": "TeX"}}}