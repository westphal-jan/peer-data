{"id": "1606.01792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Neural Machine Translation with External Phrase Memory", "abstract": "in this paper, we propose flea, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from mathematical maps specified by human speakers. encompassing any given source sentence, skinner filters the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. the decoder utilizes a mixture of word - generating component and phrase - generating component, with therefore specifically designed strategy to generate a sequence of multiple words all at once. the puzzle not only approaches one step towards incorporating external knowledge into neural machine translation, no also makes an effort to extend the word - by - word generation - whereby recurrent neural network. our empirical study implementing chinese - to - english translation conclude that, detecting carefully - chosen phrase table in memory, phrasenet yields 3. 45 bleu improvement over the generic process machine response.", "histories": [["v1", "Mon, 6 Jun 2016 15:45:41 GMT  (309kb,D)", "http://arxiv.org/abs/1606.01792v1", "8 figures, 9 pages"]], "COMMENTS": "8 figures, 9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yaohua tang", "fandong meng", "zhengdong lu", "hang li", "philip l h yu"], "accepted": false, "id": "1606.01792"}, "pdf": {"name": "1606.01792.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with External Phrase Memory", "authors": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip L.H. Yu"], "emails": ["tangyh@hku.hk", "mengfandong@ict.ac.cn", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com", "plhyu@hku.hk"], "sections": [{"heading": null, "text": "In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator."}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT), although only proposed recently, has shown great potential, and arguably surpassed statistical machine translation on tasks like English-German translation (Sennrich et al., 2015). In addition to its superior ability in modeling the semantics of source sentence and language fluency of the target sentence, NMT as a framework also has remarkable flexibility in accommodating other form of knowledge.\nIn this paper, we explore the possibility of equipping regular neural machine translator with an external memory storing rules that specify phrase-level correspondence between the source and target languages. Those rules are in symbolic form, which can be either extracted from a parallel corpus or given by experts. We tailor the encoder, decoder and the attention model of the neural translator to help locate phrases in the source and generate their translations in the target. The proposed model is called phraseNet. phraseNet is not only one step towards incorporating external knowledge in to neural machine translation, but also an effort to extend the word-by-word generation mechanism of recurrent neural network."}, {"heading": "1.1 Model Overview", "text": "The overall diagram of phraseNet is displayed in Figure 1. Basically, for a given source sentence, phraseNet first encodes the sentence to a representation with an RNN encoder, scans the phrase memory to select candidate phrase pairs and then tags the source representation accordingly (Section 3.2). phraseNet then generates both words and phrases with an RNN decoder. It dynamically determines at each time step with its probabilistic model consisting of a mixture of word-generation mode and phrase-generation mode (Section 3). To maintain the state consistency of RNN when running in different modes, the decoder of phraseNet will go through \u201cidle run\u201d (Section 3.6) after generating a multipleword phrase. Our contribution is of three-folds:\n1. we propose an end-to-end learning algorithm for neural machine translation with an external\nar X\niv :1\n60 6.\n01 79\n2v 1\n[ cs\n.C L\n] 6\nJ un\n2 01\nphrase memory, which to our knowledge it is the first effort in this direction;\n2. we propose a way to handle the generation of multiple words with an RNN decoder;\n3. our empirical studies on Chinese-English translation tasks show the efficacy of our models: phraseNet achieves on average 3.45 BLEU improvement over its generic counterpart.\nRoadMap The remainder of this paper is organized in the following way. In Section 2, we will give a brief introduction to attention-based neural machine translation as the background. In Section 3, we will introduce phraseNet, including its two variants. In Section 4, we will report our experiments on applying phraseNet to Chinese-English translation tasks. Then in Section 5 and 6, we will give a brief review of related work and conclude the paper."}, {"heading": "2 Background", "text": "Our work is built upon the attention-based neural machine translation model that learns to align and translate jointly (Bahdanau et al., 2015), which will be referred to as RNNsearch.\nRNNsearch uses a bidirectional RNN (Schuster and Paliwal, 1997) to encode the source sentence. It consists of two independent RNNs. The forward RNN reads the source sentence from left to right \u2212\u2192 h = ( \u2212\u2192 h 1, . . . , \u2212\u2192 h Tx). The backward RNN reads the source sentence from right to left \u2190\u2212 h = ( \u2190\u2212 h 1, . . . , \u2190\u2212 h Tx). The representation of the source sentence h is then defined as the concatenation of\u2212\u2192 h and \u2190\u2212 h . Each element in h contains information\nabout the source sentence, focusing on the parts surrounding the corresponding word.\nAt decoding time t, the attention model uses st\u22121 (the RNN states) and eyt\u22121 (the embedding of previous target word yt\u22121) to \u201cquery\u201d the encoded h, marks each element of h a score etj = f(st\u22121,hj , eyt\u22121). The non-linear function f can take on many forms, but we concatenate the three inputs and feed it to a neural network with one hidden layer and tanh as activation function. The scores are then normalized to {\u03b1tj}, serving as the weights of {hj{ to the target word, which then gives the context vector hj , ct = \u2211Tx j=1 \u03b1tjhj . The context vector ct is then used to update the hidden state of the decoder:\nst = fu(st\u22121, ct, eyt\u22121), (1)\nwhere the function fu is GRU (Cho et al., 2014; Chung et al., 2014). To predict a target word, the decoder combines st, ct and eyt\u22121 , feeds to a one-layer MLP with tanh as activation function, followed by a softmax function,\np(yt = yi|y<t,x; \u03b8) \u221d exp { vTi Wo tanh ( Uost\u22121 +Coct +Voeyt\u22121 )} , (2)\nwhere Wo, Uo, Co and Vo are weight matrices. vi is an one-hot indicator vector for yi."}, {"heading": "3 Models", "text": "In this section, we will give more details of phraseNet, more specifically on the preprocessing, encoder and decoders. With two different variants of the mixture models used in decoder, we naturally have two variants of phraseNet, namely phraseNetgate and phraseNetsoftmax."}, {"heading": "3.1 Preprocessing", "text": "The phrase table P is a list of rules. Each rule contains a source phrase p\n\u2032 k and its translation, a\ntarget phrase pk. Figure 2 gives an example of the phrase table. For simplicity, we first limit ourselves to a subset of rules with the strongest sourcetarget correspondence, which is in contrast to that in phrase-based statistical machine translation (SMT). In SMT, a phrase could have multiple translations with a probability distribution over them. Here we restrict that for each source phrase in P , it has only one translation with probability almost equal to 1. This will limit the size of P but can guarantee that\nthe feasible rules are \u201creliable\u201d enough and greatly simplifies the model design and training.\nWe will introduce the collection of such a phrase table P later. At the moment, let us assume that we have the table P which will be utilized to preprocess the sentence pair before encoding. In order to tag a source sentence x = (x1, x2, . . . , xTx), we need to locate its contained phrases. Hence we will find out the rules in P whose source phrase appears in x, and denote these rules as Px. To calculate the likelihood during training, we also need to find out the rules in Px whose target phrase appears in the target sentence y, denote as Pxy (Pxy \u2282 Px).\nFor simplicity, we remove from Px the short rules that intersect with the others, which means if the source phrases of two rules are overlapped, we remove the rule whose source phrase has fewer words. We choose at most np (a hype-parameter) phrases for each sentence with the maximum coverage.\nThe non-overlapping rules inPx andPxy split the words of source and target sentences into groups as showing in Figure 3. The words in source sentence x are split into two groups, phrases Px and words notin-phrases Wx, while the words in target sentence y are split into two groups, phrases Pxy and words not-in-phrasesWy."}, {"heading": "3.2 Encoder", "text": "We use the regular encoder from RNNsearch, but add tags to hi (Meng et al., 2015), h \u2032 i = [hi, tagi]. The tags are used to help the model locate and discriminate different phrases.\nEach tagi is an indicator vector with length np. For example in Figure 3, suppose np = 5 and we find three source phrases p\n\u2032 1 = (x2, x3),p \u2032 2 =\n(x6, x7, x8),p \u2032 3 = (x10, x11) in sentence x, we concatenate a tag vector (1, 0, 0, 0, 0) to each of\nh2,h3, concatenate a vector (0, 1, 0, 0, 0) to each of h6,h7,h8, concatenate a vector (0, 0, 1, 0, 0) to h10 and h11. For all other not-in-phrase words Wx, we also add a trivial tag vector (0, 0, 0, 0, 0) to their hi. Figure 4 shows an example of such concatenating."}, {"heading": "3.3 Decoder", "text": "Unlike the decoder of RNNsearch that only has word mode, our decoders also have phrase mode. For a two-word target phrase pt = (yt, yt+1), it can either be generated by word mode (one by one) or phrase mode (as a whole). In our models, we add upon the RNNsearch another component which has two functions, (1) makes decision between phrase mode and word mode, (2) chooses the right target phrase if the decision is phrase mode.\nWith the help of attention model, the new component tries to capture the signals from the encoded representations of a source sentence and translate part of the source sentence (source phrase) directly to the target output as a whole at proper decoding moments. The tags added in Section 3.2 will play an important role in the process. If we view h as a short-term memory as it changes from sentence to sentence, then the phrase table could be called a phrase memory which is a long-term memory. The decoder queries the short-term memory to choose\nthe segment of source while it consults the phrase memory to choose the right phrase.\nWe have two model variants, namely phraseNetgate and phraseNetsoftmax, each corresponding to a different implementation of the mixture model in decoder.\n3.4 phraseNetgate\nThe decoder for phraseNetgate is illustrated in Figure 5. In phraseNetgate, the decoder will first use a gate fz to determine the mode at time t by issuing a binary indicator variable (zt \u2208 {0, 1}), where 0 represents word mode and 1 represents phrase mode. Then for each mode, it will calculate the word probabilities and phrase probabilities respectively. In word mode, a classifier fw outputs a probability distribution over the words in target vocabulary V; while in phrase-mode a phrase classifier fp determines the probability distribution over the phrases in Px. The final probability of output is given by the probabilities of modes as well as the probability of individual output generated in each mode. It is a mixture model since a phrase like china daily can be generated in both modes, and the final probability of it is therefore the sum of the probabilities of it being generated from each. For a target phrase that is not in word vocabulary, i.e., it contains UNK, the probability of that can only be from the phrase mode.\nOne snapshot of the decoding is the following. Let us suppose that at time t = 1, the decoder has generated the word y1 in the word mode, and the current state is s1 , moving to time t = 2. With the state s1 , the attention model (same as in RNNsearch) first generates the context c2 as a weighted sum of the h\n(from encoder). With s1, c2 and ey1 , the decoder could move to update the next state s2 and generating the next word (or phrase). More specifically, in phraseNetgate, the state s2 is updated in the same way as in RNNsearch (Equation (1)). The generation of the next word/phrase can be described as follows, let\u2019s denote St = (st, ct, eyt\u22121):\n1. STEP-1: generate the decision variable z2 with\np(z2 = 1|S2; \u03b8) = fz(S2) p(z2 = 0|S2; \u03b8) = 1\u2212 fz(S2)\n2. STEP-2a: if z2 = 0 (word mode), generate a word based on s2 with the regular word vocabulary V , same as RNNsearch (Equation 2);\n3. STEP-2b: if z2 = 1 (phrase mode), generate target phrase pj \u2208 Px with probability:\npp(y2 = pj |S2, 1; \u03b8) \u221d exp { uTj Wp tanh (Ups2 +Cpc2 +Vpey1) } ,\nwhere Wp, Up, Cp and Vp are weight matrices. uj is an one-hot indicator vector for pj .\n4. STEP-3: calculate the final probabilities and sample the next word (or phrase):\np(y2 = wi) = p(z2 = 0|S2; \u03b8)p(wi|S2, 0; \u03b8) p(y2 = pj) = p(z2 = 1|S2; \u03b8)p(pj |S2, 1; \u03b8)\np(y2) = [ p(y2 = w) p(y2 = p) ] ,\nwhere the size of p(y2) is np plus the number of words in vocabulary V . The next word or phrase will be sampled according to p(y2). If next generation is a phrase, the decoder will go through an \u201cidle run\u201d process (Section 3.6) to generate words in p2, after that the decoder replaces the tags tagi of those source words of p \u2032 2 to all-zero vectors as p \u2032 2 has already been decoded.\nSimilar to (Gulcehre et al., 2016a), in phraseNetgate, fz is a three-layered neural network using noisy-tanh activation for the first two layers (Gulcehre et al., 2016b) and we add residual connection (He et al., 2015) from the first layer to the second hidden layer, The output layer uses sigmoid as activation function.\n3.5 phraseNetsoftmax With phraseNetgate the decision of phrase mode is made before seeing the actual content of the target phrase, which fails to make use of the language model and semantic relevance on the target side. To address this drawback we devise phraseNetsoftmax, which takes the candidate phrases and candidate words in the same softmax, as illustrated in Figure 6. To do this, all the phrases need to embedded as vectors, where the embedding model is also learned in the NMT training. It is also worth to mention that phraseNetsoftmax can potentially handle the case where one source phrase may correspond to multiple candidate target phrases, since the decoder can distinguish them based on their content. This modeling advantage, however, will not be explored in this paper.\nGiven a rule (p \u2032 k,pk), we have several choices to calculate its embedding. In this paper, we choose to use a separate backward RNN to encode pk and choose the last state as the embedding for it. That way, the embedding will keep more information of the first word of pk, therefore facilitate a potential language model in scoring (yi\u22121, si,pk).\nSuppose that at time t = 1, the decoder has generated the word y1 in the word mode, and the current state is s1 , moving to time t = 2. With the state s1 , the decoder makes an attentive read to h to obtain c2. The state s2 is updated in the same way as in Equation (1). The generation of the next word/phrase can be described as follows:\n1. STEP-1: calculate the word score for each word in V ,\n\u03c8wi = v T i Ww tanh(Uws2 +Cwc2 +Vwey1),\nwhere Ww, Uw, Cw and Vw are weight matrices.\n2. STEP-2: calculate the phrase score for each phrase in Px \u03c8pj = Wq tanh(Uqs2+Cqc2+Vqey1+Rqepj ),\nwhere epj is the embeddings of rule (p \u2032 j ,pj). Wq, Uq, Cq, Vq and Rq are weight matrices.\n3. STEP-3: calculate the probabilities of all words and phrases through softmax\np(y2|s2, c2, ey1) = softmax([\u03c8w, \u03c8p])\nIn the softmax, the phrases will compete directly with words, which is different from the phraseNetgate where phrase probabilities and word probabilities are calculated independently. While phraseNetgate has difficulties in calculating the scores for each phrase, phraseNetsoftmax has the flexibility to adapt to the new setting with embeddings. If the choice is a phrase, the decoder will go through \u201cidle run\u201d process and the tags of those source words of the chosen phrase will be set to all-zero."}, {"heading": "3.6 Idle Run for Multi-word Phrases", "text": "Our decoder is vastly different from that of RNNsearch, but it is still generally built on the basic word-by-word decoding mechanism. To further accommodate the phrase mode in which multiple words are generated all at once, we introduce the \u201cidle run\u201d. Basically, if at time t a multiple-word phrase is chosen, the decoding RNN will run exactly the same way as in word mode with regard to state update and attention, only that the generation of words in the rest of phrase is pre-determined at t.\nThis process is called idle run, which can be illustrated through the following example. In Figure 3, if at time t = 5, the decoder decides to go with phrase mode and generate p1, the decoder will not really output p1 = (y5, y6) at once. To keep the updating of st (Equation (1)), the decoder will first output y5 and use ey5 and other required elements to update s5\nto s6, uses s6 to generates y6. With ey6 and other required elements, the decoder will update the state to s7 and at the time t = 7, the decoder starts to make its next decision, phrase mode or word mode for the coming words. During the output of p1, the decoder does not need to make decisions or sample words as it is already in one phrase mode."}, {"heading": "3.7 The Probabilistic Model for Phrases", "text": "Given a target phrase p = {yt, yt+1, yt+2}, in principle, its words could be chosen either from vocabulary V or entirely retrieved from phrase table P . So in general each word is potentially generated from a mixture probability model. In the case that there are out-of-vocabulary words (UNKs) in the phrases, which are faily common in practice, the mixture model degenerates to phrase mode only.\nFor an unified notation, we introduce an indicator variable Iunk into the mixture probability model, which is summarized as follows,\np(yt, yt+1,yt+2|y<t,x; \u03b8)\n=Iunk \u00d7 t+2\u220f i=t p(zi = 0, yi|Si; \u03b8) +p(zt = 1,pt = p|St; \u03b8).\n(3)\nwhere Iunk = 1 means there is no UNKs in the phrase, and 0 otherwise.\nFor phraseNetgate, p(zt = 0, yt|St; \u03b8) factorizes into p(zt = 0|St; \u03b8)p(yt|St, 0; \u03b8), and p(zt = 1,pt = p|St; \u03b8) factorizes into p(zt = 1|St; \u03b8)pp(pt = p|St, 1; \u03b8). For phraseNetsoftmax, there is no explicit variable zt, the indicator of mode is implicitly absorbed into the choice of words and phrases. The probability p(zt = 0, yt|St; \u03b8) can therefore be re-written as p(yt|St; \u03b8) and p(zt = 1,pt = p|St; \u03b8) as p(pt = p|St; \u03b8).\nFor normal words that are not part of phrasesWy, they can only be generated by word mode, which is the same as RNNsearch.\nGiven a pair of source and target sentence x = (x1, x2, . . . , xTx) and y = (y1, y2, . . . , yTy), the probability of this pair of sentences is:\np(y|x; \u03b8) = \u220f\nyi\u2208Wy\np(yi|y<i,x; \u03b8) \u220f\npj\u2208Pxy\np(pj |y<j ,x; \u03b8),\nhere p(pj |y<j ,x; \u03b8) refers to the mixture probability (Equation (3)) of output the words in pj . For\na given batch of the source and target sequences {X}N and {YN}, the objective is to minimize the negative log-likelihood:\nL = \u2212 1 N N\u2211 k=1 p(y(k)|x(k); \u03b8)"}, {"heading": "4 Experiments", "text": "We report our empirical study on applying phraseNetgate and phraseNetsoftmax to Chineseto-English translation, and comparing it against RNNsearch and SMT models."}, {"heading": "4.1 Phrase table P", "text": "As mentioned before, when we design our model, our definition for \u201cphrase\u201d is different from that used in phrase-based statistical machine translation. For each source phrase, our models only support an unique translation (target phrase). Therefore, we only choose those phrase pairs that the source phrase almost always translates to the target phrase. We also hope the contexts for the source phrases are relatively fixed so that the models can learn the patterns of translation easier. With these considerations, we focus our attention on five categories of phrases: dates, names, numbers, locations and organizations. Apart from these five categories, we also collect some other phrases that fulfil our requirements. Figure 7 shows several examples of our Phrase table.\nThe phrase pairs are collected from several sources. The first source consists of extracted phrase pairs from a bi-lingual corpus using the method described in (Meng et al., 2014), (Ren et al., 2009) and (Frantzi et al., 2000). The second source is the LDC dictionary. The third source is from proper nouns dictionaries, which contain many commonly\nused Chinese-to-English translation pairs for proper nouns. We have also generated some Chinese-toEnglish phrase translations, especially dates, numbers and Chinese names, by predefined rules. There are two formats of Chinese names, Mandarin names and Cantonese names, both of their English counterparts could be generated according to their pronunciation rules. Numbers can also be generated by predefined rules, like \u201c1345\u2192 1,345\u201d. Using these rules, we transform several formats of Chinese numbers to English numbers."}, {"heading": "4.2 Setup", "text": "Our training data contains 1.25M sentence pairs obtained from LDC corpora1, with 27.9M Chinese words and 34.5M English words respectively. We use NIST 2002 (NIST02) dataset as our development set, and the NIST 2003 (NIST03), NIST 2004 (NIST04), NIST 2005 (NIST05), 2006 (NIST06) and 2008 (NIST08) datasets as our test sets. The case-insensitive 4-gram NIST BLEU score (Papineni et al.2002) is used as our evaluation metric.\nIn training the neural networks, we limit the source and target vocabularies to the most frequent 16K words (one of the words is reserved for the unknown words (UNK)) in Chinese and English, covering approximately 95.8% and 98.3% of the two corpora respectively. We train each model with the sentences of length up to 50 words in training data. The word embedding dimension is 620 and the size of a hidden layer is 1000. We set np as 10. In both the RNNsearch and our models, we adopt the coverage models introduced in (Tu et al., 2016) to mitigate the problem of over-translation.\nWe compare our models with state-of-the-art SMT and RNNsearch:\n1. Moses (Koehn et al.2007): an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data;\n2. RNNsearch (Bahdanau et al.2015): an attentional NMT model with default setting.2\n1The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n2We use the code from (https://github.com/"}, {"heading": "4.3 Translation Performance", "text": "Table 1 shows the translation performances measured in BLEU score. Clearly both the proposed phraseNetgate and phraseNetsoftmax significantly improves the translation quality in all cases. More specifically, On average, phraseNetgate yields about 3.45 BLEU score improvement over our baseline, phraseNetsoftmax yields about 2.13 BLEU score improvement over our baseline. Also, RNNsearch with expanded vocabulary (30K words) is 1.65 BLEU behind phraseNetgate. Surprisingly, phraseNetsoftmax comes behind phraseNetgate, despite its potential ability to take the content of the target phrase into the decision. We conjecture that this might be due to the difficulty in directly comparing the scores from two different types of scoring functions in the same pool of softmax.\nIt is also reasonable to doubt that our models are just generate the target phrases without considering the positions, as this will also (almost surely) increase the 1-gram and 2-gram BLEU scores and hence increase the final BLEU scores. To further verfy this, Table 2 compares our models with RNNsearch measured in 4-gram BLEU score, which capture overlapping of generated targets and reference on longer segments.\nOur models, especially phraseNetgate, still perform better than RNNsearch, incidating that the phrases are put into the right places."}, {"heading": "4.4 Samples of Translation", "text": "We also give two examples from test set comparing our phraseNetgate with RNNsearch, and more examples can be found in supplementary materials. As demonstrated through those examples, when there are phrases found in the source sentences, phraseNetgate has a better chance to generate the corresponding target phrases correctly at proper locations. This could happen when the source phrases consist words all in the vocabulary, but more frequently when there are UNK words there, showing that phraseNetgate is also a strong model to solve the UNK problem. Another interesting observation, e.g., the second example in Figure 8 is that for some common phrases phraseNet sometimes ignores the\nkyunghyuncho/dl4mt-material) with minor modifications.\nsuggestion of phrase mode, but still generate the entire phrase correctly from its word mode. This shows phraseNet maintains a healthy and flexible balance between word and phrase mode."}, {"heading": "5 Related Work", "text": "Probably the work that is closest to phraseNet is the recently proposed Neural Generative QA (genQA) (Yin et al., 2015), where a set of triples are stored in a QA memory, and a neural network queries this memory for words to use in generating the answer. More specifically, phraseNetgate has the same gating strategy as in genQA. Still, phraseNet is different from that in several important ways: 1) phraseNet can handle multiple phrases in one sentence, and 2)\nphraseNet can generate multi-word expression. The softmax with multiple modes in phraseNetsoftmax is very similar to the recently proposed CopyNet (Gu et al., ). But the generative mode in CopyNet still follows a strict word-by-word fashion and therefore a soft-decision between modes has to be made for each mode. In a similar way, phraseNet is related to (Gulcehre et al., 2016a) and (Cheng and Lapata, 2016)."}, {"heading": "6 Conclusions and Future Work", "text": "We propose a neural machine translator which can leverage an external phrase memory, and empirically show its efficacy on Chinese-English translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural summarization by extracting sentences and words. arXiv preprint arXiv:1603.07252", "author": ["Cheng", "Lapata2016] Jianpeng Cheng", "Mirella Lapata"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Automatic recognition of multi-word terms", "author": ["Sophia Ananiadou", "Hideki Mima"], "venue": "the c-value/nc-value method. International Journal on Digital Libraries,", "citeRegEx": "Frantzi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Frantzi et al\\.", "year": 2000}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu et al.] Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "In ACL2016", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "2016a. Pointing the unknown words. arXiv preprint arXiv:1603.08148", "author": ["Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Noisy activation functions", "author": ["Marcin Moczulski", "Misha Denil", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.00391", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Modeling term translation for document-informed machine translation", "author": ["Meng et al.2014] Fandong Meng", "Deyi Xiong", "Wenbin Jiang", "Qun Liu"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Meng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2014}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Meng et al.2015] Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "In Proceedings of the 53rd Annual Meeting", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Improving statistical machine translation using domain bilingual multiword expressions", "author": ["Ren et al.2009] Zhixiang Ren", "Yajuan L\u00fc", "Jie Cao", "Qun Liu", "Yun Huang"], "venue": "In Proceedings of the Workshop on Multiword Expressions: Identification,", "citeRegEx": "Ren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2009}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Modeling coverage for neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Neural machine translation (NMT), although only proposed recently, has shown great potential, and arguably surpassed statistical machine translation on tasks like English-German translation (Sennrich et al., 2015).", "startOffset": 190, "endOffset": 213}, {"referenceID": 0, "context": "Our work is built upon the attention-based neural machine translation model that learns to align and translate jointly (Bahdanau et al., 2015), which will be referred to as RNNsearch.", "startOffset": 119, "endOffset": 142}, {"referenceID": 2, "context": "where the function fu is GRU (Cho et al., 2014; Chung et al., 2014).", "startOffset": 29, "endOffset": 67}, {"referenceID": 3, "context": "where the function fu is GRU (Cho et al., 2014; Chung et al., 2014).", "startOffset": 29, "endOffset": 67}, {"referenceID": 10, "context": "We use the regular encoder from RNNsearch, but add tags to hi (Meng et al., 2015), h \u2032 i = [hi, tagi].", "startOffset": 62, "endOffset": 81}, {"referenceID": 8, "context": ", 2016b) and we add residual connection (He et al., 2015) from the first layer to the second hidden layer, The output layer uses sigmoid as activation function.", "startOffset": 40, "endOffset": 57}, {"referenceID": 9, "context": "The first source consists of extracted phrase pairs from a bi-lingual corpus using the method described in (Meng et al., 2014), (Ren et al.", "startOffset": 107, "endOffset": 126}, {"referenceID": 11, "context": ", 2014), (Ren et al., 2009) and (Frantzi et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 4, "context": ", 2009) and (Frantzi et al., 2000).", "startOffset": 12, "endOffset": 34}, {"referenceID": 14, "context": "In both the RNNsearch and our models, we adopt the coverage models introduced in (Tu et al., 2016) to mitigate the problem of over-translation.", "startOffset": 81, "endOffset": 98}], "year": 2016, "abstractText": "In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.", "creator": "LaTeX with hyperref package"}}}