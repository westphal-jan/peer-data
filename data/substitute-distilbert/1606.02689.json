{"id": "1606.02689", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Continuously Learning Neural Dialogue Management", "abstract": "we describe a two - step approach regarding response management in task - oriented spoken dialogue systems. a unified neural network modeling is designed to enable the system to first learn by supervision reading a combination of dialogue data and then continuously improve stimulus behaviour via reinforcement learning, all using gradient - based algorithms on one single model. the experiments simulated the supervised model's effectiveness in the corpus - based evaluation, with user assistants, and with paid human subjects. the use combined reinforcement learning further improves the model's performance in both interactive settings, especially under higher - intervention conditions.", "histories": [["v1", "Wed, 8 Jun 2016 19:03:06 GMT  (749kb,D)", "http://arxiv.org/abs/1606.02689v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pei-hao su", "milica gasic", "nikola mrksic", "lina rojas-barahona", "stefan ultes", "david vandyke", "tsung-hsien wen", "steve young"], "accepted": false, "id": "1606.02689"}, "pdf": {"name": "1606.02689.pdf", "metadata": {"source": "CRF", "title": "Continuously Learning Neural Dialogue Management", "authors": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen"], "emails": ["sjy}@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Developing a robust Spoken Dialogue System (SDS) traditionally requires a substantial amount of hand-crafted rules combined with various statistical components. In a task-oriented SDS, teaching a system how to respond appropriately is nontrivial. More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jurc\u030c\u0131\u0301c\u030cek et al., 2011; Young et al., 2013). In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016). To enable the system to be trained on-line, sample-efficient\nlearning algorithms have been proposed (Gas\u030cic\u0301 and Young, 2014; Daubigney et al., 2014) which can learn policies from a minimal number of dialogues. However, even with such methods, performance is still poor in the early training stages, and this can impact negatively on the user experience. For these and other reasons, most commercial systems still handcraft the dialogue management to ensure its stability.\nSupervised learning (SL) has also been used in dialogue research where a policy is trained to produce an example response given the dialogue state. Wizard-of-Oz (WoZ) methods (Kelley, 1984; Dahlba\u0308ck et al., 1993) have been widely used for collecting domain-specific training corpora. Recently an emerging line of research has focused on training a network-based dialogue model, mostly in text-input schemes (Vinyals and Le, 2015; Serban et al., 2015; Wen et al., 2016; Bordes and Weston, 2016). These systems were directly trained on past dialogues without detailed specification of the internal dialogue state. However, there are two key limitations of using the SL approach for SDS. Firstly, the effects of selecting an action on the future course of the dialogue are not considered. Secondly, there may be a very large number of dialogue states for which an appropriate response must be generated. Hence, the SL training set may lack sufficient coverage. Another issue is that there is no reason to suppose a human wizard is acting optimally, especially at high noise levels. These problems exacerbate in larger domains where multi-step planning is needed. Thus, learning to mimic a human wizard does not necessary lead to optimal behaviour.\nTo get the best of both SL- and RL-based dialogue\nar X\niv :1\n60 6.\n02 68\n9v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\nmanagement, this paper describes a network-based model which is initially trained with a supervised spoken dialogue dataset. Since the training data may be mismatched to the deployment environment, the model is further improved by RL in interaction with a simulated user or human users. The advantage of the proposed framework is that a single model can be trained using both SL and RL without modifying the system architecture. This resembles the training process used in AlphaGo (Silver et al., 2016) for the game of Go. In addition, unlike most of thestate-of-the-art RL-based dialogue systems (Gas\u030cic\u0301 and Young, 2014; Cuaya\u0301huitl et al., 2015) which operate on a constrained set of summary actions to limit the policy space and minimise expensive training costs, our model operates on a full action set."}, {"heading": "2 Neural Dialogue Management", "text": "The proposed framework addresses the dialogue management component in a modular SDS. As depicted in Figure 1, the input to the model is the belief state s which encodes the understood user intents along with the dialogue history (Henderson et al., 2014b; Mrks\u030cic\u0301 et al., 2015), and the output is the master dialogue action a that decides the semantic reply. This is subsequently passed to the natural language generator (Wen et al., 2015).\nDialogue management is represented as a Policy Network, a neural network with one hidden layer exploiting tanh non-linearities, an output layer consisting of two softmax partitions and six sigmoid partitions. For the softmax outputs, one is for predicting DiaAct, a multi-class label over five dialogue acts: {request, offer, confirm, select, bye}, and the other for predicting Query, containing four options for the search constraint: {food, pricerange, area, none}. Query options only matter if the dialogue act in {request, confirm, select} is used. The sigmoid partitions are for Offer, each of which is used to determine a binary prediction when making system offer1.\nGiven the system\u2019s understanding of the user, the model\u2019s role is to determine what the intent of the system response should be and which slot to talk about. The exact value in each slot is decided by a\n1System-offer slots are slots the system can mention, such as area, phone number and postcode.\nseparate database parser, where the query is the top prediction of each user-informable slot2 from the dialogue state tracker and the output is a matched entity. This output forms the system\u2019s semantic reply, the master dialogue action."}, {"heading": "2.1 Phase I: Supervised Learning", "text": "In the first phase, the policy network is trained on corpus data. This data may come from WoZ collection or from interactions between users and an already existing policy, such as a hand-crafted baseline. The objective here is to \u2018mimic\u2019 the response behaviour within the supervised data.\nThe training objective for each sample is to minimise a joint cross-entropy loss L(\u03b8) between model action labels y defined in \u00a72 and predictions p:\nL(\u03b8) = \u2211\nk\u2208{da,q,Os}\nH(yk, pk), (1)\nwhere DiaAct da and Query q outputs are categorical distributions, and the Offer set Os contains six binary offer slots. \u03b8 are the network parameters."}, {"heading": "2.2 Phase II: Reinforcement Learning", "text": "The policy trained in phase I on a fixed dataset may not generalise well. In spoken dialogue, the noise level may vary across conditions and thus can significantly affect performance. Hence, the second phase of the training pipeline aims at improving the SL trained policy network by further training using policy-gradient based RL. The model is given the freedom to select any combination of\n2User-informable slots are slots used by the user to constrain the search, such as area and price range.\nmaster action. The training objective is to find a parametrised policy \u03c0\u03b8 that maximises the expected reward J(\u03b8) of a dialogue with T turns: J(\u03b8) = E [\u2211T\nt=1 \u03b3 tr(st, at) \u2223\u2223\u2223\u03c0\u03b8], where \u03b3 is the discount factor and r(st, at) is the reward when taking master action at in dialogue state st. Note that the structure and initial weights of \u03c0\u03b8 are fixed by the SL pre-training phase, since the RL training aims to improve the SL trained model.\nHere a batch algorithm is adopted, and all transitions are sampled under the current policy. At each update iteration, N episodes were generated, where the ith episode consists of a set of transition tuples {(sit, ait, rit)} Ti t=0. The estimated gradient is estimated using the likelihood ratio trick:\n\u2207\u03b8J(\u03b8) = 1\nNTi N\u2211 i=1 Ti\u2211 t=0 \u2207\u03b8 log \u03c0(ait|sit; \u03b8)Rit, (2)\nwhere Rit = \u2211Ti t\u2032=t \u03b3 t\u2032\u2212trit\u2032 is the cumulative return from time-step t to Ti. Gradient descent is, however, slow and has poor convergence properties.\nNatural gradient (Amari, 1998) improves upon the above \u2019vanilla\u2019 gradient method by computing an ascent direction that approximately ensures a small change in the policy distribution. This direction is w = F (\u03b8)\u22121\u2207\u03b8J(\u03b8), where F (\u03b8) is the Fisher information matrix (FIM). Based on this, Peters and Schaal (2006) developed the Natural Actor-Critic (NAC). In its episodic case (eNAC), the FIM does not need to be explicitly computed to obtain the natural gradient w. eNAC uses a least square method:\nRn = [ T\u2211 t=0 \u2207\u03b8 log \u03c0(ait|sit; \u03b8)T ] \u00b7 w + C, (3)\nwhere C is a constant and \u2200n \u2208 {1, ..., N} an analytical solution can be obtained. For larger models with more parameters, a truncated variant (Schulman et al., 2015) can also be used to practically calculate the natural gradient.\nExperience replay (Lin, 1992) is utilised to randomly sample mini-batches of experiences from a reply pool P . This increases data efficiency by reusing experience samples in multiple updates and reduces the data correlation. As the gradient is highly correlated with the return R, to ensure stable training, a unity-based reward normalisation is adopted to normalise the total return Rn between 0 and 1."}, {"heading": "3 Experimental Results", "text": "The target application is a live telephone-based SDS providing restaurant information for the Cambridge (UK) area. The domain consists of approximately 150 venues, each having 6 slots out of which 3 can be used by the system to constrain the search (foodtype, area and price-range) and 3 are informable properties (phone-number, address and postcode) available once a database entity has been found.\nThe model was implemented using the Theano library (Theano Development Team, 2016). The size of the hidden layer was set to 32 and all the weights were randomly initialised between -0.1 and 0.1."}, {"heading": "3.1 Supervised Learning on Corpus Data", "text": "A corpus consisting of 720 user dialogues in the Cambridge restaurant domain was split into 4:1:1 for training, validation and testing. This corpus was collected via the Amazon Mechanical Turk (AMT) service, where paid subjects interacted through speech with a well-behaved dialogue system as proposed in (Su et al., 2016). The raw data contains the top N speech recognition (ASR) results which were passed to a rule-based semantic decoder and the focus belief state tracker (Henderson et al., 2014a) to obtain the belief state that serves as the input feature to the proposed policy network. The turn-level labels were tagged according to \u00a72. Adagrad (Duchi et al., 2011) per dialogue was used during backpropagation to train each model based on the objective in Equation 1. To prevent over-fitting, early stopping was applied based on the held-out validation set.\nTable 1 shows the weighted F-1 scores computed on the test set for each label. We can clearly see that the model accurately determines the type of reply (DiaAct) and generally provides the right information (Offer). The hypothesised reason for the lower accuracy of Query is that the SL training data contains robust ASR results and thus the system examples contain more offers and less queries. This can be mitigated with a larger dataset covering more diverse situations, or improved via an RL approach."}, {"heading": "3.2 Policy Network in Simulation", "text": "The policy network was tested with a simulated user (Schatzmann et al., 2006) which provides the interaction at the semantic level. As shown in Figure 2, the first grid points labelled \u2018SL:0\u2019 represent the performance of the SL model under various semantic error rates (SER), averaged over 500 dialogues.\nThe SL model was then further trained using RL at different SERs. As the SL model is already performing well, the exploration parameter was set to 0.1. The size of the experience replay pool L was 2,000, and the mini-batch size was set to 32. For each update, natural gradient was calculated by eNAC to update the model weights of size \u223c2600. The total return given to each dialogue was set to 20 \u00d7 1(D) \u2212 T , where T is the dialogue turn number and 1(D) is the success indicator for dialogue D. Maximum dialogue length was set to 30 turns. Return normalisation was used to stabilise training.\nThe success rate of the SL model can be seen to increase for all SERs during 6,000 training dialogues, spreading between 1-8% improvement. Generally speaking, the greatest improvement occurs when the SER is most different to the SL training set, which are the higher SER conditions here. In this case, as the semantic hypotheses were more corrupted, the model learned to double-check more on what the user really wanted. This indicates the model\u2019s ability to refine its own behaviour via RL."}, {"heading": "3.3 Policy Network with Real Users", "text": "Starting from the same SL policy network as in \u00a73.2, the model was improved via RL using human subjects recruited via AMT. The policy network was plugged-in to a modular SDS, comprising the Microsoft\u2019s Bing speech recogniser 3, a rule-based semantic decoder, the focus belief state tracker, and a template-based natural language generator.\nTo ensure the dialogue quality, only those dialogues whose objective system check matched with the user rating were considered (Gas\u030cic\u0301 et al., 2013). Based on this, two parallel policies were trained with 200 dialogues. To evaluate the resulting policies, policy learning was disabled and a further 110 dialogues were collected with both the SL only and SL+RL models. The AMT users were asked to rate\n3www.microsoft.com/cognitive-services/en-us/speech-api.\nthe dialogue quality by answering the question \u201cDo you think this dialogue was successful?\u201d on a 6-point Likert scale and also providing a binary rating on dialogue success. The average quality rating (scaled from 0 to 5) is shown in Table 2 with one standard error. The results indicate that the SL-model could work quite well with humans, but was improved by RL on the 200 training dialogues. This demonstrates that on-line RL is a viable approach to adapt a dialogue system to changing environmental conditions."}, {"heading": "4 Conclusion", "text": "This paper has presented a two-step development for the dialogue management in SDS using a unified neural network framework, where the model can be trained on a fixed dialogue dataset using SL and subsequently improved via RL through simulated or spoken interactions. The experiments demonstrated the efficiency of the proposed model with only a few hundred supervised dialogue examples. The model was further tested in simulated and real user settings. In a mismatched environment, the model was capable of modifying its behaviour via a delayed reward signal and achieved better success rate."}, {"heading": "Acknowledgments", "text": "Pei-Hao Su is supported by Cambridge Trust and the Ministry of Education, Taiwan."}], "references": [{"title": "Learning end-to-end goal-oriented dialog", "author": ["Bordes", "Weston2016] Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint:", "citeRegEx": "Bordes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2016}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["Simon Keizer", "Oliver Lemon"], "venue": "arXiv preprint arXiv:1511.08099", "citeRegEx": "Cuay\u00e1huitl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cuay\u00e1huitl et al\\.", "year": 2015}, {"title": "Wizard of oz studies: why and how", "author": ["Arne J\u00f6nsson", "Lars Ahrenberg"], "venue": "In Proc of Intelligent user interfaces", "citeRegEx": "Dahlb\u00e4ck et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dahlb\u00e4ck et al\\.", "year": 1993}, {"title": "A comprehensive reinforcement learning framework for dialogue management optimisation", "author": ["Matthieu Geist", "Senthilkumar Chandramohan", "Olivier Pietquin"], "venue": "Journal of Selected Topics in Signal Processing,", "citeRegEx": "Daubigney et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daubigney et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Task completion transfer learning for reward inference", "author": ["Romain Laroche", "Olivier Pietquin"], "venue": "In Proc of MLIS", "citeRegEx": "Asri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asri et al\\.", "year": 2014}, {"title": "Gaussian processes for pomdp-based dialogue manager optimization", "author": ["Ga\u0161i\u0107", "Young2014] Milica Ga\u0161i\u0107", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Ga\u0161i\u0107 et al.2013] Milica Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve J. Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "The Second Dialog State Tracking Challenge", "author": ["B. Thomson", "J. Williams"], "venue": "In Proc of SIGdial", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based Dialog State Tracking with Recurrent Neural Networks", "author": ["B. Thomson", "S.J. Young"], "venue": "In Proc of SIGdial", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as pomdps", "author": ["Blaise Thomson", "Steve Young"], "venue": "TSLP,", "citeRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.", "year": 2011}, {"title": "An iterative design methodology for user-friendly natural language office information applications", "author": ["John F. Kelley"], "venue": "ACM Transaction on Information Systems", "citeRegEx": "Kelley.,? \\Q1984\\E", "shortCiteRegEx": "Kelley.", "year": 1984}, {"title": "A stochastic model of computerhuman interaction for learning dialogue strategies. Eurospeech", "author": ["Levin", "Pieraccini1997] Esther Levin", "Roberto Pieraccini"], "venue": null, "citeRegEx": "Levin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Long-Ji Lin"], "venue": "Machine learning,", "citeRegEx": "Lin.,? \\Q1992\\E", "shortCiteRegEx": "Lin.", "year": 1992}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Mrk\u0161i\u0107 et al.2015] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "PeiHao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proc of ACL", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Policy gradient methods for robotics", "author": ["Peters", "Schaal2006] Jan Peters", "Stefan Schaal"], "venue": "In IEEE RSJ", "citeRegEx": "Peters et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2006}, {"title": "Spoken dialogue management using probabilistic reasoning", "author": ["Roy et al.2000] Nicholas Roy", "Joelle Pineau", "Sebastian Thrun"], "venue": "In Proc of SigDial", "citeRegEx": "Roy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2000}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Karl Weilhammer", "Matt Stuttle", "Steve Young"], "venue": "The knowledge engineering review,", "citeRegEx": "Schatzmann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Trust region policy optimization", "author": ["Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "Proc of ICML", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neu", "author": ["Silver et al.2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems", "author": ["Su et al.2015] Pei-Hao Su", "David Vandyke", "Milica Ga\u0161i\u0107", "Dongho Kim", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": null, "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "On-line active reward learning for policy optimisation in spoken dialogue systems", "author": ["Su et al.2016] Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proc of ACL", "citeRegEx": "Su et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Multi-domain dialogue success classifiers for policy training", "author": ["Pei-Hao Su", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Tsung-Hsien Wen", "Steve Young"], "venue": "ASRU", "citeRegEx": "Vandyke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vandyke et al\\.", "year": 2015}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "In Proceedings of the 2015 Conference", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen et al.2016] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D. Williams", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}, {"title": "Pomdp-based statistical spoken dialogue systems: a review", "author": ["Young et al.2013] Steve Young", "Milica Ga\u0161ic", "Blaise Thomson", "Jason Williams"], "venue": "In Proc of IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jur\u010d\u0131\u0301\u010dek et al., 2011; Young et al., 2013).", "startOffset": 169, "endOffset": 285}, {"referenceID": 10, "context": "More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jur\u010d\u0131\u0301\u010dek et al., 2011; Young et al., 2013).", "startOffset": 169, "endOffset": 285}, {"referenceID": 28, "context": "More recently, this dialogue management task has been formulated as a reinforcement learning (RL) problem which can be automatically optimised through human interaction (Levin and Pieraccini, 1997; Roy et al., 2000; Williams and Young, 2007; Jur\u010d\u0131\u0301\u010dek et al., 2011; Young et al., 2013).", "startOffset": 169, "endOffset": 285}, {"referenceID": 21, "context": "In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016).", "startOffset": 172, "endOffset": 250}, {"referenceID": 23, "context": "In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016).", "startOffset": 172, "endOffset": 250}, {"referenceID": 22, "context": "In this framework, the system learns by a trial and error process governed by a potentially delayed learning objective, a reward function, that determines dialogue success (El Asri et al., 2014; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016).", "startOffset": 172, "endOffset": 250}, {"referenceID": 3, "context": "To enable the system to be trained on-line, sample-efficient learning algorithms have been proposed (Ga\u0161i\u0107 and Young, 2014; Daubigney et al., 2014) which can learn policies from a minimal number of dialogues.", "startOffset": 100, "endOffset": 147}, {"referenceID": 11, "context": "Wizard-of-Oz (WoZ) methods (Kelley, 1984; Dahlb\u00e4ck et al., 1993) have been widely used for collecting domain-specific training corpora.", "startOffset": 27, "endOffset": 64}, {"referenceID": 2, "context": "Wizard-of-Oz (WoZ) methods (Kelley, 1984; Dahlb\u00e4ck et al., 1993) have been widely used for collecting domain-specific training corpora.", "startOffset": 27, "endOffset": 64}, {"referenceID": 19, "context": "training a network-based dialogue model, mostly in text-input schemes (Vinyals and Le, 2015; Serban et al., 2015; Wen et al., 2016; Bordes and Weston, 2016).", "startOffset": 70, "endOffset": 156}, {"referenceID": 26, "context": "training a network-based dialogue model, mostly in text-input schemes (Vinyals and Le, 2015; Serban et al., 2015; Wen et al., 2016; Bordes and Weston, 2016).", "startOffset": 70, "endOffset": 156}, {"referenceID": 20, "context": "This resembles the training process used in AlphaGo (Silver et al., 2016) for the game of Go.", "startOffset": 52, "endOffset": 73}, {"referenceID": 1, "context": "In addition, unlike most of thestate-of-the-art RL-based dialogue systems (Ga\u0161i\u0107 and Young, 2014; Cuay\u00e1huitl et al., 2015) which operate on a constrained set of summary actions to", "startOffset": 74, "endOffset": 122}, {"referenceID": 25, "context": "This is subsequently passed to the natural language generator (Wen et al., 2015).", "startOffset": 62, "endOffset": 80}, {"referenceID": 18, "context": "For larger models with more parameters, a truncated variant (Schulman et al., 2015) can also be used to practically calculate the natural gradient.", "startOffset": 60, "endOffset": 83}, {"referenceID": 13, "context": "Experience replay (Lin, 1992) is utilised to randomly sample mini-batches of experiences from a reply pool P .", "startOffset": 18, "endOffset": 29}, {"referenceID": 22, "context": "with a well-behaved dialogue system as proposed in (Su et al., 2016).", "startOffset": 51, "endOffset": 68}, {"referenceID": 4, "context": "Adagrad (Duchi et al., 2011) per dialogue was used during backpropagation to train each model based on the objective in", "startOffset": 8, "endOffset": 28}, {"referenceID": 17, "context": "The policy network was tested with a simulated user (Schatzmann et al., 2006) which provides the interaction at the semantic level.", "startOffset": 52, "endOffset": 77}, {"referenceID": 7, "context": "To ensure the dialogue quality, only those dialogues whose objective system check matched with the user rating were considered (Ga\u0161i\u0107 et al., 2013).", "startOffset": 127, "endOffset": 147}], "year": 2016, "abstractText": "We describe a two-step approach for dialogue management in task-oriented spoken dialogue systems. A unified neural network framework is proposed to enable the system to first learn by supervision from a set of dialogue data and then continuously improve its behaviour via reinforcement learning, all using gradientbased algorithms on one single model. The experiments demonstrate the supervised model\u2019s effectiveness in the corpus-based evaluation, with user simulation, and with paid human subjects. The use of reinforcement learning further improves the model\u2019s performance in both interactive settings, especially under higher-noise conditions.", "creator": "LaTeX with hyperref package"}}}