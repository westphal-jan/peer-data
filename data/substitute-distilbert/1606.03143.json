{"id": "1606.03143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "PerSum: Novel Systems for Document Summarization in Persian", "abstract": "developing this paper we explore the problem, document summarization in gujarati language from two distinct angles. in our first aspect, we modify a popular and widely cited persian document mapping framework ; teach how it works underneath a realistic corpus of news articles. human evaluation on generated summaries shows that graph - linked methods perform better than the modified systems. colleagues carry this intuition forward in our second approach, and probe deeper into the nature against graph - based systems by designing several summarizers based on centrality measures. ad hoc evaluation using rouge algorithms on these categories suggests that there is a small class of centrality measures that perform better than three strong unsupervised verbs.", "histories": [["v1", "Thu, 9 Jun 2016 23:32:41 GMT  (1132kb)", "http://arxiv.org/abs/1606.03143v1", "42 pages, 9 figures"]], "COMMENTS": "42 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["saeid parvandeh", "shibamouli lahiri", "fahimeh boroumand"], "accepted": false, "id": "1606.03143"}, "pdf": {"name": "1606.03143.pdf", "metadata": {"source": "CRF", "title": "PerSum: Novel Systems for Document Summarization in Persian", "authors": ["Saeid Parvandeh", "Shibamouli Lahiri", "Fahimeh Boroumand", "Saadi Shirazi", "Omar Khayyam"], "emails": ["saeid-parvandeh@utulsa.edu,", "lahiri@umich.edu,", "f.boroomand92@gmail.com"], "sections": [{"heading": null, "text": "In this paper we explore the problem of document summarization in Persian language from two distinct angles. In our first approach, we modify a popular and widely cited Persian document summarization framework to see how it works on a realistic corpus of news articles. Human evaluation on generated summaries shows that graph-based methods perform better than the modified systems. We carry this intuition forward in our second approach, and probe deeper into the nature of graph-based systems by designing several summarizers based on centrality measures. Ad hoc evaluation using ROUGE score on these summarizers suggests that there is a small class of centrality measures that perform better than three strong unsupervised baselines.\nKeywords Persian language, Summarization, Hamshahri, Pasokh, Human evaluation, ROUGE, Centrality, JROUGE, Parsumist."}, {"heading": "1. Introduction", "text": "The Persian language is spoken by 110 million people worldwide, and is considered one of the official languages in three different countries \u2013 Iran, Afghanistan, and Tajikistan.1 Persian (also known as Farsi) has a rich literary and political history spanning at least two\n1Source: https://en.wikipedia.org/wiki/Persian_language.\nand a half millennia, and has influenced a number of modern languages including Arabic, Urdu, Hindi, Turkish, and Uzbek. Prominent Persian poets from the Middle Ages include Rumi, Hafez, Saadi Shirazi, Omar Khayyam, Ferdowsi, Nizami, Shams Tabrizi, and Attar of Nishapur. Persian literature is vast and complex, and spans the geographical boundaries of a multitude of countries, permeating and pervading in the process their unique culture and art forms, and infusing their social psyche since time immemorial with ever-fresh forms of literary tapestry.2\nDespite having such a large speaker population and a vibrant presence in many countries, Natural Language Processing in Persian has been stymied by a lack of publicly available corpora and tools, which has only very recently started to change (Seraji, 2013; Seraji, 2015). In particular, research studies in Persian document summarization are fairly new and limited, and there is still a lot of room to try different approaches (please see Section 2 for a discussion of related recent work).\nIn this paper, we address the problem of document summarization in Persian language from two different angles. In our first approach, we used a well-known Persian news corpus (Hamshahri (AleAhmad et al., 2009)) to evaluate Parsumist \u2013 a popular Persian document summarization framework (Sections 3 and 4). The reason we chose to re-implement Parsumist is because it is one of the most highly cited and most detailed Persian summarization systems. Also, it is simple, flexible, modular, intuitive, and relatively easy to understand and implement. Therefore, a re-implementation of Parsumist, and evaluation on a large realistic corpus would give us a unique opportunity to peer into the strengths and weaknesses of such a traditional system. Our results indicate that graph-based systems perform better than Parsumist-based systems, and the choice of threshold is of particular importance for Parsumist. In our second approach, we used centrality measures on sentence networks (similar to TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004)) to perform single and multi-document summarization on the recently released Pasokh corpus (Sections 5 and 6). Pasokh consists of human-constructed summaries of news articles, and happens to be the first publicly available research corpus of its kind in the Persian language (Behmadi Moghaddas et al., 2013). It should be noted that we are the first to bring centrality-based approaches in Persian document summarization, and we are also the first to use ROUGE (Lin, 2004) in the above setting. Our results indicate that different centrality measures perform differently in this task, and only about half of the centrality indices perform better than strong unsupervised baselines.\nOur contributions include: (a) extending a popular Persian document summarization\n2https://en.wikipedia.org/wiki/Persian_literature\nframework in multiple ways, and measuring its impact on a large, realistic corpus of news articles, (b) extensive human evaluation to identify the strengths and weaknesses of particular extensions, (c) introduction of centrality-based approaches to Persian document summarization for the first time (both single and multi-document), and (d) introduction of three intuitive baselines, and variations of the standard evaluation measure ROUGE \u2013 to build the first meaningful and valid comparison point on a publicly available annotated dataset.\nThis paper is organized as follows. Section 2 discusses existing work (mostly recent) in Persian document summarization. We describe Parsumist (Shamsfard et al., 2009a; Shamsfard et al., 2009b) and our extensions in Section 3, followed by human evaluation on the Hamshahri corpus in Section 4. 3 Centrality-based summarization methods are explained in detail in Section 5, followed by the results on single and multi-document summarization on Pasokh corpus in Section 6. Section 7 concludes the paper, offering future research directions, and discussing and outlining potential problems and limitations of our work. Systems, tools, and corpora are introduced and explained as and when they appear in the text for the first time."}, {"heading": "2. Related Work", "text": "Arguably the first Persian text summarization system was FarsiSum (Hassel and Mazdak, 2004). It used a client-server application written in Perl, and was little more than an earlier implementation \u2013 SweSum (Dalianis, 2000) \u2013 augmented by a Persian stop word list. No evaluation was reported in Hassel and Mazdak (2004)\u2019s study.\nZamanifar et al. (2008) reported the next study using lexical chains, word clustering, and a summary coefficient for each cluster. Words were clustered by their co-occurrence degree assessed by a bigram language model. Results on 60 Persian news articles showed that Zamanifar et al. (2008)\u2019s approach was superior to FarsiSum (in terms of precision and recall) at compression ratios of 30%, 40%, and 50%.\nBerenjkoob et al. (2009) explored the qualitative and quantitative merits of\nincorporating stemming and stop word removal in Persian document summarization.\nParsumist \u2013 one of the most important research efforts in Persian summarization \u2013 came in 2009 (Shamsfard et al., 2009a; Shamsfard et al., 2009b). Parsumist used an elaborate pipeline of three stages \u2013 preprocessing, analysis, and content selection. In the preprocessing stage, stop words were removed and content words were mapped to a sense hierarchy akin to WordNet (Miller, 1995). In the analysis stage, ten features were used to\n3Note that we were not the first to use Hamshahri in Persian document summarization. Kamyar et al. (2011) were the first.\nscore a sentence, followed by a selection and redundancy removal step. In the final selection stage, anaphora were resolved and sentences were re-ordered to restore coherence and temporal cues. Parsumist was shown to perform better than FarsiSum (in terms of precision and recall) on documents from different genres.\nA flurry of research activity ensued after Parsumist. Kiyoumarsi and Esfahani (2011) introduced fuzzy logic in Persian document summarization. They reported performance superior to four other systems in a simulation study. Zamanifar and Kashefi (2011) discussed AZOM \u2013 a summarizer that takes into account the implicit or explicit structure of a document (in terms of paragraphs, blocks, etc). AZOM performed better than two state-of-the-art approaches, and a \u201cflat summary\u201d baseline. Tofighy et al. (2011) used a more rigorous structure-based approach, leveraging block nesting and sibling blocks. The performance was better than FarsiSum at 30-40% compression ratio, but the precision and recall values were lower compared to AZOM.\nBazghandi et al. (2012) clustered sentences based on the semantic similarity of their content words. Semantic similarity between two words was defined as a variant of their PMI (pointwise mutual information). Sentences were clustered using a particle swarm optimization (PSO) algorithm. The system achieved competitive results with traditional clustering approaches.\nShakeri et al. (2012) constructed an undirected sentence network similar to LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). They applied the system to ten Persian scientific papers at a compression ratio of 50%. The network-based system performed significantly better than FarsiSum against a human-generated gold standard (in terms of precision, recall, F-score, and ROUGE-1).\nAn interesting study employing Analytical Hierarchy Process (AHP) to Persian document summarization was discussed in (Tofighy et al., 2013). The authors arranged six existing summarization features (word frequency, keywords, headline word, cue word, sentence position, and sentence length) in an AHP matrix to assess their relative importance, and to come up with an optimal selection of sentences. Results showed better F-score than FarsiSum at compression ratios of 30% and 40%.\nFinally, Nia (2013) did his Master\u2019s thesis in generating gold standards for Persian\ndocument summarization using the pyramid method (Nenkova et al., 2007).\nIt should be noted from the discussion in this section that while the studies in Persian document summarization have been numerous, they are mostly recent, and as a result, not very mature. For example, few of them used the recently released Pasokh corpus of annotated summaries, and very few explored a large and realistic corpus such as Hamshahri. Furthermore, no studies looked into centrality-based (also known as graph-based)\napproaches to summarization. In this paper, we bridge these gaps."}, {"heading": "3. Parsumist and Its Extensions", "text": "Document summarization systems have traditionally been classified into several categories: extractive vs. abstractive, single-document vs. multi-document, generic vs. query-focused, and neutral vs. opinionative. For the purpose of illustration, we briefly define these terms here. An extractive summarization system chooses sentences and/or sub-sentence units (e.g., phrases and clauses) from the original document(s) to build the summary; an abstractive one, on the other hand, creates its own language \u2013 usually by some form of a natural language generation component \u2013 to build a summary. There could be some extractive components within an abstractive summarizer. It should be noted that abstractive summarization is a much more difficult problem than extractive, and most existing research literature is on extractive summarization. Parsumist follows the extractive approach.\nSummarization systems can work on single documents and/or multiple documents.\nMultiple-document summarization is generally harder than single-document summarization, owing to the additional complexity of inter-relationship between documents, which may involve comparison, contrast, conflicting points of view, differing opinions, redundant information, spurious information, and style mismatch \u2013 among others. Parsumist can work in both single-document and multi-document modes.\nAn interesting take on document-summarization is generic vs. query-focused. Generic summarizers give an overview of a document, without regard to any other source of information. Query-focused summarizers, on the other hand, present information that is relevant to a particular information need (i.e., a query). Parsumist can act as either a generic summarization system, or as a query-focused one.\nLastly, oftentimes a document presents opinionated content. This is especially prevalent in social media \u2013 news, blogs, forums, tweets, product reviews, comments, Facebook posts, etc. A good summarizer should therefore include some of the opinionated content to give its users an idea about contrasting points of view, and the resulting conflict, if any. Parsumist does not provide such a functionality, and is therefore categorized as a neutral summarizer, rather than an opinionative one.\nThe overall structure of Parsumist is shown in Figure 1. As mentioned before, Parsumist is an extractive summarizer, and thus saves much of the design complexity that naturally comes into play while building a full-blown abstractive system. Furthermore, Parsumist is flexible because it can perform both single and multi-document summarization. It follows a pipelined approach that is highly modular, extensible, and language-independent save the resources (Figure 1). There are four main modules \u2013 preprocessing, analysis and scoring,\nselection and redundancy checking, and smoothing the summary for coherence. The authors mostly focused on the first three modules, and mentioned the fourth module rather cursorily. While re-implementing Parsumist, we therefore needed to exercise a fair bit of creative license on our own. We maintained the form and spirit of Parsumist, while altering its content as and when necessary. One example is the resources used: Parsumist used a stop word list, a cue word list, and a list of Persian synsets (the authors came up with their own version of synsets). While we did have access to a stop word list, we did not have a publicly available cue word list or a list of synsets. Thus, we had to resort to alternative resources that will be described as they are introduced.\nThe remainder of this section is organized as follows. In Subsections 3.1-3.4, we describe the four components of Parsumist, along with details on our re-implementation of the same. In the end, we designed four systems based on Parsumist. Subsection 3.5 gives details on the multi-document summarization part of Parsumist, and Subsection 3.6 outlines Parsumist\u2019s evaluation strategy \u2013 and how we re-implemented it."}, {"heading": "3.1. Preprocessing", "text": "Parsumist uses a combination of statistical, semantic, and heuristic methods. The preprocessing step consists of tokenization, stop word removal, and conceptual mapping (i.e., mapping words and phrases to synsets). Note that tokenization of free-form Persian text is rather non-trivial because of encoding problems, mixed ordering (i.e., presence of intermingled right-to-left and left-to-right sequences of characters), omitting Ezafe in Ezafe construction, and irregularities in word segmentation. While Parsumist uses a home-grown tokenizer, we used the tokenizer made available by Mojgan Seraji as part of the Persian language processing toolkit she developed (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015). The version we used is based on a Ruby-on-Rails framework for basic preprocessing, and a couple of Perl scripts for sentence segmentation and subsequent word segmentation. Note further that we used Hamshahri (AleAhmad et al., 2009) as our input corpus for this part, and Hamshahri is released in NCR (numeric character encoding) encoded XML format, so we needed to convert NCR into UTF-8 Unicode first. We used the ltchinese Python library4 for this purpose.\nStop word removal requires a standard stop word list. Since the stop word list used by Parsumist authors was not publicly available, we used the stop word list that comes with the Hamshahri corpus. Similarly, Parsumist authors mapped each word and phrase to a concept (i.e., a synset) in a small concept hierarchy akin to WordNet (Miller, 1995). This concept hierarchy was later used to construct an undirected sentence network (please see Subsection 3.2). Since the concept hierarchy was not publicly available, we constructed our sentence networks using Latent Semantic Analysis (LSA; see Subsection 3.2 for details).5"}, {"heading": "3.2. Analysis and Scoring", "text": "As mentioned before, Parsumist uses an extractive summarization strategy for the most part. This strategy requires scoring sentences for their importance (salience), and subsequently selecting the highest-scored sentences that are presumably also the most important. There are many different ways to score sentences; Parsumist uses a very simple additive strategy based on surface, syntactic, and semantic features. The overall scoring function is as follows:\n j ijji pcsW )( (1)\n4https://pypi.python.org/pypi/ltchinese 5Note that we are not the first to introduce LSA in Persian document summarization. Poormasoomi et al. (2011) were the first.\nwhere W(si) is the score of the i-th sentence, cj is the weight of feature j (assumed to be 1 by the authors), and pij is the value of feature j in sentence i. The features are as follows:\n1. Number of main words, title words, and query words. Presence of important\nwords in a sentence (esp. words that appeared in the document title) often indicates that the sentence is important, and should probably be included in the summary. Furthermore, note that Parsumist can work as a query-focused summarizer by enhancing the importance of sentences that contain query words. In our implementation, we only considered title words (i.e., words that appeared in the document title), and disregarded main words and query words. Thus, our summarizer is generic (as opposed to query-focused). Moreover, we used the frequency of title words in a sentence (normalized by sentence length) as the feature value. 2. Length of the sentence. Authors posited that shorter sentences were more likely\nto appear in a summary. While the merit of this decision is debatable (longer sentences presumably contain more information, and should therefore be given a higher weight), in our implementation we simply added a feature \u2013 reciprocal of the sentence length \u2013 to simulate the correct behavior. 3. Proper nouns. Authors correctly observed that the importance of proper nouns\n(named entities \u2013 more generally) depends on the type of document. If we are summarizing news articles, academic papers, or short stories, proper nouns usually are important because they signal the presence of a named entity. For informal media like blogs, emails, tweets and online forums on the other hand, proper nouns may be much less important. In our implementation, we used the frequency of proper nouns in a sentence (normalized by sentence length) as the feature value. 4. English words and phrases. The authors made an interesting observation about\nEnglish words in Persian text; presence of English words usually indicates the first appearance of a new term in a document. Such terms are especially relevant if the document is talking about a scientific study (e.g., research articles). Our implementation uses the frequency of English words in a sentence (normalized by sentence length) as the feature value. 5. Quotation marks. Quotation marks in a sentence should be given due importance\nbecause they signify an important previous commentary. We used the frequency of quotation marks in a sentence (normalized by sentence length) as the feature value. 6. Pronouns. Most pronouns indicate some form of anaphora. Authors correctly\npointed out that we should select the sentence that contained the referent, in addition to selecting the sentence with the pronoun. However, this would require anaphora resolution, which is a hard problem for Persian. The authors therefore opted for an easier solution \u2013 reduce the score of a sentence that contains a pronoun, but do not delete it. We chose to use the same solution \u2013 with a twist. We used the frequency of non-pronoun words in a sentence (normalized by sentence length) as the feature value. 7. Percentage sign (\u201c%\u201d). The percentage sign is used to represent results (esp.\nimprovement) in scientific documents, and profit margins and general increments-decrements in financial documents. Hence derives its importance. We used the frequency of percentage signs in a sentence (normalized by sentence length) as the feature value. 8. Parenthetical or descriptive sentences. Embedded sentences and phrases \u2013 such\nas parentheticals \u2013 often do not add much value to the information content of a sentence, and hence can safely be deleted from a summary. This is a form of abstractive summarization that Parsumist designers performed. In our implementation, we chose to simply include parentheses as features. We used the frequency of non-parenthesis tokens in a sentence (normalized by sentence length) as the feature value. This was done to ensure that sentences containing parentheses are given reduced importance. Note that the original implementation probably required a dependency parser to obtain this feature. We chose to sidestep dependency parsing of Persian text, because as the tools are not very mature yet, it may yield poor results. 9. Referential phrases. Authors maintained that phrases that refer to other parts of a\ndocument (\u201cin the last section\u201d, \u201cin the previous figure\u201d, etc) should not appear unaltered in the summary. They should be changed, and/or gain lower scores. While it is true that such phrases do not contribute (much) to the information content of a sentence, we are of the opinion that they do contribute to the style \u2013 esp. coherence of the document. Leaving them out in the final summary may therefore hurt coherence. In our implementation, we chose not to include this feature, as it may require anaphora resolution and/or discourse parsing \u2013 for which tools are not that mature in Persian. 10. Punctuation marks. Parsumist authors were not very clear about this feature, so\nwe chose not to include it in our implementation.\nIn the end, we had eight distinct features, as opposed to ten in the original study. Note\nthat in almost all the above cases, we needed to exercise our own discretion to come up with an appropriate feature value. This was due to under-specification and lack of clarity on the part of Parsumist authors, who did not adequately specify and/or quantify most of the above features. We therefore needed to come up with our own best (educated) guesses as to what they could have been. We also had to alter parameters and introduce new ones as appropriate. For example, we came up with sentence length normalization to combat the variability of sentence length. Many of the features needed a part-of-speech tagger; we used Mojgan Seraji\u2019s tagger (Hal\u00e1csy et al., 2007; Seraji, 2015) for this purpose.6\nOnce the score of each sentence is known, Parsumist constructs an undirected graph between sentences \u2013 each node is a sentence, and two nodes are connected if their similarity exceeds a certain threshold. Similarity between two nodes (sentences) is computed using lexical chains and a synset hierarchy akin to WordNet (Miller, 1995). The authors are very vague in describing how they computed this similarity metric. They do mention that the similarity is computed using the number of common or related words due to lexical chains and synsets, but no further details are provided. Moreover, the authors came up with their own synset hierarchy, and never released it to the public. Hence, we had to come up with our own similarity measure (instead of using the original method).\nWe defined the similarity between two sentences as the cosine between their LSA vectors.7 Note that LSA relies on Singular Value Decomposition, a matrix factorization technique that comes from Linear Algebra. The basic premise of LSA is very simple; if two words have similar meaning (say \u201crat\u201d and \u201cmouse\u201d), then they should occur in similar contexts. Hence, if we were to construct a co-occurrence matrix between words, and somehow compress this matrix into a low-dimensional form (known as low-rank approximation), then the resulting matrix will yield vectorial representations for semantically similar words that are \u201cclose\u201d to each other in terms of cosine similarity \u2013 even if the original vectors were not close together.\nThe particular way LSA works is as follows: we first construct a term-document matrix, where rows are words, and columns are documents. Each word is represented as a row vector, and each document is a column vector. The particular value of a single matrix cell could be term frequency, tfidf, or even binary presence/absence (see, e.g., Chung and Pennebaker (2008)). For our implementation, we chose simple term frequency as the value.\nOnce the term-document matrix (say, X) is constructed, we perform a Singular Value\nDecomposition (SVD) as follows:\n6Available at http://stp.lingfil.uu.se/~mojgan/tagper.html. 7Latent Semantic Analysis (Landauer and Dumais, 2008).\nTVUX  (2)\nwhere U and V are orthogonal matrices, and \u03a3 is a diagonal matrix. The diagonal entries \u03c3i of \u03a3 are known as singular values. Now, say X was an M-by-N matrix; then U is an M-by-M matrix, V is an N-by-N matrix, and \u03a3 is an M-by-N matrix.8\nWith that, we are now ready to describe the low-rank approximation (compression) step\nof LSA. Essentially, we perform a truncated SVD:\nT\nkkkk VUX  (3)\nwhere Xk is an M-by-N low-rank approximation of the original term-document matrix X, Uk is an M-by-k matrix (the first k columns of U), \u03a3k is a k-by-k square diagonal matrix (the first k rows and first k columns of \u03a3), and Vk is an N-by-k matrix (the first k columns of V). As it turns out, this truncated SVD representation is a form of lossy compression that is the \u201cclosest\u201d to the original term-document matrix X, in the sense that it minimizes the Frobenius Norm of the difference between X and Xk. Xk is known as the \u201crank-k approximation\u201d of X, since we took the largest k singular values, and the leftmost k columns of U and V to construct Xk. What comes out as the most important upshot of this decomposition procedure is a fact we alluded to in the above discussion: words that are semantically close to each other (say \u201crat\u201d and \u201cmouse\u201d), become closer in their vectorial representation (i.e., on the vector space). In other words, the original sparse matrix X becomes a dense matrix Xk, where semantically similar terms occupy similar regions of the vector space.\nFinally, we project a sentence onto this dense LSA space Xk, as follows. We first construct a term-vector for the sentence (say, s), whose dimensionality is M-by-1. The individual elements of this vector are simple term frequencies (as before). Once this vector is constructed, we use the following equation to turn it into a compressed (low-ranked) vector sk of dimensionality k-by-1:\nsUs Tkkk 1 (4)\n8M is the number of unique words. N is the number of documents.\nThis sk is what we will henceforth refer to as the \u201cLSA vector\u201d of a sentence. Note that the reason we needed to perform LSA on sentences is because Parsumist uses a semantic similarity between sentences, and LSA yields a very good proxy for semantic similarity. It is one of the cornerstones of modern distributional semantics (see, e.g., Bruni et al. (2014)), and is reasonably free from statistical assumptions.9 Parsumist authors used lexical chains and synsets to come up with semantic associations; we believe LSA is a finer-grained solution than that. We performed LSA on the full Hamshahri corpus, and chose the value of k to be 200 after initial parameter tuning."}, {"heading": "3.3. Selection and Redundancy Checking", "text": "As we mentioned before, sentence selection is a key step in any extractive summarization system. Redundancy checking, however, is more germane to multi-document summarization than single-document summarization. In single-document summarization, the sentences usually describe different aspects of an underlying issue. In multi-document summarization, there is no such guarantee. Two sentences from two different documents may describe exactly the same aspect of the same issue. Since Parsumist has the option of working as a multi-document summarizer, checking and managing redundancy becomes a paramount objective.\nThe way Parsumist handles redundancy consists of a three-step process that is merged\nwith sentence selection:\na) Begin with an empty summary. b) As long as the summary length is shorter than desired, choose the sentence with\nhighest score and minimum resemblance with the already-selected sentences.\nc) Continue until the desired summary length is reached.\nAs we can see, the above description leaves several questions un-answered (e.g., what is the minimum resemblance threshold? How many of the previously selected sentences to consider? How to choose among previously selected sentences?). We conducted extensive parameter tuning, and came up with the following algorithm (described in Python pseudocode):\n9With concomitant limitations, however; the discussion of which is outside the scope of this paper.\nWhat this algorithm does, is as follows. It keeps selecting highest-scored sentences for inclusion in the summary until the desired summary length (10 sentences) is reached. The algorithm makes sure that too long and too short sentences are excluded, by only taking into account sentences whose length falls within the 1st and 3rd quartiles of all Hamshahri sentences. It also excludes sentences that are too similar to already selected sentences, by enforcing a threshold on the minimum cosine similarity with the previously selected ones. Note that the 10 sentences length and minimum cosine similarity were selected as part of preliminary parameter tuning. Later, we observed that cosine thresholds of 0.1 and 0.2 produced reasonably good summaries. Among median, maximum, and minimum thresholds, minimum yielded the best results. This gives us two systems:\n1. Top 10 sentences, modified Parsumist, minimum cosine threshold of 0.1 2. Top 10 sentences, modified Parsumist, minimum cosine threshold of 0.2\nNext we went ahead and experimented with two more systems that are graph-based (also called centrality-based). Graph-based summarizers are one of the mainstays in extractive summarization, and we will discuss them in detail in Section 5. Briefly, they construct a network (graph) where each node is a sentence, and an edge appears between two sentences if their similarity exceeds a certain threshold. For our implementation, we chose to use the complete graph of sentences, and weighted edges with cosine similarity on LSA vectors. Then we followed the popular TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) formalism that prescribed running a random walk on this sentence network, and scoring nodes by their PageRank value (Page et al., 1998).10 Following recent studies, we also experimented with the weighted degree (or strength) of a\n10More discussion appears in Section 5.\nsorted_list = sort(sentences) by score in descending order selected_sentences = [] for sentence in sorted_list:\nif len(selected_sentences) == 10: break if sentence_length is outside 1st and 3rd quartiles: continue if min_cosine_sim(sentence, selected_sentences) > threshold:\ncontinue\nappend(selected_sentences, sentence)\nsentence instead of PageRank.11 This gives rise to our last two systems in this section:\n3. Top 10 sentences, PageRank on LSA vector cosine similarity graph 4. Top 10 sentences, weighted degree (strength) on LSA vector cosine similarity\ngraph\nThe reason we chose to use two graph-based systems at this point was to see how (modified) Parsumist competes against these popular and powerful frameworks \u2013 on the same kind of sentence vectors. If Parsumist works better than graph-based systems, then we have a strong contender for state-of-the-art extractive summarization in Persian language. On the other hand, if graph-based systems perform better, then we have yet one more strand of evidence in support of their power, pervasiveness, ubiquity, and applicability. Our results show that graph-based systems do in fact perform better than Parsumist-based systems (cf. Section 4)."}, {"heading": "3.4. Smoothing the Summary for Coherence", "text": "Coherence is a thorny issue in extractive summarizers, esp. multi-document ones. In single-document summarization, different sentences present different aspects of an underlying issue. Hence, presenting extracted sentences in temporal order largely resolves the coherence problem. However, for multi-document summarization, since two sentences may refer to the same aspect of the same issue at the same time point, it becomes difficult to maintain logical coherence \u2013 since two successive sentences may talk about the same issue from two completely opposite angles.\nNote that abstractive summarizers by default suffer less from the coherence problem, because the summary can be smoothed in different ways to give it a more polished and coherent look. Parsumist, however, is an extractive summarizer for the most part, so addressing coherence becomes a key issue. The way Parsumist handles coherence is by penalizing sentences containing anaphora and/or taboo words. The taboo word list can be specified by the user, which is an important flexibility.12 Anaphoric sentences are one of the principal contributors to the coherence problem, because they do not \u2013 by definition \u2013 refer to the sources of anaphora.\nParsumist employs a few other heuristics to manage the lack of coherence in generated\n11Please see Boudin (2013) and Lahiri et al. (2014) for evidence that non-PageRank centrality measures often perform as well as or better than PageRank, when it comes to keyphrase extraction. We suspect that similar observations will hold true in the summarization domain. 12Parsumist authors came up with their own taboo words list \u2013 which was never made public.\nsummaries. For bulleted lists, it chooses to either ignore the list, or select at least one sentence from each bullet. It also curbs redundancy in both single and multi-document modes at the sentence, morphological, and word-semantic levels \u2013 as described before. Sentences deemed too similar are collapsed into a single cluster, and only one representative from each cluster is chosen. Furthermore, Parsumist strives to retain the temporal ordering of sentences as much as possible \u2013 without violating the \u201cSelection and Redundancy Removal\u201d algorithm.\nIn our implementation, we chose to skip coherence handling and management \u2013 for two reasons: (a) details of implementation were unclear in the Parsumist paper, and (b) existing publicly available Persian tools and resources were inadequate to handle issues like anaphora and taboo words."}, {"heading": "3.5. Multi-Document Summarization by Parsumist", "text": "Existing literature on multi-document summarization prescribes two distinct ways of approaching this problem:\n1. Concatenate all documents, and then run the single-document summarizer. 2. Generate single-document summaries for all documents, concatenate the\nsummaries, and then run the single-document summarizer (esp. the redundancy eliminator) on this concatenated document to produce a more compact summary.\nParsumist authors found that there was no significant difference in performance between the two approaches. It should be noted, however, that the second approach has the obvious advantage that the redundancy eliminator is run twice, thereby (potentially) resulting in a more compact, coherent, and readable summary. The trade-off, however, betrays itself in time complexity. The second approach is more time-consuming, because the whole summarization process needs to run twice.\nIn our implementation, since we performed redundancy elimination implicitly rather than explicitly, and since we did not have an explicit coherence management module, we opted for the first approach \u2013 concatenate all documents, and then run the single-document summarizer once."}, {"heading": "3.6. Evaluation of Parsumist", "text": "Evaluation turns out to be the Achilles\u2019 heel for most summarization systems, simply because there is no agreed-upon or universal standard of summarization. Human summarizers tend to vary a lot regarding content selection and style, and most human\nsummarizers follow an abstractive approach to summarization, as opposed to the extractive approach followed by computational systems. Evaluation is further complicated by the fact that it is often unclear (esp. for long documents or multiple documents) which units of information are important enough to be included in the summary. This concerns both the granularity of linguistic units (Words? Phrases? Clauses? Sentences? Paragraphs?), as well as their salience (i.e., importance).\nExtractive summarizers deal with the granularity issue by (mostly) focusing on sentences. The salience issue is resolved by adopting a scoring function (also variously known as an objective function) \u2013 perhaps in addition to other constraints and/or heuristics \u2013 and thus imposing a ranking (i.e., an order) on the otherwise unordered set of linguistic units. Evaluation is usually performed by comparing system-generated summaries (also known as \u201cpeer summaries\u201d) with multiple human-generated summaries (also known as \u201cmodel summaries\u201d) to reduce variability. As one can deduce from this discussion, agreement among human judges tends to be fairly low for extractive summarization.\nParsumist authors created a new gold standard to evaluate their system. The gold standard consisted of documents from different domains and genres \u2013 short news articles of a few sentences to short stories comprising a few hundred sentences. No further details were given, and nor was the gold standard publicly released. The authors enlisted 20 students from Computer Engineering as human judges, and obtained at least six human (extractive) summaries for each document. Sentences were ranked for importance by the annotators; this ranking accommodates different compression ratios. Note in particular that the gold standard was single-document, so special adjustments and accommodations needed to be made for evaluating the multi-document mode of Parsumist. Parsumist was compared against \u2013 and found to perform better than \u2013 two then state-of-the-art baselines: FarsiSum (Hassel and Mazdak, 2004), and Karimi and Shamsfard\u2019s summarizer.\nSince we did not have access to the annotated gold standard data Parsumist authors generated, we instead used the Hamshahri corpus of news articles (AleAhmad et al., 2009), and evaluated our four systems (cf. Section 3.3) post hoc, i.e., after the summaries have been generated. This way, we can evaluate which systems performed the best in the opinion of human judges. Our judges were three native speakers of Persian, all male, and all in their twenties, holding baccalaureate degrees in Civil Engineering, Industrial Engineering, and Management, respectively. Our judges were knowledgeable about the subject matter of Hamshahri news articles."}, {"heading": "4. Results from Modified Parsumist and Graph-Based Systems", "text": "In this section, we describe the human evaluation results of our four systems (cf. Section\n3.3). The systems were evaluated on 50 topics of Hamshahri corpus. Each topic in Hamshahri consists of several Persian news articles. Hamshahri was built for Cross-Lingual Information Retrieval (CLIR), which is a different task than summarization. We adapted the dataset to our purpose. Adapting a CLIR dataset for summarization is a challenging task, and the trade-offs are not well-understood. We are the first to experiment with such an idea in Persian, and we will show (later in this section) that Hamshahri, in fact, is very well-suited for the task of summarization.\nWe treated the problem as multi-document summarization, and concatenated all documents within a single topic as a big topic-document. This way, we ended up with 50 topic-documents, and ran our summarization systems on them. Parsumist, as already noted, can perform multi-document summarization in this way; so can graph-based systems like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004). We posit that our fourth system based on weighted degree (strength) should be able to do the same.\nWe gave our human judges the title and description of each of the 50 topics, to facilitate understanding and acclimatization. Then we gave them four summaries for each topic (totaling 200 summaries), and instructed them to read the summaries very carefully. Once they have read the summaries of a particular topic, they were asked to assign a score between 1 and 7 as to how good the summaries were, where \u201cgoodness\u201d is measured by relevance to the topic title and description,13 and overall presentation. The scale is as follows:\n1. Very bad summary 2. Bad summary 3. Somewhat bad summary 4. Borderline summary (not too good, not too bad) 5. Somewhat good summary 6. Good summary 7. Very good summary\nThis way, each summary of each topic was given a score by each human annotator. Note that the above scheme corresponds to a Likert Scale style annotation (Likert, 1932), and is widely used in summarization research. It allows for some amount of variability in human judgment, rather than forcing the judges to make a binary or ternary judgment. This\n13Note that it is in general impossible for the human judges to read all documents of all topics, and then assign a score to a summary. As a proxy, we asked them to merely read the topic title and description.\nturns out to be an important decision, because as we will see later in this section, on average the judges converge toward the middle of the scale \u2013 a phenomenon known in the social sciences as the central tendency bias. With strict binary or ternary judgment, the presence of such bias is eliminated, which could be both good (because now we have 2-3 crisp classes of summaries) and bad (because now the bias is removed rather artificially, and the resulting 2-3 classes may contain a jumble of different types of good/bad summaries). We are of the opinion that a nuanced judgment should be the way to go \u2013 rather than a binary or ternary decision.\nThe rest of this section is organized around several research questions we asked of the data obtained from our human judges. The questions probe several topics, ranging from inter-annotator agreement and intra-annotator bias to inter-system difference and inter-topic difference."}, {"heading": "4.1. Question 1: How Much do the Judges Agree?", "text": "Inter-annotator agreement is very important for all summarization systems, not only because it gives us an idea of how hard the task is (lower agreement implies greater difficulty), but also because it gives an upper bound (roofline) on the performance of a proposed computational system.\nWe represented each human judge as a vector of 200 elements (50 topics, 4 summaries per topic), and measured agreement between those vectors. As shown in Table 1, the three judges indeed have a very high cosine similarity among themselves. This is reassuring, but not completely satisfying, because the correlation among the judges is fairly low (cf. Tables 2-4), and in fact statistically indistinguishable from zero in at least one of the cases (between Judge 2 and Judge 3).\nThis shows that the task of multi-document summarization in Persian is in fact very\ndifficult, and we should expect the performance of existing summarizers to be low."}, {"heading": "4.2. Question 2: Are the Judges Biased? If yes, by How Much?", "text": "It is instructive to look into intra-annotator bias, and see how much individual judges tilt towards a certain direction of the Likert Scale. Also, it allows us to identify relatively lenient judges and relatively harsh judges, and then adjust our evaluation accordingly.\nWe represented each judge as a vector of 200 elements (as before), and computed the means and standard deviations of those vectors. Table 5 shows the results. Note that Judge 2 is the most \u201clenient\u201d, in the sense that he gave the highest ratings on average. Judge 3 was\nthe most \u201charsh\u201d from that perspective. Compared between themselves, judges were actually quite similar in terms of their average ratings, which may point to the presence of a central tendency bias (more discussion below).\nConsidering variability, Judge 3\u2019s ratings are the most spread out, indicating the fact that he had the most varied set of opinions. On the other hand, Judge 1 has the lowest standard deviation, which implies that his opinion is much more concentrated and focused. Overall, all three annotators exhibit central tendency bias, because all three means are very close to the central point of the Likert Scale (which is 4), and far from the two extremities (1 and 7, respectively). This may indicate two things \u2013 not necessarily mutually exclusive: (a) the task was difficult, and annotators resorted to a middling scoring tactic, and (b) the annotators were unable to judge the goodness/badness of the summaries. We opine that the first of these alternatives is true, because all three annotators were background-checked and had no reason not to understand simple newswire text in their native language. So the upshot of this section is that: (a) yes, the annotators were biased, but not too much when compared between themselves; (b) there was a clear central tendency bias; and (c) the central tendency bias was due to the difficulty of the task, and not due to annotator incompetence or laziness."}, {"heading": "4.3. Question 3: Are the Judges Similar?", "text": "We already observed that owing to central tendency bias, mean ratings of all three judges were quite close to each other. We performed repeated measures ANOVA (also called paired ANOVA (equivalent to paired t-test on multiple samples)) to find out if the mean ratings are statistically significantly different.14 The results show that they are in fact not significantly different from each other (i.e., we failed to reject the null hypothesis) at 95% confidence level. That is, judges are similar to each other in terms of mean ratings. This establishes beyond doubt the presence of central tendency bias as a ubiquitous phenomenon in our Likert Scale style annotation."}, {"heading": "4.4. Question 4: Are the Four Systems Different from Each Other? How Different Are", "text": "They? We would like to know how our systems performed on Hamshahri as multi-document summarizers. Recall that we have four systems (cf. Section 3.3) \u2013 two based on Parsumist, and two graph-based. How did they perform compared to each other? To answer this question, we first needed to do annotator standardization.\n14The p-value was computed from this website: http://graphpad.com/quickcalcs/PValue1.cfm\nRecall from Section 4.2 that our annotators were biased \u2013 they had different mean overall ratings, and different standard deviations. To remove this intra-annotator bias, we computed a z-score for each annotator:\n\n  x z (5)\nwhere x is a particular rating, \u00b5 is the mean of all ratings assigned by one annotator, and \u03c3 is the standard deviation of the annotator. The z-score effectively tells us how many standard deviations away from the mean a particular rating lies.\nEach annotator was represented as a vector of 200 elements, and each element was standardized by Equation (5), thus yielding a 200-length z-score vector. Next, we represented a summarization system as a vector of 50 elements, where each element corresponds to a Hamshahri topic (Hamshahri had 50 topics), and the element value is the mean of three z-scores from three annotators. Then we performed paired ANOVA (like Section 4.3) on the four vectors (corresponding to four summarization systems). Results indicate that there is \u2013 in fact \u2013 some statistically significant difference among the mean ratings of the four systems at 95% confidence level. Mean ratings of the four systems are shown in Table 6. Note that the ratings could be positive or negative, because they are averaged across multiple z-scores, and z-scores can be positive or negative. Note also that System 3 (PageRank on sentence network) has the highest mean rating, followed by System 4 (weighted degree on sentence network), System 2 (modified Parsumist), and System 1 (modified Parsumist), respectively. This shows that graph-based systems are undoubtedly better than the systems we implemented by modifying Parsumist. It also shows that the choice of minimum cosine threshold is important \u2013 in particular, System 2 performs better than System 1, and the only difference between the two systems is in cosine threshold.\nThe next question that arises is: is any of the systems statistically significantly better\nthan any other system? To answer this question, we performed paired t-tests among all system pairs (with Bonferroni Correction for multiple comparison (adjusted \u03b1 = 0.05/6)). Results indicate that indeed, Systems 1 and 3, Systems 1 and 4, and Systems 2 and 3 are statistically significantly different from each other at 95% confidence level (by one-tailed p-value). According to magnitude of the p-value, Systems 1 and 3 are maximally different from each other (lowest p-value), followed by Systems 1 and 4, and then by Systems 2 and 3. What this implies is that Systems 3 and 4 (the graph-based systems) are significantly better than System 1 (modified Parsumist), and System 3 (graph-based) is significantly better than System 2 (Parsumist-based). Hence, graph-based systems are better on Persian extractive summarization than Parsumist-based systems."}, {"heading": "4.5. Question 5: Are Similar Systems Similar in Terms of Performance?", "text": "Now consider two \u201csystem clusters\u201d: one on graph-based systems, and another on Parsumist-based systems. Are the two graph-based systems and the two Parsumist-based systems more similar within themselves than between each other? In other words, are the clusters more tightly knit within themselves than between each other? To see this, we measured cosine similarity between all pairs of system vectors (each of length 50). Table 7 shows the results.\nAs seen from Table 7, the highest cosine similarities occur within clusters: graph-based systems (Systems 3 and 4) achieve a cosine of 91% between themselves, whereas Parsumist-based systems (Systems 1 and 2) achieve a cosine of 61% between themselves. All other pairs are between-group pairs, and they achieve much lower cosines.\nTo probe this phenomenon further, we took the vector average of Systems 1 and 2, and Systems 3 and 4, to come up with two prototype vectors \u2013 one for each of the \u201csystem clusters\u201d. These two prototype vectors achieve a cosine of 38% between themselves, which\nis indeed much lower than within-cluster cosines.\nWhat this discussion shows is that similar systems are similar in terms of\nhuman-assigned ratings, and dissimilar systems are dissimilar."}, {"heading": "4.6. Question 6: How Are the 50 Topics Different in Terms of Summarization?", "text": "Now that we have analyzed and dissected the annotators and the four systems, the next question to ask is: how are the 50 Hamshahri topics different in terms of summarization? Are they all equally easy/hard to summarize? Or is there some difference? Note that the answer to these questions has important implications in designing future Persian summarizers. For example, if we can show that some topics are inherently more difficult to summarize than some other topics, then we can follow different summarization strategies for those two classes of topics. Perhaps we can come up with class-based strategies to orient summarization systems towards different topical difficulty levels.\nTo probe these issues, we first define the notion of summarizability. The \u201csummarizability\u201d of a topic is defined as the mean of four ratings (from four systems) for that topic \u2013 each rating being an average across three annotators. This way, we obtain a single number for each of the 50 topics.\nNext, we sorted the summarizability values in descending order, and plotted them against topic ID (Figure 2). This plot shows a clear trend: a few topics are highly\nsummarizable, a few are highly un-summarizable, and most fall in between. Hence, there are three classes of topics:\n1. Highly summarizable topics (easiest to summarize) 2. Agnostic topics (in-between difficulty in terms of summarization) 3. Highly un-summarizable topics (hardest to summarize)\nFor Hamshahri, most of the topics turn out to be agnostic \u2013 which is a fairly broad range.\nInterestingly, this shows that Hamshahri \u2013 originally conceived as a dataset for Cross-Lingual Information Retrieval (CLIR) \u2013 is also a very good corpus for summarization research, with a mix of topics that touch different summarizability levels, and most of the topics either highly summarizable or agnostic. We show the topics along with their English description and summarizability values in Table 8 (sorted in descending order of summarizability).\nTopic English Description Summarizability\n607-AH Commemorations of Sadi Shirazi 1.05 642-AH Shahr Theater Programs 0.72 608-AH House Prices 0.71 613-AH Children\u2019s Rights 0.61 643-AH Earthquake Damage in Iran 0.60 628-AH NATO vs. Yugoslavia in 1998 0.55 650-AH Fluctuations in Gas Imports 0.55 629-AH Global Drought Predictions 0.53 611-AH Information Technology and Employment 0.49 612-AH Internet Users 0.48 635-AH Iran in 1998 World Cup 0.48 616-AH Hand-woven Carpet Exports 0.45 619-AH Iranian Non-oil Exports 0.43 622-AH Tehran Car Accidents 0.40 644-AH Electronic Commerce 0.35 617-AH Tourist Attractions 0.32 609-AH Fruit Packing 0.31 634-AH University Acceptance Limits 0.30 623-AH North Iran Forestry Conservation 0.22\nNote that about half of the topics have positive summarizability values, and the other half have negative. This is intuitive, because as we mentioned before, summarizability is\ncoming from averaging several z-scores, and taken together, z-scores cancel each other out. Hence, we may expect to see (almost) equal number of positive and negative summarizability values.\nAlso of interest is the fact that the topics, when organized in this order, clearly show a trend: more concrete (and focused) topics tend to be highly summarizable, whereas more abstract (and diverse) topics \u2013 also, topics fraught with differing opinions, political controversy, etc \u2013 tend to be highly un-summarizable. Furthermore, local and national topics tend to be more summarizable than technical and international topics. A curious upshot of the above analysis is that most topics are from local and national news, and very few come from international. Even fewer relate to the United States, and contemporary friction arising from September 11 attacks. This shows a social distinctiveness of news topics. There is an inner core of densely connected and most popular regional topics, then there is an intermediate layer of loosely connected and somewhat less popular national topics, and finally there is the outermost layer (periphery) of sparsely connected and least popular international topics."}, {"heading": "4.7. Question 7: Is There a Relationship Between Summarizability and Number of", "text": "Documents? We can reason that the less number of documents a topic has, the more focused (and marginal) it should be, and hence easier to summarize. In other words, there should be an inverse correlation between number of documents and summarizability. However, the following plot (Figure 3) \u2013 where X-axis is Hamshahri topics (sorted in descending order of summarizability), and Y-axis is the number of documents for each topic \u2013 shows that this is clearly not the case.\nAs seen from Figure 3, there is almost no correlation between the two variables. The Pearson correlation between summarizability and document count was found to be 0.09, which was not only too low, but also statistically indistinguishable from zero at 95% confidence level.\nWhat this shows is that the issue of summarizability is complex, and needs to be investigated further. Perhaps there are other variables at play, e.g., document length, that have a greater bearing on how summarizability works, and hence can better explain the underlying process. Besides, it will be interesting to explore classification of Persian documents according to summarizability, because then we can predict \u2013 even before the actual summarization begins \u2013 whether the resulting summary will be any good. We leave this line of research to future work."}, {"heading": "5. Centrality-Based Summarization", "text": "We already observed that centrality-based systems outperformed Parsumist-based systems in a post hoc analysis of multi-document summarization on the Hamshahri corpus. The next question we asked, is: if we were given an annotated corpus of summarized Persian documents, which centrality measures would perform the best in single and multi-document summarization? To answer this question, we resorted to the recently released Pasokh corpus of annotated Persian summaries (Behmadi Moghaddas et al., 2013). Note that we could have done this analysis post hoc, but that would have required our human annotators to go\nthrough hundreds of system-generated summaries manually \u2013 a clearly untenable approach. Instead, we used the popular ROUGE package (Lin, 2004) \u2013 in particular, its recent Java implementation called JROUGE (Ganesan et al., 2010) \u2013 to evaluate our centrality-based systems.\nThe rest of this section is organized as follows. In Section 5.1, we give an overview of centrality-based approaches, and discuss \u2013 using a cognitive science argument \u2013 why it makes sense to use them and why they perform so well in practice. Section 5.2 describes the systems we designed, and Section 5.3 details the evaluation strategy, including a description of Pasokh and ROUGE."}, {"heading": "5.1. Centrality-Based Methods", "text": "It is well-known in distributional semantics that a word is known by the company it keeps (Firth, 1957). Words can have both local (and long-range) syntactic dependencies, and mostly global semantic dependencies. Hence, a word network (also variously known as a collocation network or a collocation graph; cf. (Lahiri, 2014)), where nodes are words and edges are word co-occurrence relationships, can succinctly and explicitly capture such dependencies. It has been observed (Mihalcea and Tarau, 2004) that in such networks, words with the highest centrality also happen to be the most important words (keywords) in the document. It has been similarly observed that if we construct the network on sentences, where nodes are sentences and edges are weighted (perhaps pruned, too) by sentence similarity, then the most important sentences in the document turn out to have highest centrality in the resulting network (Mihalcea and Tarau, 2004; Erkan and Radev, 2004).\nThese two observations started the field of graph-based summarization and keyword extraction (Mihalcea and Radev, 2011). Apart from the curious and interesting underpinnings as mentioned above, graph-based methods have the additional advantage that they are unsupervised, do not require any Natural Language Processing tools such as part-of-speech tagger, parser, or semantic role labeler, and still deliver acceptable (and in many cases, state-of-the-art) performance. The beauty and elegance of these approaches led to a flurry of research activities (cf. (Boudin, 2013; Lahiri et al., 2014)), and graph-based approaches are still being pursued by several research groups around the world.\nIt is interesting to note that the \u201cword neighborhood\u201d analogy holds for sentences too. It may initially come as a surprise how (or why) graph-based methods perform so well in practice, esp. when they take almost no help from existing Natural Language Processing tools. This is an important question, and the jury is still out. However, as Mihalcea and Tarau (2004) pointed out using a cognitive science argument, any network \u2013 be it words or sentences or people (social networks) \u2013 encodes a system of endorsements and\nrecommendations, where each edge denotes a form of endorsement. In essence, therefore, these networks are representing a reputation model where less important (thus, less reputed) words connect to more important (hence, more reputed) words, and less reputed sentences connect to more reputed sentences. If this model is taken as the prima facie proof of the success of graph-based approaches, then we can clearly see (and evidence suggests) that words and sentences with the most connections (i.e., highest centrality) are \u2013 in fact \u2013 the most important (reputed) words and sentences. Intriguingly enough, for summarization purposes, most important sentences are exactly what we need. Hence comes the general three-step approach to graph-based (or centrality-based) summarization:\n1. Construct a network of sentences. 2. Rank the sentences by their centrality in the network. 3. Take top k of those sentences, and present them in temporal order.\nNote that the above framework is very general, and can accommodate several types of networks and centrality measures. We under-specified the framework on purpose, so that we can play with different parameters. In practice, the concept of centrality has been studied in the social sciences for several decades (Borgatti, 2005). As Boldi and Vigna (2014) point out, there exist at least six different categories (families) of centrality indices that measure different aspects of a node in a network, as follows:\n1. The node with largest degree 2. The node that is closest to other nodes 3. The node through which most shortest paths pass 4. The node with the largest number of incoming paths of length k, for every k 5. The node that maximizes the dominant eigenvector of the graph matrix 6. The node with the highest probability in the stationary distribution of the natural\nrandom walk on the graph\nHowever, as Borgatti and Everett (2006) observed, most (if not all) of these indices assess a node\u2019s involvement in the walk structure of a network, and summarize a node\u2019s contribution to the cohesiveness of the network. Note that if language is represented as a network (of words or sentences), then the most important words/sentences should be the ones that contribute most highly to the cohesiveness of the document. So even from Borgatti and Everett\u2019s perspective, centrality-based approaches to summarization do make sense, and can be predicted to work well in practice."}, {"heading": "5.2. Centrality-Based Systems", "text": "Following the general framework for graph-based summarization that we described in the previous section, it can be observed that there are essentially two free parameters: network type, and centrality measure. Network type can have important implications in a summarization system. Parsumist, for example, constructed a complete graph of sentences \u2013 then pruned the nodes that were deemed too similar to their neighbors. LexRank, on the other hand, went ahead with a complete graph representation and no pruning whatsoever. We, too, followed the same approach in Sections 3 and 4. Sentence similarity can also be an important aspect of network construction, because they allow us to weight the edges differently. While LexRank employs simple cosine similarity, TextRank used a modified form of Dice Similarity.\nIn this part of the study, we constructed sentence networks for both single and multi-document summarization. Sentences were extracted from the Pasokh corpus using the normalizer and tokenizer developed by Mojgan Seraji (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015), in the same spirit as Section 3.1. Our networks are undirected and weighted complete graphs, where edge weights are given by cosine similarity between two sentences. Note that the network structure and choice of similarity measure will have an impact on the final result, but since our goal in this part of the study was to compare centrality measures, and not to tweak the network structure, we went ahead with the standard LexRank construction.\nWe experimented with three different vector representations of sentences \u2013 tf, tfidf, and binary (presence/absence of words). Vector elements are all unique words in the Pasokh corpus, and idf was computed on the Hamshahri corpus. Furthermore, we constructed sentence vectors in two ways \u2013 removing all stop words, and keeping them.15 This gives rise to six types of sentence networks.\nOnce the sentence networks have been constructed, the next step is to select the most central sentences in the network. Note that the use of centrality measures in document summarization is not new, but it is new in Persian. We used the following seven centrality measures:16\n1. Strength: sum of the weights of the edges incident to a node (also called\n\u201cweighted degree\u201d).\n15Stop words came from the Hamshahri collection, available at http://ece.ut.ac.ir/dbrg/hamshahri/files/HAM1/persian.stop.txt. 16Centrality was computed using the igraph package (Csardi and Nepusz, 2006).\n2. Clustering Coefficient: density of edges among the immediate neighbors of a\nnode (Watts and Strogatz, 1998).\n3. Structural Diversity Index: normalized entropy of the weights of the edges\nincident to a node (Eagle et al., 2010).\n4. PageRank: importance of a node based on how many important nodes it is\nconnected to (Page et al., 1998).\n5. Betweenness: fraction of shortest paths that pass through a node, summed over all\nnode pairs (Anthonisse, 1971; Brandes, 2001).\n6. Closeness: reciprocal of the sum of distances of all nodes to a node (Bavelas,\n1950).\n7. Eigenvector Centrality: element of the first eigenvector of a graph adjacency\nmatrix corresponding to a node (Bonacich, 1987).\nThe above seven centrality measures touch all six different centrality families identified by Boldi and Vigna (2014), hence they may be deemed comprehensive for the purposes of our study. Note further that all the above measures except Structural Diversity Index above are true centrality indices, hence sentences need to be sorted in their descending order. For the Structural Diversity Index, however, we want sentences to have minimum possible values, so they need to be sorted in the ascending order. Lastly, we used strength instead of degree, because our networks are complete graphs, hence all nodes have the same degree.\nThe seven centrality measures we used gave us 42 graph-based systems in the end (six types of graphs, seven centrality measures on each of them). In the evaluation, we will see that most systems behave rather similarly, and differences emerge from centrality measures rather than narrow, document-specific optimizations. In particular, strength, PageRank, eigenvector centrality and Structural Diversity Index perform the best in terms of ROUGE score."}, {"heading": "5.3. Evaluation of Centrality-Based Systems", "text": "As mentioned in Section 3.6, evaluation is the Achilles\u2019 heel for most summarization systems, which is exacerbated by the fact that oftentimes, no human-annotated gold standard dataset is available on which competing systems can be evaluated. We were rather fortunate in this regard, because recently such a gold standard corpus \u2013 Pasokh \u2013 has been released by a team of researchers that specifically looks into Persian single and multi-document summarization (Behmadi Moghaddas et al., 2013).\nPasokh is a corpus of Persian news articles annotated with human-generated summaries \u2013 both extractive and abstractive \u2013 for single-document as well as multi-document\nsummarization. It took over 2000 man-hours of work to construct Pasokh. The meaning of the word \u201cPasokh\u201d is \u201canswer\u201d; it is a contraction of the original Persian: Peykare-ye (A)est\u00e2nd\u00e2rd-e S\u00e2m\u00e2neh\u00e2-ye (O is added for ease of pronunciation) KHol\u00e2se-s\u00e2z. The news content of Pasokh comes from seven popular Persian news agencies (including Hamshahri), and six genres of documents (Economic, Cultural, Social, Political, Sports, and Scientific). Pasokh has 100 documents in the single-document summarization section. Each document has five extractive and five abstractive summaries that come from five different human annotators. In total, this gives us 500 extractive and 500 abstractive summaries to work with.\nThe multi-document section of Pasokh comprises 50 news topics, 20 documents per topic. The same seven news agencies were used. Five extractive and five abstractive summaries were created for each topic \u2013 by five different human annotators \u2013 at a 30% compression ratio. So in this section, we obtain 250 extractive and 250 abstractive summaries.\nTen male and ten female undergraduate students constructed the corpus. They were trained to represent the central content of each document/topic in the summary, while at the same time avoiding repetition and redundancy; according with the key points of the original text; maintaining coherence and readability, particularly in the abstractive summaries; maintaining cohesion; and not exceeding the set compression ratio (30%). The large size and complexity of the corpus forced its creators to design a separate software interface called Khol\u00e2se-y\u00e2r (roughly translates as \u201csummarization aid\u201d) to help the human annotators. The annotators had to supply reasons for why they chose a particular piece of text to be included in the summary. Thus, Pasokh turned out to be a very exhaustive, complete, and holistic corpus, and a great exercise in human annotation in Persian language. For single documents, Pasokh summaries are mostly three to seven sentences long.\nWe used the extractive portion of Pasokh, and left the abstractive part for future work. Note that the extractive summaries in Pasokh do not always obey strict extraction rules. For example, many of them extract phrases and clauses rather than full sentences, and then \u201cglue\u201d together components in an abstractive fashion. Hence, the extractive summaries are not purely extractive \u2013 they oftentimes contain abstractive components. This observation makes standard evaluation using precision, recall, and F-score (at sentence level) difficult on the Pasokh extractive set. We used the ROUGE score (Lin, 2004) instead.\nThe word \u201cROUGE\u201d stands for Recall-Oriented Understudy for Gisting Evaluation. ROUGE essentially measures overlap between system-generated summaries (also called \u201cpeer summaries\u201d) with human-generated summaries (also called \u201cmodel summaries\u201d). Note that recall is more important in the summarization setting than precision, because we\nwould like to have as many important units of information as possible in the final summary, rather than only the top few most salient ones. There are four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. ROUGE-N is based on n-gram co-occurrence statistics, ROUGE-L is based on longest common subsequences (LCS) at the summary level, ROUGE-W represents weighted longest common subsequences, and ROUGE-S involves skip-bigram co-occurrence statistics.\nAs one can notice, the ROUGE scores become progressively more complex from ROUGE-N to ROUGE-S. In practice, most researchers use ROUGE-N, because it is simple to understand, and gives good results. We, too, used ROUGE-N. The formula for ROUGE-N is as follows:\n \n \n \n  \n}{\n}{\n)(\n)(\nriesModelSummaS Sgram\nn\nriesModelSummaS Sgram\nnmatch\nN\nn\nn\ngramCount\ngramCount\nROUGE (6)\nwhere n is the length of the n-gram, S is a particular human-generated summary, gramn is a particular n-gram, and Countmatch(gramn) is the maximum number of n-grams co-occurring in a system-generated summary and a set of human-generated summaries.\nNote that ROUGE talks about a set of human-generated summaries \u2013 not a single human-generated summary. This is typical in summarization research. Owing to the high variability between human annotators, we would like to generate as many model summaries as we can, and then evaluate the peer summaries against all of them. This is a time-tested way to combat variability in Natural Language Processing tasks such as keyword extraction and summarization. Note further that the standard ROUGE-N formula above can be adapted to make way for a precision-oriented measure, which in turn gives rise to an F-score-style measure. In our evaluation, we used all three (precision, recall, and F-score). F-score combines precision and recall in a theoretically sound way, but as we will see in Section 6, precision and recall showed their own interesting trends.\nInterestingly, it would not have been possible to use ROUGE in Sections 3 and 4 (i.e., on the Hamshahri corpus), because there we did not have human-annotated gold-standard summaries (model summaries). A particular problem with ROUGE is that its vanilla implementation in Perl does not handle Unicode characters. Hence, we resorted to the recently released JROUGE package written in Java (Ganesan et al., 2010). We used three baselines to compare our centrality-based systems against:\n1. Random k sentences extracted from the document. 2. First k sentences extracted from the document. 3. Last k sentences extracted from the document.\nWhile random baseline is a common staple in almost all Natural Language Processing evaluation tasks, the first and last k sentences baselines are special in summarization. They derive their importance from the fact that in a document (esp. news articles like Pasokh and Hamshahri), first k sentences often contain the most important and/or the most central pieces of information. Hence, a summary consisting of first k sentences often constitutes a very strong and very effective baseline in practice. On the other hand, the last k sentences in a document often include concluding remarks and thoughts on future directions or courses of events. Hence, those sentences could turn out to be important in terms of summarization. We therefore opted for all three baselines in this study. We used the 42 centrality-based systems, and the above three baseline systems at compression ratios of 25%, 50%, 75%, and 100%, respectively."}, {"heading": "6. Results on the Pasokh Corpus", "text": "As mentioned in Section 5.3, we used ROUGE-N for evaluation. We experimented with three values of N: 1, 2, and 3. These correspond to ROUGE on unigrams, ROUGE on bigrams, and ROUGE on trigrams, respectively. They are standard metrics in summarization literature. What we observed is that qualitatively, ROUGE-1, ROUGE-2 and ROUGE-3 are similar to each other. Hence, we report ROUGE-3 only. Quantitatively, ROUGE-1 is higher than ROUGE-2, which in turn is higher than ROUGE-3. This is expected, because between any two pieces of text, the number of common unigrams should be at least as many as the number of common bigrams, and the number of common bigrams should be at least as many as the number of common trigrams. This makes ROUGE-3 the most pessimistic (conservative) of all three ROUGE-N scores we tried.\nFurthermore, note that we used three vectorial representations: binary, tf, and tfidf. We observed that they had very little difference among themselves \u2013 qualitatively and quantitatively. Hence, we only report tfidf. In the following, we encode our centrality-based systems and baseline systems using a three-character code, as follows:\n Bet: Betweenness centrality  Clo: Closeness centrality  Clu: Clustering Coefficient  Div: Structural Diversity Index\n Eig: Eigenvector Centrality  Pag: PageRank  Str: Strength  Fir: First k sentences baseline  Las: Last k sentences baseline  Ran: Random k sentences baseline\nLastly, in the following stacked bar diagrams, the lowest level corresponds to 25% compression ratio, second lowest level 50% compression ratio, third lowest (i.e., second highest) level 75% compression ratio, and the highest level 100% compression ratio. They are denoted by four different colors for ease of visualization.\nAs seen from Figures 4 and 5, with stop words and without stop words systems perform quite similarly in terms of F-score across all compression ratios. Overall, best F-scores are realized by strength, PageRank, eigenvector centrality, and Structural Diversity Index. It is intriguing to see that PageRank is not the only best-performing measure. Presence of Structural Diversity Index among the best performers is another intriguing finding, and should be researched in more depth. Interestingly, all three baselines perform quite well, esp. compared to betweenness, closeness, and clustering coefficient. Compression-ratio-wise, 50% and up seems good enough for single-document summarization. However, we should keep in mind that for long documents, 50% may be too high. For multi-document summarization, best results come at 25% compression ratio. This is intuitive, because for multiple documents, 25% is already large enough to contain most (if not all) units of importance.\nWe report ROUGE-3 recall in Figures 6 and 7. Note from the discussion in Section 5.3 that recall is in fact the most important performance measure in summarization. As seen from Figures 6 and 7, recall increases with increasing compression ratio, and the best values are obtained at the maximum compression ratio of 100% (i.e., no compression). This makes intuitive sense, because as more and more units of information are included in the summary, it becomes more and more probable to find a particular relevant piece of information in them. Hence, recall goes up.\nThe opposite happens with ROUGE-3 precision (Figures 8 and 9). As compression ratio goes up, precision goes down, because we are including many more irrelevant pieces of information in the summary than relevant ones. This dichotomy of precision and recall is a\nstandard phenomenon in Information Retrieval (IR), but it was interesting to observe the same dichotomy hold true in the domain of summarization and ROUGE \u2013 especially in Persian."}, {"heading": "7. Conclusion and Future Work", "text": "We have described three classes of novel unsupervised systems for single and multi-document summarization in Persian \u2013 one based on Parsumist, a popular Persian summarizer, and two based on sentence networks and centrality measures. Experimental results indicate that graph-based methods perform better than Parsumist-based methods, and there are some centrality measures (strength, PageRank, eigenvector centrality, and Structural Diversity Index) that perform better than others. Detailed human evaluation (post hoc), as well as evaluation using ROUGE score have been reported.\nThere are several limitations of our study, all of which can be improved upon in future\nwork:\n Our study does not consider the problem of supervised summarization, where a\nmachine learning system is trained to identify sentences and/or other linguistic units for inclusion in the summary. Existing literature suggests that supervised systems perform better than unsupervised ones.  We have not been able to compare our systems with state-of-the-art Persian\nsummarizers, because their implementation is not publicly available. It will be an interesting and revealing exercise to follow up on this lead.  Parameter tuning and optimization: we performed minimal parameter tuning.\nThere are many opportunities to do so, esp. in Parsumist, and/or graph-based systems. Future work should look into parameter tuning with more rigor and depth.  We did not attempt abstractive summarization in this work, because it involves\nnon-trivial amount of syntactic judgment calls. Since we did not have a Persian linguist in our team, such judgment calls were deemed arbitrary and hard to justify. Future work should look into this very interesting and exciting line of research.  We did not use syntactic and semantic features. Dependency treebanks have been\ndeveloped and are currently available for Persian (Rasooli et al., 2013; Seraji, 2015). It will be a very promising and interesting research direction to integrate such resources with extractive and/or abstractive summarization in Persian.\nOverall, we believe that we have presented a convincing study in Persian document\nsummarization with existing resources, and hope that our work will spark interest in the still nascent but growing area of research in document summarization in Persian."}, {"heading": "8. References", "text": "AleAhmad, A., Amiri, H., Darrudi, E., Rahgozar, M., and Oroumchian, F., 2009,\nHamshahri: A standard Persian text collection. Knowledge-Based Systems, 22(5):382\u2013387, July. Anthonisse, J. M., 1971, The rush in a directed graph. Technical report, Stichting\nMathematisch Centrum, Amsterdam, October.\nBavelas, A., 1950, Communication Patterns in Task-Oriented Groups. The Journal of the\nAcoustical Society of America, 22(6):725\u2013730.\nBazghandi, M., Tabrizi, G. T., and Jahan, M. V., 2012, Extractive Summarization Of Farsi\nDocuments Based On PSO Clustering. International Journal of Computer Science Issues (IJCSI), 9(3):329\u2013332. Behmadi Moghaddas, B., Kahani, M., Toosi, S. A., Pourmasoumi, A., and Estiri, A., 2013,\nPasokh: A Standard Corpus for the Evaluation of Persian Text Summarizers, In Proceedings of the 3rd International eConference on Computer and Knowledge Engineering (ICCKE), pages 471\u2013475, October. Berenjkoob, M., Mehri, R., Khosravi, H., and Nematbakhsh, M. A., 2009, A Method for\nStemming and Eliminating Common Words for Persian Text Summarization, In Proceedings of the International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE), pages 1\u20136, September. Boldi, P. and Vigna, S., 2014. Axioms for Centrality. Internet Mathematics, 10(3-4), pages\n222\u2013262.\nBonacich, P., 1987, Power and Centrality: A Family of Measures. American Journal of\nSociology, 92(5):1170\u20131182.\nBorgatti, S. P., 2005, Centrality and network flow. Social Networks, 27(1):55\u201371. Borgatti, S. P., and Everett, M. G., 2006, A Graph-theoretic perspective on centrality.\nSocial Networks, 28(4):466\u2013484.\nBoudin, F., 2013, A Comparison of Centrality Measures for Graph-Based Keyphrase\nExtraction, In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 834\u2013838, Nagoya, Japan, October. Asian Federation of Natural Language Processing. Brandes, U., 2001. A Faster Algorithm for Betweenness Centrality. The Journal of\nMathematical Sociology, 25(2):163\u2013177.\nBruni, E., Tran, N. K., and Baroni, M., 2014, Multimodal Distributional Semantics. Journal\nof Artificial Intelligence Research, 49(1):1\u201347, January.\nChung, C. K., and Pennebaker, J. W., 2008, Revealing Dimensions of Thinking in\nOpen-Ended Self-Descriptions: An Automated Meaning Extraction Method for Natural Language. Journal of Research in Personality, 42(1):96\u2013132, February. Csardi, G., and Nepusz, T., 2006, The igraph software package for complex network\nresearch. Inter-Journal, Complex Systems:1695.\nDalianis, H., 2000, SweSum \u2013 a text summarizer for Swedish. Technical Report\nTRITA-NA-P0015, Department of Numerical Analysis and Computing Science, Royal Institute of Technology (KTH), Stockholm, Sweden, August. Eagle, N., Macy, M., and Claxton, R., 2010, Network Diversity and Economic\nDevelopment. Science, 328(5981):1029\u20131031, May.\nErkan, G., and Radev, D. R., 2004, LexRank: Graph-based Lexical Centrality as Salience in\nText Summarization. Journal of Artificial Intelligence Research, 22(1):457\u2013479, December.\nFirth, J. R., 1957, Papers in linguistics, 1934-1951. Oxford University Press. Ganesan, K., Zhai, C., and Han, J., 2010, Opinosis: A Graph-Based Approach to\nAbstractive Summarization of Highly Redundant Opinions, In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 340\u2013348, Beijing, China, August.\nHal\u00e1csy, P., Kornai, A., and Oravecz, C., 2007, HunPos \u2013 an open source trigram tagger, In\nProceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 209\u2013212, Stroudsburg, PA, USA. Association for Computational Linguistics. Hassel, M., and Mazdak, N., 2004, FarsiSum \u2013 A Persian text summarizer, In Proceedings\nof the Workshop on Computational Approaches to Arabic Script-based Languages (Semitic 2004), pages 82\u201384, Stroudsburg, PA, USA. Association for Computational Linguistics. Kamyar, H., Kahani, M., Kamyar, M., and Poormasoomi, A., 2011, An Automatic\nLinguistics Approach for Persian Document Summarization, In Proceedings of the International Conference on Asian Language Processing (IALP), pages 141\u2013144, November. Kiyoumarsi, F., and Esfahani, F. R., 2011, Optimizing Persian Text Summarization Based\non Fuzzy Logic Approach, In Proceedings of the International Conference on Intelligent Building and Management, volume 5.\nLahiri, S., 2014, Complexity of Word Collocation Networks: A Preliminary Structural\nAnalysis, In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 96\u2013105, April. Lahiri, S., Ray Choudhury, S., and Caragea, C., 2014, Keyword and Keyphrase Extraction\nUsing Centrality Measures on Collocation Networks, ArXiv e-print, January.\nLandauer, T. K., and Dumais, S. T., 2008, Latent semantic analysis. Scholarpedia,\n3(11):4356.\nLikert, R., 1932, A Technique for the Measurement of Attitudes. Archives of Psychology,\n22(140):1\u201355.\nLin, C.-Y., 2004, ROUGE: A Package for Automatic Evaluation of Summaries, In Text\nSummarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\u201381, Barcelona, Spain, July. Association for Computational Linguistics. Mihalcea, R., and Radev, D., 2011, Graph-based Natural Language Processing and\nInformation Retrieval. Cambridge University Press.\nMihalcea, R., and Tarau, P., 2004, TextRank: Bringing Order into Texts, In Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing (EMNLP), July.\nMiller, G. A., 1995, WordNet: A Lexical Database for English. Communications of the\nACM, 38(11):39\u201341, November.\nNenkova, A., Passonneau, R., and McKeown, K., 2007, The Pyramid Method:\nIncorporating Human Content Selection Variation in Summarization Evaluation. ACM Transactions on Speech and Language Processing, 4(2), May. Nia, M. J., 2013, Using the Pyramid Method for generating gold standards for Persian texts. Page, L., Brin, S., Motwani, R., and Winograd, T., 1998, The PageRank Citation Ranking:\nBringing Order to the Web, In Proceedings of the 7th International World Wide Web Conference, pages 161\u2013172, Brisbane, Australia. Poormasoomi, A., Kahani, M., Yazdi, S. V., and Kamyar, H., 2011, Context-Based Persian\nMulti-Document Summarization (global view), In Proceedings of the International Conference on Asian Language Processing (IALP), pages 145\u2013149, November. Rasooli, M. S., Kouhestani, M., and Moloodi, A., 2013, Development of a Persian\nSyntactic Dependency Treebank, In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 306\u2013314, Atlanta, Georgia, June. Association for Computational Linguistics. Seraji M., 2013, PrePer: A Pre-processor for Persian, In Proceedings of the Fifth\nInternational Conference on Iranian Linguistics (ICIL5), Bamberg, Germany.\nSeraji, M., 2015, Morphosyntactic Corpora and Tools for Persian, Ph.D. Thesis, Uppsala\nUniversity.\nShakeri, H., Gholamrezazadeh, S., Amini Salehi, M., and Ghadamyari, F., 2012, A New\nGraph-based Algorithm for Persian Text Summarization. Computer Science and Convergence, vol. 114 of Lecture Notes in Electrical Engineering, pages 21\u201330. Springer. Shamsfard, M., Akhavan, T., Erfani Joorabchi, M., 2009a, Persian Document\nSummarization by Parsumist. World Applied Sciences Journal, 7:199\u2013205.\nShamsfard, M., Akhavan, T., Erfani Jourabchi, M., 2009b, Parsumist: A Persian Text\nSummarizer, In Proceedings of the International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE), pages 1\u20137, September. Tofighy, M., Kashefi, O., Zamanifar, A., and Seyyed Javadi, H. H., 2011, Persian Text\nSummarization Using Fractal Theory. Informatics Engineering and Information Science, vol. 252 of Communications in Computer and Information Science, pages 651\u2013662. Springer. Tofighy, S. M., Raj, R. G., and Seyyed Javadi, H. H., 2013, AHP Techniques for Persian\nText Summarization. Malaysian Journal of Computer Science, 26(1):1\u20138.\nWatts, D. J., and Strogatz, S. H., 1998, Collective dynamics of \u2018small-world\u2019 networks.\nNature, 393(6684):409\u2013410.\nZamanifar, A., and Kashefi, O., 2011, AZOM: A Persian Structured Text Summarizer.\nNatural Language Processing and Information Systems, vol. 6716 of Lecture Notes in Computer Science, pages 234\u2013237. Springer. Zamanifar, A., Minaei-Bidgoli, B., and Sharifi, M., 2008, A New Hybrid Farsi Text\nSummarization Technique Based on Term Co-Occurrence and Conceptual Property of the Text. In Proceedings of the Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2008), pages 635\u2013639, August."}], "references": [{"title": "Hamshahri: A standard Persian text collection", "author": ["A. AleAhmad", "H. Amiri", "E. Darrudi", "M. Rahgozar", "F. Oroumchian"], "venue": "Knowledge-Based Systems,", "citeRegEx": "AleAhmad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "AleAhmad et al\\.", "year": 2009}, {"title": "The rush in a directed graph", "author": ["J.M. Anthonisse"], "venue": "Technical report, Stichting Mathematisch Centrum,", "citeRegEx": "Anthonisse,? \\Q1971\\E", "shortCiteRegEx": "Anthonisse", "year": 1971}, {"title": "Communication Patterns in Task-Oriented Groups", "author": ["A. Bavelas"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Bavelas,? \\Q1950\\E", "shortCiteRegEx": "Bavelas", "year": 1950}, {"title": "Extractive Summarization Of Farsi Documents Based On PSO Clustering", "author": ["M. Bazghandi", "G.T. Tabrizi", "M.V. Jahan"], "venue": "International Journal of Computer Science Issues (IJCSI),", "citeRegEx": "Bazghandi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bazghandi et al\\.", "year": 2012}, {"title": "Pasokh: A Standard Corpus for the Evaluation of Persian Text Summarizers", "author": ["B. Behmadi Moghaddas", "M. Kahani", "S.A. Toosi", "A. Pourmasoumi", "A. Estiri"], "venue": "In Proceedings of the 3 International eConference on Computer and Knowledge Engineering (ICCKE),", "citeRegEx": "Moghaddas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moghaddas et al\\.", "year": 2013}, {"title": "A Method for Stemming and Eliminating Common Words for Persian Text Summarization", "author": ["M. Berenjkoob", "R. Mehri", "H. Khosravi", "M.A. Nematbakhsh"], "venue": "In Proceedings of the International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE),", "citeRegEx": "Berenjkoob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Berenjkoob et al\\.", "year": 2009}, {"title": "Axioms for Centrality", "author": ["P. Boldi", "S. Vigna"], "venue": "Internet Mathematics,", "citeRegEx": "Boldi and Vigna,? \\Q2014\\E", "shortCiteRegEx": "Boldi and Vigna", "year": 2014}, {"title": "Power and Centrality: A Family of Measures", "author": ["P. Bonacich"], "venue": "American Journal of Sociology,", "citeRegEx": "Bonacich,? \\Q1987\\E", "shortCiteRegEx": "Bonacich", "year": 1987}, {"title": "Centrality and network flow", "author": ["S.P. Borgatti"], "venue": "Social Networks,", "citeRegEx": "Borgatti,? \\Q2005\\E", "shortCiteRegEx": "Borgatti", "year": 2005}, {"title": "A Graph-theoretic perspective on centrality", "author": ["S.P. Borgatti", "M.G. Everett"], "venue": "Social Networks,", "citeRegEx": "Borgatti and Everett,? \\Q2006\\E", "shortCiteRegEx": "Borgatti and Everett", "year": 2006}, {"title": "A Comparison of Centrality Measures for Graph-Based Keyphrase Extraction", "author": ["F. Boudin"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Boudin,? \\Q2013\\E", "shortCiteRegEx": "Boudin", "year": 2013}, {"title": "A Faster Algorithm for Betweenness Centrality", "author": ["U. Brandes"], "venue": null, "citeRegEx": "Brandes,? \\Q2001\\E", "shortCiteRegEx": "Brandes", "year": 2001}, {"title": "Multimodal Distributional Semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Revealing Dimensions of Thinking in Open-Ended Self-Descriptions: An Automated Meaning Extraction Method for Natural Language", "author": ["C.K. Chung", "J.W. Pennebaker"], "venue": "Journal of Research in Personality,", "citeRegEx": "Chung and Pennebaker,? \\Q2008\\E", "shortCiteRegEx": "Chung and Pennebaker", "year": 2008}, {"title": "The igraph software package for complex network research. Inter-Journal, Complex Systems:1695", "author": ["G. Csardi", "T. Nepusz"], "venue": null, "citeRegEx": "Csardi and Nepusz,? \\Q2006\\E", "shortCiteRegEx": "Csardi and Nepusz", "year": 2006}, {"title": "SweSum \u2013 a text summarizer for Swedish. Technical Report TRITA-NA-P0015, Department of Numerical Analysis and Computing Science, Royal Institute of Technology (KTH), Stockholm, Sweden, August", "author": ["H. Dalianis"], "venue": null, "citeRegEx": "Dalianis,? \\Q2000\\E", "shortCiteRegEx": "Dalianis", "year": 2000}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "Papers in linguistics, 1934-1951", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth,? \\Q1957\\E", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "Opinosis: A Graph-Based Approach to Abstractive Summarization of Highly Redundant Opinions", "author": ["K. Ganesan", "C. Zhai", "J. Han"], "venue": "In Proceedings of the 23 International Conference on Computational Linguistics (COLING", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "HunPos \u2013 an open source trigram tagger", "author": ["P. Hal\u00e1csy", "A. Kornai", "C. Oravecz"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "Hal\u00e1csy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hal\u00e1csy et al\\.", "year": 2007}, {"title": "FarsiSum \u2013 A Persian text summarizer, In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages", "author": ["M. Hassel", "N. Mazdak"], "venue": "(Semitic", "citeRegEx": "Hassel and Mazdak,? \\Q2004\\E", "shortCiteRegEx": "Hassel and Mazdak", "year": 2004}, {"title": "An Automatic Linguistics Approach for Persian Document Summarization", "author": ["H. Kamyar", "M. Kahani", "M. Kamyar", "A. Poormasoomi"], "venue": "In Proceedings of the International Conference on Asian Language Processing (IALP),", "citeRegEx": "Kamyar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kamyar et al\\.", "year": 2011}, {"title": "Optimizing Persian Text Summarization Based on Fuzzy Logic Approach", "author": ["F. Kiyoumarsi", "F.R. Esfahani"], "venue": "In Proceedings of the International Conference on Intelligent Building and Management,", "citeRegEx": "Kiyoumarsi and Esfahani,? \\Q2011\\E", "shortCiteRegEx": "Kiyoumarsi and Esfahani", "year": 2011}, {"title": "Complexity of Word Collocation Networks: A Preliminary Structural Analysis", "author": ["S. Lahiri"], "venue": "In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lahiri,? \\Q2014\\E", "shortCiteRegEx": "Lahiri", "year": 2014}, {"title": "Keyword and Keyphrase Extraction Using Centrality Measures on Collocation Networks, ArXiv e-print, January", "author": ["S. Lahiri", "S. Ray Choudhury", "C. Caragea"], "venue": null, "citeRegEx": "Lahiri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lahiri et al\\.", "year": 2014}, {"title": "A Technique for the Measurement of Attitudes", "author": ["R. Likert"], "venue": "Archives of Psychology,", "citeRegEx": "Likert,? \\Q1932\\E", "shortCiteRegEx": "Likert", "year": 1932}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries, In Text Summarization Branches Out", "author": ["Lin", "C.-Y"], "venue": "Proceedings of the ACL-04 Workshop,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "Graph-based Natural Language Processing and Information Retrieval", "author": ["R. Mihalcea", "D. Radev"], "venue": null, "citeRegEx": "Mihalcea and Radev,? \\Q2011\\E", "shortCiteRegEx": "Mihalcea and Radev", "year": 2011}, {"title": "TextRank: Bringing Order into Texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Mihalcea and Tarau,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea and Tarau", "year": 2004}, {"title": "WordNet: A Lexical Database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller,? \\Q1995\\E", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation", "author": ["A. Nenkova", "R. Passonneau", "K. McKeown"], "venue": "ACM Transactions on Speech and Language Processing,", "citeRegEx": "Nenkova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2007}, {"title": "Using the Pyramid Method for generating gold standards for Persian texts", "author": ["M.J. Nia"], "venue": null, "citeRegEx": "Nia,? \\Q2013\\E", "shortCiteRegEx": "Nia", "year": 2013}, {"title": "The PageRank Citation Ranking: Bringing Order to the Web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "In Proceedings of the 7 International World Wide Web Conference,", "citeRegEx": "Page et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Page et al\\.", "year": 1998}, {"title": "Context-Based Persian Multi-Document Summarization (global view)", "author": ["A. Poormasoomi", "M. Kahani", "S.V. Yazdi", "H. Kamyar"], "venue": "In Proceedings of the International Conference on Asian Language Processing (IALP),", "citeRegEx": "Poormasoomi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poormasoomi et al\\.", "year": 2011}, {"title": "Development of a Persian Syntactic Dependency Treebank", "author": ["M.S. Rasooli", "M. Kouhestani", "A. Moloodi"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Rasooli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rasooli et al\\.", "year": 2013}, {"title": "PrePer: A Pre-processor for Persian", "author": ["M. Seraji"], "venue": "In Proceedings of the Fifth International Conference on Iranian Linguistics (ICIL5),", "citeRegEx": "Seraji,? \\Q2013\\E", "shortCiteRegEx": "Seraji", "year": 2013}, {"title": "Morphosyntactic Corpora and Tools for Persian", "author": ["M. Seraji"], "venue": "Ph.D. Thesis,", "citeRegEx": "Seraji,? \\Q2015\\E", "shortCiteRegEx": "Seraji", "year": 2015}, {"title": "A New Graph-based Algorithm for Persian Text Summarization", "author": ["H. Shakeri", "S. Gholamrezazadeh", "M. Amini Salehi", "F. Ghadamyari"], "venue": "Computer Science and Convergence,", "citeRegEx": "Shakeri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shakeri et al\\.", "year": 2012}, {"title": "2009a, Persian Document Summarization by Parsumist", "author": ["M. Shamsfard", "T. Akhavan", "M. Erfani Joorabchi"], "venue": "World Applied Sciences", "citeRegEx": "Shamsfard et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shamsfard et al\\.", "year": 2009}, {"title": "2009b, Parsumist: A Persian Text Summarizer", "author": ["M. Shamsfard", "T. Akhavan", "M. Erfani Jourabchi"], "venue": "In Proceedings of the International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE),", "citeRegEx": "Shamsfard et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shamsfard et al\\.", "year": 2009}, {"title": "Persian Text Summarization", "author": ["M. Tofighy", "O. Kashefi", "A. Zamanifar", "H.H. Seyyed Javadi"], "venue": "Using Fractal Theory. Informatics Engineering and Information Science,", "citeRegEx": "Tofighy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tofighy et al\\.", "year": 2011}, {"title": "AHP Techniques for Persian Text Summarization", "author": ["S.M. Tofighy", "R.G. Raj", "H.H. Seyyed Javadi"], "venue": "Malaysian Journal of Computer Science,", "citeRegEx": "Tofighy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tofighy et al\\.", "year": 2013}, {"title": "Collective dynamics of \u2018small-world", "author": ["D.J. Watts", "S.H. Strogatz"], "venue": null, "citeRegEx": "Watts and Strogatz,? \\Q1998\\E", "shortCiteRegEx": "Watts and Strogatz", "year": 1998}, {"title": "AZOM: A Persian Structured Text Summarizer", "author": ["A. Zamanifar", "O. Kashefi"], "venue": "Natural Language Processing and Information Systems,", "citeRegEx": "Zamanifar and Kashefi,? \\Q2011\\E", "shortCiteRegEx": "Zamanifar and Kashefi", "year": 2011}, {"title": "A New Hybrid Farsi Text Summarization Technique Based on Term Co-Occurrence and Conceptual Property of the Text", "author": ["A. Zamanifar", "B. Minaei-Bidgoli", "M. Sharifi"], "venue": "In Proceedings of the Ninth ACIS International Conference on Software Engineering,", "citeRegEx": "Zamanifar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zamanifar et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 35, "context": "Despite having such a large speaker population and a vibrant presence in many countries, Natural Language Processing in Persian has been stymied by a lack of publicly available corpora and tools, which has only very recently started to change (Seraji, 2013; Seraji, 2015).", "startOffset": 243, "endOffset": 271}, {"referenceID": 36, "context": "Despite having such a large speaker population and a vibrant presence in many countries, Natural Language Processing in Persian has been stymied by a lack of publicly available corpora and tools, which has only very recently started to change (Seraji, 2013; Seraji, 2015).", "startOffset": 243, "endOffset": 271}, {"referenceID": 0, "context": "In our first approach, we used a well-known Persian news corpus (Hamshahri (AleAhmad et al., 2009)) to evaluate Parsumist \u2013 a popular Persian document summarization framework (Sections 3 and 4).", "startOffset": 75, "endOffset": 98}, {"referenceID": 28, "context": "In our second approach, we used centrality measures on sentence networks (similar to TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004)) to perform single and multi-document summarization on the recently released Pasokh corpus (Sections 5 and 6).", "startOffset": 94, "endOffset": 120}, {"referenceID": 16, "context": "In our second approach, we used centrality measures on sentence networks (similar to TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004)) to perform single and multi-document summarization on the recently released Pasokh corpus (Sections 5 and 6).", "startOffset": 133, "endOffset": 156}, {"referenceID": 15, "context": "It used a client-server application written in Perl, and was little more than an earlier implementation \u2013 SweSum (Dalianis, 2000) \u2013 augmented by a Persian stop word list.", "startOffset": 113, "endOffset": 129}, {"referenceID": 14, "context": "It used a client-server application written in Perl, and was little more than an earlier implementation \u2013 SweSum (Dalianis, 2000) \u2013 augmented by a Persian stop word list. No evaluation was reported in Hassel and Mazdak (2004)\u2019s study.", "startOffset": 114, "endOffset": 226}, {"referenceID": 14, "context": "It used a client-server application written in Perl, and was little more than an earlier implementation \u2013 SweSum (Dalianis, 2000) \u2013 augmented by a Persian stop word list. No evaluation was reported in Hassel and Mazdak (2004)\u2019s study. Zamanifar et al. (2008) reported the next study using lexical chains, word clustering, and a summary coefficient for each cluster.", "startOffset": 114, "endOffset": 259}, {"referenceID": 14, "context": "It used a client-server application written in Perl, and was little more than an earlier implementation \u2013 SweSum (Dalianis, 2000) \u2013 augmented by a Persian stop word list. No evaluation was reported in Hassel and Mazdak (2004)\u2019s study. Zamanifar et al. (2008) reported the next study using lexical chains, word clustering, and a summary coefficient for each cluster. Words were clustered by their co-occurrence degree assessed by a bigram language model. Results on 60 Persian news articles showed that Zamanifar et al. (2008)\u2019s approach was superior to FarsiSum (in terms of precision and recall) at compression ratios of 30%, 40%, and 50%.", "startOffset": 114, "endOffset": 526}, {"referenceID": 5, "context": "Berenjkoob et al. (2009) explored the qualitative and quantitative merits of incorporating stemming and stop word removal in Persian document summarization.", "startOffset": 0, "endOffset": 25}, {"referenceID": 29, "context": "In the preprocessing stage, stop words were removed and content words were mapped to a sense hierarchy akin to WordNet (Miller, 1995).", "startOffset": 119, "endOffset": 133}, {"referenceID": 21, "context": "Kamyar et al. (2011) were the first.", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "Kiyoumarsi and Esfahani (2011) introduced fuzzy logic in Persian document summarization.", "startOffset": 0, "endOffset": 31}, {"referenceID": 21, "context": "Kiyoumarsi and Esfahani (2011) introduced fuzzy logic in Persian document summarization. They reported performance superior to four other systems in a simulation study. Zamanifar and Kashefi (2011) discussed AZOM \u2013 a summarizer that takes into account the implicit or explicit structure of a document (in terms of paragraphs, blocks, etc).", "startOffset": 0, "endOffset": 198}, {"referenceID": 21, "context": "Kiyoumarsi and Esfahani (2011) introduced fuzzy logic in Persian document summarization. They reported performance superior to four other systems in a simulation study. Zamanifar and Kashefi (2011) discussed AZOM \u2013 a summarizer that takes into account the implicit or explicit structure of a document (in terms of paragraphs, blocks, etc). AZOM performed better than two state-of-the-art approaches, and a \u201cflat summary\u201d baseline. Tofighy et al. (2011) used a more rigorous structure-based approach, leveraging block nesting and sibling blocks.", "startOffset": 0, "endOffset": 453}, {"referenceID": 3, "context": "Bazghandi et al. (2012) clustered sentences based on the semantic similarity of their content words.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Bazghandi et al. (2012) clustered sentences based on the semantic similarity of their content words. Semantic similarity between two words was defined as a variant of their PMI (pointwise mutual information). Sentences were clustered using a particle swarm optimization (PSO) algorithm. The system achieved competitive results with traditional clustering approaches. Shakeri et al. (2012) constructed an undirected sentence network similar to LexRank", "startOffset": 0, "endOffset": 389}, {"referenceID": 16, "context": "(Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004).", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "(Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004).", "startOffset": 37, "endOffset": 63}, {"referenceID": 41, "context": "An interesting study employing Analytical Hierarchy Process (AHP) to Persian document summarization was discussed in (Tofighy et al., 2013).", "startOffset": 117, "endOffset": 139}, {"referenceID": 16, "context": "(Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). They applied the system to ten Persian scientific papers at a compression ratio of 50%. The network-based system performed significantly better than FarsiSum against a human-generated gold standard (in terms of precision, recall, F-score, and ROUGE-1). An interesting study employing Analytical Hierarchy Process (AHP) to Persian document summarization was discussed in (Tofighy et al., 2013). The authors arranged six existing summarization features (word frequency, keywords, headline word, cue word, sentence position, and sentence length) in an AHP matrix to assess their relative importance, and to come up with an optimal selection of sentences. Results showed better F-score than FarsiSum at compression ratios of 30% and 40%. Finally, Nia (2013) did his Master\u2019s thesis in generating gold standards for Persian", "startOffset": 1, "endOffset": 819}, {"referenceID": 30, "context": "document summarization using the pyramid method (Nenkova et al., 2007).", "startOffset": 48, "endOffset": 70}, {"referenceID": 19, "context": "While Parsumist uses a home-grown tokenizer, we used the tokenizer made available by Mojgan Seraji as part of the Persian language processing toolkit she developed (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015).", "startOffset": 164, "endOffset": 214}, {"referenceID": 35, "context": "While Parsumist uses a home-grown tokenizer, we used the tokenizer made available by Mojgan Seraji as part of the Persian language processing toolkit she developed (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015).", "startOffset": 164, "endOffset": 214}, {"referenceID": 36, "context": "While Parsumist uses a home-grown tokenizer, we used the tokenizer made available by Mojgan Seraji as part of the Persian language processing toolkit she developed (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015).", "startOffset": 164, "endOffset": 214}, {"referenceID": 0, "context": "Note further that we used Hamshahri (AleAhmad et al., 2009) as our input corpus for this part, and Hamshahri is released in NCR (numeric character encoding) encoded XML format, so we needed to convert NCR into UTF-8 Unicode first.", "startOffset": 36, "endOffset": 59}, {"referenceID": 29, "context": ", a synset) in a small concept hierarchy akin to WordNet (Miller, 1995).", "startOffset": 57, "endOffset": 71}, {"referenceID": 33, "context": "Poormasoomi et al. (2011) were the first.", "startOffset": 0, "endOffset": 26}, {"referenceID": 19, "context": "Many of the features needed a part-of-speech tagger; we used Mojgan Seraji\u2019s tagger (Hal\u00e1csy et al., 2007; Seraji, 2015) for this purpose.", "startOffset": 84, "endOffset": 120}, {"referenceID": 36, "context": "Many of the features needed a part-of-speech tagger; we used Mojgan Seraji\u2019s tagger (Hal\u00e1csy et al., 2007; Seraji, 2015) for this purpose.", "startOffset": 84, "endOffset": 120}, {"referenceID": 29, "context": "Similarity between two nodes (sentences) is computed using lexical chains and a synset hierarchy akin to WordNet (Miller, 1995).", "startOffset": 113, "endOffset": 127}, {"referenceID": 12, "context": ", Bruni et al. (2014)), and is reasonably free from statistical assumptions.", "startOffset": 2, "endOffset": 22}, {"referenceID": 28, "context": "Then we followed the popular TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) formalism that prescribed running a random walk on this sentence network, and scoring nodes by their PageRank value (Page et al.", "startOffset": 38, "endOffset": 64}, {"referenceID": 16, "context": "Then we followed the popular TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) formalism that prescribed running a random walk on this sentence network, and scoring nodes by their PageRank value (Page et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 32, "context": "Then we followed the popular TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) formalism that prescribed running a random walk on this sentence network, and scoring nodes by their PageRank value (Page et al., 1998).", "startOffset": 217, "endOffset": 236}, {"referenceID": 10, "context": "Please see Boudin (2013) and Lahiri et al.", "startOffset": 11, "endOffset": 25}, {"referenceID": 10, "context": "Please see Boudin (2013) and Lahiri et al. (2014) for evidence that non-PageRank centrality measures often perform as well as or better than PageRank, when it comes to keyphrase extraction.", "startOffset": 11, "endOffset": 50}, {"referenceID": 20, "context": "Parsumist was compared against \u2013 and found to perform better than \u2013 two then state-of-the-art baselines: FarsiSum (Hassel and Mazdak, 2004), and Karimi and Shamsfard\u2019s summarizer.", "startOffset": 114, "endOffset": 139}, {"referenceID": 0, "context": "Since we did not have access to the annotated gold standard data Parsumist authors generated, we instead used the Hamshahri corpus of news articles (AleAhmad et al., 2009), and evaluated our four systems (cf.", "startOffset": 148, "endOffset": 171}, {"referenceID": 28, "context": "Parsumist, as already noted, can perform multi-document summarization in this way; so can graph-based systems like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004).", "startOffset": 124, "endOffset": 150}, {"referenceID": 16, "context": "Parsumist, as already noted, can perform multi-document summarization in this way; so can graph-based systems like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004).", "startOffset": 163, "endOffset": 186}, {"referenceID": 25, "context": "Note that the above scheme corresponds to a Likert Scale style annotation (Likert, 1932), and is widely used in summarization research.", "startOffset": 74, "endOffset": 88}, {"referenceID": 18, "context": "Instead, we used the popular ROUGE package (Lin, 2004) \u2013 in particular, its recent Java implementation called JROUGE (Ganesan et al., 2010) \u2013 to evaluate our centrality-based systems.", "startOffset": 117, "endOffset": 139}, {"referenceID": 17, "context": "Centrality-Based Methods It is well-known in distributional semantics that a word is known by the company it keeps (Firth, 1957).", "startOffset": 115, "endOffset": 128}, {"referenceID": 23, "context": "(Lahiri, 2014)), where nodes are words and edges are word co-occurrence relationships, can succinctly and explicitly capture such dependencies.", "startOffset": 0, "endOffset": 14}, {"referenceID": 28, "context": "It has been observed (Mihalcea and Tarau, 2004) that in such networks, words with the highest centrality also happen to be the most important words (keywords) in the document.", "startOffset": 21, "endOffset": 47}, {"referenceID": 28, "context": "where nodes are sentences and edges are weighted (perhaps pruned, too) by sentence similarity, then the most important sentences in the document turn out to have highest centrality in the resulting network (Mihalcea and Tarau, 2004; Erkan and Radev, 2004).", "startOffset": 206, "endOffset": 255}, {"referenceID": 16, "context": "where nodes are sentences and edges are weighted (perhaps pruned, too) by sentence similarity, then the most important sentences in the document turn out to have highest centrality in the resulting network (Mihalcea and Tarau, 2004; Erkan and Radev, 2004).", "startOffset": 206, "endOffset": 255}, {"referenceID": 27, "context": "These two observations started the field of graph-based summarization and keyword extraction (Mihalcea and Radev, 2011).", "startOffset": 93, "endOffset": 119}, {"referenceID": 10, "context": "(Boudin, 2013; Lahiri et al., 2014)), and graph-based approaches are still being pursued by several research groups around the world.", "startOffset": 0, "endOffset": 35}, {"referenceID": 24, "context": "(Boudin, 2013; Lahiri et al., 2014)), and graph-based approaches are still being pursued by several research groups around the world.", "startOffset": 0, "endOffset": 35}, {"referenceID": 28, "context": "However, as Mihalcea and Tarau (2004) pointed out using a cognitive science argument, any network \u2013 be it words or sentences or people (social networks) \u2013 encodes a system of endorsements and", "startOffset": 12, "endOffset": 38}, {"referenceID": 8, "context": "In practice, the concept of centrality has been studied in the social sciences for several decades (Borgatti, 2005).", "startOffset": 99, "endOffset": 115}, {"referenceID": 6, "context": "As Boldi and Vigna (2014) point out, there exist at least six different categories (families) of centrality indices", "startOffset": 3, "endOffset": 26}, {"referenceID": 8, "context": "However, as Borgatti and Everett (2006) observed, most (if not all) of these indices assess a", "startOffset": 12, "endOffset": 40}, {"referenceID": 19, "context": "Sentences were extracted from the Pasokh corpus using the normalizer and tokenizer developed by Mojgan Seraji (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015), in the same spirit as Section 3.", "startOffset": 110, "endOffset": 160}, {"referenceID": 35, "context": "Sentences were extracted from the Pasokh corpus using the normalizer and tokenizer developed by Mojgan Seraji (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015), in the same spirit as Section 3.", "startOffset": 110, "endOffset": 160}, {"referenceID": 36, "context": "Sentences were extracted from the Pasokh corpus using the normalizer and tokenizer developed by Mojgan Seraji (Hal\u00e1csy et al., 2007; Seraji, 2013; Seraji, 2015), in the same spirit as Section 3.", "startOffset": 110, "endOffset": 160}, {"referenceID": 14, "context": "Centrality was computed using the igraph package (Csardi and Nepusz, 2006).", "startOffset": 49, "endOffset": 74}, {"referenceID": 42, "context": "Clustering Coefficient: density of edges among the immediate neighbors of a node (Watts and Strogatz, 1998).", "startOffset": 81, "endOffset": 107}, {"referenceID": 32, "context": "PageRank: importance of a node based on how many important nodes it is connected to (Page et al., 1998).", "startOffset": 84, "endOffset": 103}, {"referenceID": 1, "context": "Betweenness: fraction of shortest paths that pass through a node, summed over all node pairs (Anthonisse, 1971; Brandes, 2001).", "startOffset": 93, "endOffset": 126}, {"referenceID": 11, "context": "Betweenness: fraction of shortest paths that pass through a node, summed over all node pairs (Anthonisse, 1971; Brandes, 2001).", "startOffset": 93, "endOffset": 126}, {"referenceID": 2, "context": "Closeness: reciprocal of the sum of distances of all nodes to a node (Bavelas, 1950).", "startOffset": 69, "endOffset": 84}, {"referenceID": 7, "context": "Eigenvector Centrality: element of the first eigenvector of a graph adjacency matrix corresponding to a node (Bonacich, 1987).", "startOffset": 109, "endOffset": 125}, {"referenceID": 6, "context": "The above seven centrality measures touch all six different centrality families identified by Boldi and Vigna (2014), hence they may be deemed comprehensive for the purposes of our study.", "startOffset": 94, "endOffset": 117}, {"referenceID": 18, "context": "Hence, we resorted to the recently released JROUGE package written in Java (Ganesan et al., 2010).", "startOffset": 75, "endOffset": 97}, {"referenceID": 34, "context": "Dependency treebanks have been developed and are currently available for Persian (Rasooli et al., 2013; Seraji, 2015).", "startOffset": 81, "endOffset": 117}, {"referenceID": 36, "context": "Dependency treebanks have been developed and are currently available for Persian (Rasooli et al., 2013; Seraji, 2015).", "startOffset": 81, "endOffset": 117}], "year": 2016, "abstractText": "In this paper we explore the problem of document summarization in Persian language from two distinct angles. In our first approach, we modify a popular and widely cited Persian document summarization framework to see how it works on a realistic corpus of news articles. Human evaluation on generated summaries shows that graph-based methods perform better than the modified systems. We carry this intuition forward in our second approach, and probe deeper into the nature of graph-based systems by designing several summarizers based on centrality measures. Ad hoc evaluation using ROUGE score on these summarizers suggests that there is a small class of centrality measures that perform better than three strong unsupervised baselines.", "creator": "Microsoft\u00ae Word 2016"}}}