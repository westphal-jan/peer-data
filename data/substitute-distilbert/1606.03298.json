{"id": "1606.03298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Structured Factored Inference: A Framework for Automated Reasoning in Probabilistic Programming Languages", "abstract": "reasoning on large and evolving real - world models is essentially computationally difficult task, yet everything that is required for effective use beside many ai problems. a plethora of inference algorithms have been developed that work well on animal models or only on parts of general models. consequently, allowing system that can intelligently transport these inference algorithms to different parts of a society for fast convergence is highly desirable. we introduce a new framework called structured factored reasoning ( sfi ) that provides the foundation for such a system. using models conceived in a probabilistic programming scenario, id provides a conceptual means to decompose a model into sub - models, apply an inference algorithm to each sub - model, and combine unexpected resulting information to answer a query. our results show that sfi is nearly as accurate as exact facts yet retains the benefits of approximate inference methods.", "histories": [["v1", "Fri, 10 Jun 2016 12:53:01 GMT  (1033kb,D)", "http://arxiv.org/abs/1606.03298v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["avi pfeffer", "brian ruttenberg", "william kretschmer"], "accepted": false, "id": "1606.03298"}, "pdf": {"name": "1606.03298.pdf", "metadata": {"source": "CRF", "title": "Structured Factored Inference: A Framework for Automated Reasoning in Probabilistic Programming Languages", "authors": ["Avi Pfeffer", "Brian Ruttenberg", "William Kretschmer"], "emails": ["apfeffer@cra.com", "bruttenberg@cra.com", "kretsch@mit.edu"], "sections": [{"heading": null, "text": "Reasoning on large and complex real\u2013world models is a computationally difficult task, yet one that is required for effective use of many AI applications. A plethora of inference algorithms have been developed that work well on specific models or only on parts of general models. Consequently, a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable. We introduce a new framework called structured factored inference (SFI) that provides the foundation for such a system. Using models encoded in a probabilistic programming language, SFI provides a sound means to decompose a model into sub\u2013models, apply an inference algorithm to each sub\u2013model, and combine the resulting information to answer a query. Our results show that SFI is nearly as accurate as exact inference yet retains the benefits of approximate inference methods."}, {"heading": "1 INTRODUCTION", "text": "Probabilistic modeling is at the core of many artificial intelligence (AI) applications. The complexity, richness, and diversity of these models are rapidly growing as AI takes on a larger role in everyday life. As a result, the efficiency of probabilistic inference is critical for effective use of these models. However, despite significant research into efficient algorithms and techniques, probabilistic inference remains a significant bottleneck in many real\u2013world AI applications.\nMany different algorithms have been explored to reason on general models. Unfortunately, no single algorithm performs sufficiently on every model, and often there are\n\u2217Work performed while interning at Charles River Analytics\ntrade\u2013offs that must be made. For example, sampling methods such as Metropolis-Hastings [1] are often the \u201cgo to\u201d algorithms for reasoning on continuous models, but convergence can be painfully slow and they suffer from high variance estimates. Exact methods such as variable elimination [2] work well on discrete problems, but are intractable for all but the simplest models. Recent work on generalized variational inference [3] shows promise, but still requires some hand\u2013tuning to work effectively. Once an algorithm has been found to work well on a specific problem, even slight modifications are no guarantee of continued success; adding a single continuous variable to an otherwise discrete problem can vastly affect the performance of the existing algorithm. Thus, AI developers are faced with selecting and configuring the appropriate algorithm that will work well on their problem, a task that is often more time consuming than constructing the actual model.\nOne solution to reduce this burden is to develop a method that can automatically select an algorithm that should perform well on one\u2019s specific problem. One major impediment to this approach is that the size and complexity of real\u2013world models makes it difficult to determine the best single algorithm. Indeed, different algorithms might be appropriate for different parts of the model. For example, one algorithm might be appropriate for a continuous portion of the model while another is used for a discrete portion; indeed, the Rao\u2013Blackwellization algorithm [4] exploits this fact. As a result, one approach to achieving automated inference is to not just select a single algorithm, but rather a set of algorithms to apply to different parts of a model, and combine the results in the appropriate manner. Central to this approach is developing a sound method to decompose a model into manageable sub\u2013models with the appropriate granularity. Such a method would provide a framework to intelligently select algorithms for different parts of a model, and the means to combine results to answer the query.\nThe emerging field of probabilistic programming (PP) provides the opportunity to support this decomposition framework. PP [5, 6] provides expressive and general purpose languages to encode a probabilistic model as an executable\nar X\niv :1\n60 6.\n03 29\n8v 1\n[ cs\n.A I]\n1 0\nJu n\n20 16\nprogram. This allows one to leverage the power of programming languages to create rich and complex models, and use built\u2013in inference algorithms that can operate on any model written in the language. More importantly, since the models are encoded as a program, we can use the program structure to understand the properties of a model before inference is even attempted.\nWe introduce a new PP\u2013based inference framework called structured factored inference (SFI). SFI uses simple PP semantics to identify decomposition points within a probabilistic model, creating an abstract hierarchy of sub\u2013 models. Each sub\u2013model is independently reduced to a joint distribution over variables relevant to answering the query, using any inference method. Using factors to represent this joint distribution, the results are incorporated into the inference algorithms applied on other sub\u2013models. The SFI framework brings many significant advantages. First, SFI provides the capability to apply decomposition strategies to decomposition point so that sub\u2013models can be created with small interfaces (and thus small joint distributions). For example, a strategy could choose to create a sub\u2013model defined by a single decomposition point or combine several decomposition points into a single sub\u2013 model. Second, it has the ability to apply inference strategies that choose algorithms to \u201csolve\u201d a particular sub\u2013 model during the inference process. This means that one need not decide on a single inference algorithm to apply to an entire model.\nWe show the benefits of SFI on three realistic models using a combination of exact and approximate inference algorithms. Our experiments show that even with simple strategies for decomposition and inference, probabilistic reasoning using SFI achieves performance equal to or better than approximate inference methods and is nearly as accurate as exact inference methods. The SFI framework is extremely general and expandable, providing the opportunity to use more complex decomposition strategies or intelligent inference strategies. The SFI framework has the potential to be the foundation for a general automated inference system."}, {"heading": "2 RELATED WORK", "text": "Automated algorithm selection has been a long desired goal in computer science, with possibly the first formulation by Rice [7]. As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10]. Most efforts, however, focus on methods to analyze and learn how to apply the single best algorithm to solve a problem. For example, Guo [11] uses Bayesian networks to learn and select the best algorithm to solve a problem; neither the problems or algorithms are specific, and can be applied generally to a variety of problems. Our SFI framework is complementary to much of\nthis existing work. SFI can decompose complex models into smaller sub\u2013models that these sophisticated learning algorithms can operate on, potentially providing even better performance than learning on a single model.\nProbabilistic inference is unique in some respects as the independence properties of models provides the opportunity to apply many algorithms to different parts of a problem. There has not been significant amount of progress in the probabilistic modeling community, however, to address this or take advantage of it. Our approach is similar in spirit to the current work on black box variational inference [12]. These recent methods have attempted to reduce the programmer burden of configuring and applying variational inference to general probabilistic models, and in a sense are attempting to automatically find the best configuration of the algorithm that produces optimal inference results. While these approaches are promising, they still only consider a single algorithm. The decomposition strategies in SFI also bears similarity to structured variable elimination (SVE) [13]. Like SVE, SFI enjoys the benefits of decomposition in exploiting small interfaces and reusing work. However, SVE applies the same algorithm to each problem, whereas SFI is a general framework for decomposing problems and optimizing each sub\u2013model separately. Finally, our work is similar to decomposition methods that solve maximum a posteriori (MAP) queries presented by Duchi et. al. [14]. This work, however, only applies to specific decompositions of Markov random fields and only applies to MAP problems."}, {"heading": "3 PROBABILISTIC PROGRAMMING LANGUAGE", "text": "The fundamental concepts in SFI are strongly tied to probabilistic programming languages (PPL), and SFI has been implemented in a publicly available PPL. As such, understanding PPL semantics is critical for understanding SFI. However, since the focus of this paper is introducing the SFI concept, we present a simplified and abstract PPL for purposes of explanation. We call this abstract language SimplePPL."}, {"heading": "3.1 SimplePPL Language", "text": "The central concept in SimplePPL is a random variable (RV). Intuitively, a RV represents a random process that stochastically produces a value. For simplicity, we use an untyped language, so an RV can produce any value in the value set T , where T is a countable finite set. A program Q has a set of free RVs FQ, and consists of a sequence of definitions of the formRV = expr. The set of RVs defined by Q is denoted RVQ. An RV is available if it is either in FQ or defined previously in Q. The set of available RVs with respect to an RV r is denoted Ar.\nAn expression defining an RV r is one of the following:\n\u2022 A value v \u2022 A primitive defining a probability distribution over\nvalues. Examples include Flip(p), which produces true with probability p, and Uniform(x, y). \u2022 Apply(r1, . . . , rn, f), where r1, . . . , rn are available RVs and f is a function Tn \u2192 T . \u2022 Chain(r1, f), where r1 is an available RV and f is a\nfunction T \u2192 Q, where Q is the space of programs such that for each Q\u2032 \u2208 Q, FQ\u2032 \u2286 Ar and the final RV in the program is named \u201coutcome\u201d."}, {"heading": "3.2 SimplePPL Semantics", "text": "Although there are clear semantics for recursive programs in SimplePPL, for simplicity in this paper it will suffice to assume that the expansions of a program Q are non\u2013 recursive and thus finite. Under the SimplePPL semantics, each program Q defines a conditional probability distribution P (RVQ|FQ). This is achieved by defining, for each RV r defined in Q, a conditional distribution P (r|Ar) and then using the chain rule so that\nP (RVQ|FQ) = \u220f\nr\u2208RVQ\nP (r|Ar)\nP (r|Ar) is defined as follows:\n\u2022 For r defined by value v, P (r|Ar) assigns probability 1 to v. \u2022 For r defined by a primitive distribution \u03c0, P (r|Ar)\nis P (\u03c0). \u2022 For r defined by Apply(r1, . . . , rn, f), by assump-\ntion {r1, . . . , rn} \u2286 Ar. Therefore, P (r|Ar) assigns probability 1 to f(v1, . . . , vn) for any values v1, . . . , vn in the support of r1, . . . , rn. \u2022 For r defined by Chain(r1, f), let v1 be the value\nof r1. Let Q\u2032 be the program f(v1). By induction, Q\u2032 defines a distribution Pv1(RVQ\u2032 |FQ\u2032). By definition, FQ\u2032 = Ar and because \u201coutcome\u201d \u2208 RVQ\u2032 , Q\u2032 defines a distribution Pv1(outcome|Ar). As r1 \u2208 Ar, the distribution P (r|Ar) is equal to Pv1(outcome|Ar) for any value v1 \u2208 r1.\nSimplePPL is purely functional, and evidence (conditions or constraints) can be applied as part of a query on a model. While SimplePPL lacks the complexity of many PPLs, it is as expressive as full\u2013fledged PPLs."}, {"heading": "4 STRUCTURED FACTORED INFERENCE", "text": ""}, {"heading": "4.1 Overview", "text": "There is a simple intuition behind SFI: If a model can be broken down into smaller sub\u2013models (i.e., programs)\n2\nthat can solved independently (i.e., marginalizing out non\u2013 relevant variables), then different algorithms can be applied to different parts of a model. Combined with methods for intelligent inference algorithm selection, this framework could then lead to improved inference on a wide variety of problems. SFI is fundamentally a framework for applying two types of strategies: A decomposition strategy that divides a model into smaller sub\u2013models, and an inference strategy that appropriately applies an inference algorithm to each sub\u2013model. SFI uses factors to combine information from solved sub\u2013models to answer queries.\nAs an example, consider the following model written in SimplePPL, shown in Figs. 1 and 2. We have three RVs defined in Q, a, b, and c. RVs b and c generate a value using the Q\u2032 that is generated by f(a), where outcomebT refers to the outcome for RV b when a is true, and so forth. Each Chain generates a program Q\u2032 for both true and false conditions. With the exception of \u201coutcome\u201d, the RVs defined byQ\u2032 are not directly needed to reason about a, b, and c. That is, all of the RVs defined in Q\u2032 except \u201coutcome\u201d and FQ\u2032 can be marginalized out of Q\u2032. A joint distribution over \u201coutcome\u201d \u222a FQ\u2032 is all that is needed to reason at the top\u2013level program Q. Since this marginalization is self\u2013contained, any inference algorithm that can compute a\njoint distribution can be applied to each sub\u2013model (shown in the boxes in Fig. 2). This joint distribution represented as a factor; these factors are then \u201crolled up\u201d and used by another algorithm to answer queries on the program Q. This is the core operation of SFI: Given a sub\u2013model, use an algorithm to marginalize away internal variables and return a joint distribution over \u201coutcome\u201d and the free variables, and repeat the process until the query is answered."}, {"heading": "4.2 Model Decomposition", "text": "The following discussion of SFI is cast in the context of a SimplePPL program. However, the method is applicable to graphical models in general.\nThe key operation of SFI is model decomposition. This operation decomposes a model into semantically meaningful sub\u2013models (i.e., programs) that can be reduced to a joint distribution over relevant variables. First, we define two key concepts: uses and external. A RV r uses a RV x if:\nx \u2208 Ar \u2227 P (r|Ar) 6= P (r|Ar \\ x)\nWe denote the set of variables Ur for a variable r as the set of all variables r uses, either directly or recursively, plus r. This definition of uses can be difficult to verify in a program based on the semantics of SimplePPL. However, in our implementation of SimplePPL, we have a syntactic (but stronger) condition for r using x based on x appearing in the expression for r, or in any expression for a variable used by r. Such a condition is necessary for uses and thus still guarantees SFI\u2019s soundness.\nWe denote that a variable x is external to r if:\nx \u2208 Ur \u2227 \u2203 y \u2208 RVQ \\ Ur |x \u2208 Uy\nThat is, an external variable to r is used in the generative process of a variable that r does not use. We denote the set of variables external to r as Er.\nA decomposition of the model with respect to a RV d \u2208 RVQ is an operation that partitions RVQ into two disjoint sets of variables, RVdQ and RV d Q. The RV d is called a decomposition point. We defineRVdQ andRV d Q as:\nRVdQ = Ud \u2212 Ed\nRVdQ = RVQ \u2212RV d Q\n(1)\nIn other words,RVdQ is the set of variables exclusively used in the generation of d (i.e., no external uses), and RVdQ is all remaining variables in Q.\nAs an example, consider the program in Fig. 1. Since d can be any variable, let us choose outcomebT as the decomposition point. In this example, RVdQ is the set of variables that outcomebT exclusively uses, so it would be\n{outcomebT , x1bT , y1bT , z1bT , x2bT , y2bT , z2bT } (all the variables in the left\u2013most box of Fig. 2). RVdQ would include all other variables in the model."}, {"heading": "4.3 Factored Representation", "text": "Using factors in SFI has several advantages. First, it provides an interface to communicate the joint distribution of a sub\u2013model to other parts of the model. Second, factors make SFI algorithm\u2013agnostic; any algorithm that can compute a joint distribution and return a factor can be used. For example, sampling algorithms that can post\u2013process a joint distribution into a factor can also be used."}, {"heading": "4.3.1 Factor Creation in SimplePPL", "text": "Once the variables in Q have been split into two sets via a decomposition point, we convert the decomposition to a factored representation. Each variable r \u2208 RVQ can be converted to a set of factors \u03a8r that describe the generative semantics of the variable. For variables defined as values or primitives, we create a factor over the support of the variable using the probability distribution defining the variable. For variables defined by r = Apply(r1, . . . , rn, f), we create a single factor over {r, r1, . . . , rn} whose value is 1 when r = f(r1, . . . , rn) and 0 otherwise.\nFinally, for variables defined by r = Chain(r1, f), we create a set of factors that represents the joint distribution of {r, r1, outcome1 . . . , outcomen}. Because representing the joint distribution of {r, r1, outcome1 . . . , outcomen} could be prohibitively large, we decompose this joint distribution to keep the sum\u2013product operations tractable. For each value v of r1, we create a factor \u03c8v over {r, r1, outcomev}, where outcomev is the \u201coutcome\u201d variable generated from applying f(v). We then generate probabilities for the factor in the following manner:\n\u2022 For each v\u2032 \u2208 r1, if v\u2032 6= v, then the probability is 1. This is a \u201cdont care\u201d case. \u2022 For each v\u2032 \u2208 r1, w \u2208 r, and x \u2208 outcomev , if v\u2032 = v\nand w = x, then the probability is 1. \u2022 Otherwise, the probability is 0.\nWe also create a selector factor over {r, r1} that selects a factor over outcomev for the appropriate value of f(v)."}, {"heading": "4.3.2 SFI with Factors", "text": "We denote the set of factors created from a program Q as \u03a8, and each factor \u03c8 \u2208 \u03a8 is defined over a set of variables x\u03c8 \u2208 RVQ. Given \u03a8, the probability distribution of a RV r in the program, P (r), is formulated as:\nP (r) = 1\nZ \u2211 x\u2208RVQ\\r \u220f \u03c8\u2208\u03a8 \u03c8(x\u03c8) (2)\nwhere Z is the normalizing constant.\nAs d dividesRVQ into two sets, it naturally divides \u03a8 into two sets, \u03a8d and \u03a8d, as explained in Sec.r\u0303effactorCreation. As such, with y\u03c8 \u2208 RVQ, we can rewrite Eqn. 2 as:\nP (r) = 1\nZ \u2211 x\u2208RVdQ\\r \u2211 y\u2208RVdQ\\r \u220f \u03c8\u2208\u03a8d \u03c8(x\u03c8) \u220f \u03c8\u2208\u03a8d \u03c8(y\u03c8)\n(3) Note that even though the variables in RVdQ and RV d Q are disjoint, the variables used in the sets of factors \u03a8d and \u03a8d are not disjoint. From the definition of the sets in Eqn. 1, the only variables that are shared between \u03a8d and \u03a8d can be Ed, the set of variables external to d, and d itself. As such, we can move the summation over RVdQ in Eqn. 3 inwards and the summation over d to the outer summation, so that we get:\nP (r) = 1\nZ \u2211 x\u2208{RVdQ\u222ad}\\r \u220f \u03c8\u2208\u03a8d \u03c8(x\u03c8) \u2211 y\u2208{RVdQ\\d}\\r \u220f \u03c8\u2208\u03a8d \u03c8(y\u03c8)\n= 1\nZ \u2211 x\u2208{RVdQ\u222ad}\\r \u220f \u03c8\u2208\u03a8d \u03c8(x\u03c8)\u03c8 Ed\nwhere \u03c8E d\nis a joint factor over d and the external variables defined with respect to d. Again looking at Fig. 1 as an example, with outcomebT as the decomposition point, we perform the summation over {x1bT , y1bT , z1bT , x2bT , y2bT , z2bT } and are left with a factor over only outcomebT . This factor can them be multiplied with the remaining factors in the program and outcomebT can be summed out.\nIn this formulation, a decomposition point d implies a structured process to compute P (r) from a set of factors defined on a model: First, compute a joint distribution with respect to the decomposition point, then compute P (r) using the joint distribution and the remaining factors. Computing the joint distribution over the external variables can be accomplished by any algorithm, as can the computation of P (r) once the joint distribution is computed.\nSo far, we have only mentioned a single decomposition point in a model. However, multiple decomposition points can be defined on a model. In Fig. 2, for example, there are four natural decomposition points (outcomebT , outcome b F , outcome c T , outcome c F ) that can be marginalized independently (shown as the boxes in Fig. 2). Eqn. 3 can be reformulated for multiple points as:\nP (r) = 1\nZ \u2211 x\u2208{RVDQ\u222aD}\\r \u220f \u03c8\u2208\u03a8D \u03c8(x\u03c8) n\u220f k=1 \u03c8E dk (4)\nwhere there are n decomposition points, and RVDQ is the intersection of RVdkQ , k = 1, . . . n. Decomposition points can be nested inside other decomposition points, allowing\nAlgorithm 1 Overview of the SFI algorithm function DECOMPOSE(program Q, variables E , dStrategy DS, iStrategy IS)\n\u03a8\u2190 \u2205 for c \u2208 Qchain, v \u2208 rc do\nQ\u2032 \u2190 fc(v) 5: EQ\u2032 \u2190 EQ \u2032.outcome Q\u2032 \u222aQ\u2032.outcome\n\u03a8EQ\u2032 \u2190 DS(Q\u2032, EQ\u2032 , DS, IS) \u03a8\u2190 \u03a8 \u222a\u03a8EQ\u2032\nend for \u03c8E \u2190 IS(\u03a8 \u222a\u03a8D, E)\n10: return \u03c8E end function function SFI(program Q, query q, dStrategy DS, iStrategy IS)\n\u03c8q \u2190 Decompose(Q, q,DS, IS) return Normalize(\u03c8q)\n15: end function\ninference to proceed in any hierarchical structure implied by the model.\nIn principle, any RV could potentially be a decomposition point. However, we would like to choose a decomposition point d that leads to a small joint factor \u03c8E d\nand eliminates as many variables in RVQ as possible. Chains present a natural decomposition point, which have the benefit of being automatically derived from the program and don\u2019t need to be specified by the programmer. When we apply the Chain function f to a parent value v, f(v) is a program that defines a sequence of RVs, ending in a definition of a variable named \u201coutcome\u201d. By the semantics of Chain, only the outcome RV can be used anywhere else in the program. For each Chain defined in Q, we create a decomposition point at outcomev for every value v in the support of r1. This also implies that Ed = FQ\u2032 for a decomposition point. Thus, we know that the joint factor created at each decomposition point will only be over each \u201coutcome\u201d variable and free variables defined in the program Q\u2032 generated from f ."}, {"heading": "5 USING SFI", "text": ""}, {"heading": "5.1 SFI Operation", "text": "Algorithm 1 outlines inference in SFI. To query for the distribution over an RV q, a user calls the SFI function with the program Q (written in SimplePPL), q, a decomposition strategy DS, and an inference strategy IS. DS and IS are functions that guide the decomposition and inference of the model, and are explained in more detail below. The SFI function calls the Decompose function, and the resulting factor over q is normalized to compute P (q).\nThe Decompose function visits each decomposition point in Q, applies DS to the sub\u2013model (i.e., program) defined\nAlgorithm 2 A recursive decomposition strategy function RECURSIVEDECOMPOSITION(program Q, variables E , iStrategy IS)\nreturn Decompose(Q, E , this, IS) end function\nby each point, and marginalizes out the internal variables using IS. On lines 3, SFI iterates over all Chains defined in Q and each value v of the parent variable rc. On each iteration, it generates Q\u2032, the program created by applying the function fc to a value v (line 4). Next, it creates the set of relevant variables to programQ\u2032 as the external variables in Q\u2032 and the \u201coutcome\u201d variable (line 5). It then invokes DS on the new program, which returns a set of factors that is added to the current set for Q (lines 6 and 7). Note that a decomposition may also be recursive, as described below.\nOnce all decomposition points have been visited, the set of factors not generated from a decomposition point (\u03a8D, via the factor generation described in Sec. 4.3.1) is added to \u03a8 and IS is applied which returns a factor \u03c8E over the variables in E (lines 9 and 10). Much of the work of the SFI framework is performed by the decomposition and inference strategies, so we explain these in detail below."}, {"heading": "5.2 Strategies for Decomposition", "text": "A decomposition strategy DS is a method that defines how a program should be decomposed. It is a function that receives a program Q\u2032 and set of relevant variables EQ\u2032 , and returns a set of factors \u03a8EQ\u2032 over at least EQ\u2032 . The simplest DS is what we call \u201craising\u201d: For a point d, return the set of factors over all variables defined in Q\u2032. This strategy performs no inference, and as a result, all of the factors fromRVQ\u2032 are \u201crolled up\u201d to the top\u2013level. If each d is raised, we get a \u201cflat\u201d strategy. This is how typical inference works; factors are created for all variables and all non\u2013query variables are marginalized out in a flat operation.\nTo take advantage of different inference algorithms, it is clearly beneficial to have a DS that actually reduces the number of variables in the returned factor set \u03a8EQ\u2032 . As such, we define a recursive strategy as one that will recursively apply the Decompose function until no more decomposition points are found, shown in Alg. 2.\nHere, each decomposition point in a model is recursively visited in a depth\u2013first traversal. Once a program is reached with no decompositions, IS is applied to the factors in the program, and a joint distribution over the external variables and outcome is returned, and the process is repeated. This is referred to as hierarchical inference in SFI.\nMore complex and sophisticated strategies can also be applied. For example, a strategy could decompose only if E is at most n variables (nwould have to be specified at compile time). If the number of external variables is greater than\nn, then the function returns all of the factors defined for the program without running any inference strategy. Otherwise, it calls Decompose again to continue the recursive decomposition."}, {"heading": "5.3 Strategies for Inference", "text": "A strategy for inference applies an inference algorithm to a set of factors defined by program Q and returns a joint distribution over E , the set of external variables in the factors. While SFI uses factors communicate the joint distribution to other programs, there is no restriction that an algorithm operate on factors. As long as the algorithm can ingest factors from other decompositions and output a joint factor over E , then any algorithm can be used.\nSimplePPL\u2019s implementation of SFI uses factor\u2013based algorithms. There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16]. VE and BP are standard implementations of these algorithms on factors, and as such we do not provide any details. GS is implemented on a set of factors, but integrating it into SFI is not trivial. Much of the effort is due to the determinism frequently found in PPLs. Our implementation uses automated blocking schemes to ensure proper convergence of the Markov chain. Details on GS can be found in the supplementary material."}, {"heading": "5.3.1 Choosing an Algorithm", "text": "SFI provides the opportunity to develop schemes that dynamically select the best inference algorithm for a decomposition point, serving as the foundation for an automated inference framework. At the application of the inference strategy, there is opportunity to analyze and estimate the complexity of various algorithms applied to the factors, and choose the one with the smallest cost (e.g., speed or accuracy). For example, methods that estimate the running time of various inference algorithms on a model [17] can be encoded into an inference strategy, and the algorithm with the lowest estimated running time can be chosen.\nWe created a simple heuristic to choose an inference algorithm, but yet still demonstrates the potential of the approach. As VE is an exact algorithm, it is always preferred over other algorithms if it is not too expensive, but unfortunately is impractical on most problems. We therefore have a heuristic to use VE on a set of factors. We first compute an elimination order, O, to marginalize to the external variables. The cost of eliminating a single variable is the increase in cost between the new factor involving the variable and the existing factors, and the cost of VE is the maximum over all variables, using O. If the cost is less than some threshold we use VE, otherwise, BP or GS.\nTo choose between BP and GS, we also use another heuristic. As the degree of determinism in a model strongly cor-\nrelates with the convergence rate of BP [18], we use the amount of determinism in the model as a choice between using BP or GS. We mark a variable as deterministic if, when using GS, we must create a block of more than one variable. If the fraction of deterministic variables (as compared to all variables) in the model is greater than a threshold, then we invoke BP and otherwise GS. While these strategies are heuristics, they do demonstrate the proof of concept for automated inference, and the results presented in the next section show that they are effective."}, {"heading": "6 EXPERIMENTS", "text": "We tested SFI using three models. First, we encoded a version of the QMR medical diagnosis model [19] in SimplePPL. Like the standard QMR model, this one is a Bayesian network of causal diseases associated with observable symptoms. However, this model inserts a layer of intermediate diseases between the causal and symptom layer. Thus the intermediate diseases are conditioned on the causal diseases, and the symptoms conditioned on the intermediate diseases. The number of diseases and the number of parents per symptom are varied during testing, and the network is constructed by randomly connecting symptoms to intermediate diseases then subsequently the intermediate diseases to random causal diseases. In each test, a random number of symptoms and causal diseases are observed as evidence.\nThe second model is a mixed directed\u2013undirected model. The undirected portion of the model is an Ising model [20], where each Boolean variable v in an n \u00d7 n grid has a potential to its four vertical and horizontal neighbors. The prior over each variable, however, is a modeled as a small Bayesian network conditioned on a causal variable cn. Thus this model can be viewed as n2 Bayesian networks that are joined together in a top\u2013level Ising model. The grid size is varied during testing, but for each test a random 20% of the cn variables are observed as either true or false.\nThe last model we used is a simplified version of the Bayesian seismic monitoring system presented in [21]. This model is designed to detect and localize seismic events on a simulated two\u2013dimensional representation of earth (with semi\u2013realistic physics). The model consists of a set of monitoring stations at different locations that detect a variety of seismic signals over time. Based on a generative process for both true seismic events and false detections, the model is designed to infer the actual number of seismic events using measurement data (i.e., observations) from each of the detection stations. Continuous variables from the original model were discretized for factored inference; for most tests each distribution was discretized to five bins, but some tests varied the number of bins. We used 10 detection stations with one true event, and varied the total number of false detections from zero to 27. Ob-\nservation data was generated from a third\u2013party simulation of the seismic generative process. Note that as the number of false detections and discrete bins increases, the model quickly scales up in the number and size of the factors. On some tests, we were unable to attain ground truth since no exact algorithms could complete with available memory.\nAll of these models are well suited for SFI. First, they contain a significant amount of structure that can be used for decomposition; the diseases and symptoms in the QMR model represent a series of Chain RVs, each Boolean variable in the Ising model is a Chain that uses each cn as a parent, and each seismic station\u2019s observations are Chain RVs. Second, they are fairly common models used in practice and are realistic. All queries for testing were posterior distribution queries over a random subset of diseases (QMR model), Boolean variables (Ising model), or the number of true seismic events."}, {"heading": "6.1 Results", "text": ""}, {"heading": "6.1.1 QMR and Mixed Model Results", "text": "We first tested how different decomposition strategies affect inference. We used two strategies, flat and hierarchical, on the QMR model using BP and VE. For the hierarchical version of BP, each instance of BP ran for 10 iterations, whereas on the flat version BP was run once for 100 iterations. The results of the running time and accuracy (of BP) are shown in Figs. 3(a) and 3(b), respectively.\nOn VE, the results show that the hierarchical strategy generally is faster than the flat one. Mathematically both strategies are performing the same operations. However, the hierarchical strategy imposes a partial elimination order; an elimination order is found for each instance of VE, but the order that the decomposition points are visited is fixed by the strategy. The flat strategy uses a heuristic to find the best elimination order given all the factors in the model. From these results, it appears that the structure imposed by programmer (i.e., by using Chains) finds a better elimination order than the heuristics used to solve this NP\u2013hard problem [22]. These results are consistent with previous work on structured VE [13].\nFor BP, the hierarchical strategy consistently runs faster than the flat strategy. This comparison is not exact though as it is hard to determine how many iterations in a flat strategy \u201cequals\u201d hierarchical iterations. However, looking at the accuracy of the methods in Fig. 3(b), the hierarchical method is consistently more accurate. Thus, even if the flat BP is run for more iterations to improve its accuracy (assuming it has not converged), it is already dominated in running time and accuracy by the hierarchical method.\nNext, we applied our hybrid strategy (with hierarchical decomposition strategies) to determine if we can improve the speed and/or accuracy using multiple algorithms as compared to a single algorithm. The results on the QMR model for running time and accuracy are shown in Figs. 4(a) and 4(b), respectively. The inference strategy only chose to run VE and BP during inference, so we only compare to those\nalgorithms.\nFor running time, VE remains competitive until the number of diseases reaches about 9. Again, comparing BP to the combined VE/BP method directly is difficult. However, combined with the accuracy results in Fig. 4(b), we can analyze the relationship between running time and accuracy for all the methods. Both BP\u201310 and BP\u201350 have comparable accuracy. The hybrid methods, however, are much more accurate. The hybrid VE/BP\u201310 method is nearly 4 times more accurate than BP\u201310/BP\u201350 and VE/BP\u201350 approaches nearly zero error. The running time for the hybrid methods are both faster than their \u201crespective\u201d BP version (i.e., comparing BP\u201310 to the hybrid VE/BP\u201310 test). While VE/BP\u201350 has a longer running time than the single BP\u201310 iteration strategy, it is nearly as accurate as VE with significantly less running time.\nThe results on the mixed model are also shown in Figs. 5(a) and 5(b). The strategy only chose to run VE and GS during inference, so we only compare to those algorithms. Similar to the QMR model, VE has the best performance until the model becomes large, at which point the hybrid strategy has the best running time. Looking at Fig. 5(b), the accuracy of the hybrid methods (GS\u2013100 and GS\u20131000 iterations) is better than the single GS approaches. Overall, the hybrid approach dominates the GS approach in terms of accuracy and time. However, as more GS iterations are performed, this performance gap will decrease, as the running times of both the single and hybrid approaches are dominated by applying GS to the undirected portion of the model."}, {"heading": "6.1.2 Seismic Monitoring Results", "text": "On the seismic monitoring model, we ran two experiments where we either varied the number of false detections (Fig. 6) or discretization in the models (Fig. 7). Fig. 6 shows the running time of our hybrid VE/BP\u201315 strategy, hierarchical VE, and flat versions of VE and BP\u201330 as the number of false detections in the seismic model increases. That is, each test increased the noise in the model, making it harder to detect the true seismic events and significantly increasing the complexity of inference. On the hybrid strategy tests, the SFI framework ran VE for most of the sub\u2013models with the exception of BP on a few sub\u2013 models with large tree\u2013widths. Because the two versions of VE ran out of memory on several tests, we are unable to determine ground truth values. The difference in accuracy between VE/BP\u201315 and BP\u201330, however, is negligible. We can see that the hybrid strategy has a smaller running time\nthan all the other strategies. As before, it is hard to determine \u201cequivalence\u201d between VE/BP\u201315 and BP\u201330, but even if 30 iterations is enough to reach convergence, the hybrid strategy still dominates in terms of time.\nFigs. 7(a) and 7(b) compare strategies as the number of discretization points in the model increases. This has the effect of increasing the size of the factors while keeping the number of factors in the model constant (and assumed no false detections). In Fig. 7(a), hierarchical VE actually has the fastest running time. The hybrid strategy is faster than BP\u201330 and flat VE, but hierarchical VE clearly dominates the hybrid strategy. Again, this demonstrates that programmer imposed elimination orderings can be much better than heuristic approaches. Clearly, however, this shows that our heuristic to choose an algorithm is not sophisticated enough to understand that VE is preferred on this model. A strategy that uses more than just the increase in factor size to select an algorithm is required in this situation.\nFinally, since structured VE completed on all tests, we show the accuracy of the VE/BP\u201330 and BP\u201330 methods in Fig. 7(b). As can be seen, the hybrid method that uses a combination of VE and BP is more accurate than flat BP. In addition, as shown in Fig. 7(a), the VE/BP\u201330 method dominates BP\u201330 in terms of running time."}, {"heading": "7 CONCLUSION", "text": "In this work, we described a new framework for inference in probabilistic modeling called Structured Factored Inference. Leveraging the capabilities of probabilistic programming, we have shown a semantically sound method to decompose a model into smaller sub\u2013models, and detailed how the application of strategies can guide the inference process. Using simple heuristics to analyze the complexity of sub\u2013models, we demonstrated that SFI can be used to implement a basic automated inference scheme that reasons faster than approximate inference and is nearly as accurate as exact inference methods.\nThis work serves as a starting point for a more robust automated inference framework, but more analysis is still required. First, there needs to be more theoretical analysis on the criteria for declaring that a sub\u2013model is \u201csolved.\u201d That is, how many iterations of GS or BP should be invoked on each sub\u2013model? Most likely answering this question touches upon issues of algorithm convergence, but its impact in the SFI framework is an open research problem.\nSecond, as shown by the seismic monitoring model, more intelligent algorithm selection methods need to be developed. Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24]. Finally, new decomposition points would also need to be developed to enable more sophisticated model\ndecomposition. Chain decomposition is effective, but user\u2013 defined or object\u2013oriented decomposition points may be more effective at decomposing a model into sub\u2013models that facilitate faster inference. Our hope is that the SFI framework will be the catalyst for future research in these areas."}, {"heading": "Acknowledgements", "text": "This work was supported by DARPA contract FA8750-14C-0011."}], "references": [{"title": "Understanding the metropolis-hastings algorithm", "author": ["S. Chib", "E. Greenberg"], "venue": "The american statistician, vol. 49, no. 4, pp. 327\u2013335, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 1, no. 1-2, pp. 1\u2013305, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Rao-blackwellised particle filtering for dynamic bayesian networks", "author": ["A. Doucet", "N. De Freitas", "K. Murphy", "S. Russell"], "venue": "Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 2000, pp. 176\u2013 183.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Effective bayesian inference for stochastic programs", "author": ["D. Koller", "D. McAllester", "A. Pfeffer"], "venue": "AAAI/IAAI, 1997, pp. 740\u2013747.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "The principles and practice of probabilistic programming", "author": ["N.D. Goodman"], "venue": "Proceedings of the 40th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages. ACM, 2013, pp. 399\u2013402.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "The algorithm selection problem", "author": ["J.R. Rice"], "venue": "Advances in Computers, vol. 15, pp. 65\u2013118, 1976.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1976}, {"title": "Pythia-ii: a knowledge/database system for managing performance data and recommending scientific software", "author": ["E.N. Houstis", "A.C. Catlin", "J.R. Rice", "V.S. Verykios", "N. Ramakrishnan", "C.E. Houstis"], "venue": "ACM Transactions on Mathematical Software (TOMS), vol. 26, no. 2, pp. 227\u2013253, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning techniques for automatic algorithm portfolio selection", "author": ["A. Guerri", "M. Milano"], "venue": "ECAI, vol. 16, 2004, p. 475.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Satzilla: portfolio-based algorithm selection for sat", "author": ["L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Journal of Artificial Intelligence Research, pp. 565\u2013 606, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A bayesian approach for automatic algorithm selection", "author": ["H. Guo"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI03), Workshop on AI and Autonomic Computing, Acapulco, Mexico, 2003, pp. 1\u20135.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "arXiv preprint arXiv:1401.0118, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Object-oriented bayesian networks", "author": ["D. Koller", "A. Pfeffer"], "venue": "Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1997, pp. 302\u2013313.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Using combinatorial optimization within max-product belief propagation", "author": ["J. Duchi", "D. Tarlow", "G. Elidan", "D. Koller"], "venue": "Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, vol. 19. MIT Press, 2007, p. 369.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding belief propagation and its generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Exploring artificial intelligence in the new millennium, vol. 8, pp. 236\u2013239, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, no. 6, pp. 721\u2013741, 1984.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1984}, {"title": "Predicting the size of depth-first branch and bound search trees", "author": ["L.H. Lelis", "L. Otten", "R. Dechter"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence. AAAI Press, 2013, pp. 594\u2013600.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["A.T. Ihler", "J. Iii", "A.S. Willsky"], "venue": "Journal of Machine Learning Research, 2005, pp. 905\u2013936.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Variational probabilistic inference and the qmr-dt network", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Journal of artificial intelligence research, pp. 291\u2013322, 1999.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Time-dependent statistics of the ising model", "author": ["R.J. Glauber"], "venue": "Journal of mathematical physics, vol. 4, no. 2, pp. 294\u2013307, 1963.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1963}, {"title": "Netvisa: Network processing vertically integrated seismic analysis", "author": ["N.S. Arora", "S. Russell", "E. Sudderth"], "venue": "Bulletin of the Seismological Society of America, vol. 103, no. 2A, pp. 709\u2013729, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Preliminary empirical evaluation of anytime weighted and/or best-first search for map", "author": ["N. Flerova", "R. Marinescu", "R. Dechter"], "venue": "Proceedings of 4th NIPS workshop on Discrete Optimization in Machine Learning. Citeseer, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Slicing probabilistic programs", "author": ["C.-K. Hur", "A.V. Nori", "S.K. Rajamani", "S. Samuel"], "venue": "ACM SIGPLAN Notices, vol. 49, no. 6. ACM, 2014, pp. 133\u2013144.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For example, sampling methods such as Metropolis-Hastings [1] are often the \u201cgo to\u201d algorithms for reasoning on continuous models, but convergence can be painfully slow and they suffer from high variance estimates.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "Exact methods such as variable elimination [2] work well on discrete problems, but are intractable for all but the simplest models.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Recent work on generalized variational inference [3] shows promise, but still requires some hand\u2013tuning to work effectively.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "For example, one algorithm might be appropriate for a continuous portion of the model while another is used for a discrete portion; indeed, the Rao\u2013Blackwellization algorithm [4] exploits this fact.", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "PP [5, 6] provides expressive and general purpose languages to encode a probabilistic model as an executable ar X iv :1 60 6.", "startOffset": 3, "endOffset": 9}, {"referenceID": 5, "context": "PP [5, 6] provides expressive and general purpose languages to encode a probabilistic model as an executable ar X iv :1 60 6.", "startOffset": 3, "endOffset": 9}, {"referenceID": 6, "context": "Automated algorithm selection has been a long desired goal in computer science, with possibly the first formulation by Rice [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "As such, it has been applied to a variety of disciplines in the field, such as scientific computing [8], game theory [9], and artificial intelligence problems such as satisfiability [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "For example, Guo [11] uses Bayesian networks to learn and select the best algorithm to solve a problem; neither the problems or algorithms are specific, and can be applied generally to a variety of problems.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Our approach is similar in spirit to the current work on black box variational inference [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "The decomposition strategies in SFI also bears similarity to structured variable elimination (SVE) [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16].", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "There are three algorithms available: Variable elimination (VE) [2], belief propagation (BP) [15] and Gibbs sampling (GS) [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "For example, methods that estimate the running time of various inference algorithms on a model [17] can be encoded into an inference strategy, and the algorithm with the lowest estimated running time can be chosen.", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "relates with the convergence rate of BP [18], we use the amount of determinism in the model as a choice between using BP or GS.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "First, we encoded a version of the QMR medical diagnosis model [19] in SimplePPL.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "The undirected portion of the model is an Ising model [20], where each Boolean variable v in an n \u00d7 n grid has a potential to its four vertical and horizontal neighbors.", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "The last model we used is a simplified version of the Bayesian seismic monitoring system presented in [21].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": ", by using Chains) finds a better elimination order than the heuristics used to solve this NP\u2013hard problem [22].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "These results are consistent with previous work on structured VE [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24].", "startOffset": 48, "endOffset": 56}, {"referenceID": 16, "context": "Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24].", "startOffset": 48, "endOffset": 56}, {"referenceID": 23, "context": "Recent work provides a starting point for this ([23, 17]), but new methods that leverage the analyzability of PP can make the estimation of complexity more accurate [24].", "startOffset": 165, "endOffset": 169}], "year": 2016, "abstractText": "Reasoning on large and complex real\u2013world models is a computationally difficult task, yet one that is required for effective use of many AI applications. A plethora of inference algorithms have been developed that work well on specific models or only on parts of general models. Consequently, a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable. We introduce a new framework called structured factored inference (SFI) that provides the foundation for such a system. Using models encoded in a probabilistic programming language, SFI provides a sound means to decompose a model into sub\u2013models, apply an inference algorithm to each sub\u2013model, and combine the resulting information to answer a query. Our results show that SFI is nearly as accurate as exact inference yet retains the benefits of approximate inference methods.", "creator": "LaTeX with hyperref package"}}}