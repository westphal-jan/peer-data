{"id": "1606.03391", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Simple Question Answering by Attentive Convolutional Neural Network", "abstract": "this work focuses on answering single - relation factoid values over expressions. each question can acquire the answer from designated single fact of speech ( subject, predicate, object ) in freebase. this task, simple question answering ( simpleqa ), can be accomplished via a two - step pipeline : entity linking and fact selection. in fact selection, we match the subject entity in fact with the entity mention in question by a character - independent convolutional neural network ( char - cnn ), and match the entity in fact with the question by a word - level value ( word - trigger ). this work makes two main contributions. ( i ) a desirable and effective entity relationship over freebase is proposed. our entity linker outperforms the state - of - \" - art entity tree of simpleqa task. ( es ) their novel attentive maxpooling is stacked over word - cnn, so that it predicate requirement can be matched with the predicate - focused question representation more positively. experiments mean that hierarchical system sets automatic state - of - grand - art in this task.", "histories": [["v1", "Fri, 10 Jun 2016 16:54:51 GMT  (261kb,D)", "http://arxiv.org/abs/1606.03391v1", "10 pages, 2 figures"], ["v2", "Tue, 11 Oct 2016 14:32:13 GMT  (259kb,D)", "http://arxiv.org/abs/1606.03391v2", "Accepted as an oral long paper by COLING'2016"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "mo yu", "bing xiang", "bowen zhou", "hinrich sch\\\"utze"], "accepted": false, "id": "1606.03391"}, "pdf": {"name": "1606.03391.pdf", "metadata": {"source": "CRF", "title": "Simple Question Answering by Attentive Convolutional Neural Network", "authors": ["Wenpeng Yin", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de", "yum@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a\nquestion does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called \u201csimple\u201d, it is in reality not simple at all and far from solved.\nIn SimpleQA, a question, such as \u201cwhat\u2019s the hometown of Obama?\u201d, asks a single and direct topic of an entity. In this example, the entity is \u201cObama\u201d and the topic is hometown. So our task is reduced to finding the fact (subject, predicate, object) in Freebase that answers the question, which roughly means the subject and predicate are the best matches for the topical entity \u201cObama\u201d and for the topic description \u201cwhat\u2019s the hometown of\u201d, respectively. Thus, we aim to design a method that picks a fact from Freebase, so that this fact matches the question best. This procedure resembles answer selection (Yu et al., 2014) in which a system is asked to choose the best answer from a list of candidates for a given question. In this work, we formulate the SimpleQA task as a fact selection problem and the key issue lies in the system design for how to match the fact to the question.\nThe first obstacle is that Freebase has an overwhelming number of facts. A common and effective way is to first conduct entity linking of a question over Freebase, so that only a small subset of facts remain as candidates. Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). Then, facts whose subject entities match those n-grams are kept. Our first contribution in this work is to present a simple while effective\nar X\niv :1\n60 6.\n03 39\n1v 1\n[ cs\n.C L\n] 1\n0 Ju\nn 20\nentity linker specific to SimpleQA. Our entity linker first uses each word of a question to search entity vocabulary, all entities are kept if their names contain one of the query words. Then, we design three simple factors to give a raw ranking score for each entity candidate: (i) the ratio of words in the entity name that are covered by the question; (ii) the ratio of words in the question that are covered by the entity name; (iii) the position of the entity mention in the question. We choose top-N ranked entities as candidates. Our entity linker does not consider the semantics or topic of an entity; it considers only the string surface. Nevertheless, experiments show that these three factors are the basis for a top-performing entity linker for SimpleQA.\nBased on entity linking results, we consider each fact as a fact candidate that has one of the entity candidates as subject. Then our system solves the task of fact selection, i.e., matching the question with each fact candidate and picking the best match. Our system is built based on two observations. (i) Surface-form match between a subject entity and its mention in the question provides more straight-forward and effective clue than their semantic match. For example, \u201cBarack Obama\u201d matches with \u201cObama\u201d in surface-form, which acts as a fundamental indicator that the corresponding fact and the question are possibly about the same \u201cObama\u201d. (ii) Predicate in a fact is a paraphrase of the question\u2019s pattern where we define the pattern to be the topic that the question is asking about the entity and represent it as the question in which the entity mention has been replaced by a special symbol. Often the predicate corresponds to a keyword or a rephrased token of the pattern and this means we need to create a flexible model of this relationship.\nThese observations motivate us to include two kinds of convolutional neural networks (CNN, LeCun et al. (1998)) in our deep learning system. (i) A character-level CNN (char-CNN) that models the match between an entity and its mention in the question on surface-form. We consider CNN over character-level rather than the commonly-used word-level, so that the generated representation is more robust even in the presence of typos, spaces and other noise character sequences. (ii) A wordlevel CNN (word-CNN) with attentive maxpooling that learns the match of the predicate with the pat-\ntern. A Freebase predicate is a predefined relation, mostly consisting of a few words: \u201cplace of birth\u201d, \u201cnationality\u201d, \u201cauthor editor\u201d etc. In contrast, a pattern is highly variable in length and word choice, i.e., the subsequence of the question that represents the predicate in a question can take many different forms. Convolution-maxpooling slides a window over the input and identifies the best matching subsequence for a task, using a number of filters that support flexible matching. Thus, convolutionmaxpooling is an appropriate method for finding the subsequence that best matches the predicate description. We add attention to this basic operation of convolution-maxpooling. We learn the attentions of predicate over all n-gram phrases in the pattern, and finally pool phrase features by considering the feature values as well as the attentions to those features. Phrases more similar to the predicate, i.e., with higher attention values, will be selected with higher probability than other phrases to represent the pattern.1\nOur overall approach is for the entity linker to identify top-N entity candidates for a question. All facts that contain one of these entities as subject are then the fact search space for this question. CharCNN and word-CNN decompose each question-fact match into an entity-mention surface-form match and a predicate-pattern semantic match. Our approach has a simple architecture, but it outperforms the state-of-the-art, a system that has a much more complicated structure."}, {"heading": "2 Related Work", "text": "As mentioned in Section 1, factoid QA against Freebase can be categorized into single-relation QA and multi-relation QA. Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013). Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The an-\n1Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed.\nswers to the question can then be retrieved simply by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions.\nSimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours \u2013 they employ CNNs to match entity-mention and predicate-pattern. Our model differs two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word is first preprocessed into a count vector of character-trigram vocabulary, then forwarded into the CNN as input. We treat entities and mentions as character sequences. Our char-CNN for entity-mention match is more end-to-end without data preprocessing. (ii) We introduce attentive maxpooling for better predicatepattern match.\nThe latest benchmark SimpleQuestions in SimpleQA was introduced by Bordes et al. (2015). (Golub and He, 2016) is the state of the art. Bordes et al. (2015) tackle this problem by an embeddingbased QA system developed under the framework of Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015). The setting of the SimpleQA corresponds to the elementary operation of performing a single lookup in the memory. They investigate the performance of training on the combination of SimpleQuestions, WebQuestions and Reverb training sets. Golub and He (2016) propose a characterlevel attention-based encoder-decoder framework to encode the question and subsequently decode into (subject, predicate) tuple. Specifically, their encoder, modeling question on character level, is robust to unseen entities and predicates. Our model in this work is much simpler than these prior systems.\nTreating SimpleQA as fact selection is inspired by work on answer selection (e.g., Yu et al. (2014), Yin et al. (2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given\nquestion. The answer candidates in those tasks are raw text, not structured information as facts in Freebase are. We are also inspired by work that generates natural language questions given knowledge graph triples (Seyler et al., 2015; Serban et al., 2016). It hints that there exists a kind of match between natural language questions and FB triples."}, {"heading": "3 Task Definition and Data Introduction", "text": "We first describe the SimpleQuestions task (Berant et al., 2013) and Freebase (Bollacker et al., 2008).\nSimpleQuestions benchmark, a typical SimpleQA task, provides a set of single-relation questions; each question is accompanied by a ground truth fact. The object entity in the fact is the answer by default. The dataset is split into train (75,910), dev (10,845) and test (21,687) sets. While single-relation questions are easier to handle than questions with more complex and multiple relations, single-relation question answering is still far from being solved. Even in this restricted domain there are a large number of paraphrases of the same question. Thus, the problem of mapping from a question to a particular predicate and entity in Freebase is hard.\nFreebase is a structured knowledge base in which entities are connected by predefined predicates or \u201crelations\u201d. All predicates are directional, connecting from the subject to the object. A triple (subject, predicate, object), denoted as (h, p, t), describes a fact; e.g., (U.S. Route 2, major cities, Kalispell) refers to the fact that U.S. Route 2 runs through the city of Kalispell. SimpleQuestions provides two subsets of Freebase, FB2M and FB5M; see Table 1.\nThe task assumes that single-relation questions can be answered by querying a knowledge base such as Freebase with a single subject and predicate argument. Hence, only the tuple (h, p) is used to match the question. The evaluation metric is accuracy. Only a fact that matches the ground truth in both subject and predicate is counted as correct."}, {"heading": "4 Entity Linking and Mention Detection", "text": "Given a question, the entity linker provides a set of top-N entity candidates. The input of our deep learning model are (subject, predicate) and (mention, pattern) pairs. Thus, given a question, two problems we have to solve are (i) identifying candidate entities in Freebase that the question refers to and (ii) identifying the span (i.e., mention) in the question that refers to the entity.\nWe perform entity linking by deriving the longest consecutive common subsequence (LCCS) between a question and entity candidates and refer to it as \u03c3. Given a question q and all entity names from Freebase, we perform the following three steps.\n(i) Lowercase/tokenize entity names and question (ii) Use each component word of q to retrieve entities whose names contain this word. We refer to the set of all these entities as Ce.\n(iii) For each entity candidate e in Ce, compute its LCCS \u03c3 with the question q. Let p be the position of the last token of \u03c3 in q. Compute a = |\u03c3|/|q|, b = |\u03c3|/|e| and c = p/|q| where | \u00b7 | is length in words. Finally, entity candidate e is scored by the weighted sum se = \u03b1a+ \u03b2b+ c. Parameters \u03b1 and\n\u03b2 are tuned on dev. Top-N ranked entities are kept for each question.\nDiscussion. Factor a = |\u03c3|/|q| means we prefer candidates that cover more consecutive words of the question. Factor b = |\u03c3|/|e| means that we prefer the candidates that have more consecutive words appearing in the question. Factor c = p/|q| means that we prefer candidates that appear close to the end of the question; this is based on the observation that most entity mentions are far from the beginning of the question. Despite the simplicity of our entity linker, it outperforms other state-of-theart entity linkers of this SimpleQuestions task by a big margin. Besides, this entity linker is unsupervised and runs fast. We will show its promise and investigate the individual contributions of the three factors in experiments.\nThis work considers two perspectives for mention detection, Freebase-dependent and Freebaseindependent. Pros and cons will be discussed in the experimental section.\nBoth approaches return a mention (i.e., a span) in the question. We then convert the question into the tuple (mention, pattern) where pattern is created by\nreplacing the mention in the question with <e>. FB-dependent Mention Detection. Each question q is provided top-N entity candidates from Freebase by entity linker.\nWe first compute the LCCS \u03c3 on word level between q and entity e. If the entity is longer than \u03c3 and has l (resp. r) words on the left (resp. right) of \u03c3, then we extend \u03c3 in the question by l left (resp. r right) words and select this subsequence as the candidate mention. For example, entity \u201cU.S. Route 2\u201d and question \u201cwhat major cities does us route 2 run through\u201d have LCCS \u03c3 \u201croute 2\u201d. The FB entity \u201cU.S. Route 2\u201d has one extra word \u201cu.s.\u201d on the left of \u03c3, so we extend \u03c3 by one left word and the candidate mention is \u201cus route 2\u201d. We do this so that the mention has the same word size as the entity string.2\nIn rare cases that the LCCS on the word level has length 0, we treat both entity string and question as character sequence, then compute LCCS \u03c3 on character level. Finally, mention is formed by expanding \u03c3 on both sides up to a space or the text boundary.\nThis approach to mention detection usually produces more than one (mention, pattern) pair.\nFB-independent Mention Detection. In this case, we take a named entity recognition (NER) approach. We tried Stanford NER (Finkel et al., 2005), but it did not work well for our noisy text. We instead use an approach based on part-of-speech (POS) tagging. We observed that a single POS tagger cannot be trusted as questions contain confusing noise tokens. We thus resort to three top-performing POS taggers: (i) Stanford POS tagger3 (Toutanova et al., 2003); (ii) FLORS4 (Schnabel and Schu\u0308tze, 2014; Yin et al., 2015), the state-of-the-art POS tagger in domain adaptation scenario (its strength in domain adaptation applies to the QA task as the question domains can not be estimated); and (iii) NLTK default POS tagger.5 For each word, we choose the majority tag or, if there is none, Stanford tagger\u2019s prediction.\nFor a question, we identify all subsequences that have POS tags only in {NN, NNS, NNP, NNPS, FW, JJ, JJR, CD}. We rank them by \u03b3|\u03c3|/|q| + p/|q| (similar to our entity linker, but without factor b)\n2Only using LCCS as mention performed worse. 3http://nlp.stanford.edu/software/tagger.shtml 4http://cistern.cis.lmu.de/flors/ 5http://www.nltk.org/book/ch05.html\nand select the top-scoring subsequence as the FBindependent mention.\nTo tune this approach, we count as a success an FB-independent mention that has a non-zero LCCS with its gold entity. We then tune \u03b3 = 4.5 on dev, giving us an accuracy of 84.5% on dev.\nThis approach to mention detection produces one single (mention, pattern) pair."}, {"heading": "5 Fact Selection", "text": "Entity linker provides top-N entity candidates for each question. All facts having those entities as subject form a fact pool, then we build the system to seek the best.\nOur whole system is depicted in Figure 1(a). It consists of match from two aspects: (i) a CNN on character level (char-CNN) to detect the similarity of entity string and the mention string in surface-form (the left column); (ii) a CNN with attentive maxpooling (AMP) in word level (word-AMPCNN) to detect if the predicate is a paraphrase of the pattern.\nWord-AMPCNN is motivated by the observation that the FB predicate name is short and fixed whereas the corresponding pattern in the question is highly variable in length and word choice. Our hypothesis is that the predicate-pattern match is best done based on keywords in the pattern (and perhaps humans also do something similar) and that the CNN therefore should identify helpful keywords. Traditional maxpooling treats all n-grams equally. In this work, we propose attentive maxpooling (AMP). AMP gives higher weights to ngrams that better match the predicate. As a result, the predicate-pattern match computed by the CNN is more likely to be correct.\nNext, we introduce the CNN combined with maxpooling for both char-CNN and word-CNN, then present AMPCNN. Figure 1(b) shows the common framework of char-CNN and word-CNN; only input granularity and maxpooling are different."}, {"heading": "5.1 Framework of CNN-Maxpooling", "text": "Both char-CNN and word-CNN have two weightsharing CNNs, as they model two pieces of text. In what follows, we use \u201centry\u201d as a general term for both character and word.\nThe input layer is a sequence of entries of length\nswhere each entry is represented by a d-dimensional randomly initialized embedding; thus the sequence is represented as a feature map of dimensionality d\u00d7 s. Figure 1(b) shows the input layer as the lower rectangle with multiple columns.\nConvolution Layer is used for representation learning from sliding n-grams. For an input sequence with s entries: v1, v2, . . . , vs, let vector ci \u2208 Rnd be the concatenated embeddings of n entries vi\u2212n+1, . . . , vi where n is the filter width and 0 < i < s + n. Embeddings for vi, i < 1 or i > s, are zero padded. We then generate the representation pi \u2208 Rd for the n-gram vi\u2212n+1, . . . , vi using the convolution weights W \u2208 Rd\u00d7nd:\npi = tanh(W \u00b7 ci + b) (1) where bias b \u2208 Rd.\nMaxpooling. All n-gram representations pi (i = 1 \u00b7 \u00b7 \u00b7 s + n \u2212 1) are used to generate the representation of input sequence s by maxpooling: sj = max(p1j ,p2j , \u00b7 \u00b7 \u00b7) (j = 1, \u00b7 \u00b7 \u00b7 , d)."}, {"heading": "5.2 AMPCNN: CNN-Attentive-Maxpooling", "text": "Figure 2 shows TMP (Traditional MaxPooling) and AMP (Attentive MaxPooling) as we apply them to\nSimpleQA. Recall that we use standard CNNs to produce (i) the predicate representation vp (see Figure 1(a)) and (ii) a feature map of the pattern, i.e., a matrix with columns denoting n-gram representations (shown in Figure 1(b), the matrix below \u201ccolwise (attentive) maxpooling\u201d). In Figure 2, we refer to the feature map as Fpattern and to the predicate representation as vp.\nTMPCNN, i.e., traditional maxpooling, outputs the vector shown as vTMP; the same vTMP is produced for different vp. The basic idea of AMPCNN is to let the predicate vp bias the selection and weighting of subsequences of the question to compute the representation of the pattern. The first step in doing that is to compute similarity scores s between the predicate representation vp and each column vector of Fpattern:\nsi = cos(vp,Fpattern[:, i]) (2)\nThese cosines are then transformed into decay values by setting negative values to 0 (negatively correlated column vectors are likely to be unrelated to the predicate) and normalizing the positive values by dividing them by the largest cosine (.97 in this case), so that the largest decay value is 1.0. This is shown as \u201cdecay\u201d and s in the figure. Finally, we compute the reweighted feature map Fdecay as follows:\nFdecay[:, i] = Fpattern[:, i] \u2217 si (3)\nIn Fdecay, the matrix with four green values, we can locate the maximal values in each dimension. Notice that they are not the true features by CNN any more, instead, they convey the original feature values as well as their importance to be considered. In Fdecay, we can see that the maximal values in each dimension mostly come from the first column and the third column which have relatively higher similarity scores 0.97 and 0.76 respectively to the predicate. We use the coordinates of those maximal values to retrieve features from Fpattern as a final pattern representation vAMP, the blue column vector6.\nIn summary, TMP has no notion of context. The novelty of AMP is that it is guided by attentions from the context, in this case attentions from the predicate. In contrast to TMP, we expect AMP to mainly extract features that come from n-grams that\n6We tried max-pooling over Fdecay as vAMP directly, but much worse performance was observed.\nare related to the predicate."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Training Setup", "text": "Our fact pool consists of all facts whose subject entity is in the top-N entity candidates. For train, we sample 99 negative facts for each ground truth fact; for dev and test, all fact candidates are kept.\nFigure 1(a) shows two-way match between a tuple t and a question q: entity-mention match by charCNN (score me), predicate-pattern match by wordAMPCNN (score mr). The overall ranking score of the pair is st(q, t) = me +mr + se where se is the entity ranking score in entity linking phase.\nOur objective is to minimize ranking loss:\nl(q, t+, t\u2212) = max(0, \u03bb+st(q, t \u2212)\u2212st(q, t+)) (4)\nwhere \u03bb is a constant. We build word and character vocabularies on train. OOV words and characters from dev and test are mapped to an OOV index. Then, words (resp. characters) are randomly initialized into dworddimensional (resp. dchar-dimensional) embeddings. The output dimensionality in convolution, i.e., Equation 1, is the same as input dimensionality. We employ Adagrad (Duchi et al., 2011), L2 regularization and diversity regularization (Xie et al., 2015). Hyperparameters (Table 2) are tuned on dev."}, {"heading": "6.2 Entity Linking", "text": "In Table 3, we compare our entity linker with the state-of-the-art entity linker (Golub and He, 2016) in this SimpleQA task. Golub and He (2016) report the coverage of ground truth by top-N cases (N \u2208 {1, 10, 20, 50, 100}). In addition, they explore a reranking algorithm to refine the entity ranking list. We do not have reranking step in this work.\nTable 3 shows overall performance of our entity linker and performance without factor a, b or c (-a, -b, -c). Our entity linker outperforms the baseline\u2019s raw results by big margins and is 2\u20133 percent above their reranked scores. This shows the outstanding\nperformance of our entity linker despite its simplicity. The table also shows that all three factors (a, b, c) matter. Observations: (i) Each factor matters more when N is smaller. This makes sense because whenN reaches the entity vocabulary size, all methods will have coverage 100%. (ii) The positionrelated factor c has less influence. From top1 to top100, its contribution decreases from 4.3 to .9. Our entity linker still outperforms the reranked baseline for N \u2265 20. (iii) Factor a is dominant for small N , presumably because it chooses the longer one when two candidates exist, which is critical for small N . (iv) Factor b plays a more consistent role across different N ."}, {"heading": "6.3 SimpleQuestions", "text": "Table 4 compares AMPCNN with two baselines. (i) MemNN (Bordes et al., 2015), an implementation of memory network for SimpleQuestions task. (ii) Encoder-Decoder (Golub and He, 2016), the state-of-the-art system in this task, a character-level, attention-based encoder-decoder LSTM (Hochreiter and Schmidhuber, 1997) model. Both MemNN and Encoder-Decoder use multiple data resources (WebQuestion, SimpleQuestions and Paraphrases (Fader et al., 2014)) and they are ensembles with improvements over non-ensemble single systems from 62.2 to 63.9 on FB5M (Bordes et al., 2015) and from 65.9 to 66.2 on FB2M (Golub and He, 2016). Our system outperforms both baselines significantly even though it uses only a single dataset (SimpleQuestions) and uses no ensemble.\nWe report results for FB-dependent and FBindependent mention detection. Furthermore, we compare AMPCNN to TMPCNN, i.e., we remove attention and representations for the predicatepattern match are computed without attention. We choose top-20 (i.e., N = 20) entities returned by entity linker. Table 4 shows that AMPCNN with\nFB-dependent mention detection has optimal performance: 2.1 and 1.5 points above state-of-the-art for FB2M and FB5M, respectively. Performance on FB5M is slightly lower than on FB2M, which should be mainly due to the lower coverage for entity linking on FB5M \u2013 about 2% below that on FB2M. In addition, our CNN can still outperform baselines significantly even if the attention mechanism is removed (TMPCNN result). This hints that CNN is promising for SimpleQA.\nInterestingly, FB-independent makes the performance worse by about 5%. This can be attributed to the fact that FB-independent\u2019s accuracy of 84.5% (Section 4) is not good enough for a pipeline system. FB-dependent can make the probability of mention detection for ground truth entities close to 100% recall at the cost of extracting false positive mentions (for subjects of negative facts). But the mismatch of predicates in negative facts and patterns is expected to diminish the bad effect dramatically.\nDespite the worse (albeit still better than MemNN on FB2M) performance of FB-independent, it is more intuitive than FB-dependent because each question should have one mention in SimpleQuestions and this mention should not vary in dependence of entity candidates that were falsely linked. What we need in the future is a more advanced approach for mention detection or NER.7"}, {"heading": "6.4 Effect of Attentive Maxpooling (AMP)", "text": "We compare AMP (one main contribution of ours) with three CNN attention mechanisms that are representative of related work in modeling two pieces of text: (i) HABCNN: Hierarchical attention-based CNN (Yin et al., 2016a); (ii) ABCNN: Attentionbased CNN (Yin et al., 2016b); (iii) APCNN: CNN with attentive pooling (Santos et al., 2016).\n7Stanford NER (Finkel et al., 2005) performed worse.\nSince attentive matching of predicate-pattern is only one part of our jointly trained system, it is hard to judge whether or not an attentive CNN performs better than alternatives. We therefore create a relation classification (RC) subtask to compare AMP with baseline schemes. RC task is created based on SimpleQuestions: label each question (converted into a pattern first) with the ground truth predicate; all other predicates of the gold subject entity are labeled as negative. The resulting datasets have sizes 72,239 (train), 10,310 (dev) and 20,610 (test).\nIn the three baselines, two pieces of text apply attention to each other. We adapt them into oneway attention (OWA) as AMP does in this work: fix predicate representation, and use it to guide the learning of pattern representation. To be specific, ABCNN first gets predicate representation by mean pooling, then uses this representation to derive similarity scores of each n-gram in pattern as attention scores, finally averages all n-gram embeddings weighted by attentions as pattern representation. HABCNN first gets predicate representation by max pooling, then computes attention scores the same way as ABCNN, finally does maxpooling over representations of top-k similar n-grams. APCNN is similar to ABCNN except that the similarity scores are computed by a nonlinear bilinear form.\nTable 5 shows that AMPCNN performs well on relation classification, outperforming the best baseline APCNN by 0.8%. AMPCNN also has fewer parameters and runs faster than APCNN."}, {"heading": "7 Conclusion", "text": "In this work, we explored CNNs for the SimpleQuestions task. We made two main contributions. (i) A simple and effective entity linker that brings higher coverage of ground truth entities. (ii) An attentive maxpooling stacked above convolution layer that models the relationship between predicate and question pattern more effectively. Our experiments\nshow outstanding performance on both simpleQA and relation classification."}], "references": [{"title": "More accurate question answering on freebase", "author": ["Bast", "Haussmann2015] Hannah Bast", "Elmar Haussmann"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Bast et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of SIGMOD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Question answering over freebase with multi-column convolutional neural networks", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader et al.2014] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of KDD,", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Incorporating nonlocal information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Character-level question answering with attention", "author": ["Golub", "He2016] David Golub", "Xiaodong He"], "venue": "arXiv preprint arXiv:1604.00727", "citeRegEx": "Golub et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Golub et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1998] Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Attentive pooling networks. arXiv preprint arXiv:1602.03609", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2016}, {"title": "FLORS: Fast and simple domain adaptation for part-of-speech tagging", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Alberto Garc\u0131\u0301aDur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generating quiz questions from knowledge graphs", "author": ["Mohamed Yahya", "Klaus Berberich"], "venue": "In Proceedings of WWW,", "citeRegEx": "Seyler et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seyler et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "On the generalization error bounds of neural networks under diversity-inducing mutual angular regularization", "author": ["Xie et al.2015] Pengtao Xie", "Yuntian Deng", "Eric Xing"], "venue": "arXiv preprint arXiv:1511.07110", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Enhancing freebase question answering using textual evidence", "author": ["Xu et al.2016] Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao"], "venue": "arXiv preprint arXiv:1603.00957", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Robust question answering over the web of linked data", "author": ["Yahya et al.2013] Mohamed Yahya", "Klaus Berberich", "Shady Elbassuoni", "Gerhard Weikum"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Yahya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yahya et al\\.", "year": 2013}, {"title": "Information extraction over structured data: Question answering with Freebase", "author": ["Yao", "Van Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Freebase QA: Information extraction or semantic parsing", "author": ["Yao et al.2014] X. Yao", "J. Berant", "B. Van Durme"], "venue": "In Proceedings of ACL Workshop on Semantic Parsing,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Lean question answering over freebase from scratch", "author": ["Xuchen Yao"], "venue": "In Proceedings of NAACLHLT,", "citeRegEx": "Yao.,? \\Q2015\\E", "shortCiteRegEx": "Yao.", "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["Yih et al.2014] Wen-tau Yih", "Xiaodong He", "Christopher Meek"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih et al.2015] Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Online updating of word representations for part-of-speech tagging", "author": ["Yin et al.2015] Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "2016a. Attention-based convolutional neural network for machine comprehension", "author": ["Yin et al.2016a] Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "Proceedings of NAACL Human-Computer QA Workshop", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "2016b. ABCNN: Attentionbased convolutional neural network for modeling sentence pairs. TACL", "author": ["Yin et al.2016b] Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Deep learning for answer sentence selection", "author": ["Yu et al.2014] Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "Proceedings of ICLR workshop", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.", "startOffset": 70, "endOffset": 94}, {"referenceID": 4, "context": "This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts.", "startOffset": 94, "endOffset": 115}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al.", "startOffset": 71, "endOffset": 161}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al.", "startOffset": 71, "endOffset": 180}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al.", "startOffset": 71, "endOffset": 207}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al.", "startOffset": 71, "endOffset": 226}, {"referenceID": 2, "context": "Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)).", "startOffset": 71, "endOffset": 244}, {"referenceID": 8, "context": "munity QA sites (Fader et al., 2013) and in search query logs.", "startOffset": 16, "endOffset": 36}, {"referenceID": 32, "context": "This procedure resembles answer selection (Yu et al., 2014) in which a system is asked", "startOffset": 42, "endOffset": 59}, {"referenceID": 4, "context": "Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016).", "startOffset": 98, "endOffset": 139}, {"referenceID": 14, "context": "These observations motivate us to include two kinds of convolutional neural networks (CNN, LeCun et al. (1998)) in our deep learning system.", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013).", "startOffset": 120, "endOffset": 141}, {"referenceID": 1, "context": "Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.", "startOffset": 33, "endOffset": 117}, {"referenceID": 23, "context": "Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.", "startOffset": 33, "endOffset": 117}, {"referenceID": 28, "context": "Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.", "startOffset": 33, "endOffset": 117}, {"referenceID": 3, "context": "2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016).", "startOffset": 74, "endOffset": 131}, {"referenceID": 5, "context": "2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016).", "startOffset": 74, "endOffset": 131}, {"referenceID": 22, "context": "2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016).", "startOffset": 74, "endOffset": 131}, {"referenceID": 8, "context": "SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 7, "context": ", 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011).", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "(i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and", "startOffset": 73, "endOffset": 93}, {"referenceID": 7, "context": "SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours \u2013 they employ CNNs to match entity-mention and predicate-pattern.", "startOffset": 36, "endOffset": 150}, {"referenceID": 3, "context": "pleQA was introduced by Bordes et al. (2015). (Golub and He, 2016) is the state of the art.", "startOffset": 24, "endOffset": 45}, {"referenceID": 3, "context": "pleQA was introduced by Bordes et al. (2015). (Golub and He, 2016) is the state of the art. Bordes et al. (2015) tackle this problem by an embeddingbased QA system developed under the framework of Memory Networks (Weston et al.", "startOffset": 24, "endOffset": 113}, {"referenceID": 32, "context": ", Yu et al. (2014), Yin", "startOffset": 2, "endOffset": 19}, {"referenceID": 15, "context": "(2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given FB2M FB5M", "startOffset": 9, "endOffset": 30}, {"referenceID": 18, "context": "We are also inspired by work that generates natural language questions given knowledge graph triples (Seyler et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 17, "context": "We are also inspired by work that generates natural language questions given knowledge graph triples (Seyler et al., 2015; Serban et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 2, "context": ", 2013) and Freebase (Bollacker et al., 2008).", "startOffset": 21, "endOffset": 45}, {"referenceID": 10, "context": "We tried Stanford NER (Finkel et al., 2005), but it did not work well for our noisy text.", "startOffset": 22, "endOffset": 43}, {"referenceID": 20, "context": "We thus resort to three top-performing POS taggers: (i) Stanford POS tagger3 (Toutanova et al., 2003); (ii) FLORS4 (Schnabel and Sch\u00fctze, 2014; Yin et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 29, "context": ", 2003); (ii) FLORS4 (Schnabel and Sch\u00fctze, 2014; Yin et al., 2015), the state-of-the-art POS tagger in domain adaptation scenario (its strength in do-", "startOffset": 21, "endOffset": 67}, {"referenceID": 6, "context": "We employ Adagrad (Duchi et al., 2011), L2 regularization and diversity regularization (Xie et al.", "startOffset": 18, "endOffset": 38}, {"referenceID": 21, "context": ", 2011), L2 regularization and diversity regularization (Xie et al., 2015).", "startOffset": 56, "endOffset": 74}, {"referenceID": 4, "context": "(i) MemNN (Bordes et al., 2015), an implementation of memory network for SimpleQuestions task.", "startOffset": 10, "endOffset": 31}, {"referenceID": 9, "context": "Both MemNN and Encoder-Decoder use multiple data resources (WebQuestion, SimpleQuestions and Paraphrases (Fader et al., 2014)) and they are ensembles with improvements over non-ensemble single systems from 62.", "startOffset": 105, "endOffset": 125}, {"referenceID": 4, "context": "9 on FB5M (Bordes et al., 2015) and from 65.", "startOffset": 10, "endOffset": 31}, {"referenceID": 15, "context": ", 2016b); (iii) APCNN: CNN with attentive pooling (Santos et al., 2016).", "startOffset": 50, "endOffset": 71}, {"referenceID": 10, "context": "Stanford NER (Finkel et al., 2005) performed worse.", "startOffset": 13, "endOffset": 34}, {"referenceID": 15, "context": "902 0 OWA-APCNN (Santos et al., 2016) .", "startOffset": 16, "endOffset": 37}], "year": 2016, "abstractText": "This work focuses on answering singlerelation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a twostep pipeline: entity linking and fact selection. In fact selection, we match the subject entity in fact with the entity mention in question by a character-level convolutional neural network (char-CNN), and match the predicate in fact with the question by a word-level CNN (wordCNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker of SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task.", "creator": "LaTeX with hyperref package"}}}