{"id": "1606.03676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "External Lexical Information for Multilingual Part-of-Speech Tagging", "abstract": "morphosyntactic lexicons and word vector representations have both proven detrimental for improving the accuracy of statistical part - of - speech taggers. simultaneously we compare the performances of four systems : datasets covering differentiated languages, two comprising these systems being feature - infused ( memms and crfs ) and two of them being neural - based ( pseudo - lstms ). we show that, on performance, all four approaches perform similarly and satisfy state - of - the - art results. yet better performances are obtained matching our feature - based models producing lexically richer datasets ( e. mac. for morphologically proficient languages ), whereas neural - based results are higher on datasets with less lexical structure ( cr. g. for english ). these conclusions hold in particular for contrasting memm environments relying on our system melt, which follow from newly studied features. literature shows that, under certain conditions, feature - based approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.", "histories": [["v1", "Sun, 12 Jun 2016 08:06:55 GMT  (723kb,D)", "https://arxiv.org/abs/1606.03676v1", null], ["v2", "Tue, 9 Aug 2016 08:41:46 GMT  (723kb,D)", "http://arxiv.org/abs/1606.03676v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["beno\\^it sagot"], "accepted": false, "id": "1606.03676"}, "pdf": {"name": "1606.03676.pdf", "metadata": {"source": "CRF", "title": "External Lexical Information for Multilingual Part-of-Speech Tagging", "authors": ["Beno\u00eet Sagot"], "emails": [], "sections": [{"heading": null, "text": "IS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 89\n24 --\nFR +E\nN G\nRESEARCH REPORT N\u00b0 8924 June 2016\nProject-Teams ALPAGE"}, {"heading": "External Lexical", "text": ""}, {"heading": "Information for", "text": ""}, {"heading": "Multilingual", "text": ""}, {"heading": "Part-of-Speech Tagging", "text": "Beno\u00eet Sagot\nar X\niv :1\n60 6.\n03 67\n6v 2\n[ cs\n.C L\n] 9\nA ug\n2 01\n6\nRESEARCH CENTRE PARIS \u2013 ROCQUENCOURT\nDomaine de Voluceau, - Rocquencourt B.P. 105 - 78153 Le Chesnay Cedex\nExternal Lexical Information for Multilingual Part-of-Speech Tagging\nBeno\u00eet Sagot\nProject-Teams ALPAGE\nResearch Report n\u00b0 8924 \u2014 June 2016 \u2014 15 pages\nAbstract: Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, featurebased approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods.\nKey-words: Part-of-Speech Tagging, Feature-based models, Neural models, MEMM, CRF, biLSTM, Multilingual Analysis\nUtilisation d\u2019informations lexicales externes pour l\u2019annotation multilingue en parties du discours\nR\u00e9sum\u00e9 : Les lexiques morphosyntaxiques et les repr\u00e9sentations vectorielles des mots ont chacun montr\u00e9 leur utilit\u00e9 pour am\u00e9liorer la pr\u00e9cision d\u2019\u00e9tiqueteurs morphosyntaxiques statistiques. Nous comparons ici les performances de quatre syst\u00e8mes sur des jeux de donn\u00e9es couvrant 16 langues, deux de ces syst\u00e8mes reposant sur des traits (MEMM et CRF) et deux autres sur des approches neuronales (bi-LSTM). Nous montrons qu\u2019en moyenne les quatre approches obtiennent des performances similaires de niveau \u00e9tat-de-l\u2019art. N\u00e9anmoins, nos mod\u00e8les reposant sur des traits ont de meilleures performances sur les jeux de donn\u00e9es lexicalement plus riches (par exemple sur des langues \u00e0 morphologie riche), alors que les r\u00e9sultats obtenus par les approches neuronales sont meilleurs sur les jeux de donn\u00e9es dont la variabilit\u00e9 lexicale est moindre (par exemple pour l\u2019anglais). Ces conclusions sont vraies en particulier pour nos mod\u00e8les de type MEMM faisant usage de notre syst\u00e8me MElt, qui s\u2019appuie sur un jeu de traits renouvel\u00e9. Ceci montre que, sous certaines conditions, les approches par traits enrichies par des lexiques morphosyntaxiques sont comp\u00e9titifs par rapport aux approches neuronales.\nMots-cl\u00e9s : \u00c9tiquetage en partie du discours, Mod\u00e8les reposant sur des traits, Mod\u00e8les neuronaux, MEMM, CRF, bi-LSTM, Analyse multilingue"}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 3", "text": ""}, {"heading": "1 Introduction", "text": "Part-of-speech tagging is now a classic task in natural language processing, for which many systems have been developed or adapted for a large variety of languages. Its aim is to associate each \u201cword\u201d with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, etc.).\nThe use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). With such machine learning algorithms, it is possible to build PoS taggers for any language, provided adequate training data is available.\nAs a complement to annotated corpora, it has previously been shown that external lexicons are valuable sources of information, in particular morphosyntactic lexicons, which provide a large inventory of (word, PoS) pairs. Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Haji\u010d, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupa\u0142a et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012).\nIn recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015). Such representations, which are generally extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level and/or character-level long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016).\nBoth approaches to representing lexical properties and to integrating them into a PoS tagger improve tagging results. Yet they rely on resources of different natures. The main advantage of word vectors is that they are built in an unsupervised way, only requiring large amounts of raw textual data. They also encode finer-grained information than usual morphosyntactic lexicons, most of which do not include any quantitative data, not even simple frequency information. Conversely, lexical resources often provide information about scarcely attested words, for which corpus-based approaches such as word vector representations are of limited relevance. Moreover, morphological or morphosyntactic lexicons already exist for a number of languages, including less-resourced langauges for which it might be difficult to obtain the large amounts of raw data necessary to extract word vector representations.\nOur main goal is therefore to compare the respective impact of external lexicons and word vector representations on the accuracy of PoS models. This question has already been investigated for 6 languages by M\u00fcller and Sch\u00fctze (2015) using the state-of-the-art CRF-based tagging system MarMoT. The authors found that their best-performing word-vector-based PoS tagging models outperform their models that rely on morphosyntactic resources (lexicons or morphological analysers). In this paper, we report on larger comparison, carried out in a larger multilingual setting and comparing different tagging models. Using different 16 datasets, we compare the performances of two feature-based models enriched with external lexicons and of two LSTMbased models enriched with word vector representations. A secondary goal of our work is to compare the relative improvements linked to the use of external lexical information in the two\nRR n\u00b0 8924"}, {"heading": "4 Sagot", "text": "feature-based models, which use different models (MEMM vs. CRF) and feature sets. More specifically, our starting point is the MElt system (Denis and Sagot, 2012), an MEMM tagging system. We first briefly describe this system and the way we adapted it by integrating our own set of corpus-based and lexical features. We then introduce the tagging models we have trained for 16 different languages using our adapted version of MElt. These models are trained on the Universal Dependencies (v1.2) corpus set (Nivre and al., 2015), complemented by morphosyntactic lexicons. We compare the accuracy of our models with the scores obtained by the CRF-based system MarMoT (M\u00fcller et al., 2013; M\u00fcller and Sch\u00fctze, 2015), retrained on the same corpora and the same external morphosyntactic lexicons. We also compare our results to those obtained by the best bidirectional LSTM models described by Plank et al. (2016), which both make use of Polyglot word vector representations published by Al-Rfou et al. (2013). We will show that an optimised enrichment of feature-based models with morphosyntactic lexicon results in significant accuracy gains. The macro-averaged accuracy of our enriched MElt models is above that of enriched MarMoT models and virtually identical to that of LSTMs enriched with word vector representations. More precisely, per-language results indicate that lexicons provide more useful information for languages with a high lexical variability (such as morphologically rich languages), whereas word vectors are more informative for languages with a lower lexical variability (such as English)."}, {"heading": "2 MElt", "text": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996). The basic set of features used by MElt is given in (Denis and Sagot, 2012). It is a superset of the feature sets used by Ratnaparkhi (1996) and Toutanova and Manning (2000) and includes both local standard features (for example the current word itself and its prefixes and suffixes of length 1 to 4) and contextual standard features (for example the tag just assigned to the preceding word). In particular, with respect to Ratnaparkhi\u2019s feature set, MElt\u2019s basic feature set lifts the restriction that local standard features used to analyse the internal composition of the current word should only apply to rare words.\nOne of the advantages of feature-based models such as MEMMs and CRFs is that complementary information can be easily added in the form of additional features. This was investigated for instance by K\u00fcbler et al. (2010), whose best-performing model for PoS tagging dialogues was obtained with a version of MElt extended with dialogue-specific features. Yet the motivation of MElt\u2019s developers was first and foremost to investigate the best way to integrate lexical information extracted from large-scale morphosyntactic lexical resources into their models, on top of the training data (Denis and Sagot, 2012). They showed that performances are better when this external lexical information is integrated in the form of additional lexical features than when the external lexicon is used as constraints at tagging time.1 These lexical features can also be divided into local lexical features (for example the list of possible tags known to the external lexicon for the current word) and contextual lexical features (for example the list of possible tags known to the external lexicon for surrounding words). In particular, lexical contextual features provide a means to model the right context of the current word, made of words that have not yet been tagged by the system but for which the lexicon often provides a list of possible tags. Moreover, tagging accuracy for out-of-vocabulary (OOV) words is improved, as a result of the fact that words unknown to the training corpus might be known to the external lexicon.\n1For instance by constraining the tagger in such a way that words known to the lexicon can only be associated with tags provided by the lexicon.\nInria"}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 5", "text": ""}, {"heading": "Local standard features", "text": ""}, {"heading": "Contextual standard features", "text": ""}, {"heading": "Local lexical features", "text": "RR n\u00b0 8924"}, {"heading": "6 Sagot", "text": "Despite a few experiments published with MElt on languages other than French (Denis and Sagot, 2012; Le Roux et al., 2012; Seddah et al., 2013a), the original feature set used by MElt (standard and lexical features) was designed and tested mostly on this language, by building and evaluating tagging models on a variant of the French TreeBank. Since our goal was to carry out experiments in a multilingual setting, we have decided to design our own set of features, using the standard MElt features as a starting point. With respect to the original MElt feature set, we have added new ones, such as prefixes and suffixes of the following word, as well as a hybrid contextual feature obtained by concatenating the tag predicted for the preceding word and the tag(s) provided by the external lexicon for the following word.\nIn order to select the best performing feature set, we carried out a series of experiments using the multilingual dataset provided during the SPMRL parsing shared task (Seddah et al., 2013b). This included discarding useless or harmful features and selecting the maximal length of the prefixes and suffixes to be used as features, both for the current word and for the following word.2\nWe incorporated in MElt the best performing feature set, described in Table 1. All models discussed in this paper are based on this feature set."}, {"heading": "3 Datasets", "text": ""}, {"heading": "3.1 Corpora", "text": "We carried out our experiments on the Universal Dependencies v1.2 treebanks (Nivre and al., 2015), hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags,3 which is an extension of the tagset proposed by Petrov et al. (2012).\n2During these tuning experiments, we used development sets for comparing feature sets, without evaluating any of our models on test sets.\n3http://universaldependencies.org/u/pos/all.html\nInria"}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 7", "text": "As our goal is to study the impact of lexical information for PoS tagging, we have restricted our experiments to UD1.2 corpora that cover languages for which we have morphosyntactic lexicons at our disposal, and for which Plank et al. (2016) provide results.4 We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. Although this language list contains only one non-Indo-European (Indonesian), four major IndoEuropean sub-families are represented (Germanic, Romance, Slavic, Indo-Iranian). Overall, the 16 languages considered in our experiments are typologically, morphologically and syntactically fairly diverse."}, {"heading": "3.2 Lexicons", "text": "We generate our external lexicons using the set of source lexicons listed in Table 2. Since external lexical information is exploited via features, there is no need for the external lexicons and the annotated corpora to use the same PoS inventory. Therefore, for each language, we simply extracted from the corresponding lexicon the PoS of each word based on its morphological tags, by removing all information provided except for its coarsest-level category.5 We also added entries for punctuations when the source lexicons did not contain any.\nWe also performed experiments in which we retained the full original tags provided by the lexicons, with all morphological features included. On average, results were slightly better than those presented in the paper, although not statistically significantly. Moreover, the granularity of tag inventories in the lexicons is diverse, which makes it difficult to draw general conclusions about results based on full tags. This is why we only report results based on (coarse) PoS extracted from the original lexicons."}, {"heading": "4 Experiments and results", "text": ""}, {"heading": "4.1 Baseline models", "text": "In order to assess the respective contributions of external lexicons and word vector representations, we first compared the results of the three above-mentioned systems when trained without such additional lexical information. Table 3 provides the results of MElt and MarMoT retrained on UD1.2 corpora, together with the results publised on the same corpora by Plank et al. (2016), using their best model not enhanced by external word vector representations \u2014i.e. the model they call ~w +~c, which is a bidirectional LSTM that combines both word and character embeddings.\nThese results show that Plank et al.\u2019s (2016) bi-LSTM performs extremely well, surpassed by MarMoT on only 3 out of 16 datasets (Czech, French and Italian), and by MElt only once (Indonesian)."}, {"heading": "4.2 Models enriched with external lexical information", "text": "Table 4 provides the results of four systems enriched with lexical information. The feature-based systems MElt and MarMoT, respectively based on MEMMs and CRFs, are extended with the lexical information provided by our morphosyntactic lexicons. This extension takes the form of additional features, as described in Section 2 for MElt. The results reported by Plank et al.\n4They discarded all corpora containing fewer than 60k tokens in the training set, maybe as a result of the sensitivy of LSMTs to training set size.\n5However, for French, we used the morphosyntactic variant of the Lefff that is included in the MElt distribution, and which relies on a variant of the French TreeBank known as FTB-UC (Candito and Crabb\u00e9, 2009).\nRR n\u00b0 8924"}, {"heading": "8 Sagot", "text": "(2016) for their bidirectional LSTM when initialised with Polyglot embeddings trained on full wikipedias are also included, together with their new system FREQBIN, also initialised with Polyglot embeddings. FREQBIN trains bi-LSTMs to predict for each input word both a PoS and a label that represents its log frequency in the training data. As they word it, \u201cthe idea behind this model is to make the representation predictive for frequency, which encourages the model not to share representations between common and rare words, thus benefiting the handling of rare tokens.\u201d\nThe results, which are also displayed in Figures 1 and 2, show that all systems reach very similar results on average, although discrepancies can be observed from one dataset to another, on which we shall comment shortly. The best performing system in terms of macro-average is MElt (96.60%). Both bi-LSTM systems reach the same score (96.58%), the difference with MElt\u2019s results being non significant, whereas MarMoT is only 0.14% behind (96.46%). Given the better baseline scores of the neural approaches, these results show that the benefit of using external lexicons in the feature-based models MElt and MarMoT are much higher than those using Polyglot word vector representations as initialisations for bi-LSTMs.\nYet these very similar overall results reflect a different picture when focusing on OOV tagging accuracy. The best models for OOV tagging accuracy are, by far, FREQBIN models, which are beaten by MarMoT and by MElt only once each (on English and Danish respectively). The comparison on OOV tagging between MElt and MarMoT shows that MElt performs better on average than MarMoT, despite the fact that MarMoT\u2019s baseline results were better than those reached by MElt. This shows that the information provided by external morphosyntactic lexicons is better exploited by MElt\u2019s lexical features than by those used by MarMoT. On the other hand, the comparison of both bi-LSTM-based approaches confirm that the FREQBIN models is better by over 10% absolute on OOV tagging accuracy (94.28% vs. 83.59%), with 65% lower error rate.\nOne of the important differences between the lexical information provided by an external lexicon and word vectors built from raw corpora, apart from the very nature of the lexical\nInria"}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 9", "text": "RR n\u00b0 8924\n10 Sagot\nInria"}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 11", "text": "information provided, is the coverage and accuracy of this lexical information on rare words. All words in a morphosyntactic lexicon are associated with information of a same granularity and quality, which is not the case with word representations such as provided by Polyglot. Models that take advantage of external lexicons should therefore perform comparatively better on datasets containing a higher proportion of rarer words, provided the lexicons\u2019 coverage is high. In order to confirm this intuition, we have used a lexical richness metric based on the type/token ratio. Since this ratio is well-known for being sensitive to corpus length, we normalised it by computing it over the 60,000 first tokens of each training set. When this normalised type/token ratio is plotted against the difference between the results of MElt and both bi-LSTM-based models, the expected correlation is clearly visible (see Figure 3). This explains why MElt obtains better results on the morphologically richer Slavic datasets (average normalised type/token ratio: 0.28, average accuracy difference: 0.32 compared to both bi-LSTM+Polyglot and FREQBIN+Polyglot) and, at the other end of the spectrum, significantly worse results on the English dataset (normalised type/token ratio: 0.15, average accuracy difference: -0.56 compared to bi-LSTM+Polyglot, -0.57 compared to FREQBIN+Polyglot)."}, {"heading": "5 Conclusion", "text": "Two main conclusions can be drawn from our comparative results. First, feature-based tagging models adequately enriched with external morphosyntactic lexicons perform, on average, as well as bi-LSTMs enriched with word embeddings. Per-language results show that the best accuracy levels are reached by feature-based models, and in particular by our improved version of the MEMM-based system MElt, on datasets with high lexical variability (in short, for morphologically rich languages), whereas neural-based results perform better on datatsets with lower lexical variability (e.g. for English).\nWe have only compared the contribution of morphosyntactic lexicons to feature-based models (MEMMs, CRFs) and that of word vector representations to bi-LSTM-based models as reported by Plank et al. (2016). As mentioned above, work on the contribution of word vector representations to feature-based approaches has been carried out by M\u00fcller and Sch\u00fctze (2015). However, the exploitation of existing morphosyntactic or morphological lexicons in neural models is a less studied question. Improvements over the state of the art might be achieved by integrating lexical information both from an external lexicon and from word vector representations into tagging models.\nIn that regard, further work will be required to understand which class of models perform the best. An option would be to integrate feature-based models such as a CRF with an LSTM-based layer, following recent proposals such as the one proposed by Lample et al. (2016) for named entity recognition."}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 MElt 4", "text": ""}, {"heading": "3 Datasets 6", "text": "3.1 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Lexicons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nRR n\u00b0 8924"}, {"heading": "12 Sagot", "text": ""}, {"heading": "4 Experiments and results 7", "text": "4.1 Baseline models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.2 Models enriched with external lexical information . . . . . . . . . . . . . . . . . . 7"}, {"heading": "5 Conclusion 11", "text": ""}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 13", "text": "Erjavec, T. (2010). Multext-east version 4: Multilingual morphosyntactic specifications, lexicons and corpora. In Proc. of LREC 2010, Valletta, Malta.\nGoldberg, Y., Tsarfaty, R., Adler, M., and Elhadad, M. (2009). Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and em-hmm-based lexical probabilities. In Proc. of the 12th Conf. of the European Chapter of the ACL, pages 327\u2013335.\nHagen, K. and N\u00f8klestad, A. (2010). Bruk av et norsk leksikon til tagging og andre spr\u00e5kteknologiske form\u00e5l. LexicoNordica, 17:55\u201372.\nHaji\u010d, J. (2000). Morphological Tagging: Data vs. Dictionaries. In Proc. of ANLP\u201900, pages 94\u2013101, Seattle, Washington, USA.\nHaji\u010d, J. and Hlav\u00e1\u010dov\u00e1, J. (2013). MorfFlex CZ. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neur. Comp., 9(8):1735\u2013 1780.\nKim, J.-D., Lee, S.-Z., and Rim, H.-C. (1999). HMM Specialization with Selective Lexicalization. In Proc. of the join SIGDAT Conf. on Empirical Methods in Natural Lang. Processing and Very Large Corpora.\nKrek, S., Erjavec, T., and Holozan, P. (2008). Specifikacije za leksikon besednih oblik (kazalnik 3). Technical report, Projekt Sporazumevanje v slovenskem jeziku, Ljubljana, Slovenia.\nK\u00fcbler, S., Scheutz, M., Baucom, E., and Israel, R. (2010). Adding context information to part of speech tagging for dialogues. In Proc. of the Ninth International Workshop on Treebanks and Linguistic Theories (TLT), pages 115\u2013126.\nLafferty, J. D., McCallum, A., and Pereira, F. C. N. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282\u2013289.\nLample, G., Ballesteros, M., Kawakami, K., Subramanian, S., and Dyer, C. (2016). Neural architectures for named entity recognition. In Proc. of NAACL-HLT (NAACL 2016), San Diego, California, USA.\nLe Roux, J., Sagot, B., and Seddah, D. (2012). Statistical parsing of spanish and data driven lemmatization. In Proc. of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages (SP-Sem-MRL 2012), Jeju, Korea.\nLing, W., Lu\u00eds, T., Marujo, L., Astudillo, R. F., Amir, S., Dyer, C., Black, A. W., and Trancoso, I. (2015). Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation. In Proc. of the 2015 Conf. on Empirical Methods in Natural Lang. Processing, pages 1520\u20131530, Lisbon, Portugal.\nMagerman, D. M. (1995). Statistical decision-tree models for parsing. In Proc. of the 33rd Annual Meeting on ACL, pages 276\u2013283, Cambridge, Mass., USA.\nMerialdo, B. (1994). Tagging english text with a probabilistic model. Computational Linguistics, 20(2):155\u2013171.\nMolinero, M. A., Sagot, B., and Nicolas, L. (2009). A morphological and syntactic wide-coverage lexicon for Spanish: The Leff e. In Proc. of the 7th conference on Recent Advances in Natural Language Processing (RANLP 2009), Borovets, Bulgaria.\nRR n\u00b0 8924"}, {"heading": "14 Sagot", "text": "M\u00fcller, T., Schmid, H., and Sch\u00fctze, H. (2013). Efficient higher-order CRFs for morphological tagging. In Proc. of the 2013 Conf. on Empirical Methods in Natural Language Processing, pages 322\u2013332, Seattle, Washington, USA.\nM\u00fcller, T. and Sch\u00fctze, H. (2015). Robust morphological tagging with word representations. In Proc. of the 2015 Conf. of the North American Chapter of the ACL: Human Language Technologies, Denver, Colorado, USA.\nNivre, J. and al. (2015). Universal dependencies 1.2. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague.\nOliver, A. and Tadi\u0107, M. (2004). Enlarging the Croatian morphological lexicon by automatic lexical acquisition from raw corpora. In Proc. of LREC 2004, pages 1259\u20131262, Lisbon, Portugal.\nPetrov, S., Das, D., and McDonald, R. (2012). A universal part-of-speech tagset. In Proc. of LREC 2012, Istanbul, Turkey.\nPlank, B., S\u00f8gaard, A., and Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proc. of the 54th Annual Meeting of the ACL, Berlin, Germany. To appear.\nRanchhod, E., Mota, C., and Baptista, J. (1999). A Computational Lexicon of Portuguese for Automatic Text Parsing. In Proc. of the SIGLEX99 workshop on Standardizing Lexical Resources, College Park, Maryland, USA.\nRatnaparkhi, A. (1996). A maximum entropy model for part-of-speech tagging. In Proc. of International Conf. on Empirical Methods in Natural Language Processing, pages 133\u2013142.\nSagot, B. (2007). Building a morphosyntactic lexicon and a pre-syntactic processing chain for Polish. In Proc. of LTC 2005, pages 423\u2013427, Pozna\u0144, Poland.\nSagot, B. (2010). The Lefff , a freely available, accurate and large-coverage lexicon for french. In Proc. of LREC 2010, Valletta, Malta.\nSagot, B. (2014). DeLex, a freely-avaible, large-scale and linguistically grounded morphological lexicon for German. In Language Resources and Evaluation Conf., Reykjavik, Iceland.\nSagot, B. and Walther, G. (2010). A morphological lexicon for the Persian language. In Proc. of LREC 2010, Valletta, Malta.\nSchmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proc. of International Conf. on New Methods in Language Processing, Manchester, UK.\nSeddah, D., Le Roux, J., and Sagot, B. (2013a). Data driven lemmatization and parsing of italian. In Magnini, B., Cutugno, F., Falcone, M., and Pianta, E., editors, Evaluation of Natural Language and Speech Tools for Italian (Revised Selected Papers), volume 7689 of Lecture Notes in Computer Science, pages 249\u2013256. Springer-Verlag, Rome, Italy.\nSeddah, D., Tsarfaty, R., K\u00fcbler, S., Candito, M., Choi, J. D., Farkas, R., Foster, J., Goenaga, I., Gojenola Galletebeitia, K., Goldberg, Y., Green, S., Habash, N., Kuhlmann, M., Maier, W., Nivre, J., Przepi\u00f3rkowski, A., Roth, R., Seeker, W., Versley, Y., Vincze, V., Woli\u0144ski, M., Wr\u00f3blewska, A., and Villemonte de La Clergerie, E. (2013b). Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proc. of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146\u2013182, Seattle, Washington, USA.\nInria"}, {"heading": "External Lexical Information for Multilingual Part-of-Speech Tagging 15", "text": "Toutanova, K. and Manning, C. D. (2000). Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proc. of International Conf. on New Methods in Language Processing, pages 63\u201370, Hong Kong, China.\nZanchetta, E. and Baroni, M. (2005). Morph-it! a free corpus-based morphological resource for the Italian language. In Proc. of the Corpus linguistics Conf., pages 1\u201312, Birmingham, UK.\nRR n\u00b0 8924\nRESEARCH CENTRE PARIS \u2013 ROCQUENCOURT\nDomaine de Voluceau, - Rocquencourt B.P. 105 - 78153 Le Chesnay Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399"}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "Proc. of the Seventeenth Conf. on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bulgaria.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs", "author": ["M. Ballesteros", "C. Dyer", "N.A. Smith"], "venue": "Proc. of the 2015 Conf. on Empirical Methods in Natural Language Processing, pages 349\u2013359, Lisbon, Portugal.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res., 3(1):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The hunting of the BLARK - SALDO, a freely available lexical database for swedish language technology", "author": ["L. Borin", "M. Forsberg", "L. L\u00f6nngren"], "venue": "Resourceful language technology. Festschrift in honor of Anna S\u00e5gvall Hein, pages 21\u201332. Uppsala University, Uppsala, Sweden.", "citeRegEx": "Borin et al\\.,? 2008", "shortCiteRegEx": "Borin et al\\.", "year": 2008}, {"title": "Danish monolingual lexicon, documentation", "author": ["A. Braasch", "C. Navarretta", "S. Olsen", "B.S. Pedersen"], "venue": null, "citeRegEx": "Braasch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Braasch et al\\.", "year": 2008}, {"title": "Estimating markov model structures", "author": ["T. Brants"], "venue": "Proc. of the 4th Conf. on Spoken Language Processing (ICSLP-96), pages 893\u2013896.", "citeRegEx": "Brants,? 1996", "shortCiteRegEx": "Brants", "year": 1996}, {"title": "TnT: A Statistical Part-of-speech Tagger", "author": ["T. Brants"], "venue": "Proc. of the Sixth Conf. on Applied Natural Language Processing, pages 224\u2013231, Seattle, Washington, USA.", "citeRegEx": "Brants,? 2000", "shortCiteRegEx": "Brants", "year": 2000}, {"title": "Improving generative statistical parsing with semi-supervised word clustering", "author": ["M. Candito", "B. Crabb\u00e9"], "venue": "Proc. of the 11th International Conf. on Parsing Technologies, pages 138\u2013 141, Paris, France.", "citeRegEx": "Candito and Crabb\u00e9,? 2009", "shortCiteRegEx": "Candito and Crabb\u00e9", "year": 2009}, {"title": "Text segmentation with character-level text embeddings", "author": ["G. Chrupa\u0142a"], "venue": "Proc. of the ICML Workshop on Deep Learning for Audio, Speech and Lang. Processing, Atlanta, Georgia, USA.", "citeRegEx": "Chrupa\u0142a,? 2013", "shortCiteRegEx": "Chrupa\u0142a", "year": 2013}, {"title": "Learning morphology with morfette", "author": ["G. Chrupa\u0142a", "G. Dinu", "J. van Genabith"], "venue": "In Proc. of the 6th Language Resource and Evaluation Conf.,", "citeRegEx": "Chrupa\u0142a et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chrupa\u0142a et al\\.", "year": 2008}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of the 25th International Conf. on Machine Learning, pages 160\u2013167, Helsinki, Finland.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Evaluating the Impact of External Lexical Resources into a CRF-based Multiword Segmenter and Part-of-Speech Tagger", "author": ["M. Constant", "I. Tellier"], "venue": "Proc. of LREC\u201912, pages 646\u2013650, Istanbul, Turkey.", "citeRegEx": "Constant and Tellier,? 2012", "shortCiteRegEx": "Constant and Tellier", "year": 2012}, {"title": "Coupling an annotated corpus and a lexicon for state-of-the-art POS tagging", "author": ["P. Denis", "B. Sagot"], "venue": "Language Resources and Evaluation, 46(4):721\u2013736.", "citeRegEx": "Denis and Sagot,? 2012", "shortCiteRegEx": "Denis and Sagot", "year": 2012}, {"title": "Multext-east version 4: Multilingual morphosyntactic specifications, lexicons and corpora", "author": ["T. Erjavec"], "venue": "Proc. of LREC 2010, Valletta, Malta.", "citeRegEx": "Erjavec,? 2010", "shortCiteRegEx": "Erjavec", "year": 2010}, {"title": "Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and em-hmm-based lexical probabilities", "author": ["Y. Goldberg", "R. Tsarfaty", "M. Adler", "M. Elhadad"], "venue": "Proc. of the 12th Conf. of the European Chapter of the ACL, pages 327\u2013335.", "citeRegEx": "Goldberg et al\\.,? 2009", "shortCiteRegEx": "Goldberg et al\\.", "year": 2009}, {"title": "Bruk av et norsk leksikon til tagging og andre spr\u00e5kteknologiske form\u00e5l", "author": ["K. Hagen", "A. N\u00f8klestad"], "venue": "LexicoNordica, 17:55\u201372.", "citeRegEx": "Hagen and N\u00f8klestad,? 2010", "shortCiteRegEx": "Hagen and N\u00f8klestad", "year": 2010}, {"title": "Morphological Tagging: Data vs", "author": ["J. Haji\u010d"], "venue": "Dictionaries. In Proc. of ANLP\u201900, pages 94\u2013101, Seattle, Washington, USA.", "citeRegEx": "Haji\u010d,? 2000", "shortCiteRegEx": "Haji\u010d", "year": 2000}, {"title": "MorfFlex CZ", "author": ["J. Haji\u010d", "J. Hlav\u00e1\u010dov\u00e1"], "venue": "LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague.", "citeRegEx": "Haji\u010d and Hlav\u00e1\u010dov\u00e1,? 2013", "shortCiteRegEx": "Haji\u010d and Hlav\u00e1\u010dov\u00e1", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neur. Comp., 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "HMM Specialization with Selective Lexicalization", "author": ["Kim", "J.-D.", "Lee", "S.-Z.", "Rim", "H.-C."], "venue": "Proc. of the join SIGDAT Conf. on Empirical Methods in Natural Lang. Processing and Very Large Corpora.", "citeRegEx": "Kim et al\\.,? 1999", "shortCiteRegEx": "Kim et al\\.", "year": 1999}, {"title": "Specifikacije za leksikon besednih oblik (kazalnik 3)", "author": ["S. Krek", "T. Erjavec", "P. Holozan"], "venue": "Technical report, Projekt Sporazumevanje v slovenskem jeziku, Ljubljana, Slovenia.", "citeRegEx": "Krek et al\\.,? 2008", "shortCiteRegEx": "Krek et al\\.", "year": 2008}, {"title": "Adding context information to part of speech tagging for dialogues", "author": ["S. K\u00fcbler", "M. Scheutz", "E. Baucom", "R. Israel"], "venue": "Proc. of the Ninth International Workshop on Treebanks and Linguistic Theories (TLT), pages 115\u2013126.", "citeRegEx": "K\u00fcbler et al\\.,? 2010", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "ICML, pages 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "K. Kawakami", "S. Subramanian", "C. Dyer"], "venue": "Proc. of NAACL-HLT (NAACL 2016), San Diego, California, USA.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Statistical parsing of spanish and data driven lemmatization", "author": ["J. Le Roux", "B. Sagot", "D. Seddah"], "venue": "Proc. of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages (SP-Sem-MRL 2012), Jeju, Korea.", "citeRegEx": "Roux et al\\.,? 2012", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "author": ["W. Ling", "T. Lu\u00eds", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "Proc. of the 2015 Conf. on Empirical Methods in Natural Lang. Processing, pages 1520\u20131530, Lisbon, Portugal.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Statistical decision-tree models for parsing", "author": ["D.M. Magerman"], "venue": "Proc. of the 33rd Annual Meeting on ACL, pages 276\u2013283, Cambridge, Mass., USA.", "citeRegEx": "Magerman,? 1995", "shortCiteRegEx": "Magerman", "year": 1995}, {"title": "Tagging english text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational Linguistics, 20(2):155\u2013171.", "citeRegEx": "Merialdo,? 1994", "shortCiteRegEx": "Merialdo", "year": 1994}, {"title": "A morphological and syntactic wide-coverage lexicon for Spanish: The Leff e", "author": ["M.A. Molinero", "B. Sagot", "L. Nicolas"], "venue": "Proc. of the 7th conference on Recent Advances in Natural Language Processing (RANLP 2009), Borovets, Bulgaria.", "citeRegEx": "Molinero et al\\.,? 2009", "shortCiteRegEx": "Molinero et al\\.", "year": 2009}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["T. M\u00fcller", "H. Schmid", "H. Sch\u00fctze"], "venue": "Proc. of the 2013 Conf. on Empirical Methods in Natural Language Processing, pages 322\u2013332, Seattle, Washington, USA.", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Robust morphological tagging with word representations", "author": ["T. M\u00fcller", "H. Sch\u00fctze"], "venue": "Proc. of the 2015 Conf. of the North American Chapter of the ACL: Human Language Technologies, Denver, Colorado, USA.", "citeRegEx": "M\u00fcller and Sch\u00fctze,? 2015", "shortCiteRegEx": "M\u00fcller and Sch\u00fctze", "year": 2015}, {"title": "Universal dependencies 1.2. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["J. Nivre", "al"], "venue": null, "citeRegEx": "Nivre and al.,? \\Q2015\\E", "shortCiteRegEx": "Nivre and al.", "year": 2015}, {"title": "Enlarging the Croatian morphological lexicon by automatic lexical acquisition from raw corpora", "author": ["A. Oliver", "M. Tadi\u0107"], "venue": "Proc. of LREC 2004, pages 1259\u20131262, Lisbon, Portugal.", "citeRegEx": "Oliver and Tadi\u0107,? 2004", "shortCiteRegEx": "Oliver and Tadi\u0107", "year": 2004}, {"title": "A universal part-of-speech tagset", "author": ["S. Petrov", "D. Das", "R. McDonald"], "venue": "Proc. of LREC 2012, Istanbul, Turkey.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss", "author": ["B. Plank", "A. S\u00f8gaard", "Y. Goldberg"], "venue": "Proc. of the 54th Annual Meeting of the ACL, Berlin, Germany. To appear.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "A Computational Lexicon of Portuguese for Automatic Text Parsing", "author": ["E. Ranchhod", "C. Mota", "J. Baptista"], "venue": "Proc. of the SIGLEX99 workshop on Standardizing Lexical Resources, College Park, Maryland, USA.", "citeRegEx": "Ranchhod et al\\.,? 1999", "shortCiteRegEx": "Ranchhod et al\\.", "year": 1999}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["A. Ratnaparkhi"], "venue": "Proc. of International Conf. on Empirical Methods in Natural Language Processing, pages 133\u2013142.", "citeRegEx": "Ratnaparkhi,? 1996", "shortCiteRegEx": "Ratnaparkhi", "year": 1996}, {"title": "Building a morphosyntactic lexicon and a pre-syntactic processing chain for Polish", "author": ["B. Sagot"], "venue": "Proc. of LTC 2005, pages 423\u2013427, Pozna\u0144, Poland.", "citeRegEx": "Sagot,? 2007", "shortCiteRegEx": "Sagot", "year": 2007}, {"title": "The Lefff , a freely available, accurate and large-coverage lexicon for french", "author": ["B. Sagot"], "venue": "Proc. of LREC 2010, Valletta, Malta.", "citeRegEx": "Sagot,? 2010", "shortCiteRegEx": "Sagot", "year": 2010}, {"title": "DeLex, a freely-avaible, large-scale and linguistically grounded morphological lexicon for German", "author": ["B. Sagot"], "venue": "Language Resources and Evaluation Conf., Reykjavik, Iceland.", "citeRegEx": "Sagot,? 2014", "shortCiteRegEx": "Sagot", "year": 2014}, {"title": "A morphological lexicon for the Persian language", "author": ["B. Sagot", "G. Walther"], "venue": "Proc. of LREC 2010, Valletta, Malta.", "citeRegEx": "Sagot and Walther,? 2010", "shortCiteRegEx": "Sagot and Walther", "year": 2010}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "Proc. of International Conf. on New Methods in Language Processing, Manchester, UK.", "citeRegEx": "Schmid,? 1994", "shortCiteRegEx": "Schmid", "year": 1994}, {"title": "Data driven lemmatization and parsing of italian", "author": ["D. Seddah", "J. Le Roux", "B. Sagot"], "venue": "Magnini, B., Cutugno, F., Falcone, M., and Pianta, E., editors, Evaluation of Natural Language and Speech Tools for Italian (Revised Selected Papers), volume 7689 of Lecture Notes in Computer Science, pages 249\u2013256. Springer-Verlag, Rome, Italy.", "citeRegEx": "Seddah et al\\.,? 2013a", "shortCiteRegEx": "Seddah et al\\.", "year": 2013}, {"title": "Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages", "author": ["D. Seddah", "R. Tsarfaty", "S. K\u00fcbler", "M. Candito", "J.D. Choi", "R. Farkas", "J. Foster", "I. Goenaga", "K. Gojenola Galletebeitia", "Y. Goldberg", "S. Green", "N. Habash", "M. Kuhlmann", "W. Maier", "J. Nivre", "A. Przepi\u00f3rkowski", "R. Roth", "W. Seeker", "Y. Versley", "V. Vincze", "M. Woli\u0144ski", "A. Wr\u00f3blewska", "E. Villemonte de La Clergerie"], "venue": "In", "citeRegEx": "Seddah et al\\.,? 2013b", "shortCiteRegEx": "Seddah et al\\.", "year": 2013}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "Proc. of International Conf. on New Methods in Language Processing, pages 63\u201370, Hong Kong, China.", "citeRegEx": "Toutanova and Manning,? 2000", "shortCiteRegEx": "Toutanova and Manning", "year": 2000}, {"title": "Morph-it! a free corpus-based morphological resource for the Italian language", "author": ["E. Zanchetta", "M. Baroni"], "venue": "Proc. of the Corpus linguistics Conf., pages 1\u201312, Birmingham, UK.", "citeRegEx": "Zanchetta and Baroni,? 2005", "shortCiteRegEx": "Zanchetta and Baroni", "year": 2005}], "referenceMentions": [{"referenceID": 27, "context": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al.", "startOffset": 133, "endOffset": 169}, {"referenceID": 41, "context": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al.", "startOffset": 186, "endOffset": 216}, {"referenceID": 26, "context": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al.", "startOffset": 186, "endOffset": 216}, {"referenceID": 36, "context": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al.", "startOffset": 256, "endOffset": 275}, {"referenceID": 22, "context": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012).", "startOffset": 313, "endOffset": 364}, {"referenceID": 11, "context": "A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012).", "startOffset": 313, "endOffset": 364}, {"referenceID": 19, "context": "Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Haji\u010d, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupa\u0142a et al.", "startOffset": 80, "endOffset": 111}, {"referenceID": 16, "context": "Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Haji\u010d, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupa\u0142a et al.", "startOffset": 80, "endOffset": 111}, {"referenceID": 9, "context": ", 1999; Haji\u010d, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupa\u0142a et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012).", "startOffset": 142, "endOffset": 211}, {"referenceID": 14, "context": ", 1999; Haji\u010d, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupa\u0142a et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012).", "startOffset": 142, "endOffset": 211}, {"referenceID": 12, "context": ", 1999; Haji\u010d, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupa\u0142a et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012).", "startOffset": 142, "endOffset": 211}, {"referenceID": 2, "context": "In recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 213, "endOffset": 349}, {"referenceID": 10, "context": "In recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 213, "endOffset": 349}, {"referenceID": 8, "context": "In recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 213, "endOffset": 349}, {"referenceID": 25, "context": "In recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 213, "endOffset": 349}, {"referenceID": 1, "context": "In recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 213, "endOffset": 349}, {"referenceID": 30, "context": "In recent years, a different approach to modelling lexical information and integrating it into natural language processing systems has emerged, namely the use of vector representations for words or word sequences (Bengio et al., 2003; Collobert and Weston, 2008; Chrupa\u0142a, 2013; Ling et al., 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015).", "startOffset": 213, "endOffset": 349}, {"referenceID": 18, "context": "Such representations, which are generally extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level and/or character-level long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 334, "endOffset": 433}, {"referenceID": 25, "context": "Such representations, which are generally extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level and/or character-level long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 334, "endOffset": 433}, {"referenceID": 1, "context": "Such representations, which are generally extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level and/or character-level long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 334, "endOffset": 433}, {"referenceID": 34, "context": "Such representations, which are generally extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level and/or character-level long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 334, "endOffset": 433}, {"referenceID": 1, "context": ", 2015; Ballesteros et al., 2015; M\u00fcller and Sch\u00fctze, 2015). Such representations, which are generally extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level and/or character-level long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016). Both approaches to representing lexical properties and to integrating them into a PoS tagger improve tagging results. Yet they rely on resources of different natures. The main advantage of word vectors is that they are built in an unsupervised way, only requiring large amounts of raw textual data. They also encode finer-grained information than usual morphosyntactic lexicons, most of which do not include any quantitative data, not even simple frequency information. Conversely, lexical resources often provide information about scarcely attested words, for which corpus-based approaches such as word vector representations are of limited relevance. Moreover, morphological or morphosyntactic lexicons already exist for a number of languages, including less-resourced langauges for which it might be difficult to obtain the large amounts of raw data necessary to extract word vector representations. Our main goal is therefore to compare the respective impact of external lexicons and word vector representations on the accuracy of PoS models. This question has already been investigated for 6 languages by M\u00fcller and Sch\u00fctze (2015) using the state-of-the-art CRF-based tagging system MarMoT.", "startOffset": 8, "endOffset": 1632}, {"referenceID": 12, "context": "More specifically, our starting point is the MElt system (Denis and Sagot, 2012), an MEMM tagging system.", "startOffset": 57, "endOffset": 80}, {"referenceID": 31, "context": "2) corpus set (Nivre and al., 2015), complemented by morphosyntactic lexicons.", "startOffset": 14, "endOffset": 35}, {"referenceID": 29, "context": "We compare the accuracy of our models with the scores obtained by the CRF-based system MarMoT (M\u00fcller et al., 2013; M\u00fcller and Sch\u00fctze, 2015), retrained on the same corpora and the same external morphosyntactic lexicons.", "startOffset": 94, "endOffset": 141}, {"referenceID": 30, "context": "We compare the accuracy of our models with the scores obtained by the CRF-based system MarMoT (M\u00fcller et al., 2013; M\u00fcller and Sch\u00fctze, 2015), retrained on the same corpora and the same external morphosyntactic lexicons.", "startOffset": 94, "endOffset": 141}, {"referenceID": 11, "context": "More specifically, our starting point is the MElt system (Denis and Sagot, 2012), an MEMM tagging system. We first briefly describe this system and the way we adapted it by integrating our own set of corpus-based and lexical features. We then introduce the tagging models we have trained for 16 different languages using our adapted version of MElt. These models are trained on the Universal Dependencies (v1.2) corpus set (Nivre and al., 2015), complemented by morphosyntactic lexicons. We compare the accuracy of our models with the scores obtained by the CRF-based system MarMoT (M\u00fcller et al., 2013; M\u00fcller and Sch\u00fctze, 2015), retrained on the same corpora and the same external morphosyntactic lexicons. We also compare our results to those obtained by the best bidirectional LSTM models described by Plank et al. (2016), which both make use of Polyglot word vector representations published by Al-Rfou et al.", "startOffset": 58, "endOffset": 826}, {"referenceID": 0, "context": "(2016), which both make use of Polyglot word vector representations published by Al-Rfou et al. (2013). We will show that an optimised enrichment of feature-based models with morphosyntactic lexicon results in significant accuracy gains.", "startOffset": 81, "endOffset": 103}, {"referenceID": 12, "context": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996).", "startOffset": 5, "endOffset": 28}, {"referenceID": 36, "context": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996).", "startOffset": 95, "endOffset": 114}, {"referenceID": 36, "context": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996).", "startOffset": 190, "endOffset": 209}, {"referenceID": 12, "context": "The basic set of features used by MElt is given in (Denis and Sagot, 2012).", "startOffset": 51, "endOffset": 74}, {"referenceID": 12, "context": "Yet the motivation of MElt\u2019s developers was first and foremost to investigate the best way to integrate lexical information extracted from large-scale morphosyntactic lexical resources into their models, on top of the training data (Denis and Sagot, 2012).", "startOffset": 232, "endOffset": 255}, {"referenceID": 12, "context": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996). The basic set of features used by MElt is given in (Denis and Sagot, 2012). It is a superset of the feature sets used by Ratnaparkhi (1996) and Toutanova and Manning (2000) and includes both local standard features (for example the current word itself and its prefixes and suffixes of length 1 to 4) and contextual standard features (for example the tag just assigned to the preceding word).", "startOffset": 6, "endOffset": 351}, {"referenceID": 12, "context": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996). The basic set of features used by MElt is given in (Denis and Sagot, 2012). It is a superset of the feature sets used by Ratnaparkhi (1996) and Toutanova and Manning (2000) and includes both local standard features (for example the current word itself and its prefixes and suffixes of length 1 to 4) and contextual standard features (for example the tag just assigned to the preceding word).", "startOffset": 6, "endOffset": 384}, {"referenceID": 12, "context": "MElt (Denis and Sagot, 2012) is a tagging system based on maximum entropy Markov models (MEMM) (Ratnaparkhi, 1996), a class of discriminative models that are suitable for sequence labelling (Ratnaparkhi, 1996). The basic set of features used by MElt is given in (Denis and Sagot, 2012). It is a superset of the feature sets used by Ratnaparkhi (1996) and Toutanova and Manning (2000) and includes both local standard features (for example the current word itself and its prefixes and suffixes of length 1 to 4) and contextual standard features (for example the tag just assigned to the preceding word). In particular, with respect to Ratnaparkhi\u2019s feature set, MElt\u2019s basic feature set lifts the restriction that local standard features used to analyse the internal composition of the current word should only apply to rare words. One of the advantages of feature-based models such as MEMMs and CRFs is that complementary information can be easily added in the form of additional features. This was investigated for instance by K\u00fcbler et al. (2010), whose best-performing model for PoS tagging dialogues was obtained with a version of MElt extended with dialogue-specific features.", "startOffset": 6, "endOffset": 1049}, {"referenceID": 13, "context": "Language Source Lexicon #entries tagset size Reference Bulgarian Multext-EAST 53056 12 (Erjavec, 2010) Croatian HML 1360687 22 (Oliver and Tadi\u0107, 2004) Czech Morfflex (parts) 2094860 65 (Haji\u010d and Hlav\u00e1\u010dov\u00e1, 2013) Danish STO 566184 13 (Braasch et al.", "startOffset": 87, "endOffset": 102}, {"referenceID": 32, "context": "Language Source Lexicon #entries tagset size Reference Bulgarian Multext-EAST 53056 12 (Erjavec, 2010) Croatian HML 1360687 22 (Oliver and Tadi\u0107, 2004) Czech Morfflex (parts) 2094860 65 (Haji\u010d and Hlav\u00e1\u010dov\u00e1, 2013) Danish STO 566184 13 (Braasch et al.", "startOffset": 127, "endOffset": 151}, {"referenceID": 17, "context": "Language Source Lexicon #entries tagset size Reference Bulgarian Multext-EAST 53056 12 (Erjavec, 2010) Croatian HML 1360687 22 (Oliver and Tadi\u0107, 2004) Czech Morfflex (parts) 2094860 65 (Haji\u010d and Hlav\u00e1\u010dov\u00e1, 2013) Danish STO 566184 13 (Braasch et al.", "startOffset": 186, "endOffset": 213}, {"referenceID": 4, "context": "Language Source Lexicon #entries tagset size Reference Bulgarian Multext-EAST 53056 12 (Erjavec, 2010) Croatian HML 1360687 22 (Oliver and Tadi\u0107, 2004) Czech Morfflex (parts) 2094860 65 (Haji\u010d and Hlav\u00e1\u010dov\u00e1, 2013) Danish STO 566184 13 (Braasch et al., 2008) English EnLex 472850 22 (Sagot, 2010) French Lefff 539278 25 (Sagot, 2010) German DeLex 465434 52 (Sagot, 2014) Indonesian Kateglo 72217 118 https://github.", "startOffset": 235, "endOffset": 257}, {"referenceID": 38, "context": ", 2008) English EnLex 472850 22 (Sagot, 2010) French Lefff 539278 25 (Sagot, 2010) German DeLex 465434 52 (Sagot, 2014) Indonesian Kateglo 72217 118 https://github.", "startOffset": 32, "endOffset": 45}, {"referenceID": 38, "context": ", 2008) English EnLex 472850 22 (Sagot, 2010) French Lefff 539278 25 (Sagot, 2010) German DeLex 465434 52 (Sagot, 2014) Indonesian Kateglo 72217 118 https://github.", "startOffset": 69, "endOffset": 82}, {"referenceID": 39, "context": ", 2008) English EnLex 472850 22 (Sagot, 2010) French Lefff 539278 25 (Sagot, 2010) German DeLex 465434 52 (Sagot, 2014) Indonesian Kateglo 72217 118 https://github.", "startOffset": 106, "endOffset": 119}, {"referenceID": 45, "context": "com/ivanlanin/kateglo Italian Morph-it! 422756 31 (Zanchetta and Baroni, 2005) Norwegian OrdBank 679763 19 (Hagen and N\u00f8klestad, 2010) Persian PerLex 511840 37 (Sagot and Walther, 2010) Polish PolLex 390370 28 (Sagot, 2007) Portuguese LABEL-LEXsw 971514 29 (Ranchhod et al.", "startOffset": 50, "endOffset": 78}, {"referenceID": 15, "context": "com/ivanlanin/kateglo Italian Morph-it! 422756 31 (Zanchetta and Baroni, 2005) Norwegian OrdBank 679763 19 (Hagen and N\u00f8klestad, 2010) Persian PerLex 511840 37 (Sagot and Walther, 2010) Polish PolLex 390370 28 (Sagot, 2007) Portuguese LABEL-LEXsw 971514 29 (Ranchhod et al.", "startOffset": 107, "endOffset": 134}, {"referenceID": 40, "context": "com/ivanlanin/kateglo Italian Morph-it! 422756 31 (Zanchetta and Baroni, 2005) Norwegian OrdBank 679763 19 (Hagen and N\u00f8klestad, 2010) Persian PerLex 511840 37 (Sagot and Walther, 2010) Polish PolLex 390370 28 (Sagot, 2007) Portuguese LABEL-LEXsw 971514 29 (Ranchhod et al.", "startOffset": 160, "endOffset": 185}, {"referenceID": 37, "context": "com/ivanlanin/kateglo Italian Morph-it! 422756 31 (Zanchetta and Baroni, 2005) Norwegian OrdBank 679763 19 (Hagen and N\u00f8klestad, 2010) Persian PerLex 511840 37 (Sagot and Walther, 2010) Polish PolLex 390370 28 (Sagot, 2007) Portuguese LABEL-LEXsw 971514 29 (Ranchhod et al.", "startOffset": 210, "endOffset": 223}, {"referenceID": 35, "context": "com/ivanlanin/kateglo Italian Morph-it! 422756 31 (Zanchetta and Baroni, 2005) Norwegian OrdBank 679763 19 (Hagen and N\u00f8klestad, 2010) Persian PerLex 511840 37 (Sagot and Walther, 2010) Polish PolLex 390370 28 (Sagot, 2007) Portuguese LABEL-LEXsw 971514 29 (Ranchhod et al., 1999) Slovenian SloLeks 957525 25 (Krek et al.", "startOffset": 257, "endOffset": 280}, {"referenceID": 20, "context": ", 1999) Slovenian SloLeks 957525 25 (Krek et al., 2008) Spanish Leff e 755858 34 (Molinero et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 28, "context": ", 2008) Spanish Leff e 755858 34 (Molinero et al., 2009) Swedish Saldo 747959 38 (Borin et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 3, "context": ", 2009) Swedish Saldo 747959 38 (Borin et al., 2008)", "startOffset": 32, "endOffset": 52}, {"referenceID": 12, "context": "Despite a few experiments published with MElt on languages other than French (Denis and Sagot, 2012; Le Roux et al., 2012; Seddah et al., 2013a), the original feature set used by MElt (standard and lexical features) was designed and tested mostly on this language, by building and evaluating tagging models on a variant of the French TreeBank.", "startOffset": 77, "endOffset": 144}, {"referenceID": 42, "context": "Despite a few experiments published with MElt on languages other than French (Denis and Sagot, 2012; Le Roux et al., 2012; Seddah et al., 2013a), the original feature set used by MElt (standard and lexical features) was designed and tested mostly on this language, by building and evaluating tagging models on a variant of the French TreeBank.", "startOffset": 77, "endOffset": 144}, {"referenceID": 43, "context": "In order to select the best performing feature set, we carried out a series of experiments using the multilingual dataset provided during the SPMRL parsing shared task (Seddah et al., 2013b).", "startOffset": 168, "endOffset": 190}, {"referenceID": 31, "context": "2 treebanks (Nivre and al., 2015), hereafter UD1.", "startOffset": 12, "endOffset": 33}, {"referenceID": 31, "context": "2 treebanks (Nivre and al., 2015), hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted. All UD1.2 corpora use a common tag set, the 17 universal PoS tags,3 which is an extension of the tagset proposed by Petrov et al. (2012).", "startOffset": 13, "endOffset": 266}, {"referenceID": 34, "context": "2 corpora that cover languages for which we have morphosyntactic lexicons at our disposal, and for which Plank et al. (2016) provide results.", "startOffset": 105, "endOffset": 125}, {"referenceID": 34, "context": "2 corpora, together with the results publised on the same corpora by Plank et al. (2016), using their best model not enhanced by external word vector representations \u2014i.", "startOffset": 69, "endOffset": 89}, {"referenceID": 34, "context": "2 corpora, together with the results publised on the same corpora by Plank et al. (2016), using their best model not enhanced by external word vector representations \u2014i.e. the model they call ~ w +~c, which is a bidirectional LSTM that combines both word and character embeddings. These results show that Plank et al.\u2019s (2016) bi-LSTM performs extremely well, surpassed by MarMoT on only 3 out of 16 datasets (Czech, French and Italian), and by MElt only once (Indonesian).", "startOffset": 69, "endOffset": 327}, {"referenceID": 7, "context": "5However, for French, we used the morphosyntactic variant of the Lefff that is included in the MElt distribution, and which relies on a variant of the French TreeBank known as FTB-UC (Candito and Crabb\u00e9, 2009).", "startOffset": 183, "endOffset": 209}, {"referenceID": 34, "context": "MElt and MarMoT models trained without external lexicons, and Plank et al.\u2019s (2016) ~c + ~ w models, which do not make use of Polyglot embeddings.", "startOffset": 62, "endOffset": 84}, {"referenceID": 34, "context": "Model type MEMM+lexicon CRF+lexicon bi-LSTM+Polyglot FREQBIN+Polyglot System MElt MarMoT (Plank et al., 2016) overall OOV overall OOV overall OOV overall OOV Bulgarian (bg) 98.", "startOffset": 89, "endOffset": 109}, {"referenceID": 34, "context": "Table 4: Accuracy (in %) of the feature-based systems MElt and MarMoT as well as the two best LSTM-based systems by Plank et al. (2016) on UD1.", "startOffset": 116, "endOffset": 136}, {"referenceID": 32, "context": "We have only compared the contribution of morphosyntactic lexicons to feature-based models (MEMMs, CRFs) and that of word vector representations to bi-LSTM-based models as reported by Plank et al. (2016). As mentioned above, work on the contribution of word vector representations to feature-based approaches has been carried out by M\u00fcller and Sch\u00fctze (2015).", "startOffset": 184, "endOffset": 204}, {"referenceID": 29, "context": "As mentioned above, work on the contribution of word vector representations to feature-based approaches has been carried out by M\u00fcller and Sch\u00fctze (2015). However, the exploitation of existing morphosyntactic or morphological lexicons in neural models is a less studied question.", "startOffset": 128, "endOffset": 154}, {"referenceID": 23, "context": "An option would be to integrate feature-based models such as a CRF with an LSTM-based layer, following recent proposals such as the one proposed by Lample et al. (2016) for named entity recognition.", "startOffset": 148, "endOffset": 169}], "year": 2016, "abstractText": "Morphosyntactic lexicons and word vector representations have both proven useful for improving the accuracy of statistical part-of-speech taggers. Here we compare the performances of four systems on datasets covering 16 languages, two of these systems being feature-based (MEMMs and CRFs) and two of them being neural-based (bi-LSTMs). We show that, on average, all four approaches perform similarly and reach state-of-the-art results. Yet better performances are obtained with our feature-based models on lexically richer datasets (e.g. for morphologically rich languages), whereas neural-based results are higher on datasets with less lexical variability (e.g. for English). These conclusions hold in particular for the MEMM models relying on our system MElt, which benefited from newly designed features. This shows that, under certain conditions, featurebased approaches enriched with morphosyntactic lexicons are competitive with respect to neural methods. Key-words: Part-of-Speech Tagging, Feature-based models, Neural models, MEMM, CRF, biLSTM, Multilingual Analysis Utilisation d\u2019informations lexicales externes pour l\u2019annotation multilingue en parties du discours R\u00e9sum\u00e9 : Les lexiques morphosyntaxiques et les repr\u00e9sentations vectorielles des mots ont chacun montr\u00e9 leur utilit\u00e9 pour am\u00e9liorer la pr\u00e9cision d\u2019\u00e9tiqueteurs morphosyntaxiques statistiques. Nous comparons ici les performances de quatre syst\u00e8mes sur des jeux de donn\u00e9es couvrant 16 langues, deux de ces syst\u00e8mes reposant sur des traits (MEMM et CRF) et deux autres sur des approches neuronales (bi-LSTM). Nous montrons qu\u2019en moyenne les quatre approches obtiennent des performances similaires de niveau \u00e9tat-de-l\u2019art. N\u00e9anmoins, nos mod\u00e8les reposant sur des traits ont de meilleures performances sur les jeux de donn\u00e9es lexicalement plus riches (par exemple sur des langues \u00e0 morphologie riche), alors que les r\u00e9sultats obtenus par les approches neuronales sont meilleurs sur les jeux de donn\u00e9es dont la variabilit\u00e9 lexicale est moindre (par exemple pour l\u2019anglais). Ces conclusions sont vraies en particulier pour nos mod\u00e8les de type MEMM faisant usage de notre syst\u00e8me MElt, qui s\u2019appuie sur un jeu de traits renouvel\u00e9. Ceci montre que, sous certaines conditions, les approches par traits enrichies par des lexiques morphosyntaxiques sont comp\u00e9titifs par rapport aux approches neuronales. Mots-cl\u00e9s : \u00c9tiquetage en partie du discours, Mod\u00e8les reposant sur des traits, Mod\u00e8les neuronaux, MEMM, CRF, bi-LSTM, Analyse multilingue External Lexical Information for Multilingual Part-of-Speech Tagging 3", "creator": "LaTeX with hyperref package"}}}