{"id": "1606.03777", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "abstract": "belief tracking is a core component and modern spoken dialogue system pipelines. however, most current approaches would have difficulty scaling to larger, more complex dialogue domains. stakeholders suffers due to their dependency on either : nw ) spoken language understanding models that require large leaps of annotated training modules ; or w ) cognitive - crafted semantic lexicons that capture the lexical variation in users'language. we described a consistent procedural belief tracking ( nbt ) algorithm which managed to overcome these problems by building on recent advances in semantic representation learning. the nbt models reason over continuous distributed arrays of words, utterances and dialogue context. our combination on two datasets shows that this approach overcomes both limitations, matching the performance of state - of - the - art models that have greater resource requirements.", "histories": [["v1", "Sun, 12 Jun 2016 22:59:14 GMT  (3391kb,D)", "http://arxiv.org/abs/1606.03777v1", "Submission under review for EMNLP 2016"], ["v2", "Fri, 21 Apr 2017 15:15:03 GMT  (628kb,D)", "http://arxiv.org/abs/1606.03777v2", "Accepted as a long paper for the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)"]], "COMMENTS": "Submission under review for EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["nikola mrksic", "diarmuid \u00f3 s\u00e9aghdha", "tsung-hsien wen", "blaise thomson", "steve j young"], "accepted": true, "id": "1606.03777"}, "pdf": {"name": "1606.03777.pdf", "metadata": {"source": "CRF", "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young"], "emails": ["sjy}@cam.ac.uk", "blaisethom}@apple.com"], "sections": [{"heading": "1 Introduction", "text": "Spoken dialogue systems (SDS) allow users to interact with computer applications through conversation. Task-oriented dialogue systems help users achieve goals such as finding restaurants or booking flights. The dialogue state tracking (DST) component of the SDS serves to interpret user input and update the belief state, which is the system\u2019s internal representation of the state of the conversation. This is a probability distribution over dialogue states used by the dialogue manager component to decide which action the system should perform next. The Dialogue State Tracking Challenge (DSTC)\nseries of shared tasks has provided a common evaluation framework accompanied by labelled datasets (Williams et al., 2016). In this framework, systems must track the search constraints expressed by users (goals) and questions the users ask about search results (requests), taking into account the user\u2019s utterance (passed through a speech recogniser) and the dialogue context (e.g., what the system just said). The following example shows the true state after each user utterance in a three-turn conversation:\nUser: I\u2019m looking for a cheap restaurant inform(price=cheap)\nSystem: How about Thai food? User: Yes please inform(price=cheap, food=Thai)\nSystem: The House serves cheap Thai food User: Where is it? inform(price=cheap, food=Thai); request(address)\nSystem: The House is at 106 Regent Street\nA task-based dialogue system is backed by an ontology which describes the range of user intents that the system can process and provides the building blocks of the belief state (i.e. price=cheap, address, etc.). DST systems depend on identifying mentions of ontology items in user utterances, which becomes a non-trivial task when confronted with lexical variation, noisy speech recognition and the effect of context. Some approaches assume that a separate Spoken Language Understanding (SLU) module will solve this problem for them. However, coupling SLU and DST in a single model has been\nar X\niv :1\n60 6.\n03 77\n7v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\nshown to improve tracking performance (Henderson et al., 2014d). On the other hand, these coupled models often rely on manually constructed dictionaries to identify inexact mentions of ontology items; this is clearly not scalable to larger, more complex domains.\nIn this paper, we present two new models, collectively called the Neural Belief Tracker (NBT) family. The NBT models couple SLU and DST, efficiently learning to handle lexical variation and input noise without requiring any hand-crafted resources. In evaluations on two datasets, we show that NBT models match the performance of state-of-the-art models that have greater resource requirements and outperform models with comparable task-specific requirements. Consequently, we believe this work proposes a framework better-suited to scaling belief tracking models for deployment in real-world dialogue systems operating over sophisticated application domains."}, {"heading": "2 Background", "text": "Models for probabilistic dialogue state tracking, or belief tracking, were introduced as components of spoken dialogue systems in order to better handle noisy speech recognition output and other sources of uncertainty in understanding a user\u2019s goals (Bohus and Rudnicky, 2006; Williams and Young, 2007; Young et al., 2010). Modern dialogue management policies can learn to use a tracker\u2019s distribution over intentions to decide whether to execute an action or request clarification from the user. As mentioned above, the DSTC shared tasks have spurred research on this problem and institutionalised a standard evaluation paradigm (Williams et al., 2013; Henderson et al., 2014b; Henderson et al., 2014a). In this setting, the task is defined by an ontology that enumerates the goals a user can inform and the attributes of entities that the user can request information about. Many different belief tracking models have been proposed in the literature, from generative (Thomson and Young, 2010) and discriminative (Henderson et al., 2014d) statistical models to rule-based systems (Wang and Lemon, 2013). In order to motivate the work presented here, we can categorise prior research according to their reliance (or otherwise) on a separate SLU module for interpreting the user\u2019s utterances:1\n1The best-performing models in DSTC2 all used both raw ASR output and the output of SLU decoders (Williams, 2014;\nSeparate SLU: Traditional SDS pipelines use Spoken Language Understanding (SLU) decoders to detect slot-value pairs expressed in the Automated Speech Recognition (ASR) output. The downstream DST model then combines this information with the past dialogue context to update the belief state (Thomson and Young, 2010; Wang and Lemon, 2013; Lee and Kim, 2016; Perez, 2016; Sun et al., 2016). In the DSTC challenges, some systems used the output of template-based matching systems such as Phoenix (Wang, 1994). However, more robust and accurate statistical SLU systems are available. Many discriminative approaches to spoken dialogue SLU train independent binary models that decide whether each slot-value pair was expressed in the user utterance. Given enough data, these models can learn which lexical features are good indicators for a given value and can capture elements of paraphrasing (Mairesse et al., 2009). This line of work later shifted focus to robust handling of rich ASR output (Henderson et al., 2012; Tur et al., 2013). SLU has also been treated as a sequence labelling problem, where each word in an utterance is labelled according to its role in the user\u2019s intent; standard labelling models such as CRFs or Recurrent Neural Networks can then be used (Raymond and Ricardi, 2007; Yao et al., 2014; Celikyilmaz and Hakkani-Tur, 2015; Mesnil et al., 2015). Other approaches adopt a more complex modelling structure inspired by semantic parsing (Saleh et al., 2014; Vlachos and Clark, 2014). One drawback shared by these methods is their resource requirements, either because they need to learn independent parameters for each slot and value or because they need finegrained manual annotation at the word level. This hinders scaling to larger, more realistic domains.\nJoint SLU/DST: Research on belief tracking has found it advantageous to reason about SLU and DST jointly, taking ASR predictions as input and generating belief states as output (Henderson et al., 2014d; Sun et al., 2014; Zilka and Jurcicek, 2015). In DSTC2, systems with no external SLU component outperformed all systems that only used external SLU features. Joint models often rely on a strategy known as delexicalisation whereby slots and values mentioned in the text are replaced with generic labels.\nWilliams et al., 2016). This does not mean that those models are immune to the drawbacks identified here for the two model categories; in fact, they share the drawbacks of both.\nOnce the dataset is transformed in this manner, one can extract a collection of template-like n-gram features such as [want tagged-value food]. To perform belief tracking, the shared model iterates over all slot-value pairs, extracting delexicalised feature vectors and making a separate binary decision regarding each pair. Delexicalisation introduces a hidden dependency that is rarely discussed: how do we identify slot/value mentions in text? For toy domains, one can manually construct semantic dictionaries which list the potential rephrasings for all slot values. As shown by Mrks\u030cic\u0301 et al. (2016), the use of such dictionaries is essential for the performance of current delexicalisation-based models. Again though, this will not scale to the rich variety of user language or to general domains.\nThe primary motivation for the work presented in this paper is to address the limitations that affect previous belief tracking models: we learn efficiently from the available data by maximising the number of parameters that are shared across the ontology, while still giving the system the flexibility to learn paraphrasings and other kinds of variation that make it infeasible to rely on exact matching and delexicalisation as a robust strategy."}, {"heading": "3 Neural Belief Tracker", "text": "The Neural Belief Tracker (NBT) is a model designed to detect the slot-value pairs expressed in any given user utterance during the flow of dialogue. Its input consists of the system dialogue acts preceding the user input, the user utterance itself, and a single slotvalue pair it needs to make a decision about. To do belief tracking, we iterate over all candidate pairs (defined by the domain ontology) and use the NBT to decide which ones have been expressed by the user.\nFigure 1 presents the flow of information in the model. The first layer in the NBT hierarchy performs representation learning for the three model inputs, producing vector representations for the user utterance (r), the current candidate slot-value pair (c) and the system dialogue acts (tq, ts, tv). Subsequently, the learned vector representations interact through the context modelling and semantic decoding submodules to obtain the intermediate interaction summary vectors dr,dc and d. These are used as input to the final decision-making module which decides\nwhether the user expressed the intent represented by the current candidate slot-value pair."}, {"heading": "3.1 Representation Learning", "text": "For any given user utterance, system act(s) and candidate slot-value pair, the representation learning submodules produce vector representations which act as input for the downstream components of the model. All representation learning subcomponents make use of pre-trained collections of word vectors. As shown by Mrks\u030cic\u0301 et al. (2016), specialising word vectors to express semantic similarity rather than relatedness-by-association is essential for improving belief tracking performance. For this reason, we use the semantically-specialised Paragram-SL999 word vectors (Wieting et al., 2015) throughout this work. The NBT training procedure keeps these vectors fixed: that way, at test time, unseen words semantically related to familiar slot values (i.e. inexpensive to cheap) will be recognised purely by their position in the original vector space. This also means that candidate slot-value pairs implicitly parametrise the NBT model, allowing its parameters to be shared across all values of a slot, or even across all slots.\nWe consider a user utterance u consisting of ku words u1, u2, . . . , uku . Each word has an associated word vector u1, . . . ,uku . We propose two different models for producing a representation of u: NBTDNN and NBT-CNN. Both act over the constituent ngrams of the utterance. Let vni be the concatenation of the n word vectors starting at index i, so that:\nvni = ui \u2295 . . .\u2295 ui+n\u22121 (1)\nwhere \u2295 denotes vector concatenation. The simpler version of the two models, which we term NBT-DNN, is shown in Figure 2. This model computes cumulative n-gram representation vectors r1, r2 and r3, which are the n-gram \u2018summaries\u2019 of the unigrams, bigrams and trigrams in the user utterance:\nrn = ku\u2212n+1\u2211 i=1 vni (2)\nEach of these vectors is then non-linearly mapped to intermediate representations of the same size:\nr\u2032n = \u03c3(W s nrn + b s n) (3)\nwhere the weight matrices and bias terms map the cumulative n-grams to vectors of the same size and \u03c3 denotes the sigmoid activation function. Here we maintain a separate set of parameters for each slot (indicated by superscript s); we later investigate tying parameters across all slots (Section 6.2). The three vectors are summed to obtain a single representation for the user utterance:\nr = r\u20321 + r \u2032 2 + r \u2032 3 (4)\nThe cumulative n-gram representations used by this model are just an unweighted sum of all word vectors in the utterance. Ideally, the model should learn to recognise which parts of the utterance are more relevant for the subsequent classification task. For instance, it could learn to ignore verbs or stop words and pay more attention to adjectives and nouns which are more likely to express slot values.\nOur second model, which we term NBT-CNN, draws inspiration from successful applications of Convolutional Neural Networks (CNNs) for language understanding (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). These models typically apply a number of convolutional filters to n-grams in the input sentence, followed by non-linear activation functions and max-pooling. Following this approach, the NBT-CNN model applies L = 300 different filters for n-gram lengths of 1, 2 and 3 (Figure 3). Let F sn \u2208 RL\u00d7nD denote the collection of filters for each value of n, where D = 300 is the word vector dimensionality. If vni denotes the concatenation of n word vectors starting at index i, let mn = [v n 1 ;v n 2 ; . . . ;v n ku\u2212n+1] be the list of n-grams that convolutional filters of length n run over. The three intermediate representations are then given by:\nRn = F s n mn (5)\nEach column of the intermediate matrices Rn is produced by a single convolutional filter of length n. We obtain summary n-gram representations by pushing these representations through a rectified linear unit (ReLu) activation function and max-pooling over time (i.e. columns) to get a single feature for each of the L filters applied to the utterance:\nr\u2032n = maxpool (ReLu (Rn + b s n)) (6)\nwhere bsn is a bias term broadcast across all filters. Finally, the three summary n-gram representations are\nsummed to obtain the final utterance representation vector r (as in Equation 4). The NBT-CNN model is (by design) better suited to longer utterances, as its convolutional filters interact with subsequences of the utterance, and not just the noisy summaries given by the NBT-DNN\u2019s cumulative n-grams."}, {"heading": "3.2 Semantic Decoding", "text": "The NBT diagram in Figure 1 shows that the utterance representation r and the candidate slot-value pair representation c directly interact through the semantic decoding module. This component decides whether the user explicitly expressed an intent matching the current candidate pair (i.e. without taking the dialogue context into account). Examples of such matches would be \u2018I want Thai food\u2019 with food=Thai or more demanding ones such as \u2018a pricey restaurant\u2019 with price=expensive. This is where the use of high-quality pre-trained word vectors comes into play: a delexicalisation-based model could deal with the former example but would be helpless in the latter case, unless a human expert had provided a semantic dictionary listing all potential rephrasings for each value in the domain ontology.\nLet the vector space representations of a candidate pair\u2019s slot name and value be given by cs and cv (with word vectors of multi-word slot names or values summed together). The NBT model learns to map this vector tuple into a single vector c of the same dimensionality as the utterance representation r. These two representations are then forced to interact in order to learn a similarity metric which discriminates between interactions of utterances with slot-value pairs that they either do or do not express:\nc = \u03c3 (W sc (cs + cv) + b s c) (7) d = r\u2297 c (8)\nwhere \u2297 denotes element-wise vector multiplication. The dot product, which may seem like the more intuitive similarity metric, would reduce the rich set of features in d to a single scalar. The element-wise multiplication allows the downstream network to make better use of its parameters by learning non-linear interactions between sets of features in r and c. This network (Binary Decision Maker in Figure 1) uses one intermediate hidden layer of size 100 to make the final decision about the current candidate pair."}, {"heading": "3.3 Context Modelling", "text": "This \u2018decoder\u2019 does not yet suffice to extract intents from utterances in human-machine dialogue. To understand some queries, the belief tracker must be aware of context, i.e. the flow of dialogue leading up to the latest user utterance. While all previous system and user utterances are important, the most relevant one is the last system utterance, in which the dialogue system could have performed (among others) one of the following two system acts:\n1. System Request: The system asks the user about the value of a specific slot Tq. If the system utterance is: \u2018what price range would you like?\u2019 and the user answers with any, the model must infer the reference to price range, and not to other slots such as area or food type.\n2. System Confirm: The system asks the user to confirm whether a specific slot-value pair (Ts, Tv) is part of their desired constraints. For example, if the user responds to \u2018how about Turkish food?\u2019 with \u2018yes\u2019, the model must be aware of the system act in order to correctly update the belief state.\nIf we make the Markovian decision to only consider the last set of system acts, we can incorporate context modelling into the NBT model. Let tq and (ts, tv) be the word vector representations of the arguments for the system request and confirm acts (zero vectors if none). The model computes the following measures of similarity between the system acts, candidate pair (cs, cv) and utterance representation r:\ndr = (cs \u00b7 tq)r (9) dc = (cs \u00b7 ts)(cv \u00b7 tv)r (10)\nwhere \u00b7 denotes dot product. The computed similarity terms act as gating mechanisms which only pass the utterance representation through if the system asked about the current candidate slot or slot-value pair. This type of interaction is particularly useful for the confirm system act: if the system asks the user to confirm, the user is likely not to mention any slot values, but to just respond affirmatively or negatively. This means that the model must consider the three-way interaction between the utterance, candidate slot-value pair and the slot value pair offered by the system. If\n(and only if) the latter two are the same should the model consider the affirmative or negative polarity of the user utterance when making the subsequent binary decision. Finally, these two context modelling summary representations are passed to the decision making module, which combines them with the semantic decoding output to make the final decision."}, {"heading": "4 Belief Tracking", "text": "In spoken dialogue systems, belief tracking models operate over the output of automated speech recognition (ASR). Despite ever-improving speech recognition, the need to make the most out of imperfect ASR will persist as dialogue systems are used in increasingly noisy environments. In this work, we use a simple rule-based belief state update mechanism which can be applied to ASR N -best lists.\nLet u denote a list of N ASR hypotheses hi with posterior probabilities pi which follow system output sys. For any hypothesis h, slot s and slot value v \u2208 Vs, the NBT model estimates P(s, v | h, sys), i.e. the probability that (s, v) was expressed in h. The predictions for N such hypotheses are combined as:\nP(s, v | u) = N\u2211 i=1 pi P(s, v | hi, sys) (11)\nFor slot s, the set of its detected values is given by:\nV \u2217s = {v \u2208 Vs | P (s, v | u) \u2265 0.5} (12)\nFor informable slots, the value in V \u2217s with the highest probability is chosen (if V \u2217s 6= \u2205). This goal value persists in subsequent turns until a new value is detected for this slot. For requests, all slots in V \u2217req are deemed to have been requested. As requestable slots serve to model users\u2019 single-turn queries, they require no belief tracking across turns."}, {"heading": "5 Experiments", "text": "Two datasets were used for training and evaluation. Both consist of user conversations with task-oriented dialogue systems designed to help users find suitable restaurants around Cambridge, UK. The two corpora share the same domain ontology, which contains three informable (i.e. goal) slots: food type, area and price. The users can specify values for these slots in order to find restaurants\nwhich best meet their criteria. Once the system suggests a restaurant, the users can ask about the values of up to eight requestable slots (phone number, address, etc.). The two datasets are:\n1. DSTC2: we use the transcriptions, ASR hypotheses and turn-level semantic labels provided for the Dialogue State Tracking Challenge 2 (Henderson et al., 2014a). The official transcriptions contain various spelling errors which we corrected manually; the cleaned version of the dataset will be made available on the first author\u2019s website. The training data contains 2207 dialogues (15611 turns) and the test set consists of 1117 dialogues. We train the NBT models on transcriptions and report their belief tracking performance on test set ASR hypotheses.\n2. WOZ: Wen et al. (2016) performed a Wizard of Oz style experiment in which Amazon Mechanical Turk users assumed the role of the system or the user of a task-oriented dialogue system based on the DSTC2 ontology. Users typed instead of using speech, which means performance in the WOZ experiments is more indicative of the model\u2019s capacity for semantic understanding than its robustness to ASR errors. The experimental design gave users more freedom to use sophisticated language, making understanding much more difficult. The WOZ dataset contains 536 training dialogues (2209 turns) and 140 test dialogues.\nThe two corpora are used to synthesise training data for the NBT models. We iterate over all utterances, generating one example for each of the slot-value pairs in the DSTC2 ontology. Examples consist of a transcription, its context (i.e. list of system acts) and a candidate slot-value pair. The binary label for each example indicates whether or not its utterance and context express the example\u2019s candidate pair. To train the model, we used the Adam optimizer (Kingma and Ba, 2015) with a cross-entropy loss function, backpropagating through all the NBT subcomponents (but keeping the word vectors fixed). Both models were implemented using the TensorFlow framework (Abadi et al., 2015).\nTraining data: WOZ; Test data: WOZ\nModel Joint Goals Requests prec. recall fscore prec. recall fscore\nRNN-CNN 0.99 0.90 0.94 0.97 0.94 0.95 + sem. dict. 1.00 0.94 0.96 0.97 0.95 0.96 NBT-DNN 1.00 0.82 0.90 0.99 0.95 0.97 NBT-CNN 0.99 0.95 0.97 0.94 0.97 0.96\nTraining data: DSTC2; Test data: WOZ"}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Belief Tracking Performance", "text": "Table 1 shows the belief tracking performance of models trained and evaluated on DSTC2 data. We compare to a delexicalisation-based RNN model introduced by Henderson et al. (2014d; 2014c). The baseline model uses no semantic dictionary, while the improved baseline uses a hand-crafted semantic dictionary designed for the DSTC2 ontology by Henderson et al. The NBT models clearly outperform the first baseline in terms of average goal accuracy but fare marginally worse on requests. The NBT models\u2019 goal tracking performance is comparable to the improved baseline. This shows that these models can handle semantic relations which otherwise had to be explicitly encoded in semantic dictionaries. As NBT\u2019s rule-based belief state update is less sophisticated than Henderson et al.\u2019s RNN, we believe that further gains are possible.\nThe WOZ dataset provides a more focused benchmark for assessing the semantic understanding capacity of belief tracking models. The baseline performance is a delexicalisation-based model incorpo-\nrating an RNN for belief state updates and a CNN for turn-level feature extraction (Wen et al., 2016). Again, the improved baseline uses a hand-crafted semantic dictionary. Table 2 shows the performance of NBT models trained on either DSTC2 or WOZ transcriptions. The NBT-CNN model outperforms the baselines across the board, even when it is trained using the (simpler) DSTC2 transcriptions. This shows that the CNN-based model is capable of handling long utterances and previously unseen vocabulary. Conversely, the NBT-DNN model falls short of both goal tracking baselines, as its cumulative n-gram representations cannot cope with some of the very long and complex WOZ utterances. Contrary to the results in Table 1, both NBT models trained on indomain WOZ data match or exceed the baselines\u2019 performance over requestable slots."}, {"heading": "6.2 General Models: Slot-Parameter Tying", "text": "To train a single model which can be applied to all slots in the ontology (as opposed to using a separate model for each slot), we coalesce all training data and use it to train an NBT model. As shown by Mrks\u030cic\u0301 et al. (2015), such parameter tying can facilitate transfer learning and lead to improved belief tracking performance. Table 3 shows the impact of tying the parameters across all slots for NBT models trained on DSTC2 and WOZ together: it boosts NBT-DNN performance and leads to an improved trade-off between NBT-CNN\u2019s performance on informable and\nrequestable slots. Figure 4 shows the t-SNE visualisation (van der Maaten and Hinton, 2008) of the user utterance representations produced by the NBT-DNN model with tied parameters. The figure shows that the model learns meaningful representations which cluster utterances with related intents together. Moreover, it reveals that utterances such as \u2018yes\u2019 and \u2018right\u2019 or \u2018I don\u2019t care\u2019 and \u2018It doesn\u2019t matter\u2019 are mapped close together, proving that the NBT utterance representations capture a lot of semantic information from the underlying word vectors and the (relatively few) annotated transcriptions that they are exposed to."}, {"heading": "6.3 The Importance of Word Vector Spaces", "text": "The NBT models use the semantic relations embedded in the pre-trained word vectors to handle semantic variation and produce high-quality intermediate representations. Table 4 shows the effect of using different word vectors to train the two NBT models. Using GloVe vectors (Pennington et al., 2014) in place of Paragram-SL999 (Wieting et al., 2015) drastically reduced the models\u2019 goal tracking capabilities. Interestingly, it had much less of an effect on requests, suggesting that the training set is large enough for the NBT models to learn specific phrases which trigger these (\u2018How much?\u2019, \u2018Where is it?\u2019), but not to ensure coverage for the numerous rephrasings of categorical slot values."}, {"heading": "7 Conclusion", "text": "In this paper, we have proposed a novel belief tracking framework designed to overcome current obstacles to deploying dialogue systems in larger dialogue domains. The Neural Belief Tracker can use a single set of parameters to decide whether the given user utterance (and its context) express each of the intents defined by the domain ontology. This is achieved by learning similarity metrics which operate over intermediate representations composed from semantically specialised word vector representations. The NBT framework offers the known advantages of coupling Spoken Language Understanding and Dialogue State Tracking, without requiring the use of (hand-crafted) semantic lexicons to boost SLU recall. Our evaluation demonstrates these benefits, with NBT matching the performance of models that rely on such lexicons. Finally, we showed that sharing NBT models\u2019 parameters across slots can boost overall belief tracking performance, supporting our argument that these models promise to scale to complex domains with diverse and unequally distributed slots."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Ulrich Paquet and members of the Cambridge Dialogue Systems Group for helpful discussions."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["sudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "sudevan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "sudevan et al\\.", "year": 2015}, {"title": "A \u201ck hypotheses + other\u201d belief updating model", "author": ["Bohus", "Rudnicky2006] Dan Bohus", "Alex Rudnicky"], "venue": "In Proceedings of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue Systems", "citeRegEx": "Bohus et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bohus et al\\.", "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Discriminative Spoken Language Understanding Using Word Confusion Networks", "author": ["Milica Ga\u0161i\u0107", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young"], "venue": "In Spoken Language Technology Workshop,", "citeRegEx": "Henderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2012}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The Third Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Robust Dialog State Tracking using Delexicalised Recurrent Neural Networks and Unsupervised Adaptation", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-Based Dialog State Tracking with Recurrent Neural Networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Dialog History Construction with Long-Short Term Memory for Robust Generative Dialog State Tracking", "author": ["Lee", "Kim2016] Byung-Jun Lee", "Kee-Eung Kim"], "venue": "Dialogue & Discourse,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Spoken Language Understanding from Unaligned Data using Discriminative Classification Models", "author": ["Mairesse et al.2009] F. Mairesse", "M. Gasic", "F. Jurcicek", "S. Keizer", "B. Thomson", "K. Yu", "S. Young"], "venue": "Proceedings of ICASSP", "citeRegEx": "Mairesse et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairesse et al\\.", "year": 2009}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Hakkani-Tur", "Xiaodong He", "Larry Heck", "Dong Yu", "Geoffrey Zweig."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(3):530\u2013539.", "citeRegEx": "Hakkani.Tur et al\\.,? 2015", "shortCiteRegEx": "Hakkani.Tur et al\\.", "year": 2015}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Mrk\u0161i\u0107 et al.2015] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proceedings of ACL", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Counter-fitting Word Vectors to Linguistic Constraints", "author": ["Mrk\u0161i\u0107 et al.2016] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proceedings of HLT-NAACL", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Spectral decomposition method of dialog state tracking via collective matrix factorization", "author": ["Julien Perez"], "venue": "Dialogue & Discourse,", "citeRegEx": "Perez.,? \\Q2016\\E", "shortCiteRegEx": "Perez.", "year": 2016}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Raymond", "Ricardi2007] Christian Raymond", "Giuseppe Ricardi"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Raymond et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raymond et al\\.", "year": 2007}, {"title": "A study of using syntactic and semantic structures for concept segmentation and labeling", "author": ["Saleh et al.2014] Iman Saleh", "Shafiq Joty", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Preslav Nakov", "Scott Cyphers", "Jim Glass"], "venue": "Proceedings of COLING", "citeRegEx": "Saleh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saleh et al\\.", "year": 2014}, {"title": "The SJTU System for Dialog State Tracking Challenge", "author": ["Sun et al.2014] Kai Sun", "Lu Chen", "Su Zhu", "Kai Yu"], "venue": "Proceedings of SIGDIAL", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Recurrent Polynomial Network for Dialogue State Tracking", "author": ["Sun et al.2016] Kai Sun", "Qizhe Xie", "Kai Yu"], "venue": "Dialogue & Discourse,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems", "author": ["Thomson", "Young2010] Blaise Thomson", "Steve Young"], "venue": "Computer Speech and Language", "citeRegEx": "Thomson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2010}, {"title": "Semantic Parsing Using Word Confusion Networks With Conditional Random Fields", "author": ["Tur et al.2013] Gokhan Tur", "Anoop Deoras", "Dilek Hakkani-Tur"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Tur et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tur et al\\.", "year": 2013}, {"title": "Visualizing High-Dimensional Data Using t-SNE", "author": ["van der Maaten", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "A new corpus and imitation learning framework for context-dependent semantic parsing", "author": ["Vlachos", "Clark2014] Andreas Vlachos", "Stephen Clark"], "venue": null, "citeRegEx": "Vlachos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vlachos et al\\.", "year": 2014}, {"title": "A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information", "author": ["Wang", "Lemon2013] Zhuoran Wang", "Oliver Lemon"], "venue": "Proceedings of SIGDIAL", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Extracting Information From Spontaneous Speech", "author": ["Wayne Wang"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Wang.,? \\Q1994\\E", "shortCiteRegEx": "Wang.", "year": 1994}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Wen et al.2016] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["Wieting et al.2015] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["Williams", "Young2007] Jason D. Williams", "Steve Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Williams et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2007}, {"title": "The Dialogue State Tracking Challenge", "author": ["Antoine Raux", "Deepak Ramachandran", "Alan W. Black"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "The Dialog State Tracking Challenge series: A review", "author": ["Antoine Raux", "Matthew Henderson"], "venue": "Dialogue & Discourse,", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Web-style ranking and SLU combination for dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "Proceedings of ASRU", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "The hidden information state model: A practical framework for POMDP-based spoken dialogue management", "author": ["Young et al.2010] Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Young et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 32, "context": "The Dialogue State Tracking Challenge (DSTC) series of shared tasks has provided a common evaluation framework accompanied by labelled datasets (Williams et al., 2016).", "startOffset": 144, "endOffset": 167}, {"referenceID": 35, "context": "Models for probabilistic dialogue state tracking, or belief tracking, were introduced as components of spoken dialogue systems in order to better handle noisy speech recognition output and other sources of uncertainty in understanding a user\u2019s goals (Bohus and Rudnicky, 2006; Williams and Young, 2007; Young et al., 2010).", "startOffset": 250, "endOffset": 322}, {"referenceID": 31, "context": "As mentioned above, the DSTC shared tasks have spurred research on this problem and institutionalised a standard evaluation paradigm (Williams et al., 2013; Henderson et al., 2014b; Henderson et al., 2014a).", "startOffset": 133, "endOffset": 206}, {"referenceID": 17, "context": "The downstream DST model then combines this information with the past dialogue context to update the belief state (Thomson and Young, 2010; Wang and Lemon, 2013; Lee and Kim, 2016; Perez, 2016; Sun et al., 2016).", "startOffset": 114, "endOffset": 211}, {"referenceID": 21, "context": "The downstream DST model then combines this information with the past dialogue context to update the belief state (Thomson and Young, 2010; Wang and Lemon, 2013; Lee and Kim, 2016; Perez, 2016; Sun et al., 2016).", "startOffset": 114, "endOffset": 211}, {"referenceID": 27, "context": "In the DSTC challenges, some systems used the output of template-based matching systems such as Phoenix (Wang, 1994).", "startOffset": 104, "endOffset": 116}, {"referenceID": 12, "context": "Given enough data, these models can learn which lexical features are good indicators for a given value and can capture elements of paraphrasing (Mairesse et al., 2009).", "startOffset": 144, "endOffset": 167}, {"referenceID": 3, "context": "robust handling of rich ASR output (Henderson et al., 2012; Tur et al., 2013).", "startOffset": 35, "endOffset": 77}, {"referenceID": 23, "context": "robust handling of rich ASR output (Henderson et al., 2012; Tur et al., 2013).", "startOffset": 35, "endOffset": 77}, {"referenceID": 34, "context": "current Neural Networks can then be used (Raymond and Ricardi, 2007; Yao et al., 2014; Celikyilmaz and Hakkani-Tur, 2015; Mesnil et al., 2015).", "startOffset": 41, "endOffset": 142}, {"referenceID": 19, "context": "Other approaches adopt a more complex modelling structure inspired by semantic parsing (Saleh et al., 2014; Vlachos and Clark, 2014).", "startOffset": 87, "endOffset": 132}, {"referenceID": 20, "context": "Joint SLU/DST: Research on belief tracking has found it advantageous to reason about SLU and DST jointly, taking ASR predictions as input and generating belief states as output (Henderson et al., 2014d; Sun et al., 2014; Zilka and Jurcicek, 2015).", "startOffset": 177, "endOffset": 246}, {"referenceID": 14, "context": "As shown by Mrk\u0161i\u0107 et al. (2016), the use of such dictionaries is essential for the performance of current delexicalisation-based models.", "startOffset": 12, "endOffset": 33}, {"referenceID": 29, "context": "For this reason, we use the semantically-specialised Paragram-SL999 word vectors (Wieting et al., 2015) throughout this work.", "startOffset": 81, "endOffset": 103}, {"referenceID": 14, "context": "As shown by Mrk\u0161i\u0107 et al. (2016), specialising word vectors to express semantic similarity rather than relatedness-by-association is essential for improving belief tracking performance.", "startOffset": 12, "endOffset": 33}, {"referenceID": 28, "context": "WOZ: Wen et al. (2016) performed a Wizard of Oz style experiment in which Amazon Mechanical Turk users assumed the role of the system or the user of a task-oriented dialogue system based on the DSTC2 ontology.", "startOffset": 5, "endOffset": 23}, {"referenceID": 28, "context": "rating an RNN for belief state updates and a CNN for turn-level feature extraction (Wen et al., 2016).", "startOffset": 83, "endOffset": 101}, {"referenceID": 14, "context": "As shown by Mrk\u0161i\u0107 et al. (2015), such parameter tying can facilitate transfer learning and lead to improved belief tracking performance.", "startOffset": 12, "endOffset": 33}, {"referenceID": 16, "context": "Using GloVe vectors (Pennington et al., 2014) in place of Paragram-SL999 (Wieting et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 29, "context": ", 2014) in place of Paragram-SL999 (Wieting et al., 2015) drastically reduced the models\u2019 goal tracking capabilities.", "startOffset": 35, "endOffset": 57}], "year": 2016, "abstractText": "Belief tracking is a core component of modern spoken dialogue system pipelines. However, most current approaches would have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted semantic lexicons that capture the lexical variation in users\u2019 language. We propose a novel Neural Belief Tracking (NBT) framework which aims to overcome these problems by building on recent advances in semantic representation learning. The NBT models reason over continuous distributed representations of words, utterances and dialogue context. Our evaluation on two datasets shows that this approach overcomes both limitations, matching the performance of state-of-the-art models that have greater resource requirements.", "creator": "LaTeX with hyperref package"}}}