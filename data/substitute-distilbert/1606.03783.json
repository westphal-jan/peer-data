{"id": "1606.03783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2016", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression", "abstract": "presented herein is a novel model emphasizing similar question scenarios within collaborative question answer formats. the presented approach integrates a regression stage to relate topics derived from questions to those stemming from stimulus - answer pairs. this helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the stability for questions to be shorter than answers. the performance of a model is shown to outperform translation methods beyond topic modelling ( without contradiction ) on several real - world datasets.", "histories": [["v1", "Sun, 12 Jun 2016 23:50:19 GMT  (67kb,D)", "http://arxiv.org/abs/1606.03783v1", "International Conference on Theory and Practice of Digital Libraries 2016 (accepted)"]], "COMMENTS": "International Conference on Theory and Practice of Digital Libraries 2016 (accepted)", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["pedro chahuara", "thomas lampert", "pierre gancarski"], "accepted": false, "id": "1606.03783"}, "pdf": {"name": "1606.03783.pdf", "metadata": {"source": "CRF", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives Using Topic Modelling and Topic Distribution Regression", "authors": ["Pedro Chahuara", "Thomas Lampert", "Pierre Gan\u00e7arski", "P. Gan\u00e7arski"], "emails": ["pedro.chahuara@unistra.fr", "lampert@unistra.fr", "pierre.gancarski@unistra.fr"], "sections": [{"heading": null, "text": "Keywords: Collaborative Question Answering, Question and Answer Retrieval, LDA, Neural Network, Topic Modelling, Regression"}, {"heading": "1 Introduction", "text": "During the last decade internet based Collaborative Question Answering (CQA) platforms have increased in popularity. These platforms offer a social environment for people to seek answers to questions, and where the answers are offered by other community members. Users pose questions in natural language, as opposed to queries in web search engines, and community members propose answers in addition to voting and rating the information posted on the platform. Some of the most popular CQA sites are Yahoo! Questions, Quora, and StackExchange. Besides public CQA websites, similar systems can be found in industry, for example in retail and business websites where users can pose questions about a company\u2019s product and a group of specialists can give support.\nThis content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted. Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8]. The latter is the problem that is covered in the present work. As such, the system helps to remove the delay needed for other community members to answer; and the list of related questions provides material for users to acquire more knowledge on the topic of their question. ar X\niv :1\n60 6.\n03 78\n3v 1\n[ cs\n.I R\n] 1\n2 Ju\nn 20\n16\nSolving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9]. For instance the questions \u201cWhere can I watch movies on the internet for free?\u201d and \u201cAre there any sites for streaming films?\u201d are semantically related but lexically different. The opposite case is also possible\u2014questions having words in common may have different semantic meanings. Besides the need for accurately identifying a question\u2019s semantics, a solution to the problem must deal with noisy information such as: misspelt words, polysemy, and short questions.\nSimilar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5]. Nevertheless Xue et al. demonstrated that combining information derived from existing questions and their answers outperforms the other strategies [5]. In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy. These approaches, however, have thus far only been used to model the questions in the archives. As such, the contribution of the present work is twofold: firstly, the application of Latent Dirichlet Allocation (LDA) to model topics among the questions and answers in the archive; and secondly, the use of a regression step to estimate the appropriate QA topic distribution from that of a novel question.\nThis paper is organized as follows: Section 2 presents the state of the art, Section 3 the study\u2019s methodology, Section 4 the experimental setup and results, which are discussed in Section 5, and conclusions are presented in Section 6."}, {"heading": "2 Related Work", "text": "The principal challenge when retrieving related questions and answers in a QA database given a new question is the lexical gap that may exist between two semantically similar questions. In general, a method that intends to solve the problem of question retrieval should be composed at least of two main parts: a document representation that can properly express the semantics and context of QAs in the database; and a mechanism for comparing the similarity of documents given their representations. The most widespread document representation methods in the literature are those based on bag-of-words (BOW), which explicitly represents each of the document\u2019s words. Comparison is achieved by computing the number of matching words between two BOW representations. There exist several variations of this class of methods, each weighting words that have specific properties in the dataset, such as tf-idf and BM25 [12]. This class of methods is able to measure two documents\u2019 lexical similarity but it does not capture information regarding their semantics and context.\nIn QA databases, questions and answers are often short and contain many word variations resulting from grammatical inflection, misspelling, and informal abbreviations. As a consequence, BOW representations in QA corpora produce a vector representation that can be too sparse. Besides sparsity, BOW represen-\ntations do not provide a measure of co-occurrence or shared contextual information, which can increase the similarity of related documents.\nAn approach that overcomes these limitations is the translation model, first proposed for use in this context by Jeon et al. [1]. Their method consists of two stages: first a set of semantically similar questions are found by matching their answers using a query-likelihood language model; and subsequently, word translation probabilities are estimated using the IBM translation model 1 [13]. Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia. Xue et al. [5] propose an extension that combines the IBM translation model (applied to the questions) with a query likelihood language model (applied to the answers). Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.\nIn the last decade Topic Modeling has become an important method for text analysis. Since the topics that characterise a document can be considered a semantic representation, it is possible to use topic distributions inferred using a method such as Latent Dirichlet Allocation (LDA) [17] to measure the semantic similarity between documents in a corpora. Consequently, several approaches for applying topic modelling to QA archives have been proposed: Zhang et al. [2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al. [10] combine the result of LDA and translation models; Vasiljevic\u0301 et al. [11] explore combining a document\u2019s word count and topic model similarity into one measure; and Yang et al. [8] form a generative probabilistic method to jointly model QA topic distributions and user expertise. In all of the above-mentioned topic modelling approaches similarity is calculated using the questions that exist in the database.\nThis work explores the possibility of deriving topic distributions from existing questions and answers, and proposes a method to relate these to the topic distribution of a novel question. Some work has been done in this direction; Zolaktaf et al. [18] model the question topics and then use them to condition the answer topics. This work proposes to model question and question-answer topics independently and then to learn a mapping between them. Furthermore, it extends topic modeling to include distributed word representations."}, {"heading": "3 Methodology", "text": "A corpora C of size L = |C| consists of many question-answer pairs: C = {(q1, a1), (q2, a2), . . . , (qL, aL)}, where Q = {q1, q2, . . . , qL} and A = {a1, a2, . . . , aL}, \u2200(qi, ai) \u2208 C : qi \u2208 Q, ai \u2208 A, are question and answer sets (respectively).\nQuestions in a CQA corpora tend to be shorter than answers and may contain few relevant words, which limits a model\u2019s ability to discover underlying trends. An approach to mitigate this is to assume that each question qi contains its text, and possibly keywords, a title, and a description. This assumption is not a requirement as meta-data may not always be available; however its absence may limit the ability of a model to represent the questions. We discuss this further in\nthis section and propose methods to overcome the problem of question sparsity. Furthermore, each question in a QA corpora may have multiple answers and these are concatenated to form each element ai as they all provide contextual information that can be exploited to determine the question\u2019s relevance.\nFigure 1 presents the proposed methodology. The task of similar question retrieval implies ranking the pairs contained in the QA Corpora (C) according to their similarity to a query question q\u2217, producing a partially ordered set C \u2032 such that its first element has the highest similarity (the top, say, ten elements of which can then be returned as suggestions). In the learning phase of the proposed methodology, the QA corpora is used to train two topic models (Section 3.1): LDA on the set Q, and LDA on the set QA, in which each pair (qi, ai) \u2208 C is concatenated to form a single document. This results in topic distributions associated with the sets Q and QA and each element contained therein (\u03b8Qi and \u03b8QAi respectively). A regression model is trained using the samples \u03b8 Q i and \u03b8QAi (Train NN) to learn the translation function between the Q and QA topic distributions (Section 3.2). During inference the Q set LDA model is used to determine the topic distribution of a query question (\u03b8Q\u2217 ) which is translated to a QA topic distribution (\u03b8QA\u2217 ) using the regression model. Finally, a similarity measure (Section 3.2) is used to rank the QA Corpora (QA) according the similarity between each pair\u2019s topic distribution (\u03b8QAi ) and the query question\u2019s QA topic distribution (\u03b8QA\u2217 ) obtained from the regression model. The LDA and regression models are discussed in more detail in the following subsections."}, {"heading": "3.1 Latent Dirichlet Allocation & Distributed Word Representation", "text": "In this work we assert that topic modeling provides a representation of the elements in C that facilitates the discovery of semantically similar questions; particularly when these similar questions do not have words in common.\nLatent Dirichlet Allocation (LDA) [17] is a generative probabilistic model that enables us to describe a collection of discrete observations in terms of latent variables. The plate notation representing LDA is presented in Figure 2. When applied to a corpora, LDA models the generation of each document by means of two stochastic independent processes and can be summarised as follows 1. For each document d in the collection D, randomly choose a distribution over\ntopics \u03b8d \u223c Dir(\u03b1), where \u03b1 is the Dirichlet prior. 2. For each word wn in document d:\n(a) choose a topic from the distribution over topics in Step 1. zd,n \u223c Mult(\u03b8d); (b) choose a word from the vocabulary distribution wd,n \u223c Mult(\u03c6zd,n). After learning a corpora\u2019s latent variables a topic is represented as a multinomial distribution of words, and a document by a multinomial distribution of topics.\nThe LDA algorithm described above treats words as explicit constraints, which inhibits its effectiveness when words are rare. A solution is to treat words as features [19] and the method used to calculate a word\u2019s features then influences its topic membership. This allows us to exploit a word\u2019s semantic similarity to augment information in short questions by giving similar topic membership probabilities to semantically equivalent words. For example, the words \u201ceducator\u201d, \u201ceducation\u201d, \u201ceducational\u201d, and \u201cinstruction\u201d should have similar probabilities within a certain topic, even if some of these words appear rarely in the corpus.\nMikolov et al. [20] introduced the continuous bag-of-words and Skip-gram neural network models that produce a continuous-valued vectorial word representation by exploiting the content of large textual databases. Distances between these vectors are proportional to the semantic difference of the words they represent, and thus these vectors can be used as features in many NLP tasks. In this work, the Word2vec vector representation is used to group semantically related words; its use for this application was first proposed by Petterson et al. [19].\nIn the original LDA algorithm, a word is generated by the process wd,n \u223c Mult(\u03c6zd,n) where \u03c6zd,n is the multinomial distribution (Mult) of word probabilities in topic zd,n over the whole vocabulary. In order to introduce the distributed representation of words, we define a function v : R \u2192 Rr that maps a word to its vectorial representation learnt by Word2vec, where r the number\nof latent features used for the distributed word representation (in practice this function is represented by a matrix \u03c9 \u2208 RN\u00d7r, where N is the vocabulary size). Given two words w and w\u2032 their semantic similarity can be found by applying the cosine similarity function, see Eq. (4), to their vectorial representations, i.e. Similarity(v(w), v(w\u2032)). A set of words that are similar to w, \u2126w, can be obtained by defining a threshold \u03c4 such that\u2126w = {w\u2032 | Similarity(v(w), v(w\u2032)) > \u03c4}. This set can be used to define an alternative distribution of word probabilities \u03c6\u2032zd,n for topic zd,n, in which the probability of a word w is given by\n\u03c6\u2032zd,n(w) = 1\nc \u2211 w\u2032\u2208\u2126w exp ( \u03c6zd,n(w \u2032) Similarity (v(w), v(w\u2032)) ) , (1)\nwhere c is a normalisation factor. This modified distribution gives a high probability to semantically related words. Finally we consider each word w to be sampled from a linear combination of the original and modified distributions\nwd,n \u223c \u03bbMult(\u03c6zd,n) + (1\u2212 \u03bb) Mult(\u03c6\u2032zd,n), 0 \u2264 \u03bb \u2264 1. (2)\nWe fixed \u03bb to 0.9 so that the results of standard LDA are not excessively altered. In order to implement this modification, the Gibbs Sampling-based algorithm proposed by [21] was adapted so that at each step the probability of topic t being present in document d given word w is estimated as follows:\np(z = t | w) = \u03b1\u03b2 \u03b2V + n.|t + nt|d\u03b2 \u03b2V + n.|t +\n( \u03b1+ nt|d ) \u03bbnw|t\n\u03b2V + n.|t\n+ 1\u2212 \u03bb c \u2211 w\u2032\u2208\u2126w exp ( nw\u2032|t Similarity (v(w), v(w \u2032)) n.|t ) , (3)\nwhere nw|t is the number of words w assigned to topic t, nt|d is the total number of words in document d assigned to topic t, n.|t = \u2211 w nw|t, \u03b1 = 35/T is the Dirichlet prior of the per document topic distribution (for number of topics T ), and \u03b2 = 0.01 [21, 22]. Small values of \u03b1 and \u03b2 result in a fine-grained decomposition into topics that address specific areas [21,22].\nThis method is applied to two document collections (Figure 1), Q and QA, which results in two topic models: TQ = {1, . . . ,KQ} in which each question qi is represented by the distribution of topics \u03b8Qi ; and TQA = {1, . . . ,KQA} in which each pair (qi, ai) is represented by the distribution of topics \u03b8 QA i ."}, {"heading": "3.2 Nonlinear Multinomial Regression", "text": "When a query question q\u2217 is entered the left-to-right method [23] is used to infer its topic distribution, \u03b8Q\u2217 . A regression model is therefore needed to obtain an estimate of \u03b8Q\u2217 mapped to a distribution of topics in the QA set, \u03b8 QA \u2217 . Mapping the distribution of question topics to the distribution of question-answer topics avoids problems that occur when limited vocabularies are used in a question.\nThis information is augmented with that derived from the set of answer terms, thus by mapping a query question to the space of question-answers it is possible to calculate its similarity using words that do not exist in the question vocabulary (and therefore are not represented in the topic distribution TQ). Performing this mapping also provides a means to model the relationship between question semantics and existing question-answer semantics (which will be discussed further in Section 5): given a query question q\u2217 the model estimates a topic distribution in the space of concatenated questions and answers, which can be compared to the distributions of existing QA pairs.\nDetermining the topic distribution in the space of documents comprising questions and answers, given the topic distribution of a new question is a problem of multinomial regression. For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25]. The NN is trained using the set of topic distributions for each document in Q and QA, \u03b8Qi and \u03b8 QA i (respectively) where i = 1, . . . , L, and therefore the input and output layers have as many nodes as the number of topics used to model these sets, KQ and KQA (respectively). Sigmoid activation functions are used in the hidden layer and softmax in the output layer to ensure that outputs sum to one.\nIn application the input of the NN is the topic distribution of the query question according to latent topic model of the existing questions, represented by \u03b8Q\u2217 , and its output is an estimate of its distribution in the QA latent topic model, \u03b8QA\u2217 . The cosine similarity measure allows us to rank existing questions qi according to their similarity to \u03b8 QA \u2217 , i.e.\nSimilarity ( \u03b8QA\u2217 , \u03b8 QA i ) =\n\u03b8QA\u2217 \u00b7 \u03b8QAi \u2016\u03b8QA\u2217 \u2016\u2016\u03b8QAi \u2016 , (4)\nwhere \u2016x\u2016 is the length of vector x, and therefore the most similar existing questions appear at the top of the ranked list that is output by the system."}, {"heading": "4 Evaluation", "text": "This section describes the data, experimental setup, and comparison algorithms used to evaluate the proposed approach."}, {"heading": "4.1 Data", "text": "Four categories, derived from two different CQA sources, are used for the evaluation. The first two are the Health and Computers & Internet (referred to herein as Computers) categories in the publicly available Yahoo! Questions L6 (Yahoo! Answers Comprehensive Questions and Answers version 1.0) dataset1. The second two are the Physics and Geographic Information Systems (GIS) categories taken from the publicly available StackExchange (SE) dataset2. The question\n1 Available from http://webscope.sandbox.yahoo.com/catalog.php?datatype=l 2 Available from https://archive.org/details/stackexchange\nsets extracted from the Yahoo! dataset were created by concatenating the question text and description (when available), and the question sets extracted from the SE dataset were created by concatenating the question title, tags, and text. The answer sets were created by concatenating all the answers provided by different users for a particular question. Table 1 summarises these datasets.\nPreprocessing was performed before data use: stop words were removed using Mallet\u2019s standard English list (543 words), non-English characters were removed, and lemmatization was performed to reduce the number of inflected word forms.\nFifty randomly selected questions from each category were used for testing and the remaining pairs were used as training data. Therefore four models were calculated using each algorithm, one for each category. The output of each model (the top ten most similar results for each test question) were manually labelled as relevant or not and this was used to calculate the evaluation statistics.\nThe Word2vec model requires training in order to learn the word embedding space, and this was realised using an additional corpus of Google news and Yahoo! Questions QA pairs (from categories other than those presented previously). The reason for including documents form Yahoo! Questions in this corpus is that it enables words that are specific to the dataset\u2014such as abbreviations, misspellings, and technical jargon\u2014to be learnt.\nA modified version of Mallet, which implements the Gibbs sampling method proposed by Yao et al. [21], was used for Topic Modeling. The number of topics were empirically set to 140 and 160 for the Q and QA sets (respectively) and the size of the neural network\u2019s hidden layer was empirically set to 180 using 100 questions-answer pairs (these were subsequently removed from the corpus)."}, {"heading": "4.2 Results", "text": "The proposed method, referred to henceforth as LDA+, was compared to four state-of-the-art algorithms: Translation1, the IBM translation approach proposed by Jeon et al. [1]; Translation2, the combined translation and querylikelihood language model proposed by Xue et al. [5]; an autoencoder based method proposed by Socher et al. [26]; to establish the benefit of word2vec, LDA\u2217 (as described within Section 3 excluding word2vec); and to establish the benefit of the regression stage, LDA\u2020 (as described within Section 3 excluding the regression step).\nMean Average Precision (MAP) and Precision at N (P@N) are used to summarise retrieval performance within each category. The autoencoder was found to be computationally infeasible when applied to the described datasets and therefore its retrieval performance is not presented. The results obtained using the remaining methods are presented in Table 2. A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].\nThe results show that in all of the datasets, LDA+ outperforms all other methods. However, the difference is much more pronounced when the length of the question and answers increase (as is the case in the SE datasets). In this situation, the translation methods fail to find relevant documents whereas all of the LDA methods do (due to the increase in information). It is difficult to separate the performances of LDA with Word2vec and LDA with regression (LDA\u2020 and LDA\u2217), but when combined (LDA+) a performance increase is observed."}, {"heading": "5 Discussion", "text": "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1. This forces questions that share words in common with the query question to be highly ranked. Conversely,\nLDA\u2020, LDA\u2217, and LDA+ perform matching based upon shared topics, and inherently accounts for words that represent multiple concepts by decreasing their probabilities in the topics that they appear. To illustrate this, Table 3 presents an example of retrieved questions using LDA+ and the two translation based approaches3 (the points discussed in this section were observed in all of the categories but to save space we present examples from the Health category). In the first example, presented in the top half of the table, the QA pairs retrieved by LDA+ do not contain the words \u201clift\u201d and \u201cweight\u201d even though they are relevant to the query. The excessive contribution from the word \u201cweight\u201d causes the translation models to retrieve questions that are related to body weight instead of weight lifting. The second example illustrates a query in which all the retrieved QA pairs are relevant. As before, the translation methods result in questions that have words in common with the query question (as does LDA+); in this case Translation2 associates a high translation probability between \u201chair\u201d and \u201cmustache\u201d (sic).\nTable 4 demonstrates the benefit of performing the multinomial regression. It presents the representative words (those that have high probability in the topic\u2019s word distribution) of three of the topics derived from the question (Q) set and the question+answers (QA) set. It demonstrates that the topics derived from the QA set better represent the themes that appear in health documents, whilst the topics of derived from the Q set are less distinguishable. For example, the words in Topic 3 appear to represent depression, however, the words derived from the QA set are more coherent. This is because of the limited vocabulary used in questions and their typically short length.\nFurthermore, the topics derived from the Q set tend to represent the semantics of expressions commonly used in questions (and not in answers), for example the phrases \u201can effective method\u201d and \u201ceffective treatment\u201d. The word\n3 Mistakes in the questions are original to the data\n\u201ceffective\u201d in the topics derived from the QA set is associated with the topic representing medical products. Consequently, when a question such as \u201cWhat is an effective sleeping aid?\u201d is posed to a model trained on the QA set, topics in which the words \u201cmethod\u201d and \u201ctreatment\u201d have high probability would not be considered. The model trained on the Q set, however, results in a high probability of Topic 1, and the regression stage of LDA+ causes this to be mapped to the distribution in which the words \u201ctreatment\u201d and \u201cmethod\u201d have higher probabilities. Another example is provided by Topic 2, here the word \u201cresult\u201d is often mentioned in questions posed by those who have performed medical tests, while in answers the word usually refers to the results of health research studies."}, {"heading": "6 Conclusions", "text": "This paper has presented a novel model that fuses topic modelling with Word2vec and a regression stage for ranking relevant questions-answer pairs within Collaborative Question Answering platforms. The performance of the proposed method has been evaluated using several real-world datasets, and it has been shown to outperform translation based methods and LDA with each innovation separately in all cases. Most notably when the dataset contains long questions and answers. It achieves this by allowing the model to overcome the differences in vocabulary used in questions and answers, helps to deal with the sparsity often encountered in questions (due to their relatively short length), and allows the method to exploit all available information."}], "references": [{"title": "Finding similar questions in large question and answer archives", "author": ["J. Jeon", "B.W. Croft", "J. Ho Lee"], "venue": "CIKM", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "A topic clustering approach to finding similar questions from large question and answer archives", "author": ["Zhang", "W.N"], "venue": "PLoS ONE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A syntactic tree matching approach to finding similar questions in community-based QA services", "author": ["K. Wang", "Z. Ming", "T.S. Chua"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Question answering passage retrieval using dependency relations", "author": ["H. Cui", "R. Sun", "K. Li", "M.Y. Kan", "T.S. Chua"], "venue": "In: SIGIR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Retrieval models for question and answer archives", "author": ["X. Xue", "J. Jeon", "W.B. Croft"], "venue": "In: SIGIR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Bridging lexical gaps between queries and questions on large online Q&A collections with compact translation models", "author": ["Lee", "J.T"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Combining lexical semantic resources with question & answer archives for translation-based answer finding", "author": ["D. Bernhard", "I. Gurevych"], "venue": "In: ACL-IJCNLP. Volume", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "CQArank: Jointly model topics and expertise in community question answering", "author": ["L Yang"], "venue": "CIKM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Bridging the lexical chasm: statistical approaches to answer-finding", "author": ["A. Berger", "R. Caruana", "D. Cohn", "D. Freitag", "V. Mittal"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Learning the latent topics for question retrieval in community QA", "author": ["L. Cai", "G. Zhou", "K. Liu", "J. Zhao"], "venue": "IJCNLP", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The application of the topic modeling to question answer retrieval", "author": ["J. Vasiljevi\u0107", "M. Ivanovi\u0107", "T. Lampert"], "venue": "ICIST", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["S.E. Robertson", "S. Walker"], "venue": "In: SIGIR", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "The mathematics of statistical machine translation: paramter estimation", "author": ["P Brown"], "venue": "Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Improving question retrieval in community question answering using world knowledge", "author": ["G Zhou"], "venue": "In: IJCAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Entity based Q&A retrieval", "author": ["A. Singh"], "venue": "EMNLP", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Statistical machine translation improves question retrieval in community question answering via matrix factorization", "author": ["G Zhou"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["Blei", "D.M"], "venue": "JMLR", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Modeling community questionanswering archives", "author": ["Z. Zolaktaf", "F. Riahi", "M. Shafiei", "E. Milios"], "venue": "Proceedings of the Workshop on Computational Social Science and the Wisdom of Crowds at NIPS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Word features for latent dirichlet allocation", "author": ["J Petterson"], "venue": "In: NIPS. Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["L. Yao", "D. Mimno", "A. McCallum"], "venue": "In: SIGKDD", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Evaluation methods for topic models", "author": ["H. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "In: ICML", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Pattern recognition and neural networks", "author": ["B. Ripley"], "venue": "Camb UP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Neural networks and the multinomial logit for brand choice modelling: a hybrid approach", "author": ["Y. Bentz", "D. Merunka"], "venue": "J. Forec", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R Socher"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 1, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 2, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 3, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 5, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 6, "context": "This content has attracted the attention of researchers from a number of domains [1\u20137] who aim to automatically return existing, relevant information from the CQA database when a novel question is submitted.", "startOffset": 81, "endOffset": 86}, {"referenceID": 7, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 98, "endOffset": 104}, {"referenceID": 8, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 140, "endOffset": 147}, {"referenceID": 4, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 140, "endOffset": 147}, {"referenceID": 7, "context": "Proposed approaches fall into two categories: determining the most relevant answers to a question [8, 9]; and determining similar questions [1,5,8].", "startOffset": 140, "endOffset": 147}, {"referenceID": 0, "context": "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9].", "startOffset": 119, "endOffset": 125}, {"referenceID": 1, "context": "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9].", "startOffset": 119, "endOffset": 125}, {"referenceID": 8, "context": "Solving this problem is not a trivial matter as semantically similar questions and answers can be lexically dissimilar [1, 2], referred to as the \u2018lexical chasm\u2019 [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "Similar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5].", "startOffset": 211, "endOffset": 216}, {"referenceID": 4, "context": "Similar questions are typically found by comparing the query question to the content of existing questions as it has been shown that finding similar questions based solely on their answers does not perform well [1,5].", "startOffset": 211, "endOffset": 216}, {"referenceID": 4, "context": "demonstrated that combining information derived from existing questions and their answers outperforms the other strategies [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.", "startOffset": 65, "endOffset": 76}, {"referenceID": 9, "context": "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.", "startOffset": 65, "endOffset": 76}, {"referenceID": 10, "context": "In recent years topic modelling has been applied to this problem [2, 10, 11] as it reduces the dimensionality of textual information when compared to classical methods such as bag-of words and efficiently handles polysemy and synonymy.", "startOffset": 65, "endOffset": 76}, {"referenceID": 11, "context": "There exist several variations of this class of methods, each weighting words that have specific properties in the dataset, such as tf-idf and BM25 [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Their method consists of two stages: first a set of semantically similar questions are found by matching their answers using a query-likelihood language model; and subsequently, word translation probabilities are estimated using the IBM translation model 1 [13].", "startOffset": 257, "endOffset": 261}, {"referenceID": 4, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 38, "endOffset": 43}, {"referenceID": 5, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 38, "endOffset": 43}, {"referenceID": 6, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 38, "endOffset": 43}, {"referenceID": 13, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 82, "endOffset": 89}, {"referenceID": 14, "context": "Several extensions have been proposed [5\u20137] including the use of external corpora [14,15] such as Wikipedia.", "startOffset": 82, "endOffset": 89}, {"referenceID": 4, "context": "[5] propose an extension that combines the IBM translation model (applied to the questions) with a query likelihood language model (applied to the answers).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.", "startOffset": 76, "endOffset": 84}, {"referenceID": 15, "context": "Translation-based models have become the stateof-the-art in query retrieval [10, 16] but they suffer from some limitations: they do not capture word co-occurrences nor word distributions in the corpora.", "startOffset": 76, "endOffset": 84}, {"referenceID": 16, "context": "Since the topics that characterise a document can be considered a semantic representation, it is possible to use topic distributions inferred using a method such as Latent Dirichlet Allocation (LDA) [17] to measure the semantic similarity between documents in a corpora.", "startOffset": 199, "endOffset": 203}, {"referenceID": 1, "context": "[2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] retrieve similar questions by measuring lexical and topical similarities [2]; Cai et al.", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "[10] combine the result of LDA and translation models; Vasiljevi\u0107 et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] explore combining a document\u2019s word count and topic model similarity into one measure; and Yang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] form a generative probabilistic method to jointly model QA topic distributions and user expertise.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] model the question topics and then use them to condition the answer topics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Latent Dirichlet Allocation (LDA) [17] is a generative probabilistic model that enables us to describe a collection of discrete observations in terms of latent variables.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "A solution is to treat words as features [19] and the method used to calculate a word\u2019s features then influences its topic membership.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "[20] introduced the continuous bag-of-words and Skip-gram neural network models that produce a continuous-valued vectorial word representation by exploiting the content of large textual databases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In order to implement this modification, the Gibbs Sampling-based algorithm proposed by [21] was adapted so that at each step the probability of topic t being present in document d given word w is estimated as follows:", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "01 [21, 22].", "startOffset": 3, "endOffset": 11}, {"referenceID": 20, "context": "Small values of \u03b1 and \u03b2 result in a fine-grained decomposition into topics that address specific areas [21,22].", "startOffset": 103, "endOffset": 110}, {"referenceID": 21, "context": "When a query question q\u2217 is entered the left-to-right method [23] is used to infer its topic distribution, \u03b8 \u2217 .", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25].", "startOffset": 112, "endOffset": 120}, {"referenceID": 23, "context": "For which we use a multilayer perceptron neural network (NN), which are nonlinear multinomial regression models [24, 25].", "startOffset": 112, "endOffset": 120}, {"referenceID": 20, "context": "[21], was used for Topic Modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1]; Translation2, the combined translation and querylikelihood language model proposed by Xue et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]; an autoencoder based method proposed by Socher et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[26]; to establish the benefit of word2vec, LDA\u2217 (as described within Section 3 excluding word2vec); and to establish the benefit of the regression stage, LDA\u2020 (as described within Section 3 excluding the regression step).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 9, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 13, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 13, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 14, "context": "A cursory validation of these results was performed by comparing the translation methods\u2019 figures to those presented in the literature using the same method and data source (but not the same partitioning) and they fall within the observed range [6, 10,14,14,15].", "startOffset": 245, "endOffset": 261}, {"referenceID": 0, "context": "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1.", "startOffset": 40, "endOffset": 46}, {"referenceID": 4, "context": "Within the translation based approaches [1, 5] the translation probabilities of equal source and target words are fixed to 1.", "startOffset": 40, "endOffset": 46}], "year": 2016, "abstractText": "Presented herein is a novel model for similar question ranking within collaborative question answer platforms. The presented approach integrates a regression stage to relate topics derived from questions to those derived from question-answer pairs. This helps to avoid problems caused by the differences in vocabulary used within questions and answers, and the tendency for questions to be shorter than answers. The performance of the model is shown to outperform translation methods and topic modelling (without regression) on several real-world datasets.", "creator": "LaTeX with hyperref package"}}}