{"id": "1606.03976", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Estimating individual treatment effect: generalization bounds and algorithms", "abstract": "there is intense interest in applying machine learning methods to problems of data equivalence which arise in applications such as healthcare, economic policy, global education. in this genre we use the counterfactual analytic approach to causal interpretation, often employ new intuitive results and new algorithms for performing counterfactual inference. building on an idea recently proposed by johansson et al., our results and methods focused on novel so - called \" balanced \" representations : relationships that are similar between the factual and counterfactual accounts. we give a novel, simple objective intuitive bound, showing that the expected counterfactual error of a representation is bounded by direct sum of the factual error of that representation and the distance between the factual and counterfactual distributions induced by the representation. we use integral probability metrics to measure distances between distributions, and focus on two special products : the wasserstein distance and the maximum mean discrepancy ( mmd ) distance. our bound leads directly propose new scales, which are simpler and greater to consider compared to which suggested in johansson et n.. bases on real and simulated data show the new algorithms match or outperform state - of - the - art methods.", "histories": [["v1", "Mon, 13 Jun 2016 14:40:57 GMT  (205kb,D)", "http://arxiv.org/abs/1606.03976v1", null], ["v2", "Fri, 24 Jun 2016 13:13:05 GMT  (205kb,D)", "http://arxiv.org/abs/1606.03976v2", "added missing definition of PEHE_nn"], ["v3", "Fri, 28 Oct 2016 18:17:38 GMT  (861kb,D)", "http://arxiv.org/abs/1606.03976v3", "Re-focus the theoretical section of the paper on individual level treatment effect prediction, using counterfactual error as the key component. Added new experiments"], ["v4", "Wed, 1 Mar 2017 15:44:15 GMT  (1191kb,D)", "http://arxiv.org/abs/1606.03976v4", "Added out-of-sample experiments, better hyperparameter selection, new algorithm architecture, bound is IPM based instead of previous UIPM, new policy-curve figures"], ["v5", "Tue, 16 May 2017 15:11:15 GMT  (1191kb,D)", "http://arxiv.org/abs/1606.03976v5", "Added name \"TARNet\" to refer to version with alpha = 0. Removed supp"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["uri shalit", "fredrik d johansson", "david sontag"], "accepted": true, "id": "1606.03976"}, "pdf": {"name": "1606.03976.pdf", "metadata": {"source": "CRF", "title": "Bounding and Minimizing Counterfactual Error", "authors": ["Uri Shalit", "Fredrik D. Johansson"], "emails": ["shalit@cs.nyu.edu", "frejohk@chalmers.se", "dsontag@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Causal inference questions are central to policy makers and scientists across many fields. Examples abound: in healthcare one is often interested in the relative efficacy of different medications; in economic policy, policy makers debate the effect of job training; in marketing, companies are interested in the causal effect of an online ad on a customer\u2019s buying habits. However, traditional causal inference methods are sometimes ill-suited to take advantage of the vast increase in the size and scope of data available across these fields. Here, we build on earlier work by [20], and show how modern machine learning techniques of representation learning [4] along with using integral probability metrics [30] can be fruitfully applied to problems of causal inference.\nOur work relies on the framework of counterfactual inference as a means towards causal inference, associated with the Rubin-Neyman potential outcomes framework [27]. It assumes that for a context x \u2208 X , and a treatment or intervention t \u2208 {0, 1}, there are two potential outcomes: Y0(x) for t = 0, and Y1(x) for t = 1. For example, x can denote the set of lab tests and demographic factors of a diabetic patient, t = 0 denote the standard medication for controlling blood sugar, t = 1 denotes a\n\u2217Equal contribution\nar X\niv :1\n60 6.\n03 97\n6v 1\n[ st\nat .M\nL ]\nnew medication, and Y0(x) and Y1(x) indicate the patient\u2019s blood sugar level after treatments t = 0 and t = 1, respectively. We are usually interested in knowing \u03c4(x) = Y1(x)\u2212 Y0(x), i.e. what is the treatment effect of t = 1. The fundamental problem of causal inference is, that, for a given context x, we only ever observe Y1(x) or Y0(x), but never both of them. Moreover, in many cases of interest, the distribution of the treatment assignment t is dependent on the context x, as is often the case in observational studies [26]. For example, medications might be prescribed based on specific coverage policies, and job training might only be given to those motivated enough to seek it.\nWe call the joint distribution over contexts and treatment the factual distribution, pF (x, t). The counterfactual distribution is pCF (x, t) = pF (x, 1 \u2212 t). The factual (observed) outcomes for unit xi \u2208 X with treatment ti \u2208 {0, 1} are: yF (xi) = tiY1(xi) + (1 \u2212 ti)Y0(xi). Similarly, the counterfactual outcomes are yF (xi) = (1\u2212 ti)Y1(xi) + tiY0(xi). Although we never have access to yCF (xi), we do have access to yF (xi). Therefore, one approach would to this problem would be to estimate Y0 and Y1 directly from the factual sample, using ordinary machine learning techniques. However, when generalizing to the counterfactual sample we have another source of variance beyond those considered in the classic supervised learning formulation. This is the variance incurred by learning from one distribution and performing inference on another [2, 23]. This problem is strongly related to the problem of covariate shift [20], and indeed our bound and algorithm could be easily adapted for use in covariate shift problems, in line with recently proposed methods [13].\nRecently, [20] have argued that the covariate shift variance can be controlled by learning a so-called \u201cbalanced representation\u201d: a representation of the data that makes the factual and counterfactual distributions more similar. In their work, [20] study the performance of learning a balanced representation and then fitting a linear ridge-regression model on top of it. They bound the relative error of fitting a ridge-regression using the counterfactual distribution versus fitting a ridge-regression using the factual distribution. Unfortunately, the relative error term they bound is not informative regarding the absolute quality of the representation, neither on the factual nor on the counterfactual distribution.\nIn this paper we prove what we consider a much more informative bound. We show that under mild assumptions about the invertability of the learned representation, the expected counterfactual loss is upper-bounded by a sum of two terms: the expected factual loss, and a constant C times the distance between the distributions of the treated (t = 1) and control (t = 0) populations in the representation space. The intuition behind this bound is clear: generalizing to the counterfactual distribution is no harder than generalizing to the factual distribution from which we can sample, plus a term measuring the difference between the two distributions. Here, we use a general family of distance functions between distributions called Intergral Probability Metrics (IPM) [24, 30]. In particular we concentrate on two IPMs: The Wasserstein metric [32, 8]and the Maximum Mean Discrepancy metric (MMD, [15]). Both have been successfully applied in recent years to a wide variety of applications [16, 25, 11, 9, 12]. For the Wasserstein metric, we give an explicit term upper-bounding the constant C. Our bound is not limited to linear hypotheses, and holds for a wide class of loss functions, including the squared loss over a compact domain, the absolute loss, and the logistic loss, making our approach applicable to both classification and regression tasks.\nThe bound we derive points the way to family of algorithms: jointly learn a hypothesis and a representation which minimize a weighted sum of the factual loss (the standard supervised machine learning objective), and the distance between the control and treated distributions induced by the representation. In the Experiments section 4 we apply algorithms based on multi-layer neural nets as representations and hypotheses, along with MMD or Wasserstein distributional distance metrics; see Figure 1 for the basic architercture. These algorithms are conceptually simpler than the ones proposed by [20], since they do not require a two-stage fitting procedure, and avoid the need to compute nearest neighbors. We show that our methods achieve better performance on the synthetic tasks presented by [20]. We also show that our methods can achieve competitive results on a real world causal inference benchmark: the widely used National Supposed Work survey [22, 10, 29].\nOur contributions in this paper are as follows: (1) We prove a novel, simple and meaningful upper bound on the counterfactual loss. (2) We show the relation between minimizing counterfactual loss and minimizing the error in estimating treatment effect, which is often the quantity we are truly interested in. (3) Our bound and algorithm are applicable to a very wide family of loss functions and models for classification and regression, compared with the previous paper\u2019s restriction to linear\nmodels and absolute loss. (4) Our experiments show the benefit of our method, and we show results on a real-world causal inference benchmark, the National Work Survey dataset [22, 29]."}, {"heading": "2 Counterfactual learning bound", "text": "In this section we will prove several bounds relating the learning of representations, factual and counterfactual loss, and error in estimating the treatment effect for unit x. The most important quantity we bound is `CF : the expected loss of a model, with expectation taken over the counterfactual distribution, defined below. The bounds are expressed in terms of `F : the expected loss of the same model over the factual distribution, along with a distance measure between the distribution of treated and control units. The term `F is the classic machine learning generalization error, and in turn can be upper bounded using the empirical error over the observations and model complexity terms, applying well-known machine learning theory [28]. All proofs are in the supplement."}, {"heading": "2.1 Problem setup", "text": "We will employ the following assumptions and notations. The most important notations are in the Notation box in the supplement. The space of covariates is a bounded subset X \u2282 Rd. The outcome space is Y \u2282 R. Treatment is a binary variable. There exists a factual distribution pF (x, t) defined jointly over the covariate and treatment space X \u00d7{0, 1}. The treated and control distributions are the factual distribution conditioned on treatment: pt=1(x) := pF (x|t = 1), and pt=0(x) := pF (x|t = 0), respectively. The counterfactual distribution is the factual distribution with the treatment assignment flipped: pCF (x, t) := pF (x, 1 \u2212 t) = pF (1 \u2212 t|x)pF (x). Note that if treatment assignment is independent of the context x, and pF (t|x) = 12 , then the counterfactual and factual distributions are identical; this is exactly the case of a randomized-control trial, where treatment is assigned randomly with equal probability to units.\nIn this paper we employ the Rubin-Neyman potential outcomes framework [27]: We assume there exist two deterministic functions which are the true, unknown labeling functions Yt : X \u2192 Y , for the two treatment assignments t = 0, 1. For a labeled sample (x1, t1, yF1 ), . . . (xn, tn, y F n ) \u2282 X \u00d7 {0, 1} \u00d7 Y from the factual distribution, we have that the factual labels are yFi = tiY1(xi) + (1\u2212 ti)Y0(xi). Throughout this paper we will discuss representation functions of the form \u03a6 : X \u2192 R, whereR is the representation space. We make the following assumption about \u03a6:\nAssumption 1. The representation \u03a6 is a twice-differentiable, one-to-one function. Without loss of generality we will assume thatR is the image of X under \u03a6.\nDefinition 1. Define \u03a8 : R \u2192 X to be the inverse of \u03a6, such that \u03a8(\u03a6(x)) = x for all x \u2208 X .\nThe representation \u03a6 pushes forward the treated and control distributions into the new spaceR: we denote the induced distribution by pF\u03a6 , defined overR\u00d7 {0, 1}. Define pCF\u03a6 analogously. We also define pt=1\u03a6 (r) := p F \u03a6(r|t = 1), pt=0\u03a6 (r) := pF\u03a6(r|t = 0), to be the treated and control distributions induced overR. For a one-to-one \u03a6, the distributions pF\u03a6 and pCF\u03a6 can be obtained by the standard change of variables formula, using the determinant of the Jacobian of \u03a8(r).\nLet L : Y \u00d7 Y \u2192 R+ be a loss function.\nDefinition 2. Let h : R \u00d7 {0, 1} \u2192 R be an hypothesis defined over the representation space R. The expected factual loss and counterfactual losses of h and \u03a6 are, respectively:\n`F (h,\u03a6) = \u222b X\u00d7{0,1} L (Yt(x), h(\u03a6(x), t)) p F (x, t) dxdt = \u222b R\u00d7{0,1} !L (Yt(\u03a8(r)), h(r, t)) p F \u03a6(r, t) drdt,\n`CF (h,\u03a6) = \u222b X\u00d7{0,1} !L (Yt(x), h(\u03a6(x), t)) p CF (x, t) dxdt = \u222b R\u00d7{0,1} L (Yt(\u03a8(r)), h(r, t)) p CF \u03a6 (r, t) drdt,\nwhere the equalities follow from change of variables in integration and the definition of pF\u03a6(r, t), pCF\u03a6 (r, t). As noted above, the term `F is simply the generalization error for the hypothesis h(\u03a6, t) over the factual distribution. The term `CF is the generalization of the same hypothesis over the counterfactual distribution.\nOur proof relies on the notion of an Integral Probability Metric, which is a family of metrics between probability distributions [30, 24]. For two probability density functions p, q defined over S \u2286 Rd, and for a function family F of functions g : S \u2192 R, we have that\nIPMF(p, q) := sup g\u2208F \u2223\u2223\u2223\u2223\u222b S g(s)(p(s)\u2212 q(s)) ds \u2223\u2223\u2223\u2223 . Integral probability metrics are always symmetric and obey the triangle inequality, and trivially satisfy IPMF(p, p) = 0. For rich enough function families F, we also have that IPMF(p, q) = 0 =\u21d2 p = q, and then IPMF is a true metric. Examples of function families F for which IPMF is a true metric are the family of all bounded countinuous functions, the family of all 1-Lipschitz functions [30], and the unit-ball of functions in a universal reprodcuing Hilbert kernel space [15]. Here, we will employ an extended notion of IPM, where the probabilities are not necessarily normalized. We call this Unnormalized Integral Probability Metric. For two probability distribution functions as above, and positive scalars up, uq , we have:\nUIPMF(upp, uqq) := sup g\u2208F \u2223\u2223\u2223\u2223\u222b S g(s) (upp(s)\u2212 uqq(s)) ds \u2223\u2223\u2223\u2223 ."}, {"heading": "2.2 Bounds", "text": "Assumption 2. Let u = pF (t = 1) be the marginal probability of treatment, and assume 0 < u < 1. Theorem 1 (General bound). Let F be a family of functions f : R \u2192 R, and UIPMF(\u00b7, \u00b7) the unnormalized integral probability metric induced by F. Let Y0, Y1 be the true labeling functions. Let h : R\u00d7 {0, 1} \u2192 Y be an hypothesis. Let \u03a6 : X \u2192 R be a one-to-one representation function, with inverse \u03a8. Assume that for t = 0, 1, we have that f\u03a6,h(r) := L(Yt(\u03a8(r)), h(r, t)) \u2208 F. Then:\n`CF (h,\u03a6) \u2264 `F (h,\u03a6) + 2UIPM ( u \u00b7 pt=1\u03a6 , (1\u2212 u) \u00b7 pt=0\u03a6 ) . (1)\nThe upper bound in Eq (1) is itself a sum of two expectation terms. For an empirical sample from pF (x, t), and a family of representations and hypotheses, we can further upper bound `F by the empirical loss and a model complexity term using standard arguments [28]. The UIPM term can be consistently estimated from finite samples for the function families we use below [30].\nChoosing a small function family F will make the bound tighter.However, choosing too small a family could result in an imcomputable bound. For example, for F = {f\u03a6,h} we will have that IPMF = `CF (\u03a6, h)\u2212 `F (\u03a6, h), which we cannot compute since we do not know the counterfactual outcomes. In addition, for some function families there is no known way to efficiently compute the IPM distance or to take its gradients. In this paper we employ two function families for which there are available optimization tools. The first is the family of 1-Lipschitz functions, which leads to IPM being the Wasserstein distance [32, 30], denoted Wass(p, q). The second is the family of norm-1 reproducing kernel Hilbert space (RKHS) functions, leading to the MMD metric [15, 30], denoted MMD(p, q). Both the Wasserstein and MMD metrics have efficient algorithms which are consistent and can be applied in the finite sample case [30], and have been used for various machine learning tasks in recent years [16, 15, 8, 9]. In supplement C we show how to reduce the calculation of unnormalized Wasserstein and MMD to calculating standard Wasserstein and MMD distances.\nThe Wasserstein metric Assumption 3. For the loss L, for all y \u2208 Y , both L(\u00b7, y) and L(y, \u00b7) are Lipschitz functions with Lipschitz constant upper bounded by KL.\nExamples of loss function which follow Assumption 3 are the absolute loss |y1\u2212 y2|, the logistic loss for binary labels log(1 + e\u2212y1y2), and the squared loss (y1 \u2212 y2)2 if Y is bounded. We now employ a measure bounding the degree to which \u03a6 is information-preserving. For this we use the reciprocal condition number of the Jacobian matrix of \u03a6.\nDefinition 3. Let \u2202\u03a6(x)\u2202x be the Jacobian matrix of \u03a6 at point x, i.e. the matrix of the partial derivatives of \u03a6. Let \u03c3max(A) and \u03c3min(A) denote respectively the largest and smallest singular\nvalues of the matrix A. Define \u03c1(\u03a6) = supx\u2208X \u03c3max ( \u2202\u03a6(x) \u2202x ) /\u03c3min ( \u2202\u03a6(x) \u2202x ) .\nIt is an immediate result that \u03c1(\u03a6) \u2265 1. We will call a representation function \u03a6 : X \u2192 R Jacobiannormalized if supx\u2208X \u03c3max ( \u2202\u03a6(x) \u2202x ) = 1. Note that any non-constant representation function \u03a6 can\nbe Jacobian-normalized by scaling it with 1/ supx\u2208X \u03c3max ( \u2202\u03a6(x) \u2202x ) .\nTheorem 2 (Wasserstein bound). Let \u03a6 be a one-to-one, Jacobian-normalized representation function. Let K be the Lipschitz constant of the functions Y0, Y1 on X . Let KL be the Lipschitz constant of the loss function L. Let h : R\u00d7 {0, 1} \u2192 R be an hypothesis with Lipschitz constant bK. Then:\n`CF (h,\u03a6) \u2264 `F (h,\u03a6) + 2 (\u03c1(\u03a6) + b) \u00b7K \u00b7KL \u00b7Wass(u \u00b7 pt=1\u03a6 , (1\u2212 u) \u00b7 pt=0\u03a6 ). (2)\nWe examine the constant (\u03c1(\u03a6) + b) \u00b7K in Theorem 2. K, the Lipschitz constant of Y0 and Y1, is not under our control and measures an aspect of the complexity of the true underlying functions we wish to approximate. The term KL depends on our choice of loss function. The term b comes from our assumption that the hypothesis h has norm bK. Note that smaller b, while reducing the bound, might force the factual loss term `F (h,\u03a6) to be larger since a small b implies a less flexible h. Finally, consider the term \u03c1(\u03a6). For specific families of representation \u03a6, \u03c1(\u03a6) can be upper bounded. For example, if \u03a6 is a single-layer neural net with weight matrix W and tanh non-linearity, then one can show that if the norm and the inverse condition number of W are bounded, then so is \u03c1(\u03a6).\nThe MMD metric We prove a Theorem similar to 2, for the case of Reproducing Kernel Hilbert Space function spaces, with the corresponding Maximum Mean Discrepancy metric. For lack of space we state the results in the supplement. Unlike in Theorem 2, the MMD case has a constant which is harder to explicitly bound; we defer a deeper look into this issue to future work.\nThe above Theorems hold for any given h and \u03a6 obeying the theorems\u2019 conditions. Therefore, we can attempt to minimize the upper bounds in Eqs. (1) and (2) with respect to \u03a6 and h, as we suggest in Algorithm 1, in order to minimize the counterfactual loss.\nProof idea The full proof of the Theorems and Lemmas above is given in the supplement. The main idea is bounding the difference `CF \u2212 `F in terms of an IPM between the counterfactual and factual distributions. Once we have that, we have that\n`CF = `F + (`CF \u2212 `F ) \u2264 `F + IPMF(PCF , pF ).\nWe then show that we can in fact calculate IPMF(PCF , pF ) in terms of an IPM between the treatment and control distributions IPMF(pt=1, pt=0).\nCounterfactual loss and treatment effect estimation In section B of the supplement, we show how the counterfactual loss relates to the error in estimating the true treatment effect Y1(x)\u2212 Y0(x). We show that in the transductive setting where one of the potential outcomes is known, minimizing counterfactual loss is equivalent to minimizing the error in treatment effect estimation. For the inductive setting we show that the error can be bounded by the sum of the factual and counterfactual errors. Putting this together with our bounds above, we see that the error in estimating treatment effect is also upper bounded by a weighted sum of the factual error and an IPM distance term, justifying our theoretical and empirical approach.\nAlgorithm 1 Counterfactual balanced regression with integral probability metrics\n1: Input: Factual sample (x1, t1, yF1 ), . . . , (xn, tn, yFn ), scaling parameter \u03b1 > 0, loss function L (\u00b7, \u00b7), representation network \u03a6W with initial weights by W, outcome network hV with intial weights V, function family F for IPM loss 2: while not converged do 3: Sample m control {(xij , 0, yFij )} m j=1 and m \u2032 treated units {(xik , 1, yFik)} m+m\u2032 k=m 4: Calculate the gradient of the imbalance penalty: g1 = \u2207W IPMF ( {\u03a6W(xij )}mj=1, {\u03a6W(xik)}m \u2032 k=m+1\n) 5: Calculate the gradients of the empirical loss:\ng2 = \u2207V \u2211 j L ( hV(\u03a6W(xij ),tij ),y F ij ) m+m\u2032 , g3 = \u2207W 1 m+m\u2032 \u2211 j L ( hV(\u03a6W(xij ),tij ),y F ij ) m+m\u2032\n6: Obtain step size scalar or matrix \u03b7 with standard neural net methods e.g. RMSProp 7: update W\u2190W \u2212 \u03b7(\u03b1g1 + g3) and V\u2190 V \u2212 \u03b7(g2) 8: check convergence criterion 9: end while"}, {"heading": "3 Our approach", "text": "We propose a general framework for counterfactual estimation, based on the theoretical results of Section 2. Our algorithm is a single regularized minimization procedure which simultaneously fits both a balanced representation of the data, and a hypothesis for the outcome. This is in contrast to [20] who proposed a two-step procedure corresponding to their theoretical results based on the discrepancy distance [7]. We note that our framework is also more flexible in practice, as our theory supports multiple measures of balance that can can be minimized efficiently; this is only rarely true for variants of the discrepancy distance We minimize the following objective.\nmin \u03a6,h\n1\nn n\u2211 i=1 L (h(\u03a6(xi), ti) , yi) + \u03b1 \u00b7 IPMF ({\u03a6(xi)}i:ti=0, {\u03a6(xi)}i:ti=1) (3)\nHere, IPMF(\u00b7, \u00b7) is the (empirical) integral probability metric defined by the function family F. In Section 2, we show that under certain conditions when IPMF is the maximum mean discrepancy or the Wasserstein distance, (3) is an upper bound on the counterfactual error.\nIn this work, we let \u03a6(x) and h(\u03a6, t) be parameterized by a single neural network. This means that we can learn rich, non-linear representations and hypotheses with large flexibility. In [20], the authors considered using a linear variable selection model, and while our model allows for a sparse diagonal first layer which generalizes their framework, in practice, we have observed no gain from the variable selection. Our approach, in the neural network parameterization, is visualized in Figure 1. We train our models by minimizing (3) using stochastic gradient descent, as described in Algorithm 1. The details of how to obtain the gradient g1 with respect to the IPM can be found in the supplement."}, {"heading": "4 Experiments", "text": "We evaluate our framework in counterfactual regression and classification tasks, using different functions to measure imbalance, including the Wasserstein distance and the MMD, and compare to established methods. We report the absolute error or bias in estimating the average and individual treatment effects, ATE and ITE , as well as the Precision in Estimation of Heterogeneous Effect\n(PEHE)[19], PEHE = \u221a\n1 n \u2211n i=1[(Y1(xi)\u2212 Y0(xi))\u2212 (g(xi, 1)\u2212 g(xi, 0))]2, where g(x, t) is the\npredicted outcome for individual x under treatment t. We also report the RMSE of the predicted factual outcome (RMSEfact) or the binary classification error (Errfact). Standard methods for hyperparameter selection such as cross-validation are not applicable, as there are no samples of the counterfactual outcome. In simulated experiments, counterfactuals are available and we follow [20] by fitting hyperparameters on held-out set of repeated experiments. In experiments with real outcome, we use a surrogate for the counterfactual outcome to estimate the error, namely yFj(i) \u2013 the factual outcome of the nearest neighbor j(i) to i, that is in the opposite treatment group.\nOur approach is implemented as a feed-forward neural network with fully-connected ReLU layers, trained using RMSProp, with a small l2 weight decay, \u03bb = 10\u22123. Two architectures are evaluated. CFR-4-0 consists of 4 ReLU layers used for representation, and a single linear output layer. Following the notation of Fiugre 1, dr = 4, do = 0. CFR-2-2 consists of 2 ReLU representation layers, 2 ReLU layers after the treatment has been added, and a single linear output layer, dr = 2, do = 2, see Figure 1. For the IHDP data we use layers of 25 hidden units each. For Jobs, layers have 50 units. These architectures were selected based on factual validation error alone, by splitting the data.\nIn regression tasks, we compare our method to Ordinary Least Squares (OLS), Doubly Robust Regression (DR) [1], Bayesian Additive Regression Trees2 (BART) [5], Causal Forests3 (C.Forests) [33] as well as the Balancing Linear Regression (BLR) and Balancing Neural Network (BNN) methods proposed by [20]. For DR, we estimate propensity scores using logistic regression. We also compare to a variable selection procedure dubbed LASSO + Ridge (L+R) in which a ridge regression model is fit to the variables selected by LASSO. In classification tasks we compare to Logistic Regression (LR), `1-regularized Logistic Regression (`1-LR) instead of OLS and L+R."}, {"heading": "4.1 Simulated outcome - IHDP", "text": "Hill [19] compiled a semi-simulated dataset for counterfactual inference based on the Infant Health and Development Program (IHDP), in which the outcome is simulated. The covariates stem from a randomized experiment studying the effects of child care and home visits on (future) cognitive test scores. Imbalance in the covariates has been artificially introduced by removing a biased subset of the treatment population. The dataset comprises 747 observations (139 treated, 608 control) and 25 covariates measuring aspects of children and their mothers, see Hill (2011) [19]. We use the log-linear outcome model implemented as setting \u201cB\u201d in the NPCI package4. We also evaluate our method on another semi-synthetic dataset called News [20], the results on which are in the supplement.\nThe results of the experiments on IHDP are presented in Table 1. We see that in general, non-linear estimators such as BART, CFR, BNN and Causal Forests fair the best on both datasets. On the whole, using neural networks, even without balancing, give very good results. For a comparison of different distance measures, the MMD and its square (MMD2) with linear and Gaussian-RBF kernels, the Wasserstein distance (Wass), and the linear discrepancy (LinDisc), see Figure 2. We see that MMD2 (lin) performs the best, and the Wasserstein and MMD (rbf) the worst, although this might improve by chosing the variance parameter \u03c3.\nWe investigate the effects of varying imbalance by constructing subsampled variants of the IHDP dataset. First, we fit a propensity score model, to form estimates p\u0302(t) of the treatment probability p(t). Then, repeatedly with probability q we remove the remaining control observation that has propensity score p\u0302(t) closest to 1 and with probability 1 \u2212 q, we remove a control observation uniformly at random. For high values of q, this gives a more imbalanced dataset, and for small q a more balanced. In our experiments, we consider q = 0, 0.5, 1.0. In total, we remove m = 347 observations from\n2https://cran.r-project.org/web/packages/BayesTree 3https://github.com/susanathey/causalTree/tree/forestCode 4https://github.com/vdorie/npci\neach set, leaving 400. In Figure 2, we see that as imbalance (q) increases, the relative gain from using our method, and the penalty needed, increase as well. This is expected as for data with no overlap at all, the best you can do is to disregard the covariates completely, corresponding to \u03b1 =\u221e."}, {"heading": "4.2 Real-world outcome - Jobs", "text": "LaLonde [22] conducted a well-known observational study based on the National Supported Work (NSW) program. It combines a randomized study based on the NSW with observational data to form a larger, observational dataset [29]. We refer to this dataset as Jobs. The original outcome to predict is the 1978 earnings and the 8 original covariates include age, education, ethnicity, as well as earnings in 1974 and 1975. To evaluate our framework for classification, we construct an alternative binary task of predicting unemployment in the Jobs study, i.e. the event that y = 0. We use the augmented feature set of Dehejia & Wahba [10]. In the notation of Smith et al. [29], we use the LaLonde experimental sample (297 treated, 425 control) and the PSID comparison group (2490 control). We can compute the average treatment effect on the treated (ATT = $886), as all treated were part of the original randomized experiment. In total, there were 482 (15%) people unemployed by the end of the study. We select parameters based on the PEHEnn criteria defined above.\nThe results of the binary Jobs experiments are presented in Table 1. The results for the original, continuous task is in the supplement. On the whole, our proposed methods CFR-2-2 and CFR-4-0 perform well. The results only show error in the average effect, something that even linear methods can estimate well. However, since we achieve comparable average effect error, as well as better individual factual error, we expect that the counterfactual error is lower for our method."}, {"heading": "5 Conclusion", "text": "In this paper we give a meaningful and intuitive bound for the problem of learning representations for counterfactual inference. Our bound relates counterfactual inference to the classic machine learning problem of learning from finite samples, along with methods for measuring distributional distances from finite samples. The bound lends itself naturally to the creation of learning algorithms; we focus on using neural nets as representations and hypotheses. We apply our theory-guided approach leads to both synthetic and real-world tasks, showing that in every case our method matches or outperforms the most recent methods proposed for these tasks. Important open questions are theoretical considerations in choosing the imbalance weight \u03b1, and how to best derive confidence intervals for our model\u2019s predictions, as well as exploring in more depths the connection to domain adaptation problems."}, {"heading": "Acknowledgments", "text": "We wish to thank Esteban Tabak and Marco Cuturi for fruitful conversations. We also wish to thank Stefan Wager for his help with the code for Causal Forests. DS and US were supported by NSF CAREER award #1350965."}, {"heading": "A Proofs", "text": "A.1 Definitions, assumptions, and auxiliary lemmas\nNotation: pF (x, t), pCF (x, t): factual and counterfactual distributions on X \u00d7 {0, 1} u = pF (t = 1): the marginal probability of treatment. pt=1(x) = pF (x|t = 1): treated distribution. pt=0(x) = pF (x|t = 0): control distribution. \u03a6: representation function mapping from X toR. \u03a8: the inverse function of \u03a6, mapping fromR to X . pF\u03a6(r, t), p CF \u03a6 (r, t): factual and counterfactual distributions induced by \u03a6 onR\u00d7 {0, 1}. pt=1\u03a6 (r), p t=0 \u03a6 (r): treated and control distributions induced by \u03a6 onR. Y0(x), Y1(x): true labeling functions for t = 0, t = 1. L(\u00b7, \u00b7): loss function, from Y \u00d7 Y to R+. `F (h,\u03a6), `CF (h,\u03a6): expected loss of h(\u03a6(x), t) with respect to the factual, counterfactual distributions. IPMF(p, q): the integral probability metric distance induced by function family F between distributions p and q. UIPMF(up \u00b7 p, uq \u00b7 q): the unnormalized integral probability metric distance induced by function family F between distributions p and q scaled respectively by up, uq \u2208 R+\nWe first define the necessary distributions and prove some simple results about them. Definition 4. Let the factual and counterfactual distributions pF (x, t) and pCF (x, t) be distributions over X \u00d7 {0, 1} such that pF (t|x) = pCF (1\u2212 t|x), where X \u2282 Rd. Assumption 4. The marginal distribution 0 < pF (t = 1) < 1. Assumption 5. The marginal distribution of the covariates x is equal for the factual and counterfactual distributions: pF (x) = pCF (x).\nDefinition 5. Let pt=1(x) \u2261 pF (x|t = 1), and pt=0(x) \u2261 pF (x|t = 0) denote respectively the treatment and control distributions.\nLet \u03a6 : X \u2192 R be a representation function. We will assume that \u03a6 is differentiable.\nAssumption 6. The representation function \u03a6 is one-to-one. Without loss of generality we will assume thatR is the image of X under \u03a6, and define \u03a8 : R \u2192 X to be the inverse of \u03a6, such that \u03a8(\u03a6(x)) = x for all x \u2208 X . Definition 6. For a representation function \u03a6 : X \u2192 R, and for a distribution pF defined over X , let pF\u03a6 be the distribution induced by \u03a6 over R. Define pCF\u03a6 analogously. Also define pt=1\u03a6 (r) \u2261 pF\u03a6(r|t = 1), pt=0\u03a6 (r) \u2261 pF\u03a6(r|t = 0), to be the treatment and control distributions induced overR.\nFor a one-to-one \u03a6, the distributions pF\u03a6 and p CF \u03a6 can be obtained by the standard change of variables formula, using the determinant of the Jacobian of \u03a8(r). See [3] for the case of a mapping \u03a6 between spaces of different dimensions. Lemma 1. pF\u03a6(r) = pCF\u03a6 (r)\nProof. Let J\u03a8(r) be the absolute of the determinant of the Jacobian of \u03a8(r). Then by the change of variable formula we have:\npF\u03a6(r) = J\u03a8(r) \u00b7 pF (\u03a8(r)) = J\u03a8(r) \u00b7 pCF (\u03a8(r)) = pCF\u03a6 (r), since by Assumption 5 pF (\u03a8(r)) = pCF (\u03a8(r)).\nLemma 2. For all r \u2208 R, t \u2208 {0, 1}:\npF\u03a6(t|r) = pF (t|\u03a8(r)), pCF\u03a6 (t|r) = pCF (t|\u03a8(r)\nProof. Let J\u03a8(r) be the absolute of the determinant of the Jacobian of \u03a8(r).\npF\u03a6(t|r) = pF\u03a6(t, r)\npF\u03a6(r)\n(a) =\npF (t,\u03a8(r))J\u03a8(r) pF (\u03a8(r))J\u03a8(r) = pF (t,\u03a8(r)) pF (\u03a8(r)) = pF (t|\u03a8(r)),\nwhere equality (a) is by the change of variable formula. The proof is identical for pCF\u03a6 .\nLemma 3. For all r \u2208 R: pCF\u03a6 (r, t) = p F \u03a6(r|1\u2212 t)pF\u03a6(1\u2212 t)\nProof. pCF\u03a6 (r, t) = p CF \u03a6 (r)p CF \u03a6 (t|r) = pF\u03a6(r)pF\u03a6(1\u2212 t|r), since by Lemma 1 we have pCF\u03a6 (r) = p F \u03a6(r), and by Definition 4 and Lemma 2\nLet L : Y \u00d7 YR+ be a loss function, e.g. the logistic loss or squared loss. Definition 7. Let \u03a6 : X \u2192 R be a representation function. Let h : R\u00d7{0, 1} \u2192 Y be an hypothesis defined over the representation spaceR. Let Yt : X \u2192 R be the true labeling functions, for t = 0, 1. The expected factual loss and counterfactual losses of h and \u03a6 are, respectively:\n`F (h,\u03a6) = \u222b R\u00d7{0,1} L (Yt(\u03a8(r)), h(r, t) ) p F \u03a6(r, t) drdt\n`CF (h,\u03a6) = \u222b R\u00d7{0,1} L (Yt(\u03a8(r)), h(r, t) ) p CF \u03a6 (r, t) drdt,\nwhere \u03a8 is the inverse function of \u03a6. Definition 8. Let F be a function family consisting of functions g : S \u2192 R. For a pair of distributions p1, p2 over S, define the Integral Probability Metric:\nIPMF(p1, p2) = sup g\u2208F \u2223\u2223\u2223\u2223\u222b S g(s) (p1(s)\u2212 p2(s)) ds \u2223\u2223\u2223\u2223 IPMF(\u00b7, \u00b7) defines a pseudo-metric on the space of probability functions over S , and for sufficiently large function families, IPMF(\u00b7, \u00b7) is a proper metric [24]. Examples of sufficiently large functions families includes the set of bounded continuous functions, the set of 1-Lipschitz functions, and the set of unit norm functions in a universal Reproducing Norm Hilbert Space. The latter two give rise to the Wasserstein and Maximum Mean Discrepancy metrics, respectively [15, 30]. We note that for function families F such as the three mentioned above, for which g \u2208 F =\u21d2 \u2212g \u2208 F, the absolute value can be omitted from definition 8.\nDefinition 9. Let F be a function family consisting of functions g : S \u2192 R. For a pair of distributions p1, p2 over S, and a pair of positive scalar u1, u2, define the Unnormalized Integral Probability Metric:\nUIPMF(u1p1, u2p2) = sup g\u2208F \u2223\u2223\u2223\u2223\u222b S g(s) (u1p1(s)\u2212 u2p2(s)) ds \u2223\u2223\u2223\u2223 UIPMF can be considered as a distance function between positive measures on S , see also [18, 14, 6].\nLemma 4. For constant v \u2265 0, we have that UIPMF(v \u00b7 u1p1, v \u00b7 u2p2) = v \u00b7 UIPMF(u1p1, u2p2). In particular, UIPMF(vp1, vp2) = v \u00b7 IPMF(p1, p2).\nThe proof is immediate from Definitions 8 and 9.\nA.2 General IPM bound\nLemma 5. Let u = pF (t = 1). For a function g : R\u00d7 {0, 1} \u2192 R and the distributions pCF\u03a6 , pF\u03a6:\u222b R\u00d7{0,1} g(r, t) ( pCF\u03a6 (r, t)\u2212 pF\u03a6(r, t) ) drdt =\u222b\nR g(r, 0)\n( u \u00b7 pt=1\u03a6 (r)\u2212 (1\u2212 u) \u00b7 pt=0\u03a6 (r) ) dr + \u222b R g(r, 1) ( (1\u2212 u) \u00b7 pt=0\u03a6 (r)\u2212 u \u00b7 pt=1\u03a6 (r) ) dr\nProof.\u222b R\u00d7{0,1} g(r, t) ( pCF\u03a6 (r, t)\u2212 pF\u03a6(r, t) ) drdt =\u222b\nR g(r, 0)\n( pCF\u03a6 (r, 0)\u2212 pF\u03a6(r, 0) ) dr + \u222b R g(r, 1) ( pCF\u03a6 (r, 1)\u2212 pF\u03a6(r, 1) ) dr (a) = (4)\u222b\nR g(r, 0)\n( u \u00b7 pF\u03a6(r|t = 1)\u2212 (1\u2212 u) \u00b7 pF\u03a6(r|t = 0) ) dr+\u222b\nR g(r, 1)\n( (1\u2212 u) \u00b7 pF\u03a6(r|t = 0)\u2212 u \u00b7 pF\u03a6(r|t = 1) ) dr =\u222b\nR g(r, 0)\n( u \u00b7 pt=1\u03a6 (r)\u2212 (1\u2212 u) \u00b7 pt=0\u03a6 (r) ) dr + \u222b R g(r, 1) ( (1\u2212 u) \u00b7 pt=0\u03a6 (r)\u2212 u \u00b7 pt=1\u03a6 (r) ) dr,\nwhere equality (a) is due to Lemma 3 and since pF\u03a6(t) = p F (t).\nWe prove a very slightly more general version of Theorem 1 from the main paper. The only difference being that we allow a scaled version of L(Yt(\u03a8(r)), h(r, t)) to be in the function family F. This modification simplifies the following proofs.\nTheorem 3. Let pt=1\u03a6 , pt=0\u03a6 be defined as in Definition 6. Let u = pF (t = 1). Let F be a family of functions f : R \u2192 R, and denote by UIPMF(\u00b7, \u00b7) the unnormalized integral probability metric induced by F. Let Y0, Y1 be the true labeling functions. Let h : R\u00d7 {0, 1} \u2192 Y be an hypothesis. Let \u03a6 : X \u2192 R be a one-to-one representation function, with inverse \u03a8. Assume there exists a constant B > 0, such that for t = 0, 1, we have that f\u03a6,h(r) := 1B \u00b7 L(Yt(\u03a8(r)), h(r, t)) \u2208 F. Then we have:\n`CF (h,\u03a6) \u2264 `F (h,\u03a6) + 2B \u00b7 UIPM ( u \u00b7 pt=1\u03a6 , (1\u2212 u) \u00b7 pt=0\u03a6 ) . (5)\nProof.\n`CF (h,\u03a6) =\n`F (h,\u03a6) + (`CF (h,\u03a6)\u2212 `F (h,\u03a6)) =\n`F (h,\u03a6) + \u222b R\u00d7{0,1} L (Yt(\u03a8(r)), h(r, t) ) ( pCF\u03a6 (r, t)\u2212 pF\u03a6(r, t) ) drdt = (6)\n`F (h,\u03a6) + \u222b R L (Y0(\u03a8(r)), h(r, 0) ) ( u \u00b7 pt=1\u03a6 (r)\u2212 (1\u2212 u) \u00b7 pt=0\u03a6 (r) ) dr\n+ \u222b R L (Y1(\u03a8(r)), h(r, 1) ) ( (1\u2212 u) \u00b7 pt=0\u03a6 (r)\u2212 u \u00b7 pt=1\u03a6 (r) ) dr =\n`F (h,\u03a6) +B \u00b7 \u222b R 1 B L (Y0(\u03a8(r)), h(r, 0) ) ( u \u00b7 pt=1\u03a6 (r)\u2212 (1\u2212 u) \u00b7 pt=0\u03a6 (r) ) dr\n+B \u00b7 \u222b R 1 B L (Y1(\u03a8(r)), h(r, 1) ) ( (1\u2212 u) \u00b7 pt=0\u03a6 (r)\u2212 u \u00b7 pt=1\u03a6 (r) ) dr \u2264 (7)\n`F (h,\u03a6) +B \u00b7 sup g\u2208F \u2223\u2223\u2223\u2223\u222b R g(r) ( u \u00b7 pt=1\u03a6 (r)\u2212 (1\u2212 u) \u00b7 pt=0\u03a6 (r) ) dr \u2223\u2223\u2223\u2223 +B \u00b7 sup\ng\u2208F \u2223\u2223\u2223\u2223\u222b R g(r) ( (1\u2212 u) \u00b7 pt=0\u03a6 (r)\u2212 u \u00b7 pt=1\u03a6 (r) ) dr \u2223\u2223\u2223\u2223 = `F (h,\u03a6) + 2B \u00b7 UIPM ( u \u00b7 pt=1\u03a6 , (1\u2212 u) \u00b7 pt=0\u03a6 ) ,\nwhere equality (6) is by Lemma 5, and inequality (7) is by the assumption that L(Yt(\u03a8(r)), h(r, t)) \u2208 F for t = 0, 1.\nWe also have an immediate corollary: Corollary 1. Under the conditions of Theorem 3 above, if pF (t = 1) = 12 , then:\n`CF (h,\u03a6) \u2264 `F (h,\u03a6) + IPM ( pt=1\u03a6 , p t=0 \u03a6 ) . (8)\nProof. The result follows using Lemma 4.\nThe essential point in the proof of Theorem 3 is inequality 7. Note that on the l.h.s. of the inequality, we need to evaluate the expectations of L (Y0(\u03a8(r)), h(r, 0) ) over pt=1 and L (Y1(\u03a8(r)), h(r, 1) ) over pt=0. Both of these expectations are in general unavailable, since they require us to evaluate treatment outcomes on the control, and control outcomes on the treated. We therefore upper bound these unknowable quantities by taking a supremum over a function family which includes L (Y0(\u03a8(r)), h(r, 0) ) and L (Y1(\u03a8(r)), h(r, 1) ). The upper bound ignores the outcome information, and amounts to measuring a distance between two distributions we have samples from: the control and treated distribution. Note that for a randomized trial with p(t = 1) = 12 , we have that IPM(pt=1\u03a6 , p t=0 \u03a6 ) = 0, and indeed in that case `CF (h,\u03a6) = `F (h,\u03a6).\nThe crucial condition in Theorem 3 is that the function f\u03a6,h(r) := L (Yt(\u03a8(r)), h(r, t) ) is in F. We now look into two specific function families F, and evaluate what does this inclusion condition entail.\nA.3 The family of 1-Lipschitz functions\nFor S \u2282 Rd, a function f : S \u2192 R has Lipschitz constant K if for all x, y \u2208 S, |f(x) \u2212 f(y)| \u2264 K\u2016x\u2212 y\u2016. If f is differentiable, then a sufficient condition for K-Lipschitz constant is if \u2016\u2202f\u2202s \u2016 \u2264 K for all s \u2208 S. For simplicity\u2019s sake we assume throughout this subsection that the true labeling functions Y0, Y1 and the loss L are differentiable. However, this assumption could be relaxed to a mere Lipschitzness assumption.\nAssumption 7. There exists a constant K > 0 such that for all x \u2208 X , t \u2208 {0, 1}, \u2016\u2202Yt(x)\u2202x \u2016 \u2264 K. Assumption 8. The loss function L is differentiable and there exists a constant KL > 0 such that\u2223\u2223\u2223dL(y1,y2)dyi \u2223\u2223\u2223 \u2264 KL for i = 1, 2.\nLoss functions which obey Assumption 8 include the log-loss, hinge-loss, absolute loss, and for compact Y also the squared loss. When we let F in Definition 8 be the family of 1-Lipschitz functions, we obtain the so-called 1-Wasserstein distance between distributions, which we denote Wass(\u00b7, \u00b7). It is well known that Wass(\u00b7, \u00b7) is indeed a metric between distributions [32]. The Wasserstein distance has been extended to unnormalized distributions (i.e. positive measures) by several authors. We use the formulation of [18], who has shown how to reduce the problem of Wasserstein distance between unnormalized distributions into the ordinary Wasserstein distance, by adding a point \u201cat infinity\u201d. This has been followed through by [14], and we use the latter\u2019s algorithmic framework in the experimental section of our paper. For a much deeper mathematical treatment of the subject, we refer the reader to [6].\nDefinition 10. Let \u2202\u03a6(x)\u2202x be the Jacobian matrix of \u03a6 at point x, i.e. the matrix of the partial derivatives of \u03a6. Let \u03c3max(A) and \u03c3min(A) denote respectively the largest and smallest singular\nvalues of a matrix A. Define \u03c1(\u03a6) = supx\u2208X \u03c3max ( \u2202\u03a6(x) \u2202x ) /\u03c3min ( \u2202\u03a6(x) \u2202x ) .\nIt is an immediate result that \u03c1(\u03a6) \u2265 1. Definition 11. We will call a representation function \u03a6 : X \u2192 R Jacobian-normalized if supx\u2208X \u03c3max ( \u2202\u03a6(x) \u2202x ) = 1.\nNote that any non-constant representation function \u03a6 can be Jacobian-normalized by a simple scalar multiplication.\nLemma 6. Assume that \u03a6 is a Jacobian-normalized representation, and let \u03a8 be its inverse. Define Y\u0303t : R \u2192 R by Y\u0303t(r) \u2261 Yt(\u03a8(r)). The Lipschitz constant of Y\u0303t is bounded by \u03c1(\u03a6)K, where K is from Assumption 7, and \u03c1(\u03a6) as in Definition 10.\nProof. Let \u03a8 : R \u2192 X be the inverse of \u03a6, which exists by the assumption that \u03a6 is one-to-one. Let \u2202\u03a6(x) \u2202x be the Jacobian matrix of \u03a6 evaluated at x, and similarly let \u2202\u03a8(r) \u2202r be the Jacobian matrix of \u03a8 evaluated at r. Note that \u2202\u03a8(r)\u2202r \u00b7 \u2202\u03a6(x) \u2202x = I for r = \u03a6(x), since \u03a8 \u25e6 \u03a6 is the identity function on X . Therefore for any r \u2208 R and x = \u03a8(r):\n\u03c3max\n( \u2202\u03a8(r)\n\u2202r\n) =\n1\n\u03c3min ( \u2202\u03a6(x) \u2202x ) , (9) where \u03c3max(A) and \u03c3min(A) are respectively the largest and smallest singular values of the matrix A, i.e. \u03c3max(A) is the spectral norm of A.\nFor x = \u03a8(r) and t \u2208 {0, 1}, we have by the chain rule:\n\u2016\u2202Y\u0303t(r) \u2202r \u2016 = \u2016\u2202Yt(\u03a8(r)) \u2202r \u2016 = \u2016\u2202Yt(\u03a8(r)) \u2202\u03a8(r) \u2202\u03a8(r) \u2202r \u2016 \u2264 (10)\n\u2016\u2202\u03a8(r) \u2202r \u2016\u2016\u2202Yt(\u03a8(r)) \u2202\u03a8(r) \u2016 = (11)\n1\n\u03c3min ( \u2202\u03a6(x) \u2202x )\u2016\u2202Yt(x) \u2202x \u2016 \u2264 (12)\nK\n\u03c3min ( \u2202\u03a6(x) \u2202x ) \u2264 \u03c1(\u03a6)K, (13) where inequality (10) is by the matrix norm inequality, equality (11) is by (9), inequality (12) is by assumption 7 on the norms of the gradient of Yt(x) w.r.t x , and inequality (13) is by Definition 10 of \u03c1(\u03a6), the assumption that \u03a6 is Jacobian-normalized, and noting that singular values are necessarily non-negative.\nLemma 7. Under the conditions of Theorem 3, further assume that Y0, Y1 have gradients bounded by K as in 7, that h has bounded gradient norm bK, that the loss L has bounded gradient norm KL, and that \u03a6 is Jacobian-normalized. Then the Lipschitz constant of L (Yt(\u03a8(r)), h(r, t) ) is upper bounded by KL \u00b7K (\u03c1(\u03a6) + b) for t = 0, 1.\nProof. Using the chain rule, we have that:\n\u2202L \u2202r =\n\u2202L\n\u2202Yt(\u03a8(r))\n\u2202Yt(\u03a8(r))\n\u2202r +\n\u2202L\n\u2202h(r, t)\n\u2202h(r, t)\n\u2202r \u2264\nKlK\u03c1(\u03a6) +Kl \u00b7 bK = KL \u00b7K (\u03c1(\u03a6) + b) ,\nwhere the inequality is due to Lemma 6.\nTheorem 4. Let u = pF (t = 1) be the marginal probability of treatment, and assume 0 < u < 1. Let \u03a6 : X \u2192 R be a one-to-one, Jacobian-normalized representation function. Let K be the Lipschitz constant of the functions Y0, Y1 on X . Let KL be the Lipschitz constant of the loss function L. Let h : R\u00d7 {0, 1} \u2192 R be an hypothesis with Lipschitz constant bK. Then:\n`CF (h,\u03a6) \u2264 `F (h,\u03a6) + 2 (\u03c1(\u03a6) + b) \u00b7K \u00b7KL \u00b7Wass(u \u00b7 pt=1\u03a6 , (1\u2212 u) \u00b7 pt=0\u03a6 ). (14)\nProof. We will apply Theorem 3 with F = {f : R \u2192 R s.t. f is 1-Lipschitz}. By Lemma 7, we have that for B = (\u03c1(\u03a6) + b) \u00b7K \u00b7KL, the function 1BL (Yt(\u03a8(r)), h(r, t) ) \u2208 F. Inequality (14) then holds as a special case of Theorem 3.\nThe assumption that \u03a6 is normalized is rather natural, as we do not expect a certain scale from a representation. Furthermore, below we show that in fact the Wasserstein distance is positively homogeneous with respect to the representation \u03a6. Therefore, in Theorem 4, we can indeed assume that \u03a6 is normalized. The specific choice of Jacobian-normalized scaling yields what is in our opinion a more interpretable result in terms of the inverse condition number \u03c1(\u03a6).\nLemma 8. The Wasserstein distance is positive homogeneous for scalar transformations of the underlying space. Let p, q be probability density functions defined over X . For alpha > 0 and the mapping \u03a6(x) = \u03b1x, let p\u03b1 and q\u03b1 be the distributions on \u03b1X induced by \u03a6. Then:\nWass (p\u03b1, q\u03b1) = \u03b1Wass (p, q) .\nProof. Following [32, 21], we use another characterization of the Wasserstein distance. LetMp,q be the set of mass preserving maps from X to itself which map the distribution p to the distribution q. That is,Mp,q = {M : X \u2192 X s.t. q(M(S)) = p(S) for all measurable bounded S \u2282 X}. We then have that:\nWass(p, q) = inf M\u2208Mp,q \u222b X \u2016M(x)\u2212 x\u2016p(x) dx. (15)\nIt is known that the infimum in (15) is actually achievable [32, Theorem 5.2]. Denote byM\u2217 : X \u2192 X the map achieving the infimum for Wass(p, q) . Define M\u2217\u03b1 : \u03b1X \u2192 \u03b1X , by M\u2217\u03b1(x\u2032) = \u03b1M\u2217(x \u2032\n\u03b1 ), where x\u2032 = \u03b1x. M\u2217\u03b1 maps p\u03b1 to q\u03b1, and we have that \u2016M\u2217\u03b1(x\u2032)\u2212 x\u2032\u2016 = \u03b1\u2016M\u2217(x)\u2212 x\u2016. Therefore M\u2217\u03b1 achieves the infimum for the pair (p\u03b1, q\u03b1), and we have that Wass (p\u03b1, q\u03b1) = \u03b1Wass (p, q).\nA.4 Functions in the unit ball of a RKHS\nLetHx,Hr be a reproducing kernel Hilbert space, with corresponding kernels kx(\u00b7, \u00b7), kr(\u00b7, \u00b7). We have for all x \u2208 X that kx(\u00b7, x) is its Hilbert space mapping, and similarly kr(\u00b7, r) for all r \u2208 R. Recall that the major condition in Theorem 3 is that L (Yt(\u03a8(r)), h(r, t) ) \u2208 F. The function space F we use here is F = {g \u2208 Hr s.t. \u2016g\u2016Hr \u2264 1}. We will focus on the case where L is the squared loss, and we will make the following two assumptions:\nAssumption 9. There exist fY1 , fY2 \u2208 Hx such that Yt(x) = \u2329 fYt , kx(x, \u00b7) \u232a Hx\n, i.e. the true labeling functions Y0, Y1 are inHx. Further assume that \u2016fYt \u2016Hx \u2264 K.\nAssumption 10. Let \u03a6 : X \u2192 Y be an invertible representation function, and let \u03a8 be its inverse. We assume there exists a bounded linear operator \u0393\u03a6 : Hr \u2192 Hx such that \u2329 fYt , kx(\u03a8(r), \u00b7) \u232a Hx\n=\u2329 fYt ,\u0393\u03a6kr(r, \u00b7) \u232a Hx\n. We further assume that the Hilbert-Schmidt norm (operator norm) \u2016\u0393\u03a6\u2016HS of \u0393\u03a6 is bounded by K\u03a6.\nThe two assumptions above amount to assuming that \u03a6 can be represented as one-to-one linear map between the two Hilbert spacesHx andHr. Under Assumptions 9 and 10 about Y0, Y1, and \u03a6, we have that Yt(\u03a8(r)) = \u2329 \u0393\u2217\u03a6f Y t , kr(r, \u00b7) \u232a Hr\n, where \u0393\u2217\u03a6 is the adjoint operator of \u0393\u03a6 [17].\nLemma 9. Let h : R\u00d7{0, 1} \u2192 R be an hypothesis, and assume that there exist fht \u2208 Hr such that h(r, t) = \u2329 fht , kr(r, \u00b7) \u232a Hr\n, such that \u2016fht \u2016Hr \u2264 b. Under Assumption 9 about Y0, Y1, we have that L (Yt(\u03a8(r)), h(r, t) ) = (Yt(\u03a8(r))\u2212 h(r, t))2 is in the tensor Hilbert space Hr \u2297Hr. Moreover, the norm of (Yt(\u03a8(r))\u2212 h(r, t))2 inHr \u2297Hr is upper bounded by 4 ( K2\u03a6K 2 + b2 ) . Proof. By linearity of the Hilbert space, we have that Yt(\u03a8(r))\u2212 h(r, t) = \u2329 \u0393\u2217\u03a6f Y t , kr(r, \u00b7) \u232a Hr \u2212\u2329\nfht , kr(r, \u00b7) \u232a Hr = \u2329 \u0393\u2217\u03a6f Y t \u2212 fht , kr(r, \u00b7) \u232a Hr\n. By a well known result [31, Theorem 7.25], the product (Yt(\u03a8(r))\u2212 h(r, t)) \u00b7 (Yt(\u03a8(r))\u2212 h(r, t)) lies in the tensor product space Hr \u2297Hr, and is equal to \u2329 (\u0393\u2217\u03a6f Y t \u2212 fht )\u2297 (\u0393\u2217\u03a6fYt \u2212 fht ), kr(r, \u00b7)\u2297 kr(r, \u00b7) \u232a Hr\u2297Hr\n. The norm of this function in Hr \u2297Hr is \u2016\u0393\u2217\u03a6fYt \u2212 fht \u20162Hr . This is the general Hilbert space version of the fact that for a vector w \u2208 Rd one has that \u2016ww>\u2016F = \u2016w\u201622, where \u2016 \u00b7 \u2016F is the matrix Frobenius norm, and \u2016 \u00b7 \u201622 is the square of the standard Euclidean norm. Now we have that:\n\u2016\u0393\u2217\u03a6fYt \u2212 fht \u20162Hr \u2264 (16) 2\u2016\u0393\u2217\u03a6fYt \u20162Hr + 2\u2016f h t \u20162Hr \u2264 (17) 2\u2016\u0393\u2217\u03a6\u20162HS\u2016fYt \u20162Hx + 2\u2016f h t \u20162Hr = (18) 2\u2016\u0393\u03a6\u20162HS\u2016fYt \u20162Hx + 2\u2016f h t \u20162Hr leq (19) 2K2\u03a6K 2 + 2b2. (20)\nInequality (16) is because for any Hilbert spaceH, \u2016a\u2212 b\u20162H \u2264 2\u2016a\u20162H + 2\u2016b\u20162H. Inequality (17) is by the definition of the operator norm. Equality (18) is because the norm of the adjoint operator is equal to the norm of the original operator, where we abused the notation \u2016 \u00b7 \u2016HS to mean both the norm of operators fromHx toHr and vice-versa. Finally, inequality (19) is by Assumptions 9 and 10, and by the premise on the norm of fht .\nTheorem 5. Let u = pF (t = 1) be the marginal probability of treatment, and assume 0 < u < 1. Let \u03a6 : X \u2192 R be a one-to-one representation function which obeys Assumption 10 with corresponding operator \u0393\u03a6 with operator norm K\u03a6. Let the functions Y0, Y1 obey Assumption 9, with bounded Hilbert space norm K . Let h : R \u00d7 {0, 1} \u2192 R be an hypothesis, and assume that there exist fht \u2208 Hr such that h(r, t) = \u2329 fht , kr(r, \u00b7) \u232a Hr\n, such that \u2016fht \u2016Hr \u2264 b. Assume that `F and `CF are defined with respect to L being the squared loss. Then:\n`CF (h,\u03a6) \u2264 `F (h,\u03a6) + 4 ( K2\u03a6K 2 + b2 ) \u00b7MMD(u \u00b7 pt=1\u03a6 , (1\u2212 u) \u00b7 pt=0\u03a6 ), (21)\nwhere `CF and `F use the squared loss.\nProof. We will apply Theorem 3 with F = f \u2208 Hr \u2297Hr s.t. \u2016f\u2016Hr\u2297Hr \u2264 1. By Lemma 9, we have that for B = 2 ( K2\u03a6K 2 + b2 )\nand L being the squared loss, 1BL (Yt(\u03a8(r)), h(r, t) ) \u2208 F. Inequality (21) then holds as a special case of Theorem 3."}, {"heading": "B Treatment effect estimation loss", "text": "Definition 12. The treatment effect for unit x is:\n\u03c4(x) = Y1(x)\u2212 Y0(x).\nWe now show that minimizing the counterfactual loss `CF is closely tied to minimizing the error in estimating the individualized treatment effect \u03c4(x) = Y1(x)\u2212 Y0(x). We note that the statements in this subsection do not specifically rely on the representation learning aspect of our work, and could also be phrased without using the representation function \u03a6.\nLet g : X \u00d7 {0, 1} \u2192 Y by an hypothesis. For example, we could have that g(x, t) = h(\u03a6(x), t) for a representation \u03a6 and hypothesis h defined over the output of \u03a6. Definition 13. The inductive treatment effect estimate for unit x is:\n\u03c4\u0302g(x) = g(x, 1)\u2212 g(x, 0). Definition 14. The transductive treatment effect estimate for unit xi with treatment assignment ti is:\n\u03c4 \u2032g(xi, ti) = { Y1(xi)\u2212 g(x1, 0) if ti = 1 g(xi, 1)\u2212 Y0(xi) if ti = 0\nDefinition 15. Let Yt : X \u2192 R be the true labeling functions for t = 0, 1. The expected Precision in Estimation of Heterogeneous Effect (PEHE) loss of g is:\n`PEHE(g) = \u222b X |\u03c4\u0302g(x)\u2212 \u03c4(x)|a p(x) dx\nThe expected Individualized Treatment Effect (ITE) loss of g is:\n`ITE(g) = \u222b X\u00d7{0,1} \u2223\u2223\u03c4 \u2032g(x, t)\u2212 \u03c4(x)\u2223\u2223a pF (x, t) dxdt where a = 1 corresponds to the absolute loss, and a = 2 corresponds to the squared loss.\nNote that the transductive treatment effect and the corresponding `ITE loss are defined with respect to the treatment assignment, whereas the inductive treatment effect and the corresponding `PEHE loss are defined irrespective of the treatment assignment.\nWe show that for the transductive case where we assume that for a unit xi either Y1(xi) or Y0(xi) is known, minimizing `CF with a squared loss is equivalent to minimizing Ex\u223cp\u0302(x) [ (\u03c4 \u2032(x)\u2212 \u03c4(x))2 ] .\nFor the inductive case, we show that Ex\u223cp(x) [ (\u03c4\u0302(x)\u2212 \u03c4(x))2 ] is upper bounded by 2`F + 2`CF\nwhere `F and `CF are w.r.t. to the squared loss, while Ex\u223cp(x) [|\u03c4\u0302(x)\u2212 \u03c4(x)|] is upper bounded by `F + `CF where `F and `CF are w.r.t. to the absolute loss.\nThe intuition for both results is simple: for the transductive case, the only unknown quantity are the counterfactual outcomes, which we seek to know. For the inductive case, we incur an additional error on top of the transductive case. This error comes from inferring the unknown factual outcomes for new samples, which is the classic machine learning inductive learning scenario. Lemma 10. For an hypothesis g : X \u00d7 {0, 1} \u2192 Y such that g(x, t) = h(\u03a6(x), t), and using the squared loss or absolute loss, we have:\n`ITE(g) = `CF (h,\u03a6).\nProof. `ITE(g) = (22)\u222b X |(Y1(x)\u2212 g(x, 0))\u2212 (Y1(x)\u2212 Y0(x))|a pF (x, t = 1) dx\n+ \u222b X |(g(x, 1)\u2212 Y0(x))\u2212 (Y1(x)\u2212 Y0(x))|a pF (x, t = 0) dx =\u222b\nX |Y0(x)\u2212 g(x, 0)|a pF (x, t = 1)dx+ \u222b X |g(x, 1)\u2212 (Y1(x)|a pF (x, t = 0) dx = (23)\u222b\nX |Y0(\u03a8(r))\u2212 h(r, 0)|a pF\u03a6(r, t = 1) dr + \u222b R |h(r, 1)\u2212 (Y1(\u03a8(r))|a pF\u03a6(r, t = 0) dx =\u222b\nX |Y0(\u03a8(r))\u2212 h(r, 0)|a pCF\u03a6 (r, t = 0) dr + \u222b R |h(r, 1)\u2212 (Y1(\u03a8(r))|a pF\u03a6(r, t = 0) dx,=\n`CF (h,\u03a6)\nAlgorithm 2 Counterfactual balanced regression with integral probability metrics\n1: Input: Factual sample (x1, t1, yF1 ), . . . , (xn, tn, yFn ), scaling parameter \u03b1 > 0, loss function L (\u00b7, \u00b7), representation network \u03a6W with initial weights by W, outcome network hV with intial weights V, function family F for IPM loss 2: while not converged do 3: Sample a mini-batch with m\u2032 control and m treated units\n(xi1 , 0, y F i1 ), . . . , (xim , 0, y F im ), (xim+1 , 1, y F im+1 ), . . . , (xim+m\u2032 , 1, y F im+m\u2032 )\n4: Calculate the gradient of the imbalance penalty: g1 = \u2207W IPMF ( {\u03a6W(xij )}mj=1, {\u03a6W(xik)}m \u2032 k=m+1 ) 5: Calculate the gradients of the empirical loss:\ng2 = \u2207V 1m+m\u2032 \u2211m+m\u2032 j=1 L ( hV(\u03a6W(xij ), tij ), y F ij ) g3 = \u2207W 1m+m\u2032 \u2211m+m\u2032 j=1 L ( hV(\u03a6W(xij ), tij ), y F ij\n) 6: Obtain step size scalar or matrix \u03b7 with standard neural net methods e.g. RMSProp 7: update W\u2190W \u2212 \u03b7(\u03b1g1 + g3) and V\u2190 V \u2212 \u03b7(g2) 8: check convergence criterion 9: end while\nwhere (22) is by decomposing \u03c4 \u2032g over t = 0, 1, (23) is by the change of variable formula and definition of pF\u03a6 , and \u03a8 is the inverse function of \u03a6.\nLemma 11. For an hypothesis g : X \u00d7 {0, 1} \u2192 Y such that g(x, t) = h(\u03a6(x), t), and using the loss L(y1, y2) = |y1 \u2212 y2|a for a = 1, 2, we have:\n`PEHE(g) \u2264 a \u00b7 (`CF (h,\u03a6) + `F (h,\u03a6)) .\nProof. For a = 1, 2:\n`PEHE(g) =\u222b X |(g(x, 1)\u2212 g(x, 0))\u2212 (Y1(x)\u2212 Y0(x))|a p(x) dx =\u222b\nX |(g(x, 1)\u2212 Y1(x)) + (Y0(x)\u2212 g(x, 0))|a p(x) dx \u2264 (24) a \u00b7 \u222b X |(g(x, 1)\u2212 Y1(x))|a + |(Y0(x)\u2212 g(x, 0))|a p(x) dx = (25)\na \u222b X |(g(x, 1)\u2212 Y1(x))|a + |(Y0(x)\u2212 g(x, 0))|a pF (x, t = 0) dx\n+ a \u222b X |(g(x, 1)\u2212 Y1(x))|a + |(Y0(x)\u2212 g(x, 0))|a pF (x, t = 1) dx =\na \u222b X |g(x, 1)\u2212 Y1(x)|a + |(Y0(x)\u2212 g(x, 0))|a pCF (x, t = 1) dx\u222b\nX |g(x, 1)\u2212 Y1(x)|a + |(Y0(x)\u2212 g(x, 0))|a pF (x, t = 1) dx = (26)\na (`CF (h,\u03a6) + `F (h,\u03a6)) ,\nwhere (24) is by the inequality |x + y|a \u2264 a(|x|a + |y|a) for a = 1, 2, (25) is because p(x) = p(x, t = 0) + p(x, t = 1), and (26) follows from the same change of variable substitution made in (23) in Lemma 10 above."}, {"heading": "C Algorithmic details", "text": "We give details about the algorithms used in our framework. First, we restate Algorithm 1.\nAlgorithm 3 Computing the stochastic gradient of the Wasserstein distance\n1: Input: Factual (x1, t1, yF1 ), . . . , (xn, tn, yFn ), representation network \u03a6W with current weights by W 2: Randomly sample a mini-batch with m treated and m\u2032 control units (xi1 , 0, y F i1 ), . . . , (xim , 0, y F im ), (xim+1 , 1, y F im+1 ), . . . , (xi2m , 1, y F i2m ) 3: Calculate the m\u00d7m pairwise distance matrix between all treatment and control pairs M(\u03a6W): Mkl(\u03a6) = \u2016\u03a6W(xik)\u2212 \u03a6W(xim+l)\u2016 4: Calculate the approximate optimal transport matrix T \u2217 using Algorithm 3 of [9], with input M(\u03a6W) 5: Calculate the gradient: g1 = \u2207W \u3008T \u2217,M(\u03a6W)\u3009\nC.1 Minimizing the Wasserstein distance\nIn general, computing (and minimizing) the Wasserstein distance involves solving a linear program, which may be prohibitively expensive for many practical applications. Cuturi citecuturi2013sinkhorn showed that an approximation based on entropic regularization can be obtained through the SinkhornKnopp matrix scaling algorithm, at orders of magnitude faster speed. Dubbed Sinkhorn distances [8], the approximation is computed using a fixed-point iteration involving repeated multiplication with a kernel matrix K. We can use the algorithm of [8] in our framework. See Algorithm 3 for an overview of how to compute the gradient g1 in Algorithm 2. When computing g1, disregarding the gradient \u2207WT \u2217 amounts to minimizing an upper bound on the Sinkhorn transport. For non-uniform populations p(t) 6= 1/2, our method can still be applied using the unnormalized version of the Wasserstein distance, using ideas presented by [18, 14], and explored much more deeply by [6]. The Sinkhorn transport can still be used to approximate the Wasserstein distance, but with small modifications. Let \u03bb and \u03b4 be parameters and M\u0303 the matrix\nM\u0303 = [ M \u03b4 \u03b4 0 ] .\nThen, define an nt + 1-dimensional vector a\na = [p(t), ..., p(t), 1\u2212 p(t)]>\nand an nc + 1-dimensional vector b\nb = [1\u2212 p(t), ..., 1\u2212 p(t), p(t)]>\nwhere nt and nc are the number of treated and controls. Then, to get T \u2217, apply Algorithm 3 of [9] on M\u0303, a and b.\nWhile our framework is agnostic to the parameterization of \u03a6, our experiments focus on the case where \u03a6 is a neural network. For convenience of implementation, we may represent the fixedpoint iterations of the Sinkhorn algorithm as a recurrent neural network, where the states ut evolve according to\nut+1 = nt./(ncK(1./(u > t K) >)) .\nHere, K is a kernel matrix corresponding to a metric such as the euclidean distance, Kij = e\u2212\u03bb\u2016\u03a6(xi)\u2212\u03a6(xj)\u20162 , and nc, nt are the sizes of the control and treatment groups. In this way, we can minimize our entire objective with most of the frameworks commonly used for training neural networks, out of the box.\nC.2 Minimizing the maximum mean discrepancy\nThe MMD of treatment populations in the representation \u03a6, for a kernel k(\u00b7, \u00b7) can be written as,\nMMDk({\u03a6W(xij )}mj=1, {\u03a6W(xik)}m \u2032 k=m+1) = (27)\n1\nm(m\u2212 1) m\u2211 j=1 m\u2211 k=1,k 6=j k(\u03a6W(xij ),\u03a6W(xik)) (28)\n+ 2\nmm\u2032 m\u2211 j=1 m+m\u2032\u2211 k=m k(\u03a6W(xij ),\u03a6W(xik)) (29)\n+ 1\nm\u2032(1\u2212m\u2032) m\u2211 j=1 m\u2032\u2211 k=m,k 6=j k(\u03a6W(xij ),\u03a6W(xik)) (30)\nThe unnormalized version, where the marginal probability of treatment u 6= 1/2, is\nMMDk({\u03a6W(xij )}mj=1, {\u03a6W(xik)}m \u2032 k=m+1) = (31)\n2p(t)2\nm(m\u2212 1) m\u2211 j=1 m\u2211 k=1,k 6=j k(\u03a6W(xij ),\u03a6W(xik)) (32)\n+ 4p(t)(1\u2212 p(t))\nmm\u2032\nm\u2211 j=1 m+m\u2032\u2211 k=m k(\u03a6W(xij ),\u03a6W(xik)) (33)\n+ 2(1\u2212 p(t))2\nm\u2032(1\u2212m\u2032) m\u2211 j=1 m\u2032\u2211 k=m,k 6=j k(\u03a6W(xij ),\u03a6W(xik)) (34)\nThe (unnormalized) linear maximum-mean discrepancy can be written as a distance between means. In the notation of Algorithm 2,\nMMD = 2 \u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 p(t)\u03a6W(xij )\u2212 (1\u2212 p(t)) 1 m\u2032 m\u2032\u2211 k=m+1 \u03a6W(xik) \u2225\u2225\u2225\u2225\u2225\u2225 2\nLet\nf(W) = p(t) 1\nm m\u2211 j=1 \u03a6W(xij )\u2212 (1\u2212 p(t)) 1 m\u2032 m\u2032\u2211 k=m+1 \u03a6W(xik)\nThen the gradient of the MMD with respect to W is,\ng1 = 2 df(W)\ndW\nf(W)\n\u2016f(W)\u20162 ."}, {"heading": "D Empirical results", "text": "D.1 General observation\nWhile not visible in the tables due to averaging, for some of the 1000 realizations of the IHDP dataset the imbalance penalty does not have an effect (neither positive nor negative). We believe this is due to these sets already being fairly balanced. We also note that in some cases, methods with linear hypothesis in the treatment, perform the best by removing any influence of the covariates on the model. This is indeed what happens for L+R and BLR on IHDP.\nD.2 IHDP and News\nJohansson et al. [20] introduced another semi-simulated dataset dubbed News, based on topicmodeling of a collection of news articles. Covariates represent counts of words in a pre-specified\nvocabulary representing words that are significant to at least one of the topics. The outcome and treatment models are based on the topic-distribution of documents. The set comprises 5000 observations (articles) with 3477 covariates (words). The results of both the IHDP and News experiments are presented in Table 2. For the News data, we train representation layers with 400 units and output layers with 200 units. For DR on News, we perform the logistic regression on the first 100 principal components of the data.\nOn News, we don\u2019t see a significant gain from using the proposed imbalance penalty (compare e.g. MMD and \u03b1 = 0), but neither does the penalty hurt the performance. A possible explanation is that the dataset is not very imbalanced to begin with.\nD.3 IHDP - Increasing imbalance\nT-SNE visualizations of the subsampled IHDP for q = 0 and q = 1 respectively can be seen in Figure 3.\nThe results of both the continuous and binary task are presented in Table 3.\nFigure 4 shows the bias in the estimated ATE for our CFR-4-0 method, OLS, and a naive estimator. We can see that when we don\u2019t impose balance penalties, our neural network method is slightly better than OLS, in terms of estimating the ATE. This is expected as we are free to use non-linear interactions between the covariates, but still restricted to hypotheses that are linear functions of the treatment variable. As we impose balance penalties, increasing \u03b1, the performance improves until it dominates the behavior and the method reduces to the naive estimator. This is also expected as the model disregards the covariates completely when \u03b1\u2192\u221e."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "There is intense interest in applying machine learning methods to problems of<lb>causal inference which arise in applications such as healthcare, economic policy,<lb>and education. In this paper we use the counterfactual inference approach to causal<lb>inference, and propose new theoretical results and new algorithms for performing<lb>counterfactual inference. Building on an idea recently proposed by Johansson et al.<lb>[20], our results and methods rely on learning so-called \u201cbalanced\u201d representations:<lb>representations that are similar between the factual and counterfactual distributions.<lb>We give a novel, simple and intuitive bound, showing that the expected counter-<lb>factual error of a representation is bounded by a sum of the factual error of that<lb>representation and the distance between the factual and counterfactual distributions<lb>induced by the representation. We use Integral Probability Metrics to measure<lb>distances between distributions, and focus on two special cases: the Wasserstein<lb>distance and the Maximum Mean Discrepancy (MMD) distance. Our bound leads<lb>directly to new algorithms, which are simpler and easier to employ compared to<lb>those suggested in [20]. Experiments on real and simulated data show the new<lb>algorithms match or outperform state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}