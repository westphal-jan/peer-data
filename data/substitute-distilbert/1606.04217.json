{"id": "1606.04217", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "abstract": "dealing with analyzing complex word forms in morphologically sensitive languages is an open problem in term processing, and is particularly hard in translation. in contrast to most modern neural systems of translation, philosophers discard the identity for rare words, in this paper we propose several architectures for processing word representations beyond character and morpheme involving word processors. we incorporate these representations proposing a novel global translation model which jointly learns word alignments and translations via comparatively hard attention mechanism. evaluating on translating from several morphologically rich languages into english, we show consistent improvements over strong baseline methods, of between 1 and 1. 5 bleu points.", "histories": [["v1", "Tue, 14 Jun 2016 07:04:37 GMT  (382kb,D)", "http://arxiv.org/abs/1606.04217v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["ekaterina vylomova", "trevor cohn", "xuanli he", "gholamreza haffari"], "accepted": false, "id": "1606.04217"}, "pdf": {"name": "1606.04217.pdf", "metadata": {"source": "CRF", "title": "Word Representation Models for Morphologically Rich Languages in Neural Machine Translation", "authors": ["Ekaterina Vylomova", "Trevor Cohn", "Xuanli He", "Gholamreza Haffari"], "emails": ["evylomova@gmail.com", "tcohn@unimelb.edu.au", "xuanlih@student.unimelb.edu.au", "gholamreza.haffari@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "Models of end-to-end machine translation based on neural networks have been shown to produce excellent translations, rivalling or surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). A central challenge in neural MT is handling rare and uncommon words. Conventional neural MT models use a fixed modest-size vocabulary, such that the identity of rare words are lost, which makes their translation exceedingly difficult. Accordingly sentences containing rare words tend to be translated much more poorly than those containing only common words (Sutskever et al., 2014; Bahdanau et al., 2015). The rare word problem is particularly exacerbated when translating from morphology rich languages, where the several morphological variants of words result in a huge vocabulary\nwith a heavy tail distribution. For example in Russian, there are at least 70 words for dog, encoding case, gender, age, number, sentiment and other semantic connotations. Many of these words share a common lemma, and contain regular morphological affixation; consequently much of the information required for translation is present, but not in an accessible form for models of neural MT.\nIn this paper, we propose a solution to this problem by constructing word representations compositionally from smaller sub-word units, which occur more frequently than the words themselves. We show that these representations are effective in handling rare words, and increase the generalisation capabilities of neural MT beyond the vocabulary observed in the training set. We propose several neural architectures for compositional word representations, and systematically compare these methods integrated into a novel neural MT model.\nMore specifically, we make use of character sequences or morpheme sequences in building word representations. These sub-word units are combined using recurrent neural networks (RNNs), convolutional neural networks (CNNs), or simple bag-ofunits. This work was inspired by research into compositional word approaches proposed for language modelling (e.g., Botha and Blunsom (2014), Kim et al. (2016)), with a few notable exceptions (Ling et al., 2015b; Sennrich et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016), these approaches have not been applied to the more challenging problem of translation. We integrate these word representations into a novel neural MT model to build robust word representations for the source language.\nOur novel neural MT model, is based on the operation sequence model (OSM; Durrani et al. (2011),\nar X\niv :1\n60 6.\n04 21\n7v 1\n[ cs\n.N E\n] 1\n4 Ju\nn 20\nFeng and Cohn (2013)), which considers translation as a sequential decision process. The decisions involved in generating each target word is decomposed into separate translation and alignment factors, where each factor is modelled separately and conditioned on a rich history of recent translation decisions. Our OSM can be considered as a form of attentional encoder-decoder Bahdanau et al. (2015) with hard attention in which each decision is contextualised by at most one source word, contrasting with the soft attention in Bahdanau et al. (2015).\nIntegrating the word models into our neural OSM, we provide \u2013 for the first time \u2013 a comprehensive and systematic evaluation of the resulting word representations when translating into English from several morphologically rich languages, Russian, Estonian, and Romanian. Our evaluation includes both intrinsic and extrinsic metrics, where we compare these approaches based on their translation performance as well as their ability to recover synonyms for the rare words. We show that morpheme and character representation of words leads much better heldout perplexity although the improvement on the translation BLEU scores is more modest. Intrinsic analysis shows that the recurrent encoder tends to capture more morphosyntactic information about words, whereas convolutional network better encodes the lemma. Both these factors provide different strengths as part of a translation model, which might use lemmas to generalise over words sharing translations, and morphosyntax to guide reordering and contextualise subsequent translation decisisions. These factors are also likely to be important in other language processing applications."}, {"heading": "2 Related Work", "text": "Most neural models for NLP rely on words as their basic units, and consequently face the problem of how to handle tokens in the test set that are out-ofvocabulary (OOV), i.e., did not appear in the training set (or are considered too rare in the training set to be worth including in the vocabulary.) Often these words are either assigned a special UNK token, which allows for application to any data, however it comes at the expense of modelling accuracy especially in structured problems like language modelling and translation, where the identity of the word is paramount in making the next decision.\nOne solution to OOV problem is modelling sub-\nword units, using a model of a word from its composite morphemes. Luong et al. (2013) proposed a recursive combination of morphs using affine transformation, however this is unable to differentiate between the compositional and non-compositional cases. Botha and Blunsom (2014) aim to address this problem by forming word representations from adding a sum of each word\u2019s morpheme embeddings to its word embedding.\nMorpheme based methods rely on good morphological analysers, however these are only available for a limited set of languages. Unsupervised analysers (Creutz and Lagus, 2007) are prone to segmentation errors, particularly on fusional or polysynthetic languages. In these settings, character-level word representations may be more appropriate. Several authors have proposed convolutional neural networks over character sequences, as part of models of part of speech tagging (Santos and Zadrozny, 2014), language models (Kim et al., 2015) and machine translation (Costa-juss\u00e0 and Fonollosa, 2016). These models are able to capture not just orthographic similarity, but also some semantics. Another strand of research has looked at recurrent architectures, using long-short term memory units (Ling et al., 2015a; Ballesteros et al., 2015) which can capture long orthographic patterns in the character sequence, as well as non-compositionality.\nAll of the aforementioned models were shown to consistently outperform standard word-embedding approaches. But there is no systematic investigation of the various modelling architectures or comparison of characters versus morpheme as atomic units of word composition. In our work we consider both morpheme and character levels and study 1) wether character-based approaches can outperform morpheme-based, and, importantly, 2) what linguistic lexical aspects are best encoded in each type of architecture, and their efficacy as part of a machine translation model when translating from morphologically rich languages."}, {"heading": "3 Operation sequence model", "text": "The first contribution of this paper is a neural network variant of the Operational Sequence Model (OSM) (Durrani et al., 2011; Feng and Cohn, 2013). In OSM, the translation is modelled as a sequential decision process. The words of the target sentence are generated one at a time in a left-to right\nFigure 1: Illustration of the neural operation sequence model for an example sentence-pair.\norder, similar to the decoding strategy in traditional phrase-based SMT. The decisions involved in generating each target word is decomposed into a number of separate factors, where each factor is modelled separately and conditioned on a rich history of recent translation decisions.\nIn previous work (Durrani et al., 2011; Feng and Cohn, 2013), the sequence of operations is modelled as Markov chain with a bounded history, where each translation decision is conditioned on a finite history of past decisions. Using deep neural architectures, we model the sequence of translation decisions as a non-Markovian chain, i.e. with unbounded history. Therefore, our approach is able to capture long-range dependencies which are commonplace in translation and missed by previous approaches.\nMore specifically, the operations are (i) generation of a target word, (ii) jumps over the source sentence to capture re-ordering (to allow different sentence reordering in the target vs. source language), (iii) aligning to NULL to capture gappy phrases, and (iv) finishing the translation process. The probability of a sequence of operations to generate a target translation t for a given source sentence s is p(t,a|s) =\n|t|+1\u220f j=1 p(\u03c4j |t<j , \u03c4<j , s) |t|\u220f j=1 p(tj |t<j , \u03c4j , s) (1)\nwhere \u03c4j is a jump action moving over the source sentence (to align a target word to a source word or null) or finishing the translation process \u03c4|t|+1 = FINISH. It is worth noting that the sequence of operations for generating a target translation (in a left-to-\nright order) has a 1-to-1 correspondence to an alignment a, so the use of P (t,a|s) in the left-hand-side.\nOur model generates the target sentence and the sequence of operations with a recurrent neural network (Figure 1). At each stage, the RNN state is a function of the previous state, the previously generated target word, and an aligned source word, hj =\nMLP ( hj\u22121, r (t) tj\u22121 , r (s) sij ) using a single layer perceptron (MLP) which applies an affine transformation to the concatentated input vectors followed by a tanh activation function, where R(t) \u2208 RVT\u00d7ET and R(s) \u2208 RVS\u00d7ES are word embedding matrices with VS the size of the source vocabulary, VT the size of the target vocabulary, and ET and ES the word embedding sizes for the target and source languages, respectively.\nThe model then generates the target word ti and index of the source word to be translated next,1\ntj \u223c softmax ( affine(hj) )\nij+1 \u223c softmax ( \u03a6(s, i\u2264j , t\u2264j)b (f)\n+ r(s)W(sh)hj + r (s)W(st)r (t) tj ) where affine performs an affine transformation of its input,2 and the parameters include W(sh) \u2208 RES\u00d7H , W(ts) \u2208 RET\u00d7H , b(f) \u2208 RF , and F is the dimensionality of the feature vector \u03a6(.) representing the\n1The indices 0 and |s|+ 1 represent the NULL and FINISH operations.\n2An affine transform multiplies the input vector by a matrix and adding a bias vector, equivalent to a full connected hidden layer with linear activation.\ninduced alignment structure (explained in the next paragraph). The matrix encoding of the source sentence r(s) \u2208 R(|s|+2)\u00d7ES is defined as\nr(s) = [ rNULL, r (s) s1 , . . . , r (s) s|s| , rFINISH ] (2)\nwhere it includes the embeddings of the source sentence words and the NULL and FINISH actions.\nThe feature matrix \u03a6(|s|, i\u2264j , t\u2264j) \u2208 R(|s|+2)\u00d7F captures the important aspects between a candidate position for the next alignment and the current alignment position; this is reminiscent of the features captured in the HMM alignment model. The feature vector in each row is composed of two parts :3 (i) the first part is a one-hot vector activating the proper feature depending whether ij+1 \u2212 ij is equal to {0, 1,\u2265 2,\u2264 \u22121} or if the action is NULL or FINISH, and (ii) the second part consists of two features ij+1 \u2212 ij and ij+1\u2212ij|s| .\nNote that the neural OSM can be considered as a hard attentional model, as opposed to the soft attentional neural translation model (Bahdanau et al., 2015). In their soft attentional model, a dynamic summary of the source sentence is used as context to each translation decision, which is formulated as a weighted average of the encoding of all source positions. In the hard attentional model this context comes from the encoding of a single fixed source position. This has the benefit of allowing external information to be included into the model, here the predicted alignments from high quality word alignment tools, which have complementary strengths compared to neural network translation models."}, {"heading": "4 Word Representation Models", "text": "Now we turn to the problem of learning word representations. As outlined above, when translating morphologically rich languages, treating word types as unique discrete atoms is highly naive and will compromise translation quality. For better accuracy, we would need to characterise words by their subword units, in order to capture the lemma and morphological affixes, thereby allowing better generalisation between similar word forms.\nIn order to test this hypothesis, we consider both morpheme and character level encoding methods\n3More generally, \u03a6(.) can capture any aspects of the past alignment decisions, hence it can be used to impose structural biases to constrain the alignment space in neural OSM, e.g. symmetry, fertility, and position bias.\nwhich we compare to the baseline word embedding approach. For each type of sub-word encoder we learn two word representations: one estimated from the sub-units and the word embedding.4 Then we run max pooling over both embeddings to obtain the word representation, rw = mw ew, where mw is the embedding of word w and ew is the sub-word encoding. The max pooling operation captures non-compostionality in the semantic meaning of a word relative to its sub-parts. We assume that the model would favour unit-based embeddings for rare words and word-based for more common ones.\nLet U be the vocabulary of sub-word units, i.e., morphemes or characters, Eu be the dimensionality of unit embeddings, and M \u2208 REu\u00d7|U| be the matrix of unit embeddings. Suppose that a word w from the source dictionary is made up of a sequence of units Uw := [u1, . . . , u|w|], where |w| stands for the number of constituent units in the word. We combine the representation of sub-word units using a LSTM recurrent neural networks (RNN), convolutional neural network (CNN), or simple bag-of-units (described below). The resulting word representations are then fed to our neural OSM in eqn (2) as the source word embeddings."}, {"heading": "4.1 Bag of Sub-word Units", "text": "This method is inspired by (Botha and Blunsom, 2014) in which the embeddings of sub-word units are simply added together, ew = \u2211 u\u2208Uw mu, where mu is the embedding of sub-word unit u."}, {"heading": "4.2 Bidirectional LSTM Encoder", "text": "The encoding of the word is formulated using a pair of LSTMs (denoted bi-LSTM) one operating leftto-right over the input sequence and another operating right-to-left, h\u2192j = LSTM(h \u2192 j\u22121,muj ) and h\u2190j = LSTM(h \u2192 j+1,muj ) where h \u2192 j and h \u2190 j are the LSTM hidden states.5 The source word is then represented as a pair of hidden states, from left- and right-most states of LSTMs. These are fed into multilayer perception (MLP) with a single hidden layer and a tanh activation function to form the word representation, ew = MLP ( h\u2192|Uw|,h \u2190 1 ) .\n4We only include word embeddings for common words; rare words share a UNK embedding.\n5The memory cells are computed as part of the recurrence, suppressed here for clarity."}, {"heading": "4.3 Convolutional Encoder", "text": "The last word encoder we consider is a convolutional neural network, inspired by a similar approach in language modelling (Kim et al., 2016). Let Uw \u2208 REu\u00d7|U|w denote the unit-level representation of w, where the jth column corresponds to the unit embedding of uj . The idea of unitlevel CNN is to apply a kernel Ql \u2208 REu\u00d7kl with the width kl to Uw to obtain a feature map fl \u2208 R|U|w\u2212kl+1. More formally, for the jth element of the feature map the convolutional representation is fl(j) = tanh(\u3008Uw,j ,Ql\u3009+b), whereUw,j \u2208 REu\u00d7kl is a slice from Uw which spans the representations of the jth unit and its preceding kl \u2212 1 units, and \u3008A,B\u3009 = \u2211 i,j AijBij = Tr ( ABT ) denotes the Frobenius inner product. For example, suppose that the input has size [4 \u00d7 9], and a kernel has size [4 \u00d7 3] with a sliding step being 1. Then, we obtain a [1\u00d7 7] feature map. This process implements a character n-gram, where n is equal to the width of the filter. The word representation is then derived by max pooling the feature maps of the kernels: \u2200l : rw(l) = maxj fl(j). In order to capture interactions between the character n-grams obtained by the filters, a highway network (Srivastava et al., 2015) is applied after the max pooling layer, ew = t MLP(rw) + (1 \u2212 t) rw, where t = MLP\u03c3(rw) is a sigmoid gating function which modulates between a tanh MLP transformation of the input (left component) and preserving the input as is (right component)."}, {"heading": "5 Experiments", "text": "The Setup. We compare the different word representation models based on three morphologically rich languages using both exterinsic and intrinsic evaluations. For exterinsic evaluation, we investigate their effects in translating to English from Estonian, Romanian, and Russian using our neural OSM. For intrinsic evaluation, we investigate how accurately the models recover semantically/syntactically related words to a set of given words.\nDatasets. We use parallel bilingual data from Europarl for Estonian-English and Romanian-English (Koehn, 2005), and web-crawled parallel data for Russian-English (Antonova and Misyurev, 2011). For preprocessing, we tokenize, lower-case, and filter out sentences longer than 30 words. Further-\nmore, we apply a frequency threshold of 5, and replace all low-frequency words with a special UNK token. We split the corpora into three partitions: training (100K), development(10K), and test(10K); Table 1 provides the datasets statistics.\nMorfessor Training. We use Morfessor CATMAP (Creutz and Lagus, 2007) to perform morphological analysis needed for morph-based neural models. Morfessor does not rely on any linguistic knowledge, instead it relays on minimum description length principle to construct a set of stems, affixes and paradigms that explains the data. Each word form is then represented as (prefix)\u2217(stem)+(suffix)\u2217.\nWe ran Morfessor on the entire initial datasets, i.e before filtering out long sentences. The word perplexity is the only Morfessor parameter that has to be adjusted. The parameter depends on the vocabulary size: larger vocabulary requires higher perplexity number; setting the perplexity threshold to a small value results in over-splitting. We experimented with various thresholds and tuned these to yield the most reasonable morpheme inventories.6\nTable 1 presents the percentage of unknown words in the test for each source language . For reconstruction we considered the words from the native alphabet only. The recovering rate depends on the model. For characters all the words could be easily rebuilt. In case of morpheme-based approach the quality mainly depends on the Morfessor output and the level of word segmentation. In terms of morphemes, Estonian presents the highest reconstruction rate, therefore we expect it to benefit the most from the morpheme-based models. Romanian, on the other hand, presents the lowest unknown words rate being the most morphologically simple out of the three languages. Morfessor quality for Russian\n6 The selected thresholds were 600, 60 and 240 for Russian, Romanian and Estonian, respectively.\nwas the worst one, so we expect that Russian should mainly benefit from character-based models."}, {"heading": "5.1 Extrinsic Evaluation: MT", "text": "Training. We annotate the training sentence-pairs with their sequence of operations to training the neural OSM model. We first run a word aligner7 to align each target word to a source word. We then read off the sequence of operations by scanning the target words in a left-to-right order. As a result, the training objective consists of maximising the joint probability of target words and their alignments eqn 1, which is performed by stochastic gradient descent (SGD). The training stops when the likelihood objective on the development set starts decreasing.\nFor the re-ranker, we use the standard features generated by moses8 as the underlying phrase-based MT system plus two additional features coming from the neural MT model. The neural features are based on the generated alignment and the translation probabilities, which correspond to the first and second terms in eqn 1, respectively. We train the reranker using MERT (Och, 2003) with 100 restarts.\nTranslation Metrics. We use BLEU (Papineni et al., 2002) and METEOR9 (Denkowski and Lavie, 2014) to measure the translation quality against the reference. BLEU is purely based on the exact match of n-grams in the generated and reference translation, whereas METEOR takes into account matches based on stem, synonym, and paraphrases as well. This is particularly suitable for our morphology rep-\n7We made use of fast_align in our experiments https: //github.com/clab/fast_align.\n8https://github.com/moses-smt. 9http://www.cs.cmu.edu/~alavie/METEOR/.\nresentation learning methods since they may result in using the translation of paraphrases. We train the paraphrase table of METEOR using the entire initial bilingual corpora based on pivoting (Bannard and Callison-Burch, 2005).\nResults. Table 3 shows the translation and alignment perplexities of the development sets when the models are trained. As seen, the CNNchar model leads to lower word and alignment perplexities in almost all cases. This is interesting, and shows the power of this model in fitting to morphologically complex languages using only their characters. Table 2 presents BLEU and METEOR score results, where the re-ranker is optimised by the METEOR and BLEU when reporting the corresponding score. As seen, re-ranking based on neural models\u2019 scores outperforms the phrase-based baseline. Furthermore, the translation quality of the BILSTMmorph model outperforms others for Romanian and Estonian, whereas the CNNchar model outperforms others for Russian which is consistent with our expectations. We assume that replacing Morfessor with real morphology analyser for each language should improve the performance of morpheme-based models, but leave it for future research. However, the translation quality of the neural models are not significantly different, which may be due to the convoluted contributions of high and low frequency words into BLEU and METEOR. Therefore, we investigate our representation learning models intrinsically in the next section."}, {"heading": "5.2 Intrinsic Evaluation", "text": "We now take a closer look at the embeddings learned by the models, based on how well they capture the\nsemantic and morphological information in the nearest neighbour words. Learning representations for low frequency words is harder than that for highfrequency words, since they cannot capitalise as reliably on their contexts. Therefore, we split the test lexicon into 6 subsets according to their frequency in the training set: [0-4], [5-9], [10-14], [15-19], [20-50], and 50+. Since we set out word frequency threshold to 5 for the training set, all words appearing in the frequency band [0,4] are in fact OOVs for the test set. For each word of the test set, we take its top-20 nearest neighbours from the whole training lexicon (without threshold) using cosine metric.\nSemantic Evaluation. We investigate how well the nearest neighbours are interchangable with a query word in the translation process. So we formalise the notion of semantics of the source words based on their translations in the target language. We use pivoting to define the probability of a candidate word e\u2032 to be the synonym of the query word e, p(e\u2032|e) = \u2211 f p(f |e)p(e\u2032|f), where f is a target language word, and the translation probabilities inside the summation are estimated using a wordbased translation model trained on the entire bilingual corpora (i.e. before splitting into train/dev/test sets). We then take the top-5 most probable words as the gold synonyms for each query word of the test set.10\nWe measure the quality of predicted nearest neighbours using the multi-label accuracy,11 1 |S| \u2211\nw\u2208S 1[G(w)\u2229N(w) 6=\u2205], where G(w) and N(w) are the sets of gold standard synonyms and nearest neighbors for w respectively; the function 1[C] is one if the condition C is true, and zero otherwise. In other words, it is the fraction of words in S whose nearest neighbours and gold standard synonyms have non-empty overlap.\nTable 4 presents the semantic evaluation results. As seen, on words with frequency\u2264 50, the CNNchar model performs best across all of the three languages. Its superiority is particularly interesting for the OOV words (i.e. the frequency band [0,4]) where the model has cooked up the representations com-\n10We remove query words whose frequency is less than a threshold in the initial bilingual corpora, since pivoting may not result in high quality synonyms for such words.\n11We evaluated using mean reciprocal rank (MRR) measure as well, and obtained results consistent with the multi-label accuracy (omitted due to space constraints).\npletely based on the characters. For high frequency words (> 50), the BILSTMword outperforms the other models.\nMorphological Evaluation. We now turn to evaluating the morphological component. For this evaluation, we focus on Russian since it has a notoriously hard morphology. We run another morphological analyser, mystem (Segalovich, 2003), to generate linguistically tagged morphological analyses for a word, e.g. POS tags, case, person, plurality, etc. We represent each morphological analysis with a bit vector showing the presence of these grammatical features. Each word is then assigned a set of bit vectors corresponding to the set of its morphological analyses. As the morphology similarity between two words, we take the minimum of Hamming similarity12 between the corresponding two sets of bit vectors. Table 5(a) shows the average morphology similarity between the words and their nearest neighbours across the frequency bands. Likewise, we represent the words based on their lemma features; Table 5(b) shows the average lemma similarity. We can see that both character-based models capture morphology far better than morpheme-based ones, especially in the cases of OOV words. But it is also clear that CNN tends to outperform bi-LSTM in case where we compare lemmas, and bi-LSTM seems to be better at capturing affixes.\n12The Hamming similarity is the number of bits having the same value in two given bit vectors.\nNow we take a closer look at the character-based models. We manually created a set of non-existing Russian words of three types. Words in the first set consist of known root and affixes, but their combination is atypical, although one might guess the meaning. The second type corresponds to the words with non-existing(nonsense) root, but meaningful affixes, so one might guess its part of speech and some other properties, e.g. gender, plurality, case. Finally, a third type comprises of the words with all known root and morphemes, but the combination is absolutely not possible in the language and the meaning is hard to guess.\nTable 6 shows that CNN is strongly biased towards longest substring matching from the beginning of the word, and it yields better recall in retrieving words sharing same lemma. Bi-LSTM, on the other hand, is mainly focused on matching the patterns from both ends regardless the middle of the word. And it results in higher recall of the words sharing same grammar features."}, {"heading": "6 Conclusion", "text": "This paper proposes a novel translation model incorporating a hard attentional mechanism. In this context, we have compared four different models of morpheme- and character-level word representations for the source language. These models lead to more robust encodings of words in morphological rich languages, and overall better translations than simple word embeddings. Our detailed analyses have shown that word-embeddings are superior for frequent words, whereas convolutional method\nis best for handling rare words. Comparison of the convolutional and recurrent methods over character sequences has shown that the convolutional method better captures the lemma, which is of critical importance for translating out-of-vocabulary words, and would also be key in many other semantic applications."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Bannard", "Chris Callison-Burch"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Bannard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bannard et al\\.", "year": 2005}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Botha", "Blunsom2014] Jan A Botha", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1405.4273", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Character-based neural machine translation. arXiv preprint arXiv:1603.00810", "author": ["Costa-juss\u00e0", "Fonollosa2016] Marta Costa-juss\u00e0", "Jose Fonollosa"], "venue": null, "citeRegEx": "Costa.juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2016}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Creutz", "Lagus2007] Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Citeseer", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "A joint sequence translation model with integrated reordering", "author": ["Helmut Schmid", "Alexander M. Fraser"], "venue": "In The 49th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Durrani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2011}, {"title": "A markov model of machine translation using nonparametric bayesian inference", "author": ["Feng", "Cohn2013] Yang Feng", "Trevor Cohn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015a] Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015b] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan Black"], "venue": "arXiv preprint arXiv:1511:04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong et al.2013] Thang Luong", "Richard Socher", "Christopher D Manning"], "venue": "In CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "Models of end-to-end machine translation based on neural networks have been shown to produce excellent translations, rivalling or surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 193, "endOffset": 272}, {"referenceID": 0, "context": "Models of end-to-end machine translation based on neural networks have been shown to produce excellent translations, rivalling or surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 193, "endOffset": 272}, {"referenceID": 21, "context": "Accordingly sentences containing rare words tend to be translated much more poorly than those containing only common words (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 123, "endOffset": 170}, {"referenceID": 0, "context": "Accordingly sentences containing rare words tend to be translated much more poorly than those containing only common words (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 123, "endOffset": 170}, {"referenceID": 19, "context": "(2016)), with a few notable exceptions (Ling et al., 2015b; Sennrich et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016), these approaches have not been applied to the more challenging problem of translation.", "startOffset": 39, "endOffset": 115}, {"referenceID": 10, "context": ", Botha and Blunsom (2014), Kim et al. (2016)), with a few notable exceptions (Ling et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 7, "context": "Our novel neural MT model, is based on the operation sequence model (OSM; Durrani et al. (2011), ar X iv :1 60 6.", "startOffset": 74, "endOffset": 96}, {"referenceID": 0, "context": "Our OSM can be considered as a form of attentional encoder-decoder Bahdanau et al. (2015) with hard attention in which each decision is contextualised by at most one source word, contrasting with the soft attention in Bahdanau et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "Our OSM can be considered as a form of attentional encoder-decoder Bahdanau et al. (2015) with hard attention in which each decision is contextualised by at most one source word, contrasting with the soft attention in Bahdanau et al. (2015).", "startOffset": 67, "endOffset": 241}, {"referenceID": 15, "context": "Luong et al. (2013) proposed a recursive combination of morphs using affine transformation, however this is unable to differentiate between the compositional and non-compositional cases.", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "Luong et al. (2013) proposed a recursive combination of morphs using affine transformation, however this is unable to differentiate between the compositional and non-compositional cases. Botha and Blunsom (2014) aim to address this problem by forming word representations from adding a sum of each word\u2019s morpheme embeddings to its word embedding.", "startOffset": 0, "endOffset": 212}, {"referenceID": 10, "context": "Several authors have proposed convolutional neural networks over character sequences, as part of models of part of speech tagging (Santos and Zadrozny, 2014), language models (Kim et al., 2015) and machine translation (Costa-juss\u00e0 and Fonollosa, 2016).", "startOffset": 175, "endOffset": 193}, {"referenceID": 1, "context": "Another strand of research has looked at recurrent architectures, using long-short term memory units (Ling et al., 2015a; Ballesteros et al., 2015) which can capture long orthographic patterns in the character sequence, as well as non-compositionality.", "startOffset": 101, "endOffset": 147}, {"referenceID": 7, "context": "The first contribution of this paper is a neural network variant of the Operational Sequence Model (OSM) (Durrani et al., 2011; Feng and Cohn, 2013).", "startOffset": 105, "endOffset": 148}, {"referenceID": 7, "context": "In previous work (Durrani et al., 2011; Feng and Cohn, 2013), the sequence of operations is modelled as Markov chain with a bounded history, where each translation decision is conditioned on a finite history of past decisions.", "startOffset": 17, "endOffset": 60}, {"referenceID": 0, "context": "Note that the neural OSM can be considered as a hard attentional model, as opposed to the soft attentional neural translation model (Bahdanau et al., 2015).", "startOffset": 132, "endOffset": 155}, {"referenceID": 11, "context": "The last word encoder we consider is a convolutional neural network, inspired by a similar approach in language modelling (Kim et al., 2016).", "startOffset": 122, "endOffset": 140}, {"referenceID": 20, "context": "In order to capture interactions between the character n-grams obtained by the filters, a highway network (Srivastava et al., 2015) is applied after the max pooling layer, ew = t MLP(rw) + (1 \u2212 t) rw, where t = MLP\u03c3(rw) is a sigmoid gating function which modulates between a tanh MLP transformation of the input (left component) and preserving the input as is (right component).", "startOffset": 106, "endOffset": 131}, {"referenceID": 12, "context": "We use parallel bilingual data from Europarl for Estonian-English and Romanian-English (Koehn, 2005), and web-crawled parallel data for Russian-English (Antonova and Misyurev, 2011).", "startOffset": 87, "endOffset": 100}, {"referenceID": 16, "context": "We train the reranker using MERT (Och, 2003) with 100 restarts.", "startOffset": 33, "endOffset": 44}, {"referenceID": 17, "context": "We use BLEU (Papineni et al., 2002) and METEOR9 (Denkowski and Lavie, 2014) to measure the translation quality against the reference.", "startOffset": 12, "endOffset": 35}], "year": 2016, "abstractText": "Dealing with the co mplex word forms in morphologically rich languages is an open problem in language processing, and is particularly important in translation. In contrast to most modern neural systems of translation, which discard the identity for rare words, in this paper we propose several architectures for learning word representations from character and morpheme level word decompositions. We incorporate these representations in a novel machine translation model which jointly learns word alignments and translations via a hard attention mechanism. Evaluating on translating from several morphologically rich languages into English, we show consistent improvements over strong baseline methods, of between 1 and 1.5 BLEU points.", "creator": "LaTeX with hyperref package"}}}