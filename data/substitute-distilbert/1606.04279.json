{"id": "1606.04279", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Cross-Lingual Morphological Tagging for Low-Resource Languages", "abstract": "morphologically driven languages often create the annotated description resources required to develop meaningful natural language processing tools. we propose models suitable for composing morphological taggers with rich tagsets for low - resource languages without using direct supervision. 1 approach extends existing approaches of projecting part - of - speech tags across languages, using bitext to infer constraints on the possible tags for a given class type or token. we propose a tagging model using wsabie, a discriminative embedding - based model ; rank - based learning. in our evaluation on 11 languages, on average this model performs on par with constrained persistent weakly - supervised hmm, while being more careful. subsequent experiments show that the method performs best when projecting between related candidate pairs. despite the inherently lossy methods, we show that the morphological tags predicted by our models simulate the downstream persistence of a parser by + 0. 74 ^ on average.", "histories": [["v1", "Tue, 14 Jun 2016 09:43:36 GMT  (90kb,D)", "http://arxiv.org/abs/1606.04279v1", "11 pages. ACL 2016"]], "COMMENTS": "11 pages. ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan buys", "jan a botha"], "accepted": true, "id": "1606.04279"}, "pdf": {"name": "1606.04279.pdf", "metadata": {"source": "CRF", "title": "Cross-Lingual Morphological Tagging for Low-Resource Languages", "authors": ["Jan Buys", "Jan A. Botha"], "emails": ["jan.buys@cs.ox.ac.uk", "jabot@google.com"], "sections": [{"heading": "1 Introduction", "text": "Morphologically rich languages pose significant challenges for Natural Language Processing (NLP) due to data-sparseness caused by large vocabularies. Intermediate processing is often required to address the limitations of only using surface forms, especially for small datasets. Common morphological processing tasks include segmentation (Creutz and Lagus, 2007; Snyder and Barzilay, 2008), paradigm learning (Durrett and DeNero, 2013; Ahlberg et al., 2015) and morphological tagging (Mu\u0308ller and Schuetze, 2015). In this\npaper we focus on the latter. Parts-of-speech (POS) tagging is the most common form of syntactic annotation. However, the granularity of POS varies across languages and annotation-schemas, and tagsets have often been extended to include tags for morphologicallymarked properties such as number, case or degree. To enable cross-lingual learning, a small set of universal (coarse-grained) POS tags have been proposed (Petrov et al., 2012). For morphological processing this can be complemented with a set of attribute-feature values that makes the annotation more fine-grained (Zeman, 2008; Sylak-Glassman et al., 2015b).\nTagging text with morphologically-enriched labels has been shown to benefit downstream tasks such as parsing (Tsarfaty et al., 2010) and semantic role labelling (Hajic\u030c et al., 2009). In generation tasks such as machine translation these tags can help to generate the right form of a word and to model agreement (Toutanova et al., 2008). Morphological information can also benefit automatic speech recognition for low-resource languages (Besacier et al., 2014).\nHowever, annotating sufficient data to learn accurate morphological taggers is expensive and relies on linguistic expertise, and is therefore currently only feasible for the world\u2019s most widelyused languages. In this paper we are interested in learning morphological taggers without the availability of supervised data. A successful paradigm for learning without direct supervision is to make use of word-aligned parallel text, with a resourcerich language on one side and a resource-poor language on the other side (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Ta\u0308ckstro\u0308m et al., 2013).\nIn this paper we extend these methods, that have mostly been proposed for universal POS-taggers, to learn weakly-supervised morphological taggers.\nar X\niv :1\n60 6.\n04 27\n9v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n16\nOur approach is based on projecting token and type constraints across parallel text, learning a tagger in a weakly-supervised manner from the projected constraints (Ta\u0308ckstro\u0308m et al., 2013). We propose an embedding-based model trained with the Wsabie algorithm (Weston et al., 2011), and compare this approach against a baseline HMM model.\nWe evaluate the projected tags for a set of languages for which morphological tags are available in the Universal Dependency corpora. To show the feasibility of our approach, and to compare the performance of different models, we use English as source language. Then we perform an evaluation on all language pairs in the set of target languages which shows that the best performance is obtained when projecting between genealogically related languages.\nAs an extrinsic evaluation of our approach, we show that NLP models can benefit from using these induced tags even if they are not as accurate as tags produced by supervised models, by evaluating the effect of features obtained from tags predicted by the induced morphological taggers in dependency parsing."}, {"heading": "2 Universal Morphological Tags", "text": "In order to do cross-lingual learning we require a common morphological tagset. To evaluate these models we require datasets in multiple languages which have been annotated with such a consistent schema. The treebanks annotated in the Universal Dependencies (UD) project (de Marneffe et al., 2014) are suitable for this purpose.\nAll the data is annotated with universal POS tags, a set of 17 tags1. We use UD v1.2 (Nivre et al., 2015), which contain 25 languages annotated with morphological attributes (called features). In addition to POS, there are 17 universal attributes, which each takes one of a set of values when annotated. The morphological tag of a token denotes the union of its morphological attributevalue pairs, including its POS.\nAlthough the schema is consistent across languages, there are language-specific phenomena and considerations that result in some mismatches for a given pair of languages. One source of this is that the UD treebanks were mostly constructed by fully or semi-automatic conversion of exist-\n1This extends, but is not fully consistent with, the set of 12 tags proposed by Petrov et al. (2012).\ning treebanks which had used different annotation schemes. Furthermore, not all the attributes and values appear in all languages (e.g. additional cases in morphologically-rich languages such as Finnish), and there are still a number of languagespecific tags not in the universal schema. Finally, in some instances properties that are not realised in the surface word form are absent from the annotation (e.g. in English the person and number of verbs are only annotated for third-person singular, as there are no distinct morphological forms for their other values).\nAn example of the morphological annotation employed is given in Figure 1. Note that the annotations for aligned word-pairs are not fully consistent. Some attributes appear only in the English treebank (e.g. Voice), while others appear only in the Dutch treebank (e.g. Aspect, Subcat)."}, {"heading": "3 Tag Projection across Bitext", "text": "Our approach to train morphological taggers is based on the paradigm of projecting token and type constraints as proposed by Ta\u0308ckstro\u0308m et al. (2013). The training data consist of parallel text with the resource-rich language on the source-side and the low-resource language on the target side. The source-side text is tagged with a supervised morphological tagger. For every target-side sentence, the type and token constraints are used to construct a set of permitted tags for each token in the sentence. These constraints will then be used to train morphological taggers."}, {"heading": "3.1 Type and token constraints", "text": "To extract constraints from the parallel text, we first obtain bidirectional word alignments. To ensure high quality alignments, alignment pairs with a confidence below a fixed threshold \u03b1 are removed. The motivation for using only highconfidence alignments is that incorrect alignments will hurt the performance of the model, while it is easier to use more parallel text to obtain a sufficient number of alignments for training.\nThe first class of constraints that we extract from the parallel text is type constraints. For each word type, we construct a distribution over tags for the word by accumulating counts of the morphological tags of source-side tokens that are aligned to instances of the word type. The set of tags with probability above some threshold \u03b2 is taken as the tag dictionary entry for that word type. To\nconstruct the training examples, each token whose type occurs in the tag dictionary is restricted to the set of tags in the dictionary entry. For tokens for which the dictionary entry is empty, all the tags are included in the set of permitted tags (this happens when the tag distribution is too flat and all the probabilities are below the threshold). In principle, type constraints can also be obtained from an external dictionary, but in this paper we assume we do not have such a resource.\nThe second class of constraints places restrictions on word tokens. Every target token is constrained to the tag of its aligned source token, while unaligned tokens can take any tag.\nToken constraints are combined with type constraints as proposed by Ta\u0308ckstro\u0308m et al. (2013): If a token is unaligned, its type constraints are used. If the token is aligned, and there is no dictionary entry for the token type, the token constraint is used. If there is a dictionary entry for the token type, and the token constraint tag is in the dictionary, the token constraint is used. If the token constraint tag is not in the dictionary entry, the type constraints are used."}, {"heading": "4 Learning from Projected Tags", "text": "Next we propose models to learn a morphological tagger from cross-lingually projected constraints."}, {"heading": "4.1 Related work", "text": "HMMs have previously been used for weaklysupervised learning from token or type con-\nstraints (Das and Petrov, 2011; Li et al., 2012; Ta\u0308ckstro\u0308m et al., 2013). HMMs are generative models, and in this setting the words in the target sentence form the observed sequence and the morphological tags the hidden sequence. The projected constraints are used as partially observed training data for the hidden sequence.\nTa\u0308ckstro\u0308m et al. (2013) proposed a discriminative CRF model that relies on incorporating two sets of constraints, of which one is a subset of the other. Ganchev and Das (2013) used a similar CRF model, but instead of using the projected tags as hard constraints, they were employed as soft constraints with posterior regularization.\nThe model of Wisniewski et al. (2014) makes greedy predictions with a history-based model, that includes previously predicted tags in the sequence, during training and testing. The model is trained with a variant of the perceptron algorithm that allows a set of positive labels. When an incorrect prediction is made during training, the parameters are updated in the direction of all the positive labels."}, {"heading": "4.2 HMM model", "text": "As a baseline model we use an HMM where the transition and emission distributions are parameterized by log-linear models (a feature-HMM). Training is performed with L-BFGS rather than with the EM algorithm. This parameterization was proposed by Berg-Kirkpatrick et al. (2010) and applied to cross-lingual POS induction by Das and\nPetrov (2011) and Ta\u0308ckstro\u0308m et al. (2013). Let w be the target sentence and t the sequence of tags for the sentence. The marginal probability of a sequence during training is\np(w1:n) = \u2211\nt1:n\u2208T n\u220f i=1 p(ti|ti\u22121)p(wi|ti),\nwhere T is the set of tag sequences allowed by the type and token constraints. The probability of all other tag sequences are assumed to be 0.\nThe features in our model are similar to those used by Ta\u0308ckstro\u0308m et al. (2013), including features based on word and tag identity, suffixes up to length 3, punctuation and word clusters. Word clusters are obtained by clustering frequent words into 256 clusters with the Exchange algorithm (Uszkoreit and Brants, 2008), using the data and methodology detailed in Ta\u0308ckstro\u0308m et al. (2012)."}, {"heading": "4.3 Wsabie model", "text": "We propose a discriminative model based on Wsabie (Weston et al., 2011), a shallow neural network that learns to optimize precision at the top of a ranked list of labels. In our application, the goal is to learn to rank the set of tags allowed by the projected constraints in the training data above all other tags. In contrast to the HMM, which performs inference over the entire sequence, Wsabie makes the predictions at each token independently, based on a large context-size. Therefore, Wsabie inference is linear in the number of tags, while for an HMM it is quadratic, making the Wsabie model much faster during training and decoding.\nWsabie maps the input features and output labels into a low-dimensional joint space. The input vector x for a wordw consists of the concatenation of word embeddings and sparse features extracted from w and the surrounding context. A mapping\n\u0398I(x) = V x\nmaps x \u2208 Rd into RD, with matrix V \u2208 RD\u00d7d of parameters. The output tag t is mapped into the same space by\n\u0398O(t) = Wt,\nwhere W \u2208 RD\u00d7L is a matrix of output tag embeddings andWt selects the column embedding of tag t. The model score for tag t given input token with feature vector x is the dot product\nft(x) = \u0398O(t) T\u0398I(x),\nwhere the tags are ranked by the magnitude of ft(x). The norms of the columns of V and W are constrained, which acts as a regularizer.\nThe loss function is a margin-based hinge loss based on the rank of a tag given by ft(x). The rank is estimated by sampling an incorrect tag uniformly with replacement until the sampled tag violates the margin with a correct tag. Training is performed with stochastic gradient descent by performing a gradient step against the violating tag.\nThe word embedding features for the Wsabie models consist of 64-dimensional word vectors of the 5 words on either side of a token and of the token itself. The embeddings are trained with word2vec (Mikolov et al., 2013) on large corpora of newswire text.\nSparse features are based on prefixes and suffixes up to length 3 as well as word cluster features for a window size 3 around the token, using the clusters described in the previous section."}, {"heading": "5 Experiments", "text": "We evaluate our model in two settings. The first evaluation measures the accuracy of the crosslingual taggers on language pairs where annotated data is available for both languages. The annotated target language data is used only during evaluation and not for training. Second, we perform a downstream evaluation by including the morphological attributes predicted by the tagger as features in a dependency parser to guage the effectiveness of our approach in a setting where one does not have access to gold morphological annotations."}, {"heading": "5.1 Experimental setup", "text": "As source of parallel training data we use Europarl2 (Koehn, 2005) version 7. Sentences are tokenized but not lower-cased, and sentences longer than 80 words are excluded. In our experiments we learn taggers for a set of 11 European languages that have both UD training data with morphological features, and parallel data in Europarl: Bulgarian, Czech, Danish, Dutch, Finnish, Italian, Polish, Portuguese, Slovene, Spanish and Swedish. We train cross-lingual models in two setups: The first uses English as source language; in the second we train models with different source languages for each target language.\nWord alignments over the parallel data are obtained using FastAlign (Dyer et al., 2013). High-\n2http://www.statmt.org/europarl/\nconfidence bidirectional word alignments are constructed by intersecting the alignments in the two directions and including alignment points only if the posterior probabilities in both directions are above the alignment threshold \u03b1. For each language pair all the word-aligned parallel data available (between 10 and 50 million target-side tokens per language) are used to extract the type constraints, and the models are trained on a subset of 2 million target-side tokens (optionally with their token constraints).\nThe number of distinct attribute-value pairs appearing in the tagsets depends on the language pair and ranges between 35 and 79, with 54 on average (including POS tags). The number of distinct composite morphological tags is 423 on average, with a much larger range, between 81 and 1483. The English UD data has 116 tags composed out of 51 distinct attribute-value pairs. Therefore, we can project a reasonable number of morpho-syntactic attributes from English, although the number of attribute combinations that occur in the data is less than for morphologically richer languages.\nThe source text is tagged with supervised taggers, trained with Wsabie on the UD training data for each of the source languages used. For each language pair, we train a distinct source-side model covering only the attribute types appearing in both languages. This is meant to obtain a maximally accurate source-side tagger, while accepting that our approach cannot predict target-side attributes that are absent from the source language. The average accuracy of the English taggers on the UD test data is 94.96%. The source-side taggers over all the language pairs we experiment on have an average accuracy of 95.75%, with a minimum of 89.14% and a maximum of 98.59%."}, {"heading": "5.2 Tuning", "text": "The hyperparameters of the Wsabie taggers are tuned on the English development set, and the same parameters are used for the Wsabie targetside models trained on the projected tags. The optimal setting is a learning rate of 0.01, embedding dimension size D = 50, margin 0.1, and 25 training iterations.\nHyperparameters for the projection models are set by tuning on the UD dev set accuracy for English to Danish. English was chosen as it is the language with the most available data and the most\nlikely to be used when projecting to other languages; Danish simply because its corpus size is typical of the larger languages in Europarl. Using a small grid search, we choose the parameters that give the best average accuracy across all four projection model instances we consider. This allows using the same hyperparameters for all these models, an important factor in making them comparable in the evaluation, since the hyperparameters determine the effective training data. The parameters tuned in this manner are the alignment threshold \u03b1, which is set to 0.8, and the type distribution threshold \u03b2, set to 0.3."}, {"heading": "5.3 Tagging evaluation setup", "text": "In order to evaluate the induced taggers on the annotated UD data for the target languages, we define two settings that circumvent mismatches between source and target language annotations to different degrees.\nThe STANDARD setting involves first making minor corrections to certain predicted POS values to account for inconsistencies in the original annotated data. When predicted by the model, the POS tag values absent from the target language training corpus are deterministically mapped to the mostrelated value present in the target language in the following way: PROPN to NOUN; SYM and INTJ to X; SYM and X to PUNCT. Besides POS, the evaluation considers only those attribute types that appear in both languages\u2019 training corpora, i.e., the set of attributes for which the model was trained. Note that this leaves cases intact where the model predicts certain attribute values that appear only in one of the two languages; it is thus penalised for making mistakes on values that it cannot learn under our projection approach.\nThe second evaluation setting, INTERSECTED, relaxes the latter aspect: it only considers attribute-value pairs appearing in the training corpora of both languages. The motivation for this is to get a better measurement of the accuracy of our method, assuming that the tagsets are consistent.\nIn both settings we report macro-averaged F1 scores over all the considered attribute types. Results for Wsabie are averaged over 3 random restarts because it uses stochastic optimization during training."}, {"heading": "5.4 Tagging results projecting from English", "text": "Following previous work on projecting POS tags and the assumption that it is easier to obtain paral-\nlel data between a low-resource language and English than with another language, we start by training cross-lingual taggers using English as source language.\nThe overall tagging results are given in Table 1. In addition to evaluating the morphological tags in the two settings described above, we also report accuracies for POS tags only, projected jointly with the morphological attributes.\nWe find that for both the HMM and Wsabie models the performance with type and token constraints is worse than when only using type constraints. Ta\u0308ckstro\u0308m et al. (2013) similarly found that for HMMs for POS projection, models with joint constraints do not perform better than those using only type constraints. They postulated that this is due to the type dictionaries having the same biases as token projections, and therefore the model with joint constraints not being able to filter out systematic errors in the projections.\nFor both sets of constraints the performance of the Wsabie model is close to that of the corresponding HMM, despite the Wsabie model having a linear runtime against the quadratic runtime of the HMM.\nAs another baseline we train a Wsabie model on unambiguous type constraints, i.e., we only extract training examples for words which only have a single tag in the tag dictionary. Including ambiguous type constraints gives an average improvement of 2.2%.\nAs a target ceiling on performance we train a Wsabie model with supervised type constraints. This model uses type constraints based on an oracle morphological tag dictionary extracted from the gold training data of the target language. It is trained on the same training data as the projected models (without token constraints). The\nmodel scores higher on STANDARD than on INTERSECTED, as it has access to annotations for the full set of tags used in the target language, not just the restricted set that can be projected. This oracle performs on average 17% better than the projected type constraints model on INTERSECTED. Therefore, despite the promising results of our approach, there is still a considerable amount of noise in the type constraints extracted from the aligned data.\nWe also compare the performance of the model to that of a supervised model trained on a small annotated corpus. Average performance when training on 1000 annotated tokens is only a few points higher than that of the best projected model for INTERSECTED. Given that is it expensive to let annotators learn to annotate a large set of attributes, even for a small corpus, it shows that our model can bring considerable benefits in practice to the development of NLP models for low-resource languages. It is possible to obtain further improvements in performance by learning jointly from a small annotated dataset and parallel data (Duong et al., 2014), but we leave that for future work.\nThe results when evaluating only the POS tags follow the same pattern, except that the overall level of accuracy is much higher than when considering all morphological attributes. For POS, the models with projected constraints actually perform better than those with supervised type constraints. In this case the benefits from learning constraints from a larger set of word types seem to outweigh the noise in the projections. The projected models are also more accurate than the supervised model trained on 1000 tokens."}, {"heading": "5.5 Multilingual tagging results", "text": "Results for cross-lingual experiments on all pairs of the target languages under consideration are\ngiven in Table 2, using the STANDARD evaluation setup. We make use of Wsabie for these experiments, as it is a more efficient model, which is especially significant when training models with large tagsets.\nWe see that there is large variance in the morphological tagging accuracies across language pairs. In most cases the source language for which we learn the most accurate model for morphological tagging on the target language is a related language. The Romance languages we consider (Spanish, Italian and Portuguese) seem to transfer particularly well across each other. Swedish and Danish also transfer well to each other, while English transfers best to Dutch, which the former is most closely related to among the languages compared here. However, there are also some cases of unrelated source languages performing best: Using Danish as source language gives the highest performing models for both Bulgarian and Czech. When comparing these results, however, one should keep in mind that the attribute type sets used to train taggers from different source languages for the same target language is not always the same (due to our definition of the STANDARD evaluation), therefore these results should not be interpreted directly as indicating which source language gives the best target language performance on a particular tagset.\nWe compare the results of the STANDARD and INTERSECTED evaluations, both when using English as source language, and when using the source language which gives the highest accuracy on STANDARD for each target language (Table 3). We see that the gap in performance between the two evaluations tends to be larger when project-\ning from English than when projecting from the source language which performs best for each target language.\nOne of the main causes of variation in performance is annotation differences. Languages that are morphologically rich tend to have lower performance, but we also see variation between similar languages: There is a 10% performance gap between Danish and Swedish when projecting from English, even though they are closely related.\nWe also investigate the effect of the choice of source language on the accuracy of the projected POS tags (Table 4). Again, we compare the performance with English as source (which is standard for previous work on POS projection) to that of the best source language for each target. Although the gap in performance is smaller than for\nthe full evaluation, we see that for most target languages we can still do better by projecting from a language other than English.\nDetailed per attribute results for the STANDARD evaluation are given in Table 5, again comparing the results of projecting from English to that of the most accurate model for each target language. We see that there are large differences in accuracy across attributes and across languages. In some cases, the transfer is unsuccessful. For example, degree accuracy in Italian is 2% F1 when projecting from English and 14% F1 projecting from Portuguese. Some of the cases can be explained by differences in where an attribute is marked: For example, for definiteness the performance is 1% from English to Bulgarian, as Bulgarian marks definiteness on nouns and adjectives rather than on determiners. Other attributes are very languagedependent. Gender transfers well between Romance languages, but poorly when transferring from English."}, {"heading": "5.6 Parsing evaluation", "text": "To evaluate the effect of our models on a downstream task, we apply the cross-lingual taggers induced using English as source language to dependency parsing. This is applicable to a scenario where a language might have a corpus annotated with dependency trees and universal POS, but not morphological attributes. We want to determine how much of the performance gain from features based on supervised morphological tags we can recover with the tags predicted by our model.\nAs baseline we use a reimplementation of\nZhang and Nivre (2011), an arc-eager transitionbased dependency parser with a rich feature-set, with beam-size 8, trained for 10 epochs with a structured perceptron. We assume that universal POS tags are available, using a supervised SVM POS tagger for training and evaluation.\nTo include the morphology, we add features based on the predicted tags of the word on top of the stack and the first two words on the buffer.\nParsing results are given in Table 6. We report labelled attachment scores (LAS) for the baseline with no morphological tags, the model with features predicted by Wsabie with projected type constraints, and the model with features predicted by the supervised morphological tagger.\nWe obtain improvements in parsing accuracies for all languages except Bulgarian when adding the induced morphological tags. Using the projected tags as features recovers 24.67% (0.6 LAS absolute) of the average gain that supervised morphology features delivers over the baseline parser. The parser with features from the supervised tagger trained on 1000 tokens obtains 73.63 LAS on average. This improvement of +0.15 LAS over the baseline versus the +0.6 of our method shows that the tags predicted by our projected models are more useful as features than those predicted by a small supervised model.\nTo investigate the effect of source language choice for the projected models in this evaluation, we trained a model for Swedish using Danish as source language. The parsing performance is insignificantly different from using English as source, despite the accuracy of the tags projected\nfrom Danish being higher. Faruqui et al. (2016) show that features from induced morpho-syntactic lexicons can also improve dependency parsing accuracy. However, their method relies on having a seed lexicon of 1000 annotated word types, while our method does not require any morphological annotations in the target language."}, {"heading": "6 Future Work", "text": "A big challenge in cross-lingual morphology is that of relatedness between source and target languages. Although we evaluate our models on multiple source-target language pairs, more work is required to investigate strategies for choosing which source language to use for a low-resource target language. A related direction is to constructing models from multiple source languages, as our results show that the overall best-performing source language for a given target language may not always have the best performance on all attributes.\nAnother direction is to make use of dictionaries such as Wiktionary to obtain type constraints, similar to previous work on weakly-supervised POS tagging (Li et al., 2012; Ta\u0308ckstro\u0308m et al., 2013). Sylak-Glassman et al. (2015b) and SylakGlassman et al. (2015a) proposed a morphological schema and method to extract annotations in that schema from Wiktionary. Although different from the schema used in this paper, their method can be used to extract type dictionaries for morphological tags that can be used to complement constraints extracted from parallel data.\nFinally, greater use can be made of syntactic in-\nformation: There is a close relation between the syntactic structure expressed in dependency parses and inflections in morphologically rich languages; by including this syntactic structure in our models we can induce morphological tags, e.g. related to case, that is also expressed in dependency parses."}, {"heading": "7 Conclusion", "text": "In this paper we proposed a method that can successfully induce morphological taggers for resource-scarce languages using tags projected across bitext. It relies on access to a morphological tagger for a source-language and a moderate amount of bitext. The method obtains strong performance on a range of language pairs. We showed that downstream tasks such as dependency parsing can be improved by using the predictions from the tagger as features. Our results provide a strong baseline for future work in weaklysupervised morphological tagging."}, {"heading": "Acknowledgments", "text": "This research was primarily performed while the first author was an intern at Google Inc. We thank Oscar Ta\u0308ckstro\u0308m, Kuzman Ganchev, Bernd Bohnet and Ryan McDonald for valuable assistance and discussions about this work."}], "references": [{"title": "Paradigm classification in supervised learning of morphology", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden."], "venue": "Proceedings of NAACL, pages 1024\u20131029.", "citeRegEx": "Ahlberg et al\\.,? 2015", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2015}, {"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Proceedings of NAACL, pages 582\u2013590.", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["Lauent Besacier", "Ettiene Barnard", "Alexey Karpov", "Tanja Schultz."], "venue": "Speech Communication, 56:85\u2013100.", "citeRegEx": "Besacier et al\\.,? 2014", "shortCiteRegEx": "Besacier et al\\.", "year": 2014}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Transactions on Speech and Language Processing, 4(1).", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of ACL, pages 600\u2013609.", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Universal dependencies: A cross-linguistic typology", "author": ["Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning."], "venue": "Proceedings of LREC.", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "What can we get from 1000 tokens? A case study of multilingual pos tagging for resource-poor languages", "author": ["Long Duong", "Trevor Cohn", "Karin Verspoor", "Steven Bird", "Paul Cook."], "venue": "Proceedings of EMNLP, pages 886\u2013897.", "citeRegEx": "Duong et al\\.,? 2014", "shortCiteRegEx": "Duong et al\\.", "year": 2014}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "Proceedings of NAACL, pages 1185\u20131195.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "A simple, fast, and effective reparameterization of IBM Model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith."], "venue": "Proceeding of NAACL, pages 682\u2013686.", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Morpho-syntactic lexicon generation using graph-based semi-supervised learning", "author": ["Manaal Faruqui", "Ryan McDonald", "Radu Soricut."], "venue": "Transactions of the Association for Computational Linguistics, 4:1\u201316.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora", "author": ["Victoria Fossum", "Steven Abney."], "venue": "Proceedings of IJCNLP, pages 862\u2013873.", "citeRegEx": "Fossum and Abney.,? 2005", "shortCiteRegEx": "Fossum and Abney.", "year": 2005}, {"title": "Crosslingual discriminative learning of sequence models with posterior regularization", "author": ["Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of EMNLP, pages 1996\u20132006.", "citeRegEx": "Ganchev and Das.,? 2013", "shortCiteRegEx": "Ganchev and Das.", "year": 2013}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit, volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Wiki-ly supervised part-of-speech tagging", "author": ["Shen Li", "Jo\u00e3o Gra\u00e7a", "Ben Taskar."], "venue": "Proceedings of EMNLP-CoNLL, pages 1389\u20131398, July.", "citeRegEx": "Li et al\\.,? 2012", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Robust morphological tagging with word representations", "author": ["Thomas M\u00fcller", "Hinrich Schuetze."], "venue": "Proceedings of NAACL, pages 526\u2013536, Denver, Colorado, May\u2013June.", "citeRegEx": "M\u00fcller and Schuetze.,? 2015", "shortCiteRegEx": "M\u00fcller and Schuetze.", "year": 2015}, {"title": "Universal dependencies 1.2. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Zhu"], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proceedings of LREC.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Benjamin Snyder", "Regina Barzilay."], "venue": "Proceedings of ACL, pages 737\u2013 745.", "citeRegEx": "Snyder and Barzilay.,? 2008", "shortCiteRegEx": "Snyder and Barzilay.", "year": 2008}, {"title": "A universal feature schema for rich morphological annotation and fine-grained cross-lingual part-of-speech tagging", "author": ["John Sylak-Glassman", "Christo Kirov", "Matt Post", "Roger Que", "David Yarowsky."], "venue": "In", "citeRegEx": "Sylak.Glassman et al\\.,? 2015a", "shortCiteRegEx": "Sylak.Glassman et al\\.", "year": 2015}, {"title": "A language-independent feature schema for inflectional morphology", "author": ["John Sylak-Glassman", "Christo Kirov", "David Yarowsky", "Roger Que."], "venue": "Proceedings of ACL-IJCNLP (short papers), pages 674\u2013 680.", "citeRegEx": "Sylak.Glassman et al\\.,? 2015b", "shortCiteRegEx": "Sylak.Glassman et al\\.", "year": 2015}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Jakob Uszkoreit."], "venue": "Proceedings of NAACL, pages 477\u2013487.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2012", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre."], "venue": "Transactions of the Association for Computational Linguistics, 1:1\u201312.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2013", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Applying morphology generation models to machine translation", "author": ["Kristina Toutanova", "Hisami Suzuki", "Achim Ruopp."], "venue": "Proceedings of ACL-HLT, pages 558\u2013566.", "citeRegEx": "Toutanova et al\\.,? 2008", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Statistical parsing of morphologically rich languages (SPMRL): what, how and whither", "author": ["Reut Tsarfaty", "Djam\u00e9 Seddah", "Yoav Goldberg", "Sandra K\u00fcbler", "Marie Candito", "Jennifer Foster", "Yannick Versley", "Ines Rehbein", "Lamia Tounsi."], "venue": "In", "citeRegEx": "Tsarfaty et al\\.,? 2010", "shortCiteRegEx": "Tsarfaty et al\\.", "year": 2010}, {"title": "Distributed word clustering for large scale class-based language modeling in machine translation", "author": ["Jakob Uszkoreit", "Thorsten Brants."], "venue": "Proceedings of ACL-HLT, pages 755\u2013762.", "citeRegEx": "Uszkoreit and Brants.,? 2008", "shortCiteRegEx": "Uszkoreit and Brants.", "year": 2008}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Weston et al\\.,? 2011", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Crosslingual part-of-speech tagging through ambiguous learning", "author": ["Guillaume Wisniewski", "Nicolas Pcheux", "Souhir Gahbiche-Braham", "Franois Yvon."], "venue": "Proceedings of EMNLP, pages 1779\u2013 1785.", "citeRegEx": "Wisniewski et al\\.,? 2014", "shortCiteRegEx": "Wisniewski et al\\.", "year": 2014}, {"title": "Incuding multilingual text analysis tools via robust projection across aligned corpora", "author": ["David Yarowsky", "Grace Ngai", "Richard Wicentowski."], "venue": "Proceedings of HLT.", "citeRegEx": "Yarowsky et al\\.,? 2001", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Reusable tagset conversion using tagset drivers", "author": ["Daniel Zeman."], "venue": "Proceedings of LREC.", "citeRegEx": "Zeman.,? 2008", "shortCiteRegEx": "Zeman.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of ACL-HLT, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Common morphological processing tasks include segmentation (Creutz and Lagus, 2007; Snyder and Barzilay, 2008), paradigm learning (Durrett and DeN-", "startOffset": 59, "endOffset": 110}, {"referenceID": 18, "context": "Common morphological processing tasks include segmentation (Creutz and Lagus, 2007; Snyder and Barzilay, 2008), paradigm learning (Durrett and DeN-", "startOffset": 59, "endOffset": 110}, {"referenceID": 15, "context": ", 2015) and morphological tagging (M\u00fcller and Schuetze, 2015).", "startOffset": 34, "endOffset": 61}, {"referenceID": 17, "context": "To enable cross-lingual learning, a small set of universal (coarse-grained) POS tags have been proposed (Petrov et al., 2012).", "startOffset": 104, "endOffset": 125}, {"referenceID": 29, "context": "attribute-feature values that makes the annotation more fine-grained (Zeman, 2008; Sylak-Glassman et al., 2015b).", "startOffset": 69, "endOffset": 112}, {"referenceID": 20, "context": "attribute-feature values that makes the annotation more fine-grained (Zeman, 2008; Sylak-Glassman et al., 2015b).", "startOffset": 69, "endOffset": 112}, {"referenceID": 24, "context": "Tagging text with morphologically-enriched labels has been shown to benefit downstream tasks such as parsing (Tsarfaty et al., 2010) and seman-", "startOffset": 109, "endOffset": 132}, {"referenceID": 23, "context": "In generation tasks such as machine translation these tags can help to generate the right form of a word and to model agreement (Toutanova et al., 2008).", "startOffset": 128, "endOffset": 152}, {"referenceID": 2, "context": "Morphological information can also benefit automatic speech recognition for low-resource languages (Besacier et al., 2014).", "startOffset": 99, "endOffset": 122}, {"referenceID": 28, "context": "A successful paradigm for learning without direct supervision is to make use of word-aligned parallel text, with a resourcerich language on one side and a resource-poor language on the other side (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 196, "endOffset": 289}, {"referenceID": 10, "context": "A successful paradigm for learning without direct supervision is to make use of word-aligned parallel text, with a resourcerich language on one side and a resource-poor language on the other side (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 196, "endOffset": 289}, {"referenceID": 4, "context": "A successful paradigm for learning without direct supervision is to make use of word-aligned parallel text, with a resourcerich language on one side and a resource-poor language on the other side (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 196, "endOffset": 289}, {"referenceID": 22, "context": "A successful paradigm for learning without direct supervision is to make use of word-aligned parallel text, with a resourcerich language on one side and a resource-poor language on the other side (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 196, "endOffset": 289}, {"referenceID": 22, "context": "Our approach is based on projecting token and type constraints across parallel text, learning a tagger in a weakly-supervised manner from the projected constraints (T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 164, "endOffset": 188}, {"referenceID": 26, "context": "We propose an embedding-based model trained with the Wsabie algorithm (Weston et al., 2011), and compare this approach against a baseline HMM model.", "startOffset": 70, "endOffset": 91}, {"referenceID": 17, "context": "This extends, but is not fully consistent with, the set of 12 tags proposed by Petrov et al. (2012). ing treebanks which had used different annotation schemes.", "startOffset": 79, "endOffset": 100}, {"referenceID": 21, "context": "type constraints as proposed by T\u00e4ckstr\u00f6m et al. (2013). The training data consist of parallel text with the resource-rich language on the source-side and the low-resource language on the target side.", "startOffset": 32, "endOffset": 56}, {"referenceID": 21, "context": "Token constraints are combined with type constraints as proposed by T\u00e4ckstr\u00f6m et al. (2013): If a token is unaligned, its type constraints are used.", "startOffset": 68, "endOffset": 92}, {"referenceID": 4, "context": "HMMs have previously been used for weaklysupervised learning from token or type constraints (Das and Petrov, 2011; Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 92, "endOffset": 155}, {"referenceID": 13, "context": "HMMs have previously been used for weaklysupervised learning from token or type constraints (Das and Petrov, 2011; Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 92, "endOffset": 155}, {"referenceID": 22, "context": "HMMs have previously been used for weaklysupervised learning from token or type constraints (Das and Petrov, 2011; Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 92, "endOffset": 155}, {"referenceID": 11, "context": "Ganchev and Das (2013) used a similar CRF model, but instead of using the projected tags as hard constraints, they were employed as soft constraints with posterior regularization.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "The model of Wisniewski et al. (2014) makes greedy predictions with a history-based model, that includes previously predicted tags in the sequence, during training and testing.", "startOffset": 13, "endOffset": 38}, {"referenceID": 1, "context": "This parameterization was proposed by Berg-Kirkpatrick et al. (2010) and applied to cross-lingual POS induction by Das and", "startOffset": 38, "endOffset": 69}, {"referenceID": 21, "context": "Petrov (2011) and T\u00e4ckstr\u00f6m et al. (2013). Let w be the target sentence and t the sequence of tags for the sentence.", "startOffset": 18, "endOffset": 42}, {"referenceID": 21, "context": "The features in our model are similar to those used by T\u00e4ckstr\u00f6m et al. (2013), including features based on word and tag identity, suffixes up to length 3, punctuation and word clusters.", "startOffset": 55, "endOffset": 79}, {"referenceID": 25, "context": "words into 256 clusters with the Exchange algorithm (Uszkoreit and Brants, 2008), using the data and methodology detailed in T\u00e4ckstr\u00f6m et al.", "startOffset": 52, "endOffset": 80}, {"referenceID": 21, "context": "words into 256 clusters with the Exchange algorithm (Uszkoreit and Brants, 2008), using the data and methodology detailed in T\u00e4ckstr\u00f6m et al. (2012).", "startOffset": 125, "endOffset": 149}, {"referenceID": 26, "context": "We propose a discriminative model based on Wsabie (Weston et al., 2011), a shallow neural network that learns to optimize precision at the top of a ranked list of labels.", "startOffset": 50, "endOffset": 71}, {"referenceID": 14, "context": "The embeddings are trained with word2vec (Mikolov et al., 2013) on large corpora of newswire text.", "startOffset": 41, "endOffset": 63}, {"referenceID": 12, "context": "As source of parallel training data we use Europarl2 (Koehn, 2005) version 7.", "startOffset": 53, "endOffset": 66}, {"referenceID": 8, "context": "Word alignments over the parallel data are obtained using FastAlign (Dyer et al., 2013).", "startOffset": 68, "endOffset": 87}, {"referenceID": 21, "context": "T\u00e4ckstr\u00f6m et al. (2013) similarly found that for HMMs for POS projection, models with joint constraints do not perform better than those using only type constraints.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "It is possible to obtain further improvements in performance by learning jointly from a small annotated dataset and parallel data (Duong et al., 2014), but we leave that for future work.", "startOffset": 130, "endOffset": 150}, {"referenceID": 13, "context": "Another direction is to make use of dictionaries such as Wiktionary to obtain type constraints, similar to previous work on weakly-supervised POS tagging (Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 154, "endOffset": 195}, {"referenceID": 22, "context": "Another direction is to make use of dictionaries such as Wiktionary to obtain type constraints, similar to previous work on weakly-supervised POS tagging (Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 154, "endOffset": 195}, {"referenceID": 13, "context": "Another direction is to make use of dictionaries such as Wiktionary to obtain type constraints, similar to previous work on weakly-supervised POS tagging (Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013). Sylak-Glassman et al. (2015b) and SylakGlassman et al.", "startOffset": 155, "endOffset": 227}, {"referenceID": 13, "context": "Another direction is to make use of dictionaries such as Wiktionary to obtain type constraints, similar to previous work on weakly-supervised POS tagging (Li et al., 2012; T\u00e4ckstr\u00f6m et al., 2013). Sylak-Glassman et al. (2015b) and SylakGlassman et al. (2015a) proposed a morphological schema and method to extract annotations in that schema from Wiktionary.", "startOffset": 155, "endOffset": 260}], "year": 2016, "abstractText": "Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embeddingbased model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average.", "creator": "TeX"}}}