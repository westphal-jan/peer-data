{"id": "1606.04351", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "abstract": "this effort encourages the participation of marketing team \" twise \" in the semeval 2016 challenge. specifically, we participated in task 4, namely \" sentiment analysis for twitter \" for which we implemented sentiment classification tool for subtasks a, b, r and d. our approach consists of two steps. in with first step, we generate and collect semantic feature collections for twitter sentiment evaluation, accomplished by the work of participants of previous editions discussing such challenges. in the second step, we cooperate on the optimization of the evaluation measures among the different subtasks. to this end, investigators examine different learning strategies by validating them on the data provided by the task organisers. for our collaborative success we used an ensemble learning approach ( stacked generalization ) regarding subtask a and single linear models for the rest of theirs subtasks. in the official leaderboard we were ranked 9 / 35, 8 / 19, 1 / 11 and 2010 / 14 for subtasks a, dd, c and d.. \\ footnote { we make the code available for research purposes at \\ url {", "histories": [["v1", "Tue, 14 Jun 2016 13:36:00 GMT  (24kb,D)", "http://arxiv.org/abs/1606.04351v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["georgios balikas", "massih-reza amini"], "accepted": false, "id": "1606.04351"}, "pdf": {"name": "1606.04351.pdf", "metadata": {"source": "CRF", "title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "authors": ["Georgios Balikas", "Massih-Reza Amini"], "emails": ["firstnane.lastname@imag.fr"], "sections": [{"heading": "1 Introduction", "text": "During the last decade, short-text communication forms, such as Twitter microblogging, have been widely adopted and have become ubiquitous. Using such forms of communication, users share a variety of information. However, information concerning one\u2019s sentiment on the world around her has attracted a lot of research interest (Nakov et al., 2016; Rosenthal et al., 2015).\n1We make the code available for research purposes at https://github.com/balikasg/ SemEval2016-Twitter_Sentiment_Evaluation.\nWorking with such short, informal text spans poses a number of different challenges to the Natural Language Processing (NLP) and Machine Learning (ML) communities. Those challenges arise from the vocabulary used (slang, abbreviations, emojis) (Maas et al., 2011), the short size, and the complex linguistic phenomena such as sarcasm (Rajadesingan et al., 2015) that often occur.\nWe present, here, our participation in Task 4 of SemEval 2016 (Nakov et al., 2016), namely Sentiment Analysis in Twitter. Task 4 comprised five different subtasks: Subtask A: Message Polarity Classification, Subtask B: Tweet classification according to a two-point scale, Subtask C: Tweet classification according to a five-point scale, Subtask D: Tweet quantification according to a two-point scale, and Subtask E: Tweet quantification according to a fivepoint scale. We participated in the first four subtasks under the team name \u201cTwiSE\u201d (Twitter Sentiment Evaluation). Our work consists of two steps: the preprocessing and feature extraction step, where we implemented and tested different feature sets proposed by participants of the previous editions of SemEval challenges (Tang et al., 2014; Kiritchenko et al., 2014a), and the learning step, where we investigated and optimized the performance of different learning strategies for the SemEval subtasks. For Subtask A we submitted the output of a stacked generalization (Wolpert, 1992) ensemble learning approach using the probabilistic outputs of a set of linear models as base models, whereas for the rest of the subtasks we submitted the outputs of single models, such as Support Vector Machines and Logistic ar X iv :1 60 6. 04 35 1v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\nRegression.2\nThe remainder of the paper is organised as follows: in Section 2 we describe the feature extraction and the feature transformations we used, in Section 3 we present the learning strategies we employed, in Section 4 we present a part of the in-house validation we performed to assess the models\u2019 performance, and finally, we conclude in Section 5 with remarks on our future work."}, {"heading": "2 Feature Engineering", "text": "We present the details of the feature extraction and transformation mechanisms we used. Our approach is based on the traditional N -gram extraction and on the use of sentiment lexicons describing the sentiment polarity of unigrams and/or bigrams. For the data pre-processing, cleaning and tokenization3 as well as for most of the learning steps, we used Python\u2019s Scikit-Learn (Pedregosa et al., 2011) and NLTK (Bird et al., 2009)."}, {"heading": "2.1 Feature Extraction", "text": "Similar to (Kiritchenko et al., 2014b) we extracted features based on the lexical content of each tweet and we also used sentiment-specific lexicons. The features extracted for each tweet include:\n\u2022 N-grams with N \u2208 [1, 4], character grams of dimension M \u2208 [3, 5],\n\u2022 # of exclamation marks, # of question marks, # of both exclamation and question marks,\n\u2022 # of capitalized words and # of elongated words,\n\u2022 # of negated contexts; negation also affected the N -grams features by transforming a word w in a negated context to w NEG,\n\u2022 # of positive emoticons, # of negative emoticons and a binary feature indicating if emoticons exist in a given tweet, and\n2To enable replicability we make the code we used available at https://github.com/balikasg/ SemEval2016-Twitter_Sentiment_Evaluation.\n3We adapted the tokenizer provided at http://sentiment.christopherpotts.net/tokenizing.html\n\u2022 Part-of-speech (POS) tags (Gimpel et al., 2011) and their occurrences partitioned regarding the positive and negative contexts.\nWith regard to the sentiment lexicons, we used:\n\u2022 manual sentiment lexicons: the Bing liu\u2019s lexicon (Hu and Liu, 2004), the NRC emotion lexicon (Mohammad and Turney, 2010), and the MPQA lexicon (Wilson et al., 2005),\n\u2022 # of words in positive and negative context belonging to the word clusters provided by the CMU Twitter NLP tool4\n\u2022 positional sentiment lexicons: sentiment 140 lexicon (Go et al., 2009) and the Hashtag Sentiment Lexicon (Kiritchenko et al., 2014b)\nWe make, here, more explixit the way we used the sentiment lexicons, using the Bing Liu\u2019s lexicon as an example. We treated the rest of the lexicons similarly. For each tweet, using the Bing Liu\u2019s lexicon we obtain a 104-dimensional vector. After tokenizing the tweet, we count how many words (i) in positive/negative contenxts belong to the positive/negative lexicons (4 features) and we repeat the process for the hashtags (4 features). To this point we have 8 features. We generate those 8 features for the lowercase words and the uppercase words. Finally, for each of the 24 POS tags the (Gimpel et al., 2011) tagger generates, we count how many words in positive/negative contenxts belong to the positive/negative lexicon. As a results, this generates 2\u00d7 8 + 24\u00d7 4 = 104 features in total for each tweet.\nFor each tweet we also used the distributed representations provided by (Tang et al., 2014) using the min, max and average composition functions on the vector representations of the words of each tweet."}, {"heading": "2.2 Feature Representation and Transformation", "text": "We describe the different representations of the extracted N -grams and character-grams we compared when optimizing our performance on each of the classification subtasks we participated. In the rest of this subsection we refer to both N-grams and\n4http://www.cs.cmu.edu/ ark/TweetNLP/\ncharacter-grams as words, in the general sense of letter strings. We evaluated two ways of representing such features: (i) a bag-of-words representation, that is for each tweet a sparse vector of dimension |V | is generated, where |V | is the vocabulary size, and (ii) a hashing function, that is a fast and spaceefficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector (Weinberger et al., 2009). We found that the performance using hashing representations was better. Hence, we opted for such representations and we tuned the size of the feature space for each subtask.\nConcerning the transformation of the features of words, we compared the tf-idf weighing scheme and the \u03b1-power transformation. The latter, transforms each vector x = (x1, x2, . . . , xd) to x\u2032 = (x\u03b11 , x \u03b1 2 , . . . , x \u03b1 d ) (Jegou et al., 2012). The main intuition behind the \u03b1-power transformation is that it reduces the effect of the most common words. Note that this is also the rationale behind the idf weighting scheme. However, we obtained better results using the \u03b1-power transformation. Hence, we tuned \u03b1 separately for each of the subtasks."}, {"heading": "3 The Learning Step", "text": "Having the features extracted we experimented with several families of classifiers such as linear models, maximum-margin models, nearest neighbours approaches and trees. We evaluated their performance using the data provided by the organisers, which were already split in training, validation and testing parts. Table 1 shows information about the tweets we managed to download. From the early validation schemes, we found that the two most competitive models were Logistic Regression from the family of linear models, and Support Vector Machines (SVMs) from the family of maximum margin models. It is to be noted that this is in line with the previous research (Mohammad et al., 2013; Bu\u0308chner and Stein, 2015)."}, {"heading": "3.1 Subtask A", "text": "Subtask A concerns a multiclass classification problem, where the general polarity of tweets has to be classified in one among three classes: \u201cPositive\u201d, \u201cNegative\u201d and \u201cNeutral\u201d, each denoting the tweet\u2019s overall polarity. The evaluation measure used for the\nsubtask is the Macro-F1 measure, calculated only for the Positive and Negative classes (Nakov et al., 2016).\nInspired by the wining system of SemEval 2015 Task 10 (Bu\u0308chner and Stein, 2015) we decided to employ an ensemble learning approach. Hence, our goal is twofold: (i) to generate a set of models that perform well as individual models, and (ii) to select a subset of models of (i) that generate diverse outputs and to combine them using an ensemble learning step that would result in lower generalization error.\nWe trained four such models as base models. Their details are presented in Table 2. In the stacked generalization approach we employed, we found that by training the second level classifier on the probabilistic outputs, instead of the predictions of the base models, yields better results. Logistic Regression generates probabilities as its outputs. In the case of SVMs, we transformed the confidence scores into probabilities using two methods, after adapting them to the multiclass setting: the Platt\u2019s method (Platt and others, 1999) and the isotonic regression (Zadrozny and Elkan, 2002). To solve the optimization problems of SVMs we used the Liblinear solvers (Fan et al., 2008). For Logistic Regression we used either Liblinear or LBFGS, with the latter being a limited memory quasi Newton method for general unconstrained optimization problems (Yu et al., 2011). To attack the multiclass problem, we selected among the traditional one-vs-rest approach, the crammer-singer approach for SVMs (Crammer and Singer, 2002), or the multinomial approach for Logistic Regression (also known as MaxEnt classifier), where the multinomial loss is minimised across the entire probability distribution (Malouf, 2002).\nFor each of the four base models the tweets are represented by the complete feature set described in Section 2. For transforming the ngrams and character-grams, the value of \u03b1 \u2208 {0.2, 0.4, 0.6, 0.8, 1}, the dimension of the space where the hashing function projects them, as well as the value of \u03bb \u2208 {10\u22127, . . . , 106} that controls the importance of the regularization term in the SVM and Logistic regression optimization problems were selected by grid searching using 5-fold crossvalidation on the training part of the provided data. We tuned each model independently before integrat-\ning it in the stacked generalization. Having the fine-tuned probability estimates for each of the instances of the test sets and for each of the base learners, we trained a second layer classifier using those fine-grained outputs. For this, we used SVMs, using the crammer-singer approach for the multi-class problem, which yielded the best performance in our validation schemes. Also, since the classification problem is unbalanced in the sense that the three classes are not equally represented in the training data, we assigned weights to make the problem balanced. Those weights were inversely proportional to the class frequencies in the input data for each class."}, {"heading": "3.2 Subtask B", "text": "Subtask B is a binary classification problem where given a tweet known to be about a given topic, one has to classify whether the tweet conveys a positive or a negative sentiment towards the topic. The evaluation measure proposed by the organisers for this subtask is macro-averaged recall (MaR) over the positive and negative class.\nOur participation is based on a single model. We used SVMs with a linear kernel and the Liblinear optimizer. We used the full feature set described in Section 2, after excluding the distributed embeddings because in our local validation experiments we found that they actually hurt the performance. Similarly to subtask A and due to the unbalanced nature of the problem, we use weights for the classes of the problem. Note that we do not consider the topic of the tweet and we classify the tweet\u2019s overall polarity.\nHence, we ignore the case where the tweet consists of more than one parts, each expressing different polarities about different parts."}, {"heading": "3.3 Subtask C", "text": "Subtask C concerns an ordinal classification problem. In the framework of this subtask, given a tweet known to be about a given topic, one has to estimate the sentiment conveyed by the tweet towards the topic on a five-point scale. Ordinal classification differs from standard multiclass classification in that the classes are ordered and the error takes into account this ordering so that not all mistakes weigh equally. In the tweet classification problem for instance, a classifier that would assign the class \u201c1\u201d to an instance belonging to class \u201c2\u201d will be less penalized compared to a classifier that will assign \u201c-1\u201d as the class . To this direction, the evaluation measure proposed by the organisers is the macroaveraged mean absolute error.\nSimilarly to Subtask B, we submitted the results of a single model and we classified the tweets according to their overall polarity ignoring the given topics. Instead of using one of the ordinal classification methods proposed in the bibliography, we use a standard multiclass approach. For that we use a Logistic Regression that minimizes the multinomial loss across the classes. Again, we use weights to cope with the unbalanced nature of our data. The distributed embeddings are excluded by the feature sets.\nWe elaborate here on our choice to use a conventional multiclass classification approach instead\nof an ordinal one. We evaluated a selection of methods described in (Pedregosa-Izquierdo, 2015) and in (Gutie\u0301rrez et al., 2015). In both cases, the results achieved with the multiclass methods were marginally better and for simplicity we opted for the multiclass methods. We believe that this is due to the nature of the problem: the feature sets and especially the fine-grained sentiment lexicons manage to encode the sentiment direction efficiently. Hence, assigning a class of completely opposite sentiment can only happen due to a complex linguistic phenomenon such as sarcasm. In the latter case, both methods may fail equally."}, {"heading": "3.4 Subtask D", "text": "Subtask D is a binary quantification problem. In particular, given a set of tweets known to be about a given topic, one has to estimate the distribution of the tweets across the Positive and Negative classes. For instance, having 100 tweets about the new iPhone, one must estimate the fractions of the Positive and Negative tweets respectively. The organisers proposed a smoothed version of the KullbackLeibler Divergence (KLD) as the subtask\u2019s evaluation measure.\nWe apply a classify and count approach for this task (Bella et al., 2010; Forman, 2008), that is we first classify each of the tweets and we then count the instances that belong to each class. To this end, we compare two approaches both trained on the features sets of Section 2 excluding the distributed representations: the standard multiclass SVM and a structure learning SVM that directly optimizes KLD (Esuli and Sebastiani, 2015). Again, our final submission uses the standard SVM with weights to cope with the imbalance problem as the model to classify the tweets. That is because the method of (Gao and Sebastiani, 2015) although competitive was outperformed in most of our local validation schemes."}, {"heading": "4 The evaluation framework", "text": "Before reporting the scores we obtained, we elaborate on our validation strategy and the steps we used when tuning our models. in each of the subtasks we only used the data that were realised for the 2016 edition of the challenge. Our validation had the following steps:\n1. Training using the released training data,\n2. validation on the validation data,\n3. validation again, in the union of the devtest and trial data (when applicable), after retraining on training and validation data.\nFor each parameter, we selected its value by averaging the optimal parameters with respect to the output of the above-listed steps (2) and (3). It is to be noted, that we strictly relied on the data released as part of the 2016 edition of SemEval; we didn\u2019t use past data.\nWe now present the performance we achieved both in our local evaluation schemas and in the official results released by the challenge organisers. Table 3 presents the results we obtained in the \u201cDevTest\u201d part of the challenge dataset and the scores on the test data as they were released by the organisers. In the latter, we were ranked 9/35, 8/19, 1/11 and 2/14 for subtasks A, B, C and D respectively. Observe, that for subtasks A and B, using just the \u201cdevtest\u201d part of the data as validation mechanism results in a quite accurate performance estimation."}, {"heading": "5 Future Work", "text": "That was our first contact with the task of sentiment analysis and we achieved satisfactory results. We relied on features proposed in the framework of previous SemEval challenges and we investigated the performance of different classification algorithms.\nIn our future work we will investigate directions both in the feature engineering and in the algorithmic/learning part. Firstly, we aim to deal with tweets in a finer level of granularity. As discussed in Section 3, in each of the tasks we classified the overall polarity of the tweet, ignoring cases where the tweets were referring to two or more subjects. In the\nsame line, we plan to improve our mechanism for handling negation. We have used a simple mechanism where a negative context is defined as the group of words after a negative word until a punctuation symbol. However, our error analysis revealed that punctuation is rarely used in tweets. Finally, we plan to investigate ways to integrate more data in our approaches, since we only used this edition\u2019s data.\nThe application of an ensemble learning approach, is a promising direction towards the short text sentiment evaluation. To this direction, we hope that an extensive error analysis process will help us identify better classification systems that when integrated in our ensemble (of subtask A) will reduce the generalization error."}, {"heading": "Acknowledgments", "text": "We would like to thank the organisers of the Task 4 of SemEval 2016, for providing the data, the guidelines and the infrastructure. We would also like to thank the anonymous reviewers for their insightful comments."}], "references": [{"title": "Quantification via probability estimators", "author": ["Bella et al.2010] Antonio Bella", "Cesar Ferri", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Maria Jose Ramirez-Quintana"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Bella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bella et al\\.", "year": 2010}, {"title": "Webis: An ensemble for twitter sentiment detection", "author": ["B\u00fcchner", "Benno Stein"], "venue": null, "citeRegEx": "B\u00fcchner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "B\u00fcchner et al\\.", "year": 2015}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Crammer", "Singer2002] Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2002}, {"title": "Optimizing text quantifiers for multivariate loss functions", "author": ["Esuli", "Sebastiani2015] Andrea Esuli", "Fabrizio Sebastiani"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "citeRegEx": "Esuli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Esuli et al\\.", "year": 2015}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Quantifying counts and costs via classification. Data Mining and Knowledge Discovery, 17(2):164\u2013206", "author": ["George Forman"], "venue": null, "citeRegEx": "Forman.,? \\Q2008\\E", "shortCiteRegEx": "Forman.", "year": 2008}, {"title": "Tweet sentiment: From classification to quantification", "author": ["Gao", "Sebastiani2015] Wei Gao", "Fabrizio Sebastiani"], "venue": "In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining", "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Part-of-speech tagging for twitter: Annotation", "author": ["Gimpel et al.2011] Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Go et al.2009] Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "CS224N Project", "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Ordinal regression methods: survey and experimental study", "author": ["M. P\u00e9rez-Ortiz", "J. S\u00e1nchez-Monedero", "F. Fernandez-Navarro", "C. Herv\u00e1s-Mart\u0131\u0301nez"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Guti\u00e9rrez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guti\u00e9rrez et al\\.", "year": 2015}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Aggregating local image descriptors into compact codes", "author": ["Jegou et al.2012] H. Jegou", "F. Perronnin", "M. Douze", "J. Sanchez", "P. Perez", "C. Schmid"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Jegou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2012}, {"title": "Nrccanada-2014: Detecting aspects and sentiment in customer reviews", "author": ["Xiaodan Zhu", "Colin Cherry", "Saif Mohammad"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Kiritchenko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Sentiment analysis of short informal texts", "author": ["Xiaodan Zhu", "Saif M Mohammad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kiritchenko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Multi-dimensional sentiment analysis with learned representations", "author": ["Maas et al.2011] Andrew L Maas", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "A comparison of algorithms for maximum entropy parameter estimation", "author": ["Robert Malouf"], "venue": "In proceedings of the 6th conference on Natural language learning-Volume", "citeRegEx": "Malouf.,? \\Q2002\\E", "shortCiteRegEx": "Malouf.", "year": 2002}, {"title": "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon", "author": ["Mohammad", "Turney2010] Saif M Mohammad", "Peter D Turney"], "venue": "In Proceedings of the NAACL HLT 2010 workshop on computational approaches", "citeRegEx": "Mohammad et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2010}, {"title": "Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets. arXiv preprint arXiv:1308.6242", "author": ["Svetlana Kiritchenko", "Xiaodan Zhu"], "venue": null, "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "SemEval-2016 task 4: Sentiment analysis in Twitter", "author": ["Nakov et al.2016] Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanov", "Fabrizio Sebastiani"], "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016),", "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["John Platt"], "venue": "Advances in large margin classifiers,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Rajadesingan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "Semeval-2015 task 10: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Svetlana Kiritchenko", "Saif M Mohammad", "Alan Ritter", "Veselin Stoyanov"], "venue": "Proceedings of SemEval-2015", "citeRegEx": "Rosenthal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Feature hashing for large scale multitask learning", "author": ["Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on human language technology and empirical methods in natural language", "author": ["Janyce Wiebe", "Paul Hoffmann"], "venue": null, "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Dual coordinate descent methods for logistic regression and maximum entropy models", "author": ["Yu et al.2011] Hsiang-Fu Yu", "Fang-Lan Huang", "Chih-Jen Lin"], "venue": "Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["Zadrozny", "Elkan2002] Bianca Zadrozny", "Charles Elkan"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Zadrozny et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 18, "context": "However, information concerning one\u2019s sentiment on the world around her has attracted a lot of research interest (Nakov et al., 2016; Rosenthal et al., 2015).", "startOffset": 113, "endOffset": 157}, {"referenceID": 21, "context": "However, information concerning one\u2019s sentiment on the world around her has attracted a lot of research interest (Nakov et al., 2016; Rosenthal et al., 2015).", "startOffset": 113, "endOffset": 157}, {"referenceID": 14, "context": "the vocabulary used (slang, abbreviations, emojis) (Maas et al., 2011), the short size, and the complex linguistic phenomena such as sarcasm (Rajadesingan et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": ", 2011), the short size, and the complex linguistic phenomena such as sarcasm (Rajadesingan et al., 2015) that often occur.", "startOffset": 78, "endOffset": 105}, {"referenceID": 18, "context": "We present, here, our participation in Task 4 of SemEval 2016 (Nakov et al., 2016), namely Senti-", "startOffset": 62, "endOffset": 82}, {"referenceID": 22, "context": "preprocessing and feature extraction step, where we implemented and tested different feature sets proposed by participants of the previous editions of SemEval challenges (Tang et al., 2014; Kiritchenko et al., 2014a), and the learning step, where we inves-", "startOffset": 170, "endOffset": 216}, {"referenceID": 7, "context": "html \u2022 Part-of-speech (POS) tags (Gimpel et al., 2011) and their occurrences partitioned regarding the positive and negative contexts.", "startOffset": 33, "endOffset": 54}, {"referenceID": 24, "context": "\u2022 manual sentiment lexicons: the Bing liu\u2019s lexicon (Hu and Liu, 2004), the NRC emotion lexicon (Mohammad and Turney, 2010), and the MPQA lexicon (Wilson et al., 2005),", "startOffset": 146, "endOffset": 167}, {"referenceID": 8, "context": "\u2022 positional sentiment lexicons: sentiment 140 lexicon (Go et al., 2009) and the Hashtag Sentiment Lexicon (Kiritchenko et al.", "startOffset": 55, "endOffset": 72}, {"referenceID": 22, "context": "For each tweet we also used the distributed representations provided by (Tang et al., 2014) using the min, max and average composition functions on the vector representations of the words of each tweet.", "startOffset": 72, "endOffset": 91}, {"referenceID": 23, "context": "turning arbitrary features into indices in a vector (Weinberger et al., 2009).", "startOffset": 52, "endOffset": 77}, {"referenceID": 11, "context": ", x \u03b1 d ) (Jegou et al., 2012).", "startOffset": 10, "endOffset": 30}, {"referenceID": 17, "context": "It is to be noted that this is in line with the previous research (Mohammad et al., 2013; B\u00fcchner and Stein, 2015).", "startOffset": 66, "endOffset": 114}, {"referenceID": 18, "context": "The evaluation measure used for the subtask is the Macro-F1 measure, calculated only for the Positive and Negative classes (Nakov et al., 2016).", "startOffset": 123, "endOffset": 143}, {"referenceID": 4, "context": "To solve the optimization problems of SVMs we used the Liblinear solvers (Fan et al., 2008).", "startOffset": 73, "endOffset": 91}, {"referenceID": 25, "context": "being a limited memory quasi Newton method for general unconstrained optimization problems (Yu et al., 2011).", "startOffset": 91, "endOffset": 108}, {"referenceID": 15, "context": "To attack the multiclass problem, we selected among the traditional one-vs-rest approach, the crammer-singer approach for SVMs (Crammer and Singer, 2002), or the multinomial approach for Logistic Regression (also known as MaxEnt classifier), where the multinomial loss is minimised across the entire probability distribution (Malouf, 2002).", "startOffset": 325, "endOffset": 339}, {"referenceID": 9, "context": "We evaluated a selection of methods described in (Pedregosa-Izquierdo, 2015) and in (Guti\u00e9rrez et al., 2015).", "startOffset": 84, "endOffset": 108}, {"referenceID": 0, "context": "We apply a classify and count approach for this task (Bella et al., 2010; Forman, 2008), that is we first classify each of the tweets and we then count the", "startOffset": 53, "endOffset": 87}, {"referenceID": 5, "context": "We apply a classify and count approach for this task (Bella et al., 2010; Forman, 2008), that is we first classify each of the tweets and we then count the", "startOffset": 53, "endOffset": 87}], "year": 2016, "abstractText": "This paper describes the participation of the team \u201cTwiSE\u201d in the SemEval 2016 challenge. Specifically, we participated in Task 4, namely \u201cSentiment Analysis in Twitter\u201d for which we implemented sentiment classification systems for subtasks A, B, C and D. Our approach consists of two steps. In the first step, we generate and validate diverse feature sets for twitter sentiment evaluation, inspired by the work of participants of previous editions of such challenges. In the second step, we focus on the optimization of the evaluation measures of the different subtasks. To this end, we examine different learning strategies by validating them on the data provided by the task organisers. For our final submissions we used an ensemble learning approach (stacked generalization) for Subtask A and single linear models for the rest of the subtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14 for subtasks A, B, C and D respectively.1", "creator": "LaTeX with hyperref package"}}}