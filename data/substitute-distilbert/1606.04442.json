{"id": "1606.04442", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "DeepMath - Deep Sequence Models for Premise Selection", "abstract": "we understand experimental models of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. we make a two stage approach for this task that yields good fit for the premise selection task on discovering mizar corpus while avoiding the hand - formed features of existing state - than - the - art models. to our knowledge, this is the first time deep learning has been applied to theorem proving.", "histories": [["v1", "Tue, 14 Jun 2016 16:27:41 GMT  (904kb,D)", "http://arxiv.org/abs/1606.04442v1", null], ["v2", "Thu, 26 Jan 2017 19:35:16 GMT  (1067kb,D)", "http://arxiv.org/abs/1606.04442v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["geoffrey irving", "christian szegedy", "alexander a alemi", "niklas e\u00e9n", "fran\u00e7ois chollet", "josef urban"], "accepted": true, "id": "1606.04442"}, "pdf": {"name": "1606.04442.pdf", "metadata": {"source": "CRF", "title": "DeepMath - Deep Sequence Models for Premise Selection", "authors": ["Alexander A. Alemi", "Fran\u00e7ois Chollet", "Geoffrey Irving"], "emails": ["alemi@google.com", "fchollet@google.com", "geoffreyi@google.com", "szegedy@google.com", "josef.urban@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Mathematics underpins all scientific disciplines. Machine learning itself rests on measure and probability theory, calculus, linear algebra, functional analysis, and information theory. Complex mathematics underlies computer chips, transit systems, communication systems, and financial infrastructure \u2013 thus the correctness of many of these systems can be reduced to mathematical proofs.\nUnfortunately, these correctness proofs are often impractical to produce without automation, and present-day computers have only limited ability to assist humans in developing mathematical proofs and formally verifying human proofs. There are two main bottlenecks: (1) lack of automated methods for semantic or formal parsing of informal mathematical texts (autoformalization), and (2) lack of strong automated reasoning methods to fill in the gaps in already formalized human-written proofs.\nThe two bottlenecks are related. Strong automated reasoning can act as a semantic filter for autoformalization, and successful autoformalization would provide a large corpus of computer-understandable facts, proofs, and theory developments. Such a corpus would serve as both background knowledge to fill in gaps in human-level proofs and as a training set to guide automated reasoning. Such guidance is crucial: exhaustive deductive reasoning tools such as today\u2019s resolution/superposition automated theorem provers (ATPs) quickly hit combinatorial explosion, and are unusable when reasoning with a very large number of facts without careful selection [5].\nIn this work, we focus on the latter bottleneck. We develop deep neural networks that learn from a large repository of manually formalized computer-understandable proofs. We learn the task that is essential for making today\u2019s ATPs usable over large formal corpora: the selection of a limited number of most relevant facts for proving a new conjecture. This is known as premise selection.\nThe main contributions of this work are:\n\u2022 A demonstration for the first time that neural network models are useful for aiding in large scale automated logical reasoning without the need for hand-engineered features.\n\u2217Authors listed alphabetically. All contributions are considered equal.\nar X\niv :1\n60 6.\n04 44\n2v 1\n[ cs\n.A I]\n1 4\nJu n\n\u2022 The comparison of various network architectures (including convolutional, recurrent and hybrid models) and their effect on premise selection performance.\n\u2022 A method of semantic-aware \u201cdefinition\u201d-embeddings for function symbols that improves the generalization of formulas with symbols occuring infrequently. This model outperforms previous approaches at relaxed cutoff-thresholds.\n\u2022 Analysis that shows that the neural network based premise selection models are complementary to those with hand-engineered features and can be ensembled with previous results to produce superior results."}, {"heading": "2 Formalization and Theorem Proving", "text": "In the last two decades, large corpora of complex mathematical knowledge have been formalized: encoded in complete detail so that computers can fully understand the semantics of complicated mathematical objects. The process of writing such formal and verifiable theorems, definitions, proofs, and theories is called Interactive Theorem Proving (ITP).\nThe ITP field dates back to 1960s [18] and the Automath system by N.G. de Bruijn [10]. ITP systems include HOL (Light) [17], Isabelle [41], Mizar [14], Coq [8], and ACL2 [25]. The development of ITP has been intertwined with the development of its cousin field of Automated Theorem Proving (ATP) [33], where proofs of conjectures are attempted fully automatically. Unlike ATP systems, ITP systems allow human-assisted formalization and proving of theorems that are often beyond the capabilities of the fully automated systems.\nLarge ITP libraries include the Mizar Mathematical Library (MML) with over 50,000 lemmas, and the core Isabelle, HOL, Coq, and ACL2 libraries with thousands of lemmas. These core libraries are a basis for large projects in formalized mathematics and software and hardware verification. Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3]. ITP verifications of the seL4 kernel [27] and CompCert compiler [29] show comparable progress in large scale software verification.\nWhile these large projects mark a coming of age of formalization, ITP remains labor-intensive. For example, Flyspeck took about 20 person-years, with twice as much for Feit-Thompson. Behind this cost are our two bottlenecks: lack of tools for autoformalization and strong proof automation.\nRecently the field of Automated Reasoning in Large Theories (ARLT) [39] has developed, including AI/ATP/ITP (AITP) systems called hammers that assist ITP formalization [5]. Hammers analyze the full set of theorems and proofs in the ITP libraries, estimate the relevance of each theorem, and apply optimized translations from the ITP logic to simpler ATP formalisms. Then they attack new conjectures using the most promising combinations of existing theorems and ATP search strategies. Recent evaluations have proved 40% of all Mizar and Flyspeck theorems fully automatically [22, 23].\nThis AITP performance speeds up formalization and motivates further research combining statistical learning and deductive tools. However, there is significant room for improvement: with perfect premise selection (a perfect choice of library facts) ATPs can prove at least 56% of Mizar and Flyspeck instead of today\u2019s 40% [5]. In the next section we explain the premise selection task in more detail and explain the experimental setting for measuring such improvements."}, {"heading": "3 Premise Selection, Experimental Setting and Previous Results", "text": "Given a formal corpus of facts and proofs expressed in an ATP-compatible format, our task is Definition (Premise selection problem). Given a large set of premises P , an ATP system A with given resource limits, and a new conjecture C, predict those premises from P that will most likely lead to an automatically constructed proof of C by A.\nWe use the Mizar Mathematical Library (MML) version 4.181.11472 as the formal corpus and E prover [34] version 1.9 as the underlying ATP system. The following list examplifies a small\n2ftp://mizar.uwb.edu.pl/pub/system/i386-linux/mizar-7.13.01_4.181. 1147-i386-linux.tar\n:: t99_jordan: Jordan curve theorem in Mizar for C being Simple_closed_curve holds C is Jordan;\n:: Translation to first order logic fof(t99_jordan, axiom, (! [A] : ( (v1_topreal2(A) & m1_subset_1(A, k1_zfmisc_1(u1_struct_0(k15_euclid(2))))) => v1_jordan1(A)) ) ).\nFigure 1: (top) The final statement of the Mizar formalization of the Jordan curve theorem. (bottom) The translation to first-order logic, using name mangling to ensure uniqueness across the entire corpus.\nnon-representative sample of topics and theorems that are included in the Mizar Mathematical Library:\n\u2022 Cauchy-Riemann Differential Equations of Complex Functions \u2022 Characterization and Existence of Gr\u00f6bner Bases \u2022 Maximum Network Flow Algorithm by Ford and Fulkerson \u2022 G\u00f6del\u2019s Completeness Theorem \u2022 Brouwer Fixed Point Theorem \u2022 Arrow\u2019s Impossibility Theorem \u2022 The Borsuk-Ulam Theorem \u2022 Dickson\u2019s Lemma \u2022 The Sylow Theorems \u2022 Hahn Banach Theorem \u2022 Gauss Lemma and Law of Quadratic Reciprocity \u2022 Public-Key Cryptography and Pepin\u2019s Primality Test \u2022 Ramsey\u2019s Theorem\nThis version of MML was used for the latest AITP evaluation reported in [23]. There are 57,917 proved Mizar theorems and unnamed top-level lemmas in this MML organized into 1,147 articles. This set is chronologically ordered by the order of articles in MML and by the order of theorems in the articles. Proofs of later theorems can only refer to earlier theorems. This ordering also applies to 88,783 other Mizar formulas (encoding the type system and other automations known to Mizar) used in the problems. The formulas have been translated into the TPTP format [37] used by first-order ATPs by the MPTP system [38] (see Figure 1).\nOur goal is to automatically prove as many theorems as possible, using at each step all previous theorems and proofs. We can learn from both human proofs and ATP proofs, but previous experiments [28, 22] show that learning only from the ATP proofs is preferable to including human proofs if the set of ATP proofs is sufficiently large. Since for 32,524 (56.2%) of the 57,917 theorems an ATP proof was previously found by a combination of manual and learning-based premise selection [23], we use only these ATP proofs for training.\nThe 40% success rate from [23] used a portfolio of 14 AITP methods using different learners, ATPs, and numbers of premises. The best single method proved 27.3% of the theorems. Only fast and simple learners such as k-nearest-neighbors, naive Bayes, and their ensembles were used, based on hand-crafted features such as the set of (normalized) subterms and symbols in each formula."}, {"heading": "4 Motivation for the use of Deep Learning", "text": "Strong premise selection requires models capable of reasoning over mathematical statements, here encoded as variable-length strings of first-order logic. In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36]. These results have demonstrated the ability of deep networks to extract useful representations from sequential inputs without hand-tuned feature engineering. Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20]. Here, we extract learned representations of mathematical statements to assist in premise selection and proof.\n100 101 102 103 104 105\n100 101 102 103 104\n(a) Length in chars.\n100 101 102 103 104 105\n100 101 102 103 104\n(b) Length in words.\n0 200 400 600 800 1000\n100 101 102 103 104 105\n(c) Word occurrences.\n0 20 40 60 80 100\n100 101 102 103 104\n(d) Dependencies.\nFigure 2: Histograms of statement lengths, occurrences of each word, and statement dependencies in the Mizar corpus translated to first order logic. The wide length distribution poses difficulties for RNN models and batching, and the large number of rarely occurring words makes it important to take definitions of words into account.\nAxiom first order logic sequence\nCNN/RNN Sequence model\nConjecture first order logic sequence\nCNN/RNN Sequence model\nConcatenate embeddings\nFully connected layer with 1024 outputs\nFully connected layer with 1 output\nLogistic loss\n! [ A , B ] : ( g t a ...\nWx+b Wx+b Wx+b Wx+b Wx+b\nUx+c Ux+c Ux+c\nMaximum\nFigure 3: (left) Our network structure. The input sequences are either character-level (section 5.1) or word-level (section 5.2). We use separate models to embed conjecture and axiom, and a logistic layer to predict whether the axiom is useful for proving the conjecture. (right) A convolutional model.\nThe Mizar data set is also an interesting case study in neural network sequence tasks, as it differs from natural language problems in several ways. It is highly structured with a simple context free grammar \u2013 the interesting task occurs only after parsing. The distribution of lengths is wide, ranging from 5 to 84,299 characters with mean 304.5, and from 2 to 21,251 tokens with mean 107.4 (see Figure 2). Fully recurrent models would have to backpropagate through 100s to 1000s of characters or 100s of tokens to embed a whole statement. Finally, there are many rare words \u2013 60.3% of the words occur fewer than 10 times \u2013 motivating the definition-aware embeddings in section 5.2."}, {"heading": "5 Overview of our approach", "text": "The full premise selection task takes a conjecture and a set of axioms and chooses a subset of axioms to pass to the ATP. We simplify from subset selection to pairwise relevance by predicting the probability that a given axiom is useful for proving a given conjecture. This approach depends on a relatively sparse dependency graph: we ignore the issue of pruning redundant sets of axioms. Our general architecture is shown in Figure 3(left): the conjecture and axiom sequences are separately embedded into fixed length real vectors, then concatenated and passed to a third network with a few fully connected layers and logistic loss. During training time, the two embedding networks and the joined predictor path is treated as a single large neural network and trained jointly.\nAs discussed in section 3, we train our models on premise selection data generated by a combination of various methods, including k-nearest-neighbor search on hand-engineered similarity metrics. We start with a first stage of character-level models, and then build second and later stages of word-level models on top of the results of earlier stages."}, {"heading": "5.1 Stage 1: Character-level models", "text": "We begin by avoiding special purpose engineering by treating formulas on the character-level. We use an 80 dimensional one-hot encoding of the character sequence, including 79 unique characters that occur in the translated Mizar corpus and a special character to denote the start and end of each statement. These sequences are passed to a network with weight sharing for variable length input. For the embedding computation, we have explored the following architectures:\n1. Pure recurrent LSTM [19] and GRU [7] networks. 2. A pure multi-layer convolutional network with various numbers of convolutional layers\n(with strides) followed by a global temporal max-pooling reduction (see Figure 3(right)). 3. A recurrent-convolutional network, that uses convolutional layers to produce a shorter\nsequence which is processed by a LSTM.\nThe exact architectures used are specified in the experimental section.\nGiven the cost of sequence embedding, it is computationally prohibitive to compute a large number of (conjecture, axiom)-pairs. Fortunately, our architecture allows caching the embeddings for conjectures and axioms and evaluating only the shared portion of the network for a given pair, making it practical to consider all pairs during evaluation."}, {"heading": "5.2 Stage 2: Word-level models", "text": "The character-level models are limited to word and structure similarity within the axiom or conjecture being embedded. However, many of the symbols occurring in a formula are defined by formulas earlier in the corpus, and we can use these definitions to improve model performance.\nSince Mizar is based on first-order set theory, definitions of names can be either explicit or implicit. An explicit definition of x sets x = e for some expression e, while an implicit definition states a property of the defined object, such as defining a function f(x) by \u2200x.f(f(x)) = g(x). To avoid manually encoding the structure of implicit definitions, we embed the entire statement defining an identifier x, and then use these definition embeddings as word-level embeddings.\nIdeally, we would train a single network that embeds statements by recursively expanding and embedding the definitions of words. Unfortunately, this recursion would dramatically increase the cost of training since the definition chains can be quite deep. For example, Mizar defines real numbers in terms of nonnegative reals, which are defined as Dedekind cuts of nonnegative rationals, which are defined as ratios of naturals, etc. As an inexpensive alternative, we reuse the axiom embeddings computed by a previously trained character-level model, mapping each defined identifier to the axiom embedding of its defining statement. Other tokens such as brackets and operators are mapped to fixed pseudorandom vectors of the same dimension. An extra binary feature distinguishes between defined tokens and random embeddings so that the network does not have to learn the difference.\nSince we embed one token at a time ignoring grammatical structure, our approach does not require a parser, only a trivial lexer implemented in a few lines of Python.\nOnce we have word-level embeddings, we use the same architectures from stage 1 to reduce down to axiom and conjecture embeddings and then classify whether an (axiom, conjecture) pair is relevant.\nWe also tried iterating this approach of using definition embeddings as word embeddings multiple times, using the output embeddings of a trained definition-based model as the input word embeddings for another definition-based model. This extension did not result in measurable gains."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Experimental Setup", "text": "For training and evaluation we use a subset of 32,524 out of 57,917 theorems that are known to be provable by an ATP given the right set of premises. We split off a random 10% of these (3,124 statements) for holdout testing and validation. This may lead to learning from future proofs: a proof Pj of theorem Tj written after theorem Ti may guide the premise selection for Ti. However, previous k-NN experiments show similar performance between a full 10-fold cross-validation and incremental evaluation as long as chronologically preceding formulas participate in proofs of only later theorems.\nWe additionally held out 400 statements from the 3,124 for monitoring training progress, as well as for model and checkpoint selection. Final evaluation was done on the remaining 2,724 conjectures. Note that we only held out conjectures, but we trained on all statements as axioms. This may lead to learning from future proofs: a proof Pj of theorem Tj written after theorem Ti may guide the premise selection for Ti. This is comparable to our k-NN baseline which is also trained on all statements as axioms, and where similar results were obtained from incremental evaluation. This practice is\njustified by the fact that the hard problem is to find proofs for new conjectures where we can utilize all our knowledge about the usefulness of statements for other purposes."}, {"heading": "6.2 Metrics", "text": "For each conjecture, our models output a ranking of the possible premises. Our primary metric is the number of conjectures proved from the top-k premises, where k = 16, 32, . . . , 1024. This metric can accommodate alternative proofs but is computationally expensive. Therefore we additionally measure the ranking quality using the average maximum relative rank of the testing premise set. Formally, average max relative rank is\naMRR = mean C max P\u2208Ptest(C) rank(P,Pavail(C)) |Pavail(C)|\nwhere C ranges over conjectures, Pavail(C) is the set of premises available to prove C, Ptest(C) is the set of premises for conjecture C from the test set, and rank(P,Pavail(C)) is the rank of premise P among the set Pavail(C) according to the model. The motivation for aMRR is that conjectures are easier to prove if all their dependencies occur early in the ranking.\nSince it is too expensive to rank all axioms for a conjecture during continuous evaluation, we approximate our objective. For our holdout set of 400 conjectures, we select all true dependencies Ptest(C) and 128 fixed random false dependencies from Pavail(C)\u2212Ptest(C) and compute the average max relative rank in this ordering. Note that aMRR is nonzero even if all true dependencies are ordered before false dependencies; the best possible value is 0.051."}, {"heading": "6.3 Network Architectures", "text": "All our neural network models use the general architecture from Fig 3: a classifier on top of the concatenated embeddings of an axiom and a conjecture. The same classifier architecture was used for all models: a fully-connected neural network with one hidden layer of size 1024. For each model, the axiom and conjecture embedding networks have the same architecture but non-shared weights. The details of the embedding networks are shown in Fig 4.\n(a) Training accuracy for different character-level models. Recurrent models seem underperform, while pure convolutional models yield the best results. For each architecture, we trained three models with different random initialization seeds. Only the best runs are shown on this graph; we did not see much variance between runs on the same architecture. (b) Test average max relative rank for different models. The best is a word-level CNN using definition embeddings from a character-level 2-layer CNN. An identical word-embedding model with random starting embedding overfits after only 250,000 iterations and underperforms the best character-level model."}, {"heading": "6.4 Network Training", "text": "The neural networks were trained using asynchronous distributed stochastic gradient descent using the Adam optimizer [26] with up to 20 parallel NVIDIA K-80 GPU workers per model. We used the TensorFlow framework [1] and the Keras library [6]. We used [11] to initialize convolutional and fully connected layers and Polyak averaging with 0.9999 decay to produce the final weights [32]. We experimented with gradient clipping at various thresholds, which helped stabilize training but yielded inferior results vs. non-clipped models. The character level models were trained with maximum sequence length 2048 characters, where the word-level (and definition embedding) based models were trained with a maximum sequence length of 500 words."}, {"heading": "6.5 Experimental Results", "text": "Our best selection pipeline uses a stage-1 character-level convolutional neural network model to produce word-level embeddings for the second stage. The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24]. For all conjectures in our holdout set, we consider each preceding statement (lemma, definition axiom) in the chronological ordering as a premise candidate. In the DeepMath case, premises were ordered by their logistic scores. E prover was applied to the top-k of the premise-candidates for each of the cutoffs k \u2208 (16, 32, . . . , 1024) until a proof is found or k = 1024 fails. Table 1 reports the number of theorems proved with a cutoff value at most the k in the leftmost column. For E prover, we use a soft time limit of 90 seconds, a hard time limit of 120 seconds, a memory limit of 4 GB, and a processed clauses limit of 500,000. These settings are generous: even dramatically increasing the limits proves at most 10 extra theorems.\nOur most successful models employ simple convolutional networks followed by max pooling (as opposed to recurrent networks like LSTM/GRU), and the two stage definition-based def-CNN outperforms the na\u00efve word-CNN word embedding significantly. In the latter the word embeddings were learned in a single pass; in the former they are fixed from the stage-1 character-level model. For each architecture (cf. Figure 4) two convolutional layers perform best. Although our models differ significantly from each other, they differ even more from the k-NN baseline based on hand-crafted features. The right column of Table 1 shows the result if we average the prediction score of the stage-1 model with that of the definition based stage-2 model. We also experimented with shorter character-based RNN models using shorter sequences: these lagged behind our long-sequence (at max 2048) character models but performed significantly better than those trained on longer sequences. This suggest that these RNNs could be improved by more sophisticated optimization techniques such as curriculum learning. Finally, our definition-based CNN model is capable of proving 415 theorems unproved in E by previous methods, including those using dependencies from human Mizar proofs.\nCutoff k-NN Baseline char-CNN word-CNN word-CNN-LSTM def-CNN def+char-CNN 16 503 390 364 256 375 473 32 810 660 638 502 669 778 64 1153 1013 932 780 1029 1120\n128 1403 1336 1254 1080 1339 1454 256 1541 1547 1454 1297 1549 1617 512 1626 1627 1558 1447 1644 1696 1024 1627 1670 1608 1526 1682 1726 % proved 59.34 60.90 58.64 55.65 61.34 62.95"}, {"heading": "7 Conclusions", "text": "In this work we provide evidence that even simple neural models can compete with hand-engineered features for premise selection, helping to find many new proofs. This translates to real gains in automatic theorem proving. Despite these encouraging results, our models are relatively shallow networks with inherent limitations to representational power and are incapable of capturing high level properties of mathematical statements. We believe theorem proving is a challenging and important domain for deep learning methods, and that more sophisticated optimization techniques and training methodologies will prove more useful than in less structured domains."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Globally normalized transition-based neural networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "arXiv preprint arXiv:1603.06042,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A Compendium of Continuous Lattices in MIZAR", "author": ["G. Bancerek", "P. Rudnicki"], "venue": "J. Autom. Reasoning, 29(3-4):189\u2013224,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Sentence pair scoring: Towards unified framework for text comprehension", "author": ["P. Baudi\u0161", "J. Pichl", "T. Vysko\u010dil", "J. \u0160ediv\u00fd"], "venue": "arXiv preprint arXiv:1603.06127,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Hammering towards QED", "author": ["J.C. Blanchette", "C. Kaliszyk", "L.C. Paulson", "J. Urban"], "venue": "J. Formalized Reasoning, 9(1):101\u2013148,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3061\u20133069,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The mathematical language AUTOMATH, its usage, and some of its extensions", "author": ["N. de Bruijn"], "venue": "Proceedings of the Symposium on Automatic Demonstration,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1968}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "The four colour theorem: Engineering of a formal proof", "author": ["G. Gonthier"], "venue": "D. Kapur, editor, Computer Mathematics, 8th Asian Symposium, ASCM 2007, Singapore, December 15-17, 2007. Revised and Invited Papers, volume 5081 of Lecture Notes in Computer Science, page 333. Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "A machine-checked proof of the Odd Order Theorem", "author": ["G. Gonthier", "A. Asperti", "J. Avigad", "Y. Bertot", "C. Cohen", "F. Garillot", "S.L. Roux", "A. Mahboubi", "R. O\u2019Connor", "S.O. Biha", "I. Pasca", "L. Rideau", "A. Solovyev", "E. Tassi", "L. Th\u00e9ry"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Korni\u0142owicz", "A. Naumowicz"], "venue": "J. Formalized Reasoning, 3(2):153\u2013245,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural Turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A formal proof of the Kepler conjecture", "author": ["T.C. Hales", "M. Adams", "G. Bauer", "D.T. Dang", "J. Harrison", "T.L. Hoang", "C. Kaliszyk", "V. Magron", "S. McLaughlin", "T.T. Nguyen", "T.Q. Nguyen", "T. Nipkow", "S. Obua", "J. Pleso", "J. Rute", "A. Solovyev", "A.H.T. Ta", "T.N. Tran", "D.T. Trieu", "J. Urban", "K.K. Vu", "R. Zumkeller"], "venue": "CoRR, abs/1501.02155,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "HOL Light: A tutorial introduction", "author": ["J. Harrison"], "venue": "M. K. Srivas and A. J. Camilleri, editors, FMCAD, volume 1166 of LNCS, pages 265\u2013269. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "History of interactive theorem proving", "author": ["J. Harrison", "J. Urban", "F. Wiedijk"], "venue": "J. H. Siekmann, editor, Computational Logic, volume 9 of Handbook of the History of Logic, pages 135 \u2013 214. North-Holland,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural gpus learn algorithms", "author": ["\u0141. Kaiser", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Stronger automation for Flyspeck by feature weighting and strategy evolution", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. C. Blanchette and J. Urban, editors, PxTP 2013, volume 14 of EPiC Series, pages 87\u201395. EasyChair,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning, 53(2):173\u2013213,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning, 55(3):245\u2013256,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient semantic features for automated reasoning over large theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Q. Yang and M. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3084\u20133090. AAAI Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "seL4: formal verification of an operatingsystem kernel", "author": ["G. Klein", "J. Andronick", "K. Elphinstone", "G. Heiser", "D. Cock", "P. Derrin", "D. Elkaduwe", "K. Engelhardt", "R. Kolanski", "M. Norrish", "T. Sewell", "H. Tuch", "S. Winwood"], "venue": "Commun. ACM, 53(6):107\u2013115,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from multiple proofs: First experiments", "author": ["D. Kuehlwein", "J. Urban"], "venue": "P. Fontaine, R. A. Schmidt, and S. Schulz, editors, PAAR-2012, volume 21 of EPiC Series, pages 82\u201394. EasyChair,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Formal verification of a realistic compiler", "author": ["X. Leroy"], "venue": "Commun. ACM, 52(7):107\u2013115,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, volume 2, page 3,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Handbook of Automated Reasoning (in 2 volumes)", "author": ["J.A. Robinson", "A. Voronkov", "editors"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "E - A Brainiac Theorem Prover", "author": ["S. Schulz"], "venue": "AI Commun., 15(2-3):111\u2013126,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151\u2013161. Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "The TPTP world - infrastructure for automated reasoning", "author": ["G. Sutcliffe"], "venue": "E. M. Clarke and A. Voronkov, editors, LPAR (Dakar), volume 6355 of LNCS, pages 1\u201312. Springer,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "MPTP 0.2: Design, implementation, and initial experiments", "author": ["J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Theorem proving in large formal mathematics as an emerging AI field", "author": ["J. Urban", "J. Vysko\u010dil"], "venue": "M. P. Bonacina and M. E. Stickel, editors, Automated Reasoning and Mathematics: Essays in Memory of William McCune, volume 7788 of LNAI, pages 240\u2013257. Springer,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Such guidance is crucial: exhaustive deductive reasoning tools such as today\u2019s resolution/superposition automated theorem provers (ATPs) quickly hit combinatorial explosion, and are unusable when reasoning with a very large number of facts without careful selection [5].", "startOffset": 266, "endOffset": 269}, {"referenceID": 16, "context": "The ITP field dates back to 1960s [18] and the Automath system by N.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "de Bruijn [10].", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "ITP systems include HOL (Light) [17], Isabelle [41], Mizar [14], Coq [8], and ACL2 [25].", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "ITP systems include HOL (Light) [17], Isabelle [41], Mizar [14], Coq [8], and ACL2 [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "The development of ITP has been intertwined with the development of its cousin field of Automated Theorem Proving (ATP) [33], where proofs of conjectures are attempted fully automatically.", "startOffset": 120, "endOffset": 124}, {"referenceID": 14, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": "Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) [16], the Coq proofs of the Feit-Thompson theorem [13] and Four Color theorem [12], and the verification of most of the Compendium of Continuous Lattices in Mizar [3].", "startOffset": 259, "endOffset": 262}, {"referenceID": 24, "context": "ITP verifications of the seL4 kernel [27] and CompCert compiler [29] show comparable progress in large scale software verification.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "ITP verifications of the seL4 kernel [27] and CompCert compiler [29] show comparable progress in large scale software verification.", "startOffset": 64, "endOffset": 68}, {"referenceID": 35, "context": "Recently the field of Automated Reasoning in Large Theories (ARLT) [39] has developed, including AI/ATP/ITP (AITP) systems called hammers that assist ITP formalization [5].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "Recently the field of Automated Reasoning in Large Theories (ARLT) [39] has developed, including AI/ATP/ITP (AITP) systems called hammers that assist ITP formalization [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 20, "context": "Recent evaluations have proved 40% of all Mizar and Flyspeck theorems fully automatically [22, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 21, "context": "Recent evaluations have proved 40% of all Mizar and Flyspeck theorems fully automatically [22, 23].", "startOffset": 90, "endOffset": 98}, {"referenceID": 4, "context": "However, there is significant room for improvement: with perfect premise selection (a perfect choice of library facts) ATPs can prove at least 56% of Mizar and Flyspeck instead of today\u2019s 40% [5].", "startOffset": 192, "endOffset": 195}, {"referenceID": 30, "context": "11472 as the formal corpus and E prover [34] version 1.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "This version of MML was used for the latest AITP evaluation reported in [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 33, "context": "The formulas have been translated into the TPTP format [37] used by first-order ATPs by the MPTP system [38] (see Figure 1).", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "The formulas have been translated into the TPTP format [37] used by first-order ATPs by the MPTP system [38] (see Figure 1).", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "We can learn from both human proofs and ATP proofs, but previous experiments [28, 22] show that learning only from the ATP proofs is preferable to including human proofs if the set of ATP proofs is sufficiently large.", "startOffset": 77, "endOffset": 85}, {"referenceID": 20, "context": "We can learn from both human proofs and ATP proofs, but previous experiments [28, 22] show that learning only from the ATP proofs is preferable to including human proofs if the set of ATP proofs is sufficiently large.", "startOffset": 77, "endOffset": 85}, {"referenceID": 21, "context": "2%) of the 57,917 theorems an ATP proof was previously found by a combination of manual and learning-based premise selection [23], we use only these ATP proofs for training.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "The 40% success rate from [23] used a portfolio of 14 AITP methods using different learners, ATPs, and numbers of premises.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 170, "endOffset": 173}, {"referenceID": 31, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 194, "endOffset": 198}, {"referenceID": 36, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 222, "endOffset": 226}, {"referenceID": 32, "context": "In natural language processing, deep neural networks have proven useful in language modeling [30], text classification [9], sentence pair scoring [4], dependency parsing [2], sentiment analysis [35], conversation modeling [40], and simple question answering [36].", "startOffset": 258, "endOffset": 262}, {"referenceID": 13, "context": "Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20].", "startOffset": 87, "endOffset": 99}, {"referenceID": 37, "context": "Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20].", "startOffset": 87, "endOffset": 99}, {"referenceID": 18, "context": "Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks [15, 42, 20].", "startOffset": 87, "endOffset": 99}, {"referenceID": 17, "context": "Pure recurrent LSTM [19] and GRU [7] networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "Pure recurrent LSTM [19] and GRU [7] networks.", "startOffset": 33, "endOffset": 36}, {"referenceID": 23, "context": "The neural networks were trained using asynchronous distributed stochastic gradient descent using the Adam optimizer [26] with up to 20 parallel NVIDIA K-80 GPU workers per model.", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "We used the TensorFlow framework [1] and the Keras library [6].", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "We used the TensorFlow framework [1] and the Keras library [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "We used [11] to initialize convolutional and fully connected layers and Polyak averaging with 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "9999 decay to produce the final weights [32].", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24].", "startOffset": 40, "endOffset": 48}, {"referenceID": 21, "context": "The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24].", "startOffset": 40, "endOffset": 48}, {"referenceID": 22, "context": "The baseline uses distance-weighted kNN [21, 23] with handcrafted semantic features [24].", "startOffset": 84, "endOffset": 88}], "year": 2016, "abstractText": "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the handengineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving.", "creator": "LaTeX with hyperref package"}}}