{"id": "1606.04443", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification", "abstract": "we present a general framework for classification of sparse and irregularly - sampled time series. the retention of such time series can result in query uncertainty maintaining the values of the underlying temporal domains, while making the data reliable to query with using standard classification methods that assume medium - dimensional feature spaces. to address these challenges, we propose an access - aware classification framework based on existing special computation layer we refer to as the gaussian process adapter that can connect irregularly sampled time queue data to to any black - box classifier learnable using gradient descent. we analyze how to scale up the required computations based on adapting the structured kernel interpolation framework and the lanczos approximation method, and how to discriminatively train exponential gaussian process adapter in compatible with ample number of simpler end - to - end using backpropagation.", "histories": [["v1", "Tue, 14 Jun 2016 16:31:14 GMT  (84kb,D)", "https://arxiv.org/abs/1606.04443v1", null], ["v2", "Fri, 28 Oct 2016 20:05:02 GMT  (58kb,D)", "http://arxiv.org/abs/1606.04443v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["steven cheng-xian li", "benjamin m marlin"], "accepted": true, "id": "1606.04443"}, "pdf": {"name": "1606.04443.pdf", "metadata": {"source": "CRF", "title": "A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification", "authors": ["Steven Cheng-Xian", "Li Benjamin Marlin"], "emails": ["cxl@cs.umass.edu", "marlin@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we propose a general framework for classification of sparse and irregularly-sampled time series. An irregularly-sampled time series is a sequence of samples with irregular intervals between their observation times. These intervals can be large when the time series are also sparsely sampled. Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21]. Classification in this setting is challenging both because the data cases are not naturally defined in a fixed-dimensional feature space due to irregular sampling and variable numbers of samples, and because there can be substantial uncertainty about the underlying temporal processes due to the sparsity of observations.\nRecently, Li and Marlin [13] introduced the mixture of expected Gaussian kernels (MEG) framework, an uncertainty-aware kernel for classifying sparse and irregularly sampled time series. Classification with MEG kernels is shown to outperform models that ignore uncertainty due to sparse and irregular sampling. On the other hand, various deep learning models including convolutional neural networks [12] have been successfully applied to fields such as computer vision and natural language processing, and have been shown to achieve state-of-the-art results on various tasks. Some of these models have desirable properties for time series classification, but cannot be directly applied to sparse and irregularly sampled time series.\nInspired by the MEG kernel, we propose an uncertainty-aware classification framework that enables learning black-box classification models from sparse and irregularly sampled time series data. This framework is based on the use of a computational layer that we refer to as the Gaussian process (GP) adapter. The GP adapter uses Gaussian process regression to transform the irregular time series data into a uniform representation, allowing sparse and irregularly sampled data to be fed into any black-box classifier learnable using gradient descent while preserving uncertainty. However, the\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n04 44\n3v 2\n[ st\nat .M\nL ]\n2 8\nO ct\nO(n3) time and O(n2) space of exact GP regression makes the GP adapter prohibitively expensive when scaling up to large time series.\nTo address this problem, we show how to speed up the key computation of sampling from a GP posterior based on combining the structured kernel interpolation (SKI) framework that was recently proposed by Wilson and Nickisch [25] with Lanczos methods for approximating matrix functions [3]. Using the proposed sampling algorithm, the GP adapter can run in linear time and space in terms of the length of the time series, and O(m logm) time when m inducing points are used. We also show that GP adapter can be trained end-to-end together with the parameters of the chosen classifier by backpropagation through the iterative Lanczos method. We present results using logistic regression, fully-connected feedforward networks, convolutional neural networks and the MEG kernel. We show that end-to-end discriminative training of the GP adapter outperforms a variety of baselines in terms of classification performance, including models based only on GP mean interpolation, or with GP regression trained separately using marginal likelihood."}, {"heading": "2 Gaussian processes for sparse and irregularly-sampled time series", "text": "Our focus in this paper is on time series classification in the presence of sparse and irregular sampling. In this problem, the data D contain N independent tuples consisting of a time series Si and a label yi. Thus, D = {(S1, y1), . . . , (SN , yN )}. Each time series Si is represented as a list of time points ti = [ti1, . . . , ti|Si|] >, and a list of corresponding values vi = [vi1, . . . , vi|Si|] >. We assume that each time series is observed over a common time interval [0, T ]. However, different time series are not necessarily observed at the same time points (i.e. ti 6= tj in general). This implies that the number of observations in different time series is not necessary the same (i.e. |Si| 6= |Sj | in general). Furthermore, the time intervals between observation within a single time series are not assumed to be uniform.\nLearning in this setting is challenging because the data cases are not naturally defined in a fixeddimensional feature space due to the irregular sampling. This means that commonly used classifiers that take fixed-length feature vectors as input are not applicable. In addition, there can be substantial uncertainty about the underlying temporal processes due to the sparsity of observations.\nTo address these challenges, we build on ideas from the MEG kernel [13] by using GP regression [17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series. We fix a set of reference time points x = [x1, . . . , xd]> and represent a time series S = (t,v) in terms of its posterior marginal distribution at these time points. We use GP regression with a zero-mean GP prior and a covariance function k(\u00b7, \u00b7) parameterized by kernel hyperparameters \u03b7. Let \u03c32 be the independent noise variance of the GP regression model. The GP parameters are \u03b8 = (\u03b7, \u03c32).\nUnder this model, the marginal posterior GP at x is Gaussian distributed with the mean and covariance given by\n\u00b5 = Kx,t(Kt,t + \u03c3 2I)\u22121v, (1)\n\u03a3 = Kx,x \u2212Kx,t(Kt,t + \u03c32I)\u22121Kt,x (2)\nwhere Kx,t denotes the covariance matrix with [Kx,t]ij = k(xi, tj). We note that it takesO(n3+nd) time to exactly compute the posterior mean \u00b5, and O(n3 + n2d+ nd2) time to exactly compute the full posterior covariance matrix \u03a3, where n = |t| and d = |x|."}, {"heading": "3 The GP adapter and uncertainty-aware time series classification", "text": "In this section we describe our framework for time series classification in the presence of sparse and irregular sampling. Our framework enables any black-box classifier learnable by gradient-based methods to be applied to the problem of classifying sparse and irregularly sampled time series."}, {"heading": "3.1 Classification frameworks and the Gaussian process adapter", "text": "In Section 2 we described how we can represent a time series through the marginal posterior it induces under a Gaussian process regression model at any set of reference time points x. By fixing a common\nset of reference time points x for all time series in a data set, every time series can be transformed into a common representation in the form of a multivariate Gaussian N (z|\u00b5,\u03a3;\u03b8) with z being the random vector distributed according to the posterior GP marginalized over the time points x.1 Here we assume that the GP parameters \u03b8 are shared across the entire data set.\nIf the z values were observed, we could simply apply a black-box classifier. A classifier can be generally defined by a mapping function f(z;w) parameterized by w, associated with a loss function `(f(z;w), y) where y is a label value from the output space Y . However, in our case z is a Gaussian random variable, which means `(f(z;w), y) is now itself a random variable given a label y. Therefore, we use the expectation Ez\u223cN (\u00b5,\u03a3;\u03b8) [ `(f(z;w), y) ] as the overall loss between the label y and a time series S given its Gaussian representation N (\u00b5,\u03a3;\u03b8). The learning problem becomes minimizing the expected loss over the entire data set:\nw\u2217,\u03b8\u2217 = argmin w,\u03b8 N\u2211 i=1 Ezi\u223cN (\u00b5i,\u03a3i;\u03b8) [ `(f(zi;w), yi) ] . (3)\nOnce we have the optimal parameters w\u2217 and \u03b8\u2217, we can make predictions on unseen data. In general, given an unseen time series S and its Gaussian representation N (\u00b5,\u03a3;\u03b8\u2217), we can predict its label using (4), although in many cases this can be simplified into a function of f(z;w\u2217) with the expectation taken on or inside of f(z;w\u2217).\ny\u2217 = argmin y\u2208Y\nEz\u223cN (\u00b5,\u03a3;\u03b8\u2217) [ `(f(z;w\u2217), y) ] (4)\nWe name the above approach the Uncertainty-Aware Classification (UAC) framework. Importantly, this framework propagates the uncertainty in the GP posterior induced by each time series all the way through to the loss function. Besides, we call the transformation S 7\u2192 (\u00b5,\u03a3) the Gaussian process adapter, since it provides a uniform representation to connect the raw irregularly sampled time series data to a black-box classifier.\nVariations of the UAC framework can be derived by taking the expectation at various position of f(z;w) where z \u223c N (\u00b5,\u03a3;\u03b8). Taking the expectation at an earlier stage simplifies the computation, but the uncertainty information will be integrated out earlier as well.2 In the extreme case, if the expectation is computed immediately followed by the GP adapter transformation, it is equivalent to using a plug-in estimate \u00b5 for z in the loss function, `(f(Ez\u223cN (\u00b5,\u03a3;\u03b8)[z];w), y) = `(f(\u00b5;w), y). We refer to this as the IMPutation (IMP) framework. The IMP framework discards the uncertainty information completely, which further simplifies the computation. This simplified variation may be useful when the time series are more densely sampled, where the uncertainty is less of a concern.\nIn practice, we can train the model using the UAC objective (3) and predict instead by IMP. In that case, the predictions would be deterministic and can be computed efficiently without drawing samples from the posterior GP as described later in Section 4."}, {"heading": "3.2 Learning with the GP adapter", "text": "In the previous section, we showed that the UAC framework can be trained using (3). In this paper, we use stochastic gradient descent to scalably optimize (3) by updating the model using a single time series at a time, although it can be easily modified for batch or mini-batch updates. From now on, we will focus on the optimization problem minw,\u03b8 Ez\u223cN (\u00b5,\u03a3;\u03b8) [ `(f(z;w), y) ] where \u00b5,\u03a3 are the output of the GP adapter given a time series S = (t,v) and its label y. For many classifiers, the expected loss Ez\u223cN (\u00b5,\u03a3;\u03b8) [ `(f(z;w), y) ] cannot be analytically computed. In such cases, we use the Monte Carlo average to approximate the expected loss:\nEz\u223cN (\u00b5,\u03a3;\u03b8) [ `(f(z;w), y) ] \u2248 1 S S\u2211 s=1 `(f(zs;w), y), where zs \u223c N (\u00b5,\u03a3;\u03b8). (5)\nTo learn the parameters of both the classifier w and the Gaussian process regression model \u03b8 jointly under the expected loss, we need to be able to compute the gradient of the expectation given in (5).\n1 The notation N (\u00b5,\u03a3;\u03b8) explicitly expresses that both \u00b5 and \u03a3 are functions of the GP parameters \u03b8. Besides, they are also functions of S = (t,v) as shown in (1) and (2).\n2 For example, the loss of the expected output of the classifier `(Ez\u223cN (\u00b5,\u03a3;\u03b8)[f(z;w)], y).\nTo achieve this, we reparameterize the Gaussian random variable using the identity z = \u00b5 + R\u03be where \u03be \u223c N (0, I) and R satisfies \u03a3 = RR> [11]. The gradients under this reparameterization are given below, both of which can be approximated using Monte Carlo sampling as in (5). We will focus on efficiently computing the gradient shown in (7) since we assume that the gradient of the base classifier f(z;w) can be computed efficiently.\n\u2202\n\u2202w Ez\u223cN (\u00b5,\u03a3;\u03b8)\n[ `(f(z;w), y) ] = E\u03be\u223cN (0,I)\n[ \u2202\n\u2202w `(f(z;w), y)\n] (6)\n\u2202\n\u2202\u03b8 Ez\u223cN (\u00b5,\u03a3;\u03b8)\n[ `(f(z;w), y) ] = E\u03be\u223cN (0,I) [\u2211 i \u2202`(f(z;w), y) \u2202zi \u2202zi \u2202\u03b8 ] (7)\nThere are several choices for R that satisfy \u03a3 = RR>. One common choice of R is the Cholesky factor, a lower triangular matrix, which can be computed using Cholesky decomposition in O(d3) for a d\u00d7 d covariance matrix \u03a3 [7]. We instead use the symmetric matrix square root R = \u03a31/2. We will show that this particular choice of R leads to an efficient and scalable approximation algorithm in Section 4.2."}, {"heading": "4 Fast sampling from posterior Gaussian processes", "text": "The computation required by the GP adapter is dominated by the time needed to draw samples from the marginal GP posterior using z = \u00b5+ \u03a31/2\u03be. In Section 2 we noted that the time complexity of exactly computing the posterior mean \u00b5 and covariance \u03a3 is O(n3 + nd) and O(n3 + n2d+ nd2), respectively. Once we have both \u00b5 and \u03a3 we still need to compute the square root of \u03a3, which requires an additional O(d3) time to compute exactly. In this section, we show how to efficiently generate samples of z."}, {"heading": "4.1 Structured kernel interpolation for approximating GP posterior means", "text": "The main idea of the structured kernel interpolation (SKI) framework recently proposed by Wilson and Nickisch [25] is to approximate a stationary kernel matrix Ka,b by the approximate kernel K\u0303a,b defined below where u = [u1, . . . , um]> is a collection of evenly-spaced inducing points.\nKa,b \u2248 K\u0303a,b = WaKu,uW>b . (8)\nLetting p = |a| and q = |b|, Wa \u2208 Rp\u00d7m is a sparse interpolation matrix where each row contains only a small number of non-zero entries. We use local cubic convolution interpolation (cubic interpolation for short) [10] as suggested in Wilson and Nickisch [25]. Each row of the interpolation matrices Wa,Wb has at most four non-zero entries. Wilson and Nickisch [25] showed that when the kernel is locally smooth (under the resolution of u), cubic interpolation results in accurate approximation. This can be justified as follows: with cubic interpolation, the SKI kernel is essentially the two-dimensional cubic interpolation of Ka,b using the exact regularly spaced samples stored in Ku,u, which corresponds to classical bicubic convolution. In fact, we can show that K\u0303a,b asymptotically converges to Ka,b as m increases by following the derivation in Keys [10].\nPlugging the SKI kernel into (1), the posterior GP mean evaluated at x can be approximated by\n\u00b5 = Kx,t ( Kt,t + \u03c3 2I )\u22121 v \u2248WxKu,uW>t ( WtK \u22121 u,uW > t + \u03c3 2I )\u22121 v. (9)\nThe inducing points u are chosen to be evenly-spaced because Ku,u forms a symmetric Toeplitz matrix under a stationary covariance function. A symmetric Toeplitz matrix can be embedded into a circulant matrix to perform matrix vector multiplication using fast Fourier transforms [7].\nFurther, one can use the conjugate gradient method to solve for (WtK\u22121u,uW > t +\u03c3 2I)\u22121v which only involves computing the matrix-vector product (WtK\u22121u,uW > t + \u03c3\n2I)v. In practice, the conjugate gradient method converges within only a few iterations. Therefore, approximating the posterior mean \u00b5 using SKI takes onlyO(n+d+m logm) time to compute. In addition, since a symmetric Toeplitz matrix Ku,u can be uniquely characterized by its first column, and Wt can be stored as a sparse matrix, approximating \u00b5 requires only O(n+ d+m) space.\nAlgorithm 1: Lanczos method for approximating \u03a31/2\u03be Input: covariance matrix \u03a3, dimension of the Krylov subspace k, random vector \u03be \u03b21 = 0 and d0 = 0 d1 = \u03be/\u2016\u03be\u2016 for j = 1 to k do\nd = \u03a3dj \u2212 \u03b2jdj\u22121 \u03b1j = d > j d d = d\u2212 \u03b1jdj \u03b2j+1 = \u2016d\u2016 dj+1 = d/\u03b2j+1\nD = [d1, . . . ,dk] H = tridiagonal(\u03b2,\u03b1,\u03b2) return \u2016\u03be\u2016DH1/2e1 // e1 = [1, 0, . . . , 0]>\nH = tridiagonal(\u03b2,\u03b1,\u03b2) =  \u03b11 \u03b22 \u03b22 \u03b12 \u03b23 \u03b23 \u03b13 . . .\n. . . . . . \u03b2k \u03b2k \u03b1k\n"}, {"heading": "4.2 The Lanczos method for covariance square root-vector products", "text": "With the SKI techniques, although we can efficiently approximate the posterior mean \u00b5, computing \u03a3\n1/2\u03be is still challenging. If computed exactly, it takes O(n3 + n2d+ nd2) time to compute \u03a3 and O(d3) time to take the square root. To overcome the bottleneck, we apply the SKI kernel to the Lanczos method, one of the Krylov subspace approximation methods, to speed up the computation of \u03a31/2\u03be as shown in Algorithm 1. The advantage of the Lanczos method is that neither \u03a3 nor \u03a31/2 needs to be computed explicitly. Like the conjugate gradient method, another example of the Krylov subspace method, it only requires the computation of matrix-vector products with \u03a3 as the matrix.\nThe idea of the Lanczos method is to approximate \u03a31/2\u03be in the Krylov subspace Kk(\u03a3, \u03be) = span{\u03be,\u03a3\u03be, . . . ,\u03a3k\u22121\u03be}. The iteration in Algorithm 1, usually referred to the Lanczos process, essentially performs the Gram-Schmidt process to transform the basis {\u03be,\u03a3\u03be, . . . ,\u03a3k\u22121\u03be} into an orthonormal basis {d1, . . . ,dk} for the subspace Kk(\u03a3, \u03be).\nThe optimal approximation of \u03a31/2\u03be in the Krylov subspace Kk(\u03a3, \u03be) that minimizes the `2-norm of the error is the orthogonal projection of \u03a31/2\u03be onto Kk(\u03a3, \u03be) as y\u2217 = DD>\u03a3\n1/2\u03be. Since we choose d1 = \u03be/\u2016\u03be\u2016, the optimal projection can be written as y\u2217 = \u2016\u03be\u2016DD>\u03a3\n1/2De1 where e1 = [1, 0, . . . , 0] > is the first column of the identify matrix.\nOne can show that the tridiagonal matrix H defined in Algorithm 1 satisfies D>\u03a3D = H [20]. Also, we have D>\u03a31/2D \u2248 (D>\u03a3D)1/2 since the eigenvalues of H approximate the extremal eigenvalues of \u03a3 [19]. Therefore we have y\u2217 = \u2016\u03be\u2016DD>\u03a31/2De1 \u2248 \u2016\u03be\u2016DH1/2e1. The error bound of the Lanczos method is analyzed in Ilic\u0301 et al. [9]. Alternatively one can show that the Lanczos approximation converges superlinearly [16]. In practice, for a d\u00d7 d covariance matrix \u03a3, the approximation is sufficient for our sampling purpose with k d. As H is now a k\u00d7 k matrix, we can use any standard method to compute its square root in O(k3) time [2], which is considered O(1) when k is chosen to be a small constant. Now the computation of the Lanczos method for approximating \u03a31/2\u03be is dominated by the matrix-vector product \u03a3d during the Lanczos process.\nHere we apply the SKI kernel trick again to efficiently approximate \u03a3d by\n\u03a3d \u2248WxKu,uW>x d\u2212WxKu,uW>t ( WtKu,uW > t + \u03c3 2I )\u22121 WtKu,uW > x d. (10)\nSimilar to the posterior mean, \u03a3d can be approximated inO(n+d+m logm) time and linear space. Therefore, for k = O(1) basis vectors, the entire Algorithm 1 takes O(n+ d+m logm) time and O(n+ d+m) space, which is also the complexity to draw a sample from the posterior GP. To reduce the variance when estimating the expected loss (5), we can draw multiple samples from the posterior GP: {\u03a31/2\u03bes}s=1,...,S where \u03bes \u223c N (0, I). Since all of the samples are associated with the same covariance matrix \u03a3, we can use the block Lanczos process [8], an extension to the single-vector Lanczos method presented in Algorithm 1, to simultaneously approximate \u03a31/2\u039e for all S random\nvectors \u039e = [\u03be1, . . . , \u03beS ]. Similarly, during the block Lanczos process, we use the block conjugate gradient method [6, 5] to simultaneously solve the linear equation (WtKu,uW>t + \u03c3\n2I)\u22121\u03b1 for multiple \u03b1."}, {"heading": "5 End-to-end learning with the GP adapter", "text": "The most common way to train GP parameters is through maximizing the marginal likelihood [17]\nlog p(v|t,\u03b8) = \u22121 2 v> ( Kt,t + \u03c3 2I )\u22121 v \u2212 1 2 log \u2223\u2223Kt,t + \u03c32I\u2223\u2223\u2212 n 2 log 2\u03c0. (11)\nIf we follow this criterion, training the UAC framework becomes a two-stage procedure: first we learn GP parameters by maximizing the marginal likelihood. We then compute \u00b5 and \u03a3 given each time series S and the learned GP parameters \u03b8\u2217. Both \u00b5 and \u03a3 are then fixed and used to train the classifier using (6).\nIn this section, we describe how to instead train the GP parameters discriminatively end-to-end using backpropagation. As mentioned in Section 3, we train the UAC framework by jointly optimizing the GP parameters \u03b8 and the parameters of the classifier w according to (6) and (7).\nThe most challenging part in (7) is to compute \u2202z = \u2202\u00b5 + \u2202(\u03a31/2\u03be).3 For \u2202\u00b5, we can derive the gradient of the approximating posterior mean (9) as given in Appendix A. Note that the gradient \u2202\u00b5 can be approximated efficiently by repeatedly applying fast Fourier transforms and the conjugate gradient method in the same time and space complexity as computing (9).\nOn the other hand, \u2202(\u03a31/2\u03be) can be approximated by backpropagating through the Lanczos method described in Algorithm 1. To carry out backpropagation, all operations in the Lanczos method must be differentiable. For the approximation of \u03a3d during the Lanczos process, we can similarly compute the gradient of (10) efficiently using the SKI techniques as in computing \u2202\u00b5 (see Appendix A).\nThe gradient \u2202H1/2 for the last step of Algorithm 1 can be derived as follows. From H = H1/2H1/2, we have \u2202H = (\u2202H1/2)H1/2 + H1/2(\u2202H1/2). This is known as the Sylvester equation, which has the form of AX + XB = C where A,B,C are matrices and X is the unknown matrix to solve for. We can compute the gradient \u2202H1/2 by solving the Sylvester equation using the Bartels-Stewart algorithm [1] in O(k3) time for a k \u00d7 k matrix H, which is considered O(1) for a small constant k. Overall, training the GP adapter using stochastic optimization with the aforementioned approach takes O(n+ d+m logm) time and O(n+ d+m) space for m inducing points, n observations in the time series, and d features generated by the GP adapter."}, {"heading": "6 Related work", "text": "The recently proposed mixtures of expected Gaussian kernels (MEG) [13] for classification of irregular time series is probably the closest work to ours. The random feature representation of the MEG kernel is in the form of \u221a 2/m Ez\u223cN (\u00b5,\u03a3) [ cos(w>i z + bi) ] , which the algorithm described in Section 4 can be applied to directly. However, by exploiting the spectral property of Gaussian kernels, the expected random feature of the MEG kernel is shown to be analytically computable by\u221a 2/m exp(\u2212w>i \u03a3wi/2) cos(w>i \u00b5+ bi). With the SKI techniques, we can efficiently approximate both w>i \u03a3wi and w > i \u00b5 in the same time and space complexity as the GP adapter. Moreover, the random features of the MEG kernel can be viewed as a stochastic layer in the classification network, with no trainable parameters. All {wi, bi}i=1,...,m are randomly initialized once in the beginning and associated with the output of the GP adapter in a nonlinear way described above.\nMoreover, the MEG kernel classification is originally a two-stage method: one first estimates the GP parameters by maximizing the marginal likelihood and then uses the optimized GP parameters to compute the MEG kernel for classification. Since the random feature is differentiable, with the approximation of \u2202\u00b5 and \u2202(\u03a3d) described in Section 5, we can form a similar classification network that can be efficiently trained end-to-end using the GP adapter. In Section 7.2, we will show that training the MEG kernel end-to-end leads to better classification performance.\n3 For brevity, we drop 1/\u2202\u03b8 from the gradient notation in this section."}, {"heading": "7 Experiments", "text": "In this section, we present experiments and results exploring several facets of the GP adapter framework including the quality of the approximations and the classification performance of the framework when combined with different base classifiers."}, {"heading": "7.1 Quality of GP sampling approximations", "text": "The key to scalable learning with the GP adapter relies on both fast and accurate approximation for drawing samples from the posterior GP. To assess the approximation quality, we first generate a synthetic sparse and irregularly-sampled time series S by sampling from a zero-mean Gaussian process at random time points. We use the squared exponential kernel k(ti, tj) = a exp(\u2212b(ti\u2212 tj)2) with randomly chosen hyperparameters. We then infer \u00b5 and \u03a3 at some reference x given S. Let z\u0303 denote our approximation of z = \u00b5+ \u03a31/2\u03be. In this experiment, we set the output size z to be |S|, that is, d = n. We evaluate the approximation quality by assessing the error \u2016z\u0303\u2212 z\u2016 computed with a fixed random vector \u03be.\nThe leftmost plot in Figure 1 shows the approximation error under different numbers of inducing points m with k = 10 Lanczos iterations. The middle plot compares the approximation error as the number of Lanczos iterations k varies, with m = 256 inducing points. These two plots show that the approximation error drops as more inducing points and Lanczos iterations are used. In both plots, the three lines correspond to different sizes for z: 1000 (bottom line), 2000 (middle line), 3000 (top line). The separation between the curves is due to the fact that the errors are compared under the same number of inducing points. Longer time series leads to lower resolution of the inducing points and hence the higher approximation error.\nNote that the approximation error comes from both the cubic interpolation and the Lanczos method. Therefore, to achieve a certain normalized approximation error across different data sizes, we should simultaneously use more inducing points and Lanczos iterations as the data grows. In practice, we find that k \u2265 3 is sufficient for estimating the expected loss for classification. The rightmost plot in Figure 1 compares the time to draw a sample using exact computation versus the approximation method described in Section 4 (exact and Lanczos in the figure). We also compare the time to compute the gradient with respect to the GP parameters by both the exact method and the proposed approximation (exact BP and Lanczos BP in the figure) because this is the actual computation carried out during training. In this part of the experiment, we use k = 10 and m = 256. The plot shows that Lanczos approximation with the SKI kernel yields speed-ups of between 1 and 3 orders of magnitude. Interestingly, for the exact approach, the time for computing the gradient roughly doubles the time of drawing samples. (Note that time is plotted in log scale.) This is because computing gradients requires both forward and backward propagation, whereas drawing samples corresponds to only the forward pass. Both the forward and backward passes take roughly the same computation in the exact case. However, the gap is relatively larger for the approximation approach due to the recursive relationship of the variables in the Lanczos process. In particular, dj is defined recursively in terms of all of d1, . . . ,dj\u22121, which makes the backpropagation computation more complicated than the forward pass."}, {"heading": "7.2 Classification with GP adapter", "text": "In this section, we evaluate the performance of classifying sparse and irregularly-sampled time series using the UAC framework. We test the framework on the uWave data set,4 a collection of gesture samples categorized into eight gesture patterns [14]. The data set has been split into 3582 training instances and 896 test instances. Each time series contains 945 fully observed samples. Following the data preparation procedure in the MEG kernel work [13], we randomly sample 10% of the observations from each time series to simulate the sparse and irregular sampling scenario. In this experiment, we use the squared exponential covariance function k(ti, tj) = a exp(\u2212b(ti \u2212 tj)2) for a, b > 0. Together with the independent noise parameter \u03c32 > 0, the GP parameters are {a, b, \u03c32}. To bypass the positive constraints on the GP parameters, we reparameterize them by {\u03b1, \u03b2, \u03b3} such that a = e\u03b1, b = e\u03b2 , and \u03c32 = e\u03b3 .\nTo demonstrate that the GP adapter is capable of working with various classifiers, we use the UAC framework to train three different classifiers: a multi-class logistic regression (LogReg), a fullyconnected feedforward network (MLP), and a convolutional neural network (ConvNet). The detailed architecture of each model is described in Appendix C.\nWe use m = 256 inducing points, d = 254 features output by the GP adapter, k = 5 Lanczos iterations, and S = 10 samples. We split the training set into two partitions: 70% for training and 30% for validation. We jointly train the classifier with the GP adapter using stochastic gradient descent with Nesterov momentum. We apply early stopping based on the validation set. We also compare to classification with the MEG kernel implemented using our GP adapter as described in Section 6. We use 1000 random features trained with multi-class logistic regression.\nTable 1 shows that among all three classifiers, training GP parameters discriminatively always leads to better accuracy than maximizing the marginal likelihood. This claim also holds for the results using the MEG kernel. Further, taking the uncertainty into account by sampling from the posterior GP always outperforms training using only the posterior means. Finally, we can also see that the classification accuracy improves as the model gets deeper."}, {"heading": "8 Conclusions and future work", "text": "We have presented a general framework for classifying sparse and irregularly-sampled time series and have shown how to scale up the required computations using a new approach to generating approximate samples. We have validated the approximation quality, the computational speed-ups, and the benefit of the proposed approach relative to existing baselines.\nThere are many promising directions for future work including investigating more complicated covariance functions like the spectral mixture kernel [24], different classifiers including the encoder LSTM [23], and extending the framework to multi-dimensional time series and GPs with multidimensional index sets (e.g., for spatial data). Lastly, the GP adapter can also be applied to other problems such as dimensionality reduction by combining it with an autoencoder."}, {"heading": "Acknowledgements", "text": "This work was supported by the National Science Foundation under Grant No. 1350522.\n4 The data set UWaveGestureLibraryAll is available at http://timeseriesclassification.com."}, {"heading": "A Gradients for GP approximation", "text": "A.1 Gradients of the approximate posterior GP covariance-vector product\nThroughout we denote the independent noise variance \u03c32 as \u03c1 for clarity. Let \u03a3\u0303 be the approximate posterior covariance derived by the SKI kernel, and \u03b8 be one of the GP hyperparameters. For any vector d, the gradient \u2202\u03a3\u0303d/\u2202\u03b8 is given below. Note that during the Lanczos process, d is a function of \u03b8, which should be properly handled in backpropagation.\n\u2202\n\u2202\u03b8\n{ WxKu,uW > x d\u2212WxKu,uW>t ( WtKu,uW > t + \u03c1I )\u22121 WtKu,uW > x d }\n= Wx \u2202Ku,u \u2202\u03b8 W>x d\n\u2212Wx \u2202Ku,u \u2202\u03b8\nW>t ( WtKu,uW > t + \u03c1I )\u22121 WtKu,uW > x d\n\u2212WxKu,uW>t ( WtKu,uW > t + \u03c1I )\u22121 Wt\n\u2202Ku,u \u2202\u03b8 W>x d\n+ WxKu,uW > t ( WtKu,uW > t + \u03c1I )\u22121 Wt\n\u2202Ku,u \u2202\u03b8 W>t( WtKu,uW > t + \u03c1I )\u22121 WtKu,uW > x d.\nTo reduce redundant computations, we introduce the following variables:\n\u03b1 = W>x d,\n\u03b2 = \u2202Ku,u \u2202\u03b8 \u03b1,\n\u03b3 = Ku,u\u03b1, \u03b4 = ( WtKu,uW > t + \u03c1I )\u22121 Wt\u03b3,\n\u03b6 = \u2202Ku,u \u2202\u03b8 W>t \u03b4,\n\u03b7 = Ku,uW > t ( WtKu,uW > t + \u03c1I )\u22121 Wt (\u03b6 \u2212 \u03b2) .\nThe gradient with respect to \u03b8 is therefore \u2202\u03a3\u0303d/\u2202\u03b8 = Wx (\u03b2 \u2212 \u03b6 + \u03b7).\nThe gradient \u2202\u03a3\u0303d/\u2202\u03c1 with respect to the noise variance \u03c1 is given by\n\u2202\n\u2202\u03c1\n{ WxKu,uW > x \u2212WxKu,uW>t ( WtKu,uW > t + \u03c1I )\u22121 WtKu,uW > x d }\n= WxKu,uW > t ( WtKu,uW > t + \u03c1I )\u22121 ( WtKu,uW > t + \u03c1I )\u22121 WtKu,uW > x d\n= WxKu,uW > t ( WtKu,uW > t + \u03c1I )\u22121 \u03b4.\nA.2 Gradients of the approximate posterior GP mean\nLet \u00b5\u0303 denote the approximate posterior mean derived by the SKI kernel. The gradient \u2202\u00b5\u0303/\u2202\u03b8 with respect to the GP hyperparameter \u03b8 is given by\n\u2202\n\u2202\u03b8 WxKu,uW\n> t ( WtKu,uW > t + \u03c1I )\u22121 v\n= Wx \u2202Ku,u \u2202\u03b8\nW>t ( WtKu,uW > t + \u03c1I )\u22121 v\n\u2212WxKu,uW>t ( WtKu,uW > t + \u03c1I )\u22121 Wt\n\u2202Ku,u \u2202\u03b8\nW>t ( WtKu,uW > t + \u03c1I )\u22121 v.\nTo reduce redundant computations, we introduce the following variables: \u03b1 = ( WtKu,uW > t + \u03c1I )\u22121 v,\n\u03b2 = \u2202Ku,u \u2202\u03b8 W>t \u03b1,\n\u03b3 = Ku,uW > t ( WtKu,uW > t + \u03c1I )\u22121 Wt\u03b2.\nThe gradient with respect to \u03b8 is therefore \u2202\u00b5\u0303/\u2202\u03b8 = Wx (\u03b2 \u2212 \u03b3). The gradient \u2202\u00b5\u0303/\u2202\u03c1 with respect to the noise variance \u03c1 is given by\n\u2202\n\u2202\u03c1 WxKu,uW\n> t ( WtKu,uW > t + \u03c1I )\u22121 v\n= \u2212WxKu,uW>t ( WtKu,uW > t + \u03c1I )\u22121 ( WtKu,uW > t + \u03c1I )\u22121 v\n= \u2212WxKu,uW>t ( WtKu,uW > t + \u03c1I )\u22121 \u03b1."}, {"heading": "B Cubic interpolation in backpropagation", "text": "The choice of cubic convolution interpolation proposed by Keys [10] is preferable over other interpolation methods such as spline interpolation when training the GP parameters. If spline interpolation is used to construct the SKI kernel K\u0303a,b, the interpolation matrix Wa depends not only on a and u but also on the kernel Ku,u, which depends on the GP parameters \u03b8. As a result, the gradient \u2202Wa/\u2202\u03b8 needs to be computed and thus introduces a huge overhead in backpropagation. On the other hand, the interpolation matrix based on the cubic convolution interpolation depends only on a and u, which are fixed once the data are given. Therefore, with cubic convolution interpolation, both Wa and Wb are constant matrices throughout the entire training process."}, {"heading": "C Architectures used in the experiment", "text": "The architecture of each classifier compared in Section 7.2 are described as follows. The fullyconnected network consists of two fully-connected layers, each of which contains 256 units. The convolutional network contains a total of five layers: the first and the third layer are both onedimensional convolutional layers with four filters of size 5. The second and the fourth layer are one-dimensional max-pooling layers of size 2. The last layer is a fully-connected layer with 256 units. We apply rectified linear activation to all of the convolutional and fully-connected layers. Each classifier takes d = 254 input features produced by the GP adapter."}], "references": [{"title": "Solution of the matrix equation AX +XB = C", "author": ["Richard H. Bartels", "GW Stewart"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1972}, {"title": "A Schur method for the square root of a matrix", "author": ["\u00c5ke Bj\u00f6rck", "Sven Hammarling"], "venue": "Linear algebra and its applications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1983}, {"title": "Preconditioned krylov subspace methods for sampling multivariate gaussian distributions", "author": ["Edmond Chow", "Yousef Saad"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Population time series: process variability, observation errors, missing values, lags, and hidden states", "author": ["J.S. Clark", "O.N. Bj\u00f8rnstad"], "venue": "Ecology, 85(11):3140\u20133150,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Retooling the method of block conjugate gradients", "author": ["Augustin A Dubrulle"], "venue": "Electronic Transactions on Numerical Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "A block conjugate gradient method applied to linear systems with multiple right-hand sides", "author": ["YT Feng", "DRJ Owen", "D Peri\u0107"], "venue": "Computer methods in applied mechanics and engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "The block Lanczos method for computing eigenvalues", "author": ["Gene Howard Golub", "Richard Underwood"], "venue": "Mathematical software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1977}, {"title": "A restarted Lanczos approximation to functions of a symmetric matrix", "author": ["M Ili\u0107", "Ian W Turner", "Daniel P Simpson"], "venue": "IMA journal of numerical analysis, page drp003,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Cubic convolution interpolation for digital image processing", "author": ["Robert G Keys"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1981}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "Proceedings of the 2nd International Conference on Learning Representations (ICLR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"], "venue": "In Proceedings of Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Classification of sparse and irregularly sampled time series with mixtures of expected Gaussian kernels and random features", "author": ["Steven Cheng-Xian Li", "Benjmain M. Marlin"], "venue": "In 31st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "uwave: Accelerometer-based personalized gesture recognition and its applications", "author": ["Jiayang Liu", "Lin Zhong", "Jehan Wickramasuriya", "Venu Vasudevan"], "venue": "Pervasive and Mobile Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering models", "author": ["Benjamin M. Marlin", "David C. Kale", "Robinder G. Khemani", "Randall C. Wetzel"], "venue": "In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "The symmetric eigenvalue problem, volume", "author": ["Beresford N Parlett"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "Gaussian processes for machine learning", "author": ["Carl Edward Rasmussen"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The lomb-scargle periodogram in biological rhythm research: analysis of incomplete and unequally spaced time-series", "author": ["T. Ruf"], "venue": "Biological Rhythm Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "On the rates of convergence of the Lanczos and the block-Lanczos methods", "author": ["Yousef Saad"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1980}, {"title": "Iterative methods for sparse linear systems", "author": ["Yousef Saad"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Studies in astronomical time series analysis. ii-statistical aspects of spectral analysis of unevenly spaced data", "author": ["Jeffrey D Scargle"], "venue": "The Astrophysical Journal,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1982}, {"title": "Spectrum: Spectral analysis of unevenly spaced paleoclimatic time series", "author": ["M. Schulz", "K. Stattegger"], "venue": "Computers & Geosciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Gaussian process kernels for pattern discovery and extrapolation", "author": ["Andrew Gordon Wilson", "Ryan Prescott Adams"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 91, "endOffset": 94}, {"referenceID": 16, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Such time series data are studied in various areas including climate science [22], ecology [4], biology [18], medicine [15] and astronomy [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "Recently, Li and Marlin [13] introduced the mixture of expected Gaussian kernels (MEG) framework, an uncertainty-aware kernel for classifying sparse and irregularly sampled time series.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "On the other hand, various deep learning models including convolutional neural networks [12] have been successfully applied to fields such as computer vision and natural language processing, and have been shown to achieve state-of-the-art results on various tasks.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "To address this problem, we show how to speed up the key computation of sampling from a GP posterior based on combining the structured kernel interpolation (SKI) framework that was recently proposed by Wilson and Nickisch [25] with Lanczos methods for approximating matrix functions [3].", "startOffset": 283, "endOffset": 286}, {"referenceID": 11, "context": "To address these challenges, we build on ideas from the MEG kernel [13] by using GP regression [17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "To address these challenges, we build on ideas from the MEG kernel [13] by using GP regression [17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "To achieve this, we reparameterize the Gaussian random variable using the identity z = \u03bc + R\u03be where \u03be \u223c N (0, I) and R satisfies \u03a3 = RR> [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "We use local cubic convolution interpolation (cubic interpolation for short) [10] as suggested in Wilson and Nickisch [25].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In fact, we can show that K\u0303a,b asymptotically converges to Ka,b as m increases by following the derivation in Keys [10].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "One can show that the tridiagonal matrix H defined in Algorithm 1 satisfies D>\u03a3D = H [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "Also, we have D>\u03a31D \u2248 (D>\u03a3D)/2 since the eigenvalues of H approximate the extremal eigenvalues of \u03a3 [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 7, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Alternatively one can show that the Lanczos approximation converges superlinearly [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "As H is now a k\u00d7 k matrix, we can use any standard method to compute its square root in O(k) time [2], which is considered O(1) when k is chosen to be a small constant.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "Since all of the samples are associated with the same covariance matrix \u03a3, we can use the block Lanczos process [8], an extension to the single-vector Lanczos method presented in Algorithm 1, to simultaneously approximate \u03a31\u039e for all S random", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Similarly, during the block Lanczos process, we use the block conjugate gradient method [6, 5] to simultaneously solve the linear equation (WtKu,uW t + \u03c3 2I)\u22121\u03b1 for multiple \u03b1.", "startOffset": 88, "endOffset": 94}, {"referenceID": 4, "context": "Similarly, during the block Lanczos process, we use the block conjugate gradient method [6, 5] to simultaneously solve the linear equation (WtKu,uW t + \u03c3 2I)\u22121\u03b1 for multiple \u03b1.", "startOffset": 88, "endOffset": 94}, {"referenceID": 15, "context": "The most common way to train GP parameters is through maximizing the marginal likelihood [17] log p(v|t,\u03b8) = \u2212 2 v> ( Kt,t + \u03c3 I )\u22121 v \u2212 1 2 log \u2223\u2223Kt,t + \u03c32I\u2223\u2223\u2212 n 2 log 2\u03c0.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "We can compute the gradient \u2202H/2 by solving the Sylvester equation using the Bartels-Stewart algorithm [1] in O(k) time for a k \u00d7 k matrix H, which is considered O(1) for a small constant k.", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "The recently proposed mixtures of expected Gaussian kernels (MEG) [13] for classification of irregular time series is probably the closest work to ours.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "We test the framework on the uWave data set,4 a collection of gesture samples categorized into eight gesture patterns [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "Following the data preparation procedure in the MEG kernel work [13], we randomly sample 10% of the observations from each time series to simulate the sparse and irregular sampling scenario.", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "There are many promising directions for future work including investigating more complicated covariance functions like the spectral mixture kernel [24], different classifiers including the encoder LSTM [23], and extending the framework to multi-dimensional time series and GPs with multidimensional index sets (e.", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "There are many promising directions for future work including investigating more complicated covariance functions like the spectral mixture kernel [24], different classifiers including the encoder LSTM [23], and extending the framework to multi-dimensional time series and GPs with multidimensional index sets (e.", "startOffset": 202, "endOffset": 206}], "year": 2016, "abstractText": "We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixeddimensional feature spaces. To address these challenges, we propose an uncertaintyaware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.", "creator": "LaTeX with hyperref package"}}}