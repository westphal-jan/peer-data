{"id": "1606.04686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Natural Language Generation as Planning under Uncertainty Using Reinforcement Learning", "abstract": "we present and evaluate a new model for natural cue production ( nlg ) in spoken dialogue systems, based on statistical effects, given noisy feedback from the current evolving context ( e. g. a user and a surface realiser ). historians report its use in classic standard behavioral problem : how to present information ( in this case a set of search results ) to users, given the complex face - offs between utterance length, amount potential error conveyed, specific cognitive load. we set these tell - offs by analysing semantic match data. we frequently train a nlg pol - icy using reinforcement learning ( rl ), which shares its behaviour allowing noisy feed - back from the current generation context. this policy is compared to several base - lines derived from previous work in this area. the learned policy significantly out - performs all the prior approaches.", "histories": [["v1", "Wed, 15 Jun 2016 09:05:56 GMT  (55kb,D)", "http://arxiv.org/abs/1606.04686v1", "published EACL 2009"]], "COMMENTS": "published EACL 2009", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["verena rieser", "oliver lemon"], "accepted": false, "id": "1606.04686"}, "pdf": {"name": "1606.04686.pdf", "metadata": {"source": "CRF", "title": "Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems", "authors": ["Verena Rieser", "Oliver Lemon"], "emails": ["vrieser@inf.ed.ac.uk", "olemon@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Natural language allows us to achieve the same communicative goal (\u201cwhat to say\u201d) using many different expressions (\u201chow to say it\u201d). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realised as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007).\nPrevious work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006).\nIn extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. (Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and \u201cconversation as action under uncertainty\u201d (Paek and Horvitz, 2000). In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping. Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user. There is therefore an optimisation problem to be solved. Moreover, the user judgements or next (most likely) action after each NLG action are unpredictable, and the behaviour of the surface realiser may also be variable (see Section 6.2).\nNLG could therefore fruitfully be approached as a sequential statistical planning task, where there are trade-offs and decisions to make, such as whether to choose another NLG action (and which one to choose) or to instead stop generating. Reinforcement Learning (RL) allows us to optimise such trade-offs in the presence of uncertainty, i.e. the chances of achieving a better state, while engaging in the risk of choosing another action.\nIn this paper we present and evaluate a new model for NLG in Spoken Dialogue Systems as planning under uncertainty. In Section 2 we argue for applying RL to NLG problems and explain the overall framework. In Section 3 we discuss challenges for NLG for Information Presentation. In Section 4 we present results from our analysis of the MATCH corpus (Walker et al., 2004). In Section 5 we present a detailed example of our proposed NLG method. In Section 6 we report on experimental results using this framework for exploring Information Presentation policies. In Section 7 we conclude and discuss future directions.\nar X\niv :1\n60 6.\n04 68\n6v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n16"}, {"heading": "2 NLG as planning under uncertainty", "text": "We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning.\nNLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multi-modal interaction.\nMost SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce additional noise, which higher level NLG decisions will need to react to. In our framework, the NLG component must achieve a high-level Communicative Goal from the Dialogue Manager (e.g. to present a number of items) through planning a sequence of lowerlevel generation steps or actions, for example first to summarise all the items and then to recommend the highest ranking one. Each such action has unpredictable effects due to the stochastic realizer. For example the realizer might employ 6 attributes when recommending item i4, but it might use only 2 (e.g. price and cuisine for restaurants), depending on its own processing constraints (see e.g. the realizer used to collect the MATCH project data). Likewise, the user may be likely to choose an item after hearing a summary, or they may wish to hear more. Generating appropriate language in context (e.g. attributes presented so far) thus has the following important features in general:\n\u2022 NLG is goal driven behaviour\n\u2022 NLG must plan a sequence of actions\n\u2022 each action changes the environment state or context\n\u2022 the effect of each action is uncertain.\nThese facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such\nas classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998)."}, {"heading": "3 The Information Presentation Problem", "text": "We will tackle the well-studied problem of Information Presentation in NLG to show the benefits of this approach. The task here is to find the best way to present a set of search results to a user (e.g. some restaurants meeting a certain set of constraints). This is a task common to much prior work in NLG, e.g. (Walker et al., 2004; Demberg and Moore, 2006; Polifroni and Walker, 2008).\nIn this problem, there there are many decisions available for exploration. For instance, which presentation strategy to apply (NLG strategy selection), how many attributes of each item to present (attribute selection), how to rank the items and attributes according to different models of user preferences (attribute ordering), how many (specific) items to tell them about (conciseness), how many sentences to use when doing so (syntactic planning), and which words to use (lexical choice) etc. All these parameters (and potentially many more) can be varied, and ideally, jointly optimised based on user judgements.\nWe had two corpora available to study some of the regions of this decision space. We utilise the MATCH corpus (Walker et al., 2004) to extract an evaluation function (also known as \u201creward function\u201d) for RL. Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer. Both corpora contain data from \u201coverhearer\u201d experiments targeted to Information Presentation in dialogues in the restaurant domain. While we are ultimately interested in how hearers engaged in dialogues judge different Information Presentations, results from overhearers are still directly relevant to the task."}, {"heading": "4 MATCH corpus analysis", "text": "The MATCH project made two data sets available, see (Stent et al., 2002) and (Whittaker et al., 2003), which we combine to define an evaluation function for different Information Presentation strategies.\nThe first data set, see (Stent et al., 2002), comprises 1024 ratings by 16 subjects (where we only use the speech-based half, n = 512) on the following presentation strategies: RECOMMEND, COMPARE, SUMMARY. These strategies are realised using templates as in Table 2, and varying numbers of attributes. In this study the users rate the individual presentation strategies as significantly different (F (2) = 1361, p < .001). We find that SUMMARY is rated significantly worse (p = .05 with Bonferroni correction) than RECOMMEND and COMPARE, which are rated as equally good.\nThis suggests that one should never generate a SUMMARY. However, SUMMARY has different qualities from COMPARE and RECOMMEND, as it gives users a general overview of the domain, and probably helps the user to feel more confident when choosing an item, especially when they are unfamiliar with the domain, as shown by (Polifroni and Walker, 2008).\nIn order to further describe the strategies, we extracted different surface features as present in the data (e.g. number of attributes realised, number of sentences, number of words, number of database items talked about, etc.) and performed a stepwise linear regression to find the features which were important to the overhearers (following the PARADISE framework (Walker et al., 2000)). We discovered a trade-off between the length of the utterance (#sentence) and the number of attributes realised (#attr), i.e. its informativeness, where overhearers like to hear as many attributes as possible in the most concise way, as indicated by the regression model shown in Equation 1 (R2 =\n.34). 1\nscore = .775\u00d7#attr + (\u2212.301)\u00d7#sentence; (1)\nThe second MATCH data set, see (Whittaker et al., 2003), comprises 1224 ratings by 17 subjects on the NLG strategies RECOMMEND and COMPARE. The strategies realise varying numbers of attributes according to different \u201cconciseness\u201d values: concise (1 or 2 attributes), average (3 or 4), and verbose (4, 5, or 6). Overhearers rate all conciseness levels as significantly different (F (2) = 198.3, p < .001), with verbose rated highest and concise rated lowest, supporting our findings in the first data set. However, the relation between number of attributes and user ratings is not strictly linear: ratings drop for #attr = 6. This suggests that there is an upper limit on how many attributes users like to hear. We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007). We therefore include \u201ccognitive load\u201d as a variable when training the policy (see Section 6).\nIn addition to the trade-off between length and informativeness for single NLG strategies, we are interested whether this trade-off will also hold for generating sequences of NLG actions. (Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOMMEND one specific item/restaurant. For example \u201cThe 4 restaurants are all French, but differ in\n1For comparison: (Walker et al., 2000) report on R2 between .4 and .5 on a slightly lager data set.\nfood quality, and cost. Le Madeleine has the best overall value among the selected restaurants. Le Madeleine\u2019s price is 40 dollars and It has very good food quality. It\u2019s in Midtown West.\u201d\nWe therefore extend the set of possible strategies present in the data for exploration: we allow ordered combinations of the strategies, assuming that only COMPARE or RECOMMEND can follow a SUMMARY, and that only RECOMMEND can follow COMPARE, resulting in 7 possible actions:\n1. RECOMMEND\n2. COMPARE\n3. SUMMARY\n4. COMPARE+RECOMMEND\n5. SUMMARY+RECOMMEND\n6. SUMMARY+COMPARE\n7. SUMMARY+COMPARE+RECOMMEND\nWe then analytically solved the regression model in Equation 1 for the 7 possible strategies using average values from the MATCH data. This is solved by a system of linear inequalities. According to this model, the best ranking strategy is to do all the presentation strategies in one sequence, i.e. SUMMARY+COMPARE+RECOMMEND. However, this analytic solution assumes a \u201cone-shot\u201d generation strategy where there is no intermediate feedback from the environment: users are simply static overhearers (they cannot \u201cbarge-in\u201d for example), there is no variation in the behaviour of the surface realizer, i.e. one would use fixed templates as in MATCH, and the user has unlimited cognitive capabilities. These assumptions are not realistic, and must be relaxed. In the next Section we\ndescribe a worked through example of the overall framework."}, {"heading": "5 Method: the RL-NLG model", "text": "For the reasons discussed above, we treat the NLG module as a statistical planner, operating in a stochastic environment, and optimise it using Reinforcement Learning. The input to the module is a Communicative Goal supplied by the Dialogue Manager. The CG consists of a Dialogue Act to be generated, for example present items(i1, i2, i5, i8), and a System Goal (SysGoal) which is the desired user reaction, e.g. to make the user choose one of the presented items (user choose one of(i1, i2, i5, i8)). The RL-NLG module must plan a sequence of lowerlevel NLG actions that achieve the goal (at lowest cost) in the current context. The context consists of a user (who may remain silent, supply more constraints, choose an item, or quit), and variation from the sentence realizer described above.\nNow let us walk-through one simple utterance plan as carried out by this model, as shown in Table 2. Here, we start with the CG present items(i1, i2, i5, i8)& user choose one of(i1, i2, i5, i8) from the system\u2019s DM. This initialises the NLG state (init). The policy chooses the action SUMMARY and this transitions us to state s1, where we observe that 4 attributes and 1 sentence have been generated, and the user is predicted to remain silent. In this state, the current NLG policy is to RECOMMEND the top ranked item (i5, for this user), which takes us to state s2, where 8 attributes have been generated in a total of 4 sentences, and the user chooses an item. The policy holds that in states like s2 the\nState Action State change/effect init SysGoal: present items(i1, i2, i5, i8)& user choose one of(i1, i2, i5, i8) initialise state s1 RL-NLG: SUMMARY(i1, i2, i5, i8) att=4, sent=1, user=silent s2 RL-NLG: RECOMMEND(i5) att=8, sent=4, user=choose(i5) end RL-NLG: stop calculate Reward\nTable 2: Example utterance planning sequence for Figure 2\nbest thing to do is \u201cstop\u201d and pass the turn to the user. This takes us to the state end, where the total reward of this action sequence is computed (see Section 6.3), and used to update the NLG policy in each of the visited state-action pairs via backpropagation."}, {"heading": "6 Experiments", "text": "We now report on a proof-of-concept study where we train our policy in a simulated learning environment based on the results from the MATCH corpus analysis in Section 4. Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (Rieser and Lemon, 2008b). Note, that we cannot directly learn from the MATCH data, as therefore we would need data from an interactive dialogue. We are currently collecting such data in a Wizard-of-Oz experiment."}, {"heading": "6.1 User simulation", "text": "User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. However, this user act does not actually change the overall dialogue\nstate (e.g. by filling slots) but it only changes the generator state. In other words, the NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. It also tells us the probability whether the user chooses to \u201cbarge-in\u201d after a system NLG action (by either choosing an item or providing more information).\nThe user simulation for this study is a simple bi-gram model, which relates the number of attributes presented to the next likely user actions, see Table 3. The user can either follow the goal provided by the DM (SysGoal), for example choosing an item. The user can also do something else (userElse), e.g. providing another constraint, or the user can quit (userQuit).\nFor simplification, we discretise the number of attributes into concise-average-verbose, reflecting the conciseness values from the MATCH data, as described in Section 4. In addition, we assume that the user\u2019s cognitive abilities are limited (\u201ccognitive load\u201d), based on the results from the second MATCH data set in Section 4. Once the number of attributes is more than the \u201cmagic number 7\u201d (reflecting psychological results on shortterm memory) (Baddeley, 2001) the user is more likely to become confused and quit.\nThe probabilities in Table 3 are currently man-\nually set heuristics. We are currently conducting a Wizard-of-Oz study in order to learn these probabilities (and other user parameters) from real data."}, {"heading": "6.2 Realizer model", "text": "The sequential NLG model assumes a realizer, which updates the context after each generation step (i.e. after each single action). We estimate the realiser\u2019s parameters from the mean values we found in the MATCH data (see Table 1). For this study we first (randomly) vary the number of attributes, whereas the number of sentences is fixed (see Table 4). In current work we replace the realizer model with an implemented generator that replicates the variation found in the SPaRKy realizer (Stent et al., 2004)."}, {"heading": "6.3 Reward function", "text": "The reward function defines the final goal of the utterance generation sequence. In this experiment the reward is a function of the various data-driven trade-offs as identified in the data analysis in Section 4: utterance length and number of provided attributes, as weighted by the regression model in Equation 1, as well as the next predicted user action. Since we currently only have overhearer data, we manually estimate the reward for the next most likely user act, to supplement the datadriven model. If in the end state the next most likely user act is userQuit, the learner gets a penalty of \u2212100, userElse receives 0 reward, and SysGoal gains +100 reward. Again, these hand coded scores need to be refined by a more targeted data collection, but the other components of the reward function are data-driven.\nNote that RL learns to \u201cmake compromises\u201d with respect to the different trade-offs. For example, the user is less likely to choose an item\nif there are more than 7 attributes, but the realizer can generate 9 attributes. However, in some contexts it might be desirable to generate all 9 attributes, e.g. if the generated utterance is short. Threshold-based approaches, in contrast, cannot (easily) reason with respect to the current context."}, {"heading": "6.4 State and Action Space", "text": "We now formulate the problem as a Markov Decision Process (MDP), relating states to actions. Each state-action pair is associated with a transition probability, which is the probability of moving from state s at time t to state s\u2032 at time t+1 after having performed action a when in state s. This transition probability is computed by the environment model (i.e. user and realizer), and explicitly captures noise/uncertainty in the environment. This is a major difference to other non-statistical planning approaches. Each transition is also associated with a reinforcement signal (or reward) rt+1 describing how good the result of action a was when performed in state s.\nThe state space comprises 9 binary features representing the number of attributes, 2 binary features representing the predicted user\u2019s next action to follow the system goal or quit, as well as a discrete feature reflecting the number of sentences generated so far, as shown in Figure 3. This results in 211 \u00d7 6 = 12, 288 distinct generation states. We trained the policy using the well known SARSA algorithm, using linear function approximation (Sutton and Barto, 1998). The policy was trained for 3600 simulated NLG sequences.\nIn future work we plan to learn lower level decisions, such as lexical adaptation based on the vocabulary used by the user."}, {"heading": "6.5 Baselines", "text": "We derive the baseline policies from Information Presentation strategies as deployed by current dialogue systems. In total we utilise 7 different baselines (B1-B7), which correspond to single branches in our policy space (see Figure 1):\nB1: RECOMMEND only, e.g. (Young et al., 2007)\nB2: COMPARE only, e.g. (Henderson et al., 2008)\nB3: SUMMARY only, e.g. (Polifroni and Walker, 2008)\nB4: SUMMARY followed by RECOMMEND, e.g. (Whittaker et al., 2002)\nB5: Randomly choosing between COMPARE and RECOMMEND, e.g. (Walker et al., 2007)\nB6: Randomly choosing between all 7 outputs\nB7: Always generating whole sequence, i.e. SUMMARY+COMPARE+RECOMMEND, as suggested by the analytic solution (see Section 4)."}, {"heading": "6.6 Results", "text": "We analyse the test runs (n=200) using an ANOVA with a Post-Hoc T-Test (with Bonferroni correction). RL significantly (p < .001) outperforms all baselines in terms of final reward, see Table 5. RL is the only policy which significantly improves the next most likely user action by adapting to features in the current context. In contrast to conventional approaches, RL learns to \u2018control\u2019 its environment according to the estimated transition probabilities and the associated rewards.\nThe learnt policy can be described as follows: It either starts with SUMMARY or COMPARE after the init state, i.e. it learnt to never start with a RECOMMEND. It stops generating after COMPARE if the userGoal is (probably) reached (e.g. the user is most likely to choose an item in the next turn, which depends on the number of attributes generated), otherwise it goes on and generates a RECOMMEND. If it starts with SUMMARY, it always generates a COMPARE afterwards. Again, it stops if the userGoal is (probably) reached, otherwise it generates the full sequence (which corresponds to the analytic solution B7).\nThe analytic solution B7 performs second best, and significantly outperforms all the other baselines (p < .01). Still, it is significantly worse (p < .001) than the learnt policy as this \u2018one-shotstrategy\u2019 cannot robustly and dynamically adopt to noise or changes in the environment.\nIn general, generating sequences of NLG actions rates higher than generating single actions only: B4 and B6 rate directly after RL and B7,\nwhile B1, B2, B3, B5 are all equally bad given our data-driven definition of reward and environment. Furthermore, the simulated environment allows us to replicate the results in the MATCH corpus (see Section 4) when only comparing single strategies: SUMMARY performs significantly worse, while RECOMMEND and COMPARE perform equally well."}, {"heading": "7 Conclusion", "text": "We presented and evaluated a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning. After motivating and presenting the model, we studied its use in Information Presentation.\nWe derived a data-driven model predicting users\u2019 judgements on different information presentation actions (reward function), via a regression analysis on MATCH data. We used this regression model to set weights in a reward function for Reinforcement Learning, and so optimise a context-adaptive presentation policy. The learnt policy was compared to several baselines derived from previous work in this area, where the learnt policy significantly outperforms all the baselines.\nThere are many possible extensions to this model, e.g. using the same techniques to jointly optimise choosing the number of attributes, aggregation, word choice, referring expressions, and so\non, in a hierarchical manner. We are currently collecting data in targeted Wizard-of-Oz experiments, to derive a fully datadriven training environment and test the learnt policy with real users, following (Rieser and Lemon, 2008b). The trained NLG strategy will also be integrated in an end-to-end statistical system within the CLASSiC project (www. classic-project.org)."}, {"heading": "Acknowledgments", "text": "We thank Marilyn Walker for access to the MATCH corpus. The research leading to these results has received funding from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 216594 (CLASSiC project project: www. classic-project.org) and from the EPSRC project no. EP/E019501/1."}], "references": [{"title": "Working memory and language: an overview", "author": ["A. Baddeley"], "venue": "Journal of Communication Disorder,", "citeRegEx": "Baddeley.,? \\Q2001\\E", "shortCiteRegEx": "Baddeley.", "year": 2001}, {"title": "Information presentation in spoken dialogue systems", "author": ["Demberg", "Moore2006] Vera Demberg", "Johanna D. Moore"], "venue": "In Proceedings of EACL", "citeRegEx": "Demberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Demberg et al\\.", "year": 2006}, {"title": "Hybrid reinforcement / supervised learning of dialogue policies from fixed datasets", "author": ["Oliver Lemon", "Kallirroi Georgila"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2008}, {"title": "User simulations for online adaptation and knowledgealignment in Troubleshooting dialogue systems", "author": ["Janarthanam", "Oliver Lemon."], "venue": "Proc. of SEMdial.", "citeRegEx": "Janarthanam and Lemon.,? 2008", "shortCiteRegEx": "Janarthanam and Lemon.", "year": 2008}, {"title": "Experiences with planning for natural language generation", "author": ["Koller", "Petrick2008] Alexander Koller", "Ronald Petrick"], "venue": "In ICAPS", "citeRegEx": "Koller et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2008}, {"title": "Sentence generation as planning", "author": ["Koller", "Stone2007] Alexander Koller", "Matthew Stone"], "venue": "In Proceedings of ACL", "citeRegEx": "Koller et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2007}, {"title": "Adaptive Natural Language Generation in Dialogue using Reinforcement Learning", "author": ["Oliver Lemon"], "venue": "In Proceedings of SEMdial", "citeRegEx": "Lemon.,? \\Q2008\\E", "shortCiteRegEx": "Lemon.", "year": 2008}, {"title": "Generating tailored, comparative descriptions in spoken dialogue", "author": ["Moore et al.2004] Johanna Moore", "Mary Ellen Foster", "Oliver Lemon", "Michael White"], "venue": "In Proc. FLAIRS", "citeRegEx": "Moore et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2004}, {"title": "Learning to say it well: Reranking realizations by predicted synthesis quality", "author": ["Nakatsu", "White2006] Crystal Nakatsu", "Michael White"], "venue": "In Proceedings of ACL", "citeRegEx": "Nakatsu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nakatsu et al\\.", "year": 2006}, {"title": "Stochastic natural language generation for spoken dialog systems", "author": ["Oh", "Rudnicky2002] Alice Oh", "Alexander Rudnicky"], "venue": "Computer, Speech & Language,", "citeRegEx": "Oh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2002}, {"title": "Conversation as action under uncertainty", "author": ["Paek", "Horvitz2000] Tim Paek", "Eric Horvitz"], "venue": "In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Paek et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Paek et al\\.", "year": 2000}, {"title": "Intensional Summaries as Cooperative Responses in Dialogue Automation and Evaluation", "author": ["Polifroni", "Walker2008] Joseph Polifroni", "Marilyn Walker"], "venue": "In Proceedings of ACL", "citeRegEx": "Polifroni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Polifroni et al\\.", "year": 2008}, {"title": "Does this list contain what you were searching for? Learning adaptive dialogue strategies for Interactive Question Answering", "author": ["Rieser", "Lemon2008a] Verena Rieser", "Oliver Lemon"], "venue": "J. Natural Language Engineering,", "citeRegEx": "Rieser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2008}, {"title": "Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz data: Bootstrapping and Evaluation", "author": ["Rieser", "Lemon2008b] Verena Rieser", "Oliver Lemon"], "venue": "In Proceedings of ACL", "citeRegEx": "Rieser et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2008}, {"title": "Optimizing dialogue management with Reinforcement Learning: Experiments with the NJFun system", "author": ["Singh et al.2002] S. Singh", "D. Litman", "M. Kearns", "M. Walker"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "Usertailored generation for spoken dialogue: an experiment", "author": ["Stent et al.2002] Amanda Stent", "Marilyn Walker", "Steve Whittaker", "Preetam Maloor"], "venue": "Proc. of ICSLP", "citeRegEx": "Stent et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2002}, {"title": "Trainable sentence planning for complex information presentation in spoken dialog systems", "author": ["Stent et al.2004] Amanda Stent", "Rashmi Prasad", "Marilyn Walker"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Stent et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Stent et al\\.", "year": 2004}, {"title": "Reinforcement Learning", "author": ["Sutton", "Barto1998] R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Towards developing general models of usability with PARADISE", "author": ["Candace A. Kamm", "Diane J. Litman"], "venue": "Natural Language Engineering,", "citeRegEx": "Walker et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2000}, {"title": "User tailored generation in the match multimodal dialogue system", "author": ["S. Whittaker", "A. Stent", "P. Maloor", "J. Moore", "M. Johnston", "G. Vasireddy"], "venue": null, "citeRegEx": "Walker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2004}, {"title": "Individual and domain adaptation in sentence planning for dialogue", "author": ["Amanda Stent", "Fran\u00e7ois Mairesse", "Rashmi Prasad"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Walker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2007}, {"title": "Fish or Fowl: A Wizard of Oz evaluation of dialogue strategies in the restaurant domain", "author": ["Marilyn Walker", "Johanna Moore"], "venue": "In Proc. of the International Conference on Language Resources and Evaluation", "citeRegEx": "Whittaker et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Whittaker et al\\.", "year": 2002}, {"title": "Should i tell all? an experiment on conciseness in spoken dialogue", "author": ["Marilyn Walker", "Preetam Maloor"], "venue": "In Proc. European Conference on Speech Processing (EUROSPEECH)", "citeRegEx": "Whittaker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Whittaker et al\\.", "year": 2003}, {"title": "The influence of user tailoring and cognitive load on user performance in spoken dialogue systems", "author": ["Jiang Hu", "Johanna D. Moore", "Clifford Nass"], "venue": "In Proc. of the 10th International Conference of Spoken Lan-", "citeRegEx": "Winterboer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Winterboer et al\\.", "year": 2007}, {"title": "The Hidden Information State Approach to Dialog Management", "author": ["Young et al.2007] SJ Young", "J Schatzmann", "K Weilhammer", "H Ye"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Young et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 19, "context": "For example, the CG to present database results to the user can be realised as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 24, "context": ", 2004), or by picking one item and recommending it to the user (Young et al., 2007).", "startOffset": 64, "endOffset": 84}, {"referenceID": 19, "context": "(Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "(Janarthanam and Lemon, 2008), or predicted TTS quality, e.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "(Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and \u201cconversation as action under uncertainty\u201d (Paek and Horvitz, 2000).", "startOffset": 0, "endOffset": 69}, {"referenceID": 2, "context": "(Singh et al., 2002; Henderson et al., 2008; Rieser and Lemon, 2008a) and \u201cconversation as action under uncertainty\u201d (Paek and Horvitz, 2000).", "startOffset": 0, "endOffset": 69}, {"referenceID": 19, "context": "In Section 4 we present results from our analysis of the MATCH corpus (Walker et al., 2004).", "startOffset": 70, "endOffset": 91}, {"referenceID": 6, "context": "We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach).", "startOffset": 73, "endOffset": 86}, {"referenceID": 16, "context": "Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004).", "startOffset": 79, "endOffset": 99}, {"referenceID": 7, "context": "These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.", "startOffset": 191, "endOffset": 232}, {"referenceID": 19, "context": "These facts make it clear that the problem of planning how to generate an utterance falls naturally into the class of statistical planning problems, rather than rule-based approaches such as (Moore et al., 2004; Walker et al., 2004), or supervised learning as explored in previous work, such as classifier learning and re-ranking, e.", "startOffset": 191, "endOffset": 232}, {"referenceID": 16, "context": "(Stent et al., 2004; Oh and Rudnicky, 2002).", "startOffset": 0, "endOffset": 43}, {"referenceID": 19, "context": "(Walker et al., 2004; Demberg and Moore, 2006; Polifroni and Walker, 2008).", "startOffset": 0, "endOffset": 74}, {"referenceID": 19, "context": "We utilise the MATCH corpus (Walker et al., 2004) to extract an evaluation function (also known as \u201creward function\u201d) for RL.", "startOffset": 28, "endOffset": 49}, {"referenceID": 16, "context": "Furthermore, we utilise the SPaRKy corpus (Stent et al., 2004) to build a high quality stochastic realizer.", "startOffset": 42, "endOffset": 62}, {"referenceID": 15, "context": "The MATCH project made two data sets available, see (Stent et al., 2002) and (Whittaker et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 22, "context": ", 2002) and (Whittaker et al., 2003), which we combine to define an evaluation function for different Information Presentation strategies.", "startOffset": 12, "endOffset": 36}, {"referenceID": 15, "context": "The first data set, see (Stent et al., 2002), comprises 1024 ratings by 16 subjects (where we only use the speech-based half, n = 512) on the following presentation strategies: RECOMMEND, COMPARE, SUMMARY.", "startOffset": 24, "endOffset": 44}, {"referenceID": 18, "context": ") and performed a stepwise linear regression to find the features which were important to the overhearers (following the PARADISE framework (Walker et al., 2000)).", "startOffset": 140, "endOffset": 161}, {"referenceID": 22, "context": "The second MATCH data set, see (Whittaker et al., 2003), comprises 1224 ratings by 17 subjects on the NLG strategies RECOMMEND and COMPARE.", "startOffset": 31, "endOffset": 55}, {"referenceID": 23, "context": "We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007).", "startOffset": 96, "endOffset": 121}, {"referenceID": 21, "context": "(Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOMMEND one specific item/restaurant.", "startOffset": 0, "endOffset": 24}, {"referenceID": 18, "context": "For comparison: (Walker et al., 2000) report on R between .", "startOffset": 16, "endOffset": 37}, {"referenceID": 24, "context": "User simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007).", "startOffset": 96, "endOffset": 116}, {"referenceID": 0, "context": "Once the number of attributes is more than the \u201cmagic number 7\u201d (reflecting psychological results on shortterm memory) (Baddeley, 2001) the user is more likely to become confused and quit.", "startOffset": 119, "endOffset": 135}, {"referenceID": 16, "context": "In current work we replace the realizer model with an implemented generator that replicates the variation found in the SPaRKy realizer (Stent et al., 2004).", "startOffset": 135, "endOffset": 155}, {"referenceID": 24, "context": "(Young et al., 2007)", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "(Henderson et al., 2008)", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "(Whittaker et al., 2002)", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "(Walker et al., 2007)", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches.", "creator": "LaTeX with hyperref package"}}}