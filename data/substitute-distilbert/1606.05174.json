{"id": "1606.05174", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Deep Reinforcement Learning Discovers Internal Models", "abstract": "deep complexity learning ( drl ) involves a trending field of research, showing great promise in challenging problems such as controlling atari, solving go and controlling robots. while drl agents perform well in practice we are still lacking the tools to analayze their performance. in future work we present the semi - interactive mdp ( samdp ) model. a model best suited from describe policies exhibiting parallel spatial and temporal hierarchies. we describe economic advantages for analyzing trained software over other modeling approaches, and show that under exactly right state representation, like that of software agents, samdp can help to identify behaviors. we detail the automatic process of creating decisions on recorded trajectories, up to presenting it on t - sne maps. we explain how to evaluate its fitness and found surprising results indicating high compatibility on the policy at hand. we conclude by exposing how using the samdp methods, an extra performance gain can be squeezed from measured agent.", "histories": [["v1", "Thu, 16 Jun 2016 13:09:16 GMT  (1904kb,D)", "http://arxiv.org/abs/1606.05174v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nir baram", "tom zahavy", "shie mannor"], "accepted": false, "id": "1606.05174"}, "pdf": {"name": "1606.05174.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning Discovers Internal Models", "authors": ["Nir Baram", "Tom Zahavy"], "emails": ["{nirb@campus,", "tomzahavy@campus,", "shie@ee}.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Deep Q Network (DQN) is an off-policy learning algorithm that uses a Convolutional Neural Network (CNN; [Krizhevsky et al., 2012]) to represent the action-value function. Agents trained using DQN are showing superior performance on a wide range of problems [Mnih et al., 2015]. Their success, and that of Deep Neural Network (DNN) in general, is explained by its ability to learn good representations automatically. Unfortunately, its high expressiveness is also the source of its unclarity, making it very hard to analyze. Visualization methods for DNN try to tackle this problem by analyzing and interpreting the learned representations [Zeiler and Fergus, 2014, Erhan et al., 2009, Yosinski et al., 2014]. However, these methods were developed for supervised learning tasks, assuming the data is i.i.d, thus overlooking the temporal structure of the learned representation.\nA major challenge in Reinforcement Learning (RL) is scaling to higher dimensions in order to solve realworld applications. Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics. On the other hand, temporal abstractions (i.e., options or skills [Sutton et al., 1999a]) can help an agent to focus less on lower level details of a task and more on high level planning [Dietterich, 2000, Parr, 1998]. The problem with these methods is that finding good abstractions is typically done manually which hampers their wide use. The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system. In this work we follow the same line of thought and claim that DQNs are learning an underlying spatio-temporal model of the problem, without implicitly being trained to. We identify this model as an Semi Aggregated Markov Decision Process (SAMDP), an approximation of the true MDP that allows human interpretability. Zahavy et al. [2016] used hand-crafted features in order to interpret policies learned by DQN agents. They revealed that DQNs are automatically learning spatio-temporal representations such as hierarchical state aggregation and skills. The main drawback of their approach is that they used a manual reasoning of a t-Distributed Stochastic Neighbour\nar X\niv :1\n60 6.\n05 17\n4v 1\n[ cs\n.A I]\n1 6\nJu n\nEmbedding (t-SNE) map [Van der Maaten and Hinton, 2008], a tedious process that requires careful inspection as well as an experienced eye. Moreover, their claim to observe skills is not supported with any quantitative evidence. In contrast, we use temporal aware clustering algorithms in order to aggregate the state space, and automatically reveal the underlying spatio-temporal structure of the t-SNE map. The aggregated states uniquely identify skills and allow us to estimate the SAMDP transition probabilities and reward signal empirically. In particular our main contributions are\n1. SAMDP: a model that gives a simple explanation on how DRL agents solve a task - by hierarchically decomposing it into a set of sub-problems and learning specific skills at each.\n2. Automatic analysis: we suggest quantitative criteria that allows us to select good models and evaluate their consistency.\n3. Interpretation: we developed a novel visualization tool that gives a qualitative understanding of the learned policy.\n4. Shared autonomy: the SAMDP model allows us to predict situations where the DQN agent is not performing well. In such occasions we suggest to take the control from the agent and ask for expert advice."}, {"heading": "2 Background", "text": "We briefly review the standard reinforcement learning framework of discrete-time, finite Markov decision processes (MDPs). In this framework, the goal of an RL agent is to maximize its expected return by learning a policy \u03c0 : S \u2192 \u2206A, a mapping from states s \u2208 S to probability distribution over actions A. At time t the agent observes a state st \u2208 S, selects an action at \u2208 A, and receives a reward rt. Following the agents action choice, it transitions to the next state st+1 \u2208 S. We consider infinite horizon problems where the cumulative return at time t is given by Rt = \u2211\u221e t\u2032=t \u03b3\nt\u2032\u2212trt, and \u03b3 \u2208 [0, 1] is the discount factor. The action-value function Q\u03c0(s, a) = E[Rt|st = s, at = a, \u03c0] represents the expected return after observing state s, taking action a after which following policy \u03c0. The optimal action-value function obeys a fundamental recursion known as the optimal Bellman Equation: Q\u2217(st, at) = E [ rt + \u03b3max\na\u2032 Q\u2217(st+1, a\n\u2032) ] .\nDeep Q Networks: The DQN algorithm approximates the optimal Q function using a CNN. The training objective it to minimize the expected TD error of the optimal Bellman Equation:\nEst,at,rt,st+1 \u2016Q\u03b8 (st, at)\u2212 yt\u2016 2 2\n[Mnih et al., 2015]. DQN is an offline learning algorithm that collects experience tuples {st,at, rt, st+1, \u03b3} and stores them in the Experience Replay (ER) [Lin, 1993]. At each training step, a mini-batch of experience tuples are sampled at random from the ER. The DQN maintains two separate Q-networks. The current Q-network with parameters \u03b8, and the target Q-network with parameters \u03b8target. The parameters \u03b8target are set to \u03b8 every fixed number of iterations. In order to capture the MDP dynamics, the final DQN representation is a concatenation of several consecutive states.\nSkills, Options, Macro-actions, [Sutton et al., 1999a] are temporally extended control structures, denoted by \u03c3. A skill is defined by a triplet: \u03c3 =< I, \u03c0, \u03b2 > . I defines the set of states where the skill can be initiated. \u03c0 is the intra-skill policy, and \u03b2 is the set of termination probabilities determining when a skill will stop executing. \u03b2 is typically either a function of state s or time t. Any MDP with a fixed set of skills is a Semi-Markov Decision Process (SMDP). Planning with skills can be performed by learning for each state the value of choosing each skill. More formally, an SMDP can be defined by a five-tuple < S,\u03a3, P,R, \u03b3 >, where S is the set of states, \u03a3 is the set of skills, P is the SMDP transition matrix, \u03b3 is the discount factor and the SMDP reward is defined by:\nR\u03c3s = E[r\u03c3s ] = E[rt+1 + \u03b3rt+2 + \u00b7 \u00b7 \u00b7+ \u03b3k\u22121rt+k|st = s, \u03c3] (1)\nThe Skill Policy \u00b5 : S \u2192 \u2206\u03a3 is a mapping from states to a probability distribution over skills. The actionvalue function Q\u00b5(s, \u03c3) = E[ \u2211\u221e t=0 \u03b3\ntRt|(s, \u03c3), \u00b5] represents the value of choosing skill \u03c3 \u2208 \u03a3 at state s \u2208 S, and thereafter selecting skills according to policy \u00b5. The optimal skill value function is given by: Q\u2217\u03a3(s, \u03c3) = E[R\u03c3s + \u03b3kmax\n\u03c3\u2032\u2208\u03a3 Q\u2217\u03a3(s \u2032, \u03c3\u2032)] [Stolle and Precup, 2002]."}, {"heading": "3 Semi Aggregated Markov Decision Processes", "text": "Reinforcement Learning problems are typically modeled using the MDP formulation. The abundant theory developed for MDP throughout the years gave rise to various algorithms for efficiently solving MDPs, and finding good\npolicies. MDP however, is not the optimal modeling choice when one wishes to analyze a given policy. Policy analysis methods typically suffer from the cardinality of the state space and the length of the planning horizon. For example, building a graphical model that explains the policy will be too large (in terms of states), and complex (in terms of planning horizon) for a human to comprehend. If the policy one wishes to analyze is known to be planning using temporally-extended actions (i.e. skills), then one may resort to SMDP modeling. The SMDP model reduces the planning horizon dramatically and simplifies the graphical model. There are two problems however with this approach. First, it requires to identify the set of skills used by the policy, a long-standing challenging problem with no easy solution. Second, one is still facing the high complexity of the state space.\nA different modeling approach is to aggregate similar states first. This is useful when there is a reason to believe that groups of states share common attributes such as similar policy, value function or dynamics. State aggregation is a well studied problem that can be solved by applying clustering on the MDP state representation. These models are not necessarily Markovian, however they can provide great simplification of the state space. With a slight abuse of notation we denote this model as Aggregated MDP (AMDP). Under the right staterepresentation, the AMDP can also help to identify skills (if exist). We argue that this is possible if the AMDP dynamics is such that the majority of the transitions are done within the clusters, followed by rare transitions between clusters. As we will show in the experiments section, DQN indeed provides a good state representation that allows skill identification.\nIf the state representation contains both spatial and temporal hierarchies, then the AMDP model can be further simplified into an SAMDP model. Under SAMDP modeling, both the state-space cardinality and the planning horizon are reduced, making policy reasoning more feasible. We summarize our observations about the different modeling approaches in Figure 1. In the remaining of this section we explain the SAMDP modeling in detail and focus on explaining how to empirically build an SAMDP model from experience. To do so we explain how to aggregate states, identify skills and estimate the transition probabilities and reward measures. Finally we discuss how to evaluate the fitness of an empiric SAMDP model to the data."}, {"heading": "3.1 State aggregation", "text": "We evaluate a DQN agent, by letting it play multiple trajectories with an -greedy policy. During evaluation we record all visited states, neural activations, value estimations, and index them by their visitation order. We treat the neural activations as the state representation that the DQN agent has learned. Zahavy et al. [2016] showed that this state representation captures a spatio-temporal hierarchy and therefore makes a good candidate for state aggregation. We then apply t-SNE on the neural activations data, a non-linear dimensionality reduction method that is particularly good at creating a single map that reveals structure at many different scales. t-SNE reduces the tendency of points to crowd together in the center of the map by using a heavy tailed Student-t distribution in\nthe low dimensional space. The result is a compact, well separated representation, that is easy to visualize and interpret.\nWe represent an MDP state si by a feature vector xi \u2208 R3, comprised of the two t-SNE coordinates and the DQN value estimate. Using this representation we aggregate the state space by applying clustering algorithms and define the AMDP states C as the resulting clusters. Standard clustering algorithms assume that the data is drawn from an i.i.d distribution, however our data is generated from an MDP which violates this assumption.\nAlgorithm 1 K-means [MacQueen et al., 1967] for state aggregation Input: MDP sates feature representation (x1, x2, \u00b7 \u00b7 \u00b7 , xn). Output: SAMDP states (c1, c2, \u00b7 \u00b7 \u00b7 , ck). Objective: minimize the within-cluster sum of squares:\narg min C\n\u2211k i=1 \u2211 x\u2208Ci \u2016x\u2212 \u00b5i\u2016 2\nwhere \u00b5i is the mean of points in ci. Repeat until convergence:\n1. Assignment step, each observation xi is assigned to its closest cluster center:\nC (t) i = { xp : \u2225\u2225xp \u2212 \u00b5(t)i \u2225\u22252 \u2264 \u2225\u2225xp \u2212 \u00b5(t)j \u2225\u22252\u2200j, 1 \u2264 j \u2264 k}. 2. Update step, each cluster center \u00b5j is updated to be the mean of its constituent instances:\n\u00b5 (t+1) i = 1\n|C(t)i | \u2211 xj\u2208C(t)i xj .\nIn order to alleviate this problem, we suggest two versions of K-Means (Algorithm 1) that take into account the temporal structure of the data. (1) Spatio-Temporal Cluster Assignment that encourages temporal coherency by modifying the assignment step in the following way:\nC (t) i = { xp : \u2225\u2225Xp\u2212w:p+w \u2212 \u00b5(t)i \u2225\u22252 \u2264 \u2225\u2225Xp\u2212w:p+w \u2212 \u00b5(t)j \u2225\u22252,\u2200j, 1 \u2264 j \u2264 k} (2) Where p is the time index of observation xp, Xp\u2212w:p+w is the set of 2w points before and after xp along the trajectory. In this way, a point xp is assigned to a cluster \u00b5j , if its neighbours along the trajectory are also close to \u00b5j . (2) Entropy Regularization Cluster Assignment that creates simpler models by adding an entropy regularization term to the K-mean assignment step:\nC (t) i = { xp : \u2225\u2225xp \u2212 \u00b5(t)i \u2225\u22252 + d \u00b7 et\u22121xp\u2192i \u2264 \u2225\u2225xp \u2212 \u00b5(t)j \u2225\u22252 + d \u00b7 et\u22121xp\u2192j ,\u2200j, 1 \u2264 j \u2264 k}. (3) Where d is a penalty weight, and et\u22121xp\u2192i indicates the entropy (as defined in Section 3.3) gain of changing xp assignment to cluster i in the SMDP obtained at iteration t \u2212 1. This is equivalent to minimizing an energy function, the sum of the K-means objective function and an entropy term. We also considered Agglomerative Clustering, a bottom-up hierarchical approach. Starting with a mapping from points to clusters (e.g., each point is a singular cluster), the algorithm advances by merging pairs of clusters such that a linkage criteria is minimized. In order to encourage temporal coherency in cluster assignments we define a new linkage criteria based on Ward [1963]:\nc(A,B) = (1\u2212 \u03bb) \u00b7mean{\u2016xa \u2212 xb\u2016 : a \u2208 A, b \u2208 B}+ \u03bb \u00b7 e{A,B}\u2192AB (4)\nwhere e{A,B}\u2192AB measures the difference between the entropy of the corresponding SMDP before and after merging clusters A,B."}, {"heading": "3.2 Temporal abstractions", "text": "We define the SAMDP skills by their initiation and termination AMDP states C:\n\u03c3ij =< {ci}, \u03c0i,j , {cj} > . (5)\nMore implicit, once the DQN agent enters an AMDP state ci at an MDP state st \u2208 ci, it follows the skill policy \u03c0i,j for k steps, until it reaches a state st+k \u2208 cj , s.t i 6= j. Note that we do not define the skill policy implicitly, but we will observe later that our model successfully captures spatio-temporal defined skill policies. We set the SAMDP discount factor \u03b3 same as was used to train the DQN. We now turn to estimate the SAMDP probability matrix and reward signal. For that goal we make the following assumptions:\nDefinition 1. A deterministic probability matrix, is a probability matrix such that each of its rows contains one element that equals to 1 and the others equal to 0.\nAssumption 1. The MDP transition matrices PA : P a\u2208Ai,j = Pr(xj |xi, a) are deterministic. This assumption limits our analysis for environments with deterministic dynamics. However, many interesting problems are in fact deterministic, e.g., Atari2600 benchmarks, Go, Chess etc.\nAssumption 2. The policy played by the DQN agent is deterministic. Although DQN chooses actions deterministically (by selecting the action that corresponds to the maximal Q value in each state), we allow 5% stochastic exploration. This introduces errors into our model that we will later analyze. Given the DQN policy, the MDP is reduced into a Markov Reward Process (MRP) with probability matrix P\u03c0 DQN\ni,j = Pr(xj |xi, a = \u03c0DQN (xi)). Note that by Assumptions 1 and 2, this is also a deterministic probability matrix.\nThe SAMDP transition probability matrix P\u03a3 : P\u03c3\u2208\u03a3i,j = Pr(cj |ci, \u03c3), indicates the probability of moving from state ci to cj given that skill \u03c3 is chosen. It is also a deterministic probability matrix by our definition of skills (Equation 5). Our goal is to estimate the probability matrix that the DQN policy induces on the SAMDP model: P\u03c0 DQN\ni,j = Pr(cj |ci, \u03c3 = \u03c0DQN (ci)). We do not require this policy to be deterministic from two reasons. First, we evaluate the DQN agent with an -greedy policy. While almost deterministic in the view of a single time step, the variance of its behaviour increases as more moves are played. Second, the aggregation process is only an approximation. For example, a given state may contain more than one \u201dreal\u201d state and therefore hold more than one skill with different transitions. A stochastic policy can solve this disagreement by allowing to choose skills at random.\nThis type of modeling does not guarantee that our SAMDP model is Markovian and we are not claiming it to be. SAMDP is an approximation of the the true dynamics that simplifies it over space and time to and allow human interpretation. Finally, we estimate the skill length k\u03c3 and SAMDP reward for each skill from the data using Equation 1. In the experiments section we show that this model is in fact consistent with the data by evaluating its value function:\nVSAMDP = (I + \u03b3 kP )\u22121r (6)\nand the greedy policy with respect to it:\n\u03c0greedy(ci) = argmax j {R\u03c3i,j + \u03b3 k\u03c3i,j vSAMDP (cj)} (7)"}, {"heading": "3.3 Evaluation criteria", "text": "We follow the analysis of [Hallak et al., 2013] and define criteria to measure the fitness of a model empirically. We define the Value Mean Square Error(VMSE) as the normalized distance between two value estimations: VMSE = \u2016v\nDQN\u2212vSAMDP \u2016 \u2016vDQN\u2016 . The SAMDP value is given by Equation 6 and the DQN value is evaluated\nby averaging the DQN value estimates over all MDP states in a given cluster (SAMDP state): vDQN (cj) = 1 |Cj | \u2211 i:si\u2208cj v\nDQN (si) . The Minimum Description Length (MDL; [Rissanen, 1978]) principle is a formalization of the celebrated Occams Razor. It copes with the over-fitting problem for the purpose of model selection. According to this principle, the best hypothesis for a given data set is the one that leads to the best compression of the data. Here, the goal is to find a model that explains the data well, but is also simple in terms of the number of parameters. In our work we follow a similar logic and look for a model that best fits the data but is still simple. Instead of considering \u201dsimple\u201d in terms of the number of parameters, we measure the simplicity of the spatiotemporal state aggregation. For spatial simplicity we define the Inertia: I = \u2211n i=0 min\u00b5j\u2208C(||xj \u2212 \u00b5i||2) which measures the variance of MDP states inside a cluster (AMDP state). For temporal simplicity we define the entropy: e = \u2212 \u2211 i{|Ci| \u00b7 \u2211 j Pi,j logPi,j} , and the Intensity Factor which measures the fraction of in/out cluster\ntransitions: F = \u2211 j Pjj\u2211 i Pji . To summarize, the stages of building an SAMDP model are:\n1. Evaluate : Run the trained (DQN) agent, record visited states, representations and Q-values.\n2. Reduce : Apply t-SNE on the state representations to obtain a low dimensional map.\n3. Aggregate : Cluster states in the map.\n4. Model : Fit an SAMDP model, select the best model.\n5. Visualize : Visualize the SAMDP on top of the t-SNE map."}, {"heading": "4 Experiments", "text": "Setup. We evaluate our method on three Atari2600 games, Breakout, Pacman and Seaquest. For each game we collect 120k game states (each represented by 512 features), and Q-values for all actions. We apply PCA to reduce the data to 50 dimensions, then we apply t-SNE using the Barnes Hut approximation to reach the desired low 2 dimension. We run the t-SNE algorithm for 3000 iterations with perplexity of 30. We use Spatio-Temporal K-means clustering (Section 3.1) to create the AMDP states (clusters), and evaluate the transition probabilities between them using the trajectory data. We overlook flicker-transitions where a cluster is visited for less than f time steps before transiting out. Finally we truncate transitions with less than 0.1 probability.\nModel Selection. We perform a grid search on two parameters: i) number of clusters N c \u2208 [15, 25]. ii) window size w \u2208 [1, 7]. We found that models larger (smaller) than that are too cumbersome (simplistic) to analyze. We select the best model in the following way: Let e(w, n), i(w, n), v(e, n), f(e, n) be the entropy, inertia, VMSE, and intensity factor respective measures of configuration (w, n) in the greed search. Let E = {e(w, n)}, I = {i(w, n)}, V = {v(w, n)}, F = {f(w, n)} be the corresponding sets grouped over all grid search configurations. We sort each set from good to bad, i.e. from minimum to maximum (except for intensity factor where larger values are considered better). We then iteratively intersect the p-prefix of all sets (i.e. the first p elements of each set) starting with 1-prefix. We stop when the intersection is non empty and choose the configuration at the intersection. Figure 2 shows the correlation between pairs of criteria (for Breakout).\nOverall, we see a tradeoff between spatial and temporal complexity. For example, in the bottom left plot, we observe correlation between the Inertia and the Intensity Factor; a small window size w leads to well-defined clusters in space (low Inertia) at the expense of a complex transition matrix (small intensity factor). A large w causes the clusters to be more spread in space (large Inertia), but has the positive effect of intensifying the in-cluster transitions (high intensity factor). We also measure the p-value of the chosen model with the null hypothesis being the SAMDP model constructed with randomly clustered states. We tested 10000 random SAMDP models, none of which scored better than the chosen model (for any of the evaluation criteria). Qualitative Evaluation. Examining the resulting SAMDP (Figure 3) it is interesting to note the sparsity of transitions. This indicate that clusters are well located in time. Inspecting the mean image of each cluster also reveal some insights about the nature of the skills hiding within. We also see evidence for the \u201dtunnel-digging\u201d option described in [Zahavy et al., 2016] in the transitions between clusters 11,12,14 and 4.\nModel Evaluation. We evaluate our model using three different methods. First, the VMSE criteria (Figure 4, top): high correlation between the DQN values and the SAMDP values gives a clear indication to the fitness\nof the model to the data. Second, we evaluate the correlation between the transitions induced by the policy improvement step and the trajectory reward Rj . To do so, we measure P ji : the empirical distribution of choosing the greedy policy at state ci in that trajectory. Finally we present the correlation coefficients at each state: corri = corr(P ji , R\nj) (Figure 4, center). Positive correlation indicates that following the greedy policy leads to high reward. Indeed for most of the states we observe positive correlation, supporting the consistency of the model. The third evaluation is close in spirit to the second one. We create two transition matrices T+, T\u2212 using k toprewarded trajectories and k least-rewarded trajectories respectively. We measure the correlation of the greedy policy TG with each of the transition matrices for different values of k (Figure 4 bottom). As clearly seen, the correlation of the greedy policy and the top trajectories is higher than the correlation with the bad trajectories. Eject Button: Performance improvement. In the following experiment we show how the SAMDP model can help to improve the performance of a trained policy. The motivation for this experiment stems from the idea of shared autonomy [Pitzer et al., 2011]. There are domains where errors are not permitted and performance must be as high as possible. The idea of shared autonomy is to allow an operator to intervene in the decision loop in critical times. For example, it is known that in 20% of commercial flights, the auto-pilot returns the control to the human pilots. For this experiment we first build an SAMDP model and then let the agent to play new (unseen) trajectories. We project the online state visitations onto our model and monitor its transitions along it. We define T+, T\u2212 as above. If the likelihood of T\u2212 with respect to the online trajectory is greater than the likelihood of T+, we press the Eject button and terminate this execution (a procedure inspired by option interruption [Sutton et al., 1999b]). We\u2019re interested to measure the average performance of the un-terminated trajectories with respect to all trajectories. The performance improvement achieved with and without using the Eject button is presented in Table 1."}, {"heading": "5 Discussion", "text": "In this work we considered the problem of automatically building an SAMDP model for analyzing trained policies. Starting from a t-SNE map of neural activations, and ending up with a compact model that gives a clear interpretation for complex RL tasks. We showed how SAMDP can help in identifying skills that are well defined in terms of initiation and termination sets. However, the SAMDP doesn\u2019t offer much information about the skill policy and we suggest to further investigate it in future work. It would also be interesting to see whether skills of different states actually represent the same behaviour. Most importantly, the skills we find are determined by the state aggregation. Therefore, they are impaired by the artifacts of the clustering method used. In future work we will consider other clustering methods that better relate to the topology (such as spectral-clustering), to see if they lead to better skills.\nIn the Eject experiment we showed how SAMDP model can help to improve the policy at hand without the need to re-train it. It would be even more interesting to use the SAMDP model to improve the training phase itself. The strength of SAMDP in identifying spatio and temporal hierarchies could be used for harnessing DRL hierarchical algorithms [Tessler et al., 2016, Kulkarni et al., 2016]. For example by automatically detecting subgoals or skills.\nAnother question we\u2019re interested in answering is whether a global control structure exists? Motivated by the success of policy distillation ideas [Rusu et al., 2015], it would be interesting to see how well an SAMDP built for game A, explains game B? Finally we would like to use this model to interpret other DRL agents that are not specifically trained to approximate value such as deep policy gradient methods."}], "references": [{"title": "Adaptive aggregation methods for infinite horizon dynamic programming", "author": ["Dimitri P Bertsekas", "David A Castanon"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Bertsekas and Castanon.,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Castanon.", "year": 1989}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Dept. IRO, Universite\u0301 de Montre\u0301al, Tech. Rep,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "The internal model principle for linear multivariable regulators", "author": ["Bruce A Francis", "William M Wonham"], "venue": "Applied mathematics and optimization,", "citeRegEx": "Francis and Wonham.,? \\Q1975\\E", "shortCiteRegEx": "Francis and Wonham.", "year": 1975}, {"title": "Model selection in markovian processes", "author": ["Assaf Hallak", "Dotan Di-Castro", "Shie Mannor"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "Hallak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hallak et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Long-Ji Lin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin.,? \\Q1993\\E", "shortCiteRegEx": "Lin.", "year": 1993}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["James MacQueen"], "venue": null, "citeRegEx": "MacQueen,? \\Q1967\\E", "shortCiteRegEx": "MacQueen", "year": 1967}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision problems", "author": ["Ronald Parr"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Parr.,? \\Q1998\\E", "shortCiteRegEx": "Parr.", "year": 1998}, {"title": "Towards perceptual shared autonomy for robotic mobile manipulation", "author": ["Benjamin Pitzer", "Michael Styer", "Christian Bersch", "Charles DuHadway", "Jan Becker"], "venue": "In IEEE International Conference on Robotics Automation (ICRA),", "citeRegEx": "Pitzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pitzer et al\\.", "year": 2011}, {"title": "Modeling by shortest data", "author": ["Jorma Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Policy distillation", "author": ["Andrei A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Adaptation and regulation with signal detection implies internal model", "author": ["Eduardo D Sontag"], "venue": "Systems & control letters,", "citeRegEx": "Sontag.,? \\Q2003\\E", "shortCiteRegEx": "Sontag.", "year": 2003}, {"title": "Learning options in reinforcement learning", "author": ["Martin Stolle", "Doina Precup"], "venue": null, "citeRegEx": "Stolle and Precup.,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup.", "year": 2002}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["Chen Tessler", "Shahar Givony", "Tom Zahavy", "Daniel J Mankowitz", "Shie Mannor"], "venue": "arXiv preprint arXiv:1604.07255,", "citeRegEx": "Tessler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tessler et al\\.", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Joe H. Ward"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ward.,? \\Q1963\\E", "shortCiteRegEx": "Ward.", "year": 1963}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": null, "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02658,", "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": null, "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Deep Q Network (DQN) is an off-policy learning algorithm that uses a Convolutional Neural Network (CNN; [Krizhevsky et al., 2012]) to represent the action-value function.", "startOffset": 104, "endOffset": 129}, {"referenceID": 9, "context": "Agents trained using DQN are showing superior performance on a wide range of problems [Mnih et al., 2015].", "startOffset": 86, "endOffset": 105}, {"referenceID": 0, "context": "Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics.", "startOffset": 47, "endOffset": 77}, {"referenceID": 3, "context": "The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system.", "startOffset": 29, "endOffset": 55}, {"referenceID": 0, "context": "Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics. On the other hand, temporal abstractions (i.e., options or skills [Sutton et al., 1999a]) can help an agent to focus less on lower level details of a task and more on high level planning [Dietterich, 2000, Parr, 1998]. The problem with these methods is that finding good abstractions is typically done manually which hampers their wide use. The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system.", "startOffset": 48, "endOffset": 730}, {"referenceID": 0, "context": "Spatial abstractions such as state aggregation [Bertsekas and Castanon, 1989], tries to tackle this problem by grouping states with similar characteristics such as policy behaviour, value function or dynamics. On the other hand, temporal abstractions (i.e., options or skills [Sutton et al., 1999a]) can help an agent to focus less on lower level details of a task and more on high level planning [Dietterich, 2000, Parr, 1998]. The problem with these methods is that finding good abstractions is typically done manually which hampers their wide use. The internal model principle [Francis and Wonham, 1975], \u201dEvery good key must be a model of the lock it opens\u201d, was formulated mathematically for control systems by Sontag [2003], claiming that if a system is solving a control task, it must necessarily contain a subsystem which is capable of predicting the dynamics of the system. In this work we follow the same line of thought and claim that DQNs are learning an underlying spatio-temporal model of the problem, without implicitly being trained to. We identify this model as an Semi Aggregated Markov Decision Process (SAMDP), an approximation of the true MDP that allows human interpretability. Zahavy et al. [2016] used hand-crafted features in order to interpret policies learned by DQN agents.", "startOffset": 48, "endOffset": 1221}, {"referenceID": 9, "context": "[Mnih et al., 2015].", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "DQN is an offline learning algorithm that collects experience tuples {st,at, rt, st+1, \u03b3} and stores them in the Experience Replay (ER) [Lin, 1993].", "startOffset": 136, "endOffset": 147}, {"referenceID": 15, "context": "The optimal skill value function is given by: Q\u03a3(s, \u03c3) = E[R s + \u03b3max \u03c3\u2032\u2208\u03a3 Q\u03a3(s \u2032, \u03c3\u2032)] [Stolle and Precup, 2002].", "startOffset": 88, "endOffset": 113}, {"referenceID": 22, "context": "Zahavy et al. [2016] showed that this state representation captures a spatio-temporal hierarchy and therefore makes a good candidate for state aggregation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "In order to encourage temporal coherency in cluster assignments we define a new linkage criteria based on Ward [1963]: c(A,B) = (1\u2212 \u03bb) \u00b7mean{\u2016xa \u2212 xb\u2016 : a \u2208 A, b \u2208 B}+ \u03bb \u00b7 e{A,B}\u2192AB (4)", "startOffset": 106, "endOffset": 118}, {"referenceID": 4, "context": "3 Evaluation criteria We follow the analysis of [Hallak et al., 2013] and define criteria to measure the fitness of a model empirically.", "startOffset": 48, "endOffset": 69}, {"referenceID": 12, "context": "The Minimum Description Length (MDL; [Rissanen, 1978]) principle is a formalization of the celebrated Occams Razor.", "startOffset": 37, "endOffset": 53}, {"referenceID": 22, "context": "We also see evidence for the \u201dtunnel-digging\u201d option described in [Zahavy et al., 2016] in the transitions between clusters 11,12,14 and 4.", "startOffset": 66, "endOffset": 87}, {"referenceID": 11, "context": "The motivation for this experiment stems from the idea of shared autonomy [Pitzer et al., 2011].", "startOffset": 74, "endOffset": 95}, {"referenceID": 13, "context": "Another question we\u2019re interested in answering is whether a global control structure exists? Motivated by the success of policy distillation ideas [Rusu et al., 2015], it would be interesting to see how well an SAMDP built for game A, explains game B? Finally we would like to use this model to interpret other DRL agents that are not specifically trained to approximate value such as deep policy gradient methods.", "startOffset": 147, "endOffset": 166}], "year": 2016, "abstractText": "Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still lacking the tools to analayze their performance. In this work we present the Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies exhibiting both spatial and temporal hierarchies. We describe its advantages for analyzing trained policies over other modeling approaches, and show that under the right state representation, like that of DQN agents, SAMDP can help to identify skills. We detail the automatic process of creating it from recorded trajectories, up to presenting it on t-SNE maps. We explain how to evaluate its fitness and show surprising results indicating high compatibility with the policy at hand. We conclude by showing how using the SAMDP model, an extra performance gain can be squeezed from the agent.", "creator": "LaTeX with hyperref package"}}}