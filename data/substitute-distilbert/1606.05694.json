{"id": "1606.05694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs", "abstract": "this paper finalized our approach for the detecting stance in tweets task ( semeval - interactive task approach ). we utilized recent advances in short text categorization using depth learning to evaluate word - level and character - level models. the choice between word - level and character - level models in each particular work was informed through optimal performance. our final system is a combination of classifiers using word - level or character - level models. we also employed novel textual storage techniques to expand and diversify our training dataset, thus making our system more robust. our system achieved a macro - accuracy precision, recall weighted mis - scores of 0. 67, 68. 61 and 49. 635 respectively.", "histories": [["v1", "Fri, 17 Jun 2016 22:32:50 GMT  (442kb,D)", "http://arxiv.org/abs/1606.05694v1", "SemEval 2016, San Diego, California. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). San Diego, California"]], "COMMENTS": "SemEval 2016, San Diego, California. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). San Diego, California", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["prashanth vijayaraghavan", "ivan sysoev", "soroush vosoughi", "deb roy"], "accepted": false, "id": "1606.05694"}, "pdf": {"name": "1606.05694.pdf", "metadata": {"source": "CRF", "title": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs", "authors": ["Prashanth Vijayaraghavan", "Ivan Sysoev", "Soroush Vosoughi"], "emails": ["pralav@mit.edu,", "isysoev@mit.edu,", "soroush@mit.edu,", "dkroy@media.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Stance detection is the task of automatically determining whether the authors of a text are against or in favour of a given target. For instance, take the following sentence: \u201dIt has been such a cold April, so much for global warming.\u201d This sentence\u2019s author is most likely against the concept of global warming (i.e., does not believe in it). The work presented here is specifically targeted towards detecting stance in tweets. The noisy and idiosyncratic nature of tweets make this a particularly hard task.\nAutomatic identification of stance in tweets has practical applications for a range of domains. For instance, it can be used as a sensor to measure the attitude of Twitter users on various issues, such as: political issues, candidates, brand names, TV shows, etc.\nThere has been extensive research done on modelling and automatic detection of stance in political arenas (e.g., debates) (Thomas et al., 2006) and on online forums (Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010). However, as we alluded to earlier, the peculiar nature of tweets make techniques that have been developed for other platforms unsuitable. The field closest to this work is the field of Twitter sentiment classification, where the task is to detect the sentiment of a given tweet, usually as positive, negative, or neutral. Nonetheless, it is important to note that there are substantial differences between sentiment classification and stance detection. Sentiment classifiers determine the polarity of a given tweet, without considering any targets (see Vosoughi et al. (Vosoughi et al., 2015) for an example of a Twitter sentiment classifier). For instance, consider the tweet: \u201dI love Donald Trump\u201d, this tweet has a positive sentiment, and the author of the tweet has a positive stance towards Donald Trump, but it can also be inferred that the author is most likely against or at best neutral towards Bernie Sanders. In this paper, we present a system for automatic detection of stance in Tweets."}, {"heading": "2 Our Approach", "text": "We trained a different model for each of the five targets. Models for some of the targets used characterlevel convolutional neural networks(CNN), while other used word-level models. In one particular target (Hillary Clinton), a combination of characterlevel and word-level models was used. Though Character-level models are robust to the idiosyncratic and noisy nature of tweets, they require a larger dataset compared to word-level models. Our\nar X\niv :1\n60 6.\n05 69\n4v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\nchoice between the models was informed by validation performance (as explained in section 5). The character and word-level models are explained in the section below."}, {"heading": "2.1 Character-Level CNN Tweet Model", "text": "Character-level CNN (CharCNN) is a slight variant of the deep character level convolutional neural network introduced by Zhang et al (Zhang and LeCun, 2015), based on the success of CNNs in image recognition tasks (Girshick et al., 2014) (Hinton et al., 2012). In this model, we perform temporal (one-dimensional) convolutional and max-pooling operations. Given a discrete input function f(x) \u2208 [1, l] 7\u2192 R, a discrete kernel function k(x) \u2208 [1,m] 7\u2192 R and stride s, the convolution g(y) \u2208 [1, (l\u2212m+1)/s] 7\u2192 R between k(x) and f(x) and pooling operation h(y) \u2208 [1, (l \u2212m + 1)/s] 7\u2192 R of f(x) is calculated as:\ng(y) = m\u2211\nx=1\nk(x) \u00b7 f(y \u00b7 s\u2212 x+ c) (1)\nh(y) = maxmx=1f(y \u00b7 s\u2212 x+ c) (2)\nwhere c = m \u2212 s + 1 is an offset constant. In our implementation of the model, the stride s is set to 1.\nThis model is illustrated in Figure 1. We adapted this model for the size limit of tweets (140 characters). The character set includes English alphabets, numbers, special characters and unknown character. There are 70 characters in total, given below:\nabcdefghijklmnopqrstuvw xyz0123456789-,;.!?:\u2019\"/ \\| #$%&\u02c6*\u02dc\u2018+-=<>()[]{}\nEach character in the tweet can be encoded using one-hot vector xi \u2208 {0, 1}70. Hence, a tweet is represented as a binary matrix x1..150 \u2208 {0, 1}150x70 with padding wherever necessary, where 150 is the maximum number of characters in a tweet plus padding and 70 is the size of the character set. Each tweet, in the form of a matrix, is now fed into a deep model consisting of four 1-d convolutional layers. A convolution operation employs a filter w, to extract l-gram character feature from a sliding window of l characters at the first layer and learns abstract textual features in the subsequent layers. This filter\nw is applied across all possible windows of size l to produce a feature map. A sufficient number (f ) of such filters are used to model the rich structures in the composition of characters. Generally, with tweet s, each element c(h,F )i (s) of a feature map F at the layer h is generated by:\nc (h,F ) i (s) = g(w (h,F ) c\u0302(h\u22121)i (s) + b (h,F )) (3)\nwhere w(h,F ) is the filter associated with feature map F at layer h; c\u0302(h\u22121)i denotes the segment of output of layer h\u22121 for convolution at location i (where c\u0302 (0) i = xi...i+l\u22121 \u2014 one-hot vectors of l characters from tweet s); b(h,F ) is the bias associated with that filter at layer h; g is a rectified linear unit and is element-wise multiplication. The output of the convolutional layer ch(s) is a matrix, the columns of which are feature maps c(h,Fk)(s)|k \u2208 1..f .\nThe output of the convolutional layer is followed by a 1-d max-overtime pooling operation (Collobert et al., 2011) over the feature map and selects the maximum value as the prominent feature from the current filter. Pooling size may vary at each layer (given by p(h) at layer h). The pooling operation shrinks the size of the feature representation and filters out trivial features like unnecessary combination of characters (in the initial layer). The window length l, number of filters f , pooling size p at each layer can vary for each classification task.\nThe output from the last convolutional layer is flattened and passed into a series of fully connected layers. The output of the final fully connected layer (sigmoid or softmax) gives a probability distribution over categories in our classification task. For regularization we apply a dropout (Hinton et al., 2012) mechanism after the first fully connected layer. This prevents co-adaptation of hidden units by randomly setting a proportion \u03c1 of the hidden units to zero (Generally, we set \u03c1 = 0.5). CharCNN can be robust to misspellings and noise, provided there is sufficiently large dataset to train the model."}, {"heading": "2.2 Convolutional Word-Embedding Model", "text": "The convolutional embedding model (see Figure 2) assigns a d dimensional vector to each of the n words of an input tweet resulting in a matrix of size n\u00d7d. Each of these vectors are initialized with uniformly distributed random numbers i.e. xi \u2208 Rd. The model, though randomly initialized, will eventually learn a look-up matrix R|V |\u00d7d where |V | is the vocabulary size, which represents the word embedding for the words in the vocabulary.\nA convolution layer is then applied to the n \u00d7 d input tweet matrix, which takes into consideration all the successive windows of size l, sliding over the entire tweet. A filter w \u2208 Rh\u00d7d operates on the tweet to give a feature map c \u2208 Rn\u2212l+1. We apply a max-pooling function (Collobert et al., 2011) of size p = (n \u2212 l + 1) shrinking the size of the resultant matrix by p. In this model, we do not have several hierarchical convolutional layers - instead we apply convolution and max-pooling operations with f filters on the input tweet matrix for different window sizes (l).\nThe vector representations derived from various window sizes can be interpreted as prominent ngram word features for the tweets. These features are concatenated to give a vector of size f \u00d7 L, where L is the number of different l values which is further compressed to a size k before passing it to a fully connected softmax or a sigmoid layer whose output is the probability distribution over different categories of our classification task."}, {"heading": "3 Model Training", "text": "We trained the CharCNN model and the WordEmbedding convolutional model for different targets and selected the best model for each of them. In our task, the tweets are classified into three categories: Favor, Against, and None. We defined the ground truth vector p as a one-hot vector. The com-\nmonly used hyperparameters for the convolutional layers of our CharCNN are: f = 256, l = 7 (first two layers) and l = 3 (other 3 layers). The sizes of the fully connected layers in our CharCNN model are 1,024 and 512.\nSimilarly, the commonly used hyperparameters of the Convolutional Word-Embedding model are: l = 2, 3, 4, f = 200, d = 300, k = 256. Softmax layer takes the output from the penultimate layers of the corresponding models, thereby generating a distribution over the three classes in our task. The class with the maximum probability is the label for the given input tweet.\nTo learn the parameters of the model we minimize the cross-entropy loss as the training objective using the Adam Optimization algorithm (Kingma and Ba, 2014). It is given by\nCrossEnt(p, q) = \u2212 \u2211 p(x) log(q(x)) (4)\nwhere p is the true distribution (1-of-C representation of ground truth) and q is the output of the softmax. This, in turn, corresponds to computing the negative log-probability of the true class. Each of the classifiers were trained for approximately 8-10 epochs.\nIn order to deal with the imbalance in the data, we used a simple balancing technique: to choose a sample on each training step, we randomly picked a class and then randomly selected a tweet associated with this class."}, {"heading": "4 Training Set Expansion", "text": "We expanded the training set by collecting additional tweets for each target-stance pair from the Twitter historical archives. To form a query for the historical API, we automatically selected 40 representative hashtags for each target-stance pair and manually filtered the resulting hashtags lists. The total amount of additional tweets was 1.7 million. Since number of collected tweets vastly exceeded the size of the official dataset, we decided to abstain from using the latter for training and instead use it for validation purposes. For some targets (mentioned in the section 5.2), we augmented the collected set with tweets obtained by replacing some words and phrases with similar ones, using Word2Vec."}, {"heading": "4.1 Identifying Representative Hashtags", "text": "We found hashtags well-suited for forming a data expansion query. Hashtags are commonly used to represent a \u201ctopic\u201d or \u201ctheme\u201d of a tweet and thus often convey information of both the target and the stance (e.g. #stophillary2016).\nWe measured the strength of association between a hashtag and a particular target-stance pair by computing mutual information between them. More precisely, we defined two indicator variables for hashtag occurrences in tweets:\n1. Whether the current hashtag is equal to the hashtag of interest.\n2. Whether the tweet has the target and the stance of interest.\nThe mutual information between two random variables is computed as:\nI(X,Y ) = \u2211\nx\u2208X,y\u2208Y p(x, y)log\np(x, y)\np(x)p(y) (5)\nWe estimate mutual information between our indicator variables using a Bayesian approach. We find the expected value of mutual information, assuming an uninformed Dirichlet prior on the joint distribution of the two variables. It can be approximately computed using the formula provided in (Hutter, 2002):\nE[I] \u2248 \u2211\ni,j\u2208{0,1}\nnij n log nijn ni+n+j + 0.5 n (6)\nWhere nij is the count of samples with indicator variables assuming values i and j respectively, corrected by a pseudo-count of 0.5; ni+ = \u2211 j nij and\nn+j = \u2211\ni nij . To get a more reliable estimation of hashtag frequencies for tweets unrelated to the targets, we collected a \u201cbackground\u201d sample of 1.2 million English-language tweets. We treated these tweets as having no stance in relation to any of the targets and used them in computation of the counts above.\nFor each target-stance pair, we selected 40 hashtags with highest mutual information for further manual filtering. Samples of selected hashtags can\nbe seen in Table 1. The manual filtering step was necessary, since the statistical association with a target-stance pair could only serve as a proxy for the fact that the tag explicitly expresses the target and the stance. For example, #tcot (standing for \u201ctop conservatives on Twitter\u201d) was highly associated with the stance \u201cAGAINST\u201d for the target \u201cClimate Change is a Real Concern\u201d, but not explicitly expressing this stance. Although we did not make the identification of representative hashtags completely automatic, we found that hashtag filtering is a very manageable task for the annotator, taking only an hour of time for all five targets, making it an ideal place to introduce minimal human input."}, {"heading": "4.2 Collecting and Preprocessing Tweets", "text": "As can be seen in Table 2, the collected tweets are very unevenly distributed between target-stance pairs. There are two causes for this: the uneven distribution of different stances on Twitter in general, and uneven number of representative hashtags that we were able to associate with each target-stance pair. For instance, the pair \u201cClimate Change is a Global Concern: AGAINST\u201d was represented by only 15 tweets in the training data that was provided, limiting us to only two representative hashtags. Since our deep learning models require balanced amount of samples, we used the balancing technique described in the previous section.\nTo eliminate the possibility that resulting classifiers would only learn the hashtags in the query,\nwe removed these hashtags from the majority of the collected tweets, keeping them only in 25% of the tweets."}, {"heading": "4.3 Augmenting Data Using Word2Vec", "text": "Data augmentation techniques are widely used to enhance generalization of models with respect to input transformations that are known to not affect the output significantly. An example application of data augmentation in NLP can be found in (Zhang and LeCun, 2015), where they used thesaurus-based synonym replacement (WordNet (Fellbaum, 1998)) to generate additional training samples. We applied the technique used by Zhang et al (Zhang and LeCun, 2015) to our task, with the difference that we used Word2Vec (Mikolov et al., 2013) instead of a thesaurus to find similar words. The underlying intuition was that Word2Vec can provide better coverage for phrases related to our targets.\nThe algorithm of the data augmentation is as follows. At every step, we randomly selected a tweet from the non-augmented training set. We sampled a number r of words/phrases we would like to replace from a geometric distribution with parameter p. We then randomly sampled r words/phrases, that are part of the Word2Vec vocabulary from the current tweet. (if r was larger than number of available words/phrases n, we used r mod n.) For each of these words/phrases, we retrieved a list of most similar ones in terms of cosine similarity of Word2Vec vectors. We ordered the list in decreasing order of similarity and truncated it to not include items with similarity less than threshold t. We then sampled index s of selected replacement from another geometric distribution with parameter q (again, we used modulo if s was too big). The original words/phrases were then replaced, and the tweet was added to the augmented dataset. The particular values of p, q and t were 0.5, 0.5 and 0.25 respectively. Using this method, we generated 500,000 extra tweets for each target-stance pair."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Baseline", "text": "To have a better sense of our approach\u2019s performance, we compared results against a simple baseline. We built a set of Naive Bayes classifiers using bag-of-word features and optimized their pa-\nrameters using 20-fold cross validation on original training data. We experimented with different thresholds on word count for a word to be included into vocabulary. We also set up separate thresholds for hashtags and at-mentions. After selecting the most promising values of thresholds, priors and the smoothing parameter, we ran the Naive Bayes classifiers on the test data to obtain results shown in Table 3."}, {"heading": "5.2 Validation Results", "text": "We trained the models using the collected dataset and validated them on the training set provided for the task. The validation results informed the choice between the word-level and the character-level classifiers for each target. Without Word2Vec augmentation, character-level classifier achieved the best performance only for the target \u201cFeminist Movement\u201d. When Word2Vec augmentation was introduced, the character-level model achieved the best performance for the target \u201cClimate Change\u201d and the stance \u201dFAVOR\u201d of the target \u201dHillary Clinton\u201d. The word-level model performed better for the targets: \u201cLegalization of Abortion\u201d, \u201cAtheism\u201d and the stance \u201dAGAINST\u201d of the target \u201cHillary Clinton\u201d. We were able to achieve better average performance for the target \u201dHillary Clinton\u201d by combining character-level and word-level classifiers with a simple heuristic: whenever character-level model\npredicts \u201cAGAINST\u201d, use that decision, otherwise resort to the decision of word-level model.\nTable 4 compares the performance of the character-level and word-level classifiers for the targets where character-level classifiers yielded an advantage. The macro-average F1 validation score was 0.65."}, {"heading": "5.3 SemEval Competition Results", "text": "Our model was able to achieve a Macro F-score of 0.6354 (placing us eighth out of 19 teams), while the best performing model had a Macro F-score of 0.6782. Table 5 details results on the test data for each target and stance."}, {"heading": "6 Discussion and Future Work", "text": "An interesting result of our work was that given enough data, character-level models outperformed word-level models for tweet classification (with a\ndramatic improvement in case of \u201dHillary Clinton: FAVOR\u201d). Due to the lack of data, it was necessary to resort to a data augmentation technique to generate sufficient amount and diversity of data for character-level model to show its advantage. Another interesting finding from our work is the suitability of word2vec-based substitution as a data augmentation technique. As far as we know, word2vec has not previously been used for data augmentation in this manner.\nAs can be seen in Table 5, our system did not perform too well for certain target-stance pairs (e.g., Atheism-Against). We hypothesize that the reason for this is the noise and the limited size of the collected training data. Thus, we believe that the performance of the system can be improved through better data expansion and cleaning techniques.\nWe see several avenues for future improvements. First, it might be beneficial to use unsupervised pretraining for our models (e.g., using autoencoders for Twitter (Vosoughi et al., 2016)). Second, data cleaning can potentially be improved using bootstrapping. This would entail using our current models (optimized for high precision) to gather cleaner data for the second tier of models. It could be repeated while validation performance improves. Finally, because of the constraints of this SemEval task, we did not manually select hashtags or terms commonly associated with target-stance pairs. Inclusion of such hashtags can potentially boost the quality of the dataset, leading to better performance of our models."}], "references": [{"title": "Natural language processing (almost) from scratch", "author": ["Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kuksa.,? \\Q2011\\E", "shortCiteRegEx": "Kuksa.", "year": 2011}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Distribution of mutual information", "author": ["Marcus Hutter"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Support or oppose?: classifying positions in online debates from reply activities and opinion expressions", "author": ["Murakami", "Raymond2010] Akiko Murakami", "Rudy Raymond"], "venue": "In Proceedings of the 23rd International Conference on Computational", "citeRegEx": "Murakami et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murakami et al\\.", "year": 2010}, {"title": "Recognizing stances in online debates", "author": ["Somasundaran", "Wiebe2009] Swapna Somasundaran", "Janyce Wiebe"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language", "citeRegEx": "Somasundaran et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2009}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Thomas et al.2006] Matt Thomas", "Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 2006 conference on empirical methods", "citeRegEx": "Thomas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Enhanced twitter sentiment classification using contextual information. In 6TH Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA", "author": ["Helen Zhou", "Deb Roy"], "venue": null, "citeRegEx": "Vosoughi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vosoughi et al\\.", "year": 2015}, {"title": "Tweet2vec: Learning tweet embeddings using character-level cnn-lstm encoder-decoder", "author": ["Prashanth Vijayaraghavan", "Deb Roy"], "venue": "In Proceedings of the 39th International ACM SIGIR Conference on Research and De-", "citeRegEx": "Vosoughi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vosoughi et al\\.", "year": 2016}, {"title": "Text understanding from scratch. arXiv preprint arXiv:1502.01710", "author": ["Zhang", "LeCun2015] Xiang Zhang", "Yann LeCun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": ", debates) (Thomas et al., 2006) and on online forums (Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010).", "startOffset": 11, "endOffset": 32}, {"referenceID": 9, "context": "(Vosoughi et al., 2015) for an example of a Twitter sentiment classifier).", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Character-level CNN (CharCNN) is a slight variant of the deep character level convolutional neural network introduced by Zhang et al (Zhang and LeCun, 2015), based on the success of CNNs in image recognition tasks (Girshick et al., 2014) (Hinton et al.", "startOffset": 214, "endOffset": 237}, {"referenceID": 2, "context": ", 2014) (Hinton et al., 2012).", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "For regularization we apply a dropout (Hinton et al., 2012) mechanism after the first fully connected layer.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": "It can be approximately computed using the formula provided in (Hutter, 2002):", "startOffset": 63, "endOffset": 77}, {"referenceID": 5, "context": "We applied the technique used by Zhang et al (Zhang and LeCun, 2015) to our task, with the difference that we used Word2Vec (Mikolov et al., 2013) instead of a thesaurus to find similar words.", "startOffset": 124, "endOffset": 146}, {"referenceID": 10, "context": ", using autoencoders for Twitter (Vosoughi et al., 2016)).", "startOffset": 33, "endOffset": 56}], "year": 2016, "abstractText": "This paper describes our approach for the Detecting Stance in Tweets task (SemEval-2016 Task 6). We utilized recent advances in short text categorization using deep learning to create word-level and character-level models. The choice between word-level and characterlevel models in each particular case was informed through validation performance. Our final system is a combination of classifiers using word-level or character-level models. We also employed novel data augmentation techniques to expand and diversify our training dataset, thus making our system more robust. Our system achieved a macro-average precision, recall and F1-scores of 0.67, 0.61 and 0.635 respectively.", "creator": "LaTeX with hyperref package"}}}