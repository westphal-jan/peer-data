{"id": "1606.05759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2016", "title": "Egyptian Arabic to English Statistical Machine Translation System for NIST OpenMT'2015", "abstract": "the paper describes the egyptian arabic - to - english statistical machine translation ( smt ) system that israeli qcri - columbia - nyuad ( qcn ) group submitted to the nist openmt'2015 competition. the competition focused on teaching dialectal arabic, as used in chemistry, chat, and speech. thus, our group focused on processing and standardizing sequences, e. g., using tools such as 3arrib and swift. we further trained a phrase - filling smt translation using state - of - the - art features and transformations such into operation sequence model, class - based language model, sparse translation, neural pattern database model, genre - based hierarchically - patterned language evolution, unsupervised transliteration mining, phrase - table merging, and hypothesis combination. our dissertation ranked second on considering three genres.", "histories": [["v1", "Sat, 18 Jun 2016 14:34:07 GMT  (26kb)", "http://arxiv.org/abs/1606.05759v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hassan sajjad", "nadir durrani", "francisco guzman", "preslav nakov", "ahmed abdelali", "stephan vogel", "wael salloum", "ahmed el kholy", "nizar habash"], "accepted": false, "id": "1606.05759"}, "pdf": {"name": "1606.05759.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 75\n9v 1\n[ cs\n.C L\n] 1\n8 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "We describe the Egyptian Arabic-to-English statistical machine translation (SMT) system of the QCN team for the NIST OpenMT\u20192015 evaluation campaign. The QCN team included the Qatar Computing Research Institute, Columbia University, and New York University in Abu Dhabi.\nThe OpenMT 2015 translation task asked participants to build systems that can translate Egyptian Arabic from three different genres (SMS, chat, and speech) into English. The challenges presented by this multigenre task were many, ranging from scarcity of parallel data for training, to noisy source text and heterogeneity of the references. For example, large portions of the provided data consisted of romanized Egyptian Arabic (aka Arabizi) rather than using the Arabic script.\nTherefore, several preprocessing steps were needed in order to clean the data before building our SMT system. First, we converted all Arabizi to Arabic script; we then normalized it, trying to make it more like MSA. We also morphologically segmented long Arabic words using segmentation schemes that are standard for MSA but harder to do for dialectal Arabic. We used a statistical phrase-based MT system \u2013 Moses (Koehn et al., 2007). We experimented with different data processing schemes and different SMT system settings to achieve better translation quality. Here are the major settings: Egyptian Arabic segmentation (ATB, S2, D3), Egyptian Arabic to MSA conversion, sparse features, class-based models, neural network joint language model, hierarchically-interpolated language model, unsupervised transliteration mining, domain adaptation, and data selection for tuning.\nIt is worth mentioning that given the abovementioned challenges, preprocessing by itself yielded the largest gains. The Egyptian Arabic segmentation gave us an improvement of up to 3 BLEU points. The hierarchically-interpolated language model added 1 extra BLEU point. The sparse features, class-based models, and neural network joint language models further improved translation quality by 0.66, 0.70 and 0.43 BLEU points absolute, respectively. In the next sections, we discuss in detail the different settings and decisions we made when preparing our submission.\nThe remainder of the paper is organized as follows: Section 2 explains the data preprocessing techniques and tools we tried, Section 3 explains in detail the non-standard features and components we tried in our SMT experiments, and Section 4 describes the actual system we submitted for the competition. Finally, Section 5 concludes with a summary of the main points."}, {"heading": "2 Data Preprocessing", "text": "The NIST dataset contained text from three different genres: short text messages (SMS), chat (CHT), and transcribed conversational speech (CTS). We tackled each of the three genres separately, i.e., we built genre-specific systems. Furthermore, we split the provided training data for each genre into separate training and development sets by reserving approximately 3,000 sentences (the number of sentences is approximate as we did the splitting at the document level) from each set for development, and we used the rest for training.\nFor evaluation, the organizers provided two additional datasets: (i) an official devtest dataset (Test), and (ii) a small gold dataset (TestG), which is a subset of Test. The genres of the datasets were specified, and thus there was no need to train a system for automatic genre identification."}, {"heading": "2.1 Modern Standard Arabic Preprocessing", "text": "For training purposes, some datasets for Modern Standard Arabic (MSA) were provided in addition to the Egyptian Arabic data (SMS, CHT, CTS) described above. These datasets consisted mostly of newswire text (i.e., a different genre), and thus we processed them using a standard MSA tool: MADAMIRA in MSA mode (Pasha et al., 2014). The MSA data was used to build a second phrase table to be then combined with some of the genrespecific phrase tables. We planned to match the same morphological tokenization schemes used for Egyptian Arabic, but at the end, we only used ATB segmentation for MSA. For more information on Arabic morphology challenges and tokenization schemes, see Habash (2010)."}, {"heading": "2.2 Genre-Specific Preprocessing", "text": "The CTS data contained speech tags, markers for spelling corrections, etc., which we removed before training. The Arabic side of the SMS and CHT data had elongations and spelling mistakes. Therefore, we normalized the elongations and we standardized the spelling variations.\nParts of the SMS and CHT data contained romanized Arabic text (aka Arabizi), which is Arabic written using the Roman alphabet. In order to homogenize the input data, we converted the Arabizi into the standard Arabic script (utf8).\nGiven the conversational nature of SMS and chat, a further complication with converting romanized text into the Arabic script was that the text was usually affected by code-switching into English; therefore, it was important for us to identify the English words and not to try to convert them to Arabic. We solved these issues using the 3ARRIB tool (Al-Badrashiny et al., 2014).\nOn the English side of the data, there were multiple translation options, where typically the first option was the intended meaning and the second one the literal meaning of the Arabic text. We chose to use the intended meaning only.\nFinally, in all processing, for both Arabic and English, we made sure the emoticons were not affected by the tokenization or the translation."}, {"heading": "2.3 Egyptian Arabic Segmentation", "text": "A major issue when training an SMT system for the present edition of the task is the small size of the provided SMS, CHT and CTS datasets, which means that data sparseness is a severe problem. One common way to reduce it, at least on the Arabic side, where it is more severe, is to segment the Arabic words into multiple tokens, e.g., by separating the main word from the attached conjunctions, pronouns, articles, etc. Since these are separate words in English, such a segmentation not only reduces sparseness, but also yields improved word mapping to English, thus ultimately helping word alignments and translation model estimation for SMT. The value of Arabic tokenization for SMT, especially under low resource conditions, has been demonstrated by a number of researchers in the past, e.g., (Badr et al., 2008; El Kholy and Habash, 2012; Habash et al., 2013; Al-Mannai et al., 2014).\nWe experimented with common segmen-\ntation schemes such as D3, S2 and ATB (Badr et al., 2008; Habash, 2010). For tokenization, we used MADAMIRA (Pasha et al., 2014), a fast and efficient implementation of MADA for MSA (Habash and Rambow, 2005; Habash et al., 2009), and MADA-ARZ, a version of MADA for Egyptian Arabic (Habash et al., 2013). Table 1 compares the results of using different segmentation schemes including no segmentation. We see gains of up to 3 BLEU points absolute on Test when using segmentation compared to no segmentation. However, the differences between the various schemes (ATB, S2 and D3) are small."}, {"heading": "2.4 Egyptian to MSA Conversion", "text": "Another way to reduce data sparseness is by using additional out-of-domain data, e.g., newswire; this means a double domain shift: (i) from an informal text genre to newswire, and also (ii) from dialectal Arabic to MSA. While it is hard to do anything about the domain shift, the dialectal shift is somewhat easier to address. Changes between dialects are often systematic and many of the differences are at the level of individual words.\nPrevious work has shown that converting Egyptian to MSA makes it easier to use MSA resources for translating dialectal Arabic (Mohamed et al., 2012; Salloum and Habash, 2011; Zbib et al., 2012; Salloum and Habash, 2013; Sajjad et al., 2013a; Durrani et al., 2014a). So, we experimented with converting Egyptian to MSA using an inhouse tool (Sajjad et al., 2013a), which performs character-level transformations for each Egyptian word in isolation to generate an MSA version thereof. We then trained an SMT system on this converted data.\nThe results are shown in Table 2. We can see that converting Egyptian to MSA yields improvements that are systematic across the three genres and also across the two test datasets. However, this improvement is not very large and ranges from 0.18 to 0.70 BLEU points absolute.\nFurther segmenting the MSA-like Egyptian using MADA ATB yielded very mixed results: in some cases, it added 0.93 BLEU points absolute, but in other there was a drop of 0.76. This could be because of the highly dialectal nature of the NIST data, which differs from MSA in lexical choice, which our tool cannot handle. Our Egyptian to MSA tool works at the character level and converts to MSA only those Egyptian words that are different at the character level.\nIn general, full conversion of dialectal Arabic to MSA would require not only word-level transformations but also phrase-level ones (Wang et al., 2012), while taking context into account (Nakov and Tiedemann, 2012), and also modeling morphological phenomena (Nakov and Ng, 2011). There are also potential gains from a smarter character alignment models (Tiedemann and Nakov, 2013), or even from using a specialized decoder (Wang and Ng, 2013). Ultimately, the real benefit is when combining the adapted version of the smaller dialect with a large dataset in the bigger dialect (Nakov and Ng, 2012), which we will do below."}, {"heading": "3 Translation System Characteristics", "text": "We started our experiments from a strong baseline system, which was originally designed for MSA to English translation (Sajjad et al., 2013b). We then extended it with some additional models and features (Durrani et al., 2014b). Most notably, we used minimum Bayes risk decoding (MBR) (Kumar and Byrne, 2004), monotone-at-punctuation reordering, dropping of out-of-vocabulary words, operation sequence model for reordering (OSM) (Durrani et al., 2011; Durrani et al., 2013b), a smoothed BLEU+1 version of PRO for parameter tuning (Nakov et al., 2012), etc.\nGiven this baseline system, we experimented with several further extensions, which we will describe below. Some of them were eventually included in our final submission."}, {"heading": "3.1 Genre-Based Hierarchically-Interpolated Language Model", "text": "First, we experimented with building a hierarchically-interpolated language model for each genre (i.e., CTS, SMS, and CHT). We tuned each model to minimize the perplexity on a held-out set for that target genre.\nWe examined the text resources that were available for training an English language model, and we split them into six groups: (1) Egyptian-source (the target sides of the CTS, CHT and CTS training bi-texts), (2) MSA GALE News (GALE P3 {R1,R2},P4{R1,2,3}), (3) Chinese GALE (GALE P2 {BC,BC,BL,NG}), (4) MSA NEWS (newsetirr, news-par, news-trans, ISI), (5) MSA GALE non-news (GALE P1 {BLOG} P2 {BC1, BC2, WEB}), and (6) Gigaword v5, split into four subgroups by year (Guzma\u0301n et al., 2012) (1994-1997, 1998-2001, 2002-2005, 2006-2010).\nFor each such group, (i) we built an individual 5-gram language model with Kneser-Ney smoothing for each member of the group, and (ii) we interpolated these language models into a single language model, minimizing the perplexity for the target genre. Then, (iii) we performed a second (hierarchical) interpolation, this time combining the resulting six group language models, again minimizing the perplexity for the target genre. We used the SRILM toolkit (Stolcke, 2002) to build the language models.\nTable 4 shows the results of using interpolated language model for each domain in comparison with using a language model built on the concatenation of the English side of the SMS, CHT and CTS corpora. The interpolated language model consistently improved all genres with a maximum\nimprovement of up to 1 BLEU points on Test."}, {"heading": "3.2 Translation Model with Sparse Features", "text": "The next thing we experimented with were sparse features (Chiang et al., 2009), which are a recent addition to the Moses SMT toolkit. In particular, we used target and source word insertion features: (i) top 50, and (ii) all. The latter worked better, and thus we only show results for it.\nThe results for all are shown in Table 5, where we can see that sparse features only helped for CHT, while they were harmful for CTS, and yielded mixed results for SMS. Thus, in our final system, we only used them for CHT."}, {"heading": "3.3 Class-Based Language Models", "text": "Next, we experimented with using automatic word clusters, which we computed on the source and on the target sides of the training bi-text using mkcls. We also experimented with OSM models (Durrani et al., 2013a) over cluster IDs (Durrani et al., 2014c; Bisazza and Monz, 2014). Normally, the lexically-driven OSM model falls back to context sizes of 2-3 operations due to data sparseness, but learning operation sequences over cluster IDs enabled us to learn richer translation and reordering patterns that can generalize better in sparse conditions.\nTable 6 shows the experimental results when adding a target language model and an OSM model over cluster IDs. We can see that these class-based models yielded consistent improvements in all cases. We also tried using word2vec (Mikolov et al., 2013) for clustering, but the results did not improve any further and they were occasionally worse than those with mkcls. We tried\nboth with 50 and 500 classes, but using additional classes did not help."}, {"heading": "3.4 Unsupervised Transliteration Models", "text": "A consequence of data sparseness is that at test time, the SMT system would see many unknown or out-of-vocabulary (OOV) words. One way to cope with them is to just pass them through untranslated. This works somewhat fine with newswire text and for languages with (roughly) the same alphabet, e.g., English and Spanish, as many OOVs are likely to be named entities (persons, locations, organizations), and are thus likely to be preserved in translation.\nHowever, for languages with different scripts, such as Arabic and English, passing through is not a good idea, especially when translating into English as words in Arabic script do not naturally appear in English. In that case, it is much safer just to drop the OOVs, which is best done at decoding time; this was indeed our baseline strategy.\nA better way is to transliterate OOV words either during decoding or in a post-processing step (Sajjad et al., 2013c). We also experimented with this approach. For this purpose, we built an unsupervised transliteration model (Durrani et al., 2014d) based on EM as proposed in (Sajjad et al., 2011). Unfortunately, it did not help much, probably because in these informal genres the OOVs are rarely named entities; they are real words, which need actual translation, not transliteration."}, {"heading": "3.5 Neural Network Joint Language Model", "text": "Recently, neural networks have come back from oblivion with the promise to revolutionize NLP. Major performance gains have already been demonstrated for speech recognition, and there have been successful applications to semantics.\nMost importantly for us, last year, very sizable performance gains were also reported for SMT using a neural joint language model or NNJM (Devlin et al., 2014).\nWe tried the Moses implementation of JNLM using the settings described in (Birch et al., 2014). While we managed to achieve consistent improvements for all three genres and for both test sets, as Table 7 shows, these gains are modest, 0.04\u20130.57 BLEU points absolute, which is far from what was reported in (Devlin et al., 2014). It is unclear what the reasons are, but it could have to do with the small size of our training bitexts and the informal genres we are dealing with."}, {"heading": "3.6 Domain Adaptation", "text": "We experimented with different techniques for domain adaptation, trying to combine bi-texts from different genres, e.g., our Egyptian SMS, CHT, and CTS, but also MSA newswire.\nFirst, we experimented with concatenating our SMS, CHT and CTS bitexts for training, but then using genre-specific tuning sets; this did not work as well as some other alternatives. Next, we experimented with building separate phrase tables, one in-domain and one out-of-domain, and then (a) using phrase table backoff, or (b) merging phrase tables and reordering tables as in (Nakov, 2008; Nakov and Ng, 2009; Sajjad et al., 2013b).\nThe results of our domain adaptation experiments when testing on SMS as the target genre are shown in Table 8. Note that the results on Test and on TestG differ a lot, and thus we focus on Test as it is much larger. We can see that the best way to combine SMS, CHT and CTS is simply to concatenate them, which yields +2.5 BLEU points of improvement absolute over training on SMS data only. Further small gains can be achieved by\nmerging the resulting SMS+CHT+CTS phrase table with a phrase table trained on MSA, where the two tables are merged using extra indicator features as described in (Nakov, 2008)."}, {"heading": "3.7 Tuning", "text": "Our phrase-based SMT system combines different features in a log-linear model. We tune the weights for the individual features of that model by optimizing BLEU (Papineni et al., 2002) on a tuning dataset from the same genre as that in the test. We use PRO (Hopkins and May, 2011), but with smoothed BLEU+1 as proposed in (Nakov et al., 2012). We allowed the optimizer to run for up to 25 iterations, and to extract 1000-best lists on each iteration.\nThe choice of tuning set has been shown to have a huge impact on the quality of the learned parameters (Nakov et al., 2013a). In particular, PRO is very sensitive to length, which can result in pathological translations in some circumstances (Nakov et al., 2013b).\nGiven that there were no official development sets for this year, we synthesized datasets that are specific for CTS and SMS, based on sentencelength as a selection criterion.1 This is crucial to remove potentially noisy data. It can also help to speed up the tuning process.\nIn order to achieve this, we filtered all sentence\n1The CHT sentences were generally of reasonable length, and thus we did not apply filtering for them.\npairs for which either the source or the target sentences were shorter than 4 words or longer than 25 words. The cut-offs were determined empirically by analyzing the kernel density estimations (KDE \u03b1 = 0.3).\nTable 9 compares tuning on an unfiltered and on a filtered tuning set. For SMS, using a filtered tuning set yields mixed results: we see a drop in BLEU on Test and a gain on TestG. For CTS, filtering helped both on Test and on TextG, with a gain on the former of +0.63."}, {"heading": "4 Final Submission and Output Combination", "text": "We recombined hypotheses produced (a) by our best individual systems and (b) by other systems that are both relatively strong and can contribute diversity, e.g., using a different word segmentation scheme. For this purpose, we used the Multi-Engine MT system, or MEMT, (Heafield et al., 2009), which has been proven effective in such a setup and has contributed to achieving state-of-the-art results in a related competition in the past (Sajjad et al., 2013b).\nThe results are shown in Table 10. We can see that using output combination yields notable improvement for SMS and CHT. However, for CTS, BLEU dropped by 0.45 points on Test. Thus, we submitted as a primary system the output combination for SMS and CHT, but our best individual system for CTS (which uses D3 segmentation)."}, {"heading": "5 Conclusion", "text": "We presented the Egyptian Arabic-to-English SMT system of the QCN team for the NIST\nOpenMT\u20192015 evaluation campaign. The system was ranked second in the competition on all three genres: SMS, chat, and speech.\nGiven the informal dialectal nature of these genres, we benefited from careful pre-processing, cleaning, and normalization, which yielded an improvement of up to 3 BLEU points over a strong baseline.\nWe further added a number of extra advanced features, which yielded 2.5 more BLEU points of absolute improvement on top of that due to pre-processing. In particular, sparse features contributed 0.7 BLEU points for CHT, class-based models added 0.7 and 0.6 BLEU points for CHT and SMS, respectively, and NNJM yielded gains of up to 0.4 BLEU points absolute."}], "references": [{"title": "Automatic transliteration of romanized dialectal Arabic", "author": ["Ramy Eskander", "Nizar Habash", "Owen Rambow"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language", "citeRegEx": "Al.Badrashiny et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Al.Badrashiny et al\\.", "year": 2014}, {"title": "Unsupervised word segmentation improves dialectal Arabic to English machine translation", "author": ["Hassan Sajjad", "Alaa Khader", "Fahad Al Obaidli", "Preslav Nakov", "Stephan Vogel"], "venue": null, "citeRegEx": "Al.Mannai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Al.Mannai et al\\.", "year": 2014}, {"title": "Segmentation for Englishto-Arabic statistical machine translation", "author": ["Badr et al.2008] Ibrahim Badr", "Rabih Zbib", "James R. Glass"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Badr et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Badr et al\\.", "year": 2008}, {"title": "Edinburgh SLT and MT system description for the IWSLT 2014 evaluation", "author": ["Matthias Huck", "Nadir Durrani", "Nikolay Bogoychev", "Philipp Koehn"], "venue": "In Proceedings of the 11th International Workshop on Spo-", "citeRegEx": "Birch et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Birch et al\\.", "year": 2014}, {"title": "Class-based language modeling for translating into morphologically rich languages", "author": ["Bisazza", "Monz2014] Arianna Bisazza", "Christof Monz"], "venue": "In Proceedings of the 25th Annual Conference on Computational Linguistics,", "citeRegEx": "Bisazza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bisazza et al\\.", "year": 2014}, {"title": "11,001 new features for statistical machine translation", "author": ["Chiang et al.2009] David Chiang", "Kevin Knight", "Wei Wang"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Chiang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A joint sequence translation model with integrated reordering", "author": ["Helmut Schmid", "Alexander Fraser"], "venue": "In Proceedings of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Durrani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2011}, {"title": "Model with minimal translation units, but decode with phrases", "author": ["Alexander Fraser", "Helmut Schmid"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Durrani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Can Markov models over minimal translation units help phrase-based SMT", "author": ["Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn"], "venue": null, "citeRegEx": "Durrani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Improving Egyptian-to-English SMT by mapping Egyptian into MSA", "author": ["Yaser Al-Onaizan", "Abraham Ittycheriah"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "Durrani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14", "author": ["Barry Haddow", "Philipp Koehn", "Kenneth Heafield"], "venue": "In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Durrani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Investigating the usefulness of generalized word representations in SMT", "author": ["Philipp Koehn", "Helmut Schmid", "Alexander Fraser"], "venue": "In Proceedings of the 25th Annual Conference on Computational Linguistics,", "citeRegEx": "Durrani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Integrating an unsupervised transliteration model into statistical machine translation", "author": ["Hassan Sajjad", "Hieu Hoang", "Philipp Koehn"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the ACL,", "citeRegEx": "Durrani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Orthographic and morphological processing for English\u2013Arabic statistical machine translation", "author": ["El Kholy", "Habash2012] Ahmed El Kholy", "Nizar Habash"], "venue": "Machine Translation,", "citeRegEx": "Kholy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kholy et al\\.", "year": 2012}, {"title": "QCRI at WMT12: Experiments in Spanish-English and German-English machine translation of news text", "author": ["Preslav Nakov", "Ahmed Thabet", "Stephan Vogel"], "venue": "In Proceedings of the Seventh Workshop", "citeRegEx": "Guzm\u00e1n et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guzm\u00e1n et al\\.", "year": 2012}, {"title": "Arabic tokenization, part-ofspeech tagging and morphological disambiguation in one fell swoop", "author": ["Habash", "Rambow2005] Nizar Habash", "Owen Rambow"], "venue": "In Proceedings of the 43rd Annual Meeting on Association", "citeRegEx": "Habash et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2005}, {"title": "MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization", "author": ["Habash et al.2009] Nizar Habash", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Habash et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2009}, {"title": "Morphological analysis and disambiguation for dialectal Arabic", "author": ["Habash et al.2013] Nizar Habash", "Ryan Roth", "Owen Rambow", "Ramy Eskander", "Nadi Tomeh"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Associ-", "citeRegEx": "Habash et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2013}, {"title": "Introduction to Arabic Natural Language Processing", "author": ["Nizar Habash"], "venue": null, "citeRegEx": "Habash.,? \\Q2010\\E", "shortCiteRegEx": "Habash.", "year": 2010}, {"title": "Machine translation system combination with flexible word ordering", "author": ["Greg Hanneman", "Alon Lavie"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation,", "citeRegEx": "Heafield et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2009}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL \u201907, pages 177\u2013180, Prague, Czech Republic.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["Kumar", "Byrne2004] Shankar Kumar", "William Byrne"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association", "citeRegEx": "Kumar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2004}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Transforming Standard Arabic to Colloquial Arabic", "author": ["Mohamed et al.2012] Emad Mohamed", "Behrang Mohit", "Kemal Oflazer"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "Improved statistical machine translation for resource-poor languages using related resource-rich languages", "author": ["Nakov", "Ng2009] Preslav Nakov", "Hwee Tou Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Nakov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2009}, {"title": "Translating from morphologically complex languages: A paraphrase-based approach", "author": ["Nakov", "Ng2011] Preslav Nakov", "Hwee Tou Ng"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Nakov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2011}, {"title": "Improving statistical machine translation", "author": ["Nakov", "Ng2012] Preslav Nakov", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages", "author": ["Nakov", "Tiedemann2012] Preslav Nakov", "J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Optimizing for sentencelevel BLEU+1 yields short translations", "author": ["Nakov et al.2012] Preslav Nakov", "Francisco Guzm\u00e1n", "Stephan Vogel"], "venue": "In Proceedings of the International Conference on Computational Linguistics,", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Parameter optimization for statistical machine translation: It pays to learn from hard examples", "author": ["Nakov et al.2013a] Preslav Nakov", "Fahad Al Obaidli", "Francisco Guzm\u00e1n", "Stephan Vogel"], "venue": null, "citeRegEx": "Nakov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2013}, {"title": "A tale about PRO and monsters", "author": ["Nakov et al.2013b] Preslav Nakov", "Francisco Guzm\u00e1n", "Stephan Vogel"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Nakov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2013}, {"title": "Improving English-Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing", "author": ["Preslav Nakov"], "venue": "In Proceedings of the Third Workshop on Statistical Machine Trans-", "citeRegEx": "Nakov.,? \\Q2008\\E", "shortCiteRegEx": "Nakov.", "year": 2008}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "MADAMIRA: A fast, comprehensive tool for morphological anal", "author": ["Pasha et al.2014] Arfath Pasha", "Mohamed AlBadrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan M Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "An algorithm for unsupervised transliteration mining with an application to word alignment", "author": ["Sajjad et al.2011] Hassan Sajjad", "Alexander Fraser", "Helmut Schmid"], "venue": "In Proceedings of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Sajjad et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sajjad et al\\.", "year": 2011}, {"title": "Translating dialectal Arabic to English", "author": ["Kareem Darwish", "Yonatan Belinkov"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Sajjad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sajjad et al\\.", "year": 2013}, {"title": "QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic spoken language translation", "author": ["Francisco Guzmn", "Preslav Nakov", "Ahmed Abdelali", "Kenton Murray", "Fahad Al Obaidli", "Stephan Vogel"], "venue": null, "citeRegEx": "Sajjad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sajjad et al\\.", "year": 2013}, {"title": "QCRI-MES submission at WMT13: Using transliteration mining to improve statistical machine translation", "author": ["Svetlana Smekalova", "Nadir Durrani", "Alexander Fraser", "Helmut Schmid"], "venue": null, "citeRegEx": "Sajjad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sajjad et al\\.", "year": 2013}, {"title": "Dialectal to standard Arabic paraphrasing to improve Arabic-English statistical machine translation", "author": ["Salloum", "Habash2011] Wael Salloum", "Nizar Habash"], "venue": "In Proceedings of the First Workshop on Algorithms and Resources for Modelling", "citeRegEx": "Salloum et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Salloum et al\\.", "year": 2011}, {"title": "Dialectal Arabic to English machine translation: Pivoting through Modern Standard Arabic", "author": ["Salloum", "Habash2013] Wael Salloum", "Nizar Habash"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Associ-", "citeRegEx": "Salloum et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Salloum et al\\.", "year": 2013}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of the International Speech Communication Association,", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets", "author": ["Tiedemann", "Nakov2013] J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": "In Proceedings of the International Conference Recent Advances in Natural Language", "citeRegEx": "Tiedemann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2013}, {"title": "A beam-search decoder for normalization of social media text with application to machine translation", "author": ["Wang", "Ng2013] Pidong Wang", "Hwee Tou Ng"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Source language adapta", "author": ["Wang et al.2012] Pidong Wang", "Preslav Nakov", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Machine translation of Arabic dialects", "author": ["Zbib et al.2012] Rabih Zbib", "Erika Malchiodi", "Jacob Devlin", "David Stallard", "Spyros Matsoukas", "Richard Schwartz", "John Makhoul", "Omar F. Zaidan", "Chris Callison-Burch"], "venue": "In Proceedings of the 2012 Conference", "citeRegEx": "Zbib et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zbib et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 35, "context": ", a different genre), and thus we processed them using a standard MSA tool: MADAMIRA in MSA mode (Pasha et al., 2014).", "startOffset": 97, "endOffset": 117}, {"referenceID": 19, "context": "For more information on Arabic morphology challenges and tokenization schemes, see Habash (2010).", "startOffset": 83, "endOffset": 97}, {"referenceID": 0, "context": "We solved these issues using the 3ARRIB tool (Al-Badrashiny et al., 2014).", "startOffset": 45, "endOffset": 73}, {"referenceID": 2, "context": "tation schemes such as D3, S2 and ATB (Badr et al., 2008; Habash, 2010).", "startOffset": 38, "endOffset": 71}, {"referenceID": 19, "context": "tation schemes such as D3, S2 and ATB (Badr et al., 2008; Habash, 2010).", "startOffset": 38, "endOffset": 71}, {"referenceID": 35, "context": "For tokenization, we used MADAMIRA (Pasha et al., 2014), a fast and efficient implementation of MADA for MSA (Habash and Rambow, 2005; Habash et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 17, "context": ", 2014), a fast and efficient implementation of MADA for MSA (Habash and Rambow, 2005; Habash et al., 2009), and MADA-ARZ, a version of MADA for Egyptian Arabic (Habash et al.", "startOffset": 61, "endOffset": 107}, {"referenceID": 18, "context": ", 2009), and MADA-ARZ, a version of MADA for Egyptian Arabic (Habash et al., 2013).", "startOffset": 61, "endOffset": 82}, {"referenceID": 25, "context": "Previous work has shown that converting Egyptian to MSA makes it easier to use MSA resources for translating dialectal Arabic (Mohamed et al., 2012; Salloum and Habash, 2011; Zbib et al., 2012; Salloum and Habash, 2013; Sajjad et al., 2013a; Durrani et al., 2014a).", "startOffset": 126, "endOffset": 264}, {"referenceID": 46, "context": "Previous work has shown that converting Egyptian to MSA makes it easier to use MSA resources for translating dialectal Arabic (Mohamed et al., 2012; Salloum and Habash, 2011; Zbib et al., 2012; Salloum and Habash, 2013; Sajjad et al., 2013a; Durrani et al., 2014a).", "startOffset": 126, "endOffset": 264}, {"referenceID": 45, "context": "In general, full conversion of dialectal Arabic to MSA would require not only word-level transformations but also phrase-level ones (Wang et al., 2012), while taking context into account (Nakov and Tiedemann, 2012), and also modeling morphological phenomena (Nakov and Ng, 2011).", "startOffset": 132, "endOffset": 151}, {"referenceID": 7, "context": "monotone-at-punctuation reordering, dropping of out-of-vocabulary words, operation sequence model for reordering (OSM) (Durrani et al., 2011; Durrani et al., 2013b), a smoothed BLEU+1 version of PRO for parameter tuning (Nakov et al.", "startOffset": 119, "endOffset": 164}, {"referenceID": 28, "context": ", 2013b), a smoothed BLEU+1 version of PRO for parameter tuning (Nakov et al., 2012), etc.", "startOffset": 64, "endOffset": 84}, {"referenceID": 15, "context": "We examined the text resources that were available for training an English language model, and we split them into six groups: (1) Egyptian-source (the target sides of the CTS, CHT and CTS training bi-texts), (2) MSA GALE News (GALE P3 {R1,R2},P4{R1,2,3}), (3) Chinese GALE (GALE P2 {BC,BC,BL,NG}), (4) MSA NEWS (newsetirr, news-par, news-trans, ISI), (5) MSA GALE non-news (GALE P1 {BLOG} P2 {BC1, BC2, WEB}), and (6) Gigaword v5, split into four subgroups by year (Guzm\u00e1n et al., 2012) (1994-1997,", "startOffset": 465, "endOffset": 486}, {"referenceID": 42, "context": "We used the SRILM toolkit (Stolcke, 2002) to build", "startOffset": 26, "endOffset": 41}, {"referenceID": 5, "context": "The next thing we experimented with were sparse features (Chiang et al., 2009), which are a recent addition to the Moses SMT toolkit.", "startOffset": 57, "endOffset": 78}, {"referenceID": 24, "context": "We also tried using word2vec (Mikolov et al., 2013) for clustering, but the results did not improve any further and they were oc-", "startOffset": 29, "endOffset": 51}, {"referenceID": 36, "context": ", 2014d) based on EM as proposed in (Sajjad et al., 2011).", "startOffset": 36, "endOffset": 57}, {"referenceID": 6, "context": "Most importantly for us, last year, very sizable performance gains were also reported for SMT using a neural joint language model or NNJM (Devlin et al., 2014).", "startOffset": 138, "endOffset": 159}, {"referenceID": 3, "context": "We tried the Moses implementation of JNLM using the settings described in (Birch et al., 2014).", "startOffset": 74, "endOffset": 94}, {"referenceID": 6, "context": "57 BLEU points absolute, which is far from what was reported in (Devlin et al., 2014).", "startOffset": 64, "endOffset": 85}, {"referenceID": 33, "context": "Next, we experimented with building separate phrase tables, one in-domain and one out-of-domain, and then (a) using phrase table backoff, or (b) merging phrase tables and reordering tables as in (Nakov, 2008; Nakov and Ng, 2009; Sajjad et al., 2013b).", "startOffset": 195, "endOffset": 250}, {"referenceID": 33, "context": "merging the resulting SMS+CHT+CTS phrase table with a phrase table trained on MSA, where the two tables are merged using extra indicator features as described in (Nakov, 2008).", "startOffset": 162, "endOffset": 175}, {"referenceID": 34, "context": "We tune the weights for the individual features of that model by optimizing BLEU (Papineni et al., 2002) on a tuning dataset from the same genre as that in the test.", "startOffset": 81, "endOffset": 104}, {"referenceID": 28, "context": "(Nakov et al., 2012).", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "For this purpose, we used the Multi-Engine MT system, or MEMT, (Heafield et al., 2009), which has been proven ef-", "startOffset": 63, "endOffset": 86}], "year": 2016, "abstractText": "The paper describes the Egyptian Arabicto-English statistical machine translation (SMT) system that the QCRI-ColumbiaNYUAD (QCN) group submitted to the NIST OpenMT\u20192015 competition. The competition focused on informal dialectal Arabic, as used in SMS, chat, and speech. Thus, our efforts focused on processing and standardizing Arabic, e.g., using tools such as 3arrib and MADAMIRA. We further trained a phrase-based SMT system using state-of-the-art features and components such as operation sequence model, class-based language model, sparse features, neural network joint model, genrebased hierarchically-interpolated language model, unsupervised transliteration mining, phrase-table merging, and hypothesis combination. Our system ranked second on all three genres.", "creator": "LaTeX with hyperref package"}}}