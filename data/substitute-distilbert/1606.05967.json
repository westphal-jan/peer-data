{"id": "1606.05967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "A Nonparametric Bayesian Approach for Spoken Term detection by Example Query", "abstract": "state of the art speech recognition systems use data - intensive context - dependent phonemes as acoustic units. commonly, these components don't translate well to low skill languages where large amounts grammatical training data is not available. for such languages, automatic discovery of acoustic units is critical. accompanying this paper, we demonstrate the application of nonparametric bayesian models exploring acoustic unit discovery. we show that the sensory units are correlated with productivity and therefore are linguistically meaningful. we demonstrate present applied spoken term detection ( std ) simulation example query algorithm performs on these automatically learned units. we show that commonly proposed system produces a p @ n of 15. 2 % and an eer of 13. 95 % upon the timit dataset. the benefit in the eer is 5 % while p @ n is only slightly lower than the best reported system behind the literature.", "histories": [["v1", "Mon, 20 Jun 2016 04:06:23 GMT  (648kb)", "http://arxiv.org/abs/1606.05967v1", "interspeech 2016"]], "COMMENTS": "interspeech 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir hossein harati nejad torbati", "joseph picone"], "accepted": false, "id": "1606.05967"}, "pdf": {"name": "1606.05967.pdf", "metadata": {"source": "CRF", "title": "A NONPARAMETRIC BAYESIAN APPROACH FOR SPOKEN TERM DETECTION BY EXAMPLE QUERY", "authors": ["Amir Hossein Harati", "Nejad Torbati", "Joseph Picone"], "emails": ["Amir.harati@gmail.com,", "joseph.picone@temple.edu", "P@N", "P@N"], "sections": [{"heading": null, "text": "1. \u00a0 Introduction Acoustic unit discovery is a critical issue in many speech recognition tasks where there are limited resources available. Though traditional context-dependent phone models perform well when there is ample data, automatic discovery of acoustic units (ADU) offers the potential to provide good performance for resource deficient languages with complex linguistic structures (e.g., African click languages). Since only a small fraction of the world\u2019s 6,000 languages are currently supported by robust speech technology, this remains a critical problem.\nMost approaches to automatic discovery of acoustic units [1]-[3] do this in two steps: segmentation and clustering. Segmentation is accomplished using a heuristic method that detects changes in energy and/or spectrum. Similar segments are then clustered using an agglomerative method such as a decision tree. Advantages of this approach include the potential for higher performance than that obtained using traditional linguistic units, and the ability to automatically discover pronunciation lexicons.\nIn this paper, we propose the use of a nonparametric Bayesian (NPB) model for automatically discovering acoustic units in speech recognition. In our formulation of the problem, the number of acoustic units is unknown. One brute force approach to this problem is to exhaustively search through a model space consisting of many possible parameterizations. However in an NPB model [4][5], the model complexity can be inferred directly from the data. Segmenting an utterance into acoustic units can be approached in a manner similar to that\nused in speaker diarization, where the goal is to segment audio into regions that correspond to a specific speaker. Fox et al. [6] used one state per speaker and demonstrated segmentation without knowing the number of speakers a priori. We have also previously reported on the use of a similar model for speech segmentation that achieves state of the art results [7]. This paper is continuation of our previous work and includes the application of two nonparametric hidden Markov models (HMMs) to the problem of acoustic unit discovery. These models include a Hierarchical Dirichlet Process HMM (HDPHMM) [6] and a Doubly Hierarchical Dirichlet Process HMM (DHDPHMM) [8][9].\nSpoken term detection by example query (STD-EQ) is a problem that involves searching an audio corpus using a spoken query. One simple approach is to convert both the search query and the corpus into text form, often by using a conventional ASR system, and then to perform a text search. A slightly more flexible approach is to use a phonetic lattice to represent the search query and the corpus. However, converting the corpus and the query to text or to phonetic lattices involves using a state of the art speech recognizer that requires significant resources (e.g. dictionaries and transcribed training data). In this paper, we propose a new unsupervised algorithm based on ADUs that can approach state of the art performance amongst unsupervised approaches.\nThe rest of the paper is organized as follows: in Section 2, some background material related to nonparametric approaches used in the rest of the paper is introduced. In Section 33, the ADU transducer is presented. The STD-EQ problem is described in Section 4. In Section 5 some experimental results are discussed. These results include a comparison of ADU units with phonemes and a comparison of our STD by query algorithm to other unsupervised algorithms for the TIMIT dataset [10].\nRelationship to Previous Work: In [7] we have used an NPB model for speech segmentation that achieves state of the art performance for unsupervised algorithms. The work presented here is a continuation of that work. Varadarajan et al. [11] proposed an algorithm to learn a speaker-dependent transducer that maps the acoustic observation to acoustic units. However, our proposed model is speaker independent and follows a different modeling approach.\nLee & Glass [12][13] proposed a nonparametric Bayesian model based on Dirichlet process mixtures (DPMs) that jointly segments the data and discovers the acoustic units. Our approach, however, discovers more homogenous units. We model each unit with a mixture of Gaussians while Lee & Glass use a 3-state HMM. Our ADU transducer also learns the sequential relationships between different units in the form of\nthe probability of a transition from one to the next. Lee & Glass do not model these relationships.\n2. \u00a0 Background A Dirichlet process (DP) [14] is a discrete distribution that consists of a countably infinite number of probability masses. A DP is denoted by DP(\u03b1,H), and is defined as:\n1\n, ~ , kk k k G H\u03b8\u03b2 \u03b4 \u03b8\n\u221e\n=\n=\u2211 (1)\nwhere \u03b1 is the concentration parameter, H is the base distribution, and\nk\u03b8\u03b4 is the unit impulse function at \u03b8k, often\nreferred to as an atom [15]. The weights \u03b2k are sampled through a stick-breaking construction [16] and are denoted by \u03b2~GEM(\u03b1). One of the applications of a DP is to define a nonparametric prior distribution on the components of a mixture model that can be used to define a mixture model with an infinite number of mixture components [15].\nAn HDP extends a DP to grouped data [17]. In this case there are several related groups and the goal is to model each group using a mixture model. These models can be linked using traditional parameter sharing approaches. One approach is to use a DP to define a mixture model for each group and to use a global DP, DP(\u03b3,H), as the common base distribution for all DPs [17]. An HDP is defined as:\n( )\n0\n0 0\n| , ~ ( , )\n| , ~ ( , )\n| ~\n| ~ ,\nj\nji j j\nji ji ji\nG H DP H\nG G DP G\nG G\nx F for j J\n\u03b3 \u03b3\n\u03b1 \u03b1\n\u03b8\n\u03b8 \u03b8 \u2208\n(2)\nwhere H provides a prior distribution for the factor \u03b8ji, \u03b3 governs the variability of G0 around H and \u03b1 controls the variability of Gj around G0. H, \u03b3 and \u03b1 are hyperparameters of the HDP. We use a DP to define a mixture model for each group and use a global DP, DP(\u03b3,H), as the common base distribution for all DPs.\nAn HDPHMM [6] is an HMM with an unbounded number of states. The transition distribution from each state is modeled by an HDP. This lets each state have a different distribution for its transitions while the set of reachable states would be shared amongst all states. The definition for HDPHMM is given by [6]:\n{ }\n{ } { } ( )\n1\n**\n1 1\n1 **\n, 1\n| ~ ( )\n| , ~ ( , )\n| ~ ( )\n| , ~ ( )\n| , ~\n| , ~\n| , ~ .\nt\nt\nt t\nj j\nj\nkj\nt t j zj\nt j t zj\nt kj t z sk j\nGEM\nDP\nGEM\nH H\nz z\ns z\nx z F\n\u03b2 \u03b3 \u03b3\n\u03b1\u03b2 \u03ba\u03b4 \u03c0 \u03b1 \u03b2 \u03b1 \u03ba \u03b1 \u03ba \u03c8 \u03c3 \u03c3\n\u03b8 \u03bb \u03bb\n\u03c0 \u03c0\n\u03c8 \u03c8\n\u03b8 \u03b8\n\u2212\n\u221e\n\u2212 =\n\u221e\n= \u221e\n=\n+ +\n+\n(3)\nThe state, mixture component and observation are represented by zt, st and xt respectively. The indices j and k are indices of the state and mixture components respectively. The base distribution that links all DPs together is represented by \u03b2 and can be interpreted as the expected value of state transition distributions. The transition distribution for state j is a DP denoted by \u03c0j with a concentration parameter \u03b1. Another DP, \u03c8j,\nwith a concentration parameter \u03ed, is used to model an infinite mixture model for each state (zj). The distribution H is the prior for the parameters \u03b8kj.\nA DHDPHMM extends the definition of HDPHMM by allowing mixture components to be shared amongst different states [9]. The model definition for an ergodic DHDPHMM is given by:\n{ }\n{ }\n{ } ( )\n1\n**\n1 1\n1\n**\n, 1 .\n| ~ ( )\n| , ~ ( , )\n| ~ ( )\n| , ~ ( , )\n| , ~ ( )\n| , ~\n| , ~\n| , ~\nt\nt\nt t\nj j\nj\nkj\nt t j zj\nt j t zj\nt kj t z s k j\nGEM\nDP\nGEM\nDP\nH H\nz z\ns z\nx z F\n\u03b2 \u03b3 \u03b3\n\u03b1\u03b2 \u03ba\u03b4 \u03c0 \u03b1 \u03b2 \u03b1 \u03ba \u03b1 \u03ba \u03be \u03c4 \u03c4\n\u03c8 \u03c3 \u03be \u03c3 \u03be\n\u03b8 \u03bb \u03bb\n\u03c0 \u03c0\n\u03c8 \u03c8\n\u03b8 \u03b8\n\u2212 \u221e \u2212 =\n\u221e\n=\n\u221e\n=\n+ +\n+\n(4)\nWe have previously shown DHDPHMM can improve performance in problems such as acoustic modeling [9].\n3. \u00a0 An ADU Transducer The goal in speech segmentation is to map each acoustic observation into a segment and optionally label these segments. Our goal can be expressed as mapping a string of acoustic observations to a string of labels. In speech recognition, observations are vectors of real numbers (instead of symbols in text processing) and segment labels can be replaced with a vector that represents the posterior probability of a set of predefined symbols. This representation is called a posteriorgram [18].\nA transducer specifies a binary relationship for a pair of strings [19]. Two strings are related if there is a path in the transducer that maps one string to the other. A weighted transducer also assigns a weight for each pair of strings [19]. Based on this definition our problem is to find a transducer that maps a string of acoustic features onto a string of units. It should be noted that based on this definition any HMM can be considered to be a transducer. We chose the term transducer here to emphasize the operation of converting acoustic observations into acoustic units. The problem can be further divided into two sub-problems: learning a transducer and decoding a string of observations into a string of units (or their equivalent posteriorgram representation).\nLet\u2019s assume we already knew the acoustic units (e.g. phonemes) and have trained models for each unit (e.g. HMMs). One way to construct a transducer is to connect all these HMMs using an ergodic network. The final transducer can be some form of ergodic HMM. However, we don\u2019t have the units and the number of units in the data is unknown.\nIn [7] we used HDPHMM for speech segmentation. In [9] we introduced a DHDPHMM that allows sharing mixture components across states. These models can learn different structures including ergodic structures. Both of these models are good candidates to train a transducer. A C++ implementation of both algorithms that also includes DPM and HDP is available at [20].\nWe use an HDPHMM or DHDPHMM to train the transducer. Learning HDPHMM and DHDPHMM models is extensively discussed in [6][9]. Here we train the models in a completely unsupervised fashion. Unlike Lee & Glass [12][13] we don\u2019t utilize a speech/non-speech classifier and model everything including silence with one transducer. For read speech, this does not present any problems. However, for other domains such as conversational speech, it might be a problem, and in that case we can employ a speech/non-speech classifier as well. Training is executed by sequentially presenting utterances to the HDPHMM/DHDPHMM inference algorithm and iterating using Gibbs sampling.\nFor our transducer, state labels (or their posteriorgrams) are the output string. Since each state is modeled by a Gaussian mixture, the segments defined by this transducer are stationary and the discovered units are sub-phonetic. However, it should be noted that this limitation can be overcome by replacing each state (e.g. mixture model) with an HMM which transforms the model into a hierarchical HMM [21]. The resulting model can model dynamic segments.\nGiven a transducer and a string of observations the goal of the decoder is to find the most likely path through states of the transducer that implicitly maps the input string to the output string. This objective can be written as:\n1 2\n1 2 1 2 ... argmax ( ... | ... ) , M M N s s s P s s s o o o (5)\nwhere s1, s2, ..., sM represent state labels and o1, o2, ..., oN represent observations. Alternately, we can also estimate the posteriorgram of the state sequence. To optimize (5) we can utilize the Viterbi algorithm [22]. The resulting transducer is the engine used to convert new acoustic observations into acoustic units.\n4. \u00a0 Unsupervised STD by Example Query Spoken term detection by query is a system that can recover data containing a word or phrase given a query example. An STD system can be built based on a complex state of the art speech recognizer and work either by acoustic or text queries. However, building such a system requires all the resources needed to build a state of the art speech recognizer including a lexicon, a language model and plenty of transcribed data. Moreover, if a given word does not exist in the lexicon we might never recover it using an ASR-based system.\nAn alternative approach is to use a phoneme sequence or some other low level equivalent. In this paper, we have used an ADU transducer for this goal. Our unsupervised STD-EQ algorithm is as follows:\n1. \u00a0 Convert the target audio data using the ADU transducer into posteriorgrams.\n2. \u00a0 For each query generate its posteriorgram representation using the transducer.\n3. \u00a0 Use a subsequence dynamic time warping (DTW) algorithm [23] to obtain a score for each utterance.\n4. \u00a0 Compare the final score for each utterance with a threshold and return it if the score is greater than the threshold.\nDTW is used to align two sequences X and Y with different lengths. The distance between X and Y is defined as [23]:\n{ }\n*( , ) ( , )\nmin ( , ) | is the warping path between and ,\np\np\nDTW X Y C X Y\nC X Y p X Y= (6)\nwhere ( ),pC X Y is defined as:\n1\n( , ) ( , ) . L\np nl ml i C X Y c x y =\n=\u2211 (7)\nNote that in this formulation c(xnl,yml) is an element of the cost matrix between X and Y. Since X and Y are posteriorgrams, this cost is defined as the dot product between them [18]:\n( , ) log( ).c x y x y= \u2212 (8) The subsequence DTW algorithm computes the distance between two strings. The goal is to find a subsequence of Y such as Y(a*,b*) that minimizes the distance from X. Denoting the length of Y as M, this objective can be expressed as:\n( ) ( ) ( )* * , :1 , argmin ( , ( , ) . a b a b M a b DTW X Y a b \u2264 \u2264 \u2264 = (9)\nIn our implementation we have replaced the strings with posteriorgrams and used the appropriate cost described in (8).\n5. \u00a0 Experiments In this section some experimental results are presented. First ADU units are compared with phonemes. Second we compare our STD-EQ algorithm with some other unsupervised algorithms on the TIMIT dataset [10]. TIMIT contains 6,300 utterances. We used the training subset to extract example queries and the test subset to search for the query. A standard 39-dimensional MFCC feature vector was used (12 MFCC plus energy and their first and second derivatives) to convert speech data into feature streams.\n5.1. \u00a0Relationship with Phonemes It is important to explore the relationship between the ADUs and phonemes because we need to determine if the ADUs are linguistically meaningful. The first experiment involves aligning manually transcribed phonemes with ADUs. First, each utterance is passed through the transducer to generate the sequence of ADUs. Then these ADUs are aligned with manual transcriptions using timing information contained in the transcription. Finally, a confusion matrix is calculated.\nA confusion matrix between 48 English phonemes and 251 ADU units is shown in Figure 1. A general correlation between ADUs and phonemes can be observed because the diagonal region of the matrix is heavily populated. However the mapping\nFigure 1: A confusion matrix that shows the relationship between ADUs and phonemes.\nis not consistently one to one. Some of the ADUs align with multiple phonemes. These phonemes are generally similar phonemes. For example, we can see ADUs that are aligned with \u201csil\u201d (silence) can also be aligned with \u201cvcl\u201d and \u201ccl\u201d models (both \u201cvcl\u201d and \u201ccl\u201d are special types of silence). ADUs aligned with \u201cz\u201d can also be aligned with \u201cs\u201d. This is not surprising because \u201cz\u201d and \u201cs\u201d are similar acoustically and therefore confusable.\n5.2. \u00a0STD by Example Query Table 2 shows the queries used to assess the quality of our ADU transducer. Since some words are similar (year vs. years), we do not count confusions between two words that share the same stem. We report the average precision of the top N hits or P@N [24] which is computed as the ratio of correct hits for top N scores for a given keyword. The average P@N is computed by averging the score for all keywords. We also reported the average equal error rate (EER). EER is the point on detection error tradeoff (DET) curve where the false acceptance error rate is equal to false rejection error rate. The reported EER is the average of EER for all keywords.\nA comparison of the average P@N and EER is reported in Table 1 for TIMIT. The first row shows a system that utilizes a GMM to directly decode the posteriorgrams of the feature frames [18]. The second row shows the result of an algorithm based on a Deep Boltzmann Machine (DBM) [25]. The third row contains the results for the nonparametric Bayesian approaches described earlier [13]. Rows four and five contain the results of our ADU-based unsupervised systems.\nWe can see for both HDPHMM and DHDPHMM, the EER is lower than other unsupervised models (5% improvement relative to the best system) while the P@N is only slightly lower than the NPM system in [13]. The primary reason that the HDPHMM transducer works better than the DHDPHMM transducer is the fact that for HDPHMM each state is modeled with a single Gaussian and this distribution is unique to that state, while for DHDPHMM all states share a pool of Gaussians. Each state can have more than one Gaussian associated with it, and this can make some states more confusable.\nRow 6 shows the result of combining the output of HDPHMM and DHDPHMM-based models. The reason for this experiment was to investigate how much improvement can be obtained if we can use both systems. We have selected the best results of each system and therefore the result of row 6 is not reported as the result of our algorithm. However, it shows that HDPHMM and DHDPHMM are complementary. It also shows if we can combine the results of these two systems we can get\nvery close to the results of a semi-supervised system shown in row 7 (e.g., using tied triphones in a conventional ASR system).\nTable 3 shows some of the typical error pairs. It can be seen that for some cases we have a partial acoustic match between the search query and the retrieved word (e.g. message and age) and for others we have partial similarity (e.g. year and hear). These errors are clearly related to Figure 1 since they happen often for more confusable phonemes. From this table we can see our algorithm effectively finds all acoustically similar examples that might actually be pronounced in a manner similar to the target keyword in the dataset. Given the fact that ADU units are discovered automatically based on the acoustic similarities, this is an expected result.\n6. \u00a0 Conclusions In this paper we proposed the application of HDPHMM/DHDPHMM to the problem of learning acoustic units automatically. We have shown discovered ADU units have a meaningful relationship with phonemes. We have also proposed an unsupervised STD by example query algorithm based on these ADU units. We have shown our system can achieve state of the art results among unsupervised systems.\nIn the future, we intend to study an NPB model that models nonstationary units. As mentioned above, our current model assumes each unit can be represented with a Gaussian mixture. As a result our model discovers sub-phonetic units. If we can model each state of the HDPHMM with another HMM then this limitation would be eliminated.\nAnother direction for future research is to study the relationship between the quality of a search query and its textual form. It has been shown in [26] that in a standard STD system the quality of a search query can be predicated based on its spelling. However, it is an open question if this is also the case for the proposed algorithm.\n7. \u00a0 Acknowledgements This research was supported in part by the National Science Foundation through Major Research Instrumentation Grant No. CNS-09-58854.\nTable 1: A comparison of unsupervised approaches to STD by query is shown.\nSystem P@N EER GMM [18] 52.50% 16.40% DBM [24] 51.10% 14.70% NPM [13] 63.00% 16.90% DHDPHMM ADU 56.21% 14.33% HDPHMM ADU 61.20% 13.95% Combined HDPHMM/DHDPHHMM 64.91% 11.83%\nSemi-supervised triphone [13] 75.90% 11.70%\nTable 2: A list of query terms used for the STD by example query task is shown.\nQuery No. Training No. Test age 3 8 warm 10 5 year 11 5 problem 22 13 artists 7 6 money 19 9 organizations 7 6 development 9 8 surface 3 8 children 18 10\n8. \u00a0 References [1] \u00a0 M. Bacchiani and M. Ostendorf, \u201cJoint lexicon, acoustic unit\ninventory and model design,\u201d Speech Communication, vol. 29, no. 2\u20134, pp. 99\u2013114, 1999. [2] \u00a0 B. Ma, et al., \u201cAn Acoustic Segment Modeling Approach to Automatic Language Identification,\u201d in Proceedings of INTERSPEECH, 2005, pp. 2829\u20132832. [3] \u00a0 K. Paliwal, \u201cLexicon-building methods for an acoustic sub-word based speech recognizer,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1990, pp. 729\u2013 732. [4] \u00a0 P. Muller and F. A. Quintana, \u201cNonparametric Bayesian Data Analysis,\u201d Statistical Science, vol. 19, no. 1, pp. 95\u2013110, Feb. 2004. [5] \u00a0 A. Harati, J. Picone, and M. Sobel, \u201cApplications of Dirichlet Process Mixtures to Speaker Adaptation,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2012, pp. 4321\u20134324. [6] \u00a0 E. Fox, et al., \u201cA Sticky HDP-HMM with Application to Speaker Diarization.,\u201d The Annalas of Applied Statistics, vol. 5, no. 2A, pp. 1020\u20131056, 2011. [7] \u00a0 A. Harati and J. Picone, \u201cSpeech Acoustic Unit Segmentation Using Hierarchical Dirichlet Processes,\u201d in Proceedings of INTERSPEECH, 2013, pp. 637\u2013641. [8] \u00a0 A. Harati Nejad Torbati, J. Picone, and M. Sobel, \u201cA Left-to-Right HDP-HMM with HDPM Emissions,\u201d Proceedings of the CISS, 2014, pp. 1-6. [9] \u00a0 A. Harati and J. Picone, \u201cA Doubly Hierarchical Dirichlet Process Hidden Markov Model with a Non-Ergodic Structure,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 1, pp. 174\u2013184, 2016. [10] \u00a0 J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet, N. Dahlgren, and V. Zue, \u201cTIMIT Acoustic-Phonetic Continuous Speech Corpus,\u201d The Linguistic Data Consortium Catalog, 1993. [11] \u00a0 B. Varadarajan, S. Khudanpur, and E. Dupoux, \u201cUnsupervised Learning of Acoustic Sub-word Units,\u201d in Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics Short Papers, 2008, pp. 165\u2013168. [12] \u00a0 C. Lee and J. Glass, \u201cA Nonparametric Bayesian Approach to Acoustic Model Discovery,\u201d in Proceedings of the Association for Computational Linguistics, 2012, pp. 40\u201349. [13] \u00a0 C. Lee, \u201cDiscovering Linguistic Structures in Speech: Models and Applications,\u201d Massachusetts Institute of Technology, 2014. [14] \u00a0 Y.-W. Teh, \u201cDirichlet process,\u201d in Encyclopedia of Machine Learning, Springer, 2010, pp. 280\u2013287. [15] \u00a0 C. E. Rasmussen, \u201cThe Infinite Gaussian Mixture Model,\u201d in Proceedings of Advances in Neural Information Processing Systems, 2000, pp. 554\u2013560. [16] \u00a0 J. Sethuraman, \u201cA constructive definition of Dirichlet priors,\u201d Statistica Sinica, vol. 4, no. 2, pp. 639\u2013650, 1994. [17] \u00a0 Y. Teh, M. Jordan, M. Beal, and D. Blei, \u201cHierarchical Dirichlet Processes,\u201d Journal of the American Statistical Association, vol. 101, no. 47, pp. 1566\u20131581, 2006. [18] \u00a0 Y. Zhang and J. R. Glass, \u201cUnsupervised Spoken Keyword Spotting via Segmental DTW on Gaussian Posteriorgrams,\u201d in Proceedings of the IEEE Workshop on Speech Recognition & Understanding, 2009, pp. 398\u2013403. [19] \u00a0 M. Mohri, F. Pereira, and M. Riley, \u201cSpeech Recognition with Weighted Finite-State Transducers,\u201d in Springer Handbook on Speech Processing and Speech Communication, 2008, pp. 559\u2013 584. [20] \u00a0 https://github.com/amir1981/hdphmm_lib [21] \u00a0 S. Fine, Y. Singer, and N. Tishby, \u201cThe Hierarchical Hidden\nMarkov Model:Analysis and Applications,\u201d Machine Learning, vol. 32, no. 1, pp. 41\u201362, 1998. [22] \u00a0 A. Viterbi, \u201cError Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm,\u201d IEEE Transactions on Information Theory, vol. 13, no. 2, pp. 260\u2013269, Apr. 1967. [23] \u00a0 M. M\u00fcller, \u201cDynamic Time Warping,\u201d in Information Retrieval for Music and Motion, Springer, 2007, pp. 69\u201382.\n[24] \u00a0 T. J. Hazen, W. Shen, and C. White, \u201cQuery-By-Example Spoken Term Detection Using Phonetic Posteriorgram Templates,\u201d in Proceedings of the IEEE Workshop on Speech Recognition and Understanding, 2009, pp. 421\u2013426. [25] \u00a0 Y. Zhang, R. Salakhutdinov, H.-A. Chang, and J. Glass, \u201cResource Configurable Spoken Query Detection using Deep Boltzmann Machines,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2012, pp. 5161\u20135164. [26] \u00a0 A. Harati and J. Picone, \u201cPredicting Search Term Reliability for Spoken Term Detection Systems,\u201d International Journal of Speech Technology, vol. 17, no. 1, pp. 1\u20139, 2014."}], "references": [{"title": "Joint lexicon, acoustic unit inventory and model design", "author": ["M. Bacchiani", "M. Ostendorf"], "venue": "Speech Communication, vol. 29, no. 2\u20134, pp. 99\u2013114, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "An Acoustic Segment Modeling Approach to Automatic Language Identification", "author": ["B. Ma"], "venue": "Proceedings of INTERSPEECH, 2005, pp. 2829\u20132832.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Lexicon-building methods for an acoustic sub-word based speech recognizer", "author": ["K. Paliwal"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1990, pp. 729\u2013 732.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Nonparametric Bayesian Data Analysis", "author": ["P. Muller", "F.A. Quintana"], "venue": "Statistical Science, vol. 19, no. 1, pp. 95\u2013110, Feb. 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Applications of Dirichlet Process Mixtures to Speaker Adaptation", "author": ["A. Harati", "J. Picone", "M. Sobel"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2012, pp. 4321\u20134324.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "A Sticky HDP-HMM with Application to Speaker Diarization", "author": ["E. Fox"], "venue": "The Annalas of Applied Statistics, vol. 5, no. 2A, pp. 1020\u20131056, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech Acoustic Unit Segmentation Using Hierarchical Dirichlet Processes", "author": ["A. Harati", "J. Picone"], "venue": "Proceedings of INTERSPEECH, 2013, pp. 637\u2013641.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "A Left-to-Right HDP-HMM with HDPM Emissions", "author": ["A. Harati Nejad Torbati", "J. Picone", "M. Sobel"], "venue": "Proceedings of the CISS, 2014, pp. 1-6.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A Doubly Hierarchical Dirichlet Process Hidden Markov Model with a Non-Ergodic Structure", "author": ["A. Harati", "J. Picone"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 1, pp. 174\u2013184, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "TIMIT Acoustic-Phonetic Continuous Speech Corpus", "author": ["J. Garofolo", "L. Lamel", "W. Fisher", "J. Fiscus", "D. Pallet", "N. Dahlgren", "V. Zue"], "venue": "The Linguistic Data Consortium Catalog, 1993.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Unsupervised Learning of Acoustic Sub-word Units", "author": ["B. Varadarajan", "S. Khudanpur", "E. Dupoux"], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics Short Papers, 2008, pp. 165\u2013168.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "A Nonparametric Bayesian Approach to Acoustic Model Discovery", "author": ["C. Lee", "J. Glass"], "venue": "Proceedings of the Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovering Linguistic Structures in Speech: Models and Applications", "author": ["C. Lee"], "venue": "Massachusetts Institute of Technology, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dirichlet process", "author": ["Y.-W. Teh"], "venue": "Encyclopedia of Machine Learning, Springer, 2010, pp. 280\u2013287.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The Infinite Gaussian Mixture Model", "author": ["C.E. Rasmussen"], "venue": "Proceedings of Advances in Neural Information Processing Systems, 2000, pp. 554\u2013560.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica, vol. 4, no. 2, pp. 639\u2013650, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Hierarchical Dirichlet Processes", "author": ["Y. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": "Journal of the American Statistical Association, vol. 101, no. 47, pp. 1566\u20131581, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised Spoken Keyword Spotting via Segmental DTW on Gaussian Posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proceedings of the IEEE Workshop on Speech Recognition & Understanding, 2009, pp. 398\u2013403.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech Recognition with Weighted Finite-State Transducers", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Springer Handbook on Speech Processing and Speech Communication, 2008, pp. 559\u2013 584.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "The Hierarchical Hidden Markov Model:Analysis and Applications", "author": ["S. Fine", "Y. Singer", "N. Tishby"], "venue": "Machine Learning, vol. 32, no. 1, pp. 41\u201362, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm", "author": ["A. Viterbi"], "venue": "IEEE Transactions on Information Theory, vol. 13, no. 2, pp. 260\u2013269, Apr. 1967.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1967}, {"title": "Dynamic Time Warping", "author": ["M. M\u00fcller"], "venue": "Information Retrieval for Music and Motion, Springer, 2007, pp. 69\u201382.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Query-By-Example Spoken Term Detection Using Phonetic Posteriorgram Templates", "author": ["T.J. Hazen", "W. Shen", "C. White"], "venue": "Proceedings of the IEEE Workshop on Speech Recognition and Understanding, 2009, pp. 421\u2013426.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Resource Configurable Spoken Query Detection using Deep Boltzmann Machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J. Glass"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2012, pp. 5161\u20135164.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Predicting Search Term Reliability for Spoken Term Detection Systems", "author": ["A. Harati", "J. Picone"], "venue": "International Journal of Speech Technology, vol. 17, no. 1, pp. 1\u20139, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Most approaches to automatic discovery of acoustic units [1]-[3] do this in two steps: segmentation and clustering.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Most approaches to automatic discovery of acoustic units [1]-[3] do this in two steps: segmentation and clustering.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "However in an NPB model [4][5], the model complexity can be inferred directly from the data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "However in an NPB model [4][5], the model complexity can be inferred directly from the data.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "[6] used one state per speaker and demonstrated segmentation without knowing the number of speakers a priori.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "We have also previously reported on the use of a similar model for speech segmentation that achieves state of the art results [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "These models include a Hierarchical Dirichlet Process HMM (HDPHMM) [6] and a Doubly Hierarchical Dirichlet Process HMM (DHDPHMM) [8][9].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "These models include a Hierarchical Dirichlet Process HMM (HDPHMM) [6] and a Doubly Hierarchical Dirichlet Process HMM (DHDPHMM) [8][9].", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "These models include a Hierarchical Dirichlet Process HMM (HDPHMM) [6] and a Doubly Hierarchical Dirichlet Process HMM (DHDPHMM) [8][9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 9, "context": "These results include a comparison of ADU units with phonemes and a comparison of our STD by query algorithm to other unsupervised algorithms for the TIMIT dataset [10].", "startOffset": 164, "endOffset": 168}, {"referenceID": 6, "context": "Relationship to Previous Work: In [7] we have used an NPB model for speech segmentation that achieves state of the art performance for unsupervised algorithms.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "[11] proposed an algorithm to learn a speaker-dependent transducer that maps the acoustic observation to acoustic units.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Lee & Glass [12][13] proposed a nonparametric Bayesian model based on Dirichlet process mixtures (DPMs) that jointly segments the data and discovers the acoustic units.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Lee & Glass [12][13] proposed a nonparametric Bayesian model based on Dirichlet process mixtures (DPMs) that jointly segments the data and discovers the acoustic units.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "A Dirichlet process (DP) [14] is a discrete distribution that consists of a countably infinite number of probability masses.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "where \u03b1 is the concentration parameter, H is the base distribution, and k \u03b8 \u03b4 is the unit impulse function at \u03b8k, often referred to as an atom [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "The weights \u03b2k are sampled through a stick-breaking construction [16] and are denoted by \u03b2~GEM(\u03b1).", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "One of the applications of a DP is to define a nonparametric prior distribution on the components of a mixture model that can be used to define a mixture model with an infinite number of mixture components [15].", "startOffset": 206, "endOffset": 210}, {"referenceID": 16, "context": "An HDP extends a DP to grouped data [17].", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "One approach is to use a DP to define a mixture model for each group and to use a global DP, DP(\u03b3,H), as the common base distribution for all DPs [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "An HDPHMM [6] is an HMM with an unbounded number of states.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "The definition for HDPHMM is given by [6]:", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "A DHDPHMM extends the definition of HDPHMM by allowing mixture components to be shared amongst different states [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "We have previously shown DHDPHMM can improve performance in problems such as acoustic modeling [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 17, "context": "This representation is called a posteriorgram [18].", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "A transducer specifies a binary relationship for a pair of strings [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "A weighted transducer also assigns a weight for each pair of strings [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "In [7] we used HDPHMM for speech segmentation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [9] we introduced a DHDPHMM that allows sharing mixture components across states.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Learning HDPHMM and DHDPHMM models is extensively discussed in [6][9].", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Learning HDPHMM and DHDPHMM models is extensively discussed in [6][9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "Unlike Lee & Glass [12][13] we don\u2019t utilize a speech/non-speech classifier and model everything including silence with one transducer.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "Unlike Lee & Glass [12][13] we don\u2019t utilize a speech/non-speech classifier and model everything including silence with one transducer.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "mixture model) with an HMM which transforms the model into a hierarchical HMM [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "To optimize (5) we can utilize the Viterbi algorithm [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Use a subsequence dynamic time warping (DTW) algorithm [23] to obtain a score for each utterance.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "The distance between X and Y is defined as [23]: { } * ( , ) ( , )", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Since X and Y are posteriorgrams, this cost is defined as the dot product between them [18]: ( , ) log( ).", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Second we compare our STD-EQ algorithm with some other unsupervised algorithms on the TIMIT dataset [10].", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "We report the average precision of the top N hits or P@N [24] which is computed as the ratio of correct hits for top N scores for a given keyword.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "The first row shows a system that utilizes a GMM to directly decode the posteriorgrams of the feature frames [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "The second row shows the result of an algorithm based on a Deep Boltzmann Machine (DBM) [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "The third row contains the results for the nonparametric Bayesian approaches described earlier [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "We can see for both HDPHMM and DHDPHMM, the EER is lower than other unsupervised models (5% improvement relative to the best system) while the P@N is only slightly lower than the NPM system in [13].", "startOffset": 193, "endOffset": 197}, {"referenceID": 24, "context": "It has been shown in [26] that in a standard STD system the quality of a search query can be predicated based on its spelling.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "System P@N EER GMM [18] 52.", "startOffset": 19, "endOffset": 23}, {"referenceID": 22, "context": "40% DBM [24] 51.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "70% NPM [13] 63.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "83% Semi-supervised triphone [13] 75.", "startOffset": 29, "endOffset": 33}], "year": 2016, "abstractText": "State of the art speech recognition systems use data-intensive context-dependent phonemes as acoustic units. However, these approaches do not translate well to low resourced languages where large amounts of training data is not available. For such languages, automatic discovery of acoustic units is critical. In this paper, we demonstrate the application of nonparametric Bayesian models to acoustic unit discovery. We show that the discovered units are correlated with phonemes and therefore are linguistically meaningful. We also present a spoken term detection (STD) by example query algorithm based on these automatically learned units. We show that our proposed system produces a P@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the EER is 5% while P@N is only slightly lower than the best reported system in the literature.", "creator": "Word"}}}