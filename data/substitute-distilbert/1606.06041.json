{"id": "1606.06041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Bandit-Based Random Mutation Hill-Climbing", "abstract": "the random mutation hill - climbing algorithm is called direct search technique mostly present in discrete domains. it repeats the process of randomly selecting a neighbour of a best - so - far solution and accepts the neighbour if it is better than or equal to it. in this work, we propose to use a novel method to select the neighbour solution using a panel of independent co - armed bandit - style selection units therefore results in a neighbor - based total mutation hill - climbing algorithm. the simultaneous operation significantly outperforms random mutation hill - hanging problem simple onemax ( in noise - laden and noisy cases ) and royal road problems ( in the noise - free model ). the algorithm shows particular promise for discrete optimisation problems where each fitness evaluation is expensive.", "histories": [["v1", "Mon, 20 Jun 2016 09:53:29 GMT  (86kb,D)", "http://arxiv.org/abs/1606.06041v1", "7 pages, 10 figures"]], "COMMENTS": "7 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["jialin liu", "diego pe\\'rez-lie\\'bana", "simon m lucas"], "accepted": false, "id": "1606.06041"}, "pdf": {"name": "1606.06041.pdf", "metadata": {"source": "CRF", "title": "Bandit-Based Random Mutation Hill-Climbing", "authors": ["Jialin Liu", "Diego P\u00e9rez-Li\u00e9bana", "Simon M. Lucas"], "emails": ["jialin.liu@essex.ac.uk", "dperez@essex.ac.uk", "sml@essex.ac.uk"], "sections": [{"heading": null, "text": "Index Terms\u2014RMHC, bandit, OneMax, Royal Road\nI. INTRODUCTION\nEvolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].\nTheir essence is relatively simple: to generate an initial set of candidate solutions at random, and then to iteratively improve the candidate set via a process of variation, evaluation and selection. They have been the subject of much analysis, development and a diverse range of applications. They have also spawned related approaches (i.e. methods which can be characterised by the outline description above) such as particle swarm optimisation, and have been extended for application to multi-objective optimisation.\nThis paper introduces a significant variation: the BanditBased Evolutionary Algorithm. Bandit algorithms [8], [9] have become popular for optimising either simple regret (the best final decision after a number of exploratory trials) or cumulative regret (best sum of rewards over a number of trials) in A/B testing.\nA popular bandit algorithm is the Upper Confidence Bound (UCB) algorithm [10], [11] which balances the trade-off between exploration and exploitation. The UCB-style algorithms have achieved widespread use within Monte Carlo Tree Search (MCTS) [12], called UCT when applied to trees, the \u201cT\u201d being for Trees.\nA wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested. There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the\nnon-stationary nature of bandit problems in optimization portfolios. St-Pierre and Liu [21] applied the Differential Evolution algorithm [22] to some non-stationary bandit problem, which outperformed the classical bandit algorithm on the selection over a portfolio of solvers.\nBrowne et al. [12] noted the great potential for hybridising MCTS with other approaches to optimisation and learning, and in this paper we provide a hybridisation of an evolutionary algorithm with a bandit algorithm.\nThere are examples of using evolution to tune MCTS parameters [23], [24], [25]. Albeit robust, this application of EA is not widespread, due to the computational cost involved in performing fitness evaluations. It should be noted that Lucas et al. [25] made fitness evaluations after each rollout, so they could be rapidly optimised, albeit noisily.\nThe algorithm reported in this paper is a very different hybrid: it uses bandits to represent the state of the evolving system. This has some similarities with Estimation of Distribution Algorithms (EDAs) [26] but the details are significantly different.\nTo our knowledge, this is one of the very few times that this type of hybridisation has been attempted; the only other paper we are aware of in the same vein is Zhang et al. [27]. Zhang et al. used a bandit algorithm as a form of Adaptive Operator Selection: the variation operators used within the evolutionary algorithm were selected using a bandit-based approach, showing promising results.\nIn this paper we develop a bandit-based version of the Random Mutation Hill-Climbing (RMHC) algorithm, and compare the two methods, i.e., the original and the banditbased algorithms.\nTo put this in some context, it should be noted that while the RMHC algorithm is very simple, it is often surprisingly competitive with more complex algorithms, especially when deployed with random restarts.\nFor instance, Lucas and Reynolds evolved Deterministic Finite Automata (DFA) [28], [29], using a multi-start RMHC algorithm with very competitive results, outperforming more complex evolutionary algorithms, and for some classes of problems also outperforming the state of the art EvidenceDriven State Merging (EDSM) algorithms.\nAlthough Goldberg [30] used bandit models, they were used to help understand the operation of a Simple Genetic Algorithm. Our approach is different: we use them as the very\nar X\niv :1\n60 6.\n06 04\n1v 1\n[ cs\n.A I]\n2 0\nJu n\n20 16\nbasis of the algorithm. The bandit model provides a natural way to balance exploitation (sticking with what appears to be good) versus exploration (trying things which have not been sampled much).\nIn this paper we model the genome as an array of bandits. In the case where the genome is a binary string of length n, we model the state of the evolutionary system as an array of 2-armed bandits. If each element of the string can take on m possible values, then each element is represented by an marmed bandit.\nThe main contribution in this work is this new bandit-based RMHC algorithm, together with results on some standard benchmark problems. The new algorithm significantly outperforms the standard RMHC in both OneMax (in noise-free and noisy cases) and Royal Road problems in the noise-free case. Tests on noisy Royal Road problems will be studied in future work."}, {"heading": "II. TEST PROBLEMS", "text": "In this work, we consider two benchmark optimisation problems in a binary search space."}, {"heading": "A. OneMax Problem", "text": "The OneMax problem [31] is a simple linear problem aiming at maximising the number of 1 of a binary string, i.e., for a given n-bit string s\nf(s) = n\u2211 i=1 si, (1)\nwhere si, denoting the ith bit in the string s, is either 1 or 0. The complexity of OneMax problem is O(n log(n)) for a n-bit string [32]. Doerr et al. proved that the black-box complexity with memory restriction one is at most 2n [33]. More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35]. In their elitist model, only the best-so-far solution can be kept in the memory. Our banditbased RMHC stores the best-so-far solution in the noise-free environment and stores additionally its evaluation number in the noisy environment (detailed in Section III-C)."}, {"heading": "B. Noisy OneMax Problem", "text": "We modify the OneMax problem by introducing an additive noise with constant variance 1:\nf \u2032(s) = f(s) +N (0, 1), (2)\nN denotes a Gaussian noise. Thus, the noise standard deviation is of same order as the differences between fitness values. It is notable that our noise model is very different from the one in [36], which used (1+1)-EA and a one-bit noise.\nThe influence of the noise strength on the runtime of (1+1)- EA for the OneMax problem corrupted by one-bit noise is firstly analysed by Droste [37]. In [37] and [36], the misranking occurs due to the change of exactly one uniformly chosen bit of s by noise with probability p \u2208 (0, 1), p is the noise strength. Thus, the noise acts before fitness evaluation and the\nevaluated individual (solution or search point) is possibly not the correct one. The individual is infected by noise in their model while in our model, the fitness function is infected by noise."}, {"heading": "C. Royal Road Function", "text": "The Royal Road functions are firstly introduced by Mitchell et al. [38]. The function fitness gains only if all the bits in one block are flipped to 1.1 The objective was to designing some hierarchical fitness landscapes and studying the performance of Genetic Algorithms (GA). Surprisingly, a simple Random Mutation Hill-Climbing Algorithm outperforms GA on a simple Royal Road function, namely R1 in [38], [39]. R1 consists of a list of block-composite bit strings as shown in Fig. 4, in which \u2018*\u2019 denotes a either 0 or 1. The fitness R1(x) is recalled as follows:\nR1 = n\u2211 i=1 ci\u03b4i(x), (3)\nwhere \u03b4i = 1 if x \u2208 si, otherwise, \u03b4i = 0.\nIt\u2019s notable that, due to the landscapes in the Royal Road function and 1-bit mutation per generation, introducing noise to the fitness is not trivial. When introducing a noise with constant variance, with high probability, the mutated genome has an identical noise-free fitness value to the one of its ancestor. As a result, only the samples of introduced noise are compared."}, {"heading": "III. BANDIT-BASED RMHC", "text": "In contrast to the standard bandit terminology, where an arm is pulled to gain some reward, the purpose of our bandits is to select the element to mutate at each iteration of the algorithm.\nWe create an m-armed bandit for each gene of the genome that can take on m possible values. Each bandit works by recording how many times each arm has been pulled, i.e., the number of evaluations of each arm, and the difference in empirical reward between the previous fitness of the genome and the fitness obtained as a result of the selected mutation.\nNote that instead of pulling an arm to gain some reward as in the normal bandit terminology, each bandit stores a state and has m arms where each arm i \u2208 {1, . . . ,m} stores the statistics of a transition: Ti \u2208 Transitions(S) with\n1Note that OneMax can be considered as a special case of the Royal Road function with a block size of 1.\n|Transitions(S)| = m. Transitions(S) denotes the set of transitions at state S. Thus, for a genome of n genes, n multiarmed bandits are created, assuming they are independent.\nEach bandit can have a different number of arms, depending on the problem and transition sets. In a n-dimensional OneMax problem or Royal Road function R1, n 2-armed bandits are required.\nFor the rest of this paper we assume that m = 2 i.e. we are dealing with binary strings, though the extension to larger alphabets should be straightforward.\nFor any position (gene) at a given state S, there is one single possible action flip and two transitions, the next state will be\nS\u2032 = { 1, if it\u2019s 0 at state S 0, otherwise."}, {"heading": "A. Urgency", "text": "At each iteration of the Bandit-based RMHC algorithm, the bandit agents manage the selection of the gene with maximal urgency to mutate:\ni\u2217 = argmax i\u2208{1,2,...,n} urgencyi. (4)\nThe urgency of each bandit is derived from the standard UCB equation, except that we invert the normal use of the exploitation term, i.e. the first term in the RHS of Equation 5. Intuitively, this says that if a particular state of a bandit is already good, then it\u2019s value should not be changed. The exploration term is there to ensure that as the total number of iterations Ni increases, so occasionally an apparently poorer option will be tried.\nFor any 2-armed bandit i \u2208 {1, 2, . . . , n}, the urgencyi is defined as\nurgencyi = \u2212 max j\u2208{0,1} \u2206\u0304i(j)+\n\u221a log(Ni + 1)\n2Ni(j) +U(1e\u22126), (5)\nwhere Ni is the number of times the ith bit is selected; Ni(j) is the number of times the state j is reached when the ith bit is selected; \u2206\u0304i(j) is the empirical mean difference between the fitness values if the state j is reached when the ith bit is selected, i.e., the changing of fitness value; U(1e\u22126) denotes a uniformly distributed value between 0 and 1e\u22126 which is used to randomly break ties.\nThis means that for each position in the bit string (i.e. for each gene) we have a simple bandit model that requires only 3 additional parameters for book-keeping: one parameter to model the fitness change when flipping a bit from one to zero, another one for the opposite flip, and one to count the number of times that a bandit has been selected (Ni(j))."}, {"heading": "B. Noise-free case", "text": "Algorithm 1 presents the bandit-based RMHC in the noisefree case. To solve a noise-free problem, no resampling is necessary if the evaluation number and fitness value of the best-so-far genome can be saved. It is worth noting that, for problems in which computing the fitness value is difficult or requires high computational cost, saving the fitness of a solution\nis far less expensive than re-evaluating it again. For the further work, we are interested in applying our proposed approach to more difficult problems (such as game level generation and evaluation [40]). Our main interest is in improving the speed of convergence to approximately optimal states.\nAlgorithm 1 Bandit-based RMHC in the noise-free case. Require: n \u2208 N\u2217: genome length Require: m \u2208 N\u2217: dimension of search space\n1: Randomly initialise a genome x \u2208 Rm 2: bestF itSoFar \u2190 fitness(x) 3: N \u2190 1 . Total evaluation number 4: while time not elapsed do 5: Select the element i\u2217 to mutate using Eqs. 4 and 5 6: y\u2190 after mutating the element i\u2217 of x 7: Fity \u2190 fitness(y) 8: N \u2190 N + 1 . Update the counter 9: if Fity \u2265 bestF itSoFar then 10: x\u2190 y . Update the best-so-far genome 11: bestF itSoFar \u2190 Fity 12: end if 13: end while 14: return x"}, {"heading": "C. Noisy case", "text": "We now consider the noisy case. The Bandit-based RMHC in the noisy case is formalised in Algorithm 2. In the noisy case, the best-so-far genome requires multiple evaluations to reduce the effect of noise, this is called resampling.\nThe statistics of the best-so-far genome are stored, thanks to which, instead of comparing directly the fitness values of the offspring to the one of the best-so-far genome, the average fitness value of the best-so-far genome is compared at each generation. Therefore, the computational cost involved in the evaluation of the genome determines the computational cost of this algorithm.\nResampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43]. Interestingly, Qian et al. [44] proved theoretically and empirically that under some conditions, resampling is not beneficial for a (1+1)-EA optimizing 10-OneMax under additive Gaussian noise. This is contrary to our findings, though we use much larger bit string lengths for our main results (from 50 to 1000, unlike the length of 10 used by Qian et al.) and different algorithm (RMHC using 1-bit mutation, unlike (1+1)-EA where every bit could mutate with a probability), and we also found resampling to not improve results for small strings (e.g. for N = 10)."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "We apply first our proposed algorithm on the OneMax problem and the Royal Road function R1 in a noise-free case, and then evaluate the performance of our algorithm on the\nAlgorithm 2 Bandit-based RMHC in the noisy case. Require: n \u2208 N\u2217: genome length Require: m \u2208 N\u2217: dimension of search space\n1: Randomly initialise a genome x \u2208 Rm 2: bestF itSoFar \u2190 fitness(x) 3: M \u2190 1 . Evaluation number of the best-so-far genome 4: N \u2190 1 . Total evaluation number 5: while time not elapsed do 6: Select the element i\u2217 to mutate using Eqs. 4 and 5 7: y\u2190 after mutating the element i\u2217 of x 8: Fitx \u2190 fitness(x) 9: Fity \u2190 fitness(y) 10: N \u2190 N + 2 . Update the counter 11: averageF itness\u2190 bestF itSoFar\u2217M+FitxM+1 12: if Fity \u2265 averageF itness then 13: x\u2190 y . Update the best-so-far genome 14: bestF itSoFar \u2190 Fity 15: M \u2190 1 16: else 17: bestF itSoFar \u2190 averageF itness 18: M \u2190M + 1 19: end if 20: end while 21: return x\nOneMax problem with the presence of noise. Each experiment is repeated 100 times using randomly initialised strings."}, {"heading": "A. OneMax", "text": "The results in noise-free OneMax problem of different dimensions is presented in Fig. 2. In the noise-free case, the average fitness evaluations used by bandit-based RMHC to solve the problem is close to the problem dimension, while the original RMHC required approximate 5 times more budget.\nFig. 3 illustrates the empirical average number of fitness evaluations required to reach the optimum value using RMHC\nand bandit-based RMHC in the OneMax problem of different dimensions with constant variance noise (\u223c N (0,1)). The resampling number in the noisy case is given between brackets. For comparison, the results in noise-free OneMax is also included (blue curves).\nAs is exhibited in the graph, with the presence of constant variance noise:\n\u2022 Using RMHC, larger resampling number (10) leads to a faster convergence to the optimum (Fig. 3a) on highdimension problems; when the problem dimension is low, resampling number equals to 3, 4 or 5 leads to a faster convergence to the optimum (Fig. 3c); the ratio of average fitness evaluations to the problem dimension increases noticeably when a small resampling number is used (Fig. 3c). \u2022 Using bandit-based RMHC, the ratio of average fitness evaluations to the problem dimension remains stable when resampling is used (Fig. 3d); the total evaluation number scales almost linearly with the problem dimension; the optimal resampling number is 2 (red curve). \u2022 As can be seen clearly from the figures, for an identical OneMax problem with the presence of noise, our proposed bandit-based RMHC significantly outperforms the standard RMHC by a very large margin - in some cases requiring a factor of ten fewer fitness evaluations."}, {"heading": "B. Royal Road", "text": "Fig. 4 shows the empirical average fitness evaluations required to find the optimum of the noise-free Royal Road function using RMHC and bandit-based RMHC, respectively.\nFor a fixed length, bigger block size results in a harder problem and more fitness evaluations. The ratio of average fitness evaluations to the problem dimension increases with the problem dimension when a small resampling number is used (Fig. 3c). For an identical problem, bandit-based RMHC required far fewer fitness evaluations than RMHC to find the optimal solution. It can be seen from the curves that for an identical block size, using bandit-based RMHC, the total evaluation number scales linearly with the problem dimension.\nIn addition, to find the optimum string of 8 blocks of size 8, our bandit-based RMHC used half number of function evaluations than the RMHC used by Mitchell et al. [39], which was the most efficient algorithm in their experiments.\nThe Royal Road functions involve a harder credit assignment [45] problem than standard OneMax, an important aspect of sequential decision making. The reward for correctly mutating a bit is usually delayed, and dependent on many other correct bit settings.\nRegarding credit assignment within the algorithm, the bandit-based RMHC uses urgency (Eq. 5) to model this, by attempting to track the fitness gained when switching a gene to a particular value. More use is made of the available information, leading to faster learning (see [46] and [47] for more analysis of the information rates of simple evolutionary algorithms).\nIf this information was exploited in a way that was too na\u0131\u0308ve or too greedy, this could lead the algorithm to rapidly become stuck on poor values, especially for the noisy problems tested in this paper. However, the exploration term naturally counteracts such tendencies."}, {"heading": "V. CONCLUSION", "text": "This paper presented the first bandit-based Random Mutation Hill Climber (RMHC) - a simple but effective type of evolutionary algorithm. The algorithm was compared with the standard RMHC on the OneMax problem and Royal Road function. Tests were also made using a noisy OneMax problem together with resampling in each algorithm to ameliorate the effects of the noise.\nOn noise-free and noisy OneMax problems and Royal Road function, our bandit-based RMHC algorithm significantly out-\nperforms the RMHC, in some cases using a factor of ten fewer evaluations in the noisy case.\nFurthermore, the fitness evaluations required by the banditbased RMHC finding the optimal solution is an approximately linear function of the problem dimension when the resampling number is 2. For an identical Royal Road function 8 blocks of size 8), the bandit-based RMHC used half the number of function evaluations than the RMHC used by Mitchell et al. in [39], which was the most efficient algorithm in their experiments.\nWe obtain very promising results using this simple but effective bandit-based RMHC. The algorithm is designed for a large set of discrete optimisation problems where each fitness evaluation is expensive and the fitness is possibly noisy due to some uncertainties, which is quite common in real-world applications.\nFURTHER WORK The main work in progress is the theoretical analysis on expected evaluation number and accuracy, and the tests on noisy Royal Road problems. Also, because the method makes such efficient use of the fitness evaluations, it is ready to be applied to expensive optimisation problems such as game level design evaluation [40]."}], "references": [{"title": "Evolutionary Operation: A Method for Increasing Industrial Productivity", "author": ["G.E. Box"], "venue": "Applied Statistics, pp. 81\u2013101, 1957.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1957}, {"title": "Digital Simulation of an Evolutionary Process", "author": ["G.J. Friedman"], "venue": "General Systems Yearbook, vol. 4, no. 171-184, 1959.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1959}, {"title": "The Use of Biological Concepts in the Analytical Study of Systems", "author": ["W. Bledsoe"], "venue": "OPERATIONS RESEARCH, vol. 9. INST OPERATIONS RESEARCH MANAGEMENT SCIENCES 901 ELKRIDGE LAND- ING RD, STE 400, LINTHICUM HTS, MD 21090-2909, 1961, pp. B145\u2013B146.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1961}, {"title": "Optimization through Evolution and Recombination", "author": ["H.J. Bremermann"], "venue": "Self-organizing systems, vol. 93, p. 106, 1962.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1962}, {"title": "Cybernetic Solution Path of an Experimental Problem", "author": ["I. Rechenberg"], "venue": "1965.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1965}, {"title": "Artificial Intelligence Through Simulated Evolution.[By", "author": ["L.J. Fogel"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1966}, {"title": "Simulation of Biological Evolution and Machine Learning: I. Selection of Self-Reproducing Numeric Patterns by Data Processing Machines, Effects of Hereditary Control, Mutation Type and Crossing", "author": ["J. Reed", "R. Toombs", "N.A. Barricelli"], "venue": "Journal of theoretical biology, vol. 17, no. 3, pp. 319\u2013342, 1967.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1967}, {"title": "Bandit Processes and Dynamic Allocation Indices", "author": ["J.C. Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 148\u2013177, 1979.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1979}, {"title": "Bandit Problems: Sequential Allocation of Experiments (Monographs on Statistics and Applied Probability)", "author": ["D.A. Berry", "B. Fristedt"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Asymptotically Efficient Adaptive Allocation Rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, vol. 6, pp. 4\u201322, 1985.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-Time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Using Confidence Bounds for Exploitation-Exploration Tradeoffs", "author": ["P. Auer"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 397\u2013422, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Use of Variance Estimation in the Multi-Armed Bandit Problem", "author": ["J.-Y. Audibert", "R. Munos", "C. Szepesvari"], "venue": "2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Pure Exploration in Multi-Armed Bandits Problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Algorithmic Learning Theory. Springer, 2009, pp. 23\u201337.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The KL-UCB Algorithm for Bounded Stochastic Bandits and beyond", "author": ["A. Garivier", "O. Capp\u00e9"], "venue": "arXiv preprint arXiv:1102.2490, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-Armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Discounted UCB", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "2nd PASCAL Challenges Workshop, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A Sublinear-Time Randomized Approximation Algorithm for Matrix Games", "author": ["M.D. Grigoriadis", "L.G. Khachiyan"], "venue": "Operations Research Letters, vol. 18, no. 2, pp. 53\u201358, Sep 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Gambling in a Rigged Casino: the Adversarial Multi-Armed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "Proceedings of the 36th Annual Symposium on Foundations of Computer Science. IEEE Computer Society Press, Los Alamitos, CA, 1995, pp. 322\u2013331.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Differential Evolution Algorithm Applied to Non-Stationary Bandit Problem", "author": ["D.L. St-Pierre", "J. Liu"], "venue": "2014 IEEE Congress on Evolutionary Computation (IEEE CEC 2014), Beijing, Chine, Jul. 2014. [Online]. Available: http://hal.inria.fr/hal-00979456", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Differential Evolution \u2013 A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces", "author": ["R. Storn", "K. Price"], "venue": "Journal of global optimization, vol. 11, no. 4, pp. 341\u2013359, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Evolving Artificial Neural Networks with FINCH", "author": ["A. Benbassat", "M. Sipper"], "venue": "Proceedings of the 15th annual conference companion on Genetic and evolutionary computation. ACM, 2013, pp. 1719\u20131720.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Using Genetic Programming to Evolve Heuristics for a Monte Carlo Tree Search Ms Pac-Man agent", "author": ["A.M. Alhejali", "S.M. Lucas"], "venue": "Computational Intelligence in Games (CIG), 2013 IEEE Conference on. IEEE, 2013, pp. 1\u20138.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast Evolutionary Adaptation for Monte Carlo Tree Search", "author": ["S.M. Lucas", "S. Samothrakis", "D. Perez"], "venue": "Applications of Evolutionary Computation. Springer, 2014, pp. 349\u2013360.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Bandit-Based Estimation of Distribution Algorithms for Noisy Optimization: Rigorous Runtime Analysis", "author": ["P. Rolet", "O. Teytaud"], "venue": "Learning and Intelligent Optimization. Springer, 2010, pp. 97\u2013110.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive Operator Selection with Bandits for a Multiobjective Evolutionary Algorithm based on Decomposition", "author": ["K. Li", "A. Fialho", "S. Kwong", "Q. Zhang"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 18, no. 1, pp. 114\u2013130, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning DFA: Evolution versus Evidence Driven State Merging", "author": ["S.M. Lucas", "T.J. Reynolds"], "venue": "Evolutionary Computation, 2003. CEC\u201903. The 2003 Congress on, vol. 1. IEEE, 2003, pp. 351\u2013358.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Deterministic Finite Automata with a Smart State  Labeling Evolutionary Algorithm", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 7, pp. 1063\u20131074, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Genetic Algorithms in Search Optimization and Machine Learning", "author": ["D.E. Goldberg"], "venue": "Addison-wesley Reading Menlo Park,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1989}, {"title": "On Crossover as an Evolutionarily Viable Strategy", "author": ["J.D. Schaffer", "L.J. Eshelman"], "venue": "ICGA, vol. 91, 1991, pp. 61\u201368.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1991}, {"title": "Upper and Lower Bounds for Randomized Search Heuristics in Black-Box Optimization", "author": ["S. Droste", "T. Jansen", "I. Wegener"], "venue": "Theory of computing systems, vol. 39, no. 4, pp. 525\u2013544, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Memory-Restricted Black-Box Complexity of OneMax", "author": ["B. Doerr", "C. Winzen"], "venue": "Information Processing Letters, vol. 112, no. 1, pp. 32\u201334, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Black-Box Complexity: from Complexity Theory to Playing Mastermind", "author": ["B. Doerr", "C. Doerr"], "venue": "Proceedings of the 15th annual conference companion on Genetic and evolutionary computation. ACM, 2013, pp. 617\u2013640.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "OneMax in Black-Box Models with Several Restrictions", "author": ["C. Doerr", "J. Lengler"], "venue": "Algorithmica, pp. 1\u201331, 2016. [Online]. Available: http://dx.doi.org/10.1007/s00453-016-0168-1", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Analyzing Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "Z.-H. Zhou"], "venue": "Evolutionary computation, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of the (1+ 1) EA for a Noisy OneMax", "author": ["S. Droste"], "venue": "Genetic and Evolutionary Computation\u2013GECCO 2004. Springer, 2004, pp. 1088\u20131099.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "The Royal Road for Genetic Algorithms: Fitness Landscapes and GA Performance", "author": ["M. Mitchell", "S. Forrest", "J.H. Holland"], "venue": "Proceedings of the first european conference on artificial life. Cambridge: The MIT Press, 1992, pp. 245\u2013254.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1992}, {"title": "General Video Game Level Generation", "author": ["A. Khalifa", "D. Perez-Liebana", "S.M. Lucas", "J. Togelius"], "venue": "2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "A General Noise Model and its Effects on Evolution Strategy Performance", "author": ["D.V. Arnold", "H.-G. Beyer"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 10, no. 4, pp. 380\u2013391, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "The Theory of Evolution Strategies", "author": ["H.-G. Beyer"], "venue": "Springer Science & Business Media,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Portfolio Methods in Uncertain Contexts", "author": ["J. Liu"], "venue": "Ph.D. dissertation, INRIA, 12 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Effectiveness of Sampling for Evolutionary Optimization in Noisy Environments", "author": ["C. Qian", "Y. Yu", "Y. Jin", "Z.-H. Zhou"], "venue": "Parallel Problem Solving from Nature\u2013PPSN XIII. Springer, 2014, pp. 302\u2013311.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal Credit Assignment in Reinforcement Learning", "author": ["R.S. Sutton"], "venue": "1984.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1984}, {"title": "Investigating Learning Rates for Evolution and Temporal Difference Learning", "author": ["S.M. Lucas"], "venue": "Computational Intelligence and Games, 2008. CIG\u201908. IEEE Symposium On. IEEE, 2008, pp. 1\u20137.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Estimating Learning Rates in Evolution and TDL: Results on a Simple Grid-World Problem", "author": ["\u2014\u2014"], "venue": "Computational Intelligence and Games (CIG), 2010 IEEE Symposium on. IEEE, 2010, pp. 372\u2013379.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Evolutionary Algorithms (EA) have achieved widespread use since their developments in the 1950s and 1960s [1], [2], [3], [4], [5], [6], [7].", "startOffset": 136, "endOffset": 139}, {"referenceID": 7, "context": "Bandit algorithms [8], [9] have become popular for optimising either simple regret (the best final decision after a number of exploratory trials) or cumulative regret (best sum of rewards over a number of trials) in A/B testing.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Bandit algorithms [8], [9] have become popular for optimising either simple regret (the best final decision after a number of exploratory trials) or cumulative regret (best sum of rewards over a number of trials) in A/B testing.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "A popular bandit algorithm is the Upper Confidence Bound (UCB) algorithm [10], [11] which balances the trade-off between exploration and exploitation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "A popular bandit algorithm is the Upper Confidence Bound (UCB) algorithm [10], [11] which balances the trade-off between exploration and exploitation.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "The UCB-style algorithms have achieved widespread use within Monte Carlo Tree Search (MCTS) [12], called UCT when applied to trees, the \u201cT\u201d being for Trees.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "A wide literature exists on bandits [10], [11], [13], [14], [15], [16], [17] and many tools have been proposed for distributing the computational power over the stochastic arms to be tested.", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the non-stationary nature of bandit problems in optimization portfolios.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the non-stationary nature of bandit problems in optimization portfolios.", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "There are also some adaptations to other contexts: time varying as in [18]; adversarial [19], [20]); or involving the non-stationary nature of bandit problems in optimization portfolios.", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "St-Pierre and Liu [21] applied the Differential Evolution algorithm [22] to some non-stationary bandit problem, which outperformed the classical bandit algorithm on the selection over a portfolio of solvers.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "St-Pierre and Liu [21] applied the Differential Evolution algorithm [22] to some non-stationary bandit problem, which outperformed the classical bandit algorithm on the selection over a portfolio of solvers.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "[12] noted the great potential for hybridising MCTS with other approaches to optimisation and learning, and in this paper we provide a hybridisation of an evolutionary algorithm with a bandit algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "There are examples of using evolution to tune MCTS parameters [23], [24], [25].", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "There are examples of using evolution to tune MCTS parameters [23], [24], [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "There are examples of using evolution to tune MCTS parameters [23], [24], [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "[25] made fitness evaluations after each rollout, so they could be rapidly optimised, albeit noisily.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "This has some similarities with Estimation of Distribution Algorithms (EDAs) [26] but the details are significantly different.", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "For instance, Lucas and Reynolds evolved Deterministic Finite Automata (DFA) [28], [29], using a multi-start RMHC algorithm with very competitive results, outperforming more complex evolutionary algorithms, and for some classes of problems also outperforming the state of the art EvidenceDriven State Merging (EDSM) algorithms.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "For instance, Lucas and Reynolds evolved Deterministic Finite Automata (DFA) [28], [29], using a multi-start RMHC algorithm with very competitive results, outperforming more complex evolutionary algorithms, and for some classes of problems also outperforming the state of the art EvidenceDriven State Merging (EDSM) algorithms.", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "Although Goldberg [30] used bandit models, they were used to help understand the operation of a Simple Genetic Algorithm.", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "The OneMax problem [31] is a simple linear problem aiming at maximising the number of 1 of a binary string, i.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "The complexity of OneMax problem is O(n log(n)) for a n-bit string [32].", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "proved that the black-box complexity with memory restriction one is at most 2n [33].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35].", "startOffset": 93, "endOffset": 97}, {"referenceID": 34, "context": "More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35].", "startOffset": 99, "endOffset": 103}, {"referenceID": 34, "context": "More lower and upper bounds of the complexity of OneMax in the different models are analysed [34], [35] and then summarised in Table 1 of [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 35, "context": "It is notable that our noise model is very different from the one in [36], which used (1+1)-EA and a one-bit noise.", "startOffset": 69, "endOffset": 73}, {"referenceID": 36, "context": "The influence of the noise strength on the runtime of (1+1)EA for the OneMax problem corrupted by one-bit noise is firstly analysed by Droste [37].", "startOffset": 142, "endOffset": 146}, {"referenceID": 36, "context": "In [37] and [36], the misranking occurs due to the change of exactly one uniformly chosen bit of s by noise with probability p \u2208 (0, 1), p is the noise strength.", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "In [37] and [36], the misranking occurs due to the change of exactly one uniformly chosen bit of s by noise with probability p \u2208 (0, 1), p is the noise strength.", "startOffset": 12, "endOffset": 16}, {"referenceID": 37, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Surprisingly, a simple Random Mutation Hill-Climbing Algorithm outperforms GA on a simple Royal Road function, namely R1 in [38], [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "For the further work, we are interested in applying our proposed approach to more difficult problems (such as game level generation and evaluation [40]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 39, "context": "Resampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43].", "startOffset": 111, "endOffset": 115}, {"referenceID": 40, "context": "Resampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43].", "startOffset": 117, "endOffset": 121}, {"referenceID": 41, "context": "Resampling has been proved to be a powerful tool to improve the local performance of EAs in noisy optimization [41], [42] and a variety of resampling rules applied to EAs in continuous noisy optimization are studied in [43].", "startOffset": 219, "endOffset": 223}, {"referenceID": 42, "context": "[44] proved theoretically and empirically that under some conditions, resampling is not beneficial for a (1+1)-EA optimizing 10-OneMax under additive Gaussian noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "The Royal Road functions involve a harder credit assignment [45] problem than standard OneMax, an important aspect of sequential decision making.", "startOffset": 60, "endOffset": 64}, {"referenceID": 44, "context": "More use is made of the available information, leading to faster learning (see [46] and [47] for more analysis of the information rates of simple evolutionary algorithms).", "startOffset": 79, "endOffset": 83}, {"referenceID": 45, "context": "More use is made of the available information, leading to faster learning (see [46] and [47] for more analysis of the information rates of simple evolutionary algorithms).", "startOffset": 88, "endOffset": 92}, {"referenceID": 38, "context": "Also, because the method makes such efficient use of the fitness evaluations, it is ready to be applied to expensive optimisation problems such as game level design evaluation [40].", "startOffset": 176, "endOffset": 180}], "year": 2016, "abstractText": "The Random Mutation Hill-Climbing algorithm is a direct search technique mostly used in discrete domains. It repeats the process of randomly selecting a neighbour of a bestso-far solution and accepts the neighbour if it is better than or equal to it. In this work, we propose to use a novel method to select the neighbour solution using a set of independent multiarmed bandit-style selection units which results in a bandit-based Random Mutation Hill-Climbing algorithm. The new algorithm significantly outperforms Random Mutation Hill-Climbing in both OneMax (in noise-free and noisy cases) and Royal Road problems (in the noise-free case). The algorithm shows particular promise for discrete optimisation problems where each fitness evaluation is expensive.", "creator": "LaTeX with hyperref package"}}}