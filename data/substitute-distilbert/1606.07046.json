{"id": "1606.07046", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering", "abstract": "situated question answering is the problem of semantic questions defending an environment such as an image. this problem requires avoiding both a question and relative environment, and is challenging making the set selection interpretations is large, typically superexponential in the number such environmental objects. existing models handle this equation by making strong - - then untrue - - independence interpretations. we present parsing inference probabilistic programs ( p3 ), a novel situated question answering model that utilizes approximate inference to eliminate these independence assumptions essentially enable the use of expressive features of the question / np interpretation. our key insight approach to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. we evaluate our approach on a new, publicly - released data set of internal science diagram questions, finding that our approach outperforms several traditional baselines.", "histories": [["v1", "Wed, 22 Jun 2016 19:19:29 GMT  (306kb,D)", "http://arxiv.org/abs/1606.07046v1", "EMNLP 2016 submission, 10 pages"], ["v2", "Sat, 24 Sep 2016 00:37:43 GMT  (3772kb,D)", "http://arxiv.org/abs/1606.07046v2", "EMNLP 2016, 11 pages"]], "COMMENTS": "EMNLP 2016 submission, 10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jayant krishnamurthy", "oyvind tafjord", "aniruddha kembhavi"], "accepted": true, "id": "1606.07046"}, "pdf": {"name": "1606.07046.pdf", "metadata": {"source": "CRF", "title": "Semantic Parsing to Probabilistic Programs for Situated Question Answering", "authors": ["Jayant Krishnamurthy", "Oyvind Tafjord"], "emails": ["jayantk@allenai.org", "oyvindt@allenai.org"], "sections": [{"heading": "1 Introduction", "text": "Situated question answering is the problem of answering questions about an environment such as an image. Examples of this problem include visual question answering (Antol et al., 2015), robot direction following (Kollar et al., 2010), and even text question answering (Berant et al., 2014). This problem requires interpreting both a question and the environment, and is challenging because the set of possible interpretations is large and exhibits complex dependence.\n1. According to the given food chain, what is the number of organisms that eat deer? (A) 3 (B) 2 (C) 4 (D) 1\n2. Based on the given food web, what would happen if there were no insect-eating birds? (A) The grasshopper population would increase. (B) The grasshopper population would decrease. (C) There would be no change in grasshopper number.\nFigure 1: Two example food web questions. Answering these questions requires both question and environment (image) interpretation.\nFigure 1 shows questions from an 8th grade science test that exemplify this challenge. These questions require the student to interpret an image depicting a food web, which shows the organisms in an ecosystem with arrows indicating what organisms eat. First, a computer could potentially extract many different food webs from the image. If each of the 15 text labels represents an organism, there are 152 = 225 possible eats relations, and hence 2225 possible food webs. Second, these relations are not independent because food webs have global structural properties \u2013 e.g., they are typically directed acyclic graphs. Finally, the question and environment interpretations are dependent \u2013 e.g., an interar X\niv :1\n60 6.\n07 04\n6v 1\n[ cs\n.C L\n] 2\n2 Ju\nn 20\npretation is wrong if it identifies multiple answer options as correct. In order to reason about the large set of interpretations, existing possible worlds models for situated question answering make strong independence assumptions that preclude them from representing these complex dependencies (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014).\nThis paper presents Parsing to Probabilistic Programs (P 3), a novel situated question answering model that eliminates these strong independence assumptions by embracing approximate inference. Our key idea is to perform semantic parsing and treat the semantic parse as a probabilistic program. Unlike a deterministic program, which always executes the same way, a probabilistic program may include nondeterministic choice primitives that enable it to have multiple executions, each of which may return a different value. P 3 uses this set of executions to represent uncertainty over environment interpretations. Inference over environment representations becomes inference over a sequence of nondeterministic choice points in the program, which we approximate using beam search. This approximate inference procedure has linear running time in the number of choices and enables the model to use arbitrary global features of the question, semantic parse, environment, and current execution.\nWe present an experimental evaluation of P 3 on a new data set of 8th grade science food web diagram questions (Figure 1). We compare our approach to several baselines, including possible worlds and neural network approaches, finding that P 3 outperforms both. An ablation study demonstrates that global features help the model achieve high accuracy. Finally, we demonstrate that P 3 improves performance on a previously published data set."}, {"heading": "2 Prior Work", "text": "Possible world approaches to situated question answering combine both semantic parsing and environment interpretation models using a possible worlds semantics. Most work has assumed that these models are independent and the environment interpretation consists of independent predicate instances (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014). Seo et\nal., (2015) incorporate hard constraints on the joint question/environment interpretation; however, their approach does not generalize to soft constraints or arbitrary logical forms.\nAnother line of work has assumed a semantic parser is given, focusing exclusively on environment interpretation. This work includes robot direction following (Kollar et al., 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a) and question answering against knowledge representations extracted from text (Berant et al., 2014; Krishnamurthy and Mitchell, 2015).\nNeural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a). Our work is most closely related to DNMNs (Andreas et al., 2016b), the only neural model that learns to semantically parse the question. The key distinction is that DNMNs use a continuous environment representation whereas we use a discrete representation. Continuous and discrete representations are well-suited to different tasks \u2013 in our case, food webs are naturally discrete. P 3 can also use declaratively-specified background knowledge in the form of predefined functions.\nPreliminaries for our work are semantic parsing and probabilistic programming. Semantic parsing is used to interpret questions against a deterministic execution model (e.g., a database), for applications such as question answering (Zettlemoyer and Collins, 2005; Liang et al., 2011), direction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and information extraction (Krishnamurthy and Mitchell, 2012). Our work extends semantic parsing to settings without a deterministic execution model.\nProbabilistic programming languages extend deterministic languages with primitives for nondeterministic choice (Goodman and Stuhlmu\u0308ller, 2014). We express logical forms in a probabilistic variant of Scheme similar to Church (Goodman et al., 2008); however, in this paper we use Python-like pseudocode for clarity. The language has a single nondeterministic primitive called choose that nondeterministically returns one of its arguments. For example choose(1,2,3) has three executions where it returns either 1, 2, or 3. Multiple calls to choose can be combined in a single program; for example,\nchoose(1,2)+choose(1,2) has four executions that return 2, 3, 3 and 4. Calls to choose are ordered such that the possible executions of a logical form can be viewed as a tree where each node represents a call to choose and each outbound edge represents a value that can be returned."}, {"heading": "3 Parsing to Probabilistic Programs (P 3)", "text": "This section describes our model, Parsing to Probabilistic Programs (P 3). The input to the model is a question and an environment and its output is a denotation, which is a formal answer to the question. P 3 has two factors, a semantic parser and an execution model. The semantic parser scores syntactic parses and logical forms for the question; Figure 2 shows an example parse. These logical forms are probabilistic programs with multiple possible executions, each of which may return a different denotation. The set of executions is determined by an initialization program that defines the logical form\u2019s functions using choose. Figure 4 shows pseudocode for the food web initialization program and Figure 3 shows how a logical form is executed. The execution model assigns a score to each of these executions given the environment. We defer additional application-specific discussion to Section 4.\nFormally, P 3 is a loglinear model that predicts a denotation \u03b3 for a question q in an environment v using three latent variables:\nP (\u03b3|v, q; \u03b8) = \u2211 e,`,t P (e, `, t|v, q; \u03b8)1(ret(e) = \u03b3)\nP (e, `, t|v, q; \u03b8) = 1 Zq,v fex(e, `|v; \u03b8ex)fprs(`, t|q; \u03b8prs)\nThe model is composed of two factors. fprs represents a semantic parser that scores logical forms ` and syntactic parse trees t given question q and parameters \u03b8prs. fex represents a nondeterministic ex-\nfilter(lambda f.cause( decrease(getOrganism(\"mice\")), f(getOrganism(\"snakes\"))),\nset(decrease, increase, unchanged))\n(a) \u03bbf.CAUSE(DECREASE(MICE), f(SNAKES)) expressed as a probabilistic program. Entities are created using getOrganism and logical forms with functional types are wrapped in a filter.\nORGANISM(MICE)\nfail ORGANISM(SNAKES)\nfail EATS(MICE, SNAKES)\nEATS(SNAKES,MICE)\n{UNCH.} {DEC.}\nEATS(SNAKES,MICE)\n{INC.} {}\nfalse true\nfalse true\nfalse\nfalse true\ntrue\nfalse true\n(b) Possible executions of the program. Each path from root to leaf represents a single execution that returns the indicated denotation or fails. Each internal node represents a nondeterministic decision made with choose. The ORGANISM nodes originate from getOrganism and the EATS nodes from cause; see Figure 4.\nFigure 3: Executing the logical form \u03bbf.CAUSE(DECREASE(MICE), f(SNAKES)).\necution model for logical forms. Given parameters \u03b8ex, this factor assigns a score to a logical form ` and its execution e. The denotation \u03b3, i.e., the formal answer to the question, is simply the value returned by e. Zq,v represents the model\u2019s partition function. The following sections describe P 3 in more detail."}, {"heading": "3.1 Semantic Parsing", "text": "The factor fprs represents a Combinatory Categorial Grammar (CCG) semantic parser (Zettlemoyer and Collins, 2005) that scores logical forms for a question. Given a lexicon1 mapping words to syntactic categories and logical forms, CCG defines a set of possible syntactic parses t and logical forms ` for a question q. Figure 2 shows an example CCG parse. fprs is a loglinear model over parses (`, t):\nfprs(`, t|q; \u03b8prs) = exp{\u03b8Tprs\u03c6(`, t, q)} 1In our experiments, we automatically learn the lexicon in a\npreprocessing step (Section 5.2).\nThe function \u03c6maps parses to feature vectors. We use a rich set of features similar to those for syntactic CCG parsing (Clark and Curran, 2007); a full description is provided in an online appendix."}, {"heading": "3.2 Execution Model", "text": "The factor fex is a loglinear model over the executions of a logical form given an environment. Recall that logical forms in P 3 are probabilistic programs with a set of possible executions, each of which may return a different value. Each execution is a sequence, e = [e0, e1, e2, ..., en], where e0 is the program\u2019s starting state, ei represents the state immediately after the ith call to choose, and en is the state at termination. The score of an execution is:\nfex(e, `|v; \u03b8ex) = n\u220f i=1 exp{\u03b8Tex\u03c6(ei\u22121, ei, `, v)}\nIn the above equation, \u03b8ex represents the model\u2019s parameters and \u03c6 represents a feature function that produces a feature vector representing the difference between sequential program states ei\u22121 and ei given environment v and logical form `. Importantly, \u03c6 can include arbitrary features of the execution, logical form and environment. Such features are important in diagram question answering, for example, to detect cycles in the food web (see Section 4.4)."}, {"heading": "3.3 Inference", "text": "P 3 is designed to rely on approximate inference: our goal is to use rich features to accurately make local decisions, as in linear-time parsers (Nivre et al., 2006). We perform approximate inference using a two-stage beam search. Given a question q, the first stage performs a beam search over CCG parses to produce a list of logical forms scored by fprs. This step is performed by using a CKY-style chart parsing algorithm then marginalizing out the syntactic parses. The second stage performs a beam search over executions of each logical form. This search first rewrites the logical form into continuationpassing style, then maintains a beam of continuations \u2013 each of which represents a partial execution \u2013 scored by fex. The continuation-passing transformation allows choose to be implemented as a function that adds multiple continuations \u2013 one per return value \u2013 to the search queue (Goodman and\nStuhlmu\u0308ller, 2014). Executing a continuation runs the program until either the next call to choose or termination. Each step of the search executes every continuation in the beam, adding new continuations to the beam for the next step and storing any terminated executions in a separate queue. In our experiments, we use a beam size of 100 in the semantic parser, executing each of the 10 highest-scoring logical forms with a beam of 100 continuations."}, {"heading": "3.4 Training", "text": "P 3 is trained by optimizing loglikelihood with stochastic gradient ascent. The training data {(qi, vi, ci)}ni=1 is a collection of questions qi and environments vi paired with supervision oracles ci. ci(e) = 1 for a correct execution e and ci(e) = 0 otherwise. The oracle ci can be used to implement various kinds of supervision, including: (1) labeled denotations, by verifying the return value of e at termination and (2) labeled environments, by verifying decisions made by e about the environment interpretation. The oracle for diagram question answering combines both forms of supervision (Section 4.5).\nThe objective function O is the loglikelihood of predicting a correct execution:\nO(\u03b8) = n\u2211 i=1 log \u2211 e,l,t ci(e)P (e, `, t|qi, vi; \u03b8)\nWe optimize this objective function using stochastic gradient ascent, using the approximate inference algorithm from Section 3.3 to estimate the necessary marginals. When computing the marginal distribution over correct executions, we filter each step of the beam search using the supervision oracle ci to improve the approximation."}, {"heading": "4 Diagram Question Answering with P 3", "text": "As a case study, we apply P 3 to the task of answering diagram questions from an 8th grade science domain. There are a few steps required to apply P 3. First, we write an initialization program that defines the constituent functions of logical forms, the environment representation and the corresponding uncertainty. Second, we create a component to select an answer given a denotation. Finally, we define the model\u2019s features and supervision oracle."}, {"heading": "4.1 Food Web Diagram Questions", "text": "We consider the task of answering food web diagram questions. The input consists of an image depicting a food web, a natural language question and a list of natural language answer options (Figure 1). The goal is to select the correct answer from these options. A food web represents the energy flow in an ecosystem as a directed graph where each vertex x represents an organism and an edge from x to y indicates that organism y eats organism x. Many different kinds of questions can be asked about a food web, from simply what eats what, to the roles of animals and the consequences of population changes. This task has many regularities that require global features: for example, food webs are usually acyclic, and certain animals usually have certain roles (e.g., mice are herbivores). We have collected and released a data set for this task (Section 5.1).\nWe preprocess the images in the data set using a computer vision system that identifies candidate diagram elements (Kembhavi et al., 2016). This system extracts a collection of text labels (via OCR), arrows, arrowheads, and objects, each with corresponding scores. It also extracts a collection of scored linkages between these elements. These extractions are noisy and contain many discrepancies such as overlapping text labels and spurious linkages. We use these extractions to define a set of candidate organisms (using the text labels), and also to define features of the execution model."}, {"heading": "4.2 Initialization Program", "text": "The initialization program for diagram questions defines a distribution over partial food webs given a logical form. It assumes that each diagram has a known set of entities, which in our case is the set of extracted text labels. The program defines a collection of learned predicates over these entities to represent depicted food web; these predicates invoke choose to represent uncertainty. The program also includes deterministic functions that encode background knowledge. Figure 4 shows pseudocode for a portion of the initialization program.\nFood webs are represented using two learned predicates: ORGANISM(x) indicates whether the text label x is an organism (as opposed to, e.g., the image title); and EATS(x, y) indicates whether\norganism x eats organism y. One way to represent uncertainty over food webs is to use choose to nondeterministically select the truth value of every predicate instance in the initialization program, thereby defining a possible worlds-style distribution over all food webs. However, this representation may resolve more uncertainty than necessary for a particular logical form; intuitively, executing \u03bbx.EATS(x,MOUSE) only requires the values of a handful of predicate instances involving the mouse. We instead represent uncertainty using a just-in-time approach that only chooses values for predicate instances necessary to produce the logical form\u2019s denotation. This approach initializes every predicate instance\u2019s value to undef, delaying choosing its value until an execution tries to use it.\nThe initialization program also includes several other deterministic functions of the learned predicates. One set of functions consists of animal role predicates, such as HERBIVORE(x), that are defined in terms of EATS. Another set of functions represents population change events and reasons about their consequences. Finally, the program also defines operators such as COUNT."}, {"heading": "4.3 Answer Selection", "text": "The answer selection component uses string match heuristics and an LSTM to produce a distribution over multiple choice answers given a distribution over denotations predicted by P 3. The string match heuristics score each answer option given a denotation then select the highest scoring answer, abstaining in the case of a tie. The score computation depends on the denotation\u2019s type. If the denotation is a set of entities, the score is an approximate count of the number of entities in the denotation that are mentioned in the answer based on a fuzzy string match. If the denotation is a set of change events, the score is a fuzzy match of both the change direction and the animal name. If the denotation is a number, string matching is straightforward. Applying these heuristics and marginalizing out denotations yields a distribution over answer options.\nA limitation of the above approach is that it does not directly incorporate linguistic prior knowledge about likely answers. For example, \u201csnake\u201d is usually a good answer to \u201cwhat eats mice?\u201d regardless of the diagram. Such knowledge is known to be essential for visual question answering (Antol et al., 2015; Andreas et al., 2016b) and important in our task as well. We incorporate this knowledge in a standard way, by training a neural network on question/answer pairs (without the diagram) and combining its predictions with the string match heuristics above. The network is a sequence LSTM that is applied to the question concatenated with each answer option a to produce a 50-dimensional vector va for each answer. The distribution over answers is the softmax of the inner product of these vectors with a learned parameter vector w. For simplicity, we combine these two components using a 50/50 mix of their answer distributions."}, {"heading": "4.4 Execution Features", "text": "The execution model uses three sets of features: instance features, predicate features, and denotation features. Instance features treat each predicate instance independently, while the other features are functions of multiple predicate instances and the logical form. We provide a complete listing of features in an online appendix.\nInstance features fire whenever an execution\nchooses a truth value for a predicate instance. These features are similar to the per-predicate-instance features used in prior work to produce a distribution over possible worlds. For ORGANISM(x), our features are the vision model\u2019s extraction score for x and indicator features for the number of tokens in x. For EATS(x, y), our features are various combinations of the vision model\u2019s scores for arrows that may connect the text labels x and y.\nPredicate features fire based on the global assignment of truth values to all instances of a single predicate. The features for ORGANISM count occurrences of overlapping text labels among true instances. The features for EATS include cycle count features for various cycle lengths and arrow reuse features. The cycle count features help the model learn that food webs are typically, but not always, acyclic and the arrow reuse features aim to prevent the model from predicting two different EATS instances on the basis of a single arrow.\nDenotation features fire on the return value of an execution. There are two kinds of denotation features: size features that count the number of entities in denotations of various types; and denotation element features for specific logical forms. The second kind of feature can be used to learn that the denotation of \u03bbx.HERBIVORE(x) is likely to contain the entity MOUSE, but unlikely to contain WOLF."}, {"heading": "4.5 Supervision Oracle", "text": "The supervision oracle for diagram question answering combines supervision of both answers and environment interpretations. We assume that each diagram has been labeled with a food web. An execution is correct if and only if (1) all of its global variables encoding the food web are consistent with the labeled food web, and (2) string match answer selection applied to its denotation chooses the correct answer. The first constraint guarantees that every logical form has at most one correct execution for any given diagram."}, {"heading": "5 Evaluation", "text": "Our evaluation compares P 3 to both possible worlds and neural network approaches on our data set of food web diagram questions. An ablation study demonstrates that both sets of global features im-\nprove accuracy. Finally, we demonstrate P 3\u2019s generality by applying it to a previously-published data set, obtaining state-of-the-art results.\nWe have included the data set with the submission, and will release code if the paper is published."}, {"heading": "5.1 FOODWEBS Data Set", "text": "FOODWEBS consists of \u223c500 food web diagrams and \u223c5000 questions designed to imitate actual questions encountered on 8th grade science exams. The train/validation/test sets contain \u223c300/100/100 diagrams and their corresponding questions. The data set has three kinds of annotations in addition to the correct answer for each question. First, each diagram is annotated with the food web that it depicts using the ORGANISM and EATS predicates. Second, each diagram has predictions from a vision system for various diagram elements such as arrows and text labels (Kembhavi et al., 2016). These are noisy predictions, not ground truth. Finally, each question is annotated by the authors with a logical form or null if its meaning cannot be represented using our predicates. These logical forms are not used to train P 3 but are useful to measure per-component error. We collected FOODWEBS in two stages. First, we collected a number of food web diagrams using an image search engine. Second, we generated questions for these diagrams using Mechanical Turk. Workers were shown a food web diagram and a real exam question for inspiration and asked to write a new question and its answer options. We validated each generated question by asking 3 workers to answer it, discarding questions where at least 2 did not choose the correct answer. The authors also manually corrected any ambiguous (e.g., two answer options are correct) and poorly-formatted (e.g., two answer options have the same letter) questions."}, {"heading": "5.2 Baseline Comparison", "text": "Our first experiment compares P 3 with several baselines for situated question answering. The first baseline, WORLDS, is a possible worlds model based on Malinowski and Fritz (2014). This baseline learns a semantic parser P (`, t|q) and a distribution over food webs P (w|v), then evaluates ` on w to produce a distribution over denotations. This model is implemented by independently training P 3\u2019s CCG parser (on question/answer pairs and labeled food\nwebs) and a possible-worlds execution model (on labeled food webs). The CCG lexicon for both P 3 and WORLDS was generated by applying PAL (Krishnamurthy, 2016) to the same data. Finally, both models select answers as described in Section 4.3.\nWe also compared P 3 to several neural network baselines. The first baseline, LSTM, is the textonly answer selection model described in Section 4.3. The second baseline, DQA, is a diagram question answering model that uses a predicted \u201cdiagram parse\u201d of the image (Kembhavi et al., 2016). This model is trained with question/answer pairs and diagram parses, which are roughly comparable to labeled food webs. The third baseline, VQA, is a neural network for visual question answering. This model represents each image as a vector by using the final layer of a pre-trained VGG19 model (Simonyan and Zisserman, 2014) and applying a single fully-connected layer. It scores answer options by using a sequence LSTM to encode the question/answer pair, then computing a dot product between the text and image vectors.\nTable 1 compares the accuracy of P 3 to these baselines. Accuracy is the fraction of questions answered correctly. LSTM performs well on this data set, suggesting that many questions can be answered without using the image. This result is consistent\nwith other multiple-choice situated question answering tasks (Antol et al., 2015). Only P 3 substantially improves accuracy over this strong baseline. We conducted a second experiment that limits the value of linguistic prior knowledge in order to better understand this behavior. We ran each model on a test set with unseen organisms created by reversing the animal names in the question and diagram. On this test set, VQA and DQA again perform similarly to LSTM, suggesting that these models largely learn the same linguistic prior. However, WORLDS and P 3 outperform LSTM because they learn to interpret the diagram. On both data sets, P 3 outperforms WORLDS due to its global features."}, {"heading": "5.3 Ablation Study", "text": "We performed an ablation study to further understand the impact of LSTM answer selection and global features. Table 2 shows the accuracy of P 3 trained without these components. We find that LSTM answer selection improves accuracy by 9 points, as expected due to the importance of linguistic prior knowledge. Global features improve accuracy by 7 points, which is roughly comparable to the delta between P 3 and WORLDS in Table 1."}, {"heading": "5.4 Component Error Analysis", "text": "Our third experiment analyses the sources of error by training and evaluating P 3 while providing the gold logical form, food web, or both as input. Table 3 shows the accuracy of these three models. The final entry shows the maximum accuracy possible given our logical form language and answer selection. The larger performance improvement with gold food webs suggests that the execution model is responsible for more error than semantic parsing, although both components contribute."}, {"heading": "5.5 SCENE Experiments", "text": "Our final experiment applies P 3 to the SCENE data set of Krishnamurthy and Kollar (2013). The task in this data set is to identify the set of objects in an image denoted by a natural language expression, such as \u201cblue mug to the left of the monitor.\u201d We use the provided CCG lexicon and predicate vocabulary, creating a learned predicate in the initialization program for each. We use the provided instance features, adding predicate and denotation size features.\nTable 4 compares P 3 to prior work on SCENE. We consider three different supervision conditions: QA trains with question/answer pairs, QA+E further includes labeled environments, and QA+E+LF further includes labeled logical forms. We trained P 3 in the first two conditions, while prior work trained in the first and third conditions. P 3 slightly outperforms in the QA condition and P 3 trained with labeled environments outperforms prior work trained with labeled environments and logical forms."}, {"heading": "6 Conclusion", "text": "We present Parsing to Probabilistic Programs (P 3), a novel model for situated question answering that embraces approximate inference to enable the use of arbitrary features of the language and environment. P 3 trains a semantic parser to predict logical forms that are probabilistic programs whose possible executions represent environmental uncertainty. We demonstrate this model on a challenging new data set of 5000 science diagram questions, finding that it outperforms several competitive baselines and that its global features improve accuracy. P 3 has several advantageous properties. First, P 3 can be easily applied to new problems: one simply has to write an initialization program and define the execution features. Second, the initialization program can be used to encode a wide class of assumptions about the environment. For example, the model can assume that every noun refers to a single object. The combination of semantic parsing and probabilistic programming makes P 3 an expressive and flexible model with many potential applications."}], "references": [{"title": "2016a. Deep compositional question answering with neural module", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer Vision (ICCV)", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics", "author": ["Artzi", "Zettlemoyer2013] Yoav Artzi", "Luke Zettlemoyer"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2013}, {"title": "Modeling biological processes for reading comprehension", "author": ["Vivek Srikumar", "PeiChun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D. Manning"], "venue": "Proceedings of EMNLP", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["Chen", "Mooney2011] David L. Chen", "Raymond J. Mooney"], "venue": "In Proceedings of the 25th AAAI Conference on Artificial Intelligence", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Wide-coverage efficient statistical parsing with CCG and log-linear models", "author": ["Clark", "Curran2007] Stephen Clark", "James R. Curran"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2007}, {"title": "The Design and Implementation of Probabilistic Programming Languages. http://dippl.org", "author": ["Goodman", "Stuhlm\u00fcller2014] Noah D Goodman", "Andreas Stuhlm\u00fcller"], "venue": null, "citeRegEx": "Goodman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2014}, {"title": "Church: A language for generative models", "author": ["Vikash K. Mansinghka", "Daniel M. Roy", "Keith Bonawitz", "Joshua B. Tenenbaum"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Efficient natural language interfaces for assistive robots", "author": ["Istvan Chung", "Oron Propp", "Matthew R. Walter", "Nicholas Roy"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "Howard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Howard et al\\.", "year": 2014}, {"title": "A natural language planner interface for mobile manipulators", "author": ["Stefanie Tellex", "Nicholas Roy"], "venue": "IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Howard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Howard et al\\.", "year": 2014}, {"title": "A diagram is worth a dozen images. CoRR, abs/1603.07396", "author": ["Mike Salvato", "Eric Kolve", "Min Joon Seo", "Hannaneh Hajishirzi", "Ali Farhadi"], "venue": null, "citeRegEx": "Kembhavi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kembhavi et al\\.", "year": 2016}, {"title": "Toward understanding natural language directions", "author": ["Kollar et al.2010] Thomas Kollar", "Stefanie Tellex", "Deb Roy", "Nicholas Roy"], "venue": "In Proceedings of the 5th ACM/IEEE International Conference on HumanRobot Interaction", "citeRegEx": "Kollar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kollar et al\\.", "year": 2010}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world", "author": ["Krishnamurthy", "Kollar2013] Jayant Krishnamurthy", "Thomas Kollar"], "venue": "Transactions of the Association of Computational Linguistics \u2013 Volume", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2013}, {"title": "Weakly supervised training of semantic parsers", "author": ["Krishnamurthy", "Tom M. Mitchell"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Learning a compositional semantics for freebase with an open predicate vocabulary. Transactions of the Association for Computational Linguistics, 3:257\u2013270", "author": ["Krishnamurthy", "Tom M. Mitchell"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2015}, {"title": "Probabilistic models for learning a semantic parser lexicon", "author": ["Jayant Krishnamurthy"], "venue": "In NAACL", "citeRegEx": "Krishnamurthy.,? \\Q2016\\E", "shortCiteRegEx": "Krishnamurthy.", "year": 2016}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2011] Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": "In International Conference on Computer Vision", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Maltparser: A data-driven parsergenerator for dependency parsing", "author": ["Nivre et al.2006] Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Solving geometry problems: Combining text and diagram interpretation", "author": ["Seo et al.2015] Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni", "Clint Malcolm"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Seo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Thomas Kollar", "Steven Dickerson", "Matthew Walter", "Ashis Banerjee", "Seth Teller", "Nicholas Roy"], "venue": "In AAAI Conference on Artifi-", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "arXiv preprint arXiv:1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] Luke S. Zettlemoyer", "Michael Collins"], "venue": "In UAI \u201905, Proceedings of the 21st Conference in Uncertainty in Artificial In-", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Examples of this problem include visual question answering (Antol et al., 2015), robot direction following (Kollar et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 12, "context": ", 2015), robot direction following (Kollar et al., 2010), and even text question answering (Berant et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 4, "context": ", 2010), and even text question answering (Berant et al., 2014).", "startOffset": 42, "endOffset": 63}, {"referenceID": 20, "context": "In order to reason about the large set of interpretations, existing possible worlds models for situated question answering make strong independence assumptions that preclude them from representing these complex dependencies (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014).", "startOffset": 224, "endOffset": 307}, {"referenceID": 20, "context": "Most work has assumed that these models are independent and the environment interpretation consists of independent predicate instances (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014).", "startOffset": 135, "endOffset": 218}, {"referenceID": 16, "context": ", 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014). Seo et al., (2015) incorporate hard constraints on the joint question/environment interpretation; however, their approach does not generalize to soft constraints or arbitrary logical forms.", "startOffset": 8, "endOffset": 88}, {"referenceID": 12, "context": "This work includes robot direction following (Kollar et al., 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a) and question answering against knowledge representations extracted from text (Berant et al.", "startOffset": 45, "endOffset": 131}, {"referenceID": 24, "context": "This work includes robot direction following (Kollar et al., 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a) and question answering against knowledge representations extracted from text (Berant et al.", "startOffset": 45, "endOffset": 131}, {"referenceID": 4, "context": ", 2014a) and question answering against knowledge representations extracted from text (Berant et al., 2014; Krishnamurthy and Mitchell, 2015).", "startOffset": 86, "endOffset": 141}, {"referenceID": 2, "context": "Neural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a).", "startOffset": 70, "endOffset": 157}, {"referenceID": 19, "context": "Neural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a).", "startOffset": 70, "endOffset": 157}, {"referenceID": 25, "context": "Neural networks have been recently used for visual question answering (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015; Andreas et al., 2016a).", "startOffset": 70, "endOffset": 157}, {"referenceID": 17, "context": ", a database), for applications such as question answering (Zettlemoyer and Collins, 2005; Liang et al., 2011), direction following (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and information extraction (Krishnamurthy and Mitchell, 2012).", "startOffset": 59, "endOffset": 110}, {"referenceID": 8, "context": "We express logical forms in a probabilistic variant of Scheme similar to Church (Goodman et al., 2008); however, in this paper we use Python-like pseudocode for clarity.", "startOffset": 80, "endOffset": 102}, {"referenceID": 21, "context": "P 3 is designed to rely on approximate inference: our goal is to use rich features to accurately make local decisions, as in linear-time parsers (Nivre et al., 2006).", "startOffset": 145, "endOffset": 165}, {"referenceID": 11, "context": "We preprocess the images in the data set using a computer vision system that identifies candidate diagram elements (Kembhavi et al., 2016).", "startOffset": 115, "endOffset": 138}, {"referenceID": 2, "context": "Such knowledge is known to be essential for visual question answering (Antol et al., 2015; Andreas et al., 2016b) and important in our task as well.", "startOffset": 70, "endOffset": 113}, {"referenceID": 11, "context": "Second, each diagram has predictions from a vision system for various diagram elements such as arrows and text labels (Kembhavi et al., 2016).", "startOffset": 118, "endOffset": 141}, {"referenceID": 16, "context": "The CCG lexicon for both P 3 and WORLDS was generated by applying PAL (Krishnamurthy, 2016) to the same data.", "startOffset": 70, "endOffset": 91}, {"referenceID": 11, "context": "The second baseline, DQA, is a diagram question answering model that uses a predicted \u201cdiagram parse\u201d of the image (Kembhavi et al., 2016).", "startOffset": 115, "endOffset": 138}, {"referenceID": 2, "context": "with other multiple-choice situated question answering tasks (Antol et al., 2015).", "startOffset": 61, "endOffset": 81}, {"referenceID": 16, "context": "Our final experiment applies P 3 to the SCENE data set of Krishnamurthy and Kollar (2013). The task in this data set is to identify the set of objects in an image denoted by a natural language expression, such as \u201cblue mug to the left of the monitor.", "startOffset": 58, "endOffset": 90}, {"referenceID": 16, "context": "KK2013 results are from Krishnamurthy and Kollar (2013).", "startOffset": 24, "endOffset": 56}], "year": 2016, "abstractText": "Situated question answering is the problem of answering questions about an environment such as an image. This problem requires interpreting both a question and the environment, and is challenging because the set of interpretations is large, typically superexponential in the number of environmental objects. Existing models handle this challenge by making strong \u2013 and untrue \u2013 independence assumptions. We present Parsing to Probabilistic Programs (P ), a novel situated question answering model that utilizes approximate inference to eliminate these independence assumptions and enable the use of global features of the question/environment interpretation. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, finding that our approach outperforms several competitive baselines.", "creator": "LaTeX with hyperref package"}}}