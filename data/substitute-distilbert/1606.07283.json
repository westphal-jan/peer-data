{"id": "1606.07283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Event Abstraction for Process Mining using Supervised Learning Techniques", "abstract": "process mining techniques focus on generating insight in processes from event logs. today many cases, events recorded off the event log are too fine - crafted, causing process discovery researchers directly discover incomprehensible process models or process models that are not representative of the event log. we show here when process discovery algorithms are only able to calculate an unrepresentative process model from a low - level event log, structure in the process can in some cases still be discovered by first abstracting the event log without assure higher level of granularity. this gives rise to the challenge to bridge the gap allowing an original low - level event log and a desired high - level perspective on this log, such that a fairly structured or theoretically comprehensible process model can be discovered. we show that initial flows can gain leveraged for the event abstraction task when annotations with high - level interpretations of the low - level events are available for a subset of the sequences ( an. e., traces ). we present a method to denote abstract vector representations of events based on xes extensions, and describe an approach to abstract events in an event log with condition template fields using these event features. importantly, we propose a sequence - equivalence metric to evaluate supervised event abstraction parameters that fits closely amid comparable tasks of process discovery and conformance checking. we conclude out paper by demonstrating the usefulness needed learned event abstraction for obtaining more structured and / or more comprehensible process models describing both real life event data yet synthetic event data.", "histories": [["v1", "Thu, 23 Jun 2016 12:12:45 GMT  (1178kb,D)", "http://arxiv.org/abs/1606.07283v1", "paper to appear in the proceedings of the SAI Intelligent Systems Conference 2016"]], "COMMENTS": "paper to appear in the proceedings of the SAI Intelligent Systems Conference 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["niek tax", "natalia sidorova", "reinder haakma", "wil m p van der aalst"], "accepted": false, "id": "1606.07283"}, "pdf": {"name": "1606.07283.pdf", "metadata": {"source": "CRF", "title": "Event Abstraction for Process Mining using Supervised Learning Techniques", "authors": ["Niek Tax", "Natalia Sidorova", "Reinder Haakma"], "emails": ["n.tax@tue.nl", "n.sidorova@tue.nl", "reinder.haakma@philips.com", "w.m.p.v.d.aalst@tue.nl"], "sections": [{"heading": null, "text": "Keywords\u2014Process Mining, Event Abstraction, Probabilistic Graphical Models\nI. INTRODUCTION\nProcess mining is a fast growing discipline that combines knowledge and techniques from computational intelligence, data mining, process modeling and process analysis [1]. Process mining focuses on the analysis of event logs, which consists of sequences of real-life events observed from process executions, originating e.g. from logs from ERP systems. An important subfield of process mining is process discovery, which is concerned with the task of finding a process model\nthat is representative of the behavior seen in an event log. Many different process discovery algorithms exist ([2], [3], [4], [5], [6]), and many different types of process models can be discovered by process discovery methods, including BPMN models, Petri nets, process trees, and statecharts.\nAs event logs are often not generated specifically for the application of process mining, events granularity of the event log at hand might be too low level. It is vital for successful application of process discovery techniques to have event logs at an appropriate level of abstraction. Process discovery techniques when the input event log is too low level might result in process model with one or more undesired properties. First of all, the resulting process model might be \u201dspaghetti\u201dlike, as shown in Figure 1, containing of an uninterpretable mess of nodes and connections. The aim of process discovery is to discover a structured, \u201dlasagna\u201d-like, process model as shown in Figure 2, which is much more interpretable than a \u201dspaghetti\u201d-like model. Secondly, the activities in the process\nIEEE 1 | P a g e\nar X\niv :1\n60 6.\n07 28\n3v 1\n[ cs\n.L G\nmodel might have too specific, non-meaningful, names. Third, as we show in section IV, process discovery algorithms are sometimes not able to discover a process model that represents the low-level event log well, while being able to discover to discover a representative process model from a corresponding high-level event log. The problems mentioned illustrate the need for a method to abstract too low-level event logs into higher level event logs.\nSeveral methods have been explored within the process mining field that address the challenge of abstracting low-level events to higher level events ([7], [8], [9]). Existing event abstraction methods rely on unsupervised learning techniques to abstract low-level into high-level events by clustering together groups of low-level events into one high-level event. However, using unsupervised learning introduces two new problems. First, it is unclear how to label high-level events that are obtained by clustering low-level events. Current techniques require the user / process analyst to provide high-level event labels themselves based on domain knowledge, or generate long labels by concatenating the labels of all low-level events incorporated in the cluster. However, long concatenated labels quickly become unreadable for larger clusters, and it is far from trivial for a user to come up with sensible labels manually. In addition, unsupervised learning approaches for event abstraction give no guidance with respect to the desired level of abstraction. Many existing event abstraction methods contain one or more parameters to control the degree in which events are clustered into higher level events. Finding the right level of abstraction providing meaningful results is often a matter of trial-and-error.\nIn some cases, training data with high-level target labels of low-level events are available, or can be obtained, for a subset of the traces. In many settings, obtaining high-level annotations for all traces in an event log is infeasible or too expensive. Learning a supervised learning model on the set of\ntraces where high-level target labels are available, and applying that model to other low-level traces where no high-level labels are available, allows us to build a high-level interpretation of a low-level event log, which can then be used as input for process mining techniques.\nIn this paper we describe a method for supervised event abstraction that enables process discovery from too finegrained event logs. This method can be applied to any event log where higher level training labels of low level events are available for a subset of the traces in the event log. We start by giving an overview of related work from the activity recognition field in Section II. In Section III we introduce basic concepts and definitions used throughout the rest of the paper. Section IV explains the problem of not being able to mine representative process models from low-level data in more detail. In Section V we describe a method to automatically retrieve a feature vector representation of an event that can be used with supervised learning techniques, making use of certain aspects of the XES standard definition for event logs [10]. In the same section we describe a supervised learning method to map low-level events into target high-level events. Sections VI and VII respectively show the added value of the described supervised event abstraction method for process mining on a real life event log from a smart home environment and on a synthetic log from a digital photocopier respectively. Section VIII concludes the paper."}, {"heading": "II. RELATED WORK", "text": "Supervised event abstraction is an unexplored problem in process mining. A related field is activity recognition within the field of ubiquitous computing. Activity recognition focuses on the task of detecting human activity from either passive sensors [11], [12], wearable sensors [13], [14], or cameras [15]. Activity recognition methods generally work on discrete time windows over the time series of sensor values and aim to map each time window onto the correct type of human activity, e.g. eating or sleeping. Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17]. The strength of probabilistic system based approaches compared to methods based on ontology reasoning is their ability to handle noise, uncertainty and incomplete in sensor data [16].\nTapia [12] was the first to explore supervised learning methods to infer human activity from passive sensors, using a naive Bayes classifier. More recently, probabilistic graphical models started to play an important role in the activity recognition field [11], [18]. Van Kasteren et al. [11] explored the use Conditional Random Fields [19] and Hidden Markov Models [20]. Van Kasteren and Kro\u0308se [18] applied Bayesian Networks [21] on the activity recognition task. Kim et al. [22] found Hidden Markov Models to be incapable of capturing longrange or transitive dependencies between observations, which results in difficulties recognizing multiple interacting activities (concurrent or interwoven). Conditional Random Fields do not posses these limitations.\nThe main differences between existing work in activity recognition and the approach presented in this paper are the input data on which they can be applied and the generality of the approach. Activity recognition techniques consider the\nIEEE 2 | P a g e\ninput data to be a multidimensional time series of the sensor values over time based on which time windows are mapped onto human activities. An appropriate time window size is determined based on domain knowledge of the data set. In supervised event abstraction we aim for a generic method that works for all XES event logs in general. A time window based approach contrasts with our aim for generality, as no single time window size will be appropriate for all event logs. Furthermore, the durations of the events within a single event log might differ drastically (e.g. one event might take seconds, while another event takes months), in which case time window based approaches will either miss short events in case of larger time windows or resort to very large numbers of time windows resulting in very long computational time. Therefore, we map each individual low-level event to a high-level event and do not use time windows. In a smart home environment context with passive sensors, each change in a binary sensor value can be considered to be a low-level event."}, {"heading": "III. PRELIMINARIES", "text": "In this section we introduce basic concepts used throughout the paper.\nWe use the usual sequence definition, and denote a sequence by listing its elements, e.g. we write \u3008a1, a2, . . . , an\u3009 for a (finite) sequence s : {1, . . . , n} \u2192 S of elements from some alphabet S, where s(i) = ai for any i \u2208 {1, . . . , n}.\nA. XES Event Logs\nWe use the XES standard definition of event logs, an overview of which is shown in Figure 3. XES defines an event log as a set of traces, which in itself is a sequence of events. The log, traces and events can all contain one or more attributes, which consist of a key and a value of a certain type. Event or trace attributes may be global, which indicates that the attribute needs to be defined for each event or trace respectively. A log contains one or more classifiers, which can be seen as labeling functions on the events of a log, defined on global event attributes. Extensions define a set of attributes on log, trace, or event level, in such a way that the semantics of these attributes are clearly defined. One can view XES extensions as a specification of attributes that events, traces, or event logs themselves frequently contain. XES defines the following standard extensions:\nConcept Specifies the generally understood name of the event/trace/log (attribute \u2019Concept:name\u2019). Lifecycle Specifies the lifecycle phase (attribute \u2019Lifecycle:transition\u2019) that the event represents in a transactional model of their generating activity. The Lifecycle extension also specifies a standard transactional model for activities. Organizational Specifies three attributes for events, which identify the actor having caused the event (attribute \u2019Organizational:resource\u2019), his role in the organization (attribute \u2019Organizational:role\u2019), and the group or department within the organization\nwhere he is located (attribute \u2019Organizational:group\u2019).\nTime Specifies the date and time at which an event occurred (attribute \u2019Time:timestamp\u2019). Semantic Allows definition of an activity metamodel that specifies higher-level aggregate views on events (attribute \u2019Semantic:modelReference\u2019).\nWe introduce a special attribute of type String with key label, which represents a high-level version of the generally understood name of an event. The concept name of a event is then considered to be a low-level name of an event. The Semantic extension closely resembles the label attribute, however, by specifying relations between low-level and high-level events in a meta-model, the Semantic extension assumes that all instances of a low-level event type belong to the same highlevel event type. The label attribute specifies the high-level label for each event individually, allowing for example one low-level event of low-level type Dishes & cups cabinet to be of high-level type Taking medicine, and another low-level event of the same type to be of high-level type Eating. Note that for some traces high-level annotations might be available, in which case its events contain the label attribute, while other traces might not be annotated. High-level interpretations of unannotated traces, by inferring the label attribute based on information that is present in the annotated traces, allow the use of unannotated traces for process discovery and conformance checking on a high level."}, {"heading": "B. Petri nets", "text": "A process modeling notation frequently used as output of process discovery techniques is the Petri net. Petri nets are\nIEEE 3 | P a g e\ndirected bipartite graphs consisting of transitions and places, connected by arcs. Transitions represent activities, while places represent the status of the system before and after execution of a transition. Labels are assigned to transitions to indicate the type of activity that they represent. A special label \u03c4 is used to represent invisible transitions, which are only used for routing purposes and do not represent any real activity.\nDefinition 1 (Labeled Petri net): A labeled Petri net is a tuple N = (P, T, F,R, `) where P is a finite set of places, T is a finite set of transitions such that P \u2229 T = \u2205, and F \u2286 (P \u00d7 T ) \u222a (T \u00d7 P ) is a set of directed arcs, called the flow relation, R is a finite set of labels representing event types, with \u03c4 /\u2208 R is a label representing an invisible action, and ` : T \u2192 R \u222a \u03c4 is a labeling function that assigns a label to each transition.\nThe state of a Petri net is defined w.r.t. the state that a process instance can be in during its execution. A state of a Petri net is captured by the marking of its places with tokens. In a given state, each place is either empty, or it contains a certain number of tokens. A transition is enabled in a given marking if all places with an outgoing arc to this transitions contain at least one token. Once a transition fires (i.e. is executed), a token is removed from all places with outgoing arcs to the firing transition and a token is put to all places with incoming arcs from the firing transition, leading to a new state.\nDefinition 2 (Marking, Enabled transitions, and Firing): A marked Petri net is a pair (N,M), where N = (P, T, F, L, `) is a labeled Petri net and where M \u2208 B(P ) denotes the marking of N . For n \u2208 (P \u222a T ) we use \u2022n and n\u2022 to denote the set of inputs and outputs of n respectively. Let C(s, e) indicate the number of occurrences (count) of element e in multiset s. A transition t \u2208 T is enabled in a marking M of net N if \u2200p \u2208 \u2022t : C(M,p) > 0. An enabled transition t may fire, removing one token from each of the input places \u2022t and producing one token for each of the output places t\u2022.\nFigure 4 shows three Petri nets, with the circles representing places, the squares representing transitions. The black squares represent invisible transitions, or, \u03c4 transitions. Places annotated with an f belong to the final marking, indicating that the process execution can terminate in this marking.\nThe topmost Petri net in Figure 4 initially has one token in the place p1, indicated by the dot. Firing of silent transition t1 takes the token from p1 and puts a token in both p2 and p3, enabling both t2 and t3. When t2 fires, it takes the token from p2 and puts a token in p4. When t3 fires, it takes the token from p3 and puts a token in p5. After t2 and t3 have both fired, resulting in a token in both p4 and p5, t4 is enabled. Executing t4 takes the token from both p4 and p5, and puts a token in p6. The f indicates that the process execution can stop in the marking consisting of this place. Alternatively, it can fire t5, taking the token from p6 and placing a token in p2 and p5, which allows for execution of MC and W to reach the marking consisting of p6 again. We refer the interested reader to [23] for an extensive review of Petri nets."}, {"heading": "C. Conditional Random Field", "text": "We view the recognition of high-level event labels as a sequence labeling task in which each event is classified\nas one of the higher-level events from a high-level event alphabet. Conditional Random Fields (CRFs) [19] are a type of probabilistic graphical model which has become popular in the fields of language processing and computer vision for the task of sequence labeling. A Conditional Random Field models the conditional probability distribution of the label sequence given an observation sequence using a log-linear model. We use Linear-chain Conditional Random Fields, a subclass of Conditional Random Fields that has been widely used for sequence labeling tasks, which takes the following form:\np(y|x) = 1Z(x)exp( \u2211 t=1 \u2211 k \u03bbkfk(t, yt\u22121, yt, x))\nwhere Z(x) is the normalization factor, X = \u3008x1, . . . , xn\u3009 is an observation sequence, Y = \u3008y1, . . . , yn\u3009 is the associated label sequence, fk and \u03bbk respectively are feature functions and their weights. Feature functions, which can be binary or real valued, are defined on the observations and are used to compute label probabilities. In contrast to Hidden Markov Models [20], feature functions are not assumed to be mutually independent."}, {"heading": "IV. MOTIVATING EXAMPLE", "text": "Figure 4 shows on a simple example how a process can be structured at a high level while this structure is not discoverable from a low-level log of this process. The bottom right Petri net shows the example process at a high-level. The high-level process model allows for any finite length alternating sequence of Taking medicine and Eating activities. The Taking medicine high-level activity is defined as a subprocess, corresponding\nIEEE 4 | P a g e\nto the topmost Petri net, which consists of low-level events Medicine cabinet (MC), Dishes & cups cabinet (DCC), and Water (W). The Eating high-level event is also defined as a subprocess, shown in the bottom left Petri net, which consists of low-level events Dishes & cups cabinet (DCC) and Cutlery drawer (CD) that can occur an arbitrary number of times in any order and low-level event Dishwasher (D) which occurs exactly once, but at an arbitrary point in the Eating process.\nWhen we apply the Inductive Miner process discovery algorithm [6] to low-level traces generated by the hierarchical process of Figure 4, we obtain the process model shown in Figure 5. The obtained process model allows for almost all possible sequences over the alphabet {CD,D,DCC,MC,W}, as the only constraint introduced by the model is that DCC and D are required to be executed starting from the initial marking to end up with the same marking. Firing of all other transitions in the model can be skipped. Behaviorally this model is very close to the so called \u201dflower\u201d model [1], the model that allows for all behavior over its alphabet. The alternating structure between Taking medicine and Eating that was present in the high-level process in Figure 4 cannot be observed in the process model in Figure 5. This is caused by high variance in start and end events of the high-level event subprocesses of Taking medicine and Eating as well as by the overlap in event types between these two subprocesses.\nWhen the event log would have consisted of the highlevel Eating and Taking medicine events, process discovery techniques have no problems to discover the alternating structure in the bottom right Petri net of Figure 4. To discover the high-level alternating structure from a low-level event log it is necessary to first abstract the events in the event log. Through supervised learning techniques the mapping from low-level events to high-level events can be learned from examples, without requiring a hand-made ontology. Similar approaches have been explored in activity recognition in the field of ubiquitous computing, where low-level sensor signals are mapped to high-level activities from a human behavior perspective. The input data in this setting are continuous time series from sensors. Change points in these time series are triggered by low-level activities like opening/closing the fridge door, and the annotations of the higher level events (e.g. cooking) are often obtained through manual activity diaries. In contrast to unsupervised event abstraction, the annotations in supervised event abstraction provide guidance on how to label higher level events and guidance for the target level of abstraction."}, {"heading": "V. EVENT ABSTRACTION AS A SEQUENCE LABELING TASK", "text": "In this section we describe an approach to supervised abstraction of events based on Conditional Random Fields. Additionally, we describe feature functions on XES event logs in a general way by using XES extensions. Figure 6 provides a conceptual overview of the supervised event abstraction method. The approach takes two inputs, 1) a set of annotated traces, which are traces where the high-level event that a lowlevel event belongs to (the label attribute of the low-level event) is known for all low-level events in the trace, and 2) a set of unannotated traces, which are traces where the lowlevel events are not mapped to high-level events. Conditional\nRandom Fields are trained of the annotated traces to create a probabilistic mapping from low-level events to high-level events. This mapping, once obtained, can be applied to the unannotated traces in order to estimate the corresponding high-level event for each low-level event (its label attribute). Often sequences of low-level events in the traces with highlevel annotations will have the same label attribute. We make the working assumption that multiple high-level events are executed in parallel. This enables us to interpret a sequence of identical label attribute values as a single instance of a high-level event. To obtain a true high-level log, we collapse sequences of events with the same value for the label attribute into two events with this value as concept name, where the first event has a lifecycle \u2019start\u2019 and the second has the lifecycle \u2019complete\u2019. Table I illustrates this collapsing procedure through an input and output event log.\nThe method described in this section is implemented and available for use as a plugin for the ProM 6 [25] process mining toolkit and is based on the GRMM [26] implementation of Conditional Random Fields.\nWe now show for each XES extension how it can be translated into useful feature functions for event abstraction. Note that we do not limit ourselves to XES logs that contain all XES extensions; when a log contains a subset of the extensions, a subset of the feature functions will be available for the supervised learning step. This approach leads to a feature space of unknown size, potentially causing problems related to the curse of dimensionality, therefore we use L1-regularized Conditional Random Fields. L1 regularization causes the vector of feature weights to be sparse, meaning that only a small fraction of the features have a non-zero weight and are actually used by the prediction model. Since the L1-norm is non-differentiable, we use OWL-QN [27] to optimize the model."}, {"heading": "A. From a XES Log to a Feature Space", "text": "1) Concept extension: The low-level labels of the preceding events in a trace can contain useful contextual information for high-level label classification. Based on the n-gram of n last-seen events in a trace, we can calculate the probability that the current event has a label l. A multinoulli distribution is estimated for each n-gram of n consecutive events, based\nIEEE 5 | P a g e\non the training data. The Conditional Random Field model requires feature functions with numerical range. A concept extension based feature function with two parameters, n and l, is valued with the multinoulli-estimated probability of the current event having high-level label l given the n-gram of the last n low-level event labels.\n2) Organizational extension: Similar to the concept extension feature functions, multinoulli distributions can be estimated on the training set for n-grams of resource, role, or group attributes of the last n events. Likewise, an organizational extension based feature function with three parameters, n-gram size n, o \u2208 {resource, role, group}, and label l, is valued with the multinoulli-estimated probability of label l given the n-gram of the last n event resources/roles/groups.\n3) Time extension: In terms of time, there are several potentially existing patterns. A certain high-level event might for example be concentrated in a certain parts of a day, of a week, or of a month. This concentration can however not be modeled with a single Gaussian distribution, as it might be the case that a high-level event has high probability to occur in the morning or in the evening, but low probability to occur in the afternoon in-between. Therefore we use a Gaussian Mixture Model (GMM) to model the probability of a high-level label l given the timestamp. Bayesian Information Criterion (BIC) [28] is used to determine the number of components of the GMM, which gives the model an incentive to not combine more Gaussians in the mixture than needed. A GMM is estimated on training data, modeling the probabilities of each label based on the time passed since the start of the day, week or month. A time extension based feature function with two parameters, t \u2208 {day,week,month, . . . } and label l, is valued with the GMM-estimated probability of label l given the t view on the event timestamp.\n4) Lifecycle extension & Time extension: The XES standard [10] defines several lifecycle stages of a process. When an event log possesses both the lifecycle extension and the time extension, time differences can be calculated between different stages of the life cycle of a single activity. For a complete event for example, one could calculate the time difference with the associated start event of the same activity. Finding the associated start event becomes nontrivial when multiple instances of the same activity are in parallel, as it is then unknown which complete event belongs to which start event. We assume consecutive lifecycle steps of activities running in parallel to occur in the same order as the preceding lifecycle step. For example, when we observe two start events of an activity of type A in a row, followed by two complete events of type A, we assume the first complete to belong to the first start, and the second complete to belong to the second start.\nWe estimate a Gaussian Mixture Model (GMM) for each tuple of two lifecycle steps for a certain activity on the time differences between those two lifecycle steps for this activity. A feature based on both the lifecycle and the time extension, with a label parameter l and lifecycle c, is valued with the GMM-estimated probability of label l given the time between the current event and lifecycle c. Bayesian Information Criterion (BIC) [28] is again used to determine the number of components of the GMM."}, {"heading": "B. Evaluating High-level Event Predictions for Process Mining Applications", "text": "Existing approaches in the field of activity recognition take as input time windows where each time window is represented by a feature vector that describes the sensor activity or status during that time window. Hence, evaluation methods in the activity recognition field are window-based, using evaluation metrics like the percentage of correctly classified time slices\nIEEE 6 | P a g e\n[12], [18], [11]. There are two reasons to deviate from this evaluation methodology in a process mining setting. First, our method operates on events instead of time windows. Second, the accuracy of the resulting high level sequences is much more important for many process mining techniques (e.g. process discovery, conformance checking) than the accuracy of predicting each individual minute of the day.\nWe use Levenshtein similarity that expresses the degree in which two traces are similar using a metric based on the Levenshtein distance (also known as edit distance) [29], which is defined as Levenshtein similarity(a, b) = 1 \u2212 Levenshtein distance(a,b) max(|a|,|b|) . The division of the Levenshtein distance by max(|a|, |b|), which is the worst case number of edit operations needed to transform any sequence of length |a| into any sequence of length |b|, causes the result to be a number between 0 (completely different traces) and 1 (identical traces)."}, {"heading": "VI. CASE STUDY 1: SMART HOME ENVIRONMENT", "text": "We use the smart home environment log described by Van Kasteren et al. [11] to evaluate our supervised event log abstraction method. The Van Kasteren log consists of multidimensional time series data with all dimensions binary, where each binary dimension represents the state of an inhome sensor. These sensors include motion sensors, open/close sensors, and power sensors (discretized to 0/1 states)."}, {"heading": "A. Experimental setup", "text": "We transform the multidimensional time series data from sensors into events by regarding each sensor change point as an event. Cases are created by grouping events together that occurred in the same day, with a cut-off point at midnight. High-level labels are provided for the Van Kasteren data set.\nThe generated event log based on the Van Kasteren data set has the following XES extensions:\nConcept The sensor that generated the event. Time The time stamp of the sensor change point. Lifecycle Start when the event represents a sensor value\nchange from 0 to 1 and Complete when it represents a sensor value change from 1 to 0.\nNote that annotations are provided for all traces in the obtained event log. To evaluate how well the supervised event abstraction method generalized to unannotated traces, we artificially use a part of the traces to train the abstraction model and apply them on a test set where we regard the annotations to be non-existent. We evaluate the obtained high-level labels against the ground truth labels. We use a variation on LeaveOne-Out-Cross-Validation where we leave out one trace to evaluate how well this mapping generalizes to unseen events and cases."}, {"heading": "B. Results", "text": "Figure 7a shows the result of the Inductive Miner [6] for the low-level events in the Van Kasteren data set. The resulting process model starts with many parallel activities that can be executed in any order and contains many unobservable transitions back. This closely resembles the flower model,\nwhich allows for any behavior in any arbitrary order. From the process model we can learn that toilet flush and cups cupboard frequently co-exists. Furthermore, the process model indicates that groceries cupboard is often followed by dishwasher. There seems to be very little structure on this level of event granularity.\nThe average Levenshtein similarity between the predicted high-level traces in the Leave-One-Trace-Out-Cross-Validation experimental setup and the ground truth high-level traces is 0.7042, which shows that the supervised event abstraction method produces traces which are fairly similar to the ground truth.\nFigure 7b shows the result of the Inductive Miner on the aggregated set of predicted test traces. Figure 7b shows that the process discovered at the high level of granularity is more structured than the process discovered at the original level of granularity (Figure 7a). In Figure 7b, we can see that the main daily routine starts with breakfast, followed by a shower, after which the subject leaves the house to go to work. After work the subject prepares dinner and has a drink. The subject mainstream behavior is to go to the toilet before going to bed, but he can then wake up later to go to the toilet and then continue sleeping. Note that the day can also start with going to bed. This is related to the case cut-off point of a trace at midnight. Days when the subject went to bed after midnight result in a case where going to bed occurs at the start of the trace. On these days, the subject might have breakfast and then perform the activity sequence use toilet, take shower, and leave house, possibly multiple times. Another possibility on days when the subject went to bed after midnight is that he starts by using the toilet, then has breakfast, then has the possibility to leave the house, then takes a shower, after which he always leaves the house. Prepare dinner activity is not performed on these days.\nThis case study shows that we can find a structured highlevel process from a low-level event log where the low-level process is unstructured, using supervised event abstraction and process discovery."}, {"heading": "VII. CASE STUDY 2: ARTIFICIAL DIGITAL PHOTOCOPIER", "text": "Bose et al. [30], [31] created a synthetic event log based on a digital photocopier to evaluate his unsupervised methods of event abstraction. In this case study we show that the described supervised event abstraction method can accurately abstract to high-level labels."}, {"heading": "A. Experimental setup", "text": "We annotated each low-level event with the correct highlevel event using domain knowledge from the actual process model as described by Bose et al. [30], [31]. This event log is generated by a hierarchical process, where high-level events Capture Image, Rasterize Image, Image Processing and Print Image are defined in terms of a process model. The Print Image subprocess amongst others contains the events Writing, Developing and Fusing, which are themselves defined as a subprocess. In this case study we set the task to transform the log such that subprocesses Capture Image, Rasterize Image and Image Processing, Writing, Fusing and Developing.\nIEEE 7 | P a g e\nSubprocesses Writing and Developing both contain the lowlevel event types Drum Spin Start and Drum Spin Stop. In this case study we focus in particular on the Drum Spin Start and Drum Spin Stop events, as they make the abstraction task nontrivial in the sense that no one-to-one mapping from low-level to high-level events exists.\nThe artificial digital photocopier data set has the concept, time and lifecycle XES extensions. On this event log annotations are available for all traces. On this data set we use a 10-Fold Cross-Validation setting on the traces to evaluate how well the supervised event abstraction method abstracts low-\nlevel events to high-level events on unannotated traces, as this data set is larger than the Van Kasteren data set and LeaveOne-Out-Cross Validation would take too much time."}, {"heading": "B. Results", "text": "The confusion matrix in Table II shows the aggregated results of the mapping of low-level events Drum Spin Start and Drum Spin Stop to high-level events Developing and Writing. The results show that the supervised event abstraction method is capable of detecting the many-to-many mappings between the low-level and high-level labels, as it maps these low-level\nIEEE 8 | P a g e\nevents to the correct high-level event without making errors. The Levenshtein similarity between the aggregated set of test fold high-level traces and the ground truth high-level traces is close to perfect: 0.9667.\nFigure 8a shows the process model obtained with the Inductive Miner on the low-level events in the artificial digital photocopier dataset. The two sections in the process model that are surrounded by dashed lines are examples of high-level events within the low-level process model. Even though the low-level process contains structure, the size of the process model makes it hard to comprehend. Figure 8b shows the process model obtained with the same process discovery algorithm on the aggregated high-level test traces of the 10- fold cross validation setting. This model is in line with the official artificial digital photocopier model specification, with the Print Image subprocess unfolded, as provided in [30], [31]. In contrast to the event abstraction method described by Bose et al. [31] which found the high-level events that match specification, supervised event abstraction is also able to find suitable event labels for the generated high-level events. This allows us to discover human-readable process models on the abstracted events without performing manual labeling, which\ncan be a tedious task and requires domain knowledge.\nInstead of event abstraction on the level of the event log, unsupervised abstraction methods that work on the level of a model (e.g. [32]) can also be applied to make large complex models more comprehensible. Note that such methods also do not give guidance on how to label resulting transitions in the process model. Furthermore, such methods do not help in cases where the process on a low-level is unstructured, like in the case study as described in Section VI.\nThis case study shows that supervised event abstraction can help generating a comprehensible high-level process model from a low-level event log, when a low-level process model would be too large to be understandable."}, {"heading": "VIII. CONCLUSION", "text": "In this paper we described a method to abstract events in a XES event log that is too low-level, based on supervised learning. The method consists of an approach to generate a feature representation of a XES event, and of a Conditional Random Field based learning step. An implementation of the method described has been made available as a plugin to the ProM 6 process mining toolkit. We introduced an evaluation metric for predicted high-level traces that is closer to process mining than time-window based methods that are often used in the sequence labeling field. Using a real life event log from a smart home domain, we showed that supervised event abstraction can be used to enable process discovery techniques to generate high-level process insights even when process models discovered by process mining techniques on the original low-level events are unstructured. Finally, we showed on\nIEEE 9 | P a g e\na synthetic event log that supervised event abstraction can be used to discover smaller, more comprehensible, high-level process models when the process model discovered on low level events is too large to be interpretable."}], "references": [{"title": "Process mining: Discovery, conformance and enhancement of business processes", "author": ["W.M.P. van der Aalst"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Workflow mining: Discovering process models from event logs", "author": ["W.M.P. van der Aalst", "A.J.M.M. Weijters", "L. Maruster"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 16, no. 9, pp. 1128\u20131142, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Fuzzy mining\u2013adaptive process simplification based on multi-perspective metrics", "author": ["C.W. G\u00fcnther", "W.M.P. van der Aalst"], "venue": "Business Process Management. Springer, 2007, pp. 328\u2013343.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Process discovery using integer linear programming", "author": ["J.M.E.M. Van der Werf", "B.F. van Dongen", "C.A.J. Hurkens", "A. Serebrenik"], "venue": "Applications and Theory of Petri Nets. Springer, 2008, pp. 368\u2013387.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Flexible heuristics miner (fhm)", "author": ["A.J.M.M. Weijters", "J.T.S. Ribeiro"], "venue": "Proceedings of the 2011 IEEE Symposium on Computational Intelligence and Data Mining. IEEE, 2011, pp. 310\u2013317.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering block-structured process models from event logs - a constructive approach", "author": ["S.J.J. Leemans", "D. Fahland", "W.M.P. van der Aalst"], "venue": "Application and Theory of Petri Nets and Concurrency, ser. LNCS. Springer, 2013, pp. 311\u2013329.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Abstractions in process mining: A taxonomy of patterns", "author": ["R.P.J.C. Bose", "W.M.P. van der Aalst"], "venue": "Business Process Management, ser. LNCS. Springer, 2009, pp. 159\u2013175.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Activity mining by global trace segmentation", "author": ["C.W. G\u00fcnther", "A. Rozinat", "W.M.P. van der Aalst"], "venue": "Business Process Management Workshops, ser. LNBIP. Springer, 2010, pp. 128\u2013139.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Process mining: Fuzzy clustering and performance visualization", "author": ["B.F. van Dongen", "A. Adriansyah"], "venue": "Business Process Management Workshops, ser. LNBIP. Springer, 2010, pp. 158\u2013169.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "XES-standard definition", "author": ["C.W. G\u00fcnther", "H.M.W. Verbeek"], "venue": "BPMcenter.org, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Accurate activity recognition in a home setting", "author": ["T. van Kasteren", "A. Noulas", "G. Englebienne", "B. Kr\u00f6se"], "venue": "Proceedings of the 10th International Conference on Ubiquitous Computing. ACM, 2008, pp. 1\u20139.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Activity recognition in the home using simple and ubiquitous sensors", "author": ["E.M. Tapia", "S.S. Intille", "K. Larson"], "venue": "Pervasive Computing, ser. LNCS, A. Ferscha and F. Mattern, Eds. Springer, 2004, pp. 158\u2013 175.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Activity recognition from user-annotated acceleration data", "author": ["L. Bao", "S.S. Intille"], "venue": "Pervasive Computing, ser. LNCS, A. Ferscha and F. Mattern, Eds. Springer, 2004, pp. 1\u201317.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Activity recognition using cell phone accelerometers", "author": ["J.R. Kwapisz", "G.M. Weiss", "S.A. Moore"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, no. 2, pp. 74\u201382, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on vision-based human action recognition", "author": ["R. Poppe"], "venue": "Image and Vision Computing, vol. 28, no. 6, pp. 976\u2013990, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Ontology-based activity recognition in intelligent pervasive environments", "author": ["L. Chen", "C. Nugent"], "venue": "International Journal of Web Information Systems, vol. 5, no. 4, pp. 410\u2013430, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "OWL 2 modeling and reasoning with complex human activities", "author": ["D. Riboni", "C. Bettini"], "venue": "Pervasive and Mobile Computing, vol. 7, no. 3, pp. 379\u2013395, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian activity recognition in residence for elders", "author": ["T. van Kasteren", "B. Kr\u00f6se"], "venue": "Proceedings of the 3rd IET International Conference on Intelligent Environments. IEEE, 2007, pp. 209\u2013212.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the 18th International Conference on Machine Learning. Morgan Kaufmann, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "An introduction to hidden Markov models", "author": ["L.R. Rabiner", "B.-H. Juang"], "venue": "ASSP Magazine, vol. 3, no. 1, pp. 4\u201316, 1986.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1986}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning, vol. 29, no. 2-3, pp. 131\u2013163, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Human activity recognition and pattern discovery", "author": ["E. Kim", "S. Helal", "D. Cook"], "venue": "Pervasive Computing, vol. 9, no. 1, pp. 48\u201353, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Petri nets: an introduction", "author": ["W. Reisig"], "venue": "Springer Science & Business Media,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Petri nets: Properties, analysis and applications", "author": ["T. Murata"], "venue": "Proceedings of the IEEE, vol. 77, no. 4, pp. 541\u2013580, 1989.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1989}, {"title": "ProM 6: The process mining toolkit", "author": ["H.M.W. Verbeek", "J.C.A.M. Buijs", "B.F. Van Dongen", "W.M.P. van der Aalst"], "venue": "Proceedings of the Business Process Management Demonstration Track, ser. CEUR- WS.org, 2010, pp. 34\u201339.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "GRMM: Graphical models in mallet", "author": ["C. Sutton"], "venue": "Implementation available at http://mallet.cs.umass.edu/grmm, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Scalable training of L1-regularized loglinear models", "author": ["G. Andrew", "J. Gao"], "venue": "Proceedings of the 24th International Conference on Machine Learning. ACM, 2007, pp. 33\u201340.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, vol. 6, no. 2, pp. 461\u2013464, 1978.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1978}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet Physics Doklady, vol. 10, pp. 707\u2013710, 1966.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1966}, {"title": "Discovering hierarchical process models using ProM", "author": ["R.P.J.C. Bose", "H.M.W. Verbeek", "W.M.P. van der Aalst"], "venue": "IS Olympics: Information Systems in a Diverse World, ser. LNBIP. Springer, 2012, pp. 33\u201348.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Process mining in the large: Preprocessing, discovery, and diagnostics", "author": ["R.P.J.C. Bose"], "venue": "Ph.D. dissertation, Technische Universiteit Eindhoven, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "The refined process structure tree", "author": ["J. Vanhatalo", "H. V\u00f6lzer", "J. Koehler"], "venue": "Data & Knowledge Engineering, vol. 68, no. 9, pp. 793\u2013818, 2009. IEEE  10 | P a g e", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Process mining is a fast growing discipline that combines knowledge and techniques from computational intelligence, data mining, process modeling and process analysis [1].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "Many different process discovery algorithms exist ([2], [3], [4], [5], [6]), and many different types of process models can be discovered by process discovery methods, including BPMN models, Petri nets, process trees, and statecharts.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "Many different process discovery algorithms exist ([2], [3], [4], [5], [6]), and many different types of process models can be discovered by process discovery methods, including BPMN models, Petri nets, process trees, and statecharts.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Many different process discovery algorithms exist ([2], [3], [4], [5], [6]), and many different types of process models can be discovered by process discovery methods, including BPMN models, Petri nets, process trees, and statecharts.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Many different process discovery algorithms exist ([2], [3], [4], [5], [6]), and many different types of process models can be discovered by process discovery methods, including BPMN models, Petri nets, process trees, and statecharts.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Many different process discovery algorithms exist ([2], [3], [4], [5], [6]), and many different types of process models can be discovered by process discovery methods, including BPMN models, Petri nets, process trees, and statecharts.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Several methods have been explored within the process mining field that address the challenge of abstracting low-level events to higher level events ([7], [8], [9]).", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Several methods have been explored within the process mining field that address the challenge of abstracting low-level events to higher level events ([7], [8], [9]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 8, "context": "Several methods have been explored within the process mining field that address the challenge of abstracting low-level events to higher level events ([7], [8], [9]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 9, "context": "In Section V we describe a method to automatically retrieve a feature vector representation of an event that can be used with supervised learning techniques, making use of certain aspects of the XES standard definition for event logs [10].", "startOffset": 234, "endOffset": 238}, {"referenceID": 10, "context": "Activity recognition focuses on the task of detecting human activity from either passive sensors [11], [12], wearable sensors [13], [14], or cameras [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "Activity recognition focuses on the task of detecting human activity from either passive sensors [11], [12], wearable sensors [13], [14], or cameras [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "Activity recognition focuses on the task of detecting human activity from either passive sensors [11], [12], wearable sensors [13], [14], or cameras [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "Activity recognition focuses on the task of detecting human activity from either passive sensors [11], [12], wearable sensors [13], [14], or cameras [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Activity recognition focuses on the task of detecting human activity from either passive sensors [11], [12], wearable sensors [13], [14], or cameras [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Activity recognition methods can be classified into probabilistic approaches [11], [12], [13], [14] and approaches based on ontology reasoning [16], [17].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "The strength of probabilistic system based approaches compared to methods based on ontology reasoning is their ability to handle noise, uncertainty and incomplete in sensor data [16].", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": "Tapia [12] was the first to explore supervised learning methods to infer human activity from passive sensors, using a naive Bayes classifier.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "More recently, probabilistic graphical models started to play an important role in the activity recognition field [11], [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "More recently, probabilistic graphical models started to play an important role in the activity recognition field [11], [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "[11] explored the use Conditional Random Fields [19] and Hidden Markov Models [20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[11] explored the use Conditional Random Fields [19] and Hidden Markov Models [20].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "[11] explored the use Conditional Random Fields [19] and Hidden Markov Models [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "Van Kasteren and Kr\u00f6se [18] applied Bayesian Networks [21] on the activity recognition task.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Van Kasteren and Kr\u00f6se [18] applied Bayesian Networks [21] on the activity recognition task.", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "[22] found Hidden Markov Models to be incapable of capturing longrange or transitive dependencies between observations, which results in difficulties recognizing multiple interacting activities (concurrent or interwoven).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "3: XES event log meta-model, as defined in [10].", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "We refer the interested reader to [23] for an extensive review of Petri nets.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Conditional Random Fields (CRFs) [19] are a type of probabilistic graphical model which has become popular in the fields of language processing and computer vision for the task of sequence labeling.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "In contrast to Hidden Markov Models [20], feature functions are not assumed to be mutually independent.", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "When we apply the Inductive Miner process discovery algorithm [6] to low-level traces generated by the hierarchical process of Figure 4, we obtain the process model shown in Figure 5.", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "Behaviorally this model is very close to the so called \u201dflower\u201d model [1], the model that allows for all behavior over its alphabet.", "startOffset": 70, "endOffset": 73}, {"referenceID": 24, "context": "The method described in this section is implemented and available for use as a plugin for the ProM 6 [25] process mining toolkit and is based on the GRMM [26] implementation of Conditional Random Fields.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "The method described in this section is implemented and available for use as a plugin for the ProM 6 [25] process mining toolkit and is based on the GRMM [26] implementation of Conditional Random Fields.", "startOffset": 154, "endOffset": 158}, {"referenceID": 26, "context": "Since the L1-norm is non-differentiable, we use OWL-QN [27] to optimize the model.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "5: Result of the Inductive Miner on the low-level traces, reduced using Murata reduction rules [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "Bayesian Information Criterion (BIC) [28] is used to determine the number of components of the GMM, which gives the model an incentive to not combine more Gaussians in the mixture than needed.", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "4) Lifecycle extension & Time extension: The XES standard [10] defines several lifecycle stages of a process.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Bayesian Information Criterion (BIC) [28] is again used to determine the number of components of the GMM.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "[12], [18], [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[12], [18], [11].", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "[12], [18], [11].", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "We use Levenshtein similarity that expresses the degree in which two traces are similar using a metric based on the Levenshtein distance (also known as edit distance) [29], which is defined as Levenshtein similarity(a, b) = 1 \u2212", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "[11] to evaluate our supervised event log abstraction method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Figure 7a shows the result of the Inductive Miner [6] for the low-level events in the Van Kasteren data set.", "startOffset": 50, "endOffset": 53}, {"referenceID": 29, "context": "[30], [31] created a synthetic event log based on a digital photocopier to evaluate his unsupervised methods of event abstraction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[30], [31] created a synthetic event log based on a digital photocopier to evaluate his unsupervised methods of event abstraction.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "[30], [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[30], [31].", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "This model is in line with the official artificial digital photocopier model specification, with the Print Image subprocess unfolded, as provided in [30], [31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "This model is in line with the official artificial digital photocopier model specification, with the Print Image subprocess unfolded, as provided in [30], [31].", "startOffset": 155, "endOffset": 159}, {"referenceID": 30, "context": "[31] which found the high-level events that match specification, supervised event abstraction is also able to find suitable event labels for the generated high-level events.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32]) can also be applied to make large complex models more comprehensible.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Process mining techniques focus on extracting insight in processes from event logs. In many cases, events recorded in the event log are too fine-grained, causing process discovery algorithms to discover incomprehensible process models or process models that are not representative of the event log. We show that when process discovery algorithms are only able to discover an unrepresentative process model from a low-level event log, structure in the process can in some cases still be discovered by first abstracting the event log to a higher level of granularity. This gives rise to the challenge to bridge the gap between an original low-level event log and a desired high-level perspective on this log, such that a more structured or more comprehensible process model can be discovered. We show that supervised learning can be leveraged for the event abstraction task when annotations with high-level interpretations of the low-level events are available for a subset of the sequences (i.e., traces). We present a method to generate feature vector representations of events based on XES extensions, and describe an approach to abstract events in an event log with Condition Random Fields using these event features. Furthermore, we propose a sequence-focused metric to evaluate supervised event abstraction results that fits closely to the tasks of process discovery and conformance checking. We conclude this paper by demonstrating the usefulness of supervised event abstraction for obtaining more structured and/or more comprehensible process models using both real life event data and synthetic event data. Keywords\u2014Process Mining, Event Abstraction, Probabilistic Graphical Models", "creator": "LaTeX with hyperref package"}}}