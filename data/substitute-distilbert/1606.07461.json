{"id": "1606.07461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks", "abstract": "recurrent neural networks, and in particular long short - term memory networks ( sonar ), are a remarkably effective tool for cognitive modeling that learn a complicated black - box hidden representation of their sequential input. researchers interested pursuing better integrating these models have studied the changes in hidden state representations over time and noticed some interpretable information but also significant noise. in this work, we present lstmvis a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. the tool allows this sociologist to select a hypothesis input range to focus on local state patterns, to match these states changes to similar patterns in a large data set, and to mimic these results with overly specific structural annotations. we hence show several use characteristics of analytic tool for analyzing specific hidden state properties on datasets containing nesting, phrase structure, and chord descriptions, may demonstrate. modern tool can be used on isolate patterns for further statistical analysis.", "histories": [["v1", "Thu, 23 Jun 2016 20:20:39 GMT  (4883kb,D)", "http://arxiv.org/abs/1606.07461v1", null], ["v2", "Mon, 30 Oct 2017 15:11:54 GMT  (6434kb,D)", "http://arxiv.org/abs/1606.07461v2", "InfoVis 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["hendrik strobelt", "sebastian gehrmann", "bernd huber", "hanspeter pfister", "alexander m rush"], "accepted": false, "id": "1606.07461"}, "pdf": {"name": "1606.07461.pdf", "metadata": {"source": "META", "title": "Visual Analysis of Hidden State Dynamics in  Recurrent Neural Networks ", "authors": ["Hendrik Strobelt", "Sebastian Gehrmann", "Bernd Huber", "Hanspeter Pfister"], "emails": ["rush}@seas.harvard.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recurrent neural networks (RNNs) [6] have proven to be a very effective general-purpose model for capturing long-term dependencies in textual applications. Recent strong empirical results indicate that internal representations learned by RNNs capture complex relationships between the words within a sentence or document. These improved representation have led directly to end applications in machine translation [12, 22], speech recognition [2], music generation [4], and text classification [5], among a variety of other applications.\nWhile RNNs have shown clear improvements for sequence modeling, the models themselves are black boxes, and it remains unclear\nexactly how a particular model is representing long-distance relationships within a sequence. Typically, RNNs contain millions of parameters and utilize repeated non-linear transformations of large hidden representations under time-varying conditions. These factors make the model inter-dependencies challenging to interpret without sophisticated mathematical tools. How do we enable users to explore complex network interactions in an RNN and directly connect these abstract representations to human understandable inputs?\nIn this paper, we focus on visual analysis to allow experimenters to explore and form hypotheses about RNN hidden state dynamics in their\nar X\niv :1\nmodels.\n\u2022 We develop a visual encoding for exploring hidden state dynamics around a selected input phrase and finding similar hidden state patterns in a large dataset.\n\u2022 We present use cases applying this technique to identify and explore patterns in RNNs trained on large datasets for text and other domains.\n\u2022 We introduce the LSTMVIS tool to allows users to analyze a set of pre-trained models. A live system can be accessed via lstm.seas.harvard.edu and the source code is provided.\nWe start in Section 2 by formally introducing the recurrent neural network model and in Section 3 by describing related techniques for visualizing RNNs in practice. In Section 4 we describe domain goals and their mapping to visualization tasks, and then in Section 5 present the visual design choices made to satisfy these goals. In Section 6 we turn toward practical use cases and demonstrate the application of the tool to three different problems. We conclude by discussing implementation details and future challenges."}, {"heading": "2 BACKGROUND: RECURRENT NEURAL NETWORKS", "text": "In recent years, deep neural networks have become a central modeling tool for many artificial cognition tasks, such as image recognition, speech recognition, and text classification. While the architectures for these tasks differ, the models each learn a series of non-linear transformations to map an input into a hidden black-box feature representation. This hidden representation is learned to perform an end task.\nFor text processing and other sequence modeling tasks, recurrent neural networks (RNNs) are a central architecture. A major challenge of working with variable-length text sequences is producing compact representations which capture or summarize long-distance relations in the text. These relationships are particularly important for tasks that require processing and generating sequences such as machine translation. RNN-based models seem to effectively learn representations for this information.\nThroughout this work, we will assume that we are given a sequence of words w1, . . . ,wT for time 1 to T . These might consist of English words that we want to translate or a sentence whose sentiment we would like to detect, or even some other symbolic input such as musical notes or code. Additionally we will assume that we have a mapping from each word into vector representation x1, . . . ,xT . This representation can either be a standard fixed mapping, such as word2vec [20], or can be learned with the rest of the model.\nFormally, RNNs are a class of neural networks that sequentially map input word vectors x1 . . .xT to a sequence of fixed-length representations h1, . . . ,hT . This is achieved by learning a function RNN, which is applied recursively at each time-step t \u2208 1 . . .T :\nht\u2190 RNN(xt,ht\u22121)\nwhich takes input vector xt and a hidden state vector ht\u22121 and gives a new hidden state vector ht . Each hidden state vector ht is in RD. These vectors, and particularly how they change over time, will be the main focus of this work. We are interested in each c \u2208 {1 . . .D} and the change of a single hidden state ht,c as t varies.\nThe model learns these hidden states to represent the features of the input words. As such they can be learned for any modeling tasks utilizing discrete sequential input. In this paper we will focus primarily on the task of RNN language modeling [19, 27], a core task in natural language processing. In language modeling, at time t the prefix of words w1, . . . ,wt is taken as input and the goal is to model the distribution over the next word p(wt+1|w1, . . . ,wt). An RNN is used to produce this distribution by applying a linear model over the hidden state vector ht . Formally we define this as p(wt+1|w1, . . . ,wt) = softmax(Wht+b) where W,b are parameters. The full computation of an RNN language model is shown in Figure 2.\nIt has been widely observed that the hidden states are able to capture important information about the structure of the input sentence necessary to perform this prediction. However, it has been difficult to trace how this is captured and what exactly is learned. For instance, it has been shown in some cases that RNNs can count parentheses or match quotes, but is unclear whether RNNs naturally discover aspects of language such as phrases, grammar, or topics. In this work, we focus particularly on exploring this question by examining the dynamics of the hidden states through time.\nFinally, we note that our experiments will mainly focus on long short-term memory networks (LSTM) (hence the name LSTMVIS) [9]. LSTMs define a variant of the function RNN that has a modified hidden state update which can more effectively learn long-term interactions1. As such these models are widely used in practice. In addition, LSTMs and RNNs can be stacked in layers to produce multiple hidden state vectors at each time step, which further improves performance. While our results mainly use stacked LSTMs, our visualization only requires access to some time evolving abstract vector representation, and therefore can be used for any layer of the model."}, {"heading": "3 RELATED WORK", "text": "Understanding RNNs through Visualization Our core contribution, visualizing the state dynamics of LSTM in a structured way, is inspired by previous work on convolutional networks for in vision applications [21, 28]. In linguistic tasks, visualizations have shown to be useful tool for understanding certain aspects of LSTMs. In [14], static visualization techniques are used to help understand LSTM hidden states in language models. This work demonstrates that selected cells can model clear events such as open parentheses and the start of URLs. In [17], additional techniques are presented, particularly the use of gradient-based saliency to find important words. This work also looks at several different models and datasets including text classification and auto-encoders. In [10, 11], the authors show that RNNs specifically learn lexical categories and grammatical functions that carry semantic information, partially by modifying the inputs fed to the model. While inspired by these techniques, our approach tries to extend beyond single examples and provide a general interactive visualization approach of the raw data for exploratory analysis.\nExtending RNN Models for Interpretability Recent work has also developed methods for extending RNNs for certain problems to make them easier to interpret (along with improving the models). One\n1Note that LSTMs maintain both a cell state vector and a hidden state vector at each time step. Our system can be used to analyze either or both of these vectors (or even the LSTM gates), and in our experiments we found that the cell states are easier to work with. For simplicity, however, we refer to these vectors generically as \u201chidden states\u201d throughout the paper.\npopular technique has been to use a neural attention-mechanism to allow the model to focus in on a particular aspect of the input. In [3] attention is used for soft alignment in machine translation, in [26] attention is used to identify important aspects of an image for captioning, and in [8] attention is used to find important aspects of a document for an extraction task. These approaches have the side benefit that they \u201cshow\u201d what aspect of the model they are using. This approach differs from our work in that it requires changing the underlying model structure, whereas we attempt to interpret the hidden states of a fixed model directly.\nInteractive Visualization of Neural Networks There has been some work on interactive visualization for interpreting machine learning models. In [24], the authors present a visualization system for feedforward neural networks with the goal of interpretation, and in [13], the authors give a user-interface for tuning the learning itself. The recent Prospector system [15] provides a general-purpose tool for practitioners to better understand their ML model and its predictions. There has also been work on user interfaces for constructing models such as TensorBoard [1] and the related playground for convolutional neural models playground.tensorflow.org/. Our work is most similar in spirit to [24] in that we are mostly concerned with interpreting the hidden states of a particular model, however our specific goals and visual design are significantly different."}, {"heading": "4 GOALS AND TASKS", "text": "Given that RNNs act as a black-box, their success leaves open the question of why they are so effective at representing the history of words. LSTMVIS focuses particularly on the dynamics of RNN hidden states and targets the related question:\u201cWhat information does an RNN capture in its hidden states?\u201d. Addressing this question is the main goal of our project and the focus of a series of discussions. During this iterative process, we identified the following domain goals for a user of LSTMVIS:\n\u2022 G1 - Formulate a hypothesis about (linguistic) properties that the hidden states might learn to capture for a specific model. This hypothesis requires an initial understanding of hidden state values over time and a close read of the original text.\n\u2022 G2 - Refine the hypothesis based on insights about learned textual similarities based on patterns in the dynamics of the hidden states. Refining a hypothesis may also mean rejecting it.\n\u2022 G3 - Compare models and datasets to allow early generalization about the insights the representations provide, and to observe how task and domain may alter the patterns in the hidden states.\nFrom these three goals we propose tasks for visual data analysis. The mapping of these tasks to domain goals is indicated by square brackets:\n\u2022 T1 - Visualize hidden states over time to allow exploration of the hidden state dynamics in their raw form. [G1]\n\u2022 T2 - Filter hidden states by using discrete textual selection along with continuous thresholding. These selections methods allow the user to form hypotheses and to separate visual signal from noise. [G1,G2]\n\u2022 T3 - Match selections to similar examples based on hidden state activation pattern. A matched phrase should have intuitively similar characteristics as the selection to support or reject a hypothesis. [G2]\n\u2022 T4 - Align textual annotations visually to matched phrases. These annotations allow the user to compare the learned representation with alternative structural hypotheses such as partof-speech tags or known grammars. The set of annotation data should be easily extensible. [G2,G3]\n\u2022 TX - Provide a general interface that can be used with any RNN model and text-like dataset. It should make it easy to generate crowd knowledge and trigger discussions on similarities and differences between a wide variety of models. [G3]"}, {"heading": "5 VISUAL DESIGN", "text": "LSTMVIS supports the formulation of a hypothesis (T1, T2, G1) in the Select View (Sect. 5.1) and can trigger refinement of a hypothesis (T3, T4, G2) in the Match View (Section 5.2), while remaining agnostic to the underlying data or model (TX). We first describe the visual design and interaction paradigms used in the two views and how they facilitate the domain goals, and then discuss design iterations for LSTMVIS in Section 5.4."}, {"heading": "5.1 Select View", "text": "The Select View, shown in the top half of Figure 1, is centered around a single time-series plot. The x-axis is labeled with the word inputs w1, . . . ,wT for the corresponding time step. (If words do not fit into the fixed width for time steps they are distorted). In the plot itself, we show the hidden state vectors h1, . . . ,hT at each time step (one point for each of the D values). The hidden state dynamics are encoded through time to form a parallel coordinates plot (T1). That is there is one line for each of the D hidden states between each time-steps. Figure 1 shows the movement of each hidden state though the full sequence.\nThe full plot of hidden state dynamics can be difficult to comprehend directly. Therefore, LSTMVIS allows the user to formulate a hypothesis (G1) about the semantics of a subset of hidden states localized to a range of text. The user selects a phrase that may express an interesting property. For instance, the user may select a range within a shared nesting levels in tree-structured text (see Section 6.1), a representative noun phrase in a text corpus (see Section 6.2), or a chord progression in a musical corpus(see Section 6.3).\nTo select, the user brushes over a range of words that form the pattern of interest. In this process, she implicitly focuses on the hidden states that are \u201con\u201d in the selected range. The dashed red line on the parallel coordinates plot indicates a user-defined threshold value, `, that partitions the hidden states into \u201con\u201d (all timesteps \u2265 `) and\n\u201coff\u201d (any < `) within this range. In addition to selecting a range, the user can modify the brush slider below (gray) to define that hidden states must also be \u201coff\u201d immediately before or after the selected range. Figure 3 shows different combinations of slider configurations and the corresponding hidden state selections. We call this set of selected hidden states S1 \u2282 {1 . . .D}.\nThe defined selection of hidden states is mirrored in a discrete plot below the word labels. Blue bar charts indicate the percentage of \u201con\u201d cells at each visible time step that are in the set selected S1. The gray lines underneath reveal the ranges that are covered by each of the selected hidden states. These elements enable the user to preview the coverage of sequences in the local neighborhood. To eliminate highfrequency changes along the time axis, a length filter can be applied.\nAt the bottom of the Select View, the full set S1 is listed. Hovering over a hidden state representation in one of the described plots highlights this hidden state across all plots. Hidden states can also be deselected individually. Figure 1 shows a selected hidden state highlighted in red.\nThe described interactive methods allow the user to define a hypothesis range which results in the selection of a subset of hidden states based on the definition of a specific threshold (T2, G1) and only relies on the hidden state vectors themselves (TX). To refine or reject the hypothesis the user can then make use of the Match View."}, {"heading": "5.2 Match View", "text": "The Match View, shown in the bottom half of Figure 1, provides evidence for or against the selected hypothesis. The view provides a set of relevant matched phrases that have similar hidden state patterns as the phrase selected by the user. This style of nearest neighbors search can provide an intuitive view of the hidden states that are on for the hypothesis.\nWith the goal of maintaining an intuitive match interface, we define the matches to be \u201cranges in the data set that would have lead to a similar set of on hidden states under the selection criteria\u201d. Formally, assume that the user has selected a threshold ` with hidden states S1 and has not limited the selection to the right or left further. We rank all possible candidate ranges in the dataset starting at time a and ending at time b with a two step process\n1. collect the set of all hidden states that are \u201con\u201d for the range,\nS2 = {c \u2208 {1 . . .D} : ht,c \u2265 ` for all a\u2264 t \u2264 b}\n2. rank the candidates by the number of overlapping states |S1\u2229S2| using the inverse of number of additional \u201con\u201d cells \u2212|S1\u222aS2| and candidate length b\u2212a as tiebreaks.\nIf the original selection is limited on either side (as in Figure 3), we modify step (2) to take this into account for the candidates. For instance if there is a limit on the left, we only include state indices c in S2 in that also satisfy ha\u22121,c < `.\nFor efficiency, in practice we do not score at all possible candidate ranges (datasets typically have T > 1 million). We limit the candidate set by filtering to ranges with a minimum number of hidden states from S1 over the threshold `. These candidate sets can be computed efficiently using run-length encoding.\nA length histogram is also generated that indicates the distribution of phrase lengths in the matches. Hovering over a histogram bin reveals details about this bin and clicking on one filters the matches to the desired length.\nThe top 50 results are shown in the Match View. For each time step, the matches are encoded as a linked heatmap, which indicates the amount of overlap with S1 at each timestep. The color of the heatmap for each time step can be applied directly to the background of the results to better see the matches.\nFurthermore the user can provide additional annotations which are displayed as categorical heatmaps (T4). We imagine these annotations can act as ground truth data, e.g. part-of-speech tags for a text corpora, or as further information to help calibrate the hypotheses. Mapping\nannotation data to the matches is a simple method to reveal pattern across results. These results can lead to further data analysis or a refinement of the current hypothesis."}, {"heading": "5.3 Navigation Along the Time Axis", "text": "LSTMVIS provides several convenience methods to navigate to specific time steps. Buttons on the timeline can be used to move forward and backward. LSTMVIS also offers search functionality to find specific phrases. Finally, the selection panel on the top left can be used to efficiently switch between the different layers of the same model and between datasets (TX). As all different layers and datasets can be displayed in the same way, the user can easily compare models."}, {"heading": "5.4 Design Iterations", "text": "During the course of the project we developed seven interactive prototypes of varying complexity highlighting different aspects of the data. In this section we present two fundamental design decisions that lead to the final system."}, {"heading": "5.4.1 Visual Encoding of State Dynamics", "text": "Inspired by a standard static visualization in the RNN literature, we first encoded hidden state vectors as a heatmap along the time-axis (Figure 4(a)). This style has been favored as a view of the complete set of hidden states h1, . . . ,hT . However, this approach has several drawbacks in an interactive visualization. Foremost, the heatmaps do not scale well with increasing dimensionality D of hidden state vectors. They use a non-effective encoding for the most important information, i.e. hidden state values by color hue. Additionally they emphasize the order of hidden states in each vector, but this relative order of abstract hidden states is not actually used by the model itself.\nInstead we decided to consider each hidden state as a data item and time-steps as dimensions for each data item in a parallel coordinates plot. Doing so, we encode the hidden state value using the more\neffective visual variable position. Figure 4(b) shows the first iteration on using a parallel coordinates plot. The abundance of data points along the plot is additionally encoded with a heatmap in the background to emphasize dense regions (e.g. around the zero value) but also highlight sparse regions. In the final iteration, we omitted this redundant encoding for the sake of clarity and to highlight wider regions of text."}, {"heading": "5.4.2 Formulating a Hypothesis", "text": "One challenge we faced in early design iterations was allowing the user to easily express hypotheses with selection. In Figure 4(b), we show a preliminary draft using a common filter method for parallel coordinates along each axis. When experimenting with this kind of selection, two major drawbacks of this approach became evident. First, it was very cumbersome to formulate a hypothesis for a longer range by adjusting many y-axis brush selectors at a fine granularity. Second, selecting directly on the hidden state values felt decoupled from the original source of information \u2013 the text. The key idea to facilitate this selection process was allow the user to easily discretize the data based on a threshold and select on and off ranges directly on top of the words (as described in Section 5). This idea generalizes and adds interactivity to the manual approaches developed in [14]."}, {"heading": "6 USE CASES", "text": "In experimenting with the system we trained and explored many different RNN models, datasets and tasks, including word and character language models, neural machine translation systems, auto-encoders, summarization systems, and classifiers. Additionally we also experimented with other types of real and synthetic input data.\nIn this section we highlight three findings that demonstrate the general applicability of LSTMVIS paradigms for analysis of hidden states."}, {"heading": "6.1 Proof-of-Concept: Parenthesis Language", "text": "As proof of concept we trained an LSTM as language model on synthetic data generated from a very simple counting language with a\nparenthesis and letter alphabet \u03a3 = {( ) 0 1 2 3 4 }. The language is constrained to match parentheses, and nesting is limited to at most 4 levels deep, where each opening parenthesis increases nesting level and each closing parenthesis decreases the nesting level. Numbers are generated randomly, but are constrained to indicate the nesting level at their position. For example a string in the language looks like:\n, where blue lines indicates ranges of nesting level \u22651. Similarly, orange and green lines indicates nesting level \u22652 and \u22653.\nTo analyze this language, we view the states in LSTMVIS (we show the the cell states of a multi-layer 2x300 LSTM model). An example is shown in Figure 5(a). Here even the initial parallel coordinates plot shows a strong regularity, as hidden state changes occur predominately at parentheses.\nOur hypothesis is that the hidden states mainly reflex the nesting level. To test this, we select a range spanning nesting level four by selecting the phrase ( 4. We immediately see that several hidden states seem to cover this pattern and that in the local neighborhood several other occurrences of our hypothesis are covered as well, e.g. the empty parenthesis and the full sequence ( 4 4 4 . This observation simply confirms earlier observations that has demonstrate simple context-free models in RNNs and LSTMs [7, 25]."}, {"heading": "6.2 Phrase Separation in Language Modeling", "text": "Next we consider the case of a real-world natural language model. For this experiment we trained a 2-layer LSTM language model with 650 hidden states on the Penn Treebank [18] following the medium-sized model of [27]. While the model is trained for language modeling (predict the next word), we were interested in seeing if it additionally learned properties about the underlying language structure. To test this, we additionally include annotations in the model from the Penn Treebank. We experimented with including part-of-speech tags, named entities, and parse structure.\nHere we focus on the case of phrase chunking. We annotated the dataset with the gold-standard phrase chunks provided by the CoNLL 2003 shared task [23] for a subset of the treebank (Sections 15-18).\nThese include annotations for noun phrases and verb phrases, along with prepositions and several other less common phrase types.\nWhile running experimental analysis, we found a strong pattern that selecting noun phrases as hypotheses leads to almost entirely noun phrase matches. Additionally we found that selecting verb phrase prefixes would lead to primarily verb phrase matches. In Figure 5.4.2(a,b) we show two examples of these selections and matches.\nThis hints that the model has implicitly learned a representation for language modeling that can differentiate between the two types of phrases. Of course the tool itself cannot confirm or deny this type of hypothesis, but the aim is to provide clues for further analysis. We can check, outside of the tool, if the model is clearly differentiating between the classes in the phrase dataset. To do this we compute the set S1 for every noun and verb phrase in the shared task. We then run PCA on the vector representation for each set. The results are shown in\nFigure 6.2, which shows that indeed these on-off patterns are enough to partition the noun phrases and verb phrases."}, {"heading": "6.3 Musical Chord Progressions", "text": "Finally we looked at some non-text data sets to get a better understanding of long-range patterns. Past work on LSTM structure has emphasized cases where single hidden states are semantically interpretable. For text data sets, we found that with a few exceptions (quotes, brackets, and commas) this was rarely the case. However, for datasets with more regular long-term structure, single states could be quite meaningful.\nAs a simple example, we collected a large set of songs with annotated chords for rock and pop songs to use as a training data set, 219k chords in total. We then trained an LSTM language model to predict the next chord wt+1 in the sequence, conditioned on previous chord symbols (chords are left in their raw format).\nWhen we viewed the results in LSTMVIS we found that the regular repeating structure of the chord progressions is strongly reflected in the hidden states. Certain states will turn on at the beginning of a standard progression, and remain on though variant-length patterns until a resolution is reached. In Figure 6.3, we examine three very common general chord progressions in rock and pop music. We select a prototypical instance of the progression and show a single state that captures the pattern, i.e. remains on when the progression begins and turns off upon resolution."}, {"heading": "7 IMPLEMENTATION", "text": "LSTMVIS consists of two modules, the visualization system and the RNN modeling component.\nThe visualization is a client-server system that uses Javascript and D3 on client side and Python, Flask, h5py, and numpy on server side. Timeseries data (RNN hidden states and input) is loaded dynamically through HDF5 files. Optional annotation files can be specified to map categorical data to labels (T4). New data sets can be added easily by a declarative YAML configuration file.\nThe RNN modeling system is completely separated from the visualization to allow compatibility with any deep learning framework (TX). For our experiments we utilized the Torch framework and the Element RNN library [16]. We trained our models separately and exported results to the visualization.\nThe source code and models are available at lstm.seas.harvard. edu."}, {"heading": "8 CONCLUSION", "text": "LSTMVIS provides an interactive visualization to facilitate data analysis of recurrent neural network hidden states. The tool is based on a two-step process where a user can select a range of text to represent a hypothesis about the RNN representation, the tool then can match this selection to other examples in the data set. The tool easily allows for external annotations to verify or reject hypothesizes. It minimally requires a time-series of hidden states, which makes it easy to adopt for a wide range of visual analyses of different data sets and models, and even different tasks (language modeling, translation etc.).\nTo demonstrate the use of the model we presented three case studies describing how the tool can be applied to different data sets. On synthetic data, the tool clearly separates out the core underlying structure. On natural language data, states are noisier, but we can find clear splits between known linguistic structures like noun and verb phrases. For these tasks the tool not only helps narrow down hypotheses but also provides specific information such as hidden states and textual annotations to spur on further statistical testing. For future work, we would like to explore different matching criteria, to allow other forms of annotation, and to analyze the usage of the tool in practice."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by the Air Force Research Laboratory and DARPA grant FA8750-12-C-0300."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B.C. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A.Y. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A.Y. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": "CoRR, abs/1512.02595,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised sequence learning", "author": ["A.M. Dai", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pp. 3079\u20133087,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "author": ["F.A. Gers", "E. Schmidhuber"], "venue": "IEEE Transactions on Neural Networks, 12(6):1333\u20131340,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, eds., Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1693\u20131701,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Lingusitic analysis of multi-modal recurrent neural networks", "author": ["A. K\u00e1d\u00e1r", "G. Chrupa\u0142a", "A. Alishahi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1. K\u00e1d\u00e1r", "G. Chrupa\u0142a", "A. Alishahi"], "venue": "arXiv preprint arXiv:1602.08952,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP, vol. 3, p. 413,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Interactive optimization for steering machine classification", "author": ["A. Kapoor", "B. Lee", "D. Tan", "E. Horvitz"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1343\u20131352. ACM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Interacting with predictions: Visual inspection of black-box machine learning models", "author": ["J. Krause", "A. Perer", "K. Ng"], "venue": "Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp. 5686\u20135697. ACM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Rnn: Recurrent library for torch", "author": ["N. L\u00e9onard", "S. Waghmare", "Y. Wang"], "venue": "arXiv preprint arXiv:1511.07889,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding neural models in nlp", "author": ["J. Li", "X. Chen", "E. Hovy", "D. Jurafsky"], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 681\u2013691. Association for Computational Linguistics, San Diego, California, June", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Interspeech, vol. 2, p. 3,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pp. 3111\u20133119,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pp. 3104\u20133112,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Introduction to the conll- 2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Opening the black box-data driven visualization of neural networks", "author": ["F.-Y. Tzeng", "K.-L. Ma"], "venue": "VIS 05. IEEE Visualization, 2005., pp. 383\u2013390. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Recurrent neural networks can learn to implement symbolsensitive counting", "author": ["P.R.J. Wiles"], "venue": "Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference, vol. 10, p. 87. MIT Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "F. R. Bach and D. M. Blei, eds., Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol. 37 of JMLR Proceedings, pp. 2048\u20132057. JMLR.org,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv:1409.2329,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pp. 818\u2013833. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Recurrent neural networks (RNNs) [6] have proven to be a very effective general-purpose model for capturing long-term dependencies in textual applications.", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "These improved representation have led directly to end applications in machine translation [12, 22], speech recognition [2], music generation [4], and text classification [5], among a variety of other applications.", "startOffset": 91, "endOffset": 99}, {"referenceID": 21, "context": "These improved representation have led directly to end applications in machine translation [12, 22], speech recognition [2], music generation [4], and text classification [5], among a variety of other applications.", "startOffset": 91, "endOffset": 99}, {"referenceID": 1, "context": "These improved representation have led directly to end applications in machine translation [12, 22], speech recognition [2], music generation [4], and text classification [5], among a variety of other applications.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "These improved representation have led directly to end applications in machine translation [12, 22], speech recognition [2], music generation [4], and text classification [5], among a variety of other applications.", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "These improved representation have led directly to end applications in machine translation [12, 22], speech recognition [2], music generation [4], and text classification [5], among a variety of other applications.", "startOffset": 171, "endOffset": 174}, {"referenceID": 19, "context": "This representation can either be a standard fixed mapping, such as word2vec [20], or can be learned with the rest of the model.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "In this paper we will focus primarily on the task of RNN language modeling [19, 27], a core task in natural language processing.", "startOffset": 75, "endOffset": 83}, {"referenceID": 26, "context": "In this paper we will focus primarily on the task of RNN language modeling [19, 27], a core task in natural language processing.", "startOffset": 75, "endOffset": 83}, {"referenceID": 8, "context": "Finally, we note that our experiments will mainly focus on long short-term memory networks (LSTM) (hence the name LSTMVIS) [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 20, "context": "Understanding RNNs through Visualization Our core contribution, visualizing the state dynamics of LSTM in a structured way, is inspired by previous work on convolutional networks for in vision applications [21, 28].", "startOffset": 206, "endOffset": 214}, {"referenceID": 27, "context": "Understanding RNNs through Visualization Our core contribution, visualizing the state dynamics of LSTM in a structured way, is inspired by previous work on convolutional networks for in vision applications [21, 28].", "startOffset": 206, "endOffset": 214}, {"referenceID": 13, "context": "In [14], static visualization techniques are used to help understand LSTM hidden states in language models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], additional techniques are presented, particularly the use of gradient-based saliency to find important words.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In [10, 11], the authors show that RNNs specifically learn lexical categories and grammatical functions that carry semantic information, partially by modifying the inputs fed to the model.", "startOffset": 3, "endOffset": 11}, {"referenceID": 10, "context": "In [10, 11], the authors show that RNNs specifically learn lexical categories and grammatical functions that carry semantic information, partially by modifying the inputs fed to the model.", "startOffset": 3, "endOffset": 11}, {"referenceID": 2, "context": "In [3] attention is used for soft alignment in machine translation, in [26] attention is used to identify important aspects of an image for captioning, and in [8] attention is used to find important aspects of a document for an extraction task.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "In [3] attention is used for soft alignment in machine translation, in [26] attention is used to identify important aspects of an image for captioning, and in [8] attention is used to find important aspects of a document for an extraction task.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "In [3] attention is used for soft alignment in machine translation, in [26] attention is used to identify important aspects of an image for captioning, and in [8] attention is used to find important aspects of a document for an extraction task.", "startOffset": 159, "endOffset": 162}, {"referenceID": 23, "context": "In [24], the authors present a visualization system for feedforward neural networks with the goal of interpretation, and in [13], the authors give a user-interface for tuning the learning itself.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [24], the authors present a visualization system for feedforward neural networks with the goal of interpretation, and in [13], the authors give a user-interface for tuning the learning itself.", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "The recent Prospector system [15] provides a general-purpose tool for practitioners to better understand their ML model and its predictions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "There has also been work on user interfaces for constructing models such as TensorBoard [1] and the related playground for convolutional neural models playground.", "startOffset": 88, "endOffset": 91}, {"referenceID": 23, "context": "Our work is most similar in spirit to [24] in that we are mostly concerned with interpreting the hidden states of a particular model, however our specific goals and visual design are significantly different.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "This idea generalizes and adds interactivity to the manual approaches developed in [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "This observation simply confirms earlier observations that has demonstrate simple context-free models in RNNs and LSTMs [7, 25].", "startOffset": 120, "endOffset": 127}, {"referenceID": 24, "context": "This observation simply confirms earlier observations that has demonstrate simple context-free models in RNNs and LSTMs [7, 25].", "startOffset": 120, "endOffset": 127}, {"referenceID": 17, "context": "For this experiment we trained a 2-layer LSTM language model with 650 hidden states on the Penn Treebank [18] following the medium-sized model of [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "For this experiment we trained a 2-layer LSTM language model with 650 hidden states on the Penn Treebank [18] following the medium-sized model of [27].", "startOffset": 146, "endOffset": 150}, {"referenceID": 22, "context": "We annotated the dataset with the gold-standard phrase chunks provided by the CoNLL 2003 shared task [23] for a subset of the treebank (Sections 15-18).", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "For our experiments we utilized the Torch framework and the Element RNN library [16].", "startOffset": 80, "endOffset": 84}], "year": 2016, "abstractText": "Recurrent neural networks, and in particular long short-term memory networks (LSTMs), are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows a user to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with domain specific structural annotations. We further show several use cases of the tool for analyzing specific hidden state properties on data sets containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis.", "creator": "LaTeX with hyperref package"}}}