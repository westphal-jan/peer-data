{"id": "1606.07481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks", "abstract": "neural comparison to sequence generation recently became a very promising paradigm in machine translation, achieving competitive results with similar phrase - based systems. in this system description paper, we attempt to utilize several recently published databases used for neural sequential learning summer 2003 to build systems for wmt 2016 shared tasks of automatic post - editing and multimodal machine translation.", "histories": [["v1", "Thu, 23 Jun 2016 21:23:29 GMT  (2709kb,D)", "http://arxiv.org/abs/1606.07481v1", "Accepted to the First Conference of Machine Translation (WMT16)"]], "COMMENTS": "Accepted to the First Conference of Machine Translation (WMT16)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jind\\v{r}ich libovick\\'y", "jind\\v{r}ich helcl", "marek tlust\\'y", "pavel pecina", "ond\\v{r}ej bojar"], "accepted": false, "id": "1606.07481"}, "pdf": {"name": "1606.07481.pdf", "metadata": {"source": "CRF", "title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks", "authors": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina"], "emails": ["libovicky@ufal.mff.cuni.cz", "helcl@ufal.mff.cuni.cz", "tlusty@ufal.mff.cuni.cz", "bojar@ufal.mff.cuni.cz", "pecina@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "Neural sequence to sequence models are currently used for variety of tasks in Natural Language Processing including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), text summarization (Rush et al., 2015), natural language generation (Wen et al., 2015), and others. This was enabled by the capability of recurrent neural networks to model temporal structure in data, including the long-distance dependencies in case of gated networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).\nThe deep learning models\u2019 ability to learn a dense representation of the input in the form of a real-valued vector recently allowed researchers to combine machine vision and natural language processing into tasks believed to be extremely difficult only few years ago. The distributed representations of words, sentences and images can be understood as a kind of common data type for language and images within the models. This is then used in tasks like automatic image captioning (Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al., 2015) or in attempts to ground lexical semantics in vision (Kiela and Clark, 2015).\nIn this system description paper, we bring a summary of the Recurrent Neural Network (RNN)-based system we have submitted to the automatic post-editing task and to the multimodal translation task. Section 2 describes the architecture of the networks we have used. Section 3 summarizes related work on the task of automatic post-editing of machine translation output and describes our submission to the Workshop of Machine Translation (WMT) competition. In a similar fashion, Section 4 refers to the task of multimodal translation. Conclusions and ideas for further work are given in Section 5."}, {"heading": "2 Model Description", "text": "We use the neural translation model with attention (Bahdanau et al., 2014) and extend it to include multiple encoders, see Figure 1 for an illustration. Each input sentence enters the system simultaneously in several representations xi. An encoder used for the i-th representation Xi = (x1i , . . . , x k i ) of k words, each stored as a one-hot vector xji , is a bidirectional RNN implementing a function\nf(Xi) = Hi = (h 1 i , . . . , h k i ) (1)\nwhere the states hji are concatenations of the outputs of the forward and backward networks after processing the j-th token in the respective order.\nThe initial state of the decoder is computed as a weighted combination of the encoders\u2019 final states.\nThe decoder is an RNN which receives an embedding of the previously produced word as an input in every time step together with the hidden state from the previous time step. The RNN\u2019s output is then used to compute the attention and the next word distribution.\nThe attention is computed over each encoder separately as described by Bahdanau et al. (2014). The attention vector ami of the i-th encoder in the\nar X\niv :1\n60 6.\n07 48\n1v 1\n[ cs\n.C L\n] 2\n3 Ju\nn 20\n16\nm-th step of the decoder is\nami = \u2211\nhki in Hi hki \u03b1 k,m i (2)\nwhere the weights \u03b1mi is a distribution estimated as\n\u03b1mi = softmax ( vT \u00b7 tanh(sm +WHiHi) ) (3)\nwith sm being the hidden state of the decoder in time m. Vector v and matrix WHi are learned parameters for projecting the encoder states.\nThe probability of the decoder emitting the word ym in the j-th step, denoted as P (ym|H1, . . . ,Hn,Y0..m\u22121), is proportional to\n(4)exp ( Wos j + n\u2211\ni=1\nWaia j i\n)\nwhere Hi are hidden states from the i-th encoder and Y0..m\u22121 is the already decoded target sentence (represented as matrix, one-hot vector for each produced word). Matrices Wo and Wai are learned parameters; Wo determines the recurrent dependence on the decoder\u2019s state and Wai determine the dependence on the (attention-weighted) encoders\u2019 states.\nFor image captioning, we do not use the attention model because of its high computational demands and rely on the basic model by Vinyals\net al. (2015) instead. We use Gated Recurrent Units (Cho et al., 2014) and apply the dropout of 0.5 on the inputs and the outputs of the recurrent layers (Zaremba et al., 2014) and L2 regularization of 10\u22128 on all parameters. The decoding is done using a beam search of width 10. Both the decoders and encoders have hidden states of 500 neurons, word embeddings have the dimension of 300. The model is optimized using the Adam optimizer (Kingma and Ba, 2014) with learning rate of 10\u22123.\nWe experimented with recently published improvements of neural sequence to sequence learning: scheduled sampling (Bengio et al., 2015), noisy activation function (Gu\u0308lc\u0327ehre et al., 2016), linguistic coverage model (Tu et al., 2016). None of them were able to improve the systems\u2019 performance, so we do not include them in our submissions.\nSince the target language for both the task was German, we also did language dependent pre- and post-processing of the text. For the training we split the contracted prepositions and articles (am \u2194 an dem, zur\u2194 zu der, . . . ) and separated some pronouns from their case ending (keinem \u2194 kein -em, unserer\u2194 unser -er, . . . ). We also tried splitting compound nouns into smaller units, but on the relatively small data sets we have worked with, it did not bring any improvement."}, {"heading": "3 Automatic Post-Editing", "text": "The task of automatic post-editing (APE) aims at improving the quality of a machine translation system treated as black box. The input of an APE system is a pair of sentences \u2013 the original input sentence in the source language and the translation generated by the machine translation (MT) system. This scheme allows to use any MT system without any prior knowledge of the system itself. The goal of this task is to perform automatic corrections on the translated sentences and generate a better translation (using the source sentence as an additional source of information).\nFor the APE task, the organizers provided tokenized data from the IT domain (Turchi et al., 2016). The training data consist of 12,000 triplets of the source sentence, its automatic translation and a reference sentence. The reference sentences are manually post-edited automatic translations. Additional 1,000 sentences were provided for validation, and another 2,000 sentences for final evaluation. Throughout the paper, we report scores on the validation set; reference sentences for final evaluation were not released for obvious reasons.\nThe performance of the systems is measured using Translation Error Rate (Snover et al., 2006) from the manually post-edited sentences. We thus call the score HTER. This means that the goal of the task is more to simulate manual post-editing, rather than to reconstruct the original unknown reference sentence."}, {"heading": "3.1 Related Work", "text": "In the previous year\u2019s competition (Bojar et al., 2015), most of the systems were based on the phrase-base statistical machine translation (SMT) in a monolingual setting (Simard et al., 2007).\nThere were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013).\nAlthough the use of neural sequential model is very straightforward in this case, to the best of our knowledge, there have not been experiments with RNNs for this task."}, {"heading": "3.2 Experiments & Results", "text": "The input sentence is fed to our system in a form of multiple input sequences without explicitly telling which sentence is the source one and which one\nis the MT output. It is up to the network to discover their best use when producing the (single) target sequence. The initial experiments showed that the network struggles to learn that one of the source sequences is almost correct (even if it shares the vocabulary and word embeddings with the expected target sequence). Instead, the network seemed to learn to paraphrase the input.\nTo make the network focus more on editing of the source sentence instead of preserving the meaning of the sentences, we represented the target sentence as a minimum-length sequence of edit operations needed to turn the machine-translated sentence into the reference post-edit. We extended the vocabulary by two special tokens keep and delete and then encoded the reference as a sequence of keep, delete and insert operations with the insert operation defined by the placing the word itself. See Figure 2 for an example.\nAfter applying the generated edit operations on the machine-translated sentences in the test phase, we perform a few rule-based orthographic fixes for punctuation. The performance of the system is given in Table 1. The system was able to slightly improve upon the baseline (keeping the translation as it is) in both the HTER and BLEU score. The system was able to deal very well with the frequent error of keeping a word from the source in the translated sentence. Although neural sequential models usually learn the basic output structure very quickly, in this case it made a lot of errors in pairing parentheses correctly. We ascribe this to the edit-operation notation which obfuscated the basic orthographic patterns in the target sentences."}, {"heading": "4 Multimodal Translation", "text": "The goal of the multimodal translation task is to generate an image caption in a target language (German) given the image itself and one or more captions in the source language (English).\nRecent experiments of Elliott et al. (2015) showed that including the information from the images can help disambiguate the source-language captions.\nThe participants were provided with the Multi30k dataset (Elliott et al., 2016) which is an extension of the Flickr30k dataset (Plummer et al., 2015). In the original dataset, 31,014 images were taken from the users collections on the image hosting service Flickr. Each of the images were given five independent crowd-sourced captions in English. For the Multi30k dataset, one of the English captions for each image was translated into German and five other independent German captions were provided. The data are split into a training set of 29,000 images, a validation set of 1,014 images and a test set with 1,000 images.\nThe two ways in which the image annotation were collected also lead to two sub-tasks. The first one is called Multimodal Translation and its goal is to generate a translation of an image caption to the target language given the caption in source language and the image itself. The second task is the Cross-Lingual Image Captioning. In this setting, the system is provided five captions in the source language and it should generate one caption in target language given both sourcelanguage captions and the image itself. Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011). The translation task is evaluated against a single reference sentence which is the direct human translation of the source sentence. The cross-lingual captioning task is evaluated against the five reference captions in the target language created independently of the source captions."}, {"heading": "4.1 Related Work", "text": "The state-of-the-art image caption generators use a remarkable property of the Convolutional Neural Network (CNN) models originally designed for ImageNet classification to capture the semantic features of the images. Although the images in ImageNet (Deng et al., 2009; Russakovsky et al., 2015) always contain a single object to classify, the networks manage to learn a representation that is usable in many other cases including image captioning which usually concerns multiple objects in the image and also needs to describe complex actions and spacial and temporal relations within the image.\nPrior to CNN models, image classification used to be based on finding some visual primitives in the image and transcribing automatically estimated relations between the primitives. Soon after Kiros et al. (2014) showed that the CNN features could be used in a neural language model, Vinyals et al. (2015) developed a model that used an RNN decoder known from neural MT for generating captions from the image features instead of the vector encoding the source sentence. Xu et al. (2015) later even improved the model by adapting the soft alignment model (Bahdanau et al., 2014) nowadays known as the attention model. Since then, these models have become a benchmark for works trying to improve neural sequence to sequence models (Bengio et al., 2015; Gu\u0308lc\u0327ehre et al., 2016; Ranzato et al., 2015)."}, {"heading": "4.2 Phrase-Based System", "text": "For the translation task, we trained Moses SMT (Koehn et al., 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word\nis concatenated with its aligned source word into one bitoken (e.g.\u201cKatze-cat\u201d). For unaligned target words, we create a bitoken with NULL as the source word (e.g. \u201cwird-NULL\u201d). Unaligned source words are dropped. For more than one-toone alignments, we join all aligned word pairs into one bitoken (e.g. \u201chat-had+gehabt-had\u201d). These word-level bitokens are afterwards clustered into coarse classes (Brown et al., 1992) and a standard n-gram language model is trained on these classes. Following the notation of Stewart et al. (2014), \u201c400bi\u201d indicates a LM trained on 400 bitoken classes, \u201c200bi\u201d stands for 200 bitoken classes, etc. Besides bitokens based on aligned words, we also use class-level bitokens. For example \u201c(200,400)\u201d means that we clustered source words into 200 classes and target words into 400 classes and only then used the alignment to extract bitokens of these coarser words. The last type is \u201c100bi(200,400)\u201d, a combination of both independent clustering in the source and target \u201c(200,400)\u201d and the bitoken clustering \u201c100bi\u201d.\nAltogether, we tried 26 configurations combining various coarse language models. The best three were \u201c200bi\u201d (a single bitoken LM), \u201c200bi&(1600,200)&100tgt\u201d (three LMs, each with its own weight, where 100tgt means a language model over 100 word classes trained on the target side only) and \u201c200bi&100tgt\u201d.\nManual inspection of these three best configurations reveals almost no differences; often the outputs are identical. Comparing to the baseline (a single word-based LM), it is evident that coarse models prefer to ensure agreement and are much more likely to allow for a different word or preposition choice to satisfy the agreement."}, {"heading": "4.3 Neural System", "text": "For the multimodal translation task, we combine the RNN encoders with image features. The image features are extracted from the 4096-dimensional penultimate layer (fc7) of the VGG-16 Imagenet network (Simonyan and Zisserman, 2014) before applying non-linearity. We keep the weights of the convolutional network fixed during the training. We do not use attention over the image features, so the image information is fed to the network only via the initial state.\nWe also try a system combination and add an encoder for the phrase-based output. The SMT encoder shares the vocabulary and word embeddings with the decoder. For the combination with SMT output, we experimented with the CopyNet architecture (Gu et al., 2016) and with encoding the sequence the way as in the APE task (see Section 3.2). Since neither of these variations seems to have any effect on the performance, we report only the results of the simple encoder combina-\ntion. Systems targeted for the multimodal translation task have a single English caption (and eventually its SMT and the image representation) on its input and produce a single sentence which is a translation of the original caption. Every input appears exactly once in the training data paired with exactly one target sentence. On the other hand, systems targeted for the cross-lingual captioning use all five reference sentences as a target, i.e. every input is present five times in the training data with five different target sentences, which are all independent captions in German. In case of the crosslingual captioning, we use five parallel encoders sharing all weights combined with the image features in the initial state.\nResults of the experiments with different input combinations are summarized in the next section."}, {"heading": "4.4 Results", "text": "The results of both the tasks are given in Table 2. Our system significantly improved since the competition submission, therefore we report both the performance of the current system and of the submitted systems. Examples of the system output can be found in Figure 3.\nThe best performance has been achieved by the neural system that combined all available input both for the multimodal translation and crosslingual captioning. Although, using the image as the only source of information led to poor results, adding the image information helped to improve\nthe performance in both tasks. This supports the hypothesis that for the translation of an image caption, knowing the image can add substantial piece of information.\nThe system for cross-lingual captioning tended to generate very short descriptions, which were usually true statements about the images, but the sentences were often too general or missing important information. We also needed to truncate the vocabulary which brought out-of-vocabulary tokens to the system output. Unlike the translation task where the vocabulary size was around 20,000 different forms for both languages, having 5 source and 5 reference sentences increased the vocabulary size more than twice.\nSimilarly to the automatic postediting task, we were not able to come up with a setting where the combination with the phrase-based system would improve over the very strong Moses system with bitoken-classes language model. We can therefore hypothesize that the weakest point of the models is the weighted combination of the inputs for the initial state of the decoder. The difficulty of learning relatively big combination weighting matrices which are used just once during the model execution (unlike the recurrent connections having approximately the same number of parameters) probably over-weighted the benefits of having more information on the input. In case of system combination, more careful exploration of explicit copy mechanism as CopyNet (Gu et al., 2016) may be useful."}, {"heading": "5 Conclusion", "text": "We applied state-of-the art neural machine translation models to two WMT shared tasks. We showed that neural sequential models could be successfully applied to the APE task. We also showed that information from the image can significantly help while producing a translation of an image caption. Still, with the limited amount of data provided, the neural system performed comparably to a very well tuned SMT system.\nThere is still a big room for improvement of the performance using model ensembles or recently introduced techniques for neural sequence to sequence learning. An extensive hyper-parameter testing could be also helpful."}, {"heading": "Acknowledgment", "text": "We would like to thank Toma\u0301s\u030c Musil, Milan Straka and Ondr\u030cej Dus\u030cek for discussing the problem with us and countless tips they gave us during our work.\nThis work has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement no. 645 452 (QT21) and no. 644 753 (KConnect) and the Czech Science Foundation (grant n. P103/12/G084). Computational resources were provided by the CESNET LM2015042, provided under the programme \u201cProjects of Large Research, Development, and Innovations Infrastructures.\u201d"}], "references": [{"title": "VQA: Visual question answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In The IEEE International Conference on Computer Vision", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam M. Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F. Brown", "Peter V. de Souza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proceedings of SSST-8,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems", "author": ["Denkowski", "Lavie2011] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2011}, {"title": "Multi-language image description with neural sequence models. CoRR, abs/1510.04709", "author": ["Stella Frank", "Eva Hasler"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual EnglishGerman Image Descriptions", "author": ["Elliott et al.2016] D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning. CoRR, abs/1603.06393", "author": ["Gu et al.2016] Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Multi- and cross-modal semantics beyond vision: Grounding in auditory perception", "author": ["Kiela", "Clark2015] Douwe Kiela", "Stephen Clark"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,", "citeRegEx": "2007.,? \\Q2007\\E", "shortCiteRegEx": "2007.", "year": 2007}, {"title": "A three-layer architecture for automatic post editing system using rule-based paradigm", "author": ["Abdolhossein Sarrafzadeh", "Mehdi Mohammadi"], "venue": "In Proceedings of the 4th Workshop on South and South-", "citeRegEx": "Mohaghegh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohaghegh et al\\.", "year": 2013}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["Liwei Wang", "Chris M Cervantes", "Juan C Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Sequence level training with recurrent neural networks. CoRR, abs/1511.06732", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Depfix, a tool for automatic rule-based post-editing of SMT", "author": ["Rudolf Rosa"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Rosa.,? \\Q2014\\E", "shortCiteRegEx": "Rosa.", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Khosla and Bernstein,? \\Q2015\\E", "shortCiteRegEx": "Khosla and Bernstein", "year": 2015}, {"title": "Rulebased translation with statistical phrase-based postediting", "author": ["Simard et al.2007] Michel Simard", "Nicola Ueffing", "Pierre Isabelle", "Roland Kuhn"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Simard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation", "author": ["Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": "In Proceedings AMTA,", "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Coarse split and lump bilingual language models for richer source information in SMT", "author": ["Roland Kuhn", "Eric Joanis", "George Foster"], "venue": "In Proceedings of the Eleventh Conference of the Association for Machine", "citeRegEx": "Stewart et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Coverage-based neural machine translation. CoRR, abs/1601.04811", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": null, "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "WMT16 APE shared task data. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Turchi et al.2016] Marco Turchi", "Rajen Chatterjee", "Matteo Negri"], "venue": null, "citeRegEx": "Turchi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Turchi et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "used for variety of tasks in Natural Language Processing including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), text summarization (Rush et al.", "startOffset": 87, "endOffset": 134}, {"referenceID": 1, "context": "used for variety of tasks in Natural Language Processing including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), text summarization (Rush et al.", "startOffset": 87, "endOffset": 134}, {"referenceID": 20, "context": ", 2014), text summarization (Rush et al., 2015), natural language generation (Wen et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 30, "context": ", 2015), natural language generation (Wen et al., 2015), and others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": "This was enabled by the capability of recurrent neural networks to model temporal structure in data, including the long-distance dependencies in case of gated networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 168, "endOffset": 220}, {"referenceID": 29, "context": "This is then used in tasks like automatic image captioning (Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al.", "startOffset": 59, "endOffset": 98}, {"referenceID": 31, "context": "This is then used in tasks like automatic image captioning (Vinyals et al., 2015; Xu et al., 2015), visual question answering (Antol et al.", "startOffset": 59, "endOffset": 98}, {"referenceID": 0, "context": ", 2015), visual question answering (Antol et al., 2015) or in attempts to ground lexical semantics in vision (Kiela and Clark, 2015).", "startOffset": 35, "endOffset": 55}, {"referenceID": 1, "context": "We use the neural translation model with attention (Bahdanau et al., 2014) and extend it to include", "startOffset": 51, "endOffset": 74}, {"referenceID": 1, "context": "The attention is computed over each encoder separately as described by Bahdanau et al. (2014). The attention vector ai of the i-th encoder in the ar X iv :1 60 6.", "startOffset": 71, "endOffset": 94}, {"referenceID": 29, "context": "For image captioning, we do not use the attention model because of its high computational demands and rely on the basic model by Vinyals et al. (2015) instead.", "startOffset": 129, "endOffset": 151}, {"referenceID": 4, "context": "Units (Cho et al., 2014) and apply the dropout of 0.", "startOffset": 6, "endOffset": 24}, {"referenceID": 32, "context": "5 on the inputs and the outputs of the recurrent layers (Zaremba et al., 2014) and L2 regularization of 10\u22128 on all parameters.", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "We experimented with recently published improvements of neural sequence to sequence learning: scheduled sampling (Bengio et al., 2015), noisy activation function (G\u00fcl\u00e7ehre et al.", "startOffset": 113, "endOffset": 134}, {"referenceID": 27, "context": ", 2016), linguistic coverage model (Tu et al., 2016).", "startOffset": 35, "endOffset": 52}, {"referenceID": 28, "context": "For the APE task, the organizers provided tokenized data from the IT domain (Turchi et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 24, "context": "The performance of the systems is measured using Translation Error Rate (Snover et al., 2006) from the manually post-edited sentences.", "startOffset": 72, "endOffset": 93}, {"referenceID": 22, "context": ", 2015), most of the systems were based on the phrase-base statistical machine translation (SMT) in a monolingual setting (Simard et al., 2007).", "startOffset": 122, "endOffset": 143}, {"referenceID": 19, "context": "There were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013).", "startOffset": 166, "endOffset": 202}, {"referenceID": 15, "context": "There were also several rule-based post-editing systems benefiting from the fact that errors introduced by statistical and rule-based systems are of a different type (Rosa, 2014; Mohaghegh et al., 2013).", "startOffset": 166, "endOffset": 202}, {"referenceID": 7, "context": "Recent experiments of Elliott et al. (2015)", "startOffset": 22, "endOffset": 44}, {"referenceID": 8, "context": "Multi30k dataset (Elliott et al., 2016) which is an extension of the Flickr30k dataset (Plummer et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": ", 2016) which is an extension of the Flickr30k dataset (Plummer et al., 2015).", "startOffset": 55, "endOffset": 77}, {"referenceID": 16, "context": "Both tasks are evaluated using the BLEU (Papineni et al., 2002) score and METEOR score (Denkowski and Lavie, 2011).", "startOffset": 40, "endOffset": 63}, {"referenceID": 5, "context": "ImageNet (Deng et al., 2009; Russakovsky et al., 2015) always contain a single object to classify, the networks manage to learn a representation that is usable in many other cases including image captioning which usually concerns multiple objects in the image and also needs to describe complex ac-", "startOffset": 9, "endOffset": 54}, {"referenceID": 1, "context": "(2015) later even improved the model by adapting the soft alignment model (Bahdanau et al., 2014) nowadays known as the attention model.", "startOffset": 74, "endOffset": 97}, {"referenceID": 2, "context": "Since then, these models have become a benchmark for works trying to improve neural sequence to sequence models (Bengio et al., 2015; G\u00fcl\u00e7ehre et al., 2016; Ranzato et al., 2015).", "startOffset": 112, "endOffset": 178}, {"referenceID": 18, "context": "Since then, these models have become a benchmark for works trying to improve neural sequence to sequence models (Bengio et al., 2015; G\u00fcl\u00e7ehre et al., 2016; Ranzato et al., 2015).", "startOffset": 112, "endOffset": 178}, {"referenceID": 11, "context": "ter Kiros et al. (2014) showed that the CNN features could be used in a neural language model, Vinyals et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 11, "context": "ter Kiros et al. (2014) showed that the CNN features could be used in a neural language model, Vinyals et al. (2015) developed a model that used an RNN decoder known from neural MT for generating captions from the image features instead of the vector encoding the source sentence.", "startOffset": 4, "endOffset": 117}, {"referenceID": 11, "context": "ter Kiros et al. (2014) showed that the CNN features could be used in a neural language model, Vinyals et al. (2015) developed a model that used an RNN decoder known from neural MT for generating captions from the image features instead of the vector encoding the source sentence. Xu et al. (2015) later even improved the model by adapting the soft alignment model (Bahdanau et al.", "startOffset": 4, "endOffset": 298}, {"referenceID": 14, "context": ", 2007) with additional language models based on coarse bitoken classes. We follow the approach of Stewart et al. (2014): Based on the word alignment, each target word", "startOffset": 2, "endOffset": 121}, {"referenceID": 3, "context": "word-level bitokens are afterwards clustered into coarse classes (Brown et al., 1992) and a standard n-gram language model is trained on these classes.", "startOffset": 65, "endOffset": 85}, {"referenceID": 3, "context": "word-level bitokens are afterwards clustered into coarse classes (Brown et al., 1992) and a standard n-gram language model is trained on these classes. Following the notation of Stewart et al. (2014), \u201c400bi\u201d indicates a LM trained on 400 bitoken classes, \u201c200bi\u201d stands for 200 bitoken classes, etc.", "startOffset": 66, "endOffset": 200}, {"referenceID": 9, "context": "For the combination with SMT output, we experimented with the CopyNet architecture (Gu et al., 2016) and with encoding the sequence the way as in the APE task (see Section 3.", "startOffset": 83, "endOffset": 100}, {"referenceID": 9, "context": "In case of system combination, more careful exploration of explicit copy mechanism as CopyNet (Gu et al., 2016) may be useful.", "startOffset": 94, "endOffset": 111}], "year": 2016, "abstractText": "Neural sequence to sequence learning recently became a very promising paradigm in machine translation, achieving competitive results with statistical phrase-based systems. In this system description paper, we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for WMT 2016 shared tasks of Automatic Post-Editing and Multimodal Machine Translation.", "creator": "LaTeX with hyperref package"}}}