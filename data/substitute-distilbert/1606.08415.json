{"id": "1606.08415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "we propose the behavioral error linear unit ( gelu ), a high - performing neural network activation condition. the gelu nonlinearity is underlying expected transformation of a stochastic process which randomly applies the identity function zero map, combining the intuitions of dropout and zoneout while respecting neuron values. this connection suggests a new probabilistic understanding of nonlinearities. we perform careful empirical evaluation of the conditional nonlinearity against the relu and elu activations against find specific strengths across all aspects.", "histories": [["v1", "Mon, 27 Jun 2016 19:20:40 GMT  (435kb,D)", "http://arxiv.org/abs/1606.08415v1", null], ["v2", "Fri, 8 Jul 2016 18:32:46 GMT  (608kb,D)", "http://arxiv.org/abs/1606.08415v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dan hendrycks", "kevin gimpel"], "accepted": false, "id": "1606.08415"}, "pdf": {"name": "1606.08415.pdf", "metadata": {"source": "CRF", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "authors": ["Dan Hendrycks", "Kevin Gimpel"], "emails": ["dan@ttic.edu", "kgimpel@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "The sigmoid function was once the most popular nonlinear activation function for neural networks. Despite having a probabilistic interpretation, it has fallen out of favor due to slow and inaccurate convergence. In contrast, the widely-used ReLU activation usually demonstrates superior convergence yet is lacking in terms of its probabilistic interpretation [8]. Both nonlinearities increase monotonically and thus yield greater outputs for greater inputs\u2014their utility lies in how they respond to their input.\nSome neural network regularization strategies, in contrast, act irrespectively of a neuron\u2019s value. For example, dropout randomly sets neuron values to zero, and zoneout randomly preserves previously computed neuron values [10, 5]. Dropout can act as a stochastic zero map, and zoneout can act as a stochastic identity map. The ReLU is a deterministic zero map or identity map, depending on the input value. But this determinism of the ReLU is limiting; the stochasticity of dropout and zoneout may allow for a pseudo-ensemble [1] of networks with stochastic width or stochastic depth, respectively, and this leads to improved performance [3, 11]. Since ReLUs lack stochasticity and since the aforementioned regularizers are irrespective of their input, the innovations have remained distinct. In this paper, we bridge the gap between nonlinearities and stochastic regularizers by considering a stochastic process that is dependent upon input values. We encapsulate the stochastic process into a deterministic activation function that we call the Gaussian Error Linear Unit (GELU). In experiments across several tasks, we find GELU activations to outperform both ReLU and ELU activations.\n2 GELUs and the Stochastic 0-I Map\nWe begin by describing the stochastic process that underlies the GELU nonlinearity. We call the process the stochastic 0-I map (SOI map). Before expounding on this map, we establish\n\u2217Work done while the author was at TTIC.\nar X\niv :1\n60 6.\n08 41\n5v 1\n[ cs\n.L G\n] 2\n7 Ju\nn 20\nmotivating assumptions. To start, let us initialize each column of the neural networks weights W \u2208 Rnin\u00d7nout uniformly on the surface of (not inside) the unit hypersphere, and let the input data X \u2208 Rbatch size\u00d7nin have a standard normal distribution. As a consequence, it we let Z = XW , it follows that Zij is from a standard normal distribution. Now, we have seen that both the sigmoid and ReLU assign greater values to greater inputs; we wish to have a stochastic process which tends to do this. So, we define the SOI map as randomly applying the identity map to an input with probability \u03a6(Zij) = P (Y \u2264 Zij), Y \u223c N (0, 1) and applying the zero map otherwise. This introduces nondeterminism but maintains dependency upon the input value.\nWe can recover a deterministic function from this process to obtain a traditional nonlinearity. The expected transformation to an input x is \u03a6(x) \u00d7 Ix + (1 \u2212 \u03a6(x)) \u00d7 0x = x\u03a6(x). We now make an obvious generalization. Since the cumulative distribution function is computed with the error function, we define the Gaussian Error Linear Unit (GELU) as\nGELU(x) = xFX(x) = xP (X \u2264 x)\nwhere X \u223c N (\u00b5, \u03c32). Both \u00b5 and \u03c3 are possibly parameters to optimize, but throughout this paper we simply let \u00b5 = 0 and \u03c3 = 1.\nThe GELU bears semblance to the ReLU and ELU (see [2] for an ELU description). For example, as \u03c3 \u2192 0 and if \u00b5 = 0, the GELU becomes a ReLU. More, the ReLU and GELU are equal asymptotically. Unlike the ReLU, the GELU and ELU can be both negative and positive. In fact, if we used the cumulative distribution function of the standard Cauchy distribution, then the ELU (when \u03b1 = 1/\u03c0 or ELU(x) = 1(x > 0)x + 1\u03c01(x \u2264 0)(e\nx \u2212 1), where 1 is the indicator function) is xFC(x), C \u223c Cauchy(0, 1) asymptotically. These are some fundamental relations to previous nonlinearities.\nHowever, the GELU has several notable differences. This non-convex, non-monotonic function is not linear in the positive domain and exhibits curvature at all points. Meanwhile ReLUs and ELUs, which are convex and monotonic activations, are linear in the positive domain and do not always exhibit curvature. Increased curvature and non-monotonicity may allow GELUs to more easily approximate complicated functions than can RELUs or ELUs. In addition and significantly, the GELU has a probabilistic interpretation given that it is the expected SOI map."}, {"heading": "3 Experiments", "text": "We evaluate the GELU, ReLU, and ELU on MNIST classification (grayscale images with 10 classes, 60k training examples and 10k test examples), MNIST autoencoding, and CIFAR-10 (color images with 10 classes, 50k training examples and 10k test examples) tasks. We do not evaluate the LReLU because of its similarity to ReLUs (see [6] for a description of LReLUs)."}, {"heading": "3.1 MNIST Classification", "text": "Let us verify that this nonlinearity competes with previous activation functions. We train a fully connected neural network with GELUs (\u00b5 = 0, \u03c3 = 1), ReLUs, and ELUs (\u03b1 = 1). Each 7-layer, 128 neuron wide neural network is trained for 50 epochs with a batch size of 128. We use the Adam optimizer and its suggested learning rate of 0.001 [4]. The weights are uniformly initialized on the unit hypersphere as a consequence of our argument in the previous section; this has positive impact on each nonlinearity\u2019s performance [7, 9]. Figure 2 shows that the GELU tends to have the lowest median training log loss."}, {"heading": "3.2 MNIST Autoencoder", "text": "We now move to a self-supervised setting and train a deep autoencoder on MNIST. To accomplish this, we use a network with layers of width 1000, 500, 250, 30, 250, 500, 1000, in that order. We again use the Adam optimizer and a batch size of 64. Our loss is the mean squared\n0 50 100 150 Epoch\n10 2\nRe co\nns tru\nct io\nn Er\nro r\nGELU ReLU ELU\n0 50 100 150 Epoch\n10 2\nGELU ReLU ELU\n0 50 100 150 Epoch\n10 2\nGELU ReLU ELU\nFigure 3: MNIST Autoencoding Results. Each curve is the median of three runs, and we plot each on a logarithmic scale because the mean squared losses are small. Left are loss curves for a learning rate of 10\u22123, the middle figure has a 10\u22124 learning rate, and right has 10\u22125.\nloss. We vary the learning rate from 10\u22123 to 10\u22125.1 The results are shown in Figure 3. The GELU either ties or significantly outperforms the other nonlinearities. This shows the GELU nonlinearity is both stable and accurate under different learning rates."}, {"heading": "3.3 CIFAR-10", "text": "Next, we demonstrate that for more intricate architectures with dropout the GELU nonlinearity again outperforms other nonlinearities. We use a convolutional architecture with the stacks (2\u00d73\u00d732), (2\u00d73\u00d764) representing the number of layers, receptive field, and number of filters, respectively. We then feed this output through a two layer network with 512 and 256 neurons for the two layers. We apply max-pooling after every stack, and we run two experiments: we apply no dropout or use dropout rates of 0.25 after the first stack, 0.25 after the second stack, and 0.5 before the last fully-connected layer. As ever, we use the Adam optimizer with a learning rate of 0.001 and initialize weights on the unit hypersphere (each filter has an `2 norm of one). Figure 4 shows the results. In both situations, GELUs provide faster and superior convergence. Consequently, although the GELU is inspired by a different stochastic process, it comports well with dropout."}, {"heading": "4 Discussion", "text": "Across several experiments, the GELU outperformed previous nonlinearities, but there are minor cautions. The MNIST experiments are similar to those run by the architects of the ELU, but unlike their experiments, we use Adam instead of ordinary gradient descent because Adam\n1Afterward, we tried a 10\u22122 learning rate but all solutions poorly converged, GELUs tied with ReLUs, and ELUs diverged.\nbetter reflects what is typically used in practice for training neural networks. Interestingly, this change causes ELUs to occasionally perform worse than ReLUs. Furthermore, by the GELU\u2019s difference in convexity and monotonicity, we find that an optimizer with momentum is important for allowing GELUs to converge well, while previous nonlinearities do not have this (minor) drawback. This suggests GELUs generate an error surface unlike those generated by ReLUs. Also, like exp(\u00b7) and \u03c3(\u00b7), the error function is expensive to compute accurately, but fast approximations exist. It is also worth mentioning why we did not display the SOI map performance in the above experiments. We now show one such example of its performance in Figure 5. Clearly, it can replace nonlinearities occasionally. For example, on the MNIST Classification tasks, it outperforms the ELU nonlinearity\u2014a neural network with only a regularization process can outperform a network with a nonlinearity. It achieves low validation log loss as a consequence of its regularizing stochasticity. However, adjusting the extent to which the SOI map regularizes is currently unclear, thus we cannot yet recommend using the SOI map as a nonlinearity replacement."}, {"heading": "5 Conclusion", "text": "We observed that the GELU outperforms previous nonlinearities across several tasks. Importantly for future work, this nonlinearity has a probabilistic interpretation and may thus lead to a deeper understanding of the feedforward process as a whole. Other future research avenues include researching the LaLU, xFL(x), L \u223c Laplace(0, 1), as this may encourage neuron sparsity. Another avenue is finding a useful way to apply the SOI map across many different tasks because it remains unclear how best to modify the extent to which the SOI map regularizes. Fortunately, the GELU does not require any apparent adjustment to exceed the accuracy of previous nonlinearities."}], "references": [{"title": "Learning with pseudo-ensembles", "author": ["Philip Bachman", "Ouais Alsharif", "Doina Precup"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["Djork-Arne Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "All You Need Is a Good Init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "In contrast, the widely-used ReLU activation usually demonstrates superior convergence yet is lacking in terms of its probabilistic interpretation [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "For example, dropout randomly sets neuron values to zero, and zoneout randomly preserves previously computed neuron values [10, 5].", "startOffset": 123, "endOffset": 130}, {"referenceID": 0, "context": "But this determinism of the ReLU is limiting; the stochasticity of dropout and zoneout may allow for a pseudo-ensemble [1] of networks with stochastic width or stochastic depth, respectively, and this leads to improved performance [3, 11].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "The GELU bears semblance to the ReLU and ELU (see [2] for an ELU description).", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "We do not evaluate the LReLU because of its similarity to ReLUs (see [6] for a description of LReLUs).", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "001 [4].", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "The weights are uniformly initialized on the unit hypersphere as a consequence of our argument in the previous section; this has positive impact on each nonlinearity\u2019s performance [7, 9].", "startOffset": 180, "endOffset": 186}, {"referenceID": 6, "context": "The weights are uniformly initialized on the unit hypersphere as a consequence of our argument in the previous section; this has positive impact on each nonlinearity\u2019s performance [7, 9].", "startOffset": 180, "endOffset": 186}], "year": 2017, "abstractText": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic process which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "creator": "LaTeX with hyperref package"}}}