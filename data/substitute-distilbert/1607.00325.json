{"id": "1607.00325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "abstract": "members propose a novel deep learning model, which supports permutation invariant training ( pit ), for speaker independent multi - talker speech separation, commonly known as the cocktail - party problem. different from most of the prior arts that treat behavior separation as a multi - class regression case and also deep selection technique that considers it a random ( or clustering ) problem, ap model optimizes for the separation regression theory, ignoring the order of mixing sources. this strategy cleverly addresses the long - lasting label permutation problem that has prevented progress on functional matching based techniques for speech separation. experiments on the equal - energy mixing setup of a danish corpus confirms the effectiveness over pit. people believe improvements built upon pit can eventually solve the fork - party problem and enable real - world realization of, e. g., automatic meeting transcription and lone - party human - computer extraction, where cooperative speech is common.", "histories": [["v1", "Fri, 1 Jul 2016 17:34:16 GMT  (226kb,D)", "http://arxiv.org/abs/1607.00325v1", "9 pages"], ["v2", "Tue, 3 Jan 2017 19:57:37 GMT  (131kb,D)", "http://arxiv.org/abs/1607.00325v2", "5 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["dong yu", "morten kolb{\\ae}k", "zheng-hua tan", "jesper jensen"], "accepted": false, "id": "1607.00325"}, "pdf": {"name": "1607.00325.pdf", "metadata": {"source": "CRF", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "authors": ["Dong Yu", "Morten Kolb\u00e6k", "Zheng-Hua Tan", "Jesper Jensen"], "emails": ["dongyu@microsoft.com", "mok@es.aau.dk}", "zt@es.aau.dk}", "jje@es.aau.dk}"], "sections": [{"heading": "1 Introduction", "text": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32]. Just six years ago, the word error rate (WER) on the widely accepted Switchboard conversational transcription benchmark task was over 20% and today it has been reduced to below 7% [19].\nDespite the significant progress made in dictating single-speaker speech, the progress made in multitalker mixed speech separation and recognition, often referred to as the cocktail-party problem [2, 3], has been less impressive. Although human listeners can easily perceive separate sources in an acoustic mixture, the same task seems to be extremely difficult for automatic computing systems, especially when only a single microphone recording of the mixed-speech is available [5, 28].\nNevertheless, solving the cocktail-party problem is critical to enable scenarios such as automatic meeting and lecture transcription, automatic captioning for audio/video recordings (e.g., Youtube), and multi-party human-machine interactions (e.g., in the world of Internet of things (IoT)), where speech overlapping is commonly observed. It could also pave the way for a new processing paradigm for smart, wearable, communication devices such as hearing aids.\nOver the decades, many attempts have been made to attack this problem. Before the deep learning era, the most popular technique was computational auditory scene analysis (CASA) [4, 7]. In this approach, certain segmentation rules based on perceptual grouping cues [30] are (often semimanually) designed to operate on low-level features to estimate a time-frequency mask that isolates the signal components belonging to different speakers. This mask is then used to reconstruct the signal. Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.\nar X\niv :1\n60 7.\n00 32\n5v 1\n[ cs\n.C L\n] 1\nJ ul\nBoth CASA and NMF led to very limited success in separating different sources in multi-talker mixed speech [5]. The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics. Unfortunately this model assumes and only works under closed-set speaker condition. In addition, inference in model based approaches is hardly scalable to a larger task with either more speakers or larger vocabulary.\nMotivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years. Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31]. The networks are optimized to predict the source belonging to the target class, usually for each time-frequency bin. Unfortunately, these works often focus on, and only work for, separating speech from (often challenging) background noise (or music). Since speech has very different characteristics than noise/music, this can be a much easier task than separating multiple talkers. Note that, there are indeed works that are aiming at separating multi-talker mixed speech (e.g., [14]). However, these works rely on speaker-dependent models by assuming that the (often few) target speakers are known during training, and they often work only on limited vocabulary and grammar.\nThe difficulty in speaker-independent multi-talker speech separation comes from the label ambiguity or permutation problem which we will describe in detail in Section 2. To the best of our knowledge, only two deep leaning based works [11, 28] have tried to address and solve this harder problem. In Weng et al. [28], which achieved the best result on the dataset used in 2006 monaural speech separation and recognition challenge [5], the instantaneous energy was used to solve the label ambiguity problem and a two-speaker joint-decoder with speaker switching penalty was used to separate and trace speakers. This approach tightly couples with the decoder and is difficult to scale up to more than two speakers due to the way labels are determined. Hershey et al. [11] made significant progress in solving the cocktail-party problem. In their work, they trained an embedding for each time-frequency bin to optimize a segmentation (clustering) criterion. During evaluation, each time-frequency bin was first mapped into the embedding space upon which a clustering algorithm was used to generate a partition of the time-frequency bins. Impressively, their systems trained on two-speaker mixed-speech perform well on three-speaker mixed-speech. However, their approach has two major drawbacks. First, it assumes that each time-frequency bin belongs to only one speaker (i.e., a partition) due to the clustering step. Although this is a valid assumption on tasks such as image segmentation [13, 23], it\u2019s only a very rough approximation on speech separation. Second, the clustering step is not jointly trained with the embedding. Both these drawbacks limit how good the system can perform.\nIn this paper, we propose a novel deep learning model for speaker independent multi-talker speech separation. The key ingredient of our model is permutation invariant training (PIT). Most prior arts treat speech separation as either a multi-class regression problem or a segmentation (or clustering) problem. Our model, however, considers it as a separation problem by optimizing for the separation regression error, ignoring the order of mixing sources during training. More specifically, our model first determines the best output-target assignment and then minimizes the error given the assignment. This strategy, which is directly implemented inside the network structure, elegantly solves the longlasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Furthermore, our proposed algorithm is not limited to a particular number of mixed talkers - it works, in principle, for any number of mixed-talkers.\nWe evaluated our novel model on the hardest setup of a Danish corpus in which equal energy speech signals are mixed. Experimental results indicate that the proposed multi-talker speech separation technique performs very well on speakers unseen in the training process. Even more exciting, the system trained on Danish can separate English speech sources well. In fact, during the training process our model learned acoustic cues, which are both speaker and language independent, for source separation, similar to humans. We believe improvements built upon PIT can eventually solve the cocktail-party problem."}, {"heading": "2 Speech separation problem", "text": "Although our approach is applicable to multi-channel speech signals, in this study we focus on monaural speech separation, which is more challenging and more widely deployable.\nThe goal of monaural speech separation is to estimate the individual source signals in a linearly mixed, single-microphone signal, in which the source signals generally overlap in the time-frequency domain. Let us denote the S source signal sequences in the time domain as xs(t), s = 1, \u00b7 \u00b7 \u00b7 , S and the mixed signal sequence as y(t) = \u2211S s=1 xs(t). The corresponding short time Fourier\ntransformation (STFT) of these signals are Xs(t, f) and Y(t, f) = \u2211S\ns=1 Xs(t, f), respectively, for each time t and frequency f . Given Y(t, f), the goal of monaural speech separation is to recover each source Xs(t, f). In many application scenarios, it is often sufficient to recover the top two or three high-energy mixing sources and treat the remaining low-energy sources as noise.\nAlthough the signal recovery can be carried out, and our approach can work, in the complex-valued STFT domain, we follow the convention and apply our technique on STFT magnitude spectra. In other words, our processing does not involve phase information, and the STFT phase spectrum of the mixed signal is used in recovering time domain waveforms of the sources.\nObviously, given only the magnitude of the mixed spectrum |Y(t, f)|, the problem of recovering |Xs(t, f)| is ill-posed, as there are an infinite number of possible |Xs(t, f)| combinations that lead to the same |Y(t, f)|. To overcome this basic problem, the system has to learn from some training set S that contains pairs of |Y(t, f)| and |Xs(t, f)| to look for regularities. More specifically, we train a deep learning model g(\u00b7) such that g (f(|Y)|) ; \u03b8) = |X\u0303s|, s = 1, \u00b7 \u00b7 \u00b7 , S, where \u03b8 is a model parameter vector, and f(|Y|) is some feature representation of |Y|. For simplicity and clarity we have omitted, and will continue to omit, time-frequency indexes when there is no ambiguity.\nIt is well-known (e.g., [26]) that better results can be achieved if, instead of estimating |Xs| directly, we first estimate a set of masks Ms(t, f) using a deep learning model h (f(|Y|; \u03b8)) = M\u0303s(t, f) with the constraint that M\u0303s(t, f) \u2265 0 and \u2211S s=1 M\u0303s(t, f) = 1 for all time-frequency bins (t, f). This constraint can be easily satisfied with the softmax operation. We then estimate |Xs| as |X\u0303s| = M\u0303s \u25e6 |Y|, where \u25e6 is the element-wise product of two operands. This strategy is adopted in this study.\nNote that since we first estimate masks, the model parameters can be optimized to minimize the mean square error (MSE) between the estimated mask M\u0303s and the ideal mask Ms = |Xs| |Y| ,\nJm = 1\nT \u00d7 F \u00d7 S S\u2211 s=1 \u2016M\u0303s \u2212Ms\u20162,\nwhere T and F denote the number of time frames and frequency bins, respectively. This approach comes with two problems. First, in silence segments, |Xs| = 0 and |Y| = 0, so that Ms is not well defined. Second, what we really care about is the error between the estimated magnitude and the true magnitude of each source, while a smaller error on masks may not lead to a smaller error on magnitude.\nTo overcome these limitations, recent works[26] directly minimize the MSE\nJx = 1\nT \u00d7 F \u00d7 S S\u2211 s=1 \u2016 \u02dc|Xs| \u2212 |Xs|\u2016 2\nbetween the estimated magnitude and the true magnitude. Note that in silence segments |Xs| = 0 and |Y| = 0, and so the accuracy of mask estimation does not affect the training criterion for those segments. In this study, we estimate masks M\u0303s which minimize Jx."}, {"heading": "3 Permutation invariant training", "text": "Except for Hershey et al.\u2019s work [11], all other recent speech separation works use the architecture depicted in Figure 1 in which a two-talker condition is illustrated. In this architecture, N frames of feature vectors of the mixed signal |Y| are used as the input to some deep learning models, such as deep neural networks (DNNs), convolutional neural networks (CNNs), and long short-term memory (LSTM) recurrent neural networks (RNNs), to generate one (often the center) frame of masks for each talker. These masks are then used to construct one frame of single-source speech |X\u03031| and |X\u03032|, for source 1 and 2, respectively.\nDuring training we need to provide the correct reference (or target) magnitude |X1| and |X2| to the corresponding output layers for supervision. Since the model has multiple output layers, one for each mixing source, and they depend on the same input mixture, reference assigning can be tricky esp. if the training set contains many utterances spoken by many speakers. This problem is referred to as the label ambiguity problem in [28] and label permutation problem in [11]. Due to this problem, prior arts perform poorly on speaker-independent multi-talker speech separation.\nThe solution proposed in this work is illustrated in Figure 2. There are two key inventions in this novel model: permutation invariant training (PIT) and segment-based decision making.\nComparing Figures 1 and 2 we can notice that in our new model the reference source streams are given as a set instead of an ordered list. In other words, the same training result is obtained, no matter in which order these sources are listed. This behavior is achieved with PIT highlighted inside the\ndashed rectangular in Figure 2. In order to associate references to the output layers, we first compute the pairwise MSE between each reference |Xs| and the estimated source |X\u0303s|. We then determine the (total number of S!) possible assignments between the references and the estimated sources, and compute the total MSE for each assignment. The assignment with the least MSE is chosen and the model is optimized to reduce this least MSE. In other words we simultaneously conduct label assignment and error evaluation. With PIT, we optimize for the separation accuracy and choose whatever label assignment that can lead to lower separation error.\nThe main information used to determine the separation is the input of the network which is the mixed signal. Often, better results can be achieved by using as input N successive frames (i.e., an input meta-frame) of features to the network for each shift of frame so that the contextual information can be exploited. However, exploiting contextual information in the input alone is not sufficient. Due to the correlation between the time-frequency bins, further improvement may be achieved if we estimate M > 1 frames of the separated speech for each meta-frame of input so that the assignment decision is made on a segment instead of one frame of reconstructed speech. Estimating multiple frames of the separated speech (i.e., an output meta-frame) also allows us to trace the same speaker during inference which we will describe in Section 4."}, {"heading": "4 Speech separation and tracing", "text": "During inference, the only information available is the mixed speech. Speech separation can be directly carried out for each input meta-frame, for which an output meta-frame with M frames of speech is estimated for each stream. The input meta-frame is then shifted by one (or more) frames. Since the output meta-frames have overlap, we can trace the speech by selecting the assignment that minimizes MSE on the overlapping frames. This simple speaker tracing algorithm mainly depends on the MSE between overlapping frames, we can improve the tracing accuracy by optimizing not only the reconstruction error as described in Section 3, but also the MSE between a subset (e.g., the center) of the overlapped frames, under the multi-task optimization framework.\nFurther tracing accuracy improvement can be achieved by mapping each estimated source into an embedding space and optimizing for the correlation between frames from the same source. For evaluation, we assume that reference signals |X1| and |X2| are available, from which we can estimate the assignment.\nOnce the relationship between the outputs and source streams are determined for each output metaframe, the separated speech can be estimated, taking into account all meta-frames. The simplest approach is picking the center frame in each meta-frame that is corresponding to the same source, or directly concatenating the output of the non-overlapping output meta-frames. Since the same frame is contained in M meta-frames, another solution is to average over meta-frames, potentially weighted based on the distance of the corresponding frame to the center frame in each meta-frame."}, {"heading": "5 Related work", "text": "There are only two prior arts that are closely related to our work. In Weng et al.\u2019s work [28], the label ambiguity problem is alleviated by using instantaneous energy as the cue. This approach carries with three problems. First, in many cases instantaneous energy is not sufficient to separate sources and thus the system performs poorly for those cases. In our approach, however, the PIT algorithm automatically determines the best cues for separation for each mixture. Second, it is difficult to extend the instantaneous energy based approach to more than two sources. PIT, however, does not have this limitation. Third, because instantaneous energy changes frequently and there is no additional cues to determine the streams, a complicated non-scalable joint-decoder is used for speaker tracing. Using PIT, however, it\u2019s possible to build tracing mechanisms directly into the model.\nAnother related work is the deep clustering algorithm by Hershey et al. [11], which estimates the likelihood that two time-frequency bins belong to the same speaker. A clustering algorithm (e.g., k-means) is then used to cluster the time-frequency bins into partitions, each of which represents a source. While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem. This view carries the limitation that each time-frequency bin belongs to one and only one source, while in fact each bin is a mixture of multi-talker speech. Our approach sits between the approach in [11] and\nother prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem. While it is difficult to incorporate, for example, complex masks into the system in [11], it is straight-forward in our model. In addition, the approach in [11] requires a post-DNN clustering step which cannot be jointly trained easily. In our model, however, all components are jointly trained end-to-end."}, {"heading": "6 Experimental results", "text": "We have evaluated our approach on a Danish corpus [18] which consists of approximately 560 speakers each speaking 312 utterances. The utterances are a mix of normal sentences, commands and sequences of numbers. The average utterance duration is approximately 5 seconds.\nThe training requires both the mixed speech and each mixing source. Following other works (e.g., [11]) in this space, the training data were mixed artificially in our experiments. There are various ways to mix data from different sources. Since for both humans and machines the most difficult setup is when the mixing sources have the same energy (i.e, 0 dB) [5, 28], our evaluation focused on this condition. When the mixing utterances have different lengths the shorter ones were padded with small noise. We used a 257-dimensional STFT magnitude spectrum as the target and input feature to the network.\nOur model was implemented using the computational network toolkit (CNTK) [8]. In all our experiments we used simple feed-forward DNNs with three hidden layers each with 1024 units, instead of more powerful CNNs [1, 17] and LSTMs. We show that even with this simple model, our approach can already perform surprisingly well, indicating the effectiveness of the proposed solution."}, {"heading": "6.1 Training behavior", "text": "In Figure 3 we plotted the training progress as measured by the MSE on the training and validation set with conventional training and PIT. From the figure we can see clearly that training goes nowhere with the conventional approach no matter how we adjust hyper-parameters due to the label permutation problem discussed in [11, 28]. In contrast, training converges quickly to very small MSE for both two- and three-talker mixed speech when PIT is used."}, {"heading": "6.2 Signal-to-distortion ratio improvement", "text": "We further evaluated PIT on its potential to improve the signal-to-distortion ratio (SDR) [24], a metric widely used to evaluate speech enhancement performance, on a two-talker mixed-speech dataset. The dataset was constructed by randomly selecting a set of 45 male and 45 female speakers from the Danish corpus, and then allocating 232, 40, and 40 utterances from each speaker to generate mixed speech in the training, validation and closed-condition (CC) (seen speaker) test set, respectively. 40 utterances from each of another 45 male and 45 female speakers were randomly selected to construct the open-condition (OC) (unseen speaker) test set. To mix the speech for each set, we simply draw, at random, two utterances, from two different speakers and mix them. In this way we get same-gender (SG) and opposite-gender (OG) mixtures with 50/50 chance. We constructed 10k and 1k mixtures in total in the training and validation set, respectively, and 1k mixtures for each of the CC and OC test sets. Finally, an 1k mixture test set was constructed using 45 male and 45 female speakers, each with 140 utterances, from the si_tr_s part of the Wall Street Journal (WSJ0) [9] English corpus. This test set is used to evaluate the performance of the system on an unseen language.\nIn Table 1 we summarized the SDR improvement in dB from different separation configurations for same-gender (SG) and opposite-gender (OG) two-talker mixed speech in closed condition (CC), open condition (OC) and open language (WSJ0) datasets. In these experiments each frame was reconstructed by averaging over all output meta-frames that contain the same frame. Similar to [11] we have used the true mixing sources to determine the correct assignment during evaluation.\nFrom the table we can make several observations. First, better SDR improvement can be achieved by estimating less output frames. However, we achieve better results on five frames over one and three frames. Since using five output frames has the additional benefit of providing additional information for reconstruction it is the preferred configuration. Second, an input window of 31 frames seems to be sufficient to achieve best results for the simple DNN used in the experiments. Third, The performance is almost always better when separating opposite-gender than same-gender mixed-speech and the difference increases as the output window size increases. Fourth, the system performs better on the closed condition than the open condition. However, the gap is so small that the difference may be ignored. Last, although the system has never seen English speech, it can improve SDR by over 9dB in the best condition. This best improvement is better than that reported in [11] which was trained on English data, used more powerful BLSTMs, and evaluated on simpler setups with SNRs across 0-10 dB. These results generally indicate that the system can learn some complicated time-frequency bin grouping cues automatically and can perform robustly across speakers and languages."}, {"heading": "7 Conclusion and discussion", "text": "In this paper, we have described a novel permutation invariant training technique for speakerindependent multi-talker speech separation. To the best of our knowledge this is the first successful work that employs the separation view of the task, instead of the multi-class regression or segmentation view that are used in prior arts but with intrinsic limitations. This is a big step toward solving the important cocktail-party problem in a real-world setup, where the set of speakers are unknown during the training time.\nOur experiments on the equal-energy setup of two-talker mixed speech separation tasks indicate that PIT trained models generalize well to unseen speakers and languages. The key insight and idea in this work can be applied to many tasks where symmetry is a property.\nThe purpose of this paper is to describe the key and novel technique that enables training for the separation of multi-talker speech. The overall system can be improved in many ways. For example, we can use deep CNNs/LSTMs to increase the modeling power and add more training data to improve generalization. In addition, since the acoustic cues learned by the model is speaker and language independent, it\u2019s possible for us to train a universal speech separation model using speech in various languages and noise under different conditions. For real-world deployment, more powerful speaker tracing algorithms need to be developed esp. for long utterances.\nAlthough we reported our experiments on monaural speech separation, the same technique can be deployed in the multi-channel setup and combined with techniques such as beam-forming. In fact, since beam-forming and PIT separate speech with different information, they complement with each other. Further more, in the multi-channel setup, the training data for PIT can be automatically generated since each microphone gets the mixed speech, and the mixture sources can be estimated using beam-forming techniques."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Auditory scene analysis: The perceptual organization of sound", "author": ["Albert S. Bregman"], "venue": "MIT press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Some experiments on the recognition of speech, with one and with two ears", "author": ["E. Colin Cherry"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1953}, {"title": "Modelling auditory processing and organisation, volume 7", "author": ["Martin Cooke"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Monaural speech separation and recognition challenge", "author": ["Martin Cooke", "John R. Hershey", "Steven J. Rennie"], "venue": "Computer Speech and Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E. Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Prediction-driven computational auditory scene analysis", "author": ["Daniel P.W. Ellis"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Amit"], "venue": "Technical report, Microsoft Technical Report MSR-TR-2014-112,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "CSR-I (WSJ0) Complete LDC93S6A", "author": ["Garofolo", "John"], "venue": "Philadelphia: Linguistic Data Consortium,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Factorial hidden markov models", "author": ["Zoubin Ghahramani", "Michael I. Jordan"], "venue": "Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["John R. Hershey", "Zhuo Chen", "Jonathan Le Roux", "Shinji Watanabe"], "venue": "arXiv preprint arXiv:1508.04306,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Deep embedding network for clustering", "author": ["Peihao Huang", "Yan Huang", "Wei Wang", "Liang Wang"], "venue": "In ICPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Joint optimization of masks and deep recurrent neural networks for monaural source separation", "author": ["Po-Sen Huang", "Minje Kim", "Mark Hasegawa-Johnson", "Paris Smaragdis"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Super-human multi-talker speech recognition: the ibm 2006 speech separation challenge system", "author": ["Trausti T. Kristjansson", "John R. Hershey", "Peder A. Olsen", "Steven J. Rennie", "Ramesh A. Gopinath"], "venue": "In INTERSPEECH,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Sparse nmf\u2013half-baked or well done", "author": ["Jonathan Le Roux", "Felix Weninger", "J Hershey"], "venue": "Mitsubishi Electric Research Labs (MERL),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "The ibm 2016 english conversational telephone speech recognition system", "author": ["George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J. Kuo"], "venue": "arXiv preprint arXiv:1604.08242,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Single-channel speech separation using sparse non-negative matrix factorization", "author": ["Mikkel N. Schmidt", "Rasmus Kongsgaard Olsson"], "venue": "In INTERSPEECH,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Convolutive speech bases and their application to supervised speech separation", "author": ["Paris Smaragdis"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Learning deep representations for graph clustering", "author": ["Fei Tian", "Bin Gao", "Qing Cui", "Enhong Chen", "Tie-Yan Liu"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Speech recognition using factorial hidden markov models for separation in the feature space", "author": ["Tuomas Virtanen"], "venue": "In INTERSPEECH. Citeseer,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "On training targets for supervised speech separation", "author": ["Yuxuan Wang", "Arun Narayanan", "DeLiang Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Monaural speech separation using source-adapted models", "author": ["Ron J. Weiss", "Daniel P.W. Ellis"], "venue": "In Applications of Signal Processing to Audio and Acoustics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Deep neural networks for single-channel multi-talker speech recognition", "author": ["Chao Weng", "Dong Yu", "Michael L. Seltzer", "Jasha Droppo"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr", "author": ["Felix Weninger", "Hakan Erdogan", "Shinji Watanabe", "Emmanuel Vincent", "Jonathan Le Roux", "John R. Hershey", "Bj\u00f6rn Schuller"], "venue": "In Latent Variable Analysis and Signal Separation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Yong Xu", "Jun Du", "Li-Rong Dai", "Chin-Hui Lee"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition", "author": ["Dong Yu", "Li Deng", "George E. Dahl"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 11, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 19, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 29, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 17, "context": "Just six years ago, the word error rate (WER) on the widely accepted Switchboard conversational transcription benchmark task was over 20% and today it has been reduced to below 7% [19].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "Despite the significant progress made in dictating single-speaker speech, the progress made in multitalker mixed speech separation and recognition, often referred to as the cocktail-party problem [2, 3], has been less impressive.", "startOffset": 196, "endOffset": 202}, {"referenceID": 2, "context": "Despite the significant progress made in dictating single-speaker speech, the progress made in multitalker mixed speech separation and recognition, often referred to as the cocktail-party problem [2, 3], has been less impressive.", "startOffset": 196, "endOffset": 202}, {"referenceID": 4, "context": "Although human listeners can easily perceive separate sources in an acoustic mixture, the same task seems to be extremely difficult for automatic computing systems, especially when only a single microphone recording of the mixed-speech is available [5, 28].", "startOffset": 249, "endOffset": 256}, {"referenceID": 26, "context": "Although human listeners can easily perceive separate sources in an acoustic mixture, the same task seems to be extremely difficult for automatic computing systems, especially when only a single microphone recording of the mixed-speech is available [5, 28].", "startOffset": 249, "endOffset": 256}, {"referenceID": 3, "context": "Before the deep learning era, the most popular technique was computational auditory scene analysis (CASA) [4, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "Before the deep learning era, the most popular technique was computational auditory scene analysis (CASA) [4, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 15, "context": "Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.", "startOffset": 40, "endOffset": 52}, {"referenceID": 18, "context": "Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.", "startOffset": 40, "endOffset": 52}, {"referenceID": 20, "context": "Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.", "startOffset": 40, "endOffset": 52}, {"referenceID": 4, "context": "Both CASA and NMF led to very limited success in separating different sources in multi-talker mixed speech [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 87, "endOffset": 99}, {"referenceID": 23, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 87, "endOffset": 99}, {"referenceID": 25, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 87, "endOffset": 99}, {"referenceID": 9, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 11, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 19, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 29, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 13, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 24, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 27, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 28, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 13, "context": ", [14]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "To the best of our knowledge, only two deep leaning based works [11, 28] have tried to address and solve this harder problem.", "startOffset": 64, "endOffset": 72}, {"referenceID": 26, "context": "To the best of our knowledge, only two deep leaning based works [11, 28] have tried to address and solve this harder problem.", "startOffset": 64, "endOffset": 72}, {"referenceID": 26, "context": "[28], which achieved the best result on the dataset used in 2006 monaural speech separation and recognition challenge [5], the instantaneous energy was used to solve the label ambiguity problem and a two-speaker joint-decoder with speaker switching penalty was used to separate and trace speakers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[28], which achieved the best result on the dataset used in 2006 monaural speech separation and recognition challenge [5], the instantaneous energy was used to solve the label ambiguity problem and a two-speaker joint-decoder with speaker switching penalty was used to separate and trace speakers.", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "[11] made significant progress in solving the cocktail-party problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Although this is a valid assumption on tasks such as image segmentation [13, 23], it\u2019s only a very rough approximation on speech separation.", "startOffset": 72, "endOffset": 80}, {"referenceID": 21, "context": "Although this is a valid assumption on tasks such as image segmentation [13, 23], it\u2019s only a very rough approximation on speech separation.", "startOffset": 72, "endOffset": 80}, {"referenceID": 24, "context": ", [26]) that better results can be achieved if, instead of estimating |Xs| directly, we first estimate a set of masks Ms(t, f) using a deep learning model h (f(|Y|; \u03b8)) = M\u0303s(t, f) with the constraint that M\u0303s(t, f) \u2265 0 and \u2211S s=1 M\u0303s(t, f) = 1 for all time-frequency bins (t, f).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "To overcome these limitations, recent works[26] directly minimize the MSE", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "\u2019s work [11], all other recent speech separation works use the architecture depicted in Figure 1 in which a two-talker condition is illustrated.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "This problem is referred to as the label ambiguity problem in [28] and label permutation problem in [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "This problem is referred to as the label ambiguity problem in [28] and label permutation problem in [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "\u2019s work [28], the label ambiguity problem is alleviated by using instantaneous energy as the cue.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "[11], which estimates the likelihood that two time-frequency bins belong to the same speaker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 24, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 27, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 28, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 10, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Our approach sits between the approach in [11] and", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 24, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 27, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 28, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 10, "context": "While it is difficult to incorporate, for example, complex masks into the system in [11], it is straight-forward in our model.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "In addition, the approach in [11] requires a post-DNN clustering step which cannot be jointly trained easily.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": ", [11]) in this space, the training data were mixed artificially in our experiments.", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "e, 0 dB) [5, 28], our evaluation focused on this condition.", "startOffset": 9, "endOffset": 16}, {"referenceID": 26, "context": "e, 0 dB) [5, 28], our evaluation focused on this condition.", "startOffset": 9, "endOffset": 16}, {"referenceID": 7, "context": "Our model was implemented using the computational network toolkit (CNTK) [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "In all our experiments we used simple feed-forward DNNs with three hidden layers each with 1024 units, instead of more powerful CNNs [1, 17] and LSTMs.", "startOffset": 133, "endOffset": 140}, {"referenceID": 16, "context": "In all our experiments we used simple feed-forward DNNs with three hidden layers each with 1024 units, instead of more powerful CNNs [1, 17] and LSTMs.", "startOffset": 133, "endOffset": 140}, {"referenceID": 10, "context": "From the figure we can see clearly that training goes nowhere with the conventional approach no matter how we adjust hyper-parameters due to the label permutation problem discussed in [11, 28].", "startOffset": 184, "endOffset": 192}, {"referenceID": 26, "context": "From the figure we can see clearly that training goes nowhere with the conventional approach no matter how we adjust hyper-parameters due to the label permutation problem discussed in [11, 28].", "startOffset": 184, "endOffset": 192}, {"referenceID": 22, "context": "We further evaluated PIT on its potential to improve the signal-to-distortion ratio (SDR) [24], a metric widely used to evaluate speech enhancement performance, on a two-talker mixed-speech dataset.", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "Finally, an 1k mixture test set was constructed using 45 male and 45 female speakers, each with 140 utterances, from the si_tr_s part of the Wall Street Journal (WSJ0) [9] English corpus.", "startOffset": 168, "endOffset": 171}, {"referenceID": 10, "context": "Similar to [11] we have used the true mixing sources to determine the correct assignment during evaluation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "This best improvement is better than that reported in [11] which was trained on English data, used more powerful BLSTMs, and evaluated on simpler setups with SNRs across 0-10 dB.", "startOffset": 54, "endOffset": 58}], "year": 2016, "abstractText": "We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common.", "creator": "LaTeX with hyperref package"}}}