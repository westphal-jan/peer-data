{"id": "1608.04414", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back", "abstract": "in stochastic convex optimization the goal consists to minimize a convex function $ f ( x ) \\ doteq { \\ quad t } _ { { \\ mathbf f } \\ sim d } [ { \\ mathbf f } ( x ) ] $ over a convex set $ \\ cal k \\ subset { \\ mathbb r } ^ d $ where $ d $ uses some regular distribution and each $ f ( \\ cdot ) $ in the support of $ d $ l convex over $ \\ cal k $. the optimization is commonly based on i. i. ing. ~ \u00ab $ f ^ 1, f ^ 2, \\ ldots, f ^ n $ from $ d $. a standard approach to such problems is empirical risk minimization ( erm ) that optimizes $ f _ bf ( x ) \\ doteq \\ frac { 1 } { n } \\ sum _ { i \\ leq n } f ^ f1 ( x ) $. now we assert immediate question of because multiple samples are configured for erm to succeed because any closely related question of uniform convergence over $ f _ s $ 4 $ f $ over $ \\ cal k $. we demonstrate that in the standard $ \\ var _ p / \\ ell _ q $ setting of ground - bounded quantities over. $ \\ < k $ of bounded radius, erm @ global size accuracy scales linearly with the dimension $ d $. this nearly matches standard upper bounds and consistent through $ \\ omega ( \\ log d ) $ dependence proved for $ \\ ell _ 2 / \\ m _ 2 $ setting by perez - shwartz et al. ( 2009 ). in geometric contrast, these needs can be solved using dimension - independent number of cases for $ \\ ell _ 2 / \\ ell _ 2 $ setting $ $ \\ log d $ dependence for $ \\ le _ 1 / \\ ell _ \\ infty $ setting using other fields. we also demonstrate that for a more general class of range - ordered ( but not lipschitz - bounded ) stochastic convex programs an even loop gap appears continuously utilizing criterion 2.", "histories": [["v1", "Mon, 15 Aug 2016 21:19:51 GMT  (17kb)", "http://arxiv.org/abs/1608.04414v1", null], ["v2", "Fri, 28 Oct 2016 00:46:58 GMT  (20kb)", "http://arxiv.org/abs/1608.04414v2", "Added a lower bound construction based on efficiently computable functions"], ["v3", "Mon, 26 Dec 2016 06:37:48 GMT  (299kb,D)", "http://arxiv.org/abs/1608.04414v3", "Added illustrations of functions used in some of the constructions"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["vitaly feldman"], "accepted": true, "id": "1608.04414"}, "pdf": {"name": "1608.04414.pdf", "metadata": {"source": "CRF", "title": "Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back", "authors": ["Vitaly Feldman"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n04 41\n4v 1\n[ cs\n.L G\n] 1\n5 A\nug 2\nIn stochastic convex optimization the goal is to minimize a convex function F (x) . = Ef\u223cD[f(x)] over a convex set K \u2282 Rd where D is some unknown distribution and each f(\u00b7) in the support of D is convex over K. The optimization is commonly based on i.i.d. samples f1, f2, . . . , fn from D. A standard approach to such problems is empirical risk minimization (ERM) that optimizes FS(x) . = 1 n \u2211 i\u2264n f i(x). Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of FS to F over K. We demonstrate that in the standard \u2113p/\u2113q setting of Lipschitz-bounded functions over a K of bounded radius, ERM requires sample size that scales linearly with the dimension d. This nearly matches standard upper bounds and improves on \u2126(log d) dependence proved for \u21132/\u21132 setting in [SSSS09]. In stark contrast, these problems can be solved using dimension-independent number of samples for \u21132/\u21132 setting and log d dependence for \u21131/\u2113\u221e setting using other approaches. We also demonstrate that for a more general class of rangebounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2."}, {"heading": "1 Introduction", "text": "Numerous central problems in machine learning, statistics and operations research are special cases of stochastic optimization from i.i.d. data samples. In this problem the goal is to optimize the value of the expected function F (x) = Ef\u223cD[f(x)] over some set K given i.i.d. samples f1, f2, . . . , fn of f . For example, in supervised learning the set K consists of hypothesis functions from Z to Y and each sample is an example described by a pair (z, y) \u2208 (Z, Y ). For some fixed loss function L : Y \u00d7 Y \u2192 R, an example (z, y) defines a function from K to R given by f(z,y)(h) = L(h(z), y). The goal is to find a hypothesis h that (approximately) minimizes the expected loss relative to some distribution P over examples: E(z,y)\u223cP [L(h(z), y)] = E(z,y)\u223cP [f(z,y)(h)].\nHere we are interested in stochastic convex optimization (SCO) problems in which K is some convex subset of Rd and each function in the support of D is convex over K. The importance of this setting stems from the fact that such problems can be solved efficiently via a large variety of known techniques. Therefore in many applications even if the original optimization problem is not convex, it is replaced by a convex relaxation.\nA classic and widely-used approach to solving stochastic optimization problems is empirical risk minimization (ERM) also referred to as stochastic average approximation (SAA) in the optimization literature. In this approach, given a set of samples S = (f1, f2, . . . , fn) the empirical estimate of F : FS(x) . = 1 n \u2211 i\u2264n f i(x) is optimized (sometimes with an additional regularization term such as \u03bb\u2016x\u20162 for some \u03bb > 0). The question we address here is the number of samples required for this approach to work\ndistribution-independently. More specifically, for some fixed convex body K and fixed set of convex functions F over K, what is the smallest number of samples n such that for every probability distribution D supported on F , any algorithm that minimizes FS given n i.i.d. samples from D will produce an \u01eb-optimal solution x\u0302 to the problem (namely, F (x\u0302) \u2264 minx\u2208K F (x)+ \u01eb) with probability at least 1\u2212 \u03b4? We will refer to this number as the sample complexity of ERM for \u01eb-optimizing F over K (we will fix \u03b4 = 1/2 for now).\nThe sample complexity of ERM for \u01eb-optimizing F over K is lower bounded by the sample complexity of \u01eb-optimizing F over K, that is the number of samples that is necessary to find an \u01eb-optimal solution for any algorithm. On the other hand, it is upper bounded by the number of samples that ensures uniform convergence of FS to F . Namely, if with probability \u2265 1 \u2212 \u03b4, for all x \u2208 K, |FS(x) \u2212 F (x)| \u2264 \u01eb/2 then, clearly, any algorithm based on ERM will succeed. As a result, ERM and uniform convergence are the primary tool for analysis of the sample complexity of learning problems and are the key subject of study in statistical learning theory. Fundamental results in VC theory imply that in some settings, such as binary classification and least-squares regression, uniform convergence is also a necessary condition for learnability (e.g. [Vap98, SSBD14]) and therefore the three measures of sample complexity mentioned above nearly coincide.\nIn the context of stochastic convex optimization the study of sample complexity of ERM and uniform convergence was initiated in an insightful and influential work of Shalev-Shwartz, Shamir, Srebro and Sridharan [SSSS09]. They demonstrated that the relationships between these notions of sample complexity are substantially more delicate even in the most well-studied SCO settings. Specifically, let K be a unit \u21132 ball and F be the set of all convex sub-differentiable functions with Lipschitz constant relative to \u21132 bounded by 1 or, equivalently, \u2016\u2207f(x)\u20162 \u2264 1 for all x \u2208 K. Then, known algorithm for SCO imply that sample complexity of this problem is O(1/\u01eb2) and often expressed as 1/ \u221a n rate of convergence (e.g. [NJLS09, SSBD14]). On the other hand, Shalev-Shwartz et al. [SSSS09] show1 that the sample complexity of ERM for solving this problem with \u01eb = 1/2 is \u2126(log d). The only known upper bound for sample complexity of ERM is O\u0303(d/\u01eb2) and relies only on the uniform convergence of Lipschitz-bounded functions [SN05, SSSS09].\nAs can seen from this discussion, the work of Shalev-Shwartz et al. [SSSS09] still leaves a major gap between known bounds on sample complexity of ERM (and also uniform convergence) for this basic Lipschitzbounded \u21132/\u21132 setup. Another natural question is whether the gap is present in the popular \u21131/\u2113\u221e setup. In this setup K is a unit \u21131 ball (or in some cases a simplex) and \u2016\u2207f(x)\u2016\u221e \u2264 1 for all x \u2208 K. The sample complexity of SCO in this setup is \u03b8(log d/\u01eb2) (e.g. [NJLS09, SSBD14]) and therefore, even an appropriately modified lower bound in [SSSS09], does not imply any gap. More generally, the choice of norm can have a major impact on the relationship between these sample complexities and hence needs to be treated carefully. For example, for (the reversed) \u2113\u221e/\u21131 setting the sample complexity of the problem is \u03b8(d/\u01eb2) (e.g. [FGV15]) and nearly coincides with the number of samples sufficient for uniform convergence."}, {"heading": "1.1 Overview of Results", "text": "In this work we substantially strengthen the lower bound in [SSSS09] proving that a linear dependence on the dimension d is necessary for ERM (and, consequently, uniform convergence). We then extend the lower bound to all \u2113p/\u2113q setups and examine several related questions. Finally, we examine a more general setting of range-bounded SCO (that is |f(x)| \u2264 1 for all x \u2208 K). While the sample complexity of this setting is still low (for example O\u0303(1/\u01eb2) when K is an \u21132 ball) and efficient algorithms are known, we show that ERM might require an infinite number of samples already for d = 2. A (somewhat counterintuitive) conclusion from these lower bounds is that, from the point of view of generalization of ERM and uniform convergence,\n1The dependence on d is not stated explicitly but follows immediately from their analysis.\nconvexity does not reduce the sample complexity in the worst case.\nOur basic construction is fairly simple and its analysis is inspired by the technique in [SSSS09]. It is based on functions of the form maxv\u2208V \u3008v, x\u3009. Note that the maximum operator preserves both convexity and Lipschitz bound (relative to any norm). The distribution over the sets V that define such functions is uniform over all subsets of some set of vectors W of size 2d/6. Equivalently, each element of W is included in V with probability 1/2 independently of other elements in W . This implies that if the number of samples is less than d/6 then, with probability > 1/2, at least one of the vectors in W (say w) will not be observed in any of the samples. For an appropriate choice of W , this implies that FS can be minimized while maximizing \u3008w, x\u3009 (the maximum over the unit \u21132 ball is w). Note that a function randomly chosen from our distribution includes the term \u3008w, x\u3009 in the maximum operator with probability 1/2. Therefore the value of the expected function F at w is much larger than the minimum of F . In particular, there exists an ERM algorithm with generalization error of at least 1/4. The details of the construction appear in Sec. 3.1 and Thm. 3.3 gives the formal statement of the lower bound. We also show (see Thm. 3.5) that essentially the same construction gives the same lower bound for any \u2113p/\u2113q setup with 1/p + 1/q = 1.\nThe use of maximum operator results in functions that are highly non-smooth (that is, their gradient is not Lipschitz-bounded) whereas the construction in [SSSS09] uses smooth functions. Smoothness plays a crucial role in many algorithms for convex optimization (see [Bub15] for examples). It reduces the sample complexity of SCO in \u21132/\u21132 setup to O(1/\u01eb) when the smoothness parameter is a constant (e.g. [NJLS09, SSBD14]). Therefore it is natural to ask whether our strong lower bound holds for smooth functions as well. We describe a modification of our construction that proves a similar lower bound in the smooth case (with generalization error of 1/128). The main idea is to replace each linear function \u3008v, x\u3009 with some smooth function \u03bd(\u3008v, x\u3009) guaranteing that for different vectors v1, v2 \u2208 W and every x \u2208 K, only one of \u03bd(\u3008v1, x\u3009) and \u03bd(\u3008v2, x\u3009) can be non-zero. This allows to easily control the smoothness of maxv\u2208V \u03bd(\u3008v, x\u3009). The details of this construction appear in Sec. 3.2 and the formal statement in Thm. 3.7.\nAnother important contribution in [SSSS09] is the demonstration of the important role that strong convexity plays for generalization in SCO: Minimization of FS(x) + \u03bbR(x) ensures that ERM will have low generalization error whenever R(x) is strongly convex (for a sufficiently large \u03bb). This result is based on the proof that ERM of a strongly convex Lipschitz function is replace-one stable and the connection between such stability and generalization showed in [BE02] (see also [SSSSS10] for a detailed treatment of the relationship between generalization and stability).\nIt is natural to ask whether other approaches to regularization will ensure generalization. We demonstrate that for the commonly used \u21131 regularization the answer is negative. We prove this using a simple modification of our lower bound construction: We shift the functions to the positive orthant where the regularization terms \u03bb\u2016x\u20161 is just a linear function. We then subtract this linear function from each function in our construction, thereby balancing the regularization (while maintaining convexity and Lipschitz-boundedness). The details of this construction appear in Sec. 3.3 (see Thm. 3.8).\nFinally, we consider a more general class of range-bounded convex functions (note that the Lipschitz bound of 1 and the bound of 1 on the radius of K imply a bound of 1 on the range up to a constant shift which does not affect the optimization problem). While this setting is not as well-studied, efficient algorithms for it are known. For example, the online algorithm in a recent work of Rakhlin and Sridharan [RS15] together with standard online-to-batch conversion arguments [CCG04], imply that the sample complexity of this problem is O\u0303(1/\u01eb2) for any K that is an \u21132 ball (of any radius). For general convex bodies K, the problems can be solved via random walk-based approaches [BLNR15, FGV15] or an adaptation of the center-ofgravity method given in [FGV15]. Here we show that for this setting ERM might completely fail already for K being the unit 2-dimensional ball. The construction is based on ideas similar to those we used in the\nsmooth case and is formally described in Sec. 4."}, {"heading": "2 Preliminaries", "text": "For an integer n \u2265 1 let [n] .= {1, . . . , n}. Random variables are denoted by bold letters, e.g., f . Given p \u2208 [1,\u221e] we denote the ball of radius R > 0 in \u2113p norm by Bdp(R), and the unit ball by Bdp .\nFor a convex body (i.e., compact convex set with nonempty interior) K \u2286 Rd, we consider problems of the form\nmin K\n(FD) . = min\nx\u2208K\n{\nFD(x) . = E\nf\u223cD [f(x)]\n}\n,\nwhere f is a random variable defined over some set of convex, sub-differentiable functions F on K and distributed according to some unknown probability distribution D. We denote F \u2217 = minK(FD). For an approximation parameter \u01eb > 0 the goal is to find x \u2208 K such that FD(x) \u2264 F \u2217 + \u01eb and we call any such x an \u01eb-optimal solution. For an n-tuple of functions S = (f1, . . . , fn) we denote by FS . = 1n \u2211 i\u2208[n] f i.\nWe say that a point x\u0302 is an empirical risk minimum for an n-tuple S of functions over K, if FS(x\u0302) = minK(FS). In some cases there are many points that minimize FS and in this case we refer to a specific algorithm that selects one of the minimums of FS as an empirical risk minimizer. To make this explicit we refer to the output of such a minimizer by x\u0302(S) .\nGiven x \u2208 K, and a convex function f we denote by \u2207f(x) \u2208 \u2202f(x) an arbitrary selection of a subgradient. Let us make a brief reminder of some important classes of convex functions. Let p \u2208 [1,\u221e] and q = p\u2217 . = 1/(1 \u2212 1/p). We say that a subdifferentiable convex function f : K \u2192 R is in the class\n\u2022 F(K, B) of B-bounded-range functions if for all x \u2208 K, |f(x)| \u2264 B.\n\u2022 F0p (K, L) of L-Lipschitz continuous functions w.r.t. \u2113p, if for all x, y \u2208 K, |f(x)\u2212f(y)| \u2264 L\u2016x\u2212y\u2016p;\n\u2022 F1p (K, \u03c3) of functions with \u03c3-Lipschitz continuous gradient w.r.t. \u2113p, if for all x, y \u2208 K, \u2016\u2207f(x) \u2212 \u2207f(y)\u2016q \u2264 \u03c3\u2016x\u2212 y\u2016p.\nWe will omit p from the notation when p = 2."}, {"heading": "3 Lower Bounds for Lipschitz-Bounded SCO", "text": "In this section we present our main lower bounds for SCO of Lipschitz-bounded convex functions. For comparison purposes we start by formally stating some known bounds on sample complexity of solving such problems:\nUniform convergence upper bound: The following uniform convergence bounds can be easily derived from the standard covering number argument (e.g. [SN05, SSSS09])\nTheorem 3.1. For p \u2208 [1,\u221e], let K \u2286 Bdp(R) and let D be any distribution supported on functions L-Lipschitz on K relative to \u2113p (not necessarily convex). Then, for every \u01eb, \u03b4 > 0 and n \u2265 n1 = O (\nd\u00b7(LR)2\u00b7log(dLR/(\u01eb\u03b4)) \u01eb2\n)\nPr S\u223cDn\n[\u2203x \u2208 K, |FD(x)\u2212 FS(x)| \u2265 \u01eb] \u2264 \u03b4.\nAlgorithms: The following upper bounds on sample complexity of Lipschitz-bounded SCO can be obtained from several known algorithms [NJLS09, SSSS09] (see [SSBD14] for a textbook exposition for p = 2).\nTheorem 3.2. For p \u2208 [1, 2], let K \u2286 Bdp(R). Then, there is an algorithm Ap that given \u01eb, \u03b4 > 0 and n = np(d,R,L, \u01eb, \u03b4) i.i.d. samples from any distribution D supported on F0p (K, L), outputs an \u01eb-optimal solution to FD over K with probability \u2265 1\u2212 \u03b4. For p \u2208 (1, 2], np = O((LR/\u01eb)2 \u00b7 log(1/\u03b4)) and for p = 1, np = O((LR/\u01eb)\n2 \u00b7 log d \u00b7 log(1/\u03b4)). Stronger results are known under additional assumptions on smoothness and/or strong convexity (e.g. [NJLS09,\nRSS12, SZ13, BM13])."}, {"heading": "3.1 Non-smooth construction", "text": "We will start with a simpler lower bound for non-smooth functions. For simplicity, we will also restrict R = L = 1. Lower bounds for the general setting can be easily obtained from this case by scaling the domain and desired accuracy (see Thm. 3.10 for additional details).\nWe will need a set of vectors W \u2286 {\u22121, 1}d with the following property: for any distinct w1, w2 \u2208 W , \u3008w1, w2\u3009 \u2264 d/2. The Chernoff bound together with a standard packing argument imply that there exists a set W with this property of size \u2265 ed/8 \u2265 2d/6.\nFor any subset V of W we define a function\ngV (x) . = max{1/2,max\nw\u2208V \u3008w\u0304, x\u3009}, (1)\nwhere w\u0304 . = w/\u2016w\u2016 = w/ \u221a d. We first observe that gV is convex and 1-Lipschitz (relative to \u21132). This immediately follows from \u3008w\u0304, x\u3009 being convex and 1-Lipschitz for every w and gV being the maximum of convex and 1-Lipschitz functions. Theorem 3.3. Let K = Bd2 and we define H2 . = {gV | V \u2286 W} for gV defined in eq. (1). Let D be the uniform distribution over H2. Then for n \u2264 d/6 and every set of samples S there exists an ERM x\u0302(S) such that\nPr S\u223cDn\n[FD(x\u0302(S))\u2212 F \u2217 \u2265 1/4] > 1/2.\nProof. We start by observing that the uniform distribution over H2 is equivalent to picking the function gV where V is obtained by including every element of W with probability 1/2 randomly and independently of all other elements. Further, by the properties of W , for every w \u2208 W , and V \u2286 W , gV (w\u0304) = 1 if w \u2208 V and gV (w\u0304) = 1/2 otherwise. For gV chosen randomly with respect to D, we have that w \u2208 V with probability exactly 1/2. This implies that FD(w\u0304) = 3/4.\nLet S = (gV1 , . . . , gVn) be the random samples. Observe that minK(FS) = 1/2 and F \u2217 = minK(FD) =\n1/2 (the minimum is achieved at the origin 0\u0304). Now, if \u22c3 i\u2208[n]Vi 6= W then let x\u0302(S) . = w\u0304 for any w \u2208 W \\ \u22c3i\u2208[n]Vi. Otherwise x\u0302(S) is defined to be the origin 0\u0304. Then by the property of H2 mentioned above, we have that for all i, gVi(x\u0302(S)) = 1/2 and hence FS(x\u0302(S)) = 1/2. This means that x\u0302(S) is a minimizer of FS.\nCombining these statements, we get that, if \u22c3 i\u2208[n]Vi 6= W then there exists an ERM x\u0302(S) such that FS(x\u0302(S)) = minK(FS) and FD(x\u0302(S)) \u2212 F \u2217 = 1/4. Therefore to prove the claim it suffices to show that for n \u2264 d/6 we have that\nPr S\u223cDn\n\n\n\u22c3\ni\u2208[n]\nVi 6= W\n\n > 1\n2 .\nThis easily follows from observing that for the uniform distribution over subsets of W , for every w \u2208 W ,\nPr S\u223cDn\n\nw \u2208 \u22c3\ni\u2208[n]\nVi\n\n = 1\u2212 2\u2212n\nand this event is independent from the inclusion of other elements in \u22c3\ni\u2208[n]Vi. Therefore\nPr S\u223cDn\n\n\n\u22c3\ni\u2208[n]\nVi = W\n\n = ( 1\u2212 2\u2212n )|W | \u2264 e\u22122\u2212n\u00b72d/6 \u2264 e\u22121 < 1\n2 .\nRemark 3.4. In our construction there is a different ERM algorithm that does solve the problem (and generalizes well). For example, the algorithm that always outputs the origin 0\u0304. Therefore it is natural to ask whether the same lower bound holds when there exists a unique minimizer. Shalev-Shwartz et al. [SSSS09] show that their lower bound construction can be slightly modified to ensure that the minimizer is unique while still having large generalization error. An analogous modification appears to be much harder to analyze in our construction and it is unclear to us how to ensure uniqueness in our strong lower bounds. A further question in this direction is whether it is possible to construct a distribution for which the empirical minimizer with large generalization error is unique and its value is noticeably (at least by 1/poly(d)) smaller than the value of FS at any point x that generalizes well. Such distribution would imply that the solutions that \u201coverfits\u201d can be found easily (for example, in a polynomial number of iterations of the gradient descent).\nOther \u2113p norms: We now observe that exactly the same approach can be used to extend this lower bound to \u2113p/\u2113q setting. Specifically, for p \u2208 [1,\u221e] and q = p\u2217 we define\ngp,V (x) . = max\n{\n1 2 ,max w\u2208V \u3008w, x\u3009 d1/q\n}\n.\nIt is easy to see that for every V \u2286 W , gq,V \u2208 F0p (Bdp , 1). We can now use the same argument as before with the appropriate normalization factor for points in Bdp . Namely, instead of w\u0304 for w \u2208 W we consider the values of the minimized functions at w/d1/p \u2208 Bdp. This gives the following generalization of Thm. 3.3.\nTheorem 3.5. For every p \u2208 [1,\u221e] let K = Bdp and we define Hp . = {gp,V | V \u2286 W} and let D be the uniform distribution over Hp. Then for n \u2264 d/6 and every set of samples S there exists an ERM x\u0302(S) such that\nPr S\u223cDn\n[FD(x\u0302(S))\u2212 F \u2217 \u2265 1/4] > 1/2."}, {"heading": "3.2 Smoothness does not help", "text": "We now extend the lower bound to smooth functions. We will for simplicity restrict our attention to \u21132 but analogous modifications can be made for other \u2113p norms. The functions gV that we used in the construction use two maximum operators each of which introduces non-smoothness. To deal with maximum with 1/2 we simply replace the function max{1/2, \u3008w\u0304, x\u3009} with a quadratically smoothed version (in the same way\nas hinge loss is sometimes replaced with modified Huber loss). To deal with the maximum over all w \u2208 V , we show that it is possible to ensure that individual components do not \u201cinteract\u201d. That is, at every point x, the value, gradient and Hessian of at most one component function are non-zero (value, vector and matrix, respectively). This ensures that maximum becomes addition and Lipschitz/smoothness constants can be upper-bounded easily.\nFormally, we define\n\u03bd(a) . =\n{\n0 if a \u2264 0 a2 otherwise.\nNow, for V \u2286 W , we define\nhV (x) . = \u2211\nw\u2208V\n\u03bd(\u3008w\u0304, x\u3009 \u2212 7/8). (2)\nWe first prove that hV is 1/4-Lipschitz and 1-smooth.\nLemma 3.6. For every V \u2286 W and hV defined in eq. (2) we have hV \u2208 F02 (Bd2 , 1/4) \u2229 F12 (Bd2 , 1). Proof. It is easy to see that \u03bd(\u3008w\u0304, x\u3009\u22127/8) is convex for every w and hence hV is convex. Next we observe that for every point x \u2208 Bd2 , there is at most one w \u2208 W such that \u3008w\u0304, x\u3009 > 7/8. If \u3008w\u0304, x\u3009 > 7/8 then \u2016w\u0304 \u2212 x\u20162 = \u2016w\u0304\u20162 + \u2016x\u20162 \u2212 2\u3008w\u0304, x\u3009 < 1 + 1\u2212 2(7/8) = 1/4. On the other hand, by the properties of W , for distinct w1, w2 we have that \u2016w\u03041 \u2212 w\u03042\u20162 = 2 \u2212 2\u3008w\u03041, w\u03042\u3009 \u2265 1. Combining these bounds on distances we obtain that if we assume that \u3008w\u03041, x\u3009 > 7/8 and \u3008w\u03042, x\u3009 > 7/8 then we obtain a contradiction\n\u2016w\u03041 \u2212 w\u03042\u2016 \u2264 \u2016w\u03041 \u2212 x\u2016+ \u2016w\u03042 \u2212 x\u2016 < 1.\nFrom here we can conclude that\n\u2207hV (x) = { 2(\u3008w\u0304, x\u3009 \u2212 7/8) \u00b7 w\u0304 if \u2203w \u2208 V, \u3008w\u0304, x\u3009 > 7/8 0 otherwise .\nThis immediately implies that \u2016\u2207hV (x)\u2016 \u2264 1/4 and hence hV is 1/4-Lipschitz. We now prove smoothness. Given two points x, y \u2208 Bd2 we consider two cases. First the simpler case when there is at most one w \u2208 V such that either \u3008w\u0304, x\u3009 > 7/8 or \u3008w\u0304, y\u3009 > 7/8. In this case \u2207hV (x) = \u2207\u03bd(\u3008w\u0304, x\u3009\u22127/8) and \u2207hV (y) = \u2207\u03bd(\u3008w\u0304, y\u3009\u22127/8). This implies that the 1-smoothness condition is implied by 1-smoothness of \u03bd(\u3008w\u0304, \u00b7\u3009 \u2212 7/8). That is one can easily verify that \u2016\u2207hV (x)\u2212\u2207hV (y)\u2016 \u2264 \u2016x\u2212 y\u2016.\nNext we consider the case where for x there is w1 \u2208 V such that \u3008w\u03041, x\u3009 > 7/8, for y there is w2 \u2208 V such that \u3008w\u03042, y\u3009 > 7/8 and w1 6= w2. Then there exists a point z \u2208 Bd2 on the line connecting x and y such that \u3008w\u03041, z\u3009 \u2264 7/8 and \u3008w\u03042, z\u3009 \u2264 7/8. Clearly, \u2016x \u2212 y\u2016 = \u2016x \u2212 z\u2016 + \u2016z \u2212 y\u2016. On the other hand, by the analysis of the previous case we have that \u2016\u2207hV (x) \u2212\u2207hV (z)\u2016 \u2264 \u2016x \u2212 z\u2016 and \u2016\u2207hV (z) \u2212\u2207hV (y)\u2016 \u2264 \u2016z \u2212 y\u2016. Combining these inequalities we obtain that\n\u2016\u2207hV (x)\u2212\u2207hV (y)\u2016 \u2264 \u2016\u2207hV (x)\u2212\u2207hV (z)\u2016+ \u2016\u2207hV (z)\u2212\u2207hV (y)\u2016 \u2264 \u2016x\u2212 z\u2016+ \u2016z\u2212 y\u2016 = \u2016x\u2212 y\u2016.\nFrom here we can use the proof approach from Thm. 3.3 but with hV in place of gV .\nTheorem 3.7. Let K = Bd2 and we define H . = {hV | V \u2286 W} for hV defined in eq, (2). Let D be the uniform distribution over H. Then for n \u2264 d/6 and every set of samples S there exists an ERM x\u0302(S) such that\nPr S\u223cDn\n[FD(x\u0302(S))\u2212 F \u2217 \u2265 1/128] > 1/2.\nProof. Let S = (hV1 , . . . , hVn) be the random samples. As before we first note that minK(FS) = 0 and F \u2217 = 0. Further, for every w \u2208 W , hV (w\u0304) = 1/64 if w \u2208 V and hV (w\u0304) = 0 otherwise. Hence FD(w\u0304) = 1/128. Now, if \u22c3 i\u2208[n]Vi 6= W then let x\u0302(S) . = w\u0304 for some w \u2208 W \\ \u22c3i\u2208[n]Vi. Then for all i, hVi(x\u0302(S)) = 0 and hence FS(x\u0302(S)) = 0. This means that x\u0302(S) is a minimizer of FS and FD(x\u0302(S))\u2212 F \u2217 = 1/128.\nNow, exactly as in Thm. 3.3, we can conclude that \u22c3 i\u2208[n]Vi 6= W with probability > 1/2."}, {"heading": "3.3 \u21131 Regularization does not help", "text": "Next we show that the lower bound holds even with an additional \u21131 regularization term \u03bb\u2016x\u2016 for positive \u03bb \u2264 1/ \u221a d. (Note that if \u03bb > 1/ \u221a d then the resulting program is no longer 1-Lipschitz relative to \u21132. Any constant \u03bb can be allowed for \u21131/\u2113\u221e setup). To achieve this we shift the construction to the positive orthant (that is x such that xi \u2265 0 for all i \u2208 [d]). In this orthant the subgradient of the regularization term is simply \u03bb1\u0304 where 1\u0304 is the all 1\u2019s vector. We can add a linear term to each function in our distribution that balances this term thereby reducing the analysis to non-regularized case. More formally, we define the following family of functions. For V \u2286 W ,\nh\u03bbV (x) . = hV (x\u2212 1\u0304/ \u221a d)\u2212 \u03bb\u30081\u0304, x\u3009. (3)\nNote that over Bd2(2), h\u03bbV (x) is L-Lipschitz for L \u2264 2(2\u2212 7/8) +\u03bb \u221a d \u2264 9/4. We now state and prove this formally. Theorem 3.8. Let K = Bd2(2) and for a given \u03bb \u2208 (0, 1/ \u221a d], we define H\u03bb .= {h\u03bbV | V \u2286 W} for h\u03bbV defined in eq. (3). Let D be the uniform distribution over H\u03bb. Then for n \u2264 d/6 and every set of samples S there exists x\u0302(S) such that\n\u2022 FS(x\u0302(S)) = minx\u2208K(FS(x) + \u03bb\u2016x\u20161);\n\u2022 PrS\u223cDn [FD(x\u0302(S))\u2212 F \u2217 \u2265 1/128] > 1/2.\nProof. Let S = (h\u03bb V1 , . . . , h\u03bb Vn ) be the random samples. We first note that F \u2217 = FD(0\u0304) = 0 and\nmin x\u2208K (FS(x) + \u03bb\u2016x\u20161) = min x\u2208K\n\n\n\u2211\ni\u2208[n]\nhVi\n(\nx\u2212 1\u0304\u221a d\n)\n\u2212 \u03bb\u30081\u0304, x\u3009+ \u03bb\u2016x\u20161\n\n\n\u2265 min x\u2208K\n\n\n\u2211\ni\u2208[n]\nhVi\n(\nx\u2212 1\u0304\u221a d\n)\n\n \u2265 0.\nFurther, for every w \u2208 W , w\u0304 + 1\u0304/ \u221a d is in the positive orthant and in Bd2(2). Hence h\u03bbV (w\u0304 + 1\u0304/ \u221a d) =\nhV (w\u0304). We can therefore apply the analysis from Thm. 3.7 to obtain the claim."}, {"heading": "3.4 Dependence on \u01eb", "text": "We now briefly consider the dependence of our lower bound on the desired accuracy. Note that the upper bound for uniform convergence scales as O\u0303(d/\u01eb2).\nWe first observe that our construction implies a lower bound of \u2126(d/\u01eb2) for uniform convergence nearly matching the upper bound (we do this for the simpler non-smooth \u21132 setting but the same applies to other setting we consider).\nTheorem 3.9. Let K = Bd2 and we define H2 . = {gV | V \u2286 W} for gV defined in eq. (1). Let D be the uniform distribution over H2. Then for any \u01eb > 0 and n \u2264 n1 = \u2126(d/\u01eb2) and every set of samples S there exists a point x\u0302(S) such that\nPr S\u223cDn\n[FD(x\u0302(S))\u2212 FS(x\u0302(S)) \u2265 \u01eb] > 1/2.\nProof. For every w \u2208 W ,\nFS(w\u0304) = 1\nn\n\u2211\ni\u2208[n]\ngVi(w\u0304) = 1\n2 +\n1\n2n\n\u2211\ni\u2208[n]\n1{w\u2208Vi},\nwhere 1{w\u2208Vi} is the indicator variable of w being in Vi. If for some w, 1 2n\n\u2211\ni\u2208[n] 1{w\u2208Vi} \u2265 1/4 + \u01eb then we will obtain a point w\u0304 that violates the uniform convergence by \u01eb. For every w, \u2211\ni\u2208[n] 1{w\u2208Vi} is distributed according to the binomial distribution. Using a standard approximation of the partial binomial sum up to (1/2 \u2212 2\u01eb)n, we obtain that for some constant c > 0, the probability that this sum is \u2265 1/2 + 2\u01eb is at least\n1 2n \u00b7 1\u221a 8n(1/4 \u2212 \u01eb2) \u00b7 ( 1 2 + 2\u01eb )(1/2+2\u01eb)n \u00b7 ( 1 2 \u2212 2\u01eb )(1/2\u22122\u01eb)n \u2265 2\u2212cn\u01eb2 .\nNow, using independence between different w \u2208 W , we can conclude that, for n \u2264 d/(6c\u01eb2), the probability that there exists w for which uniform convergence is violated is at least\n1\u2212 ( 1\u2212 2\u2212cn\u01eb2 )|W | \u2265 1\u2212 e\u22122\u2212cn\u01eb 2 \u00b72d/6 \u2265 1\u2212 e\u22121 > 1 2 .\nA natural question is whether the d/\u01eb2 dependence also holds for ERM. We could not answer it and prove only a weaker \u2126(d/\u01eb) lower bound. For completeness, we also make this statement for general radius R and Lipschitz bound L.\nTheorem 3.10. For L,R > 0 and \u01eb \u2208 (0, LR/4), let K = Bd2(R) and we define H2 . = {L \u00b7 gV | V \u2286 W} \u2286 F0(Bd2(R), L) for gV defined in eq. (1). We define the random variable V\u03b1 as a random subset of W obtained by including each element of W with probability \u03b1 . = 2\u01eb/(LR) randomly and independently. Let D\u03b1 be the probability distribution of the random variable gV\u03b1 . Then for n \u2264 d/32 \u00b7LR/\u01eb and every set of samples S there exists an ERM x\u0302(S) such that\nPr S\u223cDn\u03b1\n[FD\u03b1(x\u0302(S)) \u2212 F \u2217 \u2265 \u01eb] > 1/2.\nProof. By the same argument as in the proof of Thm. 3.3 we have that: For every w \u2208 W , and V \u2286 W , L \u00b7 gV (Rw\u0304) = LR if w \u2208 V and L \u00b7 gV (Rw\u0304) = LR/2 otherwise. For gV chosen randomly with respect to D\u03b1, we have that w \u2208 V with probability 2\u01eb/(LR). This implies that FD\u03b1(Rw\u0304) = LR/2 + \u01eb. Similarly, minK(FS) = LR/2 and F \u2217 = minK(FD\u03b1) = LR/2.\nTherefore, if \u22c3 i\u2208[n]Vi 6= W then there exists an ERM x\u0302(S) such that FS(x\u0302(S)) = minK(FS) and FD\u03b1(x\u0302(S))\u2212 F \u2217 = \u01eb. For the distribution D\u03b1 and every w \u2208 W ,\nPr S\u223cDn\u03b1\n\nw \u2208 \u22c3\ni\u2208[n]\nVi\n\n = 1\u2212 (1\u2212 \u03b1)n \u2264 1\u2212 e\u22122\u03b1n\nand this event is independent from the inclusion of other elements in \u22c3 i\u2208[n]Vi (where we used that 1\u2212\u03b1 \u2265 e\u22122\u03b1 for \u03b1 < 1/2). Therefore\nPr S\u223cDn\u03b1\n\n\n\u22c3\ni\u2208[n]\nVi = W\n\n = ( 1\u2212 e\u22122\u03b1n )|W | \u2264 e\u2212e\u22122\u03b1n\u00b7ed/8 \u2264 e\u22121 < 1\n2 ."}, {"heading": "4 Range-Bounded Convex Optimization", "text": "As we have outlined in the introduction, SCO is solvable in the more general setting in which instead of the Lipschitz bound and radius of K we have a bound on the range of functions in the support of distribution. Recall that for a bound on the absolute value B we denote this class of functions by F(K, B). This setting is more challenging algorithmically and has not been studied extensively. For comparison purposes and completeness, we state a recent result for this setting from [RS15] (converted from the online to the stochastic setting in the standard way).\nTheorem 4.1 ([RS15]). Let K = Bdp(R) for some R > 0 and B > 0. There is an efficient algorithm A that given \u01eb, \u03b4 > 0 and n = O(log(B/\u01eb) log(1/\u03b4)B2/\u01eb2)) i.i.d. samples from any distribution D supported on F(K, B) outputs an \u01eb-optimal solution to FD over K with probability \u2265 1\u2212 \u03b4.\nThe case of general K can be handled by generalizing the approach in [RS15] or using the algorithms in [BLNR15, FGV15]. Note that for those algorithms the sample complexity will have a polynomial dependence on d (which is unavoidable in this general setting).\nIn contrast to these results, we will now demonstrate that for such problems an ERM algorithm will require an infinite number of samples to succeed already for d = 2. As in the proof of Thm. 3.7 we define fV (x) = \u2211\nw\u2208V \u03c6(\u3008w, x\u3009). However we can now use the lack of bounds on the Lipschitz constant (or smoothness) to use \u03c6(a) that is equal to 0 for a \u2264 1\u2212 \u03b1 and \u03c6(1) = 1. For every m \u2265 2, we can choose a set of m vectors W evenly spaced on the unit circle such that for a sufficiently small \u03b1 > 0, \u03c6(\u3008w, x\u3009) will not interact with \u03c6(\u3008w\u2032, x\u3009), for any two distinct w,w\u2032 \u2208 W . More formally, let m be any positive integer, let wi . = (sin(2\u03c0 \u00b7 i/m), cos(2\u03c0 \u00b7 i/m)) and let Wm .= {wi | i \u2208 [m]}. Let\n\u03c6\u03b1(a) . =\n{\n0 if a \u2264 1\u2212 \u03b1 (a\u2212 1 + \u03b1)/\u03b1 otherwise.\nFor V \u2286 Wm we set \u03b1 .= 2/m2 and define\nfV (x) . = \u2211\nw\u2208V\n\u03c6\u03b1(\u3008w, x\u3009). (4)\nIt is easy to see that fV is convex. We now verify that the range of fV is [0, 1] on B22. Clearly, for any unit vector wi \u2208 Wm, and x \u2208 B22, \u3008wi, x\u3009 \u2208 [\u22121, 1] and therefore \u03c6\u03b1(\u3008wi, x\u3009) \u2208 [0, 1]. Now it suffices to establish that for every x \u2208 B22, there exists at most one vector w \u2208 Wm such that \u03c6\u03b1(\u3008w, x\u3009) > 0. To see this, as in Lemma 3.6, we note that if \u03c6\u03b1(\u3008w, x\u3009) > 0 then \u3008w, x\u3009 > 1 \u2212 \u03b1. For w \u2208 Wm and x \u2208 B22, this implies that \u2016w \u2212 x\u2016 < \u221a 1 + 1\u2212 2(1 \u2212 \u03b1) = \u221a 2\u03b1. For our choice of \u03b1 = 2/m2, this implies that \u2016w \u2212 x\u2016 < 2/m. On the other hand, for i 6= j \u2208 [m], we have\n\u2016wi \u2212wj\u2016 \u2265 \u2016w1 \u2212 wm\u2016 \u2265 sin(2\u03c0/m) \u2265 2\u03c0/m\u2212 (2\u03c0/m)3/6 \u2265 4/m.\nTherefore there does not exist x such that \u03c6\u03b1(\u3008wi, x\u3009) > 0 and \u03c6\u03b1(\u3008wj , x\u3009) > 0. Now we can easily establish the lower bound.\nTheorem 4.2. Let K = B22 and m \u2265 2 be an integer. We define Hm . = {fV | V \u2286 W} for fV defined in eq. (4). Let Dm be the uniform distribution over Hm. Then for n \u2264 logm and every set of samples S there exists an ERM x\u0302(S) such that\nPr S\u223cDnm\n[FD(x\u0302(S))\u2212 F \u2217 \u2265 1/2] > 1/2.\nProof. Let S = (fV1 , . . . , fVn) be the random samples. Clearly, F \u2217 = 0 and minK(FS) = 0. Further, the analysis above implies that for every w \u2208 Wm and V \u2286 Wm, fV (w) = 1 if w \u2208 V and fV (w) = 0 otherwise. Hence FDm(w) = 1/2. Now, if \u22c3 i\u2208[n]Vi 6= Wm then let x\u0302(S) . = w for any w \u2208 Wm\\ \u22c3\ni\u2208[n]Vi. Then for all i, hVi(x\u0302(S)) = 0 and hence FS(x\u0302(S)) = 0. This means that x\u0302(S) is a minimizer of FS and FDm(x\u0302(S)) \u2212 F \u2217 = 1/2.\nNow, exactly as in Thm. 3.3, we can conclude that \u22c3\ni\u2208[n]Vi = Wm with probability at most\n( 1\u2212 2\u2212n )m \u2264 e\u22122\u2212n\u00b7m \u2264 e\u22121 < 1\n2 .\nThis lower bound holds for every m. This implies that the sample complexity of 1/2-optimizing F(B22 , 1) over B22 is infinite."}, {"heading": "5 Discussion", "text": "Our work points out to substantial limitations of the classic approach to understanding and analysis of generalization in the context of general SCO. One way to bypass our lower bounds is to use additional structural assumptions. For example, for generalized linear regression problems uniform convergence gives nearly optimal bounds on sample complexity [KST08]. One natural question is whether there exist more general classes of functions that capture most of the practically relevant SCO problems and enjoy dimensionindependent (or, scaling as log d) uniform convergence bounds. Note that the functions constructed in our lower bounds have description of size exponential in d and therefore are unlikely to apply to natural classes of functions.\nAn alternative approach is to bypass uniform convergence (and possibly also ERM) altogether. Among a large number of techniques that have been developed for ensuring generalization, the most general ones are based on notions of stability [BE02, SSSSS10]. However, known analyses based on stability often do not provide the strongest known generalization guarantees (e.g. high probability bounds require very strong assumptions). Another issue is that we lack general algorithmic tools for ensuring stability of the output. Therefore many open problems remain and significant progress is required to obtain a more comprehensive understanding of this approach. Some encouraging new developments in this area are the use of notions of stability derived from differential privacy [DFH+15] and the use of tools for analysis of convergence of convex optimization algorithms for proving stability [HRS15]."}, {"heading": "Acknowledgements", "text": "I am grateful to Ken Clarkson, Sasha Rakhlin and Thomas Steinke for discussions and insightful comments related to this work."}], "references": [{"title": "Escaping the local minima via simulated annealing: Optimization of approximately convex functions", "author": ["Alexandre Belloni", "Tengyuan Liang", "Hariharan Narayanan", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "Belloni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Belloni et al\\.", "year": 2015}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)", "author": ["Francis R. Bach", "Eric Moulines"], "venue": "In NIPS,", "citeRegEx": "Bach and Moulines.,? \\Q2013\\E", "shortCiteRegEx": "Bach and Moulines.", "year": 2013}, {"title": "Convex optimization: Algorithms and complexity", "author": ["S\u00e9bastien Bubeck"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck.", "year": 2015}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Generalization in adaptive data analysis and holdout", "author": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": "reuse. CoRR,", "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "Statistical query algorithms for stochastic convex optimization", "author": ["Vitaly Feldman", "Cristobal Guzman", "Santosh Vempala"], "venue": "CoRR, abs/1512.09170,", "citeRegEx": "Feldman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2015}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "CoRR, abs/1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In NIPS,", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Sequential probability assignment with binary alphabets and large classes of experts", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "CoRR, abs/1501.07340,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2015\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2015}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "On complexity of stochastic programming problems", "author": ["A. Shapiro", "A. Nemirovski"], "venue": null, "citeRegEx": "Shapiro and Nemirovski.,? \\Q2005\\E", "shortCiteRegEx": "Shapiro and Nemirovski.", "year": 2005}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "Shamir and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shamir and Zhang.", "year": 2013}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Wiley-Interscience, New York,", "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "In stochastic convex optimization the goal is to minimize a convex function F (x) . = Ef\u223cD[f(x)] over a convex set K \u2282 R where D is some unknown distribution and each f(\u00b7) in the support of D is convex over K. The optimization is commonly based on i.i.d. samples f, f, . . . , f from D. A standard approach to such problems is empirical risk minimization (ERM) that optimizes FS(x) . = 1 n \u2211 i\u2264n f (x). Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of FS to F over K. We demonstrate that in the standard lp/lq setting of Lipschitz-bounded functions over a K of bounded radius, ERM requires sample size that scales linearly with the dimension d. This nearly matches standard upper bounds and improves on \u03a9(log d) dependence proved for l2/l2 setting in [SSSS09]. In stark contrast, these problems can be solved using dimension-independent number of samples for l2/l2 setting and log d dependence for l1/l\u221e setting using other approaches. We also demonstrate that for a more general class of rangebounded (but not Lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.", "creator": "LaTeX with hyperref package"}}}