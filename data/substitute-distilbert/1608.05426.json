{"id": "1608.05426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments", "abstract": "while cross - lingual word embeddings have been cultivated extensively in recent years, the specific differences between the different sources remains mixed. we observe that whether or not every algorithm uses a particular feature set ( sentence ids ) accounts for a reduced performance gap among these algorithms. this feature set is also found when traditional alignment algorithms, such as ibm model - 1, which demonstrate similar performance to state - of - the - art embedding algorithms on a variety of databases. overall, we observe which different algorithmic approaches for utilizing the sentence id feature space result in similar scores. this paper draws both empirical and theoretical parallels between the embedding and alignment literature, and best if adding additional sources representing information, which go beyond the traditional signal of employing sentence - class corpora, is an appealing approach for substantially improving crosslingual word embeddings.", "histories": [["v1", "Thu, 18 Aug 2016 20:27:46 GMT  (24kb)", "http://arxiv.org/abs/1608.05426v1", null], ["v2", "Mon, 9 Jan 2017 20:49:18 GMT  (28kb)", "http://arxiv.org/abs/1608.05426v2", "EACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["omer levy", "ers s{\\o}gaard", "yoav goldberg"], "accepted": false, "id": "1608.05426"}, "pdf": {"name": "1608.05426.pdf", "metadata": {"source": "CRF", "title": "Reconsidering Cross-lingual Word Embeddings", "authors": ["Omer Levy"], "emails": ["omerlevy@gmail.com", "soegaard@hum.ku.dk", "yoav.goldberg@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 42\n6v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 6\nWhile cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remains vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, is an appealing approach for substantially improving crosslingual word embeddings."}, {"heading": "1 Introduction", "text": "Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and S\u00f8gaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of translation-oriented benchmarks.\nWe observe that the top embedding algorithms share the same underlying feature space \u2013 sentence IDs \u2013 while their different algorithmic approaches seem to have a negligible impact on performance. We also notice that several statistical alignment algorithms, such as IBM Model-1 (Brown et al., 1993), operate under the same data assumptions. Specifically, we find that using the translation probabilities learnt by Model-1 as the cross-lingual similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na\u0131\u0308ve, the actual difference in performance between different approaches is marginal.\nThis leads us to revisit another statistical alignment algorithm from the literature that uses the same sentence-based signal \u2013 the Dice aligner (Och and Ney, 2003). We first observe that the vanilla Dice aligner is significantly outperformed by the Model-1 aligner. We then recast Dice as the dot-product between two word vectors (based on the sentence ID feature space), which allows us to generalize it, resulting in an embedding model that is as effective as Model-1 and other sophisticated state-of-the-art embedding methods, but takes a fraction of the time to train.\nExisting approaches for creating cross-lingual word embeddings are typically restricted to training bilingual embeddings, mapping exactly two languages onto a common space. We show that our generalization of the Dice coefficient can be augmented to jointly train multi-lingual embeddings for any number of languages. We do this by leveraging the fact that the space of sentence IDs is shared among\n\u2217These authors contributed equally to this work.\nall languages in the parallel corpus; the verses of the Bible, for example, are identical across all translations. Introducing this new multi-lingual signal shows a significant performance boost, which eclipses the variance in performance among pre-existing embedding algorithms."}, {"heading": "2 Background: Cross-lingual Embeddings", "text": "Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013).\nThe second category makes a much weaker assumption, document-level alignments, and uses comparable texts in different languages (not necessarily translations) such as Wikipedia articles or news reports of the same event. Algorithms in this category try to leverage massive amounts of data to make up for the lack of lower-level alignments (S\u00f8gaard et al., 2015; Vulic\u0301 and Moens, 2016).\nAlgorithms in the third category take the middle ground; they use sentence-level alignments, common in legal translations and religious texts. Also known as \u201cparallel corpora\u201d, sentence-aligned data maps each sentence (as a whole) to its translation. We focus on this third category, because it does not require the strict assumption of word-aligned data (which is difficult to obtain), while still providing a cleaner and more accurate signal than document-level alignments (which have been shown, in monolingual data, to capture mainly syntagmatic relations (Sahlgren, 2006)).\nAlgorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence\u2019s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final layer, while Chandar et al. (2014) proposed a shallower autoencoder-based model, representing both source and target language sentences as the same intermediate sentence vector. Recently, a simpler model, BilBOWA (Gouws et al., 2015), showed similar performance without using a hidden sentence-representation layer, giving it a dramatic speed advantage over its predecessors. BilBOWA is essentially an extension of skip-grams with negative sampling (SGNS) (Mikolov et al., 2013b), which simultaneously optimizes each word\u2019s similarity to its inter-lingual context (words that appeared in the aligned target language sentence) and its intra-lingual context (as in the original monolingual model). Luong et al. (2015) proposed a similar SGNS-based model over the same features.\nThis paper tries to determine which factors determine the success of cross-lingual word embedding algorithms that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions."}, {"heading": "3 Which Features Make Better Cross-lingual Embeddings?", "text": "We group state-of-the-art cross-lingual embedding algorithms according to their feature sets, and compare their performance on two cross-lingual benchmarks: word alignment and bilingual dictionary induction. In doing so, we hope to learn which features are more informative."}, {"heading": "3.1 Features of Sentence-aligned Data", "text": "We observe that cross-lingual embeddings typically use parallel corpora in one of two ways:\nSource + Target Language Words Each word w is represented using all the other words that appeared with it in the same sentence (source language words) and all the words that appeared in target language sentences that were aligned to sentences in which the word w appeared (target language words). This representation also stores the number of times each pair of word w and feature (context) word f cooccurred.\nThese features are analogous to the ones used by Vulic\u0301 and Moens (2016) for document-aligned data, and can be built in a similar manner: create a pseudo-bilingual sentence from each aligned sentence,\nand for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance from the word in question, and defines a slightly different interaction with target language words.\nSentence IDs Each word is represented by the set of sentences in which it appeared, indifferent to the number of times it appeared in each one. This feature set is also indifferent to the word ordering within each sentence. This approach is implicitly used by Chandar et al. (2014), who encode the bag-of-words representation of two parallel sentences into the same vector. Thus, each word is not matched directly to another word (as in the previous feature space), but rather used to create the sentence\u2019s language-independent representation; this is analogous to matching a word with a sentence ID. S\u00f8gaard et al. (2015) use similar features, document IDs, for leveraging comparable Wikipedia articles in different languages. In \u00a76 we show that when using sentence IDs, even a small amount of sentencealigned data is more powerful than a huge amount of comparable documents."}, {"heading": "3.2 Experiment Setup", "text": "Algorithms We use the four algorithms mentioned in \u00a73.1: BilBOWA (Gouws et al., 2015), BWESkipGram (Vulic\u0301 and Moens, 2016), Bilingual Autoencoders (Chandar et al., 2014), and Inverted Index (S\u00f8gaard et al., 2015). While both BWE-SkipGram and Inverted Index were originally trained on document-aligned data, in this work, we apply them to sentence-aligned data.\nData Christodouloupoulos and Steedman (2015) collected translations of the Bible (or parts of it) in over 100 languages, naturally aligned by book, chapter, and verse (31,102 verses in total).1 This corpus allows us to evaluate methods across many different languages, while controlling for the training set\u2019s size. The corpus was decapitalized and tokenized using white spaces after splitting at punctuation.\nBenchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets \u2013 Hansards2 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) \u2013 as well as 16 bilingual dictionaries from Wiktionary.\nIn the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence \u2013 this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not aligned with anything, whereas target language out-of-vocabulary words are given a default minimal similarity score, and never aligned to any candidate source language word in practice. We use the inverse of alignment error rate (1-AER) as described in Koehn (2010) to measure performance, where higher scores mean better alignments.\nHigh quality, freely available, manually annotated word alignment datasets are rare, especially for non-European languages. We therefore include experiments on bilingual dictionary induction. We obtain bilingual dictionaries from Wiktionary for five non-Indo-European languages, namely: Arabic, Finnish, Hebrew, Hungarian, and Turkish (all represented in the Edinburgh Bible Corpus). We emphasize that unlike most previous work, we experiment with finding translation equivalents of all words and do not filter the source and target language words by part of speech.\nHyperparameters Levy et al. (2015) exposed a collection of hyperparameters that affect the performance of monolingual embeddings. We assume that the same is true for cross-lingual embeddings, and use their recommended settings across all algorithms (where applicable). Specifically, we used 500 dimensions for every algorithm, context distribution smoothing with \u03b1 = 0.75 (applicable to BilBOWA and BWE-SkipGram), the symmetric version of SVD (applicable to Inverted Index), and run iterative algorithms for 100 epochs to ensure convergence (applicable to all algorithms except Inverted Index). For\n1homepages.inf.ed.ac.uk/s0787820/bible/ 2www.isi.edu/natural-language/download/hansard/\nBilBOWA\u2019s monolingual context window, we used the default of 5. Similarity is always measured by the vectors\u2019 cosine. Most importantly, we use a shared vocabulary, consisting of every word that appeared at least twice in the corpus."}, {"heading": "3.3 Results", "text": "Table 1 shows that algorithms based on the sentence-ID feature space perform consistently better than those using source+target words. We suspect that the source+target feature set might be capturing more information than is actually needed for translation, such as syntagmatic or topical similarity between words (e.g. \u201cdog\u201d \u223c \u201ckennel\u201d). This might be distracting for cross-lingual tasks such as word alignment and bilingual dictionary induction. Sentence ID features, on the other hand, are simpler, and might therefore contain a cleaner translation-oriented signal.\nIt is important to state that, in absolute terms, these results are quite poor. The fact that the best inverse AER is around 50% calls into question the ability to actually utilize these embeddings in a reallife scenario. While one may suggest that this is a result of the small training dataset (Edinburgh Bible Corpus), previous work (e.g. (Chandar et al., 2014)) used an even smaller dataset (the first 10K sentences in Europarl (Koehn, 2005)). To ensure that our results are not an artifact of the Edinburgh Bible Corpus, we repeated our experiments on the full Europarl corpus (180K sentences) for a subset of languages (English, French, and Spanish), and observed similar trends. As this is a comparative study focused on analyzing the qualitative differences between algorithms, we place the issue of low absolute performance aside for the moment, and reopen it in \u00a75.4."}, {"heading": "4 Comparing Cross-lingual Embeddings with Traditional Alignment Methods", "text": "Sentence IDs are not unique to modern embedding methods, and have been used by statistical machine translation from the very beginning. In particular, the Dice coefficient (Och and Ney, 2003), which is often used as a baseline for more sophisticated alignment methods, measures the cross-lingual similarity of words according to the number of aligned sentences in which they appeared. IBM Model-1 (Brown et al., 1993) also makes exactly the same data assumptions as other sentence-ID methods. It therefore makes sense to use Dice similarity and the translation probabilities derived from IBM Model-1 as baselines for cross-lingual embeddings that use sentence IDs.\nFrom Table 2 we learn that the existing embedding methods are not really better than IBM Model-1. In fact, their average performance is even slightly lower than Model-1\u2019s. Although Bilingual Autoencoders, Inverted Index, and Model-1 reflect entirely different algorithmic approaches (respectively: neural networks, matrix factorization, and EM), the overall difference in performance seems to be rather marginal. This suggests that the main performance factor is not the algorithm, but the feature space: sentence IDs.\nHowever, Dice also relies on sentence IDs, yet its performance is significantly worse. We suggest that Dice uses the sentence-ID feature set na\u0131\u0308vely, resulting in degenerate performance with respect to the other methods. In the following section, we analyze this shortcoming and show that generalizations of Dice actually do yield similar performance Model-1 and other sentence-ID methods."}, {"heading": "5 Generalized Dice", "text": "In this section, we show that the Dice coefficient (Och and Ney, 2003) can be seen as the dot-product between two word vectors represented over the sentence-ID feature set. After providing some background, we demonstrate the mathematical connection between Dice and word-feature matrices. We then introduce a new variant of Dice, SID-SGNS, which performs on-par with Model-1 and the other embedding algorithms. This variant is able to seamlessly leverage the multi-lingual nature of sentence IDs, giving it a small but significant edge over Model-1."}, {"heading": "5.1 Word-Feature Matrices", "text": "In the word similarity literature, it is common to represent words as real-valued vectors and compute their \u201csemantic\u201d similarity with vector similarity metrics, such as the cosine of two vectors. These word vectors are traditionally derived from sparse word-feature matrices, either by using the matrix\u2019s rows as-is (also known as \u201cexplicit\u201d representation) or by inducing a lower-dimensional representation via matrix factorization (Turney and Pantel, 2010). Many modern methods, such as those in word2vec (Mikolov et al., 2013b), also create vectors by factorizing word-feature matrices, only without representing these matrices explicitly.\nFormally, we are given a vocabulary of words VW and a feature space (\u201cvocabulary of features\u201d) VF . These features can be, for instance, the set of sentences comprising the corpus. We then define a matrix M of |VW | rows and |VF | columns. Each entry in M represents some statistic pertaining to that combination of word and feature. For example, Mw,f could be the number of times the word w appeared in the document f .\nThe matrix M is typically processed into a \u201csmarter\u201d matrix that reflects the strength of association between each given word w and feature f . We present three common association metrics: L1 row normalization (Equation (1)), Inverse Document Frequency (IDF, Equation (2)), and Pointwise Mutual Information (PMI, Equation (3)). The following equations show how to compute their respective matrices:\nML1w,f = I(w,f) I(w,\u2217) (1)\nM IDFw,f = log |VF | I(w,\u2217) (2)\nMPMIw,f = log #(w,f)\u00b7#(\u2217,\u2217) #(w,\u2217)\u00b7#(\u2217,f) (3)\nwhere #(\u00b7, \u00b7) is the co-occurrence count function, I(\u00b7, \u00b7) is the co-occurrence indicator function, and \u2217 is a wildcard.3\nTo obtain word vectors of lower dimensionality (VF may be huge), the processed matrix is then decomposed, typically with SVD. An alternative way to create low-dimensional word vectors without explicitly constructing M is to use the negative sampling algorithm (SGNS) (Mikolov et al., 2013b).4 This algorithm factorizes MPMI using a weighted non-linear objective (Levy and Goldberg, 2014)."}, {"heading": "5.2 Reinterpreting the Dice Coefficient", "text": "In statistical machine translation, the Dice coefficient is commonly used as a baseline for word alignment (Och and Ney, 2003). Given sentence-aligned data, it provides a numerical measure of how likely two words \u2013 a source-language word ws and a target-language word wt \u2013 are each other\u2019s translation:\nDice(ws, wt) = 2\u00b7S(ws,wt)\nS(ws,\u2217)\u00b7S(\u2217,wt) (4)\nwhere S(\u00b7, \u00b7) is the number of aligned sentences in the data where both arguments occurred.\n3A function with a wildcard should be interpreted as the sum of all possible instantiations, e.g. I(w, \u2217) = \u2211\nx I(w,x).\n4For consistency with prior art, we refer to this algorithm as SGNS (skip-grams with negative sampling), even when it is applied without the skip-gram feature model.\nWe claim that this metric is mathematically equivalent to the dot-product of two L1-normalized sentence-ID word-vectors, multiplied by 2. In other words, if we use the combination of sentence-ID features and L1-normalization to create our word vectors, then for any ws and wt:\nws \u00b7 wt = Dice(ws,wt)\n2 (5)\nTo demonstrate this claim, let us look at the dot-product of ws and wt:\nws \u00b7 wt = \u2211\ni\n(\nI(ws,i) I(ws,\u2217) \u00b7 I(wt,i) I(wt,\u2217)\n)\n(6)\nwhere i is the index of an aligned sentence. Since I(ws, \u2217) = S(ws, \u2217) and I(wt, \u2217) = S(\u2217, wt), and both are independent of i, we can rewrite the equation as follows:\nws \u00b7 wt = \u2211 i I(ws,i)\u00b7I(wt,i)\nS(ws,\u2217)\u00b7S(\u2217,wt) (7)\nSince I(w, i) is an indicator function of whether the word w appeared in sentence i, it stands to reason that the product I(ws, i) \u00b7 I(wt, i) is an indicator of whether both ws and wt appeared in i. Ergo, the numerator of Equation (7) is exactly the number of aligned sentences in which both ws and wt occurred: S(ws, wt). Therefore:\nws \u00b7 wt = S(ws,wt) S(ws,\u2217)\u00b7S(\u2217,wt) = Dice(ws,wt)2 (8)\nThis theoretical result implies that the cross-lingual similarity function derived from embeddings based on sentence IDs is essentially a generalization of the Dice coefficient."}, {"heading": "5.3 SGNS with Sentence IDs", "text": "The Dice coefficient appears to be a particularly na\u0131\u0308ve variant of matrix-based methods that use sentence IDs. For example, Inverted Index (S\u00f8gaard et al., 2015)), which uses SVD over IDF followed by L2 normalization (instead of L1 normalization), shows significantly better performance. We propose using a third variant, sentence-ID SGNS (SID-SGNS), which simply applies SGNS (Mikolov et al., 2013b) to the word/sentence-ID matrix.\nTable 3 compares its performance (Bilingual SID-SGNS) to the other methods, and shows that indeed, this algorithm behaves similarly to other sentence-ID-based methods. We observe similar results for other variants as well, such as SVD over positive PMI (not shown)."}, {"heading": "5.4 Embedding Multiple Languages", "text": "Up until now, we used bilingual data to train cross-lingual embeddings, even though our parallel corpus (the Bible) is in fact multi-lingual. Can we make better use of this fact?\nAn elegant property of the sentence-ID feature set is that it is a truly inter-lingual representation. This means that multiple languages can be represented together in the same matrix before factorizing it. This raises a question: does dimensionality reduction over a multi-lingual matrix produce better cross-lingual vectors than doing so over a bilingual matrix?\nWe test our hypothesis by comparing the performance of embeddings trained with SID-SGNS over all 57 languages of the Bible corpus to that of the bilingual embeddings we used earlier. This consistently improves performance across all the development benchmarks, providing a 4.69% average increase in performance (Table 3). With this advantage, SID-SGNS performs significantly better than the other methods combined.5\nIn absolute terms, Multilingual SID-SGNS\u2019s performance is still very low. However, this experiment demonstrates that one way of making significant improvement in cross-lingual embeddings is by considering additional sources of information, such as the multi-lingual signal demonstrated here. We hypothesize that, regardless of the algorithmic approach, relying solely on sentence IDs from bilingual parallel corpora will probably not be able to improve much beyond IBM Model-1.\n5We observed a similar increase in performance when applying the multi-lingual signal to S\u00f8gaard et al.\u2019s (2015) IDF-based method and to SVD over positive PMI."}, {"heading": "6 Data Paradigms", "text": "In \u00a72, we assumed that using sentence-aligned data is a better approach than utilizing document-aligned data. Is this the case?\nTo compare the data paradigms, we run the same algorithm, SID-SGNS, also on document IDs from Wikipedia.6 We use the bilingual version for both data types to control for external effects. During evaluation, we use a common vocabulary for both sentence-aligned and document-aligned embeddings (their intersection).\nTable 4 shows that using sentence IDs from the Bible usually outperforms Wikipedia. This remarkable result, where a small amount of parallel sentences is enough to outperform one of the largest collections of multi-lingual texts in existence, indicates that document-aligned data is an inferior paradigm for translation-related tasks such as word alignment and dictionary induction."}, {"heading": "7 Conclusions", "text": "In this paper, we draw both empirical and theoretical parallels between modern cross-lingual word embeddings and traditional alignment algorithms. Our results suggest that apart from faster algorithms and more compact representations, recent cross-lingual word embedding algorithms are still unable to\n6We use the word-document matrix mined by S\u00f8gaard et al. (2015), which contains only a subset of our target languages: English, French, and Spanish.\noutperform the traditional methods. However, introducing our new multi-lingual signal considerably improves performance. Therefore, we hypothesize that the information in bilingual sentence-aligned data has been thoroughly mined by existing methods, and suggest that future work explore additional sources of information in order to make substantial progress."}], "references": [{"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Word alignment for english-turkish language pair", "author": ["Cakmak et al.2012] Talha Cakmak", "S\u00fcleyman Acar", "G\u00fclsen Eyrigit"], "venue": null, "citeRegEx": "Cakmak et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cakmak et al\\.", "year": 2012}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "A massively parallel corpus: The bible in 100 languages. Language Resources and Evaluation, 49(2):375\u2013395", "author": ["Christodouloupoulos", "Mark Steedman"], "venue": null, "citeRegEx": "Christodouloupoulos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Christodouloupoulos et al\\.", "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard2015] Stephan Gouws", "Anders S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Building a golden collection of parallel multi-language word alignments", "author": ["Graca et al.2008] Joao Graca", "Joana Pardal", "Lu\u0131\u0301sa Coheur", "Diamantino Caseiro"], "venue": null, "citeRegEx": "Graca et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graca et al\\.", "year": 2008}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "A gold standard for englishswedish word alignment", "author": ["Holmqvist", "Ahrenberg2011] Maria Holmqvist", "Lars Ahrenberg"], "venue": "NODALIDA", "citeRegEx": "Holmqvist et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Holmqvist et al\\.", "year": 2011}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of COLING", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "Koehn.,? \\Q2010\\E", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Guidelines for word aligment evaluation and manual alignment", "author": ["Adria de Gispert", "Rafael Banchs", "Jose Marino"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Lambert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2005}, {"title": "Neural word embeddings as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Hieu Pham", "Christopher Manning"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "An evaluation exercise for word alignment", "author": ["Mihalcea", "Pedersen2003] Rada Mihalcea", "Ted Pedersen"], "venue": "In HLT-NAACL", "citeRegEx": "Mihalcea et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2003}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "The Word-Space Model", "author": ["Magnus Sahlgren"], "venue": null, "citeRegEx": "Sahlgren.,? \\Q2006\\E", "shortCiteRegEx": "Sahlgren.", "year": 2006}, {"title": "Inverted indexing for cross-lingual nlp", "author": ["\u017deljko Agi\u0107", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Barbara Plank", "Bernd Bohnet", "Anders Johannsen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Bilingual distributed word representations from document-aligned comparable data", "author": ["Vuli\u0107", "Moens2016] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Distributed word representation learning for cross-lingual dependency parsing", "author": ["Xiao", "Guo2014] Min Xiao", "Yuhong Guo"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We also notice that several statistical alignment algorithms, such as IBM Model-1 (Brown et al., 1993), operate under the same data assumptions.", "startOffset": 82, "endOffset": 102}, {"referenceID": 10, "context": ", 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014).", "startOffset": 71, "endOffset": 138}, {"referenceID": 26, "context": ", 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014).", "startOffset": 71, "endOffset": 138}, {"referenceID": 26, "context": "It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013).", "startOffset": 117, "endOffset": 135}, {"referenceID": 22, "context": "Algorithms in this category try to leverage massive amounts of data to make up for the lack of lower-level alignments (S\u00f8gaard et al., 2015; Vuli\u0107 and Moens, 2016).", "startOffset": 118, "endOffset": 163}, {"referenceID": 21, "context": "We focus on this third category, because it does not require the strict assumption of word-aligned data (which is difficult to obtain), while still providing a cleaner and more accurate signal than document-level alignments (which have been shown, in monolingual data, to capture mainly syntagmatic relations (Sahlgren, 2006)).", "startOffset": 309, "endOffset": 325}, {"referenceID": 5, "context": "Recently, a simpler model, BilBOWA (Gouws et al., 2015), showed similar performance without using a hidden sentence-representation layer, giving it a dramatic speed advantage over its predecessors.", "startOffset": 35, "endOffset": 55}, {"referenceID": 7, "context": ", 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). The second category makes a much weaker assumption, document-level alignments, and uses comparable texts in different languages (not necessarily translations) such as Wikipedia articles or news reports of the same event. Algorithms in this category try to leverage massive amounts of data to make up for the lack of lower-level alignments (S\u00f8gaard et al., 2015; Vuli\u0107 and Moens, 2016). Algorithms in the third category take the middle ground; they use sentence-level alignments, common in legal translations and religious texts. Also known as \u201cparallel corpora\u201d, sentence-aligned data maps each sentence (as a whole) to its translation. We focus on this third category, because it does not require the strict assumption of word-aligned data (which is difficult to obtain), while still providing a cleaner and more accurate signal than document-level alignments (which have been shown, in monolingual data, to capture mainly syntagmatic relations (Sahlgren, 2006)). Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence\u2019s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final layer, while Chandar et al.", "startOffset": 72, "endOffset": 1575}, {"referenceID": 2, "context": "Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final layer, while Chandar et al. (2014) proposed a shallower autoencoder-based model, representing both source and target language sentences as the same intermediate sentence vector.", "startOffset": 138, "endOffset": 160}, {"referenceID": 2, "context": "Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final layer, while Chandar et al. (2014) proposed a shallower autoencoder-based model, representing both source and target language sentences as the same intermediate sentence vector. Recently, a simpler model, BilBOWA (Gouws et al., 2015), showed similar performance without using a hidden sentence-representation layer, giving it a dramatic speed advantage over its predecessors. BilBOWA is essentially an extension of skip-grams with negative sampling (SGNS) (Mikolov et al., 2013b), which simultaneously optimizes each word\u2019s similarity to its inter-lingual context (words that appeared in the aligned target language sentence) and its intra-lingual context (as in the original monolingual model). Luong et al. (2015) proposed a similar SGNS-based model over the same features.", "startOffset": 138, "endOffset": 841}, {"referenceID": 5, "context": "BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance from the word in question, and defines a slightly different interaction with target language words.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "This approach is implicitly used by Chandar et al. (2014), who encode the bag-of-words representation of two parallel sentences into the same vector.", "startOffset": 36, "endOffset": 58}, {"referenceID": 2, "context": "This approach is implicitly used by Chandar et al. (2014), who encode the bag-of-words representation of two parallel sentences into the same vector. Thus, each word is not matched directly to another word (as in the previous feature space), but rather used to create the sentence\u2019s language-independent representation; this is analogous to matching a word with a sentence ID. S\u00f8gaard et al. (2015) use similar features, document IDs, for leveraging comparable Wikipedia articles in different languages.", "startOffset": 36, "endOffset": 399}, {"referenceID": 5, "context": "1: BilBOWA (Gouws et al., 2015), BWESkipGram (Vuli\u0107 and Moens, 2016), Bilingual Autoencoders (Chandar et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 2, "context": ", 2015), BWESkipGram (Vuli\u0107 and Moens, 2016), Bilingual Autoencoders (Chandar et al., 2014), and Inverted Index (S\u00f8gaard et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 22, "context": ", 2014), and Inverted Index (S\u00f8gaard et al., 2015).", "startOffset": 28, "endOffset": 50}, {"referenceID": 7, "context": "We use 16 manually annotated word alignment datasets \u2013 Hansards2 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) \u2013 as well as 16 bilingual dictionaries from Wiktionary.", "startOffset": 98, "endOffset": 221}, {"referenceID": 13, "context": "We use 16 manually annotated word alignment datasets \u2013 Hansards2 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) \u2013 as well as 16 bilingual dictionaries from Wiktionary.", "startOffset": 98, "endOffset": 221}, {"referenceID": 1, "context": "We use 16 manually annotated word alignment datasets \u2013 Hansards2 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) \u2013 as well as 16 bilingual dictionaries from Wiktionary.", "startOffset": 98, "endOffset": 221}, {"referenceID": 0, "context": "In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence \u2013 this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993).", "startOffset": 255, "endOffset": 275}, {"referenceID": 0, "context": "In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence \u2013 this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not aligned with anything, whereas target language out-of-vocabulary words are given a default minimal similarity score, and never aligned to any candidate source language word in practice. We use the inverse of alignment error rate (1-AER) as described in Koehn (2010) to measure performance, where higher scores mean better alignments.", "startOffset": 256, "endOffset": 601}, {"referenceID": 14, "context": "Hyperparameters Levy et al. (2015) exposed a collection of hyperparameters that affect the performance of monolingual embeddings.", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "(Chandar et al., 2014)) used an even smaller dataset (the first 10K sentences in Europarl (Koehn, 2005)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": ", 2014)) used an even smaller dataset (the first 10K sentences in Europarl (Koehn, 2005)).", "startOffset": 75, "endOffset": 88}, {"referenceID": 0, "context": "IBM Model-1 (Brown et al., 1993) also makes exactly the same data assumptions as other sentence-ID methods.", "startOffset": 12, "endOffset": 32}, {"referenceID": 22, "context": "For example, Inverted Index (S\u00f8gaard et al., 2015)), which uses SVD over IDF followed by L2 normalization (instead of L1 normalization), shows significantly better performance.", "startOffset": 28, "endOffset": 50}, {"referenceID": 22, "context": "We observed a similar increase in performance when applying the multi-lingual signal to S\u00f8gaard et al.\u2019s (2015) IDF-based method and to SVD over positive PMI.", "startOffset": 88, "endOffset": 112}, {"referenceID": 22, "context": "Our results suggest that apart from faster algorithms and more compact representations, recent cross-lingual word embedding algorithms are still unable to We use the word-document matrix mined by S\u00f8gaard et al. (2015), which contains only a subset of our target languages: English, French, and Spanish.", "startOffset": 196, "endOffset": 218}], "year": 2016, "abstractText": "While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remains vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, is an appealing approach for substantially improving crosslingual word embeddings.", "creator": "LaTeX with hyperref package"}}}