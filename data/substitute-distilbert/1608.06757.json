{"id": "1608.06757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Robust Named Entity Recognition in Idiosyncratic Domains", "abstract": "named grammar recognition often fails in idiosyncratic sentences. that causes a problem for depending tasks, such as entity linking & relation extraction. we cite a generic and robust approach for high - recall named entity names. our approach is objective software train and promote strong generalization over diverse domain - specific language, such as computational web ( e. g. reuters ) or biomedical text ( e. g. medline ). our approach is based on deep contextual sequence learning and utilizes stacked bidirectional lstm networks. our model is trained with only few hundred labeled sentences and does not rely on privileged external knowledge. we report together our results f1 scores achieving the range of 84 - 94 % on input datasets.", "histories": [["v1", "Wed, 24 Aug 2016 09:06:14 GMT  (157kb,D)", "http://arxiv.org/abs/1608.06757v1", "8 pages, 1 figure"]], "COMMENTS": "8 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sebastian arnold", "felix a gers", "torsten kilias", "alexander l\\\"oser"], "accepted": false, "id": "1608.06757"}, "pdf": {"name": "1608.06757.pdf", "metadata": {"source": "CRF", "title": "Robust Named Entity Recognition in Idiosyncratic Domains", "authors": ["Sebastian Arnold", "Felix A. Gers", "Torsten Kilias", "Alexander L\u00f6ser"], "emails": ["sarnold@beuth-hochschule.de", "gers@beuth-hochschule.de", "tkilias@beuth-hochschule.de", "aloeser@beuth-hochschule.de"], "sections": [{"heading": "1 Introduction", "text": "Information extraction tasks have become very important not only in the Web, but also for inhouse enterprise settings. One of the crucial steps towards understanding natural language is named entity recognition (NER), which aims to extract mentions of entity names in text. NER is necessary for many higher-level tasks such as entity linking, relation extraction, building knowledge graphs, question answering and intent based search. In these scenarios, NER recall is critical, as candidates that are never generated can not be recovered later (Hachey et al., 2013).\nChallenges. Pink et al. (2014) show that NER components can reduce the search space for slot filling tasks by 99.8% with a recall loss of 15%. However, large effort is required to adapt most annotators to specialized domains, such as biomedical documents. When focusing on recall for these\ndomains, we face three major problems. First, the language used in the documents is often idiosyncratic and cannot be effectively identified by standard natural language processing (NLP) tools (Prokofyev et al., 2014). Second, training these domains is difficult: data is sparse, data may contain a large number of non-linkable entity mentions (NILs) and large labeled gold standards are hardly available. Third, applications vary greatly and we cannot standardize annotation guidelines to meet all of their requirements (Ling et al., 2015). For example, NER on news texts might focus on proper named entity annotation (e.g. people, companies and locations), whereas phrase recognition on medical text might include the annotation of common concepts (e.g. medical terms and treatments). We therefore focus on a generalized NER component with high recall, which can be trained ad-hoc with only few labeled examples.\nCommon error analysis. Ling et al. (2015) point out common errors of NER systems, which yield non-recognized mentions (false negatives), invalid detections (false positives), wrong boundaries (e.g. multi-word mentions, missing determiners) and annotation errors from human labelers (e.g., correct answers are not marked as correct, unclear annotation guidelines). Consider the following example taken from the biomedical GENIA corpus (Ohta et al., 2002), with underlined named entity mentions:\nExample: Engagement of the Lewis X antigen (CD15) results in monocyte activation. Nuclear extracts of antiCD15 cross-linked cells demonstrated enhanced levels of the transcriptional factor activator protein-1, minimally changed nuclear factor-kappa B, and did not affect SV40 promoter specific protein-1.\nar X\niv :1\n60 8.\n06 75\n7v 1\n[ cs\n.C L\n] 2\n4 A\nug 2\n01 6\nWe observe that common errors originate from a manifold number of sources, which are frequently:\n\u2022 non-verbatim mentions (e.g. misspellings, alternate writings: monoyctes, Lewis-X) \u2022 part-of-speech (POS) tagging errors (e.g.\nunidentified NP tags: monoycte/JJ) \u2022 wrong capitalization (e.g. uppercase head-\nlines, lowercase proper names) \u2022 unseen or novel words (e.g. idiosyncratic lan-\nguage: anti-CD15) \u2022 irregular word context (e.g. collapsed lists,\nsemi-structured data, invalid segmentation)\nOur contribution. We contribute DATEXISNER, a generic annotator for robust named entity recognition that can be trained for various domains with low human labeling effort. DATEXIS-NER does not depend on domain-specific rules, dictionaries, fine-tuning, syntactic annotation or external knowledge bases. Instead, our approach is built from scratch and is based on core character features of text. We train our model for news and biomedical domains with raw text data and few hundred labels. From our results, we report equal performance compared to state-of-the-art NER annotators with high 90% F1 scores for common NER corpora, such as CoNLL2003, KORE50, ACE2004 and MSNBC. We show on the highly domain-specific biomedical GENIA corpus that our approach adapts to various idiosyncratic domains. In particular, we observe that bidirectional long short-term memory (LSTM) networks capture useful distributional context for NER applications and generic letter-trigram word encoding with surface forms compensates typing and capitalization errors. With a combination of these techniques, we achieve better context representation than word2vec models trained with significantly larger corpora.\nThe rest of this paper is structured as follows. In Section 2, we discuss related work. We introduce our approach of robust named entity recognition in Section 3. In Section 4, we evaluate our approach compared to state-of-the-art annotators and discuss the most common errors in the components of our system. We conclude in Section 5 and propose future work on our approach."}, {"heading": "2 Related Work", "text": "Named entity recognition. The task of NER has been extensively studied with various evaluation in the last decades: MUC-6, MUC-7,\nCoNLL2002, CoNLL2003 and ACE. The standard approach to NER is the application of discriminative tagging (Collins, 2002) to the task of NER (McCallum and Li, 2003), often with linear chain Conditional Random Field (CRF), Hidden Markov (HMM) or Maximum Entropy Hidden Markov Models (MEMM). Later, Bengio et al. (2003) used continuous-space language models, where type-to-vector word mappings can be learned using backpropagation. Mikolov et al. (2013) achieved a more effective vector representation using the skip-gram model. The model optimizes the likelihood of tokens over a window surrounding a given token. This training process produces a linear classifier that predicts words conditioned on the central token\u2019s vector representation.\nRecall bounds for idiosyncratic entity linking. Named entity linking is the task to match textual mentions of named entities to a knowledge base (Shen et al., 2015). This task requires a set of candidate mentions from sentences. As a result, the recall from the underlying NER system constitutes an upper bound for entity linking accuracy (Hachey et al., 2013). Moreover, Pink et al. (2014) show that \u201cstate-of-the-art systems are substantially limited by low recall\u201d and don\u2019t perform well especially on idiosyncratic data while Prokofyev et al. (2014) highlight that terms with high novelty or high specificity cannot efficiently be linked by current systems.\nState-of-the-art NER implementations. We distinguish between three broad categories for generating candidate entities: Babelfy (Moro et al., 2014), Entityclassifier.eu (Dojchinovski and Kliegr, 2013), DBpedia Spotlight (Mendes et al., 2011) or TagMe2 (Ferragina and Scaiella, 2010) spot noun chunks and filter them with dictionaries, often derived from Wikipedia. Stanford NER (Manning et al., 2014) or LingPipe1 utilize discriminative tagging approaches. FOX (Speck and Ngomo, 2014) or NERD-ML (Van Erp et al., 2013) combine several approaches in an ensemble learner for enhancing precision. The GENIA tagger2 is a tagger specifically tuned for biomedical text. It is trained on the GENIA-based BioNLP/NLPBA 2004 data set (Kim et al., 2004) that includes named entity recognition for biomedical text. The biomedical NER system of Zhou and\n1http://alias-i.com/lingpipe/ 2http://www.nactem.ac.uk/tsujii/GENIA/tagger/\nSu (2004) is built using HMM and an additional SVM with sigmoid. It uses lexical-level features, e.g. word formation and morphological patterns, and utilizes dictionaries. The system of Finkel et al. (2004) uses a MEMM. Settles (2004) use CRF classifiers with syntactical features and synset dictionaries. Basically, all these systems benefit from our work."}, {"heading": "3 Robust Contextual Word Labeling", "text": "We abstract the task of NER as sequential word labeling problem. Figure 1 illustrates an example for sequential transformation of a sentence into word labels. We express each sentence in a document as a sequence of words: w = (w0, w1, . . . , wn), e.g. w0 = Aspirin. We define a mention as the longest possible span of adjacent tokens that refer to a an entity or relevant concept of a real-world object, such as Aspirin (ASA). We further assume that mentions are non-recursive and non-overlapping. To encode boundaries of the mention span, we adapt the idea of Ramshaw and Marcus (1995), which has been adapted as BIO2 standard in the CoNLL2003 shared task (Tjong Kim Sang and De Meulder, 2003). We assign labels {B, I,O} to each token to mark begin, inside and outside of a mention from left to right. We use the input sequence w together with a target sequence y of the same length that contains a BIO2 label for each word: y = (y0, y1, . . . , yn), e.g. y0 = B. To predict the most likely label y\u0302t of a token regarding its context, we utilize recurrent neural networks."}, {"heading": "3.1 Robust Word Encoding Methods", "text": "We have shown that most common errors for recall loss are misspellings, POS errors, capitalization, unseen words and irregular context. Therefore we generalize our model throughout three layers: robust word encoding, in-sentence word context and contextual sequence labeling.\nLetter-trigram word hashing to overcome spelling errors. Dictionary-based word vectorization methods suffer from sparse training sets, especially in the case of non-verbatim mentions, rare words, typing and capitalization errors. For example, the word2vec model of Mikolov et al. (2013) generalizes insufficiently for rare words in idiosyncratic domains or for misspelled words, since for these words no vector representation is learned at training time. In the GENIA data set, we notice 27% unseen words (dictionary misses) in\ntokenized into words wt and converted into word vectors xt using letter-trigram hashing. These vectors are propagated through a recurrent neural network with four layers: (1) feed-forward encoding of word vectors, (2+3) bidirectional LSTM layers for context representation using forward and backward passes, (4) LSTM decoder layer for contextsensitive label prediction. The output labels yt follow the BIO2 standard and represent mention begin (B), inside (I) and outside (O) per token.\nthe pretrained word2vec model3. As training data generation is expensive, we investigate a generic approach for the generation of word vectors. We use letter-trigram word hashing as introduced by Huang et al. (2013). This technique goes beyond words and generates word vectors as a composite of discriminative three-letter \u201csyllables\u201d, that might also include misspellings. Therefore, it is robust against dictionary misses and has the advantage (despite its name) to group syntactically similar words in similar vector spaces. We com-\n3GoogleNews-vectors-negative300 embeddings (3.6 GB)\npare this approach to word embedding models such as word2vec.\nSurface form features for word vector robustness. The most important features for NER are word shape properties, such as length, initial capitalization, all-word uppercase, in-word capitalization and use of numbers or punctuation (Ling and Weld, 2012). Mixed-case word encodings implicitly include capitalization features. However, this approach impedes generalization, as words appear in various surface forms, e.g. capitalized at the beginning of sentences, uppercase in headlines, lowercase in social media text. The strong coherence between uppercase and lowercase characters \u2013 they might have identical semantics \u2013 is not encoded in the embedding. Therefore, we encode the words using lowercase letter-trigrams. To keep the surface information, we add flag bits to the vector that indicate initial capitalization, uppercase, lower case or mixed case."}, {"heading": "3.2 Deep Contextual Sequence Learning", "text": "With sparse training data in the idiosyncratic domain, we expect input data with high variance. Therefore, we require a strong generalization for the syntactic and semantic representation of language. To reach into the high 80\u201390% NER F1 performance, long-range context-sensitive information is indispensable. We apply the computational model of recurrent neural networks, in particular long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Gers et al., 2002) to the problem of sequence labeling. Like neural feed-forward networks, LSTMs are able to learn complex parameters using gradient descent, but include additional recurrent connections between cells to influence weight updates over adjacent time steps. With their ability to memorize and forget over time, LSTMs have proven to generalize context-sensitive sequential data well (Graves, 2012; Lipton and Berkowitz, 2015).\nFigure 1 shows an unfolded representation of the steps through a sentence. We feed the LSTM with letter-trigram vectors xt as input data, one word at a time. The hidden layer of the LSTM represents context from long range dependencies over the entire sentence from left to right. However, to achieve deeper contextual understanding over the boundaries of multi-word annotations and at the beginning of sentences, we require a back-\nwards pass through the sentence. We therefore implement a bidirectional LSTM and feed the output of both directions into a second LSTM layer for combined label prediction.\nBidirectional sequence learning. For the use in the neural network, word encodings xt and labels yt are real-valued vectors. To predict the most likely label y\u0302t of a token, we utilize a LSTM with input nodes gt, input gates it, forget gate ft, output gate ot and internal state st. For the bidirectional case, all gates are duplicated and combined into forward state ht and backward state zt. The network is trained using backpropagation through time (BPTT) by adapting weights W and bias parameters b to fit the training examples.\ngt = \u03c6(Wgxxt +Wghht\u22121 + bg)\nit = \u03c3(Wixxt +Wihht\u22121 + bi)\nft = \u03c3(Wfxxt +Wfhht\u22121 + bf )\not = \u03c3(Woxxt +Wohht\u22121 + bo)\nst = \u03c6(gt it + st\u22121 ft) ht = ~st ~ot / zt = ~st ~ot yt = softmax (Wyhht +Wyzzt + by)\n(1)\nWe iterate over labeled sentences in mini-batches and update the weights accordingly. The network is then used to predict label probabilities yt for unseen word sequences wt."}, {"heading": "3.3 Implementation of NER Components", "text": "To show the impact of our bidirectional LSTM model, we measure annotation performance on three different neural network configurations. We implement all components using the Deeplearning4j framework4. For preprocessing (sentence and word tokenization), we use Stanford CoreNLP5 (Manning et al., 2014). We test the sequence labeler using three input encodings:\n\u2022 DICT: We build a dictionary over all words in the corpus and generate the input vector using 1-hot encoding for each word \u2022 EMB: We use the GoogleNews word2vec\nembeddings, which encodes each word as vector of size 300 \u2022 TRI: we implement letter-trigram word hash-\ning as described in Section 3.1.\n4http://deeplearning4j.org, version 0.4-rc3.9-SNAPSHOT 5version 3.6.0\nDuring training and test, we group all tokens of a sentence as mini-batch. We evaluate three different neural network types to show the impact of the bidirectional sequence learner.\n\u2022 FF: As baseline, we train a non-sequential feed-forward model based on a fully connected multilayer perceptron network with 3 hidden layers of size 150 with relu activation, feeding into a 3-class softmax classifier. We train the model using backpropagation with stochastic gradient descent and a learning rate of 0.005. \u2022 LSTM: We use a configuration of a sin-\ngle feed-forward layer of size 150 with two additional layers of single-direction LSTM with 20 cells and a 3-class softmax classifier. We train the model using backpropagationthrough-time (BPTT) with stochastic gradient descent and a learning rate of 0.005. \u2022 BLSTM: Our final configuration consists of\na single feed-forward layer of size 150 with one bidirectional LSTM layer with 20 cells and an additional single-direction LSTM with 20 cells into a 3-class softmax classifier. The BLSTM model is trained the same way as the single-direction LSTM."}, {"heading": "4 Evaluation", "text": "We evaluate nine configurations of our model on five gold standard evaluation data sets. We show that the combination of letter-trigram word hashing with bidirectional LSTM yields the best results and outperforms sequence learners based on dictionaries or word2vec. To highlight the generalization of our model to idiosyncratic domains, we run tests on common-typed data sets as well as on specialized medical documents. We compare our system on these data sets with specialized state-ofthe-art systems."}, {"heading": "4.1 Evaluation Set Up", "text": "We train two models with identical parameterization, each with 2000 randomly chosen labeled sentences from a standard data set. To show the effectiveness of the components, we evaluate different configurations of this setting with 2000 random sentences from the remaining set. The model was trained using Deeplearning4j with nd4j-x86 backend. Training the TRI+BLSTM configuration on a commodity Intel i7 notebook with 4 cores at 2.8GHz takes approximately 50 minutes.\nEvaluation data sets. Table 1 gives an overview of the standard data sets we use for training. The GENIA Corpus (Ohta et al., 2002) contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 (Kim et al., 2004) is a standard NER dataset based on the Reuters RCV1 news corpus. It covers named entities of type person, location, organization and misc.\nFor testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. Additionally, we test on the complete KORE50 (Hoffart et al., 2012), ACE2004 (Mitchell et al., 2005) and MSNBC data sets using the GERBIL evaluation framework (Usbeck et al., 2015)."}, {"heading": "4.2 Measurements", "text": "We measure precision, recall and F1 score of our DATEXIS-NER system and state-of-the-art annotators introduced in Section 2. For the comparison with black box systems, we evaluate annotation results using weak annotation match. For a more detailed in-system error analysis, we measure BIO2 labeling performance based on each token.\nMeasuring annotation performance using NER-style F1. We measure the overall performance of mention annotation using the evaluation measures defined by Cornolti et al. (2013), which are also used by Ling et al. (2015). Let D be a set of documents with gold standard mention annotations G = {Gd | d \u2208 D} with a total of N = |G| examples. Each mention gi \u2208 G is defined by start position b and end position\ne in the source document d. To quantify the performance of the system, we compare G to the set of predicted annotations P = {Pd | d \u2208 D} with mentions pi \u2208 P:\ntpd = |{p \u2208 Pd | \u2203g \u2208 Gd : m(p, g)}| fpd = |{p \u2208 Pd | @g \u2208 Gd : m(p, g)}| tnd = |{p /\u2208 Pd | @g \u2208 Gd : m(p, g)}| fnd = |{g \u2208 Gd | @p \u2208 Pd : m(g, p)}| (2)\nWe compare using a weak annotation match:\nm : (p, g) 7\u2192(bp \u2264 bg \u2264 ep)\u2228 (bp \u2264 eg \u2264 ep)\u2228 (bg \u2264 bp \u2264 eg)\u2228 (bq \u2264 ep \u2264 eg)\n(3)\nWe measure micro-averaged precision (Prec), recall (Rec) and NER-style (F1 ) score:\nPrec = \u2211 d\u2208D tpd\u2211\nd\u2208D (tpd + fpd )\nRec = \u2211 d\u2208D tpd\u2211\nd\u2208D (tpd + fnd )\nF1 = 2 \u00b7 Prec \u00b7 Rec Prec + Rec\n(4)\nMeasuring BIO2 labeling performance. Tuning the model configuration with annotation match measurement is not always feasible. We therefore measure tpc, fpc, tnc, fnc separately for\neach label class c \u2208 {B, I,O} in our classification model and calculate binary classification precision Precc, recall Recc and F1 c scores. To avoid skewed results from the expectedly large O class, we use macro-averaging over the three classes:\nPrecBIO = 1\n3 \u2211 c\u2208{B,I,O} tpc tpc + fpc\nRecBIO = 1\n3 \u2211 c\u2208{B,I,O} tpc tpc + fnc\n(5)"}, {"heading": "4.3 Evaluation Results", "text": "We now discuss the evaluation of our DATEXISNER system on common and idiosyncratic data.\nOverall model performance on common types. Table 2 shows the comparison of DATEXIS-NER with eight state-of-the-art annotators on four common news data sets. Both common and medical models are configured identically and trained on only 2000 labeled sentences, without any external prior knowledge. We observe that DATEXISNER achieves the highest recall scores of all tested annotators, with 95%\u201398% on all measured data sets. Moreover, DATEXIS-NER precision scores are equal or better than median. Overall, we achieve high micro-F1 scores of 84%\u201394% on news entity recognition, which is slightly better than the ontology-based Entityclassifier.eu NER and reveals a better generalization than the 3-type Stanford NER with distributional semantics. We\nnotice that systems specialized on word-sense disambiguation (Babelfy, DBpedia Spotlight) don\u2019t perform well on \u201craw\u201d untyped entity recognition tasks. The highest precision scores are reached by Stanford NER. We also notice a low precision of all annotators on the ACE2004 dataset and high variance in MSNBC performance, which are probably caused by differing annotation standards.\nBiomedical recognition performance. Table 3 shows the results of biomedical entity recognition compared to the participants of the JNLPBA 2004 bio-entity recognition task (Kim et al., 2004). We notice that for these well-written Medline abstracts, there is not such a strong skew between precision and recall. Our DATEXIS-NER system outperforms the HMM, MEMM, CRF and CDN based models with a micro-F1 score of 84%. However, the highly specialized GENIA chunker for LingPipe achieves higher scores. This chunker is a very simple generative model predictor that is based on a sliding window of two tokens, word shape and dictionaries. We interpret this score as strong overfitting using a dictionary of the welldefined GENIA terms. Therefore, this model will generalize hardly considering the simple model. We can confirm this presumption in the common data sets, where the MUC-6 trained HMM LingPipe chunker performs on average on unseen data.\nEvaluation of system components. We evaluate different configurations of the components that we describe in Section 3.3. Table 4 shows the results of experiments on both CoNLL2003 and GENIA data sets. We report the highest macroF1 scores for BIO2 labeling for the configuration\nof letter-trigram word vectors and bidirectional LSTM. We notice that dictionary-based word encodings (DICT) work well for idiosyncratic medical domains, whereas they suffer from high word ambiguity in the news texts. Pretrained word2vec embeddings (EMB) perform well on news data, but cannot adapt to the medical domain without retraining, because of a large number of unseen words. Therefore, word2vec generally achieves a high precision on news texts, but low recall on medical text. The letter-trigram approach (TRI) combines both word vector generalization and robustness towards idiosyncratic language.\nWe observe that the contextual LSTM model achieves scores throughout in the 85%\u201394% range and significantly outperforms the feed-forward (FF) baseline that shows a maximum of 75%. Bidirectional LSTMs can further improve label classification in both precision and recall."}, {"heading": "4.4 Discussion and Error Analysis", "text": "We investigate different aspects of the DATEXISNER components by manual inspection of classification errors in the context of the document. For the error classes described in the introduction (false negative detections, false positives and invalid boundaries), we observe following causes:\nUnseen words and misspellings. In dictionary based configurations (e.g. 1-hot word vector encoding DICT), we observe false negative predictions caused by dictionary misses for words that do not exist in the training data. The cause can be rare unseen or novel words (e.g. T-prolymphocytic cells) or misspellings (e.g. strengthnend). These words\nyield a null vector result from the encoder and can therefore not be distinguished by the LSTM. The error increases when using word2vec, because these models are trained with stop words filtered out. This implicates that e.g. mentions surrounded by or containing a determiner (e.g. The Sunday Telegraph quoted Majorie Orr) are highly error prone towards the detection of their boundaries. We resolve this error by the letter-trigram approach. Unseen trigrams (e.g. thh) may still be missing in the word vector, but only affect single dimensions as opposed to the vector as a whole.\nMisleading surface form features. Surface forms encode important features for NER (e.g. capitalization of \u201cnew\u201d in Alan Shearer was named as the new England captain / as New York beat the Angels). However, case-sensitive word vectorization methods yield a large amount of false positive predictions caused by incorrect capitalization in the input data. An uppercase headline (e.g. TENNIS - U.S. TEAM ON THE ROAD FOR 1997 FED CUP) is encoded completely different than a lowercase one (e.g. U.S. team on the road for Fed Cup). Because of that, we achieve best results with lowercase word vectors and additional surface form feature flags, as described in Section 3.1.\nSyntagmatic word relations. We observe mentions that are composed of co-occurring words with high ambiguity (e.g. degradation of IkB alpha in T cell lines). These groups encode strong syntagmatic word relations (Sahlgren, 2008) that can be leveraged to resolve word sense and homonyms from sentence context. Therefore, correct boundaries in these groups can effectively be identified only with contextual models such as LSTMs.\nParadigmatic word relations. Orthogonal to the previous problem, different words in a paradigmatic relation (Sahlgren, 2008) can occur in the same context (e.g. cyclosporin A-treated cells / HU treated cells). These groups are efficiently represented in word2vec. However, letter-trigram vectors cannot encode paradigmatic groups and therefore require a larger training sample to capture these relations.\nContext boundaries. Often, synonyms can only be resolved regarding a larger document context than the local sentence context known by the LSTM. In these cases, word sense is redefined by a topic model local to the paragraph (e.g. sports:\nTiger was lost in the woods after divorce.). This problem does not heavily affect NER recall, but is crucial for named entity disambiguation and coreference resolution.\nLimitations. The proposed DATEXIS-NER model is restricted to recognize boundaries of generic mentions in text. We evaluate the model on annotations of isolated types (e.g. persons, organizations, locations) for comparison purposes only, but we do not approach NER-style typing. Contrary, we approach to detect mentions without type information. The detection of specific types can be realized by training multiple independent models on a selection of labels per type and nesting the resulting annotations using a longest-span semantic type heuristic (Kholghi et al., 2015)."}, {"heading": "5 Summary", "text": "Ling et al. (2015) show that the task of NER is not clearly defined and rather depends on a specific problem context. Contrary, most NER approaches are specifically trained on fixed datasets in a batch mode. Worse, they often suffer from poor recall (Pink et al., 2014). Ideally, one could personalize the task of recognizing named entities, concepts or phrases according to the specific problem. \u201cPersonalizing\u201d and adapting such annotators should happen with very limited human labeling effort, in particular for idiosyncratic domains with sparse training data.\nOur work follows this line. From our results we report F1 scores between 84\u201394% when using bidirectional multi-layered LSTMs, letter-trigram word hashing and surface form features on only few hundred training examples.\nThis work is only a preliminary step towards the vision of personalizing annotation guidelines for NER (Ling et al., 2015). In our future work, we will focus on additional important idiosyncratic domains, such as health, life science, fashion, engineering or automotive. For these domains, we will consider the process of detecting mentions and linking them to an ontology as a joint task and we will investigate simple and interactive workflows for creating robust personalized named entity linking systems.\nAcknowledgements Our work is funded by the Federal Ministry of Economic Affairs and Energy (BMWi) under grant agreement 01MD15010B (Project: Smart Data Web)."}], "references": [{"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins"], "venue": "In EMNLP\u201902,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "A Framework for Benchmarking Entity-Annotation Systems", "author": ["Paolo Ferragina", "Massimiliano Ciaramita"], "venue": "In WWW\u201913,", "citeRegEx": "Cornolti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cornolti et al\\.", "year": 2013}, {"title": "Entityclassifier. eu: Real-Time Classification of Entities in Text with Wikipedia", "author": ["Dojchinovski", "Kliegr2013] Milan Dojchinovski", "Tom\u00e1\u0161 Kliegr"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Dojchinovski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dojchinovski et al\\.", "year": 2013}, {"title": "TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities)", "author": ["Ferragina", "Scaiella2010] Paolo Ferragina", "Ugo Scaiella"], "venue": "In CIKM\u201910,", "citeRegEx": "Ferragina et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferragina et al\\.", "year": 2010}, {"title": "Exploiting Context for Biomedical Entity Recognition: from Syntax to the Web", "author": ["Finkel et al.2004] Jenny Finkel", "Shipra Dingare", "Huy Nguyen", "Malvina Nissim", "Christopher Manning", "Gail Sinclair"], "venue": "In JNLPBA\u201904,", "citeRegEx": "Finkel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2004}, {"title": "Learning Context Sensitive Languages with LSTM Trained with Kalman Filters", "author": ["Gers et al.2002] Felix Gers", "Juan Antonio Perez-Ortiz", "Douglas Eck", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Gers et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2002}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Evaluating Entity Linking with Wikipedia", "author": ["Hachey et al.2013] Ben Hachey", "Will Radford", "Joel Nothman", "Matthew Honnibal", "James R. Curran"], "venue": "Artificial intelligence,", "citeRegEx": "Hachey et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hachey et al\\.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "KORE: Keyphrase Overlap Relatedness for Entity Disambiguation", "author": ["Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum"], "venue": "In CIKM\u201912,", "citeRegEx": "Hoffart et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2012}, {"title": "Learning Deep Structured Semantic Models for Web Search using Clickthrough Data", "author": ["Huang et al.2013] Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In CIKM\u201913,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "External Knowledge and Query Strategies in Active Learning: a Study in Clinical Information Extraction", "author": ["Laurianne Sitbon", "Guido Zuccon", "Anthony Nguyen"], "venue": "In CIKM\u201915,", "citeRegEx": "Kholghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kholghi et al\\.", "year": 2015}, {"title": "Introduction to the Bio-Entity Recognition Task at JNLPBA", "author": ["Kim et al.2004] Jin-Dong Kim", "Tomoko Ohta", "Yoshimasa Tsuruoka", "Yuka Tateisi", "Nigel Collier"], "venue": "In JNLPBA\u201904,", "citeRegEx": "Kim et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2004}, {"title": "Fine-Grained Entity Recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Design Challenges for Entity Linking", "author": ["Ling et al.2015] Xiao Ling", "Sameer Singh", "Daniel S Weld"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A Critical Review of Recurrent Neural Networks for Sequence Learning", "author": ["Lipton", "Berkowitz2015] Zachary C. Lipton", "John Berkowitz"], "venue": null, "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In ACL System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-enhanced Lexicons", "author": ["McCallum", "Li2003] Andrew McCallum", "Wei Li"], "venue": "CONLL", "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "DBpedia Spotlight: Shedding Light on the Web of Documents", "author": ["Max Jakob", "Andr\u00e9s Garcia-Silva", "Christian Bizer"], "venue": "I-Semantics", "citeRegEx": "Mendes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mendes et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Entity Linking meets Word Sense Disambiguation: a Unified Approach", "author": ["Moro et al.2014] Andrea Moro", "Alessandro Raganato", "Roberto Navigli"], "venue": null, "citeRegEx": "Moro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "The GENIA Corpus: An Annotated Research Abstract Corpus in Molecular Biology Domain", "author": ["Ohta et al.2002] Tomoko Ohta", "Yuka Tateisi", "JinDong Kim"], "venue": "In International Conference on Human Language Technology Research", "citeRegEx": "Ohta et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ohta et al\\.", "year": 2002}, {"title": "Analysing Recall Loss in Named Entity Slot Filling", "author": ["Pink et al.2014] Glen Pink", "Joel Nothman", "James R. Curran"], "venue": "In EMNLP\u201914,", "citeRegEx": "Pink et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pink et al\\.", "year": 2014}, {"title": "Effective Named Entity Recognition for Idiosyncratic Web Collections", "author": ["Gianluca Demartini", "Philippe Cudr\u00e9-Mauroux"], "venue": "In WWW\u201914,", "citeRegEx": "Prokofyev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prokofyev et al\\.", "year": 2014}, {"title": "Text chunking using transformation-based learning", "author": ["Ramshaw", "Marcus1995] Lance A. Ramshaw", "Mitchell P. Marcus"], "venue": "In WVLC\u201995. ACL", "citeRegEx": "Ramshaw et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ramshaw et al\\.", "year": 1995}, {"title": "The Distributional Hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics,", "citeRegEx": "Sahlgren.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Biomedical Named Entity Recognition Using Conditional Random Fields and Rich Feature Sets", "author": ["Burr Settles"], "venue": "In JNLPBA\u201904,", "citeRegEx": "Settles.,? \\Q2004\\E", "shortCiteRegEx": "Settles.", "year": 2004}, {"title": "Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions", "author": ["Shen et al.2015] Wei Shen", "Jianyong Wang", "Jiawei Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Ensemble Learning for Named Entity Recognition", "author": ["Speck", "Ngomo2014] Ren\u00e9 Speck", "AxelCyrille Ngonga Ngomo"], "venue": "In ISWC\u201914,", "citeRegEx": "Speck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Speck et al\\.", "year": 2014}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In CoNLL\u201903,", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "GERBIL: General Entity Annotator Benchmarking Framework", "author": ["Roberto Navigli", "Francesco Piccinno", "Giuseppe Rizzo", "Harald Sack", "Ren\u00e9 Speck", "Rapha\u00ebl Troncy", "J\u00f6rg Waitelonis", "Lars Wesemann."], "venue": "WWW\u201915, pages 1133\u20131143,", "citeRegEx": "Navigli et al\\.,? 2015", "shortCiteRegEx": "Navigli et al\\.", "year": 2015}, {"title": "Learning with the Web: Spotting Named Entities on the Intersection of NERD and Machine Learning", "author": ["Giuseppe Rizzo", "Rapha\u00ebl Troncy"], "venue": "In #MSM\u201913,", "citeRegEx": "Erp et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Erp et al\\.", "year": 2013}, {"title": "Exploring Deep Knowledge Resources in Biomedical Name Recognition", "author": ["Zhou", "Su2004] GuoDong Zhou", "Jian Su"], "venue": "In JNLPBA\u201904,", "citeRegEx": "Zhou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "In these scenarios, NER recall is critical, as candidates that are never generated can not be recovered later (Hachey et al., 2013).", "startOffset": 110, "endOffset": 131}, {"referenceID": 23, "context": "First, the language used in the documents is often idiosyncratic and cannot be effectively identified by standard natural language processing (NLP) tools (Prokofyev et al., 2014).", "startOffset": 154, "endOffset": 178}, {"referenceID": 14, "context": "Third, applications vary greatly and we cannot standardize annotation guidelines to meet all of their requirements (Ling et al., 2015).", "startOffset": 115, "endOffset": 134}, {"referenceID": 20, "context": "Pink et al. (2014) show that NER components can reduce the search space for slot filling tasks by 99.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Consider the following example taken from the biomedical GENIA corpus (Ohta et al., 2002), with underlined named entity mentions:", "startOffset": 70, "endOffset": 89}, {"referenceID": 13, "context": "Ling et al. (2015) point out common errors of NER systems, which yield non-recognized mentions (false negatives), invalid detections (false positives), wrong boundaries (e.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "The standard approach to NER is the application of discriminative tagging (Collins, 2002) to the task of NER (McCallum and Li, 2003), often with linear chain Conditional Random Field (CRF), Hidden Markov (HMM) or Maximum Entropy Hidden Markov Models (MEMM).", "startOffset": 74, "endOffset": 89}, {"referenceID": 0, "context": "The standard approach to NER is the application of discriminative tagging (Collins, 2002) to the task of NER (McCallum and Li, 2003), often with linear chain Conditional Random Field (CRF), Hidden Markov (HMM) or Maximum Entropy Hidden Markov Models (MEMM). Later, Bengio et al. (2003) used continuous-space language models, where type-to-vector word mappings can be learned using backpropagation.", "startOffset": 75, "endOffset": 286}, {"referenceID": 0, "context": "The standard approach to NER is the application of discriminative tagging (Collins, 2002) to the task of NER (McCallum and Li, 2003), often with linear chain Conditional Random Field (CRF), Hidden Markov (HMM) or Maximum Entropy Hidden Markov Models (MEMM). Later, Bengio et al. (2003) used continuous-space language models, where type-to-vector word mappings can be learned using backpropagation. Mikolov et al. (2013) achieved a more effective vector representation using the skip-gram model.", "startOffset": 75, "endOffset": 420}, {"referenceID": 27, "context": "Named entity linking is the task to match textual mentions of named entities to a knowledge base (Shen et al., 2015).", "startOffset": 97, "endOffset": 116}, {"referenceID": 7, "context": "As a result, the recall from the underlying NER system constitutes an upper bound for entity linking accuracy (Hachey et al., 2013).", "startOffset": 110, "endOffset": 131}, {"referenceID": 7, "context": "As a result, the recall from the underlying NER system constitutes an upper bound for entity linking accuracy (Hachey et al., 2013). Moreover, Pink et al. (2014) show that \u201cstate-of-the-art systems are substantially limited by low recall\u201d and don\u2019t perform well especially on idiosyncratic data while Prokofyev et al.", "startOffset": 111, "endOffset": 162}, {"referenceID": 7, "context": "As a result, the recall from the underlying NER system constitutes an upper bound for entity linking accuracy (Hachey et al., 2013). Moreover, Pink et al. (2014) show that \u201cstate-of-the-art systems are substantially limited by low recall\u201d and don\u2019t perform well especially on idiosyncratic data while Prokofyev et al. (2014) highlight that terms with high novelty or high specificity cannot efficiently be linked by current systems.", "startOffset": 111, "endOffset": 325}, {"referenceID": 20, "context": "We distinguish between three broad categories for generating candidate entities: Babelfy (Moro et al., 2014), Entityclassifier.", "startOffset": 89, "endOffset": 108}, {"referenceID": 18, "context": "eu (Dojchinovski and Kliegr, 2013), DBpedia Spotlight (Mendes et al., 2011) or TagMe2 (Ferragina and Scaiella, 2010) spot noun chunks and filter them with dictionaries, often derived from Wikipedia.", "startOffset": 54, "endOffset": 75}, {"referenceID": 16, "context": "Stanford NER (Manning et al., 2014) or LingPipe1 utilize discriminative tagging approaches.", "startOffset": 13, "endOffset": 35}, {"referenceID": 12, "context": "It is trained on the GENIA-based BioNLP/NLPBA 2004 data set (Kim et al., 2004) that includes named entity recognition for biomedical text.", "startOffset": 60, "endOffset": 78}, {"referenceID": 4, "context": "The system of Finkel et al. (2004) uses a MEMM.", "startOffset": 14, "endOffset": 35}, {"referenceID": 4, "context": "The system of Finkel et al. (2004) uses a MEMM. Settles (2004) use CRF classifiers with syntactical features and synset dictionaries.", "startOffset": 14, "endOffset": 63}, {"referenceID": 19, "context": "For example, the word2vec model of Mikolov et al. (2013) generalizes insufficiently for rare words in idiosyncratic domains or for misspelled words, since for these words no vector representation is learned at training time.", "startOffset": 35, "endOffset": 57}, {"referenceID": 10, "context": "We use letter-trigram word hashing as introduced by Huang et al. (2013). This technique goes beyond words and generates word vectors as a composite of discriminative three-letter \u201csyllables\u201d, that might also include misspellings.", "startOffset": 52, "endOffset": 72}, {"referenceID": 5, "context": "We apply the computational model of recurrent neural networks, in particular long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Gers et al., 2002) to the problem of sequence labeling.", "startOffset": 117, "endOffset": 170}, {"referenceID": 6, "context": "With their ability to memorize and forget over time, LSTMs have proven to generalize context-sensitive sequential data well (Graves, 2012; Lipton and Berkowitz, 2015).", "startOffset": 124, "endOffset": 166}, {"referenceID": 16, "context": "For preprocessing (sentence and word tokenization), we use Stanford CoreNLP5 (Manning et al., 2014).", "startOffset": 77, "endOffset": 99}, {"referenceID": 21, "context": "The GENIA Corpus (Ohta et al., 2002) contains biomedical abstracts from the PubMed database.", "startOffset": 17, "endOffset": 36}, {"referenceID": 12, "context": "CoNLL2003 (Kim et al., 2004) is a standard NER dataset based on the Reuters RCV1 news corpus.", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "Additionally, we test on the complete KORE50 (Hoffart et al., 2012), ACE2004 (Mitchell et al.", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": "We measure the overall performance of mention annotation using the evaluation measures defined by Cornolti et al. (2013), which are also used by Ling et al.", "startOffset": 98, "endOffset": 121}, {"referenceID": 1, "context": "We measure the overall performance of mention annotation using the evaluation measures defined by Cornolti et al. (2013), which are also used by Ling et al. (2015). Let D be a set of documents with gold standard mention annotations G = {Gd | d \u2208 D} with a total of N = |G| examples.", "startOffset": 98, "endOffset": 164}, {"referenceID": 4, "context": "6 Finkel et al. (2004) MEMM 71.", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": "6 Finkel et al. (2004) MEMM 71.6 68.6 70.1 Settles et al. (2004) CRF 70.", "startOffset": 2, "endOffset": 65}, {"referenceID": 12, "context": "Table 3 shows the results of biomedical entity recognition compared to the participants of the JNLPBA 2004 bio-entity recognition task (Kim et al., 2004).", "startOffset": 135, "endOffset": 153}, {"referenceID": 25, "context": "These groups encode strong syntagmatic word relations (Sahlgren, 2008) that can be leveraged to resolve word sense and homonyms from sentence context.", "startOffset": 54, "endOffset": 70}, {"referenceID": 25, "context": "Orthogonal to the previous problem, different words in a paradigmatic relation (Sahlgren, 2008) can occur in the same context (e.", "startOffset": 79, "endOffset": 95}, {"referenceID": 11, "context": "The detection of specific types can be realized by training multiple independent models on a selection of labels per type and nesting the resulting annotations using a longest-span semantic type heuristic (Kholghi et al., 2015).", "startOffset": 205, "endOffset": 227}, {"referenceID": 22, "context": "Worse, they often suffer from poor recall (Pink et al., 2014).", "startOffset": 42, "endOffset": 61}, {"referenceID": 14, "context": "This work is only a preliminary step towards the vision of personalizing annotation guidelines for NER (Ling et al., 2015).", "startOffset": 103, "endOffset": 122}], "year": 2016, "abstractText": "Named entity recognition often fails in idiosyncratic domains. That causes a problem for depending tasks, such as entity linking and relation extraction. We propose a generic and robust approach for high-recall named entity recognition. Our approach is easy to train and offers strong generalization over diverse domainspecific language, such as news documents (e.g. Reuters) or biomedical text (e.g. Medline). Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM networks. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We report from our results F1 scores in the range of 84\u201394% on standard datasets.", "creator": "LaTeX with hyperref package"}}}