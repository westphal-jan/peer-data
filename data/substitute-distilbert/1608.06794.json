{"id": "1608.06794", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition", "abstract": "distributional models are derived from co - occurrences within a corpus, where defining a small criterion involves all possible plausible co - occurrences will identify identified. this results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. most methods face this challenge our ways as render the distributed word representations uninterpretable, with the consequence that semantic composition becomes hard to model. in this paper we suppose an alternative interpretation involves explicitly inferring unobserved co - occurrences using the distributional neighbourhood. arguments indicate that distributional inference improves sparse word representations on relevant word similarity benchmarks and demonstrate that our model is competitive with the state - of - the - art for adjective - noun, noun - noun and v - object compositions while being fully interpretable.", "histories": [["v1", "Wed, 24 Aug 2016 12:38:45 GMT  (53kb,D)", "http://arxiv.org/abs/1608.06794v1", "To appear at EMNLP 2016"]], "COMMENTS": "To appear at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thomas kober", "julie weeds", "jeremy reffin", "david j weir"], "accepted": true, "id": "1608.06794"}, "pdf": {"name": "1608.06794.pdf", "metadata": {"source": "CRF", "title": "Improving Sparse Word Representations with Distributional Inference for Semantic Composition", "authors": ["Thomas Kober", "Julie Weeds", "Jeremy Reffin", "David Weir"], "emails": ["d.j.weir}@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The aim of distributional semantics is to derive meaning representations based on observing cooccurrences of words in large text corpora. However not all plausible co-occurrences will be observed in any given corpus, resulting in word representations that only capture a fragment of the meaning of a word. For example the verbs \u201cwalking\u201d and \u201cstrolling\u201d may occur in many different and possibly disjoint contexts, although both verbs would be equally plausible in numerous cases. This subsequently results in incomplete representations for both lexemes. In addition, models based on counting co-occurrences face the general problem of sparsity in a very high-dimensional vector space. The most common approaches to these challenges have involved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014). The common problem in both of these approaches is that composition becomes a black-box process due to the lack of interpretability of the representations. Count-based models are therefore a very attractive line of work with regards to a number of important long-term research challenges, most notably the development of an adequate model of distributional compositional semantics. In this paper we propose the use of distributional inference (DI) to inject unobserved but plausible distributional semantic knowledge into the vector space by leveraging the intrinsic structure of the distributional neighbourhood. This results in richer word representations and furthermore mitigates the sparsity effect common in high-dimensional vector spaces, while remaining fully interpretable. Our contributions are as follows: we show that typed and untyped sparse word representations, enriched by distributional inference, lead to performance improvements on several word similarity benchmarks, and that a higher-order dependency-typed vector space model, based on \u201cAnchored Packed Dependency Trees (APTs)\u201d (Weir et al., 2016), is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions. Using our method, we are able to bridge the gap in performance between high dimensional interpretable modar X iv :1 60 8. 06 79 4v 1 [ cs .C L ] 2 4 A\nug 2\nels and low dimensional non-interpretable models and offer evidence to support a possible explanation of why high-dimensional models usually perform worse, together with a simple, practical method for over-coming this problem. We furthermore demonstrate that intersective approaches to composition benefit more from distributional inference than composition by union and highlight the ability of composition by intersection to disambiguate the meaning of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the APT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6."}, {"heading": "2 Related Work", "text": "Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In these works the authors are concerned with smoothing the probability estimate for unseen words in bigrams. This is achieved by measuring which unobserved bigrams are more likely than others on the basis of the Kullback-Leibler divergence between bigram distributions. This has led to significantly improved performance on a language modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997). More recently Pado\u0301 et al. (2013) used a distributional approach for smoothing derivationally related words, such as oldish \u2013 old, as a back-off strategy in case of data sparsity. However, none of these approaches have used distributional inference as a general technique for directly enriching sparse distributional vector representations, or have explored its behaviour for semantic composition. Compositional models of distributional semantics have become an increasingly popular topic in the research community. Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et\nal. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose APT framework is based on a higher-order dependency-typed vector space, however they do not address the issue of sparsity in their work."}, {"heading": "3 Background", "text": "Distributional vector space models can broadly be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pado\u0301 and Lapata (2007), Erk and Pado\u0301 (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model. In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with APTs as introduced by Weir et al. (2016), which we adopt in this paper. In addition to direct relations between two words, the APT model also considers inverse and higher\norder relations. Inverse relations are denoted with a horizontal bar above the dependency relation, such as amod for an inverse adjectival modifier. Higher order dependencies are separated by a colon as in the second order distributional feature dobj:nsubj. The example below illustrates how raw text is processed to retrieve elementary representations in our APT model. As an example we consider a lowercased corpus consisting of the sentences:\nwe folded the clean clothes i like your clothes we bought white shoes yesterday he folded the white sheets\nWe dependency parse the raw sentences and, following Weir et al. (2016), align and aggregate the resulting parse trees according to their dependency type as shown in Figure 1. For example the lexeme clothes has the distributional features amod:dry and dobj:nsubj:we among others. Over a large corpus, this results in a very high-dimensional and sparse vector space, which due to its typed nature is much sparser than for untyped models."}, {"heading": "Composition with APTs", "text": "Composition is linguistically motivated by the principle of compositionality, which states that the meaning of a complex expression is fully determined by its structure and the meanings of its constituents (Frege, 1884). Many simple approaches to semantic composition neglect the structure and lose information in the composition process. For example, the phrases house boat and boat house have the exact same representation when composition is done via a pointwise arithmetic operation. Despite\nperforming well in a number of studies, this commutativity is not desirable for a fine grained understanding of the semantics of natural language. When performing composition with APTs, we adopt the method introduced by Weir et al. (2016) which views distributional composition as a process of contextualisation. For composing the adjective white with the noun clothes via the dependency relation amod we need to consider how the adjective interacts with the noun in the vector space. The distributional features of white describe things that are white via their first order relations such as amod, and things that can be done to white things, such as bought via amod:dobj in the example above. Table 1 shows a number of features extracted from the aligned dependency trees in Figure 1 and highlights that adjectives and nouns do not share many features if only first order dependencies would be considered. However through the inclusion of inverse and higher order dependency paths we can observe that the second order features of the adjective align with the first order features of the noun. For composition, the adjective white needs to be offset by its inverse relation to clothes1 making it distributionally similar to a noun that has been modified by white. Offsetting can be seen as shifting the current viewpoint in the APT data structure and is necessary for aligning the feature spaces for composition (Weir et al., 2016). We are then in a position to compose the offset representation of white with the vector for clothes by the union or the intersection of their features. Table 2 shows the resulting feature spaces of the composed vectors. It is worth noting that any arithmetic operation can be used to combine the counts of the aligned features, however for this paper we use pointwise addition for both composition functions. One of the advantages of this approach to composition is that the inherent interpretability of count-based models naturally expands beyond the word level, allowing us to study the distributional semantics of phrases in the same space as words. Due to offsetting one of the constituents, the composition operation is not commutative and hence avoids identical representations for house boat and boat house. However, the typed nature of our vector space re1The inverse of amod is just amod.\nsults in extreme sparsity, for example while the untyped VSM has 130k dimensions, our APT model can have more than 3m dimensions. We therefore need to enrich the elementary vector representations with the distributional information of their nearest neighbours to ease the sparsity effect and infer missing information. Due to the syntactic nature of our composition operation it is not straightforward to apply common dimensionality reduction techniques such as SVD, as the type information needs to be preserved."}, {"heading": "4 Distributional Inference", "text": "Following Dagan et al. (1994) and Dagan et al. (1997), we propose a simple unsupervised algorithm for enriching sparse vector representations with their nearest neighbours. We show that our distributional inference algorithm improves performance for untyped and typed models on several word similarity benchmarks, as well as being competitive with the state-of-the-art on semantic composition. As shown in algorithm 1 below, we iterate over all word vectors w in a given distributional model M , and add the vector representations of the nearest neighbours n, determined by cosine similarity, to the representation of the enriched word vector w\u2032. The parameter \u03b1 in line 4 scales the contribution of the original word vector to the resulting enriched representation. In this work we always chose \u03b1 to be identical to the number of neighbours used\nfor distributional inference. For example, if we used 10 neighbours for DI, we would set \u03b1 = 10, which we found sufficient to prevent the neighbours from dominating the vector representation. In our experiments we kept the input distributional model fixed, however it is equally possible to update the given model in an online fashion, adding some amount of stochasticity to the enriched word vector representations. There is a number of possibilities for the neighbour retrieval function neighbours() and we explore several options in this paper. The algorithm furthermore is agnostic to the input distributional model, for example it is possible to use completely different vector space models for querying neighbours and enrichment.\nAlgorithm 1 Distributional Inference 1: procedure DIST INFERENCE(M ) 2: init M \u2032 3: for all w in M do 4: w\u2032 \u2190 w \u00d7 \u03b1 5: for all n in neighbours(M,w) do 6: w\u2032 \u2190 w\u2032 + n 7: add w\u2032 to M \u2032 8: end for 9: end for 10: return M \u2032 11: end procedure"}, {"heading": "Static Top n Neighbour Retrieval", "text": "The perhaps simplest way is to choose the top n most similar neighbours for each word in the vector space and enrich the respective vector representations with them."}, {"heading": "Density based Neighbour Retrieval", "text": "This approach has its roots in kernel density estimation (Parzen, 1962), however instead of defining a static global parzen window, we set the window size for every word individually, depending on the distance to its nearest neighbour, plus a threshold. For example if the cosine distance between the target vector and its top neighbour is 0.5, we use a window size of 0.5 + for that word. In our experiments we typically define to be proportional to the distance of the nearest neighbour (e.g. = 0.5\u00d7 0.1)."}, {"heading": "WordNet based Neighbour Retrieval", "text": "Instead of leveraging the intrinsic structure of our distributional vector space, we retrieve neighbours by querying WordNet (Fellbaum, 1998), and treat synsets with agreeing PoS tags as the nearest neighbours of any target vector. This restricts the retrieved neighbours to synonyms only."}, {"heading": "5 Experiments", "text": "Our model is based on a cleaned October 2013 Wikipedia dump, which excludes all pages with fewer than 20 page views, resulting in a corpus of approximately 0.6 billion tokens (Wilson, 2015). The corpus is lowercased, tokenised, lemmatised, PoS tagged and dependency parsed with the Stanford NLP tools, using universal dependencies (Manning et al., 2014; de Marneffe et al., 2014). We then build our APT model with first, second and third order relations. We remove distributional features with a count of less than 10, and vectors containing fewer than 50 non-zero entries. The raw counts are subsequently transformed to PPMI weights. The untyped vector space model is built from the same lowercased, tokenised and lemmatised Wikipedia corpus. We discard terms with a frequency of less than 50 and apply PPMI to the raw co-occurrence counts."}, {"heading": "Shifted PPMI", "text": "We explore a range of different values for shifting the PPMI scores as these have a significant impact\non the performance of the APT model. The effect of shifting PPMI scores for untyped vector space models has already been explored in Levy and Goldberg (2014), and Levy et al. (2015), thus we only present results for the APT model. As shown in equation 1, PMI is defined as the log of the ratio of the joint probability of observing a word w and a context c together, and the product of the respective marginals of observing them separately. In our APT model, a context c is defined as a dependency relation together with a word.\nPMI(w, c) = log P (w, c)\nP (w)P (c) SPPMI(w, c) = max(PMI(w, c)\u2212 log k, 0) (1)\nAs PMI is negatively unbounded, PPMI is used to ensure that all values are greater than or equal to 0. Shifted PPMI (SPPMI) subtracts a constant from any PMI score before applying the PPMI threshold. We experiment with values of 1, 5, 10, 40 and 100 for the shift parameter k."}, {"heading": "5.1 Word Similarity Experiments", "text": "We first evaluate our models on 3 word similarity benchmarks, MEN (Bruni et al., 2014), which is testing for relatedness (e.g. meronymy or holonymy) between terms, SimLex-999 (Hill et al., 2015), which is testing for substitutability (e.g. synonymy, antonymy, hyponymy and hypernymy), and WordSim-353 (Finkelstein et al., 2001), where we use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset. Baroni and Lenci (2011) have shown that untyped models are typically better at capturing relatedness, whereas typed models are better at encoding substitutability. Performance is measured by computing Spearman\u2019s \u03c1 between the cosine similarities of the vector representations and the corresponding aggregated human similarity judgements. For these experiments we keep the number of neighbours that a word vector can consume fixed at 30. This value is based on preliminary experiments on WordSim-353 (see Figure 2) using the static top n neighbour retrieval function and a PPMI shift of k = 40. Figure 2 shows that distributional inference improves performance for any number of neighbours over a model without DI (marked as horizontal dashed lines for each WordSim-353 subset) and peaks at a value of\n30. Performance slightly degrades with more neighbours. For the untyped VSM we use a symmetric window of 5 on either side of the target word.\nTable 3 highlights the effect of the SPPMI shift parameter k, while keeping the number of neighbours fixed at 30 and using the static top n neighbour retrieval function. For the APT model, a value of k = 40 performs best (except for SimLex-999, where smaller shifts give better results), with a performance drop-off for larger shifts. In our experiments we find that a shift of k = 1 results in top performance for the untyped vector space model. It appears that shifting the PPMI scores in the APT model has the effect of cleaning the vectors from noisy PPMI artefacts, which reinforces the predominant sense, while other senses get suppressed. Subsequently, this results in a cleaner neighbourhood around the word vector, dominated by a single sense. This explains why distributional inference slightly degrades performance for smaller values of k.\nTable 4 shows that distributional inference successfully infers missing information for both model types, resulting in improved performance over models without the use of DI on all datasets. The improvements are typically larger for the APT model,\nsuggesting that it is missing more distributional knowledge in its elementary representations than untyped models. The density window and static top n neighbour retrieval functions perform very similar, however the static approach is more consistent and never underperforms the baseline for either model type on any dataset. The WordNet based neighbour retrieval function performs particularly well on SimLex-999. This can be explained by the fact that antonyms, which frequently happen to be among the nearest neighbours in distributional vector spaces, are regarded as dissimilar in SimLex-999, whereas the WordNet neighbour retrieval function only returns synonyms. The results furthermore confirm the effect that untyped models perform better on datasets modelling relatedness, whereas typed models work better for substitutability tasks (Baroni and Lenci, 2011)."}, {"heading": "5.2 Composition Experiments", "text": "Our approach to semantic composition as described in section 3 requires the dimensions of our vector space models to be meaningful and interpretable. However, the problem of missing information is amplified in compositional settings as many compatible dimensions between words are not observed in the source corpus. It is therefore crucial that distributional inference is able to inject some of the missing information in order to improve the composition process. For the experiments involving semantic composition, we enrich the elementary representations of the phrase constituents before composition. We first conduct a qualitative analysis for our APT model and observe the effect of distributional inference on the nearest neighbours of composed adjective-noun, noun-noun and verb-object compounds. In these experiments, we show how dis-\ntributional inference changes the neighbourhood in which composed phrases are embedded, and highlight the difference between composition by union and composition by intersection. For this experiment we use the static top n neighbour retrieval function with 30 neighbours and k = 40. Table 5 shows a small number of example phrases together with their top 3 nearest neighbours, computed from the union of all words in the Wikipedia corpus and all phrase pairs in the Mitchell and Lapata (2010) dataset. As can be seen, nearest neighbours of phrases can be either single words or other composed phrases. Words or phrases marked with \u201c*\u201d in Table 5 mean that DI introduced, or failed to downrank, a spurious neighbour, while boldface means that performing distributional inference resulted in a neighbourhood more coherent with the query phrase than without DI. Table 5 shows that composition by union is unable to downrank unrelated neighbours introduced by distributional inference. For example large quantity is incorrectly introduced as a top ranked neighbour for the phrase small house, due to the proximity of small and large in the vector space. The phrases market leader and television programme are two examples of incoherent neighbours, which the composition function was unable to downrank and where DI could not improve the neighbourhood. Composition by intersection on the other hand vastly benefits from distributional inference. Due to the increased sparsity induced by the composition process, a neighbourhood without DI produces numer-\nous spurious neighbours as in the case of the verb have as a neighbour for win battle. Distributional inference introduces qualitatively better neighbours for almost all phrases. For example, government leader and opposition member are introduced as top ranked neighbours for the phrase party leader, and stress importance and underline are introduced as new top neighbours for the phrase emphasise need. These results show that composition by union does not have the ability to disambiguate the meaning of a word in a given phrasal context, whereas composition by intersection has that ability but requires distributional inference to unleash its full potential. For a quantitative analysis of distributional inference for semantic composition, we evaluate our model on the composition dataset of Mitchell and Lapata (2010), consisting of 108 adjective-noun, 108 noun-noun, and 108 verb-object pairs. The task is to compare the model\u2019s similarity estimates with the human judgements by computing Spearman\u2019s \u03c1. For comparing the performance of the different neighbour retrieval functions, we choose the same parameter settings as in the word similarity experiments (k = 40 and using 30 neighbours for DI).\nTable 6 shows that the static top n and density window neighbour retrieval functions perform very similar again. The density window retrieval function outperforms static top n for composition by intersection and vice versa for composition by union. The WordNet approach is competitive for composition by union, but underperfoms the other approaches for composition by intersection significantly. For further experiments we use the static top n approach as it is computationally cheap and easy to interpret due to the fixed number of neighbours. Table 6 also shows that while composition by intersection is significantly improved by distributional inference, composition by union does not appear to benefit from it.\nComposition by Union or Intersection Both model types in this study support composition by union as well as composition by intersection. In untyped models, composition by union and composition by intersection can be achieved by pointwise addition and pointwise multiplication respectively. The major difference between composition in the APT model and the untyped model is that in the former, composition is not commutative due to offsetting the modifier in a dependency relation (see section 3). Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al. (2016), as well as with the recently published state-of-the-art methods by Hashimoto et al. (2014), and Wieting et al. (2015), who are using neural network based approaches. For our models, we use the static top n approach as neighbour retrieval function and tune the remaining parameters, the SPPMI shift k (1, 5, 10, 40, 100) and the number of neighbours (10, 30, 50, 100, 500, 1000, 5000), for both model types, and the sliding window size for the untyped VSM (1, 2, 5), on the development portion of the Mitchell and Lapata (2010) dataset. We keep the vector configuration (k and window size) fixed for all phrase types and only tune the number of neighbours used for DI individually. The best vector configuration for the APT model is achieved with k = 10 and for the untyped VSM with k = 1. For composition by intersection best performance on the dev set was achieved with 1000 neighbours for ANs, 10 for NNs and 50 for VOs with DI. For composition by union, top performance was obtained with 100 neighbours for ANs, 30 neighbours for NNs and 50 for VOs. The best results for the untyped model on the dev set are achieved with a symmetric window size of 1 and using 5000 neighbours for ANs, 10 for NNs and 1000 for VOs with composition by pointwise multiplication, and 30 neighbours for ANs, 5000 for NNs and 5000 for VOs for composition by pointwise addition. The validated numbers of neighbours on the development set show that the problem of missing information appears to be more severe for semantic composition than for word similarity tasks. Even though a neighbour at rank 1000 or lower does not appear to have a close relationship to the target word,\nit still can contribute useful co-occurrence information not observed in the original vector. Table 7 shows that composition by intersection with distributional inference considerably improves upon the best results for APT models without distributional inference and for untyped count-based models, and is competitive with the state-of-the-art neural network based models of Hashimoto et al. (2014) and Wieting et al. (2015). Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012). Table 7 furthermore shows that DI has a smaller effect on the APT model based on composition by union and the untyped model based on composition by pointwise addition. The reason, as pointed out in the discussion for Table 5, is that the composition function has no disambiguating effect and thus cannot eliminate unrelated neighbours introduced by distributional inference. An intersective composition function on the other hand is able to perform the disambiguation locally in any given phrasal context. This furthermore suggests that for the APT model it is not necessary to explicitly model different word senses in separate vectors, as composition by intersection is able to disambiguate any word in context individually. Unlike the models of Hashimoto et al. (2014) and Wieting et al. (2015), the elementary word representations, as well as the representations for composed phrases and the composition process in our models are fully interpretable2.\n2We release the APT vectors and our code at https:// github.com/tttthomasssss/apt-toolkit."}, {"heading": "6 Conclusion and Future Work", "text": "One of the major challenges in count-based models is dealing with extreme sparsity and missing information. This paper contributes a number of findings relating to this challenge, in particular a simple unsupervised algorithm for enriching sparse word representations by leveraging its distributional neighbourhood. We have demonstrated its benefit to typed and untyped vector space models on a range of word similarity datasets. We have shown that distributional inference improves the performance of typed and untyped VSMs for semantic composition and that our APT model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable. With our method, we are able to bridge the gap in performance between lowdimensional non-interpretable and high-dimensional interpretable representations. Lastly, we have investigated the different behaviour of composition by union and composition by intersection and have shown that an intersective composition function, together with distributional inference, has the ability to locally disambiguate the meaning of a phrase.\nIn future work we aim to scale our approach to semantic composition with distributional inference to longer phrases and full sentences. We furthermore plan to investigate whether the number of neighbours required for improving elementary vector representations remains as high for other compositional tasks and longer phrases as in this study."}, {"heading": "Acknowledgments", "text": "This work was funded by UK EPSRC project EP/IO37458/1 \u201cA Unified Model of Compositional and Distributional Compositional Semantics: Theory and Applications\u201d. We would like to thank Miroslav Batchkarov for valuable discussions on earlier drafts of this paper and our anonymous reviewers for their helpful comments."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa."], "venue": "Proceedings of NAACL-HLT, pages 19\u201327, Boulder, Colorado, June.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics, 36(4):673\u2013 721, December.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "How we blessed distributional semantic evaluation", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Proceedings of GEMS Workshop, GEMS \u201911, pages 1\u201310, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Baroni and Lenci.,? 2011", "shortCiteRegEx": "Baroni and Lenci.", "year": 2011}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of EMNLP, pages 1183\u20131193, Cambridge, MA, October. Association for Computational", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of EMNLP, pages 546\u2013 556, Jeju Island, Korea, July. Association for Computational Linguistics.", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "J. Artif. Int. Res., 49(1):1\u201347, January.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior Research Methods, pages 510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior Research Methods, 44(3):890\u2013907.", "citeRegEx": "Bullinaria and Levy.,? 2012", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks."], "venue": "Computational Linguistics, 16(1):22\u201329, March.", "citeRegEx": "Church and Hanks.,? 1990", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "CoRR, abs/1003.4394.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "From Distributional to Semantic Similarity", "author": ["James Curran."], "venue": "Ph.D. thesis, University of Edinburgh.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Ido Dagan", "Fernando Pereira", "Lillian Lee."], "venue": "Proceedings of ACL, pages 272\u2013278, Las Cruces, New Mexico, USA, June. Association for Computational Linguistics.", "citeRegEx": "Dagan et al\\.,? 1994", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "Similarity-based methods for word sense disambiguation", "author": ["Ido Dagan", "Lillian Lee", "Fernando Pereira."], "venue": "Proceedings of ACL, pages 56\u201363, Madrid, Spain, July. Association for Computational Linguistics.", "citeRegEx": "Dagan et al\\.,? 1997", "shortCiteRegEx": "Dagan et al\\.", "year": 1997}, {"title": "Universal stanford dependencies: A cross-linguistic typology", "author": ["Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning."], "venue": "Proceedings of LREC, pages 4585\u20134592, Reykjavik,", "citeRegEx": "Marneffe et al\\.,? 2014", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."], "venue": "J. Amer. Soc. Inf. Sci., 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "A structured vector space model for word meaning in context", "author": ["Katrin Erk", "Sebastian Pad\u00f3."], "venue": "Proceedings of EMNLP, pages 897\u2013906, Honolulu, Hawaii, October. Association for Computational Linguistics.", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "WordNet: an electronic lexical database", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of WWW, WWW \u201901, pages 406\u2013414, New York, NY, USA. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Die Grundlagen der Arithmetik: Eine logisch mathematische Untersuchung \u00fcber den Begriff der Zahl", "author": ["Gottlob Frege."], "venue": "W. Koebner.", "citeRegEx": "Frege.,? 1884", "shortCiteRegEx": "Frege.", "year": 1884}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni."], "venue": "Proceedings of IWCS.", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["Gregory Grefenstette."], "venue": "Kluwer Academic Publishers, Norwell, MA, USA.", "citeRegEx": "Grefenstette.,? 1994", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Proceedings of", "citeRegEx": "Hashimoto et al\\.,? 2014", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695, December.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "A large scale evaluation of distributional semantic models: Parameters, interactions and model selection", "author": ["Gabriella Lapesa", "Stefan Evert."], "venue": "TACL, 2:531\u2013 545.", "citeRegEx": "Lapesa and Evert.,? 2014", "shortCiteRegEx": "Lapesa and Evert.", "year": 2014}, {"title": "The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of EMNLP, pages 1155\u20131164, Lisbon, Portugal, September. Association for Computational", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of NIPS, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "TACL, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin."], "venue": "Proceedings of ACL, pages 768\u2013 774, Montreal, Quebec, Canada, August. Association for Computational Linguistics.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Producing high-dimensional semantic spaces from lexical cooccurrence", "author": ["Kevin Lund", "Curt Burgess."], "venue": "Behavior Research Methods, Instruments, & Computers, 28(2):203\u2013208.", "citeRegEx": "Lund and Burgess.,? 1996", "shortCiteRegEx": "Lund and Burgess.", "year": 1996}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of ACL - System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "In Proceedings of ACL-08: HLT, pages 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "Proceedings of EMNLP, pages 2315\u20132325, Lisbon, Portugal, September. Association for Computational Linguistics.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Co-occurrence vectors from corpora vs", "author": ["Yoshiki Niwa", "Yoshihiko Nitta."], "venue": "distance vectors from dictionaries. In Proceedings of Coling, COLING \u201994,", "citeRegEx": "Niwa and Nitta.,? 1994", "shortCiteRegEx": "Niwa and Nitta.", "year": 1994}, {"title": "Dependencybased construction of semantic space models", "author": ["Sebastian Pad\u00f3", "Mirella Lapata."], "venue": "Computational Linguistics, 33(2):161\u2013199.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2007", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2007}, {"title": "Derivational smoothing for syntactic distributional semantics", "author": ["Sebastian Pad\u00f3", "Jan \u0160najder", "Britta Zeller."], "venue": "Proceedings of ACL, pages 731\u2013735, Sofia, Bulgaria, August. Association for Computational Linguistics.", "citeRegEx": "Pad\u00f3 et al\\.,? 2013", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2013}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni."], "venue": "Proceedings of ACL, pages 90\u201399, Baltimore, Maryland, June. Association for Computational Linguistics.", "citeRegEx": "Paperno et al\\.,? 2014", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "On estimation of a probability density function and mode", "author": ["Emanuel Parzen."], "venue": "Ann. Math. Statist., 33(3):1065\u20131076, 09.", "citeRegEx": "Parzen.,? 1962", "shortCiteRegEx": "Parzen.", "year": 1962}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP, pages 1532\u2013 1543, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The Word-space model", "author": ["Magnus Sahlgren."], "venue": "Ph.D. thesis, University of Stockholm (Sweden).", "citeRegEx": "Sahlgren.,? 2006", "shortCiteRegEx": "Sahlgren.", "year": 2006}, {"title": "Uncovering distributional differences between synonyms and antonyms in a word space model", "author": ["Silke Scheible", "Sabine Schulte im Walde", "Sylvia Springorum."], "venue": "Proceedings of IJCNLP, pages 489\u2013 497, Nagoya, Japan, October. Asian Federation of Nat-", "citeRegEx": "Scheible et al\\.,? 2013", "shortCiteRegEx": "Scheible et al\\.", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of EMNLP, pages 1201\u20131211, Jeju Island, Korea, July. Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of ACL, pages 1556\u20131566, Beijing, China, July. Association for Computational", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Contextualizing semantic representations using syntactically enriched vector models", "author": ["Stefan Thater", "Hagen F\u00fcrstenau", "Manfred Pinkal."], "venue": "Proceedings of ACL, pages 948\u2013957, Uppsala, Sweden, July. Association for Computational Linguistics.", "citeRegEx": "Thater et al\\.,? 2010", "shortCiteRegEx": "Thater et al\\.", "year": 2010}, {"title": "Word meaning in context: A simple and effective vector model", "author": ["Stefan Thater", "Hagen F\u00fcrstenau", "Manfred Pinkal."], "venue": "Proceedings of IJCNLP, pages 1134\u20131143, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.", "citeRegEx": "Thater et al\\.,? 2011", "shortCiteRegEx": "Thater et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Int. Res., 37(1):141\u2013188, January.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Distributional composition using higher-order dependency vectors", "author": ["Julie Weeds", "David Weir", "Jeremy Reffin."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality, pages 11\u201320, Gothenburg, Sweden, April.", "citeRegEx": "Weeds et al\\.,? 2014", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Aligning packed dependency trees: a theory of composition for distributional semantics", "author": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober."], "venue": "Computational Linguistics, in press.", "citeRegEx": "Weir et al\\.,? 2016", "shortCiteRegEx": "Weir et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "TACL, 3:345\u2013358.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "The unknown perils of mining wikipedia", "author": ["Benjamin Wilson."], "venue": "https://blog.lateral.io/2015/06/theunknown-perils-of-mining-wikipedia/, June.", "citeRegEx": "Wilson.,? 2015", "shortCiteRegEx": "Wilson.", "year": 2015}, {"title": "Squibs: When the whole is not greater than the combination of its parts: A \u201ddecompositional\u201d look at compositional distributional semantics", "author": ["Fabio Massimo Zanzotto", "Lorenzo Ferrone", "Marco Baroni."], "venue": "Computational Linguistics, 41(1):165\u2013173.", "citeRegEx": "Zanzotto et al\\.,? 2015", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al.", "startOffset": 66, "endOffset": 117}, {"referenceID": 23, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al.", "startOffset": 66, "endOffset": 117}, {"referenceID": 30, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 181, "endOffset": 228}, {"referenceID": 39, "context": "volved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 181, "endOffset": 228}, {"referenceID": 48, "context": "Our contributions are as follows: we show that typed and untyped sparse word representations, enriched by distributional inference, lead to performance improvements on several word similarity benchmarks, and that a higher-order dependency-typed vector space model, based on \u201cAnchored Packed Dependency Trees (APTs)\u201d (Weir et al., 2016), is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions.", "startOffset": 316, "endOffset": 335}, {"referenceID": 11, "context": "Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 11, "context": "Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In these works the authors are concerned with smoothing the probability estimate for", "startOffset": 60, "endOffset": 104}, {"referenceID": 11, "context": "guage modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997).", "startOffset": 109, "endOffset": 149}, {"referenceID": 12, "context": "guage modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997).", "startOffset": 109, "endOffset": 149}, {"referenceID": 8, "context": "guage modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997). More recently Pad\u00f3 et al. (2013) used a distributional approach for smoothing derivationally related words, such as oldish \u2013 old, as a back-off strategy in case of data sparsity.", "startOffset": 110, "endOffset": 184}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al.", "startOffset": 132, "endOffset": 157}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al.", "startOffset": 190, "endOffset": 219}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al.", "startOffset": 190, "endOffset": 241}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al.", "startOffset": 190, "endOffset": 269}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al.", "startOffset": 190, "endOffset": 295}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al.", "startOffset": 190, "endOffset": 362}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al.", "startOffset": 190, "endOffset": 385}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al.", "startOffset": 190, "endOffset": 404}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al.", "startOffset": 190, "endOffset": 426}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels.", "startOffset": 190, "endOffset": 450}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al.", "startOffset": 190, "endOffset": 690}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al.", "startOffset": 190, "endOffset": 712}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space.", "startOffset": 190, "endOffset": 736}, {"referenceID": 3, "context": "Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose APT framework is based on a higher-order dependency-typed", "startOffset": 190, "endOffset": 933}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010).", "startOffset": 68, "endOffset": 92}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 150}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 175}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 190}, {"referenceID": 1, "context": "be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and", "startOffset": 69, "endOffset": 207}, {"referenceID": 46, "context": "Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text.", "startOffset": 16, "endOffset": 41}, {"referenceID": 8, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 34, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 41, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 25, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014).", "startOffset": 131, "endOffset": 225}, {"referenceID": 18, "context": "Early proponents of that approach are Grefenstette (1994) and Lin (1998).", "startOffset": 38, "endOffset": 58}, {"referenceID": 18, "context": "Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al.", "startOffset": 38, "endOffset": 73}, {"referenceID": 18, "context": "Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al.", "startOffset": 38, "endOffset": 117}, {"referenceID": 14, "context": "More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 14, "context": "More recent work by Pad\u00f3 and Lapata (2007), Erk and Pad\u00f3 (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model.", "startOffset": 44, "endOffset": 87}, {"referenceID": 8, "context": "In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with APTs as introduced by Weir et al. (2016), which we adopt in this paper.", "startOffset": 132, "endOffset": 347}, {"referenceID": 48, "context": "following Weir et al. (2016), align and aggregate the resulting parse trees according to their dependency type as shown in Figure 1.", "startOffset": 10, "endOffset": 29}, {"referenceID": 18, "context": "Composition is linguistically motivated by the principle of compositionality, which states that the meaning of a complex expression is fully determined by its structure and the meanings of its constituents (Frege, 1884).", "startOffset": 206, "endOffset": 219}, {"referenceID": 18, "context": "Composition is linguistically motivated by the principle of compositionality, which states that the meaning of a complex expression is fully determined by its structure and the meanings of its constituents (Frege, 1884). Many simple approaches to semantic composition neglect the structure and lose information in the composition process. For example, the phrases house boat and boat house have the exact same representation when composition is done via a pointwise arithmetic operation. Despite performing well in a number of studies, this commutativity is not desirable for a fine grained understanding of the semantics of natural language. When performing composition with APTs, we adopt the method introduced by Weir et al. (2016) which views distributional composition as a process of contextualisation.", "startOffset": 207, "endOffset": 735}, {"referenceID": 48, "context": "Offsetting can be seen as shifting the current viewpoint in the APT data structure and is necessary for aligning the feature spaces for composition (Weir et al., 2016).", "startOffset": 148, "endOffset": 167}, {"referenceID": 11, "context": "Following Dagan et al. (1994) and Dagan et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "Following Dagan et al. (1994) and Dagan et al. (1997), we propose a simple unsupervised algorithm for enriching sparse vector representations with their nearest neighbours.", "startOffset": 10, "endOffset": 54}, {"referenceID": 38, "context": "This approach has its roots in kernel density estimation (Parzen, 1962), however instead of defining a static global parzen window, we set the window size for every word individually, depending on the distance to its nearest neighbour, plus a threshold.", "startOffset": 57, "endOffset": 71}, {"referenceID": 50, "context": "6 billion tokens (Wilson, 2015).", "startOffset": 17, "endOffset": 31}, {"referenceID": 29, "context": "The corpus is lowercased, tokenised, lemmatised, PoS tagged and dependency parsed with the Stanford NLP tools, using universal dependencies (Manning et al., 2014; de Marneffe et al., 2014).", "startOffset": 140, "endOffset": 188}, {"referenceID": 25, "context": "The effect of shifting PPMI scores for untyped vector space models has already been explored in Levy and Goldberg (2014), and Levy et al.", "startOffset": 96, "endOffset": 121}, {"referenceID": 25, "context": "The effect of shifting PPMI scores for untyped vector space models has already been explored in Levy and Goldberg (2014), and Levy et al. (2015), thus we only present results for the APT model.", "startOffset": 96, "endOffset": 145}, {"referenceID": 5, "context": "We first evaluate our models on 3 word similarity benchmarks, MEN (Bruni et al., 2014), which", "startOffset": 66, "endOffset": 86}, {"referenceID": 22, "context": "meronymy or holonymy) between terms, SimLex-999 (Hill et al., 2015), which is testing for substitutability (e.", "startOffset": 48, "endOffset": 67}, {"referenceID": 17, "context": "synonymy, antonymy, hyponymy and hypernymy), and WordSim-353 (Finkelstein et al., 2001), where we", "startOffset": 61, "endOffset": 87}, {"referenceID": 0, "context": "use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset. Baroni and Lenci (2011) have shown that untyped models are typically better at capturing relatedness, whereas typed models are better at encoding substitutability.", "startOffset": 19, "endOffset": 137}, {"referenceID": 31, "context": "Table 5: Nearest neighbours AN, NN and VO pairs in the Mitchell and Lapata (2010) dataset, with and without distributional", "startOffset": 55, "endOffset": 82}, {"referenceID": 31, "context": "Table 5 shows a small number of example phrases together with their top 3 nearest neighbours, computed from the union of all words in the Wikipedia corpus and all phrase pairs in the Mitchell and Lapata (2010) dataset.", "startOffset": 183, "endOffset": 210}, {"referenceID": 31, "context": "for semantic composition, we evaluate our model on the composition dataset of Mitchell and Lapata (2010), consisting of 108 adjective-noun, 108 noun-noun, and 108 verb-object pairs.", "startOffset": 78, "endOffset": 105}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference.", "startOffset": 0, "endOffset": 276}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al.", "startOffset": 0, "endOffset": 534}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al.", "startOffset": 0, "endOffset": 564}, {"referenceID": 4, "context": "Blacoe and Lapata (2012) showed that an intersective composition function such as pointwise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the APT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing APT model of Weir et al. (2016), as", "startOffset": 0, "endOffset": 617}, {"referenceID": 21, "context": "well as with the recently published state-of-the-art methods by Hashimoto et al. (2014), and Wieting et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 21, "context": "well as with the recently published state-of-the-art methods by Hashimoto et al. (2014), and Wieting et al. (2015), who are using neural network based approaches.", "startOffset": 64, "endOffset": 115}, {"referenceID": 31, "context": "Table 7: Results for the Mitchell and Lapata (2010) dataset.", "startOffset": 25, "endOffset": 52}, {"referenceID": 21, "context": "distributional inference considerably improves upon the best results for APT models without distributional inference and for untyped count-based models, and is competitive with the state-of-the-art neural network based models of Hashimoto et al. (2014)", "startOffset": 229, "endOffset": 253}, {"referenceID": 46, "context": "and Wieting et al. (2015). Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012).", "startOffset": 4, "endOffset": 26}, {"referenceID": 30, "context": "Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012).", "startOffset": 155, "endOffset": 182}, {"referenceID": 4, "context": "Distributional inference also improves upon the performance of an untyped VSM where composition by pointwise multiplication is outperforming the models of Mitchell and Lapata (2010), and Blacoe and Lapata (2012). Table 7", "startOffset": 187, "endOffset": 212}, {"referenceID": 21, "context": "Unlike the models of Hashimoto et al. (2014) and Wieting et al.", "startOffset": 21, "endOffset": 45}, {"referenceID": 21, "context": "Unlike the models of Hashimoto et al. (2014) and Wieting et al. (2015), the elementary word representations, as well as the representations for composed phrases and the composition process in our models are fully interpretable2.", "startOffset": 21, "endOffset": 71}], "year": 2016, "abstractText": "Distributional models are derived from cooccurrences in a corpus, where only a small proportion of all possible plausible cooccurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjectivenoun, noun-noun and verb-object compositions while being fully interpretable.", "creator": "LaTeX with hyperref package"}}}