{"id": "1608.06845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Effect of Incomplete Meta-dataset on Average Ranking Method", "abstract": "one of the simplest metalearning methods is the average ranking method. this method uses metadata in numerical form discrete test files of a given set of algorithms on given specified n datasets and calculates an average rank for each algorithm. the ranks are used to construct the average ranking. we investigate particular problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results. this issue is relevant, because if we could show that incomplete code does not affect the final results much, we obviously explore it in future design. we could simply conduct fewer tests together save thus computation time. in this paper will describe an upgraded average ranking method that is capable of dealing with incomplete metadata. our results show that the proposed method is relatively small to perform in interpreting results in current meta query.", "histories": [["v1", "Wed, 24 Aug 2016 14:44:33 GMT  (33kb)", "http://arxiv.org/abs/1608.06845v1", "8 pages, two figures and 6 tables"]], "COMMENTS": "8 pages, two figures and 6 tables", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salisu mamman abdulrahman", "pavel brazdil"], "accepted": false, "id": "1608.06845"}, "pdf": {"name": "1608.06845.pdf", "metadata": {"source": "CRF", "title": "Effect of Incomplete Meta-dataset on Average Ranking Method", "authors": ["Salisu Mamman Abdulrahman", "Abdulrahman Brazdil"], "emails": ["salisu.abdul@gmail.com", "pbrazdil@inescporto.pt"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n06 84\n5v 1\n[ cs\n.A I]\n2 4\nKeywords: Average Ranking, Aggregation of Rankings, Incomplete Metadata"}, {"heading": "1. Introduction", "text": "A large number of data mining algorithms exist, rooted in the fields of machine learning, statistics, pattern recognition, artificial intelligence. The task to recommend the most suitable algorithms has thus become rather challenging. The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al. (2008); Smith-Miles (2008)). We address the problem of robustness of one particular version of an average ranking method that uses incomplete rankings as input. These arise if we have incomplete test results in the meta-dataset. We have investigated how much the performance degrades under such circumstances.\nThe remainder of this paper is organized as follows. In the next section we present an overview of existing work in related areas. Section 3 provide details about the proposed aggregation method for incomplete meta dataset, the experimental results and future work.\n1. This paper is a slightly updated version of the paper presented at AutoML workshop at ICML 2016,\nNew York\nc\u00a9 2016 S.M. Abdulrahman & P. Brazdil."}, {"heading": "2. Related Work", "text": "In this paper we are addressing a particular case of the algorithm selection problem, oriented towards the selection of classification algorithms. Various researchers addressed this problem in the course of the last 25 years. One approach to algorithm selection/recommendation relies on metalearning. The simplest method uses just performance results on different datasets in the form of rankings. The rankings are then aggregated to obtain a single aggregated ranking. So the aggregated ranking can be used as a simple model that can be followed by the user to test the top candidates to identify the algorithm to be used. This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)).\nA more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here.\nAggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method.\nSome of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.g. k + 1) to all missing elements. We have therefore developed a simple heuristic method based on Borda\u2019s method described in Lin (2010). In our view it serves well our purpose. As Lin (2010) pointed out simple methods often compete quite well with other more complex approaches."}, {"heading": "3. Effect of Incomplete Meta-dataset on Average Ranking Method", "text": "Our aim is to investigate the issue of how the generation of the average ranking is affected by incomplete test results in the meta-dataset available. Here we focus on rankings obtained on the basis of accuracies. We wish to see how robust the method is to omissions in the meta-dataset. This issue is relevant because the meta-dataset that has been gathered by researchers is very often incomplete. Here we consider two different ways in which the meta-dataset can be incomplete: First, the test results on some datasets may be completely missing. Second, there may be certain proportion of omissions in the test results of some algorithms on each dataset.\nThe expectation is that the performance of the average ranking method would degrade when less information is available. However, an interesting question is how grave the degradation is. The answer to this issue is not straightforward, as it depends greatly on how diverse the datasets are and how this affects the rankings of algorithms. If the rankings are\nvery similar, then we expect that the omissions would not make much difference. So the issue of the effects of omissions needs to be relativized. To do this we will investigate the following issues:\n\u2022 Effects of missing test results on X% of datasets (alternative MTD);\n\u2022 Effects of missing X% of test results of algorithms on each dataset (alternative MTA).\nIf the performance drop of alternative MTA were not too different from the drop of alternative MTD, than we could conclude that X% of omissions is not unduly degrading the performance and hence the method of average ranking is relatively robust. Each of these alternatives is discussed in more detail below.\nMissing all test results on some datasets (alternative MTD): This strategy involves randomly omitting all test results on a given proportion of datasets from our meta-dataset. An example of this scenario is depicted in Table 1. In this example the test results on datasets D2 and D5 are completely missing. The aim is to show how much the average ranking degrades due these missing results.\nMissing some algorithm test results on each dataset (alternative MTA): Here the aim is to drop a certain proportion of test results on each dataset. The omissions are simply distributed uniformly across all datasets. That is, the probability that the test result of algorithm ai is missing is the same irrespective of which algorithm is chosen. An example of this scenario is depicted in Table 2. The proportion of datasets is a parameter of the\nmethod. Here we use the values shown in Table 3. The meta-dataset used in this study is\ndescribed further on in Section 3.2. This dataset was used to obtain a new one in which some datasets would be chosen at random and all test results obliterated. The resulting dataset was used to construct the average ranking. Each ranking was then used to construct a loss-time curve described further on (in Section 3.2). The whole process was repeated 10 times. This way we would obtain 10 loss-time curves, which would be aggregated into a single loss-time curve. Our aim is to upgrade the average ranking method to be able to deal with incomplete rankings. The enhanced method is described in the next section."}, {"heading": "3.1. Aggregation Method for Incomplete Rankings (AR-MTA)", "text": "Before describing the method for the calculation of the average ranking that can process incomplete rankings, let us consider a motivating example (see Table 4), illustrating why we cannot simply use the usual average ranking method (Lin (2010)), often used in comparative studies in machine learning literature. Let us compare the rankings R1 (Table 4a) and R2\n(Table 4b). We note that algorithm a2 is ranked 4 in ranking R1, but has rank 1 in ranking R2. If we used the usual method, the final ranking of a2 would be the mean of the two ranks, i.e. (4+1)/2=2.5. This seems intuitively not right, as the information in ranking R2 is incomplete. If we carry out just one test and obtain ranking R2 as a result, this information is obviously inferior to having conducted more tests leading to ranking R1. So these observations suggest that the number of tests should be taken into account to set a weight to characterize the individual elements of the ranking.\nIn our method the weight is calculated using the expression (N \u22121)/(Nmax\u22121), where N represents the number of filled-in elements in the ranking and Nmax the maximum number of elements that could be filled-in. The number N \u2212 1 represents the number of non-transitive relations between any element in the ranking to any other element in the ranking. So, for instance, in ranking R1 N = 6 and Nmax = 6. Therefore, the weight of\neach element in the ranking is 5/5 = 1. We note that N \u2212 1 (i.e. 5), represents the number of non-transitive relations in the ranking, namely a1 > a3, a3 > a4, .. , a6 > a5. Here ai > aj is used to indicate that ai is preferred to aj .\nLet us consider the incomplete ranking R2. Suppose we know a priori that the ranking could include 6 elements and so Nmax = 6, as in the previous case. Then the weight of each element will be (N \u2212 1)/(Nmax \u2212 1) = 1/5 = 0.2. The notion of weight captures the fact that ranking R2 provides less information than ranking R1. We need this concept in the process of calculating the average ranking.\nOur upgraded version of the aggregation method for incomplete rankings involves the initialization step, which consists of reading-in the first ranking and initializing the average ranking RA. Then in each subsequent step a new ranking is read-in and aggregated with the average ranking, producing a new average ranking. The aggregation is done by going through all elements in the ranking, one by one. If the element appears in both the aggregated ranking and the read-in ranking, its rank is recalculated as a weighted average of the two ranks:\nrAi := r A i \u2217 w A i /(w A i + w j i ) + r j i \u2217 w j i /(w A i + w j i ) (1)\nwhere rAi represents the rank of element i in the aggregated ranking and r j i the rank of the element in the ranking j that is being processed and wAi and w j i represent the corresponding weights. The weight is updated as follows: wAi := (w A i + w j i ). If the element appears in the aggregated ranking, but not in the new read-in ranking, the new aggregated ranking is made equal to the previous one. The weight of the element is also kept unchanged.\nSuppose the aim is to aggregate ranking R1 and R2 shown before. The new rank of a2 will be rA2 = 4 * 1/1.2 + 1*0.2/1.2 = 3.5. The weight will be w A 2 = 1 + 0.2 = 1.2. The final aggregated ranking of rankings R1 and R2 is shown in Table 4c."}, {"heading": "3.2. Results on the Effects of Omissions in the Meta-Dataset", "text": "The data used in the experiments involves the meta-dataset constructed from evaluation results retrieved from OpenML (Vanschoren et al. (2014)), a collaborative science platform for machine learning. This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets2. In the leave-oneout mode, 38 datasets are used to generate the model (e.g. average ranking), while the dataset left out is used for evaluation.\nCharacterization of our Meta-Dataset: We are interested to analyze rankings of classification algorithms on different datasets and in particular how these differ for pairs of datasets, using Spearman correlation coefficient. Fig.1 shows a histogram characterizing the meta-dataset used. The histogram is accompanied by expected value, standard deviation and coefficient of variation calculated as the ratio of standard deviation to the expected value (mean) (Witten and Frank (2005)). These measures are shown in Table 5.\nResults: The aim is to investigate how certain omissions in the meta-datasets affect the performance. The results are presented in the form of loss-time curve (van Rijn et al. (2015)) which show how performance loss depends on time. The loss is calculated as the difference between the performance of the best algorithm identified so far when following\n2. Full details: http://www.openml.org/project/tag/ActiveTestingSamples/u/1\nHistogram of correlation\nthe ranking to the ideal choice. Each loss-time curve can be characterized by a number representing the mean loss in a given interval. We want this mean interval loss (MIL) to be as low as possible. This characteristic is similar to AUC, but there is an important difference. When talking about AUCs, the x-axis values spans between 0 and 1. Our loss-time curves span between some Tmin and Tmax and both values depend on the user. Typically the user searching for a suitable algorithm would not worry about quite short times. In the experiments here we have set Tmin to 10 seconds. The value of Tmax was set to 104 seconds, i.e. about 2.78 hours.\nTable 6 presents the results for both alternative AR-MTD and AR-MTA in terms of mean interval loss (MIL). The values for ordinary average ranking method, AR, are also shown, as this method serves as a baseline. Fig.2 shows the loss-time curves for the alternatives AR-MTD and AR-MTA when the number of omissions is 90%. The values of AR relative to 90% omissions are also shown for comparison. Not all loss-time curves are shown, as the figure would be rather cluttered. Our results show that the alternative AR-MTA achieves far better results than the baseline method AR. We note also that although the proposed method AR-MTA is worse than the AR-MTD counterpart, the difference is negligible for many different values of omissions in the range from 5% till 50%. Only when we get to rather extreme values such as 90%, the difference is noticeable. But even in this case the curve of AR-MTA follows the curve of\nAR-MTD much more closely than AR. These results indicate that the proposed average ranking method is relatively robust to omissions.\nFuture work: As the incomplete meta-dataset does not affect much the final ranking and the corresponding loss, this could be explored in future design of experiments, when gathering the test results. We could investigate approaches that permit to consider also the costs (time) of off-line tests. Their cost (time) could be set to some fraction of the cost of on-line test (i.e. tests on a new dataset), but not really ignored altogether."}, {"heading": "Acknowledgements", "text": "This work is supported by TETFund 2012 Intervention for Kano University of Science and Technology, Wudil, Kano State, Nigeria for PhD Overseas Training from the Federal Government of Nigeria.\nThe authors also acknowledge the support of project NanoSTIMA: Macro-to-Nano Human Sensing: Towards Integrated Multimodal Health Monitoring and Analytics/NORTE01-0145-FEDER-000016, which is financed by the North Portugal Regional Operational Programme (NORTE 2020), under the PORTUGAL 2020 Partnership Agreement, and through the European Regional Development Fund (ERDF)."}], "references": [{"title": "Metalearning: Applications to data mining", "author": ["Pavel Brazdil", "Christophe Giraud-Carrier", "Carlos Soares", "Ricardo Vilalta"], "venue": "Springer Science & Business Media,", "citeRegEx": "Brazdil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brazdil et al\\.", "year": 2008}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Rank aggregation methods", "author": ["S. Lin"], "venue": "WIREs Computational Statistics,", "citeRegEx": "Lin.,? \\Q2010\\E", "shortCiteRegEx": "Lin.", "year": 2010}, {"title": "Tell me who can learn you and I can tell you who you are: Landmarking various learning algorithms", "author": ["B. Pfahringer", "H. Bensusan", "Ch. Giraud-Carrier"], "venue": "In Proc. of the 17th Int. Conf. on Machine Learning,", "citeRegEx": "Pfahringer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pfahringer et al\\.", "year": 2000}, {"title": "RankAggreg, an R package for weighted rank aggregation", "author": ["V. Pihur", "S. Datta", "S. Susmita Datta"], "venue": "Department of Bioinformatics and Biostatistics,", "citeRegEx": "Pihur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pihur et al\\.", "year": 2014}, {"title": "The Algorithm Selection Problem", "author": ["J.R. Rice"], "venue": "Advances in Computers,", "citeRegEx": "Rice.,? \\Q1976\\E", "shortCiteRegEx": "Rice.", "year": 1976}, {"title": "Cross-disciplinary Perspectives on Meta-Learning for Algorithm Selection", "author": ["K.A. Smith-Miles"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Smith.Miles.,? \\Q2008\\E", "shortCiteRegEx": "Smith.Miles.", "year": 2008}, {"title": "Fast Algorithm Selection using Learning Curves", "author": ["J.N. van Rijn", "S.M. Abdulrahman", "P. Brazdil", "J. Vanschoren"], "venue": "In Advances in Intelligent Data Analysis XIV. Springer,", "citeRegEx": "Rijn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rijn et al\\.", "year": 2015}, {"title": "OpenML: networked science in machine learning", "author": ["J. Vanschoren", "J.N. van Rijn", "B. Bischl", "L. Torgo"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Vanschoren et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vanschoren et al\\.", "year": 2014}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["Ian H. Witten", "Eibe Frank"], "venue": null, "citeRegEx": "Witten and Frank.,? \\Q2005\\E", "shortCiteRegEx": "Witten and Frank.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al.", "startOffset": 57, "endOffset": 69}, {"referenceID": 0, "context": "The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 201, "endOffset": 223}, {"referenceID": 0, "context": "The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 201, "endOffset": 243}, {"referenceID": 2, "context": "Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010).", "startOffset": 84, "endOffset": 95}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)).", "startOffset": 62, "endOffset": 84}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al.", "startOffset": 62, "endOffset": 285}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 62, "endOffset": 308}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 62, "endOffset": 328}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods.", "startOffset": 62, "endOffset": 854}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method. Some of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.", "startOffset": 62, "endOffset": 1419}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method. Some of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.g. k + 1) to all missing elements. We have therefore developed a simple heuristic method based on Borda\u2019s method described in Lin (2010). In our view it serves well our purpose.", "startOffset": 62, "endOffset": 1622}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method. Some of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.g. k + 1) to all missing elements. We have therefore developed a simple heuristic method based on Borda\u2019s method described in Lin (2010). In our view it serves well our purpose. As Lin (2010) pointed out simple methods often compete quite well with other more complex approaches.", "startOffset": 62, "endOffset": 1677}, {"referenceID": 2, "context": "Aggregation Method for Incomplete Rankings (AR-MTA) Before describing the method for the calculation of the average ranking that can process incomplete rankings, let us consider a motivating example (see Table 4), illustrating why we cannot simply use the usual average ranking method (Lin (2010)), often used in comparative studies in machine learning literature.", "startOffset": 286, "endOffset": 297}, {"referenceID": 6, "context": "Results on the Effects of Omissions in the Meta-Dataset The data used in the experiments involves the meta-dataset constructed from evaluation results retrieved from OpenML (Vanschoren et al. (2014)), a collaborative science platform for machine learning.", "startOffset": 174, "endOffset": 199}, {"referenceID": 1, "context": "This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets.", "startOffset": 105, "endOffset": 124}, {"referenceID": 1, "context": "This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets. In the leave-oneout mode, 38 datasets are used to generate the model (e.g. average ranking), while the dataset left out is used for evaluation. Characterization of our Meta-Dataset: We are interested to analyze rankings of classification algorithms on different datasets and in particular how these differ for pairs of datasets, using Spearman correlation coefficient. Fig.1 shows a histogram characterizing the meta-dataset used. The histogram is accompanied by expected value, standard deviation and coefficient of variation calculated as the ratio of standard deviation to the expected value (mean) (Witten and Frank (2005)).", "startOffset": 105, "endOffset": 768}, {"referenceID": 1, "context": "This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets. In the leave-oneout mode, 38 datasets are used to generate the model (e.g. average ranking), while the dataset left out is used for evaluation. Characterization of our Meta-Dataset: We are interested to analyze rankings of classification algorithms on different datasets and in particular how these differ for pairs of datasets, using Spearman correlation coefficient. Fig.1 shows a histogram characterizing the meta-dataset used. The histogram is accompanied by expected value, standard deviation and coefficient of variation calculated as the ratio of standard deviation to the expected value (mean) (Witten and Frank (2005)). These measures are shown in Table 5. Results: The aim is to investigate how certain omissions in the meta-datasets affect the performance. The results are presented in the form of loss-time curve (van Rijn et al. (2015)) which show how performance loss depends on time.", "startOffset": 105, "endOffset": 990}], "year": 2016, "abstractText": "One of the simplest metalearning methods is the average ranking method. This method uses metadata in the form of test results of a given set of algorithms on given set of datasets and calculates an average rank for each algorithm. The ranks are used to construct the average ranking. We investigate the problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results. This issue is relevant, because if we could show that incomplete metadata does not affect the final results much, we could explore it in future design. We could simply conduct fewer tests and save thus computation time. In this paper we describe an upgraded average ranking method that is capable of dealing with incomplete metadata. Our results show that the proposed method is relatively robust to omission in test results in the meta datasets.", "creator": "gnuplot 5.0 patchlevel 1"}}}