{"id": "1608.07225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "On Simulated Annealing Dedicated to Maximin Latin Hypercube Designs", "abstract": "the goal of our research was to highlight local search heuristics used to construct latin linear designs. first, we introduce my \\ textit { 1d - id } perturbation to rapid angular space exploration performed by these algorithms. second, we propose a new reference function $ \\ psi _ { p, \\ sigma } $ specifically targeting the maximin criterion.", "histories": [["v1", "Tue, 23 Aug 2016 14:55:43 GMT  (263kb,D)", "http://arxiv.org/abs/1608.07225v1", "extended version of ACM GECCO 2016 paper entitled \"Search Space Exploration and an Optimization Criterion for Hard Design Problems\""]], "COMMENTS": "extended version of ACM GECCO 2016 paper entitled \"Search Space Exploration and an Optimization Criterion for Hard Design Problems\"", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pierre berg\\'e", "kaourintin le guiban", "arpad rimmel", "joanna tomasik"], "accepted": false, "id": "1608.07225"}, "pdf": {"name": "1608.07225.pdf", "metadata": {"source": "CRF", "title": "On Simulated Annealing Dedicated to Maximin Latin Hypercube Designs", "authors": ["Pierre Berg\u00e9", "Kaourintin Le Guiban", "Arpad Rimmel", "Joanna Tomasik"], "emails": [], "sections": [{"heading": null, "text": "to construct Latin Hypercube Designs. First, we introduce the 1D-move perturbation to improve the space exploration performed by these algorithms. Second, we propose a new evaluation function \u03c8p,\u03c3 specifically targeting the Maximin criterion.\nExhaustive series of experiments with Simulated Annealing, which we used as a typically well-behaving local search heuristics, confirm that our goal was reached as the result we obtained surpasses the best scores reported in the literature. Furthermore, the \u03c8p,\u03c3 function seems very promising for a wide spectrum of optimization problems through the Maximin criterion."}, {"heading": "1 Introduction", "text": "The study of complex systems usually requires a considerable computation time. To speed up computations, the system may be replaced by a faster approximating model. To create this model, a set of outcomes for different parameter values is needed. The set of parameter values has an impact on the accuracy of the approximating model. Different sampling methods for this set of parameters has been proposed in [4]. If we note k the number of inputs of the system and n the number of possible values taken by an input variable xk, the choice of n sample vectors can be represented by points on a hypercube of size n and dimension k. Among the designs proposed, we focus on the Maximin Latin Hypercube Design (LHD).\nThe LHD implements the Latin constraint: each coordinate in [|1;n|] must appear only once in every dimension. In other words, the coordinates of any pairs of nodes differ in all dimensions. Moreover, the Maximin constraint means that we search the configuration with the maximal dmin, where dmin is the minimal distance between two points of the design. An instance is defined by\nar X\niv :1\n60 8.\n07 22\n5v 1\n[ cs\n.A I]\nthe values of the dimension k and the size n. Consequently, an instance will be noted k/n, for example 10/50.\nThere are exactly ( n 2 ) = n(n\u22121)2 distances between points. They may be ordered d1 \u2264 d2 \u2264 ... \u2264 di \u2264 di+1... \u2264 d(n2), by definition, dmin = d1. In the remainder, we often refer to square values: Di = d2i .\nAs the Maximin LHD problem is believed to be NP-hard, heuristics are widely used to solve it. The use of deterministic methods are, for the moment, rather limited: branch-and-bound was only used with k \u2264 3 by [8, 9] (a highscore is a minimal maximal distance between any pair of design points for a given instance) . The survey [6] of metaheuristics sums up their performance on Maximin LHD and reports that SA not only outperformed other evolutionary algorithms but also improved many of the previous highscores for k \u2264 10 and n \u2264 25. For this reason we choose to work on the improvement of SA applied to Maximin LHDs.\nAfter introducing usual methods used to solve Maximin LHD with SA, we propose a new mutation and a new evaluation function to improve on the best current scores. Eventually, we show how it permits to exceed a great part of the scores presented by the literature. These results should not exclusively be considered in the particular SA context as they may offer the opportunity to boost the performance of local search algorithms for different designs with the Maximin constraint.\nN.B. This reports completes our paper [2] providing the precisions and details which had to be omitted in its camera-ready version due to the page number limit. It should thus be cited together with [2]."}, {"heading": "2 Typical approach to solve Maximin LHD with Simulated Annealing", "text": "SA is a metaheuristic most commonly used for discrete search spaces inspired by a metallurgical process. It consists in visiting the search space with a perturbation on the configurations and deciding whether the mutated configuration should be selected. That is why it alternates phases of heating and slow cooling to influence this choice: as in physics, by controlling the cooling process, we give an opportunity to the configuration to find the lowest energy.\nIt is proven that SA converges to the global minimum with the Metropolis probability [1]. Several ingredients are compulsory for SA [3]: a perturbation (a mutation), an evaluation function Hpot (also called potential energy) and a temperature decrease T (k), where k is the iteration number. The acceptance probability depends on T (k) and the gap of potential energy between the new and the old configuration. When the current configuration at the iteration number k is \u03c9, a configuration \u03c9\u2032 will be accepted with the Metropolis probability:\npk = min\n( 1, e Hpot(\u03c9 \u2032)\u2212Hpot(\u03c9) KT (k) )\nConstant K is typically fixed to 1. A configuration \u03c9 is defined by n points with k coordinates respecting the Latin constraints.\nSurvey [6] examined several perturbations among which m2 was the most efficient. It deals with a pair of points: a randomly chosen one and a critical one. A critical point is a point involved in dmin. Mutation m2 transposes the coordinates of these points in one dimension. The authors of [6] proposed mutation m3 which is a variant of m2 as the transposition takes place in the dimension which ensures a better dmin for a subsequent configuration \u03c9\u2032. Mutation m3 outperformed m2 for 9/10, 4/25 and 8/20.\nArticle [6] compared two evaluation functions: \u2212dmin and \u03c6p introduced in [5]:\n\u03c6p = ( n 2)\u2211 i=1 d\u2212pi  1 p . (1)\nFunction \u03c6p is more efficient than \u2212dmin certainly because it takes into account changes on every distance whereas the function \u2212dmin only considers the shortest distance of the configuration. As the paper [6] obtained the best score of the literature, we used the same values to set the values of parameter p."}, {"heading": "3 New perturbation targeting LHD", "text": ""}, {"heading": "3.1 Principle of the perturbation", "text": "We use m2 as a basis to construct a better performing perturbation. To clarify its principle, we define the notion of the neighborhood:\nDefinition 1 (Neighbor of a point). For a given instance k/n, a point p1 of a configuration \u03c9 is a neighbor of the point p2 if and only if there is a dimension j for which coordinates of these two points are the closest possible. In other terms, \u2203j \u2208 [|1; k|] such that |p1(j)\u2212 p2(j)| = 1.\nThe new mutation 1D\u2013move consists in taking a critical point as before and taking one of its neighbor. Then we exchange the coordinates in one of the dimensions concerned by the neighborhood.\nWe choose a 3/5 instance to illustrate 1D\u2013move. Table 1 gives the coordinates of the points of a configuration \u03c9. First, we choose a critical point: dmin is determined by points p1 and p2, so we take p1. Points p2 (on axis x, y and z), p3 (on axis y) and p4 (on axis z) are neighbors of p1. For the sake of the example, we shall choose p4 as a neighbor. Then, we exchange the coordinates of p1 and p4 on axis z because p1 and p4 are neighbors through dimension z. The new configuration is also given in Table 1."}, {"heading": "3.2 Performance Evaluation", "text": "1D\u2013move outperforms not only m2 but m3 as well. We reproduced the experiments made in [6] keeping the same value of parameter p (p = 10) for 4/25, 9/10 and 8/20 to show its performance (Table 2). SA performs a linear thermal descent until temperature T = 0 is reached. The initial temperature is set thanks to a series of preliminary runs. We computed 100 effective runs and we present here the average within the 95% confidence interval.\nTo explain the efficiency of 1D\u2013move, we can refer to SA on the Traveling Salesman Problem. Article [7] shows that the perturbations which move the smallest number of edges are the best. 1D-move modifies the same number of points as m2 and m3. Consequently, 2(n\u22122) distances are modified by all these mutations. However, the changes on distances are smaller with 1D\u2013move given that modifications on coordinates are \u00b11 thanks to the neighborhood property. We prove it with Eq. (2). Our hypothesis is that this specific property explains why 1D\u2013move is more efficient.\nLet us consider a n/k configuration \u03c9 and the two mutations m2 and 1D\u2013 move. We want to prove that the changes due to these two mutations are not necessarily represented by the same order of magnitude. Let us note:\nm2 : \u03c9 \u2212\u2192 \u03c9\u2032 and 1D\u2013move : \u03c9 \u2212\u2192 \u03c9\u2032\u2032.\nWe assume the two mutations translate the point p2 on a given dimension j. We also take a point p1 of the configuration which remains invariant with these\nmutations (p\u20321 and p\u2032\u20321 equal to p1). The objective is to find a configuration for which \u2206d = \u2223\u2223dp\u20321,p\u20322 \u2212 dp1,p2\u2223\u2223 is equivalent to n in the m2 case. This difference is:\n\u2206d = \u2223\u2223\u2223\u2223\u2223\u2223 \u221a\u221a\u221a\u221a k\u2211\nl=1\n(p\u20322(l)\u2212 p1(l))2 \u2212 \u221a\u221a\u221a\u221a k\u2211 l=1 (p2(l)\u2212 p1(l))2 \u2223\u2223\u2223\u2223\u2223\u2223 . As most of the dimensions are not concerned by this move, we just note:\nk\u2211 l=1,l 6=j (p\u20322(l)\u2212 p1(l)) 2 = k\u2211 l=1,l 6=j (p2(l)\u2212 p1(l))2 = a2.\nVariable a depends on n and k. If p1 and p2 are neighbors regarding all dimensions (except j), a2 = k \u2212 1. We take a configuration for which a2 = k \u2212 1, p1(j) = 0, p2(j) = n \u2212 1 and p\u20322(j) = 1. This configuration is illustrated in Figure 1.\nBy separating j from other dimensions, we eventually find: \u2206d = \u2223\u2223\u2223\u221aa2 + (n\u2212 1)2 \u2212\u221aa2 + 1\u2223\u2223\u2223\n= \u2223\u2223\u2223\u221ak \u2212 1 + (n\u2212 1)2 \u2212\u221ak\u2223\u2223\u2223 . (2)\nSome configurations respect the property: k n2, for which \u2206d = O(n). This means that the difference between two distances may take values with the order of magnitude n. It is not possible with 1D\u2013move. Using the triangle inequality and the neighborhood property: \u2206d \u2264 1.\nWith m2 (or m3 which is more restrictive than m2), \u2206d sometimes reaches the order of magnitude n. We showed with \u2206d \u2264 1 that it was impossible with 1D\u2013move which allows the local search to be more regular."}, {"heading": "4 New evaluation function targeting Maximin", "text": ""}, {"heading": "4.1 Presentation of a Maximin effect: narrowing the distribution of distances", "text": "We study the properties of distances obtained with the evaluation function \u03c6p in SA solutions. We represent all the distances of a configuration in histograms and identify properties that will allow us to establish a better evaluation function below. From now on, we distinguish three cases relative to values taken by n and k. We note the mean of D for any configuration as D(k, n) = kn(n+1)6 as shown in [9].\nCase n \u2264 k\nIn this case (see Figure 2, 50/40), the distances of potential solutions are concentrated around the mean. It is highly probable that two points at random taken will be neighbors. This explains why a point is close to all others in SA solutions and we talk about unimodal distribution. In our example 50/40 in Figure 2, the statistical range of D relative to D, Dmax\u2212Dmin\nD = 34013667 = 2.5%,\nin fact, is narrow. The rationale for this behavior is that when the number of points is less than the number of dimensions, it happens, in absence of constraints, that all the points are equidistant. Since the Latin constraint has to be respected, the points cannot be exactly equidistant. The distances, however, do not differ significantly.\nCase k \u2264 n \u2264 2k\nIn this case (Figure 2, 30/50), distributions are concentrated around two peaks. The first peak is mainly around the average distance (actually, there is a little shift between the peak and the mean because both the peaks preserve D) and the second peak is located around the doubled average distance. Much more distances are concerned by the first peak.\nWe illustrate this phenomenon with the 30/50 instance in Figure 2. We can explain this by the fact that it is possible for this many points to be placed in an hyperoctahedron. In such a geometric object, each point is at the same distance from every other point but one, which is farther away. Thus, the distribution of distances shows two values, with the smaller being represented much more frequently.\nIn our example, D(30, 50) = 12500. Concerning the highest peak, the statistical range remains small compared with the mean: the ratio is 7.8%, larger than in the first case for the whole distribution. There are only seven distances located in the interval [13183; 24865]\nCase 2k \u2264 n\nIn this last case (Figure 2, 10/100), distances are distributed more uniformly. There is neither a dense peak nor a sparse interval. We observe a decrease of occurrences with an increase in the value of the distance.\nObservations and consequences\nFor the first case, the only peak is naturally thin thanks to SA and particularly \u03c6p action. There is a little point in trying to narrow it more. We note that for the two last cases (k \u2264 n), narrowing differences between distances lead to improve performance. We illustrate this on the 8/20 instance. We represent distance sets of several possible solutions and observe that the best solutions have the most narrowed distributions. We compare two solutions in Figure 3 with Dmin = 421 and Dmin = 446 which is the best solution found in [6]. Indeed, we note that Dmin = 446 has the most narrow peak. We formulate\nthe hypothesis that this property may be beneficial for SA performance. We introduce below a new evaluation function taking into account this aspect."}, {"heading": "4.2 Definition of evaluation function \u03c8", "text": "We propose an evaluation function \u03c8p,\u03c3 to replace the usual function \u03c6p:\n\u03c8p,\u03c3 = ( n 2)\u2211 i=1 wid \u2212p i  1 p , where wi = 1\u221a\u2211(n2)\nj=1 e \u2212| Dj\u2212Di|2 \u03c32\n. (3)\nThe idea is to add weights wi \u2265 1 for each distance term d\u2212pi . These weights determine if the distance is close to other ones. If a distance is far from the others, the weight will be high. Consequently, it forces the distances to be close to each other. A single drawback of \u03c8 is its complexity in O(n4). There are different ways to reduce this complexity. First, for instance, it is possible to consider only the differences which respect |Dj \u2212Di|2 \u2264 5\u03c32. In this way, we avoid the calculations of terms that may be considered as negligible (e\u22125 1). Instead of summing up ( n 2 ) distances, we can randomly choose O(n) distances Dj ."}, {"heading": "4.3 Tuning of parameter \u03c3 and its justification", "text": "Let us focus on the parameter \u03c3: given that we aim at furnishing a large number of scores, we need to tune it in a global way. It must depend directly on n\nand k, without preliminary experiments for each instance k/n. Looking at the definition of \u03c8p,\u03c3, this variable is introduced in order to regulate the order of magnitude of the exponential term. We see that \u03c3 should have approximately the same order of magnitude than the values taken by |Dj \u2212Di|2.\nThis is why we try to give the expression of a linear function of k and n which is similar to typical values |Dj \u2212Di|2. To establish it, we study the variance of a random variable: the tuning of \u03c3 is founded on Theorem 2.\nTheorem 2. Let D(k, n) be the random variable representing any square distance in any configuration of instance k/n. We have D (k, n) \u223c N\n( kn(n+1) 6 , g (n) )\nwith g (n) \u223c 7kn 4\n180 +O(n 3).\nProof. Thanks to [9], we know that E(D (k, n)) = kn(n+1)6 . We note (P1, P2) the random variable that gives any couple of points for n/k. The random variable D (k, n) is a function of (P1, P2). For any 1 \u2264 j \u2264 k, we note Y (j) = (P1(j)\u2212 P2(j))2 and get D (k, n) = \u2211k j=1 Y (j). As Y (i) and Y (j) are independent if i 6= j, we note Y (i) = Y to keep the notation simple. If k is high enough, we apply the Central Limit Theorem: D (k, n) \u223c N ( kn(n+1) 6 , kVar(Y ) ) . We focus first on E(Y 2) = E((P1(j)\u2212 P2(j))4):\nE(Y 2) = \u2211n x=1 \u2211 y 6=x(x\u2212 y)4\nn(n\u22121) 2\n= 2 ( n \u2211n\u22121 z=1 z 4 \u2212 \u2211n\u22121 z=1 z 5 )\nn(n\u2212 1) = n4 15 +O\n( n3 ) .\nWe thus deduce Var(Y ) = E(Y 2)\u2212 E(Y )2 = n 4 15 \u2212 n4 36 +O ( n3 ) \u223c 7n 4 180 .\nWe propose a global tuning of \u03c32 as a linear function of the variance of our configurations. As computing the variance of a configuration, at every iteration, would be expensive, we formulate the hypothesis that the variance of the square distances set of the SA solutions follows the function g (n) above. The idea of the tuning is to consider \u03c32 linearly dependent on the variance of the random variable D (k, n). In the weights wi, we compare the difference between the current distance and an extra one with \u03c3 by calculating |Dj\u2212Di| 2\n\u03c32 in order to identify which differences Dj \u2212Di have to be taken into account.\nIn the case n \u2265 2k, we assume \u03c32 = ckn4. According to several experiments series, we identify a good compromise with c = 1300 .\nIn the case n \u2264 k, leading to unimodal distributions, \u03c8 does not bring more interesting results than \u03c6. It is equivalent to assuming c to be very large (c\u2192\u221e).\nFinally, the case k \u2264 n \u2264 2k which is an intermediary of the two previous cases, can be tuned with \u03c3 = 2ckn4. This proposition does not obviously represent the best tuning for all possible instances but gives an efficient and simple solution for the tuning of \u03c3.\nIt is necessary to mention that the case k \u2264 n \u2264 2k is the case where tuning is essential: to be as efficient as possible, the value of \u03c3 has to be carefully\nselected. Table 3 shows the impact of \u03c8p,\u03c3 on SA performance with mutations m2 and 1D-move. We keep the same experimental setup as in Subsection 3.2: SA makes a thermal linear descent, the results presented come out from 100 runs and the average is within the 95% confidence interval.\nIn Table 4, we update scores for the same instances as in [6]. The results were produced with 107 iterations and p = 5. Our function \u03c8p,\u03c3 is used when k \u2264 n, \u03c6p elsewhere. We note in bold type improved results and in italics results worse than [6]. For 4 \u2264 k \u2264 8, the use of 1D\u2013move and \u03c8p,\u03c3 allows us to exceed a large number of scores but this improvement is less significant for other values. For k = 3, we suppose that the new tools are not able to outperform previous results because the results are already optimal or very good. For k = {9, 10}, a credible hypothesis is that the value of nk is so close to 1 that the effect of \u03c8p,\u03c3 is weak. Generally, results could be better with a specifically adapted tuning. Here, we established temperature, p and \u03c3 by making compromises between all the instances. However, in a real life case, by treating complex systems, we work on a defined instance with k and n fixed. In such circumstances, we naturally advice to customize the tuning of the different parameters by making preliminary experiments on this very instance. We expect that such an approach\nwould produce results outperforming those in Table 4."}, {"heading": "5 Conclusion", "text": "In this article, we introduce new techniques to treat the Maximin LHD construction. The first one is the 1D\u2013move mutation especially dedicated to the LHD structure. It is very efficient for a local search on LHDs because it makes it possible to follow a step-by-step path on the cost surface without jumping over possible minima. The second tool, the evaluation function \u03c8p,\u03c3 directly focuses upon Maximin optimization.\nAs numerous problems, among them Maximin Designs, involve this criterion, we emphasize that this function can be used for many other applications. In the Maximin LHD context, the function \u03c8p,\u03c3 tries to find solutions by narrowing a set of possible distances. SA, with 1D\u2013move and \u03c8p,\u03c3, gave results better than those considered to be \u201cthe best known\u201d for the majority of cases without any dedicated tuning."}], "references": [{"title": "Statistical cooling: A general approach to combinatorial optimization problems", "author": ["E. Aarts", "P. van Laarhoven"], "venue": "Philips Journal of Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Search Space Exploration and an Optimization Criterion for Hard Design Problems", "author": ["P. Berg\u00e9", "K. Le Guiban", "A. Rimmel", "J. Tomasik"], "venue": "In Proc. of ACM GECCO,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Optimization by simmulated annealing", "author": ["S. Kirkpatrick", "D. Gelatt Jr.", "M.P. Vecchi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1983}, {"title": "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer", "author": ["M. McKay", "R. Beckman", "W. Conover"], "venue": "Code. Technometrics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1979}, {"title": "Exploratory designs for computational experiments", "author": ["M.D. Morris", "T.J. Mitchell"], "venue": "Journal of statistical planning and inference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "A Survey of Meta-heuristics Used for Computing Maximin Latin Hypercube", "author": ["A. Rimmel", "F. Teytaud"], "venue": "In Proc. of EvoCOP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Application of the simulated annealing algorithm to the combinatorial optimisation problem with permutation property: An investigation of generation mechanism", "author": ["P. Tian", "J. Ma", "D. Zhang"], "venue": "European Journal of Operational Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Maximin latin hypercube designs in two dimensions", "author": ["E.R. van Dam", "B. Husslage", "D. den Hertog", "H. Melissen"], "venue": "Operations Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Bounds for maximin latin hypercube designs", "author": ["E.R. van Dam", "G. Rennen", "B. Husslage"], "venue": "Operations Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Different sampling methods for this set of parameters has been proposed in [4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "The use of deterministic methods are, for the moment, rather limited: branch-and-bound was only used with k \u2264 3 by [8, 9] (a highscore is a minimal maximal distance between any pair of design points for a given instance) .", "startOffset": 115, "endOffset": 121}, {"referenceID": 8, "context": "The use of deterministic methods are, for the moment, rather limited: branch-and-bound was only used with k \u2264 3 by [8, 9] (a highscore is a minimal maximal distance between any pair of design points for a given instance) .", "startOffset": 115, "endOffset": 121}, {"referenceID": 5, "context": "The survey [6] of metaheuristics sums up their performance on Maximin LHD and reports that SA not only outperformed other evolutionary algorithms but also improved many of the previous highscores for k \u2264 10 and n \u2264 25.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "This reports completes our paper [2] providing the precisions and details which had to be omitted in its camera-ready version due to the page number limit.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "It should thus be cited together with [2].", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "It is proven that SA converges to the global minimum with the Metropolis probability [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Several ingredients are compulsory for SA [3]: a perturbation (a mutation), an evaluation function Hpot (also called potential energy) and a temperature decrease T (k), where k is the iteration number.", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "Survey [6] examined several perturbations among which m2 was the most efficient.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "The authors of [6] proposed mutation m3 which is a variant of m2 as the transposition takes place in the dimension which ensures a better dmin for a subsequent configuration \u03c9\u2032.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Article [6] compared two evaluation functions: \u2212dmin and \u03c6p introduced in [5]:", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "Article [6] compared two evaluation functions: \u2212dmin and \u03c6p introduced in [5]:", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "As the paper [6] obtained the best score of the literature, we used the same values to set the values of parameter p.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "We reproduced the experiments made in [6] keeping the same value of parameter p (p = 10) for 4/25, 9/10 and 8/20 to show its performance (Table 2).", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Article [7] shows that the perturbations which move the smallest number of edges are the best.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "We note the mean of D for any configuration as D(k, n) = kn(n+1) 6 as shown in [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "We compare two solutions in Figure 3 with Dmin = 421 and Dmin = 446 which is the best solution found in [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Thanks to [9], we know that E(D (k, n)) = kn(n+1) 6 .", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "In Table 4, we update scores for the same instances as in [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "We note in bold type improved results and in italics results worse than [6].", "startOffset": 72, "endOffset": 75}], "year": 2016, "abstractText": "The goal of our research was to enhance local search heuristics used to construct Latin Hypercube Designs. First, we introduce the 1D-move perturbation to improve the space exploration performed by these algorithms. Second, we propose a new evaluation function \u03c8p,\u03c3 specifically targeting the Maximin criterion. Exhaustive series of experiments with Simulated Annealing, which we used as a typically well-behaving local search heuristics, confirm that our goal was reached as the result we obtained surpasses the best scores reported in the literature. Furthermore, the \u03c8p,\u03c3 function seems very promising for a wide spectrum of optimization problems through the Maximin criterion.", "creator": "LaTeX with hyperref package"}}}