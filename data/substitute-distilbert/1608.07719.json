{"id": "1608.07719", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "Temperature-Based Deep Boltzmann Machines", "abstract": "mutual learning techniques have been achieved in the last years, mainly due to performing outstanding results in a number of applications, that range from self recognition to face - based user identification. despite other techniques proposed for commercial purposes, deep boltzmann machines choose among the most used ones, which are composed of layers of restricted boltzmann machines ( rbms ) stacked on top among each other. in early work, we evaluate the concept of temperature in dbms, which play a key role in boltzmann - related distributions, after it has never been considered in this context up to date. therefore, the main contribution of stored information is to take into account this information and to evaluate its influence in dbms considering the task over binary image reconstruction. we expect this work can prompt future research considering the usage of entropy assumptions during learning in dbms.", "histories": [["v1", "Sat, 27 Aug 2016 15:31:21 GMT  (1169kb)", "http://arxiv.org/abs/1608.07719v1", "Submitted to Neural Processing Letters"], ["v2", "Sun, 4 Sep 2016 00:55:52 GMT  (1169kb)", "http://arxiv.org/abs/1608.07719v2", "Submitted to Neural Processing Letters"]], "COMMENTS": "Submitted to Neural Processing Letters", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["leandro aparecido passos junior", "joao paulo papa"], "accepted": false, "id": "1608.07719"}, "pdf": {"name": "1608.07719.pdf", "metadata": {"source": "CRF", "title": "Temperature-Based Deep Boltzmann Machines", "authors": ["Leandro Aparecido Passos J\u00fanior", "Jo\u00e3o Paulo Papa"], "emails": ["leandropassosjr@gmail.com", "papa@fc.unesp.br"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n07 71\n9v 1\n[ cs\n.L G\n] 2\n7 A\nug 2\nKeywords Deep Learning \u00b7 Deep Boltzmann Machines \u00b7 Machine learning"}, {"heading": "1 Introduction", "text": "Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12]. Restricted Boltzmann Machines (RBMs) [9], for instance, are among the most pursued techniques, even though they are not\nL. Passos Department of Computing, Federal University of Sa\u0303o Carlos Tel.: +55-16-3351-8232 / Fax: +55-16-3351-8233 E-mail: leandropassosjr@gmail.com\nJ. Papa Department of Computing, Sa\u0303o Paulo State University Tel./Fax: +55-14-3103-6079 E-mail: papa@fc.unesp.br\ndeep learning-oriented themselves, but by building blocks composed of stacked RBMs on top of each other one can obtain the so-called Deep Belief Networks (DBNs) [10] or the Deep Boltzmann Machines (DBMs) [21], which basically differ from each other by the way the inner layers interact among themselves.\nThe Restricted Boltzmann Machine is a probabilistic model that uses a layer of hidden units to model the distribution over a set of inputs, thus compounding a generative stochastic neural network [11,22]. RBMs were firstly idealized under the name of \u201cHarmonium\u201d by Smolensky in 1986 [25], and some years later renamed to RBM by Hinton et. al. [8]. Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].\nRoughly speaking, the key role in RBMs concerns their learning parameter step, which is usually carried out by sampling in Markov chains in order to approximate the gradient of the logarithm of the likelihood concerning the estimated data with respect to the input one. In this context, Li et. al. [13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann. In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy. Li et. al. [13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].\nHowever, as far we are concerned, the impact of different temperatures during the Markov sampling has never been considered in Deep Boltzmann Machines. Therefore, the main contributions of this work are two fold: (i) to foster the scientific literature regarding DBMs, and (ii) to evaluate the impact of temperature during DBM learning phase. Also, we considered Deep Belief Networks for comparison purposes concerning the task of binary image reconstruction over three public datasets. The remainder of this paper is organized as follows. Section 2 presents the theoretical background related to DBMs and the proposed temperature-based approach, and Section 3 describes the methodology adopted in this work. The experimental results are discussed in Section 4, and conclusions and future works are stated in Section 5."}, {"heading": "2 Deep Boltzmann Machines", "text": "In this section, we briefly explain the theoretical background related to RBMs and DBMs.\n2.1 Restricted Boltzmann Machines\nRestricted Boltzmann Machines are energy-based stochastic neural networks composed of two layers of neurons (visible and hidden), in which the learning phase is conducted by means of an unsupervised fashion. A na\u0308\u0131ve architecture of a Restricted Boltzmann Machine comprises a visible layer v with m units and a hidden layer h with n units. Additionally, a real-valued matrix Wm\u00d7n models the weights between the visible and hidden neurons, where wij stands for the weight between the visible unit vi and the hidden unit hj.\nLet us assume both v and h as being binary-valued units. In other words, v \u2208 {0, 1}m e h \u2208 {0, 1}n. The energy function of a Restricted Boltzmann Machine is given by:\nE(v,h) = \u2212 m\u2211\ni=1\naivi \u2212 n\u2211\nj=1\nbjhj \u2212 m\u2211\ni=1\nn\u2211\nj=1\nvihjwij , (1)\nwhere a e b stand for the biases of visible and hidden units, respectively. The probability of a joint configuration (v,h) is computed as follows:\nP (v,h) = 1\nZ e\u2212E(v,h), (2)\nwhere Z stands for the so-called partition function, which is basically a normalization factor computed over all possible configurations involving the visible and hidden units. Similarly, the marginal probability of a visible (input) vector is given by:\nP (v) = 1\nZ\n\u2211\nh\ne\u2212E(v,h). (3)\nSince the RBM is a bipartite graph, the activations of both visible and hidden units are mutually independent, thus leading to the following conditional probabilities:\nP (v|h) =\nm\u220f\ni=1\nP (vi|h), (4)\nand\nP (h|v) =\nn\u220f\nj=1\nP (hj |v), (5)\nwhere\nP (vi = 1|h) = \u03c6\n\n\nn\u2211\nj=1\nwijhj + ai\n\n , (6)\nand\nP (hj = 1|v) = \u03c6\n( m\u2211\ni=1\nwijvi + bj\n)\n. (7)\nNote that \u03c6(\u00b7) stands for the logistic-sigmoid function. Let \u03b8 = (W,a, b) be the set of parameters of an RBM, which can be learned through a training algorithm that aims at maximizing the product of probabilities given all the available training data V , as follows:\nargmax \u0398\n\u220f\nv\u2208V\nP (v). (8)\nOne can solve the aforementioned equation using the following derivatives over the matrix of weights W, and biases a and b at iteration t as follows:\nWt+1 = Wt + \u03b7(P (h|v)vT \u2212 P (h\u0303|v\u0303)v\u0303T ) + \u03a6 \ufe38 \ufe37\ufe37 \ufe38\n=\u2206Wt\n, (9)\nat+1 = at + \u03b7(v\u2212 v\u0303) + \u03b1\u2206at\u22121 \ufe38 \ufe37\ufe37 \ufe38\n=\u2206at\n(10)\nand\nbt+1 = bt + \u03b7(P (h|v)\u2212 P (h\u0303|v\u0303)) + \u03b1\u2206bt\u22121 \ufe38 \ufe37\ufe37 \ufe38\n=\u2206bt\n, (11)\nwhere \u03b7 stands for the learning rate, and \u03bb and \u03b1 denote the weight decay and the momentum, respectively. Notice the terms P (h\u0303|v\u0303) and v\u0303 can be obtained by means of the Contrastive Divergence [8] technique, which basically ends up performing Gibbs sampling using the training data as the visible units. Roughly speaking, Equations 9, 10 and 11 employ the well-known Gradient Descent as the optimization algorithm. The additional term \u03a6 in Equation 9 is used to control the values of matrix W during the convergence process, and it is formulated as follows:\n\u03a6 = \u2212\u03bbWt + \u03b1\u2206Wt\u22121. (12)\n2.2 Deep Boltzmann Machines\nLearning more complex and internal representations of the data can be accomplished by using stacked RBMs, such as DBNs and DBMs. In this paper, we are interested in the DBM formulation, which is slightly different from DBN one. Suppose we have a DBM with two layers, where h1 and h2 stand for the hidden units at the first and second layer, respectively.\nThe energy of a DBM can be computed as follows:\nE(v,h1,h2) = \u2212 m1\u2211\ni=1\nn1\u2211\nj=1\nvih 1 jw 1 ij \u2212\nm2\u2211\ni=1\nn2\u2211\nj=1\nh1i h 2 jw 2 ij , (13)\nwhere m1 and m2 stand for the number of visible units in the first and second layers, respectively, and n1 and n2 stand for the number of hidden units in the first and second layers, respectively. In addition, we have the weight matrices W1m1\u00d7n1 and W 2 m2\u00d7n2 , which encode the weights of the connections between vectors v and h1, and vectors h1 and h2, respectively. For the sake of simplification, we dropped the bias terms out.\nThe marginal probability the model assigns to a given input vector v is given by:\nP (v) = 1\nZ\n\u2211\nh1,h2\ne\u2212E(v,h 1,h2). (14)\nFinally, the conditional probabilities over the visible and the two hidden units are given as follows:\nP (vi = 1|h 1) = \u03c6\n\n\nn1\u2211\nj=1\nw1ijh 1 j\n\n , (15)\nP (h2z = 1|h 1) = \u03c6\n\n\nm2\u2211\ni=1\nw2izh 1 i\n\n , (16)\nand\nP (h1j = 1|v,h 2) = \u03c6\n\n\nm1\u2211\ni=1\nw1ijvi +\nn2\u2211\nz=1\nw2jzh 2 z\n\n . (17)\nAfter learning the first RBM using Contrastive Divergence, for instance, the generative model can be written as follows:\nP (v) = \u2211\nh1\nP (h1)P (v|h1), (18)\nwhere P (h1) = \u2211\nv P (h1,v). Further, we shall proceed with the learning pro-\ncess of the second RBM, which then replaces P (h1) by P (h1) = \u2211\nh2 P (h1,h2).\nRoughly speaking, using such procedure, the conditional probabilities given by Equations 15-17, and Contrastive Divergence, one can learn DBM parameters one layer at a time [21].\n2.3 Temperature-based Deep Boltzmann Machines\nLi et. al. [13] showed that a temperature parameter T controls the sharpness of the logistic-sigmoid function. In order to incorporate the temperature effect into the RBM context, they introduced this parameter to the joint distribution of the vectors v and h in Equation 2, which can be rewritten as follows:\nP (v,h, T ) = 1\nZ e\n\u2212E(v,h) T . (19)\nWhen T = 1, the aforementioned equation degenerates to Equation 2. In addition, Equation 7 can be rewritten in order to accommodate the temperature parameter as follows:\nP (hj = 1|v) = \u03c6\n(\u2211m i=1 wijvi\nT\n)\n. (20)\nNotice the temperature parameter does not affect the conditional probability of the input units (Equation 6).\nIn order to apply the very same idea to DBMs, the conditional probabilities over the two hidden layers given by Equations 16 and 17 can be derived and expressed using the following formulation, respectively:\nP (h2z = 1|h 1) = \u03c6\n(\u2211m2\ni=1 w 2 izh 1 i\nT\n)\n, (21)\nand\nP (h1j = 1|v,h 2) = \u03c6\n\n\n\u2211m1\ni=1 w 1 ijvi\nT +\nn2\u2211\nz=1\nw2jzh 2 z\n\n . (22)"}, {"heading": "3 Methodology", "text": "In this section, we present the methodology employed to evaluate the proposed approach, as well the datasets and the experimental setup.\n3.1 Datasets\nWe propose to evaluate the behaviour of DBMs under different temperatures in the context of binary image reconstruction using three public datasets, as described below:\n\u2013 MNIST dataset1: it is composed of images of handwritten digits. The original version contains a training set with 60, 000 images from digits \u20180\u2019-\u20189\u2019,\n1 http://yann.lecun.com/exdb/mnist/\nas well as a test set with 10, 000 images2. Due to the high computational burden for DBM model selection, we decided to employ the original test set together with a reduced version of the training set3. \u2013 CalTech 101 Silhouettes Data Set4: it is based on the former Caltech 101 dataset, and it comprises silhouettes of images from 101 classes with resolution of 28 \u00d7 28. We have used only the training and test sets, since our optimization model aims at minimizing the MSE error over the training set. \u2013 Semeion Handwritten Digit Data Set5: it is formed by 1, 593 images from handwritten digits \u20180\u2019 - \u20189\u2019 written in two ways: the first time in a normal way (accurately) and the second time in a fast way (no accuracy). In the end, they were stretched with resolution of 16 \u00d7 16 in a grayscale of 256 values and then each pixel was binarized.\n3.2 Experimental Setup\nWe employed a 3-layered architecture for all datasets as follows: i-500-5002000, where i stands for the number of pixels used as input for each dataset, i.e. 196 (14 \u00d7 14 images), 784 (28 \u00d7 28 images) and 256 (16 \u00d7 16 images) considering MNIST, Caltech 101 Silhouettes and Semeion Handwritten Digit datasets, respectively. Therefore, we have a first and a second hidden layers with 500 neurons each, followed by a third hidden layer with 2000 neurons. Since this architecture has been commonly employed in several works in the literature, we opted to employ it in our work either. The remaining parameters used during the learning steps were chosen empirically and fixed for each layer as follows: \u03b7 = 0.1 (learning rate), \u03bb = 0.1 (weight decay), \u03b1 = 0.00001 (penalty parameter). In addition, we compared DBMs against DBNs using the very same configuration, i.e. architecture and parameters.\nIn order to provide a statistical analysis by means of the Wilcoxon signedrank test with significance of 0.05 [30], we conducted a cross-validation procedure with 20 runnings. In regard to the temperature, we considered a set of values within the range T \u2208 {0.1, 0.2, 0.5, 0.8, 1.0, 1.2, 1.5, 2.0} for the sake of comparison purposes.\nFinally, we employed 30 epochs for DBM and DBN learning weights procedure with mini-batches of size 20. In order to provide a more precise experimental validation, we trained both DBMs and DBNs with two different algorithms6: Contrastive Divergence (CD) [8] and Persistent Contrastive Divergence (PCD) [28].\n2 The images are originally available in grayscale with resolution of 28\u00d728, but they were reduced to 14\u00d7 14 images.\n3 The original training set was reduced to 2% of its former size, which corresponds to 1, 200 images.\n4 https://people.cs.umass.edu/~marlin/data.shtml 5 https://archive.ics.uci.edu/ml/datasets/Semeion+Handwritten+Digit 6 One sampling iteration was used for all learning algorithms."}, {"heading": "4 Experimental Results", "text": "In this section, we present the experimental results concerning the proposed temperature-based Deep Boltzmann Machine over three public datasets aiming at the task of binary image reconstruction. Table I presents the results considering Semeion Handwritten Digit dataset, in which the values in bold stand for the most accurate ones by means of the Wilcoxon signed-rank test. Notice we considered the minimum squared error (MSE) over the test set as the measure for comparison purposes.\nClearly, one can not observe a statistical difference between DBM-CD and DBM-PCD when using T \u2208 {0.5, 0.8, 1.0}, except for DBM-PCD and T = 1. Also, our results confirm the ones obtained by Lin et al. [13], i.e. the lower the temperature the higher the entropy. In short, we can learn more information at low temperatures, thus obtaining better results (obviously, we are constrained to a minimum bound concerning the temperature). According to Ranzato et al. [19], sparsity in the neuron\u2019s activity favours the power of generalization of a network, which is somehow related to dropping neurons out in order to avoid overfitting [27]. We have observed the following statement: the lower the temperature, the higher the probability of turning \u201con\u201d hidden units (Equation 22), which forces DBM to push down the weights (W) looking at sparsity. When we push the weights down, we also decrease the probability of turning on the hidden units, i.e. we try to deactivate them, thus forcing the network to learn by other ways. We observed the process of pushing the weights down to be more \u201cradical\u201d at lower temperatures.\nFigure 1 displays the values of the connection weights between the input and the first hidden layer. Since we used an architecture with 500 hidden neurons in the first layer, we chose 225 neurons at random to display what sort of information they have learned. According to Table I, some of the better results were obtained using T = 0.5 (Figure 1b) and T = 1 (Figure 1c), which can be observed in the images either. Notice we can observe some digits at these images (e.g. highlighted regions in Figure 1b), while they are scarce in others. Additionally, DBNs seemed to benefit from lower temperatures, but their results were inferior to the ones obtained by DBMs.\nTable II displays the MSE results over MNIST dataset, where the best results were obtained with T = 0.1. Once again, the results confirmed the hypothesis that better results can be obtained at lower temperatures, probably due to the lower interaction between visible and hidden units, which may\nimply in a slower convergence, but avoiding local optima (learning in DBMs is essentially an optimization problem, where we aim at minimizing the energy of each training sample in order to increase its probability - Equations 13 and 14). Figure 2 displays the connection weights between the input and the first hidden layer concerning DBM-PCD, where the highlighted region depicts some important information learned from the hidden neurons. Notice the neurons do not seem to contribute a lot with respect to different information learned from each other at higher temperatures (Figure 2d), since most of them have similar information encoded.\nTable 2 Average MSE over the test set considering MNIST dataset.\n0.1 0.2 0.5 0.8 1.0 1.2 1.5 2.0 DBM-CD 0.08647 0.08768 0.08786 0.08881 0.09111 0.09222 0.09402 0.09629 DBM-PCD 0.08637 0.08759 0.08793 0.08870 0.09106 0.09231 0.09405 0.09618 DBN-CD 0.08993 0.09432 0.09259 0.09012 0.08933 0.08924 0.08966 0.09110 DBN-PCD 0.08784 0.08811 0.08919 0.08874 0.08833 0.08820 0.08838 0.08994\n(a) (b) (c) (d)\nFig. 2 Effect of different temperatures by means of DBM-PCD considering MNIST dataset with respect to the connection weights of the first hidden layer for: (a) T = 0.1, (b) T = 0.5, (c) T = 1.0, (d) T = 2.0.\nTable III presents the MSE results obtained over Caltech 101 Silhouettes dataset, where the lower temperatures obtained the best results. In this case,\nboth DBM and DBN obtained similar results. Since this dataset comprises a number of different objects and classes, it is more complicated to figure out some shape with respect to the neurons\u2019 activity in Figure 3. Curiously, the neurons\u2019 response at the lower temperatures (Figure 3a) led to a different behaviour that has been observed in the previous datasets, since the more \u201cactive\u201d neurons with respect to different information learned were the ones obtained with T = 2 at the training step. We believe such behaviour is due to the number of iterations for learning used in this paper, which might not be enough for convergence purposes at lower temperatures, since this dataset poses a greater challenge than the others (it has a great intra-class variablity).\nTable 3 Average MSE over the test set considering Caltech 101 Silhouettes dataset.\n0.1 0.2 0.5 0.8 1.0 1.2 1.5 2.0 DBM-CD 0.16072 0.16119 0.16304 0.16335 0.16335 0.16375 0.16410 0.16390 DBM-PCD 0.16068 0.16125 0.16295 0.16387 0.16359 0.16451 0.16368 0.16389 DBN-CD 0.16061 0.16107 0.16269 0.16301 0.16320 0.16310 0.16320 0.16282 DBN-PCD 0.16062 0.16085 0.16146 0.16158 0.16158 0.16190 0.16176 0.16267\n(a) (b) (c) (d)\nFig. 3 Effect of different temperatures by means of DBM-PCD considering Caltech 101 Silhouettes dataset with respect to the connection weights of the first hidden layer for: (a) T = 0.1, (b) T = 0.5, (c) T = 1.0, (d) T = 2.0.."}, {"heading": "5 Conclusions and Future Works", "text": "In this work, we dealt with the problem of different temperatures at the DBM learning step. Inspired by a very recent work that proposed the Temperaturebased Restricted Boltzmann Machines [13], we decided to evaluate the influence of the temperature when learning with Deep Boltzmann Machines aiming at the task of binary image reconstruction. Our results confirm the hypothesis raised by Li et al. [13], where the lower the temperature, the more generalized is the network. Thus, more accurate results can be obtained.\nWe observed the network pushes the weights down at lower temperatures in order to favour the sparsity, since the probability of tuning on hidden units\nis greater at lower temperatures. In regard to future works, we aim to propose an adaptive temperature, which can be linearly increased/decreased along the iterations in order to speed up the convergence process."}, {"heading": "Acknowledgments", "text": "The authors would like to thank FAPESP grant #2014/16250-9, Capes and CNPq grant #306166/2014-3."}], "references": [{"title": "Black holes and entropy", "author": ["J.D. Bekenstein"], "venue": "Physical Review D 7(8), 2333", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "2015 ieee conference on beyond principal components: Deep boltzmann machines for face modeling", "author": ["C.N. Duong", "K. Luu", "K.G. Quach", "T.D. Bui"], "venue": "Computer Vision and Pattern Recognition, CVPR \u201915, pp. 4786\u20134794", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Origin of generalized entropies and generalized statistical mechanics for superstatistical multifractal systems", "author": ["B. Gadjiev", "T. Progulova"], "venue": "International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, vol. 1641, pp. 595\u2013602", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Computer Vision \u2013 ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V, chap", "author": ["H. Goh", "N. Thome", "M. Cord", "J.H. Lim"], "venue": "Unsupervised and Supervised Visual Codes with Restricted Boltzmann Machines, pp. 298\u2013311. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Maxwell\u2013boltzmann statistics and the metaphysics of modality", "author": ["B.L. Gordon"], "venue": "Synthese 133(3), 393\u2013417", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science 313(5786), 504 \u2013 507", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Discovering binary codes for documents by learning deep generative models", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Topics in Cognitive Science 3(1), 74\u201391", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation 14(8), 1771\u20131800", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural networks: Tricks of the trade: Second edition", "author": ["G.E. Hinton"], "venue": "chap. A Practical Guide to Training Restricted Boltzmann Machines, pp. 599\u2013619. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation 18(7), 1527\u20131554", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "The Journal of Machine Learning Research 13(1), 643\u2013669", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G.E. Hinton"], "venue": "Nature 521, 436\u2013444", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Temperature based restricted boltzmann machines", "author": ["G. Li", "L. Deng", "Y. Xu", "C. Wen", "W. Wang", "J. Pei", "L. Shi"], "venue": "Scientific reports 6, 19,133", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Nonlinear kramers equation associated with nonextensive statistical mechanics", "author": ["G. Mendes", "M. Ribeiro", "R. Mendes", "E. Lenzi", "F. Nobre"], "venue": "Physical Review E 91(5), 052,106", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Exact maxwell\u2013boltzmann, bose\u2013einstein and fermi\u2013dirac statistics", "author": ["R.K. Niven"], "venue": "Physics Letters A 342(4), 286\u2013293", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "On the model selection of bernoulli restricted boltzmann machines through harmony search", "author": ["J.P. Papa", "G.H. Rosa", "K.A.P. Costa", "A.N. Marana", "W. Scheirer", "D.D. Cox"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201915, pp. 1449\u20131450. ACM, New York, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Model selection for discriminative restricted boltzmann machines through meta-heuristic techniques", "author": ["J.P. Papa", "G.H. Rosa", "A.N. Marana", "W. Scheirer", "D.D. Cox"], "venue": "Journal of Computational Science 9, 14\u201318", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Fine-tuning deep belief networks using harmony search", "author": ["J.P. Papa", "W. Scheirer", "D.D. Cox"], "venue": "Applied Soft Computing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y. Boureau", "Y. Cun"], "venue": "J. Platt, D. Koller, Y. Singer, S. Roweis (eds.) Advances in Neural Information Processing Systems 20, pp. 1185\u20131192. Curran Associates, Inc.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "On measures of entropy and information", "author": ["A. RRNYI"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1961}, {"title": "An efficient learning procedure for deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "Neural Computation 24(8), 1967\u20132006", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61, 85\u2013117", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust thermal boundary conditions applicable to a wall along which temperature varies in lattice-gas cellular automata", "author": ["J.W. Shim", "R. Gatignol"], "venue": "Physical Review E 81(4), 046,703", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical mechanics of self-gravitating systems: Mixing as a criterion for indistinguishability", "author": ["L.B. e Silva", "M. Lima", "L. Sodr\u00e9", "J. Perez"], "venue": "Physical Review D 90(12), 123,004", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Tech. rep., DTIC Document", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "H. Lee", "X. Yan"], "venue": "C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, R. Garnett (eds.) Advances in Neural Information Processing Systems 28, pp. 3465\u2013 3473. Curran Associates, Inc.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research 15(1), 1929\u20131958", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pp. 1064\u20131071. ACM, New York, NY, USA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning invariant features using subspace restricted boltzmann machine", "author": ["J.M. Tomczak", "A. Gonczarek"], "venue": "Neural Processing Letters pp. 1\u201310", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Individual Comparisons by Ranking Methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin 1(6), 80\u201383", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1945}], "referenceMentions": [{"referenceID": 3, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 157, "endOffset": 165}, {"referenceID": 1, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 157, "endOffset": 165}, {"referenceID": 25, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 157, "endOffset": 165}, {"referenceID": 11, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 288, "endOffset": 292}, {"referenceID": 8, "context": "Restricted Boltzmann Machines (RBMs) [9], for instance, are among the most pursued techniques, even though they are not", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "deep learning-oriented themselves, but by building blocks composed of stacked RBMs on top of each other one can obtain the so-called Deep Belief Networks (DBNs) [10] or the Deep Boltzmann Machines (DBMs) [21], which basically differ from each other by the way the inner layers interact among themselves.", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "deep learning-oriented themselves, but by building blocks composed of stacked RBMs on top of each other one can obtain the so-called Deep Belief Networks (DBNs) [10] or the Deep Boltzmann Machines (DBMs) [21], which basically differ from each other by the way the inner layers interact among themselves.", "startOffset": 204, "endOffset": 208}, {"referenceID": 10, "context": "The Restricted Boltzmann Machine is a probabilistic model that uses a layer of hidden units to model the distribution over a set of inputs, thus compounding a generative stochastic neural network [11,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 21, "context": "The Restricted Boltzmann Machine is a probabilistic model that uses a layer of hidden units to model the distribution over a set of inputs, thus compounding a generative stochastic neural network [11,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 24, "context": "RBMs were firstly idealized under the name of \u201cHarmonium\u201d by Smolensky in 1986 [25], and some years later renamed to RBM by Hinton et.", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 6, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 15, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 16, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 17, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 28, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 12, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 185, "endOffset": 189}, {"referenceID": 2, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy.", "startOffset": 42, "endOffset": 51}, {"referenceID": 22, "context": "In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy.", "startOffset": 42, "endOffset": 51}, {"referenceID": 14, "context": "In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy.", "startOffset": 42, "endOffset": 51}, {"referenceID": 12, "context": "[13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].", "startOffset": 239, "endOffset": 242}, {"referenceID": 19, "context": "[13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].", "startOffset": 244, "endOffset": 248}, {"referenceID": 7, "context": "Notice the terms P (h\u0303|\u1e7d) and \u1e7d can be obtained by means of the Contrastive Divergence [8] technique, which basically ends up performing Gibbs sampling using the training data as the visible units.", "startOffset": 87, "endOffset": 90}, {"referenceID": 20, "context": "Roughly speaking, using such procedure, the conditional probabilities given by Equations 15-17, and Contrastive Divergence, one can learn DBM parameters one layer at a time [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "[13] showed that a temperature parameter T controls the sharpness of the logistic-sigmoid function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "05 [30], we conducted a cross-validation procedure with 20 runnings.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In order to provide a more precise experimental validation, we trained both DBMs and DBNs with two different algorithms: Contrastive Divergence (CD) [8] and Persistent Contrastive Divergence (PCD) [28].", "startOffset": 149, "endOffset": 152}, {"referenceID": 27, "context": "In order to provide a more precise experimental validation, we trained both DBMs and DBNs with two different algorithms: Contrastive Divergence (CD) [8] and Persistent Contrastive Divergence (PCD) [28].", "startOffset": 197, "endOffset": 201}, {"referenceID": 12, "context": "[13], i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], sparsity in the neuron\u2019s activity favours the power of generalization of a network, which is somehow related to dropping neurons out in order to avoid overfitting [27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[19], sparsity in the neuron\u2019s activity favours the power of generalization of a network, which is somehow related to dropping neurons out in order to avoid overfitting [27].", "startOffset": 169, "endOffset": 173}, {"referenceID": 12, "context": "Inspired by a very recent work that proposed the Temperaturebased Restricted Boltzmann Machines [13], we decided to evaluate the influence of the temperature when learning with Deep Boltzmann Machines aiming at the task of binary image reconstruction.", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "[13], where the lower the temperature, the more generalized is the network.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Deep learning techniques have been paramount in the last years, mainly due to their outstanding results in a number of applications, that range from speech recognition to face-based user identification. Despite other techniques employed for such purposes, Deep Boltzmann Machines are among the most used ones, which are composed of layers of Restricted Boltzmann Machines (RBMs) stacked on top of each other. In this work, we evaluate the concept of temperature in DBMs, which play a key role in Boltzmann-related distributions, but it has never been considered in this context up to date. Therefore, the main contribution of this paper is to take into account this information and to evaluate its influence in DBMs considering the task of binary image reconstruction. We expect this work can foster future research considering the usage of different temperatures during learning in DBMs.", "creator": "LaTeX with hyperref package"}}}