{"id": "1608.07738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "Testing APSyn against Vector Cosine on Similarity Estimation", "abstract": "in distributional semantic models ( dsms ), vector cosine is widely used in estimate similarity between unrelated vectors, although this measure was noticed often suffer from several shortcomings. significant recent literature has embraced other approaches which attempt to mitigate such biases. in this paper, we intend to investigate apsyn, a measure that computes the extent of the intersection spanning the most associated contexts of two target words, weighting it by context relevance. we evaluated this knowledge in a similarity estimation task on several popular test sets, and our results show showing apsyn is in fact thoroughly competitive, even with respect poor consistency results reported in the tool for word embeddings. on top of performance, apsyn addresses some of the weaknesses through vector cosine, performing well also on genuine similarity assumptions.", "histories": [["v1", "Sat, 27 Aug 2016 19:57:55 GMT  (52kb,D)", "https://arxiv.org/abs/1608.07738v1", "8 pages, 1 figure, 4 tables, PACLIC, cosine, vectors, DSMs"], ["v2", "Wed, 5 Oct 2016 10:46:25 GMT  (52kb,D)", "http://arxiv.org/abs/1608.07738v2", "8 pages, 1 figure, 4 tables, PACLIC, cosine, vectors, DSMs"]], "COMMENTS": "8 pages, 1 figure, 4 tables, PACLIC, cosine, vectors, DSMs", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enrico santus", "emmanuele chersoni", "alessandro lenci", "chu-ren huang", "philippe blache"], "accepted": false, "id": "1608.07738"}, "pdf": {"name": "1608.07738.pdf", "metadata": {"source": "CRF", "title": "Testing APSyn against Vector Cosine on Similarity Estimation", "authors": ["Enrico Santus", "Emmanuele Chersoni", "Alessandro Lenci", "Chu-Ren Huang", "Philippe Blache"], "emails": ["emmanuelechersoni}@gmail.com", "alessandro.lenci@unipi.it", "churen.huang@polyu.edu.hk", "blache@lpl-aix.fr"], "sections": [{"heading": "1 Introduction", "text": "Word similarity is one of the most important and most studied problems in Natural Language Processing (NLP), as it is fundamental for a wide range of tasks, such as Word Sense Disambiguation (WSD), Information Extraction (IE), Paraphrase Generation (PG), as well as the automatic creation of semantic resources. Most of the current approaches to word similarity estimation rely on some version of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These first-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schnar X iv :1 60 8. 07 73 8v 2 [ cs .C L ] 5 O ct\n2 01\nabel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel, 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015). In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by Baroni et al. (2014). On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 DSMs, Measures of Association and Dimensionality Reduction", "text": "Count-based DSMs are built in an unsupervised way. Starting from large preprocessed corpora, a matrix M(m\u00d7n) is built, in which each row is a vector representing a target word in a vocabulary of size m, and each column is one of the n potential contexts (Turney and Pantel, 2010; Levy et al., 2015). The vector dimensions are counters recording how many times the contexts co-occur with the target words. Since raw frequency is highly skewed, most DSMs have adopted more sophisticated association measures, such as Positive PMI (PPMI; Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015) and Local Mutual Information (LMI; Evert, 2005). PPMI compares the observed joint probability of co-occurrence of w and c with their probability of co-occurrence assuming statistical indipendence. It is defined as: PPMI(w, c) = max(PMI(w, c), 0) (1) PMI(w, c) = log ( P (w, c) P (w)P (c) ) = log ( |w, c|D |w||c| ) (2)\nwhere w is the target word, c is the given context, P(w,c) is the probability of co-occurrence, and D is the collection of observed word-context pairs. Unlike frequency, PPMI was found to have a bias towards rare events. LMI could therefore be used to reduce such bias and it consists in multiplying the PPMI of the pair by its co-occurrence frequency. Since target words may occur in hundreds of thousands contexts, most of which are not informative, methods for dimensionality reduction have been investigated, such as truncated SVD (Deerwester et al., 1990; Landauer and Dumais, 1997; Turney and Pantel, 2010; Levy et al., 2015). SVD has been regarded as a method for noise reduction and for the discovery of latent dimensions of meaning, and it has been shown to improve similarity measurements when combined with PPMI (Bullinaria and Levy, 2012; Levy et al., 2015). As we will see in the next section, APSyn applies another type of reduction, which consists in selecting only the top-ranked\ncontexts in a relevance sorted context list for each word vector. Such reduction complies with the principle of cognitive economy (i.e. only the most relevant contexts are elaborated; see Finton, 2002) and with the results of behavioural studies, which supported feature saliency (Smith et al., 1974). Since APSyn was defined for linguistic contexts (Santus et al., 2016b), we did not test it on SVD-reduced spaces, leaving such test to further studies."}, {"heading": "2.2 Similarity Measures", "text": "Vector Cosine, by far the most common distributional similarity metric (Turney and Pantel, 2010; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the dimensions of two word vectors, w1 and w2 and returns a score between -1 and 1. It is described by the following equation:\ncos(w1, w2) = \u2211n i=1 f1i\u00d7 f2i\u221a\u2211n\ni=1 f1i\u00d7 \u221a\u2211n i=1 f2i (3)\nwhere fix is the i-th dimension in the vector x. Despite its extensive usage, Vector Cosine has been recently criticized for its hyper sensibility to features with high values and for the inability of identifying the actual feature intersection (Li and Han, 2013; Schnabel et al., 2015). Recalling an example by Li and Han (2013), the Vector Cosine for the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.e. 0.8944) is unexpectedly higher than the one for a and c = [2, 1, 0] (i.e. 0.8000), and even higher than the one for the toy-vectors a and d = [1, 2, 1] (i.e. 0.6325), which instead share a larger feature intersection. Since the Vector Cosine is a distance measure, it is also subject to the hubness problem, which was shown by Radovanovic et al. (2010) to be an inherent property of data distributions in highdimensional vector space. The problem consists in the fact that vectors with high frequency tend to get high scores with a large number of other vectors, thus becoming universal nearest neighbours (Dinu et al., 2014; Schnabel et al., 2015; Faruqui et al., 2016). Another measure of word similarity named APSyn 1\n1Scripts and information can be found at https://github.com/esantus/APSyn\nhas been recently introduced in Santus et al. (2016a) and Santus et al. (2016b), and it was shown to outperform the vector cosine on the TOEFL (Landauer and Dumais, 1997) and on the ESL (Turney, 2001) test sets. This measure is based on the hypothesis that words carrying similar meanings share their most relevant contexts in higher proportion compared to less similar words. The authors define APSyn as the extent of the weighted intersection between the top most salient contexts of the target words, weighting it by the average rank of the intersected features in the PPMI-sorted contexts lists of the target words:\nAPSyn(w1, w2) =\n\u2211 f N(F1)\u2229N(F2)\n1\n(rank1(f) + rank2(f))/2\n(4)\nmeaning: for every feature f included in the intersection between the top N features of w1 and the top of w2 (i.e. N(f1) and N(f2)), add 1 divided by the average rank of the feature in the PPMI-ranked features of w1 (i.e. rank1) and w2 (i.e. rank2). According to the authors, N is a parameter, generally ranging between 100 and 1000. Results are shown to be relatively stable when N varies in this range, while become worst if bigger N are used, as low informative features are also introduced. Santus et al. (2016a) have also used LMI instead of PPMI as weighting function, but achieving lower results. With respect to the limitations mentioned above for the Vector Cosine, APSyn has some advantages. First of all, it is by definition able to identify the extent of the intersection. Second, its sensibility to features with high values can be kept under control by tuning the value of N. On top of it, feature values (i.e. their weights) do not affect directly the similarity score, as they are only used to build the feature rank. With reference to the toy-vectors presented above, APSyn would assign in fact completely different scores. The higher score would be assigned to a and d, as they share two relevant features out of three. The second higher score would be assigned to a and c, for the same reason as above. The lower score would be instead assigned to a and b, as they only share one non-salient feature. In section 3.4, we briefly discuss the hubness problem."}, {"heading": "2.3 Datasets", "text": "For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014), SimLex-999 (Hill et al., 2015). These datasets have a different history, but all of them consist in word pairs with an associated score, that should either represent word association or word similarity. WordSim-353 (Finkelstein et al., 2001) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, Hill et al. (2015) claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity scores to entities that are only related by virtue of frequent association (e.g. coffee and cup; movie and theater). On top of it, WordSim-353 does not provide the POS-tags for the 439 words that it contains, forcing the users to decide which POS to assign to the ambiguous words (e.g. [white, rabbit] and [run, marathon]). An extension of this dataset resulted from the subclassification carried out by Agirre et al. (2009), which discriminated between similar and associated word pairs. Such discrimination was done by asking annotators to classify all pairs according to the semantic relation they hold (i.e. identical, synonymy, antonymy, hypernymy, meronymy and none-of-the-above). The annotation was then used to group the pairs in three categories: similar pairs (those classified as identical, synonyms, antonyms and hypernyms), associated pairs (those classified as meronyms and none-of-the-above, with an average similarity greater than 5), and non-associated pairs (those classified as none-of-the-above, with an average similarity below or equal to 5). Two gold standard were finally produced: i) one for similarity, containing 203 word pairs resulting from the union of similar and non-associated pairs; ii) one for relatedness, containing 252 word pairs resulting from the union of associated and non-associated pairs. Even though such a classification made a clear distinction between the two types of relations (i.e. similarity and association), Hill et al. (2015) argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair - among two of them - was the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to Hill et al. (2015), the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. SimLex-999 is the dataset introduced by Hill et al. (2015) to address the above mentioned criticisms of confusion between similarity and association. The dataset consists of 999 pairs containing 1,028 words, which were also evaluated in terms of POS-tags and concreteness. The pairs were annotated with a score between 0 and 10, and the instructions were strictly requiring the identification of word similarity, rather than word association. Hill et al. (2015) claim that differently from other datasets, SimLex-999 interannotator agreement has not been surpassed by any automatic approach."}, {"heading": "2.4 State of the Art Vector Space Models", "text": "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by Hill et al. (2015), who used the code (or directly the embeddings) shared by the original authors. As we trained our models on almost the same corpora used by Hill and colleagues, the results are perfectly comparable. The three models we compare our results to are: i) the convolutional neural network of Collobert and Weston (2008), which was trained on 852 million words of Wikipedia; ii) the neural network of Huang et al. (2012), which was trained on 990 million words of Wikipedia; and iii) the word2vec of Mikolov et al. (2013), which was trained on 1000 million words of Wikipedia and on the RCV Vol. 1 Corpus (Lewis et al., 2004)."}, {"heading": "3 Experiments", "text": "In this section, we describe our experiments, starting from the training corpora (Section 3.1), to move to the implementation of twenty-eight DSMs (Section 3.2), following with the application and evaluation of the measures (Section 3.3), up to the performance analysis (Section 3.4) and the scalability test (Section 3.5)."}, {"heading": "3.1 Corpora and Preprocessing", "text": "We used two different corpora for our experiments: RCV vol. 1 (Lewis et al., 2004) and the Wikipedia corpus (Baroni et al., 2009), respectively containing 150 and 820 million words. The RCV Vol. 1 and Wikipedia were automatically tagged, respectively, with the POS tagger described in Dell\u2019Orletta (2009) and with the TreeTagger (Schmid, 1994)."}, {"heading": "3.2 DSMs", "text": "For our experiments, we implemented twenty-eight DSMs, but for reasons of space only sixteen of them are reported in the tables. All of them include the pos-tagged target words used in the three datasets (i.e. MEN, WordSim-353 and SimLex-999) and the pos-tagged contexts having frequency above 100 in the two corpora. We considered as contexts the\ncontent words (i.e. nouns, verbs and adjectives) within a window of 2, 3 and 5, even though the latter was given up for its poor performances. As for SVD factorization, we found out that the best results were always achieved when the number of latent dimensions was between 300 and 500. We report here only the scores for k = 300, since 300 is one of the most common choices for the dimensionality of SVD-reduced spaces and it is always close to be an optimal value for the parameter. Fourteen out of twenty-eight models were developed for RCV1, while the others were developed for Wikipedia. For each corpus, the models differed according to the window size (i.e. 2 and 3), to the statistical association measure used as a weighting scheme (i.e. none, PPMI and LMI) and to the application of SVD to the previous combinations."}, {"heading": "3.3 Measuring Word Similarity and Relatedness", "text": "Given the twenty-eight DSMs, for each dataset we have measured the Vector Cosine and APSyn between the words in the test pairs.\nThe Spearman correlation between our scores and the gold standard was then computed for every model and it is reported in Table 1 and Table 2. In particular, Table 1 describes the performances on SimLex-999, WordSim-353 and MEN for the measures applied on RCV Vol. 1 models. Table 2, instead, describes the performances of the measures on the three datasets for the Wikipedia models. Concurrently, Table 3 and Table 4 describe the performances of the measures respectively on the RCV Vol. 1 and Wikipedia models, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009)."}, {"heading": "3.4 Performance Analysis", "text": "Table 1 shows the Spearman correlation scores for Vector Cosine and APSyn on the three datasets for the eight most representative DSMs built using RCV Vol. 1. Table 2 does the same for the DSMs built using Wikipedia. For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in Hill et al. (2015) (see Section 2.5). With a glance at the tables, it can be easily noticed that the measures perform particularly well in two models: i) APSyn, when applied on the PPMI-weighted DSM (henceforth, APSynPPMI); ii) Vector Cosine, when applied on the SVD-reduced PPMI-weighted matrix (henceforth, CosSVDPPMI). These two models perform consistently and in a comparable way across the datasets, generally outperforming the state-of-the-art DSMs, with an exception for the Wikipedia-trained models in WordSim-353. Some further observations are: i) corpus size strongly affects the results; ii) PPMI strongly outperforms LMI for both Vector Cosine and APSyn; iii) SVD boosts the Vector Cosine, especially when it is combined with PPMI; iv) N has some impact on the performance of APSyn, which generally achieves the best results for N=500. As a note about iii), the results of using SVD jointly with LMI spaces are less predictable than when combining it with PPMI. Also, we can notice that the smaller window (i.e. 2) does not always perform better than the larger one (i.e. 3). The former appears to perform better on SimLex-999, while the latter seems to have some advantages on the other datasets. This\nmight depend on the different type of similarity encoded in SimLex-999 (i.e. genuine similarity). On top of it, despite Hill et al. (2015)\u2019s claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014), we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015), using the words of the SimLex-999 dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list. It can be easily noticed that the highest ranked nearest neighbors tend to have higher rank also in the frequency list, supporting the idea that frequent words are more likely to be nearest neighbors. APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (Radovanovic et al., 2010). Further investigation is needed to see whether variations of APSyn can tackle this problem.\nFinally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015). Table 3 and Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009). It can be easily noticed that our best models work better on the similarity subset. In particular, APSynPPMI performs about 20-30% better for the similarity subset than for the relatedness one (see Table 3), as well as both APSynPPMI and CosSVDPPMI do in Wikipedia (see Table 4)."}, {"heading": "3.5 Scalability", "text": "In order to evaluate the scalability of APSyn, we have performed a pilot test on WordSim-353 and MEN with the same corpus used by Baroni et al. (2014), which consists of about 2.8B words (i.e. about 3 times Wikipedia and almost 20 times RCV1). The best scores were obtained with APSyn, N=1000, on a 2-window PPMI-weighted DSM. In such setting, we obtain a Spearman correlation of 0.72 on WordSim and 0.77 on MEN. These results are much higher than those reported by Baroni et al. (2014) for the count-based models (i.e. 0.62 on WordSim and 0.72 on MEN) and slightly lower than those reported for the predicting ones (i.e. 0.75 on WordSim and 0.80 on MEN)."}, {"heading": "4 Conclusions", "text": "In this paper, we have presented the first systematic evaluation of APSyn, comparing it to Vector Cosine in the task of word similarity identification. We developed twenty-eight count-based DSMs, each of which implementing different hyperparameters. PPMI emerged as the most efficient association measure: it works particularly well with Vector Cosine, when combined with SVD, and it boosts APSyn. APSyn showed extremely promising results, despite its conceptual simplicity. It outperforms the Vector Cosine in almost all settings, except when the latter is used on a PPMI-weighed SVD-reduced DSM. Even in this case, anyway, its performance is very competitive. Interestingly, our best models achieve results that are comparable to - or even better than - those reported by Hill et al. (2015) for the stateof-the-art word embeddings models. In Section 3.5 we show that APSyn is scalable, outperforming the state-of-the-art count-based models reported in Baroni et al. (2014). On top of it, APSyn does not suffer from some of the problems reported for the Vector Cosine, such as the inability of identifying the number of shared features. It still however seems to be affected by the hubness issue, and more research should be carried out to tackle it. Concerning the discrimination between similarity and association, the good performance of APSyn on SimLex-999 (which was built with a specific attention to genuine similarity) and the large difference in performance between the two subsets of WordSim-353 described in Table 3 and Table 4 make us conclude that APSyn is indeed efficient in quantifying genuine similarity. To conclude, being a linguistically and cognitively grounded metric, APSyn offers the possibility for further improvements, by simply combining it to other properties that were not yet considered in its definition. A natural extension would be to verify whether APSyn hypothesis and implementation holds on SVD reduced matrices and word embeddings."}, {"heading": "Acknowledgments", "text": "This paper is partially supported by HK PhD Fellowship Scheme, under PF12-13656. Emmanuele Chersoni\u2019s research is funded by a grant of the University Foundation A*MIDEX. Thanks to Davis Ozols for the support with R."}], "references": [{"title": "The wacky wide web: a", "author": ["Eros Zanchetta"], "venue": null, "citeRegEx": "Zanchetta.,? \\Q2009\\E", "shortCiteRegEx": "Zanchetta.", "year": 2009}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman."], "venue": "Journal of the American society for information science, 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Ensemble system for part-ofspeech tagging", "author": ["Felice DellOrletta."], "venue": "Proceedings of EVALITA, 9.", "citeRegEx": "DellOrletta.,? 2009", "shortCiteRegEx": "DellOrletta.", "year": 2009}, {"title": "Encyclopedia of distances", "author": ["Michel Marie Deza", "Elena Deza."], "venue": "Springer.", "citeRegEx": "Deza and Deza.,? 2009", "shortCiteRegEx": "Deza and Deza.", "year": 2009}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni"], "venue": "arXiv preprint arXiv:1603.09054.", "citeRegEx": "Dinu et al\\.,? 2014", "shortCiteRegEx": "Dinu et al\\.", "year": 2014}, {"title": "The statistics of word cooccurrences: word pairs and collocations", "author": ["Stefan Evert"], "venue": null, "citeRegEx": "Evert.,? \\Q2005\\E", "shortCiteRegEx": "Evert.", "year": 2005}, {"title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Ratogi", "Chris Dyer."], "venue": "arXiv preprint arXiv:1301.3781..", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "WordNet", "author": ["Christiane Fellbaum."], "venue": "Wiley Online Library.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013414. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Cognitive economy and the role of representation in on-line learning", "author": ["David Finton."], "venue": "Doctoral dissertation. University of Wisconsin-Madison.", "citeRegEx": "Finton.,? 2002", "shortCiteRegEx": "Finton.", "year": 2002}, {"title": "Papers in linguistics, 19341951", "author": ["John Rupert Firth."], "venue": "Oxford University Press.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Rogets thesaurus and semantic similarity1", "author": ["Mario Jarmasz", "Stan Szpakowicz."], "venue": "Recent Advances in Natural Language Processing III: Selected Papers from RANLP, 2003:111.", "citeRegEx": "Jarmasz and Szpakowicz.,? 2004", "shortCiteRegEx": "Jarmasz and Szpakowicz.", "year": 2004}, {"title": "A systematic study of semantic vector space model parameters", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) at EACL, pages 21\u201330.", "citeRegEx": "Kiela and Clark.,? 2014", "shortCiteRegEx": "Kiela and Clark.", "year": 2014}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais."], "venue": "Psychological review, 104(2):211.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "An introduction to latent semantic analysis", "author": ["Thomas K Landauer", "Peter W Foltz", "Darrell Laham."], "venue": "Discourse processes, 25(2-3):259\u2013284.", "citeRegEx": "Landauer et al\\.,? 1998", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li."], "venue": "The Journal of Machine Learning Research, 5:361\u2013397.", "citeRegEx": "Lewis et al\\.,? 2004", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Distance weighted cosine similarity measure for text classification", "author": ["Baoli Li", "Liping Han"], "venue": "Intelligent Data Engineering and Automated Learning -IDEAL", "citeRegEx": "Li and Han.,? \\Q2013\\E", "shortCiteRegEx": "Li and Han.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the existence of obstinate results in vector space models", "author": ["Milos Radovanovic", "Alexandros Nanopoulos", "Mirjana Ivanovic."], "venue": "Proceedings of SIGIR:186-193.", "citeRegEx": "Radovanovic et al\\.,? 2010", "shortCiteRegEx": "Radovanovic et al\\.", "year": 2010}, {"title": "An introduction to random indexing", "author": ["Magnus Sahlgren."], "venue": "Methods and applications of semantic indexing workshop at the 7th international conference on terminology and knowledge engineering, TKE, volume 5.", "citeRegEx": "Sahlgren.,? 2005", "shortCiteRegEx": "Sahlgren.", "year": 2005}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren."], "venue": "Italian Journal of Linguistics, 20(1):33\u201354.", "citeRegEx": "Sahlgren.,? 2008", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Unsupervised Measure of Word Similarity: How to Outperform Co-Occurrence and Vector Cosine in VSMs", "author": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-ren Huang."], "venue": "arXiv preprint arXiv:1603.09054.", "citeRegEx": "Santus et al\\.,? 2016", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets", "author": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-ren Huang."], "venue": "Proceedings of LREC.", "citeRegEx": "Santus et al\\.,? 2016", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid."], "venue": "Proceedings of the international conference on new methods in language processing, volume 12, pages 44\u201349. Citeseer.", "citeRegEx": "Schmid.,? 1994", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimmo", "Thorsten Joachims."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Schnabel et al\\.,? 2015", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Structure and process in semantic memory: A featural model for semantic decisions", "author": ["Edward Smith", "Edward Shoben", "Lance Rips."], "venue": "Psychological Review, 81(3).", "citeRegEx": "Smith et al\\.,? 1974", "shortCiteRegEx": "Smith et al\\.", "year": 1974}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computa-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research, 37(1):141\u2013 188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl", "author": ["Peter D Turney"], "venue": null, "citeRegEx": "Turney.,? \\Q2001\\E", "shortCiteRegEx": "Turney.", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "Most of the current approaches to word similarity estimation rely on some version of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008).", "startOffset": 206, "endOffset": 249}, {"referenceID": 10, "context": "Most of the current approaches to word similarity estimation rely on some version of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008).", "startOffset": 206, "endOffset": 249}, {"referenceID": 24, "context": "Most of the current approaches to word similarity estimation rely on some version of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008).", "startOffset": 206, "endOffset": 249}, {"referenceID": 31, "context": "tween targets and their linguistic contexts (Turney and Pantel, 2010).", "startOffset": 44, "endOffset": 69}, {"referenceID": 18, "context": "or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values.", "startOffset": 16, "endOffset": 86}, {"referenceID": 16, "context": "Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and", "startOffset": 121, "endOffset": 153}, {"referenceID": 23, "context": "Random Indexing (Sahlgren, 2005).", "startOffset": 16, "endOffset": 32}, {"referenceID": 30, "context": "These first-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 174, "endOffset": 286}, {"referenceID": 13, "context": "These first-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 174, "endOffset": 286}, {"referenceID": 21, "context": "These first-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013).", "startOffset": 174, "endOffset": 286}, {"referenceID": 20, "context": "(Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors.", "startOffset": 0, "endOffset": 40}, {"referenceID": 6, "context": "(Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors.", "startOffset": 0, "endOffset": 40}, {"referenceID": 3, "context": "Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is", "startOffset": 64, "endOffset": 85}, {"referenceID": 31, "context": "still by far the most popular one (Turney and Pantel, 2010).", "startOffset": 34, "endOffset": 59}, {"referenceID": 25, "context": "However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words.", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "uation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 105, "endOffset": 131}, {"referenceID": 12, "context": ", 2014) and SimLex-999 (Hill et al., 2015).", "startOffset": 23, "endOffset": 42}, {"referenceID": 12, "context": "The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015). In such comparison,", "startOffset": 88, "endOffset": 107}, {"referenceID": 32, "context": "Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity.", "startOffset": 121, "endOffset": 175}, {"referenceID": 12, "context": "Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity.", "startOffset": 121, "endOffset": 175}, {"referenceID": 31, "context": "Starting from large preprocessed corpora, a matrix M(m\u00d7n) is built, in which each row is a vector representing a target word in a vocabulary of size m, and each column is one of the n potential contexts (Turney and Pantel, 2010; Levy et al., 2015).", "startOffset": 203, "endOffset": 247}, {"referenceID": 18, "context": "Starting from large preprocessed corpora, a matrix M(m\u00d7n) is built, in which each row is a vector representing a target word in a vocabulary of size m, and each column is one of the n potential contexts (Turney and Pantel, 2010; Levy et al., 2015).", "startOffset": 203, "endOffset": 247}, {"referenceID": 5, "context": "2015) and Local Mutual Information (LMI; Evert, 2005).", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": "Since target words may occur in hundreds of thousands contexts, most of which are not informative, methods for dimensionality reduction have been investigated, such as truncated SVD (Deerwester et al., 1990; Landauer and Dumais, 1997; Turney and Pantel, 2010; Levy et al., 2015).", "startOffset": 182, "endOffset": 278}, {"referenceID": 16, "context": "Since target words may occur in hundreds of thousands contexts, most of which are not informative, methods for dimensionality reduction have been investigated, such as truncated SVD (Deerwester et al., 1990; Landauer and Dumais, 1997; Turney and Pantel, 2010; Levy et al., 2015).", "startOffset": 182, "endOffset": 278}, {"referenceID": 31, "context": "Since target words may occur in hundreds of thousands contexts, most of which are not informative, methods for dimensionality reduction have been investigated, such as truncated SVD (Deerwester et al., 1990; Landauer and Dumais, 1997; Turney and Pantel, 2010; Levy et al., 2015).", "startOffset": 182, "endOffset": 278}, {"referenceID": 18, "context": "Since target words may occur in hundreds of thousands contexts, most of which are not informative, methods for dimensionality reduction have been investigated, such as truncated SVD (Deerwester et al., 1990; Landauer and Dumais, 1997; Turney and Pantel, 2010; Levy et al., 2015).", "startOffset": 182, "endOffset": 278}, {"referenceID": 18, "context": "SVD has been regarded as a method for noise reduction and for the discovery of latent dimensions of meaning, and it has been shown to improve similarity measurements when combined with PPMI (Bullinaria and Levy, 2012; Levy et al., 2015).", "startOffset": 190, "endOffset": 236}, {"referenceID": 29, "context": "with the results of behavioural studies, which supported feature saliency (Smith et al., 1974).", "startOffset": 74, "endOffset": 94}, {"referenceID": 31, "context": "Vector Cosine, by far the most common distributional similarity metric (Turney and Pantel, 2010; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the di-", "startOffset": 71, "endOffset": 194}, {"referenceID": 16, "context": "Vector Cosine, by far the most common distributional similarity metric (Turney and Pantel, 2010; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the di-", "startOffset": 71, "endOffset": 194}, {"referenceID": 14, "context": "Vector Cosine, by far the most common distributional similarity metric (Turney and Pantel, 2010; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the di-", "startOffset": 71, "endOffset": 194}, {"referenceID": 21, "context": "Vector Cosine, by far the most common distributional similarity metric (Turney and Pantel, 2010; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the di-", "startOffset": 71, "endOffset": 194}, {"referenceID": 18, "context": "Vector Cosine, by far the most common distributional similarity metric (Turney and Pantel, 2010; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the di-", "startOffset": 71, "endOffset": 194}, {"referenceID": 20, "context": "Despite its extensive usage, Vector Cosine has been recently criticized for its hyper sensibility to features with high values and for the inability of identifying the actual feature intersection (Li and Han, 2013; Schnabel et al., 2015).", "startOffset": 196, "endOffset": 237}, {"referenceID": 28, "context": "Despite its extensive usage, Vector Cosine has been recently criticized for its hyper sensibility to features with high values and for the inability of identifying the actual feature intersection (Li and Han, 2013; Schnabel et al., 2015).", "startOffset": 196, "endOffset": 237}, {"referenceID": 20, "context": "ple by Li and Han (2013), the Vector Cosine for the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.", "startOffset": 7, "endOffset": 25}, {"referenceID": 20, "context": "ple by Li and Han (2013), the Vector Cosine for the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.e. 0.8944) is unexpectedly higher than the one for a and c = [2, 1, 0] (i.e. 0.8000), and even higher than the one for the toy-vectors a and d = [1, 2, 1] (i.e. 0.6325), which instead share a larger feature intersection. Since the Vector Cosine is a distance measure, it is also subject to the hubness problem, which was shown by Radovanovic et al. (2010) to be an inherent property of data distributions in highdimensional vector space.", "startOffset": 7, "endOffset": 455}, {"referenceID": 4, "context": "the fact that vectors with high frequency tend to get high scores with a large number of other vectors, thus becoming universal nearest neighbours (Dinu et al., 2014; Schnabel et al., 2015; Faruqui et al., 2016).", "startOffset": 147, "endOffset": 211}, {"referenceID": 28, "context": "the fact that vectors with high frequency tend to get high scores with a large number of other vectors, thus becoming universal nearest neighbours (Dinu et al., 2014; Schnabel et al., 2015; Faruqui et al., 2016).", "startOffset": 147, "endOffset": 211}, {"referenceID": 6, "context": "the fact that vectors with high frequency tend to get high scores with a large number of other vectors, thus becoming universal nearest neighbours (Dinu et al., 2014; Schnabel et al., 2015; Faruqui et al., 2016).", "startOffset": 147, "endOffset": 211}, {"referenceID": 16, "context": "(2016b), and it was shown to outperform the vector cosine on the TOEFL (Landauer and Dumais, 1997) and on the ESL (Turney, 2001)", "startOffset": 71, "endOffset": 98}, {"referenceID": 32, "context": "(2016b), and it was shown to outperform the vector cosine on the TOEFL (Landauer and Dumais, 1997) and on the ESL (Turney, 2001)", "startOffset": 114, "endOffset": 128}, {"referenceID": 24, "context": "com/esantus/APSyn has been recently introduced in Santus et al. (2016a) and Santus et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 24, "context": "com/esantus/APSyn has been recently introduced in Santus et al. (2016a) and Santus et al. (2016b), and it was shown to outperform the vector cosine on the TOEFL (Landauer and Dumais, 1997) and on the ESL (Turney, 2001)", "startOffset": 50, "endOffset": 98}, {"referenceID": 25, "context": "Santus et al. (2016a) have also used LMI instead of PPMI as", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "For our evaluation, we used three widely popular datasets: WordSim-353 (Finkelstein et al., 2001),", "startOffset": 71, "endOffset": 97}, {"referenceID": 12, "context": ", 2014), SimLex-999 (Hill et al., 2015).", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "WordSim-353 (Finkelstein et al., 2001) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10.", "startOffset": 12, "endOffset": 38}, {"referenceID": 8, "context": "WordSim-353 (Finkelstein et al., 2001) was proposed as a word similarity dataset containing 353 pairs annotated with scores between 0 and 10. However, Hill et al. (2015) claimed that the instructions to the annotators were ambiguous with respect to similarity and association, so that the subjects assigned high similarity", "startOffset": 13, "endOffset": 170}, {"referenceID": 12, "context": "similarity and association), Hill et al. (2015) argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard.", "startOffset": 29, "endOffset": 48}, {"referenceID": 12, "context": "similarity and association), Hill et al. (2015) argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair - among two of them was the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to Hill et al. (2015), the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association.", "startOffset": 29, "endOffset": 714}, {"referenceID": 12, "context": "similarity and association), Hill et al. (2015) argue that these gold standards still carry the scores they had in WordSim-353, which are known to be ambiguous in this regard. The MEN Test Collection (Bruni et al., 2014) includes 3,000 word pairs divided in two sets (one for training and one for testing) together with human judgments, obtained through Amazon Mechanical Turk. The construction was performed by asking subjects to rate which pair - among two of them was the more related one (i.e. the most associated). Every pairs-couple was proposed only once, and a final score out of 50 was attributed to each pair, according to how many times it was rated as the most related. According to Hill et al. (2015), the major weakness of this dataset is that it does not encode word similarity, but a more general notion of association. SimLex-999 is the dataset introduced by Hill et al. (2015) to address the above mentioned criticisms of", "startOffset": 29, "endOffset": 895}, {"referenceID": 12, "context": "Hill et al. (2015) claim that differently from other datasets, SimLex-999 interannotator agreement has not been surpassed by any automatic approach.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "In order to compare our results with state-of-the-art DSMs, we report the scores for the Vector Cosines calculated on the neural language models (NLM) by Hill et al. (2015), who used the code (or directly the", "startOffset": 154, "endOffset": 173}, {"referenceID": 19, "context": "1 Corpus (Lewis et al., 2004).", "startOffset": 9, "endOffset": 29}, {"referenceID": 13, "context": "The three models we compare our results to are: i) the convolutional neural network of Collobert and Weston (2008), which was trained on 852 million words of Wikipedia; ii) the neural network of Huang et al. (2012), which was trained on 990 million words of Wikipedia; and iii) the word2vec of Mikolov et al.", "startOffset": 195, "endOffset": 215}, {"referenceID": 13, "context": "The three models we compare our results to are: i) the convolutional neural network of Collobert and Weston (2008), which was trained on 852 million words of Wikipedia; ii) the neural network of Huang et al. (2012), which was trained on 990 million words of Wikipedia; and iii) the word2vec of Mikolov et al. (2013), which was trained on 1000 million words of Wikipedia and on the RCV Vol.", "startOffset": 195, "endOffset": 316}, {"referenceID": 20, "context": "In the bottom the performance of the state-of-the-art model of Mikolov et al. (2013), as reported in Hill et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 12, "context": "(2013), as reported in Hill et al. (2015).", "startOffset": 23, "endOffset": 42}, {"referenceID": 19, "context": "1 (Lewis et al., 2004) and the Wikipedia corpus (Baroni et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 27, "context": "with the POS tagger described in Dell\u2019Orletta (2009) and with the TreeTagger (Schmid, 1994).", "startOffset": 77, "endOffset": 91}, {"referenceID": 12, "context": "In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008), Huang et al. (2012), Mikolov et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 12, "context": "In the bottom the performance of the state-of-the-art models of Collobert and Weston (2008), Huang et al. (2012), Mikolov et al. (2013), as reported in Hill et al.", "startOffset": 93, "endOffset": 136}, {"referenceID": 12, "context": "(2013), as reported in Hill et al. (2015).", "startOffset": 23, "endOffset": 42}, {"referenceID": 12, "context": "For the sake of comparison, we also report the results of the state-of-the-art DSMs mentioned in Hill et al. (2015) (see Section 2.", "startOffset": 97, "endOffset": 116}, {"referenceID": 15, "context": "(2015)\u2019s claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014), we need to mention that window 5", "startOffset": 137, "endOffset": 181}, {"referenceID": 12, "context": "On top of it, despite Hill et al. (2015)\u2019s claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 28, "context": "With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015), using the words of the SimLex-999 dataset as query words and collecting", "startOffset": 105, "endOffset": 128}, {"referenceID": 22, "context": "APSyn does not seem to be able to overcome such bias, which seems to be in fact an inherent property of the DSMs (Radovanovic et al., 2010).", "startOffset": 113, "endOffset": 139}, {"referenceID": 32, "context": "Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015).", "startOffset": 137, "endOffset": 191}, {"referenceID": 12, "context": "Finally, few words need to be spent with regard to the ability of calculating genuine similarity, as distinguished from word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015).", "startOffset": 137, "endOffset": 191}, {"referenceID": 12, "context": ", 2009; Hill et al., 2015). Table 3 and Table 4 show the Spearman correlation scores for the two measures calculated on the models respectively trained on RCV1 and Wikipedia, tested on the subsets of WordSim-353 extracted by Agirre et al. (2009). It can be easily noticed that our best models work better on the similarity subset.", "startOffset": 8, "endOffset": 246}, {"referenceID": 12, "context": "Interestingly, our best models achieve results that are comparable to - or even better than - those reported by Hill et al. (2015) for the stateof-the-art word embeddings models.", "startOffset": 112, "endOffset": 131}], "year": 2016, "abstractText": "In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation.", "creator": "TeX"}}}