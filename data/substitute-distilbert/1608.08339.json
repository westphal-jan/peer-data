{"id": "1608.08339", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence", "abstract": "in advanced thesis, we study the problem of recognizing video sequences of fingerspelled letters in american sign language ( act ). fingerspelling comprises a significant previously relatively understudied adaptation of asl, and recognizing it is challenging for a number of reasons : it encompasses quick, small motions that are often highly coarticulated ; it exhibits significant variation between signers ; and there has been a dearth of continuous fingerspelling data collected. in this work, humans estimate some types of recognition approaches, and estimate the signer variation problem. our best - performing models implement novel ( semi - markov ) conditional study fields using deep sleep network - based features. in the signer - dependent setting, those recognizers achieve up to about 8 % letter error rates. the signer - independent setting is much more developed, but with neural network adaptation we gain 0 to 17 % letter error rates.", "histories": [["v1", "Tue, 30 Aug 2016 06:12:22 GMT  (4909kb,D)", "http://arxiv.org/abs/1608.08339v1", "PhD Thesis"]], "COMMENTS": "PhD Thesis", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["taehwan kim"], "accepted": false, "id": "1608.08339"}, "pdf": {"name": "1608.08339.pdf", "metadata": {"source": "CRF", "title": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence", "authors": ["Taehwan Kim", "Greg Shakhnarovich", "Yisong Yue", "Vassilis Athitsos", "Iain Matthews", "Hao Tang", "Sarah Taylor", "Raquel Urtasun", "Heejin Choi", "Andrew Cotter", "Jonathan Keane", "Jian Peng", "Karthik Sridharan", "Zhiyong Wang", "Payman Yadollahpour", "Jian Yao"], "emails": [], "sections": [{"heading": null, "text": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition\nand signer-independence\nby\nTaehwan Kim\nA thesis submitted in partial fulfillment of the requirements for\nthe degree of\nDoctor of Philosophy in Computer Science\nat the\nToyota Technological Institute at Chicago\nChicago, Illionois August 2016\nThesis Committee: Vassilis Athitsos\nKaren Livescu (Thesis Advisor) Greg Shakhnarovich\nYisong Yue\nar X\niv :1\n60 8.\n08 33\n9v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\n01 6\n2\nAmerican Sign Language fingerspelling recognition from video: Methods for unrestricted recognition\nand signer-independence by\nTaehwan Kim"}, {"heading": "Abstract", "text": "In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates.\nThesis Supervisor: Karen Livescu Title: Assistant Professor\n3\n4\nAmerican Sign Language fingerspelling recognition from video: Methods for unrestricted recognition\nand signer-independence\nA thesis presented by\nTaehwan Kim\nin partial fulfillment of the requirements for the degree of\nDoctor of Philosophy in Computer Science\nToyota Technological Institute at Chicago\nChicago, Illinois\nAugust 2016\n\u2212 Thesis Committee \u2212\nVassilis Athitsos Committee member (print/type) Signature Date\nKaren Livescu Thesis/Research Advisor (print/type) Signature Date\nGreg Shakhnarovich Committee member (print/type) Signature Date\nYisong Yue Committee member (print/type) Signature Date\nDavid McAllester Chief Academic Officer (print/type) Signature Date\n6\nTo my parents"}, {"heading": "Acknowledgments", "text": "First and foremost, I would like to thank my PhD advisor, Karen Livescu, for insightful guidance, endless patience, and kindness throughout my PhD. This thesis would not have been possible without her support and encouragement. I also thank my thesis committee members, Vassilis Athitsos, Greg Shakhnarovich, and Yisong Yue for guiding and shaping this work through their invaluable feedback and help.\nMany others have contributed, both directly and indirectly, to the work described in this thesis. I was fortunate to collaborate with and thank my co-authors: Iain Matthews, Hao Tang, Sarah Taylor, Raquel Urtasun, and Weiran Wang. I am grateful to Adam Bohlander and Chrissy Novak for their kindness and help for all things administrative. I thank David McAllester for his help and leading this great school. I thank my fellow graduate students, Avleen Bijral, Heejin Choi, Andrew Cotter, Jonathan Keane, Jian Peng, Karthik Sridharan, Zhiyong Wang, Payman Yadollahpour, Jian Yao, and Feng Zhao for their inspiration and encourgement.\nI would like to express my deepest gratitude to my family, without whose support I would not have been able to complete my PhD. I thank Taemi for her endless encouragement. I thank my parents, Kwang-kuk and Jung-soon, for their unconditional love and support. I am grateful to always be in their prayer. I dedicate this thesis to my parents.\nAt the end, I would like to give my thanks to the living God, who has guided and will guide my life, and whose steadfast love for me endures forever.\n8"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 15", "text": "1.1 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"}, {"heading": "2 Recognition methods 21", "text": "2.1 Recognizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.1.1 Tandem model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.1.2 Rescoring segmental CRF . . . . . . . . . . . . . . . . . . . . . . 24 2.1.3 First-pass segmental CRF . . . . . . . . . . . . . . . . . . . . . . 28\n2.2 DNN adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28"}, {"heading": "3 Experimental Results 31", "text": "3.1 Data and annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.2 An initial experiment: tandem models and shallow classifiers . . . . . . . . 33 3.3 DNN frame classification performance . . . . . . . . . . . . . . . . . . . . 35 3.4 Letter recognition experiments . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.4.1 Signer-dependent recognition . . . . . . . . . . . . . . . . . . . . 38 3.4.2 Signer-independent recognition . . . . . . . . . . . . . . . . . . . 39 3.4.3 Signer-adapted recognition . . . . . . . . . . . . . . . . . . . . . . 39\n3.5 Extensions and analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.5.1 Analysis: Could we do better by training entirely on adaptation data? 40 3.5.2 Analysis: Letter vs. feature DNNs . . . . . . . . . . . . . . . . . . 41 3.5.3 Analysis: DNNs vs. CNNs . . . . . . . . . . . . . . . . . . . . . . 41 3.5.4 Improving performance in the force-aligned adaptation case . . . . 42 3.5.5 Improving performance with segmental cascades . . . . . . . . . . 43 3.5.6 Analysis: decomposition of errors . . . . . . . . . . . . . . . . . . 43"}, {"heading": "4 Conclusion 47", "text": ""}, {"heading": "A Examples of hypotheses produced by models 57", "text": ""}, {"heading": "B Example images of phonological feature values 59", "text": ""}, {"heading": "C Analysis: decomposition of errors 61", "text": "9"}, {"heading": "D Word lists recorded and used in experiments 63", "text": "10"}, {"heading": "List of Tables", "text": "2.1 Definition and possible values for phonological features . . . . . . . . . . . 23\n3.1 Letter error rate (%) on four test signers. . . . . . . . . . . . . . . . . . . . 39 3.2 Letter error rates (%) for different settings of SCRF and DNN training in the signer-adapted case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.3 Phonetic features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.4 Letter error rates (%) with iterated forced-alignment (FA) adaptation. . . . . 43 3.5 Letter error rates (%) with second pass cascade. . . . . . . . . . . . . . . . 43 3.6 Decomposition of letter error rate with adaptation . . . . . . . . . . . . . . 44\nB.1 Example images of each phonological feature values . . . . . . . . . . . . 60\nC.1 Decomposition of letter error rate with signer-independent experiment . . . 61 C.2 Decomposition of letter error rate with adaptation . . . . . . . . . . . . . . 62 C.3 Decomposition of letter error rate with signer-dependent experiment . . . . 62\nD.1 Word list 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 D.2 Word list 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n11\n12"}, {"heading": "List of Figures", "text": "1-1 The ASL fingerspelled alphabet . . . . . . . . . . . . . . . . . . . . . . . 16 1-2 Example images of Q recognized as G. . . . . . . . . . . . . . . . . . . . . 17\n2-1 Images and segmentaion of fingerspelled word \u2018TULIP\u2019 . . . . . . . . . . . 21 2-2 Images and segmentaion of fingerspelled word \u2018ART\u2019 . . . . . . . . . . . . 22 2-3 Example images corresponding to SF thumb . . . . . . . . . . . . . . . . . 24 2-4 Example of the operation of the tandem model recognizers with phonological feature vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2-5 Illustration of SCRF notation . . . . . . . . . . . . . . . . . . . . . . . . . 26 2-6 DNN adaptation approaches . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3-1 Example video frames from the four signers. . . . . . . . . . . . . . . . . . 32 3-2 Frame error rates of letter and feature NNs . . . . . . . . . . . . . . . . . . 34 3-3 Letter error rates and standard deviations on two signers . . . . . . . . . . 36 3-4 Frame errors with DNN classifiers, with various settings . . . . . . . . . . 37 3-5 Confusion matrices of DNN classifiers for one test signer (Signer 1) . . . . 38 3-6 Comparisons with different DNN classifiers . . . . . . . . . . . . . . . . . 45 3-7 Comparisons between DNN and CNN frame classifiers . . . . . . . . . . . 46 3-8 Illustration of improved deletion and substitution . . . . . . . . . . . . . . 46\nA-1 Examples of hypothesis produced by models . . . . . . . . . . . . . . . . . 57 A-2 Similar to Figure A-1 for the word GEORGE, from Signer 2. . . . . . . . . 58 A-3 Similar to Figure A-1 for the word MOVEMENT, from Signer 4. . . . . . . 58\n13\n14\nChapter 1"}, {"heading": "Introduction", "text": "Sign languages are the primary means of communication for millions of Deaf people in the world1. In the US, there are about 350,000\u2013500,000 people for whom American Sign Language (ASL) is the primary language [60]. Automatic sign language recognition is a nascent technology that has the potential to improve the ability of Deaf and hearing individuals to communicate, as well as Deaf individuals\u2019 ability to take full advantage of modern information technology. For example, online sign language video blogs and news2 are currently almost completely unindexed and unsearchable as they include little accompanying annotation. While there has been extensive research over several decades on automatic recognition and analysis of spoken language, much less progress has been made for sign languages. Both signers and non-signers would benefit from technology that improves communication between these populations and facilitates search and retrieval in sign language video.\nResearch on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30]. We focus on recognition from video for applicability to existing recordings. Before the technology can be applied \u201cin the wild\u201d, it must overcome challenges posed by visual nuisance parameters (e.g., lighting, occlusions) and signer variation. Annotated data sets for this problem are scarce, in part due to the need to recruit signers and skilled annotators.\nIn this thesis we consider American Sign Language (ASL), and focus in particular on recognition of fingerspelled letter sequences. In fingerspelling, signers spell out a word as a sequence of handshapes or hand trajectories corresponding to individual letters. The handshapes used in fingerspelling are also used throughout ASL. In fact, the fingerspelling handshapes account for about 72% of ASL handshapes [10], making research on fingerspelling applicable to ASL in general.\nFigure 1-1 shows the ASL fingerspelling alphabet. Fingerspelling is a constrained but important part of ASL, accounting for up to 35% of ASL [65]. Fingerspelling is typically used for names, borrowings from English or other spoken languages, or new coinages. ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other\n1This thesis includes material previously published in several papers [45, 46, 47]. 2E.g., http://ideafnews.com, http://aslized.org.\n15\nsigns. Therefore, fingerspelling can be difficult to analyze with standard approaches for pose estimation and tracking from video."}, {"heading": "1.1 Challenges", "text": "In many ways the problem of fingerspelling recognition is analogous to that of word sequence or phone sequence recognition. However, there are some special challenges that fingerspelling introduces. We address some of these challenges here, while others are left to future work.\n1. ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other signs. Therefore, fingerspelling can be difficult to analyze with standard approaches for pose estimation and tracking from video.\n2. Existing work on fingerspelling recognition has mostly focused on static images extracted from videos, or on letter sequences from a small vocabulary.\n3. Another challenge is the high variability among signers, with the limited available training data; main sources of signer variation are speed, hand appearance, and nonsigning motion variation before/after signing as shown in Figure 2-1. These make signer-independent recognition quite difficult.\n16\n4. There are specific linguistic characteristics of sign language and fingerspelling. For example, the concept of a \u201csilence\u201d unit is quite different from that of acoustic silence. By \u201csilence\u201d, we mean any video segment that does not correspond to fingerspelling, including any hand motion that is not linguistically meaningful. This may be thought of as a \u201cgarbage\u201d unit, but its appearance is highly dependent on the context. In our data, for example, \u201csilence\u201d typically corresponds to the time at the beginning and end of each letter sequence, when the signer\u2019s hand is rising to/falling from the signing position. Also, double letters are usually not signed as two copies of the same letter, but rather as either a single longer articulation or a special sign for the doubled letter. For example, \u2019ZZ\u2019 is often signed identically to a \u2019Z\u2019 but using two extended fingers rather than the usual one.\n5. The language model over fingerspelled letter sequences is difficult to estimate. There is no large database of natural fingerspelled sequences in running sign. In our data set, the distribution of words has been chosen to maximize coverage of letter n-grams and word types rather than to follow some natural distribution. Fingerspelling does not follow the distribution of, say, English words, since fingerspelling is most often used for words lacking ASL signs, such as names. For this work, we estimate language models from large English dictionaries that include names. However, such language models are not a perfect fit, and this issue requires more attention.\nTherefore, these make fingerspelling recognition quite challenging. For instance, Figure 1-2 gives example images of Q misrecognized as G by one of our recognition systems. It shows various handshape variations of Q that are similar to G, confusing to recognizers.\nWe are interested in addressing both the technological and the linguistic issues, focusing mainly on handshape modeling. Sign language handshape has its own phonology that has been studied but does not yet enjoy a broadly agreed-upon understanding [11]. Recent linguistic work on sign language phonology has developed approaches based on articulatory features, which are closely related to motions of parts of the hand [20]. Sign language recognition research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on the details of handshape [24, 97]. At the same time, computer vision research has studied pose estimation and tracking of hands [83], but usually not in the context of a grammar that constrains the motion. There is therefore a great need to better understand and model the handshape properties of sign language. Therefore, we propose to use linguistic features suggested by linguistics research on ASL [11, 42].\n17"}, {"heading": "1.2 Contributions", "text": "This thesis advances fingerspelling recognition from video in several ways.\n1. Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72]. In this project, we consider unconstrained fingerspelling sequences. This is a more natural setting, since fingerspelling is often used for names and other \u201cnew\u201d terms, which may not appear in any closed vocabulary. The work presented here used the largest video data set of which we are aware containing unconstrained, connected fingerspelling, consisting of four signers each signing 600 word tokens for a total of \u223c 350k image frames.\n2. We develop discriminative segmental models, which allow us to introduce more flexible features of fingerspelling segments. The use of such segmental feature functions is useful for gesture modeling, where it is natural to consider the trajectory of some measurement or the statistics of an entire segment. In this work we define feature functions based on scores of deep neural network classifiers of letter and handshape linguistic features. In the category of segmental models, we compare models for rescoring lattices and a first-pass segmental model. We find that our segmental models outperform previous approaches including hidden Markov model / Gaussian mixture approaches and tandem hidden Markov models with deep neural network classifier input.\n3. Signer-dependent applications [55, 46] have achieved letter error rates (Levenshtein distances between hypothesized and true letter sequences, as a proportion of the number of true letters) of 10% or less. However, the signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We find that, using signer adaptation techniques, we are able to bridge a large part of the gap between signer-dependent and signer-independent recognition performance.\n4. Ground-truth frame-level letter labels can be difficult and time-consuming to acquire. In this thesis we study the effect of quality of labeling on recognition performance. We develop recognizers that use either manually labeled and aligned data or unaligned data labeled only at the letter sequence level. While we are able to use both with significant success, we find that there is still a large degradation in performance between manually labeled frames and automatically aligned data. This remains one of the major challenges for future work.\n18"}, {"heading": "1.3 Related work", "text": "There has been significant work on sign language recognition from video. Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.\nMuch prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well. In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].\nVideo corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6]. But there are, to date, no corpora designed for ASL fingerspelling (and linguistic features of handshape in general) to our knowledge.\nMotion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data. Most of this prior work has focused on the larger gestures of sign language rather than finger movement. One exception is [57], which includes motion-capture glove data; however, this database is aimed at learning models for ASL generation and does not include the type of naturalistic, coarticulated data that we would like to study. Also [58] collected motion capture ASL data consisting of both sign and fingerspelling, but small scale for fingerspelling.\nSign language recognition from video begins with front-end features. Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67]. In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG features) [18].\nLinguistically motivated representations of handshape and motion have been used in some prior work, e.g., [9, 20, 86, 94, 93, 92, 91, 68, 88]. However, for connected fingerspelling recognition, a much finer level of detail is needed to represent the sub-articulators of the hand. This motivates our study of linguistic handshape features.\nA subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72]. Letter error rates of 10% or less have been achieved when the recognition is constrained to a small (up to 100-word) lexicon of allowed sequences. Our work is the first of which we are aware to address the task of unrestricted fingerspelling sequence recognition, and to explicitly study the contrast between signer-dependent and signer-independent recognition.\nThe segmental models we propose are related to prior work in both vision and speech. For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38]. In natural language processing, semi-Markov CRFs have been used for named entity recognition [75], where the labeling is binary. Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15]. One aspect that our work shares with the speech recognition work\n19\nis that we have a relatively large set of labels (26 letters plus non-letter \u201cN/A\u201d labels), and widely varying lengths of segments corresponding to each label (our data includes segment durations anywhere from 2 to 40 frames), which makes the search space larger and the recognition task more difficult than in the vision and text tasks to which such models have been applied. In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [103, 105]. We compare this approach to an efficient first-pass segmental model [84].\nFor the challenging signer-independent setting in which the signers in the training and test data are different, we may consider domain adaptation [7]. Due to differences in data distributions of source and target domain, the performance of the trained model can suffer and domain adaptation approaches have been popular in this setting. Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50]. We consider adapting our trained models toward a new test signer. Since our application and models are most similar to those in speech recognition, we base our signer adaptation approaches on those used for speaker adaptation in speech recognition. The classic techniques include maximum likelihood linear regression [50] and maximum a posteriori [49]. In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21]. We therefore base our adaptation techniques for our DNN-based models on these approaches. More details are given in Section 2.2.\n20\nChapter 2"}, {"heading": "Recognition methods", "text": "Our task is to take as input a video (a sequence of images) corresponding to a fingerspelled word, as in Figure 2-1, and predict the signed letters. This is a sequence prediction task analogous to connected phone or word recognition, but there are some interesting sign language-specific properties to the data domain. For example, one striking aspect of fingerspelling sequences, such as those in Figure 2-1, is the large amount of motion and lack of any prolonged \u201csteady state\u201d for each letter. Typically, each letter is represented by a brief \u201cpeak of articulation\u201d of one or a few frames, during which the hand\u2019s motion is at a minimum and the handshape is the closest to the target handshape for the letter. This peak is surrounded by longer period of motion between the current letter and the previous/next letters.\nWe consider signer-dependent, signer-independent, and signer-adapted recognition. We next describe the recognizers we compare, as well as the techniques we explore for signer adaptation. All of the recognizers use deep neural network (DNN) classifiers of letters or handshape features.\n21"}, {"heading": "2.1 Recognizers", "text": "In designing recognizers, we keep several considerations in mind. First, the data set, while large by sign language research standards, is still quite small compared to typical speech data sets. This means that large models with many context-dependent units are infeasible to train on our data (as confirmed by our initial experiments). We therefore restrict attention here to \u201cmono-letter\u201d models, that is models in which each unit is a context-independent letter. We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13]. Second, we would like our models to be able to capture rich sign language-specific information, such as the dynamic aspects of fingerspelled letters as discussed above; this suggests the segmental models that we consider below. Finally, we would like our models to be easy to adapt to new signers. In order to enable this, all of our recognizers use independently trained deep neural network (DNN) classifiers, which can be adapted and plugged into different sequence models. Our DNNs are trained using an L2-regularized cross-entropy loss. The inputs are the image features concatenated over a multi-frame window centered at the current frame, which are fed through several fully connected layers followed by a softmax output layer with as many units as labels."}, {"heading": "2.1.1 Tandem model", "text": "The first recognizer we consider is based on the popular tandem approach to speech recognition [27]. In tandem-based speech recognition, Neural Networks (NN) are trained to classify phones, and their outputs (phone posteriors) are post-processed and used as observations in a standard HMM-based recognizer with Gaussian mixture observation distributions. The post-processing may include taking the logs of the posteriors (or simply taking the linear outputs of the NNs rather than posteriors), applying principal components analysis, and/or appending acoustic features to the NN outputs.\nIn this work, we begin with a basic adaptation of the tandem approach, where instead of phone posteriors estimated from acoustic frames, we use letter posteriors estimated from image features. We also propose to use classifiers of phonological features of fin-\n22\ngerspelling. The motivation is that, since features have fewer values, it may be possible to learn them more robustly than letters from small training sets, and certain features may be more or less difficult to classify. This is similar to the work in speech recognition of \u00c7etin et al. [13], who used articulatory feature NN classifiers rather than phone classifiers.\nWe use a phonological feature set developed by Brentari [11], who proposed seven features for ASL handshape. Of these, we use the six that are contrastive in fingerspelling. The features and their values are given in Table 2.1. Example frames for values of the \u201cSF thumb\u201d feature are shown in Figure 2-3, and entire phonological feature vectors for several letters are shown in Appendix, Figure 2-4.\nFrame-level image features are fed to seven DNN classifiers, one of which predicts the frame\u2019s letter label and six others which predict handshape phonological features. The classifier outputs and image features are applied a dimensionality reduction via PCA, respectively, and concatenated with each other. The concatenated features form the observations in an HMM-based recognizer with Gaussian mixture observation densities. We use a 3-state HMM for each (context-independent) letter, plus one HMM each for initial and final \u201csilence\u201d (non-signing segments). Figure 2-4 shows an example of the operation of the tandem model recognizers.\nIn addition, we use a bigram letter language model. In general the language model\n23\nof fingerspelled letter sequences is difficult to define or estimate, since fingerspelling does not follow the same distribution as English words and there is no large database of natural fingerspelled sequences on which to train. In addition, in our data set, the words were selected so as to maximize coverage of letter n-grams and word types rather than following a natural distribution. For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [34]. The issue of language modeling for fingerspelling deserves more attention in future work."}, {"heading": "2.1.2 Rescoring segmental CRF", "text": "The second recognizer is a segmental CRF (SCRF). SCRFs [75, 103] are conditional loglinear models with feature functions that can be based on variable-length segments of input frames, allowing for great flexibility in defining feature functions. We use an SCRF to rescore lattices produced by a baseline frame-based recognizer (in this case, the tandem model above).\nWe begin by defining the problem and our notation. Let the sequence of visual observations for a given video (corresponding to a single word) be \ud835\udc42 = \ud835\udc5c1, . . . , \ud835\udc5c\ud835\udc47 , where each \ud835\udc5c\ud835\udc61 is a multidimensional image descriptor for frame \ud835\udc61. Our goal is to predict the label (letter) sequence. Ideally we would like to predict the best label sequence, marginalizing out different possible label start and end times, but in practice we use the typical approach of predicting the best sequence of frame labels \ud835\udc46 = \ud835\udc601, . . . , \ud835\udc60\ud835\udc47 . We predict \ud835\udc46 by maximizing its conditional probability under our model, \ud835\udc46 = argmax\ud835\udc46\ud835\udc43 (\ud835\udc46|\ud835\udc42). In generative models like HMMs, we have a joint model \ud835\udc43 (\ud835\udc46,\ud835\udc42) and we make a prediction using Bayes\u2019 rule. In conditional models we directly represent the conditional distribution \ud835\udc43 (\ud835\udc46|\ud835\udc42). For\n24\nexample, in a typical linear-chain CRF, we have:\n\ud835\udc5d(\ud835\udc46|\ud835\udc42) = 1 \ud835\udc4d(\ud835\udc42) exp (\ufe03\u2211\ufe01 \ud835\udc63,\ud835\udc58 \ud835\udf06\ud835\udc58\ud835\udc53\ud835\udc58(\ud835\udc46\ud835\udc63, \ud835\udc42\ud835\udc63) + \u2211\ufe01 \ud835\udc52,\ud835\udc58 \ud835\udf07\ud835\udc58\ud835\udc54\ud835\udc58(\ud835\udc46\ud835\udc52) )\ufe03\n25\nwhere \ud835\udc4d(\ud835\udc42) is the partition function, \ud835\udc53\ud835\udc58 are the \u201cnode\u201d feature functions that typically correspond to the state in a single frame \ud835\udc46\ud835\udc63 and its corresponding observation \ud835\udc42\ud835\udc63, \ud835\udc54\ud835\udc58 are \u201cedge\u201d feature functions corresponding to inter-state edges, \ud835\udc52 ranges over pairs of frames and \ud835\udc46\ud835\udc52 is the pair of states corresponding to \ud835\udc52, and \ud835\udf06\ud835\udc58 and \ud835\udf07\ud835\udc58 are the weights.\nIt may be more natural to consider feature functions that span entire segments corresponding to the same label. Semi-Markov CRFs [75], also referred to as segmental CRFs [103] or SCRFs, provide this ability.\nFigure 2-5 illustrates the SCRF notation, which we now describe. In a SCRF, we consider the segmentation to be a latent variable and sum over all possible segmentations of the observations corresponding to a given label sequence to get the conditional probability of the label sequence \ud835\udc46 = \ud835\udc601, . . . , \ud835\udc60\ud835\udc3f, where the length of \ud835\udc46 is now the (unknown) number of distinct labels \ud835\udc3f:\n\ud835\udc5d(\ud835\udc46|\ud835\udc42) =\n\u2211\ufe00 \ud835\udc5e s.t. |\ud835\udc5e|=|\ud835\udc46| exp (\ufe01\u2211\ufe00 \ud835\udc52\u2208\ud835\udc5e,\ud835\udc58 \ud835\udf06\ud835\udc58\ud835\udc53\ud835\udc58(\ud835\udc60 \ud835\udc52 \ud835\udc59 , \ud835\udc60 \ud835\udc52 \ud835\udc5f, \ud835\udc42\ud835\udc52) )\ufe01 \u2211\ufe00\n\ud835\udc46\u2032 \u2211\ufe00 \ud835\udc5e s.t.|\ud835\udc5e|=|\ud835\udc46\u2032| exp (\ufe01\u2211\ufe00 \ud835\udc52\u2208\ud835\udc5e,\ud835\udc58 \ud835\udf06\ud835\udc58\ud835\udc53\ud835\udc58(\ud835\udc60 \ud835\udc52 \ud835\udc59 , \ud835\udc60 \ud835\udc52 \ud835\udc5f, \ud835\udc42\ud835\udc52) )\ufe01 Here, \ud835\udc46 \u2032 ranges over all possible state (label) sequences, \ud835\udc5e is a segmentation of the observation sequence whose length (number of segments) must be the same as the number of states in \ud835\udc46 (or \ud835\udc46 \u2032), \ud835\udc52 ranges over all state pairs in \ud835\udc46, \ud835\udc60\ud835\udc52\ud835\udc59 is the state which is on the left of an edge, \ud835\udc60\ud835\udc52\ud835\udc5f is the state on the right of an edge, and \ud835\udc42\ud835\udc52 is the multi-frame observation segment associated with \ud835\udc60\ud835\udc52\ud835\udc5f. In our work, we use a tadem HMM frame-based recognizer to generate a set of candidate segmentations of \ud835\udc42, and sum only over those candidate segmentations. In principle the inference over all possible segmentations can be done, but typically this is only feasible for much smaller search spaces than ours.\nWe define several types of feature functions, some of which are quite general to sequence recognition tasks and some of which are tailored to fingerspelling recognition:\n26"}, {"heading": "Language model feature", "text": "The language model feature is a smoothed bigram probability of the letter pair corresponding to an edge:\n\ud835\udc53\ud835\udc59\ud835\udc5a(\ud835\udc60 \ud835\udc52 \ud835\udc59 , \ud835\udc60 \ud835\udc52 \ud835\udc5f, \ud835\udc42\ud835\udc52) = \ud835\udc5d\ud835\udc3f\ud835\udc40(\ud835\udc60 \ud835\udc52 \ud835\udc59 , \ud835\udc60 \ud835\udc52 \ud835\udc5f)."}, {"heading": "Baseline consistency feature", "text": "To take advantage of the existence of a high-quality baseline, we use a baseline feature like the one introduced by [103]. This feature is constructed using the 1-best output hypothesis from an HMM-based baseline recognizer. The feature value is 1 when a segment spans exactly one letter label hypothesized by the baseline and the label matches it:\n\ud835\udc53\ud835\udc4f(\ud835\udc60 \ud835\udc52 \ud835\udc59 , \ud835\udc60 \ud835\udc52 \ud835\udc5f, \ud835\udc42\ud835\udc52) = \u23a7\u23a8\u23a9 +1 if \ud835\udc36(\ud835\udc61(\ud835\udc52), \ud835\udc47 (\ud835\udc52)) = 1,\nand \ud835\udc35(\ud835\udc61(\ud835\udc52), \ud835\udc47 (\ud835\udc52)) = \ud835\udc64(\ud835\udc60\ud835\udc52\ud835\udc5f) \u22121 otherwise\nwhere \ud835\udc61(\ud835\udc52) and \ud835\udc47 (\ud835\udc52) are the start and end times corresponding to edge \ud835\udc52, \ud835\udc36(\ud835\udc61, \ud835\udc47 ) is the number of distinct baseline labels in the time span from \ud835\udc61 to \ud835\udc47 , \ud835\udc35(\ud835\udc61, \ud835\udc47 ) is the label corresponding to time span (\ud835\udc61, \ud835\udc47 ) when \ud835\udc36(\ud835\udc61, \ud835\udc47 ) = 1, and \ud835\udc64(\ud835\udc60) is the letter label of state \ud835\udc60.\nHandshape classifier-based feature functions\nThe next set of feature functions measure the degree of match between the intended segment label and the appearance of the frames within the segment. For this purpose we use a set of frame classifiers, each of which classifies either letters or linguistic handshape features. As in Section 2.1.1, we use the linguistic handshape feature set developed by Brentari [11], who proposed seven features to describe handshape in ASL. Each such linguistic feature (not to be confused with feature functions) has 2-7 possible values. Of these, we use the six that are contrastive in fingerspelling (See Table 2.1 for the details.). For each linguistic feature or letter, we train a classifier that produces a score for each feature value for each video frame. We also train a separate letter classifier. Specifically, we use deep neural network (DNN) classifiers.\nFeature functions Let \ud835\udc66 be a letter and \ud835\udc63 be the value of a linguistic feature or letter, \ud835\udc41\ud835\udc52 = |\ud835\udc42\ud835\udc52| = \ud835\udc47 (\ud835\udc52) + 1\u2212 \ud835\udc61(\ud835\udc52) the length of an observation segment corresponding to edge \ud835\udc52, and \ud835\udc54(\ud835\udc63|\ud835\udc5c\ud835\udc56) the output of a DNN classifier at frame \ud835\udc56 corresponding to class \ud835\udc63. We define\n\u2219 mean: \ud835\udc53\ud835\udc66\ud835\udc63(\ud835\udc60\ud835\udc52\ud835\udc59 , \ud835\udc60\ud835\udc52\ud835\udc5f, \ud835\udc42\ud835\udc52) = \ud835\udeff(\ud835\udc64(\ud835\udc60\ud835\udc52\ud835\udc5f) = \ud835\udc66) \u00b7 1\ud835\udc41\ud835\udc52 \u2211\ufe00\ud835\udc47 (\ud835\udc52) \ud835\udc56=\ud835\udc61(\ud835\udc52) \ud835\udc54(\ud835\udc63|\ud835\udc5c\ud835\udc56)\n\u2219 max: \ud835\udc53\ud835\udc66\ud835\udc63(\ud835\udc60\ud835\udc52\ud835\udc59 , \ud835\udc60\ud835\udc52\ud835\udc5f, \ud835\udc42\ud835\udc52) = \ud835\udeff(\ud835\udc64(\ud835\udc60\ud835\udc52\ud835\udc5f) = \ud835\udc66) \u00b7max\ud835\udc56\u2208(\ud835\udc61(\ud835\udc52),\ud835\udc47 (\ud835\udc52)) \ud835\udc54(\ud835\udc63|\ud835\udc5c\ud835\udc56)\n\u2219 div\ud835\udc60: a concatenation of three mean feature functions, each computed over a third of the segment\n\u2219 div\ud835\udc5a: a concatenation of three max feature functions, each computed over a third of the segment\n27"}, {"heading": "Peak detection features", "text": "Fingerspelling a sequence of letters yields a corresponding sequence of \u201cpeaks\u201d of articulation. Intuitively, these are frames in which the hand reaches the target handshape for a particular letter. The peak frame and the frames around it for each letter tend to be characterized by very little motion as the transition to the current letter has ended while the transition to the next letter has not yet begun, whereas the transitional frames between letter peaks have more motion. To use this information and encourage each predicted letter segment to have a single peak, we define letter-specific \u201cpeak detection features\u201d as follows. We first compute approximate derivatives of the visual descriptors, consisting of the \ud835\udc592 norm of the difference between descriptors in every pair of consecutive frames, smoothed by averaging over 5-frame windows. We expect there to be a single local minimum in this approximate derivative function over the span of the segment. Then we define the feature function corresponding to each letter \ud835\udc66 as\n\ud835\udc53 peak\ud835\udc66 (\ud835\udc60 \ud835\udc52 \ud835\udc59 , \ud835\udc60 \ud835\udc52 \ud835\udc5f, \ud835\udc42\ud835\udc52) = \ud835\udeff(\ud835\udc64(\ud835\udc60 \ud835\udc52 \ud835\udc5f) = \ud835\udc66) \u00b7 \ud835\udeffpeak(\ud835\udc42\ud835\udc52)\nwhere \ud835\udeffpeak(\ud835\udc42\ud835\udc52) is 1 if there is only one local minimum in the segment \ud835\udc42\ud835\udc52 and 0 otherwise."}, {"heading": "2.1.3 First-pass segmental CRF", "text": "One of the drawbacks of a rescoring approach is that the quality of the final outputs depends both on the quality of the baseline lattices and the fit between the segmentations in the baseline lattices and those preferred by the second-pass model. We therefore also consider a first-pass segmental model, using similar features to the rescoring model. In particular, we also use a first-pass SCRF inspired by the phonetic recognizer of Tang et al. [84]. We use the same feature functions as in [84], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias, all lexicalized."}, {"heading": "2.2 DNN adaptation", "text": "In experiments, we will consider both signer-dependent and signer-independent recognition. In the latter case, the test signer is never seen in training. As we will see, there is a very large gap between signer-dependent and signer-independent recognition on our data. Inspection of data such as Fig. 2-1 and 2-2 reveals some sources of signer variation, including differences in speed, hand appearance, and non-signing motion before and after signing. The speed variation is large, with a factor of 1.8 between the fastest and slowest signers. In the absence of adaptation data, we consider a simple speed normalization: We augment the training data with resampled image features, at 0.8x and 1.2x the original frame rate.\nIf we have access to some labeled data from the test signer, but not a sufficient amount for training full signer-specific models, we can consider adapting a signer-independent model toward the test signer. The most straightforward form of adaptation for our models\n28\nis to only adapt the DNN classifiers, and then use the adapted ones in pre-trained signerindependent models. The adaptation can use either ground-truth frame-level labels (given by human annotation) or, if only word labels are available, we can obtain frame labels via forced alignment using the signer-independent model.\nA number of DNN adaptation approaches have been developed for speech recognition (e.g., [53, 3, 82, 21]). We consider several approaches, shown in Figure 2-6. Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51]. In these techniques most of the network parameters are fixed; only a limited set of weights at the input and/or output layers are learned.\nIn the approach we refer to as LIN+UP in Figure 2-6, we apply a single affine transformation \ud835\udc4aLIN to the static features at each frame (before concatenation) and use the transformed features as input to the trained signer-independent DNNs. We jointly learn \ud835\udc4aLIN and adapt the last (softmax) layer weights by minimizing the same cross-entropy loss on the adaptation data. The softmax layer adaptation is achieved by \u201cwarm-starting\u201d with the learned signer-independent weights.\nThe second approach, referred to as LIN+LON in Figure 2-6, uses the same adaptation layer at the input. However, instead of adapting the softmax weights at the top layer, it removes the softmax output activation and adds a new softmax output layer \ud835\udc4aLON trained for the test signer. The new input and output layers are trained jointly with the same crossentropy loss.\nFinally, we also consider adaptation by fine-tuning all of the DNN weights on adaptation data, starting from the signer-independent DNN weights.\n29\n30\nChapter 3"}, {"heading": "Experimental Results", "text": "We report on experiments using the fingerspelling data from four native ASL signers. We begin by describing data and annotation, some of the front-end details of hand segmentation and feature extraction, followed by experiments with the frame-level DNN classifiers (Sec. 3.3) and letter sequence recognizers (Sec. 3.4)."}, {"heading": "3.1 Data and annotation", "text": "We use video recordings of four native ASL signers1. The data were recorded at 60 frames per second in a studio environment. Each signer signed a list of 300 words as they appeared on a computer screen in front of the signer. There were two non-overlapping lists of 300 words (one for signers 1 and 2, the other for signers 3 and 4. For the complete lists, see Table D.1 and D.2). Each word was spelled twice, yielding 600 word instances signed by each signer. The lists contained English and foreign words, including proper names and common English nouns. The total number of frames in four signers\u2019 data is\u223c 350k frames. The recording settings, including differences in environment and camera placement across recording sessions, are illustrated in Figure 3-1.\nThe signers sat at a chair, and the screen showing the word to be signed was in front of them. Each word was repeated twice, and the signers were asked to press the green button if they correctly signed the word, otherwise the red button. If the green button was pressed, the screen would move to the next (the same word if not repeated yet, or the next word in the word list). The start and end of each word were indicated by pressing a button, allowing automatic partition of the recording into a separate video for every word. We recorded six sessions with conversational and fluent speed. To synchronize the video and word signed, every video was verified and manually labeled by multiple annotators with the times and letter identities of the peaks of articulation (see Sec. 2.1.2). The peak annotations are used for the training portion of the data in each experiment to segment a word into letters (the boundary between consecutive letters is defined as the midpoint between their peaks). For more details about data and annotation steps, please refer to [43].\n1This section includes material previously published in [46].\n31"}, {"heading": "Hand localization and segmentation", "text": "For every signer, we trained a model for hand detection similar to that used in [45, 55]. Using manually annotated hand regions, marked as polygonal regions of interest (ROI) in 30 frames, we fit a mixture of Gaussians \ud835\udc43\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51 to the color of the hand pixels in L*a*b color space. Using the same 30 frames, we also built a single-Gaussian color model \ud835\udc43 \ud835\udc65\ud835\udc4f\ud835\udc54 for every pixel \ud835\udc65 in the image excluding pixel values in or near marked hand ROIs. Then, given a test frame, we label each pixel as hand or background based on an odds ratio: Given the color triplet c\ud835\udc65 = [\ud835\udc59\ud835\udc65, \ud835\udc4e\ud835\udc65, \ud835\udc4f\ud835\udc65] at pixel \ud835\udc65, we assign it to hand if\n\ud835\udc43\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51(c\ud835\udc65)\ud835\udf0b\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51 > \ud835\udc43 \ud835\udc65 \ud835\udc4f\ud835\udc54(c\ud835\udc65)(1\u2212 \ud835\udf0b\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51), (3.1)\nwhere the prior \ud835\udf0b\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51 for hand size is estimated from the same 30 training frames. Since this simple model produces rather noisy output, we next clean it up by a sequence of filtering steps. We suppress pixels that fall within regions detected as faces by the ViolaJones face detector [90], since these tend to be false positives. We also suppress pixels that passed the log-odds test but have a low estimated value of \ud835\udc43\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51. These tend to correspond to movements in the scene, e.g., a signer changing position and thus revealing previously occluded portions of the background; for such pixels the value of \ud835\udc43\ud835\udc4f\ud835\udc54 may be low, but so is \ud835\udc43\u210e\ud835\udc4e\ud835\udc5b\ud835\udc51. Finally, we suppress pixels outside of a (generous) spatial region where the signing is expected to occur. The largest surviving connected component of the resulting binary map is treated as a mask that defines the detected hand region. Some examples of resulting hand regions are shown in Figures 3-8 ( and more in Figure A-1, Figure A-2 and Figure A-3). Note that while this procedure currently requires manual annotation for a small number of frames in our offline recognition setting, it could be fully automated in a realistic interactive setting, by asking the subject to place his/her hand in a few defined locations for\n32\ncalibration."}, {"heading": "Handshape descriptors", "text": "For most experiments, we use histograms of oriented gradients (HOG [18]) as the visual descriptor (feature vector) for a given hand region2. We first resize the tight bounding box of the hand region to a canonical size of 128\u00d7128 pixels, and then compute HOG features on a spatial pyramid of regions, 4\u00d74, 8\u00d78, and 16\u00d716 grids, with eight orientation bins per grid cell, resulting in 2688-dimensional descriptors. Pixels outside of the hand mask are ignored in this computation. For HMM-based recognizer, to speed up computation, these descriptors were projected to at most 200 principal dimensions; the exact dimensionality in each experiment was tuned on a development set. For DNN frame classifiers, we found that finer grids did not improve much with increasing complexities, so we use 128-dimensional descriptors."}, {"heading": "3.2 An initial experiment: tandem models and shallow", "text": "classifiers\nBefore proceeding to our main experiments, we first analyze the linguistic features we proposed with a preliminary experiment by using two native ASL signers\u2019 data3. We use neural networks for frame classifiers, and tandem HMM models for a recognizer. And we compare to Gaussian Mixture Model HMM as a baseline. For image descriptors, we use SIFT (scale invariant feature transform) feature for this experiments. SIFT features is a commonly used type of image appearance features, and we extract SIFT features from each image, based on local histograms of oriented image gradients [56].\nFirst we train single layer neural networks (NN), which predict letter or phonological features. The inputs to the NNs are the SIFT features concatenated over a window of several frames around each frame. The NNs are implemented with Quicknet [71]. We consider several choices for the NN output functions: posteriors (softmax), log posteriors, and linear outputs. We obtain NN training labels for each frame either from the manually labeled apogees or from forced alignments produced by the baseline HMM-based recognizer. To derive a letter label for each frame given only the apogees, we assume that there is a letter boundary in the middle of each segment between consecutive apogees. NNs have one hidden layer with 1000 hidden nodes.\nThe task is recognition of one fingerspelled word at a time, delimited by the signer\u2019s button presses. We use a speaker-dependent 10-fold setup: We divide each speaker\u2019s data (\u223c 600 words, \u223c 3000 letters) randomly into ten subsets. In each fold, eight of the subsets (80% of the data) are used for training both NNs and HMMs, one (10%) for tuning NN\n2In initial experiments we use SIFT descriptors, described in Section 3.2. Their performances are similar but HoG feature is slightly better.\n3This section includes material previously published in [45].\n33\nand HMM parameters, and one (10%) as a final test set. We implement the HMMs with HTK [1] and language models with SRILM [80]. We train smoothed backoff bigram letter language models using lexicons of various sizes, consisting of the most frequent words in the ARPA CSR-III text, which includes English words and names [34].\nThe tuning parameters, and their most frequently chosen values, are the SIFT pyramid depth (1+2), PCA dimensionality (80), window size (13), NN output function (linear), whether or not image features are appended to NN outputs (yes), number of states per letter HMM (3), number of silence states (9), number of Gaussians per state (8), and language model lexicon size (5k-word). We tune independently in each fold, i.e., we perform ten independent experiments, and report the average test performance over the folds. Recognition performance is measured via the letter error rate, the Levenshtein distance between the hypothesized letter sequence and the reference sequence as a percentage of the reference sequence length.\nResults Figure 3-2 gives the frame error rates of the NNs, measured with respect to frame labels produced from the manual apogee labels as described above. All of the classifiers perform much better than chance.\nFigure 3-3 shows letter sequence recognition results: a comparison of the two tandem models and the baseline HMM-based system, using different types of training labels. For\n34\nthe baseline system, there are two choices for training: either (1) the manual apogee labels are used to generate a segmentation into letters, and each letter HMM is trained only on the corresponding segments; or (2) the manual labels are ignored and the HMMs are trained without any segmentation, using Expectation-Maximization on sequences corresponding to entire words. For the tandem systems, we consider three choices: (1) the manual labels are used to generate a segmentation, and both the NNs and HMMs are trained using this labeled segmentation; (2) the segmentation is used for NN training, but HMMs are trained without it; or (3) the segmentation is not used at all, and the labels for NN training are derived via forced alignment using the baseline (segmentation-free) HMM. The reason for this comparison is to determine to what extent the (rather time-intensive) manual labeling is helpful.\nThe tandem systems consistently improve over the corresponding baselines, with the phonological feature-based tandem models outperforming the letter-based tandem models. Using the manual labels in training makes a large difference to all of the recognizers\u2019 error rates, with the forced alignment-based labels producing poorer performance (iterating the forced alignment procedure does not help). However, in all cases the tandem-based models improve over the baselines and the feature-based models improve over the letter-based tandem models."}, {"heading": "3.3 DNN frame classification performance", "text": "Since all of our fingerspelling recognition models use DNN frame classifiers as a building block, we first examine the performance of the frame classifiers4.\nThe DNNs are trained for seven tasks (letter classification and classification of each of the six phonological features). For training DNNs, we split training data, which is used for training recognizer, into training data and validation data for DNN, consisting of 90% and 10% of the original training data,respectively. The input is the 128-dimensional HOG features concatenated over a 21-frame window. The DNNs have three hidden layers, each with 3000 ReLUs [101]. Network learning is done with cross-entropy training with a weight decay penalty of 10\u22125, via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [78] at a rate of 0.5 at each hidden layer, fixed momentum of 0.95, and initial learning rate of 0.01, which is halved when held-out error rate stops improving. We pick the best-performing epoch on held-out data. The network structure and hyperparameters were tuned on held-out (signer-independent) data in initial experiments.\nWe consider the signer-dependent setting (where the DNN is trained on data from the test signer), signer-independent setting (where the DNN is trained on data from all except the test signer), and signer-adapted setting (where the signer-independent DNNs are adapted using adaptation data from the test signer). For LIN+UP and LIN+LON in adapting DNNs (Section 2.2), we adapt by running SGD over minibatches of 100 samples with a fixed momentum of 0.9 for up to 20 epochs, with initial learning rate of 0.02 (which is halved when error rate stops improving on the adaptation data). For fine-tuning, we use\n4This section includes material previously published in [47].\n35\nthe same SGD procedure as for the signer-independent DNNs. We pick the epoch with the lowest error rate on the adaptation data.\nThe frame error rates for all settings are given in Fig. 3-4. For the signer-adapted case, we consider DNN adaptation with different types and amounts of supervision. The types of supervision include fully labeled adaptation data (\u201cGT\u201d, for \u201cground truth\u201d, in the figure), where the peak locations for all letters are manually annotated; as well as adaptation data labeled only with the letter sequence but not the timing information. In the latter case, we use the baseline tandem system to generate forced alignments (\u201cFA\u201d in the figure). We consider amounts of adaptation data from 5% to 20% of the test signer\u2019s full data.\nThese results show that among the adaptation methods, LIN+UP slightly outperforms LIN+LON, and fine-tuning outperforms both LIN+UP and LIN+LON. Without adaptation, we also consider speed normalization and it provides consistent but very small improvements. For letter sequence recognition experiments in the next section, we adapt via finetuning using 20% of the test signer\u2019s data.\n36\nIn Fig. 3-5, we further analyze the DNNs via confusion matrices. One of the main effects is the large number of incorrect predictions of the non-signing classes (<s>, </s>). We observe the same effect with the phonological feature classifiers. This may be due to the previously mentioned fact that non-linguistic gestures are variable and easy to confuse with signing when given a new signer\u2019s image frames. The confusion matrices show that, as the DNNs are adapted, this is the main type of error that is corrected."}, {"heading": "3.4 Letter recognition experiments", "text": ""}, {"heading": "3.4.1 Signer-dependent recognition", "text": "Our first continuous letter recognition experiments are signer-dependent; that is, we train and test on the same signer, for each of four signers. For each signer, we use a 10-fold setup: In each fold, 80% of the data is used as a training set, 10% as a development set for tuning parameters, and the remaining 10% as a final test set. We independently tune the parameters in each fold, but to make it comparable to adaptation experiments later, we use 8 out of 10 folds to compute the final test results and report the average letter error rate (LER) over those 8 folds. For language models, we train letter bigram language models from large online dictionaries of varying sizes that include both English words and names [2]. We use HTK [1] to implement the tandem HMM-based recognizers and SRILM [80] to train the language models. The HMM parameters (number of Gaussians per state, size of language model vocabulary, transition penalty and language model weight), as well as the dimensionality of the HOG descriptor input and HOG depth, were tuned to minimize development set letter error rates for the tandem HMM system. The front-end and language model hyperparameters were kept fixed for the SCRFs (in this sense the SCRFs are slightly disadvantaged). Additional parameters tuned for the SCRF rescoring models included the\n38\nN-best list sizes, type of feature functions, choice of language models, and L1 and L2 regularization parameters. Finally, for the first-pass SCRF, we tuned step size, maximum length of segmentations, and number of training epochs.\nTable 3.1 (last row) shows the signer-dependent letter recognition results. SCRF rescoring improves over the tandem HMM, and the first-pass SCRF outperforms the others.\nNote that in our experimental setup, there is some overlap of word types between training and test data. This is a realistic setup, since in real applications some of the test words will have been previously seen and some will be new. However, for comparison, we have also conducted the same experiments while keeping the training, development, and test vocabularies disjoint; in this modified setup, letter error rates increase by about 2-3% overall, but the SCRFs still outperform the tandem HMM model."}, {"heading": "3.4.2 Signer-independent recognition", "text": "In the signer-independent setting, we would like to recognize fingerspelled letter sequences from a new signer, given a model trained only on data from other signers. For each of the four test signers, we train models on the remaining three signers, and report the performance for each test signer and averaged over the four test signers. For direct comparison with the signer-dependent experiments, each test signer\u2019s performance is itself an average over the 8 test folds for that signer.\nAs shown in the first line of Table 3.1, the signer-independent performance of the three types of recognizers is quite poor, with the rescoring SCRF somewhat outperforming the tandem HMM and first-pass SCRF. The poor performance is perhaps to be expected with such a small number of training signers."}, {"heading": "3.4.3 Signer-adapted recognition", "text": "The remainder of Table 3.1 (second and third rows) gives the connected letter recognition performance obtained with the three types of models using DNNs adapted via fine-tuning, using different types of adaptation data (ground-truth, GT, vs. forced-aligned, FA). For all models, we do not retrain the models with the adapted DNNs, but tune hyperparameters of the recognizer on 10% of the test signer\u2019s data. The tuned models are evaluated on an unseen 10% of the test signer\u2019s remaining data; finally, we repeat this for eight choices of tuning and test sets, covering the 80% of the test signer\u2019s data that we do not use for adaptation, and report the mean letter error rate over the test sets.\nAs shown in Table 3.1, adaptation allowed the performance jump to up to 30.3% letter error rate with forced-alignment adaptation labels and up to 17.3% error rate with groundtruth adaptation labels. All of the adapted models improve similarly, but interestingly, the\n39\nfirst-pass SCRF is slightly worse than the others before adaptation and better (by 4.4% absolute) after ground-truth adaptation. One hypothesis is that the first-pass SCRF is more dependent on the DNN performance, while the tandem model uses the original image features and the rescoring SCRF uses the tandem model hypotheses and scores. Once the DNNs are adapted, however, the first-pass SCRF outperforms the other models. Figure A-1,A-2,3-8,A-3 illustrate the recognition task with several models.\nAdditionally, since DNN-HMM hybrid models are recently popular for speech recognition task (e.g., [17]), we also conduct an initial experiment with them. We use Kaldi toolkit [69] for implementation. But their performance for our recognition task was quite poor, for both signer-dependent and signer-independent experiments, so they were not pursued in depth for the reported experiments. We conjecture that such models may need more data to perform well, which is not the case for our task."}, {"heading": "3.5 Extensions and analysis", "text": "We next analyze our results and consider potential extensions for improving the models.\n3.5.1 Analysis: Could we do better by training entirely on adaptation data?\nIn this section we consider alternatives to the adaptation setting we have chosen \u2013 adapting the DNNs while using sequence models (HMMs/SCRFs) trained only on signer-independent data. We fix the model to a first-pass SCRF and the adaptation data to 20% of the test signer\u2019s data annotated with ground-truth peak labels. In this setting, we consider two alternative ways of using the adaptation data: (1) using the adaptation data from the test signer to train both the DNNs and sequence model from scratch, ignoring the signer-independent training set; and (2) training the DNNs from scratch on the adaptation data, but using the SCRF trained on the training signers. We compare these options with our best results using the signer-independent SCRF and DNNs fine-tuned on the adaptation data. The results are shown in Table 3.2. Frame error rates of DNNs trained from scratch using 20% of the test signer\u2019s data is given in Figure 3-4.\nWe find that ignoring the signer-independent training set and training both DNNs and SCRFs from scratch on the test signer (option (1) above) works remarkably well, better than the signer-independent models and even better than adaptation via forced alignment (see Table 3.1). However, training the SCRF on the training signers but DNNs from scratch on the adaptation data (option (2) above) improves performance further. However, neither of these outperforms our previous best approach of signer-independent SCRFs plus DNNs fine-tuned on the adaptation data. Figure 3-4 shows that DNNs trained from scratch is slightly worse than fine-tuned DNNs, but letter recognition has some gap in Table 3.2. It may be due to fact that small improvement of DNNs can effect letter recognition performance more.\n40"}, {"heading": "3.5.2 Analysis: Letter vs. feature DNNs", "text": "We next compare the letter DNN classifiers and the phonological feature DNN classifiers in the context of the first-pass SCRF recognizers. We also consider an alternative subletter feature set, in particular a set of phonetic features introduced by Keane [42], whose feature values are listed in Table 3.3. We use the first-pass SCRF with either only letter classifiers, only phonetic feature classifiers, letter + phonological feature classifiers, and letter + phonetic feature classifiers. We do not consider the case of phonological features alone, because they are not discriminative for some letters. Figure 3-6 shows the letter recognition results for the signer-dependent and signer-adapted settings.\nWe find that using letter classifiers alone outperforms the other options in the speakerdependent setting, achieving 7.6% letter error rate. For signer-adapted recognition, phonological or phonetic features are helpful in addition to letters for two of the signers (signers 2 and 4) but not for the other two (signers 1,3); on average, using letter classifiers alone is best in both cases, achieving 16.6% error rate on average. In contrast, in an initial experiment in Section 3.2, we found that phonological features outperform letters in the tandem HMM. However, those experiments used a tandem model with neural networks with a single hidden layer; we conjecture that with more layers, we are able to do a better job at the more complicated task of letter classification."}, {"heading": "3.5.3 Analysis: DNNs vs. CNNs", "text": "We also conduct experiments with convolutional neural networks (CNNs) as frame classifiers. In all experiments so far, we have used HoG features as our image descriptor. However, using CNNs with raw image pixels without any hand-crafted image descriptor has recently become popular and has shown improved performance for image recognition tasks (e.g., [48, 77]). Thus, to see the effectiveness of CNNs for our frame classifiers, we compare the feature classification performance of letter and phonological features between DNNs and CNNs. We use a signder-dependent setting and the same 8-fold setup. Inputs to CNNs are grayscale 64\u00d7 64\u00d7 \ud835\udc47 pixels. \ud835\udc47 is the number of input frames used in the input window, as in our DNNs. For the structure of CNNs, we use 32 kernels of 3\u00d7 3 filters with stride 1 for the first and second convolutional layer. Then we add a max pooling layer with 2 \u00d7 2 pixel window, with stride 2. For the third and fourth convolutional layer, we use 64 kernels of 3 \u00d7 3 filters with stride 1. Again, we add a max pooling layer with 2 \u00d7 2 pixel window, with stride 2. For all convolutional layers, ReLUs are used for the nonlinearity. Finally, two fully-connected layers with 2000 ReLUs are added, followed by a softmax output layer. We use dropout to prevent overfitting, 0.25 probability for all convolutional layers and 0.5 for fully-connected layers. Training is done by stochastic gradient descent with learning rate 0.01, learning rate decay 1e-6, and momentum 0.9. Mini-batch size is\n41\n100. \ud835\udc47 was tuned and set to 21. For CNN implementation, we use Keras [16] with Theano [87]. Figure 3-7 shows the results for signer-dependent setting. We find that the performances of the DNN and CNN are comparable. Specifically, for letter classification, DNNs are slightly better, and for phonological feature classification, CNNs are slightly better, but the gaps are very small. Our best letter recognizer only depends on the outputs of feature classifiers, so we may assume that the performance of letter recognition using CNNs would be comparable to that using DNNs."}, {"heading": "3.5.4 Improving performance in the force-aligned adaptation case", "text": "We next attempt to improve the performance of adaptation in the absence of ground-truth (manually annotated) peak labels. Using only the letter label sequence for the adaptation data, we use the signer-independent tandem recognizer to get force-aligned frame labels. We then adapt (fine-tune) the DNNs using the force-aligned adaptation data (as before in the FA case). We then re-align the test signer\u2019s adaptation data with the adapted recognizer. Finally, we adapt the DNNs again with the re-aligned data. Throughout this experiment, we do not change the recognizer but only update the DNNs. We use first-pass SCRF with\n42\nletter classifiers for these experiments. Using this iterative realignment approach, we are able to further improve the recognition error rate in the FA case by about 1.3%, as shown in Table 3.4."}, {"heading": "3.5.5 Improving performance with segmental cascades", "text": "Finally, we consider whether we can improve upon the performance of our best models, the first-pass SCRFs, by rescoring their results in a second pass with more powerful features. We follow the discriminative segmental cascades (DSC) approach of [84], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.\nFor these experiments we start with the most successful first-pass SCRF in the above experiments, which uses letter DNNs and is adapted with 20% of the test signer\u2019s data with ground-truth peak labels. For the second-pass SCRF, we use the first-pass score as a feature, and add to it two more complex features: a segmental DNN, which takes as input an entire hypothesized segment and produces posterior probabilities for all of the letter classes; and the \u201cpeak detection\u201d feature described in Section 2. For training segmental DNNs, we use ground truth segmentation from peak annotations as training data. Because they have variable lengths, we use the means of each a third of the segment as div\ud835\udc60 in Section 2.1.2. We use the same structure and learning strategy as the DNN frame classifiers. We use the same bigram language model as Tandem HMM and rescoring SCRF models.\nAs shown in Table 3.5, for signer-independent setting, it slightly improves the average letter error rate over four signers, from 16.6% in the first pass to 16.2% in the second pass. This improvement, while small, is statistically significant at the p=0.05 level. For the statistical significance test, we use MAPSSWE test [66]. These results combine the most successful ideas from our experiments and form our final best result for signer-independent experiment. But for signer-dependent setting, this approach achieves the comparable performance and does not improve the recognition in the second pass."}, {"heading": "3.5.6 Analysis: decomposition of errors", "text": "We also decompose the letter error rate for further analysis. Here we consider signeradaptation experiments. We compute the number of substitution errors (S), deletion errors\n43\n(D) and insertion errors (I) to match hypothesis produced by models to ground truth letter sequence.\nThe results are reported in Table 3.6, which shows that first-pass SCRF model mostly improves deletion and substitution errors over other models. Figure 3-8 show a such example; tandem HMM have many deletions and rescoring SCRF has a substitution. Those are corrected with first-pass SCRF model. More details with additional analysis for signerdependent and signer-independent settings are in Appendix C.\n44\n45\n46\nChapter 4"}, {"heading": "Conclusion", "text": "This thesis tackles the problem of unconstrained fingerspelled letter sequence recognition in ASL, where the letter sequences are not restricted to any closed vocabulary. This problem is challenging due to both the small amount of available training data and the significant variation between signers. Our recognition experiments have compared HMM-based and segmental models with features based on DNN classifiers, and have investigated a range of settings including signer-dependent, signer-independent, and signer-adapted. Our main findings are:\n\u2219 Signer-independent fingerspelling recognition, where the test signer is unseen in training, is quite challenging, at least with the small data sets available to date, with letter error rates around 60%. On the other hand, even with a small amount of training data, signer-dependent recognition is quite successful, reaching letter error rates below 10%; and adaptation allows us to bridge a large part of the gap between signerindependent and signer-dependent performance.\n\u2219 Our best results are using two-pass discriminative segmental cascades with features based on frame-level DNN letter classifiers. This approach achieves an average letter error rate of 7.6% in the signer-dependent setting and 16.2% LER in the signeradapted setting. The adapted models use signer-independent SCRFs and DNNs adapted by fine-tuning on the adaptation data. The adapted results are obtained using 115 words of adaptation data with manual annotations of letter peaks.\n\u2219 If less adaptation data is available, we can still get improvements from adaptation down to about 30 words of adaptation data.\n\u2219 In the absence of manual peak annotations for the adaptation data, we can automatically align the adaptation data and still get a significant boost in performance over the signer-independent case, and we can iteratively improve the performance by re-aligning the data with the adapted models. Our best adapted models using automatically aligned adaptation data achieve 27.3% letter error rate.\n\u2219 The main types of errors that are addressed by adapting the DNN classifiers are confusions between signing and non-signing segments.\n47\nThe future directions we are interested in are as follows:\n\u2219 We would like to investigate new sequence recognition approaches. Specifically, we have used separate feature classifiers and recognizers for our models. It would be interesting to combine both and train the recognizer \u201cend-to-end\u201d, as in, e.g., [35] for speech recognition. For feature classifier, using recurrent neural networks, such as long-short term memory networks, would also be interesting.\n\u2219 We are also interested in new adaptation methods, especially for the case where no manual annotations are available for the adaptation data.\n\u2219 We still have relatively small amount of training data and would like to expand our data collection to a larger number of signers and to more realistic data collected \u201cin the wild\u201d, such as online videos from Deaf social media. Also, acquiring manual annotations is expensive, and we would like to reduce the dependence on manually aligned data. As larger data sets are collected, they might alleviate the need for manual alignments, or it may still be necessary to develop better ways of taking advantage of weakly labeled data.\n\u2219 In real data, fignerspelling is usually embedded within running ASL. Future work will consider jointly detecting and recognizing fingerspelling sequences.\n48\nBibliography\n[1] http://htk.eng.cam.ac.uk.\n[2] http://www.keithv.com/software/csr.\n[3] O. Abdel-Hamid and H. Jiang. Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code. In Proc. ICASSP, 2013.\n[4] Alex Acero, Li Deng, Trausti T Kristjansson, and Jerry Zhang. Hmm adaptation using vector taylor series for noisy speech recognition. In INTERSPEECH, pages 869\u2013872, 2000.\n[5] V. Athitsos, J. Alon, S. Sclaroff, and G. Kollios. BoostMap: A method for efficient approximate similarity rankings. In CVPR, 2004.\n[6] V. Athitsos, C. Neidle, S. Sclaroff, J. Nash, A. Stefan, A. Thangali, H. Wang, and Quan Yuan. Large lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms. In Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT), 2010.\n[7] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151\u2013175, 2010.\n[8] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128. Association for Computational Linguistics, 2006.\n[9] R. Bowden, D. Windridge, T. Kadir, A. Zisserman, and M. Brady. A linguistic feature vector for the visual interpretation of sign language. In ECCV, 2004.\n[10] D. Brentari and C. Padden. Native and foreign vocabulary in American Sign Language: A lexicon with multiple origins. In Foreign vocabulary in sign languages: A cross-linguistic investigation of word formation, pages 87\u2013119. Lawrence Erlbaum, Mahwah, NJ, 2001.\n[11] Diane Brentari. A Prosodic Model of Sign Language Phonology. MIT Press, 1998.\n49\n[12] P. Buehler, M. Everingham, D. P. Huttenlocher, and A. Zisserman. Upper body detection and tracking in extended signing sequences. International Journal of Computer Vision, 95(2):180\u2013197, 2011.\n[13] Ozgur \u00c7etin, Arthur Kantor, Simon King, Chris Bartels, Mathew Magimai-doss, Joe Frankel, and Karen Livescu. An articulatory feature-based tandem approach and factored observation modeling. In ICASSP, 2007.\n[14] D. A. Cheek. The Phonetics and Phonology of Handshape in American Sign Language. PhD thesis, University of Texas at Austin, 2001.\n[15] Seong-Sik Cho, Hee-Deok Yang, and Seong-Whan Lee. Sign language spotting based on semi-Markov conditional random field. In Workshop on the Applications of Computer Vision, 2009.\n[16] Fran\u00c3g\u0306ois Chollet. keras. https://github.com/fchollet/keras, 2015.\n[17] George E Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30\u201342, 2012.\n[18] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proc. CVPR, 2005.\n[19] Hal Daum\u00e9 III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.\n[20] L. Ding and A. M. Mart\u00ednez. Modelling and recognition of the linguistic components in American Sign Language. Image Vision Comput., 27(12), 2009.\n[21] R. Doddipatla, M. Hasan, and T. Hain. Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition. In Proc. Interspeech, 2014.\n[22] Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, and Trevor Darrell. Semisupervised domain adaptation with instance constraints. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.\n[23] P. Dreuw, J. Forster, Y. Gweth, D. Stein, H. Ney, G. Martinez, J. Verges Llahi, O. Crasborn, E. Ormel, W. Du, T. Hoyoux, J. Piater, J. M. Moya Lazaro, and M. Wheatley. SignSpeak - understanding, recognition, and translation of sign languages. In Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT), 2010.\n[24] P. Dreuw, D. Rybach, T. Deselaers, M. Zahedi, , and H. Ney. Speech recognition techniques for a sign language recognition system. In Interspeech, 2007.\n[25] Philippe Dreuw, David Rybach, Thomas Deselaers, Morteza Zahedi, and Hermann Ney. Speech recognition techniques for a sign language recognition system. In Interspeech, 2007.\n50\n[26] Thi V Duong, Hung H Bui, Dinh Q Phung, and Svetha Venkatesh. Activity recognition and abnormality detection with the switching hidden semi-Markov model. In CVPR, 2005.\n[27] Daniel P.W. Ellis, Rita Singh, and Sunil Sivadas. Tandem acoustic modeling in large-vocabulary recognition. In ICASSP, 2001.\n[28] A. Farhadi, D. Forsyth, and R. White. Transfer learning in sign language. In CVPR, 2007.\n[29] R. S. Feris, M. Turk, R. Raskar, K. Tan, and G. Ohashi. Exploiting depth discontinuities for vision-based fingerspelling recognition. In IEEE Workshop on Real-Time Vision for Human-Computer Interaction, 2004.\n[30] J. Forster, O. Koller, C. Oberd\u00f6rfer, Y. Gweth, and H. Ney. Improving continuous sign language recognition: Speech recognition techniques and system design. In Proc. SLPAT, 2013.\n[31] Mark JF Gales. Maximum likelihood linear transformations for hmm-based speech recognition. Computer speech & language, 12(2):75\u201398, 1998.\n[32] P. Goh and E.-J. Holden. Dynamic fingerspelling recognition using geometric and motion features. In ICIP, 2006.\n[33] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition: An unsupervised approach. In 2011 international conference on computer vision, pages 999\u20131006. IEEE, 2011.\n[34] D. Graff, R. Rosenfeld, and D. Paul. CSR-III text. http://http://www.ldc. upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6, 1995.\n[35] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In ICML, volume 14, pages 1764\u20131772, 2014.\n[36] K. Grobel and M. Assan. Isolated sign language recognition using hidden Markov models. In International Conference on System Man and Cybernetics, 1997.\n[37] K. Grobel and H. Hienz. Video-based recognition of fingerspelling in real-time. In Workshops Bildverarbeitung f\u00fcr die Medizin, 1996.\n[38] Yanzhang He and Eric Fosler-Lussier. Efficient segmental conditional random fields for phone recognition. In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech\u00e2A\u0306Z\u030112), Portland, OR, USA, pages 1898\u20131901, 2012.\n[39] T. Humphries and C. Padden. Learning American Sign Language (Levels I & II). Pearson Education, New York, second edition, 2004.\n51\n[40] Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In ACL, volume 7, pages 264\u2013271, 2007.\n[41] M. W. Kadous. Machine recognition of Auslan signs using powergloves: towards large lexicon integration of sign language. In Workshop on the Integration of Gesture in Language and Speech, 1996.\n[42] Jonathan Keane. Towards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language. PhD thesis, University of Chicago, December 2014. Doctoral dissertation, defended 22 August 2014 Advisors: Diane Brentari, Jason Riggle, and Karen Livescu.\n[43] Jonathan Keane, Diane Brentari, and Jason Riggle. Coarticulation in ASL fingerspelling. In NELS, 2012.\n[44] Cem Keskin, Furkan K\u0131ra\u00e7, Yunus Emre Kara, and Lale Akarun. Hand pose estimation and hand shape classification using multi-layered randomized decision forests. In Computer Vision\u2013ECCV 2012, pages 852\u2013863. Springer, 2012.\n[45] T. Kim, K. Livescu, and G. Shakhnarovich. American Sign Language fingerspelling recognition with phonological feature-based tandem models. In IEEE Workshop on Spoken Language Technology, 2012.\n[46] Taehwan Kim, Gregory Shakhnarovich, and Karen Livescu. Fingerspelling recognition with semi-markov conditional random fields. In IEEE International Conference on Computer Vision, 2013.\n[47] Taehwan Kim, Weiran Wang, Hao Tang, and Karen Livescu. Signer-independent fingerspelling recognition with deep neural network adaptation. In Proc. ICASSP, 2016.\n[48] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[49] C-H Lee and J-L Gauvain. Speaker adaptation based on map estimation of hmm parameters. In Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference on, volume 2, pages 558\u2013561. IEEE, 1993.\n[50] Christopher J Leggetter and Philip C Woodland. Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech & Language, 9(2):171\u2013185, 1995.\n[51] B. Li and K. C. Sim. Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems. In Proc. Interspeech, 2010.\n52\n[52] Shao-Zi Li, Bin Yu, Wei Wu, Song-Zhi Su, and Rong-Rong Ji. Feature learning based on sae\u2013pca network for human gesture recognition in rgbd images. Neurocomputing, 151:565\u2013573, 2015.\n[53] H. Liao. Speaker adaptation of context dependent deep neural networks. In Proc. ICASSP, 2013.\n[54] Karen Livescu, Omer Cetin, Mark Hasegawa-Johnson, Simon King, Christopher Bartels, Nash Borges, Amir Kantor, Pyare Lal, Lisa Yung, Ari Bezman, et al. Articulatory feature-based methods for acoustic and audio-visual speech recognition: Summary from the 2006 jhu summer workshop. In Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, volume 4, pages IV\u2013621. IEEE, 2007.\n[55] S. Liwicki and M. Everingham. Automatic recognition of fingerspelled words in British Sign Language. In 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis, 2009.\n[56] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91\u2013110, 2004.\n[57] P. Lu and M. Huenerfauth. Collecting a motion-capture corpus of American Sign Language for data-driven generation research. In NAACL-HLT Workshop on Speech and Language Processing for Assistive Technologies, 2010.\n[58] Pengfei Lu and Matt Huenerfauth. Cuny american sign language motion-capture corpus: first release. In Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, The 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, 2012.\n[59] A. M. Martinez, R. B. Wilbur, R. Shay, and A. C. Kak. Purdue ASL database for the recognition of American sign language. In Proceedings of IEEE International Conference on Multimodal Interfaces (ICMI), 2002.\n[60] R. E. Mitchell, T. A. Young, B. Bachleda, and M. A. Karchmer. How many people use ASL in the united states? Why estimates need updating. Sign Language Studies, 6(3), 2006.\n[61] S. Nayak, S. Sarkar, and B. Loeding. Automated extraction of signs from continuous sign language sentences using iterated conditional modes. In CVPR, 2009.\n[62] J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, and T. Robinson. Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system. In Proc. Eurospeech, 1995.\n[63] I. Oikonomidis, N. Kyriazis, and A. A. Argyros. Efficient model-based 3D tracking of hand articulations using Kinect. In BMVC, 2011.\n53\n[64] C. Oz and M. C. Leu. Recognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove. In 2nd international conference on Advances in Neural Networks, 2005.\n[65] Carol Padden and Darline Clark Gunsauls. How the alphabet came to be used in a sign language. Sign Language Studies, page 4:10\u00ef\u00a3\u00a133, 2004.\n[66] D. S. Pallet, W. M. Fisher, and J. G. Fiscus. Tools for the analysis of benchmark speech recognition tests. In ICASSP, 1990.\n[67] T. Pfister, J. Charles, M. Everingham, and A. Zisserman. Automatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts. In BMVC, 2012.\n[68] V. Pitsikalis, S. Theodorakis, C. Vogler, and P. Maragos. Advances in phoneticsbased sub-unit modeling for transcription alignment and sign language recognition. In IEEE CVPR Workshop on Gesture Recognition, 2011.\n[69] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. The kaldi speech recognition toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, December 2011. IEEE Catalog No.: CFP11SRW-USB.\n[70] Nicolas Pugeault and Richard Bowden. Spelling it out: Real-time asl fingerspelling recognition. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 1114\u20131119. IEEE, 2011.\n[71] http://www.icsi.berkeley.edu/Speech/qn.html.\n[72] S. Ricco and C. Tomasi. Fingerspelling recognition through classification of letterto-letter transitions. In ACCV, 2009.\n[73] A. Roussos, S. Theodorakis, V. Pitsikalis, and P. Maragos. Affine-invariant modeling of shape-appearance images applied on sign language handshape classification. In ICIP, 2010.\n[74] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213\u2013 226. Springer, 2010.\n[75] Sunita Sarawagi and William W. Cohen. Semi-Markov conditional random fields for information extraction. In NIPS, 2004.\n[76] Qinfeng Shi, Li Cheng, Li Wang, and Alex Smola. Human action segmentation and recognition using discriminative semi-Markov models. International Journal of Computer Vision, 93(1), 2011.\n54\n[77] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[78] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.\n[79] T. Starner, J. Weaver, and A. Pentland. Real-time American Sign Language recognition using desk and wearable computer based video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12), 1998.\n[80] A. Stolcke, J. Zheng, W. Wang, and V. Abrash. SRILM at sixteen: update and outlook. In ASRU, 2011.\n[81] Sebastian St\u00fcker, Florian Metze, Tanja Schultz, and Alex Waibel. Integrating multilingual articulatory features into speech recognition. In INTERSPEECH, 2003.\n[82] P. Swietojanski and S. Renals. Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models. In Proc. SLT, 2014.\n[83] Danhang Tang, Tsz-Ho Yu, and Tae-Kyun Kim. Real-time articulated hand pose estimation using semi-supervised transductive regression forests. In Proceedings of the IEEE International Conference on Computer Vision, pages 3224\u20133231, 2013.\n[84] H. Tang, W. Wang, K. Gimpel, and K. Livescu. Discriminative segmental cascades for feature-rich phone recognition. In Proc. ASRU, 2015.\n[85] Hao Tang, Kevin Gimpel, and Karen Livescu. A comparison of training approaches for discriminative segmental models. In INTERSPEECH, pages 1219\u20131223, 2014.\n[86] A. Thangali, J. P. Nash, S. Sclaroff, and C. Neidle. Exploiting phonological constraints for handshape inference in ASL video. In CVPR, 2011.\n[87] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016.\n[88] S. Theodorakis, V. Pitsikalis, and P. Maragos. Model-level data-driven sub-units for signs in videos of continuous sign language. In ICASSP, 2010.\n[89] M. E Tyrone, J. Kegl, and H. Poizner. Interarticulator co-ordination in deaf signers with Parkinson\u2019s disease. Neuropsychologia, 37(11), 1999.\n[90] Paul Viola and Michael J. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001.\n[91] C. Vogler and D. Metaxas. Parallel hidden Markov models for American Sign Language recognition. In ICCV, 1999.\n[92] C. Vogler and D. Metaxas. Toward scalability in ASL recognition: Breaking down signs into phonemes. In Gesture Workshop, 1999.\n55\n[93] C. Vogler and D. Metaxas. A framework for recognizing the simultaneous aspects of American Sign Language. Computer Vision and Image Understanding, 81:358\u2013384, 2001.\n[94] C. Vogler and D. Metaxas. Handshapes and movements: Multiple-channel ASL recognition. In Gesture Workshop, 2003.\n[95] R. Wang and J. Popovic. Real-time hand-tracking with a color glove. In SIGGRAPH, 2009.\n[96] S. Wilcox. The Phonetics of Fingerspelling. John Benjamins Publishing Company, 1992.\n[97] R. Yang and S. Sarkar. Detecting coarticulation in sign language using conditional random fields. In ICPR, 2006.\n[98] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, and Y. Gong. Adaptation of contextdependent deep neural networks for automatic speech recognition. In Proc. SLT, 2012.\n[99] M. Zahedi, P. Dreuw, D. Rybach, T. Deselaers, and H. Ney. Geometric features for improving continuous appearance-based sign language recognition. In BMVC, 2006.\n[100] M.M. Zaki and S.I. Shaheen. Sign language recognition using a combination of new vision based features. Pattern Recognition Letters, 32(4), 2011.\n[101] M. D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and G. E. Hinton. On rectified linear units for speech processing. In Proc. ICASSP, 2013.\n[102] Chenyang Zhang and Yingli Tian. Histogram of 3d facets: a depth descriptor for human action and hand gesture recognition. Computer Vision and Image Understanding, 139:29\u201339, 2015.\n[103] G. Zweig and P. Nguyen. Segmental CRF approach to large vocabulary continuous speech recognition. In ASRU, 2009.\n[104] Geoffrey Zweig. Classification and recognition with direct segment models. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4161\u20134164. IEEE, 2012.\n[105] Geoffrey Zweig, Patrick Nguyen, Dirk Van Compernolle, Kris Demuynck, Les Atlas, Pascal Clark, Gregory Sell, Meihong Wang, Fei Sha, Hynek Hermansky, et al. Speech recognitionwith segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5044\u20135047. Ieee, 2011.\n56"}, {"heading": "Appendix A", "text": ""}, {"heading": "Examples of hypotheses produced by", "text": "models\nFigures A-1, A-2, 3-8 and A-3 illustrate the recognition task and hypothesis produced by models. In each of these figures we show the ground truth segments. The peak frames are shown on top of each letter\u2019s segment; the hand region segmentation masks were obtained automatically using the probabilistic model described in Section 3.1. We also show intermediate frames, obtained at midpoints between peaks, as well as frames before the first peak and after the last peak. Below the ground truth segmentations are the segmentations obtained with the tandem HMM, rescoring SCRF, and at the bottom are segmentations obtained with the first-pass SCRF.\n57\n58"}, {"heading": "Appendix B", "text": "Example images of phonological feature values\n59\n60"}, {"heading": "Appendix C", "text": ""}, {"heading": "Analysis: decomposition of errors", "text": "For further analysis of recognition approaches, we decompose the letter error rate. After having recognized hypothesis from models, optimal string match using dynamic programming match each of the recognized and reference label sequences. With the optimal alignment, the number of substitution errors (S), deletion errors (D) and insertion errors (I) to match from reference label sequence to recognized hypothesis can be computed. Please note that letter error rate is computed as:\nLetter Error Rate = \ud835\udc37 + \ud835\udc46 + \ud835\udc3c\n\ud835\udc41 * 100%\nand we use implementation of HTK [1] for this analysis. The results are reported in Table C.3 with signer-dependent experiment, Table C.1 with signer-independent, and Table C.2 with adaptation. With signer-independent setting (Table C.1), 1st-pass model seems to produce more deletions, and less substitution and insertion. Note that 1st-pass model has worse letter error rates than other models (Table 3.1). But, with adaptation and more accurate DNN frame classifiers (Table C.2) , first-pass SCRF model improves deletion and substitution errors over other models. Also, for signer-dependent recognition, Table C.3 shows that rescoring SCRF improves over tandem HMM, and first-pass SCRF improves over other models, mainly for deletion error.\n61\n62"}, {"heading": "Appendix D", "text": "Word lists recorded and used in experiments\n63\n64\n65"}], "references": [{"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. ICASSP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Hmm adaptation using vector taylor series for noisy speech recognition", "author": ["Alex Acero", "Li Deng", "Trausti T Kristjansson", "Jerry Zhang"], "venue": "In INTERSPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "BoostMap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "and Quan Yuan", "author": ["V. Athitsos", "C. Neidle", "S. Sclaroff", "J. Nash", "A. Stefan", "A. Thangali", "H. Wang"], "venue": "Large lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms. In Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan"], "venue": "Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of the 2006 conference on empirical methods in natural language processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A linguistic feature vector for the visual interpretation of sign language", "author": ["R. Bowden", "D. Windridge", "T. Kadir", "A. Zisserman", "M. Brady"], "venue": "ECCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Native and foreign vocabulary in American Sign Language: A lexicon with multiple origins", "author": ["D. Brentari", "C. Padden"], "venue": "Foreign vocabulary in sign languages: A cross-linguistic investigation of word formation, pages 87\u2013119. Lawrence Erlbaum, Mahwah, NJ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "A Prosodic Model of Sign Language Phonology", "author": ["Diane Brentari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Upper body detection and tracking in extended signing sequences", "author": ["P. Buehler", "M. Everingham", "D.P. Huttenlocher", "A. Zisserman"], "venue": "International Journal of Computer Vision, 95(2):180\u2013197", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "An articulatory feature-based tandem approach and factored observation modeling", "author": ["Ozgur \u00c7etin", "Arthur Kantor", "Simon King", "Chris Bartels", "Mathew Magimai-doss", "Joe Frankel", "Karen Livescu"], "venue": "In ICASSP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The Phonetics and Phonology of Handshape in American Sign Language", "author": ["D.A. Cheek"], "venue": "PhD thesis, University of Texas at Austin", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Sign language spotting based on semi-Markov conditional random field", "author": ["Seong-Sik Cho", "Hee-Deok Yang", "Seong-Whan Lee"], "venue": "In Workshop on the Applications of Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III"], "venue": "arXiv preprint arXiv:0907.1815,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Modelling and recognition of the linguistic components in American Sign Language", "author": ["L. Ding", "A.M. Mart\u00ednez"], "venue": "Image Vision Comput., 27(12)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "Proc. Interspeech", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Semisupervised domain adaptation with instance constraints", "author": ["Jeff Donahue", "Judy Hoffman", "Erik Rodner", "Kate Saenko", "Trevor Darrell"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "SignSpeak - understanding", "author": ["P. Dreuw", "J. Forster", "Y. Gweth", "D. Stein", "H. Ney", "G. Martinez", "J. Verges Llahi", "O. Crasborn", "E. Ormel", "W. Du", "T. Hoyoux", "J. Piater", "J.M. Moya Lazaro", "M. Wheatley"], "venue": "recognition, and translation of sign languages. In Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "M", "author": ["P. Dreuw", "D. Rybach", "T. Deselaers"], "venue": "Zahedi, , and H. Ney. Speech recognition techniques for a sign language recognition system. In Interspeech", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Speech recognition techniques for a sign language recognition system", "author": ["Philippe Dreuw", "David Rybach", "Thomas Deselaers", "Morteza Zahedi", "Hermann Ney"], "venue": "In Interspeech,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Activity recognition and abnormality detection with the switching hidden semi-Markov model", "author": ["Thi V Duong", "Hung H Bui", "Dinh Q Phung", "Svetha Venkatesh"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Tandem acoustic modeling in large-vocabulary recognition", "author": ["Daniel P.W. Ellis", "Rita Singh", "Sunil Sivadas"], "venue": "In ICASSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Transfer learning in sign language", "author": ["A. Farhadi", "D. Forsyth", "R. White"], "venue": "CVPR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploiting depth discontinuities for vision-based fingerspelling recognition", "author": ["R.S. Feris", "M. Turk", "R. Raskar", "K. Tan", "G. Ohashi"], "venue": "IEEE Workshop on Real-Time Vision for Human-Computer Interaction", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving continuous sign language recognition: Speech recognition techniques and system design", "author": ["J. Forster", "O. Koller", "C. Oberd\u00f6rfer", "Y. Gweth", "H. Ney"], "venue": "Proc. SLPAT", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood linear transformations for hmm-based speech recognition", "author": ["Mark JF Gales"], "venue": "Computer speech & language,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Dynamic fingerspelling recognition using geometric and motion features", "author": ["P. Goh", "E.-J. Holden"], "venue": "ICIP", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["Raghuraman Gopalan", "Ruonan Li", "Rama Chellappa"], "venue": "In 2011 international conference on computer vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "CSR-III text", "author": ["D. Graff", "R. Rosenfeld", "D. Paul"], "venue": "http://http://www.ldc. upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Isolated sign language recognition using hidden Markov models", "author": ["K. Grobel", "M. Assan"], "venue": "International Conference on System Man and Cybernetics", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Video-based recognition of fingerspelling in real-time", "author": ["K. Grobel", "H. Hienz"], "venue": "Workshops Bildverarbeitung f\u00fcr die Medizin", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient segmental conditional random fields for phone recognition", "author": ["Yanzhang He", "Eric Fosler-Lussier"], "venue": "In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeecha\u0302A\u0306Z\u030112),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1898}, {"title": "Learning American Sign Language (Levels I & II)", "author": ["T. Humphries", "C. Padden"], "venue": "Pearson Education, New York, second edition", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jing Jiang", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Machine recognition of Auslan signs using powergloves: towards large lexicon integration of sign language", "author": ["M.W. Kadous"], "venue": "Workshop on the Integration of Gesture in Language and Speech", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Towards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language", "author": ["Jonathan Keane"], "venue": "PhD thesis, University of Chicago,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Coarticulation in ASL fingerspelling", "author": ["Jonathan Keane", "Diane Brentari", "Jason Riggle"], "venue": "NELS,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "author": ["Cem Keskin", "Furkan K\u0131ra\u00e7", "Yunus Emre Kara", "Lale Akarun"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "American Sign Language fingerspelling recognition with phonological feature-based tandem models", "author": ["T. Kim", "K. Livescu", "G. Shakhnarovich"], "venue": "IEEE Workshop on Spoken Language Technology", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Fingerspelling recognition with semi-markov conditional random fields", "author": ["Taehwan Kim", "Gregory Shakhnarovich", "Karen Livescu"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Signer-independent fingerspelling recognition with deep neural network adaptation", "author": ["Taehwan Kim", "Weiran Wang", "Hao Tang", "Karen Livescu"], "venue": "In Proc. ICASSP,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Speaker adaptation based on map estimation of hmm parameters", "author": ["C-H Lee", "J-L Gauvain"], "venue": "Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference on, volume 2, pages 558\u2013561. IEEE", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1993}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models", "author": ["Christopher J Leggetter", "Philip C Woodland"], "venue": "Computer Speech & Language,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1995}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. Interspeech", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature learning based on sae\u2013pca network for human gesture recognition", "author": ["Shao-Zi Li", "Bin Yu", "Wei Wu", "Song-Zhi Su", "Rong-Rong Ji"], "venue": "in rgbd images. Neurocomputing,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "Proc. ICASSP", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Articulatory feature-based methods for acoustic and audio-visual speech recognition: Summary from the 2006 jhu summer workshop", "author": ["Karen Livescu", "Omer Cetin", "Mark Hasegawa-Johnson", "Simon King", "Christopher Bartels", "Nash Borges", "Amir Kantor", "Pyare Lal", "Lisa Yung", "Ari Bezman"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2007}, {"title": "Automatic recognition of fingerspelled words in British Sign Language", "author": ["S. Liwicki", "M. Everingham"], "venue": "2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2004}, {"title": "Collecting a motion-capture corpus of American Sign Language for data-driven generation research", "author": ["P. Lu", "M. Huenerfauth"], "venue": "NAACL-HLT Workshop on Speech and Language Processing for Assistive Technologies", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Cuny american sign language motion-capture corpus: first release", "author": ["Pengfei Lu", "Matt Huenerfauth"], "venue": "In Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, The 8th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Purdue ASL database for the recognition of American sign language", "author": ["A.M. Martinez", "R.B. Wilbur", "R. Shay", "A.C. Kak"], "venue": "Proceedings of IEEE International Conference on Multimodal Interfaces (ICMI)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "How many people use ASL in the united states? Why estimates need updating", "author": ["R.E. Mitchell", "T.A. Young", "B. Bachleda", "M.A. Karchmer"], "venue": "Sign Language Studies, 6(3)", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "Automated extraction of signs from continuous sign language sentences using iterated conditional modes", "author": ["S. Nayak", "S. Sarkar", "B. Loeding"], "venue": "CVPR", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2009}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. Eurospeech", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1995}, {"title": "Efficient model-based 3D tracking of hand articulations using Kinect", "author": ["I. Oikonomidis", "N. Kyriazis", "A.A. Argyros"], "venue": "BMVC", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Recognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove", "author": ["C. Oz", "M.C. Leu"], "venue": "2nd international conference on Advances in Neural Networks", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "How the alphabet came to be used in a sign language", "author": ["Carol Padden", "Darline Clark Gunsauls"], "venue": "Sign Language Studies, page 4:10i\u0308\u00a3\u00a133,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Tools for the analysis of benchmark speech recognition tests", "author": ["D.S. Pallet", "W.M. Fisher", "J.G. Fiscus"], "venue": "ICASSP", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1990}, {"title": "Automatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts", "author": ["T. Pfister", "J. Charles", "M. Everingham", "A. Zisserman"], "venue": "BMVC", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in phoneticsbased sub-unit modeling for transcription alignment and sign language recognition", "author": ["V. Pitsikalis", "S. Theodorakis", "C. Vogler", "P. Maragos"], "venue": "IEEE CVPR Workshop on Gesture Recognition", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2011}, {"title": "Spelling it out: Real-time asl fingerspelling recognition", "author": ["Nicolas Pugeault", "Richard Bowden"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2011}, {"title": "Fingerspelling recognition through classification of letterto-letter transitions", "author": ["S. Ricco", "C. Tomasi"], "venue": "ACCV", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2009}, {"title": "Affine-invariant modeling of shape-appearance images applied on sign language handshape classification", "author": ["A. Roussos", "S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "ICIP", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "In European conference on computer vision,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2010}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["Sunita Sarawagi", "William W. Cohen"], "venue": "In NIPS,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "Human action segmentation and recognition using discriminative semi-Markov models", "author": ["Qinfeng Shi", "Li Cheng", "Li Wang", "Alex Smola"], "venue": "International Journal of Computer Vision,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15:1929\u20131958", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time American Sign Language recognition using desk and wearable computer based video", "author": ["T. Starner", "J. Weaver", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12)", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1998}, {"title": "SRILM at sixteen: update and outlook", "author": ["A. Stolcke", "J. Zheng", "W. Wang", "V. Abrash"], "venue": "ASRU", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating multilingual articulatory features into speech recognition", "author": ["Sebastian St\u00fcker", "Florian Metze", "Tanja Schultz", "Alex Waibel"], "venue": "In INTERSPEECH,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2003}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Proc. SLT", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time articulated hand pose estimation using semi-supervised transductive regression forests", "author": ["Danhang Tang", "Tsz-Ho Yu", "Tae-Kyun Kim"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2013}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "Proc. ASRU", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of training approaches for discriminative segmental models", "author": ["Hao Tang", "Kevin Gimpel", "Karen Livescu"], "venue": "In INTERSPEECH,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2014}, {"title": "Exploiting phonological constraints for handshape inference in ASL video", "author": ["A. Thangali", "J.P. Nash", "S. Sclaroff", "C. Neidle"], "venue": "CVPR", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-level data-driven sub-units for signs in videos of continuous sign language", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "ICASSP", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2010}, {"title": "Interarticulator co-ordination in deaf signers with Parkinson\u2019s disease", "author": ["M. E Tyrone", "J. Kegl", "H. Poizner"], "venue": "Neuropsychologia, 37(11)", "citeRegEx": "89", "shortCiteRegEx": null, "year": 1999}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["Paul Viola", "Michael J. Jones"], "venue": "In CVPR,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2001}, {"title": "Parallel hidden Markov models for American Sign Language recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "ICCV", "citeRegEx": "91", "shortCiteRegEx": null, "year": 1999}, {"title": "Toward scalability in ASL recognition: Breaking down signs into phonemes", "author": ["C. Vogler", "D. Metaxas"], "venue": "Gesture Workshop", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1999}, {"title": "A framework for recognizing the simultaneous aspects of American Sign Language", "author": ["C. Vogler", "D. Metaxas"], "venue": "Computer Vision and Image Understanding, 81:358\u2013384", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2001}, {"title": "Handshapes and movements: Multiple-channel ASL recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "Gesture Workshop", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2003}, {"title": "Real-time hand-tracking with a color glove", "author": ["R. Wang", "J. Popovic"], "venue": "SIGGRAPH", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2009}, {"title": "The Phonetics of Fingerspelling", "author": ["S. Wilcox"], "venue": "John Benjamins Publishing Company", "citeRegEx": "96", "shortCiteRegEx": null, "year": 1992}, {"title": "Detecting coarticulation in sign language using conditional random fields", "author": ["R. Yang", "S. Sarkar"], "venue": "ICPR", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptation of contextdependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Proc. SLT", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric features for improving continuous appearance-based sign language recognition", "author": ["M. Zahedi", "P. Dreuw", "D. Rybach", "T. Deselaers", "H. Ney"], "venue": "BMVC", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "Sign language recognition using a combination of new vision based features", "author": ["M.M. Zaki", "S.I. Shaheen"], "venue": "Pattern Recognition Letters, 32(4)", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2011}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "Proc. ICASSP", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2013}, {"title": "Histogram of 3d facets: a depth descriptor for human action and hand gesture recognition", "author": ["Chenyang Zhang", "Yingli Tian"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2015}, {"title": "Segmental CRF approach to large vocabulary continuous speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "ASRU", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification and recognition with direct segment models", "author": ["Geoffrey Zweig"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2012}], "referenceMentions": [{"referenceID": 56, "context": "In the US, there are about 350,000\u2013500,000 people for whom American Sign Language (ASL) is the primary language [60].", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 94, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 6, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 51, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 82, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 86, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 41, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 42, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 26, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 7, "context": "In fact, the fingerspelling handshapes account for about 72% of ASL handshapes [10], making research on fingerspelling applicable to ASL in general.", "startOffset": 79, "endOffset": 83}, {"referenceID": 61, "context": "Fingerspelling is a constrained but important part of ASL, accounting for up to 35% of ASL [65].", "startOffset": 91, "endOffset": 95}, {"referenceID": 41, "context": "ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other 1This thesis includes material previously published in several papers [45, 46, 47].", "startOffset": 246, "endOffset": 258}, {"referenceID": 42, "context": "ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other 1This thesis includes material previously published in several papers [45, 46, 47].", "startOffset": 246, "endOffset": 258}, {"referenceID": 43, "context": "ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other 1This thesis includes material previously published in several papers [45, 46, 47].", "startOffset": 246, "endOffset": 258}, {"referenceID": 35, "context": "Reproduced from [39].", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "Sign language handshape has its own phonology that has been studied but does not yet enjoy a broadly agreed-upon understanding [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "Recent linguistic work on sign language phonology has developed approaches based on articulatory features, which are closely related to motions of parts of the hand [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "Sign language recognition research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on the details of handshape [24, 97].", "startOffset": 171, "endOffset": 179}, {"referenceID": 91, "context": "Sign language recognition research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on the details of handshape [24, 97].", "startOffset": 171, "endOffset": 179}, {"referenceID": 78, "context": "At the same time, computer vision research has studied pose estimation and tracking of hands [83], but usually not in the context of a grammar that constrains the motion.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "Therefore, we propose to use linguistic features suggested by linguistics research on ASL [11, 42].", "startOffset": 90, "endOffset": 98}, {"referenceID": 38, "context": "Therefore, we propose to use linguistic features suggested by linguistics research on ASL [11, 42].", "startOffset": 90, "endOffset": 98}, {"referenceID": 28, "context": "Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72].", "startOffset": 172, "endOffset": 184}, {"referenceID": 51, "context": "Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72].", "startOffset": 172, "endOffset": 184}, {"referenceID": 67, "context": "Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72].", "startOffset": 172, "endOffset": 184}, {"referenceID": 51, "context": "Signer-dependent applications [55, 46] have achieved letter error rates (Levenshtein distances between hypothesized and true letter sequences, as a proportion of the number of true letters) of 10% or less.", "startOffset": 30, "endOffset": 38}, {"referenceID": 42, "context": "Signer-dependent applications [55, 46] have achieved letter error rates (Levenshtein distances between hypothesized and true letter sequences, as a proportion of the number of true letters) of 10% or less.", "startOffset": 30, "endOffset": 38}, {"referenceID": 37, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 33, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 60, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 89, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 25, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 66, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 40, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 96, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 48, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 74, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 85, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 32, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 20, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 91, "context": "In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].", "startOffset": 71, "endOffset": 75}, {"referenceID": 81, "context": "In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].", "startOffset": 129, "endOffset": 137}, {"referenceID": 88, "context": "In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].", "startOffset": 129, "endOffset": 137}, {"referenceID": 55, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 20, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 19, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 3, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 83, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 23, "endOffset": 35}, {"referenceID": 90, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 23, "endOffset": 35}, {"referenceID": 11, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 23, "endOffset": 35}, {"referenceID": 59, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 77, "endOffset": 85}, {"referenceID": 66, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 77, "endOffset": 85}, {"referenceID": 53, "context": "One exception is [57], which includes motion-capture glove data; however, this database is aimed at learning models for ASL generation and does not include the type of naturalistic, coarticulated data that we would like to study.", "startOffset": 17, "endOffset": 21}, {"referenceID": 54, "context": "Also [58] collected motion capture ASL data consisting of both sign and fingerspelling, but small scale for fingerspelling.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 91, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 93, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 94, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 24, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 20, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 57, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 16, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 9, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 234, "endOffset": 242}, {"referenceID": 63, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 234, "endOffset": 242}, {"referenceID": 14, "context": "In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG features) [18].", "startOffset": 210, "endOffset": 214}, {"referenceID": 6, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 16, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 81, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 88, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 87, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 86, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 85, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 64, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 82, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 2, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 108, "endOffset": 119}, {"referenceID": 68, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 108, "endOffset": 119}, {"referenceID": 66, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 108, "endOffset": 119}, {"referenceID": 28, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 160, "endOffset": 172}, {"referenceID": 51, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 160, "endOffset": 172}, {"referenceID": 67, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 160, "endOffset": 172}, {"referenceID": 97, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 80, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 79, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 98, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 34, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 70, "context": "In natural language processing, semi-Markov CRFs have been used for named entity recognition [75], where the labeling is binary.", "startOffset": 93, "endOffset": 97}, {"referenceID": 71, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15].", "startOffset": 120, "endOffset": 128}, {"referenceID": 22, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15].", "startOffset": 273, "endOffset": 277}, {"referenceID": 97, "context": "In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [103, 105].", "startOffset": 245, "endOffset": 255}, {"referenceID": 79, "context": "We compare this approach to an efficient first-pass segmental model [84].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "For the challenging signer-independent setting in which the signers in the training and test data are different, we may consider domain adaptation [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 15, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 49, "endOffset": 60}, {"referenceID": 5, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 49, "endOffset": 60}, {"referenceID": 36, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 49, "endOffset": 60}, {"referenceID": 69, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 78, "endOffset": 90}, {"referenceID": 18, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 78, "endOffset": 90}, {"referenceID": 29, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 78, "endOffset": 90}, {"referenceID": 1, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 114, "endOffset": 125}, {"referenceID": 27, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 114, "endOffset": 125}, {"referenceID": 46, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 114, "endOffset": 125}, {"referenceID": 46, "context": "The classic techniques include maximum likelihood linear regression [50] and maximum a posteriori [49].", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "The classic techniques include maximum likelihood linear regression [50] and maximum a posteriori [49].", "startOffset": 98, "endOffset": 102}, {"referenceID": 49, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 0, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 77, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 17, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 50, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13].", "startOffset": 183, "endOffset": 195}, {"referenceID": 76, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13].", "startOffset": 183, "endOffset": 195}, {"referenceID": 10, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13].", "startOffset": 183, "endOffset": 195}, {"referenceID": 23, "context": "1 Tandem model The first recognizer we consider is based on the popular tandem approach to speech recognition [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "1: Definition and possible values for phonological features based on [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "For detailed descriptions, see [11].", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "[13], who used articulatory feature NN classifiers rather than phone classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We use a phonological feature set developed by Brentari [11], who proposed seven features for ASL handshape.", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 70, "context": "SCRFs [75, 103] are conditional loglinear models with feature functions that can be based on variable-length segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 15}, {"referenceID": 97, "context": "SCRFs [75, 103] are conditional loglinear models with feature functions that can be based on variable-length segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 15}, {"referenceID": 70, "context": "Semi-Markov CRFs [75], also referred to as segmental CRFs [103] or SCRFs, provide this ability.", "startOffset": 17, "endOffset": 21}, {"referenceID": 97, "context": "Semi-Markov CRFs [75], also referred to as segmental CRFs [103] or SCRFs, provide this ability.", "startOffset": 58, "endOffset": 63}, {"referenceID": 97, "context": "Baseline consistency feature To take advantage of the existence of a high-quality baseline, we use a baseline feature like the one introduced by [103].", "startOffset": 145, "endOffset": 150}, {"referenceID": 8, "context": "1, we use the linguistic handshape feature set developed by Brentari [11], who proposed seven features to describe handshape in ASL.", "startOffset": 69, "endOffset": 73}, {"referenceID": 79, "context": "[84].", "startOffset": 0, "endOffset": 4}, {"referenceID": 79, "context": "We use the same feature functions as in [84], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias, all lexicalized.", "startOffset": 40, "endOffset": 44}, {"referenceID": 49, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 0, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 77, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 17, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 58, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51].", "startOffset": 96, "endOffset": 108}, {"referenceID": 92, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51].", "startOffset": 96, "endOffset": 108}, {"referenceID": 47, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51].", "startOffset": 96, "endOffset": 108}, {"referenceID": 39, "context": "For more details about data and annotation steps, please refer to [43].", "startOffset": 66, "endOffset": 70}, {"referenceID": 42, "context": "1This section includes material previously published in [46].", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "Hand localization and segmentation For every signer, we trained a model for hand detection similar to that used in [45, 55].", "startOffset": 115, "endOffset": 123}, {"referenceID": 51, "context": "Hand localization and segmentation For every signer, we trained a model for hand detection similar to that used in [45, 55].", "startOffset": 115, "endOffset": 123}, {"referenceID": 84, "context": "We suppress pixels that fall within regions detected as faces by the ViolaJones face detector [90], since these tend to be false positives.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Handshape descriptors For most experiments, we use histograms of oriented gradients (HOG [18]) as the visual descriptor (feature vector) for a given hand region2.", "startOffset": 89, "endOffset": 93}, {"referenceID": 52, "context": "SIFT features is a commonly used type of image appearance features, and we extract SIFT features from each image, based on local histograms of oriented image gradients [56].", "startOffset": 168, "endOffset": 172}, {"referenceID": 41, "context": "3This section includes material previously published in [45].", "startOffset": 56, "endOffset": 60}, {"referenceID": 75, "context": "We implement the HMMs with HTK [1] and language models with SRILM [80].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "We train smoothed backoff bigram letter language models using lexicons of various sizes, consisting of the most frequent words in the ARPA CSR-III text, which includes English words and names [34].", "startOffset": 192, "endOffset": 196}, {"referenceID": 95, "context": "The DNNs have three hidden layers, each with 3000 ReLUs [101].", "startOffset": 56, "endOffset": 61}, {"referenceID": 73, "context": "Network learning is done with cross-entropy training with a weight decay penalty of 10\u22125, via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [78] at a rate of 0.", "startOffset": 190, "endOffset": 194}, {"referenceID": 43, "context": "4This section includes material previously published in [47].", "startOffset": 56, "endOffset": 60}, {"referenceID": 62, "context": "An asterisk (\u2018*\u2019) indicates statistically significant improvement over the corresponding baseline (\u2018BL\u2019) using the same training labels for the HMMs, according to a MAPSSWE test [66] at p < 0.", "startOffset": 178, "endOffset": 182}, {"referenceID": 75, "context": "We use HTK [1] to implement the tandem HMM-based recognizers and SRILM [80] to train the language models.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": ", [17]), we also conduct an initial experiment with them.", "startOffset": 2, "endOffset": 6}, {"referenceID": 65, "context": "We use Kaldi toolkit [69] for implementation.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "We also consider an alternative subletter feature set, in particular a set of phonetic features introduced by Keane [42], whose feature values are listed in Table 3.", "startOffset": 116, "endOffset": 120}, {"referenceID": 44, "context": ", [48, 77]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 72, "context": ", [48, 77]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 38, "context": "3: Phonetic features [42].", "startOffset": 21, "endOffset": 25}, {"referenceID": 79, "context": "We follow the discriminative segmental cascades (DSC) approach of [84], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.", "startOffset": 66, "endOffset": 70}, {"referenceID": 62, "context": "For the statistical significance test, we use MAPSSWE test [66].", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "We compare: letter only, phonetic features only, letter + phonological feature [11] and letter + phonetic feature [42].", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "We compare: letter only, phonetic features only, letter + phonological feature [11] and letter + phonetic feature [42].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": ", [35] for speech recognition.", "startOffset": 2, "endOffset": 6}], "year": 2016, "abstractText": "In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates. Thesis Supervisor: Karen Livescu Title: Assistant Professor", "creator": "LaTeX with hyperref package"}}}