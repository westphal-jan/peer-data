{"id": "1608.08716", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Measuring Machine Intelligence Through Visual Question Answering", "abstract": "through machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. a common approach is to propose tasks performing which a human excels, considering one often machines find difficult. however, 1 ideal task should also be easy to evaluate and not be easily gameable. concepts begin, a case study exploring the recently popular task of image captioning and its limitations as a parameter worth measuring machine intelligence. an alternative and more promising task is visual question recognition that tests a machine's ability to reason about language and vision. we describe a dataset unprecedented in size created for the task once contains over 760, 000 human generated debates about images. using around 10 700 human generated answers, machines may still easily evaluated.", "histories": [["v1", "Wed, 31 Aug 2016 02:56:00 GMT  (2553kb,D)", "http://arxiv.org/abs/1608.08716v1", "AI Magazine, 2016"]], "COMMENTS": "AI Magazine, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV cs.LG", "authors": ["c lawrence zitnick", "aishwarya agrawal", "stanislaw antol", "margaret mitchell", "dhruv batra", "devi parikh"], "accepted": false, "id": "1608.08716"}, "pdf": {"name": "1608.08716.pdf", "metadata": {"source": "CRF", "title": "Measuring Machine Intelligence Through Visual Question Answering", "authors": ["C. Lawrence Zitnick", "Aishwarya Agrawal", "Virginia Tech", "Stanislaw Antol", "Margaret Mitchell", "Dhruv Batra", "Devi Parikh"], "emails": ["zitnick@fb.com", "aish@vt.edu", "santol@vt.edu", "memitc@microsoft.com", "dbatra@vt.edu", "parikh@vt.edu"], "sections": [{"heading": "1. Introduction", "text": "Humans have an amazing ability to both understand and reason about our world through a variety of senses or modalities. A sentence such as \u201cMary quickly ran away from the growling bear.\u201d, conjures both vivid visual and auditory interpretations. We picture Mary running in the opposite direction of a ferocious bear with the sound of the bear being enough to frighten anyone. While interpreting a sentence such as this is effortless to a human, designing intelligent machines with the same deep understanding is anything but. How would a machine know Mary is frightened? What is likely to happen to Mary if she doesn\u2019t run? Even simple implications of the sentence, such as \u201cMary is likely outside\u201d may be nontrivial to deduce.\nHow can we determine if a machine has achieved the same deep understanding of our world as a human? In our example sentence above, a human\u2019s understanding is rooted in multiple modalities. They can visualize a scene depict-\ning Mary running, they can imagine the sound of the bear, and even how the bear\u2019s fur might feel when touched. Conversely, if shown a picture or even an auditory recording of a woman running from a bear, a human may similarly describe the scene. Perhaps machine intelligence could be tested in a similar manner? Can a machine use natural language to describe a picture similar to a human? Similarly, could a machine generate a scene given a written description? In fact these tasks have been a goal of artificial intelligence research since its inception. Marvin Minsky famously stated in 1966 [8] to one of his students,\u201cConnect a television camera to a computer and get the machine to describe what it sees.\u201d At the time, and even today, the full complexities of this task are still being discovered."}, {"heading": "2. Image Captioning", "text": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26]. Unfortunately, tasks such as image captioning have proven problematic as actual tests of intelligence. Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27]. It has been observed that captions judged as \u201cgood\u201d by human observers may actually contain\nar X\niv :1\n60 8.\n08 71\n6v 1\n[ cs\n.A I]\n3 1\nA ug\nsignificant variance even though they describe the same image [30]. For instance see Figure 1. Many people would judge the longer more detailed captions as better. However, the details described by the captions varies significantly, e.g. \u201ctwo hands\u201d, \u201cwhite t-shirt\u201d, \u201cblack curly hair\u201d, \u201clabel\u201d, etc. How can we evaluate a caption if there is no consensus on what should be contained in a \u201cgood\u201d caption? However, for shorter less detailed captions that are commonly written by humans a rough consensus is achieved \u201cA man holding a beer bottle.\u201d This leads to the somewhat counterintuitive conclusion that captions humans like aren\u2019t necessarily \u201chuman-like\u201d.\nThe task of image captioning also suffers from another less obvious drawback. In many cases it might be too easy! Consider an example success from a recent paper on image captioning [15], Figure 2. Upon first inspection this caption appears to have been generated from a deep understanding of the image. For instance, in Figure 2 the machine must have detected a giraffe, grass and tree. It understood that the giraffe was standing, and the thing it was standing on was grass. It knows the tree and giraffe are \u201cnext to\u201d each other, etc. Is this interpretation of the machine\u2019s depth of understanding correct? When judging the results of an AI system, it is not only important to analyze its output, but the data used for its training. The results in Figure 2 were obtained by training on the Microsoft Common Objects in Context (MS COCO) dataset [23]. This dataset contains five independent captions written by humans for over 120,000 images [5]. If we examine the image in Figure 2 and the images in the training dataset we can make an interesting observation. For many testing images, there exists a significant number of semantically similar training images, Figure 2(right). If two images share enough semantic similarity, it is possible a single caption could describe them both.\nThis observation leads to a surprisingly simple algorithm for generating captions [9]. Given a test image, collect a set of captions from images that are visually similar. From\nthis set, select the caption with highest consensus [30], i.e. the caption most similar to the other captions in the set. In many cases the consensus caption is indeed a good caption. When judged by humans, 21.6% of these borrowed captions are judged to be equal to or better than those written by humans for the image specifically. Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.3% when compared to human captions. Even methods using recurrent neural networks commonly produce captions that are identical to training captions even though they\u2019re not explicitly trained to do so. If captions are \u201cgenerated\u201d by borrowing them from other images, these algorithms are clearly not demonstrating a deep understanding of language, semantics and their visual interpretation. The odds of two humans repeating a sentence is quite rare.\nOne could make the case that the fault is not with the algorithms but in the data used for training. That is, the dataset contains too many semantically similar images. However, even in randomly sampled images from the web, a photographer bias is found. Humans capture similar images to each other. Many of our tastes or preferences are universal."}, {"heading": "3. Visual Question Answering", "text": "As we demonstrated using the task of image captioning, determining a multimodal task for measuring a machine\u2019s intelligence is challenging. The task must be easy to evaluate, yet hard to solve. That is, it\u2019s evaluation shouldn\u2019t be as hard as the task itself, and it must not be solvable using \u201cshortcuts\u201d or \u201ccheats\u201d. To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].\nThe task of VQA requires a machine to answer a natural language question about an image as shown in Figure 3. Unlike the captioning task, evaluating answers to questions is relatively easy. The simplest approach is to pose the questions with multiple choice answers, much like standardized tests administered to students. Since computers don\u2019t get tired of reading through long lists of answers, we can even increase the length of the answer list. Another more challenging option is to leave the answers open-ended. Since most answers are single words such as \u201cyes\u201d, \u201cblue\u201d, or \u201ctwo\u201d evaluating their correctness is straightforward.\nIs the visual question answering task challenging? The task is inherently multimodal, since it requires knowledge of language and vision. Its complexity is further increased by the fact that many questions require commonsense knowledge to answer. For instance, if you ask \u201cDoes the man have \u201c20/20\u201d vision?\u201d, you need the commonsense knowledge that having 20/20 vision implies you don\u2019t wear glasses. Going one step further, one might be concerned\nthat commonsense knowledge is all that\u2019s needed to answer the questions. For example if the question was \u201cWhat color is the sheep?\u201d, our commonsense would tell us the answer is \u201cwhite\u201d. We may test the sufficiency of commonsense knowledge by asking subjects to answer questions without seeing the accompanying image. In this case, humans subjects did indeed perform poorly (33% correct), indicating that commonsense may be necessary but not sufficient. Similarly, we may ask subjects to answer the question given only a caption describing the image. In this case the humans performed better (57% correct), but still not as accurately as those able to view the image (78% correct). This helps indicate the VQA task requires more detailed information about an image than is typically provided in an image caption.\nHow do you gather diverse and interesting questions for 100,000\u2019s of images? Amazon\u2019s Mechanical Turk provides a powerful platform for crowdsourcing tasks, but the design and prompts of the experiments must be careful chosen. For instance, we ran trial experiments prompting the subjects to write questions that would be difficult for a \u201ctoddler\u201d, \u201calien\u201d, or \u201csmart robot\u201d to answer. Upon examination, we determined that questions written for a smart robot were most interesting given their increased diversity and difficulty. In comparison, the questions stumping a toddler were a bit too easy. We also gathered three questions per image and ensured diversity by displaying the previously written questions and stating \u201cWrite a different question from those above that would stump a smart robot.\u201d In total over 760,000 questions were gathered 1.\nThe diversity of questions supplied by the subjects on Amazon\u2019s Mechanical Turk is impressive. In Figure 4, we show the distribution of words that begin the questions.\n1http://visualqa.org\nThe majority of questions begin with \u201cWhat\u201d and \u201cIs\u201d, but other questions include \u201cHow\u201d, \u201cAre\u201d, \u201cDoes\u201d, etc. Clearly no one type of question dominates. The answers to these questions have a varying diversity depending on the type of question. Since the answers may be ambiguous, e.g. \u201cWhat is the person looking at?\u201d we collected ten answers per question. As shown in Figure 5, many question types are simply answered \u201cyes\u201d or \u201cno\u201d. Other question types such as those that start with \u201cWhat is\u201d have a greater variety of answers. An interesting comparison is to examine the distribution of answers when subjects were asked to answer the questions with and without looking at the image. As shown in Figure 5 (bottom), there is a strong bias to many questions when subjects do not see the image. For instance \u201cWhat color\u201d questions invoke \u201cred\u201d as an answer, or for questions that are answered by \u201cyes\u201d or \u201cno\u201d, \u201cyes\u201d is highly favored.\nFinally it is important to measure the difficulty of the questions. Some questions such as \u201cWhat color is the ball?\u201d or \u201cHow many people are in the room?\u201d may seem quite simple. In contrast, other questions such as \u201cDoes this person expect company?\u201d or \u201cWhat government document is needed to partake in this activity?\u201d may require quite advanced reasoning to answer. Unfortunately, the difficultly of a question is in many cases ambiguous. The question\u2019s difficultly is as much dependent on the person or machine answering the question as the question itself. Each person or machine has different competencies.\nIn an attempt to gain insight into how challenging each question is to answer, we asked human subjects to guess how old a person would need to be to answer the question. It is unlikely most human subjects have adequate knowledge of human learning development to answer the question correctly. However, this does provide an effective proxy for question difficulty. That is, questions judged to be answerable by a 3-4 year old are easier than those judged answerable by a teenager. Note, we make no claims that questions judged answerable by a 3-4 year old will actually be answered correctly by toddlers. This would require additional experiments performed by the appropriate\nage groups. Since the task is ambiguous, we collected ten respondences for each question. In Figure 6 we show several questions for which a majority of subjects picked the specified age range. Surprisingly the perceived age needed to answer the questions is fairly well distributed across the different age ranges. As expected the questions that were judged answerable by an adult (18+) generally need specialized knowledge, where those answerable by a toddler (3-4) are more generic."}, {"heading": "4. Abstract Scenes", "text": "The visual question answering task requires a variety of skills. The machine must be able to understand the image, interpret the question and reason about the answer. For many researchers exploring AI, they may not be interested in exploring the low-level tasks involved with perception and computer vision. Many of the questions may even be impossible to solve given the current capabilities of state-ofthe-art computer vision algorithms. For instance the question \u201cHow many cellphones are in the image?\u201d may not be answerable if the computer vision algorithms cannot accurately detect cellphones. In fact, even for state-of-the-art algorithms many objects are difficult to detect, especially small objects [23].\nTo enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36]. Abstract scenes or cartoon images are created from sets of clip art, Figure 7. The scenes are created by human subjects using a graphical user interface that allows them to arrange a wide variety objects. For clip art depicting humans, their\nposes and expression may also be changed. Using the interface a wide variety of scenes can be created including ordinary scenes, scary scenes, or funny scenes. Since the type of clip art and it\u2019s properties are exactly known, the problem of recognizing objects and their attributes is greatly simplified. This provides researchers an opportunity to more directly study the problems of question understanding and answering. Once computer vision algorithms \u201ccatch up\u201d, perhaps some of the techniques developed for abstract scenes can be applied to real images. The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31]."}, {"heading": "5. Discussion", "text": "While visual question answering appears to be a promising approach to measuring machine intelligence for multimodal tasks, it may prove to have unforseen shortcomings. We\u2019ve explored several baseline algorithms that perform poorly when compared to human performance. As the dataset is explored, it is possible that solutions may be found that don\u2019t require \u201ctrue AI\u201d. However, using proper analysis we hope to continuously update the dataset to reflect the current progress of the field. As certain question or image types become too easy to answer we can add new questions and images. Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].\nIn conclusion, we believe designing a multimodal challenge is essential for accelerating and measuring the progress of AI. Visual question answering offers one approach for designing such challenges that allows for easy evaluation while maintaining the difficultly of the task. As the field progresses our tasks and challenges should be continuously reevaluated to ensure they are of appropriate difficultly given the state of research. Importantly, these tasks should be designed to push the frontiers of AI research, and help ensure their solutions lead us towards systems that are truly \u201cAI complete\u201d."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning via visual abstraction", "author": ["S. Antol", "C.L. Zitnick", "D. Parikh"], "venue": "European Conference on Computer Vision, pages 401\u2013416. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning the semantics of words and pictures", "author": ["K. Barnard", "D. Forsyth"], "venue": "Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 2, pages 408\u2013415. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "et al", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White"], "venue": "Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333\u2013342. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2422\u20132431", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Neil: Extracting visual knowledge from web data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1409\u2013 1416", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "AI: The tumultuous history of the search for artificial intelligence", "author": ["D. Crevier"], "venue": "Basic Books, Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["S.K. Divvala", "A. Farhadi", "C. Guestrin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3270\u20133277", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2625\u20132634", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing automatic evaluation measures for image description", "author": ["D. Elliott", "F. Keller"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers, volume 452, page 457", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Paraphrasedriven learning for open question answering", "author": ["A. Fader", "L.S. Zettlemoyer", "O. Etzioni"], "venue": "ACL (1), pages 1608\u20131618. Citeseer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "European Conference on Computer Vision, pages 15\u201329. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "Advances in Neural Information Processing Systems, pages 2296\u20132304", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853\u2013 899", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Transactions of the Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2891\u2013 2903", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems, pages 1682\u20131690", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Researchers announce advance in imagerecognition software", "author": ["J. Markoff"], "venue": "The New York Times", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747\u2013756. Association for Computational Linguistics", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["M. Richardson", "C.J. Burges", "E. Renshaw"], "venue": "EMNLP, volume 3, page 4", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint video and text parsing for understanding events and answering queries", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.-C. Zhu"], "venue": "IEEE MultiMedia, 21(2):42\u201370", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning common sense through visual abstraction", "author": ["R. Vedantam", "X. Lin", "T. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2542\u20132550", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "B", "author": ["J. Weston", "A. Bordes", "S. Chopra", "A.M. Rush"], "venue": "van Merri\u00ebnboer, A. Joulin, and T. Mikolov. Towards aicomplete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3009\u20133016", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1681\u20131688", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 38(4):627\u2013 638", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Marvin Minsky famously stated in 1966 [8] to one of his students,\u201cConnect a television camera to a computer and get the machine to describe what it sees.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 21, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 26, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 15, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 18, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 14, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 5, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 10, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 24, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 20, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 19, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 31, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 25, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 261, "endOffset": 265}, {"referenceID": 11, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 29, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 18, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 21, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 26, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 14, "context": "(left) An example image caption generated from [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "significant variance even though they describe the same image [30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "In many cases it might be too easy! Consider an example success from a recent paper on image captioning [15], Figure 2.", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "The results in Figure 2 were obtained by training on the Microsoft Common Objects in Context (MS COCO) dataset [23].", "startOffset": 111, "endOffset": 115}, {"referenceID": 4, "context": "This dataset contains five independent captions written by humans for over 120,000 images [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "This observation leads to a surprisingly simple algorithm for generating captions [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 29, "context": "From this set, select the caption with highest consensus [30], i.", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 10, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 24, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 20, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 19, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 31, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 14, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 164, "endOffset": 168}, {"referenceID": 0, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 17, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 23, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 28, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 3, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 16, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 22, "context": "In fact, even for state-of-the-art algorithms many objects are difficult to detect, especially small objects [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 33, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 34, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 35, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 34, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 1, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 6, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 9, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 30, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 12, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}, {"referenceID": 13, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}, {"referenceID": 32, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}, {"referenceID": 27, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}], "year": 2016, "abstractText": "As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine\u2019s ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.", "creator": "LaTeX with hyperref package"}}}