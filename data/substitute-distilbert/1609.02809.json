{"id": "1609.02809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "Harassment detection: a benchmark on the #HackHarassment dataset", "abstract": "online harassment has been a problem to a greater or lesser extent since prior early conflicts like the internet. previous work has applied anti - bullying techniques like machine - message based text classification ( reynolds, 2011 ) to detecting harassing messages. however, existing public datasets are limited in size, with patterns of varying quality. hacker # hackharassment initiative ( an alliance of 1 tech companies and ngos similar to fighting espionage on the internet ) has begun to address this anomaly seeking targeting a clean dataset superior to its predecssors in terms of both size and quality. as we ( # hackharassment ) complete further rounds of reporting, detailed iterations of this dataset frequently increase the available samples by at least an order of magnitude, enabling continued improvements in the quality of machine cognitive models for harassment detection. in this section, we claim the first models built on the # hackharassment dataset v1. 0 ( a new open dataset, which we are delighted to share with any interested researcherss ) as a benchmark for future research.", "histories": [["v1", "Fri, 9 Sep 2016 14:23:02 GMT  (196kb)", "http://arxiv.org/abs/1609.02809v1", "Accepted to the Collaborative European Research Conference 2016"]], "COMMENTS": "Accepted to the Collaborative European Research Conference 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexei bastidas", "edward dixon", "chris loo", "john ryan"], "accepted": false, "id": "1609.02809"}, "pdf": {"name": "1609.02809.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["edward.dixon@intel.com "], "sections": [{"heading": null, "text": "Harassment\u00a0detection:\u00a0a\u00a0benchmark\u00a0on\u00a0the\u00a0#HackHarassment\u00a0dataset\u00a0\nAlexei\u00a0Bastidas,\u00a0Edward\u00a0Dixon,\u00a0Chris\u00a0Loo,\u00a0John\u00a0Ryan\u00a0 \u00a0Intel\u00a0\u00a0\nemail:\u00a0\u00a0edward.dixon@intel.com\u00a0\nKeywords:e.g.Machine\u00a0Learning,\u00a0Natural\u00a0Language\u00a0Processing,\u00a0Cyberbullying\u00a0\nIntroduction\u00a0\nOnline harassment has been a problem to a greater or lesser extent since the early days of the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 internet. Previous work has applied antispam techniques like machinelearning based text\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 classification (Reynolds, 2011) to detecting harassing messages. However, existing public datasets\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 are limited in size, with labels of varying quality. The #HackHarassment initiative (an alliance of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01 tech companies and NGOs devoted to fighting bullying on the internet) has begun to address this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 issue by creating a new dataset superior to its predecssors in terms of both size and quality. As we\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (#HackHarassment) complete further rounds of labelling, later iterations of this dataset will\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 increase the available samples by at least an order of magnitude, enabling corresponding\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 improvements in the quality of machine learning models for harassment detection. In this paper, we\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 introduce the first models built on the #HackHarassment dataset v1.0 (a new open dataset, which\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 we\u00a0are\u00a0delighted\u00a0to\u00a0share\u00a0with\u00a0any\u00a0interested\u00a0researcherss)\u00a0as\u00a0a\u00a0benchmark\u00a0for\u00a0future\u00a0research.\u00a0\nRelated\u00a0Work\u00a0\nPrevious work in the area by Bayzik 2011 showed that machine learing and natural language\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 processing could be successfully applied to detect bullying messages on an online forum. However,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the same work also made clear that the limiting factor on such models was the availability of a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 suitable quantity of labeled examples. For example, the Bayzick work relied of a dataset of 2,696\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 samples, only 196 of which were found to be examples of bullying behaviour. Additionally, this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 work relied on model types like J48 and JRIP (types of decision tree), and knearest neighbours\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 classifiers like IBk, as opposed to popular modern ensemble methods or deep neuralnetworkbased\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 approaches.\u00a0\nMethodology\u00a0\nOur\u00a0work\u00a0was\u00a0carried\u00a0out\u00a0using\u00a0the\u00a0#HackHarassment\u00a0Verison\u00a01\u00a0dataset,\u00a0the\u00a0first\u00a0iteration\u00a0of\u00a0which\u00a0 consists\u00a0exclusively\u00a0of\u00a0Reddit\u00a0posts.\u00a0\u00a0An\u00a0initially\u00a0random\u00a0selection\u00a0of\u00a0posts,\u00a0in\u00a0which\u00a0harassing\u00a0 content\u00a0occured\u00a0at\u00a0a\u00a0rate\u00a0of\u00a0between\u00a05%\u00a0and\u00a07%\u00a0was\u00a0culled\u00a0of\u00a0benign\u00a0content\u00a0using\u00a0models\u00a0training\u00a0 on\u00a0a\u00a0combination\u00a0of\u00a0existing\u00a0cyberbullying\u00a0datasets\u00a0(Reynolds\u00a02001,\u00a0also\u00a0\u201cImproved\u00a0cyberbullying\u00a0 detection\u00a0through\u00a0personal\u00a0profiles).\u00a0Each\u00a0post\u00a0is\u00a0labelled\u00a0independently\u00a0by\u00a0at\u00a0least\u00a0five\u00a0Intel\u00a0 Security\u00a0Web\u00a0Analysts.\u00a0\u00a0\u00a0(a\u00a0post\u00a0is\u00a0considered\u00a0\u201cbullying\u201d\u00a0if\u00a0it\u00a0labelled\u00a0as\u00a0such\u00a0by\u00a020%\u00a0or\u00a0more\u00a0of\u00a0 the\u00a0human\u00a0labelers\u00a0\u00a0as\u00a0shown\u00a0in\u00a0the\u00a0following\u00a0histogram,\u00a0a\u00a0perfect\u00a0consensus\u00a0is\u00a0relatively\u00a0rare,\u00a0and\u00a0 so\u00a0we\u00a0rate\u00a0a\u00a0post\u00a0as\u00a0\u201charassing\u201d\u00a0if\u00a020%\u00a0\u00a02\u00a0of\u00a0our\u00a05\u00a0raters\u00a0\u00a0consider\u00a0it\u00a0to\u00a0be\u00a0harassing).\u00a0\u00a0This\u00a0is\u00a0a\u00a0 relatively\u00a0balanced\u00a0dataset,\u00a0with\u00a01,280\u00a0nonbullying/harassing\u00a0posts,,\u00a0and\u00a01,118\u00a0bullying/harassing\u00a0 examples.\u00a0\n1\u00a0\"Hack\u00a0Harassment.\"\u00a02016.\u00a026\u00a0Jul.\u00a02016\u00a0<http://www.hackharassment.com/>\u00a0\n\u00a0 All preprocessing, training and evaluation was carried out in Python, using the popular\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SciKitLearn library (for feature engineering and linear models) in combination with Numpy (for\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02 3 matrix\u00a0operations),\u00a0Keras \u00a0and\u00a0TensorFlow \u00a0(for\u00a0models\u00a0based\u00a0on\u00a0deep\u00a0neural\u00a0networks\u00a0\u00a0DNNs).\u00a0 \u00a04 5\nFor the linear models, features were generated by tokenizing the text (breaking it aparting into\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 words), hashing the resulting unigrams, bigrams and trigrams (collectiojns of one, two, or three\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 adjacent words) and computing at TF/IDF for each hashed value. The resulting feature vectors\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 were used to train and test Logistic Regressioin, Support Vector Machine and Gradient Boosted\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Tree models, with 80% of data used for training and 20% held out for testing (results given are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 based\u00a0on\u00a0the\u00a0heldout\u00a020%).\u00a0\nFor the DNNbased approach, a similar approach was taken to tokenization, both bigram and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 trigram hashes were computed\u037e these were onehot encoded, and dense representations of these\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 features\u00a0were\u00a0learned\u00a0during\u00a0training,\u00a0as\u00a0per\u00a0Joulin\u00a02016.\u00a0\n2\u00a0\"scikitlearn:\u00a0machine\u00a0learning\u00a0in\u00a0Python\u00a0\u2014\u00a0scikitlearn\u00a00.17.1\u00a0...\"\u00a02011.\u00a029\u00a0Jul.\u00a02016\u00a0<http://scikitlearn.org/>\u00a0 3\u00a0\"NumPy\u00a0\u2014\u00a0Numpy.\"\u00a02002.\u00a029\u00a0Jul.\u00a02016\u00a0<http://www.numpy.org/>\u00a0 4\u00a0\"Keras\u00a0Documentation.\"\u00a02015.\u00a029\u00a0Jul.\u00a02016\u00a0<http://keras.io/>\u00a0 5\u00a0\"TensorFlow\u00a0\u2014\u00a0an\u00a0Open\u00a0Source\u00a0Software\u00a0Library\u00a0for\u00a0Machine\u00a0...\"\u00a02015.\u00a029\u00a0Jul.\u00a02016\u00a0 <https://www.tensorflow.org/>\u00a0\nThe FastText model used is a python implenmentation of the model described in \"Bag of Tricks for\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Efficient Text Classification.\u201d . For the text encoding, bigrams and trigrams are used. 20% of the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a06 data\u00a0was\u00a0held\u00a0out\u00a0for\u00a0testing.\u00a0\nThe Recurrent Character Level Neural Network model consists of 2 GRU layers of width 100\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 followed by a Dense Layer of size 2 with softmax on the output, Between each of the layers batch\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normailization is performed. The optimiser used was rmsprop. For data preperation each of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 characters was onehot encoded and each sample was truncated/padded to 500 charcters in length.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 20%\u00a0of\u00a0the\u00a0data\u00a0was\u00a0held\u00a0out\u00a0for\u00a0testing.\u00a0\nResults\u00a0\nModel\u00a0 Precision\u00a0(Harassing)\u00a0 Recall\u00a0(Harassing)\u00a0\nGradient\u00a0Boosted\u00a0Trees\u00a0 (ScikitLearn)\u00a0\n0.80\u00a0 0.71\u00a0\nBernoulli\u00a0Naive\u00a0Bayes\u00a0 0.54\u00a0 0.30\u00a0\nFastText\u00a0 0.60\u00a0 0.78\u00a0\nRecurrent\u00a0Character\u00a0Level\u00a0 Neural\u00a0Network\u00a0\n0.71\u00a0 0.73\u00a0\n\u00a0\nConclusions\u00a0\nWe\u00a0have\u00a0presented\u00a0the\u00a0first\u00a0results\u00a0on\u00a0a\u00a0new\u00a0open\u00a0cyberbullying/harassment\u00a0dataset.\u00a0\u00a0While\u00a0our\u00a0 models\u00a0clearly\u00a0demonstrate\u00a0a\u00a0degree\u00a0of\u00a0ability\u00a0to\u00a0discriminate\u00a0between\u00a0the\u00a0content\u00a0classes,\u00a0the\u00a0 achieved\u00a0precision\u00a0in\u00a0particular\u00a0falls\u00a0far\u00a0short\u00a0of\u00a0our\u00a0ambitions\u00a0for\u00a0#HackHarassment.\u00a0 \u00a0 Over\u00a0the\u00a0coming\u00a0months,\u00a0we\u2019ll\u00a0massively\u00a0expand\u00a0the\u00a0size\u00a0of\u00a0our\u00a0labelled\u00a0dataset,\u00a0and\u00a0review\u00a0our\u00a0 labelling\u00a0methodology,\u00a0anticipating\u00a0that\u00a0a\u00a0larger\u00a0dataset\u00a0will\u00a0facilitate\u00a0more\u00a0accurate\u00a0models.\u00a0\u00a0We\u00a0 look\u00a0forward\u00a0both\u00a0to\u00a0the\u00a0availability\u00a0of\u00a0a\u00a0larger\u00a0dataset,\u00a0and\u00a0to\u00a0seeing\u00a0the\u00a0development\u00a0of\u00a0classifiers\u00a0 that\u00a0improvement\u00a0on\u00a0our\u00a0work,\u00a0and\u00a0welcome\u00a0partners\u00a0able\u00a0to\u00a0contribute\u00a0either\u00a0in\u00a0terms\u00a0of\u00a0 expanding\u00a0the\u00a0dataset\u00a0or\u00a0improving\u00a0the\u00a0modelling.\u00a0\nReferences\u00a0 Reynolds,\u00a0Kelly,\u00a0April\u00a0Kontostathis,\u00a0and\u00a0Lynne\u00a0Edwards.\u00a0\"Using\u00a0machine\u00a0learning\u00a0to\u00a0detect\u00a0cyberbullying.\"\u00a0Machine\u00a0Learning\u00a0and\u00a0 Applications\u00a0and\u00a0Workshops\u00a0(ICMLA),\u00a02011\u00a010th\u00a0International\u00a0Conference\u00a0on \u00a018\u00a0Dec.\u00a02011:\u00a0241244.\u00a0\nBayzick,\u00a0Jennifer,\u00a0April\u00a0Kontostathis,\u00a0and\u00a0Lynne\u00a0Edwards.\u00a0\"Detecting\u00a0the\u00a0presence\u00a0of\u00a0cyberbullying\u00a0using\u00a0computer\u00a0software.\"\u00a0 (2011):\u00a012.\u00a0\nJoulin,\u00a0Armand\u00a0et\u00a0al.\u00a0\"Bag\u00a0of\u00a0Tricks\u00a0for\u00a0Efficient\u00a0Text\u00a0Classification.\"\u00a0arXiv\u00a0preprint\u00a0arXiv:1607.01759 \u00a0(2016).\u00a0\nImproved\u00a0Cyberbullying\u00a0Detection\u00a0Through\u00a0Personal\u00a0Profiles\u00a0\n6\u00a0\"fastText\"\u00a02015.\u00a022\u00a0Jul.\u00a02016\u00a0<https://github.com/sjhddh/fastText>\u00a0"}], "references": [{"title": "Using machine learning to detect cyberbullying.\" \u200bMachine", "author": ["Reynolds", "Kelly", "April Kontostathis", "Lynne Edwards"], "venue": "Learning and Applications and Workshops (ICMLA),", "citeRegEx": "Reynolds et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reynolds et al\\.", "year": 2011}, {"title": "Detecting the presence of cyberbullying using computer software.", "author": ["241\u00ad244. Bayzick", "Jennifer", "April Kontostathis", "Lynne Edwards"], "venue": null, "citeRegEx": "Bayzick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bayzick et al\\.", "year": 2011}], "referenceMentions": [], "year": 0, "abstractText": "tech companies and NGOs devoted to fighting bullying on the internet) has begun to address this  issue by creating a new dataset superior to its predecssors in terms of both size and quality. As we  (#HackHarassment) complete further rounds of labelling, later iterations of this dataset will  increase the available samples by at least an order of magnitude, enabling corresponding   improvements in the quality of machine learning models for harassment detection. In this paper, we  introduce the first models built on the #HackHarassment dataset v1.0 (a new open dataset, which   we are delighted to share with any interested researcherss) as a benchmark for future research.", "creator": null}}}