{"id": "1609.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks", "abstract": "a generic meta scalable reinforcement learning scheme for sequential neural communication is presented, providing a general relational learning machine. by reference to reinforcement node threshold learning features are described 1 ) a mechanism for primary reinforcement, capable of solving linearly inseparable problems 2 ) some behavioral scheme is extended to include a mechanism for conditioned reinforcement, capable of forming long term constraints 3 ) the learning scheme is modified toward use a threshold - constrained deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. the model may be used for supervised as well as unsupervised training regimes.", "histories": [["v1", "Mon, 12 Sep 2016 11:23:20 GMT  (1348kb)", "http://arxiv.org/abs/1609.03348v1", null], ["v2", "Sat, 17 Sep 2016 04:20:01 GMT  (1350kb)", "http://arxiv.org/abs/1609.03348v2", null], ["v3", "Tue, 15 Nov 2016 05:11:01 GMT  (1384kb)", "http://arxiv.org/abs/1609.03348v3", null], ["v4", "Sat, 14 Jan 2017 05:54:29 GMT  (1341kb)", "http://arxiv.org/abs/1609.03348v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["thomas h ward"], "accepted": false, "id": "1609.03348"}, "pdf": {"name": "1609.03348.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["thomas.holland.ward@gmail.com "], "sections": [{"heading": null, "text": "A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 providing a general purpose learning machine. By reference to a node threshold three features are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 forming long term strategy 3) The learning scheme is modified to use a thresholdbased deep learning\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 used\u00a0for\u00a0supervised\u00a0as\u00a0well\u00a0as\u00a0unsupervised\u00a0training\u00a0regimes.\u00a0 \u00a0\n1\u00a0Introduction\u00a0 \u00a0 This paper proposes that a general purpose learning machine can be achieved by implementing Reinforcement\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Learning in an Artificial Neural Network (ANN), a method which attempts to emulate the core mechanism of that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 process\u00a0is\u00a0presented.\u00a0\u00a0 \u00a0 AI research has characteristically followed a bottomup approach\u037e focusing on subsystems that address distinct,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 specialized and unrelated problem domains. In contrast the work presented follows a distinctly topdown approach\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attempting to model intelligence as a whole system\u037e a causal agent interacting with the environment [6]. The agent is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 not designed to solve a particular problem, but is instead assigned a reward condition. The reward condition serves\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 as a goal, and in the path a variety of unknown challenges may be present. To solve these problems efficiently the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 agent\u00a0requires\u00a0intelligence.\u00a0 \u00a0 This topdown approach assumes that the core self organizing mechanisms of learning that exist in natural\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 organisms can be replicated in artificial autonomous agents. These can then be scaled up by endowing the agent with\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 more resources (sensors, neurons & motors). Given sufficient resources and learning opportunities an agent may\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 provide a solution to a problem provided one exists. Also given the generalization properties of ANN\u2019s the agent can\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 provide\u00a0appropriate\u00a0responses\u00a0to\u00a0novel\u00a0stimuli.\u00a0\u00a0 \u00a0 A distinction is made between supervised, unsupervised and reinforcement training regimes. Supervised learning\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 regimes use a (human) trainer to assign desired inputoutput pattern pairings. Unsupervised training regimes are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 typically used to cluster a data set into related groups. Reinforcement Learning (RL) may be considered a subtype of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 unsupervised training\u037e it is sometimes called learning with a critic rather than learning with a teacher as the feedback\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 is evaluative (right or wrong) rather than instructive (where a desired output action is prescribed). Significant RL\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 successes\u00a0have\u00a0been\u00a0achieved\u00a0in\u00a0AI\u00a0notably\u00a0with\u00a0the\u00a0use\u00a0of\u00a0Qlearning\u00a0[2][7].\u00a0\u00a0 \u00a0 First\u00a0a\u00a0definition\u00a0of\u00a0intelligence\u00a0is\u00a0required:\u00a0 \u00a0\n1\u00a0\nThe\u00a0demonstration\u00a0of\u00a0beneficial\u00a0behaviors\u00a0acquired\u00a0through\u00a0learning.\u00a0 \u00a0 A beneficial action/behavior being one that would result in a positive survival outcome (eg successful feeding,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mating, self preservation) for the agent. For the most part our inherent internal reward systems encourage us to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 perform beneficial behaviors, but this is not always the case (eg substance abuse may be rewarding but not\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 beneficial). The term \u2018desirable behavior\u2019 is avoided due to existing usage of the term \u2018desired output\u2019 in supervised\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 learning\u00a0schemes.\u00a0 \u00a0 Let\u2019s\u00a0revise\u00a0our\u00a0definition,\u00a0and\u00a0expectation,\u00a0of\u00a0intelligence:\u00a0 \u00a0\nThe\u00a0demonstration\u00a0of\u00a0rewarding\u00a0behaviors\u00a0acquired\u00a0through\u00a0learning.\u00a0 \u00a0\nRewarding behaviors/actions will be selected for reinforcement (ie learnt) over non rewarding ones. Rewarding\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors are those that allow the agent to achieve the reward condition, thereby achieving its goal(s) in an\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceptably efficient manner (eg elapsed time, steps taken, energy expended). Rewarding behaviors may lead to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pleasure, or at least a reduction in pain. Goals are attained by achieving the preestablished reward condition, and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 thereby satiating active desire(s). Behaviors need not be active they may be passive\u037e inaction may lead to reward and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 therefore\u00a0be\u00a0reinforced.\u00a0\n\u00a0 From initial state s t if action a t results in an immediate reward in the subsequent state s t+1 , action a t will be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reinforced. If the same (or similar from generalization) input pattern is encountered the learnt action will be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 performed. This process of learning is termed Primary Reinforcement. Primary reinforcement reward conditions (eg\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hunger\u00a0or\u00a0thirst)\u00a0typically\u00a0drive\u00a0some\u00a0form\u00a0of\u00a0homeostatic,\u00a0adaptive\u00a0control\u00a0function\u00a0for\u00a0the\u00a0agent\u00a0[5][8].\u00a0 \u00a0\n\u00a0 This is in contrast to Secondary/Conditioned Reinforcement where from initial state s t action a t does not result in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 immediate reward in subsequent state s t+1 . If later action a t+1 does result in a reward in state s t+1 , this will reinforce\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 actions a t+1 and a t . The number of actions learnt from start to goal is arbitrary and depends on the relative size of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the\u00a0reward\u00a0relative\u00a0to\u00a0the\u00a0cost\u00a0(or\u00a0pain)\u00a0in\u00a0attaining\u00a0it.\u00a0\u00a0 \u00a0 A learning scheme is proposed that enables an embodied neural network agent to autonomously determine and learn\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 desirable behaviors. The agent may be embodied in a real or artificial environment. Artificial environments may be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelled on physical environments or even abstract problem domains. The environment may be any set of input\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 patterns, however there must be a causal relationship between the output (behavior) of the agent and the subsequent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input\u00a0pattern.\u00a0\u00a0 \u00a0 This\u00a0paper\u00a0presents\u00a0three\u00a0features:\u00a0 \u00a0\n1. A Primary Reinforcement learning scheme is achieved by wrapping a (supervised) backpropagation\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 network\u00a0within\u00a0an\u00a0unsupervised\u00a0framework.\u00a0\u00a0 \u00a0 Primary Reinforcement enables \u2018desirable\u2019 inputoutput mappings (behaviors) to be autonomously and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continuously learnt\u037e this is of particular benefit when a human supervisor is not available / does not know\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 what\u00a0the\u00a0desired\u00a0output\u00a0should\u00a0be,\u00a0or\u00a0when\u00a0the\u00a0environment,\u00a0or\u00a0agent\u00a0itself,\u00a0\u00a0is\u00a0changeable.\u00a0 \u00a0\n2. The\u00a0framework\u00a0is\u00a0then\u00a0extended\u00a0to\u00a0provide\u00a0Conditioned\u00a0(Secondary)\u00a0Reinforcement.\u00a0 \u00a0\nConditioned (Secondary) Reinforcement enables an arbitrarily long sequence of chained behaviors to be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 autonomously\u00a0learnt,\u00a0in\u00a0expectation\u00a0of\u00a0a\u00a0primary\u00a0reward\u00a0condition\u037e\u00a0this\u00a0provides\u00a0long\u00a0term\u00a0strategy.\u00a0\n2\u00a0\n\u00a0 3. An algorithm is described, termed Threshold Assignment of Connections (TAC), that replaces\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nbackpropagation within the framework, this algorithm can conversely also be used in supervised training\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 schemes.\u00a0\n\u00a0 The Threshold Assignment of Connections algorithm provides a biologically inspired alternative to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backpropagation.\u00a0 \u00a0 The examples that follow are focused mainly on target tracking, however this approach may be applied to a wide\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 range of real and abstract problem spaces. The tasks are small in scale and primarily serve as proof of concept.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 While learning rate performance has been documented, the model has not been performance tuned. The intent of this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 work\u00a0is\u00a0to\u00a0establish\u00a0a\u00a0biologically\u00a0plausible\u00a0working\u00a0model\u00a0of\u00a0intelligence\u00a0that\u00a0is\u00a0simple,\u00a0scalable\u00a0and\u00a0generic.\u00a0\u00a0\n\u00a0 \u00a0\n2\u00a0Primary\u00a0Reinforcement\u00a0 \u00a0\nIn this section a learning scheme is described that provides Primary Reinforcement learning in an Artificial Neural\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Network. Weight adjustments using backpropagation[3][9] are traditionally used in supervised learning schemes\u037e\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 that is desired activations (or training sets) are established prior to the learning phase. In this example\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backpropagation will be used in an unsupervised learning scheme\u037e desired activations will be calculated onthefly.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 This approach, termed Threshold Assignment of Patterns (TAP), relies on a threshold to determine the desired\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output\u00a0pattern.\u00a0\u00a0\nThe model presented is fundamentally a hedonistic one, learning is based on pleasure/pain drives. Although reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 results in reinforcement the model is stochastic\u037e punishment results in new candidate behaviors being randomly\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 generated. Sensing utilizes a sensory input pattern and behavior arises from a motor output pattern. Thinking (or\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 processing)\u00a0is\u00a0implemented\u00a0via\u00a0layers\u00a0of\u00a0artificial\u00a0neurons.\u00a0\nThe\u00a0learning\u00a0scheme\u00a0consists\u00a0of\u00a0the\u00a0following\u00a0components:\u00a0 \u00a0\n\u25cf Artificial\u00a0Neural\u00a0Network\u00a0 \u25cf Learning\u00a0Algorithm\u00a0 \u25cf Framework\u00a0\u00a0 \u25cf Environment\u00a0\n\u00a0 The network architecture is that of a familiar multilayer perceptron (MLP)[3]. An input (sensor) pattern representing\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the environment, produces activations that feed forward and result in an output (motor) pattern representing a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behavior.\u00a0Weight\u00a0updates\u00a0are\u00a0derived\u00a0using\u00a0a\u00a0standard\u00a0(supervised)\u00a0back\u00a0propagation\u00a0learning\u00a0algorithm[3][10].\u00a0\u00a0 \u00a0 The framework sits between the network and environment. The framework acts as an interface between network and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 environment, and is responsible for establishing a reward condition as Primary Reinforcer and determining (desired\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output) behavior based on that reward condition. The framework, network and learning algorithm constitute the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 agent.\u00a0\u00a0 \u00a0 Simulated environments consist of input patterns and \u2018physics\u2019 rules. The physics rules take the current (t) network\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output and determine which input pattern is next (t+1) presented to the network. The environment consists of states,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 and\u00a0physics\u00a0rules\u00a0which\u00a0define\u00a0the\u00a0relationship\u00a0between\u00a0states.\u00a0 \u00a0 \u00a0\u00a0 \u00a0\n\u00a0\n3\u00a0\n\u00a0\u00a0\nfig1 State\u00a0\u2018t\u2019\u00a0\n\u00a0\n\u00a0\nfig2 \u00a0State\u00a0\u2018t+1\u2019\u00a0\nBackpropagation requires a set of desirable output patterns to be established. Through learning the desired output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 patterns will inform the network what the required output node activations are for each input pattern. In this model\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 desired output patterns will be dynamically generated onthefly. But how can the agent determine potentially\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 complex desired output patterns from a simple yes/no reward condition? The agent will decide by trial and error, it\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 does\u00a0so\u00a0by\u00a0the\u00a0following\u00a0process\u00a0(Box\u00a01):\u00a0 \u00a0\nProcessing\u00a0steps\u00a0\n1. Input pattern for state \u2018t\u2019 is presented and forward propagated through the network (Box 2).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Node output activation may be in the range 01 (Box 3). A threshold value of 0.5 is assigned\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to\u00a0each\u00a0node\u00a0in\u00a0output\u00a0layer.\u00a0\n2. Agent\u2019s behavior, based on whether output layer activations have exceeded threshold values\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (fig\u00a03),\u00a0determines\u00a0subsequent\u00a0input\u00a0pattern\u00a0\u2018t+1\u2019.\u00a0\n3. Framework\u00a0determines\u00a0\u2018reward\u2019\u00a0value\u00a0based\u00a0on\u00a0\u2018t+1\u2019\u00a0input\u00a0pattern.\u00a0\n4. Weight\u00a0changes\u00a0are\u00a0made:\u00a0\na. Desired\u00a0\u2018t\u2019\u00a0activations\u00a0at\u00a0the\u00a0output\u00a0layer\u00a0are\u00a0calculated:\u00a0\ni. If reward condition then the desired \u2018t\u2019 activation for that node is set to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 maximal value (1.0) for those that were above threshold, and set to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 minimal\u00a0value\u00a0(0.0)\u00a0for\u00a0those\u00a0that\u00a0were\u00a0below\u00a0threshold.\u00a0\nii. If no reward then the desired \u2018t\u2019 activation for all nodes are random\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 moderate\u00a0values\u00a0[0.45..0.55].\u00a0\nb. Weight changes for state \u2018t\u2019 are made according to error values which are back\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 propagated\u00a0through\u00a0the\u00a0network\u00a0(Box\u00a04).\u00a0\n4\u00a0\n\u00a0 Box\u00a01\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Patterns\u00a0processing\u00a0steps\u00a0\nDesired activation values are set depending on whether a reward or punishment occurred after the output response.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Thresholds for neuron activation are widely found in animals, where sufficient \u2018excitation\u2019 is required to result in an\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 electrical action potential that can be signalled to other neurons [11]. If a reward condition occurred all actual\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activations above threshold (eg fig 4 node 1) will be reinforced by setting the corresponding desired activation to 1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 and all activations below threshold (eg fig 4 node 2) will be reinforced by setting the corresponding desired\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activation to 0.0. If a punishment condition occurred all actual activations, above or below threshold, will be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 weakened\u00a0by\u00a0setting\u00a0desired\u00a0activations\u00a0to\u00a0near\u00a0threshold\u00a0values.\u00a0\n\u00a0\nfig3 Logistic\u00a0Activation\u00a0Function\u00a0with\u00a0threshold\u00a0\n\u00a0\nInput\u00a0activation\u00a0for\u00a0unit\u00a0u.\u00a0\n=\u00a0etinputn u \u2211 \u00a0\ni eightw ui ai \u00a0\n\u00a0\n\u00a0 Box\u00a02\u00a0\u00a0Input\u00a0activation\u00a0\n\u00a0\nOutput\u00a0activation\u00a0for\u00a0unit\u00a0u.\u00a0 =\u00a0au 11\u00a0+\u00a0e\u2212netinputu \u00a0\n\u00a0\n\u00a0 Box\u00a03\u00a0\u00a0Logistic\u00a0Activation\u00a0Function\u00a0\n\u00a0\n1) Derive delta error value for an output layer node u, by finding difference between desired activation ( ) and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 du \u00a0 \u00a0 actual\u00a0activation\u00a0( ).au \u00a0 =(  ) )eltad u du au au 1( \u2212au \u00a0 \u00a0 2)\u00a0Derive\u00a0weight\u00a0change\u00a0for\u00a0connection\u00a0between\u00a0a\u00a0hidden\u00a0unit\u00a0h\u00a0and\u00a0an\u00a0output\u00a0unit\u00a0u,\u00a0using\u00a0learning\u00a0rate.\u00a0 \u00a0\n=\u00a0weight\u0394 uh rate\u00a0deltal u ah \u00a0 \u00a0\n3)\u00a0Derive\u00a0delta\u00a0error\u00a0value\u00a0for\u00a0a\u00a0hidden\u00a0unit\u00a0h,\u00a0using\u00a0weighted\u00a0sum\u00a0of\u00a0all\u00a0units\u00a0in\u00a0output\u00a0layer\u00a0.\u00a0\n5\u00a0\n\u00a0 =\u00a0eltad h ah 1\u00a0 a )( \u2212 \u00a0 h \u2211 \u00a0\nu eltad u eightw uh \u00a0\n\u00a0 4)\u00a0Derive\u00a0weight\u00a0change\u00a0for\u00a0connection\u00a0between\u00a0an\u00a0input\u00a0unit\u00a0i\u00a0and\u00a0a\u00a0hidden\u00a0unit\u00a0h,\u00a0using\u00a0learning\u00a0rate.\u00a0 \u00a0 =\u00a0weight\u0394 hi rate\u00a0deltal h ai \u00a0 \u00a0\n\u00a0 Box\u00a04\u00a0\u00a0Backpropagation\u00a0weight\u00a0update\u00a0\nIn this way randomized desired out patterns will generate a new candidate behavior on the next presentation of that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 same (or similar) stimulus. In effect the response has been established before it is first manifested, and this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rewarding response will be reinforced on future presentations. Conversely nonrewarding behaviors will be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 destroyed\u00a0in\u00a0favour\u00a0of\u00a0a\u00a0new\u00a0candidate\u00a0behavior.\u00a0Akin\u00a0to\u00a0natural\u00a0selection,\u00a0only\u00a0rewarding\u00a0behaviors\u00a0will\u00a0survive.\u00a0 \u00a0\n\u00a0\nfig4 \u00a0\u00a0Desired\u00a0activation\u00a0:\u00a0Punishment\u00a0vs\u00a0Reward\u00a0\nA mapping is formed from an input pattern stimuli to an output pattern motor response, dependent on the reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 condition that follows. No distinction is made between learning and testing phase\u037e that is the agent in a continual\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 process of learning and evaluation. A behavior is deemed to have been learnt when all output node activations are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mature\u00a0(eg\u00a0above\u00a00.9\u00a0or\u00a0less\u00a0than\u00a00.1)\u00a0for\u00a0a\u00a0given\u00a0input\u00a0pattern\u00a0and\u00a0results\u00a0in\u00a0a\u00a0reward.\u00a0\n2.1\u00a0Example:\u00a0Target\u00a0tracking\u00a0(part\u00a0I)\u00a0 \u00a0\n2.1.1\u00a0Problem\u00a0description\u00a0\nIn\u00a0this\u00a0example\u00a0the\u00a0agent\u00a0must\u00a0autonomously\u00a0learn\u00a0how\u00a0to\u00a0track\u00a0a\u00a0target\u00a0(fig\u00a05).\u00a0\n6\u00a0\n\u00a0 fig5 Target\u00a0tracking\u00a0example\u00a0start\u00a0state\u00a0 \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nby\u00a0moving\u00a0it\u00a0to\u00a0the\u00a0centre\u00a0cell.\u00a0 \u25cf The network is rewarded only if it moves the target to it\u2019s centremost cell. Once the target is moved to the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\ncentremost\u00a0cell\u00a0it\u00a0is\u00a0moved\u00a0to\u00a0a\u00a0new\u00a0starting\u00a0position\u00a0on\u00a0the\u00a0grid.\u00a0 \u25cf The agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nuntil\u00a0it\u00a0does\u00a0so.\u00a0 \u25cf The\u00a0agent\u00a0may\u00a0only\u00a0move\u00a0the\u00a0target\u00a01\u00a0step\u00a0(ie\u00a0to\u00a0an\u00a0adjacent\u00a0cell)\u00a0in\u00a0each\u00a0iteration.\u00a0 \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nrequired.\u00a0\u00a0\n\u00a0\n2.1.2\u00a0Network\u00a0configuration\u00a0\nA\u00a0network\u00a0was\u00a0created\u00a0(table\u00a01)(fig\u00a06):\u00a0 \u00a0\nInput\u00a0layer\u00a0 9 input nodes\u037e the centre node is designated as a special \u2018reward\u2019\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node\u00a0\nHidden\u00a0layer\u00a0 12\u00a0hidden\u00a0nodes\u00a0\nOutput\u00a0layer\u00a0 4\u00a0motor\u00a0nodes\u00a0\nNotes\u00a0 Each\u00a0layer\u00a0fully\u00a0connected\u00a0to\u00a0the\u00a0next\u00a0via\u00a0weights.\u00a0 Weights\u00a0initially\u00a0randomised.\u00a0 Each\u00a0hidden\u00a0and\u00a0output\u00a0nodes\u00a0assigned\u00a0an\u00a0exclusive\u00a0bias\u00a0unit.\u00a0 Learning\u00a0rate\u00a0=\u00a01.0\u00a0\n\u00a0\nTable\u00a01\u00a0Target\u00a0tracking\u00a0network\u00a0configuration\u00a0\n7\u00a0\n\u00a0 fig6 Target\u00a0tracking\u00a0network\u00a0topology\u00a0\n\u00a0\nThe\u00a0nine\u00a0input\u00a0nodes\u00a0are\u00a0mapped\u00a0to\u00a0each\u00a0cell\u00a0in\u00a0the\u00a0grid\u00a0(fig7):\u00a0\n\u00a0\nfig7 Target\u00a0tracking\u00a0input\u00a0mapping\u00a0\nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the agent is able to move the target within the grid. In order for the agent to move the target in any given direction\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Therefore\u00a0there\u00a0is\u00a0only\u00a0a\u00a01in16\u00a0chance\u00a0(24)\u00a0the\u00a0agent\u00a0will\u00a0move\u00a0in\u00a0the\u00a0correct\u00a0direction\u00a0by\u00a0chance.\u00a0\nThe framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 zero\u00a0if\u00a0it\u00a0is\u00a0to\u00a0be\u00a0punished.\u00a0A\u00a0reward\u00a0occurs\u00a0if\u00a0the\u00a0target\u00a0is\u00a0moved\u00a0to\u00a0the\u00a0centre\u00a0cell\u00a0(fig\u00a08):\u00a0 \u00a0\n\u00a0\u00a0\u00a0 \u00a0\n8\u00a0\nfig8 Target\u00a0tracking\u00a0reward\u00a0condition\u00a0\nThe behavior, or output response of the network, will causally influence the subsequent input pattern. Reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 feedback will be given to the network, thereby informing whether the output was \u2018correct\u2019. The output patterns are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 not predetermined. The network is presented a pattern and produces a response behavior. If the behavior was\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rewarding then a stronger version of the actual output is assigned as the desired output pattern. If the behavior was\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 not rewarding a random pattern is set as the desired output pattern. A threshold is required at the output layer to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 make\u00a0this\u00a0decision.\u00a0 \u00a0 2.1.3\u00a0Results\u00a0 \u00a0 Initially the agent moves the target randomly. Activations tend to be weak (eg 0.4  0.6) across all output nodes.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 When\u00a0rewarding\u00a0behaviors\u00a0are\u00a0discovered\u00a0these\u00a0are\u00a0further\u00a0reinforced\u00a0and\u00a0the\u00a0output\u00a0matures\u00a0(eg\u00a0<\u00a00.1,\u00a0>\u00a00.9).\u00a0 \u00a0 With sufficient exposure the network learns the optimal behavior to achieve a reward\u037e consistently moving one step\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 left/right/up/down towards the food reward from starting locations (cells 1,3,5,7) (table 2). If the target is placed\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 directly\u00a0on\u00a0the\u00a0centre\u00a0cell\u00a0it\u00a0will\u00a0remain\u00a0stationary.\u00a0 \u00a0\n\u00a0\nScheme\u00a0 #\u00a0pattern\u00a0presentations\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Patterns\u00a0 (Unsupervised\u00a0backpropagation)\u00a0\n116816\u00a0\n\u00a0 Table\u00a02\u00a0Target\u00a0tracking\u00a0results\u00a0\nHowever, the agent is unable to learn how to move the target when placed in corners (cells 0,2,6,8). The physics\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rules prevent the agent from moving the target diagonally in one step, instead two steps are required. Whilst the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 agent was able to solve this \u20181step\u2019 solution in 116816 presentations, it spent much of it\u2019s time \u2018stuck\u2019 in corner\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cells. The agent can only reinforce behaviors where there is an immediate reward in the subsequent input pattern.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 The network is unable to solve the \u2018temporal credit assignment\u2019 problem. In order to learn two or more consecutive\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors\u00a0secondary\u00a0(conditioned)\u00a0reinforcement\u00a0is\u00a0required\u00a0(fig\u00a09).\u00a0 \u00a0 \u00a0\n\u00a0 \u00a0\nfig9 Primary\u00a0Reinforcement\u00a0partial\u00a0solution\u00a0for\u00a0target\u00a0tracking\u00a0task\u00a0\nNote\u037e In this example the reward condition was facilitated by assigning an existing input layer node as the \u2019Reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Node\u2019. However, the reward condition could be evaluated against a combination of existing input layer nodes, a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 separate\u00a0input\u00a0layer\u00a0node(s),\u00a0or\u00a0even\u00a0no\u00a0node\u00a0at\u00a0all\u00a0(see\u00a0XOR\u00a0example\u00a0below).\u00a0\u00a0\n\u00a0\n2.2\u00a0Example:\u00a0XOR\u00a0 2.2.1\u00a0Problem\u00a0description\u00a0\nThe agent will be required to solve the XOR problem (table 3) in order to test the network's ability to map non linear\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n9\u00a0\ntransformations that require multiple layers of neurons. This test will will also provide a performance comparison\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 between\u00a0unsupervised\u00a0reinforcement\u00a0learning\u00a0and\u00a0the\u00a0supervised\u00a0regime.\u00a0\u00a0 \u00a0\n\u00a0\n(input)\u00a0 A\u00a0 (input)\u00a0 B\u00a0\n(output)\u00a0 XOR\u00a0\n0\u00a0 0\u00a0 0\u00a0\n0\u00a0 1\u00a0 1\u00a0\n1\u00a0 0\u00a0 1\u00a0\n1\u00a0 1\u00a0 0\u00a0\n\u00a0 Table\u00a03\u00a0XOR\u00a0task\u00a0\nAny input pattern can be considered an environment, and any output pattern a behavior. Thus any set of mappings\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 can be learnt as they would under a conventional supervised learning regime. In contrast to the previous experiment,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the network will now receive controlled exposure to all input patterns in turn. The behavior, or output response of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the network will not influence the subsequent input pattern. However, reward feedback will be given to the network\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 thereby informing whether the output was correct according to the desired pattern in the supervised training set. This\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tightly\u00a0controlled\u00a0presentation\u00a0of\u00a0input\u00a0patterns\u00a0is\u00a0termed\u00a0\u2018guided\u2019.\u00a0 \u00a0 For clarity, and to allow for a close comparison with the supervised backpropagation training regime, no explicit\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reward node will be established in the network topology. The framework is still responsible for setting the reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 condition. To allow exposure to all the input patterns they will be cycled through in sequence. The framework will\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 evaluate\u00a0the\u00a0output\u00a0and\u00a0decide\u00a0whether\u00a0it\u00a0should\u00a0be\u00a0reinforced\u00a0or\u00a0not.\u00a0 \u00a0 2.2.2\u00a0Network\u00a0configuration\u00a0 \u00a0 A\u00a0network\u00a0was\u00a0created\u00a0(table\u00a04)(fig\u00a010):\u00a0\nInput\u00a0layer\u00a0 2\u00a0input\u00a0nodes\u00a0\nHidden\u00a0layer\u00a0 3\u00a0hidden\u00a0nodes\u00a0\nOutput\u00a0layer\u00a0 1\u00a0output\u00a0node\u00a0\nNotes\u00a0 Each\u00a0layer\u00a0fully\u00a0connected\u00a0to\u00a0the\u00a0next\u00a0via\u00a0weights.\u00a0 Weights\u00a0initially\u00a0randomised.\u00a0 Each\u00a0hidden\u00a0and\u00a0output\u00a0nodes\u00a0assigned\u00a0an\u00a0exclusive\u00a0bias\u00a0unit.\u00a0 Learning\u00a0rate\u00a0=\u00a01.0\u00a0\n\u00a0\nTable\u00a04\u00a0XOR\u00a0network\u00a0configuration\u00a0\n\u00a0\n10\u00a0\n\u00a0 \u00a0\nfig10 XOR\u00a0network\u00a0topology\u00a0\n\u00a0 2.2.3\u00a0Results\u00a0 \u00a0\n\u00a0\nScheme\u00a0 #\u00a0pattern\u00a0presentations\u00a0\nSupervised\u00a0backpropagation\u00a0 2182\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Patterns\u00a0\u00a0 (unsupervised\u00a0backpropagation)\u00a0\n7550\u00a0\n\u00a0 Table\u00a05\u00a0XOR\u00a0results\u00a0\nThe Threshold Assignment of Patterns (unsupervised backpropagation) regime required significantly more pattern\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 presentations to learn the XOR solution than the supervised regime (table 5). Both were using the same set of initial\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 weights and learning parameters (learning rate etc). Both are using the same backpropagation algorithm to perform\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 weight updates. The difference in performance can be attributed to the manner in which desired output patterns are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 provided. In the supervised scheme the desired patterns are known a priori , in the unsupervised (guided) learning\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 regime these must be discovered by the network through trial and error. This performance gap would be expected\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to\u00a0widen\u00a0as\u00a0the\u00a0number\u00a0of\u00a0nodes\u00a0in\u00a0the\u00a0output\u00a0layer\u00a0grows\u00a0and\u00a0with\u00a0it\u00a0the\u00a0number\u00a0of\u00a0candidate\u00a0combinations.\u00a0\n\u00a0\n2.3\u00a0Summary\u00a0\u00a0 Unsupervised Primary Reinforcement can be achieved, with reference to a threshold, by dynamically generating\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 desired output activations and feeding these into a supervised learning algorithm. These experiments demonstrate\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the network's ability to learn and map arbitrary sets of patterns by reward, thereby producing behaviors that allow it\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to reach it\u2019s goal in an efficient manner. Primary Reinforcement may solve problems that are that are linearly\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inseparable. It is capable of training weights deep within the network, thereby capable of forming complex abstract\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 representations.\u00a0\nThe input patterns can be presented in any order. It can be exposed to all potential input patterns found in the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 environment. However, in order for the network to identify which output patterns (or behaviors) were correct, the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 network must be presented with subsequent reward feedback. The unsupervised Reinforcement Learning regime\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requires more iterations to learn compared to the supervised regime. The principal reason being that the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 unsupervised must discover the \u2018correct\u2019 solution through trial and error. And even when agent does not receive\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reinforcement,\u00a0the\u00a0new\u00a0random\u00a0candidate\u00a0behavior\u00a0may\u00a0be\u00a0a\u00a0repeat\u00a0of\u00a0a\u00a0prior\u00a0incorrect\u00a0one.\u00a0\n11\u00a0\nThe unsupervised Reinforcement Learning regime is limited to a binary maximal off/on (0 or 1) activation on each\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node.\u00a0Conversely\u00a0the\u00a0supervised\u00a0regime\u00a0may\u00a0set\u00a0desired\u00a0activations\u00a0of\u00a0any\u00a0value\u00a0(0..1).\u00a0\nThe unsupervised Reinforcement Learning regime is capable of finding solutions and adapting to change without\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the aid of a trainer. Alternate behaviors will be adopted in the face of unforeseen environmental hazards, or if the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 agent\u00a0suffers\u00a0damage.\u00a0Additional\u00a0sensors\u00a0and\u00a0motors\u00a0can\u00a0be\u00a0added\u00a0to\u00a0the\u00a0agent\u00a0without\u00a0requiring\u00a0a\u00a0trainer.\u00a0\nPrimary Reinforcement may only learn a single behavior sequence, therefore it is not suitable for acquiring long\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 term\u00a0strategy.\u00a0Problems\u00a0requiring\u00a0long\u00a0term\u00a0strategy\u00a0must\u00a0use\u00a0Secondary/Conditioned\u00a0Reinforcement.\u00a0\n\u00a0\n3\u00a0Secondary/Conditioned\u00a0Reinforcement\u00a0 Primary reinforcement provides a generic method of autonomously establishing beneficial output responses. But it\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 has a significant limitation\u037e it can only provide a one step mapping from start state(s) to goal. A more useful feature\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 is the ability to establish a series of behavior steps leading from start state(s) to a goal. This is the benefit provided\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 by Conditioned Reinforcement. The term \u2018Conditioned\u2019 Reinforcement is preferred over that of \u2018Secondary\u2019\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Reinforcement, since the latter may imply chained behaviors only 2 steps deep. In fact the number of chained\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors can be arbitrarily deep, depending on the strength and decay of the reward. This approach, termed\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Threshold Assignment of Rewards (TAR), relies on building an association between a rewarding stimulus and an\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 internal\u00a0proxy\u00a0reward.\u00a0\nIn Primary Reinforcement a mapping is established between an input pattern stimuli to an output pattern motor\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response. In Secondary Reinforcement a mapping is established between an input pattern stimuli to an output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern motor response AND a reward node. With sufficient reinforcement the response activation on the reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node matures\u037e the reward node is now available as a proxy reward condition (secondary/conditioned reinforcer) for\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 a given input pattern. In turn this conditioned reinforcer can help to create further conditioned reinforcers, that are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activated\u00a0only\u00a0in\u00a0response\u00a0to\u00a0a\u00a0recognized\u00a0input\u00a0pattern\u00a0stimuli.\u00a0\nThe first conditioned response to be learnt will be closest to the primary reinforcer. Thereafter a chain of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conditioned reinforcers can be established, a series of \u2018breadcrumbs\u2019 that lead ultimately to the goal. Using this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mechanism\u00a0planned\u00a0or\u00a0goal\u00a0oriented\u00a0tasks\u00a0can\u00a0be\u00a0solved.\u00a0\nThis approach is intended to overcome the temporal \u2018hill climbing\u2019 limitation outlined in the previous example. It\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 is based on the a similar architecture and learning rule as before but with one important addition: a special reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node is added to the output layer. Unlike other nodes in the output layer the special reward node is not a motor\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 neuron. Once this becomes mature it behaves like an input reward node\u037e deciding which behaviors should be learnt.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Behaviors leading to a distant reward can be chained together. Essentially mapping are now formed between input\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 patterns\u00a0and\u00a0rewards,\u00a0rather\u00a0than\u00a0just\u00a0input\u00a0patterns\u00a0and\u00a0motor\u00a0nodes\u00a0(Box\u00a05).\u00a0\n\u00a0\nProcessing\u00a0steps\u00a0 \u00a0 Differences\u00a0to\u00a0the\u00a0Primary\u00a0Reinforcement\u00a0process\u00a0described\u00a0previously\u00a0are\u00a0highlighted\u00a0in\u00a0bold.\u00a0 \u00a0\n1. Input pattern for state \u2018t\u2019 is presented and forward propagated through the network (Box 2).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Node output activation may be in the range 01 (Box 3). A threshold value of 0.5 is assigned\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to\u00a0each\u00a0node\u00a0in\u00a0output\u00a0layer.\u00a0\n2. Agent\u2019s behavior, based on whether output layer activations have exceeded threshold values\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (fig\u00a03),\u00a0determines\u00a0subsequent\u00a0input\u00a0pattern\u00a0\u2018t+1\u2019.\u00a0\n12\u00a0\n3. Framework determines \u2018reward\u2019 value based on \u2018t+1\u2019 input pattern \u2018t+1\u2019 and on activation\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 of\u00a0special\u00a0output\u00a0reward\u00a0node\u00a0(eg\u00a0>\u00a00.8\u00a0).\u00a0\n4. Weight\u00a0changes\u00a0are\u00a0made:\u00a0\na. Desired\u00a0\u2018t\u2019\u00a0activations\u00a0at\u00a0the\u00a0output\u00a0layer\u00a0are\u00a0calculated:\u00a0\ni. If reward condition then the desired \u2018t\u2019 activation for that node is set to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 maximal value (1.0) for those that were above threshold, and set to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 minimal value (0.0) for those that were below threshold. Set the desired\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activation\u00a0for\u00a0the\u00a0special\u00a0output\u00a0reward\u00a0node\u00a0to\u00a095%\u00a0of\u00a0reward\u00a0value.\u00a0\nii. If no reward then the desired \u2018t\u2019 activation for all nodes are random\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 moderate values [0.45..0.55].Set the desired activation for the special\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output\u00a0reward\u00a0node\u00a0to\u00a0minimal\u00a0value\u00a0(0.0).\u00a0\nb. Weight changes for state \u2018t\u2019 are made according to error values which are back\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 propagated\u00a0through\u00a0the\u00a0network\u00a0(Box\u00a04).\u00a0\n\u00a0 Note:\u00a0To\u00a0achieve\u00a0this\u00a0both\u00a0the\u00a0current\u00a0and\u00a0previous\u00a0activations\u00a0must\u00a0be\u00a0stored.\u00a0Weight\u00a0changes\u00a0are\u00a0 derived\u00a0using\u00a0back\u00a0propagation\u00a0on\u00a0previous\u00a0activations.\u00a0\n\u00a0 Box\u00a05\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Rewards\u00a0processing\u00a0steps\u00a0\n3.1\u00a0Example:\u00a0Target\u00a0tracking\u00a0(part\u00a0II)\u00a0 The same problem as described in section 3.1.1 is revisited. In this example the agent is equipped with the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources\u00a0required\u00a0to\u00a0facilitate\u00a0Secondary\u00a0(Conditioned)\u00a0Reinforcement.\u00a0\n\u00a0\n3.1.1\u00a0Problem\u00a0description\u00a0\nIn\u00a0this\u00a0example\u00a0the\u00a0agent\u00a0must\u00a0autonomously\u00a0learn\u00a0how\u00a0to\u00a0track\u00a0a\u00a0target\u00a0(fig\u00a011).\u00a0\n\u00a0\nfig11 Target\u00a0tracking\u00a0example\u00a0start\u00a0state\u00a0 \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nby\u00a0moving\u00a0it\u00a0to\u00a0the\u00a0centre\u00a0cell.\u00a0 \u25cf The network is rewarded only if it moves the target to it\u2019s centremost cell. Once the target is moved to the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\ncentremost\u00a0cell\u00a0it\u00a0is\u00a0moved\u00a0to\u00a0a\u00a0new\u00a0starting\u00a0position\u00a0on\u00a0the\u00a0grid.\u00a0 \u25cf The agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nuntil\u00a0it\u00a0does\u00a0so.\u00a0\n13\u00a0\n\u25cf The\u00a0agent\u00a0may\u00a0only\u00a0move\u00a0the\u00a0target\u00a01\u00a0step\u00a0(ie\u00a0to\u00a0an\u00a0adjacent\u00a0cell)\u00a0in\u00a0each\u00a0iteration.\u00a0 \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nrequired.\u00a0\u00a0\n\u00a0\n3.1.2\u00a0Network\u00a0configuration\u00a0\nA\u00a0network\u00a0was\u00a0created\u00a0(table\u00a06)(fig\u00a012):\u00a0\nInput\u00a0layer\u00a0 9 input nodes\u037e the centre node is designated as a special \u2018reward\u2019\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node\u00a0\nHidden\u00a0layer\u00a0 12\u00a0hidden\u00a0nodes\u00a0\nOutput\u00a0layer\u00a0 4\u00a0motor\u00a0nodes\u00a0+\u00a01\u00a0reward\u00a0node\u00a0\nNotes\u00a0 Each\u00a0layer\u00a0fully\u00a0connected\u00a0to\u00a0the\u00a0next\u00a0via\u00a0weights.\u00a0 Weights\u00a0initially\u00a0randomised.\u00a0 Each\u00a0hidden\u00a0and\u00a0output\u00a0nodes\u00a0assigned\u00a0an\u00a0exclusive\u00a0bias\u00a0unit.\u00a0 Learning\u00a0rate\u00a0=\u00a01.0\u00a0\nTable\u00a06\u00a0Target\u00a0tracking\u00a0network\u00a0configuration\u00a0\n\u00a0\n\u00a0\n\u00a0 fig12 \u00a0\n\u00a0\n14\u00a0\nThe\u00a0nine\u00a0input\u00a0nodes\u00a0are\u00a0mapped\u00a0to\u00a0each\u00a0cell\u00a0in\u00a0the\u00a0grid\u00a0(fig\u00a013):\u00a0\n\u00a0\nfig13 Target\u00a0tracking\u00a0input\u00a0mapping\u00a0\nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the agent is able to move the target within the grid. In order for the agent to move the target in any given direction\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Therefore\u00a0there\u00a0is\u00a0only\u00a0a\u00a01in16\u00a0chance\u00a0(24)\u00a0the\u00a0agent\u00a0will\u00a0move\u00a0in\u00a0the\u00a0correct\u00a0direction\u00a0by\u00a0chance.\u00a0\nThe framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 zero\u00a0if\u00a0it\u00a0is\u00a0to\u00a0be\u00a0punished.\u00a0A\u00a0reward\u00a0occurs\u00a0if\u00a0the\u00a0target\u00a0is\u00a0moved\u00a0to\u00a0the\u00a0centre\u00a0cell\u00a0(fig\u00a014):\u00a0 \u00a0\n\u00a0\nfig14 Target\u00a0tracking\u00a0reward\u00a0condition\u00a0\nThe\u00a0network\u00a0is\u00a0also\u00a0assigned\u00a0an\u00a0additional\u00a0special\u00a0reward\u00a0node\u00a0in\u00a0the\u00a0output\u00a0layer.\u00a0A\u00a0reward\u00a0also\u00a0occurs\u00a0if\u00a0the\u00a0 activation\u00a0of\u00a0this\u00a0output\u00a0reward\u00a0node\u00a0is\u00a0above\u00a0a\u00a0given\u00a0criteria\u00a0(eg\u00a00.8),\u00a0which\u00a0indicates\u00a0it\u00a0has\u00a0been\u00a0previously\u00a0 reinforced.\u00a0If\u00a0the\u00a0subsequent\u00a0input\u00a0reward\u00a0node\u00a0or\u00a0subsequent\u00a0output\u00a0reward\u00a0node\u00a0is\u00a0activated,\u00a0connections\u00a0to\u00a0the\u00a0 output\u00a0reward\u00a0node\u00a0and\u00a0motor\u00a0output\u00a0nodes\u00a0will\u00a0be\u00a0reinforced.\u00a0 \u00a0\nNote\u037e In this example a discrete special reward node was added to the output layer. It is possible to to have no\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dedicated reward node at output layer, instead it possible to test if activations of all motor neurons are high (and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 have therefore matured). Consequently activations, connections and therefore the strength of memories will be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 proportional\u00a0to\u00a0the\u00a0reward.\u00a0\n\u00a0 3.1.3\u00a0Results\u00a0 \u00a0 Initially the agent moves the target randomly. Activations tend to be weak (eg 0.4  0.6) across all output nodes.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 When\u00a0rewarding\u00a0behaviors\u00a0are\u00a0discovered\u00a0these\u00a0are\u00a0further\u00a0reinforced\u00a0and\u00a0the\u00a0output\u00a0matures\u00a0(eg\u00a0<\u00a00.1,\u00a0>\u00a00.9).\u00a0 \u00a0\n\u00a0\nScheme\u00a0 #\u00a0pattern\u00a0presentations\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Reward\u00a0 (Unsupervised\u00a0backpropagation)\u00a0\n110324\u00a0\n\u00a0\n15\u00a0\nTable\u00a07\u00a0Target\u00a0tracking\u00a0results\u00a0 \u00a0 \u00a0\n\u00a0 \u00a0\nfig15 Conditioned\u00a0Reinforcement\u00a0full\u00a0solution\u00a0for\u00a0target\u00a0tracking\u00a0task\u00a0\nThe agent is able to learn how to move the target when placed in corners (cells 0,2,6,8) (table 7). The agent has\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 effectively established proxy rewards in intermediate locations (cells 1,3,5,7) allowing chained sequences of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors to be learned (fig 15). The chaining of sequences of behaviors enables long term strategy to be acquired,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this\u00a0is\u00a0demonstrated\u00a0more\u00a0substantially\u00a0in\u00a0the\u00a0following\u00a0maze\u00a0navigation\u00a0problem.\u00a0\n3.2\u00a0Example:\u00a0Maze\u00a0navigation\u00a0\n3.2.1\u00a0Problem\u00a0description\u00a0\nIn\u00a0this\u00a0example\u00a0the\u00a0agent\u00a0must\u00a0autonomously\u00a0learn\u00a0how\u00a0to\u00a0navigate\u00a0a\u00a0target\u00a0through\u00a0a\u00a0maze\u00a0(fig\u00a016).\u00a0\n\u00a0\n\u00a0\nfig16 Maze\u00a0task\u00a0start\u00a0state\u00a0 \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nby\u00a0moving\u00a0it\u00a0the\u00a0top\u00a0centre\u00a0cell.\u00a0 \u25cf Once the target is moved to the target cell (cell 1) it is moved back to it\u2019s original starting position on the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\ngrid\u00a0(cell\u00a03).\u00a0 \u25cf The\u00a0agent\u00a0has\u00a0no\u00a0prior\u00a0knowledge.\u00a0It\u00a0does\u00a0not\u00a0know\u00a0that\u00a0food\u00a0will\u00a0lead\u00a0to\u00a0a\u00a0reward\u00a0until\u00a0it\u00a0happens.\u00a0 \u25cf The\u00a0agent\u00a0may\u00a0only\u00a0move\u00a0the\u00a0target\u00a01\u00a0step\u00a0(ie\u00a0to\u00a0an\u00a0adjacent\u00a0cell)\u00a0in\u00a0each\u00a0iteration.\u00a0 \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nrequired.\u00a0\u00a0 \u25cf To further increase task difficulty there is an invisible barrier between the start state and the goal. The agent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nmust\u00a0learn\u00a0to\u00a0navigate\u00a0around\u00a0the\u00a0barrier.\u00a0\n\u00a0\n3.2.2\u00a0Network\u00a0configuration\u00a0\n\u00a0 A\u00a0network\u00a0was\u00a0created\u00a0(table\u00a08)\u00a0(fig\u00a017):\u00a0\n16\u00a0\nInput\u00a0layer\u00a0 9 input nodes\u037e the centre node is designated as a special \u2018reward\u2019\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node\u00a0\nHidden\u00a0layer\u00a0 12\u00a0hidden\u00a0nodes\u00a0\nOutput\u00a0layer\u00a0 4\u00a0motor\u00a0nodes\u00a0+\u00a01\u00a0reward\u00a0node\u00a0\nNotes\u00a0 Each\u00a0layer\u00a0fully\u00a0connected\u00a0to\u00a0the\u00a0next\u00a0via\u00a0weights.\u00a0 Weights\u00a0initially\u00a0randomised.\u00a0 Each\u00a0hidden\u00a0and\u00a0output\u00a0nodes\u00a0assigned\u00a0an\u00a0exclusive\u00a0bias\u00a0unit.\u00a0 Learning\u00a0rate\u00a0=\u00a01.0\u00a0\n\u00a0\nTable\u00a08\u00a0Maze\u00a0task\u00a0network\u00a0configuration\u00a0\n\u00a0 fig17 Maze\u00a0task\u00a0network\u00a0topology\u00a0\n\u00a0 \u00a0\nThe\u00a0nine\u00a0input\u00a0nodes\u00a0are\u00a0mapped\u00a0to\u00a0each\u00a0cell\u00a0in\u00a0the\u00a0grid\u00a0(fig\u00a018):\u00a0\n\u00a0\n17\u00a0\nfig18 Maze\u00a0task\u00a0input\u00a0mapping\u00a0\nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the agent is able to move the target within the grid. In order for the agent to move the target in any given direction\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Therefore\u00a0there\u00a0is\u00a0only\u00a0a\u00a01in16\u00a0chance\u00a0(24)\u00a0the\u00a0agent\u00a0will\u00a0move\u00a0in\u00a0the\u00a0correct\u00a0direction\u00a0by\u00a0chance.\u00a0\nThe framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 zero\u00a0if\u00a0it\u00a0is\u00a0to\u00a0be\u00a0punished.\u00a0A\u00a0reward\u00a0occurs\u00a0if\u00a0the\u00a0target\u00a0is\u00a0moved\u00a0to\u00a0the\u00a0top\u00a0centre\u00a0cell\u00a0(cell\u00a01).\u00a0 \u00a0 The\u00a0network\u00a0is\u00a0also\u00a0assigned\u00a0an\u00a0additional\u00a0special\u00a0reward\u00a0node\u00a0in\u00a0the\u00a0output\u00a0layer.\u00a0A\u00a0reward\u00a0also\u00a0occurs\u00a0if\u00a0the\u00a0 activation\u00a0of\u00a0this\u00a0output\u00a0reward\u00a0node\u00a0is\u00a0above\u00a0a\u00a0given\u00a0criteria\u00a0(eg\u00a00.8),\u00a0which\u00a0indicates\u00a0it\u00a0has\u00a0been\u00a0previously\u00a0 reinforced.\u00a0Connections\u00a0to\u00a0the\u00a0output\u00a0reward\u00a0node\u00a0will\u00a0be\u00a0reinforced\u00a0in\u00a0the\u00a0same\u00a0manner\u00a0as\u00a0motor\u00a0output\u00a0\u00a0nodes\u00a0if\u00a0 the\u00a0subsequent\u00a0input\u00a0reward\u00a0node\u00a0or\u00a0subsequent\u00a0output\u00a0reward\u00a0node\u00a0is\u00a0activated.\u00a0 \u00a0 \u00a0 3.2.3\u00a0Results\u00a0 \u00a0 Initially\u00a0the\u00a0network\u00a0moves\u00a0randomly\u00a0through\u00a0the\u00a0environment.\u00a0In\u00a0time\u00a0the\u00a0agent\u00a0is\u00a0able\u00a0to\u00a0learn\u00a0how\u00a0to\u00a0move\u00a0the\u00a0 target\u00a0around\u00a0corners.\u00a0The\u00a0physics\u00a0rules\u00a0prevent\u00a0the\u00a0agent\u00a0from\u00a0moving\u00a0the\u00a0target\u00a0diagonally\u00a0in\u00a0one\u00a0step,\u00a0instead\u00a0two\u00a0 steps\u00a0are\u00a0required.\u00a0The\u00a0agent\u00a0can\u00a0only\u00a0reinforce\u00a0behaviors\u00a0where\u00a0there\u00a0is\u00a0an\u00a0immediate\u00a0reward\u00a0in\u00a0the\u00a0next\u00a0input\u00a0 pattern.\u00a0The\u00a0agent\u00a0has\u00a0effectively\u00a0established\u00a0proxy\u00a0rewards\u00a0in\u00a0intermediate\u00a0locations\u00a0(cells\u00a02,5,4)\u00a0allowing\u00a0chained\u00a0 sequences\u00a0of\u00a0behaviors\u00a0to\u00a0be\u00a0learned\u00a0(fig\u00a01922).\u00a0 \u00a0 \u00a0 \u00a0\n\u00a0\nfig19 Agent\u00a0encounters\u00a0the\u00a0Primary\u00a0 Reinforcer\u00a0and\u00a0learns\u00a0first\u00a0 behavior.\u00a0\n\u00a0\nfig20 Agent\u00a0establishes\u00a0first\u00a0 Conditioned\u00a0Reinforcer.\u00a0\n\u00a0\n\u00a0\nfig21 Agent\u00a0establishes\u00a0a\u00a0subsequent\u00a0 Conditioned\u00a0Reinforcer\u00a0by\u00a0 reference\u00a0to\u00a0the\u00a0the\u00a0initial\u00a0 Conditioned\u00a0Reinforcer.\u00a0\n\u00a0\nfig22 Agent\u00a0establishes\u00a0a\u00a0series\u00a0of\u00a0 Conditioned\u00a0Reinforcers\u00a0from\u00a0 goal\u00a0to\u00a0starting\u00a0position.\u00a0\n\u00a0 \u00a0 \u00a0 With sufficient exposure the network learns the optimal behavior to achieve a reward\u037e consistently moving one step\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 left/right/up/down\u00a0towards\u00a0the\u00a0goal\u00a0from\u00a0the\u00a0starting\u00a0location\u00a0(cell\u00a03).\u00a0\u00a0 \u00a0 The\u00a0chaining\u00a0of\u00a0sequences\u00a0of\u00a0behaviors\u00a0enables\u00a0long\u00a0term\u00a0strategy\u00a0to\u00a0be\u00a0acquired\u00a0(table\u00a09).\u00a0 \u00a0 \u00a0\n\u00a0\nScheme\u00a0 #\u00a0pattern\u00a0presentations\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Reward\u00a0 (Unsupervised\u00a0backpropagation)\u00a0\n80184\u00a0\n\u00a0 Table\u00a09\u00a0Target\u00a0tracking\u00a0results\u00a0\n\u00a0\n18\u00a0\nLearning\u00a0rates\u00a0can\u00a0be\u00a0improved\u00a0by\u00a0shaping\u00a0[9],\u00a0that\u00a0is\u00a0initially\u00a0placing\u00a0the\u00a0agent\u00a0closer\u00a0to\u00a0the\u00a0reward\u00a0and\u00a0once\u00a0learnt\u00a0 the\u00a0distance\u00a0may\u00a0be\u00a0increased.\u00a0 \u00a0\n3.3\u00a0Summary\u00a0 A\u00a0considerable\u00a0challenge\u00a0facing\u00a0Reinforcement\u00a0Learning\u00a0schemes\u00a0is\u00a0that\u00a0rewards\u00a0can\u00a0be\u00a0very\u00a0temporally\u00a0delayed.\u00a0 Secondary/Conditioned\u00a0Reinforcement\u00a0provides\u00a0an\u00a0effective\u00a0solution\u00a0to\u00a0this\u00a0\u2018Temporal\u2019\u00a0Credit\u00a0Assignment\u00a0 Problem.\u00a0The\u00a0reward\u00a0condition\u00a0was\u00a0facilitated\u00a0by\u00a0adding\u00a0single\u00a0reward\u00a0node\u00a0to\u00a0the\u00a0output\u00a0layer\u00a0in\u00a0addition\u00a0to\u00a0 evaluating\u00a0the\u00a0reward\u00a0at\u00a0the\u00a0input\u00a0layer.\u00a0The\u00a0output\u00a0reward\u00a0desired\u00a0activation\u00a0experiences\u00a0\u2018decay\u2019\u00a0as\u00a0it\u00a0becomes\u00a0 temporally\u00a0distant\u00a0(number\u00a0of\u00a0iterations)\u00a0from\u00a0the\u00a0target\u00a0input\u00a0reward.\u00a0It\u00a0is\u00a0necessary\u00a0to\u00a0enforce\u00a0decay\u00a0on\u00a0the\u00a0output\u00a0 reward\u00a0node\u00a0in\u00a0order\u00a0to\u00a0prevent\u00a0the\u00a0agent\u00a0from\u00a0becoming\u00a0infinitely\u00a0rewarded\u00a0on\u00a0a\u00a0proxy\u00a0reward\u00a0that\u00a0has\u00a0been\u00a0 established.\u00a0If\u00a0the\u00a0agent\u00a0becomes\u00a0fixated\u00a0on\u00a0a\u00a0proxy\u00a0reward\u00a0it\u00a0will\u00a0become\u00a0\u2018stuck\u2019,\u00a0repeating\u00a0the\u00a0previous\u00a0behavior.\u00a0\nIt is also possible to achieve this effect without an explicit motivator node in output layer. Since only strong\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (output) behaviors are those which have been reinforced, a test can be made against the strength of the entire output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern\u00a0rather\u00a0than\u00a0a\u00a0specific\u00a0node.\u00a0\u00a0\nWhen using a single node representation for the (mapped) output layer reward node and enforcing decay, a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diminishing desired activation is set for that node. The node is interpreted in an analogue (continuous) fashion. An\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 alternative representation would be to introduce a discrete output reward node for each time step away from the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input\u00a0node\u00a0reward.\u00a0\u00a0\n\u00a0\n4\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0 So far backpropagation, an algorithm intended for supervised learning, has been housed within an unsupervised\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 framework\u037e desired output patterns have been presented by a framework acting as a surrogate supervisor. Using the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 same framework as described in earlier sections, the backpropagation learning algorithm was replaced with an\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 alternative algorithm, termed Threshold Assignment of Connections (TAC). This approach relies on a threshold\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to determine direct \u2018onnode\u2019 delta values that can be used to calculate weight updates, rather than backpropagating\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 them\u00a0(Box\u00a06).\u00a0\n\u00a0\nProcessing\u00a0steps\u00a0 \u00a0 Differences\u00a0to\u00a0the\u00a0Conditioned\u00a0Reinforcement\u00a0process\u00a0described\u00a0previously\u00a0are\u00a0highlighted\u00a0in\u00a0bold.\u00a0 \u00a0\n1. Input pattern for state \u2018t\u2019 is presented and forward propagated through the network (Box 2).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Node output activation may be in the range 01 (Box 3). A threshold value of 0.5 is assigned\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to\u00a0all\u00a0nodes\u00a0in\u00a0every\u00a0layer.\u00a0\n2. Agent\u2019s behavior, based on whether output layer activations have exceeded threshold values\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (fig\u00a03),\u00a0determines\u00a0subsequent\u00a0input\u00a0pattern\u00a0\u2018t+1\u2019.\u00a0\n3. Framework determines \u2018reward\u2019 value based on \u2018t+1\u2019 input pattern \u2018t+1\u2019 and on activation\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 of\u00a0special\u00a0output\u00a0reward\u00a0node\u00a0(eg\u00a0>\u00a00.8\u00a0).\u00a0\n4. Weight\u00a0changes\u00a0are\u00a0made:\u00a0\n19\u00a0\na. Desired\u00a0\u2018t\u2019\u00a0activations\u00a0on\u00a0every\u00a0node\u00a0are\u00a0calculated:\u00a0\ni. If reward condition then the desired \u2018t\u2019 activation for that node is set to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 maximal value (1.0) for those that were above threshold, and set to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 minimal value (0.0) for those that were below threshold. Set the desired\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activation\u00a0for\u00a0the\u00a0special\u00a0output\u00a0reward\u00a0node\u00a0to\u00a095%\u00a0of\u00a0reward\u00a0value.\u00a0\nii. If no reward then the desired \u2018t\u2019 activation for all nodes are random\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 moderate values [0.45..0.55]. Set the desired activation for the special\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output\u00a0reward\u00a0node\u00a0to\u00a0minimal\u00a0value\u00a0(0.0).\u00a0\nb. Weight changes for state \u2018t\u2019 are made for all connections to each node in situ\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 according\u00a0to\u00a0error\u00a0values\u00a0(Box\u00a07)\u037e\u00a0these\u00a0are\u00a0not\u00a0back\u00a0propagated.\u00a0\n\u00a0 Note:\u00a0To\u00a0achieve\u00a0this\u00a0both\u00a0the\u00a0current\u00a0and\u00a0previous\u00a0activations\u00a0must\u00a0be\u00a0stored.\u00a0\u00a0\n\u00a0\nBox\u00a06\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0processing\u00a0steps\u00a0\n\u00a0\n1)\u00a0Calculate\u00a0desired\u00a0activation\u00a0for\u00a0unit\u00a0u\u00a0based\u00a0on\u00a0reward\u00a0r\u00a0and\u00a0threshold\u00a0t.\u00a0 \u00a0 \u00acr )(r a ) \u00aca ))> 0\u21d2 ( u > t\u21d2 du = 1 \u22c0 ( u > t\u21d2 du = 0 \u22c0 ( > 0\u21d2 du ~ U 0.45, .55[ 0 ] \u00a0 \u00a0 2) Derive delta error value for node u, by finding difference between desired activation ( ) and actual activation\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 du \u00a0 \u00a0 \u00a0 \u00a0 ( ).au \u00a0 =(  ) )eltad u du au au 1( \u2212au \u00a0 \u00a0 3)\u00a0Derive\u00a0weight\u00a0change\u00a0for\u00a0connection\u00a0between\u00a0unit\u00a0h\u00a0and\u00a0unit\u00a0u,\u00a0using\u00a0learning\u00a0rate.\u00a0 \u00a0 =\u00a0weight\u0394 uh rate\u00a0deltal u ah \u00a0 \u00a0\n\u00a0\nBox\u00a07\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0weight\u00a0change\u00a0\n\u00a0\n4.1\u00a0Example:\u00a0Maze\u00a0navigation\u00a0 The same problem as described in section 3.2.2 is revisited. In this example the agent is equipped with the TAC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 algorithm\u00a0rather\u00a0than\u00a0backpropagation.\u00a0\n\u00a0 4.1.1\u00a0Problem\u00a0description\u00a0\nIn\u00a0this\u00a0example\u00a0the\u00a0agent\u00a0must\u00a0autonomously\u00a0learn\u00a0how\u00a0to\u00a0navigate\u00a0a\u00a0target\u00a0through\u00a0a\u00a0maze\u00a0(fig\u00a023).\u00a0\n\u00a0\n20\u00a0\n\u00a0 fig23 Maze\u00a0task\u00a0start\u00a0state\u00a0 \u25cf The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nby\u00a0moving\u00a0it\u00a0the\u00a0top\u00a0centre\u00a0cell.\u00a0 \u25cf Once the target is moved to the target cell (cell 1) it is moved back to it\u2019s original starting position on the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\ngrid\u00a0(cell\u00a03).\u00a0 \u25cf The\u00a0agent\u00a0has\u00a0no\u00a0prior\u00a0knowledge.\u00a0It\u00a0does\u00a0not\u00a0know\u00a0that\u00a0food\u00a0will\u00a0lead\u00a0to\u00a0a\u00a0reward\u00a0until\u00a0it\u00a0happens.\u00a0 \u25cf The\u00a0agent\u00a0may\u00a0only\u00a0move\u00a0the\u00a0target\u00a01\u00a0step\u00a0(ie\u00a0to\u00a0an\u00a0adjacent\u00a0cell)\u00a0in\u00a0each\u00a0iteration.\u00a0 \u25cf To increase the task difficulty the agent may not move the target diagonally in one movement, two steps\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nare\u00a0required.\u00a0\u00a0 \u25cf To further increase task difficulty there is an invisible barrier between the start state and goal. The agent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nmust\u00a0learn\u00a0to\u00a0navigate\u00a0around\u00a0the\u00a0barrier.\u00a0\n\u00a0\n4.1.2\u00a0Network\u00a0configuration\u00a0\nA\u00a0network\u00a0was\u00a0created\u00a0(table\u00a010)(fig\u00a023):\u00a0\nInput\u00a0layer\u00a0 9 input nodes\u037e the centre node is designated as a special \u2018reward\u2019\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 node\u00a0\nHidden\u00a0layer\u00a0 12\u00a0hidden\u00a0nodes\u00a0\nOutput\u00a0layer\u00a0 4\u00a0motor\u00a0nodes\u00a0+\u00a01\u00a0reward\u00a0node\u00a0\nNotes\u00a0 Each\u00a0layer\u00a0fully\u00a0connected\u00a0to\u00a0the\u00a0next\u00a0via\u00a0weights.\u00a0 Weights\u00a0initially\u00a0randomised.\u00a0 Each\u00a0hidden\u00a0and\u00a0output\u00a0nodes\u00a0assigned\u00a0an\u00a0exclusive\u00a0bias\u00a0unit.\u00a0 Learning\u00a0rate\u00a0=\u00a01.0\u00a0\n\u00a0\nTable\u00a010\u00a0Maze\u00a0task\u00a0network\u00a0configuration\u00a0\n21\u00a0\n\u00a0 fig24 Maze\u00a0task\u00a0network\u00a0topology\u00a0\n\u00a0 The\u00a0nine\u00a0input\u00a0nodes\u00a0are\u00a0mapped\u00a0to\u00a0each\u00a0cell\u00a0in\u00a0the\u00a0grid\u00a0(fig\u00a025):\u00a0\n\u00a0\nfig25 Maze\u00a0task\u00a0input\u00a0mapping\u00a0\nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the agent is able to move the target within the grid. In order for the agent to move the target in any given direction\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Therefore\u00a0there\u00a0is\u00a0only\u00a0a\u00a01in16\u00a0chance\u00a0(24)\u00a0the\u00a0agent\u00a0will\u00a0move\u00a0in\u00a0the\u00a0correct\u00a0direction\u00a0by\u00a0chance.\u00a0\nThe framework establishes the reward condition by assigning one the input nodes as a special \u2018reward node\u2019. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 zero\u00a0if\u00a0it\u00a0is\u00a0to\u00a0be\u00a0punished.\u00a0A\u00a0reward\u00a0occurs\u00a0if\u00a0the\u00a0target\u00a0is\u00a0moved\u00a0to\u00a0the\u00a0top\u00a0centre\u00a0cell\u00a0(cell\u00a01).\u00a0 \u00a0 The network is also assigned an additional special reward node in the output layer. A reward also occurs if the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output\u00a0reward\u00a0node\u00a0and\u00a0motor\u00a0output\u00a0nodes\u00a0will\u00a0be\u00a0reinforced.\u00a0\n22\u00a0\n\u00a0 4.1.3\u00a0Results\u00a0 \u00a0 Initially the network moves randomly through the environment. In time the agent is able to learn how to move\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 around corners to the goal. The physics rules prevent the agent from moving the target diagonally in one step,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instead two steps are required. The agent can only reinforce behaviors where there is an immediate reward in the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 next input pattern. The agent has effectively established proxy rewards in intermediate locations (cells 2,5,4)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allowing\u00a0chained\u00a0sequences\u00a0of\u00a0behaviors\u00a0to\u00a0be\u00a0learned\u00a0(fig\u00a02629).\u00a0 \u00a0 \u00a0\n\u00a0\nfig26 Agent\u00a0encounters\u00a0the\u00a0Primary\u00a0 Reinforcer\u00a0and\u00a0learns\u00a0first\u00a0 behavior.\u00a0\n\u00a0\nfig27 Agent\u00a0establishes\u00a0first\u00a0 Conditioned\u00a0Reinforcer.\u00a0\n\u00a0\n\u00a0\nfig28 Agent\u00a0establishes\u00a0a\u00a0subsequent\u00a0 Conditioned\u00a0Reinforcer\u00a0by\u00a0 reference\u00a0to\u00a0the\u00a0the\u00a0initial\u00a0 Conditioned\u00a0Reinforcer.\u00a0\n\u00a0\nfig29 Agent\u00a0establishes\u00a0a\u00a0series\u00a0of\u00a0 Conditioned\u00a0Reinforcers\u00a0from\u00a0 goal\u00a0to\u00a0starting\u00a0position.\u00a0\n\u00a0 \u00a0 \u00a0 With sufficient exposure the network learns the optimal behavior to achieve a reward\u037e consistently moving one\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 step\u00a0left/right/up/down\u00a0towards\u00a0the\u00a0food\u00a0reward\u00a0from\u00a0the\u00a0starting\u00a0location\u00a0(cell\u00a03).\u00a0\n\u00a0\nThe\u00a0chaining\u00a0of\u00a0sequences\u00a0of\u00a0behaviors\u00a0enables\u00a0long\u00a0term\u00a0strategy\u00a0to\u00a0be\u00a0acquired\u00a0(table\u00a011).\u00a0 \u00a0\nScheme\u00a0 #\u00a0pattern\u00a0presentations\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Reward\u00a0 (Unsupervised\u00a0backpropagation)\u00a0\n80184\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0 (Unsupervised)\u00a0\n18212\u00a0\n\u00a0 Table\u00a011\u00a0Maze\u00a0task\u00a0tracking\u00a0results\u00a0\n\u00a0 Learning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 learnt\u00a0increasing\u00a0the\u00a0distance.\u00a0 \u00a0\n4.2\u00a0Example:\u00a0XOR\u00a0 The same problem as described in section 3.1.2 is revisited. In this example the agent is equipped with the TAC\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 algorithm\u00a0rather\u00a0than\u00a0backpropagation.\u00a0\n4.2.1\u00a0Problem\u00a0description\u00a0\nThe agent will be required to solve the XOR problem (table 12) in order to test the network's ability to map non\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 linear transformations that require multiple layers of neurons. This test will will also provide a performance\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 comparison\u00a0between\u00a0unsupervised\u00a0reinforcement\u00a0learning\u00a0and\u00a0the\u00a0supervised\u00a0regime.\u00a0\u00a0 \u00a0\n\u00a0\n23\u00a0\n(input)\u00a0 A\u00a0 (input)\u00a0 B\u00a0\n(output)\u00a0 XOR\u00a0\n0\u00a0 0\u00a0 0\u00a0\n0\u00a0 1\u00a0 1\u00a0\n1\u00a0 0\u00a0 1\u00a0\n1\u00a0 1\u00a0 0\u00a0\n\u00a0 Table\u00a012\u00a0XOR\u00a0task\u00a0\n\u00a0 The behavior, or output response of the network will not influence the subsequent input pattern. However, reward\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 feedback will be given to the network thereby informing whether the output was correct. Once again the output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 patterns are not predetermined. This tightly controlled presentation of input patterns is termed \u2018guided\u2019. The physics\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rules\u00a0are\u00a0modified\u00a0in\u00a0order\u00a0to\u00a0guide\u00a0the\u00a0agent.\u00a0 \u00a0\n4.2.2\u00a0Network\u00a0configuration\u00a0\nA\u00a0network\u00a0was\u00a0created\u00a0(table\u00a013)\u00a0(fig\u00a030):\u00a0\nInput\u00a0layer\u00a0 2\u00a0input\u00a0nodes\u00a0\nHidden\u00a0layer\u00a0 3\u00a0hidden\u00a0nodes\u00a0\nOutput\u00a0layer\u00a0 1\u00a0output\u00a0node\u00a0\nNotes\u00a0 Each\u00a0layer\u00a0fully\u00a0connected\u00a0to\u00a0the\u00a0next\u00a0via\u00a0weights.\u00a0 Weights\u00a0initially\u00a0randomised.\u00a0 Each\u00a0hidden\u00a0and\u00a0output\u00a0nodes\u00a0assigned\u00a0an\u00a0exclusive\u00a0bias\u00a0unit.\u00a0 Learning\u00a0rate\u00a0=\u00a01.0\u00a0\n\u00a0\nTable\u00a013\u00a0XOR\u00a0Network\u00a0configuration\u00a0 \u00a0\n\u00a0 fig30 XOR\u00a0topology\u00a0\n24\u00a0\n4.2.3\u00a0Results\u00a0\n\u00a0\nScheme\u00a0 #\u00a0pattern\u00a0presentations\u00a0\nSupervised\u00a0backpropagation\u00a0 2182\u00a0\nThreshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0 (unsupervised)\u00a0\n311\u00a0\n\u00a0 Table\u00a014\u00a0XOR\u00a0results\u00a0\n\u00a0 The\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0(unsupervised)\u00a0scheme\u00a0required\u00a0significantly\u00a0less\u00a0pattern\u00a0presentations\u00a0 to\u00a0learn\u00a0the\u00a0XOR\u00a0solution\u00a0than\u00a0the\u00a0supervised\u00a0regime\u00a0(table\u00a014).\u00a0Both\u00a0were\u00a0using\u00a0the\u00a0same\u00a0set\u00a0of\u00a0initial\u00a0weights\u00a0and\u00a0 learning\u00a0parameters\u00a0(learning\u00a0rate\u00a0etc).\u00a0Despite\u00a0the\u00a0fact\u00a0that\u00a0in\u00a0the\u00a0supervised\u00a0scheme\u00a0the\u00a0desired\u00a0patterns\u00a0are\u00a0known\u00a0 a\u00a0priori ,\u00a0while\u00a0in\u00a0the\u00a0unsupervised\u00a0(guided)\u00a0learning\u00a0regime\u00a0these\u00a0must\u00a0be\u00a0discovered\u00a0by\u00a0the\u00a0network\u00a0through\u00a0trial\u00a0 and\u00a0error.\u00a0However\u00a0this\u00a0performance\u00a0gap\u00a0would\u00a0be\u00a0expected\u00a0to\u00a0lessen\u00a0as\u00a0the\u00a0number\u00a0of\u00a0nodes\u00a0in\u00a0the\u00a0output\u00a0layer\u00a0 grows,\u00a0increasing\u00a0dimensionality\u00a0and\u00a0with\u00a0it\u00a0the\u00a0number\u00a0of\u00a0candidate\u00a0combinations.\u00a0\n4.3\u00a0Summary\u00a0 These experiments demonstrate TAC may solve linearly inseparable problems that are normally reserved for\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 supervised training regimes. It is capable of training weights deep within the network, thereby capable of forming\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 complex abstract representations. TAC can also solve problems that require long term strategy, when applied in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conjunction with Secondary (Conditioned) Reinforcement. Once TAC has replaced backpropagation, the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 framework\u00a0is\u00a0no\u00a0longer\u00a0required\u00a0to\u00a0act\u00a0as\u00a0a\u00a0surrogate\u00a0supervisor\u00a0\n\u00a0\n5\u00a0Discussion\u00a0 The present solution to RL requires the network to learn appropriate mappings based on the temporal ordering of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 events\u037e these mappings are contingent on subsequent reward conditions. Learning is based on cause and effect. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model does not require a past/present array of network or environment states. While Q learning with neural\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 networks have been used principally to determine policy rather than action[1][7], in the present model they are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 integrated. The presented model can be easily scaled, and does not compromise biological plausibility in the process.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 solution does not rely on recurrency. While the examples presented are based on feedforward architectures the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 present model is not mutually exclusive with recurrency. Indeed it is envisaged recurrency could be used to augment\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the\u00a0presented\u00a0model,\u00a0providing\u00a0information\u00a0of\u00a0prior\u00a0state.\u00a0\u00a0\nThe merits of the present model can be evaluated in terms of Machine Learning Applications as well as the broader\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cognitive\u00a0Science\u00a0implications\u00a0(eg\u00a0biological,\u00a0psychological,\u00a0philosophical).\u00a0\u00a0\n5.1\u00a0Applications\u00a0\n5.1.1\u00a0Primary\u00a0Reinforcement\u00a0\nOne of the key advantages of Primary Reinforcement over supervised learning schemes is that the agent is able to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 autonomously discover solutions (behaviors) to problems, of use when a human may be unable to provide training\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data. Another benefit of Primary Reinforcement is that learning is dynamic and continuous, there is no distinction\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 between training and classification phases. This is useful when the agent encounters novel stimuli, or when a once\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n25\u00a0\nuseful\u00a0response\u00a0is\u00a0no\u00a0longer\u00a0so.\u00a0\n\u00a0\nfig31 \u2018Rags\u00a0the\u00a0robot\u2019\u00a0\u00a0\n\u00a0\n\u2018Rags\u00a0the\u00a0robot\u2019\u00a0learns\u00a0how\u00a0to\u00a0track\u00a0objects\u00a0with\u00a0a\u00a0 vision\u00a0sensor.\u00a0The\u00a0robot\u00a0has\u00a0an\u00a0onboard\u00a0Arduino\u00a0 microcontroller\u00a0and\u00a0is\u00a0wirelessly\u00a0controlled\u00a0by\u00a0desktop\u00a0 computer.\u00a0\n\u00a0\nThe learning scheme can be readily applied to robotics. As proof of concept an agent was created to remotely control\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2018Rags the robot\u2019\u037e a mobile Arduino based device (fig 31). The task was to target an object, in this case a red frisbee.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 For this purpose Rags was equipped with a vision sensor (PixyCam), and was capable of rotating via a pair of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wheels. The agent begins each learning session with randomised network weights. Similar to the previous targeting\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tasks, the robot initially moves randomly, rotating passed the target. Typically within a couple of minutes\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (particularly\u00a0sensitive\u00a0on\u00a0lighting\u00a0conditions)\u00a0the\u00a0robot\u00a0successfully\u00a0comes\u00a0to\u00a0a\u00a0resting\u00a0state\u00a0aiming\u00a0at\u00a0the\u00a0target.\u00a0\nThe Primary Reinforcement solution presented is readily scalable\u037e additional sensors, motors and hidden units can\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 be easily added without reworking the underlying learning process. These additional resources will be utilized in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 problem\u00a0solving.\u00a0\nAlthough Primary Reinforcement enables autonomous agents to discover solutions where a human supervisor is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 unable, a potential issue is that the learned behavior may not be ideal but is sufficient to achieve a reward condition\u037e\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this can be termed a suboptimal behavior limitation . For example a ball may have been successfully kicked into a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 goal, but the kicking technique itself was poor. To avoid this a higher standard can be achieved by setting a more\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringent reward condition, the tradeoff being more optimal behaviors are potentially slower to be discovered and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 learnt.\u00a0\nAnother limitation of the present solution is that we are unable to set arbitrary desired output activations on output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodes, a maximal or minimal value is set (ie 1.0 or 0.0). For example in a network with three output nodes we may\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 choose a desired output pattern vector of [0.6, 0.3, 0.7] for a supervised network, but in the present model the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 desired output pattern vector would be limited to [1.0, 0.0, 1.0]. This shortcoming can be termed a binary limitation .\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 To workaround this limitation alternate output representations may be required to achieve the same level of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 granularity.\u00a0\nOne of the challenges of scaling relates to the complexity of output responses\u037e the more output nodes that are\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 required for a rewarding behavior to occur, the longer it will take for the agent to explore candidate behaviors and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 discover the requisite combination. Also it should also be noted that not all node activations may be relevant to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 producing a rewarding response. These may be included in the dynamically generated desired output pattern, and the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 agent is unable to differentiate which nodes were responsible for achieving the reward. These redundant activations\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 may be eliminated by conforming to what can be termed a laziness principle , where rewards are reduced in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 proportion\u00a0to\u00a0output\u00a0node\u00a0activations.\u00a0\u00a0\nNote\u037e while Reinforcement Learning is essentially a trial and error approach to learning, it is possible to first part\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train\u00a0\u00a0in\u00a0supervised\u00a0mode,\u00a0save\u00a0the\u00a0weights,\u00a0and\u00a0then\u00a0switch\u00a0to\u00a0unsupervised\u00a0mode.\u00a0\n\u00a0\n26\u00a0\n5.1.2\u00a0Secondary\u00a0reinforcement\u00a0\nA potential issue facing the presented model may be termed a suboptimal path limitation . At first glance this is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 similar to the suboptimal behavior limitation described in Primary Reinforcement, but on a temporal level. The\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 problem being that the while the acquired sequence of behaviors reliably leads to the goal, and the individual\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors may have been optimal, the route was not the most efficient one possible. Once learnt the agent has no\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pressure to alter its preferred route. This limitation also afflicts natural organisms. To mitigate this issue behaviors\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 may\u00a0be\u00a0\u2018shaped\u2019 \u00a0to\u00a0elicit\u00a0the\u00a0optimal\u00a0path\u00a0[9].\u00a0\n5.1.3\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0(TAC)\u00a0\nTAC is an alternative solution to the credit assignment problem in ANN\u2019s, a learning algorithm to allow weight\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 updates in multi layer networks. In this regard it does not offer any more or less functional benefit than\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backpropagation.\u00a0However,\u00a0TAC\u00a0also\u00a0suffers\u00a0from\u00a0the\u00a0binary\u00a0limitation \u00a0previously\u00a0outlined.\u00a0\nTAC is significantly easier to implement than backpropagation. This would have significant advantages in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hardware implementations. It has fewer error derivation dependencies than backpropagation, and hence more\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resilient\u00a0should\u00a0damage/information\u00a0loss\u00a0occur\u00a0during\u00a0weight\u00a0updates.\u00a0\u00a0\nIn terms of performance, TAC was found to learn XOR in fewer iterations than supervised backpropagation.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 However this advantage is expected to diminish as the number of output nodes, and therefore potential candidate\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors,\u00a0increase.\u00a0\n5.2\u00a0Implications\u00a0\nReinforcement Learning has already been achieved in the real world as evidenced in nature. Therefore if we want\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to achieve a working, scalable and reliable solution it is expedient to use nature as a reference. We may also then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 use the model as a basis for other Cognitive Science experiments. The model may predict, or be refuted by,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nature\u2019s solution to Reinforcement Learning. However that does not preclude the possibility that there is more than\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 one\u00a0underlying\u00a0mechanism\u00a0that\u00a0provides\u00a0Reinforcement\u00a0Learning\u00a0in\u00a0nature.\u00a0\u00a0\n5.2.1\u00a0Primary\u00a0Reinforcement\u00a0 \u00a0 Consider a simple organism \u2018agent V1\u2019 that does not possess any plastic connections, whose behaviors are inherited\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 and fixed. The only way this organism and members of it\u2019s species can generate new behaviors is via mutation and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 natural selection. Individuals with mutations that produce desirable behaviors are more likely to survive and pass on\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 their\u00a0genes\u00a0and\u00a0behaviors\u00a0to\u00a0the\u00a0next\u00a0generation.\u00a0 \u00a0 More mutations of this species eventually result in an individual with some plastic connections\u037e \u2018agent V2\u2019. It is able\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 to use a reward stimulus, or Primary Reinforcer, to strengthen the connections that were responsible for those\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors. This process of Primary Reinforcement allows the individual to evaluate and learn desirable behaviors.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 These desirable behaviors enhances its chances of survival and put it at a competitive advantage relative to its peers,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 making it more likely to pass on this trait to its progeny. An incremental evolutionary step has provided the agent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 with the power to learn a rewarding behavior. Likewise the model presented uses a simple mechanic to achieve this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 feature.\u00a0 \u00a0\n5.2.2\u00a0Secondary\u00a0Reinforcement\u00a0 \u00a0 While \u2018agent V2\u2019 is able to learn isolated behaviors it is incapable of planning\u037e it cannot learn a sequence of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behaviors. In order to do so this species must evolve again, this time using a resultant reward stimulus to not only\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inform the development of mappings between the previous sensory input pattern with its resultant motor output\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pattern, but also between the previous sensory input pattern and the reward. Once the association between the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sensory input pattern and the reward stimulus is sufficiently strong a secondary (conditioned) reinforcer can be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\n27\u00a0\nestablished. This proxy reward can be used to establish more conditioned reinforcers and thereby chain behaviors\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 together, by doing so goal oriented problem solving can be achieved. Likewise an incremental change to the model\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 provides\u00a0the\u00a0agent\u00a0with\u00a0the\u00a0power\u00a0to\u00a0form\u00a0long\u00a0term\u00a0strategy.\u00a0 \u00a0\n5.2.3\u00a0Threshold\u00a0Assignment\u00a0of\u00a0Connections\u00a0(TAC)\u00a0 \u00a0 A\u00a0biological\u00a0plausibility\u00a0concern\u00a0faces\u00a0supervised\u00a0learning\u00a0schemes\u00a0in\u00a0general:\u00a0\n\u25cf How\u00a0are\u00a0desired\u00a0output\u00a0patterns\u00a0selected\u00a0and\u00a0presented\u00a0to\u00a0the\u00a0network\u00a0?\u00a0\nIn previous examples, once backpropagation is placed in the Reinforcement Learning framework it is no longer a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 truly supervised learning scheme\u037e desired output patterns are not known a priori . However backpropagation faces\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 the\u00a0additional\u00a0biological\u00a0plausibility\u00a0concern:\u00a0\n\u25cf How\u00a0is\u00a0error\u00a0back\u00a0propagated\u00a0?\u00a0\nThus far research has provided little neurobiological support for backpropagation [4]. TAC does not require\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backward connections or tenuous biochemical explanations. TAC could be explained by neuromodulators rather\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 than at the neural computation level. This would provide a more robust solution with a simpler biological\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 explanation.\u00a0\nIt should be noted that while the model suggests assigning random desired activations around the threshold value\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [0.45..0.55] in punishment conditions, it was found to be at least as effective to set this in the range [01]. It should\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 be appreciated that (with moderate learning rates) this has the same effect of establishing an \u2018immature\u2019 response\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 around the threshold level [fig 3]. These alternatives have implications as to what we might expect to find in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 natural\u00a0organisms.\u00a0\u00a0\n\u00a0\n5.3\u00a0Final\u00a0words\u00a0\nThis work presents a self organizing model of cognition\u037e internal drives enforce learning and behavior. The model\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 is based on a single continuum of pleasure and pain, there is a single learning mechanism for the two\u037e pleasure\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 effectively being the alleviation of pain. Whilst the examples provided are configured with only a single drive, this\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 should be extended to multiple drives working in concert/competition. Also the examples provided entail the drive\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 being always active, and therefore continually reinforcing or weakening behaviors. However, it should be taken\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 that when not active (eg drives are satiated), behaviors will not be reinforced or weakened and the agent will be in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 something of a \u2018cruise mode\u2019 with no active learning. Also as a rule of thumb the agent should conform to a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2018laziness principle\u2019, that is they should be punished for exerting unnecessary energy, thereby achieving their goal\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 with\u00a0greater\u00a0efficiency.\u00a0\nEthical concerns may arise with such models. The designer may be inclined to set malevolent directives for the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 agents, for example setting the Primary Reinforcement reward condition to intentionally cause harm\u037e the agent will\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 be compelled to fulfill this goal. Even with benevolent directives, once assigned the agent is free to adopt any\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 behavior\u00a0to\u00a0achieve\u00a0its\u00a0goals,\u00a0also\u00a0yielding\u00a0harmful\u00a0behaviors.\u00a0\nThe proposed model may not be deemed accurate as a model of Reinforcement Learning. Would an accurate model\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 of Reinforcement Learning also be an accurate model of consciousness? While an agreed definition of\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 consciousness is more elusive than that of intelligence, such models may at least assist in refining and testing\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 concepts of consciousness. Does consciousness only arise at a certain scale and complexity? Is recurrency and\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 introspection required? Are plastic connections required? Is embodiment required? Are there even different forms,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 or levels of consciousness? Providing answers is beyond the scope of this paper, but while it may seem premature to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 conclude that such models may be conscious, neither can the question be ignored from an ethics perspective.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Experiments conducted on a valid working pleasurepain based model would likely inflict some degree of suffering\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on\u00a0the\u00a0agent.\u00a0\n28\u00a0\nThe model\u2019s true potential lies in embodiment, whether in real or artificial environments. Agents may also be placed\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 in abstract environments that do not physically exist (eg a stock market). The model is generic and readily scalable,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 and given sufficient time and resources can tackle a variety of tasks. Tasks can be assigned either by setting of the\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reward condition goal, or by placing as obstacles to that goal. If the presented model of Reinforcement Learning\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 should\u00a0be\u00a0deemed\u00a0implausible\u00a0from\u00a0a\u00a0Cognitive\u00a0Science\u00a0perspective,\u00a0the\u00a0practical\u00a0benefits\u00a0at\u00a0least\u00a0may\u00a0be\u00a0of\u00a0utility.\u00a0\u00a0\n5.4\u00a0Appendix\u00a0\n5.4.1\u00a0Platform\u00a0 Hardware:\u00a0Desktop\u00a0PC\u00a0(i3\u00a03.7GHz,\u00a016GB\u00a0ram)\u00a0\nSoftware:\u00a0OS\u00a0Linux\u00a0Mint\u00a017.3,\u00a0Neural\u00a0Network\u00a0software\u00a0written\u00a0in\u00a0C\u00a0(GCC)\u00a0\n\u00a0\n6\u00a0References\u00a0 [1] Charles W. Anderson. 1986. Learning and Problem Solving with Multilayer Connectionist Systems.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nTechnical\u00a0Report.\u00a0University\u00a0of\u00a0Massachusetts,\u00a0Amherst,\u00a0MA,\u00a0USA.\u00a0\n[2] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 M.(2013).\u00a0Playing\u00a0atari\u00a0with\u00a0deep\u00a0reinforcement\u00a0learning.arXiv\u00a0preprint\u00a0arXiv:1312.5602\u00a0\n[3] Rumelhart, David E., Hinton, Geoffrey E., Williams, Ronald J., \u201cLearning representations by\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backpropagating\u00a0errors\u201d,\u00a0Nature,\u00a01986\u00a0\n[4] D. G. Stork, \"Is backpropagation biologically plausible?,\" Neural Networks, 1989. IJCNN., International\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Joint\u00a0Conference\u00a0on ,\u00a0Washington,\u00a0DC,\u00a0USA,\u00a01989,\u00a0pp.\u00a0241246\u00a0vol.2.\u00a0\n[5] Richard\u00a0S\u00a0Sutton\u00a0and\u00a0Andrew\u00a0G\u00a0Barto.\u00a0Reinforcement\u00a0learning:\u00a0An\u00a0introduction.\u00a0MIT\u00a0press,\u00a01998.\u00a0\n[6] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 that\u00a0learn\u00a0and\u00a0think\u00a0like\u00a0people.\u00a0arXiv\u00a0preprint\u00a0arXiv:1604.00289,\u00a02016.\u00a0\n[7] Gerald Tesauro, \u201dTemporal difference learning and TDGammon\u201d, Communications of the ACM CACM\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Homepage\u00a0archive,\u00a0Volume\u00a038\u00a0Issue\u00a03,\u00a0March\u00a01995,\u00a0Pages\u00a05868\u00a0\n[8] Andrew G. Barto, Richard S. Sutton, Charles W. Anderson, \u201cNeuronlike adaptive elements that can solve\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 difficult learning control problems\u201d, IEEE Transactions on Systems, Man, and Cybernetics\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (Volume:SMC13\u00a0,\u00a0\u00a0Issue:\u00a05)\u00a0pages\u00a0834\u00a0\u00a0846,\u00a01983\u00a0\n[9] Skinner,\u00a0B.\u00a0F.\u00a0(1953).\u00a0Science\u00a0and\u00a0human\u00a0behavior.\u00a0New\u00a0York:\u00a0Macmillan,\u00a0Pages.\u00a092\u20133\u00a0\n[10]William Bechtel, Adele Abrahamsen, \u201cConnectionism and the mind\u201d, Wiley,1991, pages 7097Skinner,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 B.\u00a0F.\u00a0(1953).\u00a0Science\u00a0and\u00a0human\u00a0behavior.\u00a0New\u00a0York:\u00a0Macmillan,\u00a0Pages.\u00a092\u20133\u00a0\n[11]Hodgkin, A. L., A. F. Huxley, and B. Katz. \u201cMeasurement of CurrentVoltage Relations in the Membrane\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 of\u00a0the\u00a0Giant\u00a0Axon\u00a0of\u00a0Loligo.\u201d\u00a0The\u00a0Journal\u00a0of\u00a0Physiology\u00a0116.4\u00a0(1952):\u00a0424\u2013448.\u00a0\n[12]Sepp Hochreiter and J\u00fcrgen Schmidhuber (1997). \"Long shortterm memory\" (PDF). Neural\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Computation.\u00a09\u00a0(8):\u00a01735\u20131780.\u00a0http://dx.doi.org/10.1162/neco.1997.9.8.1735\u00a0\n[13]Elman, J. L. (1990), Finding Structure in Time. Cognitive Science, 14: 179\u2013211.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 doi:10.1207/s15516709cog1402_1\u00a0\n\u00a0\n29\u00a0"}], "references": [{"title": "Learning and Problem Solving with Multilayer Connectionist Systems", "author": ["Charles W. Anderson"], "venue": "Technical Report", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Playing atari with deep reinforcement learning.arXiv preprint arXiv:1312.5602", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "Riedmiller"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Learning representations by     back\u00adpropagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1986}, {"title": "Is backpropagation biologically plausible", "author": ["D.G. Stork"], "venue": "\u200bNeural Networks, 1989. IJCNN., International      Joint Conference on  \u200b  , Washington, DC, USA, 1989, pp. 241\u00ad246 vol.2.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Building machines   that learn and think like people", "author": ["Brenden M Lake", "Tomer D Ullman", "Joshua B Tenenbaum", "Samuel J Gershman"], "venue": "arXiv preprint arXiv:1604.00289,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Temporal difference learning and TD\u00adGammon\u201d, Communications of the ACM CACM", "author": ["Gerald Tesauro"], "venue": "Homepage archive, Volume 38 Issue", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Connectionism and the mind\u201d, Wiley,1991, pages 70\u00ad97Skinner", "author": ["William Bechtel", "Adele Abrahamsen"], "venue": "Science and human behavior", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1953}, {"title": "Measurement of Current\u00adVoltage Relations in the Membrane       of the Giant Axon of Loligo.", "author": ["A.L. Hodgkin", "A.F. Huxley", "B. Katz"], "venue": "The Journal of Physiology", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1952}, {"title": "Long short\u00adterm memory\" (PDF). Neural     Computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Finding Structure in Time", "author": ["J.L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}], "referenceMentions": [{"referenceID": 5, "context": "In contrast the work presented follows a distinctly top\u00addown approach attempting to model intelligence as a whole system; a causal agent interacting with the environment [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "Significant RL successes have been achieved in AI notably with the use of Q\u00adlearning [2][7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "Significant RL successes have been achieved in AI notably with the use of Q\u00adlearning [2][7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Primary reinforcement reward conditions (eg hunger or thirst) typically drive some form of homeostatic, adaptive control function for the agent [5][8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Weight adjustments using backpropagation[3][9] are traditionally used in supervised learning schemes; that is desired activations (or training sets) are established prior to the learning phase.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "The learning scheme consists of the following components: \u25cf Artificial Neural Network \u25cf Learning Algorithm \u25cf Framework \u25cf Environment The network architecture is that of a familiar multilayer perceptron (MLP)[3].", "startOffset": 207, "endOffset": 210}, {"referenceID": 2, "context": "Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "Thresholds for neuron activation are widely found in animals, where sufficient \u2018excitation\u2019 is required to result in an electrical action potential that can be signalled to other neurons [11].", "startOffset": 187, "endOffset": 191}, {"referenceID": 0, "context": "While \u200bQ learning with neural networks have been used principally to determine policy rather than action[1][7], in the present model they are integrated\u200b.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "While \u200bQ learning with neural networks have been used principally to determine policy rather than action[1][7], in the present model they are integrated\u200b.", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present solution does not rely on recurrency.", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present solution does not rely on recurrency.", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "However backpropagation faces the additional biological plausibility concern: \u25cf How is error back propagated ? Thus far research has provided little neurobiological support for backpropagation [4].", "startOffset": 193, "endOffset": 196}], "year": 0, "abstractText": "A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,   providing a general purpose learning machine. By reference to a node threshold three features are  described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)   The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of   forming long term strategy 3) The learning scheme is modified to use a threshold\u00adbased deep learning   algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be   used for supervised as well as unsupervised training regimes.", "creator": null}}}