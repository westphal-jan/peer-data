{"id": "1609.03490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Transfer String Kernel for Cross-Context DNA-Protein Binding Prediction", "abstract": "through sequence - based classification, this interpretation tries to accurately reproduce appropriate dna binding sites of transcription factors ( tfs ) in various unannotated cellular context. related methods in the literature desire to perform such predictions spontaneously, since they don't consider sample distribution shift of sequence segments from an annotated ( reactive ) context to itself unannotated ( target ) context. we, therefore, propose a method called \" transfer string kernel \" ( tsk ) that achieves reverse prediction of transcription factor binding site ( tfbs ) using knowledge transfer via cross - context sample adaptation. tsk maps sequence segments to a high - dimensional feature space using a custom target string kernel framework. in this high - specification structure, labeled examples of the present context are re - weighted so that the revised knowledge distribution matches the target context more closely. we have experimentally verified validity for tfbs identifications on fourteen different tfs under a tri - organism setting. we find that tsk consistently outperforms the state - of the - same tfbs tools, especially when working with tfs sequencing binding sequences cannot simultaneously conserved across contexts. readers also demonstrate the generalizability of tsk by showing its tight - edge performance after 36 different set of cross - context tasks for the mhc peptide binding predictions.", "histories": [["v1", "Mon, 12 Sep 2016 17:16:24 GMT  (2504kb)", "http://arxiv.org/abs/1609.03490v1", "To be published in IEEE/ACM Transactions on Computational Biology and Bioinformatics"]], "COMMENTS": "To be published in IEEE/ACM Transactions on Computational Biology and Bioinformatics", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ritambhara singh", "jack lanchantin", "gabriel robins", "yanjun qi"], "accepted": false, "id": "1609.03490"}, "pdf": {"name": "1609.03490.pdf", "metadata": {"source": "CRF", "title": "Transfer String Kernel for Cross-Context DNA-Protein Binding Prediction", "authors": ["Ritambhara Singh", "Jack Lanchantin", "Yanjun Qi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n03 49\n0v 1\n[ cs\n.L G\n] 1\n2 Se\np 20\n16 1\nIndex Terms\u2014Machine Learning, Bioinformatics, Support Vector Machines, Domain Adaptation, String Classification, String Kernel\nThis work will be originally published in IEEE/ACM Transactions on Computational Biology and Bioinformatics. Code available at github.com/QData/TransferStringKernel.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "S EQUENCE analysis plays an important role in the field of bioin-formatics. Genomic sequences build the basis of a large body of research on understanding the biological processes in living organisms. As an important application of sequence based mining, the task of predicting Transcription Factor Binding Sites (TFBSs) on genomes has attracted much attention over the years [1]. Transcription factors (TFs) are regulatory proteins that bind on functional sites of DNA to control the regulation of genes. Each different TF binds to specific locations (or sites) on a genomic sequence to regulate cell machinery. While many TFs are highly conserved among different species [2], conservation of their binding sequence is low [3]. These TFBSs vary across different cell types, cell stages and genomes. Owing to the development of chromatin immunoprecipitation and massively parallel DNA sequencing (ChIP-seq) technologies [4], maps of genome-wide binding sites are currently available for multiple TFs in a few cell types across human and mouse genomes via the ENCODE [1] database. Because ChIP-seq experiments are slow and expensive, they have not been performed for many important cell types or organisms. Therefore, computational methods to identify TFBS accurately remain essential for understanding the regulatory functioning and evolution of genomes. Such predictions are especially important for unannotated cellular contexts (e.g., cell types of rare diseases or rare organisms).\nString kernel techniques under the support vector machine (SVM) classification framework have been successfully used for detecting patterns of DNA or protein sequences before [5]. Through local substring (k-mer) comparisons that incorporate mismatches, this category of models extracts mismatch features and trains to classify sequence segments from a set of previously labeled sequences. Then, the learned models are used to classify a new set of sequences. Recently, [6] extended this discriminative SK+SVM classification\n\u2022 All authors are with Department of Computer Science, University of Virginia, Charlottesville VA, 22903. E-mail: yanjun@virginia.edu\nsetup for predicting TFBSs in a cell-type specific manner. Their results [6] have suggested that traditional motif-driven approaches (details in Section 2.4) for TFBS predictions are not always sufficient for accurately accounting for cell-type specific binding profiles. Despite the exciting aforementioned framework [6], one shortcoming remains significant: this method assumes that the source and target samples are drawn from the same probability distribution. This makes string kernel non-applicable to most cases of TFBS predictions in unannotated cellular contexts.\nWe take into consideration that if a genome-wide ChIP-seq experiment has been performed for a specific TF in a certain context, it is normally unnecessary to computationally predict TFBSs for that specific [TF, context] pair again. Therefore, this paper focuses on computationally predicting TFBSs for cellular contexts that have not yet been annotated. We generate the \u201cgold standard\u201d reference labels, used for training, from an existing ChIP-seq TFBS map of an annotated context that is most related to the target [TF, context] configuration of interest. Since each TF\u2019s binding sequence patterns vary with the context factor, it is likely that data samples in the annotated (source) context are distributed differently from samples in the unannotated (target) context. This also connects to observations in the literature that the TFBS sequences have low conservation across species even though many TF proteins are conserved across large evolutionary distances [7]. When implementing cross context prediction, it is desirable to design algorithms that remain effective under such distribution shifts [8]. While the problem may be unsolvable if the source and target distributions share nothing in common, recent machine learning studies [9] have provided effective adaptation for closely related distributions. Biologically, this also makes sense since closely related cell contexts should have similar gene regulation processes and similar TFBS sequence patterns [6].\nIn this paper, we propose an approach called \u201cTransfer String Kernel (TSK)\u201d that learns to transfer knowledge from an annotated to an unannotated cellular context to achieve better sequence-based TFBS predictions. Essentially, TSK re-weighs the source data so that\n2\nFig. 1: Overview of our experimental setup for cross-context TFBS prediction tasks. (a) Source (mouse) and target (human) ChIP-seq maps for cross-context TFBS classifications (a family of tasks). Four example cases are shown. (b) An example of sequence based TFBS classification shown for TF_1. \u201cy\u201d labels are obtained from ChIP-seq signals. \u201cx\u201d are corresponding sequence segments. Thus, y \u2208 {+1,\u22121}, where y = +1 indicates binding of TF and y = \u22121 means TF does not bind to the sequence. (c) The baseline (k,m)-mismatch string kernel. Here, k = 5 and m \u2208 {0, 1}.\nits distribution more closely matches that of the target data [9]. It assigns higher weights to those observations in the source context that are most similar to those in the target context, and lower weights to those that rarely occur in that context. This connects to a more general machine-learning topic called domain adaptation [10] (discussed in Section 2.4).\nIn summary, this paper includes the following contributions: \u2022 \u201cTransfer String Kernel (TSK)\u201d implements domain transfer of\nstring kernel via Kernel Mean Matching, on tasks for which target domain is unannotated; \u2022 Experimentally, we apply TSK on 14 different TFBS prediction tasks from mouse to human genomes. To the best of authors\u2019 knowledge, this is the first attempt of studying cross-context TFBS predictions across different genomes. This setting resembles the translational setup that many biologists are working on; \u2022 We demonstrate that TSK consistently outperforms state-ofthe-art TFBS prediction tools, especially for those TFs whose binding properties do not conserve across contexts; \u2022 Our experiments show that TSK exhibits robust performance despite the label imbalance issue that is common in TFBS prediction tasks; \u2022 Finally, we show that TSK generalizes well to other crosscontext sequence based prediction tasks.\nTSK is a general cross-context sequence modeling approach and not tied to the TFBS applications. We show this generality by applying it for predicting peptide binding (PB) to Major Histocompatibility Complex Molecules (MHC class I). MHC is a set of cell surface proteins that bind to peptide fragments derived from pathogens and display them on cell surface for recognition by the appropriate Tcells (i.e., consequently controlling the adaptive immune response). Knowledge of these peptide bindings helps in studying immunity and is vital for vaccine development. The protein sequence-based PB pre-\ndiction tasks are similar to TFBS tasks because: a) MHC PB bindings vary across organisms and b) obtaining such experimental data is a complex, expensive and time consuming task, therefore leading to lack of labeled samples in target (human) domain. However, it differs from TFBS prediction tasks in its larger dictionary size (due to protein sequences) as well as in the sample properties (e.g., shorter sequence length, varying ratios of positive to negative samples and size) of the datasets."}, {"heading": "2 METHODS: TRANSFER STRING KERNEL (TSK)", "text": "We propose a learning method, called \u201cTransfer String Kernel\u201d (TSK) that performs cross-context \u201cknowledge transfer\u201d and utilizes available TF binding labels from a source context to discriminatively\n3 predict TFBS for the same specific TF in an unannotated target context. We now discuss various components of our approach. Table 1 provides a list of notation symbols that we have used in this section."}, {"heading": "2.1 Basic Tasks: A family of sequence classification tasks", "text": "The sequence-based TFBS prediction problem can be casted as a string classification task. For a certain TF of interest, on a particular genome under a specific cellular context, the task aims to classify a DNA sequence segment as a potential TFBS or a non-binding background site (see a sample sequence-label pair (x, y) in Figure 1(b)). When considering multiple TFs and various cellular contexts (Figure 1(a)), this constitutes a family of sequence classification problems. For example in Figure 1(b), we train a string classifier using labeled segments (obtained from ChIP-seq) of TF_1 from the source context. The classifier is then used to predict potential TFBS sites of the same TF (TF_1) in the target context. This means that if we work on N different TFs, we are handling a family of N different classification tasks. In case of Figure 1(a), there are four different TFBS tasks for 4 TFs.\nFor each task, when performing sequence-based TFBS predictions in an unannotated cellular context, we face a data problem called distribution shift. We first define necessary notations to explain this problem. We have two spaces, X represents all possible sequence patterns and Y represents their respective labels. The set of source samples:\nZsd = {(x sd 1 , y sd 1 ), ..., (x sd nsd , ysdnsd)} \u2286 X \u00d7 Y, (1)\nfollow a probability distribution Psd(x, y) 1 in the annotated source context. Target samples (for the same TFi) form the following set:\nZtd = {(x td 1 , y td 1 ), ..., (x td ntd , ytdntd)} \u2286 X \u00d7 Y, (2)\ndrawn from another distribution Ptd(x, y) in the unannotated target context. In the training phase, we are assuming a transductive setting, in which we can access a set of xtdi . True label, y td i , is not available for training and is used in testing for evaluation."}, {"heading": "2.2 Basic Model: Mismatch String Kernels with SVM", "text": "The key idea of string kernels is to apply a function \u03c6(\u00b7), which maps sequences of arbitrary length into a vectorial feature space of fixed length. In this space, a standard classifier such as a support vector machine (SVM) [11] can then be applied. Kernel-version SVMs calculate the decision function for an input sample x as,\nf(x) = \u2211\ni\u2208sd\n\u03b1iK(x sd i , x) + b. (3)\nString kernels [5] [12], implicitly compute an inner product in the mapped feature space \u03c6(x) as:\nK(x, x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009, (4)\nwhere x = (s1, . . . , s|x|), x, x \u2032 \u2208 S, |x| denotes the length of the sequence x, S represents the set of all sequences composed of all possible dictionary items (for example, in case of DNA sequences si \u2208 {A,C, T,G}), and \u03c6 : S \u2192 Rm defines the mapping from a sequence x \u2208 S to a m-dimensional feature vector.\nThe feature representation \u03c6(\u00b7) plays a key role in the effectiveness of sequence analysis since biological sequences cannot be readily described as feature vectors. One classic representation is to treat DNA sequence segments as an unordered set of nucleotide k-mers (combinations of k adjacent nucleotide residues). A feature\n1. This probability varies with different TFi\nvector indexed by all k-mers records the number of occurrences of each k-mer in the current sequence. The string kernel using this representation is called spectrum kernel [5], where the spectrum representation counts the occurrences of each nucleotide k-mer in a DNA sequence segment. Kernel scores between sequences are then computed by taking an inner product between corresponding \u201ck-mer - indexed\u201d feature vectors:\nK(x, x\u2032) = \u2211\n\u03b3\u2208\u0393k\ncx(\u03b3) \u00b7 cx\u2032(\u03b3), (5)\nwhere \u03b3 represents a k-mer, \u0393k is the set of all possible k-mers, and cx(\u03b3) is the number of occurrences (with normalization) of k-mer \u03b3 in sequence x.\nHowever, exact kernel matching in biological sequences is ineffective due to naturally occurring letter (e.g., nucleotide) substitutions, insertions, or deletions. Therefore, inexact comparison is critical for effective matching between sequence segments. In string kernels, this is typically achieved by using different families of mismatches [12]. We use the concept of (k,m)-mismatch string kernels [12] (illustrated in Figure 1(c)), which considers k-mer counts with m inexact matching of k-mers (e.g. k=5, m \u2208 {0, 1}). For this kernel,\nK(x, x\u2032) = \u2211\n\u03b3\u2208\u0393k\nck,mx (\u03b3) \u00b7 c k,m x\u2032 (\u03b3), (6)\nck,mx = \u2211\ng\u2208S(\u03b3,m,k)\ncx(g), (7)\nS(\u03b3,m, k) denotes the set of contiguous substrings of length k that differ from \u03b3 in at most m positions (see Figure 1(c) with an example case for (k = 5,m \u2208 {0, 1})).\nThrough mismatches, (k,m)-mismatch kernel allows flexibility when matching k-mers between sequences. This flexibility partly reflects the true biological nature of DNA or proteins sequences since they are prone to mutations like deletions, insertions, substitutions etc. As expected, it performs better than spectrum kernel (as seen in Section 4.1) therefore; it is a natural choice for our TSK approach.\nIn practice, string kernel implementations typically require efficient computation without explicitly constructing potentially highdimensional feature vectors \u03c6(\u00b7). This is because the explicit feature vector mapping \u03c6(\u00b7) becomes problematic (especially in the case of inexact matching) for even small value of k (due to the dimensionality of \u03c6(\u00b7) being exponential in k). Researchers have proposed various strategies [5], [12] to address these computational difficulties. We adopt a statistical strategy from [12] that provides linear-time string kernel computations and scales well with dictionary size and input length."}, {"heading": "2.3 Proposed Model: Transfer String Kernel", "text": "String kernel assumes training and testing samples are drawn from the same probability distribution. To consider the variation between source and target samples in our application, domain adaptation [10], [13] serves as a natural candidate to tackle this computational challenge. In machine learning, domain adaptation aims to use data or a model of a well-analyzed source domain to obtain or refine a model for a less analyzed target domain. The specific \u201cdomain transfer\u201d setting being focused in this paper assumes that historical labels only exist in the source context and are not available in the target context (reasons explained in Section 1). Accordingly, we propose \u201cTransfer String Kernel (TSK)\u201d approach to achieve better cross-context TFBS predictions by transferring knowledge from an annotated context to an unannotated context.\n4 TSK revises the (k,m)-mismatch string kernel framework using a \u201cKernel Mean Matching\u201d (KMM) strategy [9] in order to perform knowledge transfer. To our knowledge, TSK has not been proposed in the literature for cross-context TFBS prediction tasks. Specifically, TSK adapts string kernel under the \u201ccovariate shift\u201d assumption [9]. It assumes that the conditional probability distribution of the output variable, given the input variable, remains fixed in both the source and target set. In other words, the data shift happens for the marginal probability distribution of feature variables (from Psd(x) to Ptd(x)) and not for the conditional distribution P (y|x).\nThe key to correcting this type of sampling bias is to estimate the \u201cimportance weight\u201d for each source sample [8]:\n\u03b2(x, y) := Ptd(x, y)\nPsd(x, y) =\nP (y|x)Ptd(x) P (y|x)Psd(x) = Ptd(x) Psd(x) . (8)\nThe KMM estimator accounts for the difference between Ptd(x, y) and Psd(x, y) by re-weighting the source points so that the means of the source and target points in a Reproducing Kernel Hilbert Space (RKHS) are close. This reweighing process is therefore called \u201cKernel Mean Matching\u201d (KMM). When the RKHS is universal, the population solution of weight vector \u03b2\u0302 (in the following Equation 9) to the KMM optimization (of matching means) is exactly the vector form of ratio Ptd(x)\nPsd(x) (in Equation 8) including all x from the source\ndomain. Let \u03b2i represent the \u201cimportance weight\u201d of a source instance xi. More specifically, KMM uses a \u201cmaximum mean discrepancy\u201d measure to minimize the difference between the empirical mean of the source and the empirical mean of the target distribution. Formally, KMM attempts to match the mean elements in a feature space induced by a kernel function K(\u00b7, \u00b7):\n\u03b2\u0302 = argmin\u03b2 L\u0302(\u03b2), (9)\nwhere,\nL\u0302(\u03b2) = \u2016 1\nnsd\nnsd\u2211\ni=1\n\u03b2i \u2217 \u03c6(x sd i )\u2212\n1\nntd\nntd\u2211\ni=1\n\u03c6(xtdi )\u2016 2 (10)\n= 1\nn2sd \u03b2\nT K\u03b2 \u2212\n2\nn2sd \u03baT\u03b2 + constant, (11)\ns.t. \u03b2i \u2208 [0, B] and | nsd\u2211\ni=1\n\u03b2i \u2212 nsd| \u2264 nsd\u01eb.\nHere, K is the kernel matrix among all source examples, and \u03bai is the weighted sum of kernel values among a source example and all target examples:\nKij :=K(x sd i , x sd j ), (12)\n\u03bai := nsd\nntd\nntd\u2211\nj=1\nK(xsdi , x td j ). (13)\nLarge values of \u03bai correspond to more important observations xsdi and are more likely to lead to larger \u03b2i. Equation 9 involves a quadratic program which can be efficiently solved using the interior point methods or any other subsequent procedures such as projected gradient optimization method [9].\nAlgorithm 1 Algorithm for Transfer String Kernel through matching means in RKHS\n1: procedure TSK 2: Implement (k,m)-mismatch string kernel to calculate the\nkernel matrix K and vector \u03ba; 3: Use Kernel Mean Matching estimator on K and \u03ba to obtain\nthe importance weight \u03b2\u0302i for each source sample i; 4: Use \u03b2\u0302i to perform instance re-weighted SVM training step; 5: Use the trained SVM model to perform sequence classifica-\ntion of target samples.\nThe derived importance weights \u03b2\u0302 are then used to re-weigh source samples in a revised support vector machine training algorithm, i.e., an instance weighted SVM that aims to optimize:\nnsd\u2211\ni=1\n\u03b1i \u2212 0.5 \u2211\ni,j\u2208sd\n\u03b1i\u03b1jyiyjK(x sd i , x sd j ), (14)\ns.t.\nnsd\u2211\ni=1\n\u03b1iyi = 0, and \u03b2\u0302iC \u2265 \u03b1i \u2265 0. (15)\nHere \u03b1i are the slack variables in the SVM, the same as \u03b1i in Eq 3. In summary, we apply KMM in conjunction with the (k,m)mismatch string kernel using the algorithm summarized in Algorithm 1, thus enabling us to perform knowledge transfer across domains when classifying strings."}, {"heading": "2.4 Connecting to Previous Studies", "text": ""}, {"heading": "2.4.1 Sequence-motif based TFBS prediction tools", "text": "Transcription factors influence gene expression by binding to specific DNA sequence in a genomic region. Therefore accurate models for identifying and describing the binding sites of TFs are essential in understanding cells. Previous techniques include many sequencemotif based computational approaches that typically use positionbased sequence information for predicting TFBS. Relying on a set of known transcription factor binding sites (TFBSs) for a given TF, the binding preference is generally represented in the form of a position weight matrix (PWM) [14], [15] (also called positionspecific scoring matrix) derived from a position frequency matrix (PFM). A PFM is essentially an occurrence table, summarizing the number of each nucleotide being observed at each position of a set of aligned TFBSs. [6] have suggested that traditional motif-driven approaches are not always sufficient to accurately account for celltype specific binding profiles. While motif-based PWMs are compact and interpretable, they can under-fit ChIP-seq data by failing to capture subtle but detectable and important sequence signals, such as direct DNA-binding preferences of certain TFs, cofactor binding sequences, accessibility signals, or other discriminative sequence features. Gene regulatory programs are primarily regulated through cell-context specific binding of TFs. Therefore, it becomes more clear that TFBS analyses should consider distinct cell contexts (see Figure 1(a)). To the best of authors\u2019 knowledge, this paper is the first attempt of direct testing of cross-context prediction performance of such methods."}, {"heading": "2.4.2 String Classification", "text": "Our formulation of TFBS prediction belongs to a general category of \"string classification\". Various methods have previously been proposed to solve string classification, including generative (e.g., Hidden Markov Models-HMMs) and discriminative approaches. Among the discriminative approaches, string kernel methods provide some of the\n5 most accurate results, such as for remote protein fold and homology detections [5], [12]. Aside from spectrum kernel [5] and (k,m)mismatch string kernel [12] introduced in the previous section, a few other notable string kernels include (but are not limited to): (1) The gapped kernel calculates dot-product of (non-contiguous) k-mer counts with gaps allowed between elements. (2) More generally, the substring kernel [16] measures similarity between sequences based on common co-occurrence of exact sub-patterns (e.g., substrings). (3) The profile kernel [17] uses the notion of similarity based on a probabilistic model (e.g. profile). (4) Under the semi-supervised setting, the so-called \u201csequence neighborhood\u201d kernel or \u201ccluster\u201d kernel [18] proposes a semi-supervised extension of string kernel, which replaces every sequence with a set of \u201csimilar\u201d (neighboring) sequences and a new representation is obtained by averaging representations of these neighboring sequences found in the unlabeled data using a certain sequence similarity measure.\nSequence labeling or sequence tagging is another related category of formulations for sequence modeling. Many successful works in the field of natural language processing [19] belong to this category in which each position of input sequences can be annotated with tags indicating parts of speech, named entities, semantic roles, etc. Popular bioinformatics tasks predicting proteins\u2019 local functional properties [20] have been modeled as a labeling or tagging of amino acids on proteins. Many important functional properties are computationally predicted in this fashion, such as secondary structure, solvent accessibility, transmembrane topology or the locations of coiled-coil regions. Multiple classic machine learning methods have been benchmark tools for sequence tagging, like conditional random field [21]. We omit a full survey of this topic due to its vast body of previous literature and loose connection to our TFBS formulations."}, {"heading": "2.4.3 Domain Transfer for Genome Sequence Mining", "text": "TSK relates to a more general machine-learning topic \u201ccovariate shift\". Traditional supervised learning assumes source and target samples are usually drawn from the same probability distribution. This assumption can be easily violated in practice, for instance, due to sampling bias or nonstationarity of the sampling environment (i.e. the case of TSK) [8]. Algorithms that remain effective under such distribution shifts are highly preferred. This problem has been investigated in both statistics and machine learning under various assumptions [8]. The covariate shift assumption assumes that the conditional probability distribution of the output variable y, given the input variable, x remains fixed in both the source and target. Under this setting, the basic motivation to correct the sampling bias is to re-weigh the source data so that its distribution matches more closely to that of the target data. A number of previous methods have been proposed to estimate the \u201cimportance weight\u201d \u03b2 from finite samples, including kernel mean matching (KMM), logistic regression, KL importance estimation and many others [8].\nMore generally, TSK belongs to topics of domain adaptation and transfer learning [13] [10], where one aims to use data or models of a well-analyzed source domain to obtain or refine models for a less analyzed target domain. This helps when datasets from the two domains belong to different feature spaces or follow different data distributions. For such scenarios, knowledge transfer will potentially improve the performance of learning by avoiding or reducing the expensive data-labeling efforts [13]. A good classifier can only be trained when a sufficient amount of annotated training data is available. However, labeled training data in biomedical fields is scarce. Obtaining such labeled datasets can be costly and time-consuming. This results in the data sparsity problem that is a major bottleneck for applying machine learning in biomedical domain. Computational\nmethods with the ability of \u201cknowledge transfer\u201d become highly crucial for such applications because biomedical data is intrinsically complex and heterogeneous at almost every level (e.g., different species, different tissue types, etc). Transfer learning can help in reusing knowledge from annotated datasets to new domains [22], reducing the knowledge gap of labeled data due to heterogenous variations. For example, one previous study [10] explored \u201cdomain adaptation\u201d strategies for a task of sequence-based prediction for acceptor splice sites. It assumed that historical labels exist for both source and target contexts, which is different from our setting. We aim to predict TFBS for a cellular context that has not yet been annotated (i.e. no label exists in the target domain). Our setting is more practical for TFBS prediction tasks, because if a ChIP-seq experiment has been performed for a specific TF in a certain context, it is normally unnecessary to computationally predict TFBS for that specific domain again. Experimental results in Section 4 successfully present a strong case of mining bio-medical data where the transfer of knowledge across domains is critical and fruitful."}, {"heading": "2.4.4 Covariate Shift & Domain Adaptation", "text": "Domain adaptation has been widely studied in machine learning literature to train an effective classifier for a target domain for which labeled data is rare, or even unavailable (this paper\u2019s case). By exploiting labeled training samples from a different but related source domain, a variety of previous studies have demonstrated its effectiveness on multiple applications including sentiment analysis [23], object classification [24], activity recognition [25], text categorization [26] and Brain-computer interface (BCI) tasks [27]. Roughly speaking, previous learning methods for domain adaptation can be summarized into two main types:\nCovariate Shift: First, many existing studies try to estimate weights of samples that account for the mismatch in distributions between a target and a source domain. Covariate shift assumes that the marginal distributions of the source and target instances differ while the conditional distribution of the target output remains the same across domains [28]. Methods for domain adaptation under covariate shift assumptions mostly try to estimate the weights of source instances so that the weighted source distributions are most similar to the target distribution. Then, an instance-weighted classifier is trained for the target domain. For instance, the authors of [29] propose density estimators that incorporate sample selection bias to adapt two distributions. The Kernel Mean Matching (KMM) method [9], [30] then learns the weights by matching the distributions in a RKHS, with a recent extension [31] developing surrogate-kernel based kernel matching. Later, [32], [33] propose to minimize the Kullback-Leibler divergence between the target distribution and the weighted source distribution. Several recent papers [25], [34], [35] aim to estimate the optimal weights by solving least-square based formulations. Furthermore, theoretical analysis of this type of domain adaptation has been studied by [36].\nThe aforementioned studies separate the estimation of importance weights and the training of weighted learning models in two stages. Several existing machine learning methods have been explored in this framework, such as importance-weighted logistic regression [33], importance-weighted kernel regression [32], importance-weighted Gaussian processes [37], and the so-called consistent distance metric learning method [38].\nMoreover, a few studies try to unify the two stages. For instance, the authors of [39] propose a discriminative learning based adaptation that trains an integrated model to obtain both importance weights and the classifier at the same time. As another example, a so-called doubly robust covariate shift correction method [40] first trains an non-\n6\nweighted classifier model, estimates the weights of source instances, and then retrains an instance-weighted classification model with the learned weights.\nSubspace Adaptation: Another direction of domain adaptation is to extract a subspace, or newer feature representations in the data space, to model invariant parts across target and source distributions. Transferring knowledge between domains is through such learned subspaces or feature representations. For example, [23], [41] propose novel feature representations for the domain adaptation purpose. Transfer component analysis [42] was introduced to find low dimensional representations in RHKS where the target and source domains are similar. The authors of [43] try to learn a linear subspace that is suitable for knowledge transfer by minimizing Bregman divergence between target and source distributions in this subspace. A related study [44] transforms target samples such that they are a linear combination of basis from the source domain. More recently, [45] learns a domain-invariant data transformation to minimize differences between source and target distributions while preserving functional relations between data and labels. Furthermore, the authors of [46] propose to identify subspaces that align to the eigenspaces of the target and source domains."}, {"heading": "2.4.5 Sequence-based Prediction of peptide binding to MHC", "text": "As mentioned in the introduction, the proposed TSK method is general to any cross-domain sequence modeling problems. We implement TSK on another sequence-based bioinformatics task for MHC peptide binding and compare TSK with state-of-art tools on benchmark datasets.\nThe first machine learning competition in Immunology (2011) [47] compared various computational algorithms for classifying peptide binding versus non-binding sites for multiple MHC molecules. They used experimental labels of peptide binding from 3 classes of MHC molecules in humans as performance benchmarks. In 2012, the second machine learning competition in Immunology [48] increased the number of experimental datasets for both human and mouse molecules, providing training and test data for both species. The task was formulated as classifying eluted peptide (naturally processed by MHC) as positive samples while simple binding and non-binding peptides were classified as negative labeled samples.\nFor peptide-protein interaction prediction, relative positions of amino acids and their physiochemical properties play a very important role [49]. The oligo kernel proposed by [50], takes the relative\npositions into consideration by assigning a weight to each common k-mer based on their relative position in the two peptide sequences. For the 2012 Machine Learning Competition in Immunology for peptide binding prediction, [49] showed that their Generic String (GS) kernel gave state-of-the-art performance when implemented on the datasets provided by the competition from [51]. According to the authors, the GS kernel includes information regarding both physiochemical property and relative position of amino acids during k-mer comparison. We demonstrate in Section 4 that our basic mismatch string kernel (SK) yields better performance than GS kernel on the benchmark data from this competition."}, {"heading": "3 EXPERIMENTAL SETUP", "text": ""}, {"heading": "3.1 TFBS Prediction Task", "text": "We chose a family of cross-genome TFBS prediction tasks for evaluations. This set of tasks transfer knowledge from mouse genome to human genome, which is the translational research setting. For a certain TF, we train a TSK model using existing ChIP-seq TFBS data of a certain cell-type of mouse genome (source) and perform TFBS predictions of the same cell-type on human genome (target)."}, {"heading": "3.2 Datasets", "text": "There exist thousands of TFs that are common between mice and humans. The ENCODE [1] database, however, contains only 14 ChIPseq TF experiments that are available for both species. Therefore, we use these 14 available ChIP-seq datasets for our cross-context predictions. The cell-type for each of the different TFs belongs to normal blood tissue, as it has the maximum number of common TF experiments across the two genomes."}, {"heading": "3.2.1 Sequence selection method", "text": "To select training and testing sequences from the ENCODE database, we use the \u201cpeak-centric standard\u201d as described in [52]. According to this standard, there exists a single central position in each ChIPseq region that represents positive binding site for a certain TF, and all other genomic positions are negative sites. Consequently, for positive sequences, we use the 100 nucleotides surrounding each peak position. For negative samples, since there is a lack of experimental evidence showing where the negative \u201cpeak\u201d is, we randomly select 100 basepair-length subsequences from the genomic regions where\n7\npositive ChIP-seq TFBS peaks are absent 2. Coordinates of TFBS peak positions are obtained from the ChIP-seq data available in the ENCODE repository [1]."}, {"heading": "3.3 Setup", "text": "Our experiments include a few important data statistics and hyperparameters to tune:\n\u2022 Dictionary size (d): Our TFBS prediction uses DNA sequence segments as input data. These strings are made up of 4 characters (also known as nucleotides): A,T,C,G. The dictionary size is d=4. \u2022 Number of source samples (n): For each of the 14 TFs, we use its top 500 (ranked according to ChIP-seq peak enrichment) mouse TFBS sequences. For each TF task, we randomly select 500 negative samples using the strategy in Section 3.2.1. Totally, for each TF, we have a training set containing n=1000 sequences (positive=500, negative=500). \u2022 Ratio of positive to negative target samples (r): In a target context, we generated a validation set for hyperparameter tuning and a test set for performance evaluation. For each of the 14 TFs, we use the subsequent 500 human TFBS sequences each for both the positive validation set and positive testing set. However, the number of negative validation and testing samples may vary. For TFBS prediction on an unannotated context, it is not practical to assume that the ratio between positive and negative sites is always balanced. This is because the number of bound sites varies from one cell-context to another cell-context as well as across different TFs. Furthermore, the number of TFBSs is generally much smaller compared to regions without the TFBSs. Accordingly, we create target sets in which positive to negative ratio varies, simulating real-world predictions on unlabeled datasets 3. By varying negative samples in the target context and keeping the positive samples constant, we obtain\n2. We first select \u201copen\u201d DNA segments where the TFBS binding can take place. Next, we remove all possible TFBS regions for a particular TF.\n3. Intuitively, searching TFBS sites should consider all genomic positions. In reality, researchers use epigenomic evidence to filter out non-possible binding sites.\ndifferent positive to negative ratios in target data, i.e r=1:1, 1:2 and 1:3 respectively. A ratio of 1:3 means we expect 3 negative sites for every observed positive site in the target data. More specifically, we generated three cases of validation and test sets for each TF task, containing n=1000 (positive=500, negative=500), n=1500 (positive=500, negative=1000) and n=2000 (positive=500, negative=1500) sequences. \u2022 String kernel hyperparameters (k,m): For both SK and TSK, we tune two hyperparameters: the length of input kmer, k \u2208 {8, 10, 12}, and the number of allowed mismatches, m \u2208 {1, 2, 3}. We first perform hyperparameter selection using our validation sets and then evaluate model performance using the selected hyperparameters on the test sets. Hyperparameters for the SVM training (C \u2208 {0.1, 1, 10, 100, 1000}) are also selected using the validation sets.\nThe setup described above is novel. Our cross-context setup differs from previous TFBS prediction works and closely resembles a real biological scenario. The data statistics and hyperparameters used in our TFBS experiments are summarized in Table 2."}, {"heading": "3.4 Evaluation Metric", "text": "Our investigation of TSK focuses on the SVM [53] [54] classifier framework. SVMs are known to provide state-of-the-art performance for many applications [11], including a large number of successes in computational biology [55]. Binary SVM learns a real-valued function that assigns a continuous score to each candidate sequence segment based on the labeled source set. Larger scores correspond to more confident predictions as the positive class. As a result, we rank all candidate samples in the target set using SVM prediction values. This naturally leads us to utilize Area Under Curve (AUC) score (from the Receiver Operating Characteristic (ROC) curve) as our main evaluation metric. The area under the ROC curve (AUC) is commonly used as a summary measure of diagnostic accuracy. It is interpreted as the probability that a randomly selected \u201cevent\u201d will be regarded with greater suspicion (in terms of its continuous measurement) than a randomly selected \u201cnon-event\u201d. AUC score ranges between 0 and 1, where values closer to 1 indicate more successful predictions.\n8 A ve ra ge A U C S co re a cr os s 14 T F s\n0.75\n0.755\n0.76\n0.765\n0.77\n0.775\n0.78\n0.785\n0.79\n0.795\n0.8\n8,1 8,2 8,3 10,1 10,2 10,3 12,1 12,2 12,3\nTSK (1:1) TSK (1:2) TSK (1:3)\nSK (1:1) SK (1:2) SK (1:3)\nParameters (k,m)\nFig. 2: Average AUC scores from TSK and SK on TFBS predictions when varying hyperparameters (k,m). For each of the three data ratios (1:1,1:2 and 1:3) about validation sets, a curve is shown to describe the change in performance for SK and TSK with varying (k,m) values. Parameter combination of k=10 and m = 1, gives the best performance for both SK and TSK. For each data ratio, TSK outperforms the basic SK."}, {"heading": "3.5 Baseline approaches used for comparison", "text": "For TFBS prediction tasks, we compare TSK to the following baselines:\n1) SK: represents the (k,m)-mismatch kernel implementation adapted from [12]. We also ran spectrum kernel and found that its performed worse than (k,m)-mismatch SK (See Section 4.1) 2) MEME: is the state-of-the-art TFBS prediction tool that scores each sequence for potential TF binding events based on Position Weight Matrices (PWMs). The PWMs are obtained from source sequences using motif discovery algorithm MEME-ChIP [56] and then these motifs are used for scoring individual target sequences using AMA tool [57]. These tools are part of the MEME-Suite [58]. 3) CISF: CISFINDER [59] is another state-of-the-art toolbox for TFBS detection that uses an exhaustive search for DNA motifs and outputs a Position Frequency Matrix (PFM). It then scores each target sequence based on the generated PFM.\nThe source and target sequences for all above baselines are same as those used in our TSK approach."}, {"heading": "3.6 MHC PB Prediction Task", "text": "To demonstrate the generalization of our model, we implement TSK on another group of cross-context prediction tasks: predict PB to MHC proteins from mice to humans. The mouse and human datasets are provided by the 2012 Machine Learning Competition in Immunology [48]. The target human datasets have peptide bindings (PBs) for 5 different MHC molecules: HLA-A0201, HLA-B0702, HLA-B4403, HLA-B5301 and HLA-5701; while for the source mouse domain we select H2-Kb PB data. This leads to 5 different PB prediction tasks. We present the details of these datasets in Table 3. Important data statistics and hyperparameters of this task are listed as follows:\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nCISF MEME SK SK\nA v e\nra g\ne A\nU C\nS c o\nre a\nc ro\ns s 1\n4 T\nF s\nContext-Specific Cross-Context"}, {"heading": "4 RESULTS", "text": "4.1 Choice of Basic Kernel: (k,m)-mismatch kernel\nWe select (k,m)-mismatch kernel as it allows mismatches during k-mer matching of biological sequences. Such sequences are prone to mutations like substitutions, insertions, or deletions. In Table 4 we present average AUC scores from 14 TF tasks for both (k,m)mismatch kernel and spectrum kernel. As expected, the inexact\n9 0.5 0.55 0.6 0.65 0.7\nHLA-A0201 HLA-B0702 HLA-4403 HLA-B5301 HLA-B5701\nA U\nC S\nc o\nre\nTSK SK\nFig. 4: Comparison using AUC scores across 5 tasks of MHC peptide binding (PB) prediction. We use mouse dataset as source and human dataset as target. TSK outperforms SK for 4 out of 5 cases.\nmatching of (k,m)-mismatch kernel is more effective and outperforms than the exact matching of spectrum kernel, making it a natural choice for our TSK approach."}, {"heading": "4.2 Evaluation of performance: TSK successfully transfers knowledge across different species", "text": "Table 5 presents the test AUC scores from different approaches for cross-context TFBS predictions of 14 selected TFs. For each TF, training was done on TFBS data from the mouse genome (source) and testing data was obtained from the human genome (target). We select k = 10 and m = 1 for SK and TSK based on validation set performance (details in Section 4.3). TSK outperforms SK and two PWM baselines for most of the cases and is generally robust for imbalanced datasets (when r=1:2 and 1:3). Position Weight/Frequency Matrix based approaches, MEME and CISFINDER, also perform well for three TFs (USF2, RAD21 and SMC3). However, their performance is worse than SK and TSK for majority of the TFs. MEME and CISFINDER also exhibit a higher variance when considering all 14 TF prediction tasks.\nFurthermore, we calculate the conservation scores for each of the 14 different TFs we used. The conservation scores indicate how well conserved (or slowly evolving) individual positions of TFBS sequences are, for a particular TF. Conservation scores are calculated for 100 basepair-length sequences. Segments obtained from top 2000 binding sites of all 14 TFs from the human genome. For these sequences, a phyloP score for each nucleotide (total of 200,000) is calculated using the Galaxy tool [60], [61], [62]. A positive phyloP score denotes conservation (slow evolution) and a negative phyloP score denotes acceleration (fast evolution) of the nucleotide. Thus, after adding and normalizing the scores according to positively and negatively scored nucleotides for each TF, we take their log (for scaling) and subtract the acceleration scores from the conservation scores. We then include information regarding fraction of nonconserved nucleotides for each TF to our final score. Non-conserved nucleotides are those nucleotides for which phyloP scores (positive or negative) are not reported. We calculate this as a penalty term :\np = log(Cn Ct )\n100 (16)\nThe equation for calculating the final conservation score (CS) is:\nCS = log(PosScore)\u2212 log(|NegScore|)\u2212 p (17)\nwhere Cn is the count of non-conserved nucleotides and Ct is the count of total number of nucleotides (= 200, 000). PosScore denotes normalized sum of all positive phyloP scores (quantifying slow evolution) while NegScore denotes normalized sum of all negative phyloP scores (quantifying fast evolution). Conservation scores for each TF are provided in the last column of Table 5.\nFor TFs that MEME and CISFINDER give high accuracy, we observe that they also have high conservation scores. When the conservation scores are not high, SK and TSK models perform better. Thus, MEME and CISFINDER perform well on TFs with strong sequence conservation across genomes. This indicates that string kernel based approaches can be used to complement Position Weight/Frequency Matrix based approaches.\nIt is important to note that TSK consistently outperforms SK even on more conserved TFs. This demonstrates that domain adaptation technique helps to improve the basic string kernel for cross-context sequence predictions."}, {"heading": "4.3 Hyperparameter Selection: k = 10 and m = 1 gives the best performance for TFBS prediction", "text": "(k,m)-mismatch string kernel requires the tuning of two hyper parameters: k and m. Here, k is the length of the k-mer or substrings being compared in kernel calculations and m is the number of mismatches being allowed. We considered k \u2208 {8, 10, 12} and m \u2208 {1, 2, 3}. Figure 2 shows the effect of varying k and m parameters using average AUC scores (across 14 TF tasks) from validation for all three considered data ratios. The combination (k=10, m=1) achieves the best average AUC results on validation sets. We observe that for each particular ratio, TSK always outperforms basic SK on validation. We also find that value k>10 decreases the performance of SK and TSK."}, {"heading": "4.4 Same context versus cross-context: Prediction of TFBS within the target context is easier than cross-context", "text": "To justify the importance of domain adaptation, we also evaluate baseline methods under the setting that train and test are within the same target domain. That is, both the training and testing are performed on human TFBS datasets 4. In Figure 3, the prediction performance, measured by average AUC test score, across 14 TF tasks, of SK decreases when under cross-context setting (in comparison to the prediction from within the same context). However, it is worthwhile to point out that basic SK outperforms MEME and CISF for TFBS when working within the same domain."}, {"heading": "4.5 TSK is generalizable to other sequence-based prediction tasks", "text": "In addition to TFBS prediction, we successfully implement TSK for predicting PB to MHC-I complex by transferring knowledge across species. Similar to TFBS prediction, we use the mouse (source) dataset to train our model and then perform predictions (validation and testing) on the human (target) dataset, i.e. a useful translational setting. Our results indicate that TSK can be generalized to any crosscontext task that involves sequence-based classification.\n4. We generate a new training set from human TFBS data (r=1:1)\n10\n\u2022 Evaluation of performance: As shown in Figure 4, TSK performs better than the basic SK model on 4 out of 5 PBprediction tasks across species. \u2022 Hyperparameter Selection: The hyperparamter tuning gave best-performing hyperparameters that were different across 5 tasks as well as across TSK and SK approaches. Figure 5 presents tuning results for all 5 MHC PB prediction tasks, where training was done on the mouse H2-Kb MHC PB dataset and validation was done on train data for human provided by 2012 competition in Immunology [48]. Each task uses a different test dataset for human model evaluation. The best performing k and m values are selected for each task respectively. \u2022 Same context versus cross-context: Once again, we observe\nin Figure 6 that the task of PB prediction is easier when done within the target (human) domain. That is, when both training and test datasets are from human domain. In this setting, our basic SK model (blue bars) gives good performance and even outperforms GS kernel [49], the best team for 2012 competition citemlcompimmune. This shows that SK is a good choice for implementation of domain adaptation technique. The grey bar in Figure 6 shows that the performance of basic SK decreased drastically when predictions are made across contexts. Therefore, indicating the need to propose TSK for cross-context settings. 5"}, {"heading": "5 DISCUSSION", "text": "Determining how TF proteins interact with DNA to regulate context specific gene regulation is essential for fully understanding biological processes and diseases. Most previous computational tools for predicting TFBSs assume the same distribution across target and source contexts. We, however, have shown that it is beneficial to consider the distribution shift. The proposed TSK method improves the performance of sequence-driven TFBS predictions by accommodating differences among underlying sample distributions and applying knowledge transfer from the source to target context. We have also examined the imbalanced (positive to negative ratio) data issue in the target domain, to ensure realistic and robust TFBS predictions. Our experimental results indicate:\n\u2022 TSK overall improves performance of string kernel on crosscontext TFBS predictions; \u2022 TSK and SK outperform the state-of-the-art Position Weight/Frequency Matrix based TFBS tools, especially for less conserved TFs, and can be used as complementary tools to the latter; \u2022 TSK can be easily generalized to other sequence based prediction tasks, e.g. prediction of PB to MHC-I complex molecules.\n5. We also apply TSK to this within-context setting and get following AUC scores: 0.8325 (HLA-A0201), 0.9233 (HLA-B0702), 0.9282 (HLA-B4403), 0.8599 (HLA-B5301) and 0.82 (HLA-B5701). Interestingly, we find that TSK adapts and reduces the differences among the training and testing datasets and gives slightly better performance on 3 out of 5 MHC PB prediction tasks (HLAA0201, HLA-B0702 and HLA-B4403). SK\u2019s performance is represented by blue bars in Figure 6\n11\nThe code for TSK has been made available at www.github.com/ QData/TransferStringKernel. TSK is a general sequence modeling architecture. Therefore, our future plan is to extend TSK to a wider variety of sequence based applications. We would also like to study and compare implementation of other kernels from the string kernel family as components of the TSK approach. In the meantime, we are aware of the limitations of TSK due to its high computational complexity when handling datasets with a large number of source and target samples. We plan to explore random sampling based techniques to scale up TSK on \u201clarge-scale\" sequence mining tasks."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Dr. Mazhar Adli (Department of Biochemistry and Molecular Genetics, University of Virginia) for helpful discussions throughout the progress of this project.\nFUNDING This work was supported by the National Science Foundation under NSF CAREER award No. 1453580. Any Opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect those of the National Science Foundation."}], "references": [{"title": "An integrated encyclopedia of dna elements in the human genome", "author": ["E.P. Consortium"], "venue": "Nature, vol. 489, no. 7414, pp. 57\u201374, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A census of human transcription factors: function, expression and evolution", "author": ["J.M. Vaquerizas", "S.K. Kummerfeld", "S.A. Teichmann", "N.M. Luscombe"], "venue": "Nature Reviews Genetics, vol. 10, no. 4, pp. 252\u2013263, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Evolution of transcription factor binding sites in mammalian gene regulatory regions: conservation and turnover", "author": ["E.T. Dermitzakis", "A.G. Clark"], "venue": "Molecular biology and evolution, vol. 19, no. 7, pp. 1114\u20131121, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Chip\u2013seq: advantages and challenges of a maturing technology", "author": ["P.J. Park"], "venue": "Nature Reviews Genetics, vol. 10, no. 10, pp. 669\u2013680, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast string kernels using inexact matching for protein sequences", "author": ["C. Leslie", "R. Kuang"], "venue": "The Journal of Machine Learning Research, vol. 5, pp. 1435\u20131455, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Sequence and chromatin determinants of cell-type\u2013specific transcription factor binding", "author": ["A. Arvey", "P. Agius", "W.S. Noble", "C. Leslie"], "venue": "Genome research, vol. 22, no. 9, pp. 1723\u20131734, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The evolution of gene regulation by transcription factors and micrornas", "author": ["K. Chen", "N. Rajewsky"], "venue": "Nature Reviews Genetics, vol. 8, no. 2, pp. 93\u2013103, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis of kernel mean matching under covariate shift", "author": ["Y.-l. Yu", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. 607\u2013614.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Covariate shift by kernel mean matching", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning, vol. 3, no. 4, p. 5, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical analysis of domain adaptation algorithms for genomic sequence analysis", "author": ["G. Schweikert", "G. R\u00e4tsch", "C. Widmer", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 1433\u20131440.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable algorithms for string kernels with inexact matching.", "author": ["P.P. Kuksa", "P.-H. Huang", "V. Pavlovic"], "venue": "in NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling the specificity of protein-dna interactions", "author": ["G.D. Stormo"], "venue": "Quantitative Biology, vol. 1, no. 2, pp. 115\u2013130, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Jaspar 2014: an extensively expanded and updated open-access database of transcription factor binding profiles", "author": ["A. Mathelier", "X. Zhao", "A.W. Zhang", "F. Parcy", "R. Worsley-Hunt", "D.J. Arenillas", "S. Buchman", "C.-y. Chen", "A. Chou", "H. Ienasescu"], "venue": "Nucleic acids research, p. gkt997, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast kernels for string and tree matching", "author": ["S. Vishwanathan", "A.J. Smola"], "venue": "Kernel methods in computational biology, pp. 113\u2013130, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Profile-based string kernels for remote homology detection and motif extraction", "author": ["R. Kuang", "E. Ie", "K. Wang", "K. Wang", "M. Siddiqi", "Y. Freund", "C. Leslie"], "venue": "Journal of bioinformatics and computational biology, vol. 3, no. 03, pp. 527\u2013550, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Cluster kernels for semisupervised learning", "author": ["O. Chapelle", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems, 2002, pp. 585\u2013592.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "A unified multitask architecture for predicting local protein properties", "author": ["Y. Qi", "M. Oja", "J. Weston", "W.S. Noble"], "venue": "PloS one, vol. 7, no. 3, p. e32235, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "2001.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey of transfer and multitask learning in bioinformatics", "author": ["Q. Xu", "Q. Yang"], "venue": "Journal of Computing Science and Engineering, vol. 5, no. 3, pp. 257\u2013268, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Information-theoretical learning of discriminative clusters for unsupervised domain adaptation", "author": ["Y. Shi", "F. Sha"], "venue": "arXiv preprint arXiv:1206.6438, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 999\u20131006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Importance-weighted least-squares probabilistic classifier for covariate shift adaptation with application to human activity recognition", "author": ["H. Hachiya", "M. Sugiyama", "N. Ueda"], "venue": "Neurocomputing, vol. 80, pp. 93\u2013101, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain invariant transfer kernel learning", "author": ["M. Long", "J. Wang", "J. Sun", "P.S. Yu"], "venue": "2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Application of covariate shift adaptation techniques in brain\u2013computer interfaces", "author": ["Y. Li", "H. Kambara", "Y. Koike", "M. Sugiyama"], "venue": "Biomedical Engineering, IEEE Transactions on, vol. 57, no. 6, pp. 1318\u20131324, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of statistical planning and inference, vol. 90, no. 2, pp. 227\u2013244, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Correcting sample selection bias in maximum entropy density estimation", "author": ["M. Dud\u00edk", "S.J. Phillips", "R.E. Schapire"], "venue": "Advances in neural information processing systems, 2005, pp. 323\u2013330.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A.J. Smola", "A. Gretton", "K.M. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems, vol. 19, p. 601, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Covariate shift in hilbert space: A solution via sorrogate kernels", "author": ["K. Zhang", "V. Zheng", "Q. Wang", "J. Kwok", "Q. Yang", "I. Marsic"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 388\u2013395.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["M. Sugiyama", "M. Krauledat", "K.-R. M\u00fcller"], "venue": "The Journal of Machine Learning Research, vol. 8, pp. 985\u20131005, 2007.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Direct density ratio estimation for large-scale covariate shift adaptation.", "author": ["Y. Tsuboi", "H. Kashima", "S. Hido", "S. Bickel", "M. Sugiyama"], "venue": "Information and Media Technologies,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1391\u20131445, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative density-ratio estimation for robust distribution comparison", "author": ["M. Yamada", "T. Suzuki", "T. Kanamori", "H. Hachiya", "M. Sugiyama"], "venue": "Neural computation, vol. 25, no. 5, pp. 1324\u20131370, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning bounds for importance weighting", "author": ["C. Cortes", "Y. Mansour", "M. Mohri"], "venue": "Advances in neural information processing systems, 2010, pp. 442\u2013450.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "No bias left behind: Covariate shift adaptation for discriminative 3d pose estimation", "author": ["M. Yamada", "L. Sigal", "M. Raptis"], "venue": "Computer Vision\u2013ECCV 2012. Springer, 2012, pp. 674\u2013687.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning under covariate shift", "author": ["B. Cao", "X. Ni", "J.-T. Sun", "G. Wang", "Q. Yang"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 1204.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative learning under covariate shift", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 2137\u20132155, 2009.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Doubly robust covariate shift correction", "author": ["S.J. Reddi", "B. P\u00f3czos", "A. Smola"], "venue": "2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum\u00e9"], "venue": "arXiv preprint arXiv:0907.1815, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1815}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 2, pp. 199\u2013210, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Bregman divergence-based regularization for transfer subspace learning", "author": ["S. Si", "D. Tao", "B. Geng"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 7, pp. 929\u2013942, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Low-rank transfer subspace learning", "author": ["M. Shao", "C. Castillo", "Z. Gu", "Y. Fu"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 1104\u20131109.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain generalization via invariant feature representation", "author": ["K. Muandet", "D. Balduzzi", "B. Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1301.2115, 2013.  12", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Subspace alignment for domain adaptation", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "arXiv preprint arXiv:1409.5241, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning competition in immunology\u2013prediction of hla class i binding peptides", "author": ["G.L. Zhang", "H.R. Ansari", "P. Bradley", "G.C. Cawley", "T. Hertz", "X. Hu", "N. Jojic", "Y. Kim", "O. Kohlbacher", "O. Lund"], "venue": "Journal of immunological methods, vol. 374, no. 1, pp. 1\u20134, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Mhc-np: Predicting peptides naturally processed by the mhc", "author": ["S. Gigu\u00e8re", "A. Drouin", "A. Lacoste", "M. Marchand", "J. Corbeil", "F. Laviolette"], "venue": "Journal of immunological methods, vol. 400, pp. 30\u201336, 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Oligo kernels for datamining on biological sequences: a case study on prokaryotic translation initiation sites", "author": ["P. Meinicke", "M. Tech", "B. Morgenstern", "R. Merkl"], "venue": "BMC bioinformatics, vol. 5, no. 1, p. 169, 2004.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2004}, {"title": "Toward more accurate pan-specific mhc-peptide binding prediction: a review of current methods and tools", "author": ["L. Zhang", "K. Udaka", "H. Mamitsuka", "S. Zhu"], "venue": "Briefings in bioinformatics, vol. 13, no. 3, pp. 350\u2013364, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Epigenetic priors for identifying active transcription factor binding sites", "author": ["G. Cuellar-Partida", "F.A. Buske", "R.C. McLeay", "T. Whitington", "W.S. Noble", "T.L. Bailey"], "venue": "Bioinformatics, vol. 28, no. 1, pp. 56\u201362, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Universit\u00e4t Dortmund, Tech. Rep., 1999.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1999}, {"title": "Advances in kernel methods: support vector learning", "author": ["B. Sch\u00f6lkopf", "C.J. Burges"], "venue": "MIT press,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1999}, {"title": "Kernel methods in computational biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "J.-P. Vert"], "venue": "MIT press,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2004}, {"title": "Meme-chip: motif analysis of large dna datasets", "author": ["P. Machanick", "T.L. Bailey"], "venue": "Bioinformatics, vol. 27, no. 12, pp. 1696\u20131697, 2011.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Assigning roles to dna regulatory motifs using comparative genomics", "author": ["F.A. Buske", "M. Bod\u00e9n", "D.C. Bauer", "T.L. Bailey"], "venue": "Bioinformatics, vol. 26, no. 7, pp. 860\u2013866, 2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Meme suite: tools for motif discovery and searching", "author": ["T.L. Bailey", "M. Boden", "F.A. Buske", "M. Frith", "C.E. Grant", "L. Clementi", "J. Ren", "W.W. Li", "W.S. Noble"], "venue": "Nucleic acids research, vol. 37, pp. W202\u2013W208, 2009.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "Exhaustive search for over-represented dna sequence motifs with cisfinder", "author": ["A.A. Sharov", "M.S. Ko"], "venue": "DNA research, vol. 16, no. 5, pp. 261\u2013273, 2009.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences", "author": ["J. Goecks", "A. Nekrutenko", "J. Taylor", "T.G. Team"], "venue": "Genome Biol, vol. 11, no. 8, p. R86, 2010.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Galaxy: A web-based genome analysis tool for experimentalists", "author": ["D. Blankenberg", "G.V. Kuster", "N. Coraor", "G. Ananda", "R. Lazarus", "M. Mangan", "A. Nekrutenko", "J. Taylor"], "venue": "Current protocols in molecular biology, pp. 19\u2013 10, 2010.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "As an important application of sequence based mining, the task of predicting Transcription Factor Binding Sites (TFBSs) on genomes has attracted much attention over the years [1].", "startOffset": 175, "endOffset": 178}, {"referenceID": 1, "context": "While many TFs are highly conserved among different species [2], conservation of their binding sequence is low [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "While many TFs are highly conserved among different species [2], conservation of their binding sequence is low [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "Owing to the development of chromatin immunoprecipitation and massively parallel DNA sequencing (ChIP-seq) technologies [4], maps of genome-wide binding sites are currently available for multiple TFs in a few cell types across human and mouse genomes via the ENCODE [1] database.", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "Owing to the development of chromatin immunoprecipitation and massively parallel DNA sequencing (ChIP-seq) technologies [4], maps of genome-wide binding sites are currently available for multiple TFs in a few cell types across human and mouse genomes via the ENCODE [1] database.", "startOffset": 266, "endOffset": 269}, {"referenceID": 4, "context": "String kernel techniques under the support vector machine (SVM) classification framework have been successfully used for detecting patterns of DNA or protein sequences before [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "Recently, [6] extended this discriminative SK+SVM classification", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Their results [6] have suggested that traditional motif-driven approaches (details in Section 2.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Despite the exciting aforementioned framework [6], one shortcoming remains significant: this method assumes that the source and target samples are drawn from the same probability distribution.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "This also connects to observations in the literature that the TFBS sequences have low conservation across species even though many TF proteins are conserved across large evolutionary distances [7].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "When implementing cross context prediction, it is desirable to design algorithms that remain effective under such distribution shifts [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 8, "context": "While the problem may be unsolvable if the source and target distributions share nothing in common, recent machine learning studies [9] have provided effective adaptation for closely related distributions.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "Biologically, this also makes sense since closely related cell contexts should have similar gene regulation processes and similar TFBS sequence patterns [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "its distribution more closely matches that of the target data [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "This connects to a more general machine-learning topic called domain adaptation [10] (discussed in Section 2.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "String kernels [5] [12], implicitly compute an inner product in the mapped feature space \u03c6(x) as:", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "String kernels [5] [12], implicitly compute an inner product in the mapped feature space \u03c6(x) as:", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "The string kernel using this representation is called spectrum kernel [5], where the spectrum representation counts the occurrences of each nucleotide k-mer in a DNA sequence segment.", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "In string kernels, this is typically achieved by using different families of mismatches [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "We use the concept of (k,m)-mismatch string kernels [12] (illustrated in Figure 1(c)), which considers k-mer counts with m inexact matching of k-mers (e.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "Researchers have proposed various strategies [5], [12] to address these computational difficulties.", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "Researchers have proposed various strategies [5], [12] to address these computational difficulties.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "We adopt a statistical strategy from [12] that provides linear-time string kernel computations and scales well with dictionary size and input length.", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "To consider the variation between source and target samples in our application, domain adaptation [10], [13] serves as a natural candidate to tackle this computational challenge.", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "To consider the variation between source and target samples in our application, domain adaptation [10], [13] serves as a natural candidate to tackle this computational challenge.", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "TSK revises the (k,m)-mismatch string kernel framework using a \u201cKernel Mean Matching\u201d (KMM) strategy [9] in order to perform knowledge transfer.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "Specifically, TSK adapts string kernel under the \u201ccovariate shift\u201d assumption [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "The key to correcting this type of sampling bias is to estimate the \u201cimportance weight\u201d for each source sample [8]:", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "Equation 9 involves a quadratic program which can be efficiently solved using the interior point methods or any other subsequent procedures such as projected gradient optimization method [9].", "startOffset": 187, "endOffset": 190}, {"referenceID": 12, "context": "Relying on a set of known transcription factor binding sites (TFBSs) for a given TF, the binding preference is generally represented in the form of a position weight matrix (PWM) [14], [15] (also called positionspecific scoring matrix) derived from a position frequency matrix (PFM).", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "Relying on a set of known transcription factor binding sites (TFBSs) for a given TF, the binding preference is generally represented in the form of a position weight matrix (PWM) [14], [15] (also called positionspecific scoring matrix) derived from a position frequency matrix (PFM).", "startOffset": 185, "endOffset": 189}, {"referenceID": 5, "context": "[6] have suggested that traditional motif-driven approaches are not always sufficient to accurately account for celltype specific binding profiles.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "most accurate results, such as for remote protein fold and homology detections [5], [12].", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "most accurate results, such as for remote protein fold and homology detections [5], [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "Aside from spectrum kernel [5] and (k,m)mismatch string kernel [12] introduced in the previous section, a few other notable string kernels include (but are not limited to): (1) The gapped kernel calculates dot-product of (non-contiguous) k-mer counts with gaps allowed between elements.", "startOffset": 27, "endOffset": 30}, {"referenceID": 10, "context": "Aside from spectrum kernel [5] and (k,m)mismatch string kernel [12] introduced in the previous section, a few other notable string kernels include (but are not limited to): (1) The gapped kernel calculates dot-product of (non-contiguous) k-mer counts with gaps allowed between elements.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "(2) More generally, the substring kernel [16] measures similarity between sequences based on common co-occurrence of exact sub-patterns (e.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "(3) The profile kernel [17] uses the notion of similarity based on a probabilistic model (e.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "(4) Under the semi-supervised setting, the so-called \u201csequence neighborhood\u201d kernel or \u201ccluster\u201d kernel [18] proposes a semi-supervised extension of string kernel, which replaces every sequence with a set of \u201csimilar\u201d (neighboring) sequences and a new representation is obtained by averaging representations of these neighboring sequences found in the unlabeled data using a certain sequence similarity measure.", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "Many successful works in the field of natural language processing [19] belong to this category in which each position of input sequences can be annotated with tags indicating parts of speech, named entities, semantic roles, etc.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Popular bioinformatics tasks predicting proteins\u2019 local functional properties [20] have been modeled as a labeling or tagging of amino acids on proteins.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "Multiple classic machine learning methods have been benchmark tools for sequence tagging, like conditional random field [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "the case of TSK) [8].", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "This problem has been investigated in both statistics and machine learning under various assumptions [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "A number of previous methods have been proposed to estimate the \u201cimportance weight\u201d \u03b2 from finite samples, including kernel mean matching (KMM), logistic regression, KL importance estimation and many others [8].", "startOffset": 207, "endOffset": 210}, {"referenceID": 11, "context": "More generally, TSK belongs to topics of domain adaptation and transfer learning [13] [10], where one aims to use data or models of a well-analyzed source domain to obtain or refine models for a less analyzed target domain.", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "More generally, TSK belongs to topics of domain adaptation and transfer learning [13] [10], where one aims to use data or models of a well-analyzed source domain to obtain or refine models for a less analyzed target domain.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "For such scenarios, knowledge transfer will potentially improve the performance of learning by avoiding or reducing the expensive data-labeling efforts [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 20, "context": "Transfer learning can help in reusing knowledge from annotated datasets to new domains [22], reducing the knowledge gap of labeled data due to heterogenous variations.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "For example, one previous study [10] explored \u201cdomain adaptation\u201d strategies for a task of sequence-based prediction for acceptor splice sites.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "By exploiting labeled training samples from a different but related source domain, a variety of previous studies have demonstrated its effectiveness on multiple applications including sentiment analysis [23], object classification [24], activity recognition [25], text categorization [26] and Brain-computer interface (BCI) tasks [27].", "startOffset": 203, "endOffset": 207}, {"referenceID": 22, "context": "By exploiting labeled training samples from a different but related source domain, a variety of previous studies have demonstrated its effectiveness on multiple applications including sentiment analysis [23], object classification [24], activity recognition [25], text categorization [26] and Brain-computer interface (BCI) tasks [27].", "startOffset": 231, "endOffset": 235}, {"referenceID": 23, "context": "By exploiting labeled training samples from a different but related source domain, a variety of previous studies have demonstrated its effectiveness on multiple applications including sentiment analysis [23], object classification [24], activity recognition [25], text categorization [26] and Brain-computer interface (BCI) tasks [27].", "startOffset": 258, "endOffset": 262}, {"referenceID": 24, "context": "By exploiting labeled training samples from a different but related source domain, a variety of previous studies have demonstrated its effectiveness on multiple applications including sentiment analysis [23], object classification [24], activity recognition [25], text categorization [26] and Brain-computer interface (BCI) tasks [27].", "startOffset": 284, "endOffset": 288}, {"referenceID": 25, "context": "By exploiting labeled training samples from a different but related source domain, a variety of previous studies have demonstrated its effectiveness on multiple applications including sentiment analysis [23], object classification [24], activity recognition [25], text categorization [26] and Brain-computer interface (BCI) tasks [27].", "startOffset": 330, "endOffset": 334}, {"referenceID": 26, "context": "Covariate shift assumes that the marginal distributions of the source and target instances differ while the conditional distribution of the target output remains the same across domains [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 27, "context": "For instance, the authors of [29] propose density estimators that incorporate sample selection bias to adapt two distributions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "The Kernel Mean Matching (KMM) method [9], [30] then learns the weights by matching the distributions in", "startOffset": 38, "endOffset": 41}, {"referenceID": 28, "context": "The Kernel Mean Matching (KMM) method [9], [30] then learns the weights by matching the distributions in", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "a RKHS, with a recent extension [31] developing surrogate-kernel based kernel matching.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "Later, [32], [33] propose to minimize the Kullback-Leibler divergence between the target distribution and the weighted source distribution.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "Later, [32], [33] propose to minimize the Kullback-Leibler divergence between the target distribution and the weighted source distribution.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "Several recent papers [25], [34], [35] aim to estimate the optimal weights by solving least-square based formulations.", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "Several recent papers [25], [34], [35] aim to estimate the optimal weights by solving least-square based formulations.", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "Several recent papers [25], [34], [35] aim to estimate the optimal weights by solving least-square based formulations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 34, "context": "Furthermore, theoretical analysis of this type of domain adaptation has been studied by [36].", "startOffset": 88, "endOffset": 92}, {"referenceID": 31, "context": "Several existing machine learning methods have been explored in this framework, such as importance-weighted logistic regression [33],", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "importance-weighted kernel regression [32], importance-weighted Gaussian processes [37], and the so-called consistent distance metric learning method [38].", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "importance-weighted kernel regression [32], importance-weighted Gaussian processes [37], and the so-called consistent distance metric learning method [38].", "startOffset": 83, "endOffset": 87}, {"referenceID": 36, "context": "importance-weighted kernel regression [32], importance-weighted Gaussian processes [37], and the so-called consistent distance metric learning method [38].", "startOffset": 150, "endOffset": 154}, {"referenceID": 37, "context": "For instance, the authors of [39] propose a discriminative learning based adaptation that trains an integrated model to obtain both importance weights and the classifier at the same time.", "startOffset": 29, "endOffset": 33}, {"referenceID": 38, "context": "As another example, a so-called doubly robust covariate shift correction method [40] first trains an non-", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "For example, [23], [41] propose novel feature representations for the domain adaptation purpose.", "startOffset": 13, "endOffset": 17}, {"referenceID": 39, "context": "For example, [23], [41] propose novel feature representations for the domain adaptation purpose.", "startOffset": 19, "endOffset": 23}, {"referenceID": 40, "context": "Transfer component analysis [42] was introduced to find low dimensional representations in RHKS where the target and source domains are similar.", "startOffset": 28, "endOffset": 32}, {"referenceID": 41, "context": "The authors of [43] try to learn a linear subspace that is suitable for knowledge transfer by minimizing Bregman divergence between target and source distributions in this subspace.", "startOffset": 15, "endOffset": 19}, {"referenceID": 42, "context": "A related study [44] transforms target samples such that they are a linear combination of basis from the source domain.", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "More recently, [45] learns a domain-invariant data transformation to minimize differences between source and target distributions while preserving functional relations between data and labels.", "startOffset": 15, "endOffset": 19}, {"referenceID": 44, "context": "Furthermore, the authors of [46] propose to identify subspaces that align to the eigenspaces of the target and source domains.", "startOffset": 28, "endOffset": 32}, {"referenceID": 45, "context": "The first machine learning competition in Immunology (2011) [47] compared various computational algorithms for classifying peptide binding versus non-binding sites for multiple MHC molecules.", "startOffset": 60, "endOffset": 64}, {"referenceID": 46, "context": "For peptide-protein interaction prediction, relative positions of amino acids and their physiochemical properties play a very important role [49].", "startOffset": 141, "endOffset": 145}, {"referenceID": 47, "context": "The oligo kernel proposed by [50], takes the relative positions into consideration by assigning a weight to each common k-mer based on their relative position in the two peptide sequences.", "startOffset": 29, "endOffset": 33}, {"referenceID": 46, "context": "For the 2012 Machine Learning Competition in Immunology for peptide binding prediction, [49] showed that their Generic String (GS) kernel gave state-of-the-art performance when implemented on the datasets provided by the competition from [51].", "startOffset": 88, "endOffset": 92}, {"referenceID": 48, "context": "For the 2012 Machine Learning Competition in Immunology for peptide binding prediction, [49] showed that their Generic String (GS) kernel gave state-of-the-art performance when implemented on the datasets provided by the competition from [51].", "startOffset": 238, "endOffset": 242}, {"referenceID": 0, "context": "The ENCODE [1] database, however, contains only 14 ChIPseq TF experiments that are available for both species.", "startOffset": 11, "endOffset": 14}, {"referenceID": 49, "context": "To select training and testing sequences from the ENCODE database, we use the \u201cpeak-centric standard\u201d as described in [52].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "Coordinates of TFBS peak positions are obtained from the ChIP-seq data available in the ENCODE repository [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 50, "context": "Our investigation of TSK focuses on the SVM [53] [54] classifier framework.", "startOffset": 44, "endOffset": 48}, {"referenceID": 51, "context": "Our investigation of TSK focuses on the SVM [53] [54] classifier framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 52, "context": "SVMs are known to provide state-of-the-art performance for many applications [11], including a large number of successes in computational biology [55].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "1) SK: represents the (k,m)-mismatch kernel implementation adapted from [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 53, "context": "The PWMs are obtained from source sequences using motif discovery algorithm MEME-ChIP [56] and then these motifs are used for scoring individual target sequences using AMA tool [57].", "startOffset": 86, "endOffset": 90}, {"referenceID": 54, "context": "The PWMs are obtained from source sequences using motif discovery algorithm MEME-ChIP [56] and then these motifs are used for scoring individual target sequences using AMA tool [57].", "startOffset": 177, "endOffset": 181}, {"referenceID": 55, "context": "These tools are part of the MEME-Suite [58].", "startOffset": 39, "endOffset": 43}, {"referenceID": 56, "context": "3) CISF: CISFINDER [59] is another state-of-the-art toolbox for TFBS detection that uses an exhaustive search for DNA motifs and outputs a Position Frequency Matrix (PFM).", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "\u2022 Baselines: Since no runnable tools exist for both oligo kernels and GS kernels, we use (k,m)-mismatch kernels from [12] as a baseline.", "startOffset": 117, "endOffset": 121}, {"referenceID": 57, "context": "For these sequences, a phyloP score for each nucleotide (total of 200,000) is calculated using the Galaxy tool [60], [61], [62].", "startOffset": 111, "endOffset": 115}, {"referenceID": 58, "context": "For these sequences, a phyloP score for each nucleotide (total of 200,000) is calculated using the Galaxy tool [60], [61], [62].", "startOffset": 117, "endOffset": 121}, {"referenceID": 46, "context": "Moreover, our mismatch SK baseline outperforms Generic String Kernel [49] on 4 out of 5 tasks, making it a good choice for implementation of TSK on this type of task.", "startOffset": 69, "endOffset": 73}, {"referenceID": 46, "context": "In this setting, our basic SK model (blue bars) gives good performance and even outperforms GS kernel [49], the best team for 2012 competition citemlcompimmune.", "startOffset": 102, "endOffset": 106}], "year": 2016, "abstractText": "Through sequence-based classification, this paper tries to accurately predict the DNA binding sites of transcription factors (TFs) in an unannotated cellular context. Related methods in the literature fail to perform such predictions accurately, since they do not consider sample distribution shift of sequence segments from an annotated (source) context to an unannotated (target) context. We, therefore, propose a method called \u201cTransfer String Kernel\u201d (TSK) that achieves improved prediction of transcription factor binding site (TFBS) using knowledge transfer via cross-context sample adaptation. TSK maps sequence segments to a high-dimensional feature space using a discriminative mismatch string kernel framework. In this high-dimensional space, labeled examples of the source context are re-weighted so that the revised sample distribution matches the target context more closely. We have experimentally verified TSK for TFBS identifications on fourteen different TFs under a cross-organism setting. We find that TSK consistently outperforms the state-of-the-art TFBS tools, especially when working with TFs whose binding sequences are not conserved across contexts. We also demonstrate the generalizability of TSK by showing its cutting-edge performance on a different set of cross-context tasks for the MHC peptide binding predictions.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}