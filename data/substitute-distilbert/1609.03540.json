{"id": "1609.03540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data", "abstract": "causal inference from observational data is a subject during active simulation and testing applications statistics and computer science. many toolkits haven been developed for this purpose that depends on statistical software. oddly, these programs don't scale to large datasets. in this paper we describe a suite of products for expressing causal inference concluded from observational entities in vivo. this suite supports the state - of - the - art methods for causal inference and run at scale within a java engine. in addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. we evaluate the quality and performance of its techniques by experiments of real datasets.", "histories": [["v1", "Mon, 12 Sep 2016 19:24:14 GMT  (359kb,D)", "https://arxiv.org/abs/1609.03540v1", null], ["v2", "Tue, 13 Sep 2016 01:59:05 GMT  (819kb)", "http://arxiv.org/abs/1609.03540v2", null]], "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.LG cs.PF", "authors": ["babak salimi", "dan suciu"], "accepted": false, "id": "1609.03540"}, "pdf": {"name": "1609.03540.pdf", "metadata": {"source": "CRF", "title": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data", "authors": ["Babak Salimi"], "emails": ["suciu}@cs.washington.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n03 54\n0v 2\n[ cs\n.D B\n] 1\n3 Se"}, {"heading": "1 Introduction", "text": "Much of the success of Big data today comes from predictive or descriptive analytics : statistical models or data mining algorithms applied to data to predict new or future observations, e.g., we observe how users click on ads, then build a model and predict how future users will click. Predictive analysis/modeling is central to many scientific fields, such as bioinformatics and natural language processing, in other fields - such as social economics, psychology, education and environmental science - researchers are focused on testing and evaluating causal hypotheses. While the distinction between causal and predictive analysis has been recognized, the conflation between the two is common.\nCausal inference has been studied extensively in statistics and computer science [9,30,13,24,25]. Many tools perform causal inference using statistical software such as SAS, SPSS, or R project. However, these toolkits do not scale to large datasets. Furthermore, in many of the most interesting Big Data settings, the data is highly relational (e.g, social networks, biological networks, sensor networks and more) and likely to pour into SQL systems. There is a rich ecosystem of tools and organizational requirements that encourage this. Transferring data from DBMS to statistical softwares or connecting these softwares to DBMS can be error prone, difficult, time consuming and inefficient. For these cases, it would be helpful to push statistical methods for causal inference into the DBMS.\nBoth predictive and causal analysis are needed to generate and test theories, policy and decision making and to evaluate hypotheses, yet each plays a different\nrole in doing so. In fact, performing predictive analysis to address questions that are causal in nature could lead to a flood of false discovery claims. In many cases, researchers who want to discover causality from data analysis settle for predictive analysis either because they think it is causal or lack of available alternatives.\nThis work introduces ZaliQL,1 a SQL-based framework for drawing causal inference that circumvents the scalability issue with the existing tools. ZaliQL supports state-of-the-art methods for causal inference and runs at scale within a database engine. We show how to express the existing advanced causal inference methods in SQL, and develop a series of optimization techniques allowing our system to scale to billions of records. We evaluate our system on a real dataset. Before describing the contributions of this paper, we illustrate causal inference on the following real example.\nExample 1. FlightDelay. Flight delays pose a serious and widespread problem in the United States and significantly strain on the national air travel system, costing society many billions of dollars each year [3]. According to FAA statistics,2 weather causes approximately 70% of the delays in the US National Airspace System (NAS). The upsetting impact of weather conditions on aviation is well known, however quantifying the causal impact of different weather types on flight delays at different airports is essential for evaluating approaches to reduce these delays. Even though predictive analysis, in this context, might help make certain policies, this problem is causal. We conduct this causal analysis as a running example through this paper. To this end, we acquired flight departure\n1 The prefix Zali refers to al-Ghzali (1058-1111), a medieval Persian philosopher. It is known that David Hume (1711-1776), a Scottish philosopher, who gave the first explicit definition of causation in terms of counterfactuals, was heavily influenced by al-Ghzali\u2019s conception of causality [36]. 2 National Aviation Statistic http://www.faa.gov/\ndetails for all commercial flights within the US from 2000 to 2015 (105M entries) and integrated it with the relevant historical weather data (35M entries) (see Section 5.1). These are relatively large data sets for causal inference that can not be handseled by the existing tools. Table 1 presents the list of attributes from each data set that is relevant to our analysis.\nWhen we make predictive analysis, whether we predict E[Y |X = x] or Pr(Y |X = x) or something more complicated, we essentially want to know the conditional distribution of Y givenX . On the other hand, when we make a causal analysis, we want to understand the distribution of Y , if the usual mechanisms controlling X were intervened and set to x. In other words, in causal analysis we are interested in interventional conditional distribution, e.g., the distribution obtained by (hypothetically) enforcing X = x uniformly over the population. In causal analysis, the difficulty arises from the fact that here the objective is to estimate (unobserved) counterfactuals from the (observed) factual premises.\nExample 2. FlightDelay (Cont.). Suppose we want to explore the effect of low-pressure on flight departure delays. High pressure is generally associated with clear weather, while low-pressure is associated with unsettled weather, e.g., cloudy, rainy, or snowy weather. Therefore, conducting any sort of predictive analysis identifies low-pressure as a predictor for flight delays. However, lowpressure does not have any causal impact on departure delay (low-pressure only requires longer takeoff distance) [39]. That is, low-pressure is most highly a correlated attribute with flight delays, however ZaliQL found that other attributes such as thunder, low-visibility, high-wind-speed and snow have the largest causal effect on flight delays (see Sec. 5.2); this is confirmed by the results reported by the FAA and [1].\nThis paper describes novel techniques implementing and optimizing state-ofthe-art causal inference methods (reviewed in Section 2) in relational databases. We make three contributions. First, in Section 3 we describe the basic relational implementation of the main causal inference methods: matching and subclassification. Second, in Section 4 we describe a suite of optimization techniques for subclassfication, both in the online and offline setting. Finally, in Section 5 we conduct an extensive empirical evaluation of ZaliQL, our system that implements these techniques, on real data from the U.S. DOT and Weather Underground [6,38]."}, {"heading": "2 Background: Causality Inference in Statistics", "text": "The basic causal model in statistics is called the Neyman-Rubin Causal Model (NRCM). This framework views causal effect as comparisons between potential outcomes defined on the same units. This section describes the basic framework.\nAverage Treatment Effect (ATE) In the NRCM we are given a table R(T,X, Y (0), Y (1)) with N rows called units, indexed by i = 1 . . .N ; see Table 2. The binary attribute T is called treatment assignment (T = 1 means the\nunit was treated; T = 0 means the unit was subjected to control); X is a vector of attributes called covariates, unaffected by treatment; and the two attributes Y (0), Y (1) represent potential outcomes: Y (1) is the outcome of the unit if it is exposed to the treatment and Y (0) is the outcome when it is exposed to the control. For any attribute Z we write Zi for the value of the i\u2019s unit. The effect caused by the treatment for the ith unit, simply called the treatment effect for the ith unit, is defined as Yi(1)\u2212Yi(0). The goal of causal analysis is to compute the average treatment effect (ATE):\n\u03c4ATE = E[Y (1)\u2212 Y (0)] = E[Y (1)]\u2212 E[Y (0)] (1)\nThroughout this paper E[Z] refers to the expected value of the attribute Z of an individual chosen at random from a large population. The population is unavailable to us, instead we have the database which is typically a random sample of N units from that population. Then E[Z] is estimated by the empirical expected value, E[Z] = \u2211 i Zi/N , where Zi is the attribute of the i\u2019th unit in the database. In this paper we do not address the sampling error problem, but we point out that the precision of the estimator increases with the sample size. Thus, a key goal of the techniques discussed below is to ensure that expected values are computed over sufficiently large subsets of the data.\nExample 3. FlightDelay (Cont.). Suppose we want to quantify the causal effect of thunder on flight departure delays. In this case the table is the spatiotemporal join of the flights and weather data in Table 1, the treatment T is the Thunder attribute, the outcome is DepDelayMinutes, while the covariates are all other attributes unaffected by Thunder, e.g. flights carrier, the origin airport, traffic, some other weather attributes such as temperature.\nSomewhat surprisingly, the model assumes that both Yi(1) and Yi(0) are available for each unit i. For example, if the treatment T is Thunder and the outcome is DepDelayMinutes, then the assumption is that we have both values DepDelayMinutes, when thunder was present and when it was absent. The inclusion of both outcomes, factual and counterfactual, in the data model is considered to be one of the key contributions of the NRCM. Of course, in reality we have only one of these outcomes for each unit, e.g., if there was a thunder during that flight then we know Yi(1) but not Yi(0), and in this case we simply write Yi to denote Yi(Ti). This missing value prevents us from computing \u03c4ATE using Eq.(1), and is called the fundamental problem of causal inference [14]. Therefore, in order to compute \u03c4ATE , the statistics literature makes further assumptions.\nRandomized Data The strongest is the independence assumption, which states that the treatment mechanism is independent of the potential outcomes, i.e., (Y (1), Y (0)) \u22a5 T . Then, it holds that E[Y (1)] = E[Y (1)|T = 1] and similarly E[Y (0)] = E[Y (0)|T = 0] and we have:3\n\u03c4ATE = E[Y (1)|T = 1]\u2212 E[Y (0)|T = 0] (2)\nEach expectation above is easily computed from the data, for example E[Y (1)|T = 1] =\n\u2211 i:Ti=1\nYi/N1 where N1 is the number of units with T = 1. The golden standard in causal analysis are randomized experiments, where treatments are assigned randomly to the units to ensure independence; for example, in medical trials each subject is randomly assigned the treatment or a placebo, which implies independence.\nObservational Data In this paper, however, we are interested in causal analysis in observational data, where the mechanism used to assign treatments to units is not known, and where independence fails in general. For example, thunders occur mostly in the summer, which is also high travel season and therefore delays may be caused by the high traffic. In that case T and Y (0) (the delay when thunder does not occur) are correlated, since they are high in the summer and low in the winter, and similarly for T, Y (1). The vast majority of datasets available to analysts today are observational data, and this motivates our interest in this case. Here, the statistics literature makes the following weaker assumption [27]:\nStrong Ignorability: Forall x the following hold: (1) Unconfoundedness (Y (0), Y (1) \u22a5 T |X = x) and (2) Overlap 0 < Pr(T = 1|X = x) < 1\nThe first part, unconfoundedness, means that, if we partition the data by the values of the covariate attributes X = x, then, within each group, the treatment assignment and the potential outcomes are independent; then we can estimate \u03c4ATE by computing Eq. 2 for each value of the covariates X = x (i.e., conditioning on X) and averaging. The second part, overlap is needed to ensure that\n3 An additional assumption is actually needed, called Stable Unit Treatment Value Assumptions (SUTVA), which states that the outcome on one unit is not affected by the treatment of another unit, and the treatment attribute T is binary. We omit some details.\nthe conditional expectations E[Y (1)|T = 1, X = x] and E[Y (0)|T = 0, X = x] are well defined. For an illustration of unconfoundedness, suppose we restrict to flights on dates with similar weather and traffic conditions, by a fixed carrier, from a fixed airport, etc; then it is reasonable to assume that T and Y (0) are independent and so are T and Y (1). In order to satisfy unconfoundedness one has to collect sufficiently many confounding attributes in X about the data in order to break any indirect correlations between the treatment and the outcome.\nHowever, once we include sufficiently many covariate attributes X (as we should) then the data becomes sparse, and many groups X = x are either empty or have a very small number of items. For example if a group has only treated units, then the overlap condition fails, in other words the conditional expectation E[Y (0)|X = x, T = 0] is undefined. In our example, if the covariate attributes include OriginAirportID, UniqueCarrier,Tempm ,Wspdm and Precipm, because in that case all units within a group will have the same value of Thunder. In general the strong ignorability assumption is not sufficient to estimate \u03c4ATE on observational data.\nPerfect Balancing The solution adopted in statistics is to increase the size of the groups, while ensuring that strong ignorability holds within each group. In other words, instead of grouping by the values of the covariatesX , one groups by the values of some function on the covariates B(X). We say that the groups are strongly ignorable if the strong ignorability condition holds within each group:\nStrong Ignorability in Groups: Forall b the following holds: (1) Unconfoundedness (Y (0), Y (1) \u22a5 T |B(X) = b) and (2) Overlap 0 < Pr(T = 1|B(X) = b) < 1\nRosenbaum and Rubin [27] gave an elegant characterization of the functions B that define strongly ignorable groups, which we review here. We say that the groups are perfectly balanced, or that B is a balancing score if X and T are independent in each group, i.e.(X \u22a5 T |B(X) = b) for all values b. Equivalently:\nPerfect Balanced Groups: Within each group b, the distribution of the covariate attributes of the treated units is the same as the distribution of the control units:\n\u2200x :Pr(X = x|T = 1, B(X) = b) = Pr(X = x|T = 0, B(X) = b) (3)\nTheorem 1. [27, Th.3] If the treatment assignment is strongly ignorable, and"}, {"heading": "B defines perfectly balanced groups, then the treatment assignment is strongly ignorable within each group.", "text": "Proof. The overlap part of the theorem is trivial, we show unconfoundendess. Abbreviating Y = (Y (0), Y (1)), we need to prove: if (a) (Y \u22a5 T |X = x) and\n(b) (X \u22a5 T |B(X) = b), then4 (Y \u22a5 T |B(X) = b). (a) implies E[T |Y = y,X = x] = E[T |X = x] and also E[T |Y = y,X = x,B = b] = E[T |X = x,B = b] since B is a function ofX ; (b) implies E[T |X = x] = E[T |X = x,B = b] = E[T |B = b]. Therefore, E[T |Y = y,B = b] = Ex[E[T |Y = y,X = x,B = b]] = Ex[E[T |X = x,B = b]] = E[T |B = b] proving the theorem.\nIf the treatment assignment is strongly ignorable within each group, then we can compute \u03c4ATE by computing the expectations of Eq. 1 in each group, then taking the average (weighted by the group probability):\n\u03c4ATE = E[Y (1)\u2212 Y (0)] = Eb[E[Y (1)\u2212 Y (0)|B(X) = b]] (4)\n= Eb[E[Y (1)|B(X) = b]]\u2212 Eb[E[Y (0)|B(X) = b]]\n= Eb[E[Y (1)|T = 1, B(X) = b]]\u2212 Eb[E[Y (0)|T = 0, B(X) = b]]\nThus, one approach to compute causal effect in observational data is to use group the items into balanced groups, using a balancing fuction B. Then, \u03c4ATE can be estimated using the formula above, which can be translated into a straightforward SQL query using simple selections, group-by, and aggregates over the relation R. This method is called subclassification in statistics. The main problem in subclassification is finding a good balancing score B. Rosenbaum and Rubin proved that the best balancing score is the function E(x) = Pr(T = 1|X = x), called propensity score. However, in practice the propensity score E is not available directly, instead needs to be learned from the data using logistic regression, and this leads to several problems [17]. When no good balancing function can be found, a related method is used, called matching.\nMatching We briefly describe matching following [27]. Consider some balancing score B(X) (for example the propensity score). One way to estimate the quantity in Eq. 4 is as follows. First randomly sample the value b, then sample one treated unit, and one control unit with B(X) = b. This results in a set of treated units i1, i2, . . . , im and a matching set of control units j1, j2, . . . , jm: then the difference of their average outcome \u2211 k(Yik (1) \u2212 Yjk(0))/m is an unbiased estimator of Eb[E[Y (1)\u2212 Y (0)|B(X) = b]] and, hence, of \u03c4ATE . Notice that there is no need to weight by the group size, because the ratio of treated/untreated units is the same in each group. Generally, the matching technique computes a subset of units consisting of all treated units and, for each treated unit, a randomly chosen sample of fixed size of control units with the same value of balancing score.\nHistorically, matching predated subclassification, and can be done even when no good balancing score is available. The idea is to match each treated unit with one or multiple control units with \u201cclose\u201d values of the covariate attributes X , where closeness is defined using some distance function \u03b4(xi, xj) between the covariate values of two units i and j. The most commonly used distance\n4 This is precisely the Decomposition Axiom in graphoids [23, the. 1]; see [11] for a discussion.\nfunctions are listed in Fig. 1. 5 The efficacy of a matching method is evaluated by measuring degree of imbalance i.e., the differences between the distribution of covariates in two groups in the matched subset. Since there is no generic metric to compare two distributions, measures such as mean, skewness, quantile and multivariate histogram are used for this purpose. A rule of thumb is to evaluate different distance metrics and matching methods until a well-balance matched subset with a reasonable size obtained.\nSummary The goal of causal analysis is to compute \u03c4ATE (Eq. 1) and the main challenge is that each record misses one of the outcomes, Y (1) or Y (0). A precondition to overcome is to ensure strong ignorability, by collecting sufficiently many covariate attributes X . For the modern data analyst this often means integrating the data with many other data sources, to have as much information available as possible about each unit. One caveat is that one should not include attributes that are themselves affected by the treatment; the principled method for choosing the covariates is based on graphical models [7]. Once the data is properly prepared, the main challenge in causal analysis 6 is matching data records such as to ensure that the distribution of the covariates attributes of the treated and untreated units are as close as possible (Eq.(3)). This will be the focus of the rest of our paper. Once matching is performed, \u03c4ATE can be computed using Eq.(4). Thus, the main computational challenge in causal analysis is the matching phase, and this paper describes scalable techniques for performing matching in a relational database system."}, {"heading": "3 Basic Techniques", "text": "In this section we review the matching and subclassification techniques used in causal inference and propose several relational encodings, discussing their pros and cons. Historically, matching was introduced before subclassification, so we present them in this order. Subclassification is the dominant technique in use today: we will discuss optimizations for subclassification in the next section.\nWe consider a single relation R(ID, T,X, Y ), where ID is an integer-valued primary-key, T and X respectively denote the treatment and covariate attributes as described in the NRCM (cf. Section 2), and Y represent the available outcome, i.e., Y = Y (z) for iff T = z. For each matching method, we define a view over R such that materializing the extension of the view over any instance ofR computes a corresponding matched subset of the instance.\n5 The distance functions in Table 1 are semi or pseudo-metrics. That is they are symmetric; they satisfy triangle inequality and xi = xj implies \u03b4(xi, xj) = 0, but the converse does not hold. 6 There are some model-based alternatives to matching e.g., covariates adjustment on random samples. However, matching have several nice properties that makes it more appealing in practice (see, [27])."}, {"heading": "3.1 Nearest Neighbor Matching", "text": "The most common matching method is that of k : 1 nearest neighbor matching (NNM) [27,12,37]. This method selects the k nearest control matches for each treated unit and can be done with or without replacement; we denote them respectively by NNMWR and NNMNR. In the former case, a control unit can be used more than once as a match, while in the latter case it is considered only once. Matching with replacement can often decrease bias because controls that look similar to the treated units can be used multiple times. This method is helpful in settings where there are few control units available. However, since control units are no longer independent, complex inference is required to estimate the causal effect [8]. In practice, matching is usually performed without replacement. Notice that NNM faces the risk of bad matches if the closest neighbor is far away. This issue can be resolved by imposing a tolerance level on the maximum distance, known as the caliper (see e.g., [19]). There are some rules of thumb for choosing the calipers (see e.g., [19]).\nNNM With ReplacementWe propose two alternative ways for computing NNMWR in SQL, shown in Figure 2. In Figure 2(a), each treated unit is joined with k closest control units that are closer than the caliper. In this solution, nearest control units are identified by means of an anti-join. In Figure 2(b), all potential matches and their distances are identified by joining the treated with the control units that are closer than the caliper. Then, this set is sorted into ascending order of distances. In addition, the order of each row in the sorted set is identified using the window function ROW_NUMBER. Finally, all units with the order of less than or equal to k are selected as the matched units.\nThe ani-join based statement requires a three-way join. The window function based solution has a quadratic complexity. It requires a nested-loop to perform a spatial-join and a window aggregate to impose minimality. Note that window functions are typically implemented in DBMS using a sort algorithm, and even more efficient algorithms have been recently proposed [18].\nNNM Without Replacement Expressing NNMNR in a declarative manner can be complicated. In fact, this method aims to minimize the average absolute distance between matched units and can performed in either greedy or optimal manner. The latter is called optimal matching [10]. Before we describe our proposed SQL implementation for NNMWR, we prove that optimal matching is not expressible in SQL: this justifies focusing on approximate matches. For our inexpressibility result, notice that in the special case when k = 1 NNMWR is the weighted bipartite graph matching problem (WBGM), which is defined as follows: given a bipartite graph G = (V,E) and a weight function w : E \u2192 R>0, find a set of vertex-disjoint edges M \u2286 E such that M minimise the total weight w(M) = \u2211 e\u2208M w(e). The exact complexity of this problem is unknown (see, e.g. [2]), however we prove a NLOGSPACE lower bound:\nProposition 1. Computing maximum weight matching for weighted bipartite graphs is hard for NLOGSPACE.\nProof. The following Graph Reachability Problem is known to be NLOGSPACE complete: given a directed graph G(V,E) and two nodes s, t, check if there exists a path from s to t. We prove a reduction from graph reachability to the bipartite perfect matching problem which is a special case of optimal WBGM. For that we construct the graph G\u2032 with V = V \u222a V \u2032 where, V \u2032 is a copy of V with primed labels and E\u2032 = {(x, x\u2032)|\u2200x \u2208 V \u2212 {s, t}} \u222a {(x, y\u2032)|\u2200(x, y) \u2208 E} \u222a {(t, s\u2032)}. Notice that the subset {(x, x\u2032) | x \u2208 V } \u2286 E is almost a perfect matching, except that it misses the nodes s, t\u2032. We prove: there exists a path from s to t in G iff G\u2032 has a perfect matching. First assume P = s, x1, x2, . . . , xm, t is a path in G. Then the following forms a perfect matching in G\u2032: M = {(s, x\u20321), (x1, x \u2032 2), . . . , (xm, t\n\u2032), (t, s\u2032)} \u222a {(y, y\u2032) | y 6\u2208 {s, x1, . . . , xm, t}}. Conversely, assume G\u2032 has a perfect matching. Write f : V \u2192 V \u2032 the corresponding bijection, i.e. every x is matched to y\u2032 = f(x). Denoting the nodes in V as V = {x1, . . . , xn}, we construct inductively the following sequence: x\u2032i1 = f(s), x \u2032 i2 = f(xi3), . . . , x \u2032 ik+1 = f(xik ). Then i1, i2, . . . are distinct (since f is a matching), hence this sequence must eventually reach t\u2032: t\u2032 = f(xim). Then s, xi1 , xi2 , . . . , xim , t forms a path from s to t in G. This completes the proof.\nThe proposition implies that optimal matching is not expressible in SQL without the use of recursion. Optimal matching can be solved in PTIME using, for example, the Hungarian algorithm, which, in theory, could be expressed using recursion in SQL. However, optimal matching is rarely used in practice and, in\nfact, it is known that it does not in general perform any better than the greedy NNM (discussed next) in terms of reducing degree of covariate imbalance [10]. For that reason, we did not implement optimal matching in our system.\n1:1 NNMWR can be approximated with a simple greedy algorithm that sorts all edges of the underlying graph in ascending order of weights and iterates through this sorted list, marking edges as \u201cmatched\u201d while maintaining the oneto-one invariant. This algorithm can return a maximal matching that is at least 1 2 -optimal [2]. Figure 3 adopts this greedy algorithm to express 1 : k NNMWR in SQL. This algorithm is very similar to that of NNMWR in Figure 2(b), with the main difference that in the matching step it imposes the restriction that a control unit is matched with a treated unit only if it is not not already matched with another treated with a lower order. This solution also has a quadratic complexity.\nChoosing the distance function We briefly discuss now the choice of the distance function \u03b4 in NNM (see Fig. 1). The propensity score distance is by far the most prominent metric in NNM. However, it has been the subject of some recent criticisms [17]. It has been shown that, unlike other matching methods, in propensity score matching the imbalance reduction is only guaranteed across the spectrum of all samples. In observational settings, we typically have only one sample, so other matching methods dominate propensity score matching [17]. An alternative is to use the mahalanobis distance. This has been shown to exhibit some odd behavior when covariates are not normally distributed, when there are relatively large number of covariates, or there are dichotomous covariates [26]. Therefore, this method has a limited practical applicability.\nWe should mention that there is huge literature in the database community on finding the nearest neighbor. In fact this type of queries are subject of an active research and development efforts in the context of spatial-databases (see, e.g., [22]). Our work is different from these efforts in that: 1) much of the work in this area has focused on finding sub-linear algorithm for identifying nearest neighbors of a single data item (e.g., by using spatial-index). In contrast, in our setting we need to find all nearest neighbors, which is by necessity quadratic; 2) these works resulted in specialized algorithm, implemented in general purposed languages. In contrast, we focus on finding a representation in SQL, in order to integrate causal analysis with other data analytic operations."}, {"heading": "3.2 Subclassification", "text": "It is easy to see that NNM does not necessarily use all the data, meaning that many control units despite being in the range of a treatment unit are discarded. In subclassification, the aim is to form subclasses for which, the distribution of covariates for the treated and control groups are as similar as possible. The use of subclassification for matching can be traced back to [4], which examined this method on a single covariate (age), investigating the relationship between lung cancer and smoking. It is shown that using just five subclasses based on univariate continues covariates or propensity score removes over 90% of covariates imbalance [4,28].\nSubclassification based on the propensity score We describe the SQL implementation of subclassification based on n quintiles of the propensity score in Figure 4. We assumed that R includes another attribute ps for the propensity score of each unit; the value of ps needs to be learned from the data, using logistic regression [27]. The SQL query seeks to partition the units into five subclasses with propensity scores as equal as possible using the window function ntile and ensure the overlap within each subclass. This solution has the order of nlog(n) if the window function computed using a sort algorithm.\nCoarsening Exact Matching (CEM) This method as proposed recently in [15], is a particular form of subclassification in which the vector of covariates X is coarsened according to a set of user-defined cutpoints or any automatic discretization algorithm. Then all units with similar coarsened covariates values are placed in unique subclasses. All subclasses with at least one treated and one control unit are retained and the rest of units are discarded. Intuitively, this is a group-by operation, followed by eliminating all groups that have no treated, or no control unit.\nFor each attribute xi \u2208 X , we assume a set of cutpoints ci = {c1 . . . c(ki\u22121)} is given, which can be used to coarsen xi into ki buckets. The view R\nc, shown in Figure 5(a), defines extra attributes X = {cx1 . . . cxn}, where cxi is the coarsened version of xi. Two alternative SQL implementations of CEM are represented in Figure 5(b) and (c). The central idea in both implementations is to partition the units based on the coarsened covariates and discard those partitions that do not enjoy the overlap assumption. Note that the maximum of unit IDs in each partition is computed and used as its unique identifier. The window function based solution has the order of nlog(n), if the window aggregate computed using a sort algorithm. The group-by based solution can becomes linear if the join is performed by a hash-join.\nSeveral benefits of CEM has been proposed in [15]. For instance, unlike other approaches, the degree of imbalance is bounded by the user (through choosing proper cut-points for covariates coarsening), therefore the laborious process of matching and checking for balance is no longer needed. More importantly, this approach meets the congruous principle, which assert that there should be a congruity between analysis space and data space. Notice that Mahalanobis dis-\ntance and propensity score, project a vector from multidimensional space into a scalar value. It has been argued that methods violating the congruous principle may lead to less robust inference with sub-optimal and highly counterintuitive properties [15]. Therefore, the reminder of the paper focuses on developing optimization techniques to speed up the computation of CEM."}, {"heading": "4 Optimization Techniques", "text": ""}, {"heading": "4.1 CEM on Base Relations", "text": "All toolkits developed for causal inference assume that the input is a single table. However, in the real world, data is normalized, and stored in multiple tables connected by key/forgien-keys. Thus, an analyst typically integrates tables to construct a single table that contains all intergradients needed to conduct causal analyses. For instance, in the FlightDelay example, the treatment and part of the covariates are stored in the weather dataset; the outcome and rest of the covariates are stored in the flight dataset. The fact that data is scattered across multiple tables raises the question of whether we can push the matching methods to normalized databases. If so, we must question whether we can take advantage of this property to optimize the cost of performing matching.\nIntegrating tables is inevitable for propensity score matching. For example, suppose we have two tables R(T, x, y, z) and S(x, u, v). To estimate the propensity scores of each unit in R \u22b2\u22b3 S, we may fit a logistic-regression between T and the covariates y, z, u, and v. This may require computing the expression w1 \u2217 y+w2 \u2217 z+w3 \u2217u+w4 \u2217 v and then applying the logit function to it. While the weights of the expression may be learned without joining the tables, using techniques such as [35], the integrated table is required to impute the leaned model with the covariate values of each unit to estimate its propensity score. In contrast, CEM can be pushed to the normalized databases. For example, Cem(R \u22b2\u22b3 S) is equivalent to Cem(Cem(R) \u22b2\u22b3 S). To see this, note that all subclasses discarded by performing CEM on R do not satisfy the overlap assumption. It is clear that joining these subclasses with S, forms new subclasses that still fail to satisfy the overlap assumption and must be discarded. In the following, we formally state this property.\nLet D be a standard relational schema with k relations R1 . . . Rk, for some constant k \u2265 1. The relation Ri has the following attributes: a primary-key IDi; a foreign-key FIDi; a vector of observed attributes Ai. Without loss of generality, assume the treatment variable, T , is in relation R1. Let XRi \u2286 Ai be a vector of coarsened covariates from the relation Ri that is associated with T . Further, assume relations are joined in the increasing order of indices.\nProposition 2. Given an instance of D, it holds that: Cem(R1 \u22b2\u22b3 . . . \u22b2\u22b3 Rk) = Cem(. . .Cem(Cem(R1) \u22b2\u22b3 R2) . . . \u22b2\u22b3 Rk).\nProposition 2 shows that CEM can be pushed to normalized databases. In the worst case, the cost of pushing CEM can be k \u2212 2 times higher than performing CEM on the integrated table. This happens when relations have a oneto-one relationship and CEM retains all the input data. However, in practice the\nrelations typically have a many-to-one relationship. Moreover, the size of the matched subset is much smaller than the input database. In the FlightDelay example, each row in the weather dataset is associated with many rows in the flight dataset. In addition, as we see in Section 5.2, the size of the matched data is much smaller than the input data. In such settings, pushing CEM down to the base relations can significantly reduce its cost."}, {"heading": "4.2 Multiple Treatment Effect", "text": "Matching methods are typically developed for estimating the causal effect of a single treatment on an outcome. However, in practice one needs to explore and quantify the causal effect of multiple treatments. For instance, in the FlightDelay example, the objective is to quantify and compare the causal effect of different weather types on flight departure delays.\nThis section introduces online and offline techniques to speed up the computation of CEM for multiple treatments. In the sequel, we consider the relational schema consists of a single relation Re(ID, T ,X , Y ) (extends that of Rc (cf. Section 3.2) to account for multiple treatments), where T = T1, . . . , Tk is a vector of k binary treatments, each of which has a vector of coarsened covariates XTi , with X = \u22c3 i=1...k XTi . Now the view R e Ti (ID, Ti,XTi , Y ) over R\ne has the same schema as Rc (cf. Section 3.2). Therefore, the view Cem (cf. Figure 5) is well-defined over ReTi .\n(online) Covariate Factoring A key observation for reducing the overall cost of performing CEM for multiple treatments is that many covariates are shared between different treatments. For instance, flights carrier, origin airport, traffic and many weather attributes are shared between the treatments Thunder and LowVisibility. The central idea in covariate factoring is to pre-process the input data wrt. the shared covariates between treatments and uses the result to perform CEM for each individual treatment. This significantly reduces the overall cost of CEM for all treatments, if covariate factoring prunes a considerable portion of the input data.\nLet S \u2286 T be a subset of treatments with X \u2032 = \u22c2\nTi\u2208S XTi 6= \u2205. Without loss of generality assume S = {T1 . . . Tk\u2032}. Consider the view PS over Re as shown in Figure 6(a). Essentially, PS describes CEM wrt. the disjunction of treatments in S and the shared covariates between them. Figure 6(b) defines the view mCemTi over PS that describes a modified version of CEM for Ti, based on the covariate factoring. Proposition 3. Given an instance of Re and any subset of treatments S \u2286 T with\n\u22c2 Ti\u2208S XTi 6= \u2205, and for any Ti \u2208 S, it holds that mCemTi(R e) = Cem(ReTi).\nProof. We sketch the proof for S = {T1, T2}. Covariate factoring on S, discard subclasses obtained from group-by on XT1 \u2229XT2 that have no overlap wrt. both T1 and T2. It is clear that group-by XT1 is more fine-grained than group-by on XT1 \u2229XT2 , thus, subclasses with no overlap in the latter, form new subclasses in the former that still fail the overlap wrt. both of T1 and T2.\nProposition 3 shows that CEM for multiple treatments can be performed by covariate factoring. Next we develop a heuristic algorithm that takes advantage of this property to speed up CEM. Before we proceed, we state two observations that lead us to the algorithm.\nFirst, the total cost of performing CEM independently for k treatments is a function of the size of the input database. However, the cost of performing the same task using covariate factoring is a function of the size of the result of covariate factoring. But, the cost of covariates factoring depends on the size of the input. Thus, partitioning the treatments into a few set of groups, which results in pruning a huge portion of the input database, reduces the overall cost of CEM.\nSecond, we observe that the correlation between the treatments plays a crucial role in the efficacy of covariate factoring. The correlation between two treatments T and T \u2032 can be measured by the phi coefficient between them, denoted by \u03c6. Consider the following table\nT = 1 T = 0 Total T \u2032 = 1 n11 n10 n1\u2022 T \u2032 = 0 n01 n00 n0\u2022 Total n\u20221 n\u20220 n\nwhere n11, n10, n01, n00, are non-negative counts of number of observations that sum to n, the total number of observations. The phi coefficient between T and T \u2032 is given by \u03c6(T, T \u2032) = n11n00\u2212n10n01\u221a\nn1\u2022n0\u2022n\u20220n\u20221 . A phi coefficient of 0 indicates inde-\npendence, while 1 and -1 indicates complete dependence between the variables (most observations falls off the diagonal). Suppose T1 and T2 are highly correlated, i.e., |\u03c6(T, T \u2032)| \u2243 1 and share the covariates X . Further assume CEM wrt. T and X , prunes 50% of the input data. Then, covariates factoring for T1 and T2 prunes almost 50% of the input data. This is because, subclasses discarded by CEM on T , are very likely to be discarded by CEM wrt. the disjunction of T and T \u2032 (because there are highly correlated).\nAlgorithm 1, was developed based on these observations. Section 5.2 shows that covariates factoring based on this algorithm significantly reduces the over all cost of CEM wrt. multiple treatments in the FlightDelay example.\nAlgorithm 1 Covariate Factoring\n1. Let T1 . . . Tk be a set of treatments, and XTi be a vector of covariates associated to Ti. 2. Construct a correlation matrix, M, between the treatments such that the [i, j] entry in M contains \u03c6(Ti, Tj). 3. Given a partition of the treatments into n groups, S1 . . . Sn, such that |Sk| \u2265 2 and\n\u22c2 Ti\u2208Sk\nXTi 6= \u2205, compute the normalized pairwise correlations in sk as CSk =\u2211 (Ti,Tj)\u2208Sk |M[i,j]|\n|Sk| .\n5: Perform covariate factoring for groups obtained by the partition that maximises\u2211 k=1...n CSk .\n(online) Data-Cube Aggregation Given a set of observed attributes X and an outcome, all attributes can be subjected to a causal analysis. For example, all weather attributes can be dichotomized and form a treatment. In addition, one may define other treatments, formed by conjunction of such dichotomized attributes. For instance, the conjunction of \u201csnow\u201d and \u201chigh-wind-speed\u201d could become the \u201csnowstorm\u201d treatment. Note that causal effect is not subadditive [16]. Thus, quantifying the causal effect of the conjunction of T1 and T2 requires an independent analysis on the treatment T = T1 \u2227T2 wrt. the covariates XT = XT1 \u222aXT2 .\nIn principle, one might be interested in exploring and quantifying the casual effect of k = 2|X| treatments. In this setting, to estimate \u03c4ATE for all possible treatments, a matching method must be performed wrt. all possible subsets of X , each of which is associated to one treatment. We argue that, in such cases, CEM for all treatments can be performed efficiently using the existing DBMS systems that support data-cube operations.\nRecall that CEM for an individual treatment is a group-by operation (cf. Figure 5(b)). Thus, CEM for all treatments requires computing some aggregates on the data-cube (X ). Now the established optimization techniques to compute data-cubes efficiently can be adopted, e.g., for computing a group-by, we pick the smallest of the previously materialized groups from which it is possible to compute the group-by. In Section 5.2, we apply this idea to the FlightDelay example and show that it significantly reduces the overall cost of CEM for multiple treatments.\n(offline) Databases Preparation for Causal Inference on Sub-populations So far, we considered causal inference as an online analysis which seeks to explore the effect of multiple treatments on an outcome, over a population. In practice, however, one needs to explore and quantify the causal effect of multiple treatments over various sub-populations. For instance, what is the causal effect of low-visibility on departure delay in San Francisco International Airport (SFO)? what is the effect of thunder at all airports in the state of Washington since 2010? such queries can be addressed by performing CEM on the entire data and selecting the relevant part of the obtained matched subset to the query.\nThus, the cost of performing CEM wrt. all possible treatments can be amortized over several causal queries. Therefore, we can prepare the database offline and pre-compute the matched subsets wrt. all possible treatments to answer online causal queries efficiently. This could be impractical for high dimensional data since the number of possible treatments can be exponential in the number of attributes (cf. Section 4.2). Alternatively, we propose Algorithm 1, which employs the covariate factoring and data-cube techniques to prepare the database so that CEM based on any subset of the database can be obtained efficiently."}, {"heading": "5 Experimental Results", "text": "We have implemented the basic techniques in Sec. 3 and the optimizations in Sec. 4 in a system called ZaliQL. This section, presents experiments that evalu-\nAlgorithm 2 Database Preparation\n1: Let T1 . . . Tk be a set of treatments, and XTi be a vector of covariates associated to Ti. 2: Apply Algorithm 1 to partition the treatments into S1 . . . Sk with XSi = \u22c3 Tj\u2208Si\nXTj and X \u2032Si = \u22c2 Tj\u2208Si XTj . 3: Partially materialize C, the cube on X1 . . .Xk to answer group-by queries for each X \u2032Si . 4: For each group Si, perform covariate factoring using C and materialize PSi . 5: For each PSi , partially materialize Ci, the cube on XTi , so that CEM for each T \u2208 gi can be computed using Ci.\nate the feasibility and efficacy of ZaliQL. We addressed the following questions. What is the end-to-end performance of ZaliQL for causal inference in observational data? Does ZaliQL support advanced methods for causal inference and produce results with the same quality as statistical software? How does ZaliQL scale up with increasingly large data sets? And how effective are the optimization techniques in ZaliQL?"}, {"heading": "5.1 Setup", "text": "Data The flight dataset we used was collected by the US Department of Transportation (U.S. DOT) [6]. It contains records of more than 90% of US domestic flights of major airlines from 1988 to the present. Table 1(shown in Section 1) lists dataset attributes that are relevant to our experiments. We restrict our analysis to about 105M data entry collected between 2000 and 2015.\nThe weather dataset was gathered using the weather underground API [38]. Its attributes are also presented in Table 1. In addition, we pre-computed two other attributes AiportTraffic and CarrierTraffic. The former is the total number of flights occur in the origin airport of a flight one hour prior to the flight departure time, the latter is the same quantity restricted to the flights from the same carrier. We managed to acquire and clean 35M weather observations between 2000 and 2015. These datasets are integrated by a spatio-temporal join.\nCausal questions and covariate selection We explore the causal effect of the following binary treatments on flight departure delays and cancellation: LowVisibility (1 if Visim< 1; 0 if Visim> 5); Snow (1 iff Precipm> 0.3 and Tempm< 0); WindSpeed (1 if Wspdm> 40; 0 if Wspdm< 20); and Thunder. In each case, data items where the attribute in question was in between the two bounds were discarded. For instance, for the treatment LowVisibility, we want to assess the following counterfactual: \u201cWhat would the flight departure delay have been, if visibility were fine, i.e., Visim> 5, when visibility is actually low, i.e., Visim< 1\u201d; for this analysis, items where Visim\u2208 [1, 5] were discarded.\nTo ensure the SUTVA (cf. Section 2), we considered the difference between the actual delay and the late aircraft delay (if one exists) as the outcome of interest. Therefore, we assumed that the potential delay of each flight did not depend on the treatment assignment to the other flights namely, there was no interference between the units.\nTo obtain quality answers for each treatment, we used graphical models to identify a minimum number of covariate attributes to ensure unconfoundedness, because minimizing the number of covariates has been shown to increase the precision of the matching estimators [7]. We used the tool provided by http://dagitty.net to construct the causal DAG (CDAG) and select the minimum number of covariates that separate the treatment assignments from the potential outcomes. The tool finds a minimal subset of variables X that forms a d-separation [24] of the treatment T from the effect Y (meaning: all paths from T to Y go through some variable in X). For example, Figure 7 shows the CDAG for the treatment LowVisibility and the effect DepDelay: the set X consists of the shaded nodes d-sperate the treatment assignment and the potential outcomes.\nSystems The experiments were performed locally on a 64-bit OS X machine with Intel Corei7 processor (16 GB RAM, 2.8 GHz). ZaliQL was deployed to Postgres version 9.5. We compared ZaliQL with R packages MatchIt and CEM, version 2.4 and 1.1 respectively, available from [5]."}, {"heading": "5.2 Results", "text": "End-to-End Performance We estimated the causal effect of different weather types on flight departure delay and cancellation at five major US airports, that are among the ten airports with the worse weather-related delays according to [1], namely: San Francisco (SFO) , John F. Kennedy (JFK), Newark Liberty (EWR), George Bush (IAH), and LaGuardia Airport (LGA). The flight and weather data associated with these airports consists of about 10M and 2M rows, respectively.\nTo this end, we estimated \u03c4ATE for each treatment associated to a weather type by performing CEM wrt. its identified covariates. Continuous covariates were coarsened into equal width buckets, and categorical covariates were matched by their exact values.\nFigure 8(a) reports the running time of performing CEM for each treatment: recall (Fig. 5) that this involves a group-by followed by an elimination of all groups that have no treated, or no control unit. Next, we evaluated the quality of the CEM, in Figure 8(b), using a standard metric in the literature: the absolute weighted mean difference (AWMD) of each continuous covariate between the treated and control group:\nEb[|E[xi|T = 1, B(X) = b]\u2212 E[xi|T = 0, B(X) = b]|] (5)\nfor each covariate attribute xi \u2208 X . This is a standard measure of imbalance, since in a perfectly balanced group the difference is 0 (see Eq. 3). Note that in the case of CEM, we assume subclasses form the balancing scores. We compared this difference for the original value of the covarites before and after CEM. As shown, CEM substantially reduced the covariates imbalance in the raw data: this graph shows the perils of attempting to do causal inference naively, without performing CEM or matching. We also observed that CEM results in quite reasonable matched sample size wrt. treatments ( more than 75% of the treated units are matched with the average rate of one treated to five control units).\nNext, Figure 8(c) shows the causal effect of weather type on departure delay: it shows the estimated \u03c4ATE for each target airport, normalized by the number of treated units to the total number of units in each aiport. Following common practice in causal inference we estimated \u03c4ATE using Eq. 4 (thus assuming that CEM produces perfectly balanced groups). The normalization lets us compare the effects of different weather types in an individual airport. In fact, \u03c4ATE reflects the potential causal effect of a treatment to an outcome. For instance, IAH barely experiences snow, but snow has a devastating effect on flight delay at this airport. The results in Figure 8(c) are in line with those in [1], which reported\nthe following major weather-related causes of flight delay at the airports under the study: snowstorms,thunderstorm and wind at EWR; thunderstorm and fog at IAH; snowstorms and visibility at JFK; snowstorms at LGA; fog and low clouds at SFO;\nFigure 8(d) a similar causal effect of weather type on a different outcome, namely Cancellation. Here, we leveraged the fact that matching and subclassification do not depend on a particular outcome variable, thus, a matched sample wrt. a treatment can be used to estimate its causal effect of several outcomes.\nQuality Comparisons with R The MatchIt and CEM packages in R are popular tools for conducting causal inference today. In this section we compare\nthe quality of the results returned by ZaliQL with those returned by the R packages. We considered all types of matchings described in Sec. 3: NN matching (both NNMNR and NNMWR), and subclassification (by propensity score, CEM and exact matching(EM)). Sine the R packages do not scale to large datasets, we conducted these experiments over a random sample of data used in Section 5.2, which consists of around 210k rows. We evaluated different matching methods for the treatment of Snow.\nTable 3 compares the size of the matched sample and the AWMD (Eq.5) obtained by ZaliQL and R, for different matching methods. These results shows that ZaliQL produced results whose quality was at least as good as R. Note that for NNMNR and NNMWR, the caliper 0.001 is applied. For subclassification all units with propensity score less than 0.1 and more than 0.9 are discarded (this is a common practice in causal inference). We observe that all matching methods produce well-balance matched samples with reasonable sizes. However, among these methods CEM works better both in terms of the imbalance reduction and size of the matched sample.\nThe slight differences between the results of NNM arose from a slight difference between the propensity score distribution we obtained using logistic regression provided by MADlib inside Postgres.\nScalability Testing We compared the scalability of different matching methods in ZaliQL and R. The experiment was carried out over a random samples of data used in Section 5.2 and for the treatment of Snow. Figure 9(a) compares NNM methods based on propensity score. We observe that NNM does not scale to large data. Note that, for the reasons mentioned in Section 3.1, optimizing this method was not the aim of this paper. Figure 9(b) compares CEM, EM and subclassification in ZaliQL and R. Note that for CEM and NNMWR in ZaliQL we respectively implemented the group-by and window function statement. As depicted, ZaliQL significantly outperforms R and scale to large data.\nEfficacy of the Optimization Techniques We tested the effectiveness of the proposed optimization techniques (cf. Section 4). Figure 9(c) compares the\nrunning time of performing CEM on the integrated weather and flight tables with CEM on base tables (cf. Section 4.1) for two treatment LowVisibility and WindSpeed. The analysis was carried out on data used in Section 5.2. Note that the cost of integrating two tables is ignored.\nFor covariates factoring and data-cube optimizations, we compared the total cost of performing matching on the treatments defined in Section 5.1, plus the treatment obtained by conjunction of Snow and WindSpeed, which we refer to as Snowstorm. This analysis was carried on the entire integrated dataset. By applying Algorithm 1, the treatments are partitioned into two groups, namely g0={Snow, WindSpeed, Snowstorm} and g2={LowVisibility, Thunder}. Figure 9(d) compares the total running time of matching for each treatment after covariate factoring with the naive matching procedure. Figure 9(d) also shows the total cost of matching with and without using data-cubes. In addition, it represents the total cost of matching on the prepared database, according to Algorithm 2, and the cost of database preparation. As depicted, matching on the prepared database reduce the over cost of matching by an order of magnitude."}, {"heading": "6 Related work and conclusion", "text": "The simple nature of the RNCM, and its adherence to a few statistical assumptions, makes it more appealing for the researchers. Therefore, it has become the prominent approach in social sciences, biostatistics, political science, economics and other disciplines. Many toolkits have been developed for performing casual inference a\u0301 la this framework that depends on statistical software such as SAS, SPSS, or R project. However, these toolkits do not scale to large datasets. This work introduce ZaliQL, a SQL-based framework for drawing causal inference that circumvents the scalability issue with the existing tools. ZaliQL supports state-of-the-art methods for causal inference and runs at scale within a database engine.\nCausality has been studied extensively in databases [20,21,29,32,31,34,33]. We note that this line of work is different than the present paper in the sense that, it aims to identify causes for an observed output of a data transformation. While these works share some aspects of the notion of causality as studied in this paper, the problems that they address are fundamentally different."}], "references": [{"title": "A survey of heuristics for the weighted matching problem", "author": ["D. Avis"], "venue": "Networks, 13(4):475\u2013493,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "et al", "author": ["M. Bal"], "venue": "Total delay impact study. In NEXTOR Research Symposium, Washington DC.,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "The effectiveness of adjustment by subclassification in removing bias in observational studies", "author": ["W.G. Cochran"], "venue": "Biometrics, pages 295\u2013313,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1968}, {"title": "Covariate selection for the nonparametric estimation of an average treatment effect", "author": ["X. De Luna", "I. Waernbaum", "T.S. Richardson"], "venue": "Biometrika, page asr041,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Causal effects in nonexperimental studies: Reevaluating the evaluation of training programs", "author": ["R.H. Dehejia", "S. Wahba"], "venue": "Journal of the American statistical Association, 94(448):1053\u20131062,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "The design of experiments", "author": ["R.A. Fisher"], "venue": "Oliver and Boyd, Oxford, England,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1935}, {"title": "Comparison of multivariate matching methods: Structures, distances, and algorithms", "author": ["X.S. Gu", "P.R. Rosenbaum"], "venue": "Journal of Computational and Graphical Statistics, 2(4):405\u2013420,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "On the completeness of the semigraphoid axioms for deriving arbitrary from saturated conditional independence statements", "author": ["M. Gyssens", "M. Niepert", "D.V. Gucht"], "venue": "Inf. Process. Lett., 114(11):628\u2013633,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference", "author": ["D.E. Ho", "K. Imai", "G. King", "E.A. Stuart"], "venue": "Political analysis, 15(3):199\u2013236,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistics and causal inference", "author": ["P.W. Holland"], "venue": "Journal of the American statistical Association, 81(396):945\u2013960,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1986}, {"title": "Statistics and causal inference", "author": ["P.W. Holland"], "venue": "Journal of the American Statistical Association, 81(396):pp. 945\u2013960,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "Causal inference without balance checking: Coarsened exact matching", "author": ["S.M. Iacus", "G. King", "G. Porro"], "venue": "Political analysis, page mpr013,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["D. Janzin"], "venue": "Quantifying causal influences. The Annals of Statistics, 41(5):2324\u20132358,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Why propensity scores should not be used for matching", "author": ["G. King", "R. Nielsen"], "venue": "Working Paper, 378,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient processing of window functions in analytical sql queries", "author": ["V. Leis", "K. Kundhikanjana", "A. Kemper", "T. Neumann"], "venue": "Proceedings of the VLDB Endowment, 8(10):1058\u20131069,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Selecting an appropriate caliper can be essential for achieving good balance with propensity score matching", "author": ["M. Lunt"], "venue": "American journal of epidemiology, 179(2):226\u2013235,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Causality in databases", "author": ["A. Meliou", "W. Gatterbauer", "J. Halpern", "C. Koch", "K.F. Moore", "D. Suciu"], "venue": "IEEE Data Engineering Bulletin, Sept.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "The complexity of causality and responsibility for query answers and non-answers", "author": ["A. Meliou", "W. Gatterbauer", "K.F. Moore", "D. Suciu"], "venue": "Proc. VLDB Endow. (PVLDB), 4(1):34\u201345,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "PostGIS in action", "author": ["R.O. Obe", "L.S. Hsu"], "venue": "Manning Publications Co.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "Causality: models, reasoning, and inference", "author": ["J. Pearl"], "venue": "Cambridge University Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Causation, Prediction and Search", "author": ["C.G. Peter Spirtes", "R. Scheines"], "venue": "MIT Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Observational studies", "author": ["P.R. Rosenbaum"], "venue": "In Observational Studies, pages 1\u201317. Springer,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "The central role of the propensity score in observational studies for causal effects", "author": ["P.R. Rosenbaum", "D.B. Rubin"], "venue": "Biometrika, 70(1):pp. 41\u201355,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1983}, {"title": "Reducing bias in observational studies using subclassification on the propensity score", "author": ["P.R. Rosenbaum", "D.B. Rubin"], "venue": "Journal of the American statistical Association, 79(387):516\u2013524,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1984}, {"title": "A formal approach to finding explanations for database queries", "author": ["S. Roy", "D. Suciu"], "venue": "SIGMOD,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Causal inference using potential outcomes", "author": ["D.B. Rubin"], "venue": "Journal of the American Statistical Association, 100(469):322\u2013331,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "den Broeck", "author": ["B. Salimi", "L. Bertossi", "D. Suciu", "G. Va"], "venue": "Quantifying causal effects on query answering in databases. In TaPP,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "From causes for database queries to repairs and model-based diagnosis and back", "author": ["B. Salimi", "L.E. Bertossi"], "venue": "In ICDT, pages 342\u2013362,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Query-answer causality in databases: Abductive diagnosis and view updates", "author": ["B. Salimi", "L.E. Bertossi"], "venue": "In Proceedings of the UAI 2015 Workshop on Advances in Causal Inference co-located with the 31st Conference on Uncertainty in Artificial Intelligence (UAI 2015), Amsterdam, The Netherlands, July 16, 2015., pages 76\u2013 85,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Causes for query answers from databases, datalog abduction and view-updates: The presence of integrity constraints", "author": ["B. Salimi", "L.E. Bertossi"], "venue": "In Proceedings of the Twenty-Ninth International Florida Artificial Intelligence Research Society Conference, FLAIRS 2016, Key Largo, Florida, May 16-18, 2016., pages 674\u2013679,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning linear regression models over factorized joins", "author": ["M. Schleich", "D. Olteanu", "R. Ciucanu"], "venue": "In ACM SIGMOD,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Advanced data analysis from an elementary point of view", "author": ["C. Shalizi"], "venue": "Cambridge University Press,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Matching methods for causal inference: A review and a look forward", "author": ["E.A. Stuart"], "venue": "Statistical science, 25(1):1,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "Causal inference has been studied extensively in statistics and computer science [9,30,13,24,25].", "startOffset": 81, "endOffset": 96}, {"referenceID": 26, "context": "Causal inference has been studied extensively in statistics and computer science [9,30,13,24,25].", "startOffset": 81, "endOffset": 96}, {"referenceID": 9, "context": "Causal inference has been studied extensively in statistics and computer science [9,30,13,24,25].", "startOffset": 81, "endOffset": 96}, {"referenceID": 20, "context": "Causal inference has been studied extensively in statistics and computer science [9,30,13,24,25].", "startOffset": 81, "endOffset": 96}, {"referenceID": 21, "context": "Causal inference has been studied extensively in statistics and computer science [9,30,13,24,25].", "startOffset": 81, "endOffset": 96}, {"referenceID": 1, "context": "Flight delays pose a serious and widespread problem in the United States and significantly strain on the national air travel system, costing society many billions of dollars each year [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 32, "context": "It is known that David Hume (1711-1776), a Scottish philosopher, who gave the first explicit definition of causation in terms of counterfactuals, was heavily influenced by al-Ghzali\u2019s conception of causality [36].", "startOffset": 208, "endOffset": 212}, {"referenceID": 10, "context": "(1), and is called the fundamental problem of causal inference [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "Name \u03b4(xi, xj) = Comments Coarsened distance 0 if C(xi) = C(xj) Where C(x) is a function that coarsen \u221e if C(xi) 6= C(xj) a vector of continues covariate [15] Propensity score |E(xi) \u2212 E(xj)| where E(x) = Pr(T = 1|X = x) distance (PS) is the propensity score [27] Mahalanobis Distance (MD) (xi \u2212 xj)\u03a3(xi \u2212 xj) where \u03a3 = covariance matrix [37]", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Name \u03b4(xi, xj) = Comments Coarsened distance 0 if C(xi) = C(xj) Where C(x) is a function that coarsen \u221e if C(xi) 6= C(xj) a vector of continues covariate [15] Propensity score |E(xi) \u2212 E(xj)| where E(x) = Pr(T = 1|X = x) distance (PS) is the propensity score [27] Mahalanobis Distance (MD) (xi \u2212 xj)\u03a3(xi \u2212 xj) where \u03a3 = covariance matrix [37]", "startOffset": 259, "endOffset": 263}, {"referenceID": 33, "context": "Name \u03b4(xi, xj) = Comments Coarsened distance 0 if C(xi) = C(xj) Where C(x) is a function that coarsen \u221e if C(xi) 6= C(xj) a vector of continues covariate [15] Propensity score |E(xi) \u2212 E(xj)| where E(x) = Pr(T = 1|X = x) distance (PS) is the propensity score [27] Mahalanobis Distance (MD) (xi \u2212 xj)\u03a3(xi \u2212 xj) where \u03a3 = covariance matrix [37]", "startOffset": 338, "endOffset": 342}, {"referenceID": 23, "context": "Here, the statistics literature makes the following weaker assumption [27]:", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Rosenbaum and Rubin [27] gave an elegant characterization of the functions B that define strongly ignorable groups, which we review here.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "However, in practice the propensity score E is not available directly, instead needs to be learned from the data using logistic regression, and this leads to several problems [17].", "startOffset": 175, "endOffset": 179}, {"referenceID": 23, "context": "Matching We briefly describe matching following [27].", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "1]; see [11] for a discussion.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "One caveat is that one should not include attributes that are themselves affected by the treatment; the principled method for choosing the covariates is based on graphical models [7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 23, "context": "However, matching have several nice properties that makes it more appealing in practice (see, [27]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "The most common matching method is that of k : 1 nearest neighbor matching (NNM) [27,12,37].", "startOffset": 81, "endOffset": 91}, {"referenceID": 8, "context": "The most common matching method is that of k : 1 nearest neighbor matching (NNM) [27,12,37].", "startOffset": 81, "endOffset": 91}, {"referenceID": 33, "context": "The most common matching method is that of k : 1 nearest neighbor matching (NNM) [27,12,37].", "startOffset": 81, "endOffset": 91}, {"referenceID": 4, "context": "However, since control units are no longer independent, complex inference is required to estimate the causal effect [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 15, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "Note that window functions are typically implemented in DBMS using a sort algorithm, and even more efficient algorithms have been recently proposed [18].", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "The latter is called optimal matching [10].", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "[2]), however we prove a NLOGSPACE lower bound:", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "fact, it is known that it does not in general perform any better than the greedy NNM (discussed next) in terms of reducing degree of covariate imbalance [10].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "This algorithm can return a maximal matching that is at least 1 2 -optimal [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "However, it has been the subject of some recent criticisms [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "In observational settings, we typically have only one sample, so other matching methods dominate propensity score matching [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "This has been shown to exhibit some odd behavior when covariates are not normally distributed, when there are relatively large number of covariates, or there are dichotomous covariates [26].", "startOffset": 185, "endOffset": 189}, {"referenceID": 18, "context": ", [22]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "The use of subclassification for matching can be traced back to [4], which examined this method on a single covariate (age), investigating the relationship between lung cancer and smoking.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "It is shown that using just five subclasses based on univariate continues covariates or propensity score removes over 90% of covariates imbalance [4,28].", "startOffset": 146, "endOffset": 152}, {"referenceID": 24, "context": "It is shown that using just five subclasses based on univariate continues covariates or propensity score removes over 90% of covariates imbalance [4,28].", "startOffset": 146, "endOffset": 152}, {"referenceID": 23, "context": "We assumed that R includes another attribute ps for the propensity score of each unit; the value of ps needs to be learned from the data, using logistic regression [27].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "Coarsening Exact Matching (CEM) This method as proposed recently in [15], is a particular form of subclassification in which the vector of covariates X is coarsened according to a set of user-defined cutpoints or any automatic discretization algorithm.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "Several benefits of CEM has been proposed in [15].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "It has been argued that methods violating the congruous principle may lead to less robust inference with sub-optimal and highly counterintuitive properties [15].", "startOffset": 156, "endOffset": 160}, {"referenceID": 31, "context": "While the weights of the expression may be learned without joining the tables, using techniques such as [35], the integrated table is required to impute the leaned model with the covariate values of each unit to estimate its propensity score.", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "Note that causal effect is not subadditive [16].", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "To obtain quality answers for each treatment, we used graphical models to identify a minimum number of covariate attributes to ensure unconfoundedness, because minimizing the number of covariates has been shown to increase the precision of the matching estimators [7].", "startOffset": 264, "endOffset": 267}, {"referenceID": 20, "context": "The tool finds a minimal subset of variables X that forms a d-separation [24] of the treatment T from the effect Y (meaning: all paths from T to Y go through some variable in X).", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}, {"referenceID": 17, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}, {"referenceID": 25, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}, {"referenceID": 28, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}, {"referenceID": 27, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}, {"referenceID": 30, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}, {"referenceID": 29, "context": "Causality has been studied extensively in databases [20,21,29,32,31,34,33].", "startOffset": 52, "endOffset": 74}], "year": 2016, "abstractText": "Causal inference from observational data is a subject of active research and development in statistics and computer science. Many toolkits have been developed for this purpose that depends on statistical software. However, these toolkits do not scale to large datasets. In this paper we describe a suite of techniques for expressing causal inference tasks from observational data in SQL. This suite supports the state-ofthe-art methods for causal inference and run at scale within a database engine. In addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. We evaluate the quality and performance of our techniques by experiments of real datasets.", "creator": "LaTeX with hyperref package"}}}