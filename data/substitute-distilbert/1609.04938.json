{"id": "1609.04938", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Image-to-Markup Generation with Coarse-to-Fine Attention", "abstract": "building on recent advances in image caption generation and optical character recognition ( ocr ), we present a general - purpose, deep emotion - based system to decompile an image given presentational markup. while this program is a well - studied problem in ocr, our method takes an inherently different, data - driven approach. our model appears not require any knowledge inside the underlying markup language, and is simply trained end - grade - end on third - world example graphs. the model extends a convolutional network for handwriting and layout recognition then tandem with an attention - based neural machine translation system. to train and evaluate the model, we introduce a new dataset of real - history rendered keyboard expressions paired with rendering markup, as well as a synthetic dataset of web pages paired beneath html blocks. experimental results show that the system is clearly inexperienced at generating accurate markup for both datasets. while his standard domain - specific latex ocr system achieves around 25 % accuracy, our model reproduces the exact rendered image on 75 % of examples.", "histories": [["v1", "Fri, 16 Sep 2016 08:14:50 GMT  (814kb,D)", "http://arxiv.org/abs/1609.04938v1", null], ["v2", "Tue, 13 Jun 2017 22:48:53 GMT  (323kb,D)", "http://arxiv.org/abs/1609.04938v2", "Accepted by ICML 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG cs.NE", "authors": ["yuntian deng", "anssi kanervisto", "jeffrey ling", "alexander m rush"], "accepted": true, "id": "1609.04938"}, "pdf": {"name": "1609.04938.pdf", "metadata": {"source": "CRF", "title": "What You Get Is What You See: A Visual Markup Decompiler", "authors": ["Yuntian Deng", "Anssi Kanervisto", "Alexander M. Rush"], "emails": ["dengyuntian@gmail.com", "anssk@student.uef.fi", "srush@seas.harvard.edu"], "sections": [{"heading": "Introduction", "text": "Optical character recognition (OCR) is most commonly used to recognize natural language from an image; however, as early as the work of (Anderson 1967), there has been research interest in converting images into structured language or markup that defines both the text itself and its presentational semantics. The primary focus of this work is OCR for mathematical expressions, and how to handle presentational aspects such as sub and superscript notation, special symbols, and nested fractions (Belaid and Haton 1984; Chan and Yeung 2000). The most effective systems combine specialized character segmentation with grammars of the underlying mathematical layout language (Miller and Viola 1998). A prime example of this approach is the INFTY system that is used to convert printed mathematical expressions to LaTeX and other markup formats (Suzuki et al. 2003).\nProblems like OCR that require joint processing of image and text data have recently seen increased research interest due to the refinement of deep neural models in these two domains. For instance, advances have been made in the areas of\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nhandwriting recognition (Ciresan et al. 2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b). At a high-level, each of these systems learns an abstract encoded representation of the input image which is then decoded to generate a textual output. In addition to performing quite well on standard tasks, these models are entirely data driven, which makes them adaptable to a wide range of datasets without requiring heavily preprocessing inputs or domain specific engineering.\nThe turn towards data-driven neural methods for image and text leads us to revisit the problem of generating structured markup. We consider whether a supervised model can learn to produce correct presentational markup from an image, without requiring a textual or visual grammar of the underlying markup language. While results from language modeling suggest that neural models can consistently generate syntactically correct markup (Karpathy, Johnson, and Li 2015; Vinyals et al. 2015a), it is unclear whether the full solution can be learned from markup-image pairs.\nOur model, WYGIWYS [What you get is what you see], is a simple extension of the attention-based encoder-decoder model (Bahdanau, Cho, and Bengio 2014), which is now standard for machine translation. Similar to work in image captioning (Xu et al. 2015), the model incorporates a multi-layer convolutional network over the image with an attention-based recurrent neural network decoder. To adapt this model to the OCR problem and capture the document\u2019s temporal layout, we also incorporate a new source encoder layer in the form of a multi-row recurrent model applied before the application of attention. The use of attention additionally provides an alignment from the generated markup to the original source image (see Figure 1).\nWe also introduce two new datasets for the image-tomarkup task. The preliminary experiments use a dataset of small synthetic geometric HTML examples rendered as web pages. For the main experiments, we introduce a new dataset, IM2LATEX-100K, that consists of a large collection of rendered real-world mathematical expressions collected from published articles1. We will publicly release this dataset as part of this work. The same model archi-\n1This dataset is based on the challenge originally proposed as an OpenAI Request for Research under the title Im2Latex.\nar X\niv :1\n60 9.\n04 93\n8v 1\n[ cs\n.C V\n] 1\n6 Se\np 20\n16\ntecture is trained to generate HTML and LaTeX markup with the goal rendering to the exact source image. Experiments compare the output of the model with several research and commercial baselines, as well as ablations of the model. The full system for mathematical expression generation is able to match images within 15% of image edit distance, and is identical on more than 75% of realworld test examples. Additionally the use of a multi-row encoder leads to a significant increase in performance. All data, models, and evaluation scripts are publicly available at http://lstm.seas.harvard.edu/latex/.\nProblem: Image-to-Markup Generation We define the image-to-markup problem as converting a rendered source image to target presentational markup that fully describes both its content and layout. The source, x \u2208 X , consists of an image with height H and width W , e.g. RH\u00d7W for grayscale inputs. The target, y \u2208 Y , consists of a sequence of tokens y1, y2, \u00b7 \u00b7 \u00b7 , yC where C is the length of the output, and each y is a token in the markup language with vocabulary \u03a3. The rendering is defined by a possibly unknown, many-to-one, compile function, compile : Y \u2192 X . In practice this function may be quite complicated, e.g a browser, or ill-specified, e.g. the LaTeX language.\nThe supervised task is to learn to approximately invert the compile function using supervised examples of its behavior. We assume that we are given instances (x(1),y(1)), ..., (x(J),y(J)), with possibly differing dimensions H,W,C and that, compile(y) \u2248 x, for all training pairs (x,y) (assuming possible noise).\nAt test time, the system is given a raw input x rendered from ground-truth y. It generates a hypothesis y\u0302 that can then be rendered by the black-box function x\u0302 = compile(y\u0302). Evaluation is done between x\u0302 and x, i.e. the aim is to produce similar rendered images while y\u0302 may or may not be similar to the ground-truth markup y."}, {"heading": "Model", "text": "Our model WYGIWYS for this task combines several standard neural components from vision and natural language\nprocessing. It first extracts image features using a convolutional neural network (CNN) and arranges the features in a grid. Each row is then encoded using a recurrent neural network (RNN). These encoded features are then used by an RNN decoder with a visual attention mechanism. The decoder implements a conditional language model over the vocabulary \u03a3, and the whole model is trained to maximize the likelihood of the observed markup. The full structure is illustrated in Figure 2. We describe the model in more detail.\nConvolutional Network The visual features of an image are extracted with a multi-layer convolutional neural network interleaved with max-pooling layers. This network architecture is now standard; we model it specifically after the network used by Shi et al. (2015) for OCR from images (specification is given in Table 2.) Unlike some recent OCR work (Jaderberg et al. 2015; Lee and Osindero 2016), we do not use final fully-connected layers (Ioffe and Szegedy 2015), since we want to preserve the locality of CNN features in order to use visual attention. The CNN takes the raw input RH\u00d7W and produces a feature grid V of size D\u00d7H \u2032 \u00d7W \u2032, where c denotes the number of channels and H \u2032 and W \u2032 are the reduced sizes from pooling.\nRow Encoder In attention-based image captioning (Xu et al. 2015), the image feature grid can be directly fed into the decoder. For OCR, the visual features fed in to the decoder contain significant relative sequential order information. Therefore we experiment with an additional RNN encoder module that re-encodes each row of the grid. Intuitively, we expect this to help in two ways: (1) many markup languages default to a left-to-right order, which can be easily learned by the encoder, (2) the RNN can utilize the surrounding horizontal context to refine the hidden representation.\nFormally, a recurrent neural network (RNN) is a parameterized function RNN that recursively maps an input vector and a hidden state to a new hidden state. At time t, the hidden state is updated with an input vt in the following manner: ht = RNN(ht\u22121,vt; \u03b8) and h0 is an initial state. In practice there are many different variants of RNN; however, long short-term memory networks (LSTMs) (Hochreiter and\nSchmidhuber 1997) have been shown to be very effective for most NLP tasks. For simplicity we will describe the model as an RNN, but all experiments use LSTM networks.\nIn this model, the new feature grid V\u0303 is created from V by running an RNN across each row of that input. Recursively for all rows h \u2208 {1, . . . ,H \u2032} and columns w \u2208 {1, . . . ,W \u2032}, the new features are defined as V\u0303h,w = RNN(V\u0303h,w\u22121,Vh,w). In order to capture the sequential order information in vertical direction, we use a trainable initial hidden state V\u0303h,0 for each row, which we name positional embeddings.\nDecoder The target markup tokens {yt} are then generated by a decoder based on a sequence of annotation grid V\u0303. The decoder is trained as a conditional language model to give the probability of the next token given the history and the annotations.\nThis language model is defined on top of a decoder RNN,\np(yt+1|y1, . . . , yt, V\u0303) = softmax(Woutot) where Wout is a learned linear transformation and ot = tanh(Wc[ht; ct]). The vector ht is used to summarize the decoding history: ht = RNN(ht\u22121, [yt\u22121;ot\u22121]). The context vector ct, defined below, is used to capture the context information from the annotation grid.\nAt each time step t, the context vector ct takes into account the annotation grid. However, since most annotation cells are likely irrelevant, the model should know which cells to attend to t. We use an attention model a(\u00b7) to model this alignment (Bahdanau, Cho, and Bengio 2014):\net = a(ht, {V\u0303h,w}) \u03b1t = softmax(et)\nct = \u03c6({V\u0303h,w}, \u03b1t)\nwhere \u03b1t are the weights calculated based on et, and the weight vector \u03b1t and all annotation vectors {V\u0303t} are combined to form a context vector ct. Note there are different choices for a and \u03c6, we follow past empirical work and use eit = \u03b2 T tanh(Whhi\u22121 + Wvv\u0303t) and ci = \u2211\nt \u03b1itvt (Luong, Pham, and Manning 2015). The vectors ct and ht are concatenated together to predict the probability of the token yt. Figure 1 shows a real example of the attention distribution \u03b1t at each step of the model.\nTraining and Generation The complete model is trained end-to-end to maximize the likelihood of the observed training data. Beyond the training data, the model is given no other information about the markup language or the generating process. To generate markup from unseen images, we simply use beam search at test time with no further hard constraints."}, {"heading": "Data", "text": "While there are some datasets available for the image-tomarkup generation problem (Mouchere et al. 2012; 2013;\nLin et al. 2012), they are all too small for training a datadriven system. We therefore introduce two new datasets for this task: a preliminary web page dataset using snippets of HTML, and a large real-world mathematical expression dataset in LaTeX."}, {"heading": "Web Page-to-HTML", "text": "Our preliminary dataset is a synthetically generated collection of \u201cweb pages\u201d, used as a test for whether the model can learn relative spatial positioning. We generate a dataset consisting of 100k unique HTML snippets and the corresponding rendered images. The images are compiled using Webkit yielding images of size 100 \u00d7 100. The task is to infer the HTML markup based on the rendered image.\nThe HTML markup is generated using a simple contextfree grammar. The grammar recursively generates div\u2019s each with a solid border, a random width, and a random float (left or right). Each div can optionally be recursively split vertically or horizontally. 2 The maximum depth of the recursion is limited to 2 nested layers of div\u2019s. Finally we impose several constraints to the widths of the elements to\n2Additionally each div contains a randomly generated span of characters of a random font size.\navoid overlaps. Figure 3 shows an example snippet sampled from the dataset."}, {"heading": "Math-to-LaTeX", "text": "Our primary dataset, IM2LATEX-100K, collects a largecorpus of real-world mathematical expressions written in LaTeX. This dataset provides a much more difficult test-bed for learning how to reproduce naturally occurring markup.\nCorpus The IM2LATEX-100K dataset provides 103,556 different LaTeX math equations along with rendered pictures. We extract formulas by parsing LaTeX sources of papers from the arXiv. LaTeX sources are obtained from tasks I and II of the 2003 KDD cup (Gehrke, Ginsparg, and Kleinberg 2003) containing over 60,000 papers.\nTo extract formulas from the LaTex sources, we match the Latex sources with simple regular expressions, namely \\begin{equation} (.*?) \\end{equation}, $$ (.*?) $$, \\[ (.*?) \\] and \\( (.*?) \\). We only keep matches whose total number of characters fall in the range from 40 to 1024 to avoid single symbols, large matrices or text sentences. With these settings we extract over 800,000 different LaTeX formulas. Out of the remaining formulas, around 100 thousand are rendered in a vanilla LaTex environment. Rendering is fulfilled with pdflatex3 and formulas that fail to compile are excluded. Then the rendered PDF files are converted to PNG format4. The final dataset we provide contains 103,556 images of resolution 1654 \u00d7 2339 with black equations against transparent background, and the corresponding LaTeX formulas.\nThe dataset is separated into training set (83,883 equations), validation set (9,319 equations) and test set (10,354 equations) for standardized experiment setup. The LaTeX formulas range from 38 to 997 characters with mean 118 and median 98.\nTokenization Training the model requires settling on a token set \u03a3. One option is to use a purely characterbased model. While this method requires fewer assumptions, character-based NMT models have been less effective, slower, and significantly more memory intensive than wordbased models. Therefore original markup is simply split into\n3LaTeX (version 3.1415926-2.5-1.40.14) 4We use the ImageMagick convert tool with parameters\n-density 200 -quality 100\nminimal meaningful LaTeX tokens, e.g. for observed characters, symbols such as \\sigma, modifier characters such as \u02c6, functions, accents, environments, brackets and other miscellaneous commands.\nOptional: Normalization Finally we note that naturally occurring LaTeX contains many different expressions that produce identical output. We therefore experiment with an optional normalization preprocessing step to eliminate spurious ambiguity (prior to training). For normalization, we wrote a LaTeX parser5 to convert the markup to an abstract syntax tree. We then apply a set of safe normalizing tree transformation to eliminate common spurious ambiguity, shown in Table 1. Note this only changes the training data, and does not change the model itself."}, {"heading": "Experimental Setup", "text": "To test this approach we compare the proposed model to several other classical OCR baselines, neural models, and ablations, on the HTML and LaTeX decompiling tasks."}, {"heading": "Baselines", "text": "The current best-available OCR-based mathematical expression recognition system is the InftyReader system, a proprietary commercial implementation of the INFTY system of (Suzuki et al. 2003). This system combines symbol recognition and structural analysis phases. Additionally we experimented with the open-source AbiWord OCR system, which\n5Based on KaTeX parser https://khan.github.io/ KaTeX/\ncontains a Tex generation mode (Wen 2002). However we found that this system performed too poorly on this task to compare.\nFor a neural model, a natural comparison is to standard image captioning approaches (Xu et al. 2015). As our model is based on this approach, we compare by removing the encoder RNN, i.e. replacing V\u0303 with V, and increasing the number of CNN such that the number of parameters is approximately the same. We name this mode textscCNNEnc.\nWe also run experiments comparing the model to traditional LM approaches, including a standard NGRAM model (5-gram trained with Kneser-Ney smoothing) and an LSTM-LM. This indicates how much of the improvement comes from improving language modeling of the underlying markup. Finally for LaTeX we also evaluate with full normalization, norm, versus simple tokenization, tok."}, {"heading": "Evaluation", "text": "Our core evaluation method is to check the accuracy of the rendered markup output image x\u0302 compared to the true image x. The main evaluation reports the column-wise edit distance between the gold and predicted images. Explicitly we discretize generated columns, and compare the edit distance sequences. The final score is the total number of edit distance ops used divided by the maximum number in the dataset. Additionally we check the exact match accuracy with the original image as well as the value after eliminating whitespace columns.6 We also include standard intrinsic text generation metrics, conditional language model perplexity and BLEU score (Papineni et al. 2002). Note that both of these metrics are sensitive to the fact that the markup languages have spurious ambiguity, so a deterministic perplexity of 1 would be impossible."}, {"heading": "Implementation Details", "text": "The same model and hyperparameters are used for both the image-to-markup tasks. The CNN specifications are summarized in Table 2. The model uses single-layers LSTMs for all RNNs. We use a bi-directional RNN for the encoder. The hidden state of the encoder RNN is of size 256, decoder RNN of 512, and token embeddings of size 80. The model has 9.48 million parameters in total. We use minibatch stochastic gradient descent to learn the parameters.\n6In practice we found that the LaTeX renderer often misaligns identical expressions by several pixels. To correct for this, only misalignments of \u2265 5 pixels wide are \u201cexact\u201d match errors."}, {"heading": "Model Train Perp Test Perp Exact Match", "text": "The initial learning rate is set to 0.1, and we halve it once the validation perplexity does not decrease. We train the model for 12 epochs and use the validation perplexity to choose the best model. During test phase, we use beam search with beam size 5.\nThe system is built using Torch (Collobert, Kavukcuoglu, and Farabet 2011) based on the Seq2seq-attn NMT system 7. Experiments are run on a 12GB NVidia Titan X GPU.\nHTML All images start as 100 \u00d7 100 color input, which are then down-sampled to be grayscale images of size 64 \u00d7 64. We then normalize pixels to the range [\u22121, 1]. During training, we only use training instances with less than 100 output tokens to accelerate the training process. The batch size is set to 100. The training process takes 4 hours.\nLaTeX Original images are cropped to only the formula area, and padded with 8 pixels to the top, left, right and bottom. For efficiency we downsample all images to half of their original sizes. To facilitate batching, we group images into similar sizes and pad with whitespace.8 All images of\n7https://github.com/harvardnlp/ seq2seq-attn\n8Width-Height groups used are (120,50), (160,40), (200,40), (200,50), (240,40), (240,50), (280,40), (280,50), (320,40),\nlarger sizes, LaTeX formulas with more than 150 tokens, or cannot be parsed by our parser are ignored during training and validation (but included at test). Training batch size is set to 20 due to the size limit of GPU memory. Training process takes about 20 hours."}, {"heading": "Results", "text": "Initial experiments on the HTML dataset are shown in Table 4. The model is able to reach a perplexity of 1.06 and an exact match accuracy of over 97.5%. These results indicate that the model is able to learn to identify and generate the correct output based on spatial cues. Much of the remaining perplexity is due to ambiguities in the underlying markup language. Typical mistakes are shown in Table 4. The few issues occur in font-size and matching relative sizes of divs.\nThe main experimental results on mathematical expressions are given in Table 3. These results compare several different systems on the task of decompiling rendered LaTeX. The classical INFTY system is able to do quite well in terms of text accuracy, but performs poorly on the more rigorous image metrics. Our reimplementation of the Imageto-Caption work CNNENC does much better, pushing that number to above 50%. Our full system with RNN encoder increases this value above 75%, achieving very high accuracy on this task. We expected the LaTeX normalizer to greatly increase performance, but only give a few points of accuracy gain, despite achieving high normalized BLEU. This indicates that the decoder LM is able to learn well despite the ambiguities in real-world LaTeX.\nTo better understand the contribution of each part of the model, we run ablation experiments removing different aspects, which are shown in Table 5. The simplest model is a standard NGRAM LM on LaTeX with a perplexity of around 8. Simply switching to an LSTM-LM reduces the value to 5, likely due to its ability to count parentheses and nestinglevels. Adding the image data with a CNN further reduces the perplexity down to 1.18. Adding the encoder LSTM adds a small gain to 1.12, but actually makes a large difference in final accuracy. Adding the positional embeddings (trainable initial states for each row) adds a tiny gain.\nThe main non-spacing errors of the model are shown in Table 6. These results show the most common presentationaffecting replacements of the norm model. Most of the errors again come from font issues, such as using small parentheses instead of large ones or using standard math font instead of escaping or using math cal. Figure 5 shows several ex-\n(320,50), (360,40), (360,50), (360,60), (360, 100), (400,50), (400,160), (500,100).\namples of common errors. Typically most of the structure of the expression is preserved, but with one or two symbol recognition errors."}, {"heading": "Conclusion and Future Work", "text": "We have presented a visual attention-based model, WYGIWYS, for OCR of presentational markup. The model acts as a \u201cvisual decompiler\u201d for markup such as HTML and LaTeX. We also introduce a new dataset IM2LATEX-100K that provides a test bed for this task of Image-to-Markup generation. These contributions provide a new view on the task of structured text OCR, and show data-driven models can be surprisingly effective, even without any knowledge of the underlying language.\nPossible future directions for this work include: scaling the system to run on full websites or for document decompilation, using similar approach for handwritten mathematical expressions or HTML from informal sketches, or combining these methods with neural inference machines such as MemNNs (Weston, Chopra, and Bordes 2014) for more complicated markup or reference variables."}], "references": [{"title": "Syntax-directed recognition of handprinted two-dimensional mathematics", "author": ["R.H. Anderson"], "venue": "Symposium on Interactive Systems for Experimental Applied Mathematics: Proceedings of the Association for Computing Machinery Inc. Symposium, 436\u2013459. ACM.", "citeRegEx": "Anderson,? 1967", "shortCiteRegEx": "Anderson", "year": 1967}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A syntactic approach for handwritten mathematical formula recognition", "author": ["A. Belaid", "Haton", "J.-P."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (1):105\u2013111.", "citeRegEx": "Belaid et al\\.,? 1984", "shortCiteRegEx": "Belaid et al\\.", "year": 1984}, {"title": "Mathematical expression recognition: a survey", "author": ["K. Chan", "D. Yeung"], "venue": "IJDAR 3(1):3\u201315.", "citeRegEx": "Chan and Yeung,? 2000", "shortCiteRegEx": "Chan and Yeung", "year": 2000}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural computation 22(12):3207\u20133220.", "citeRegEx": "Ciresan et al\\.,? 2010", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, number EPFL-CONF-192376.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Overview of the 2003 kdd cup", "author": ["J. Gehrke", "P. Ginsparg", "J. Kleinberg"], "venue": "ACM SIGKDD Explorations Newsletter 5(2):149\u2013151.", "citeRegEx": "Gehrke et al\\.,? 2003", "shortCiteRegEx": "Gehrke et al\\.", "year": 2003}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 448\u2013456.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Deep structured output learning for unconstrained text recognition", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "ICLR.", "citeRegEx": "Jaderberg et al\\.,? 2015", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Reading text in the wild with convolutional neural networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "International Journal of Computer Vision 116(1):1\u201320.", "citeRegEx": "Jaderberg et al\\.,? 2016", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Recursive recurrent nets with attention modeling for ocr in the wild", "author": ["Lee", "C.-Y.", "S. Osindero"], "venue": "arXiv preprint arXiv:1603.03101.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Performance evaluation of mathematical formula identification", "author": ["X. Lin", "L. Gao", "Z. Tang", "X. Lin", "X. Hu"], "venue": "Document Analysis Systems (DAS), 2012 10th IAPR International Workshop on, 287\u2013291. IEEE.", "citeRegEx": "Lin et al\\.,? 2012", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Ambiguity and constraint in mathematical expression recognition", "author": ["E.G. Miller", "P.A. Viola"], "venue": "AAAI/IAAI, 784\u2013791.", "citeRegEx": "Miller and Viola,? 1998", "shortCiteRegEx": "Miller and Viola", "year": 1998}, {"title": "Icfhr 2012 competition on recognition of on-line mathematical expressions (crohme 2012)", "author": ["H. Mouchere", "C. Viard-Gaudin", "D.H. Kim", "J.H. Kim", "U. Garain"], "venue": "Frontiers in Handwriting Recognition (ICFHR), 2012 International Conference on, 811\u2013816. IEEE.", "citeRegEx": "Mouchere et al\\.,? 2012", "shortCiteRegEx": "Mouchere et al\\.", "year": 2012}, {"title": "Icdar 2013 crohme: Third international competition on recognition of online handwritten mathematical expressions", "author": ["H. Mouchere", "C. Viard-Gaudin", "R. Zanibbi", "U. Garain", "D.H. Kim", "J.H. Kim"], "venue": "2013 12th International Conference on Document Analysis and Recognition, 1428\u2013", "citeRegEx": "Mouchere et al\\.,? 2013", "shortCiteRegEx": "Mouchere et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition", "author": ["B. Shi", "X. Bai", "C. Yao"], "venue": "arXiv preprint arXiv:1507.05717.", "citeRegEx": "Shi et al\\.,? 2015", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Infty: an integrated ocr system for mathematical documents", "author": ["M. Suzuki", "F. Tamari", "R. Fukuda", "S. Uchida", "T. Kanahori"], "venue": "Proceedings of the 2003 ACM symposium on Document engineering, 95\u2013104. ACM.", "citeRegEx": "Suzuki et al\\.,? 2003", "shortCiteRegEx": "Suzuki et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2755\u2013 2763.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "End-toend text recognition with convolutional neural networks", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on, 3304\u20133308. IEEE.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Abiword: Open source\u2019s answer to microsoft word", "author": ["H. Wen"], "venue": "Linux Dev Center, downloaded from http://www. linuxdevcenter. com/lpt/a/1636 1\u20133.", "citeRegEx": "Wen,? 2002", "shortCiteRegEx": "Wen", "year": 2002}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2048\u20132057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Optical character recognition (OCR) is most commonly used to recognize natural language from an image; however, as early as the work of (Anderson 1967), there has been research interest in converting images into structured language or markup that defines both the text itself and its presentational semantics.", "startOffset": 136, "endOffset": 151}, {"referenceID": 3, "context": "The primary focus of this work is OCR for mathematical expressions, and how to handle presentational aspects such as sub and superscript notation, special symbols, and nested fractions (Belaid and Haton 1984; Chan and Yeung 2000).", "startOffset": 185, "endOffset": 229}, {"referenceID": 16, "context": "The most effective systems combine specialized character segmentation with grammars of the underlying mathematical layout language (Miller and Viola 1998).", "startOffset": 131, "endOffset": 154}, {"referenceID": 21, "context": "A prime example of this approach is the INFTY system that is used to convert printed mathematical expressions to LaTeX and other markup formats (Suzuki et al. 2003).", "startOffset": 144, "endOffset": 164}, {"referenceID": 4, "context": "handwriting recognition (Ciresan et al. 2010), OCR in natural scenes (Jaderberg et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 9, "context": "2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 24, "context": "2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 11, "context": "2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b).", "startOffset": 35, "endOffset": 84}, {"referenceID": 23, "context": "2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b).", "startOffset": 35, "endOffset": 84}, {"referenceID": 22, "context": "While results from language modeling suggest that neural models can consistently generate syntactically correct markup (Karpathy, Johnson, and Li 2015; Vinyals et al. 2015a), it is unclear whether the full solution can be learned from markup-image pairs.", "startOffset": 119, "endOffset": 173}, {"referenceID": 27, "context": "Similar to work in image captioning (Xu et al. 2015), the model incorporates a multi-layer convolutional network over the image with an attention-based recurrent neural network decoder.", "startOffset": 36, "endOffset": 52}, {"referenceID": 9, "context": ") Unlike some recent OCR work (Jaderberg et al. 2015; Lee and Osindero 2016), we do not use final fully-connected layers (Ioffe and Szegedy 2015), since we want to preserve the locality of CNN features in order to use visual attention.", "startOffset": 30, "endOffset": 76}, {"referenceID": 8, "context": "2015; Lee and Osindero 2016), we do not use final fully-connected layers (Ioffe and Szegedy 2015), since we want to preserve the locality of CNN features in order to use visual attention.", "startOffset": 73, "endOffset": 97}, {"referenceID": 17, "context": "This network architecture is now standard; we model it specifically after the network used by Shi et al. (2015) for OCR from images (specification is given in Table 2.", "startOffset": 94, "endOffset": 112}, {"referenceID": 27, "context": "Row Encoder In attention-based image captioning (Xu et al. 2015), the image feature grid can be directly fed into the decoder.", "startOffset": 48, "endOffset": 64}, {"referenceID": 21, "context": "The current best-available OCR-based mathematical expression recognition system is the InftyReader system, a proprietary commercial implementation of the INFTY system of (Suzuki et al. 2003).", "startOffset": 170, "endOffset": 190}, {"referenceID": 25, "context": "contains a Tex generation mode (Wen 2002).", "startOffset": 31, "endOffset": 41}, {"referenceID": 27, "context": "For a neural model, a natural comparison is to standard image captioning approaches (Xu et al. 2015).", "startOffset": 84, "endOffset": 100}, {"referenceID": 19, "context": "6 We also include standard intrinsic text generation metrics, conditional language model perplexity and BLEU score (Papineni et al. 2002).", "startOffset": 115, "endOffset": 137}], "year": 2016, "abstractText": "Building on recent advances in image caption generation and optical character recognition (OCR), we present a generalpurpose, deep learning-based system to decompile an image into presentational markup. While this task is a wellstudied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domainspecific LaTeX OCR system achieves around 25% accuracy, our model reproduces the exact rendered image on 75% of examples.", "creator": "TeX"}}}