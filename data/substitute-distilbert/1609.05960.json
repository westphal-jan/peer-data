{"id": "1609.05960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Incremental Sampling-based Motion Planners Using Policy Iteration Methods", "abstract": "recent progress in randomized motion planners has led to the development of a fundamental class of sampling - based algorithms that provide asymptotic optimality guarantees, respectively the rrt * and the prm * algorithms. careful analysis reveals that the so - called \" rewiring \" step in these algorithms can be interpreted during a arbitrary policy iteration ( ao ) step ( i. e., a local attitude evaluation run represented by arbitrary local policy evaluate step ) so that asymptotically, as the number of samples tend to infinity, candidate algorithms converge to the optimal path almost surely ( with magnitude 1 ). policy iteration, along with value iteration ( vi ) are common methods through solving dynamic programming ( dp ) problems. based upon this observation, recently, the rrt $ ^ { \\ # } $ array successfully been proposed, which performs, during each iteration, bellman updates ( aka \" backups \" ) by those vertices of the graph that have maximal potential of being part around the optimal path ( i. e., the \" promising \" value ). the numerical $ ^ { \\ # } $ algorithm thus utilizes dynamic programming ideas and implements procedures incrementally with randomly generated graphs to obtain high quality solutions. in this work, and based on this key insight, programs explore a different class of constraint programming algorithms for solving shortest - possible problems on multiple graphs generated by iterative sampling methods. these class of algorithms utilize policy iteration instead of value iteration, and thus are better suited for massive parallelization. contrary to the rrt * algorithm, the policy improvement during the rewiring step is not performed uniquely locally but rather on a sum of vertices that were classified dangerously \" promising \" during the current iteration. this proposes to speed - drain the computational process. the resulting algorithm, possibly named policy iteration - rrt $ ^ { \\ # } $ ( pi - rrt $ ^ { \\ # } $ ) is selected first of a new branch of dp - inspired algorithms for interactive motion planning that utilize pi projections.", "histories": [["v1", "Mon, 19 Sep 2016 22:36:03 GMT  (1151kb,D)", "http://arxiv.org/abs/1609.05960v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.SY", "authors": ["oktay arslan", "panagiotis tsiotras"], "accepted": false, "id": "1609.05960"}, "pdf": {"name": "1609.05960.pdf", "metadata": {"source": "CRF", "title": "Incremental Sampling-based Motion Planners Using Policy Iteration Methods", "authors": ["Oktay Arslan", "Panagiotis Tsiotras", "Daniel Guggenheim"], "emails": ["oktay.arslan@jpl.nasa.gov", "tsiotras@gatech.edu"], "sections": [{"heading": null, "text": "cently, the RRT# algorithm has been proposed, which performs, during each iteration, Bellman updates (aka\u201cbackups\u201d) on those vertices of the graph that have the potential of being part of the optimal path (i.e., the \u201cpromising\u201d vertices). The RRT# algorithm thus utilizes dynamic programming ideas and implements them incrementally on randomly generated graphs to obtain high quality solutions. In this work, and based on this key insight, we explore a different class of dynamic programming algorithms for solving shortest-path problems on random graphs generated by iterative sampling methods. These class of algorithms utilize policy iteration instead of value iteration, and thus are better suited for massive parallelization. Contrary to the RRT\u2217 algorithm, the policy improvement during the rewiring step is not performed only locally but rather on a set of vertices that are classified as \u201cpromising\u201d during the current iteration. This tends to speed-up the whole process. The resulting algorithm, aptly named Policy Iteration-RRT# (PI-RRT#) is the first of a new class of DP-inspired algorithms for randomized motion planning that utilize PI methods."}, {"heading": "1 Introduction", "text": "Robot motion planning is one of the fundamental problems in robotics. It poses several challenges due to the high-dimensionality of the (continuous) search space, the complex geometry of unknown infeasible regions, the possibility of a continuous action space, and the presence of differential constraints [21]. A commonly used approach to solve this problem is to form a graph by uniform or non-uniform discretization of the underlying continuous search space and employ one of the popular graph-based search-based methods (e.g., A\u2217, Dijkstra) to find a low-cost discrete path between the initial and the final points. These\n\u2217Oktay Arslan performed this research during his doctoral studies as a Robotics, PhD student while at Georgia Institute of Technology, Atlanta, GA 30332-0150, USA.\nar X\niv :1\n60 9.\n05 96\n0v 1\n[ cs\n.R O\napproaches essentially work as abstractions of the underlying problem and hence the quality of the solution depends on the level and fidelity of the underlying abstraction. Despite this drawback, representing the robot motion planning problem as a graph search problem has several merits, especially since heuristic graph search-based methods provide strong theoretical guarantees such as completeness, optimality, or bounded suboptimality [7, 9, 19]. Unfortunately, graph search methods are not scalable to high-dimensional problems since the resulting graphs typically have an exponentially large number of vertices as the dimension of the problem increases. This major shortcoming of grid-based graph search planners has resulted in the development of randomized (i.e., probabilistic, sampling-based) planners that do not construct the underlying search graph a priori. Such planners, notably, PRM [13] and RRT [15, 16, 14], have been proven to be successful in solving many high-dimensional real-world planning problems. These planners are simple to implement, require less memory, work efficiently for high-dimensional problems, but come with a relaxed notion of completeness, namely, probabilistic completeness. That is, the probability that the planner fails to return a solution, if one exists, decays to zero as the number of samples approaches infinity. However, the resulting paths could be arbitrarily suboptimal [12].\nRecently, asymptotically optimal variants of these algorithms, such as PRM\u2217 and RRT\u2217 have been proposed [12], in order to remedy the undesirable behavior of the original RRT-like algorithms. The seminal work of [12] has sparked a renewed interest to asymptotically optimal probabilistic, samplingbased motion planners. Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].\nCareful analysis on the prototypical asymptotically optimal motion planning algorithm, that is, the RRT\u2217 algorithm, reveals that its optimality guarantees are the result of (at first glance) hidden ideas based on dynamic programming principles. In hindsight, this is hardly surprising. It should be noted however, that the explicit connection between asymptotically optimal, sampling-based algorithms operating on incrementally constructed random graphs and dynamic programming is not immediately obvious; indeed, in RRT\u2217 there is no mention of value functions or Bellman updates, although the local \u201crewiring\u201d step in the RRT\u2217 algorithm can be interpreted as a local policy improvement step on the underlying random graph (more details about this observation are given in Section 4).\nBased on this key insight, one can therefore draw from the rich literature of dynamic programming and reinforcement learning in order to design suitable algorithms that compute optimal paths on incrementally constructed random graphs. The recently proposed RRT# algorithm, for instance, utilizes a Gauss-Seidel version of asynchronous value iteration [2, 3] to speed up the convergence of RRT\u2217. Extensions based on similar ideas as the RRT# algorithm include the FMT* algorithm [10], the RRTx algorithm [17], and the BIT* algorithm [8]. All the previous algorithms use Bellman updates (equivalently, value iterations) to propagate the cost-to-come or cost-to-go for each vertex in the graph. Vertices are ranked according to these values (along with, perhaps, an additional heuristic) and are put into a queue. The order of vertices to be relaxed (i.e., participate in the Bellman update) are chosen from the queue. Different orderings of the vertices in the queue result in different variations of the same theme, but all of these algorithms \u2013 one way of another \u2013 perform a form of asynchronous value iteration.\nIn this work, we depart from the previous VI-based algorithms and we propose, instead, a novel class of algorithms based on policy-iteration (PI). Some preliminary results were presented in [4]. Policy iteration is an alternative to value iteration for solving dynamic programming problems and fits naturally into our framework, in the sense that a policy in a graph search amounts to nothing more but an assignment of a (unique) parent to each vertex. Use of policy iteration has the following benefits: first, no queue is needed to keep track of the cost of each vertex. A subset of vertices is selected for Bellman updates, and policy improvement on these vertices can be done in parallel at each iteration. Second, for a given graph, determination of the optimal policy is obtained after a finite number of iterations since the policy space is finite [6]. The determination of the optimal value for each vertex, on the other hand, requires an infinite number of iterations. More crucially, and in order to find the optimal policy, only the correct ordering of the vertices is needed, not their exact value. This can be utilized to develop approximation algorithms that speed up convergence. Third, although policy iteration methods are often slower than value iteration methods, they tend to be better amenable for parallelization and are faster if the structure of the problem is taken into consideration during implementation."}, {"heading": "2 Problem Formulation and Notation", "text": "Let X denote the configuration (search) space, which is assumed to be an open subset of Rd, where d \u2208 N with d \u2265 2. The obstacle region and the goal region are denoted by Xobs and Xgoal, respectively, both assumed to be closed sets. The obstacle-free space is defined by Xfree = X \\ Xobs. Elements of X are the states (or configurations) of the system. Let the initial configuration of the robot be denoted by xinit \u2208 Xfree. The (open) neighborhood of a state x \u2208 X is the open ball of radius r > 0 centered at x, that is, Br(x) = {x\u2032 \u2208 X : \u2016x\u2212 x\u2032\u2016 < r}. Given a subset S \u2286 V the notation |S| is its cardinality, that is, the number of elements of S.\nWe will approximate Xfree with an increasingly dense sequence of discrete subsets of Xfree. That is, Xfree will be approximated by a finite set of configuration points selected randomly from Xfree. Each such discrete approximation of Xfree will be encoded in a graph G = (V,E) with V being the set of vertices (the elements of the discrete approximation of Xfree) and with edge set E \u2286 V \u00d7 V encoding allowable transitions between elements of V . Hence, G is a directed graph. Transitions between two vertices x and x\u2032 in V are enabled by a control action u \u2208 U(x) such that x\u2032 is the successor vertex of x in G under the action u so that (x, x\u2032) \u2208 E. Let U = \u222ax\u2208V U(x). We use the mapping f : V \u00d7 U \u2192 V given by\nx\u2032 = f(x, u), u \u2208 U(x), (1)\nto formalize the transition from x to x\u2032 under the control action u. In this case, we say that x\u2032 is the successor of x and that x is the predecessor of x\u2032. The set of predecessors of x \u2208 V will be denoted by pred(G, x), and the set of successors of x will be denoted by succ(G, x). Also, we let pred(G, x) = pred(G, x) \u222a {x}. Note that, using the previous definitions, the set of admissible control actions at x may be equivalently defined as U(x) = {u : x\u2032 = f(x, u), x\u2032 \u2208 succ(G, x)}. (2) Thus, the control set U(x) defines unambiguously the set succ(G, x) of the successors of x, in the sense that there is one-to-one correspondence between control actions u \u2208 U(x) and elements of succ(G, x) via (1). Equivalently, once the directed graph G is given, for each edge (x, x\u2032) \u2208 E corresponds a control u \u2208 U(x) enabling this transition. It should be remarked that the latter statement, when dealing with dynamical systems (such as robots, etc) amounts to a controllability condition. Controllability is always satisfied for fully actuated systems, but may not be satisfied for underactuated systems (such as for many case of kinodynamic planning with differential constraints). For sampling-based methods such as RRT\u2217 this controllability condition is equivalent to the existence of a steering function that drives the system between any two given states.\nOnce we have abstracted Xfree using the graph G, the motion planning problem becomes one of a shortest path problem on the graph G. To this end, we define the path \u03c3 in G to be a sequence of vertices \u03c3 = (x0, x1, . . . , xN ) such that xk+1 \u2208 succ(G, xk) for all k = 0, 1, . . . , N \u2212 1. The length of the path is N , denoted by len(\u03c3) = N . When we want to specify explicitly the first node of the path we will use the first node as an argument, i.e., we will write \u03c3(x0). The kth element of \u03c3 will be denoted by \u03c3k. That is, if \u03c3(x0) = (x0, x1, . . . , xN ) then \u03c3k(x0) = xk for all k = 0, 1, . . . , N . A path is rooted at xinit if x0 = xinit. A path rooted at xinit terminates at a given goal region Xgoal \u2282 Xfree if xN \u2208 Xgoal.\nTo each edge (x, x\u2032) encoding an allowable transition from x \u2208 Xfree to x\u2032 \u2208 succ(G, x), we associate a finite cost c(x, x\u2032). Given a path \u03c3(x0), the cumulative cost along this path is then\nN\u22121\u2211\nk=0\nc(xk, xk+1). (3)\nGiven a point x \u2208 X , a mapping \u00b5 : x 7\u2192 u \u2208 U(x) that assigns a control action to be executed at each point x is called a policy. Let M denote the space of all policies. Under some assumptions on the connectivity of the graph G and the cost of the directed edges, one can use DP algorithms and the corresponding Bellman equation in order to compute optimal policies. Note that a policy \u00b5 \u2208 M for this problem defines a graph whose edges are (x, f(x, \u00b5(x))) \u2208 E for all x \u2208 V . The policy \u00b5 is proper if and only if this graph is acyclic, i.e., the graph has no cycles. Thus, there exists a proper policy \u00b5 if and only if\neach node is connected to the Xgoal with a directed path. Furthermore, an improper policy has finite cost, starting from every initial state, if and only if all the cycles of the corresponding graph have non-negative cost [6]. Convergence of the DP algorithms is proven if the graph is connected and the costs of all its cycles are positive [5]."}, {"heading": "3 Overview of Dynamic Programming", "text": "Dynamic programming solves sequential decision-making problems having a finite number of stages. In terms of DP notation, our system has the following equation\nx\u2032 = f(x, u) (4)\nwhere the cost function is defined as\ng(x, u) = c(x, f(x, u)). (5)\nGiven a sequential decision problem of the form (4)-(5), it is well known that the optimal cost function satisfying the following Bellman equation:\nJ\u2217(x) = inf u\u2208U(x)\n{ g(x, u) + J\u2217(f(x, u)) } , \u2200x \u2208 X . (6)\nThe result of the previous optimization results in an optimal policy \u00b5\u2217 \u2208M, that is,\n\u00b5\u2217(x) \u2208 arg min u\u2208U(x)\n{ g(x, u) + J\u2217(f(x, u)) } , \u2200x \u2208 X . (7)\nNote that if we are given a policy \u00b5 \u2208M (not necessarily optimal) we can compute its cost from\nJ\u00b5(x) = g(x, \u00b5(x)) + J\u00b5(f(x, \u00b5(x))), \u2200x \u2208 X . (8)\nIt follows that J\u2217(x) = inf\u00b5\u2208M J\u00b5(x), x \u2208 X . By introducing the expression\nH(x, u, J) = g(x, u) + J(f(x, u)), x \u2208 X , u \u2208 U(x). (9)\nand letting the operator T\u00b5 for a given policy \u00b5 \u2208M,\n(T\u00b5J)(x) = H(x, \u00b5(x), J), x \u2208 X , (10)\nwe can define the Bellman operator T\n(TJ)(x) = inf u\u2208U(x) H(x, u, J) = inf \u00b5\u2208M\n(T\u00b5J)(x), x \u2208 X , (11)\nwhich allows us to write the Bellman equation (6) succinctly as follows\nJ\u2217 = TJ\u2217, (12)\nand the optimality condition (7) as T\u00b5\u2217J \u2217 = TJ\u2217. (13) This interpretation of the Bellman equation states that J\u2217 is the fixed point of the Bellman operator T , viewed as a mapping from the set of real-valued functions on X into itself. Also, in a similar way, J\u00b5, the cost function of the policy \u00b5, is a fixed point of T\u00b5 (see (8)).\nThere are three different classes of DP algorithms to compute the optimal policy \u00b5\u2217 and the optimal cost function J\u2217.\nValue Iteration (VI). This algorithm computes J\u2217 by relaxing Eq. (6), starting with some J0, and generating a sequence { T kJ\n}\u221e k=0 using the iteration\nJk+1 = TJk (14)\nThe generated sequence converges to the optimal cost function due to contraction property of the Bellman operator T [5]. This method is an indirect way of computing the optimal policy \u00b5\u2217, using the information of the optimal cost function J\u2217.\nPolicy Iteration (PI). This algorithm starts with an initial policy \u00b50 and generates a sequence of policies \u00b5k by performing Bellman updates. Given the current policy \u00b5k, the typical iteration is performed in two steps:\ni) Policy evaluation: compute J\u00b5k as the unique solution of the equation\nJ\u00b5k = T\u00b5kJ\u00b5k . (15)\nii) Policy improvement: compute a policy \u00b5k+1 that satisfies\nT\u00b5k+1J\u00b5k = TJ\u00b5k . (16)\nOptimistic Policy Iteration (O-PI). This algorithm works the same as PI, but differs in the policy evaluation step. Instead of solving the system of linear equations exactly in the policy evaluation step (15), it performs an approximate evaluation of the current policy and uses this information in the subsequent policy improvement step."}, {"heading": "4 Random Geometric Graphs", "text": "The main difference between standard shortest path problems on graphs and sampling-based methods for solving motion planning problems is the fact that in the former case the graph is given a priori, whereas in the latter case the path is constructed on-the-fly by sampling randomly allowable configuration points from Xfree and by constructing the graph G incrementally, adding one, or more, vertices at each iteration step. Of course, such an iterative construction raises several questions, such as: is the resulting graph connected? under what conditions one can expect that G is an accurate representation of Xfree? how does discretizing the actions/control inputs affects the movement between sampled successor vertices, etc. All these questions have been addressed in a series of recent papers [11, 12] so we will not elaborate further on the graph construction. Suffice it to say, such random geometric graphs (RGGs) can be constructed easily and such graphs have been the cornerstone of the recent emergence of asymptotically optimal sampling based motion planners.\nFor completeness, and in order to establish the necessary connections between DP algorithms and RRGs, we provide a brief overview of random graphs as they are used in this work. For more details, the interested reader can peruse [6] or [20].\nIn graph theory, a random geometric graph (RGG) is a mathematical object that is usually used to represent spatial networks. RGGs are constructed by placing a collection of vertices drawn randomly according to a specified probability distribution. These random points constitute the node set of the graph in some topological space. Its edge set is formed via pairwise connections between these nodes if certain conditions (e.g., if their distance according to some metric is in a given range) are satisfied. Different probability distributions and connection criteria yield random graphs of different properties.\nAn important class of random geometric graphs is the random r-disc graphs. Given the number of points n and a nonnegative radius value r, a random r-disc graph in Rd is constructed as follows: first, n points are independently drawn from a uniform distribution. These points are pairwise connected if and only if the distance between them is less than r. Depending on the radius, this simple model of random geometric graphs possesses different properties as the number of nodes n increases. A natural question\nto ask is how the connectivity of the graph changes for different values of the connection radius as the number of samples goes to infinity. In the literature, it is shown that the connectivity of the random graph exhibits a phase transition, and a connected random geometric graph is constructed almost surely when the connection radius r is strictly greater than a critical value r\u2217 = { log(n)/(n\u03b6d) }d , where \u03b6d is volume of the unit ball in Rd. If the connection radius is chosen less than the critical value r\u2217, then, multiple disconnected clusters occur almost surely as n goes to infinity [20].\nRecently, novel connections have been made between motion planning algorithms and the theory of random geometric graphs [12]. These key insights have led to the development of a new class of algorithms which are asymptotically optimal (e.g., RRG, RRT\u2217, PRM\u2217). For example, in the RRG algorithm, a random geometric r-disc graph is first constructed incrementally for a fixed number of iterations. Then, a post-search is performed on this graph to extract the encoded solution. The key step is that the connection radius is shrunk as a function of vertices, while still being strictly greater than the critical radius value. By doing so, it is guaranteed to obtain a connected and sparse graph, yet the graph is rich enough to provide asymptotic optimality guarantees, almost surely. The authors in [12] showed that the RRG algorithm yields a consisted discretization of the underlying continuous configuration space, i.e., as the number of points goes to infinity, the lowest-cost solution encoded in the random geometric graph converges to the optimal solution embedded in the continuous configuration space with probability one. In this work, we leverage this nice feature of random geometric graphs to get a consistent discretization of the continuous domain of the robot motion planning problem. With the help of random geometric graphs, the robot motion planning problem boils down to a shortest path problem on a discrete graph."}, {"heading": "5 Proposed Approach", "text": ""}, {"heading": "5.1 From RRGs to DP", "text": "Let G = (V,E) denote the graph constructed by the RRG algorithm at some iteration, where V and E \u2286 V \u00d7 V are finite sets of vertices and edges, respectively. Based on the previous discussion, G is connected and all edge costs are positive, which implies that the cost of all the cycles in G are positive. Using the notation introduced in Section 2 , we can define on this graph the sequential decision system (4) where x\u2032 \u2208 succ(G, x) and with transition cost as in (5). Once a policy \u00b5 is given (optimal or not), there is a unique x\u2032 \u2208 succ(G, x) such that x\u2032 = f(x, \u00b5(x)), called the parent of x. Accordingly, x is the child of x\u2032 under the policy \u00b5. Conversely, a parent assignment for each node in G defines a policy. Note that each node has a single parent under a given policy, but may have multiple children.\nIn our case, the graph computed by the RRG algorithm is a connected graph by construction, and all edge cost values are positive, which implies that the costs of all its cycles are positive. Therefore, convergence is guaranteed and the resulting optimal policy is proper."}, {"heading": "5.2 DP Algorithms for Sampling-based Planners", "text": "The sampling-based motion planner which utilizes VI, i.e., RRT#, was presented in [2]. The RRT# algorithm implements the Gauss-Seidel version of the VI algorithm and provides a sequential implementation. In this work, we follow up on the same idea and propose a sampling-based algorithm which utilizes PI algorithm as shown in Figure 1.\nThe body of PI-RRT# algorithm is given in Algorithm 1. The algorithm initializes the graph with xgoal in Line 2 and incrementally builds the graph from xgoal toward xinit. The algorithm includes a new vertex and a couple new edges into the existing graph at each iteration. If this new information has a potential to improve the existing policy, then, a slightly modified PI algorithm is called subsequently in the Replan procedure. Specifically, and for the sake of numerical efficiency, unlike the standard PI algorithm, policy improvement is performed only for a subset vertices B which have the potential to be part of the optimal solution. As shown in Figure 1, Bk,i is the set of these vertices during the kth iteration and ith policy improvement step. The fact that this modification of the PI still ensures the asymptotic optimality, almost surely, of the proposed PI-RRT# algorithm requires extra analysis, which is presented in Section 6.\nPolicy Evaluation\nPolicy Improvement\n) goalx,initx (\n] i,k\ni,k\u00b5J i,k\u00b5kG\n[\n] ,0k\n,0k\u00b5J ,0k\u00b5kG\n[\n] 1,0\u2212k\n1,0\u2212k\u00b5J 1,0\u2212k\u00b51\u2212kG [ randx\n1\u2212i,k\u2208x\u2200)x(1\u2212i,k\u00b5=?)x(i,k\u00b5\n] 1\u2212i,k\n1\u2212i,k\u00b5J 1\u2212i,k\u00b5kG\n[\n] 1\u2212i,k\n1\u2212i,k\u00b5J i,k\u00b5kG\n[\n) goalx,initx (\nYes\nNo\nCCSampling\nExtension CC\n\u2190 \u2205E}goalx\u2190 {V \u2190 \u2205)goalx(0,0\u00b5)E,V(\u21900G 0\u2190)goalx(0,0\u00b5J}goalx\u2190 {0,0\n) X,goalx,initx (\nB\nB\nB\nB\nB\nB\nB\nThe Extend procedure is given in Algorithm 2. If a new vertex is decided for inclusion, its control is initialized by performing policy improvement in Lines 6-14. Then, it is checked in Line 15 if the new vertex has a potential to improve the existing policy where h denotes an admissible heuristic function that computes an estimate of the cost between two given points. If so, it is included to the set of vertices which are selected to perform policy improvement. Such heuristics, have been previously used to focus the search of sampling-based planner, see for example [18, 2].\nThe Replan procedure which implements the PI algorithm is shown in Algorithm 3. The policy improvement step is performed in Lines 2-9 until the cost of the existing policy becomes almost stationary. Note that the for-loop in the Replan procedure can run in parallel.\nThe policy evaluation step is implemented in Algorithm 4. Algorithm 4 solves a system of linear\nAlgorithm 2: Extend Procedure for RRT# (PI)\n1 Extend(G, B, xinit, xrand) 2 (V ,E)\u2190 G; E\u2032 \u2190 \u2205; 3 xnearest = Nearest(G, xrand); 4 xnew = Steer(xrand, xnearest); 5 if ObstacleFree(xnew, xnearest) then 6 J(xnew) = c(xnew, xnearest) + J(xnearest); 7 parent(xnew) = xnearest; 8 Xnear \u2190 Near(G, xnew, |V |); 9 foreach xnear \u2208 Xnear do\n10 if ObstacleFree(xnew, xnear) then 11 if J(xnew) > c(xnew, xnear) + J(xnear) then 12 J(xnew) = c(xnew, xnear) + J(xnear); 13 parent(xnew) = xnear; 14 E\u2032 \u2190 E\u2032 \u222a {(xnew, xnear), (xnear, xnew)};\n15 if h(xinit, parent(xnew)) + J(parent(xnew)) < J(xinit) then 16 B \u2190 B \u222a {xnew}; 17 V \u2190 V \u222a {xnew}; 18 E \u2190 E \u222a E\u2032; 19 G \u2190 (V ,E); 20 return (G, B);\nAlgorithm 3: Replan Procedure (PI) #\n1 Replan(G, B, xinit, xgoal) 2 Loop 3 foreach x \u2208 B do 4 J \u2032 = J(x); 5 foreach v \u2208 succ(G, x) do 6 if J \u2032 > c(x, v) + J(v) then 7 J \u2032 = c(x, v) + J(v); 8 parent(x) = v;\n9 \u2206J(x) = J(x)\u2212 J \u2032; 10 if maxx\u2208B \u2206J(x) \u2264 then 11 return B; 12 B \u2190 Evaluate(G, xinit, xgoal);\nequations by exploiting the underlying structure. Simply, the existing policy forms a tree in the current graph and the solution of the system of linear equations corresponds to the cost of each path connecting vertices to the goal region via edges of the tree. If xinit is already in the graph, the algorithm computes the cost of the path between xinit and xgoal by using queue q. Subsequently, the set of vertices that are promising, i.e., those in the set B and their cost-to-go values are computed by using the cost-to-go value of xinit.\nAlgorithm 4: Evaluate Procedure (PI) #\n1 Evaluate(G, xinit, xgoal) 2 (V ,E)\u2190 G; 3 if xinit \u2208 V then 4 x = xinit; 5 while x 6= xgoal do 6 q.push front(x); 7 x = parent(x);\n8 J(xgoal) = 0; 9 while q.empty() do\n10 x = q.pop front(); 11 J(x) = c(x, parent(x)) + J(parent(x));\n12 else 13 J(xinit) =\u221e; 14 B \u2190 {xgoal}; 15 q.push back(xgoal); 16 while q.nonempty() do 17 x = q.pop front(); 18 if h(xinit, x) + J(x) < J(xinit) then 19 foreach v \u2208 pred(G, x) do 20 if parent(v) = x then 21 J(v) = c(v, x) + J(x); 22 B \u2190 B \u222a {v}; 23 q.push back(v);\n24 return B;"}, {"heading": "6 Theoretical Analysis", "text": "The main purpose of this section is to show that the proposed PI-RRT# algorithm inherits the nice properties of the RRG and RRT\u2217 algorithms and thus it is asymptotically optimal, almost surely. The result follows trivially if policy iteration is performed on all the vertices of the current graph Gk at the kth iteration of the PI-RRT# algorithm. However, this will involve performing policy iteration also on vertices that may not have the potential to be part of the optimal solution. Ideally, and for the sake of numerical efficiency, we wish to preform policy improvement only on those vertices that have the potential of being part of the optimal solution, and only those. This will require a more detailed analysis, since we only have an estimate of this set of (so-called promising) vertices. The basic idea of the proof is based on the fact that each policy improvement progresses sequentially and computes best paths of length one, then of length two, then of length three, and so on. This allows us to keep track of all promising vertices that can be part of the optimal path of increasing lengths (e.g., increasing number of path edges) as we start from the goal vertex and move back towards the start vertex. The proof is rather long and thus is is split in a sequence of several lemmas.\nTo this end, let Gk = (V k, Ek) denote the graph at the end of the kth iteration of the PI-RRT# algorithm. Given a vertex x \u2208 V k, let the control set Uk(x) be divided into two disjoint sets Uk,\u2217(x) and Uk,\u2032(x) and and let the successor set succ(Gk, x) also be divided accordingly into two disjoint sets Sk,\u2217(x) and Sk,\u2032(x), as follows:\n\u2022 Uk(x) = Uk,\u2217(x) \u222a Uk,\u2032(x) where Uk,\u2217(x) = arg min u\u2208Uk(x) H(x, u, Jk,\u2217) and Uk,\u2032(x) = Uk(x) \\ Uk,\u2217(x)\n\u2022 succ(Gk, x) = Sk,\u2217(x) \u222a Sk,\u2032(x) where Sk,\u2217(x) = {x\u2217 \u2208 V k : \u2203u\u2217 \u2208 Uk,\u2217(x) s.t. x\u2217 = f(x, u\u2217)} and Sk,\u2032(x) = succ(Gk, x) \\ Sk,\u2217(x)\nLet Mk denote the set of all policies at the end of the kth iteration of the PI-RRT# algorithm, that\nis, let Mk = {\u00b5 : \u00b5(x) \u2208 Uk(x), \u2200x \u2208 V k}. Given a policy \u00b5 \u2208 Mk and an initial state x, let \u03c3\u00b5(x) denote the path resulting from executing the policy \u00b5 starting at x. That is, \u03c3\u00b5(x) = (x0, x1, . . . , xN ) such that \u03c3\u00b50 (x) = x and \u03c3 \u00b5 N (x) \u2208 Xgoal for N > 0, where \u03c3 \u00b5 j (x) is the jth element of \u03c3 \u00b5(x). By definition, xj+1 = f(xj , \u00b5(xj)) for j = 0, 1, . . . , N \u2212 1. Let now \u03a3k(x) be the set of all paths rooted at x and let \u03a3k,\u2217(x) denote the set of all lowest-cost paths rooted at x that reach the goal region at the kth iteration of the algorithm, that is,\n\u03a3k,\u2217(x) = {\u03c3 \u2208 \u03a3k(x) : \u03c3 = \u03c3\u00b5(x) such that \u2203\u00b5 \u2208Mk, J\u00b5(x) = Jk,\u2217(x)}.\nNote that the set \u03a3k,\u2217(x) may contain more than a single path. Finally, let Nk(x) denote the shortest path length in \u03a3k,\u2217(x), that is,\nNk(x) = min \u03c3\u00b5(x)\u2208\u03a3k,\u2217(x) len(\u03c3\u00b5(x))\nLet us define the following sets for a given policy \u00b5k,i \u2208Mk and its corresponding value function J\u00b5k,i at the end of the ith policy improvement step and at the kth iteration of the PI-RRT# algorithm:\na) The set of vertices in Gk whose optimal cost value is less than that of xinit,\nV kprom = {x \u2208 V k : Jk,\u2217(x) < Jk,\u2217(xinit)}\nThis is the set of promising vertices.\nb) The set of promising vertices in Gk, whose optimal cost value is achieved by executing the policy \u00b5k,i at the ith policy iteration step\nOk,i = {x \u2208 V k : J\u00b5k,i(x) = Jk,\u2217(x) < Jk,\u2217(xinit)}\nc) The set of vertices in Gk that can be connected to the goal region at iteration k with an optimal path of length less than or equal to `\nLk,` = {x \u2208 V k : \u2203\u03c3\u00b5(x) \u2208 \u03a3k,\u2217(x) s.t. Nk(x) \u2264 `}\nd) The set of promising vertices in Gk that are connected to the goal region via optimal paths whose length is less than or equal to ` P k,` = Lk,` \u2229 V kprom e) The set of vertices that are selected for a Bellman update during the beginning of the ith policy\nimprovement Bk,i = {pred(Gk, x) : x \u2208 V k, J\u00b5k,i(x) < J\u00b5k,i(xinit)}\nNote from d) that the set of promising vertices that can be connected to the goal region via optimal paths whose length is exactly `+ 1 is given by\n\u2202P k,` = P k,`+1 \\ P k,`\nIt should also be clear from these definitions that Ok,i \u2286 Bk,i for all k = 1, 2, . . . and i = 0, 1, 2, . . ..\nLemma 1 The sequence Ok,i generated by the policy iteration step of the PI-RRT# algorithm is nondecreasing, that is, Ok,i \u2286 Ok,i+1 for all i = 0, 1, . . ..\nProof First, note that Ok,0 = V k \u2229Xgoal 6= \u2205. Let now i > 0 and assume that x \u2208 Ok,i. By definition, we have that J\u00b5k,i = T\u00b5k,iJ\u00b5k,i where \u00b5 k,i is the policy computed at the end of ith policy improvement step at the kth iteration of the PI-RRT# algorithm. The previous expression implies that (T\u00b5k,iJ\u00b5k,i)(x) = J k,\u2217(x), and hence \u00b5k,i(x) \u2286 Uk,\u2217(x). Similarly, the cost function J\u00b5k,i satisfies J\u00b5k,i(x) = Jk,\u2217(x) < Jk,\u2217(xinit) \u2264\nJ\u00b5k,i(xinit) which yields J\u00b5k,i(x) < J\u00b5k,i(xinit). It follows that the vertex x and its predecessors will be selected for Bellman update during the next policy improvement, that is, pred(Gk, x) \u2208 Bk,i. After policy improvement, the updated policy and the corresponding cost function are given by\n(T\u00b5k,i+1J\u00b5k,i)(x) = (TJ\u00b5k,i)(x) = J k,\u2217(x),\nwhich implies that (T\u00b5k,i+1J\u00b5k,i)(x) = J k,\u2217(x) and hence \u00b5k,i+1(x) \u2286 Uk,\u2217(x). Similarly, Jk,\u2217(x) \u2264 J\u00b5k,i+1(x) \u2264 J\u00b5k,i(x) = Jk,\u2217(x), and hence J\u00b5k,i+1(x) = Jk,\u2217(x) < Jk,\u2217(xinit). It follows that x \u2208 Ok,i+1.\nLemma 2 The sequence Lk,` is non-decreasing, that is, Lk,` \u2286 Lk,`+1 for ` = 0, 1, . . .. Furthermore, for all x \u2208 \u2202Lk,` = Lk,`+1 \\ Lk,`, there exists x\u2217 \u2208 Lk,` \u2229 Sk,\u2217(x).\nProof For ` = 0 we have that Lk,0 = V k \u2229 Xfree 6= \u2205. Let now ` > 0, and assume that x \u2208 Lk,`. Then, by definition, there exists a policy \u00b5 \u2208Mk such that the vertex x achieves its optimal cost function value, J\u00b5(x) = J\nk,\u2217(x), and the optimal path connecting x to the goal region has length less than or equal to `, that is, len(\u03c3\u00b5(x)) \u2264 ` < `+ 1, which implies, trivially, that x \u2208 Lk,`+1.\nTo show the second part of the statement, first notice that, by definition, the vertices in the set \u2202Lk,` are the ones that can be connected to the goal region via an optimal path of length exactly `+ 1. Let us now assume that x \u2208 \u2202Lk,` and let \u03c3\u00b5(x) \u2208 \u03a3k,\u2217(x) be the optimal path of length ` + 1 between x and the goal region. Let \u03c3\u00b51 (x) = x\n\u2217 and \u03c3\u00b5(x\u2217) be the sub-arc rooted at x\u2217 resulting from applying \u00b5. By construction of the path \u03c3\u00b5(x), we have that x \u2208 pred(Gk, x\u2217). Also, since \u03c3\u00b5(x) is the optimal path rooted at x, the control action applied at vertex x needs to be optimal , that is, \u00b5(x) \u2208 Uk,\u2217(x) and \u03c3\u00b5(x\u2217) is the optimal path connecting x\u2217 to the goal region due to the principle of optimality, where \u03c3\u00b5(x\u2217) \u2208 \u03a3k,\u2217(x\u2217), which implies that x\u2217 \u2208 Sk,\u2217(x). Furthermore, x\u2217 \u2208 Lk,` since len(\u03c3\u00b5(x\u2217)) = `.\nCorollary 1 The sequence P k,` is non-decreasing, that is, P k,` \u2286 P k,`+1 for ` = 0, 1, . . .. Furthermore, for all x \u2208 \u2202P k,` = P k,`+1 \\ P k,`, there exists x\u2217 \u2208 P k,` \u2229 Sk,\u2217(x).\nProof The first part of the result follows immediately from Lemma 2. To show the second part, notice that, from the definition of the boundary set, we can rewrite \u2202P k,` as follows:\n\u2202P k,` = P k,`+1 \\ P k,` = (Lk,`+1 \u2229 V kprom) \\ (Lk,` \u2229 V kprom) = \u2202Lk,` \u2229 V kprom\nLet now x \u2208 \u2202P k,`, which implies that x \u2208 \u2202Lk,` and x \u2208 V kprom. From Lemma 2 there exists x\u2217 \u2208 Lk,`\u2229Sk,\u2217(x) such that x \u2208 pred(Gk, x\u2217). We need to show that x\u2217 \u2208 P k,`. Since x\u2217 \u2208 Lk,` we only need to show that x\u2217 \u2208 V kprom. Since x is a promising vertex, its optimal cost value satisfies Jk,\u2217(x) < Jk,\u2217(xinit). We know that the optimal cost function value of x\u2217 satisfies Jk,\u2217(x) = g(x, u\u2217) + Jk,\u2217(x\u2217) where x\u2217 = f(x, u\u2217) and u\u2217 \u2208 Uk,\u2217(x). Since g(x, u\u2217) is nonnegative, we have that Jk,\u2217(x\u2217) \u2264 Jk,\u2217(x) < Jk,\u2217(xinit) which implies x\u2217 \u2208 V kprom and hence x\u2217 \u2208 P k,`.\nLemma 3 Let x \u2208 Bk,i and assume that J\u00b5k,i(x\u2217) = Jk,\u2217(x\u2217) where x\u2217 \u2208 Sk,\u2217(x). Then \u00b5k,i+1(x) \u2286 Uk,\u2217(x) and J\u00b5k,i+1(x) = J\nk,\u2217(x) at the end of (i+ 1)th policy improvement step at the kth iteration of the PI-RRT# algorithm.\nProof We will first show that the function H(x, \u00b7, J\u00b5k,i) in (9) obeys a strict inequality when evaluated at elements of the sets Uk,\u2217(x) and Uk,\u2032(x). At the beginning of the (i + 1)th policy improvement step, the new policy \u00b5k,i+1 is computed as follows. For all x \u2208 Bk,i\n\u00b5k,i+1(x) \u2208 arg min u\u2208Uk(x) H(x, u, J\u00b5k,i)\n= arg min u\u2208Uk(x)\n{ g(x, u) + J\u00b5k,i(f(x, u)) } .\nLet u\u2217 \u2208 Uk,\u2217(x) and u\u2032 \u2208 Uk,\u2032(x). We then have the following:\nH(x, u\u2217, J\u00b5k,i) = g(x, u \u2217) + J\u00b5k,i(f(x, u \u2217))\n= g(x, u\u2217) + J\u00b5k,i(x \u2217) = g(x, u\u2217) + Jk,\u2217(x\u2217) < g(x, u\u2032) + Jk,\u2217(x\u2032) = g(x, u\u2032) + Jk,\u2217(f(x, u\u2032)) \u2264 g(x, u\u2032) + J\u00b5k,i(f(x, u\u2032)) = H(x, u\u2032, J\u00b5k,i).\nThis implies that H(x, u\u2217, J\u00b5k,i) < H(x, u\n\u2032, J\u00b5k,i) \u2200u\u2217 \u2208 U\u2217(x), u\u2032 \u2208 U \u2032(x). Hence, it follows that\n\u00b5k,i+1(x) \u2208 arg min u\u2208Uk(x) H(x, u, J\u00b5k,i) = arg min u\u2208Uk,\u2217(x) H(x, u, J\u00b5k,i)\nand thus \u00b5k,i+1(x) \u2286 Uk,\u2217(x) for all x \u2208 Bk,i. Let x \u2208 Bk,i. The cost function J\u00b5k,i+1 for x\u2217 \u2208 Sk,\u2217(x) is computed during the policy evaluation step for the new policy \u00b5k,i+1, as follows\nJk,\u2217(x\u2217) \u2264 J\u00b5k,i+1(x\u2217) \u2264 J\u00b5k,i(x\u2217) = Jk,\u2217(x\u2217) \u21d2 J\u00b5k,i+1(x\u2217) = Jk,\u2217(x\u2217) \u2200x\u2217 \u2208 Sk,\u2217(x)\nallowing us to write J\u00b5k,i+1(x) as follows:\nJ\u00b5k,i+1(x) = g(x, u \u2217) + J\u00b5k,i+1(f(x, u \u2217)) = g(x, u\u2217) + J\u00b5k,i+1(x \u2217) = g(x, u\u2217) + Jk,\u2217(x\u2217) = Jk,\u2217(x),\nwhich implies that J\u00b5k,i+1(x) = J k,\u2217(x).\nLemma 4 Let the policy \u00b5k,i and its corresponding cost function J\u00b5k,i, and assume that P k,` \u2286 Ok,i. Then \u2202P k,` \u2286 Bk,i, which implies that P k,`+1 \u2286 Bk,i before the beginning of the (i + 1)th policy improvement step. Furthermore, P k,`+1 \u2286 Ok,i+1 after the (i+ 1)th policy improvement step.\nProof As shown in Corollary 1, for all x \u2208 \u2202P k,`, there exists x\u2032 \u2208 P k,` such that x\u2032 \u2208 Sk,\u2217(x). The last inclusion which, in particular, that x\u2032 \u2208 succ(Gk, x), equivalently, x \u2208 pred(Gk, x\u2032). Since, by assumption, P k,` \u2286 Ok,i, we have that x\u2032 \u2208 Ok,i and thus the following holds:\nJ\u00b5k,i(x \u2032) = Jk,\u2217(x\u2032) < Jk,\u2217(xinit) \u2264 J\u00b5k,i(xinit) \u21d2 J\u00b5k,i(x\u2032) < J\u00b5k,i(xinit).\nTherefore, pred(Gk, x\u2032) \u2208 Bk,i, which implies that all vertices of \u2202P k,` are selected for a Bellman update before the (i+ 1)th policy improvement step, and hence \u2202P k,` \u2286 Bk,i and P k,`+1 \u2286 Bk,i.\nFrom Corollary 1 we have that P k,` \u2286 P k,`+1. Since the sequence Ok,i is non-decreasing (Lemma 1), it follows that P k,` \u2286 Ok,i \u2286 Ok,i+1. Therefore, in order to prove that P k,`+1 \u2286 Ok,i+1 we only need to show that \u2202P k,` \u2286 Ok,i+1 by the end of the (i + 1)th policy improvement. From Lemma 3, and since J\u00b5k,i(x\n\u2032) = Jk,\u2217(x\u2032), x\u2032 \u2208 Sk,\u2217(x), all vertices of \u2202P k,` achieve their optimal policy and cost function value after the end of the policy improvement step, and thus \u00b5k,i+1(x) \u2286 Uk,\u2217(x) and J\u00b5k,i+1(x) = Jk,\u2217(x). This implies that \u2202P k,` \u2286 Ok,i+1, thus completing the proof.\nLemma 5 All vertices whose optimal cost value is less than that of xinit, and which are part of an optimal path from xinit to Xgoal whose length is less than or equal to i, achieve their optimal cost value at the end of the ith policy improvement step, that is, P k,i \u2286 Ok,i for i = 0, 1, . . . when using policy \u00b5k,i.\nProof The claim P k,i \u2286 Ok,i will be shown using induction.\nBasis i = 0: First, note that V kgoal 6= \u2205. Let us now assume that x \u2208 V kgoal. Then Nk(x) = 0 for all k = 1, 2, . . ., and Jk,\u2217(x) = 0 < Jk,\u2217(xinit). Therefore, P k,0 = V kgoal. Also, for all x \u2208 P k,0, we have that J\u00b5k,0(x) = J k,\u2217(x) = 0 < Jk,\u2217(xinit), which implies P k,0 \u2286 Ok,0.\nBasis i = 1: The set of vertices along optimal paths whose length is less than or equal to 1 is a subset of goal vertices and their predecessors, that is, P k,1 = P k,0 \u222a {x \u2208 V k : \u2203x\u2032 \u2208 V k \u2229 Xgoal s.t. x \u2208 pred(Gk, x\u2032), c(x, x\u2032) < Jk,\u2217(xinit)}. For all x\u2032 \u2208 V kgoal, we have that J\u00b5k,0(x\u2032) = 0 < J\u00b5k,0(xinit). Therefore, all goal vertices and their predecessors are selected for Bellman update at the beginning of the first policy improvement step, hence Bk,0 = {pred(Gk, x\u2032) : x\u2032 \u2208 V kgoal}, which implies that P k,1 \u2286 Bk,0. All vertices in P k,1 will achieve their optimal cost values at the end of the first policy improvement step, that is, J\u00b5k,1(x) = J\nk,\u2217(x) = c(x, x\u2032), where x \u2208 P k,1, x\u2032 \u2208 Sk,\u2217(x) and x\u2032 \u2208 Vgoal, which implies that P k,1 \u2286 Ok,1.\nInductive step: Let us now assume that P k,i \u2286 Ok,i holds. We need to show that this assumption implies that P k,i+1 \u2286 Ok,i+1 at the end of (i + 1)th policy improvement step. The proof of this statement follows directly from Lemma 4 by taking ` = i.\nTheorem 1 (Optimality of Each Iteration) The optimal action and the optimal cost value for the initial vertex is achieved when the Replan procedure of the PI-RRT# algorithm terminates after a finite number of policy improvement steps.\nProof We will investigate the case in which the algorithm terminates before performing Nk(xinit) policy improvement steps, where k in the number of iterations the PI-RRT# algorithm has performed up to that point. Otherwise, optimality follows directly from Lemma 5.\nTo this end, assume, on the contrary, that the Replan procedure terminates at the end of the ith policy improvement step at the kth iteration of the PI-RRT# algorithm with a suboptimal cost function value for the initial vertex, that is, assume that J\u00b5k,i\u22121(xinit) > J\nk,\u2217(xinit). Since the termination condition holds, there will be no policy update for all vertices in Bk,i\u22121. That is, for all x \u2208 Bk,i\u22121, we have that \u00b5k,i(x) = \u00b5k,i\u22121(x). From Lemma 5 it follows that P k,i\u22121 \u2286 Ok,i\u22121 for all i = 1, 2, . . .. This implies that P k,i \u2286 Bk,i\u22121 at the beginning of the ith policy improvement step and P k,i \u2286 Ok,i at the end of ith policy improvement step because of Lemma 4. As a result, all vertices in P k,i achieve their optimal action and their optimal cost value at the end of the ith policy improvement step. Consequently, for all x \u2208 P k,i, we have that \u00b5k,i(x) = \u00b5k,\u2217(x) and J\u00b5k,i(x) = J\nk,\u2217(x). Since for all vertices in Bk,i\u22121 there is no update observed between policies \u00b5k,i and \u00b5k,i\u22121, we have that \u00b5k,i(x) = \u00b5k,i\u22121(x) = \u00b5k,\u2217(x) for all x \u2208 P k,i \u2286 Bk,i\u22121. Next, we investigate the cost function value of the vertices in P k,i at the beginning of ith policy improvement step and reach a contradiction.\nWe already know that vertices in P k,i\u22121 have achieved their optimal cost function values. Since P k,i = P k,i\u22121\u222a\u2202P k,i\u22121, we thus only need to check the cost values for all the vertices in the boundary set \u2202P k,i\u22121. For all vertices in \u2202P k,i\u22121, their cost function values can be expressed as J\u00b5k,i\u22121(x) = g(x, \u00b5\nk,i\u22121(x)) + J\u00b5k,i\u22121(f(x, \u00b5\nk,i\u22121(x))). We already know that \u00b5k,i\u22121(x) = \u00b5k,\u2217(x) holds for all vertices in \u2202P k,i\u22121 \u2286 Bk,i\u22121. Let us define \u00b5k,\u2217(x) = u\u2217 \u2208 Uk,\u2217(x) and x\u2217 \u2208 Sk,\u2217(x) such that x\u2217 = f(x, u\u2217). Since x\u2217 \u2208 P k,i\u22121, the optimal successor achieves its optimal cost value, that is, J\u00b5k,i\u22121(x\n\u2217) = Jk,\u2217(x\u2217). Then, for all vertices in \u2202P k,i\u22121, we can express their cost function value as J\u00b5k,i\u22121(x) = g(x, u\n\u2217) +Jk,\u2217(x\u2032) = Jk,\u2217(x). This implies that all vertices in P k,i already have achieved their optimal action and the cost values at the beginning of the ith policy improvement step, that is, P k,i \u2286 Ok,i\u22121. We have thus shown that P k,i\u22121 \u2286 Ok,i\u22121 implies P k,i \u2286 Ok,i\u22121 for all i = 1, 2, . . .. It follows that P k,` \u2286 Ok,i\u22121 for ` = 0, 1, . . ..\nNext, consider the case when ` = Nk(xinit) \u2212 1. From the previous analysis this implies that P k,` \u2286 Ok,i\u22121, which, in turn, implies that all vertices which may be intermediate vertices along optimal paths between xinit and the goal region achieve their optimal action and cost value at the beginning of the ith policy improvement step. Note that xinit is selected for a Bellman update at the beginning of the ith policy improvement step, since its cost function value can be written as J\u00b5k,i\u22121(xinit) = g(xinit, u) + J\u00b5k,i\u22121(x\n\u2032), where u \u2208 Uk(xinit) and x\u2032 \u2208 Sk(xinit) such that u = \u00b5k,i\u22121(xinit) and x\u2032 = f(xinit, u). This implies\nthat xinit \u2208 pred(Gk, x\u2032) and J\u00b5k,i\u22121(x\u2032) < J\u00b5k,i\u22121(xinit) and therefore, xinit \u2208 Bk,i\u22121. However, since the termination condition holds, a Bellman update for xinit does not yield any update in its action during the ith policy improvement step, and thus \u00b5k,i(xinit) = \u00b5\nk,i\u22121(xinit). We also know that, Since Sk,\u2217(xinit) \u2286 P k,` and P k,N\u22121 \u2286 Ok,i\u22121, it follows that all vertices in Sk,\u2217(xinit) have achieved their optimal action and their optimal cost value at the beginning of the ith policy iteration. That is, \u00b5k,i\u22121(x\u2032) = \u00b5k,\u2217(x\u2032) and Jk,i\u22121(x\u2032) = Jk,\u2217(x\u2032) for all x\u2032 \u2208 Sk,\u2217(xinit). It follows from Lemma 3 that \u00b5k,i(xinit) \u2286 Uk,\u2217(xinit) and J\u00b5k,i(xinit) = J\nk,\u2217(xinit) at the end of ith policy improvement step. This implies that \u00b5k,i\u22121(xinit) = \u00b5k,i(xinit) = \u00b5\nk,\u2217(xinit). The cost value of xinit at the beginning of ith policy improvement step is given by J\u00b5k,i\u22121(xinit) = g(x, \u00b5 k,i\u22121(xinit)) + J\u00b5k,i\u22121(f(x, \u00b5 k,i\u22121(xinit))). We know that \u00b5k,i\u22121(xinit) = \u00b5k,\u2217(x). Let \u00b5k,\u2217(xinit) = u\u2217 \u2208 Uk,\u2217(xinit) and x\u2032 \u2208 Sk,\u2217(xinit) such that x\u2032 = f(x, u\u2217). Since x\u2032 \u2208 P k,`, and ` = Nk(xinit) \u2212 1, x\u2032 achieves the optimal cost function value, and hence J\u00b5k,i\u22121(x\u2032) = Jk,\u2217(x\u2032). We thus have J\u00b5k,i\u22121(xinit) = g(x, u\n\u2217) + Jk,\u2217(x\u2032) = Jk,\u2217(xinit). We have thus shown that J\u00b5k,i\u22121(xinit) = J\nk,\u2217(xinit) which leads to the contradiction we seek, given the initial assumption that the algorithm terminates with a suboptimal cost value for the initial vertex.\nThe previous theorem states that when the Replan procedure terminates at the beginning of the Nk(xinit)th policy improvement step, it has already computed the optimal action and cost function value for xinit. If the algorithm terminates after more than or equal to N\nk(xinit) policy improvement steps, then optimality follows directly from Lemma 5, since the Replan procedure is thus guaranteed to terminate after a finite number of policy improvement steps, owing to the properties of policy iteration and the fact that the policy space is finite [6].\nTheorem 2 (Termination of Replan Procedure after a Finite Number of Steps) Let Gk = (V k, Ek) be the graph built at the end of kth iteration of the PI-RRT# algorithm. Then, the Replan procedure of the PI-RRT# algorithm terminates after at most (N k + 2) policy improvement steps, where N k\n= maxx\u2208V kprom N k(x) and V k prom = {pred(Gk, x) : x \u2208 V kprom}.\nProof Let us assume, on the contrary, that the Replan procedure does not terminate at the end of the (N k + 2)th policy improvement step at the kth iteration of the PI-RRT# algorithm. This implies that there exists a point x \u2208 Bk,N k +1 such that its cost function value is reduced, and its policy is updated at the end of the (N k\n+ 2)th policy improvement step. Equivalently, there exists u \u2208 Uk(x) that yields J \u00b5k,N k +1 (x) > g(x, u) + J \u00b5k,N k +1 (x\u2032) where x\u2032 \u2208 Sk(x), \u00b5k,N k +1 6= \u00b5k,N k +2(x) = u and x\u2032 = f(x, u). By definition, we have P k,N k\n= V kprom, which implies that V k prom \u2286 Ok,N k \u2286 Ok,N k +1 due to Lemma 5 and\nLemma 1. For all x \u2208 V kprom, we have J\u00b5k,Nk+1(x) = J k,\u2217(x) < Jk,\u2217(xinit) \u2264 J \u00b5k,N k +1 (xinit), which implies that pred(G, x) \u2208 Bk,N k +1. Therefore, V\nk prom \u2286 Bk,N k +1. Since V kprom \u2286 Ok,N k and V k prom \u2286 Bk,N k +1,\nit can also be shown, similarly to Lemma 4, that all vertices of V k prom achieve their optimal cost values and their optimal policies after the (N k\n+ 1)th policy improvement step. As a result, J \u00b5k,N k +1 (x) =\nJk,\u2217(x), \u00b5k,N k +1(x) \u2286 Uk,\u2217(x) for all x \u2208 V kprom.\nNext, note that for the successor vertex of xinit along the optimal path between xinit and the goal region\nwe have that Nk(x\u2032) = Nk(xinit) \u2212 1 \u2264 Nk since x\u2032 \u2208 V kprom. This implies that Nk(xinit) \u2264 N k + 1, and therefore, from Lemma 5, we have that J \u00b5k,N k +1 (xinit) = J k,\u2217(xinit). Recall now that, for all x \u2208 V k with pred(Gk, x) \u2208 Bk,N k +1, we have that J\n\u00b5k,N k +1 (x) < J \u00b5k,N k +1 (xinit). Since J \u00b5k,N k +1\n(xinit) = J k,\u2217(xinit), the\nfollowing expression holds:\nJk,\u2217(x) \u2264 J \u00b5k,N k +1 (x) < J \u00b5k,N k +1 (xinit) = J k,\u2217(xinit) \u21d2 Jk,\u2217(x) < Jk,\u2217(xinit)\nTherefore, x \u2208 V kprom and pred(Gk, x) \u2208 V k prom which implies that B k,N k +1 \u2286 V kprom. From the two preceding results, it follows that Bk,N k +1 = V\nk prom.\nLet x \u2208 BN k +1 = V k prom, whose policy is updated during the (N k + 2)th policy improvement step.\nWe therefore have that Jk,\u2217(x) = J \u00b5k,N k +1 (x) > g(x, u) + J \u00b5k,N k +1 (x\u2032) = g(x, u) + Jk,\u2217(x\u2032). This yields Jk,\u2217(x) > g(x, u) + Jk,\u2217(x\u2032), which contradicts (6), thus completing the proof.\nTheorem 3 (Asymptotic Optimality of PI-RRT#) Let Gk = (V k, Ek) be the graph built at the end of the kth iteration of the PI-RRT# algorithm and let Nk is maximum number of policy improvement steps performed at the k iteration. As k \u2192 \u221e, the policy \u00b5k,Nk(xinit) and its corresponding cost function J \u00b5k,Nk (xinit), converge to the optimal policy \u00b5 \u2217(xinit) and corresponding optimal cost function J\u00b5\u2217(xinit) with probability one.\nProof The graph Gk = (V k, Ek) is constructed by the RRG algorithm at the beginning of kth iteration. In the PI-RRT# algorithm, the optimal cost function value of xinit with respect to Gk is computed during the Replan procedure at the end of kth iteration, that is, J\n\u00b5k,Nk (xinit) = J\nk,\u2217(xinit) and \u00b5k,N k (xinit) =\n\u00b5k,\u2217(xinit). Since the RRG algorithm is asymptotically optimal with probability one, Gk will encode, almost surely, the optimal path between xinit and goal region as k \u2192\u221e. This implies that J\u00b5k,Nk (xinit) = Jk,\u2217(xinit)\u2192 J\u2217(xinit) and \u00b5k,Nk(xinit) = \u00b5k,\u2217(xinit)\u2192 \u00b5\u2217(xinit) with probability one."}, {"heading": "7 Numerical Simulations", "text": "We implemented both the baseline RRT# and PI-RRT# algorithms in MATLAB and performed Monte Carlo simulations on shortest path planning problems in two different 2D environments, namely, sparse and highly cluttered environments. The goal was to find the shortest path that minimizes the Euclidean distance from an initial point to a goal point. The initial and goal points are shown in yellow and dark blue squares in the figures below, respectively. The obstacles are shown in red and the best path computed during each iteration is shown in yellow.\nThe results were averaged over 100 trials and each trial was run for 10,000 iterations. No vertex rejection rule is applied during the extension procedure. We then computed the total time required to complete a trial and measured the time spent on the non-planning (sampling, extension, etc.) and the planning-related procedures of the algorithms, separately. The growth of the tree in each case is shown in Figure 2. At each iteration, a subset of promising vertices is determined during the policy evaluation step and policy improvement is performed only for these vertices. The promising vertices are shown in magenta in Figure 2.\nFor the first problem, the average time spent for non-planning related procedures in the RRT# and PI-RRT# algorithms are shown in blue and red colors, respectively, in Figure 3. As seen from these figures, PI-RRT# is slightly faster than the RRT# algorithm, especially when adding a new vertex to the graph. Since there is no priority queue in the PI-RRT# algorithm, it is much cheaper to include a new vertex and there is no need for vertex ordering.\nFor the first problem, the average time spent for planning related procedures for the RRT# and the PI-RRT# algorithms are shown in blue and red colors, respectively, in Figure 4. As seen ion those figures, the relation between time and iteration is linear when the number of iterations becomes large in the log-log scale plot, which implies a polynomial relationship, i.e., t(n) = cn\u03b1. One can find these parameters by using a least-square minimization based on the measured data for iterations between 100 to 10,000. These parameters can be computed as c0 = 6.4322\u00d7 10\u22125, \u03b10 = 1.2925 for RRT# and cpi = 1.0672\u00d7 10\u22126, \u03b1pi = 2.214 for the PI-RRT#. The fitted time-iteration lines (dashed) for the RRT# and the PI-RRT# are shown in magenta and green colors, respectively. In our implementation, we uses one processor to perform policy\nimprovement due to simplicity. However, as mentioned earlier, the policy improvement step can be done in parallel. One can divide the set of promising vertices into disjoint sets and assign each of them to a different processor.\nLet np denote the number of processors, and let N denote the computational load per processor, i.e, N = n/np, where n is the iteration number which can be considered as an upper bound on the number of promising vertices. Simple calculation shows that the load per processor needs to satisfy the following relationship for the PI-RRT# algorithm in order to outperform the baseline RRT# algorithm for faster planning.\nN = n np > c0 cpi n1+\u03b10\u2212\u03b1pi .\nFor the first problem, based on these empirical data, the load per processor versus iteration limit is N > 60.27n0.078562. This line is plotted in Figure 5. For example, the load per processor needs to be smaller than 124.27, so that each processor should not be assigned more than 124.27 vertices for policy improvement. This implies that the number of processors needs to be greater than 80.47 during the 10,000th iteration.\nFrom the previous simple analysis it follows that the PI-RRT# algorithm can be a better choice than RRT# algorithm for planning problems in high-dimensional search spaces. In high-dimensional search spaces, one needs to run planning algorithms for a large number of iterations in order to explore the search space densely and see a significant improvement in the computed solutions. This requirement induces a bottleneck on the RRT# algorithm and all similar VI-based algorithms since the re-planning procedure is performed sequentially and requires ordering of vertices. Therefore, this operation may take a long time, as the number of vertices increases significantly. On the other hand, the PI-RRT# algorithm does not require any ordering of the vertices, and one can keep re-planning tractable by employing more processors (e.g., spawning more threads) as needed, in order to meet the desired load per processor requirement. Given the current advancement in parallel computing technologies, such as GPUs, a well-designed parallel implementation of the PI-RRT# may yield significant real-time execution performance improvement for some problems that are known to be very challenging to handle with existing VI-based probabilistic algorithms.\nThe same analysis was carried out for the second problem and the results are shown in Figure 6, 7 and 8."}, {"heading": "8 Conclusion", "text": "We show that a connection between DP and RRGs may yield different types of sampling-based motion planning algorithms that utilize ideas from dynamic programming. These algorithms ensure asymptotic optimality (with probability one) as the number of samples tends to infinity. Use of policy iteration, instead of value iteration during the exploitation step, may offer several advantages, such as completely parallel implementation, avoidance of sorting and maintaining a queue of all sampled vertices in the graph, etc. We have implemented these ideas in the replanning step of the RRT# algorithm. The proposed PIRRT# algorithm can be massively parallelized, which can be exploited by taking advantage of the recent computational and technological advances of GPUs. This is part of ongoing work."}], "references": [{"title": "Machine Learning and Dynamic Programming Algorithms for Motion Planning and Control", "author": ["O. Arslan"], "venue": "PhD Thesis, Georgia Institute of Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Use of relaxation methods in sampling-based algorithms for optimal motion planning", "author": ["O. Arslan", "P. Tsiotras"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Dynamic programming guided exploration for sampling-based motion planning algorithms", "author": ["O. Arslan", "P. Tsiotras"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Dynamic programming principles for sampling-based motion planners", "author": ["O. Arslan", "P. Tsiotras"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Abstract Dynamic Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Dynamic Programming and Optimal Control, volume 1", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "A note on two problems in connexion with graphs", "author": ["E.W. Dijkstra"], "venue": "Numerische Mathematik,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1959}, {"title": "Batch informed trees (BIT*): Sampling-based optimal planning via the heuristically guided search of implicit random geometric graphs", "author": ["J.D. Gammell", "S.S. Srinivasa", "T.D. Barfoot"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P.E. Hart", "N.J. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1968}, {"title": "Fast marching tree: A fast marching samplingbased method for optimal motion planning in many dimensions", "author": ["L. Janson", "E. Schmerling", "A. Clark", "M. Pavone"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Optimal kinodynamic motion planning using incremental sampling-based methods", "author": ["S. Karaman", "E. Frazzoli"], "venue": "In IEEE Conference on Decision and Control,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Probabilistic roadmaps for path planning in high-dimensional configuration spaces", "author": ["L.E. Kavraki", "P. \u0160vestka", "J.-C. Latombe", "M.H. Overmars"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Planning Algorithms", "author": ["S.M. LaValle"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Rapidly-exploring random trees: Progress and prospects", "author": ["S.M. Lavalle", "Kuffner J. J"], "venue": "In Algorithmic and Computational Robotics: New Directions,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Randomized kinodynamic planning", "author": ["S.M. LaValle", "J.J. Kuffner"], "venue": "The International Journal of Robotics Research (IJRR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "RRTx: Real-time motion planning/replanning for environments with unpredictable obstacles", "author": ["M. Otte", "E. Frazzoli"], "venue": "In Algorithmic Foundations of Robotics XI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "C-FOREST: Parallel shortest-path planning with super linear speedup", "author": ["Michael Otte", "Nikolaus Correll"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Heuristics : Intelligent Search Strategies for Computer Problem Solving", "author": ["J. Pearl"], "venue": "Addison-Wesley Pub. Co,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1984}, {"title": "Random Geometric Graphs", "author": ["M.D. Penrose"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Complexity of the movers problem and generalizations extended abstract", "author": ["J.H. Reif"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1979}], "referenceMentions": [{"referenceID": 20, "context": "It poses several challenges due to the high-dimensionality of the (continuous) search space, the complex geometry of unknown infeasible regions, the possibility of a continuous action space, and the presence of differential constraints [21].", "startOffset": 236, "endOffset": 240}, {"referenceID": 6, "context": "Despite this drawback, representing the robot motion planning problem as a graph search problem has several merits, especially since heuristic graph search-based methods provide strong theoretical guarantees such as completeness, optimality, or bounded suboptimality [7, 9, 19].", "startOffset": 267, "endOffset": 277}, {"referenceID": 8, "context": "Despite this drawback, representing the robot motion planning problem as a graph search problem has several merits, especially since heuristic graph search-based methods provide strong theoretical guarantees such as completeness, optimality, or bounded suboptimality [7, 9, 19].", "startOffset": 267, "endOffset": 277}, {"referenceID": 18, "context": "Despite this drawback, representing the robot motion planning problem as a graph search problem has several merits, especially since heuristic graph search-based methods provide strong theoretical guarantees such as completeness, optimality, or bounded suboptimality [7, 9, 19].", "startOffset": 267, "endOffset": 277}, {"referenceID": 12, "context": "Such planners, notably, PRM [13] and RRT [15, 16, 14], have been proven to be successful in solving many high-dimensional real-world planning problems.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "Such planners, notably, PRM [13] and RRT [15, 16, 14], have been proven to be successful in solving many high-dimensional real-world planning problems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 15, "context": "Such planners, notably, PRM [13] and RRT [15, 16, 14], have been proven to be successful in solving many high-dimensional real-world planning problems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 13, "context": "Such planners, notably, PRM [13] and RRT [15, 16, 14], have been proven to be successful in solving many high-dimensional real-world planning problems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 11, "context": "However, the resulting paths could be arbitrarily suboptimal [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "Recently, asymptotically optimal variants of these algorithms, such as PRM\u2217 and RRT\u2217 have been proposed [12], in order to remedy the undesirable behavior of the original RRT-like algorithms.", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "The seminal work of [12] has sparked a renewed interest to asymptotically optimal probabilistic, samplingbased motion planners.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].", "startOffset": 101, "endOffset": 118}, {"referenceID": 2, "context": "Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].", "startOffset": 101, "endOffset": 118}, {"referenceID": 0, "context": "Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].", "startOffset": 101, "endOffset": 118}, {"referenceID": 9, "context": "Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].", "startOffset": 101, "endOffset": 118}, {"referenceID": 16, "context": "Several variants have been proposed that utilize the original ideas of [12]; a partial list includes [2, 3, 1, 10, 17].", "startOffset": 101, "endOffset": 118}, {"referenceID": 1, "context": "The recently proposed RRT algorithm, for instance, utilizes a Gauss-Seidel version of asynchronous value iteration [2, 3] to speed up the convergence of RRT\u2217.", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "The recently proposed RRT algorithm, for instance, utilizes a Gauss-Seidel version of asynchronous value iteration [2, 3] to speed up the convergence of RRT\u2217.", "startOffset": 115, "endOffset": 121}, {"referenceID": 9, "context": "Extensions based on similar ideas as the RRT algorithm include the FMT* algorithm [10], the RRTx algorithm [17], and the BIT* algorithm [8].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "Extensions based on similar ideas as the RRT algorithm include the FMT* algorithm [10], the RRTx algorithm [17], and the BIT* algorithm [8].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "Extensions based on similar ideas as the RRT algorithm include the FMT* algorithm [10], the RRTx algorithm [17], and the BIT* algorithm [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "Some preliminary results were presented in [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Second, for a given graph, determination of the optimal policy is obtained after a finite number of iterations since the policy space is finite [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Furthermore, an improper policy has finite cost, starting from every initial state, if and only if all the cycles of the corresponding graph have non-negative cost [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Convergence of the DP algorithms is proven if the graph is connected and the costs of all its cycles are positive [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "The generated sequence converges to the optimal cost function due to contraction property of the Bellman operator T [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "All these questions have been addressed in a series of recent papers [11, 12] so we will not elaborate further on the graph construction.", "startOffset": 69, "endOffset": 77}, {"referenceID": 11, "context": "All these questions have been addressed in a series of recent papers [11, 12] so we will not elaborate further on the graph construction.", "startOffset": 69, "endOffset": 77}, {"referenceID": 5, "context": "For more details, the interested reader can peruse [6] or [20].", "startOffset": 51, "endOffset": 54}, {"referenceID": 19, "context": "For more details, the interested reader can peruse [6] or [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "If the connection radius is chosen less than the critical value r\u2217, then, multiple disconnected clusters occur almost surely as n goes to infinity [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "Recently, novel connections have been made between motion planning algorithms and the theory of random geometric graphs [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "The authors in [12] showed that the RRG algorithm yields a consisted discretization of the underlying continuous configuration space, i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": ", RRT, was presented in [2].", "startOffset": 24, "endOffset": 27}, {"referenceID": 17, "context": "Such heuristics, have been previously used to focus the search of sampling-based planner, see for example [18, 2].", "startOffset": 106, "endOffset": 113}, {"referenceID": 1, "context": "Such heuristics, have been previously used to focus the search of sampling-based planner, see for example [18, 2].", "startOffset": 106, "endOffset": 113}, {"referenceID": 5, "context": "If the algorithm terminates after more than or equal to N (xinit) policy improvement steps, then optimality follows directly from Lemma 5, since the Replan procedure is thus guaranteed to terminate after a finite number of policy improvement steps, owing to the properties of policy iteration and the fact that the policy space is finite [6].", "startOffset": 338, "endOffset": 341}], "year": 2016, "abstractText": "Recent progress in randomized motion planners has led to the development of a new class of samplingbased algorithms that provide asymptotic optimality guarantees, notably the RRT\u2217 and the PRM\u2217 algorithms. Careful analysis reveals that the so-called \u201crewiring\u201d step in these algorithms can be interpreted as a local policy iteration (PI) step (i.e., a local policy evaluation step followed by a local policy improvement step) so that asymptotically, as the number of samples tend to infinity, both algorithms converge to the optimal path almost surely (with probability 1). Policy iteration, along with value iteration (VI) are common methods for solving dynamic programming (DP) problems. Based on this observation, recently, the RRT algorithm has been proposed, which performs, during each iteration, Bellman updates (aka\u201cbackups\u201d) on those vertices of the graph that have the potential of being part of the optimal path (i.e., the \u201cpromising\u201d vertices). The RRT algorithm thus utilizes dynamic programming ideas and implements them incrementally on randomly generated graphs to obtain high quality solutions. In this work, and based on this key insight, we explore a different class of dynamic programming algorithms for solving shortest-path problems on random graphs generated by iterative sampling methods. These class of algorithms utilize policy iteration instead of value iteration, and thus are better suited for massive parallelization. Contrary to the RRT\u2217 algorithm, the policy improvement during the rewiring step is not performed only locally but rather on a set of vertices that are classified as \u201cpromising\u201d during the current iteration. This tends to speed-up the whole process. The resulting algorithm, aptly named Policy Iteration-RRT (PI-RRT) is the first of a new class of DP-inspired algorithms for randomized motion planning that utilize PI methods.", "creator": "LaTeX with hyperref package"}}}