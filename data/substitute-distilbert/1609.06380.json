{"id": "1609.06380", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "abstract": "recognizing implicit discourse relations is a challenging equally important task in the field of natural language processing. for such a complex text processing framework, different from previous studies, we argue that it is necessary to integrate separate consecutive arguments and dynamically update the efficient features useful for recognizing discourse relations. directly mimic the distributed reading strategy, we propose the neural networks with multi - level attention ( nnma ), combining the attention mechanism and external memories to gradually reflect the emphasis on some specific words helpful to achieve these discourse relations. experiments on hierarchical pdtb dataset show that our proposed method achieves the state - of - art results. the visualization of the attention weights also illustrates the progress that hierarchical model observes earlier arguments on each level and progressively locates the important words.", "histories": [["v1", "Tue, 20 Sep 2016 22:59:19 GMT  (703kb,D)", "http://arxiv.org/abs/1609.06380v1", "Accepted as long paper at EMNLP2016"]], "COMMENTS": "Accepted as long paper at EMNLP2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yang liu", "sujian li"], "accepted": true, "id": "1609.06380"}, "pdf": {"name": "1609.06380.pdf", "metadata": {"source": "CRF", "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "authors": ["Yang Liu", "Sujian Li"], "emails": ["lisujian}@pku.edu.cn"], "sections": [{"heading": null, "text": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-ofart results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words."}, {"heading": "1 Introduction", "text": "Discourse relations (e.g., contrast and causality) support a set of sentences to form a coherent text. Automatically recognizing discourse relations can help many downstream tasks such as question answering and automatic summarization. Despite great progress in classifying explicit discourse relations where the discourse connectives (e.g., \u201cbecause\u201d, \u201cbut\u201d) explicitly exist in the text, implicit discourse relation recognition remains a challenge due to the absence of discourse connectives. Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse\nrelations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012). To some extent, these methods simulate the single-pass reading process that a person quickly skim the text through one-pass reading and directly collect important clues for understanding the text. Although single-pass reading plays a crucial role when we just want the general meaning and do not necessarily need to understand every single point of the text, it is not enough for tackling tasks that need a deep analysis of the text. In contrast with single-pass reading, repeated reading involves the process where learners repeatedly read the text in detail with specific learning aims, and has the potential to improve readers\u2019 reading fluency and comprehension of the text (National Institute of Child Health and Human Development, 2000; LaBerge and Samuels, 1974). Therefore, for the task of discourse parsing, repeated reading is necessary, as it is difficult to generalize which words are really useful on the first try and efficient features should be dynamically exploited through several passes of reading .\nNow, let us check one real example to elaborate the necessity of using repeated reading in discourse parsing. Arg-1 : the use of 900 toll numbers has been\nexpanding rapidly in recent years\nArg-2 : for a while, high-cost pornography lines and services that tempt children to dial (and redial) movie or music information earned the service a somewhat sleazy image\n(Comparison - wsj 2100)\nTo identify the \u201cComparison\u201d relation between\nar X\niv :1\n60 9.\n06 38\n0v 1\n[ cs\n.C L\n] 2\n0 Se\np 20\nthe two arguments Arg-1 and Arg-2, the most crucial clues mainly lie in some content, like \u201cexpanding rapidly\u201d in Arg-1 and \u201cearned the service a somewhat sleazy image\u201d in Arg-2, since there exists a contrast between the semantic meanings of these two text spans. However, it is difficult to obtain sufficient information for pinpointing these words through scanning the argument pair left to right in one pass. In such case, we follow the repeated reading strategy, where we obtain the general meaning through reading the arguments for the first time, re-read them later and gradually pay close attention to the key content.\nRecently, some approaches simulating repeated reading have witnessed their success in different tasks. These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015). For example, Kumar et al. (2015) drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer. In computation vision, Yang et al. (2015) pointed out that repeatedly giving attention to different regions of an image could gradually lead to more precise image representations.\nInspired by these recent work, for discourse parsing, we propose a model that aims to repeatedly read an argument pair and gradually focus on more fine-grained parts after grasping the global information. Specifically, we design the Neural Networks with Multi-Level Attention (NNMA) consisting of one general level and several attention levels. In the general level, we capture the general representations of each argument based on two bidirectional long short-term memory (LSTM) models. For each attention level, NNMA generates a weight vector over the argument pair to locate the important parts related to the discourse relation. And an external short-term memory is designed to store the information exploited in previous levels and help update the argument representations. We stack this structure in a recurrent manner, mimicking the process of reading the arguments multiple times. Finally, we use the representation output from the highest attention level to identify the discourse\nrelation. Experiments on the PDTB dataset show that our proposed model achieves the state-of-art results."}, {"heading": "2 Repeated Reading Neural Network with Multi-Level Attention", "text": "In this section, we describe how we use the neural networks with multi-level attention to repeatedly read the argument pairs and recognize implicit discourse relations.\nFirst, we get the general understanding of the arguments through skimming them. To implement this, we adopt the bidirectional Long-Short Term Memory Neural Network (bi-LSTM) to model each argument, as bi-LSTM is good at modeling over a sequence of words and can represent each word with consideration of more contextual information. Then, several attention levels are designed to simulate the subsequent multiple passes of reading. On each attention level, an external short-term memory is used to store what has been learned from previous passes and guide which words should be focused on. To pinpoint the useful parts of the arguments, the attention mechanism is used to predict a probability distribution over each word, indicating to what degree each word should be concerned. The overall architecture of our model is shown in Figure 1. For clarity, we only illustrate two attention levels in the figure. It is noted that we can easily extend our model to more attention levels."}, {"heading": "2.1 Representing Arguments with LSTM", "text": "The Long-Short Term Memory (LSTM) Neural Network is a variant of the Recurrent Neural Network which is usually used for modeling a sequence. In our model, we adopt two LSTM neural networks to respectively model the two arguments: the left argument Arg-1 and the right argument Arg2.\nFirst of all, we associate each word w in our vocabulary with a vector representation xw \u2208 RDe . Here we adopt the pre-trained vectors provided by GloVe (Pennington et al., 2014). Since an argument can be viewed as a sequence of word vectors, let x1i (x2i ) be the i-th word vector in argument Arg-1 (Arg-\n2) and the two arguments can be represented as,\nArg-1 : [x11,x 1 2, \u00b7 \u00b7 \u00b7 ,x1L1 ] Arg-2 : [x21,x 2 2, \u00b7 \u00b7 \u00b7 ,x2L2 ]\nwhere Arg-1 has L1 words and Arg-2 has L2 words. To model the two arguments, we briefly introduce the working process how the LSTM neural networks model a sequence of words. For the i-th time step, the model reads the i-th word xi as the input and updates the output vector hi as follows (Zaremba and Sutskever, 2014).\nii = sigmoid(Wi[xi,hi\u22121] + bi) (1)\nfi = sigmoid(Wf [xi,hi\u22121] + bf ) (2)\noi = sigmoid(Wo[xi,hi\u22121] + bo) (3)\nc\u0303i = tanh(Wc[xi,hi\u22121] + bc) (4) ci = ii \u2217 c\u0303i + fi \u2217 ci\u22121 (5) hi = oi \u2217 tanh(ci) (6)\nwhere [ ] means the concatenation operation of several vectors. i,f ,o and c denote the input gate, forget gate, output gate and memory cell\nrespectively in the LSTM architecture. The input gate i determines how much the input xi updates the memory cell. The output gate o controls how much the memory cell influences the output. The forget gate f controls how the past memory ci\u22121 affects the current state. Wi,Wf ,Wo,Wc, bi, bf , bo, bc are the network parameters.\nReferring to the work of Wang and Nyberg (2015), we implement the bidirectional version of LSTM neural network to model the argument sequence. Besides processing the sequence in the forward direction, the bidirectional LSTM (biLSTM) neural network also processes it in the reverse direction. As shown in Figure 1, using two bi-LSTM neural networks, we can obtain h1i = [~h1i , ~h1i ] for the i-th word in Arg-1 and h 2 i = [ ~h2i , ~h2i ] for the i-th word in Arg-2, where ~h1i , ~h 2 i \u2208 Rd and ~h1i , ~h 2 i \u2208 Rd are the output vectors from two directions. Next, to get the general-level representations of the arguments, we apply a mean pooling operation over the bi-LSTM outputs, and obtain two vectors R10 and R 2 0, which can reflect the global information of the argument pair.\nR10 = 1\nL1 L1\u2211 i=0 h1i (7)\nR20 = 1\nL2 L2\u2211 i=0 h2i (8)"}, {"heading": "2.2 Tuning Attention via Repeated Reading", "text": "After obtaining the general-level representations by treating each word equally, we simulate the repeated reading and design multiple attention levels to gradually pinpoint those words particularly useful for discourse relation recognition. In each attention level, we adopt the attention mechanism to determine which words should be focused on. An external short-term memory is designed to remember what has seen in the prior levels and guide the attention tuning process in current level.\nSpecifically, in the first attention level, we concatenate R10, R 2 0 and R 1 0\u2212R20 and apply a non-linear transformation over the concatenation to catch the general understanding of the argument pair. The use of R10\u2212R20 takes a cue from the difference between two vector representations which\nhas been found explainable and meaningful in many applications (Mikolov et al., 2013). Then, we get the memory vector M1 \u2208 Rdm of the first attention level as\nM1 = tanh(Wm,1[R 1 0,R 2 0,R 1 0\u2212R20]) (9)\nwhere Wm,1 \u2208 Rdm\u00d76d is the weight matrix. With M1 recording the general meaning of the argument pair, our model re-calculates the importance of each word. We assign each word a weight measuring to what degree our model should pay attention to it. The weights are so-called \u201cattention\u201d in our paper. This process is designed to simulate the process that we re-read the arguments and pay more attention to some specific words with an overall understanding derived from the first-pass reading. Formally, for Arg-1, we use the memory vector M1 to update the representation of each word with a non-linear transformation. According to the updated word representations o11, we get the attention vector a11.\nh1 = [h10,h 1 1, \u00b7 \u00b7 \u00b7 ,h1L1 ] (10) o11 = tanh(W 1 a,1h\n1 + W 1b,1(M1 \u2297 e)) (11) a11 = softmax(W 1 s,1o 1 1) (12)\nwhere h1 \u2208 R2d\u00d7L1 is the concatenation of all LSTM output vectors of Arg-1. e \u2208 RL1 is a vector of 1s and the M1 \u2297 e operation denotes that we repeat the vector M1 L1 times and generate a dm \u00d7 L1 matrix. The attention vector a11 \u2208 RL1 is obtained through applying a softmax operation over o11. Wa,1\n1 \u2208 R2d\u00d72d,Wb,11 \u2208 R2d\u00d7dm and Ws,1\n1 \u2208 R1\u00d72d are the transformation weights. It is noted that the subscripts denote the current attention level and the superscripts denote the corresponding argument. In the same way, we can get the attention vector a21 for Arg-2.\nThen, according to a11 and a 2 1, our model re-reads the arguments and get the new representations R11 and R21 for the first attention level.\nR11 = h 1(a11) T (13) R21 = h 2(a21) T (14)\nNext, we iterate the \u201cmemory-attentionrepresentation\u201d process and design more attention\nlevels, giving NNMA the ability to gradually infer more precise attention vectors. The processing of the second or above attention levels is slightly different from that of the first level, as we update the memory vector in a recurrent way. To formalize, for the k-th attention level (k \u2265 2), we use the following formulae for Arg-1.\nMk = tanh(Wm,k[R 1 k\u22121,R 2 k\u22121,R 1 k\u22121\u2212R 2 k\u22121,Mk\u22121])\n(15)\no1k = tanh(W 1 a,kh 1 + W 1b,k(Mk \u2297 e)) (16) a1k = softmax(W 1 s,ko 1 k) (17)\nR1k = h 1(a1k) T (18)\nIn the same way, we can computer o2k,a 2 k and R 2 k for Arg-2. Finally, we use the newest representation derived from the top attention level to recognize the discourse relations. Suppose there are totally K attention levels and n relation types, the predicted discourse relation distribution P \u2208 Rn is calculated as\nP = softmax(Wp[R 1 K ,R 2 K ,R 1 K\u2212R2K ] + bp)\n(19)\nwhere Wp \u2208 Rn\u00d76d and bp \u2208 Rn are the transformation weights."}, {"heading": "2.3 Model Training", "text": "To train our model, the training objective is defined as the cross-entropy loss between the outputs of the softmax layer and the ground-truth class labels. We use stochastic gradient descent (SGD) with momentum to train the neural networks.\nTo avoid over-fitting, dropout operation is applied on the top feature vector before the softmax layer. Also, we use different learning rates \u03bb and \u03bbe to train the neural network parameters \u0398 and the word embeddings \u0398e referring to (Ji and Eisenstein, 2015). \u03bbe is set to a small value for preventing overfitting on this task. In the experimental part, we will introduce the setting of the hyper-parameters."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Preparation", "text": "We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). In our work,\nwe experiment on the four top-level classes in this corpus as in previous work (Rutherford and Xue, 2015). We extract all the implicit relations of PDTB, and follow the setup of (Rutherford and Xue, 2015). We split the data into a training set (Sections 2- 20), development set (Sections 0-1), and test set (Section 21-22). Table 1 summarizes the statistics of the four PDTB discourse relations, i.e., Comparison, Contingency, Expansion and Temporal.\nWe first convert the tokens in PDTB to lowercase. The word embeddings used for initializing the word representations are provided by GloVe (Pennington et al., 2014), and the dimension of the embeddings De is 50. The hyper-parameters, including the momentum \u03b4, the two learning rates \u03bb and \u03bbe, the dropout rate q, the dimension of LSTM output vector d, the dimension of memory vector dm are all set according to the performance on the development set Due to space limitation, we do not present the details of tuning the hyper-parameters and only give their final settings as shown in Table 2.\nTo evaluate our model, we adopt two kinds of experiment settings. The first one is the fourway classification task, and the second one is the binary classification task, where we build a onevs-other classifier for each class. For the second setting, to solve the problem of unbalanced classes in the training data, we follow the reweighting method of (Rutherford and Xue, 2015) to reweigh the training instances according to the size of each relation class. We also use visualization methods to analyze how multi-level attention helps our model."}, {"heading": "3.2 Results", "text": "First, we design experiments to evaluate the effectiveness of attention levels and how many attention levels are appropriate. To this end, we implement a baseline model (LSTM with no attention) which directly applies the mean pooling operation over LSTM output vectors of two arguments without any attention mechanism. Then we consider different attention levels including one-level, twolevel and three-level. The detailed results are shown in Table 3. For four-way classification, macroaveraged F1 and Accuracy are used as evaluation metrics. For binary classification, F1 is adopted to evaluate the performance on each class.\nFrom Table 3, we can see that the basic LSTM model performs the worst. With attention levels added, our NNMA model performs much better. This confirms the observation above that one-pass reading is not enough for identifying the discourse relations. With respect to the four-way F1 measure, using NNMA with one-level attention produces a 4% improvement over the baseline system with no attention. Adding the second attention level gives another 2.8% improvement. We perform significance test for these two improvements, and they are both significant under one-tailed t-test (p < 0.05). However, when adding the third attention level, the performance does not promote much and almost reaches its plateau. We can see that threelevel NNMA experiences a decease in F1 and a slight increase in Accuracy compared to two-level NNMA. The results imply that with more attention levels considered, our model may perform slightly better, but it may incur the over-fitting problem due to adding more parameters. With respect to the binary classification F1 measures, we can see\nthat the \u201cComparison\u201d relation needs more passes of reading compared to the other three relations. The reason may be that the identification of the \u201cComparison\u201d depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010).\nNext, we compare our models with six state-ofthe-art baseline approaches, as shown in Table 4. The six baselines are introduced as follows.\n\u2022 P&C2012: Park and Cardie (2012) designed a feature-based method and promoted the performance through optimizing the feature set. \u2022 J&E2015: Ji and Eisenstein (2015) used two\nrecursive neural networks on the syntactic parse tree to induce the representation of the arguments and the entity spans. \u2022 Zhang2015: Zhang et al. (2015) proposed\nto use shallow convolutional neural networks to model two arguments respectively. We replicated their model since they used a different setting in preprocessing PDTB. \u2022 R&X2014, R&X2015: Rutherford and Xue\n(2014) selected lexical features, production rules, and Brown cluster pairs, and fed them into a maximum entropy classifier. Rutherford and Xue (2015) further proposed to gather extra weakly labeled data based on the discourse connectives for the classifier. \u2022 B&D2015: Braud and Denis (2015) combined\nseveral hand-crafted lexical features and word embeddings to train a max-entropy classifier.\n\u2022 Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network.\n\u2022 Ji2016: Ji et al. (2016) proposed a neural language model over sequences of words and used the discourse relations as latent variables to connect the adjacent sequences.\nIt is noted that P&C2012 and J&E2015 merged the \u201cEntRel\u201d relation into the \u201cExpansion\u201d relation1. For a comprehensive comparison, we also experiment our model by adding a Expa.+EntRel vs Other classification. Our NNMA model with two attention levels exhibits obvious advantages over the six baseline methods on the whole. It is worth noting that NNMA is even better than the R&X2015 approach which employs extra data.\nAs for the performance on each discourse relation, with respect to the F1 measure, we can see that our NNMA model can achieve the best results on the \u201cExpansion\u201d, \u201cExpansion+EntRel\u201d and \u201cTemporal\u201d relations and competitive results on the \u201cContingency\u201d relation . The performance of recognizing the \u201cComparison\u201d relation is only worse than R&X2014 and R&X2015. As (Rutherford and Xue, 2014) stated, the \u201cComparison\u201d relation is closely related to the constituent parse feature of the text, like production rules. How to represent and\n1EntRel is the entity-based coherence relation which is independent of implicit and explicit relations in PDTB. However some research merges it into the implicit Expansion relation.\nArg-1a\nth e w or ld ps\nyc hi\nat ric\nas so\nci at\nio n vo te d at an at he ns pa rle y to co nd iti on al ly re ad m it th e so vi et un io n\n1 2 3 Attention Level\nArg-2a\nm os\nco w\nco ul d be su\nsp en\nde d\nif th e m isu se of ps yc hi\nat ry\nag ai\nns t\ndi ss\nen te rs is di sc ov er ed du rin g a re vi ew w ith in a ye ar\n1 2 3 Attention Level\n(a) Example with Comparison relation\nArg-1b\nbu t ib m w ou ld ha ve w on th e bu si\nne ss\nan yw ay as a sa le to a th ird pa rt y th at w ou ld ha ve th en le as ed th e eq ui\npm en t to th e cu st om er\n1 2 3 Attention Level\nArg-2b\nib m ha s no t on ly hu rt its sh or\ntte rm re ve nu e ou tlo ok bu t ha s al so be en lo sin g m on ey on its le as es\n1 2 3 Attention Level\n(b) Example with Contingency relation\nexploit these information in our model will be our next research focus."}, {"heading": "3.3 Analysis of Attention Levels", "text": "The multiple attention levels in our model greatly boost the performance of classifying implicit discourse relations. In this subsection, we perform both qualitative and quantitative analysis on the attention levels.\nFirst, we take a three-level NNMA model for example and analyze its attention distributions on different attention levels by calculating the mean Kullback-Leibler (KL) Divergence between any two levels on the training set. In Figure 3, we use klij to denote the KL Divergence between the ith and the jthattention level and use klui to denote the KL Divergence between the uniform distribution and the ith attention level. We can see that each attention level forms different attention distributions and the difference increases in the higher levels. It can be inferred that the 2nd and 3rd levels in NNMA gradually neglect some words and pay more attention to some other words in the arguments. One point worth mentioning is that Arg-2 tends to have more non-uniform attention weights, since klu2 and klu3 of Arg-2 are much larger than those of Arg1. And also, the changes between attention levels\nare more obvious for Arg-2 through observing the values of kl12, kl13 and kl23. The reason may be that Arg-2 contains more information related with discourse relation and some words in it tend to require focused attention, as Arg-2 is syntactically bound to the implicit connective.\nArg-1 Arg-2 kl_12 0.00749 0.04177 kl_23 0.099784 0.502146 kl_13 0.085394 0.35212 kl_u1 0.123413 0.136228 kl_u2 0.156619 0.254844 kl_u3 0.162379 0.467976\nAt the same time, we visualize the attention levels of some example argument pairs which are analyzed by the three-level NNMA. To illustrate the kth attention level, we get its attention weights a1k and a2k which reflect the contribution of each word and then depict them by a row of color-shaded grids in Figure 2.\nWe can see that the NNMA model focuses on different words on different attention levels. Interestingly, from Figure 2, we find that the 1st and 3rd attention levels focus on some similar words, while the 2nd level is relatively different from them. It seems that NNMA tries to find some clues (e.g. \u201cmoscow could be suspended\u201d in Arg-2a; \u201cwon the business\u201d in Arg-1b; \u201cwith great aplomb he considers not only\u201d in Arg-2c) for recognizing the discourse relation on the 1st level, looking closely at other words (e.g. \u201cmisuse of psychiatry against dissenters\u201d in Arg-2a; \u201ca third party that\u201d in Arg-1b; \u201cand support of hitler\u201d in Arg-2c) on the 2nd level, and then reconsider the arguments, focus on some specific words (e.g. \u201cmoscow could be suspended\u201d in Arg-2a; \u201chas not only hurt\u201d in Arg-2b) and make the final decision on the last level."}, {"heading": "4 Related Work", "text": ""}, {"heading": "4.1 Implicit Discourse Relation Classification", "text": "The Penn Discourse Treebank (PDTB) (Prasad et al., 2008), known as the largest discourse corpus, is composed of 2159 Wall Street Journal articles. Each document is annotated with the predicate-argument structure, where the predicate is the discourse connective (e.g. while) and the arguments are two text spans around the connective. The discourse connective can be either explicit or implicit. In PDTB, a hierarchy of relation tags is provided for annotation. In our study, we use the four top-level tags, including Temporal, Contingency, Comparison and Expansion. These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012).\nImplicit discourse relation recognition is often treated as a classification problem. The first work to tackle this task on PDTB is (Pitler et al., 2009). They selected several surface features to train four binary classifiers, each for one of the top-level PDTB relation classes. Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015)\nadopted two recursive neural networks to exploit the representation of arguments and entity spans. Very recently, Liu et al. (2016) proposed a twodimensional convolutional neural network (CNN) to model the argument pairs and employed a multitask learning framework to boost the performance by learning from other discourse-related tasks. Ji et al. (2016) considered discourse relations as latent variables connecting two token sequences and trained a discourse informed language model."}, {"heading": "4.2 Neural Networks and Attention Mechanism", "text": "Recently, neural network-based methods have gained prominence in the field of natural language processing (Kim, 2014). Such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Collobert et al., 2011).\nAttention mechanism was first introduced into neural models to solve the alignment problem between different modalities. Graves (2013) designed a neural network to generate handwriting based on a text. It assigned a window on the input text at each step and generate characters based on the content within the window. Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribution over the input sequence when generating each target word. Tan et al. (2015) proposed an attentionbased neural network to model both questions and sentences for selecting the appropriate non-factoid answers.\nIn parallel, the idea of equipping the neural model with an external memory has gained increasing attention recently. A memory can remember what the model has learned and guide its subsequent actions. Weston et al. (2015) presented a neural network to read and update the external memory in a recurrent manner with the guidance of a question embedding. Kumar et al. (2015) proposed a similar model where a memory was designed to change the gate of the gated recurrent unit for each iteration."}, {"heading": "5 Conclusion", "text": "As a complex text processing task, implicit discourse relation recognition needs a deep analysis\nof the arguments. To this end, we for the first time propose to imitate the repeated reading strategy and dynamically exploit efficient features through several passes of reading. Following this idea, we design neural networks with multiple levels of attention (NNMA), where the general level and the attention levels represent the first and subsequent passes of reading. With the help of external short-term memories, NNMA can gradually update the arguments representations on each attention level and fix attention on some specific words which provide effective clues to discourse relation recognition. We conducted experiments on PDTB and the evaluation results show that our model can achieve the state-of-the-art performance on recognizing the implicit discourse relations.\nAcknowledgments\nWe thank all the anonymous reviewers for their insightful comments on this paper. This work was partially supported by National Key Basic Research Program of China (2014CB340504), and National Natural Science Foundation of China (61273278 and 61572049). The correspondence author of this paper is Sujian Li."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Probabilistic head-driven parsing for discourse structure", "author": ["Baldridge", "Lascarides2005] Jason Baldridge", "Alex Lascarides"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Baldridge et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baldridge et al\\.", "year": 2005}, {"title": "Comparing word representations for implicit discourse relation classification", "author": ["Braud", "Denis2015] Chlo\u00e9 Braud", "Pascal Denis"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Braud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braud et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations. Transactions of the Association for Computational Linguistics, 3:329\u2013344", "author": ["Ji", "Eisenstein2015] Yangfeng Ji", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "A latent variable recurrent neural network for discourse relation language models. arXiv preprint arXiv:1603.01913", "author": ["Ji et al.2016] Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "A novel discriminative framework for sentence-level discourse analysis", "author": ["Joty et al.2012] Shafiq Joty", "Giuseppe Carenini", "Raymond T Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Joty et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joty et al\\.", "year": 2012}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Recognizing implicit discourse relations in the penn discourse treebank", "author": ["Lin et al.2009] Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Implicit discourse relation classification via multi-task neural network", "author": ["Liu et al.2016] Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui"], "venue": "In Proceedings of AAAI", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving implicit discourse relation recognition through feature set optimization", "author": ["Park", "Cardie2012] Joonsuk Park", "Claire Cardie"], "venue": "In Proceedings of SigDial", "citeRegEx": "Park et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Park et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Automatic sense prediction for implicit discourse relations in text", "author": ["Pitler et al.2009] Emily Pitler", "Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of ACL", "citeRegEx": "Pitler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2009}, {"title": "The Penn Discourse TreeBank", "author": ["Prasad et al.2008] Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K. Joshi", "Bonnie L. Webber"], "venue": "Proceedings of LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Discovering implicit discourse relations through brown cluster pair representation and coreference patterns", "author": ["Rutherford", "Xue2014] Attapol Rutherford", "Nianwen Xue"], "venue": "In Proceedings of EACL", "citeRegEx": "Rutherford et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rutherford et al\\.", "year": 2014}, {"title": "Improving the inference of implicit discourse relations via classifying explicit discourse connectives", "author": ["Rutherford", "Xue2015] Attapol T Rutherford", "Nianwen Xue"], "venue": "In Proceedings of NAACL", "citeRegEx": "Rutherford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rutherford et al\\.", "year": 2015}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["Soricut", "Marcu2003] Radu Soricut", "Daniel Marcu"], "venue": "In Proceedings of NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2003}, {"title": "An effective discourse parser that uses rich linguistic information", "author": ["Subba", "Di Eugenio2009] Rajen Subba", "Barbara Di Eugenio"], "venue": "In Proceedings of NAACL", "citeRegEx": "Subba et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Subba et al\\.", "year": 2009}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Proceedings of NIPS", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Lstm-based deep learning models for non-factoid answer selection. arXiv preprint arXiv:1511.04108", "author": ["Tan et al.2015] Ming Tan", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Nyberg2015] Di Wang", "Eric Nyberg"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Implicit Discourse Relation Recognition by Selecting Typical Training Examples", "author": ["Wang et al.2012] Xun Wang", "Sujian Li", "Jiwei Li", "Wenjie Li"], "venue": "In Proceedings of COLING", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "arXiv preprint arXiv:1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Shallow convolutional neural network for implicit discourse relation recognition", "author": ["Zhang et al.2015] Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao"], "venue": "Proceedings of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Predicting discourse connectives for implicit discourse relation recognition", "author": ["Zhou et al.2010] Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan"], "venue": "In Proceedings of the ICCL,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012).", "startOffset": 153, "endOffset": 301}, {"referenceID": 7, "context": "Previous research mainly focus on exploring various kinds of efficient features and machine learning models to classify the implicit discourse relations (Soricut and Marcu, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Pitler et al., 2009; Joty et al., 2012).", "startOffset": 153, "endOffset": 301}, {"referenceID": 0, "context": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al.", "startOffset": 140, "endOffset": 163}, {"referenceID": 21, "context": ", 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015).", "startOffset": 90, "endOffset": 115}, {"referenceID": 0, "context": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015). For example, Kumar et al. (2015) drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer.", "startOffset": 141, "endOffset": 306}, {"referenceID": 0, "context": "These models mostly combine the attention mechanism that has been originally designed to solve the alignment problem in machine translation (Bahdanau et al., 2014) and the external memory which can be read and written when processing the objects (Sukhbaatar et al., 2015). For example, Kumar et al. (2015) drew attention to specific facts of the input sequence and processed the sequence via multiple hops to generate an answer. In computation vision, Yang et al. (2015) pointed out that repeatedly giving attention to different regions of an image could gradually lead to more precise image representations.", "startOffset": 141, "endOffset": 471}, {"referenceID": 14, "context": "Here we adopt the pre-trained vectors provided by GloVe (Pennington et al., 2014).", "startOffset": 56, "endOffset": 81}, {"referenceID": 12, "context": "has been found explainable and meaningful in many applications (Mikolov et al., 2013).", "startOffset": 63, "endOffset": 85}, {"referenceID": 16, "context": "We evaluate our model on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008).", "startOffset": 60, "endOffset": 81}, {"referenceID": 14, "context": "The word embeddings used for initializing the word representations are provided by GloVe (Pennington et al., 2014), and the dimension of the embeddings De is 50.", "startOffset": 89, "endOffset": 114}, {"referenceID": 28, "context": "The reason may be that the identification of the \u201cComparison\u201d depends more on some deep analysis such as semantic parsing, according to (Zhou et al., 2010).", "startOffset": 136, "endOffset": 155}, {"referenceID": 27, "context": "\u2022 Zhang2015: Zhang et al. (2015) proposed to use shallow convolutional neural networks to model two arguments respectively.", "startOffset": 13, "endOffset": 33}, {"referenceID": 11, "context": "\u2022 Liu2016: Liu et al. (2016) proposed to better classify the discourse relations by learning from other discourse-related tasks with a multitask neural network.", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "\u2022 Ji2016: Ji et al. (2016) proposed a neural language model over sequences of words and used the discourse relations as latent variables to connect the adjacent sequences.", "startOffset": 10, "endOffset": 27}, {"referenceID": 16, "context": "The Penn Discourse Treebank (PDTB) (Prasad et al., 2008), known as the largest discourse corpus, is composed of 2159 Wall Street Journal articles.", "startOffset": 35, "endOffset": 56}, {"referenceID": 24, "context": "These four core relations allow us to be theory-neutral, since they are almost included in all discourse theories, sometimes with different names (Wang et al., 2012).", "startOffset": 146, "endOffset": 165}, {"referenceID": 15, "context": "The first work to tackle this task on PDTB is (Pitler et al., 2009).", "startOffset": 46, "endOffset": 67}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively.", "startOffset": 26, "endOffset": 44}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem.", "startOffset": 26, "endOffset": 231}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans.", "startOffset": 26, "endOffset": 343}, {"referenceID": 8, "context": "Extending from this work, Lin et al. (2009) further identified four different feature types representing the context, the constituent parse trees, the dependency parse trees and the raw text respectively. Rutherford and Xue (2014) used brown cluster to replace the word pair features for solving the sparsity problem. Ji and Eisenstein (2015) adopted two recursive neural networks to exploit the representation of arguments and entity spans. Very recently, Liu et al. (2016) proposed a twodimensional convolutional neural network (CNN) to model the argument pairs and employed a multitask learning framework to boost the performance by learning from other discourse-related tasks.", "startOffset": 26, "endOffset": 475}, {"referenceID": 5, "context": "Ji et al. (2016) considered discourse relations as latent variables connecting two token sequences and trained a discourse informed language model.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Recently, neural network-based methods have gained prominence in the field of natural language processing (Kim, 2014).", "startOffset": 106, "endOffset": 117}, {"referenceID": 3, "context": "Such methods are primarily based on learning a distributed representation for each word, which is also called a word embedding (Collobert et al., 2011).", "startOffset": 127, "endOffset": 151}, {"referenceID": 3, "context": "Graves (2013) designed a neural network to generate handwriting based on a text.", "startOffset": 0, "endOffset": 14}, {"referenceID": 0, "context": "Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribution over the input sequence when generating each target word.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. (2014) introduced this idea into machine translation, where their model computed a probabilistic distribution over the input sequence when generating each target word. Tan et al. (2015) proposed an attentionbased neural network to model both questions and sentences for selecting the appropriate non-factoid answers.", "startOffset": 0, "endOffset": 202}, {"referenceID": 9, "context": "Kumar et al. (2015) proposed a similar model where a memory was designed to change the gate of the gated recurrent unit for each iteration.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-ofart results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words.", "creator": "LaTeX with hyperref package"}}}