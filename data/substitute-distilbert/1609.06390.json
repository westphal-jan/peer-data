{"id": "1609.06390", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices", "abstract": "recently, there has been constant surge of researchers in using sophisticated methods thus estimating latent variable models. however, work is usually assumed that the distribution about the observations conditioned on the latent variables is either discrete either belongs to a parametric family. in this context, we study the estimation of an $ m $ - \u00bd hidden markov model ( hmm ) with only smoothness assumptions, called 0 h \\ \" olderian conditions, on the emission densities. by distinguishing some recent advances promoting continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric hmms. our view is based on measuring an svd on nonparametric estimates of density functions by viewing them as \\ lambda { continuous matrices }. i derive sample complexity bounds via concentration limitations for nonparametric density estimation and novel perturbation theory results for continuous matrices. we implement our method using chebyshev polynomial approximations. our method is effective with other baselines relating synthetic and physical problems and is also very computationally efficient.", "histories": [["v1", "Wed, 21 Sep 2016 00:15:44 GMT  (2628kb,D)", "http://arxiv.org/abs/1609.06390v1", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["kirthevasan kandasamy", "maruan al-shedivat", "eric p xing"], "accepted": true, "id": "1609.06390"}, "pdf": {"name": "1609.06390.pdf", "metadata": {"source": "CRF", "title": "Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices", "authors": ["Kirthevasan Kandasamy", "Maruan Al-Shedivat", "Eric P. Xing"], "emails": ["kandasamy@cs.cmu.edu", "alshedivat@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology. In an HMM, a discrete hidden state undergoes Markovian transitions from one of m possible states to another at each time step. If the hidden state at time t is ht, we observe a random variable xt \u2208 X drawn from an emission distribution, Oj = P(xt|ht = j). In its most basic form X is a discrete set and Oj are discrete distributions. When dealing with continuous observations, it is conventional to assume that the emissions Oj belong to a parametric class of distributions, such as Gaussian.\nRecently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134]. At a high level, these methods estimate higher order moments from the data and recover the parameters via a series of matrix operations such as singular value decompositions, matrix multiplications and pseudo-inverses of the moments. In the case of discrete HMMs [2], these moments correspond exactly to the joint probabilities of the observations in the sequence.\nAssuming parametric forms for the emission densities is often too restrictive since real world distributions can be arbitrary. Parametric models may introduce incongruous biases that cannot be reduced even with large datasets. To address this problem, we study nonparametric HMMs only assuming some mild smoothness conditions on the emission densities. We design a spectral algorithm for this setting. Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices. Chebyshev polynomial approximations enable efficient computation of algebraic operations on these continuous objects [7, 8]. Using these ideas, we extend existing spectral methods for discrete HMMs to the continuous nonparametric setting. Our main contributions are:\n\u2217Joint lead authors.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 9.\n06 39\n0v 1\n[ st\nat .M\nL ]\n2 1\nSe p\n20 16\n1. We derive a spectral learning algorithm for HMMs with nonparametric emission densities. While the algorithm is similar to previous spectral methods for estimating models with a finite number of parameters, many of the ideas used to generalise it to the nonparametric setting are novel, and, to the best of our knowledge, have not been used before in the machine learning literature.\n2. We establish sample complexity bounds for our method. For this, we derive concentration results for nonparametric density estimation and novel perturbation theory results for the aforementioned continuous matrices. The perturbation results are new and might be of independent interest.\n3. We implement our algorithm by approximating the density estimates via Chebyshev polynomials which enables efficient computation of many of the continuous matrix operations. Our method outperforms natural competitors in this setting on synthetic and real data and is computationally more efficient than most of them. Our Matlab code is available at github.com/alshedivat/nphmm.\nWhile we focus on HMMs in this exposition, we believe that the ideas presented in this paper can be easily generalised to estimating other latent variable models and predictive state representations [9] with nonparametric observations using approaches developed by Anandkumar et al. [3].\nRelated Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11]. However, EM is a local search technique, and optimization of the likelihood may be difficult. Hence, recent work on spectral methods has gained appeal. Our work builds on Hsu et al. [2] who showed that discrete HMMs can be learned efficiently, under certain conditions. The key idea is that any HMM can be completely characterised in terms of quantities that depend entirely on the observations, called the observable representation, which can be estimated from data. Siddiqi et al. [4] show that the same algorithm works under slightly more general assumptions. Anandkumar et al. [3] proposed a spectral algorithm for estimating more general latent variable models with parametric observations via a moment matching technique.\nThat said, there has been little work on estimating latent variable models, including HMMs, when the observations are nonparametric. A commonly used heuristic is the nonparametric EM [12] which lacks theoretical underpinnings. This should not be surprising because EM is a maximum likelihood procedure and, for most nonparametric problems, the maximum likelihood estimate is degenerate [13]. In their work, Siddiqi et al. [4] proposed a heuristic based on kernel smoothing, with no theoretical justification, to modify the discrete algorithm for continuous observations. Further, their procedure cannot be used to recover the joint or conditional probabilities of a sequence which would be needed to compute probabilities of events and other inference tasks.\nSong et al. [14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM. While they provide theoretical guarantees, their bounds are in terms of the RKHS distance of the true and estimated embeddings. This metric depends on the choice of the kernel and it is not clear how it translates to a suitable distance measure on the observation space such as an L1 or L2 distance. While their method can be used for prediction and pairwise testing, it cannot recover the joint and conditional densities. On the contrary, our model provides guarantees in terms of the more interpretable total variation distance and is able to recover the joint and conditional probabilities."}, {"heading": "2 A Pint-sized Review of Continuous Linear Algebra", "text": "We begin with a pint-sized review on continuous linear algebra which treats functions as continuous analogues of matrices. Appendix A contains a quart-sized review. Both sections are based on [5, 6]. While these objects can be viewed as operators on Hilbert spaces which have been studied extensively in the years, the above line of work simplified and specialised the ideas to functions.\nA matrix F \u2208 Rm\u00d7n is anm\u00d7n array of numbers where F (i, j) denotes the entry in row i, column j. m or n could be (countably) infinite. A column qmatrix (quasi-matrix) Q \u2208 R[a,b]\u00d7m is a collection of m functions defined on [a, b] where the row index is continuous and column index is discrete. WritingQ = [q1, . . . , qm] where qj : [a, b]\u2192 R is the j th function,Q(y, j) = qj(y) denotes the value of the j th function at y \u2208 [a, b]. Q> \u2208 Rm\u00d7[a,b] denotes a row qmatrix with Q>(j, y) = Q(y, j). A cmatrix (continuous-matrix) C \u2208 R[a,b]\u00d7[c,d] is a two dimensional function where both row and column indices are continuous and C(y, x) is the value of the function at (y, x) \u2208 [a, b] \u00d7 [c, d]. C> \u2208 R[c,d]\u00d7[a,b] denotes its transpose with C>(x, y) = C(y, x). Qmatrices and cmatrices permit all matrix multiplications with suitably defined inner products. For example, if R \u2208 R[c,d]\u00d7m and C \u2208 R[a,b]\u00d7[c,d], then CR = T \u2208 R[a,b]\u00d7m where T (y, j) = \u222b d c C(y, s)R(s, j)ds.\nA cmatrix has a singular value decomposition (SVD). If C \u2208 R[a,b]\u00d7[c,d], it decomposes as an infinite sum, C(y, x) = \u2211\u221e j=1 \u03c3juj(y)vj(x), that converges in L\n2. Here \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 are the singular values of C. {uj}j\u22651 and {vj}j\u22651 are functions that form orthonormal bases for L2([a, b]) and L2([c, d]), respectively. We can write the SVD as C = U\u03a3V > by writing the singular vectors as infinite qmatrices U = [u1, u2 . . . ], V = [v1, v2 . . . ], and \u03a3 = diag(\u03c31, \u03c32 . . . ). If only m < \u221e first singular values are nonzero, we say that C is of rank m. The SVD of a qmatrix Q \u2208 R[a,b]\u00d7m is, Q = U\u03a3V > where U \u2208 R[a,b]\u00d7m and V \u2208 Rm\u00d7m have orthonormal columns and \u03a3 = diag(\u03c31, . . . , \u03c3m) with \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3m \u2265 0. The rank of a column qmatrix is the number of linearly independent columns (i.e. functions) and is equal to the number of nonzero singular values. Finally, as for the finite matrices, the pseudo inverse of the cmatrix C is C\u2020 = V \u03a3\u22121U> with \u03a3\u22121 = diag(1/\u03c31, 1/\u03c32, . . . ). The pseudo inverse of a qmatrix is defined similarly."}, {"heading": "3 Nonparametric HMMs and the Observable Representation", "text": "Notation: Throughout this manuscript, we will use P to denote probabilities of events while p will denote probability density functions (pdf). An HMM characterises a probability distribution over a sequence of hidden states {ht}t\u22650 and observations {xt}t\u22650. At a given time step, the HMM can be in one of m hidden states, i.e. ht \u2208 [m] = {1, . . . ,m}, and the observation is in some bounded continuous domain X . Without loss of generality, we take2 X = [0, 1]. The nonparametric HMM will be completely characterised by the initial state distribution \u03c0 \u2208 Rm, the state transition matrix T \u2208 Rm\u00d7m and the emission densities Oj : X \u2192 R, j \u2208 [m]. \u03c0i = P(h1 = i) is the probability that the HMM would be in state i at the first time step. The element T (i, j) = P(ht+1 = i|ht = j) of T gives the probability that a hidden state transitions from state j to state i. The emission function, Oj : X \u2192 R+, describes the pdf of the observation conditioned on the hidden state j, i.e. Oj(s) = p(xt = s|ht = j). Note that we have Oj(x) > 0, \u2200x and \u222b Oj(\u00b7) = 1 for all j \u2208 [m]. In this exposition, we denote the emission densities by the qmatrix, O = [O1, . . . , Om] \u2208 R[0,1]\u00d7m+ .\nIn addition, let O\u0303(x) = diag(O1(x), . . . , Om(x)), and A(x) = TO\u0303(x). Let x1:t = {x1, . . . , xt} be an ordered sequence and xt:1 = {xt, . . . , x1} denote its reverse. For brevity, we will overload notation for A for sequences and write A(xt:1) = A(xt)A(xt\u22121) . . . A(x1). It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1>mA(xt:1)\u03c0.\nKey structural assumption: Previous work on estimating HMMs with continuous observations typically assumed that the emissions, Oj , take a parametric form, e.g. Gaussian. Unlike them, we only make mild nonparametric smoothness assumptions on Oj . As we will see, to estimate the HMM well in this problem we will need to estimate entire pdfs well. For this reason, the nonparametric setting is significantly more difficult than its parametric counterpart as the latter requires estimating only a finite number of parameters. When compared to the previous literature, this is the crucial distinction and the main challenge in this work.\nObservable Representation: The observable representation is a description of an HMM in terms of quantities that depend on the observations [16]. This representation is useful for two reasons: (i) it depends only on the observations and can be directly estimated from the data; (ii) it can be used to compute joint and conditional probabilities of sequences even without the knowledge of T and O and therefore can be used for inference and prediction. First, we define the joint densities, P1, P21, P321:\nP1(t) = p(x1 = t), P21(s, t) = p(x2 = s, x1 = t), P321(r, s, t) = p(x3 = r, x2 = s, x1 = t),\nwhere xi, i = 1, 2, 3 denotes the observation at time i. Denote P3x1(r, t) = P321(r, x, t) for all x. We will find it useful to view both P21, P3x1 \u2208 R[0,1]\u00d7[0,1] as cmatrices. We will also need an additional qmatrix U \u2208 R[0,1]\u00d7m such that U>O \u2208 Rm\u00d7m is invertible. Given one such U , the observable representation of an HMM is described by the parameters b1, b\u221e \u2208 Rm and B : [0, 1]\u2192 Rm\u00d7m,\nb1 = U >P1, b\u221e = (P > 21U) \u2020P1, B(x) = (U >P3x1)(U >P21) \u2020 (1)\nAs before, for a sequence, xt:1 = {xt, . . . , x1}, we define B(xt:1) = B(xt)B(xt\u22121) . . . B(x1). The following lemma shows that the first m left singular vectors of P21 are a natural choice for U . Lemma 1. Let \u03c0 > 0, T and O be of rank m and U be the qmatrix composed of the first m left singular vectors of P21. Then U>O is invertible.\n2 We discuss the case of higher dimensions in Section 7.\nTo compute the joint and conditional probabilities using the observable representation, we maintain an internal state, bt, which is updated as we see more observations. The internal state at time t is\nbt = B(xt\u22121:1)b1 b>\u221eB(xt\u22121:1)b1 . (2)\nThis definition of bt is consistent with b1. The following lemma establishes the relationship between the observable representation and the internal states to the HMM parameters and probabilities.\nLemma 2 (Properties of the Observable Representation). Let rank(T ) = rank(O) = m and U>O be invertible. Let p(x1:t) denote the joint density of a sequence x1:t and p(xt+1:t+t\u2032 |x1:t) denote the conditional density of xt+1:t+t\u2032 given x1:t in a sequence x1:t+t\u2032 . Then the following are true.\n1. b1 = U>O\u03c0 2. b\u221e = 1>m(U >O)\u22121 3. B(x) = (U>O)A(x)(U>O)\u22121 \u2200x \u2208 [0, 1].\n4. bt+1 = B(xt)bt/(b>\u221eB(xt)bt). 5. p(x1:t) = b>\u221eB(xt:1)b1. 6. p(xt+t\u2032:t+1|x1:t) = b>\u221eB(xt+t\u2032:t+1)bt.\nThe last two claims of the Lemma 2 show that we can use the observable representation for computing the joint and conditional densities. The proofs of Lemmas 1 and 2 are similar to the discrete case and mimic Lemmas 2, 3 & 4 of Hsu et al. [2]."}, {"heading": "4 Spectral Learning of HMMs with Nonparametric Emissions", "text": "The high level idea of our algorithm, NP-HMM-SPEC, is as follows. First we will obtain density estimates for P1, P21, P321 which will then be used to recover the observable representation b1, b\u221e, B by plugging in the expressions in (1). Lemma 2 then gives us a way to estimate the joint and conditional probability densities. For now, we will assume that we have N i.i.d sequences of triples {X(j)}Nj=1 where X(j) = (X (j) 1 , X (j) 2 , X (j) 3 ) are the observations at the first three time steps. We describe learning from longer sequences in Section 4.3."}, {"heading": "4.1 Kernel Density Estimation", "text": "The first step is the estimation of the joint probabilities which requires a nonparametric density estimate. While there are several techniques [17], we use kernel density estimation (KDE) since it is easy to analyse and works well in practice. The KDE for P1, P21, and P321 take the form:\nP\u03021(t) = 1\nN N\u2211 j=1 1 h1 K ( t\u2212X(j)1 h1 ) , P\u030221(s, t) = 1 N N\u2211 j=1 1 h221 K ( s\u2212X(j)2 h21 ) K ( t\u2212X(j)1 h21 ) ,\nP\u0302321(r, s, t) = 1\nN N\u2211 j=1 1 h3321 K ( r \u2212X(j)3 h321 ) K ( s\u2212X(j)2 h321 ) K ( t\u2212X(j)1 h321 ) . (3)\nHere K : [0, 1] \u2192 R is a symmetric function called a smoothing kernel and satisfies (at the very least) \u222b 1 0 K(s)ds = 1, \u222b 1 0 sK(s)ds = 0. The parameters h1, h21, h321 are the bandwidths, and are typically decreasing with N . In practice they are usually chosen via cross-validation."}, {"heading": "4.2 The Spectral Algorithm", "text": "Algorithm 1 NP-HMM-SPEC Input: Data {X(j) = (X(j)1 , X (j) 2 , X (j) 3 )}Nj=1, number of states m.\n\u2022 Obtain estimates P\u03021, P\u030221, P\u0302321 for P1, P21, P321 via kernel density estimation (3). \u2022 Compute the cmatrix SVD of P\u030221. Let U\u0302 \u2208 R[0,1]\u00d7m be the first m left singular vectors of P\u030221. \u2022 Compute the parameters observable representation. Note that B\u0302 is a Rm\u00d7m valued function.\nb\u03021 = U\u0302 >P\u03021, b\u0302\u221e = (P > 21U\u0302) \u2020P\u03021, B\u0302(x) = (U\u0302 >P\u03023x1)(U\u0302 >P\u030221) \u2020\nThe algorithm, given above in Algorithm 1, follows the roadmap set out at the beginning of this section. While the last two steps are similar to the discrete HMM algorithm of Hsu et al. [2], the SVD, pseudoinverses and multiplications are with q/c-matrices. Once we have the estimates b\u03021, b\u0302\u221e, and B\u0302(x) the joint and predictive (conditional) densities can be estimated via (see Lemma 2):\np\u0302(x1:t) = b\u0302 > \u221eB\u0302(xt:1)\u0302b1, p\u0302(xt+t\u2032:t+1|x1:t) = b\u0302>\u221eB\u0302(xt+t\u2032:t+1)\u0302bt. (4)\nHere b\u0302t is the estimated internal state obtained by plugging in b\u03021, b\u0302\u221e, B\u0302 in (2). Theoretically, these estimates can be negative in which case they can be truncated to 0 without affecting the theoretical results in Section 5. However, in our experiments these estimates were never negative."}, {"heading": "4.3 Implementation Details", "text": "C/Q-Matrix operations using Chebyshev polynomials: While our algorithm and analysis are conceptually well founded, the important practical challenge lies in the efficient computation of the many aforementioned operations on c/q-matrices. Fortunately, some very recent advances in the numerical analysis literature, specifically on computing with Chebyshev polynomials, have rendered the above algorithm practical [6, Ch.3-4]. Due to the space constraints, we provide only a summary. Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19]. A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.6-4.8] of continuous matrices to be carried efficiently. The density estimates P\u03021, P\u030221, P\u0302321 are approximated by Chebyshev polynomials to within machine precision. Our implementation makes use of the Chebfun library [7] which provides an efficient implementation for the operations on continuous and quasi matrices.\nComputation time: Representing the KDE estimates P\u03021, P\u030221, P\u0302321 using Chebfun was roughly linear in N and is the brunt of the computational effort. The bandwidths for the three KDE estimates are chosen via cross validation which takes O(N2) effort. However, in practice the cost was dominated by the Chebyshev polynomial approximation. In our experiments we found that NPHMM-SPEC runs in linear time in practice and was more efficient than most alternatives. Training with longer sequences: When training with longer sequences we can use a sliding window of length 3 across the sequence to create the triples of observations needed for the algorithm. That is, given N samples each of length `(j), j = 1, . . . , N , we create an augmented dataset of triples { {(X(j)t , X (j) t+1, X (j) t+2)} `(j)\u22122 t=1 }Nj=1 and run NP-HMM-SPEC with the augmented data. As is with conventional EM procedures, this requires the additional assumption that the initial state is the stationary distribution of the transition matrix T ."}, {"heading": "5 Analysis", "text": "We now state our assumptions and main theoretical results. Following [2, 4, 14] we assume i.i.d sequences of triples are used for training. With longer sequences, the analysis should only be modified to account for the mixing of the latent state Markov chain, which is inessential for the main intuitions. We begin with the following regularity condition on the HMM. Assumption 3. \u03c0 > 0 element-wise. T \u2208 Rm\u00d7m and O \u2208 R[0,1]\u00d7m are of rank m.\nThe rank condition on O means that emission pdfs are linearly independent. If either T or O are rank deficient, then the learner may confuse state outputs, which makes learning difficult3. Next, while we make no parametric assumptions on the emissions, some smoothness conditions are used to make density estimation tractable. We use the H\u00f6lder class, H1(\u03b2, L), which is standard in the nonparametrics literature. For \u03b2 = 1, this assumption reduces to L-Lipschitz continuity. Assumption 4. All emission densities belong to the H\u00f6lder class,H1(\u03b2, L). That is, they satisfy,\nfor all \u03b1 \u2264 b\u03b2c, j \u2208 [m], s, t \u2208 [0, 1] \u2223\u2223\u2223\u2223d\u03b1Oj(s)ds\u03b1 \u2212 d\u03b1Oj(t)dt\u03b1 \u2223\u2223\u2223\u2223 \u2264 L|s\u2212 t|\u03b2\u2212|\u03b1|. Here b\u03b2c is the largest integer strictly less than \u03b2.\n3 Siddiqi et al. [4] show that the discrete spectral algorithm works under a slightly more general setting. Similar results hold for the nonparametric case too but will restrict ourselves to the full rank setting for simplicity.\nUnder the above assumptions we bound the total variation distance between the true and the estimated densities of a sequence, x1:t. Let \u03ba(O) = \u03c31(O)/\u03c3m(O) denote the condition number of the observation qmatrix. The following theorem states our main result. Theorem 5. Pick any sufficiently small > 0 and a failure probability \u03b4 \u2208 (0, 1). Let t \u2265 1. Assume that the HMM satisfies Assumptions 3 and 4 and the number of samples N satisfies,\nN\nlog(N) \u2265 Cm1+\n3 2\u03b2\n\u03ba(O)2+ 3 \u03b2\n\u03c3m(P21) 4+ 4\u03b2\n( t )2+ 3\u03b2 log ( 1\n\u03b4\n)1+ 32\u03b2 .\nThen, with probability at least 1 \u2212 \u03b4, the estimated joint density for a t-length sequence satisfies\u222b |p(x1:t)\u2212 p\u0302(x1:t)|dx1:t \u2264 . Here, C is a constant depending on \u03b2 and L and p\u0302 is from (4).\nSynopsis: Observe that the sample complexity depends critically on the conditioning of O and P21. The closer they are to being singular, the more samples is needed to distinguish different states and learn the HMM. It is instructive to compare the results above with the discrete case result of Hsu et al. [2], whose sample complexity bound4 is N & m \u03ba(O) 2\n\u03c3m(P21)4 t2 2 log 1 \u03b4 . Our bound is different in two\nregards. First, the exponents are worsened by additional \u223c 1\u03b2 terms. This characterizes the difficulty of the problem in the nonparametric setting. While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20\u201322], we think our bound might be unimprovable. As the smoothness of the densities increases \u03b2 \u2192 \u221e, we approach the parametric sample complexity. The second difference is the additional log(N) term on the left hand side. This is due to the fact that we want the KDE to concentrate around its expectation in L2 over [0, 1], instead of just point-wise. It is not clear to us whether the log can be avoided.\nTo prove Theorem 5, first we will derive some perturbation theory results for c/q-matrices; we will need them to bound the deviation of the singular values and vectors when we use P\u030221 instead of P21. Some of these perturbation theory results for continuous linear algebra are new and might be of independent interest. Next, we establish a concentration result for the kernel density estimator."}, {"heading": "5.1 Some Perturbation Theory Results for C/Q-matrices", "text": "The first result is an analog of Weyl\u2019s theorem which bounds the difference in the singular values in terms of the operator norm of the perturbation. Weyl\u2019s theorem has been studied for general operators [23] and cmatrices [6]. We have given one version in Lemma 21 of Appendix B. In addition to this, we will also need to bound the difference in the singular vectors and the pseudo-inverses of the truth and the estimate. To our knowledge, these results are not yet known. To that end, we establish the following results. Here \u03c3k(A) denotes the kth singular value of a c/q-matrix A.\nLemma 6 (Simplified Wedin\u2019s Sine Theorem for Cmatrices). Let A, A\u0303, E \u2208 R[0,1]\u00d7[0,1] where A\u0303 = A+ E and rank(A) = m. Let U, U\u0303 \u2208 R[a,b]\u00d7m be the first m left singular vectors of A and A\u0303 respectively. Then, for all x \u2208 Rm, \u2016U\u0303>Ux\u20162 \u2265 \u2016x\u20162 \u221a 1\u2212 2\u2016E\u20162L2/\u03c3m(A\u0303)2.\nLemma 7 (Pseudo-inverse Theorem for Qmatrices). Let A, A\u0303, E \u2208 R[a,b]\u00d7m and A\u0303 = A+E. Then, \u03c31(A \u2020 \u2212 A\u0303\u2020) \u2264 3 max{\u03c31(A\u2020)2, \u03c31(A\u2020)2}\u03c31(E)."}, {"heading": "5.2 Concentration Bound for the Kernel Density Estimator", "text": "Next, we bound the error for kernel density estimation. To obtain the best rates under H\u00f6lderian assumptions on O, the kernels used in KDE need to be of order \u03b2. A \u03b2 order kernel satisfies,\u222b 1\n0\nK(s)ds = 1,\n\u222b 1 0 s\u03b1K(s)ds = 0, for all \u03b1 \u2264 b\u03b2c, \u222b 1 0 s\u03b2K(s)ds \u2264 \u221e. (5)\nSuch kernels can be constructed using Legendre polynomials [17]. Given N i.i.d samples from a d dimensional density f , where d \u2208 {1, 2, 3} and f \u2208 {P1, P21, P321}, for appropriate choices of the bandwidths h1, h21, h321, the KDE f\u0302 \u2208 {P\u03021, P\u030221, P\u0302321} concentrates around f . Informally, we show\nP ( \u2016f\u0302 \u2212 f\u2016L2 > \u03b5 ) . exp ( \u2212 log(N) d 2\u03b2+dN 2\u03b2 2\u03b2+d \u03b52 ) . (6)\n4 Hsu et al. [2] provide a more refined bound but we use this form to simplify the comparison.\nfor all sufficiently small \u03b5 and N/ logN & \u03b5\u22122+ d \u03b2 . Here .,& denote inequalities ignoring constants. See Appendix C for a formal statement. Note that when the observations are either discrete or parametric, it is possible to estimate the distribution using O(1/\u03b52) samples to achieve \u03b5 error in a suitable metric, say, using the maximum likelihood estimate. However, the nonparametric setting is inherently more difficult and therefore the rate of convergence is slower. This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].\nA note on the Proofs: For Lemmas 6, 7 we follow the matrix proof in Stewart and Sun [26] and derive several intermediate results for c/q-matrices in the process. The main challenge is that several properties for matrices, e.g. the CS and Schur decompositions, are not known for c/q-matrices. In addition, dealing with various notions of convergences with these infinite objects can be finicky. The main challenge with the KDE concentration result is that we want an L2 bound \u2013 so usual techniques (such as McDiarmid\u2019s [13, 17]) do not apply. We use a technical lemma from Gin\u00e9 and Guillou [25] which allows us to bound the L2 error in terms of the VC characteristics of the class of functions induced by an i.i.d sum of the kernel. The proof of theorem 5 just mimics the discrete case analysis of Hsu et al. [2]. While, some care is needed (e.g. \u2016x\u2016L2 \u2264 \u2016x\u2016L1 does not hold for functional norms) the key ideas carry through once we apply Lemmas 21, 6, 7 and (6). A more refined bound on N that is tighter in polylog(N) terms is possible \u2013 see Corollary 25 and equation 13 in the appendix."}, {"heading": "6 Experiments", "text": "We compare NP-HMM-SPEC to the following. MG-HMM: An HMM trained using EM with the emissions modeled as a mixture of Gaussians. We tried 2, 4 and 8 mixtures and report the best result. NP-HMM-BIN: A naive baseline where we bin the space into n intervals and use the discrete spectral algorithm [2] with n states. We tried several values for n and report the best. NP-HMM-EM: The Nonparametric EM heuristic of [12]. NP-HMM-HSE: The Hilbert space embedding method of [14]. Synthetic Datasets: We first performed a series of experiments on synthetic data where the true distribution is known. The goal is to evaluate the estimated models against the true model. We generated triples from two HMMs with m = 4 and m = 8 states and nonparametric emissions. The details of the set up are given in Appendix E. Fig. 1 presents the results.\nFirst we compare the methods on estimating the one step ahead conditional density p(x6|x1:5). We report the L1 error between the true and estimated models. In Fig. 2 we visualise the estimated one step ahead conditional densities. NP-HMM-SPEC outperforms all methods on this metric. Next, we compare the methods on the prediction performance. That is, we sample sequences of length 6 and test how well a learned model can predict x6 conditioned on x1:5. When comparing on squared error, the best predictor is the mean of the distribution. For all methods we use the mean of p\u0302(x6|x1:5) except\nfor NP-HMM-HSE for which we used the mode since the mean cannot be computed. No method can do better than the true model (shown via the dotted line) in expectation. NP-HMM-SPEC achieves the performance of the true model with large datasets. Finally, we compare the training times of all methods. NP-HMM-SPEC is orders of magnitude faster than NP-HMM-HSE and NP-HMM-EM. Note that the error of MG-HMM\u2013 a parametric model \u2013 stops decreasing even with large data. This is due to the bias introduced by the parametric assumption. We do not train NP-HMM-EM for longer sequences because it is too slow. A limitation of the NP-HMM-HSE method is that it cannot recover conditional probabilities \u2013 so we exclude it from that experiment. We exclude NP-HMM-BIN from the time comparison because it was much faster than all other methods. We could not include the method of [4] in our comparisons since their code was not available and their method isn\u2019t straightforward to implement. Further, their method cannot compute joint/predictive probabilities.\nReal Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29]. Each model was trained using a training sequence and then the predictions were computed on a test sequence. The details on these datasets are in Appendix E. For all methods we used the mode of the conditional distribution p(xt+1|x1:t) as the prediction as it performed better. For NP-HMM-SPEC, NP-HMM-HSE,NP-HMM-BIN we follow the procedure outlined in Section 4.3 to create triples and train with the triples. In Table 1 we report the mean prediction error and the standard error. NP-HMMHSE and NP-HMM-SPEC perform better than the other two methods. However, NP-HMM-SPEC was faster to train (and has other attractive properties) when compared to NP-HMM-HSE."}, {"heading": "7 Conclusion", "text": "We proposed and studied a method for estimating the observable representation of a Hidden Markov Model whose emission probabilities are smooth nonparametric densities. We derive a bound on the sample complexity for our method. While our algorithm is similar to existing methods for discrete models, many of the ideas that generalise it to the nonparametric setting are new. In comparison to other methods, the proposed approach has some desirable characteristics: we can recover the joint/conditional densities, our theoretical results are in terms of more interpretable metrics, the method outperforms baselines and is orders of magnitude faster to train.\nIn this exposition only focused on one dimensional observations. The multidimensional case is handled by extending the above ideas and technology to multivariate functions. Our algorithm and the analysis carry through to the d-dimensional setting, mutatis mutandis. The concern however, is more practical. While we have the technology to perform various c/q-matrix operations for d = 1 using Chebyshev polynomials, this is not yet the case for d > 1. Developing efficient procedures for these operations in the high dimensional settings is a challenge for the numerical analysis community and is beyond the scope of this paper. That said, some recent advances in this direction are promising [8, 30].\nWhile our method has focused on HMMs, the ideas in this paper apply for a much broader class of problems. Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas. Going forward, we wish to focus on such models."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Alex Townsend, Arthur Gretton, and Ahmed Hefny for the helpful discussions."}, {"heading": "A A Quart-sized Review of Continuous Linear Algebra", "text": "In this section we introduce continuous analogues of matrices and their factorisations. We only provide a brief quart-sized review for what is needed in this exposition. Chapters 3 and 4 of Townsend [6] contains a reservoir-sized review.\nA matrix F \u2208 Rm\u00d7n is an m\u00d7 n array of numbers where F (i, j) denotes the entry in row i, column j. We will also look at cases where either m or n is infinite. A column qmatrix (quasi-matrix) Q \u2208 R[a,b]\u00d7m is a collection of m functions defined on [a, b] where the row index is continuous and column index is discrete. Writing Q = [q1, . . . , qm] where qj : [a, b] \u2192 R is the j th function, Q(y, j) = qj(y) denotes the value of the j th function at y \u2208 [a, b]. Q> \u2208 Rm\u00d7[a,b] denotes a row qmatrix withQ>(j, y) = Q(y, j). A cmatrix (continous-matrix)C \u2208 R[a,b]\u00d7[c,d] is a two dimensional function where both the row and column indices are continuous and C(y, x) is value of the function at (y, x) \u2208 [a, b]\u00d7 [c, d]. C> \u2208 R[c,d]\u00d7[a,b] denotes its transpose with C>(x, y) = C(y, x). Qmatrices and cmatrices permit all matrix multiplications with suitably defined inner products. Let F \u2208 Rm\u00d7n, Q \u2208 R[a,b]\u00d7m, P \u2208 R[a,b]\u00d7n, R \u2208 R[c,d]\u00d7m and C \u2208 R[a,b]\u00d7[c,d]. It follows that F (:, j) \u2208 Rm, Q(y, :) \u2208 R1\u00d7m, Q(:, i) \u2208 R[a,b], C(y, :) \u2208 R1\u00d7[c,d] etc. Then the following hold:\n\u2022 QF = S \u2208 R[a,b]\u00d7n where S(y, j) = Q(y, :)F (:, j) = \u2211m k=1Q(y, k)F (i, k).\n\u2022 Q>P = H \u2208 Rm\u00d7n where H(i, j) = Q(:, j)>P (:, j) = \u222b b a Q>(i, s)P (s, j)ds.\n\u2022 QR> = D \u2208 R[a,b]\u00d7[c,d] where D(y, x) = Q(y, :)R(x, :)> = \u2211m\n1 Q(y, k)R >(k, x). \u2022 CR = T \u2208 R[a,b]\u00d7m where T (y, j) = C(y, :)R(:, j) = \u222b d c C(y, s)R(s, j)ds.\nHere, the integrals are with respect to the Lebesgue measure.\nA cmatrix has a singular value decomposition (SVD). If C \u2208 R[a,b]\u00d7[c,d], an SVD of C is the sum C(y, x) = \u2211\u221e j=1 \u03c3juj(y)vj(x), which converges in L\n2. Here \u03c31 \u2265 \u03c32 \u2265 . . . . are the singular values of C. {uj}j\u22651 and {vj}j\u22651 are the left and right singular vectors and form orthonormal bases for L2([a, b]) and L2([c, d]) respectively, i.e. \u222b b a uj(s)uk(s)ds = 1(j = k). It is known that the SVD of a cmatrix exists uniquely with \u03c3j \u2192 0, and continuous singular vectors (Theorem 3.2, [6]). Further, if C is Lipshcitz continuous w.r.t both variables then the SVD is absolutely and uniformly convergent. Writing the singular vectors as infinite qmatrices U = [u1, u2 . . . ], V = [v1, v2 . . . ], and \u03a3 = diag(\u03c31, \u03c32 . . . ) we can write the SVD as,\nC = U\u03a3V > = \u221e\u2211 j=1 \u03c3jU(:, j)V (:, j) >.\nIf only m <\u221e singular values are nonzero then we say that C is of rank m. The SVD of a Qmatrix Q \u2208 R[a,b]\u00d7m is, Q = U\u03a3V > = \u2211m j=1 \u03c3jU(:, j)V (:, j)\n>, where U \u2208 R[a,b]\u00d7m and V \u2208 Rm\u00d7m have orthonormal columns and \u03a3 = diag(\u03c31, . . . , \u03c3m) with \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3m \u2265 0. The SVD of a qmatrix also exists uniquely (Theorem 4.1, [6]). The rank of a column qmatrix is the number of linearly independent columns (i.e. functions) and is equal to the number of nonzero singular values.\nFinally, the pseudo inverse of the cmatrix C is C\u2020 = V \u03a3\u22121U> with \u03a3\u22121 = diag(1/\u03c31, 1/\u03c32, . . . ). The p-operator norm of a cmatrix, for 1 \u2264 p \u2264 \u221e is \u2016C\u2016p = sup\u2016x\u2016p=1 \u2016Cx\u2016p where x \u2208 R [c,d],\nCx \u2208 R[a,b], \u2016x\u2016pp = \u222b d c (x(s))pds for p < \u221e and \u2016x\u2016\u221e = sups\u2208[c,d] x(s). The Frobenius\nnorm of a cmatrix is \u2016C\u2016F = (\u222b b\na \u222b d c C(y, x)2dxdy )1/2 . It can be shown that \u2016C\u20162 = \u03c31 and\n\u2016C\u20162F = \u2211 j \u03c3 2 j where \u03c31 \u2265 \u03c32 \u2265 . . . are its singular values. Note that analogous relationships hold with finite matrices. The pseudo inverse and norms of a qmatrix are similarly defined and similar relationships hold with its singular values.\nNotation: In what follows we will use 1[a,b] to denote the function taking value 1 everywhere in [a, b] and 1m to denote m-vectors of 1\u2019s. When we are dealing with Lp norms of a function we will explicitly use the subscript Lp to avoid confusion with the operator/Frobenius norms of qmatrices and cmatrices. For example, for a cmatrix \u2016C\u20162L2 = \u222b \u222b C(\u00b7, \u00b7)2 = \u2016C\u20162F . As we have already done, throughout the paper we will overload notation for inner products, multiplications and pseudo-inverses depending on whether they hold for matrices, qmatrices or cmatrices. E.g. when\np, q \u2208 Rm, p>q = \u2211m 1 piqi and when p, q \u2208 R[a,b], p>q = \u222b b a p(s)q(s)ds. P will be used to denote probabilities of events while p will denote probability density functions (pdf)."}, {"heading": "B Some Perturbation Theory Results for Continuous Linear Algebra", "text": "We recommend that readers unfamiliar with continuous linear algebra first read the review in Appendix A. Throughout this section L(\u00b7) maps a matrix (including q/cmatrices) to its eigenvalues. Similarly, \u03c3(\u00b7) maps a matrix to its singular values. When we are dealing with infinite sequences and qmatrices \u201c=\" refers to convergence in L2. When dealing with infinite sequences and cmatrices, \u201c=\" refers to convergence in the operator norm. For all theorems, we follow the template of Stewart and Sun [26] for the matrix case and hence try to stick with their notation.\nBefore we proceed, we introduce the \u201ccmatrix\" I[0,1] on [0, 1]. For any u \u2208 R[0,1] this is the operator which satisfies I[0,1]u = u. That is, (I[0,1]u)(y) = \u222b 1 0 I[0,1](y, x)u(x)dx = u(y). Intuitively, it can be thought of as the Dirac delta function along the diagonal, \u03b4(x \u2212 y). Let Q = [q1, q2, . . . , ] \u2208 R[0,1]\u00d7\u221e be a qmatrix containing an orthonormal basis for [0, 1] and Qk \u2208 R[0,1]\u00d7k denote the first k columns of Q. We make note of the following observation.\nTheorem 8. QkQ>k \u2192 I[0,1] as k \u2192\u221e. Here convergence is in the operator norm. Proof. We need to show that for all x \u2208 R[0,1], \u2016QkQ>k x\u2212 x\u20162 \u2192 0. Let x = Q\u03b1 = \u2211\u221e k=1 \u03b1kqk\nbe the representation of x in the Q-basis. Here \u03b1 = (\u03b11, \u03b12, . . . ) satisfies \u2211 k \u03b1 2 k < \u221e. We then\nhave \u2016QkQ>k x\u2212 x\u201622 = \u2211\u221e j=k+1 \u03b1 2 j \u2192 0 by the properties of sequences in `2.\nWe now proceed to our main theorems. We begin with a series of intermediary results.\nTheorem 9. Let X \u2208 R[0,1]\u00d7m. Define the linear operator T(X) = AX \u2212 XB where A \u2208 R[0,1]\u00d7[0,1] and B \u2208 Rm\u00d7m are a square cmatrix and matrix, respectively. Then, T is nonsingular if and only if L(A) \u2229 L(B) = \u2205.\nProof. Assume \u03bb \u2208 L(A) \u222a L(B). Then, let Ap = \u03bbp, q>B = \u03bbq> where p \u2208 R[0,1] and q \u2208 Rm. Then T(pq>) = 0 and T is singular. This proves one side of the theorem.\nNow, assume that L(A) \u2229 L(B) = \u2205. As the operator is linear, it is sufficient to show that AX \u2212XB = C has a unique solution for any C \u2208 R[0,1]\u00d7m. Let the Schur decomposition of B be Q = V >BV where V is orthogonal and Q is upper triangular. Writing Y = XV and D = CV it is sufficient to show that AY \u2212 Y Q = D has a unique solution. We write\nY = (y1, y2, . . . ym) \u2208 R[0,1]\u00d7m and D = (d1, d2, . . . , dm) \u2208 R[0,1]\u00d7m\nand use an inductive argument over the columns of Y .\nThe first column of Y is given by Ay1 \u2212 Q11y1 = (A \u2212 Q11I[0,1])y1 = d1. Since Q11 \u2208 L(B) and L(A) \u2229 L(B) is empty (A\u2212Q11I[0,1]) is nonsingular. Therefore y1 is uniquely determined by inverting the cmatrix (see Appendix A). Assume y1, y2 . . . , yk\u22121 are uniquely determined. Then, the kth column is given by (A\u2212QkkI[0,1])yk = dk+ \u2211k\u22121 i=1 Qikyi. Again, (A\u2212QkkI[0,1]) is nonsingular by assumption, and hence this uniquely determines yk.\nCorollary 10. Let T be as defined in Theorem 9. Then\nL(T) = L(A)\u2212 L(B) = {\u03b1\u2212 \u03b2 : \u03b1 \u2208 L(A), \u03b2 \u2208 L(B)}.\nProof. If \u03bb \u2208 L(T) there exists X such that (A\u2212 \u03bbI[0,1])X \u2212XB = 0. Therefore, by Theorem 9 there exists \u03b1 \u2208 L(A) and \u03b2 \u2208 L(B) such that \u03bb = \u03b1\u2212 \u03b2. Therefore, L(T) \u2282 L(A)\u2212 L(B).\nConversely, consider any \u03b1 \u2208 L(A) and \u03b2 \u2208 L(B). Then there exists a \u2208 R[0,1], b \u2208 Rm such that Aa = \u03b1a and b>B = \u03b2b>. Writing X = ab> we have AX \u2212XB = (\u03b1\u2212 \u03b2)ab>. Therefore, L(A)\u2212 L(B) \u2282 L(T).\nTheorem 11. Let T be as defined in Theorem 9. Then\ninf \u2016X\u2016F=1\n\u2016T(X)\u2016F = minL(T) = min |L(A)\u2212 L(B)|. (7)\nProof. For any qmatrix P = (p1, p2, . . . , pm) \u2208 R[0,1]\u00d7m let vec(P ) = [p>1 , p>2 , . . . , p>m]> \u2208 R[0,m]\u00d71 be the concatenation of all functions. Then vec(XB) = ~Bvec(X) where,\n~B =  B11I[0,1] B21I[0,1] \u00b7 \u00b7 \u00b7 Bm1I[0,1] B12I[0,1] B22I[0,1] \u00b7 \u00b7 \u00b7 Bm2I[0,1]\n... ...\n. . . ...\nB1mI[0,1] B2mI[0,1] \u00b7 \u00b7 \u00b7 BmmI[0,1]  \u2208 R[0,m]\u00d7[0,m]. Here I[0,1] have been translated and should be interpreted as being a dirac delta function on that block. Similarly, vec(AX) = ~Avec(X) where ~A = diag(A,A, . . . , A) \u2208 R[0,m]\u00d7[0,m]. Therefore vec(T(X)) = ( ~A\u2212 ~B) ~X . Now noting that \u2016X\u2016F = \u2016vec(X)\u20162 we have,\ninf \u2016X\u2016F=1 \u2016T(X)\u2016F = inf \u2016vec(X)\u20162=1 \u2016vec(T(X))\u20162 = min |L( ~A\u2212 ~B)|.\nThe theorem follows by noting that the eigenvalues of ( ~A\u2212 ~B) are the same as those of L(T).\nTheorem 12. Let X1, Y1 \u2208 R[0,1]\u00d7` have orthonormal columns. Then, there exist Q \u2208 R\u221e\u00d7[0,1] and U11, V11 \u2208 R`\u00d7` such that the following holds,\nQX1U11 = [ I` 0 ] \u2208 R\u221e\u00d7`, QY1V11 = [ \u0393 \u03a3 0 ] \u2208 R\u221e\u00d7`.\nHere \u0393 = diag(\u03b31, . . . , \u03b3`), \u03a3 = diag(\u03c31, . . . , \u03c3`) and they satisfy\n0 \u2264 \u03b31 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b3`, \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3` \u2265 0, and \u03b32i + \u03c32i = 1, i = 1, . . . , `.\nProof. Let X2, Y2 \u2208 R[0,1]\u00d7\u221e be orthonormal bases for the complementary subspaces of R(X1),R(Y1), respectively. Denote X = [X1, X2], Y = [Y1, Y2] and\nW = X>Y = ( W11 W12 W21 W22 ) \u2208 R\u221e\u00d7\u221e,\nwhereW11 = X>1 Y1 \u2208 R`\u00d7` and the rest are defined accordingly. Now, using Theorem 5.1 from [26] there exist orthogonal matrices U = diag(U11, U22), V = diag(V11, V22) where U11, V11 \u2208 R`\u00d7` and U22, V22 \u2208 R\u221e\u00d7\u221e such that the following holds,\nU>WV = ( \u0393 \u2212\u03a3 0 \u03a3 \u0393 0 0 0 I\u221e ) \u2208 R\u221e\u00d7\u221e.\nHere \u0393,\u03a3 satisfy the conditions of the theorem. Now set X\u0302 = [X\u03021, X\u03022], Y\u0302 = [Y\u03021, Y\u03022] where X\u03021 = X1U11, X\u03022 = X2U11, Y\u03021 = Y1V11, Y\u03022 = Y2V11. Then, X\u0302>Y = U>WV . Setting Q = X\u0302> and setting U11, V11 as above yields,\nQX1U11 =\n( U>11X > 1\nU>22X > 2\n) X1U11 = [ I` 0 ] , QY1V11 = ( U>11X > 1\nU>22X > 2\n) Y1V11 = [ \u0393 \u03a3 0 ]\nwhere U>11X > 1 Y1U11 = \u0393, U > 22X > 2 Y1U11 = [\u03a3 >,0>]> from the decomposition of U>WV .\nRemark 13. Stewart and Sun [26] prove Theorem 5.1 for a finite unitary W . However, it is straightforward to verify that the same holds if W is a unitary operator on the `2 sequence space, i.e., Theorem 5.1 is valid for (countably) infinite matrices.\nDefinition 14 (Canonical Angles). Let X ,Y be ` dimensional subspaces of the same dimension for functions on [0, 1] and X1, Y1 \u2208 R[0,1]\u00d7` be orthonormal functions spanning these subspaces. Then the canonical angles between X and Y are the diagonals of the matrix \u0398[X ,Y] \u2206= sin\u22121(\u03a3) where \u03a3 is from Theorem 12. It follows that cos \u0398[X ,Y] = \u0393 where sin and cos are in the usual trigonometric sense and satisfy cos2(x) + sin2(x) = 1.\nCorollary 15. Let X ,Y, X1, Y1 be as in Definition 14 and X2, Y2 be orthonormal functions for their complementary spaces. Then, the nonzero singular values of X>2 Y1 are the sines of the nonzero canonical angles between X ,Y . The singular values of X>1 Y1 are the cosines of the nonzero canonical angles.\nProof. From the proof of Theorem 12,\nX>2 Y1 = U22 ( \u03a3 0 ) U>11, X > 1 Y1 = U11\u0393U > 11.\nSince U11, U22 are orthogonal, the above are the SVDs of X>2 Y1 and X > 1 Y1.\nTheorem 16. Let X ,Y be ` dimensional subspaces of functions on [0, 1] and X1, Y1 \u2208 R[0,1]\u00d7l be an orthonormal bases. Let sin \u0398[X ,Y] = diag(\u03c31, . . . , \u03c3`). Denote PX = X1X>1 and PY = Y1Y >1 . Then, the singular values of PX (I[0,1] \u2212 PY) are \u03c31, \u03c32, . . . , \u03c3`, 0, 0, . . . .\nProof. By Theorem 12, there exists Q \u2208 R\u221e\u00d7[0,1], U11, V11 \u2208 R`\u00d7`, such that\nQPX (I[0,1] \u2212 PY)Q> = QX1X>1 Q>Q(I[0,1] \u2212 Y1Y >1 )Q>\n= (QX1U1)(U > 1 X > 1 Q >)(I[0,1] \u2212QY1V11(V >11Y >1 Q>)) = [ \u03a3 0 0 ] [\u03a3 \u2212\u0393 0]\nHere we have used I[0,1] = Q>Q. The proof of this uses a technical argument involving the dual space of the class of operators described by cmatrices. (In the discrete matrix case this is similar to how the outer product of a complete orthonormal basis results in the identity UU> = I .) The last step follows from Theorem 12 and some algebra. Noting that [\u03a3 \u2212\u0393 0] has orthonormal rows, it follows that the singular values of PX (I[0,1] \u2212 PY) are \u03a3.\nTheorem 17. Let A \u2208 R[0,1]\u00d7[0,1] satisfy,\nA = [X1 X2] [ L1 0 0 L2 ] [ X>1 X>2 ] where X1 \u2208 R[0,1]\u00d7` and [X1, X2] is unitary. Let Z \u2208 R[0,1]\u00d7m and T = AZ \u2212 ZB where B \u2208 Rm\u00d7m. Let \u03b4 = min |L(L2)\u2212 L(B)| > 0. Then,\u2225\u2225 sin \u0398[R(X1),R(Z)]\u2016F \u2264 \u2016T\u2016F\n\u03b4 .\nProof. First note that X>2 T = L2X > 2 Z \u2212X>2 ZB. The claim follows from Theorems 11 and 15.\u2225\u2225 sin \u0398[R(X1),R(Z)]\u2016F = \u2016X>2 Z\u2016F \u2264 \u2016X>2 T\u2016Fmin |L(L2)\u2212 L(B)| \u2264 \u2016T\u2016F\u03b4 .\nTheorem 18 (Wedin\u2019s Sine Theorem for cmatrices \u2013 Frobenius form). LetA, A\u0303, E \u2208 R[0,1]\u00d7[0,1] with A\u0303 = A+ E. Let A, A\u0303 have the following conformal partitions,\nA = [U1 U2] [ \u03a31 0 0 \u03a32 ] [ V >1 V >2 ] , A\u0303 = [ U\u03031 U\u03032 ] [\u03a3\u03031 0 0 \u03a3\u03032 ] [ V\u0303 >1 V\u0303 >2 ] .\nwhere U1, U\u03031 \u2208 R[0,1]\u00d7m, V1, V\u03031 \u2208 R[0,1]\u00d7m and U2, U\u03032 \u2208 R[0,1]\u00d7\u221e, V2, V\u03032 \u2208 R[0,1]\u00d7\u221e. Let R = AV\u03031 \u2212 U\u03031\u03a3\u03031 \u2208 R[0,1]\u00d7m and S = A>U\u03031 \u2212 V\u03031\u03a3\u03031 \u2208 R[0,1]\u00d7m. Assume there exists \u03b4 > 0 such that, min |\u03c3(\u03a3\u03031)\u2212 \u03c3(\u03a32)| \u2265 \u03b4 and min |\u03c3(\u03a3\u03031)| \u2265 \u03b4. Let \u03a61,\u03a62 denote the canonical angles between (R(U1),R(U\u03031)) and (R(V1),R(V\u03031)) respectively. Then,\u221a\n\u2016 sin \u03a61\u20162F + \u2016 sin \u03a62\u20162F \u2264\n\u221a \u2016R\u20162F + \u2016S\u20162F\n\u03b4 .\nRemark 19. The two conditions on \u03b4 are needed because the theorem doesn\u2019t require \u03a31,\u03a32, \u03a3\u03031, \u03a3\u03032 to be ordered. If they were ordered, then it reduces to \u03b4 = min |\u03c3(\u03a3\u03031)\u2212 \u03c3(\u03a32)| > 0.\nProof. First define Q \u2208 R[0,2]\u00d7[0,2],\nQ = [ 0 A A> 0 ] .\nIt can be verified that if ui \u2208 R[0,1], vi \u2208 R[0,1] are a left/right singular vector pair with singular value \u03c3i, then (ui, vi) \u2208 R[0,2] is an eigenvector with eigenvalue \u03c3i and (ui,\u2212vi) \u2208 R[0,2] is an eigenvector with eigenvalue \u2212\u03c3i. Writing,\nX = 1\u221a 2 ( U1 U1 V1 \u2212V1 ) , Y = 1\u221a 2 ( U2 U2 V2 \u2212V2 ) ,\nwe have,\nQ = [X Y ] \u03a31 0 0 00 \u2212\u03a31 0 00 0 \u03a32 0 0 0 0 \u2212\u03a32 [X> Y > ] .\nWe similarly define Q\u0303, X\u0303, Y\u0303 for A\u0303. Now let T = QX\u0303\u2212X\u0303diag(\u03a3\u03031,\u2212\u03a3\u03031). We will apply Theorem 17 with L1 = diag(\u03a31,\u2212\u03a31), L2 = diag(\u03a32,\u2212\u03a32), Z = X\u0303 , B = diag(\u03a3\u03031,\u2212\u03a3\u03031). Then, using the conditions on \u03b4 gives us, \u2225\u2225 sin \u0398[R(X),R(X\u0303)]\u2016F \u2264 \u2016T\u2016F\n\u03b4 .\nIt is straightforward to verify that \u2016T\u20162F = \u2016R\u20162F + \u2016S\u20162F. To conclude the proof, first note that XX>(I[0,2] \u2212 Y Y >) = [ (U1U > 1 )(I[0,1] \u2212 U\u03031U\u0303>1 ) 0\n0 (V1V > 1 )(I[0,1] \u2212 V\u03031V\u0303 >1 ) ] Now, using Theorem 16 we have \u2016 sin \u0398[R(X),R(X\u0303)]\u20162F = \u2016 sin \u03a621\u20162F + \u2016 sin \u03a622\u20162F.\nWe can now prove Lemma 6 which follows directly from Theorem 18.\nProof of Lemma 6. Let U\u0303\u22a5 \u2208 R[0,1]\u00d7m be an orthonormal basis for the complementary subspace of R(U\u0303). Then, by Corollary 15, \u2016U\u0303>\u22a5U\u20162F = \u2016 sin \u0398[R(U\u0303),R(U)]\u20162F, \u2016V\u0303 >\u22a5 V \u20162F = \u2016 sin \u0398[R(V\u0303 ),R(V )]\u20162F. For R,S as defined in Theorem 18, we have. \u2016R\u20162F, \u2016S\u20162F < \u2016E\u20162F. The lemma follows via the sin\u2013cos relationships for canonical angles,\nmin\u03c3(U\u0303>U)2 = 1\u2212max\u03c3(U\u0303>\u22a5U)2 \u2265 1\u2212 \u2016U\u0303>\u22a5U\u20162F \u2265 1\u2212 2\u2016E\u20162F \u03b42 .\nwhere \u03b4 = \u03c3m(A).\nNext we prove the pseudo-inverse theorem. Recall that for A \u2208 R[0,1]\u00d7m the SVD is A = U\u03a3V > where U \u2208 R[0,1]\u00d7m, \u03a3 \u2208 Rm\u00d7m and V \u2208 Rm\u00d7m where U, V have orthonormal columns. Denote its pseudo-inverse by A\u2020 = V \u03a3\u22121U>.\nProof of Lemma 7. Let A = U\u03a3V be the SVD of A and A\u0303 = U\u0303 \u03a3\u0303V\u0303 be the SVD of A\u0303. Let P\u0303 = U\u0303 U\u0303>, R = V V >, R\u0303 = V\u0303 V\u0303 >, P\u22a5 = I[0,1] \u2212 UU>, R\u0303\u22a5 = I[0,1] \u2212 V\u0303 V\u0303 > and P = UU>. We then have,\nA\u0303\u2020 \u2212A\u2020 = \u2212A\u0303\u2020P\u0303ERA\u2020 + (A\u0303>A\u0303)\u2020R\u0303E>P\u22a5 + R\u0303\u22a5EP (AA>)\u2020\n\u2016A\u0303\u2020 \u2212A\u2020\u20162 \u2264 \u2016A\u0303\u2020\u20162\u2016E\u20162\u2016A\u2020\u20162 + \u2016(A\u0303>A\u0303)\u2020\u20162\u2016E\u20162 + \u2016E\u20162\u2016(AA>)\u2020\u20162 = ( \u2016A\u0303\u2020\u20162\u2016A\u2020\u20162 + \u2016A\u0303\u2020\u201622 + \u2016A\u2020\u201622 ) \u2016E\u20162 \u2264 3 max{\u2016A\u0303\u201622, \u2016A\u201622}\u2016E\u20162\nThe first step is obtained by substitutine for P\u0303 , E,R, R\u0303, P\u22a5, R\u0303\u22a5 and P , the second step uses the triangle inequality, and the third step uses A\u0303>A\u0303 = U\u03a32U>, AA> = V \u03a32V >.\nRemark 20. P, P\u0303 , R, R\u0303 can be shown to be the projection operators toR(A),R(A\u0303),R(A>) and R(A\u0303>). Here, R(A) = {Ax;x \u2208 Rm} \u2282 R[0,1] is the range of A. R(A\u0303) \u2282 R[0,1], R(A>) \u2282 Rm andR(A\u0303>) \u2282 Rm are defined similarly. P\u22a5, R\u0303\u22a5 are the complementary projectors of P, R\u0303.\nFinally, we state an analogue of Weyl\u2019s theorem for cmatrices which bounds the difference in the singular values in terms of the operator norm of the perturbation. While Weyl\u2019s theorem has been studied for general operators [23], we use the form below from Townsend [6] for cmatrices.\nLemma 21 (Weyl\u2019s Theorem for Cmatrices, [6].). Let A,E \u2208 R[a,b]\u00d7[c,d] and A\u0303 = A+ E. Let the singular values of A be \u03c31 \u2265 \u03c32, . . . and those of A\u0303 be \u03c3\u03031 \u2265 \u03c3\u03032, . . . . Then,\n|\u03c3i \u2212 \u03c3\u0303i| \u2264 \u2016E\u20162 \u2200i \u2265 1."}, {"heading": "C Concentration of Kernel Density Estimation", "text": "We will first define the H\u00f6lder class in high dimensions. Definition 22. Let X \u2282 Rd be a compact space. For any r = (r1, . . . , rd), ri \u2208 N, let |r| = \u2211 i ri and Dr = \u2202 |r|\n\u2202x r1 1 ...x rd d\n. The H\u00f6lder classHd(\u03b2, L) is the set of functions of L2(X ) satisfying\n|Drf(x)\u2212Drf(y)| \u2264 L\u2016x\u2212 y\u2016\u03b2\u2212|r|, (8) for all r such that |r| \u2264 b\u03b2c and for all x, y \u2208 X .\nThe following result establishes concentration of kernel density estimators. At a high level, we follow the standard KDE analysis techniques to decompose the L2 error into bias and variance terms and bound them separately. A similar result for 2-dimensional densities was given by Liu et al. [24]. Unlike the previous work, here we deal with the general d-dimensional case as well as explicitly delineate the dependencies of the concentration bounds on the deviation, \u03b5.\nLemma 23. Let f \u2208 Hd(\u03b2, L) be a density on [0, 1]d and assume we have N i.i.d samples {Xi}Ni=1 \u223c f . Let f\u0302 be the kernel density estimate obtained using a kernel with order at least \u03b2 and bandwidth h = ( logN N ) 1 2\u03b2+d . Then there exist constants \u03ba1, \u03ba2, \u03ba3, \u03ba4 > 0 such that for all \u03b5 < \u03ba4 and number of samples satisfying NlogN > \u03ba1\n\u03b5 2+ d \u03b2 we have, P ( \u2016f\u0302 \u2212 f\u2016L2 > \u03b5 ) \u2264 \u03ba2 exp ( \u2212\u03ba3N 2\u03b2 2\u03b2+d (logN) d 2\u03b2+d \u03b52 ) (9)\nProof. First note that P ( \u2016f\u0302 \u2212 f\u2016L2 > \u03b5 ) \u2264 P ( \u2016f\u0302 \u2212 Ef\u0302\u2016L2 + \u2016Ef\u0302 \u2212 f\u2016L2 > \u03b5 ) . (10)\nUsing the H\u00f6lderian conditions and assumptions on the kernel, standard techniques for analyzing the KDE [13, 17], give us a bound on the bias, \u2016Ef\u0302 \u2212 f\u2016L2 \u2264 \u03ba5h\u03b2 , where \u03ba5 = L \u222b K(u)u\u03b2du. When the number of samples, N , satisfies\nN\nlogN > ( 2\u03ba\u20325 \u03b5 )2+ d\u03b2 = \u03ba5\n\u03b52+ d \u03b2\n, where \u03ba5 \u2206 = (2\u03ba\u20325) 2+ d\u03b2 (11)\nwe have \u2016Ef\u0302 \u2212f\u2016L2 \u2264 \u03b5/2, and hence (10) turns into P ( \u2016f\u0302 \u2212f\u2016L2 > \u03b5 ) \u2264 P ( \u2016f\u0302 \u2212Ef\u0302\u2016L2 > \u03b5/2 ) .\nThe main challenge in bounding the first term is that we want the difference to hold in L2. The standard techniques that bound the pointwise variance would not be sufficient here. To overcome the limitations, we use Corollary 2.2 from Gin\u00e9 and Guillou [25]. Using their notation we have,\n\u03c32 = sup t\u2208[0,1]d\nVX\u223cf [ 1\nhd K ( X \u2212 t h )] \u2264 sup\nt\u2208[0,1]d\n1\nh2d \u222b K2 ( x\u2212 t h ) f(x)dx\n= sup t\u2208[0,1]d\n1\nhd\n\u222b K2(u)f(t+ uh)du \u2264 \u2016f\u2016\u221e\u2016K\u2016L 2\nhd\nU = sup t\u2208[0,1]d \u2225\u2225\u2225\u2225 1hdK ( X \u2212 t h )\u2225\u2225\u2225\u2225 \u221e = \u2016K\u2016L\u221e hd .\nThen, there exist constants \u03ba2, \u03ba3, \u03ba\u20324 such that for all \u03b5 \u2208 ( \u03ba\u20324 \u03c3\u221a n \u221a log U\u03c3 , \u03c32 U \u03ba \u2032 4 ) we have,\nP ( \u2016f\u0302 \u2212 Ef\u0302\u2016L2 > \u03b5\n2\n) \u2264 \u03ba2 exp ( \u2212\u03ba3Nhd\u03b52 ) .\nSubstituting for h and then combining this with (10) gives us the probability inequality of the theorem. All that is left to do is to verify the that the conditions on \u03b5 hold. The upper bound condition requires \u03b5 \u2264 \u03ba\n\u2032 4\u2016f\u2016\u221e\u2016K\u2016L2 \u2016K\u2016L\u221e \u2206 = \u03ba4. After some algebra, the lower bound on \u03b5 reduces to NlogN > \u03ba6\n\u03b5 2+ d \u03b2 .\nCombining this with the condtion (11) and taking \u03ba1 = max(\u03ba6, \u03ba5) gives the theorem.\nIn order to apply the above lemma, we need P1, P21, P321 to satisfy the H\u00f6lder condition. The following lemma shows that if all Ok\u2019s are H\u00f6lderian, so are P1, P21, P321.\nLemma 24. Assume that the observation probabilities belong to the one dimensional H\u00f6lder class; \u2200` \u2208 [m], O` \u2208 H1(\u03b2, L). Then for some constants L1, L2, L3, P1 \u2208 H1(\u03b2, L1), P21 \u2208 H2(\u03b2, L2), P321 \u2208 H3(\u03b2, L3).\nProof. We prove the statement for P21. The other two follow via a similar argument. Let r = (r1, r2), ri \u2208 N, |r| = r1 + r2 \u2264 \u03b2, and let (s, t), (s\u2032, t\u2032) \u2208 [0, 1]d. Note that we can write,\nP21(s, t) = \u2211 k\u2208[m] \u2211 `\u2208[m] p(x2 = s, x1 = t, h2 = k, h1 = `) = \u2211 k\u2208[m] \u2211 `\u2208[m] \u03b1klOk(s)O`(t),\nwhere \u2211 k,` \u03b1k` = 1. Then,\n\u2202|r|P21(s, t)\n\u2202sr1\u2202tr2 \u2212 \u2202\n|r|P21(s \u2032, t\u2032)\n\u2202sr1\u2202tr2 = \u2211 k,` \u03b1k` ( \u2202Ok(s) \u2202sr1 \u2202O`(t) \u2202tr2 \u2212 \u2202Ok(s \u2032) \u2202sr1 \u2202O`(t \u2032) \u2202tr2 )\n\u2264 \u2211 k,` \u03b1k` (\u2223\u2223\u2223\u2223\u2202Ok(s)\u2202sr1 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202O`(t)\u2202tr2 \u2212 \u2202O`(t\u2032)\u2202tr2 \u2223\u2223\u2223\u2223 +\u2223\u2223\u2223\u2223\u2202O`(t\u2032)\u2202tr2 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202Ok(s)\u2202sr1 \u2212 \u2202O`(s\u2032)\u2202sr1\n\u2223\u2223\u2223\u2223) \u2264 \u2211 k,` \u03b1kl ( L\u2032L|t\u2212 t\u2032|\u03b2\u2212r2 + L\u2032L|s\u2212 s\u2032|\u03b2\u2212r1 ) (H\u00f6lder condition)\n\u2264 L\u2032L ( |t\u2212 t\u2032|\u03b2\u2212|r| + |s\u2212 s\u2032|\u03b2\u2212|r| ) (domain of s, s\u2032 and t, t\u2032)\n\u2264 L2 \u221a (t\u2212 t\u2032)2 + (s\u2212 s\u2032)2 \u03b2\u2212|r|\nHere, the third step uses the H\u00f6lder conditions on Ok and O` and the fact that the partial fractions are bounded in a bounded domain by a constant, which we denoted L\u2032, due to the H\u00f6lder condition. Since r1 +r2 = |r| \u2264 \u03b2 and r1, r2 are positive integers, we have x\u03b2\u2212ri \u2264 x\u03b2\u2212r, i = 1, 2 for any x \u2208 [0, 1], which implies the fourth step. The last step uses Jensen\u2019s inequality and sets L2 \u2261 L\u2032L.\nThe corollary belows follws as a direct consequence of Lemmas 23 and 24. We have absorbed the constants L1, L2, L3 into \u03ba1, \u03ba2, \u03ba3, \u03ba4.\nCorollary 25. Assume the HMM satisfies the conditions given in Section 3. Let 1, 21, 321 \u2208 (0, \u03ba4) and \u03b7 \u2208 (0, 1). If the number of samples N is large enough such that the following are true,\nN\nlogN >\n\u03ba1 2+ 1\u03b2 1 , N logN > \u03ba1 2+ 2\u03b2 21 , N logN > \u03ba1 2+ 3\u03b2 321 ,\nN(logN) 1 2\u03b2 >\n1\n2+ 1\u03b2 1\n( 1\n\u03ba3 log ( 3\u03ba2 \u03b7 ))1+ 12\u03b2 N(logN) 2 2\u03b2 > 1\n2+ 2\u03b2 21\n( 1\n\u03ba3 log ( 3\u03ba2 \u03b7 ))1+ 22\u03b2 N(logN) 3 2\u03b2 > 1\n2+ 3\u03b2 321\n( 1\n\u03ba3 log ( 3\u03ba2 \u03b7 ))1+ 32\u03b2 then with at least 1 \u2212 \u03b7 probability the L2 errors between P1, P21, P321 and the KDE estimates P\u03021, P\u030221, P\u0302321 satisfy,\n\u2016P1 \u2212 P\u03021\u2016L2 \u2264 1, \u2016P21 \u2212 P\u030221\u2016L2 \u2264 21, \u2016P321 \u2212 P\u0302321\u2016L2 \u2264 321."}, {"heading": "D Analysis of the Spectral Algorithm", "text": "Our proof is a brute force generalization of the analysis in Hsu et al. [2]. Following their template, we use establish a few technical lemmas. We mainly focus on the cases where our analysis is different.\nThroughout this section 1, 21, 321 will refer to L2 errors. Using our notation for c/q-matrices the errors can be written as,\n1 = \u2016P1 \u2212 P\u03021\u2016L2 = \u2016P1 \u2212 P\u03021\u2016F , 21 = \u2016P21 \u2212 P\u030221\u2016L2 = \u2016P21 \u2212 P\u030221\u2016F , 321 = \u2016P321 \u2212 P\u0302321\u2016L2 .\nWe begin with a series of Lemmas.\nLemma 26. Let 21 \u2264 \u03b5\u03c3m(P21) where \u03b5 < 11+\u221a2 . Denote \u03b50 = 221\n((1\u2212\u03b5)\u03c3m(P21))2 < 1. Then the following hold,\n1. \u03c3m(U\u0302>P\u030221) \u2265 (1\u2212 \u03b5)\u03c3m(P21).\n2. \u03c3m(U\u0302>P21) \u2265 \u221a 1\u2212 \u03b50\u03c3m(P21).\n3. \u03c3m(U\u0302>P21) \u2265 \u221a 1\u2212 \u03b50\u03c3m(P21).\nProof. The proof follows Hsu et al. [2] after an application of Weyl\u2019s theorem (Lemma 21) and Wedin\u2019s sine theorem (Lemma 6) for cmatrices.\nWe define an alternative observable representation for the true HMM given by, b\u0303\u221e, b\u03031 \u2208 Rm and B\u0303 : [0, 1]\u2192 Rm\u00d7m.\nb\u03031 = U\u0302 >P1 = (U\u0302 >O)\u03c0\nb\u0303\u221e = (P > 21U\u0302)P1 = (U\u0302 >O)\u221211m\nB\u0303(x) = (U\u0302>P3x1)(U\u0302 >P21) \u2020 = (U\u0302>O)A(x)(U\u0302>O)\u22121.\nAs long as U\u0302>O is invertible, the above parameters constitute a valid observable representation. This is guaranteed if U\u0302 is sufficiently close to U . We now define the following error terms,\n\u03b4\u221e = \u2016(U\u0302>O)>(\u0302b\u221e \u2212 b\u0303\u221e)\u2016\u221e = \u2016(U\u0302>O)>b\u0302\u221e \u2212 1m\u2016\u221e \u03b41 = \u2016(U\u0302>O)\u22121(B\u0302(x)\u2212 B\u0303(x))(U\u0302>O)\u20161 = \u2016(U\u0302>O)\u22121B\u0302(x)(U\u0302>O)\u2212A(x)\u20161\n\u2206(x) = \u2016(U\u0302>O)\u22121(B\u0302(x)\u2212 B\u0303(x))U\u0302>O\u20161 = \u2016(U\u0302>O)\u22121B\u0302(x)\u2212A(x)\u20161\n\u2206 = \u222b x\u2208[0,1] \u2206(x)dx\nThe next lemma bounds the above quantities in terms of 1, 21, 321.\nLemma 27. Assume 21 < \u03c3m(P21)/3. Then, there exists constants c1, c2, c3, c4 such that, \u03b4\u221e \u2264 c1 \u03c31(O) (\n21 \u03c3m(P21)2 + 1 \u03c3m(P21) ) \u03b41 \u2264 c2\n1 \u03c3m(O)\n\u2206(x) \u2264 c3 \u221a m \u03ba(O)\n( 21\n\u03c3m(P21)2 \u2016P3x1\u20162 + \u2016P3x1 \u2212 P\u03023x1\u20162 \u03c3m(P21)2\n)\n\u2206 \u2264 c4 \u221a m \u03ba(O)\n( 21\n\u03c3m(P21)2 + 321 \u03c3m(P21)2\n)\nProof. We will use .,& to denote inequalities ignoring constants. First we bound \u03b4\u221e \u2264 \u2016(U\u0302>O)>(\u0302b\u221e \u2212 b\u0303\u221e)\u20162 \u2264 \u03c31(O)\u2016b\u0302\u221e \u2212 b\u0303\u221e\u20162. Then we note,\n\u2016b\u0302\u221e \u2212 b\u0303\u221e\u20162 \u2264 \u2016(P\u0302>21U\u0302)\u2020P\u03021 \u2212 (P21U\u0302)\u2020P1\u20162 \u2264 \u2016(P\u0302>21U\u0302)\u2020 \u2212 (P>21U\u0302)\u2020\u20162\u2016P\u03021\u20162 + \u2016(P>21U\u0302)\u2020\u20162\u2016P\u030221 \u2212 P1\u20162 . 21\nmin{\u03c3m(P\u0302>21), \u03c3m(P>21U\u0302)}2 +\n1\n\u03c3m(P>21U\u0302)\n. 21\n\u03c3m(P21)2 + 1 \u03c3m(P21) ,\nwhere the third and fourth steps use Lemma 26 and Lemma 7 (the pseudoinverse theorem for qmatrices). This establishes the first result. The second result is straightforward from Lemma 26.\n\u03b41 \u2264 \u221a m\u2016(U\u0302>O)\u22121\u20162\u2016b\u03021 \u2212 b\u03031\u20162 \u2264 \u221a m \u2016b\u03021 \u2212 b\u03031\u20162 \u03c3m(U\u0302>O) . \u221a m \u2016U\u0302>(P\u03021 \u2212 P1)\u20162 \u03c3m(O) .\n\u221a m 1 \u03c3m(O) .\nFor the third result, we first note\n\u2206(x) \u2264 \u221a m\u2016(U\u0302>O)\u22121\u20162\u2016B\u0302(x)\u2212 B\u0303(x)\u20162\u2016U\u0302>O\u20162 \u2264 \u221a m \u03c31(O)\n\u03c3m(U\u0302>O) \u2016B\u0302(x)\u2212 B\u0303(x)\u20162\n. \u221a m \u03ba(O)\u2016B\u0302(x)\u2212 B\u0303(x)\u20162\nTo bound the last term we decompose it as follows.\n\u2016B\u0302(x)\u2212 B\u0303(x)\u20162 = \u2016(U\u0302>P3x1)(U\u0302>P21)\u2020 \u2212 (U\u0302>P\u03023x1)(U\u0302>P\u030221)\u2020\u20162 \u2264 \u2016(U\u0302>P3x1)((U\u0302>P21)\u2020 \u2212 (U\u0302>P\u030221)\u2020)\u20162 + \u2016U\u0302>(P3x1 \u2212 P\u03023x1)(U\u0302>P\u030221)\u2020\u20162 \u2264 \u2016P3x1\u20162\u2016\u2016(U\u0302>P21)\u2020 \u2212 (U\u0302>P\u030221)\u2020\u20162 + \u2016P3x1 \u2212 P\u03023x1\u20162\u2016(U\u0302>P\u030221)\u2020\u20162\n. \u2016P3x1\u20162 21 \u03c3m(P21)2 + \u2016P3x1 \u2212 P\u03023x1\u20162 \u03c3m(P21) .\nThis proves the third claim. For the last claim, we make use of the proven statements. Observe,\u222b \u2016P3x1\u20162dx \u2264 (\u222b \u2016P3x1\u201622dx )1/2 \u2264 (\u222b \u222b \u222b P321(s, x, t) 2dsdtdx )1/2 = \u2016P321\u2016L2 ,\nwhere the first step uses inclusion of the Lp norms in [0, 1]. The second step uses \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u2016F for cmatrices. A similar argument shows \u222b x \u2016P3x1 \u2212 P\u03023x1\u20162 \u2264 321. Combining these results gives the fourth claim.\nFinally, we need the following Lemma. The proof almost exactly replicates the proof of Lemma 12 in Hsu et al. [2], as all operations can be done with just matrices.\nLemma 28. Assume 321 \u2264 \u03c3m(P21)/3. Then \u2200t \u2265 0,\u222b |p(x1:t)\u2212 p\u0302(x1:t)|dx1:t \u2264 \u03b4\u221e + (1 + \u03b4\u221e) ( (1 + \u2206)t\u03b41 + (1 + \u2206) t \u2212 1 ) , (12)\nwhere the integral is over [0, 1]t.\nWe are now ready to prove Theorem 5.\nProof of Theorem 5. If 1, 21, 321 satisfy the following for appropriate choices of c5, c6, c7,\n1 \u2264 c5 min(\u03c3m(P21), \u03ba(O)\u221a m ) , 21 \u2264 c6 \u03c3m(P21)\n2\n\u03ba(O) , 321 \u2264 c7\n\u03c3m(P21)\n\u03c31(O)\n1\nt \u221a m , (13)\nwe then have \u03b41 \u2264 /20, \u03b4\u221e \u2264 /20 and \u2206 \u2264 0.4 /t. Plugging these expressions into Lemma 28 gives \u222b |p(x1:t) \u2212 p\u0302(x1:t)|dx1:t \u2264 . When we plug the expresssions for 1, 21, 321 in (13) into Corollary 25 we get the required sample complexity."}, {"heading": "E Addendum to Experiments", "text": "Details on Synthetic Experiments: Figure 3 shows the emission probabilities used in our synthetic experiments. For the transition matrices, we sampled the entries of the matrix from a U(0, 1) distribution and then renormalised the columns to sum to 1.\nIn our implementation, we use a Gaussian kernel for the KDE which is of order \u03b2 = 2. While higher order kernels can be constructed using Legendre polynomials [17], the Gaussian kernel was more robust in practice. The bandwidth for the kernel was chosen via cross validation on density estimation.\nDetails on Real Datasets: Here, we first estimate the model parameters using the training sequence. Given a test sequence x1:n, we predict xt+1 conditioned on the previous x1:t for t = 1 : n.\n1. Internet Traffic. Training sequence length: 10, 000. Test sequence length: 10. 2. Laser Generation. Training sequence length: 10, 000. Test sequence length: 100. 3. Physiological data. Training sequence length: 15, 000. Test sequence length: 100."}], "references": [{"title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "author": ["Lawrence R. Rabiner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "A Spectral Algorithm for Learning Hidden Markov Models", "author": ["Daniel J. Hsu", "Sham M. Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "A Method of Moments for Mixture Models and Hidden Markov Models", "author": ["Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Reduced-Rank Hidden Markov Models", "author": ["Sajid M. Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Continuous analogues of matrix factorizations", "author": ["Alex Townsend", "Lloyd N Trefethen"], "venue": "In Proc. R. Soc. A,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Computing with Functions in Two Dimensions", "author": ["Alex Townsend"], "venue": "PhD thesis, University of Oxford,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "An extension of chebfun to two dimensions", "author": ["Townsend", "Alex", "Trefethen", "Lloyd N"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Predictive representations of state", "author": ["Michael L Littman", "Richard S Sutton", "Satinder P Singh"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Maximum likelihood from incomplete data via the EM algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "SERIES B,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "Hidden Markov models and the Baum-Welch algorithm", "author": ["Lloyd R Welch"], "venue": "IEEE Information Theory Society Newsletter,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "An EM-like algorithm for semi-and nonparametric estimation in multivariate mixtures", "author": ["Tatiana Benaglia", "Didier Chauveau", "David R Hunter"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["Le Song", "Byron Boots", "Sajid M Siddiqi", "Geoffrey J Gordon", "Alex Smola"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Nonparametric Estimation of Multi-View Latent Variable Models", "author": ["Le Song", "Animashree Anandkumar", "Bo Dai", "Bo Xie"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Introduction to Nonparametric Estimation", "author": ["Alexandre B. Tsybakov"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Chebyshev polynomials in numerical analysis", "author": ["L. Fox", "I.B. Parker"], "venue": "Oxford U.P. cop.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1968}, {"title": "Approximation Theory and Approximation Practice", "author": ["Lloyd N. Trefethen"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Estimation of integral functionals of a density", "author": ["Lucien Birg\u00e9", "Pascal Massart"], "venue": "Ann. of Stat.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Nonparametric Von Mises Estimators for Entropies, Divergences and Mutual Informations", "author": ["Kirthevasan Kandasamy", "Akshay Krishnamurthy", "Barnab\u00e1s P\u00f3czos", "Larry Wasserman", "James Robins"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Weyl\u2019s theorem for operator matrices", "author": ["Woo Young Lee"], "venue": "Integral Equations and Operator Theory,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Forest Density Estimation", "author": ["Han Liu", "Min Xu", "Haijie Gu", "Anupam Gupta", "John D. Lafferty", "Larry A. Wasserman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Rates of strong uniform consistency for multivariate kernel density estimators", "author": ["Evarist Gin\u00e9", "Armelle Guillou"], "venue": "In Annales de l\u2019IHP Probabilite\u0301s et statistiques,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji-guang Sun"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "Wide area traffic: the failure of Poisson modeling", "author": ["Vern Paxson", "Sally Floyd"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared NH 3 laser", "author": ["U H\u00fcbner", "NB Abraham", "CO Weiss"], "venue": "Physical Review A,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Chebfun to three dimensions", "author": ["B. Hashemi", "L.N. Trefethen"], "venue": "In preparation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Predictive State Representations: A New Theory for Modeling Dynamical Systems", "author": ["Satinder Singh", "Michael R. James", "Matthew R. Rudary"], "venue": "In UAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Hidden Markov models (HMMs) [1] are one of the most popular statistical models for analyzing time series data in various application domains such as speech recognition, medicine, and meteorology.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134].", "startOffset": 178, "endOffset": 183}, {"referenceID": 2, "context": "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134].", "startOffset": 178, "endOffset": 183}, {"referenceID": 3, "context": "Recently, spectral methods for estimating parametric latent variable models have gained immense popularity as a viable alternative to the Expectation Maximisation (EM) procedure [2\u20134].", "startOffset": 178, "endOffset": 183}, {"referenceID": 1, "context": "In the case of discrete HMMs [2], these moments correspond exactly to the joint probabilities of the observations in the sequence.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices.", "startOffset": 71, "endOffset": 77}, {"referenceID": 5, "context": "Our methods leverage some recent advances in continuous linear algebra [5, 6] which views two-dimensional functions as continuous analogues of matrices.", "startOffset": 71, "endOffset": 77}, {"referenceID": 6, "context": "Chebyshev polynomial approximations enable efficient computation of algebraic operations on these continuous objects [7, 8].", "startOffset": 117, "endOffset": 123}, {"referenceID": 7, "context": "While we focus on HMMs in this exposition, we believe that the ideas presented in this paper can be easily generalised to estimating other latent variable models and predictive state representations [9] with nonparametric observations using approaches developed by Anandkumar et al.", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Related Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "Related Work: Parametric HMMs are usually estimated using maximum likelihood principle via EM techniques [10] such as the Baum-Welch procedure [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "[2] who showed that discrete HMMs can be learned efficiently, under certain conditions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] show that the same algorithm works under slightly more general assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] proposed a spectral algorithm for estimating more general latent variable models with parametric observations via a moment matching technique.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "A commonly used heuristic is the nonparametric EM [12] which lacks theoretical underpinnings.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "[4] proposed a heuristic based on kernel smoothing, with no theoretical justification, to modify the discrete algorithm for continuous observations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[14, 15] developed an RKHS-based procedure for estimating the Hilbert space embedding of an HMM.", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Both sections are based on [5, 6].", "startOffset": 27, "endOffset": 33}, {"referenceID": 5, "context": "Both sections are based on [5, 6].", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "Without loss of generality, we take2 X = [0, 1].", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1mA(xt:1)\u03c0.", "startOffset": 17, "endOffset": 24}, {"referenceID": 13, "context": "It is well known [2, 16] that the joint probability density of the sequence x1:t can be computed via p(x1:t) = 1mA(xt:1)\u03c0.", "startOffset": 17, "endOffset": 24}, {"referenceID": 13, "context": "Observable Representation: The observable representation is a description of an HMM in terms of quantities that depend on the observations [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "We will find it useful to view both P21, P3x1 \u2208 R[0,1]\u00d7[0,1] as cmatrices.", "startOffset": 49, "endOffset": 54}, {"referenceID": 0, "context": "We will find it useful to view both P21, P3x1 \u2208 R[0,1]\u00d7[0,1] as cmatrices.", "startOffset": 55, "endOffset": 60}, {"referenceID": 0, "context": "We will also need an additional qmatrix U \u2208 R[0,1]\u00d7m such that U>O \u2208 Rm\u00d7m is invertible.", "startOffset": 45, "endOffset": 50}, {"referenceID": 0, "context": "Given one such U , the observable representation of an HMM is described by the parameters b1, b\u221e \u2208 R and B : [0, 1]\u2192 Rm\u00d7m, b1 = U P1, b\u221e = (P > 21U) P1, B(x) = (U P3x1)(U P21) \u2020 (1) As before, for a sequence, xt:1 = {xt, .", "startOffset": 109, "endOffset": 115}, {"referenceID": 0, "context": "B(x) = (U>O)A(x)(U>O)\u22121 \u2200x \u2208 [0, 1].", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "While there are several techniques [17], we use kernel density estimation (KDE) since it is easy to analyse and works well in practice.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "Here K : [0, 1] \u2192 R is a symmetric function called a smoothing kernel and satisfies (at the very least) \u222b 1 0 K(s)ds = 1, \u222b 1 0 sK(s)ds = 0.", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "Let \u00db \u2208 R[0,1]\u00d7m be the first m left singular vectors of P\u030221.", "startOffset": 9, "endOffset": 14}, {"referenceID": 1, "context": "[2], the SVD, pseudoinverses and multiplications are with q/c-matrices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19].", "startOffset": 149, "endOffset": 157}, {"referenceID": 16, "context": "Chebyshev polynomials is a family of orthogonal polynomials on compact intervals, known to be an excellent approximator of one-dimensional functions [18, 19].", "startOffset": 149, "endOffset": 157}, {"referenceID": 4, "context": "A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.", "startOffset": 22, "endOffset": 28}, {"referenceID": 6, "context": "A recent line of work [5, 8] has extended the Chebyshev technology to two dimensional functions enabling the mentioned operations and factorisations such as QR, LU and SVD [6, Sections 4.", "startOffset": 22, "endOffset": 28}, {"referenceID": 1, "context": "Following [2, 4, 14] we assume i.", "startOffset": 10, "endOffset": 20}, {"referenceID": 3, "context": "Following [2, 4, 14] we assume i.", "startOffset": 10, "endOffset": 20}, {"referenceID": 11, "context": "Following [2, 4, 14] we assume i.", "startOffset": 10, "endOffset": 20}, {"referenceID": 0, "context": "T \u2208 Rm\u00d7m and O \u2208 R[0,1]\u00d7m are of rank m.", "startOffset": 18, "endOffset": 23}, {"referenceID": 0, "context": "for all \u03b1 \u2264 b\u03b2c, j \u2208 [m], s, t \u2208 [0, 1] \u2223\u2223\u2223\u2223d\u03b1Oj(s) ds\u03b1 \u2212 dOj(t) dt\u03b1 \u2223\u2223\u2223\u2223 \u2264 L|s\u2212 t|\u03b2\u2212|\u03b1|.", "startOffset": 33, "endOffset": 39}, {"referenceID": 3, "context": "[4] show that the discrete spectral algorithm works under a slightly more general setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], whose sample complexity bound4 is N & m \u03ba(O) 2", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20\u201322], we think our bound might be unimprovable.", "startOffset": 120, "endOffset": 127}, {"referenceID": 18, "context": "While we do not have any lower bounds, given the current understanding of the difficulty of various nonparametric tasks [20\u201322], we think our bound might be unimprovable.", "startOffset": 120, "endOffset": 127}, {"referenceID": 0, "context": "This is due to the fact that we want the KDE to concentrate around its expectation in L over [0, 1], instead of just point-wise.", "startOffset": 93, "endOffset": 99}, {"referenceID": 19, "context": "Weyl\u2019s theorem has been studied for general operators [23] and cmatrices [6].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Weyl\u2019s theorem has been studied for general operators [23] and cmatrices [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Let A, \u00c3, E \u2208 R[0,1]\u00d7[0,1] where \u00c3 = A+ E and rank(A) = m.", "startOffset": 15, "endOffset": 20}, {"referenceID": 0, "context": "Let A, \u00c3, E \u2208 R[0,1]\u00d7[0,1] where \u00c3 = A+ E and rank(A) = m.", "startOffset": 21, "endOffset": 26}, {"referenceID": 14, "context": "Such kernels can be constructed using Legendre polynomials [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "[2] provide a more refined bound but we use this form to simplify the comparison.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "This slow convergence is also observed in similar concentration bounds for the KDE [24, 25].", "startOffset": 83, "endOffset": 91}, {"referenceID": 22, "context": "A note on the Proofs: For Lemmas 6, 7 we follow the matrix proof in Stewart and Sun [26] and derive several intermediate results for c/q-matrices in the process.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "The main challenge with the KDE concentration result is that we want an L bound \u2013 so usual techniques (such as McDiarmid\u2019s [13, 17]) do not apply.", "startOffset": 123, "endOffset": 131}, {"referenceID": 21, "context": "We use a technical lemma from Gin\u00e9 and Guillou [25] which allows us to bound the L error in terms of the VC characteristics of the class of functions induced by an i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "NP-HMM-BIN: A naive baseline where we bin the space into n intervals and use the discrete spectral algorithm [2] with n states.", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "NP-HMM-EM: The Nonparametric EM heuristic of [12].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "NP-HMM-HSE: The Hilbert space embedding method of [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "We could not include the method of [4] in our comparisons since their code was not available and their method isn\u2019t straightforward to implement.", "startOffset": 35, "endOffset": 38}, {"referenceID": 23, "context": "Real Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29].", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "Real Datasets: We compare all the above methods (except NP-HMM-EM which was too slow) on prediction error on 3 real datasets: internet traffic [27], laser generation [28] and sleep data [29].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "That said, some recent advances in this direction are promising [8, 30].", "startOffset": 64, "endOffset": 71}, {"referenceID": 25, "context": "That said, some recent advances in this direction are promising [8, 30].", "startOffset": 64, "endOffset": 71}, {"referenceID": 26, "context": "Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas.", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Recent advances in spectral methods for estimating parametric predictive state representations [31], mixture models [3] and other latent variable models [32] can be generalised to the nonparamatric setting using our ideas.", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an m-state hidden Markov model (HMM) with only smoothness assumptions, such as H\u00f6lderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as continuous matrices. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.", "creator": "LaTeX with hyperref package"}}}