{"id": "1609.07075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs", "abstract": "objective information. considered vital significant supplement to universal representation learning ( krl ). there are two main challenges for constructing knowledge representations from plain texts : ( 1 ) how to take full advantages of sequential contexts of entities in plain articles for krl. ( 2 ) how to dynamically select those informative content of clearly corresponding entities for krl. in both paper, we propose the sequential text - embodied knowledge representation learning to build knowledge representations from binary sentences. given each reference sentence of an entity, we first utilize recurrent neural network hierarchical pooling or long short - term memory network to encode the semantic content of the sentence with respect to the entity. then we further design an attention matrix to measure the informativeness of each representation, and build text - based sentences of entities. we evaluate our methodology on two tasks, including triple classification and link prediction. experimental results indicating generally our tool outperforms other baselines on both tasks, which indicates that our method is capable uniquely selecting informative sentences and encoding the textual information well into arbitrary domains.", "histories": [["v1", "Thu, 22 Sep 2016 17:16:43 GMT  (145kb,D)", "http://arxiv.org/abs/1609.07075v1", "10 pages, 3 figures"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiawei wu", "ruobing xie", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1609.07075"}, "pdf": {"name": "1609.07075.pdf", "metadata": {"source": "CRF", "title": "Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs", "authors": ["Jiawei Wu", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "emails": ["(liuzy@tsinghua.edu.cn)."], "sections": [{"heading": "1 Introduction", "text": "Knowledge graphs (KGs), which provide significant well-structured information for modeling abstract concepts as well as concrete entities in real world, have attracted great attention in recent years.\n\u2217Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn).\nA typical knowledge graph usually arranges multirelational data in the form of triple facts (head entity, relation, tail entity) that is abridged as (h, r, t).\nThere are large numbers of KGs like Freebase, YAGO and DBpedia that are widely utilized in nature language processing applications such as question answering and web search (Bollacker et al., 2008). However, applications for KGs are suffering from challenges of data sparsity and computational inefficiency with KG size increasing. To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).\nMany methods have introduced representation learning to KGs, projecting both entities and relations into a continuous low-dimensional vector space (Nickel et al., 2011; Jenatton et al., 2012; Bordes et al., 2013). Among existing methods, translation-based models, which interpret relations as translating operations between head and tail entities, are effective and efficient that possess the stateof-the-art performance.\nar X\niv :1\n60 9.\n07 07\n5v 1\n[ cs\n.C L\n] 2\n2 Se\np 20\nFurthermore, rich external information is considered as supplement to triple facts that helps to represent knowledge graphs, and textual information has shown significant contribution to this goal. (Wang et al., 2014a; Zhong et al., 2015) propose a joint model projecting both entities and words into the same vector space with alignment models. However, their models only consider bag-of-words assumption when modeling words in plain texts, neglecting rich textual information embedded in word order. (Xie et al., 2016) directly builds entity representations from entity descriptions, while their model is restricted by the completeness and quality of entity descriptions.\nThere are two main shortages in existing methods for constructing knowledge representations from multiple sentences in plain texts: (1) The bag-ofwords assumption fails to encode explicit word order information into sentence representations. (2) Not all sentences containing entity names are reliable and suitable for explaining the corresponding entities. Fig. 1 demonstrates an example of multiple reference sentences for the entity economics. We can observe that the first two sentences talk about the definition and attributes of economics, which could represent the entity well, while the third one provides rather confusing and meaningless information for understanding the meaning of economics.\nAs shown in (Nagy et al., 1987), humans learn meanings of new words from their contextual information. Inspired by this, we intend to infer the meaning of an entity from its reference sentences. To overcome the shortages mentioned above, we propose the Sequential Text-embodied Knowledge Representation Learning (STKRL). Specifically, in our method, we first utilize recurrent neural network (RNN) with pooling or long short-term memory network (LSTM) to build sentence-level representations of an entity from multiple sentences. Each sentence-level representation is considered as a candidate of the corresponding entity representation. Second, we combine these sentence-level representations to form the summary text-based representation, with the favor of attention which highlights more informative sentences. Finally, we follow the margin-based score function in translationbased methods as our objective for training.\nTo the best of our knowledge, our model is the first model which combines multi-instance learning\nwith attention in knowledge representation learning with texts. We evaluate our models on triple classification and link prediction, and the significantly improved experimental results indicate our model is capable of representing knowledge graph better with textual information. Meanwhile, our model is of potential usefulness to definition extraction according to inspection towards the sentence attention component in our model."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Translation-based Methods", "text": "Translation-based methods have achieved great successes for representation learning in knowledge graphs. TransE (Bordes et al., 2013) interprets relations as translating operations between head and tail entities, and projects both entities and relations into the same continuous low-dimensional vector space. The energy function is defined as follows:\nE(h, r, t) = ||h+ r\u2212 t||, (1)\nwhich assumes that the tail embedding t should be in the neighborhood of h+ r. TransE is straightforward and effective, while this simple translating operation may have issues when modeling 1-to-N, N-to-1 and N-to-N relations. Moreover, the translating operation only focuses on a single step, regardless of rich information located in long-distance relational paths. To address the first problem, TransH (Wang et al., 2014b) models translating operations between entities on relation-specific hyperplanes. TransR (Lin et al., 2015b) interprets entities and relations in different semantic spaces, and projects entities from entity space to relation space when learning the potential relationship between entities. TransD (Ji et al., 2015) proposes dynamic mapping matrix constructed by both entities and relations for multiple representations of entities. To extend the single-step translating operation, (Gu et al., 2015; Lin et al., 2015a) encode multiple-step relation paths into representation learning of knowledge graphs and achieve significant improvements."}, {"heading": "2.2 Representation Learning of Knowledge Graphs with Textual Information", "text": "Multi-source information, especially textual information, is significant in representation learning of\nknowledge graphs, which has attracted great attention recently. It can provide useful information from different aspects, which helps modeling knowledge graphs. (Wang et al., 2014a) encodes both entities and words into a joint low-dimensional vector space by alignment models using entity names as well as Wikipedia anchors. (Zhong et al., 2015) extends the alignment model by considering entity descriptions. However, the methods of modeling plain texts in the both models above are rather simple, ignoring significant information encoded in word order. (Xie et al., 2016) proposes a new kind of representation for entities, which is directly constructed from entity descriptions using CNN and thus is capable of modeling new entities. However, their descriptionbased representation is restricted by the completeness as well as quality of entity descriptions. To the best of our knowledge, our model is the first effort which learns knowledge representations from multiple sentences extracted from noisy plain texts with word order being considered."}, {"heading": "2.3 Multi-instance Learning", "text": "Multi-instance learning, which was originally proposed in (Dietterich et al., 1997), arises in the tasks where a single object may possess multiple alternative examples or representations that describe it. Multi-instance learning aims to find out the reliability of examples in each object. (Bunescu and Mooney, 2007) extends a relation extraction method using minimal supervision with multi-instance learning. (Zhou et al., 2012) proposes multi-instance multi-label learning on multiple tasks such as scene classification and text categorization. (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction. (Zeng et al., 2015) further combines multiinstance learning with convolutional neural network for relation extraction on distant supervision data. To fully utilize the rich textual information located in multiple sentences of each entity, we propose a sentence-level attention over multiple sentences to highlight more informative instances. To the best of our knowledge, multi-instance learning combined with attention-based model hasn\u2019t been adopted in representation learning of knowledge graphs."}, {"heading": "3 Methodology", "text": "We first describe the notations used in this paper. For any triple (h, r, t) \u2208 T , it consists of two entities h, t \u2208 E and a relation r \u2208 R. E stands for the set of entities while R stands for the set of relations. T is the training set of triples, and the embeddings of entities and relations take values in Rk.\nWe have two representations for each entity. We set hK , tK as the structure-based representations of head and tail entities, which are the same as those in previous knowledge models, and hS , tS as the text-based representations, which are learned from plain texts by sentence encoders.\nFor each entity, we first scan through the corpus to extract all sentences which contain the corresponding entity name. These sentences are considered as the reference sentences of the corresponding entity."}, {"heading": "3.1 Overall Architecture", "text": "First, we introduce the overall architecture of the STKRL model. Inspired by translation-based methods, we define the energy function as follows:\nE(h, r, t) = EKK + ESS + ESK + EKS , (2)\nwhere EKK is the same energy function as TransE, ESS , ESK and EKS are new-added parts determined by the two representations jointly. ESS = \u2016hS + r \u2212 tS\u2016 in which both head and tail are text-based representations learned from reference sentences. Similarly, we also have ESK = \u2016hS + r\u2212 tK\u2016 and EKS = \u2016hK + r\u2212 tS\u2016, jointly considering the two types of entity representations.\nThe overall architecture of the STKRL model is demonstrated in Fig. 2. First, The RNN/LSTM sentence encoder takes reference sentences as inputs and learns the sentence-level representations. Second, an attention method is implemented to select the top m (m is a hyper parameter) informative sentences to generate the text-based representation of an entity. The attention is based on the semantic similarity between the sentence-level representations and the corresponding structure-based representation. Finally, those representations will be learned jointly under the translation-based method."}, {"heading": "3.2 Word Representations", "text": "In our framework, we consider the embeddings of each word token in reference sentences as inputs,\nand each entity name is also regarded as a word. Inspired by (Zeng et al., 2014), the word representations consist of two parts, including word features and position features."}, {"heading": "3.2.1 Word Features", "text": "The word features could be learned by negative sampling Skip-gram models, for these models could encode contextual information located in large corpus. The learned word embeddings are then directly considered as word features."}, {"heading": "3.2.2 Position Features", "text": "Word order information is significant that it can help us to better understand sentences, and we also intend to highlight the position of the entity name in its reference sentences. Suppose each sentence is represented as a sequence s = (x1, x2, \u00b7 \u00b7 \u00b7 , xn), where xi represents the i-th word. Given a sentence, the position feature of its entity name will be marked as 0, and the positions of other words are marked according to the relevant integer distance towards the entity name. The left words have negative position values, while the right have positive position values. The position features will be marked as \u2212d or d if their relevant distances are larger than d (d is a hyper parameter)."}, {"heading": "3.3 Sentence Encoder", "text": "We assume that the meaning of an entity could be extracted from its reference sentences. There are amounts of algorithms to represent sentence information with word order in consideration, such as re-\ncurrent neural network (RNN) and long short-term memory (LSTM). These models have been widely used in several natural language processing tasks such as machine translation. We adopt RNN with pooling and LSTM as sentence encoders to learn sentence-level representations of entities, intending to extract entity meanings from reference sentences."}, {"heading": "3.3.1 Recurrent Neural Network", "text": "Recurrent neural network takes a reference sentence as input. It works on a sequence and maintains a hidden state h over time. At each time-step t, the hidden state vector ht is updated as follows:\nht = tanh(Wxt +Uht\u22121 + b), (3)\nin which transition function is an affine transfor-\nmation followed by a non-linear function such as hyperbolic tangent. More specifically, RNN reads each word representation of the input sentence sequentially. While reading each word representation, the hidden state of the RNN changes according to Eq. (3). After finishing the whole sequence marked by an end-of-sequence symbol, we can obtain the final hidden state vector hn as the output. hn is then viewed as the sentence-level representation."}, {"heading": "3.3.2 RNN with Pooling", "text": "Recurrent neural network is powerful and widely used in various fields, while it usually suffers from gradient vanishment. Therefore, it is difficult for the final hidden state of RNN to capture the early local information when the sentence is too long. (Collobert et al., 2011) proposes a mean-pooling approach to solve gradient vanishment to some degree. To leverage efficiency and effectiveness, we add a mean-pooling layer to encode the overall information of a sentence into its sentence-level representation c. We have:\nc = \u2211\ni=1,\u00b7\u00b7\u00b7 ,n\nhi n , (4)\nin which all intermediate hidden states containing different local information should have contribution to the final sentence-level representation, and thus could be updated during back propagation. The structure is shown in Fig. 3."}, {"heading": "3.3.3 Long Short-Term Memory Network", "text": "LSTM (Hochreiter and Schmidhuber, 1997) is an enhanced neural network based on RNN, which could address the gradient vanishment when learning long-term dependencies. LSTM introduces memory cells that are able to preserve state over long periods of time. At each time step t, the LSTM unit is composed of: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. The entries of the gating vectors it, ft and ot are in [0, 1]. The LSTM transition equations are the following:\nit = \u03c3(W (i)xt +U (i)ht\u22121 + b (i)), (5)\nft = \u03c3(W (f)xt +U (f)ht\u22121 + b (f)), (6)\not = \u03c3(W (o)xt +U (o)ht\u22121 + b (o)), (7)\nut = tanh(W (u)xt +U (u)ht\u22121 + b (u)), (8)\nct = it ut + ft ct\u22121, (9) ht = ot tanh(ct), (10)\nwhere xt is the input at the current time step t, \u03c3 denotes the logistic sigmoid function and denotes the elementwise multiplication."}, {"heading": "3.4 Attention over Reference Sentences", "text": "We utilize sentence encoders to build sentence-level representations from reference sentences, next we want to integrate those sentence-level representations to the text-based representation for each entity. Simply considering the mean or max of sentence embeddings will suffer from noises or lost rich information. Instead of taking the mean/max sentence embeddings, we propose an attention method to automatically select sentences which can explicitly explain the meaning of the entity. The attention-based model is powerful and has been widely applied to machine translation (Bahdanau et al., 2015), abstractive sentence summarization (Rush et al., 2015) and speech recognition (Chorowski et al., 2014). It can automatically highlight more informative instances from multiple candidates.\nWe implement an attention-based multi-instance learning method to select top-m informative reference sentences for the corresponding entity from all candidates. More specifically, each entity e has a structure-based representation eK . For a sentencelevel representation c which belongs to entity e, the attention between c and eK is as follows:\natt(c, eK) = c \u00b7 eK\n\u2016c\u2016 \u00b7 \u2016eK\u2016 . (11)\nReference sentences with higher att(c, eK) are expected to better represent their corresponding entity information more explicitly. Finally, we pick the top-m reference sentences, and the text-based representation s is as follows:\ns = m\u2211 i=1 att(ci, eK) \u00b7 ci\u2211 i=1,\u00b7\u00b7\u00b7 ,m att(ci, eK) . (12)"}, {"heading": "3.5 Objective Formalization", "text": "We utilize a margin-based score function as our training objective. The overall score function is de-\nfined as follows: L = \u2211\n(h,r,t)\u2208T \u2211 (h\u2032,r,t\u2032)\u2208T \u2032 max(\u03b3 + E(h, r, t)\n\u2212 E(h\u2032, r, t\u2032), 0), (13)\nwhere \u03b3 is a margin hyper parameter. E(h, r, t) is the energy function stated above, which can be either L1 or L2-norm. T \u2032 is the negative sampling set of T , which is defined as\nT \u2032 = {(h\u2032, r, t) | h\u2032 \u2208 E}\u222a{(h, r, t\u2032) | t\u2032 \u2208 E}, (14)\nin which the head and tail entities are randomly replaced by another entity. Note that for each entity, there are two types of entity representations, including the text-based representation and the structurebased representation. Hence we can learn these two types of entity representations simultaneously into the same vector space.\nThe energy function (2) is used to learn structurebased and text-based representations into the same vector space, and the single EKK item in (2) is the energy function of TransE. Thus, as mentioned above, we can train our model using the objective formalization (13)."}, {"heading": "3.6 Model Initialization and Optimization", "text": "The STKRL model can be initialized either randomly or with pre-trained TransE embeddings. The word representations are pre-trained through Word2Vec with Wikipedia corpus. The optimization is a standard back propagation using mini-batch stochastic gradient descent (SGD). For efficiency, we also use GPU to accelerate the training process. Note that there is only one RNN/LSTM, which means that all RNN/LSTM in the figure shares the same parameters, and this RNN/LSTM is trained to encode reference sentences for all entities.\nOur model consists of two parts: (1) textbased RNN/LSTM with attention for encoding sentences, and (2) structure-based TransE (Bordes et al., 2013)for encoding knowledge. We train the two parts simultaneously. The training process is described as follows:\n\u2022 Given a triple, learn structure-based and textbased knowledge representations using extended TransE with negative sampling. Note\nthat, there are four terms for each triple, (hK , r, tK), (hS , r, tK), (hK , r, tS) and (hS , r, tS), with four terms of negative samples respectively.\n\u2022 Utilize RNN/LSTM to encode all reference sentences of head and tail entities.\n\u2022 Attention scheme is applied to dynamically select top-m informative sentences of head and tail from all encoded sentences and construct text-based representations respectively.\n\u2022 For selected m sentences, the structure-based representation of head, hK , (respectively, tail, tK) is used to compute training error and optimize RNN/LSTM with back-propagation."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Datasets", "text": "We adopt FB15K to evaluate our models on triple classification and link predication in this paper. FB15K is a subset of Freebase that has 14,951 entities and 1,345 relations. For each entity, we use Wikipedia as the corpus to extract reference sentences. Note that the Wikipedia article about the entity itself has the highest priority to provide reference sentences. Each entity has approximately 40 sentences. The statistics of the FB15K dataset are listed in Table 1."}, {"heading": "4.2 Parameter Settings", "text": "The detailed parameters will be released upon acceptance.\nFor STKRL, we implement 4 sentence encoders for evaluation. \u201cRNN w/o ATT\u201d represents selecting RNN as sentence encoder and using the mean vector of sentence-level representations instead of attention, while \u201cRNN+ATT\u201d, \u201cRNN+P+ATT\u201d and \u201cLSTM+ATT\u201d represent selecting the corresponding sentence encoders of RNN, RNN+pooling and LSTM with the help of attention.\nWe implement TransE and jointly (name) model in (Wang et al., 2014a) as baselines, following their\nexperimental settings. For fair comparisons, all baselines have the same dimension of entities and relations."}, {"heading": "4.3 Triple Classification", "text": "The task of triple classification aims to test whether a triple (h, r, t) is true or false, which could be viewed as a binary classification test."}, {"heading": "4.3.1 Evaluation Protocol", "text": "Since each entity has two types of representations, a triple (h, r, t) has four representations, (hK , r, tK), (hS , r, tS), (hS , r, tK) and (hK , r, tS). Following the similar protocol in (Socher et al., 2013), for each triple representation, we construct a negative example by randomly replace the head or tail entity with another entity. The new entity\u2019s representation should be the same type as the replaced one (e.g. (hK , r, tK) should be replaced with (h\u2032K , r, tK) or (hK , r, t \u2032 K)). And we also ensure that the number of true triples is equal to false ones. The evaluation strategy is described as follows: if the dissimilarity of a testing triple (h, r, t) is below the relation-specific threshold \u03b4r, it is predicted to be positive, otherwise negative. The threshold \u03b4r can be determined via maximizing the classification accuracy on the validation set."}, {"heading": "4.3.2 Results", "text": "The results of triple classification are shown in Table 3. From the results, we observe that: (1) Our attention-based models significantly outperform all baselines, which indicates the capability of our methods in modeling knowledge representations. (2) STKRL (RNN w/o ATT) outperforms TransE, which implies the significance of textual information. Moreover, the attention-based STKRL models outperform Wang\u2019s method which also encodes tex-\ntual information into knowledge representations. It confirms that our sentence encoders can better understand the meaning of sentences, and thus get better performances. (3) Attention is the key component in our model. It can automatically select the more informative reference sentences to represent an entity, alleviating the noises caused by low-quality reference sentences. (4) STKRL (LSTM+ATT) achieves the best performance, successfully capturing the rich information in long-distance dependencies. Besides, STKRL (RNN+P+ATT) outperforms STKRL (RNN+ATT), which also demonstrates the advantages of pooling strategy."}, {"heading": "4.4 Link Prediction", "text": "The task of link prediction aims to complete a triple fact (h, r, t) when h or t is missing."}, {"heading": "4.4.1 Evaluation Protocol", "text": "For each test triple, the head is replaced by each of the entities of the entity set in turn. Dissimilarities of those corrupted triples are first computed by the models and then sorted by ascending order. The rank of the correct entity is finally stored. This whole procedure is repeated while removing the tail instead of the head, and the same rule applies to the tail. Note that in Wang\u2019s and our model, each entity has two representations. As a result, the predicted rank for a certain entity is the mean of two representations.\nHowever, as for Hits@10 test, either of two representations appearing at top 10 could be viewed as a successful hit."}, {"heading": "4.4.2 Results", "text": "From Table 6, we can observe that: (1) Our model significantly outperforms all baselines in both Mean Rank and Hits@10, which demonstrates the effectiveness and robustness of our model. (2) STKRL (RNN+P+ATT) and STKRL (LSTM+ATT) significantly outperform Wang\u2019s method. It is because that we utilize sentence encoders to model textual information with the help of word orders, instead of simply considering separate words used in Wang\u2019s method. Moreover, the attention-based STKRL models could pay more attention to those informative reference sentences when constructing textbased representations. (3) STKRL (LSTM+ATT) achieves the best performances, which indicates the power in utilizing better sentence encoders.\nTable 2 demonstrates the link prediction results with different categories of relations. Relations are\nsplit into 4 categories following the same settings in (Bordes et al., 2013). From Table 2 we can observe that: (1) the STKRL model achieves great improvements consistently on all categories of relations. (2) The improvements locate more in N-to-N relations. It indicates that textual information is important when modeling complex relations, and our STKRL models could well capture the useful textual information for better knowledge representations."}, {"heading": "4.5 Case Study", "text": "In this section, we give two cases and analysis to prove that we can successfully select informative reference sentences to represent entities."}, {"heading": "4.5.1 Effectiveness of Attention", "text": "To show that the attention has reasonably ranked reference sentences, we give 4 examples of the entity economics with their ranks. The results in Table 4 indicate that STKRL can successfully select the more informative sentences of the corresponding entity. Those top-ranked sentences will usually be the definitions or descriptions of entities. Besides, the low-ranked sentences are usually less relevant to the targeted entity, which also indicates the capability of attention in filtering noises."}, {"heading": "4.5.2 Definition Extraction", "text": "Definition extraction is an important task in text mining (Espinosa-Anke et al., 2015). We want to demonstrate that the top-ranked reference sentences selected by our model are usually definitions of the corresponding entities.\nBy randomly selecting 100 entities from the entity set E, we find that 61 entities\u2019 No.1 reference sentences are entity definitions based on manual annotation. In Table 5 we give 4 No.1 reference sentence examples. The first three sentences are exactly entity definitions, while the last is not. However, all sentences are still informative to learn the meanings of entities. The results demonstrate that our model has rational selectivity to extract reference sentences and is of potential usefulness to definition extraction."}, {"heading": "5 Conclusion", "text": "In this paper, we propose the STKRL model, a novel model for knowledge representation learning, which jointly considers triple facts and sequential text information. We also explore how to extract informative sentences from lots of candidates with attentionbased method. We evaluate our models on two tasks, triple classification and link prediction, and also give some examples to prove the ability to efficiently extract useful information. Experimental results show that our models achieve significant improvements compared to other baselines. The code and dataset will be released upon acceptance.\nWe will explore the following further work: (1) We assume that for entity representations, there are naturally three types of representations including word, sentence and knowledge representations. We mainly use knowledge and sentence representations in the STKRL model, and we will explore to integrate all three representations in future. (2) The STKRL model can extract definition sentences of entities as by-products. In future, we will further explore this issue by designing a more sophisticated method for definition extraction."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Yakhnenko."], "venue": "Proceedings of NIPS, pages 2787\u20132795.", "citeRegEx": "Yakhnenko.,? 2013", "shortCiteRegEx": "Yakhnenko.", "year": 2013}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] Razvan Bunescu", "Raymond Mooney"], "venue": "In Proceedings of ACL", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: first results", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez"], "venue": "Artificial intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Dong et al.2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Weakly supervised definition extraction", "author": ["Espinosa-Anke", "Francesco Ronzano", "Horacio Saggion."], "venue": "Proceedings of Recent Advances in Natural Language Processing, page 176.", "citeRegEx": "Espinosa.Anke et al\\.,? 2015", "shortCiteRegEx": "Espinosa.Anke et al\\.", "year": 2015}, {"title": "Traversing knowledge graphs in vector space", "author": ["Gu et al.2015] Kelvin Gu", "John Miller", "Percy Liang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A latent factor model for highly multi-relational data", "author": ["Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Jenatton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "author": ["Ji et al.2015] Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of ACL,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Lin et al.2015a] Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015b] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning word meanings from context during normal reading", "author": ["Nagy et al.1987] William E Nagy", "Richard C Anderson", "Patricia A Herman"], "venue": "American educational research journal,", "citeRegEx": "Nagy et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Nagy et al\\.", "year": 1987}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "Proceedings of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of ICML,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang et al.2014a] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014b] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Xie et al.2016] Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun"], "venue": "In Proceedings of AAAI", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2014] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "Proceedings of ICLR", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["Zhong et al.2015] Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Zhong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2015}, {"title": "Multiinstance multi-label learning", "author": ["Zhou et al.2012] Zhi-Hua Zhou", "Min-Ling Zhang", "Sheng-Jun Huang", "Yu-Feng Li"], "venue": "Artificial Intelligence", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "There are large numbers of KGs like Freebase, YAGO and DBpedia that are widely utilized in nature language processing applications such as question answering and web search (Bollacker et al., 2008).", "startOffset": 173, "endOffset": 197}, {"referenceID": 26, "context": "To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).", "startOffset": 201, "endOffset": 265}, {"referenceID": 7, "context": "To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).", "startOffset": 201, "endOffset": 265}, {"referenceID": 17, "context": "To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).", "startOffset": 201, "endOffset": 265}, {"referenceID": 18, "context": "Many methods have introduced representation learning to KGs, projecting both entities and relations into a continuous low-dimensional vector space (Nickel et al., 2011; Jenatton et al., 2012; Bordes et al., 2013).", "startOffset": 147, "endOffset": 212}, {"referenceID": 12, "context": "Many methods have introduced representation learning to KGs, projecting both entities and relations into a continuous low-dimensional vector space (Nickel et al., 2011; Jenatton et al., 2012; Bordes et al., 2013).", "startOffset": 147, "endOffset": 212}, {"referenceID": 29, "context": "(Wang et al., 2014a; Zhong et al., 2015) propose a joint model projecting both entities and words into the same vector space with alignment models.", "startOffset": 0, "endOffset": 40}, {"referenceID": 25, "context": "(Xie et al., 2016) directly builds entity representations from entity descriptions, while their model is restricted by the completeness and quality of entity descriptions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "As shown in (Nagy et al., 1987), humans learn meanings of new words from their contextual information.", "startOffset": 12, "endOffset": 31}, {"referenceID": 13, "context": "TransD (Ji et al., 2015) proposes dynamic mapping matrix constructed by both entities and relations for multiple representations of entities.", "startOffset": 7, "endOffset": 24}, {"referenceID": 9, "context": "To extend the single-step translating operation, (Gu et al., 2015; Lin et al., 2015a) encode multiple-step relation paths into representation learning of knowledge graphs and achieve significant improvements.", "startOffset": 49, "endOffset": 85}, {"referenceID": 29, "context": "(Zhong et al., 2015) extends the alignment model by considering entity descriptions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "(Xie et al., 2016) proposes a new kind of representation for entities, which is directly constructed from entity descriptions using CNN and thus is capable of modeling new entities.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Multi-instance learning, which was originally proposed in (Dietterich et al., 1997), arises in the tasks where a single object may possess multiple alternative examples or representations that describe it.", "startOffset": 58, "endOffset": 83}, {"referenceID": 30, "context": "(Zhou et al., 2012) proposes multi-instance multi-label learning on multiple tasks such as scene classification and text categorization.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "(Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction.", "startOffset": 0, "endOffset": 67}, {"referenceID": 11, "context": "(Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction.", "startOffset": 0, "endOffset": 67}, {"referenceID": 22, "context": "(Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction.", "startOffset": 0, "endOffset": 67}, {"referenceID": 28, "context": "(Zeng et al., 2015) further combines multiinstance learning with convolutional neural network for relation extraction on distant supervision data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "Inspired by (Zeng et al., 2014), the word representations consist of two parts, including word features and position features.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "(Collobert et al., 2011) proposes a mean-pooling approach to solve gradient vanishment to some degree.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "The attention-based model is powerful and has been widely applied to machine translation (Bahdanau et al., 2015), abstractive sentence summarization (Rush et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 20, "context": ", 2015), abstractive sentence summarization (Rush et al., 2015) and speech recognition (Chorowski et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 4, "context": ", 2015) and speech recognition (Chorowski et al., 2014).", "startOffset": 31, "endOffset": 55}, {"referenceID": 21, "context": "Following the similar protocol in (Socher et al., 2013), for each triple representation, we construct a negative example by randomly replace the head or tail entity with another entity.", "startOffset": 34, "endOffset": 55}, {"referenceID": 8, "context": "Definition extraction is an important task in text mining (Espinosa-Anke et al., 2015).", "startOffset": 58, "endOffset": 86}], "year": 2016, "abstractText": "Textual information is considered as significant supplement to knowledge representation learning (KRL). There are two main challenges for constructing knowledge representations from plain texts: (1) How to take full advantages of sequential contexts of entities in plain texts for KRL. (2) How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations.", "creator": "LaTeX with hyperref package"}}}