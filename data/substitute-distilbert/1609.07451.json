{"id": "1609.07451", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "AMR-to-text generation as a Traveling Salesman Problem", "abstract": "the task from text - to - text generation is to generate grammatical text that sustains the semantic ambiguity for a given grammar graph. we push - tack the task by first partitioning the amr chunks into smaller fragments, and then encoding the translation for each fragment, before finally deciding the order where solving every asymmetric generalized traveling distribution problem ( agtsp ). a maximum entropy classifier is drawn to understand arbitrary traveling costs, and a tsp solver is used to find the optimized solution. the final model reports a bleu score of 22. 44 on the semeval - 2016 task8 dataset.", "histories": [["v1", "Fri, 23 Sep 2016 18:12:12 GMT  (29kb,D)", "http://arxiv.org/abs/1609.07451v1", "accepted by EMNLP 2016"]], "COMMENTS": "accepted by EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "yue zhang", "xiaochang peng", "zhiguo wang", "daniel gildea"], "accepted": true, "id": "1609.07451"}, "pdf": {"name": "1609.07451.pdf", "metadata": {"source": "CRF", "title": "AMR-to-text generation as a Traveling Salesman Problem", "authors": ["Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea"], "emails": [], "sections": [{"heading": null, "text": "AMR-to-text generation as a Traveling Salesman Problem\nLinfeng Song1, Yue Zhang3, Xiaochang Peng1, Zhiguo Wang2 and Daniel Gildea1 1Department of Computer Science, University of Rochester, Rochester, NY 14627\n2IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 3Singapore University of Technology and Design\nAbstract\nThe task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset."}, {"heading": "1 Introduction", "text": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. \u201cboy\u201d, \u201cgo-01\u201d and \u201cwant01\u201d) represent concepts, and the edges (e.g. \u201cARG0\u201d and \u201cARG1\u201d) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012).\nThe task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph\nusually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP).\nGiven an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation. These rules serve in a similar way to rules in SMT models. We learn the rules by a modified version of the sampling algorithm of Peng\nar X\niv :1\n60 9.\n07 45\n1v 1\n[ cs\n.C L\n] 2\n3 Se\np 20\n16\net al. (2015), and use the rule matching algorithm of Cai and Knight (2013).\nFor decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered. To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut. TSP is used to traverse different cuts and determine the best order. Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sentence by ordering the translations. Although the computational cost of our method is low, the initial experiment is promising, yielding a BLEU score of 22.44 on a standard benchmark."}, {"heading": "2 Method", "text": "We reformulate the problem of AMR-to-text generation as an asymmetric generalized traveling salesman problem (AGTSP), a variant of TSP."}, {"heading": "2.1 TSP and its variants", "text": "Given a non-directed graph GN with n cities, supposing that there is a traveling cost between each pair of cities, TSP tries to find a tour of the minimal total cost visiting each city exactly once. In contrast, the asymmetric traveling salesman problem (ATSP) tries to find a tour of the minimal total cost on a directed graph, where the traveling costs between two nodes are different in each direction. Given a directed graph GD with n nodes, which are clustered into m groups, the asymmetric generalized traveling salesman problem (AGTSP) tries to find a tour of the minimal total cost visiting each group exactly once."}, {"heading": "2.2 AMR-to-text Generation as AGTSP", "text": "Given an input AMR A, each node in the AGTSP graph can be represented as (c, r), where c is a concept in A and r = (Asub, Tsub) is a rule that consists of an AMR fragment containing c and a translation of the fragment. We put all nodes containing the same concept into one group, thereby translating each concept in the AMR exactly once.\nTo show a brief example, consider the AMR in Figure 1 and the following rules,\nr1 (w/want-01) ||| wants r2 (g/go-01) ||| to go r3 (w/want-01 :ARG1 g/go-01) ||| wants to go r4 (b/boy) ||| The boy\nWe build an AGTSP graph in Figure 2, where each circle represents a group and each tuple (such as (b, r4)) represents a node in the AGTSP graph. We add two nodes ns and ne representing the start and end nodes respectively. Each belongs to a specific group that only contains that node, and a tour always starts with ns and ends with ne. Legal moves are shown in black arrows, while illegal moves are shown in red. One legal tour is ns \u2192 (b, r4) \u2192 (w, r3) \u2192 (g, r3) \u2192 ne. The order in which nodes within a rule are visited is arbitrary; for a rule with N concepts, the number of visiting orders is O(N !). To reduce the search space, we enforce the breadth first order by setting costs to zero or infinity. In our example, the traveling cost from (w, r3) to (g, r3) is 0, while the traveling cost from (g, r3) to (w, r3) is infinity. Traveling from (g, r2) to (w, r3) also has infinite cost, since there is overlap on the concept \u201cw/want-01\u201d between them.\nThe traveling cost is calculated by Algorithm 1. We first add ns and ne serving the same function as Figure 2. The traveling cost from ns directly to ne is infinite, since a tour has to go through other nodes before going to the end. On the other hand, the traveling cost from ne to ns is 0 (Lines 3-4), as a tour always goes back to the start after reaching the end. The traveling cost from ns to ni = (ci, ri) is the model score only if ci is the first node of the AMR fragment of ri, otherwise the traveling cost is infinite (Lines 6-9). Similarly, the traveling cost from ni to ne is the model score only if ci is the last node of the fragment of ri. Otherwise, it is infinite (Lines 10-13). The traveling cost from ni = (ci, ri) to nj = (cj , rj) is 0 if ri and rj are the same rule and cj is the next node of ci in the AMR fragment of ri (Lines 16-17).\nA tour has to travel through an AMR fragment be-\nData: Nodes in AGTSP graph G Result: Traveling Cost Matrix T\n1 ns \u2190 (\u201c<s>\u201d,\u201c<s>\u201d ); 2 ne \u2190 (\u201c</s>\u201d,\u201c</s>\u201d ); 3 T[ns][ne]\u2190\u221e; 4 T[ne][ns]\u2190 0; 5 for ni \u2190 (ci, ri) in G do 6 if ci = ri.frag.first then 7 T[ns][ni]\u2190ModelScore(ns,ni); 8 else 9 T[ns][ni]\u2190\u221e;\n10 if ci = ri.frag.last then 11 T[ni][ne]\u2190ModelScore(ni,ne); 12 else 13 T[ni][ne]\u2190\u221e; 14 for ni \u2190 (ci, ri) in G do 15 for nj \u2190 (cj , rj) in G do 16 if ri = rj and ri.frag.next(ci) = cj then 17 T[ni][nj]\u2190 0 18 else if ri.frag \u2229 rj .frag = \u2205 and ci = ri.frag.last and cj = rj .frag.first then 19 T[ni][nj]\u2190ModelScore(ni,nj) 20 else 21 T[ni][nj]\u2190\u221e\nAlgorithm 1: Traveling cost algorithm\nfore jumping to another fragment. We choose the breadth-first order of nodes within the same rule, which is guaranteed to exist, as each AMR fragment is rooted and connected. Costs along the breadthfirst order within a rule ri are set to 0, while other costs with a rule are infinite.\nIf ri is not equal to rj , then the traveling cost is the model score if there is no overlap between ri and rj\u2019s AMR fragment and it moves from ri\u2019s last node to rj\u2019s first node (Lines 18-19), otherwise the traveling cost is infinite (Lines 20-21). All other cases are illegal and we assign infinite traveling cost. We do not allow traveling between overlapping nodes, whose AMR fragments share common concepts. Otherwise the traveling cost is evaluated by a maximum entropy model, which will be discussed in detail in Section 2.4."}, {"heading": "2.3 Rule Acquisition", "text": "We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given\nan aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discarding function words, while our task is AMR-to-text generation, and we need to be able to fill in these unaligned words. Since incorporating unaligned words will introduce noise, we rank the translation candidates for each AMR fragment by their counts in the training data, and select the top N candidates.1\nWe also generate concept rules which directly use a morphological string of the concept for translation. For example, for concept \u201cw/want-01\u201d in Figure 1, we generate concept rules such as \u201c(w/want01) ||| want\u201d, \u201c(w/want-01) ||| wants\u201d, \u201c(w/want-01) ||| wanted\u201d and \u201c(w/want-01) ||| wanting\u201d. The algorithm (described in section 2.2) will choose the most suitable one from the rule set. It is similar to most MT systems in creating a translation candidate for each word, besides normal translation rules. It is easy to guarantee that the rule set can fully cover every input AMR graph.\nSome concepts (such as \u201chave-rel-role-91\u201d) in an AMR graph do not contribute to the final translation, and we skip them when generating concept rules. Besides that, we use a verbalization list2 for concept rule generation. For rule \u201cVERBALIZE peacekeeping TO keep-01 :ARG1 peace\u201d, we will create a concept rule \u201c(k/keep-01 :ARG1 (p/peace)) ||| peacekeeping\u201d if the left-hand-side fragment appears in the target graph.\n1Our code for grammar induction can be downloaded from https://github.com/xiaochang13/AMR-generation\n2http://amr.isi.edu/download/lists/verbalization-listv1.06.txt"}, {"heading": "2.4 Traveling cost", "text": "Considering an AGTSP graph whose nodes are clustered into m groups, we define the traveling cost for a tour T in Equation 1:\ncost(ns, ne) = \u2212 m\u2211 i=0 log p(\u201cyes\u201d|nTi , nTi+1) (1)\nwhere nT0 = ns, nTm+1 = ne and each nTi (i \u2208 [1 . . .m]) belongs to a group that is different from all others. Here p(\u201cyes\u201d|nj , ni) represents a learned score for a move from nj to ni. The choices before nTi are independent from choosing nTi+1 given nTi because of the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model. However, as a result, the number of nodes in the AGTSP graph grows exponentially.\nTo tackle the problem, we treat it as a local binary (\u201cyes\u201d or \u201cno\u201d) classification problem whether we should move to nj from ni. We train a maximum entropy model, where p(\u201cyes\u201d|ni, nj) is defined as:\np(\u201cyes\u201d|ni, nj) =\n1\nZ(ni, nj) exp [ k\u2211 i=1 \u03bbifi(\u201cyes\u201d, ni, nj) ] (2)\nThe model uses 3 real-valued features: a language model score, the word count of the concatenated translation from ni to nj , and the length of the shortest path from ni\u2019s root to nj\u2019s root in the input AMR. If either ni or nj is the start or end node, we set the path length to 0. Using this model, we can use whatever N-gram we have at each time. Although language models favor shorter translations, word count will balance the effect, which is similar to MT systems. The length of the shortest path is used as a feature because the concepts whose translations are adjacent usually have lower path length than others."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup", "text": "We use the dataset of SemEval-2016 Task8 (Meaning Representation Parsing), which contains 16833\ntraining instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. To solve the AGTSP, we use Or-tool3.\nOur graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses4 to translate the linearized AMR into a sentence. To traverse the children of an AMR concept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMRgen5 (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).\nTo evaluate the importance of each module in our system, we develop the following baselines: OnlyConceptRule uses only the concept rules, OnlyInducedRule uses only the rules induced from the fragment decomposition forest, OnlyBigramLM uses both types of rules, but the traveling cost is evaluated by a bigram LM trained with gigaword."}, {"heading": "3.2 Results", "text": "The results are shown in Table 1. Our method (All) significantly outperforms the baseline (PBMT)\n3https://developers.google.com/optimization/ 4http://www.statmt.org/moses/ 5https://github.com/jflanigan/jamr/tree/Generator\non both the dev and test sets. PBMT does not outperform OnlyBigramLM and OnlyInducedRule, demonstrating that our rule induction algorithm is effective. We consider rooted and connected fragments from the AMR graph, and the TSP solver finds better solutions than beam search, as consistent with Zaslavskiy et al. (2009). In addition, OnlyInducedRule is significantly better than OnlyConceptRule, showing the importance of induced rules on performance. This also confirms the reason that All outperforms PBMT. This result confirms our expectation that concept rules, which are used for fulfilling the coverage of an input AMR graph in case of OOV, are generally not of high quality. Moreover, All outperforms OnlyBigramLM showing that our maximum entropy model is stronger than a bigram language model. Finally, JAMR-gen outperforms All, while JAMR-gen uses a higher order language model than All (5-gram VS 4-gram).\nFor rule coverage, around 31% AMR graphs and 84% concepts in the development set are covered by our induced rules extracted from the training set."}, {"heading": "3.3 Analysis and Discussions", "text": "We further analyze All and JAMR-gen with an example AMR and show the AMR graph, the reference, and results in Table 2. First of all, both All and JAMR-gen outputs a reasonable translation containing most of the meaning from the AMR. On the other hand, All fails to recognize \u201cboy\u201d as the subject. The reason is that the feature set does not include edge labels, such as \u201cARG0\u201d and \u201cARG1\u201d. Finally, neither All and JAMR-gen can handle the situation when a re-entrance node (such as \u201cb/boy\u201d in example graph of Table 2) need to be translated twice. This limitation exists for both works."}, {"heading": "4 Related Work", "text": "Our work is related to prior work on AMR (Banarescu et al., 2013). There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence. On the reverse direction, Flanigan et al. (2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012).\nOur work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation."}, {"heading": "5 Conclusion", "text": "In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search."}, {"heading": "Acknowledgments", "text": "We are grateful for the help of Jeffrey Flanigan, Lin Zhao, and Yifan He. This work was funded by NSF IIS-1446996, and a Google Faculty Research Award. Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 1699\u20131710.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the 7th Linguistic", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer", "author": ["Bernd Bohnet", "Leo Wanner", "Simon Mill", "Alicia Burga."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),", "citeRegEx": "Bohnet et al\\.,? 2010", "shortCiteRegEx": "Bohnet et al\\.", "year": 2010}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 748\u2013752.", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Generation from abstract meaning representation using tree transducers", "author": ["Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell."], "venue": "Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics", "citeRegEx": "Flanigan et al\\.,? 2016", "shortCiteRegEx": "Flanigan et al\\.", "year": 2016}, {"title": "Semanticsbased machine translation with hyperedge replacement grammars", "author": ["Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING-12),", "citeRegEx": "Jones et al\\.,? 2012", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02), pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "author": ["Xiaochang Peng", "Linfeng Song", "Daniel Gildea."], "venue": "Proceedings", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Parsing English into abstract meaning representation using syntax-based machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Building applied natural language generation systems", "author": ["Ehud Reiter", "Robert Dale."], "venue": "Natural Language Engineering, 3(1):57\u201387.", "citeRegEx": "Reiter and Dale.,? 1997", "shortCiteRegEx": "Reiter and Dale.", "year": 1997}, {"title": "Joint morphological generation and syntactic linearization", "author": ["Linfeng Song", "Yue Zhang", "Kai Song", "Qun Liu."], "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI-14), pages 1522\u2013 1528.", "citeRegEx": "Song et al\\.,? 2014", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus", "author": ["Lucy Vanderwende", "Arul Menezes", "Chris Quirk."], "venue": "Proceedings of the 2015 Meeting of the North American chapter of the Association for Computa-", "citeRegEx": "Vanderwende et al\\.,? 2015", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2015}, {"title": "Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model", "author": ["Stephen Wan", "Mark Dras", "Robert Dale", "C\u00e9cile Paris."], "venue": "Proceedings of the 12th Conference of the European", "citeRegEx": "Wan et al\\.,? 2009", "shortCiteRegEx": "Wan et al\\.", "year": 2009}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15), pages 366\u2013375.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Perceptron reranking for CCG realization", "author": ["Michael White", "Rajakrishnan Rajkumar."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-09), pages 410\u2013419.", "citeRegEx": "White and Rajkumar.,? 2009", "shortCiteRegEx": "White and Rajkumar.", "year": 2009}, {"title": "Reining in CCG chart realization", "author": ["Michael White."], "venue": "International Conference on Natural Language Generation (INLG-04), pages 182\u2013191.", "citeRegEx": "White.,? 2004", "shortCiteRegEx": "White.", "year": 2004}, {"title": "Phrase-based statistical machine translation as a traveling salesman problem", "author": ["Mikhail Zaslavskiy", "Marc Dymetman", "Nicola Cancedda."], "venue": "Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-09), pages 333\u2013341.", "citeRegEx": "Zaslavskiy et al\\.,? 2009", "shortCiteRegEx": "Zaslavskiy et al\\.", "year": 2009}, {"title": "Discriminative syntax-based word ordering for text generation", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational Linguistics, 41(3):503\u2013538.", "citeRegEx": "Zhang and Clark.,? 2015", "shortCiteRegEx": "Zhang and Clark.", "year": 2015}, {"title": "Partial-tree linearization: Generalized word ordering for text synthesis", "author": ["Yue Zhang."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-13), pages 2232\u20132238.", "citeRegEx": "Zhang.,? 2013", "shortCiteRegEx": "Zhang.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", "startOffset": 38, "endOffset": 62}, {"referenceID": 6, "context": "AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012).", "startOffset": 138, "endOffset": 158}, {"referenceID": 1, "context": "usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013).", "startOffset": 147, "endOffset": 171}, {"referenceID": 4, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 15, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 9, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 13, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 10, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 0, "context": "There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).", "startOffset": 53, "endOffset": 179}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a span-", "startOffset": 8, "endOffset": 159}, {"referenceID": 3, "context": "(2015), and use the rule matching algorithm of Cai and Knight (2013). For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003).", "startOffset": 136, "endOffset": 156}, {"referenceID": 8, "context": "We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment.", "startOffset": 76, "endOffset": 95}, {"referenceID": 7, "context": "A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides.", "startOffset": 137, "endOffset": 386}, {"referenceID": 18, "context": "Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi) by using a language model.", "startOffset": 17, "endOffset": 42}, {"referenceID": 18, "context": "Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model.", "startOffset": 18, "endOffset": 250}, {"referenceID": 8, "context": "We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.", "startOffset": 79, "endOffset": 102}, {"referenceID": 5, "context": "We also compare with JAMRgen5 (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "We consider rooted and connected fragments from the AMR graph, and the TSP solver finds better solutions than beam search, as consistent with Zaslavskiy et al. (2009). In addition, OnlyInducedRule is significantly better than OnlyConceptRule, showing the importance of induced rules on performance.", "startOffset": 142, "endOffset": 167}, {"referenceID": 1, "context": "Our work is related to prior work on AMR (Banarescu et al., 2013).", "startOffset": 41, "endOffset": 65}, {"referenceID": 4, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 15, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 9, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 13, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 10, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 0, "context": "There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence.", "startOffset": 45, "endOffset": 171}, {"referenceID": 6, "context": "In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012).", "startOffset": 130, "endOffset": 150}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence. On the reverse direction, Flanigan et al. (2016) and our work here study sentence generation from a given AMR graph.", "startOffset": 8, "endOffset": 134}, {"referenceID": 0, "context": ", 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence. On the reverse direction, Flanigan et al. (2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence.", "startOffset": 8, "endOffset": 240}, {"referenceID": 11, "context": "Our work also belongs to the task of text generation (Reiter and Dale, 1997).", "startOffset": 53, "endOffset": 76}, {"referenceID": 14, "context": "There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 19, "context": "There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al.", "startOffset": 76, "endOffset": 117}, {"referenceID": 20, "context": ", 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al.", "startOffset": 56, "endOffset": 88}, {"referenceID": 12, "context": ", 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al.", "startOffset": 56, "endOffset": 88}, {"referenceID": 2, "context": ", 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009).", "startOffset": 30, "endOffset": 51}, {"referenceID": 17, "context": ", 2010) and logical forms (White, 2004; White and Rajkumar, 2009).", "startOffset": 26, "endOffset": 65}, {"referenceID": 16, "context": ", 2010) and logical forms (White, 2004; White and Rajkumar, 2009).", "startOffset": 26, "endOffset": 65}], "year": 2016, "abstractText": "The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.", "creator": "TeX"}}}