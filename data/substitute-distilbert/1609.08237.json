{"id": "1609.08237", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Aligning Coordinated Text Streams through Burst Information Network Construction and Decipherment", "abstract": "aligning sparse text streams from multiple sources and multiple languages has opened a competing research venues enabling cross - lingual knowledge discovery. in this manner we aim to advance state - of - the - art by : ( 1 ). extending coarse - grained polynomial - level knowledge mining incorporate fine - grained information units such as entities and events ; ( 2 ). following a novel data - to - network - protocol - knowledge ( d2n2k ) framework to gather and utilize network filters to capture and propagate key evidence. but build a novel burst information network ( binet ) representation that can display the most common information as illustrate the compatibility among bursty entities, events and keywords in the corpus. we propose an optimal approach to construct and capture binets, incorporating novel criteria based on multi - dimensional clues from pronunciation, translation, burst, neighbor and collision topological structure. the experimental results concerning chinese and english coordinated text streams show that our approach can accurately decipher the nodes with high confidence in captured binets and that the algorithm can operate efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never - ending purposes and information decipherment.", "histories": [["v1", "Tue, 27 Sep 2016 01:19:41 GMT  (2821kb,D)", "http://arxiv.org/abs/1609.08237v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tao ge", "qing dou", "xiaoman pan", "heng ji", "lei cui", "baobao chang", "zhifang sui", "ming zhou"], "accepted": false, "id": "1609.08237"}, "pdf": {"name": "1609.08237.pdf", "metadata": {"source": "CRF", "title": "Aligning Coordinated Text Streams through Burst Information Network Construction and Decipherment", "authors": ["Tao Ge", "Qing Dou", "Xiaoman Pan", "Heng Ji", "Lei Cui", "Baobao Chang", "Zhifang Sui", "Ming Zhou"], "emails": ["getao@pku.edu.cn", "qdou@isi.edu", "panx2@rpi.edu", "jih@rpi.edu", "lecu@microsoft.com", "chbb@pku.edu.cn", "szf@pku.edu.cn", "mingzhou@microsoft.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications\u2014 Data mining ; I.2.7 [Artificial Intelligence]: Natural Lan-\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nc\u00a9 2016 ACM. ISBN 123-4567-24-567/08/06. . . $15.00 DOI: 10.475/123 4\nguage Processing\u2014Text analysis\nKeywords Burst Information Networks, Text stream alignment, Decipherment"}, {"heading": "1. INTRODUCTION", "text": "In this information era, a large amount of text data is continuously produced over time in various applications (e.g., news agencies and social networks), which results in massive streams of text data with much valuable information and knowledge. Streaming data is an important source for many applications such as disaster summarization [1]. There are many studies on pattern mining from text data streams but most of them focus on a single text stream. Nonetheless, there are many text streams that are topically related and indexed by the same set of time points, which are called coordinated text streams [2]. For example, news streams from a news agency in different languages are coordinated. In recent, text mining on coordinated text streams has gained attention in both data mining and natural language processing communities because one can discovery useful facts and important patterns by aligning the coordinated streams. For example, [2] and [3] tried to discover common topic patterns from coordinated text streams. In this paper, we aim to advance state-of-the-art from the following two perspectives.\n(1) Aligning Fine-grained Information Units instead of Coarse-grained Topics.\nMost documents in coordinated streams are not entirely parallel. Fortunately, topically related and temporally correlated important facts such as bursty events and entities often repeatedly co-occur across streams from different languages. Most previous work on coordinated text stream mining such as [2] analyzed the streams on the coarse-grained topic level, which inevitably leads to loss of some information. For instance, a topic-level cross-lingual text stream analysis cannot inform us the mentions of an entity in the other text\nar X\niv :1\n60 9.\n08 23\n7v 1\n[ cs\n.C L\n] 2\nstream. For fully exploiting the coordinated text streams, we propose to analyze the text streams by attempting to align important fine-grained information units (e.g., entities and events) across the streams. Specifically, in this paper, we focus on aligning cross-lingual coordinated text streams. We choose two very different languages \u2013 Chinese and English streams for alignment as a case study. Aligning cross-lingual coordinated text streams is a meaningful task because it can mine translation pairs for completing a bi-lingual lexicon, which would be helpful for cross-lingual information extraction, information retrieval and machine translation systems that suffer from bi-lingual lexicons with a limited number of entries.\n(2) Data-to-Network-to-Knowledge (D2N2K) instead of Data-to-Knowledge (D2K)\nInstead of directly turning massive, unstructured data streams into structured knowledge (D2K), we propose a new Data-to-Network-to-Knowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowlege mining algorithms more scalable and effective because we can take advantage of the graph structures to acquire and propagate knowledge. For aligning the cross-lingual text streams, we first propose a novel text stream\u2019s structured representation called \u201cBurst Information Networks (BINets)\u201d to focus on the most important information units. In BINets, nodes are bursty words including important entities and events, and edges represent their co-burst relations. An example is depicted in Figure 1. Similar to traditional Information Networks [4, 5], nodes within a community in BINets are semantically related (e.g., entities are often participants in certain events). Moreover, they are also temporally coherent. For example, the nodes in Figure 1 are all related to the 2010 Australian Open.\nAfter the BINets are constructed, our next goal is to align the nodes in BINets across text streams. In comparable corpora, bursty words which are about either a breaking news or a hot topic usually burst across languages. For example, both \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d and its translation in English \u201cAustralian Open\u201d in figure 1 burst from Jan 14 to Jan 31 in 2010. We take advantage of such characteristics of bursts across languages and propose four deci-\npherment clues from some starting points (prior knowledge) in a propagation manner. Experimental results demonstrate that our aligning approach can accurately mine cross-lingual translation pairs and that the more data provided, the more translation pairs will be discovered, which means that if our approach can run over the streaming data produced throughout the time, it will be a never-ending translation pair discovery framework."}, {"heading": "2. BURST INFORMATION NETWORK", "text": "We will start by defining some new terminologies and describing the detailed methods for constructing Burst Information Networks."}, {"heading": "2.1 Burst Detection", "text": "A word\u2019s burst refers to a remarkable increase in the number of occurrences of the word during a short period and it might indicate important events or trending topics. For example, the word\u201c\u5a01\u5ec9(Williams)\u201dhas a burst from Jan 27 to Jan 31, as shown in Figure 2 because of the wonderful performance of Serena Williams in the 2010 Australian Open held during this period. For a timestamped document collection C = {D1, D2, ..., D\u03c4 , ..., DT } where D\u03c4 is the collection of documents during time \u03c4 , we define a word w\u2019s burst sequence s = (s1, s2, ..., s\u03c4 , ..., sT ) in which s\u03c4 is either 1 or 0 to indicate if w bursts or not at the time epoch. Most burst detection approaches (e.g., [6]) detect a word\u2019s burst states mainly based on comparing a word\u2019s probability qt at a time epoch t to its base probablity q0 that is the word\u2019s global probability throughout the stream. In this paper, we use the approach of [7] which is a variant of [6] for burst detection.\nSpecifically, if a word w bursts at every time \u03c4 during a period, we call this period as a burst period of w, and w has a burst during this period. In Figure 2, we can see that the word\u201c\u5a01\u5ec9(Williams)\u201dhas 4 burst periods. Formally, we use P to denote a burst period w. P is defined as a consecutive time sequence during which the word bursts at every time epoch \u03c4 :\nP = (\u03c4i, \u03c4i+1, \u03c4i+2, ..., \u03c4i+n) \u2200\u03c4 \u2208 P s\u03c4 = 1\nwhere s\u03c4 denotes the burst state at time \u03c4 ."}, {"heading": "2.2 Burst Information Network Construction", "text": "An information network is a graph depicting connections between entities and events, in which every community should be topically and temporally coherent, as shown in Figure 1. Most of the previous approaches (e.g., [5]) relied on a large number of linguistic resources and labeled data to construct information networks. [8] proposed a co-occurrence based approach to build an information network where each node is a word type (i.e., term) and edges between nodes denote the co-occurrence of words in a document. The resulting information network can demonstrate the connections between words. It usually consists of several communities (i.e., a group of nodes that are strongly related) and each community in the information network corresponds to a topic. However, such kind of information networks did not incorporate temporal information and thus the communities are usually not temporally coherent. In addition, it did not tackle the ambiguity problem since a word may have different senses in the corpus. To address these problems, we propose a new representation called \u201cBurst Information Network (BINet)\u201d by using burst elements as nodes:\nA Burst Element is a burst of a word. It can be represented by a tuple: \u3008w,P\u3009 where w denotes the word type and P denotes one burst period of w.\nA word type may have multiple burst periods while a burst element only has one burst period. A word during its different burst periods will be regarded as different burst elements. For example, in Figure 2, \u201c\u5a01\u5ec9(Williams)\u201d has 4 bursts, each of which is regarded as a distinct burst element.\nIn general, there are two main advantages using burst elements as nodes to build the information network: \u2022 A burst element not only includes semantic informa-\ntion but also incorporates the temporal dimension. It cannot co-occur with another burst element that does not burst during the same burst period. Therefore, the communities in this information network must be both topically and temporally coherent. \u2022 Since a burst element denotes a bursty word during a\nconsecutive period, its sense is likely to be consistent. Multiple bursts of a word will be considered as different burst elements. Therefore, each node in the information network is less ambiguous. For example, even though \u201c\u5a01\u5ec9(Williams)\u201d (shown in Figure 2) refers to different entities in the corpus, its sense in a single burst is consistent. It refers to \u201cSerena Williams\u201d in the first two bursts, while it refers to\u201cPrince Williams\u201d in the last two bursts. Formally, we define the BINet G = \u3008V,E\u3009 as follows. Each node v \u2208 V is a burst element and each edge \u2208 E denotes the correlation between burst elements. Intuitively, if two burst elements frequently co-occur in a document, then they are strongly related and thus the edge between these two nodes should be highly weighted. Therefore, we define the weight \u03c9 of an edge between vi and vj as the number of documents vi and vj co-occur."}, {"heading": "3. DECIPHERMENT", "text": ""}, {"heading": "3.1 Hypothesis and Overall Framework", "text": "After constructing BINets from a foreign language, we aim to decipher them into English by translating as many nodes as possible. In this paper we use Chinese as a foreign language and conduct experiments with as few languagespecific resources as possible. Our basic hypothesis is that\nco-burst information units (entities, events, etc.) tend to co-occur across many different languages. Therefore, if we gather a collection of English documents created from a similar time frame as the Chinese corpus, we can construct BINets from this English corpus and then consult them to validate candidate translations. Formally, we define Gc = \u3008Vc, Ec\u3009 andGe = \u3008Ve, Ee\u3009 as the Chinese BINet and English BINet respectively. For those who do not know Chinese, Gc can be considered as a network of ciphers. Based on this hypothesis we design a novel BINet decipherment procedure to use Ge to decipher Gc. Formally, we define the decipherment as a process to find a counterpart e \u2208 Ve of a node c \u2208 Vc so that e is c\u2019s translation or c is a mention referring to the entity e refers to.1"}, {"heading": "3.2 Starting Point", "text": "To start deciphering the Chinese BINet, we need a few seeds based on prior knowledge as a starting point. Inspired by some previous work on bi-lingual dictionary induction (e.g., [9, 10]), decipherment (e.g., [11, 12, 13]) and name translation mining (e.g., [14, 15]), we utilize a few linguistic resources - a bi-lingual lexicon and language-universal representations such as time/calendar date, number, website URL, currency and emoticons to decipher a subset of Chinese nodes. For the example shown in Figure 1, we can decipher some nodes in the Chinese BINet such as \u201c7-6\u201d (to \u201c7-6\u201d) and \u201c\u79cd\u5b50\u201d (to \u201cseed\u201d)."}, {"heading": "3.3 Candidate Generation", "text": "Based on the prior knowledge mentioned above, we can decipher a subset of nodes in the Chinese BINets. For the remaining nodes, we first need to discover its possible candidate translations. For a node c in the Chinese BINet, its translation e can be any node in the English BINet or does not exist in the English BINet. However, if we use all nodes in the English BINet as its candidates, the candidate list will be extremely long, which makes it inefficient to validate each candidate. To address this problem, we exploit temporal constraints to dramatically narrow down the search scope.\nAs defined in Section 2, a node in a BINet is a burst element that has a burst period. For a node in the Chinese BINet, its candidate translation is likely to be a node with the same burst period in the English BINet. For example, the node \u201c\u5a01\u5ec9(Williams)\u201d in the Chinese BINet shown in Figure 1 bursts from Jan 27 to Jan 31, 2010. Thus we should look for its translation from the nodes in the English BINet whose burst period overlaps with this period. The reason is that a burst of a word usually indicates an important event which is usually reported by multiple news agencies in multiple languages. Therefore, for a node c \u2208 Vc in the Chinese BINet, its candidate translations can be derived as follows:\nCand(c) = {e|P(e) \u2229 P(c) 6= \u2205} where e \u2208 Ve, and P(c) and P(e) are the burst periods of c and e respectively."}, {"heading": "3.4 Candidate Validation", "text": "After we obtain the candidate list of c (i.e., Cand(c)), we need to validate each node e \u2208 Cand(c) and choose the 1c and e are burst elements (i.e., nodes in the BINets) defined in Section 2. Sometimes, we also use c and e to denote the nodes\u2019 words if that does not lead to misunderstanding.\nmost possible one as the translation of c so that we can decipher the BINet. Formally, we define the credibility score Score(c, e) as the credibility score of e being the correct translation of c. We propose the following novel criteria for validation.\nPronunciation For a node e \u2208 Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For example, the pronunciation of \u201cWilliams\u201d is similar to \u201c\u5a01\u5ec9 (pronounced as Wei Lian)\u201d in Chinese, which is an important clue to infer that \u201cSerena Williams\u201d is likely to be the translation of \u201c\u5a01 \u5ec9(Williams)\u201d.\nWe compute the normalized edit distance (i.e., Levenshtein distance) between Chinese pinyin2 of c and English candidate e as the pronunciation similarity:\nLeven\u2032 = Leven(pinyin(c), e)\nlen(e)\nwhere len(e) is the length of the string of e. For a multi-word candidate, we compute the normalized Levenshtein distance for each part and use the minimal value.\nWe define Sp as a score to reflect the credibility of a candidate being the correct translation based on the pronunciation similarity:\nSp = 3 if 0 \u2264 Leven \u2032 < 0.25, 1 + 1 2Leven\u2032 if 0.25 < Leven\n\u2032 \u2264 0.5, 0 else\nTranslation For a node e \u2208 Cand(c), it is possible that it or some part of it exists in the bi-lingual lexicon. We can exploit the translation clue to validate if e is the translation of c. For example, \u201cAustralian Open\u201d is a candidate of \u201c\u6fb3\u6d32\u7f51 \u7403\u516c\u5f00\u8d5b(Australian Open)\u201d as shown in figure 1. Even though \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d is not in the bilingual lexicon, \u201cAustralian\u201d and \u201copen\u201d are in the lexicon and their Chinese translations are \u201c\u6fb3\u6d32\u7684(Australian)\u201d and \u201c\u516c\u5f00\u8d5b(open)\u201d respectively. Since\u201c\u6fb3\u6d32\u7684(Australian)\u201dand \u201c\u516c\u5f00\u8d5b(open)\u201d have long common subsequences with \u201c\u6fb3 \u6d32\u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d, we can infer that \u201cAustralian Open\u201d is likely to be the appropriate translation of \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d.\nWe first extract all Chinese translations C(e) of the word type of e from the bi-lingual lexicon. If the word type of e is a multi-word, we concatenate translations of its parts. We define a score St to measure the credibility of a candidate being the correct translation based on the length of the longest common subsequence of c and translation of e:\nSt =  2 if LCS(c,c\u2217) len(c) \u2265 0.75, 1 2(1\u2212LCS(c,c \u2217) len(c) ) if 0.5 \u2264 LCS(c,c \u2217) len(c) < 0.75,\n0 else\nwhere c\u2217 = arg max c\u2032\u2208C(e) LCS(c, c\u2032), LCS(c, c\u2217) denotes the length of the longest common sequence between string c and c\u2217, and len(c) is the length of string of c.\n2pinyin is the official romanization system for Chinese. (https://en.wikipedia.org/wiki/Pinyin)\nNeighbor The graph topological structure of each BINet is also an important clue for decipherment. By analyzing a node\u2019s neighbor, we can learn useful knowledge to decipher the node. For the example in Figure 1, \u201c\u827e\u5b81(Henin)\u201d3 in the Chinese BINet has neighbors such as \u201c\u5a01\u5ec9(Williams)\u201d, \u201c\u6fb3\u6d32\u7f51\u7403 \u516c\u5f00\u8d5b(Australian Open)\u201d and \u201c\u5916\u5361(wildcard)\u201d while \u201cJustine Henin\u201d in the English BINet is connected with \u201cSerena Williams\u201d, \u201cAustralian Open\u201d and \u201cwildcard\u201d. If we know \u201cSerena Williams\u201d,\u201cAustralian Open\u201dand\u201cwildcard\u201dare the translations of \u2018\u5a01\u5ec9\u201d, \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b\u201d and \u201c\u5916\u5361\u201d respectively, then we can infer that \u201cJustine Henin\u201d is likely to be the translation of \u201c\u827e\u5b81\u201d.\nWe define N(c) and N(e) to denote the set of adjacent nodes of c in the Chinese BINet and the adjacent nodes of e in the English BINet respectively. The neighbor validation score Sn of \u3008c, e\u3009 is defined as follows:\nSn = \u2211\nc\u2032\u2208N(c) \u03c9\u0302c,c\u2032 max\ne\u2032\u2208N(e) Score(c\u2032, e\u2032) (1)\nwhere Score(c\u2032, e\u2032) is the overall score of e\u2032 being the translation of c\u2032, as defined at the beginning of Section 3.4, \u03c9\u0302c,c\u2032 =\n\u03c9c,c\u2032\u2211 c\u2032\u2032\u2208N(c\u2032) \u03c9c\u2032,c\u2032\u2032 , which is the normalized weight of the edge between c and c\u2032.\nCo-burst If the word type of e \u2208 Cand(c) frequently co-bursts with the word type of c, then e is likely to be the translation of c. For example, \u201cSerena Williams\u201din the English corpus usually cobursts with \u201c\u5c0f\u5a01\u201d in the Chinese corpus, as shown in figure 3, which is a powerful clue to infer that \u201cSerena Williams\u201d is the translation of \u201c\u5c0f\u5a01\u201d.\nWe define Sb as co-burst validation score as follows:\n3\u201c\u827e\u5b81\u201d is the translation of \u201cJustine Henin\u201d in the AFP Chinese corpus we used, while \u201cJustine Henin\u201d is more commonly translated to \u201c\u6d77\u5b81\u201d in other Chinese corpora.\nSb = sw(c) \u00b7 sw(e)\n|sw(c)|2 + |sw(e)|2\nwhere w(c) denotes the word type of the node c and sw denotes the burst sequence of the word w, as defined in Section 2. Note that in the above equation, we regard sw(e) as a vector. The numerator is the number of days when c and e co-burst and the nominator is the sum of the number of days when c and e burst."}, {"heading": "3.5 Graph-based Decipherment", "text": "Based on the clues introduced above, we define the overall validation (credibility) score by the following equation:\nScore(c, e) = f(Sp, St, Sn, Sb) (2)\nwhere f(\u00b7) is a function and Sp, St, Sn and Sb are validation scores based on the pronunciation, translation, neighbor and co-burst clues respectively. Specifically, we define:\nf(Sp,St, Sn, Sb) =\n\u03b7Sp + \u03bbSt +min(\u03b3Sn, S\u0302 max n ) + \u03b4Sb\n(3)\nwhere \u03b7, \u03bb, \u03b3 and \u03b4 are parameters for adjusting weights of the above clues. Note that in Eq (3), we constrain the effect of the neighbor validation score (no higher than S\u0302maxn ). The reason is that the neighbor validation score Sn will be interacted by other candidate pairs (i.e., Sn depends on other candidate pairs\u2019 scores and it will influence other candidate pairs\u2019 score in turn) and it is unbounded because its definition is recursive. To limit the effect on the score and avoid error propagation, we constrain the effect of Sn (i.e., the upper bound of \u03b3Sn is S\u0302 max n ) on the overall score.\nBased on Eq (3), we can now compute the score of any candidate pair \u3008c, e\u3009. For the pairs that are known to be correct alignments according to the prior knowledge, their scores will be fixed to 1. For other possible candidate pairs, we initialize their scores as follows:\nScore(c, e) = 0.5\n|Cand(c)| (4)\nwhere Cand(c) is the set of English translation candidate nodes and it is generated by the method mentioned in Section 3.3.\nAlgorithm 1 Graph-based Decipherment"}, {"heading": "1: For the determined pair \u3008c, e\u3009 based on the prior knowledge,", "text": "Score(c, e)\u2190 1 2: For other undermined pairs \u3008c, e\u3009, initialize Score(c, e) according to Eq (4); 3: for each iteration (until convergence) do 4: for each undetermined pair \u3008c, e\u3009 do 5: new score\u2190 f(Sp, St, Sn, Sb) according to Eq (3); 6: update(c, e) = min(0.99, new score) 7: end for 8: for each undetermined pair \u3008c, e\u3009 do 9: Score(c, e)\u2190 update(c, e)\n10: end for 11: end for\nSince the score of a pair \u3008c, e\u3009 is affected by the scores of other pairs, we use a label propagation algorithm to iteratively compute and update the scores so that we can decipher the entire Chinese BINet in a propagation manner. This procedure is elaborated in Algorithm 1. Note that for a candidate pair \u3008c,e\u3009, the upper bound of its score is set to 0.99 to distinguish from the known alignments whose score is fixed to 1."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Data", "text": "We use the 2010 Agence France Presse news story articles in Chinese Gigaword [16] and English Gigaword [17] as our non-parallel news stream corpora. The Chinese corpus has 17,327 documents and the English corpus contains 186,737 documents. We removed stopwords, conducted lemmatization and name tagging for the English corpus and did word segmentation and name tagging for the Chinese corpus using the Stanford CoreNLP toolkit [18]. Note that, we performed name tagging in the Chinese corpus just for avoiding segmenting a named entity to separate words and we did not use any other information (e.g., named entity type) from name tagging results because we want to prove that our approach is language-independent and does not need much language-specific knowledge. Therefore, in addition to a seed bilingual lexicon, a pinyin (pronunciation) dictionary and toolkits for Chinese word segmentation, we did not use any other Chinese-related resources in our experiments.\nWe detect bursts and construct the BINets for the Chinese and English stream respectively. The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English BINet has 8,852 nodes and 85,125 edges. The seed bi-lingual lexicon we used is released by [19] which contains 81,990 Chinese word entries, each of which has an English translation. Among those 7,360 nodes in the Chinese BINet, there are 2,242 nodes that need to be deciphered because their words are not in the bi-lingual lexicon."}, {"heading": "4.2 Experimental Setting", "text": "We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e\u2217 which has the highest score as c\u2019s counterpart in the English BINet as its translation:\ne\u2217 = arg max e\u2208Cand(c) Score(c, e)\nWe manually evaluate the quality of translation pairs with top K scores and use accuracy as the evaluation metric. Note that a pair \u3008c,e\u3009 is annotated as correct if e is correct translation of c or c is a mention referring to an entity e refers to. The annotation assignment is done by 2 human judges with 92.2% overlap. The main cause of annotation disagreement is ambiguity of some Chinese words including entities. In the evaluation, we consider \u3008c,e\u3009 correct if both human judges annotate it as correct.\nWe compare our approach to the following baselines that use various combinations of clues to validate candidates for deciphering the Chinese BINet as well as the state-of-theart algorithm for language decipherment from non-parallel corpora: \u2022 Pronunciation validation (pv): Use the pronunciation\nclue only \u2022 Translation validation (tv): Use the translation clue\nonly \u2022 Neighbor validation (nv): Use the neighbor clue only\nto mine translation in a label propagation manner. This baseline is similar to the approach of [20]. \u2022 Co-burst validation: (cv): Use the co-burst clue only \u2022 pv+tv \u2022 pv+tv+nv \u2022 Bayesian Inference: The state-of-the-art algorithm [12]\nfor language decipherment on non-parallel corpora, deciphering a language based on the alignment of its bi-\ngram language model to a bigram language model in English using Bayesian inference. In our experimental setting, we first convert BINets to bigram language models: if two nodes v1 and v2 are adjacent in BINets, we treat them as bigrams v1, v2 and v2, v1. The count of these bigrams is the weight of the edge between v1 and v2. After normalization, we run the decipherment algorithm of [12] which outputs the probability distribution of possible candidates given a cipher node (i.e., P (e|c)). For the approaches except Bayesian inference, the score computation function is almost identical to Eq (3) except that the weights of the clues which are not used are set to 0.\nWe used 2009 AFP Chinese/English Gigaword corpora as development set and tuned the parameters by grid search: \u03b7 = 0.25, \u03bb = 0.3, \u03b3 = 0.5, \u03b4 = 0.2, S\u0302maxn = 0.4. The number of iterations in Algorithm 1 is empirically set to 20 which is proven sufficient for convergence of the scores of nodes in the BINets on the development set."}, {"heading": "4.3 Results", "text": "We present the results in Figure 4. Our approach outperforms all baselines because it considers all available clues for decipherment. Among the baselines, tv achieves very high accuracy within top 100 nodes while pv \u2019s performance is much lower. The reason is that pronunciation similarity based validation is not suitable for all OOVs, especially common words. For example, the pinyin of \u80a1\u5e02(stock market) is \u201cgushi\u201d, which is very similar to the English word \u201cgush\u201d but \u201cgush\u201d is not the correct translation of \u80a1\u5e02(stock market). It is notable that the accuracy scores of pv and tv drop dramatically with K increasing because although we can accurately decipher some nodes with high Sp or St, such nodes are rare. When Sp and St are not extremely high, it is risky to decipher a Chinese node just based on Sp or St. In the dataset we use, only approximately 100 out-of-vocabulary nodes in the Chinese BINet can be effectively deciphered using either pv or tv, which results in a tumble of accuracy when K increases. pv+tv seems to alleviate the problem to some extent: its accuracy does not drop as drastically as pv or tv because multiple clues allow us to decipher more nodes than a single clue does. However, its accuracy is not desirable \u2013 even lower than tv in most cases. Among all the baselines, cv seems to perform worst, demonstrating that\nthe co-burst clue alone is far from enough for decipherment. Compared with pv, tv and cv, nv deciphers the nodes in Chinese BINet in a propagation manner but the neighbor clue alone is not sufficient for accurate decipherment. It is notable that the curve of Bayesian inference method is similar to the nv because the clues used in these two approaches are almost the same but our graph-based decipherment approach is more flexible to incorporate a variety of clues. When it is combined with pv+tv, the performance shows a significant boost and achieves approximately 90% accuracy in top 200 results though it is slightly inferior to our final approach due to the lack of awareness of co-burst.\nWe analyzed translation pairs mined by our approach to see how many of them can be obtained by a transliteration model which is often used for name translation. Among top 100 translation pairs, only 9% can be correctly transliterated by a transliteration model [21], demonstrating that our approach can discover large numbers of translation pairs that cannot be transliterated.\nMoreover, we investigated the performance of our approach under various sizes of data provided, as shown in Figure 5. As observed, when the data size is too small, the approach works poorly. One reason is that there are few nodes to be deciphered in BINets that can be aligned; the other reason is that the burst detection algorithm cannot work well in a small time frame because the base probability q0 cannot be estimated precisely. For example, an important event (e.g., the World Cup) that is frequently mentioned within a month may not be recognized as a bursty event if only the data in this month is available. When we estimate the base probability q0 based on a larger data stream (2009 news streams), the performance of our approach on 1-month data and 3- month data is significantly improved while the performance on 6-month data is not, demonstrating that 6-month data is sufficient for base probability estimation.\nIt is clearly seen from Figure 5 that the more data we have, the more translation pairs that can be mined. Considering massive streaming data generated every day, if the approach can be applied to the data streams, it is possible to monitor the streaming data and automatically mine the translation of new words or entities in a never-ending fashion.\nFor the sake of that goal, a main requirement for our ap-\nproach is that the algorithm should be efficient or can be applied in parallel. To verify the efficiency of our approach, we split the streaming data into two parts based on timestamps: first 6 months and last 6 months, and tested the approach on these two parts.\nTable 1 shows the run time of the decipherment algorithm on different sizes of streaming data, which is measured on a workstation with Intel Xeon 3.5 GHz CPU and 64GB RAM. As data increases, it will take longer time for decipherment. However, it is notable that the decipherment processes for the data of the first 6 months and the last 6 months are independent and thus it is possible to run the decipherment algorithm on these two datasets in parallel and merge the decipherment results.\nFigure 6 shows the performance of the decipherment algorithm in the parallel fashion. We can see that deciphering in parallel does not result in a significant decrease of accuracy. Therefore, we can split a text stream into several small parts and decipher them in parallel, which makes the decipherment more efficient. For the example in Table 1, if we decipher the text streams of the first 6 months and the last 6 months in parallel, the run time of decipherment would be 480.96 seconds assuming the time for merging the results is negligible.\nWe also study the effect of the size of the bi-lingual lexicon on the performance. We randomly sample different sizes of entries from the original bi-lingual lexicon as new bi-lingual lexicons. The results are shown in Figure 7. We can see that the decipherment accuracy increases as the size of bilingual lexicon increases because more prior knowledge can help better decipher the BINet.\nIn order to test the generalization ability, we evaluate our decipherment approach using the same parameters on another coordinated text streams \u2013 AFP Chinese and APW English news stream in 2008. The result is shown in Figure 8. Our decipherment approach consistently outperforms the Bayesian inference approach and still deciphers top 100\nnodes in very high accuracy even though the curve of the streams in 2008 is lower than those in 2010. The difference of the performance on different datasets mainly arises from the differences of topic overlaps. In the streams of 2010, the Chinese and English news streams are from the same news agency (i.e., AFP). Therefore, the topic overlaps between the Chinese and English streams are much more than those of the 2008 coordinated streams, which allows more nodes in the Chinese BINet to be deciphered correctly.\nAlso, we evaluated our approach on extracting translation of bursty words which are the most important information in the text stream by comparing conventional bilingual lexicon extraction methods. Specifically, we test how many OOV words appearing in our BINet are correctly translated. In total, there are 1,226 and 1,082 distinct OOV words (excluding incorrectly segmented words) in our Chinese BINets of the 2010 and 2008 streams respectively. For the BINet-based approaches, we find a Chinese word w\u2019s English translation w\u2217 as follows:\nw\u2217 = w(e\u2217)\ne\u2217 = argmaxe\u2208Ve maxc\u2208Vc(w) Score(c, e)\nwhere Vc(w) is set of Chinese nodes whose word is w, and w(e) denotes the word of node e. For the other bilingual lexicon extraction models, we use the same datasets as comparable corpora and the same bilingual lexicon as seeds to extract translation of the OOV words. Accuracy is used to measure the proportion of the words that are correctly translated, as [20] did.\nTable 2 shows the performance of our approach and typical bilingual lexicon extraction methods. Context is proposed by [22] which extracts translation pairs based on context similarity of words. Diverse is proposed by [23], which is a variant of [22] by adding pinyin and temporal similarity. CoLP and SimLP are label propagation methods proposed by [20] on word co-occurrence and similarity graph respectively. Bayesian is a language decipherment model by [12]. Specially, we evaluate it in two settings (i.e., based on traditional language models and BINets) because it can be run on the BINet as discussed in Section 4.2. According to Table 2, our approach significantly outperforms the other approaches on both datasets due to the rich clues and our candidate generation strategy that can largely narrow down the number of candidates, showing its advantages for translating bursty words in text streams. It is also notable that the BINet-based Bayesian improves the LM-based counterpart significantly, demonstrating the effectiveness of BINets on bi-lingual lexicon extraction from coordinated streams.\nFinally, we list some representative nodes deciphered by our approach in Table 3. Once the BINets are constructed and deciphered, many other powerful knowledge extraction mechanisms are enabled. For example, a natural by-product of our approach is detecting the influential roles of each community. Moreover, we can utilize the BINets to disambiguate entities, construct their profiles and predict hidden relations simultaneously during the decipherment. For example, for node 9 in Table 3, deciphering the nickname \u201c\u5c0f \u5a01\u201d into Serena Williams can greatly benefit cross-lingual en-\ntity linking. Nodes 10-11 and 12-13 also demonstrate that the potential effect on entity linking and coreference resolution. Nodes 14-15 also show that the deciphered BINets can help entity disambiguation. \u201c\u592e\u884c(Central bank)\u201d may refer to different entities during different burst periods. Moreover, the deciphered BINets can also improve entity extraction. For example, in nodes 16-17, \u7fc1\u5c71\u82cf\u59ec(Aung San Suu Kyi) is not recognized as a person name by the Chinese name tagger; instead, it is mistakenly separated into two words \u2013 \u7fc1\u5c71(Aung San) and \u82cf\u59ec(Suu Kyi). However, since \u7fc1 \u5c71(Aung San) and \u82cf\u59ec(Suu Kyi) are deciphered into the same English named entity \u2013 Suu Kyi, we can merge them back to form the correct entity."}, {"heading": "5. RELATED WORK", "text": "Burst patterns have been studied for decades in data mining and natural language processing communities. [24, 6, 25, 26, 7] studied burst detection problem from text streams. [24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.\nExtensive research has also been done on bilingual lexicon induction (e.g., [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.g., [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora. Some of these approaches also exploited temporal similarity and context information. We proposed new and richer clues including burst and graph topological structure. The nodes in BINet are not limited to named entities. More importantly, it is much cheaper to construct BINets than traditional information networks which usually rely on supervised information extraction and large numbers of language-specific resources.\nIn contrast to previous cross-lingual projection work like data transfer [47] and model transfer [48], we do not require any parallel data. Our BINet construction method was inspired by [7, 8]. Our work is the first to apply it to a cross-lingual setting. This is also the first attempt to apply the decipherment idea (e.g., [11, 12, 13]) to graph structures instead of sequence data. Another work related to ours is [41, 45], which used phonetic transliteration and frequency correlation to discover transliteration of entities. Compared with their work, this paper addresses a more general problem \u2013 we not only focus on named entities but all kinds of out-of-vocabulary words and phrases, which means that the number of candidates of a given node is much larger. Moreover, besides the named entity transliteration pairs, the BINets combined with a variety of clues (i.e., similarities\nof pronunciation, translation, context and temporal correlation) can discover word/phrase and named entity translation pairs that cannot be discovered by their work."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we propose an approach to construct and decipher burst information networks constructed from foreign languages, as a novel and unique way to align streaming data. For the first time we propose to model the translation pair mining from non-parallel corpora as a network decipherment problem. Based on the characteristics of co-burst across languages, we proposed 4 novel and effective clues for decipherment. Our approach is language-independent, efficient, effective and can be easily implemented, thus it is useful for never-ending language knowledge acquisition, machine translation and cross-lingual information retrieval systems.\nIn the future, we plan to introduce richer graph-based features (e.g., meta-paths [4]). This self-boosting framework can also mutually enhance extraction and translation acquisition, and demonstrate a successful joint inference procedure across disparate knowledge sources from multiple languages."}, {"heading": "7. REFERENCES", "text": "[1] A. Klementiev and C. Callison-Burch. Predicting\nsalient updates for disaster summarization. In ACL, 2015.\n[2] X. Wang et al. Mining correlated bursty topic patterns from coordinated text streams. In KDD, 2007.\n[3] S.H. Zheng et al. Cross-lingual topic alignment in time series japanese/chinese news. In PACLIC, 2012.\n[4] J. Han et al. Mining heterogeneous information networks. In KDD, 2010.\n[5] Q. Li et al. Constructing information networks using one single model. In EMNLP, 2014.\n[6] J. Kleinberg. Bursty and hierarchical structure in streams. Data Mining and Knowledge Discovery, 2003.\n[7] X. Zhao et al. A novel burst-based text representation model for scalable event detection. In ACL, 2012.\n[8] H. Sayyadi and L. Raschid. A graph analytical approach for topic detection. ACM Transactions on Internet Technology (TOIT), 2013.\n[9] P. Koehn and K. Knight. Learning a translation lexicon from monolingual corpora. In ACL workshop on Unsupervised lexical acquisition, 2002.\n[10] A. Irvine and C. Callison-Burch. Supervised bilingual lexicon induction with multiple monolingual signals. In NAACL, 2013.\n[11] S. Ravi et al. and K. Knight. Deciphering foreign language. In ACL, 2011.\n[12] Q. Dou et al. Large scale decipherment for out-of-domain machine translation. In EMNLP, 2012.\n[13] Q. Dou et al. Beyond parallel data: Joint word alignment and decipherment improves machine translation. In EMNLP, 2014.\n[14] H. Ji. Mining name translations from comparable corpora by creating bilingual information networks. In the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora, 2009.\n[15] W. Lin et al. Unsupervised language-independent name translation mining from wikipedia infoboxes. In\nEMNLP Workshop on Unsupervised Learning for NLP, 2011.\n[16] D. Graff and K. Chen. Chinese gigaword. LDC Catalog No.: LDC2003T09, ISBN, 1:58563\u201358230, 2005.\n[17] D. Graff et al. English gigaword. Linguistic Data Consortium, Philadelphia, 2003.\n[18] C.D. et al. Manning. The Stanford CoreNLP natural language ssing toolkit, 2014.\n[19] R. Zens and H. Ney. Improvements in phrase-based statistical machine translation. In HLT-NAACL, 2004.\n[20] A. Tamura et al. Bilingual lexicon extraction from comparable corpora using label propagation. In EMNLP, 2012.\n[21] S. Jiampojamarn et al. Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion. In NAACL, 2007.\n[22] P. Fung and L.Y. Yee. An ir approach for translating new words from nonparallel and comparable texts. In COLING-ACL, 1998.\n[23] C. Schafer and D. Yarowsky. Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL, 2002.\n[24] R. Swan and D. Jensen. Timemines: Constructing timelines with statistical models of word usage. In KDD Workshop on Text Mining, 2000.\n[25] J. Yao et al. Temporal and social context based burst detection from folksonomies. In AAAI, 2010.\n[26] K. Tamura and H. Kitakami. Location-based burst detection algorithm in spatiotemporal document stream. In DMIN, 2012.\n[27] G.P.C. Fung et al. Parameter free bursty events detection in text streams. In VLDB, 2005.\n[28] Q. He et al. Using burstiness to improve clustering of topics in news streams. In ICDM, 2007.\n[29] X. Wang et al. Mining common topics from multiple asynchronous text streams. In WSDM, 2009.\n[30] Q. Diao et al. Finding bursty topics from microblogs. In ACL, 2012.\n[31] X. Yan et al. A probabilistic model for bursty topic discovery in microblogs. In AAAI, 2015.\n[32] R. Rapp. Automatic identification of word translations from unrelated english and german corpora. In ACL, 1999.\n[33] L. Shao and H.T. Ng. Mining new word translations from comparable corpora. In COLING, 2004.\n[34] Charles F Schafer III. Translation discovery using diverse similarity measures. Johns Hopkins University, 2006.\n[35] A. Hassan et al. Improving named entity translation by exploiting comparable and parallel corpora. In RANLP, 2007.\n[36] A. Haghighi et al. Learning bilingual lexicons from monolingual corpora. In ACL, 2008.\n[37] R. Udupa et al. Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora. In EACL, 2009.\n[38] A. Klementiev and C. Callison-Burch. Bilingual lexicon induction for low-resource languages. In JHU Technical Report, 2010.\n[39] A. Irvine and C. Callison-Burch. Discriminative bilingual lexicon induction. Computational Linguistics,\n2015.\n[40] D. Kiela et al. Visual bilingual lexicon induction with transferred convnet features. In EMNLP, 2015.\n[41] R. Sproat et al. Named entity transliteration with comparable corpora. In ACL, 2006.\n[42] A. Klementiev and D. Roth. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In COLING-ACL, 2006.\n[43] R. Udupa et al. Mining named entity transliteration equivalents from comparable corpora. In CIKM, 2008.\n[44] G. You et al. Mining name translations from entity graph mapping. In EMNLP, 2010.\n[45] A. Kotov et al. Mining named entities with temporally correlated bursts from multilingual web news streams. In WSDM, 2011.\n[46] R. Sellami et al. Mining named entity translation from non parallel corpora. In FLAIRS, 2014.\n[47] S. Pado and M. Lapata. Cross-lingual annotation projection for semantic roles. Journal of Artificial Intelligence Research, 36, 2009.\n[48] R. McDonald et al. Multi-source transfer of delexicalized dependency parsers. In EMNLP, 2011."}], "references": [{"title": "Predicting salient updates for disaster summarization", "author": ["A. Klementiev", "C. Callison-Burch"], "venue": "In ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Mining correlated bursty topic patterns from coordinated text streams", "author": ["X. Wang"], "venue": "In KDD,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Cross-lingual topic alignment in time series japanese/chinese news", "author": ["S.H. Zheng"], "venue": "In PACLIC,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Mining heterogeneous information networks", "author": ["J. Han"], "venue": "In KDD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Constructing information networks using one single model", "author": ["Q. Li"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Bursty and hierarchical structure in streams", "author": ["J. Kleinberg"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "A novel burst-based text representation model for scalable event detection", "author": ["X. Zhao"], "venue": "In ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A graph analytical approach for topic detection", "author": ["H. Sayyadi", "L. Raschid"], "venue": "ACM Transactions on Internet Technology (TOIT),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["P. Koehn", "K. Knight"], "venue": "In ACL workshop on Unsupervised lexical acquisition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Supervised bilingual lexicon induction with multiple monolingual signals", "author": ["A. Irvine", "C. Callison-Burch"], "venue": "In NAACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Deciphering foreign language", "author": ["S. Ravi et al", "K. Knight"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Large scale decipherment for out-of-domain machine translation", "author": ["Q. Dou"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Q. Dou"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Mining name translations from comparable corpora by creating bilingual information networks. In the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora", "author": ["H. Ji"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised language-independent name translation mining from wikipedia infoboxes", "author": ["W. Lin"], "venue": "EMNLP Workshop on Unsupervised Learning for NLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Chinese gigaword", "author": ["D. Graff", "K. Chen"], "venue": "LDC Catalog No.: LDC2003T09, ISBN,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "English gigaword", "author": ["D. Graff"], "venue": "Linguistic Data Consortium, Philadelphia,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "The Stanford CoreNLP natural language ssing toolkit", "author": ["C.D. et al. Manning"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Improvements in phrase-based statistical machine translation", "author": ["R. Zens", "H. Ney"], "venue": "In HLT-NAACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Bilingual lexicon extraction from comparable corpora using label propagation", "author": ["A. Tamura"], "venue": "In EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion", "author": ["S. Jiampojamarn"], "venue": "In NAACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "An ir approach for translating new words from nonparallel and comparable texts", "author": ["P. Fung", "L.Y. Yee"], "venue": "In COLING-ACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Inducing translation lexicons via diverse similarity measures and bridge languages", "author": ["C. Schafer", "D. Yarowsky"], "venue": "In CoNLL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Timemines: Constructing timelines with statistical models of word usage", "author": ["R. Swan", "D. Jensen"], "venue": "In KDD Workshop on Text Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "Temporal and social context based burst detection from folksonomies", "author": ["J. Yao"], "venue": "In AAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Location-based burst detection algorithm in spatiotemporal document stream", "author": ["K. Tamura", "H. Kitakami"], "venue": "In DMIN,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Parameter free bursty events detection in text streams", "author": ["G.P.C. Fung"], "venue": "In VLDB,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Using burstiness to improve clustering of topics in news streams", "author": ["Q. He"], "venue": "In ICDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Mining common topics from multiple asynchronous text streams", "author": ["X. Wang"], "venue": "In WSDM,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Finding bursty topics from microblogs", "author": ["Q. Diao"], "venue": "In ACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "A probabilistic model for bursty topic discovery in microblogs", "author": ["X. Yan"], "venue": "In AAAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Automatic identification of word translations from unrelated english and german corpora", "author": ["R. Rapp"], "venue": "In ACL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Mining new word translations from comparable corpora", "author": ["L. Shao", "H.T. Ng"], "venue": "In COLING,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Translation discovery using diverse similarity measures", "author": ["Charles F Schafer III"], "venue": "Johns Hopkins University,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Improving named entity translation by exploiting comparable and parallel corpora", "author": ["A. Hassan"], "venue": "In RANLP,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi"], "venue": "In ACL,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora", "author": ["R. Udupa"], "venue": "In EACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Bilingual lexicon induction for low-resource languages", "author": ["A. Klementiev", "C. Callison-Burch"], "venue": "In JHU Technical Report,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Discriminative bilingual lexicon induction", "author": ["A. Irvine", "C. Callison-Burch"], "venue": "Computational Linguistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Visual bilingual lexicon induction with transferred convnet features", "author": ["D. Kiela"], "venue": "In EMNLP,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Named entity transliteration with comparable corpora", "author": ["R. Sproat"], "venue": "In ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Weakly supervised named entity transliteration and discovery from multilingual comparable corpora", "author": ["A. Klementiev", "D. Roth"], "venue": "In COLING-ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Mining named entity transliteration equivalents from comparable corpora", "author": ["R. Udupa"], "venue": "In CIKM,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Mining name translations from entity graph mapping", "author": ["G. You"], "venue": "In EMNLP,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "Mining named entities with temporally correlated bursts from multilingual web news streams", "author": ["A. Kotov"], "venue": "In WSDM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Mining named entity translation from non parallel corpora", "author": ["R. Sellami"], "venue": "In FLAIRS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["S. Pado", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["R. McDonald"], "venue": "In EMNLP,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Streaming data is an important source for many applications such as disaster summarization [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Nonetheless, there are many text streams that are topically related and indexed by the same set of time points, which are called coordinated text streams [2].", "startOffset": 154, "endOffset": 157}, {"referenceID": 1, "context": "For example, [2] and [3] tried to discover common topic patterns from coordinated text streams.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "For example, [2] and [3] tried to discover common topic patterns from coordinated text streams.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Most previous work on coordinated text stream mining such as [2] analyzed the streams on the coarse-grained topic level, which inevitably leads to loss of some information.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "Similar to traditional Information Networks [4, 5], nodes within a community in BINets are semantically related (e.", "startOffset": 44, "endOffset": 50}, {"referenceID": 4, "context": "Similar to traditional Information Networks [4, 5], nodes within a community in BINets are semantically related (e.", "startOffset": 44, "endOffset": 50}, {"referenceID": 5, "context": ", [6]) detect a word\u2019s burst states mainly based on comparing a word\u2019s probability q at a time epoch t to its base probablity q0 that is the word\u2019s global probability throughout the stream.", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "In this paper, we use the approach of [7] which is a variant of [6] for burst detection.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "In this paper, we use the approach of [7] which is a variant of [6] for burst detection.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": ", [5]) relied on a large number of linguistic resources and labeled data to construct information networks.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "[8] proposed a co-occurrence based approach to build an information network where each node is a word type (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": ", [9, 10]), decipherment (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": ", [9, 10]), decipherment (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [11, 12, 13]) and name translation mining (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": ", [11, 12, 13]) and name translation mining (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [11, 12, 13]) and name translation mining (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 13, "context": ", [14, 15]), we utilize a few linguistic resources - a bi-lingual lexicon and language-universal representations such as time/calendar date, number, website URL, currency and emoticons to decipher a subset of Chinese nodes.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [14, 15]), we utilize a few linguistic resources - a bi-lingual lexicon and language-universal representations such as time/calendar date, number, website URL, currency and emoticons to decipher a subset of Chinese nodes.", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "We use the 2010 Agence France Presse news story articles in Chinese Gigaword [16] and English Gigaword [17] as our non-parallel news stream corpora.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "We use the 2010 Agence France Presse news story articles in Chinese Gigaword [16] and English Gigaword [17] as our non-parallel news stream corpora.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "We removed stopwords, conducted lemmatization and name tagging for the English corpus and did word segmentation and name tagging for the Chinese corpus using the Stanford CoreNLP toolkit [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 18, "context": "The seed bi-lingual lexicon we used is released by [19] which contains 81,990 Chinese word entries, each of which has an English translation.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "This baseline is similar to the approach of [20].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "\u2022 Co-burst validation: (cv): Use the co-burst clue only \u2022 pv+tv \u2022 pv+tv+nv \u2022 Bayesian Inference: The state-of-the-art algorithm [12] for language decipherment on non-parallel corpora, deciphering a language based on the alignment of its bi-", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "After normalization, we run the decipherment algorithm of [12] which outputs the probability distribution of possible candidates given a cipher node (i.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "Among top 100 translation pairs, only 9% can be correctly transliterated by a transliteration model [21], demonstrating that our approach can discover large numbers of translation pairs that cannot be transliterated.", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Accuracy is used to measure the proportion of the words that are correctly translated, as [20] did.", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "Context is proposed by [22] which extracts translation pairs based on context similarity of words.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Diverse is proposed by [23], which is a variant of [22] by adding pinyin and temporal similarity.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Diverse is proposed by [23], which is a variant of [22] by adding pinyin and temporal similarity.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "CoLP and SimLP are label propagation methods proposed by [20] on word co-occurrence and similarity graph respectively.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "Bayesian is a language decipherment model by [12].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 24, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 23, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 26, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 1, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 27, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 28, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 29, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 6, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 30, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 21, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 31, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 8, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 22, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 32, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 33, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 34, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 35, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 36, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 37, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 19, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 9, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 38, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 39, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 40, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 41, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 42, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 13, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 43, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 44, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 14, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 45, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 46, "context": "In contrast to previous cross-lingual projection work like data transfer [47] and model transfer [48], we do not require any parallel data.", "startOffset": 73, "endOffset": 77}, {"referenceID": 47, "context": "In contrast to previous cross-lingual projection work like data transfer [47] and model transfer [48], we do not require any parallel data.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Our BINet construction method was inspired by [7, 8].", "startOffset": 46, "endOffset": 52}, {"referenceID": 7, "context": "Our BINet construction method was inspired by [7, 8].", "startOffset": 46, "endOffset": 52}, {"referenceID": 10, "context": ", [11, 12, 13]) to graph structures instead of sequence data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": ", [11, 12, 13]) to graph structures instead of sequence data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [11, 12, 13]) to graph structures instead of sequence data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 40, "context": "Another work related to ours is [41, 45], which used phonetic transliteration and frequency correlation to discover transliteration of entities.", "startOffset": 32, "endOffset": 40}, {"referenceID": 44, "context": "Another work related to ours is [41, 45], which used phonetic transliteration and frequency correlation to discover transliteration of entities.", "startOffset": 32, "endOffset": 40}, {"referenceID": 3, "context": ", meta-paths [4]).", "startOffset": 13, "endOffset": 16}], "year": 2016, "abstractText": "Aligning coordinated text streams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: (1). extending coarse-grained topic-level knowledge mining to fine-grained information units such as entities and events; (2). following a novel \u201cDatato-Network-to-Knowledge (D2N2K)\u201d paradigm to construct and utilize network structures to capture and propagate reliable evidence. We introduce a novel Burst Information Network (BINet) representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinated text streams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.", "creator": "LaTeX with hyperref package"}}}