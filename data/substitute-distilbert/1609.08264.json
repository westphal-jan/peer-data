{"id": "1609.08264", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Top-N Recommendation on Graphs", "abstract": "recommender algorithms play an increasingly important role in online applications to help users find what they need or prefer. collaborative filtering algorithms that generate lists by analyzing the user - item rating matrix perform poorly when the item is sparse. to alleviate this suffering, this paper proposes a simple recommendation algorithm that fully exploits competitive similarity information among users and items and intrinsic quality information of the user - item matrix. the dynamic method constructs a mesh representation which preserves affinity data structure information supporting the user - item rating matrix and then performs recommendation task. to capture proximity information about users and items, additional graphs remain constructed. manifold learning idea is used to constrain the new representation to run smooth on these graphs, so as to trace users and item behaviour. numerical model is formulated as a convex classification problem, knowing which we need to insert the well - known sylvester equation only. we carry out extensive empirical evaluations on six benchmark datasets to show the effectiveness of this approach.", "histories": [["v1", "Tue, 27 Sep 2016 05:45:03 GMT  (95kb,D)", "http://arxiv.org/abs/1609.08264v1", "CIKM 2016"]], "COMMENTS": "CIKM 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["zhao kang", "chong peng", "ming yang", "qiang cheng"], "accepted": false, "id": "1609.08264"}, "pdf": {"name": "1609.08264.pdf", "metadata": {"source": "CRF", "title": "Top-N Recommendation on Graphs", "authors": ["Zhao Kang", "Chong Peng", "Ming Yang", "Qiang Cheng"], "emails": ["qcheng}@siu.edu"], "sections": [{"heading": null, "text": "Keywords top-N recommendation; laplacian graph; collaborative filtering"}, {"heading": "1. INTRODUCTION", "text": "Recommender systems have become increasingly indispensable in many applications [1]. Collaborative filtering (CF) based methods are a fundamental building block in many recommender systems. CF based recommender systems predict the ratings of items to be given by a user based on the ratings of the items previously rated by other users who are most similar to the target user.\nCF based methods can be classified into memory-based methods [21] and model-based methods [14, 15]. The former includes two popular methods, user-oriented [9] and item-oriented, e.g., ItemKNN [6], depending on whether the\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nneighborhood information is derived from similar users or items. First they compute similarities between the active item and other items, or between the active user and other users. Then they predict the unknown rating by combining the known rating of top k neighbors. Due to the simplicity of memory-based CF, it has been successfully applied in industry. However, it suffers from several problems, including data sparsity, cold start and data correlation [3], as users typically rate only a small portion of the available items, and they also tend to rate similar items closely. Therefore, the similarities between users or items cannot be accurately obtained with the existing similarity measures such as cosine and Pearson correlation, which in turn compromises the recommendation accuracy.\nTo alleviate the problems of memory-based methods, many model-based methods have been proposed, which use observed ratings to learn a predictive model. Among them, matrix factorization (MF) based models, e.g., PureSVD [5] and weighted regularized MF (WRMF) [10], are very popular due to their capability of capturing the implicit relationships among items and their outstanding performance. Nevertheless, it introduces high computational complexity and also faces the problem of uninterpretable recommendations. Because the rating matrix is sparse, the factorization of the user-item matrix may lead to inferior solutions [7]. By learning an aggregation coefficient matrix [13], recently, sparse linear method (SLIM) [17] has been proposed and shown to be effective. However, it just captures relations between items that have been co-purchased/co-rated by at least one user [12]. Moreover, it only explores the linear relations between items. Another class of methods use Bayesian personalized ranking (BPR) criterion to measure the difference between the rankings of user-purchased items and the remaining items. For instance, BPRMF and BPRKNN [19] have been demonstrated to be effective for implicit feedback datasets.\nIn this paper, we propose a novel Top-N recommendation model based on graphs. This method not only takes into account the neighborhood information, which is encoded by our user graph and item graph, but also reveals hidden structure in the data by deploying graph regularization. In the real world, data often reside on low-dimensional manifolds embedded in a high-dimensional ambient space. Like the Netflix Prize problem, where the size of the user-item matrix can be huge, there exist relationships between users (such as their age, hobbies, education, etc.) and movies (such as their genre, release year, actors, origin country, etc.). Moreover, people sharing the same tastes for a class of movies\nar X\niv :1\n60 9.\n08 26\n4v 1\n[ cs\n.I R\n] 2\n7 Se\np 20\n16\nare likely to rate them similarly. As a result, the rows and columns of the user-item matrix possess important structural information, which should be taken advantage of in actual applications.\nTo preserve local geometric and discriminating structures embedded in a high-dimensional space, numerous manifold learning methods, such as locally linear embedding (LLE) [20], locality preserving projection (LPP) [18], have been proposed. In recent years, graph regularization based nonnegative matrix factorization [4] of data representation has been developed to remedy the failure in representing geometric structures in data. Inspired by this observation, to comprehensively consider the associations between the users and items and the local manifold structures of the user-item data space, we propose to apply both user and item graph regularizations. Unlike many existing recommendation algorithms, we first establish a new representation which is infused with the above information. It turns out that this new representation is not sparse anymore. Therefore, we perform recommendation task with this novel representation."}, {"heading": "2. DEFINITIONS AND NOTATIONS", "text": "Let U = {u1, u2, ..., um} and T = {t1, t2, ..., tn} represent the sets of all users and all items, respectively. The whole set of user-item purchases/ratings are represented by the user-item matrix X of size m \u00d7 n. Element xij is 1 or a positive value if user ui has ever purchased/rated item tj , otherwise it is marked as 0. The i-th row of X denotes the purchase/rating history of user ui on all items. The j-th column of X is the purchase/rating history of all users on item tj . \u2016X\u20162F = \u2211 i \u2211 j x 2 ij is the squared Frobenius norm of X. Tr(\u00b7) stands for the trace operator. I denotes the identity matrix. is the Hadamard product."}, {"heading": "3. PROPOSED MODEL", "text": ""}, {"heading": "3.1 User and Item Graphs", "text": "User-item rating matrix is an overfit representation of user tastes and item descriptions. This leads to problems of synonymy, computational complexity, and potentially poorer results. Therefore, a more compact representation of user tastes and item descriptions is preferred. Graph regularization is effective in preserving local geometric and discriminating structures embedded in a high-dimensional space. It is based on the well known manifold assumption [18]: If two data points such as xi and xj are close in the geodesic distance on the data manifold, then their corresponding representations yi and yj are also close to each other. In practice, it is difficult to accurately estimate the global manifold structure of the data due to the insufficient number of samples and the high dimensionality of the ambient space. Therefore, many methods resort to local manifold structures. Much effort on manifold learning [18] has shown that local geometric structures of the data manifold can be effectively modeled through a nearest neighbor graph on sampled data points.\nWe adopt graph regularization to incorporate user and item proximities. In this paper, we construct two graphs: the user graph and the item graph. We assume that users having similar tastes for items form communities in the user graph, while items having similar appeals to users form com-\nmunities in the item graph. Since \u201cbirds of a feather flock together\u201d, this assumption is plausible and turns out to indeed benefit recommender systems substantially in our experiment results. As an example for movie recommendation, the users are the vertices of a\u201csocial graph\u201dwhose edges represent relations induced by similar tastes.\nMore formally, we construct an undirected weighted graph Gc = (Vc, Ec;Sc) on items, called the item graph. The vertex set Vc corresponds to items {t1, \u00b7 \u00b7 \u00b7 , tn} with each node ti corresponding to a data point xi which is the i-th column ofX. Symmetric adjacency matrix Sc encodes the inter-item information, in which sij is the weight of the edge joining vertices ti and tj and represents how strong the relationship or similarity items ti and tj have. Ec = {eij} is the edge set with each edge eij between nodes ti and tj associated with a weight sij . The graph regularization on the item graph is formulated as\n1\n2 n\u2211 i,j=1 \u2016yi \u2212 yj\u201622sij = n\u2211 i=1 y T i yidii \u2212 n\u2211 i,j=1 y T i yjsij\n=Tr(Y DcY T )\u2212 Tr(Y ScY T ) = Tr(Y LcY T ),\n(1)\nwhereDc is a diagonal matrix with dii = \u2211n\nj=1 sij , and Lc = Dc \u2212 Sc is the graph Laplacian. To preserve the structural information of the manifold, we want (1) to be as small as possible. It is apparent that minimizing (1) imposes the smoothness of the representation coefficients; i.e., if items ti and tj are similar (with a relatively bigger sij), their lowdimensional representations yi and yj are also close to each other. Therefore, optimizing (1) is an attempt to ensure the manifold assumption.\nThe crucial part of graph regularization is the definition of the adjacency matrix Sc. There exist a number of different similarity metrics in the literature [21], e.g., cosine similarity, Pearson correlation coefficient, and adjusted cosine similarity. For simplicity, in our experiment, we use cosine similarity for explicit rating datasets and Jaccard coefficient for implicit feedback datesets. For binary variables, the Jaccard coefficient is a more appropriate similarity metric than cosine because it is insensitive to the amplitudes of ratings. It measures the fraction of users who have interactions with both items over the number of users who have interacted either of them. Formally, according to cosine definition, the similarity sij between two items ti and tj is defined as sij =\nxi\u00b7xj \u2016xi\u20162\u2016xj\u20162\n, where \u2018\u00b7\u2019 denotes the vector dot-product operation. For Jaccard coefficient, sij = |xi\u2229xj | |xi\u222axj |\n, where \u2229 and \u222a represent intersection and union operations, respectively. Likewise, by defining the user graph Gr = (Vr, Er;Sr) whose vertex set Vr corresponds to users {u1, \u00b7 \u00b7 \u00b7 , um}, we get a corresponding expression Tr(Y TLrY ). Here Lr denotes the Laplacian of Gr, which is similarly obtained from the data points corresponding to the users, that is, the rows of X."}, {"heading": "3.2 Model", "text": "By exploiting both user and item graphs, our proposed model can be written as\nmin Y \u2016X \u2212 Y \u20162F + \u03b1Tr(Y LcY T ) + \u03b2Tr(Y T LrY ). (2)\nThe first term of (2) penalizes large deviations of the predictions from the given ratings. The last two terms measure the smoothness of the predicted ratings on the graph structures and encourage the ratings of nodes with affinity to be similar. They can alleviate the data sparsity issue to some extent. When the item neighborhood information is not\navailable, user neighborhood information might exist, vice versa. The parameters \u03b1 and \u03b2 adjust the balance between the reconstruction error and graph regularizations.\nBy setting the derivative of the objective function of (2) with respect to Y to zero, we have\n(\u03b2Lr + I)Y + \u03b1Y Lc = X. (3)\nEquation (3) is the well known Sylvester equation, which costs O(m3) or O(n3) with a general solver. But in our situation, X is usually extremely sparse, and Lr and Lc can also be sparse, especially for large m and n, so the cost can be O(m2) or O(n2), or sometimes even as low as O(m) or O(n) [2]. Many packages or programs are available to solve (3).\nTo use the reconstructed matrix Y to make recommendations for user ui, we just sort ui\u2019s non-purchased/non-rated items based on their scores in non-increasing order and recommend the top N items."}, {"heading": "4. CONNECTION TO EXISTING WORK", "text": "To the best of our knowledge, there are very few studies on graph Laplacian in the context of recommendation task. Graph regularized weighted nonnegative matrix factorization (GWNMF) [7] was proposed to incorporate the neighborhood information in a MF approach. It solves the following problem\nmin U,V \u2016M (X \u2212 UV T )\u20162F + \u03b1Tr(U T LrU) + \u03b2Tr(V T LcV ) s.t. U \u2265 0, V \u2265 0, (4)\nwhere M is an indicator matrix. U and V T are in latent spaces, whose dimensionality is usually specified with an additional parameter. The latent factors are generally not obvious and might not necessarily be interpretable or intuitively understandable. Here (4) has to learn both user and item representations in the latent spaces. In our approach, we just need to learn one representation and thus the learning process is simplified. On the other hand, U and V T are supposed to be of low dimensionality, and thus useful information can be lost during the low-rank approximation of X from U and V T . The encoding of graph Laplacian on U and V T might be not accurate any more. On the contrary, our method can better preserve the information in X, so it can potentially give better recommendations than GWNMF. Moreover, it is well known that several drawbacks exist in the MF approach, e.g., low convergence rate, many local optimums of U and V T due to the non-convexity of (4). In contrast, our model (2) is strongly convex, admitting a unique, globally optimal solution."}, {"heading": "5. EXPERIMENTAL EVALUATION", "text": ""}, {"heading": "5.1 Datasets", "text": "Table 1 shows the characteristics of the datasets. Delicious, lastfm and BX have only implicit feedback. In particular, Delicious was from the bookmarking and tagging information1, in which each URL was bookmarked by at least 3 users. Lastfm represents music artist listening information2, in which each music artist was listened to by at least 10 users and each user listened to at least 5 artists. BX is derived from the Book-Crossing dataset3 such that only implicit interactions were contained and each book was read by at least 10 users.\nFilmTrust, Netflix and Yahoo contain multi-value ratings. Specifically, FilmTrust is a dataset crawled from the entire FilmTrust website4. The Netflix is derived from Netflix Prize dataset5 and each user rated at least 10 movies. The Yahoo dataset is a subset obtained from Yahoo!Movies user ratings6. In this dataset, each user rated at least 5 movies and each movie was rated by at least 3 users."}, {"heading": "5.2 Evaluation Methodology", "text": "For fair comparison, we follow the dataset preparation approach used by SLIM [17] and adopt the 5-fold cross validation. For each fold, a dataset is split into training and test sets by randomly selecting one non-zero entry for each user and putting it in the test set, while using the rest of the data for training. Then a ranked list of size-N items for each user is produced. We subsequently evaluate the method by comparing the ranked list of recommended items with the item in the test set. In the following results presented in this paper, N is equal to 10 by default.\nFor Top-N recommendation, the most direct and meaningful metrics are hit-rate (HR) and the average reciprocal hit-rank (ARHR) [6], since the users only care if a short recommendation list contains the items of interest or not rather than a very long recommendation list. HR is defined as HR = #hits\n#users , where #hits is the number of users\nwhose item in the testing set is contained (i.e., hit) in the size-N recommendation list, and #users is the total number of users. ARHR is defined as: ARHR = 1\n#users \u2211#hits i=1 1 pi ,\nwhere pi is the position of the i-th hit in the ranked Top-N list. In this metric, hits that occur earlier in the ranked list are weighted higher than those occur later, and thus ARHR indicates how strongly an item is recommended."}, {"heading": "6. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "6.1 Top-N Recommendation Performance", "text": "We use 5-fold cross-validation to choose parameters for all competing methods and report their best performance in Table 2. It can be seen that the HR improvements achieved by our method against the next best performing scheme (i.e., SLIM) are quite substantial on lastfm, Yahoo, BX, FilmTrust datasets7. For Delicious and Netflix datasets, our\n1http://www.delicious.com 2 http://www.last.fm 3http://www.informatik.uni-freiburg.de/ cziegler/BX/ 4http://www.librec.net/datasets.html 5http://www.netflixprize.com/ 6http://webscope.sandbox.yahoo.com/catalog.php?datatype=r 7Code is available at https://github.com/sckangz/CIKM16\nperformance is close to the best performance of other methods. In most cases, there is no much difference among other\nstate-of-the-art methods in terms of HR. Figure 1 shows the performance in HR of various methods for different values of\nN(i.e., 5, 10, 15, 20 and 25) on all six datasets. Our method works the best in most cases."}, {"heading": "6.2 Parameter Effects", "text": "Our model involves two trade-off parameters \u03b1 and \u03b2, which dictate how strongly item and user neighborhoods and structure information contribute to the objective and performance. In Figure 2, we depict the effects of different \u03b1 and \u03b2 values on HR and ARHR for dataset FilmTrust and Yahoo. The search for \u03b1 ranges from 1e-6 to 1e-2 with points from {1e-6, 1e-5, 1e-4, 1e-3, 1e-2}, the search points for \u03b2 are from {1e-6, 1e-4, 1e-2}. As can be seen from all figures, our algorithm performs well over a wide range of \u03b1 and \u03b2 values. HR and ARHR share the same trend with varying \u03b1 and \u03b2. Specifically, when \u03b1 is small, HR and ARHR both increase with \u03b1. After a certain point, they begin to decrease. For FilmTrust, the performance with \u03b2 = 0.01 is very stable with respect to \u03b1. This suggests that user-user similarity dominates the FilmTrust dataset."}, {"heading": "6.3 Matrix Reconstruction", "text": "To show how our method reconstructs the user-item matrix, we compare it with the method of next best performance, SLIM, on FilmTrust. The density of FilmTrust is 1.14% and the mean for those non-zero elements is 2.998. The reconstructed matrix X\u0302SLIM from SLIM has a density of 83.21%. For those 1.14% non-zero entries in X, X\u0302SLIM recovers 99.69% of them and their mean value is 1.686. In contrast, the reconstructed matrix by our proposed algorithm has a density of 91.7%. For those 1.14% non-zero entries in X, our method recovers all of them with a mean of 2.975. These facts suggest that our method better recovers X than SLIM. In other words, SLIM loses too much information. This appears to explain the superior performance of our method.\nIn fact, above analysis is equivalent to the two widely used prediction accuracy metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Since our method can recover the original ratings much better than SLIM, our algorithm gives lower MAE and RMSE. This conclusion is consistent with our HR and ARHR evaluation.\nTable 4: Results by using different graphs\nUser Graph Item Graph User-Item Graph FilmTrust 0.638 0.625 0.651\nYahoo 0.303 0.379 0.379"}, {"heading": "6.4 Graph Construction", "text": "10 50 200 500 800 k\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nA cc\nur ac\ny\nHR ARHR\nFigure 3: Influence of neighborhood size k on recommendation accuracy for FilmTrust dataset.\nAs we discussed previously, similarity is an important ingredient of graph construction. In many recommendation algorithms, the similarity computation is crucial to the recommendation quality [8]. To demonstrate the importance of similarity metric, we use the cosine measure rather than the Jaccard coefficient to measure the similarity in binary datasets. We compare the results in Table 3. As demonstrated, for lastfm dataset, HR and ARHR increase after we adopt the cosine similarity. However, for Delicious and BX dataset, the Jaccard coefficient works better. Therefore, the difference of final results can be big for certain datasets with different similarity measures. We expect that the experimental results in Table 2 can be further enhanced if one performs a more careful analysis of Lr and Lc. For example, it has been reported that normalizing the similarity scores can improve the performance [6]. Also, a number of new similarity metrics have recently been proposed, e.g., [16], which may be also exploited.\nAnother important parameter is the neighborhood size k, which cannot be known a priori [11]. For some small datasets, setting a small k may not include all useful neighbors and would infer incomplete relationships. In practice, a large number of ratings from similar users or similar items are not available, due to the sparsity inherent to rating data. We just use the fully connected graph in our experiments. To demonstrate this, we test the effects of neighborhood size k with values {10, 50, 200, 500, 800} on FilmTrust data. As can be seen from Figure 3, the neighborhood size indeed influences the performance of our proposed recommendation method. Specifically, the performance keeps increasing as k increases when k is small compared to the size of dataset, then the performance keeps almost the same as the final accuracy obtained in Table 2 as k it becomes larger. This conforms that a small neighborhood size can not capture all similarity information."}, {"heading": "6.5 Effects of User and Item Graphs", "text": "While the overall improvements are impressive, it would be interesting to see more fine-grained analysis of the impact of user-user and item-item similarity graphs. We use FilmTrust and Yahoo datasets as examples to show the effects of user and item graphs. Table 4 summarizes the HR values obtained with user graph, item graph, and both user\nand item graph. It demonstrates that we are able to obtain the best performance when we combine user and item graph. Thus neighborhood information of users and items can alleviate the problem of data sparsity by taking advantage of structural information more extensively, which in turn benefits the recommendation accuracy."}, {"heading": "7. CONCLUSION", "text": "In this paper, we address the demands for high-quality recommendation on both implicit and explicit feedback datasets. We reconstruct the user-item matrix by fully exploiting the similarity information between users and items concurrently. Moreover, the reconstructed data matrix also respects the manifold structure of the user-item matrix. We conduct a comprehensive set of experiments and compare our method with other state-of-the-art Top-N recommendation algorithms. The results demonstrate that the proposed algorithm works effectively. Due to the simplicity of our model, there is much room to improve. For instance, our model can be easily extended to include side information (e.g., user demographic information, item\u2019s genre, social trust network) by utilizing the graph representation. In some cases, external information is more informative than the neighborhood information."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "This work is supported by the U.S. National Science Foundation under Grant IIS 1218712, National Natural Science Foundation of China under grant 11241005, and Shanxi Scholarship Council of China 2015-093. Q. Cheng is the corresponding author."}, {"heading": "9. REFERENCES", "text": "[1] G. Adomavicius and A. Tuzhilin. Toward the next\ngeneration of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749, 2005.\n[2] P. Benner, R.-C. Li, and N. Truhar. On the adi method for sylvester equations. Journal of Computational and Applied Mathematics, 233(4):1035\u20131045, 2009.\n[3] F. Cacheda, V. Carneiro, D. Ferna\u0301ndez, and V. Formoso. Comparison of collaborative filtering algorithms: Limitations of current techniques and proposals for scalable, high-performance recommender systems. ACM Transactions on the Web, 5(1):2, 2011.\n[4] D. Cai, X. He, J. Han, and T. S. Huang. Graph regularized nonnegative matrix factorization for data representation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(8):1548\u20131560, 2011.\n[5] P. Cremonesi, Y. Koren, and R. Turrin. Performance of recommender algorithms on top-n recommendation tasks. In RecSys, pages 39\u201346. ACM, 2010.\n[6] M. Deshpande and G. Karypis. Item-based top-n recommendation algorithms. ACM Transactions on Information Systems (TOIS), 22(1):143\u2013177, 2004.\n[7] Q. Gu, J. Zhou, and C. H. Ding. Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs. In SDM, pages 199\u2013210. SIAM, 2010.\n[8] G. Guo, J. Zhang, and N. Yorke-Smith. A novel bayesian similarity measure for recommender systems. In IJCAI, pages 2619\u20132625. AAAI Press, 2013.\n[9] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An algorithmic framework for performing collaborative filtering. In SIGIR, pages 230\u2013237. ACM, 1999.\n[10] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for implicit feedback datasets. In ICDM, pages 263\u2013272. IEEE, 2008.\n[11] J. Huang, F. Nie, and H. Huang. A new simplex sparse learning model to measure data similarity for clustering. In Proceedings of the 24th International Conference on Artificial Intelligence, pages 3569\u20133575. AAAI Press, 2015.\n[12] Z. Kang and Q. Cheng. Top-n recommendation with novel rank approximation. In SDM, pages 126\u2013134. SIAM, 2016.\n[13] Z. Kang, C. Peng, and Q. Cheng. Robust subspace clustering via tighter rank approximation. In CIKM, pages 393\u2013401. ACM, 2015.\n[14] Z. Kang, C. Peng, and Q. Cheng. Top-n recommender system via matrix completion. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n[15] N. Koenigstein, P. Ram, and Y. Shavitt. Efficient retrieval of recommendations in a matrix factorization framework. In CIKM, pages 535\u2013544. ACM, 2012.\n[16] H. Liu, Z. Hu, A. Mian, H. Tian, and X. Zhu. A new user similarity model to improve the accuracy of collaborative filtering. Knowledge-Based Systems, 56:156\u2013166, 2014.\n[17] X. Ning and G. Karypis. Slim: Sparse linear methods for top-n recommender systems. In ICDM, pages 497\u2013506. IEEE, 2011.\n[18] X. Niyogi. Locality preserving projections. In NIPS, volume 16, page 153. MIT, 2004.\n[19] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In UAI, pages 452\u2013461. AUAI Press, 2009.\n[20] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000.\n[21] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative filtering recommendation algorithms. In WWW, pages 285\u2013295. ACM, 2001.\n1e-06 1e-05 0.0001 0.001 0.01 ,\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nH R\n-=1e-6 -=1e-4 -=0.01\n(a) FilmTrust\n1e-06 1e-05 0.0001 0.001 0.01 ,\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nA R\nH R\n-=1e-6 -=1e-4 -=0.01\n(b) FilmTrust\n1e-06 1e-05 0.0001 0.001 0.01 ,\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0.37\nH R\n-=1e-6 -=1e-4 -=0.01\n(c) Yahoo\n1e-06 1e-05 0.0001 0.001 0.01 ,\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18\n0.19\nA R\nH R\n-=1e-6 -=1e-4 -=0.01\n(d) Yahoo\nFigure 2: Influence of \u03b1 and \u03b2 on HR and ARHR."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "On the adi method for sylvester equations", "author": ["P. Benner", "R.-C. Li", "N. Truhar"], "venue": "Journal of Computational and Applied Mathematics, 233(4):1035\u20131045,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparison of collaborative filtering algorithms: Limitations of current techniques and proposals for scalable, high-performance recommender systems", "author": ["F. Cacheda", "V. Carneiro", "D. Fern\u00e1ndez", "V. Formoso"], "venue": "ACM Transactions on the Web, 5(1):2,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(8):1548\u20131560,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "RecSys, pages 39\u201346. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Item-based top-n recommendation algorithms", "author": ["M. Deshpande", "G. Karypis"], "venue": "ACM Transactions on Information Systems (TOIS), 22(1):143\u2013177,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Collaborative filtering: Weighted nonnegative matrix factorization incorporating user and item graphs", "author": ["Q. Gu", "J. Zhou", "C.H. Ding"], "venue": "SDM, pages 199\u2013210. SIAM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel bayesian similarity measure for recommender systems", "author": ["G. Guo", "J. Zhang", "N. Yorke-Smith"], "venue": "IJCAI, pages 2619\u20132625. AAAI Press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["J.L. Herlocker", "J.A. Konstan", "A. Borchers", "J. Riedl"], "venue": "SIGIR, pages 230\u2013237. ACM,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "ICDM, pages 263\u2013272. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A new simplex sparse learning model to measure data similarity for clustering", "author": ["J. Huang", "F. Nie", "H. Huang"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, pages 3569\u20133575. AAAI Press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Top-n recommendation with novel rank approximation", "author": ["Z. Kang", "Q. Cheng"], "venue": "SDM, pages 126\u2013134. SIAM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust subspace clustering via tighter rank approximation", "author": ["Z. Kang", "C. Peng", "Q. Cheng"], "venue": "CIKM, pages 393\u2013401. ACM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Top-n recommender system via matrix completion", "author": ["Z. Kang", "C. Peng", "Q. Cheng"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient retrieval of recommendations in a matrix factorization framework", "author": ["N. Koenigstein", "P. Ram", "Y. Shavitt"], "venue": "CIKM, pages 535\u2013544. ACM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A new user similarity model to improve the accuracy of collaborative filtering", "author": ["H. Liu", "Z. Hu", "A. Mian", "H. Tian", "X. Zhu"], "venue": "Knowledge-Based Systems, 56:156\u2013166,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Slim: Sparse linear methods for top-n recommender systems", "author": ["X. Ning", "G. Karypis"], "venue": "ICDM, pages 497\u2013506. IEEE,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality preserving projections", "author": ["X. Niyogi"], "venue": "NIPS, volume 16, page 153. MIT,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "UAI, pages 452\u2013461. AUAI Press,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Riedl"], "venue": "WWW, pages 285\u2013295. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Recommender systems have become increasingly indispensable in many applications [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "CF based methods can be classified into memory-based methods [21] and model-based methods [14, 15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "CF based methods can be classified into memory-based methods [21] and model-based methods [14, 15].", "startOffset": 90, "endOffset": 98}, {"referenceID": 14, "context": "CF based methods can be classified into memory-based methods [21] and model-based methods [14, 15].", "startOffset": 90, "endOffset": 98}, {"referenceID": 8, "context": "The former includes two popular methods, user-oriented [9] and item-oriented, e.", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": ", ItemKNN [6], depending on whether the", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "However, it suffers from several problems, including data sparsity, cold start and data correlation [3], as users typically rate only a small portion of the available items, and they also tend to rate similar items closely.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": ", PureSVD [5] and weighted regularized MF (WRMF) [10], are very popular due to their capability of capturing the implicit relationships among items and their outstanding performance.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": ", PureSVD [5] and weighted regularized MF (WRMF) [10], are very popular due to their capability of capturing the implicit relationships among items and their outstanding performance.", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "Because the rating matrix is sparse, the factorization of the user-item matrix may lead to inferior solutions [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 12, "context": "By learning an aggregation coefficient matrix [13], recently, sparse linear method (SLIM) [17] has been proposed and shown to be effective.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "By learning an aggregation coefficient matrix [13], recently, sparse linear method (SLIM) [17] has been proposed and shown to be effective.", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "However, it just captures relations between items that have been co-purchased/co-rated by at least one user [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "For instance, BPRMF and BPRKNN [19] have been demonstrated to be effective for implicit feedback datasets.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "To preserve local geometric and discriminating structures embedded in a high-dimensional space, numerous manifold learning methods, such as locally linear embedding (LLE) [20], locality preserving projection (LPP) [18], have been proposed.", "startOffset": 171, "endOffset": 175}, {"referenceID": 17, "context": "To preserve local geometric and discriminating structures embedded in a high-dimensional space, numerous manifold learning methods, such as locally linear embedding (LLE) [20], locality preserving projection (LPP) [18], have been proposed.", "startOffset": 214, "endOffset": 218}, {"referenceID": 3, "context": "In recent years, graph regularization based nonnegative matrix factorization [4] of data representation has been developed to remedy the failure in representing geometric structures in data.", "startOffset": 77, "endOffset": 80}, {"referenceID": 17, "context": "It is based on the well known manifold assumption [18]: If two data points such as xi and xj are close in the geodesic distance on the data manifold, then their corresponding representations yi and yj are also close to each other.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Much effort on manifold learning [18] has shown that local geometric structures of the data manifold can be effectively modeled through a nearest neighbor graph on sampled data points.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "There exist a number of different similarity metrics in the literature [21], e.", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "But in our situation, X is usually extremely sparse, and Lr and Lc can also be sparse, especially for large m and n, so the cost can be O(m) or O(n), or sometimes even as low as O(m) or O(n) [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 6, "context": "Graph regularized weighted nonnegative matrix factorization (GWNMF) [7] was proposed to incorporate the neighborhood information in a MF approach.", "startOffset": 68, "endOffset": 71}, {"referenceID": 16, "context": "For fair comparison, we follow the dataset preparation approach used by SLIM [17] and adopt the 5-fold cross validation.", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "For Top-N recommendation, the most direct and meaningful metrics are hit-rate (HR) and the average reciprocal hit-rank (ARHR) [6], since the users only care if a short recommendation list contains the items of interest or not rather than a very long recommendation list.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "In many recommendation algorithms, the similarity computation is crucial to the recommendation quality [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "For example, it has been reported that normalizing the similarity scores can improve the performance [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 15, "context": ", [16], which may be also exploited.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "Another important parameter is the neighborhood size k, which cannot be known a priori [11].", "startOffset": 87, "endOffset": 91}], "year": 2016, "abstractText": "Recommender systems play an increasingly important role in online applications to help users find what they need or prefer. Collaborative filtering algorithms that generate predictions by analyzing the user-item rating matrix perform poorly when the matrix is sparse. To alleviate this problem, this paper proposes a simple recommendation algorithm that fully exploits the similarity information among users and items and intrinsic structural information of the user-item matrix. The proposed method constructs a new representation which preserves affinity and structure information in the user-item rating matrix and then performs recommendation task. To capture proximity information about users and items, two graphs are constructed. Manifold learning idea is used to constrain the new representation to be smooth on these graphs, so as to enforce users and item proximities. Our model is formulated as a convex optimization problem, for which we need to solve the well known Sylvester equation only. We carry out extensive empirical evaluations on six benchmark datasets to show the effectiveness of this approach.", "creator": "LaTeX with hyperref package"}}}