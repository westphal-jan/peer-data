{"id": "1609.09188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis", "abstract": "academic researchers often need to face with a large collection of research papers in the literature. this problem may be even worse with postgraduate students who are new to a field and may not know opportunities to sit. to address this problem, we have inherited an outdated catalog of research papers where the papers have been automatically categorized by a topic model. the catalog contains most papers from the proceedings of eleven artificial intelligence conferences taken 2000 to 1989. rather than the commonly used latent dirichlet allocation, we use a recently proposed method called descriptive latent tree analysis for topic modeling. the resulting topic model contains a minimum of topics so that users can browse the topics under the top level to the bottom level. the topic model contains a cumulative number three general tables at the top level and allows listings of fine - grained topics at the bottom level. it hence can detect topics that have emerged recently.", "histories": [["v1", "Thu, 29 Sep 2016 03:22:01 GMT  (2044kb,D)", "http://arxiv.org/abs/1609.09188v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["leonard k m poon", "nevin l zhang"], "accepted": false, "id": "1609.09188"}, "pdf": {"name": "1609.09188.pdf", "metadata": {"source": "CRF", "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis", "authors": ["Leonard K.M. Poon", "Nevin L. Zhang"], "emails": ["kmpoon@eduhk.hk", "lzhang@cse.ust.hk"], "sections": [{"heading": "Introduction", "text": "Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. Researchers usually use keywords to search for related papers using a search engine. After reading some papers, they then try to group related papers together to discover the main topics in the field. This process can be time-consuming.\nThe approach above can be regarded as bottom-up approach. A top-down approach would be to start with topic hierarchy. Researchers can then pick a general topic and drill down to more specific topics. Papers related to any of the topics can be presented to the researchers when requested.\nTo allow the top-down approach, traditionally a taxonomy has to be defined manually. Papers can then be manually categorized according to the taxonomy. One problem with the traditional method is that it requires much effort. Besides, the topics in the taxonomy may not be able to keep up with recent development.\nTopic models can be used to automate this process. They can be used to detect topics from a collection of documents\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nand categorize documents according to the detected topics. We use a recently proposed method called hierarchical latent tree analysis (HLTA) (Liu, Zhang, and Chen 2014; Chen et al. 2016) for topic modeling. Unlike Latent Dirichlet Allocation (Blei, Ng, and Jordan 2003), HLTA yields a hierarchy of topics. hLDA (Blei, Griffiths, and Jordan 2010) and nHDP (Paisley et al. 2015) are two extensions of LDA for producing topic hierarchy. However, HLTA has been recently shown to produce better quality of topics and topic hierarchy than the two LDA extensions (Chen et al. 2016).\nTo facilitate the top-down approach, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model built with HLTA. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently.\nThis paper is organized as follows. In the next section we review background of our work. We then explain HLTA using an example. Next, we describe the procedure for building the online catalog. In the following section, we show the results and observations obtained from the online catalog. After that, we conclude the paper."}, {"heading": "Background", "text": "In topic modeling, documents are usually represented as bags of words. Consider a collection D = {d1, . . . , dN} of N documents . Suppose M words are included in the vocabulary V = {w1, . . . , wM}. Each document d can be represented as a vector d = (c1, . . . , cM ), where ci represents the count of word wi occurring in the document d. The aim of topic modeling is to detect a number K of topics z1, . . . , zK among the documents D. The number K can be given or learned. The topic model defines a distribution over words for each topic. A topic is often characterized by representative words based on the distribution.\nLatent Dirichlet Allocation (LDA) is a popular method for topic modeling (Blei, Ng, and Jordan 2003; Blei 2012). LDA assumes each document d to belong to the K topics according to a distribution P (Z = zk|d) over the topics.\nar X\niv :1\n60 9.\n09 18\n8v 1\n[ cs\n.C L\n] 2\n9 Se\np 20\n16\nIn other words, \u2211K\n1 P (zk|d) = 1. This kind of model is known as mixed membership model. For each topic zk, LDA defines a conditional distribution P (wi|zk) over words wi. A topic zk can then be characterized by the most probable words according to P (wi|zk)."}, {"heading": "Latent Tree Models", "text": "A latent tree model (LTM) is a tree-structured probabilistic graphical model (Zhang 2004; Chen et al. 2012). Figure 2a shows an example of LTM. When an LTM is used for topic modeling, the leaf nodes represent the observed word variables W , whereas the internal nodes represent the unobserved topic variables Z. All variables are binary. Each word variable Wi \u2208W indicates the presence or absence of the word wi \u2208 V in a document. Each topic variable Zi \u2208 Z indicates whether a document belongs to the i-th topic.\nFor technical convenience, we often root an LTM at one of its latent nodes and regard it as a Bayesian network (Pearl 1988). Then all the edges are directed away from the root. The numerical information of the model includes a marginal distribution for the root and one conditional distribution for each edge. For example, edge Z1314\u2192 dean is associated with probability P (dean|Z1314). The conditional distribution associated with each edge characterizes the probabilistic dependence between the two nodes that the edge connects. The product of all those distributions defines a joint distribution over all the latent variables Z and observed variables W . Denote the parent of a variable X as pa(X) and let pa(X) be an empty set when X is the root. Then the LTM defines a joint distribution over all observed and latent variables as follows:\nP (W ,Z) = \u220f\nX\u2208W\u222aZ\nP (X|pa(X))\n. Given a document d, the values of word variables W are observed. Use d = (w1, . . . , wM ) to denote also those observed values. Whether a document d belongs to a topic Z \u2208 Z can be determined by the probability P (Z|d). The LTM gives a multi-membership model since a document can belong to multiple topics. Unlike in LDA, the topic probabilities P (Z|d) in LTM do not necessarily sum to one."}, {"heading": "Hierarchical Latent Tree Analysis", "text": "For topic modeling, an LTM has to be learned from the document dataD. This requires learning the number of topic variables, the connection between the variables, and the probabilities in the model.\nWe use the method PEM-HLTA proposed by Chen et al. (2016) to build LTMs for topic modeling. The method builds LTMs level by level and is thus also known as hierarchical latent tree analysis (HLTA). In this section, we use an example to illustrate the main ideas of PEM-HLTA. Readers are referred to the original paper for details.\nAs an example, consider a data set that contains the 24 word variables in Figure 2a. PEM-HLTA is an iterative procedure that builds one level of model in each iteration. In the first iteration, it partitions the 24 word variables in 6 clusters (Figure 1c). The clusters are unidimensional in the sense that\nthe co-occurrences of words in each cluster can be properly modeled using a single latent variable. A latent variable is introduced for each cluster to form model in which all variables in a cluster are connected to the latent variable. We metaphorically refer to those models corresponding to the clusters as islands and the latent variables in them as level-1 latent variables.\nThe next step is to link up the 6 islands. This is done by estimating the mutual information (MI) (Cover and Thomas 2006) between every pair of latent variables and building a Chow-Liu tree (Chow and Liu 1968) over them, so as to form an overall model (Liu et al. 2015). The result is the model in the Figure 1b.\nTo build the next level of model, inference is carried out to compute the posterior distribution of each level-1 latent variable for each document. The document is assigned to the state with the maximum posterior probability, resulting in a data set over the level-1 latent variables. The data set is used as the input for the next iteration. In the second iteration, the level-1 latent variables are partitioned into 2 groups. The 2 islands are linked up to form the model shown in Figure 1a. The model in Figure 1a is then stacked on the model in Figure 1b and a new data set over level-2 latent variables are computed. The iteration continues until the model in Figure 2a is obtained.\nIntuitively, the co-occurrence of words are captured by the level-1 latent variables, whose co-occurring patterns are captured by higher level latent variables. Then a topic hierarchy can be extracted, with topics on top more general and topics at the bottom more specific.\nAlgorithm 1 Find-NGrams(D,m,M) Input: D \u2013 Document collection, m \u2013 maximum value of n, M \u2013 number of tokens to be selected. Output: A document collection with individual words replaced by selected n-grams.\n1: Find individual words for each d \u2208 D. 2: Set V to be the set of the M selected words. 3: for n = 2 to m do 4: For each d \u2208 D, form an n-gram for each pair of\nconsecutive tokens t1, t2 in d if t1, t2 \u2208 V and if the resulting n-gram has a length of n.\n5: Denote the set of newly formed n-grams by U . 6: Set V to be the set of M tokens t \u2208 V \u222a U that have the highest tf-idf(t). 7: For each d \u2208 D, replace all pairs of consecutive to-\nkens that can be used to form an n-gram in V . Let D be the collection of documents after replacement.\n8: end for 9: return D."}, {"heading": "Building Online Catalog with HLTA", "text": "In this section, we describe the procedure for building an online catalog of documents with HLTA. The procedure starts with each document contained in a PDF file."}, {"heading": "Extract Text", "text": "Given a PDF file, we extract the text content using Apache PDFBox.1 We remove hyphenation from the extracted text. After that we use Stanford Core NLP (Manning et al. 2014) for sentence splitting and lemmatization.2\nThe normalize the words, we convert all letters to lowercase. We also remove accents and ligatures using the java.text.Normalizer class in the Java library. We use underscore to replace all non-alphanumeric characters and starting digits in a word. We remove stop words3 and words with fewer than 4 characters."}, {"heading": "Convert Data", "text": "After text is extracted, we compute the term frequency and document frequency. The term frequency tf(w, d) is defined as the number of occurrences of a word w in document d. The document frequency df(w) is defined as the number of documents that contain the word w.\nWe remove words that occur in more than 25% of documents. In other words, a word w is removed if df(w) \u2265 0.25N , where N is the number of documents. Given a number M , we select the M words with highest TF-IDF, which is given by:\ntf-idf(w) = 1\nln df(w) \u2211 d\u2208D tf(w, d).\n1http://pdfbox.apache.org/ 2http://stanfordnlp.github.io/CoreNLP/ 3http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-\nstop-list/english.stop\nAfter selecting the words, we can represent each document d as a vector d = (w1, . . . , wM ), where wi corresponds to one of the selected words and its value indicates the presence (wi = 1) or absence (wi = 0) of the word in d.\nInspired by Deng et al. (2016), we consider also ngrams in addition to individual words. For a given value of m, we consider 1-grams, 2-grams, and so on up to m-grams. We also use the term tokens to refer to the ngrams. The leaf nodes of the model in Figure 2a show some examples of n-grams. The examples of 2-grams include vector-representation and word-vector, and an example of 3-gram is conference-world-wide.\nThe n-grams in a collection of documentsD can be found by Algorithm 1. After running the algorithm, each document can be converted to a binary vector similarly as above.\nThe tree structure of LTM limits a word variable to be connected to exactly one parent. The inclusion of ngrams mitigates this limitation. For example, the word bayesian may be related to statistics and the word network to social networks or communities. The 2-gram bayesian-network means a class of graphical models and has a different meaning from the individual words. By including 2-grams, the three tokens are allowed to three different parent nodes that are more closely to their respective meanings."}, {"heading": "Building Model", "text": "After converting the documents to binary vectors, we run PEM-HLTA using those vectors as input data. Note that PEM-HLTA automatically determines the number of levels of latent variables and the number of latent variables in each level. The result of PEM-HLTA is an LTM."}, {"heading": "Extract Topic Hierarchy", "text": "An LTM defines a tree structure of nodes. Each internal node represents a topic. Since the model defines a joint distribution over the variables, we can compute the MI between every pair of variables. For each topic, we compute the MI between each descendent word variable and the topic variable. We then pick the descendent words with highest MI to characterize the topic.\nAs an example, consider extracting topics from the model in Figure 2a. Each shaded node represents a topic. Suppose we want to characterize Z344. The descendent word variables are vector-representation, word-vector, . . ., shaw. We compute the MI between Z344 and each of those word variables. The 7 words with highest MI are shown in the shaded row in Figure 2b. The row corresponds to the topic represented by Z344. The second and third rows correspond to the topics represented by Z2121 and Z1313 respectively. The topic extraction from the model in Figure 2a results in the topic hierarchy shown in Figure 2b.\nIn addition to finding the characterizing words, we also estimate the size of each topic Z, which is given by the marginal probability P (Z). It shows how often a topic is estimated to occur in a document collection. In Figure 2b, the number on each row indicates the topic size."}, {"heading": "Build Online Catalog", "text": "The LTM can be used to classify the documents according to the topics detected. A document d is assigned to topic Z if P (Z = 1|d) > 0.5. We use a webpage to display this information. On the webpage, a topic hierarchy similar to Figure 2b is shown. The hierarchy is built with a jquery plugin called jsTree.4 When a topic is clicked, a list of documents belonging to that topic will be shown."}, {"heading": "Results and Observations", "text": "We have built an online catalog of research papers using the method discussed above. The papers were obtained from the proceedings of two AI conferences, namely AAAI Conference on Artificial Intelligence and International Joint Conference on Artificial Intelligence. Proceedings between year 2000 and 2015 were used. The resulting collection contains 7719 papers. We considered n-grams for n = 1, 2, 3 and selected 10,000 tokens based on TF-IDF."}, {"heading": "Online Catalog and Source Code", "text": "The online catalog can be accessed from the URL http://goo.gl/gtDJC8. A screenshot is shown in Figure 3. The program for building the online catalog was written in Scala and Java. The source code can be obtained from https://github.com/kmpoon/hlta."}, {"heading": "Topic Hierarchy", "text": "The topic hierarchy extracted from the LTM obtained contains 7 levels of topics. Table 1 lists the number of topics for each level.\n4https://www.jstree.com\nThe hierarchy contains 13 top-level topics (Figure 4). The 1sth topic is about kernel methods, matrix factorization, dimensionality reduction. The 2nd one is about classifier and support vector machines. The 3rd one is about probabilistic models and Bayesian methods. The 4th to 7th topics are about knowledge base, natural language processing, computer vision, and agent systems, respectively. The 8th one is about planning and heuristic search. The 9th one is search. The 10th one is about to people and user interface. The 11th one is about social networks and recommender systems. The last two are about satisfiability and logic respectively."}, {"heading": "Features", "text": "The online catalog provides some useful features for topic browsing.\nExpanding and Collapsing Topics. The topic hierarchy webpage allows expanding a topic node to see its child topics or collapsing a topic node to hide its child topics. This allows users to navigate among more general topics and more specific topics. For example, if we expand the upper 5 topics in Figure 4, we see some more specific topics as in Figure 3. The bottom part of Figure 3 shows that the top-level topic about natural language processing can be divided two more specific topics, one about natural language processing and one about information retrieval.\nLevels of Topics. Users can specify a range of levels of topics to show at the top of the webpage (Figure 3). By default the top two levels of topics are shown.\nKeyword Search. Users can enter a keyword at the top of the webpage to search for topics containing that keyword. Topics with the keyword are highlighted (Figure 5).\nDocument List. A list of document belonging to a topic can be shown when a topic node is clicked (Figure 6). The documents are sorted in descending order of membership as indicated by P (Z|d). On this page, the numbers of documents belonging to the topic for each year are also shown in a table. We see that this topic about word-representation is emerging recently. It has only 9 documents from 2000 to 2013 but has 9 documents in 2014 and 58 documents in 2015."}, {"heading": "Trends", "text": "For each document d, we can find the year of d and compute the topic indicator based on P (Z|d) Hence, we can build a linear regression model using topic indicator as a predictor variable the year variable as response variable. We can then use the regression coefficient to estimate the trend of each topic. The trend allows researcher new to a field to consider whether a topic is worth for new work.\nTable 2 shows the top three level-3 topics with an upward trend. The trends are supported by the increasing proportion of documents of the topics as shown in Figure 7."}, {"heading": "Related Works", "text": "Topic browsing tools have been built based on topic models. Many tools use LDA for topic modeling (Gardner et al. 2010; Chaney and Blei 2012; Snyder et al. 2013; Sievert and Shirley 2014). They do not show any hierarchy of topics. Smith, Hawes, and Myers (2014) attempts to build a tool with a topic hierarchy by recursively splitting and remodeling a corpus based on LDA. Unlike HLTA, the topic model does not have a strong statistical basis."}, {"heading": "Conclusions", "text": "We present an online tool that allows users to browse topics from a hierarchy. The topic hierarchy is built using the recently proposed PEM-HLTA. The tool allows users to show documents related to each topic. The tool provides several features for easy browsing.\nIn the future, we will consider using more sophisticated way to detect n-grams. We may also include the hyperlinks\nto the papers in the document list for each topic. The online catalog currently takes some time to load in a web browser. We will consider storing the data in a database so that the loading time can be substantially reduced."}, {"heading": "Acknowledgment", "text": "We thank Peixian Chen for the implementation of PEMHLTA and Zichao Li for downloading the PDF files of the research papers. The work in this paper was supported by the Education University of Hong Kong under project RG90/2014-2015R and Hong Kong Research Grants Council under grant 16202515."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. To address this problem, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet Allocation, we use a recently proposed method called hierarchical latent tree analysis for topic modeling. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently.", "creator": "LaTeX with hyperref package"}}}