{"id": "1609.09444", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation", "abstract": "understanding, predicting, and inducing object motions and transformations is a core problem in artificial intelligence. modeling sequences of exact images should provide better representations and models of motion and may ultimately be desirable for forecasting, simulation, or translation generation. diagrammatic exact reasoning indicates an avenue theory which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in complete series. developing this, we use a novel contextual generative adversarial network based on graph neural networks ( context - wb - gans ), where both the generator and underlying discriminator modules fully based the contextual history ( modeled as rnns ) and the adversarial discriminator guides the generator to produce realistic images for its expected time step in the image sequence. we evaluate the context - rnn - gan model ( and its variants ) at a novel dataset of diagrammatic integer reasoning, where it performs competitively with 10th - grade human performance plus there is still scope for interesting techniques as compared to college - grade human performance. we also evaluate our model on a standard video next - frame prediction task, achieving improved performance over comparable state - of - the - sc.", "histories": [["v1", "Thu, 29 Sep 2016 17:56:32 GMT  (176kb,D)", "http://arxiv.org/abs/1609.09444v1", null], ["v2", "Tue, 6 Dec 2016 13:14:09 GMT  (173kb,D)", "http://arxiv.org/abs/1609.09444v2", "To Appear in AAAI-17 and NIPS Workshop on Adversarial Training"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["viveka kulharia", "arnab ghosh", "amitabha mukerjee", "vinay p namboodiri", "mohit bansal"], "accepted": true, "id": "1609.09444"}, "pdf": {"name": "1609.09444.pdf", "metadata": {"source": "CRF", "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation", "authors": ["Arnab Ghosh", "Viveka Kulharia", "Amitabha Mukerjee", "Vinay Namboodiri", "Mohit Bansal"], "emails": ["mbansal@cs.unc.edu"], "sections": [{"heading": "Introduction", "text": "The recent success of machine learning and neural networks in Atari and Go (Mnih et al., 2015; Silver et al., 2016) has sparked a renewed interest in artificial intelligent models that can perform well at tasks which even humans find challenging. An important and challenging task in this category is abstract reasoning, which measures one\u2019s lateral thinking skills or fluid intelligence, i.e., the ability to quickly identify patterns, logical rules and trends in data, integrate this information, and apply it to solve new problems. Specifically, we address the problem of diagrammatic abstract reasoning (DAR), a subset of Differential Aptitude Tests (DATs), which were introduced by (Bennett et al., 1947) to judge psychometric proficiency. It has featured in Intelligence Assessment Systems since 1950s and has been validated by (Berdie, 1951), who showed that proficiency in DAT-DARs were predictors in engineering training. A DAR task involves the generation of a future diagram based on the sequential evolution of component patterns in the given problem sequence.\n\u2217Equal Contribution\nFig. 1 shows an example problem from our DAT-DAR dataset and highlights the intricacies of the reasoning involved in inferring and generating the correct answer (i.e., the next image in the sequence). Different pattern components on both the sides and both the corners are changing in different and multiple ways, making it an interesting challenge to correctly generate the next image in the sequence.1 Accurate generation models developed for such a reasoning task can be used for general AI applications such as forecasting and simulation generation. These models will also be useful for generation of real-world images and videos, a recent research direction in computer vision and deep learning (Goodfellow et al., 2014; Radford et al., 2015; Denton et al., 2015; Im et al., 2016; Mathieu et al., 2015). In this direction, we present an application of our best models to the task of next frame generation in Moving MNIST videos.\nOur sequential generation model is a temporally recurrent version of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which we name Context-RNN-GANs2, where context refers to the sequential history (modeled as RNNs). This is especially well-suited to our sequential image-based reasoning tasks. In this model, the generator\n1An explanation of the ground truth is that the dashed line first goes to the left, then to the right, and then on both sides, and also changes from single to double, hence the ground truth should have double dashed lines on both the sides. On the corners, the number of slanted lines increase by one after every two images, hence the ground truth should have four slant lines on both the corners.\n2In Context-RNN-GAN, \u2018context\u2019 refers to the adversary receiving previous images (modeled as an RNN) and the generator is also an RNN. The name distinguishes it from our simpler RNNGAN model where the adversary is not contextual (as it only uses a single image) and only the generator is an RNN.\nar X\niv :1\n60 9.\n09 44\n4v 1\n[ cs\n.C V\n] 2\n9 Se\np 20\n16\nis an RNN which tries to generate the correct next image based on the previous sequence of images. The adversarial discriminator (playing a minimax game with the generator) is also an RNN which gets the previous timesteps\u2019 images as context and the current timestep\u2019s image (either the one generated by the generator as a negative sample or the correct image from the dataset for that timestep as a positive sample). In this way, the discriminator uses the information about the images preceding the current timestep as context to better distinguish whether the generator produced realistic images for the particular timestep in the image sequence (as opposed to just producing a realistic image from the data distribution).\nWe also develop novel image representations using an unsupervised Siamese Network (Chopra et al., 2005) for modeling the joint representation of adjacent timesteps. This helps bring in much more information (as input features to the GAN model) about the temporal evolution across consecutive time steps. Modeling of the problem using an unsupervised setting and training as a sequential image language model on large quantities of sequences help the method to generalize better on unseen test sequences.\nEmpirically, we perform quantitative evaluation on the DAT-DAR dataset by first generating the successor image for each test sequence and then measuring similarity between the generated image and the candidate answer images, in a multiple-choice setting of the Intelligence Quotient (IQ) dataset. We then report the accuracy as the percentage of correct hits. We compare several baselines, model variants, and feature representations and find that our Context-RNNGAN model with Siamese CNN features performs the best. Also, compared to human performance, we promisingly find that our final model is competitive with 10th-grade high school students but there is still scope for interesting improvements as compared to advanced engineering college students. We also demonstrate that our Context-RNN-GAN model can successfully model video next-frame generation via the Moving MNIST dataset, where we achieve improved performance over comparable state-of-the-art.\nIn summary, the main contributions of this paper are:\n\u2022 Presenting a new abstract reasoning and image generation dataset with more than 1500 training problems (sequences of five images each, plus eight transformation types, leading to a total collection of more than 60000 training images); and an annotated evaluation test set of 100 problems.\n\u2022 Presenting a novel temporally contextual, RNN-based adversarial generation model, where the adversary has access to the full context of previous preceding images as context for deciding real vs fake sample for that timestep.\n\u2022 Presenting a novel feature representation of temporally adjacent images using a Siamese Network.\n\u2022 Strong performances on two datasets (DAR and MNIST videos), competitive with 10th-grade humans and comparable state-of-the-art."}, {"heading": "Related Work", "text": "(Stern, 1914) introduced IQ Tests to measure the success of an individual at adapting to a specific situation under a specific condition. Visual problems in intelligence tests have been among the earliest and continuously researched problems in AI. It has been looked at in terms of propositional logic beginning with (Evans, 1964) and more recently by (Prade and Richard, 2011). Recently, there has also been significant interest in building systems that compete with humans on a variety of tasks such as geometry-based problems (Seo et al., 2015), physics-based problems (Novak and Bulko, 1992), repetition and symmetry detection (Novak and Bulko, 1992), visual question answering (Antol et al., 2015), and verbal reasoning and analogy (Mikolov et al., 2013; Wang et al., 2015).\nOur task closely relates to the problem of Raven\u2019s Progressive Matrices. There, the problems are more constrained whereby one of the squares of the matrix is missing and the sequential pattern evolves along the columns and rows. This problem has been addressed using a propositional logic based framework (Falkenhainer et al., 1989) and via a theorem prover based approach (Bringsjord and Schimanski, 2004). However, we focus on a novel task which involves more unrestricted pattern movements, bigger datasets, and does not rely on representability in terms of first order propositional logic.\nOur task is also closely related to the task of next frame prediction in videos (Petrovic et al., 2006) which involves predicting the next frame based on previous frames. However, the change across consecutive real-world video frames is extremely small as compared to the evolving shapes and changing spatial dynamics in our diagrammatic reasoning task. Hence, this poses several challenges to us different from the task of video next-frame generation, in which modeling optical flows plays a major role in producing better-looking next frames. Other related work in the video prediction direction include language modeling based approaches (Ranzato et al., 2014), convolution-based LSTMs (Patraucean et al., 2015), adversarial CNNs (Mathieu et al., 2015), context encoders (Pathak et al., 2016) and data-conditioned GANs (Mirza and Osindero, 2014).\nLastly, generative adversarial networks (GANs) (Goodfellow et al., 2014) have also been extended with spatial (and spatio-temporal) recurrence, attention, and structure (Im et al., 2016; Gregor et al., 2015; Wang and Gupta, 2016; Vondrick et al., 2016), whereas we specifically focus on temporal recurrence constraints for frames of a video via RNNs. GANs have been used for high resolution image generation (Radford et al., 2015), image manipulation (Zhu et al., 2016), and text-to-image synthesis (Reed et al., 2016).\n(Mathieu et al., 2015) introduced a multiscale video prediction architecture that uses CNNs and provides a fixed number of previous frames as context to the discriminator. Their model therefore isn\u2019t directly applicable to our DAR task, whereas our model can harness the varying number of previous frames (via RNNs) at each time step, which is helpful for modeling short sequences."}, {"heading": "Models", "text": "We first describe our primary Context-RNN-GAN model and then briefly discuss two simplifications of that model, namely RNN-GAN and a regular RNN.\nThe major motivation for using an adversarial loss was the shortcomings of L-2 and L-1 losses (Fig. 3):\n\u2022 When using an L-2 loss function, some of the generated images were superimpositions of the component parts and were too cluttered.\n\u2022 When using an L-1 loss function, although it was sharper than using an L-2 loss, it was missing some components of the actual diagrams."}, {"heading": "Context-RNN-GAN", "text": "Our Context-RNN-GAN model (Fig. 2) uses the sequential structure of the diagrammatic abstract reasoning problem in a GAN framework, i.e., to generate the next image after a sequence of previous context images. The first basic principle underlying our model is the same as that of the original GAN model by (Goodfellow et al., 2014), which is that the discriminator and generator are trying to play the following minimax game:\nmin\u03b2max\u03b8F (\u03b2, \u03b8) = Ex\u223cpdata [logp\u03b8(y = 1|x)]+ Ex\u223cp\u03b2 [logp\u03b8(y = 0|x)] (1)\nwhere pdata is data\u2019s distribution, p\u03b2 is generator\u2019s distribution with \u03b2 as the vector of parameters for the generator, and p\u03b8 is discriminator\u2019s distribution with \u03b8 as the vector of parameters for the discriminator. The discriminator tries to distinguish between inputs x sampled from the real data and from the generator\u2019s distribution by labeling them as 1 and 0 respectively. On the other hand, the generator tries to fool the discriminator by getting its generated image also labeled as 1 by the discriminator. A GAN model is trained well when the discriminator cannot discriminate between the images generated by the generator and the images from the actual data distribution from which it is sampled. Next, in our case, we importantly also add sequential context to both the generator and the discriminator of the GAN model (via RNNs) to capture the temporal sequence-of-images nature of our task, as described in detail next.\nGenerator We want the generator to generate an output image yt given the previous images (in the question) in sequential order x1 to xt, such that this generated image yt is as close as possible to the correct next timestep\u2019s image in the sequence xt+1. Therefore, we choose the generator to be a sequential RNN model, which generates the sequentially next image, yt = G(x1 . . . xt), trying to fool the discriminator into believing that these actually follow their preceding input, x1 to xt (see Fig. 2). Note that our generator model can use LSTM-based or GRU-based or vanilla RNNs; we choose GRUs based on empirical evaluation.\nDiscriminator We want the discriminator to be able to decide whether a given image actually follows the previous sequence of images x1 . . . xt (and not just whether the given image is from the real data distribution, unlike a traditional GAN model\u2019s discriminator). For this, we inject context into the discriminator too, by including the preceding (context) images, x1 . . . xt, along with generator\u2019s generated image, yt. It provides context for the discriminator to decide whether the generator\u2019s image actually follows the previous (context) images. To model this context, we choose the discriminator to be a sequence model as well, namely a GRU-RNN. This sequence model is essentially a sequenceto-label encoding model which receives the context images as the preceding timesteps and the generator\u2019s image (or actual image) as the last timestep (see Fig. 2). The final hidden state is mapped on to a sigmoid predicting whether it is an actual or fake image for that timestep.\nTraining the Discriminator (D) For a particular timestep (t+1), xt+1 is the correct image for the timestep and yt = G(x1 . . . xt) is the generated image output by the generator G. We train the discriminator D such that (x1 . . . xt, xt+1) is classified as 1 (real) and (x1 . . . xt, yt) as 0 (fake). Therefore the loss function we use to train D is:\nLDadv = timesteps\u22121\u2211 t=1 Lbce(D(x1 . . . xt, xt+1), 1)\n+ Lbce(D(x1 . . . xt, yt), 0)\n(2)\nwhere Lbce is the binary cross entropy loss\nLbce(Y, Y\u0303 ) = \u2212 Y\u0303 log(Y ) + (1\u2212 Y\u0303 )log(1\u2212 Y ) Y\u0303 \u2208 {0, 1}, Y \u2208 [0, 1] (3)\nTraining the Generator (G) For a single sequence of images x1 . . . xtimesteps, keeping the weights of the discriminator D fixed, we minimize the adversarial loss:\nLGadv = timesteps\u22121\u2211 t=1 Lbce(D(x1 . . . xt, G(x1 . . . xt)), 1) (4) Minimizing the above loss implies that the G tries to adjust its weights so that the generated image yt = G(x1 . . . xt) is as close to a \u201dreal\u201d image following x1 . . . xt as judged by the current state of the discriminator. Just minimizing the above loss can lead to instability in training because the generator can generate images yt which are able to fool the discriminator but the manifold might be quite different to xt+1 which it wants to model correctly. Hence, similar to (Mathieu et al., 2015) and (Pathak et al., 2016), we train the model with a combination of adversarial loss (LGadv) and an (Lp) loss, which is defined as :\nLp = timesteps\u22121\u2211 t=1 \u2016xt+1 \u2212 yt\u2016pp (5)\nThe total loss to be minimized for the generator is then LG = \u03bbadvL G adv + \u03bbpLp"}, {"heading": "RNN-GAN", "text": "RNN-GAN model is a simplified version of the ContextRNN-GAN model, where the discriminator is simply a Multilayer Perceptron (i.e. a Fully Connected Network) that only gets the generated (and real) images at the current timestep and no previous context image. The objective of this discriminator is to classify whether the image provided to it is an image from the dataset\u2019s distribution or is it generated by the generator. i.e. the discriminator is replaced from being D(x1..xt, yt) to being just D(yt)."}, {"heading": "RNN", "text": "Finally, the simplest model is a regular RNN (again with GRU units, similar to the generator modules above) which just models the sequence of images by trying to predict features of each image given the features of all the previous images in the sequence. We have tried it with L1 and L2 loss functions."}, {"heading": "Feed-forward Baseline", "text": "The feed-forward network baseline is simply a fullyconnected multi-layered perceptron with 2 layers, and is trained using the first 4 images x1..xt\u22121 to predict the fifth image xt; and during testing, the last 4 images x2..xt are used to predict the features of the answer image xt+1."}, {"heading": "Feature Representations", "text": "In this section, we discuss the various image embedding methods that we used to create input features for the sequence models such as Context-RNN-GAN discussed above.\nRaw Pixels As the first baseline, each of the images was re-sized to the same dimension of 128\u00d7128 and its raw pixel values were used as features, with row-wise stacking of the pixel values. Features generated by each model were resized to visualize the generation.\nHistogram of Oriented Gradients HOG features (Dalal and Triggs, 2005) are obtained by concatenating histograms of occurrences of gradient orientation in each of the cells of the images, hence capturing features corresponding to various edge types and proving to be a good feature detector.\nAutoencoder (Vincent et al., 2008) have shown the effectiveness of autoencoders in reducing the dimensionality of the data. Hence, we used an unsupervised autoencoder with 1000 hidden units in the bottleneck layer and trained it on the images from the dataset. The activations of the bottleneck layer were then used as feature representations of the images.\nPretrained CNN (OverFeat Network) We used 4096 features from the penultimate layers of an OverFeat network (Sermanet et al., 2013) that won in the image localization task in ILSVRC 2013. Having been pre-trained on ImageNet dataset, it has been shown to be useful for tasks such as sketch recognition (Yu et al., 2015). The architecture consists of 7 stages, with 22 layers. Each stage consists of convolutions, rectified linear units (ReLU), and optionally of max-pooling layers. We extracted the output from the 21st layer that is the output of the fully connected layer in the seventh stage before final classification.\nFine-tuned AlexNet model We fine-tuned an AlexNet pre-trained on the Imagenet Challenge 2012 (Krizhevsky et al., 2012). Labels were annotated for our DAT-DAR dataset\u2019s images by dividing each image into four quadrants and each of the four feature types (horizontal/vertical lines, slanted lines, and curved lines and the number of shaded regions) are counted in each quadrant of the image to get 16 labels for multioutput-multiclass classification. Counting of the features is done using the diagram parser used in (Seo et al., 2015). The model used Euclidean loss and produced 16 outputs.\nShallow CNN We trained an end-to-end shallow CNN with only three convolutional layers (separated by ReLU and Pooling layers) followed by a ReLU, dropout and a fully connected layer. We trained it solely on our images\u2019 labels described above. The resultant features of the penultimate layer were used. The learning rate and the kernel size was similar to that used by (Krizhevsky et al., 2012).\nSiamese CNN For the initial timesteps, regular sequential models such as LSTMs face difficulty in generating the next image in the sequence because of lack of sufficient context (because initial timesteps have lesser or no previous context). To resolve this, we propose to learn a joint embedding of (temporally) adjacent images and use that as features for our sequential RNN models.\nFor this, we created a Siamese network (Chopra et al., 2005), which uses the pair of shallow CNNs followed by a fully connected layer on top of each of the CNNs. The CNNs have parameters shared between them and they help find the distance between two images in a feature plane. The Siamese network tries to minimize the distance between similar images and maximize the distance between dissimilar images using contrastive loss (Chopra et al., 2005). Our Siamese network was trained by initializing the two shared CNNs with the parameters of the shallow CNN. It was then fine-tuned on our image dataset via a contrastive loss that uses every pair of two temporally adjacent images in a problem as similar examples and uses all other pairs of images from different problems as dissimilar examples. The activations of the two fully connected layers of the network are fed as features to our GAN models."}, {"heading": "Experimental Setup", "text": ""}, {"heading": "Dataset", "text": "We collected the data from several IQ test books and online resources (Aggarwal, 2016), (Sijwali and Sijwali, 2016), (Gupta, 2016) and (Jha and Siddiquee, 2011). We collected about 1500 training problems (15000 images) and an annotated test set of 100 problems, and then we used transforms such as rotation and mirror reflections across the axes to increase the data by eight times, leading to a total of 12000 problem sequences to train on, each containing a sequence of 5 diagrams. Each test problem consists of five figures for the input sequence question and five as the answer choices, i.e., in a multiple-choice setup to allow easier quantitative evaluation. All the proposed models have been trained only on the five figures from the question part, i.e. the training set consisting of (12000x5) images was used, whereas the (sixth) correct answer figure was not used for training.\nThe models were validated (tuned) using the first 50 % of the answer figure set and tested on the remaining unseen 50 % of the answer figure set from the test/validation set of 100 questions."}, {"heading": "Training Details", "text": "The best (validated) hyperparameters for the Context-RNNGAN was a GRU with two layers of 400 hidden units each for the generator and a GRU with a single layer and 500\nhidden units for the discriminator. The best (validated) hyperparameters for the RNN-GAN model was a GRU with two layers of 400 hidden units each for the generator and a mulilayered perceptron (MLP) The final models of ContextRNN-GAN & the RNN-GAN models use \u03bbadv = 0.05 , \u03bbp = 1, and they use p = 1 for the DAT-DAR task and p = 2 for the next-frame prediction task, similar to (Mathieu et al., 2015) and based on empirical evidence from experiments. The best hyperparameters of the regular RNN was a GRU with one hidden layer of 1000 hidden units for both L1 and L2 based loss functions; adding more layers or more hidden units did not help. The best hyperparameters for the feedforward baseline was 2 layers with 1000 hidden units each for both L1 and L2 based loss functions and adding more layers or hidden units didn\u2019t help. In all of the above models a dropout of 0.50 was applied for each hidden layer. All of the models were trained using the Adam Optimizer (Kingma and Ba, 2014)."}, {"heading": "Evaluation Metrics", "text": "In addition to qualitative evaluation via visualizations of the generated images, we importantly also perform quantitative evaluation by matching the generated image embedding with the embeddings of each of the five candidate answer images (based on cosine distance), and returning the closest matching image. The accuracy is then determined by the number of correct choices made by the model with respect to the total test set size."}, {"heading": "Results and Analysis", "text": ""}, {"heading": "Human Performance on DAR", "text": "To test the proficiency of humans on our DAT-DAR dataset, we conducted a set of experiments on two sets of individuals. The problems were divided into sets of 12 problems each and they were given as much time as required to complete all the questions. They were also first given an example of a problem with an explanation of the answer.3 Advanced college students: 21 senior students from the computer science department of a premier university took part in the first experiment. 10th-grade high school students: 48 students from the 10th grade of a reputed high school took part in the second experiment.\nAs can be seen in Table 1, our diagrammatic abstract reasoning task is quite challenging even for humans, with the\n3To enable sincere participation, we set up a reward-based sytsem for top performers."}, {"heading": "Features Accuracy", "text": ""}, {"heading": "RNN", "text": ""}, {"heading": "RNN-GAN", "text": "best performance of strong undergraduate college students being roughly 44% and of 10th-grade students achieving 37% accuracy."}, {"heading": "Model Performance on DAR", "text": "We next report the model results obtained via our proposed context-RNN-GAN model and its several simpler\nvariants (specified in Models section), combined with the various feature representation methods (specified in Feature Representations section). In Table 2, we first show baseline results for a regular RNN trained on different features. The shallow CNN, Siamese CNN, and HOG features perform best here. Next, we try these best feature settings (and the raw features) on our novel context-RNN-GAN model (and its simpler RNN-GAN variant). As shown in Table 2, our primary context-RNN-GAN model, combined with our novel Siamese CNN features, obtains the best results, even competitive with the performance of 10th-grade human performance. However, there is still a lot of scope for interesting model and feature improvements compared to collegelevel human performance (and beyond), making this a new challenging task and dataset for the community.\nNext-Frame Generation on Moving-MNIST Videos We also tested our Context-RNN-GAN model on the popular Moving-MNIST task introduced by (Srivastava et al., 2015) and show results compared to their methods as well as the convolution-based LSTM model of (Patraucean et al., 2015). For fair comparison, we report their results of the AE-ConvLSTM model which does not model the optical flow. As shown in Table 3, we perform better than such comparable state-of-the-art of multiple metrics, CE (cross entropy error) and SE (squared error on image patches)."}, {"heading": "Qualitiative Generation Visualization", "text": "We also present quantitative evaluation via visualizations of the images generated by our model for both the DAR (Fig. 4) and the Moving-MNIST (Fig. 5) tasks. We show cases where the generated image corresponds closely to the correct answer image. For example, in the DAR task (Fig. 4), the model is able to correctly infer and generate next-sequence diagrams with changing arrow directions, number and type of lines in different corners and sides, multiple shapes interacting in different ways, etc. Similarly, for the MNIST task (Fig. 5), our model is able to correctly generate different digits moving in different amounts and directions."}, {"heading": "Conclusion", "text": "We presented a novel Context-RNN-GAN model that can generate images for sequential reasoning scenarios such as our task of diagrammatic abstract reasoning. When combined with useful feature representations such as those from Siamese CNNs, our model performs competitively with 10th-grade humans but there is still scope for interesting improvements as compared to college-level human performance, making this a novel challenging task for the generation community. Our sequential GAN model is also general enough to be useful for tasks such as next frame generations in a video (where we also achieve strong results on the Moving-MNIST dataset) and other similarly important AI tasks such as forecasting and simulation."}, {"heading": "Acknowledgment", "text": "We would like to thank Dhruv Batra (Georgia Tech), Kundan Kumar (IIT Kanpur), Devi Parikh (Georgia Tech), Deepak Pathak (UC Berkeley), Greg Shakhnarovich (TTI-Chicago), and Shubham Tulsiani (UC Berkeley) for their helpful feedback."}], "references": [{"title": "A Modern Approach To Non-Verbal Reasoning", "author": ["Aggarwal", "D.R.S. 2016] Aggarwal"], "venue": null, "citeRegEx": "Aggarwal and Aggarwal,? \\Q2016\\E", "shortCiteRegEx": "Aggarwal and Aggarwal", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Antol et al", "S. 2015] Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Differential aptitude tests", "author": ["Bennett et al", "G. 1947] Bennett", "H. Seashore", "A. Wesman"], "venue": "Psychological Corporation", "citeRegEx": "al. et al\\.,? \\Q1947\\E", "shortCiteRegEx": "al. et al\\.", "year": 1947}, {"title": "Pulling it all together via psychometric AI", "author": ["Bringsjord", "Schimanski", "S. 2004] Bringsjord", "B. Schimanski"], "venue": "In Proc. of AAAI symposium: achieving human-level intelligence through integrated systems and Research,", "citeRegEx": "Bringsjord et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bringsjord et al\\.", "year": 2004}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra et al", "S. 2005] Chopra", "R. Hadsell", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Triggs", "N. 2005] Dalal", "B. Triggs"], "venue": null, "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton et al", "E.L. 2015] Denton", "S. Chintala", "R Fergus"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A program for the solution of a class of geometric-analogy intelligence-test questions", "author": ["Evans", "T.G. 1964] Evans"], "venue": null, "citeRegEx": "Evans and Evans,? \\Q1964\\E", "shortCiteRegEx": "Evans and Evans", "year": 1964}, {"title": "The structure-mapping engine: Algorithm and examples", "author": ["Falkenhainer et al", "B. 1989] Falkenhainer", "K.D. Forbus", "D. Gentner"], "venue": "Artificial intelligence,", "citeRegEx": "al. et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al. et al\\.", "year": 1989}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor et al", "K. 2015] Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110", "author": ["Im et al", "D.J. 2016] Im", "C.D. Kim", "H. Jiang", "R. Memisevic"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Magical Book Series Non-Verbal Reasoning", "author": ["Jha", "Siddiquee", "P.N. 2011] Jha", "J.R. Siddiquee"], "venue": "BSC Publication Co. Pvt. Ltd.,", "citeRegEx": "Jha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jha et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba", "D. 2014] Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440", "author": ["Mathieu et al", "M. 2015] Mathieu", "C. Couprie", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov et al", "T. 2013] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784", "author": ["Mirza", "Osindero", "M. 2014] Mirza", "S. Osindero"], "venue": null, "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih et al", "V. 2015] Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M Riedmiller"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Uses of diagrams in solving physics problems", "author": ["Novak", "Bulko", "G. 1992] Novak", "W. Bulko"], "venue": "In AAAI Symposium on Reasoning with Diagrammatic Representations", "citeRegEx": "Novak et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Novak et al\\.", "year": 1992}, {"title": "Context encoders: Feature learning by inpainting", "author": ["Pathak et al", "D. 2016] Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["Patraucean et al", "V. 2015] Patraucean", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.06309", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Recursive estimation of generative models of video", "author": ["Petrovic et al", "N. 2006] Petrovic", "A. Ivanovic", "N. Jojic"], "venue": "In Proc. of CVPR,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Analogy-making for solving iq tests: A logical view", "author": ["Prade", "Richard", "H. 2011] Prade", "G. Richard"], "venue": "In CaseBased Reasoning Research and Development,", "citeRegEx": "Prade et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Prade et al\\.", "year": 2011}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434", "author": ["Radford et al", "A. 2015] Radford", "L. Metz", "S. Chintala"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604", "author": ["Ranzato et al", "M. 2014] Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Generative adversarial text to image synthesis", "author": ["Reed et al", "S. 2016] Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "arXiv preprint arXiv:1605.05396", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Solving geometry problems: Combining text and diagram interpretation", "author": ["Seo et al", "M. 2015] Seo", "H. Hajishirzi", "A. Farhadi", "O. Etzioni", "C. Malcolm"], "venue": "Proceedings of EMNLP", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229", "author": ["Sermanet et al", "P. 2013] Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "NonVerbal Reasoning. Arihant Publications (India) LTD", "author": ["Sijwali", "B. 2016] Sijwali", "I. Sijwali"], "venue": null, "citeRegEx": "Sijwali et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sijwali et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["Silver et al", "D. 2016] Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J Schrittwieser"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava et al", "N. 2015] Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent et al", "P. 2008] Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In ICML,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Generating videos with scene dynamics", "author": ["Vondrick et al", "C. 2016] Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "arXiv preprint arXiv:1609.02612", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Solving verbal comprehension questions in iq test by knowledge-powered word embedding", "author": ["Wang et al", "H. 2015] Wang", "B. Gao", "J. Bian", "F. Tian", "Liu", "T.-Y"], "venue": "arXiv preprint arXiv:1505.07909", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generative image modeling using style and structure adversarial networks. arXiv preprint arXiv:1603.05631", "author": ["Wang", "Gupta", "X. 2016] Wang", "A. Gupta"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Sketch-a-net that beats humans", "author": ["Yu et al", "Q. 2015] Yu", "Y. Yang", "Song", "Y.-Z", "T. Xiang", "T.M. Hospedales"], "venue": "In Proc. of BMVC,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Zhu et al", "J. 2016] Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A. Efros"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art.", "creator": "TeX"}}}