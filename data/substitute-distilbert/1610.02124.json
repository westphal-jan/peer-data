{"id": "1610.02124", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction", "abstract": "current methods for automatically evaluating cultural error perception ( gec ) systems rely its gold - standard references. however, these methods suffer from penalizing grammatical edits that are correct therefore not in the gold standard. we conclude that reference - less grammaticality metrics correlate very strongly with human judgments and are competitive whereas the leading reference - lesser evaluation metrics. by interpolating both methods, we achieve bird - of - an - art correlation with human judgments. there, we show that gec metrics are much more secure when they are calculated at mean sentence level instead of the corpus level. we have set up a codalab site for benchmarking gec output using a common dataset and subsequent comparison metrics.", "histories": [["v1", "Fri, 7 Oct 2016 02:17:17 GMT  (49kb,D)", "http://arxiv.org/abs/1610.02124v1", "to appear in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)"]], "COMMENTS": "to appear in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["courtney napoles", "keisuke sakaguchi", "joel r tetreault"], "accepted": true, "id": "1610.02124"}, "pdf": {"name": "1610.02124.pdf", "metadata": {"source": "CRF", "title": "There\u2019s No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction", "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault"], "emails": ["napoles@cs.jhu.edu,", "keisuke@cs.jhu.edu,", "joel.tetreault@grammarly.com"], "sections": [{"heading": null, "text": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics."}, {"heading": "1 Introduction", "text": "Grammatical error correction (GEC) has been evaluated by comparing the changes made by a system to the corrections made in gold-standard annotations. Following the recent shared tasks in this field (e.g., Ng et al. (2014)), several papers have critiqued GEC metrics and proposed new methods. Existing metrics depend on gold-standard corrections and therefore have a notable weakness: systems are penalized for making corrections that do not appear in the references.1 For example, the following output has low metric scores even though three appropriate corrections were made to the input:\n1We refer to the gold-standard corrections as references because gold standard suggests just one accurate correction.\nHowever , people now can contact communicate with anyone people all over the world who can use computers at any time , and there is no time delay of messages .\nThese changes (in red) were not seen in the references and therefore the metrics GLEU and M2 (described in \u00a72) score this output worse than 75% of 15,000 other generated sentences.\nWhile grammaticality-based, reference-less metrics have been effective in estimating the quality of machine translation (MT) output, the utility of such metrics has not been investigated previously for GEC. We hypothesize that such methods can overcome this weakness in reference-based GEC metrics. This paper has four contributions: 1) We develop three grammaticality metrics that are competitive with current reference-based measures and correlate very strongly with human judgments. 2) We achieve state-of-the-art performance when interpolating a leading reference-based metric with a grammaticality metric. 3) We identify an interesting result that the mean of sentence-level scores is substantially better for evaluating systems than the system-level score. 4) We release code for two grammaticality metrics and establish an online platform for evaluating GEC output."}, {"heading": "2 Prior work", "text": "To our knowledge, this is the first work to evaluate GEC without references. Within MT, this task is called quality estimation. MT output is evaluated by its fluency, or adherence to accepted conventions of grammaticality and style, and adequacy, which is the input\u2019s meaning conveyed in the output.\nar X\niv :1\n61 0.\n02 12\n4v 1\n[ cs\n.C L\n] 7\nO ct\n2 01\nQuality estimation targets fluency by estimating the amount of post-editing needed by the output. This has been the topic of recent shared tasks, e.g. Bojar et al. (2015). Specia et al. (2010) evaluated the quality of translations using sentence-level features from the output but not the references, predicting discrete and continuous scores. A strong baseline, QuEst, uses support vector regression trained over 17 features extracted from the output (Specia et al., 2013). Most closely related to our work, Parton et al. (2011) applied features from Educational Testing Service\u2019s e-rater R\u00a9 (Attali and Burstein, 2006) to evaluate MT output with a ranking SVM, without references, and improved performance by including features derived from MT metrics (BLEU, TERp, and METEOR).\nWithin the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations indicate improved fluency and no change in meaning.\nUnlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et al., 2015). Like BLEU (Papineni et al., 2002), GLEU captures both fluency and adequacy with n-gram overlap. Recent work has shown that GLEU has the strongest correlation with human judgments compared to the GEC metrics described above (Sakaguchi et al., 2016). These GEC metrics are all defined at the corpus level, meaning that the statistics are accumulated over the entire output and then used to calculate a single system score."}, {"heading": "3 Explicitly evaluating grammaticality", "text": "GLEU, I-measure, and M2 are calculated based on comparison to reference corrections. These Reference-Based Metrics (RBMs) credit corrections seen in the references and penalize systems for ignoring errors and making bad changes (changing a span of text in an ungrammatical way or introducing errors to grammatical text). However, RBMs make two strong assumptions: that the annotations in the references are correct and that they are complete. We argue that these assumptions are invalid and point to a deficit in current evaluation practices. In GEC, the agreement between raters can be low due to the challenging nature of the task (Bryant and Ng, 2015; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008), indicating that annotations may not be correct or complete.\nAn exhaustive list of all possible corrections would be time-consuming, if not impossible. As a result, RBMs penalize output that has a valid correction that is not present in the references or that addresses an error not corrected in the references. The example in \u00a71 has low GLEU and M2 scores, even though the output addresses two errors (GLEU=0.43 and M2 = 0.00, in the bottom half and quartile of 15k system outputs, respectively).\nTo address these concerns, we propose three metrics to evaluate the grammaticality of output without comparing to the input or a gold-standard sentence (Grammaticality-Based Metrics, or GBMs). We expect GBMs to score sentences, such as our example in \u00a71, more highly. The first two metrics are scored by counting the errors found by existing grammatical error detection tools. The error count score is simply calculated: 1 \u2212 # errors# tokens . Two different tools are used to count errors: e-rater R\u00a9\u2019s grammatical error detection modules (ER) and Language Tool (Mi\u0142kowski, 2010) (LT). We choose these because, while e-rater R\u00a9 is a large-scale, robust tool that detects more errors than Language Tool,2 it is proprietary whereas Language Tool is publicly available and open sourced.\nFor our third method, we estimate a grammaticality score with a linguistic feature-based model (LFM), which is our implementation of Heilman et\n2In the data used for this work, e-rater R\u00a9 detects approximately 15 times more errors than Language Tool.\nal. (2014).3 The LFM score is a ridge regression over a variety of linguistic features related to grammaticality, including the number of misspellings, language model scores, OOV counts, and PCFG and link grammar features. It has been shown to effectively assess the grammaticality of learner writing. LFM predicts a score for each sentence while ER and LT, like the RBMs, can be calculated with either sentence- or document-level statistics. To be consistent with LFM, for all metrics in this work we score each sentence individually and report the system score as the mean of the sentence scores. We discuss the effects of modifying metrics from a corpus-level to a sentence-level in \u00a75.\nConsistent with our hypothesis, ER and LT score the \u00a71 example in the top quartile of outputs and LFM ranks it in the top half."}, {"heading": "3.1 A hybrid metric", "text": "The obvious deficit of GBMs is that they do not measure the adequacy of generated sentences, so they could easily be manipulated with grammatical output that is unrelated to the input. An ideal GEC metric would measure both the grammaticality of a generated sentence and its meaning compared to the original sentence, and would not necessarily need references. The available data of scored system outputs are insufficient for developing a new metric due to their limited size, thus we turn to interpolation to develop a sophisticated metric that jointly captures grammaticality and adequacy.\nTo harness the advantage of RBMs (adequacy) and GBMs (fluency), we build combined metrics, interpolating each RBM with each GBM. For a sentence of system output, the interpolated score (SI) of the GBM score (SG) and RBM score (SR) is computed as follows:\nSI = (1\u2212 \u03bb)SG + \u03bbSR\nAll values of SG and SR are in the interval [0, 1], except for I-measure, which falls between [\u22121, 1], and the distribution varies for each metric.4 The system score is the average SI of all generated sentences.\n3Our implementation is slightly modified in that it does not use features from the PET HPSG parser.\n4Mean scores are GLEU 0.52 \u00b1 0.21, M2 0.21 \u00b1 0.34, IM 0.10\u00b10.30, ER 0.91\u00b10.10, LFM 0.50\u00b10.16, LT 1.00\u00b10.01."}, {"heading": "4 Experiments", "text": "To assess the proposed metrics, we apply the RBMs, GBMs, and interpolated metrics to score the output of 12 systems participating in the CoNLL-2014 Shared Task on GEC (Ng et al., 2014). Recent works have evaluated RBMs by collecting human rankings of these system outputs and comparing them to the metric rankings (Grundkiewicz et al., 2015; Napoles et al., 2015). In this section, we compare each metric\u2019s ranking to the human ranking of Grundkiewicz et al. (2015, Table 3c). We use 20 references for scoring with RBMs: 2 original references, 10 references collected by Bryant and Ng (2015), and 8 references collected by Sakaguchi et al. (2016). The motivations for using 20 references are twofold: the best GEC evaluation method uses these 20 references with the GLEU metric (Sakaguchi et al., 2016), and work in machine translation shows that more references are better for evaluation (Finch et al., 2004). Due to the low agreement discussed in \u00a73, having more references can be beneficial for evaluating a system when there are multiple viable ways of correcting a sentence. Unlike previous GEC evaluations, all metrics reported here use the mean of the sentence-level scores for each system.\nResults are presented in Table 1. The error-count metrics, ER and LT, have stronger correlation than all RBMs except for GLEU, which is the current state of the art. GLEU has the strongest correlation with the human ranking (\u03c1 = 0.852, r = 0.838), followed closely by ER, which has slightly lower Pearson correlation (r = 0.829) but equally as strong rank correlation, which is arguably more important when comparing different systems. I-measure and LFM have similar strength correlations, and M2 is the lowest performing metric, even though its correlation is still strong (\u03c1 = 0.648, r = 0.641).\nNext we compare the interpolated scores with the human ranking, testing 101 different values of \u03bb\n[0,1] to find the oracle value. Table 2 shows the correlations between the human judgments and the oracle interpolated metrics. Correlations of interpolated metrics are the upper bound and the correlations of uninterpolated metrics (in the first column and first row) are the lower bound. Interpolating GLEU and IM with GBMs has stronger correlation than any uninterpolated metric (i.e. \u03bb = 0 or 1), and the oracle interpolation of ER and GLEU manifests the strongest correlation with the human ranking (\u03c1 = 0.885, r = 0.867).5 M2 has the weakest correlation of all uninterpolated metrics and, when combined with GBMs, three of the interpolated metrics have \u03bb = 0, meaning the interpolated score is equivalent to the GBM and M2 does not contribute.\nTable 3 presents an example of how interpolation can help evaluation. The top two sentences ranked by GLEU have misspellings that were not corrected in the NUCLE references. Interpolating with a GBM rightly ranks the misspelled output below the corrected output.\nSince these experiments use a large number of references (20), we determine how different reference sizes affect the interpolated metrics by system-\n5To verify that these metrics cannot be gamed, we interpolated GBMs with RBMs scored against randomized references, and got scores 15% lower than un-gamed scores, on average.\natically increasing the number of references from 1 to 20 and randomly choosing n references to use as a gold standard when calculating the RBM scores, repeating 10 times for each value n (Figure 1). The correlation is nearly as strong with 3 and 20 references (\u03c1 = 0.884 v. 0.885), and interpolating with just 1 reference is nearly as good (0.878) and improves over any uninterpolated metric.\nWe acknowledge that using GBMs is in effect using GEC systems to score other GEC systems. Interestingly, we find that even if the GBMs are imperfect, they still boost performance of the RBMs. GBMs have been trained to recognize errors in different contexts and, conversely, can identify correct grammatical constructions in diverse contexts, where the RBMs only recognize corrections made that appear in the gold standards, which are limited. Therefore GBMs can make a nice complement to shortcomings that RBMs may have."}, {"heading": "5 Sentence-level evaluation", "text": "In the course of our experiments, we noticed that I-measure and GLEU have stronger correlations with the expert human ranking when using the\nmean sentence-level score (Table 4).6 Most notably, I-measure does not correlate at all as a corpuslevel metric but has a very strong correlation at the sentence-level (specifically, \u03c1 improves from -0.055 to 0.769). This could be because corpus-level statistics equally distribute counts of correct annotations over all sentences, even those where the output neglects to make any necessary corrections. Sentencelevel statistics would not average the correct counts over all sentences in this same way. As a result, a corpus-level statistic may over-estimate the quality of system output. Deeper investigation into this phenomenon is needed to understand why the mean sentence-level scores do better."}, {"heading": "6 Summary", "text": "We have identified a shortcoming of reference-based metrics: they penalize changes made that do not appear in the references, even if those changes are acceptable. To address this problem, we developed metrics to explicitly measure grammaticality without relying on reference corrections and showed that the error-count metrics are competitive with the best reference-based metric. Furthermore, by interpolating RBMs with GBMs, the system ranking has even stronger correlation with the human ranking. The ER metric, which was derived from counts of errors detected using e-rater R\u00a9, is nearly as good as the state-of-the-art RBM (GLEU) and the interpolation of these metrics has the strongest reported correlation with the human ranking (\u03c1 = 0.885, r = 0.867). However, since e-rater R\u00a9 is not widely available to researchers, we also tested a metric using Language Tool, which does nearly as well when interpolated with GLEU (\u03c1 = 0.857, r = 0.867).\n6The correlations in Table 4 differ from what was reported in Grundkiewicz et al. (2015) and Napoles et al. (2015) due to the references and sentence-level computation used in this work.\n7Significance is found by applying a two-tailed t-test to the Z-scores attained using Fisher\u2019s z-transformation.\nTwo important points should be noted: First, due to the small sample size (12 system outputs), none of the rankings significantly differ from one another except for the corpus-level I-measure. Secondly, GLEU and the other RBMs already have strong to very strong correlation with the human judgments, which makes it difficult for any combination of metrics to perform substantially higher. The best uninterpolated and interpolated metrics use an extremely large number of references (20), however Figure 1 shows that interpolating GLEU using just one reference has stronger correlation than any uninterpolated metric. This supports the use of interpolation to improve GEC evaluation in any setting.\nThis work is the first exploration into applying fluency-based metrics to GEC evaluation. We believe that, for future work, fluency measures could be further improved with other methods, such as using existing GEC systems to detect errors, or even using an ensemble of systems to improve coverage (indeed, ensembles have been useful in the GEC task itself (Susanto et al., 2014)). There is also recent work from the MT community, such as the use of confidence bounds (Graham and Liu, 2016) or uncertainty measurement (Beck et al., 2016), which could be adopted by the GEC community.\nFinally, in the course of our experiments, we determined that metrics calculated on the sentencelevel is more reliable for evaluating GEC output, and we suggest that the GEC community adopt this modification to better assess systems.\nTo facilitate GEC evaluation, we have set up an online platform8 for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available.9"}, {"heading": "Acknowledgments", "text": "We would like to thank Matt Post, Martin Chodorow, and the three anonymous reviews for their comments and feedback. We also thank Educational Testing Service for providing e-rater R\u00a9 output. This material is based upon work partially supported by the NSF GRF under Grant No. 1232825.\n8https://competitions.codalab.org/ competitions/12731\n9https://github.com/cnap/ grammaticality-metrics"}], "references": [{"title": "Exploring prediction uncertainty in machine translation quality estimation", "author": ["Daniel Beck", "Lucia Specia", "Trevor Cohn."], "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 208\u2013218, Berlin, Germany,", "citeRegEx": "Beck et al\\.,? 2016", "shortCiteRegEx": "Beck et al\\.", "year": 2016}, {"title": "Findings of the 2013 Workshop on Statistical Machine Translation", "author": ["Ond\u0159ej Bojar", "Christian Buck", "Chris Callison-Burch", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia."], "venue": "Proceedings of the", "citeRegEx": "Bojar et al\\.,? 2013", "shortCiteRegEx": "Bojar et al\\.", "year": 2013}, {"title": "How far are we from fully automatic high quality grammatical error correction", "author": ["Christopher Bryant", "Hwee Tou Ng"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Bryant and Ng.,? \\Q2015\\E", "shortCiteRegEx": "Bryant and Ng.", "year": 2015}, {"title": "Findings of the 2012 Workshop on Statistical Machine Translation", "author": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10\u2013", "citeRegEx": "Callison.Burch et al\\.,? 2012", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2012}, {"title": "Better evaluation for grammatical error correction", "author": ["Daniel Dahlmeier", "Hwee Tou Ng."], "venue": "Pro-", "citeRegEx": "Dahlmeier and Ng.,? 2012", "shortCiteRegEx": "Dahlmeier and Ng.", "year": 2012}, {"title": "Helping our own: The HOO 2011 pilot shared task", "author": ["Robert Dale", "Adam Kilgarriff."], "venue": "Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 242\u2013249, Nancy, France, September. Associa-", "citeRegEx": "Dale and Kilgarriff.,? 2011", "shortCiteRegEx": "Dale and Kilgarriff.", "year": 2011}, {"title": "HOO 2012: A report on the preposition and determiner error correction shared task", "author": ["Robert Dale", "Ilya Anisimoff", "George Narroway."], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54\u201362, Montr\u00e9al,", "citeRegEx": "Dale et al\\.,? 2012", "shortCiteRegEx": "Dale et al\\.", "year": 2012}, {"title": "Towards a standard evaluation method for grammatical error detection and correction", "author": ["Mariano Felice", "Ted Briscoe."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics, pages 578\u2013", "citeRegEx": "Felice and Briscoe.,? 2015", "shortCiteRegEx": "Felice and Briscoe.", "year": 2015}, {"title": "How does automatic machine translation evaluation correlate with human scoring as the number of reference translations increases", "author": ["Andrew M. Finch", "Yasuhiro Akiba", "Eiichiro Sumita"], "venue": "In Proceedings of the Fourth International Conference on Language", "citeRegEx": "Finch et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2004}, {"title": "Achieving accurate conclusions in evaluation of automatic machine translation metrics", "author": ["Yvette Graham", "Qun Liu."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Graham and Liu.,? 2016", "shortCiteRegEx": "Graham and Liu.", "year": 2016}, {"title": "Human evaluation of grammatical error correction systems", "author": ["Roman Grundkiewicz", "Marcin Junczys-Dowmunt", "Edward Gillian."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461\u2013470, Lisbon, Portu-", "citeRegEx": "Grundkiewicz et al\\.,? 2015", "shortCiteRegEx": "Grundkiewicz et al\\.", "year": 2015}, {"title": "Predicting grammaticality on an ordinal scale", "author": ["Michael Heilman", "Aoife Cahill", "Nitin Madnani", "Melissa Lopez", "Matthew Mulholland", "Joel Tetreault."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "citeRegEx": "Heilman et al\\.,? 2014", "shortCiteRegEx": "Heilman et al\\.", "year": 2014}, {"title": "Developing an open-source, rule-based proofreading tool", "author": ["Marcin Mi\u0142kowski."], "venue": "Software: Practice and Experience, 40(7):543\u2013566.", "citeRegEx": "Mi\u0142kowski.,? 2010", "shortCiteRegEx": "Mi\u0142kowski.", "year": 2010}, {"title": "Ground truth for grammatical error correction metrics", "author": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Napoles et al\\.,? 2015", "shortCiteRegEx": "Napoles et al\\.", "year": 2015}, {"title": "The CoNLL2013 Shared Task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Ng et al\\.,? 2013", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "The CoNLL-2014 Shared Task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylva-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "E-rating machine translation", "author": ["Kristen Parton", "Joel Tetreault", "Nitin Madnani", "Martin Chodorow."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 108\u2013115. Association for Computational Linguistics.", "citeRegEx": "Parton et al\\.,? 2011", "shortCiteRegEx": "Parton et al\\.", "year": 2011}, {"title": "Annotating ESL errors: Challenges and rewards", "author": ["Alla Rozovskaya", "Dan Roth."], "venue": "Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28\u201336, Los Angeles, California, June. Association for", "citeRegEx": "Rozovskaya and Roth.,? 2010", "shortCiteRegEx": "Rozovskaya and Roth.", "year": 2010}, {"title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality", "author": ["Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault."], "venue": "Transactions of the Association for Computational Linguistics, 4:169\u2013182.", "citeRegEx": "Sakaguchi et al\\.,? 2016", "shortCiteRegEx": "Sakaguchi et al\\.", "year": 2016}, {"title": "Machine translation evaluation versus quality estimation", "author": ["Lucia Specia", "Dhwaj Raj", "Marco Turchi."], "venue": "Machine translation, 24(1):39\u201350.", "citeRegEx": "Specia et al\\.,? 2010", "shortCiteRegEx": "Specia et al\\.", "year": 2010}, {"title": "QuEst \u2013 a translation quality", "author": ["Lucia Specia", "Kashif Shah", "Jose G.C. de Souza", "Trevor Cohn"], "venue": null, "citeRegEx": "Specia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2013}, {"title": "System combination for grammatical error correction", "author": ["Raymond Hendy Susanto", "Peter Phandi", "Hwee Tou Ng."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951\u2013962, Doha, Qatar, October.", "citeRegEx": "Susanto et al\\.,? 2014", "shortCiteRegEx": "Susanto et al\\.", "year": 2014}, {"title": "Native judgments of non-native usage: Experiments in preposition error detection", "author": ["Joel Tetreault", "Martin Chodorow."], "venue": "Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 24\u201332, Manchester, UK, August.", "citeRegEx": "Tetreault and Chodorow.,? 2008", "shortCiteRegEx": "Tetreault and Chodorow.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": ", Ng et al. (2014)), several papers have critiqued GEC metrics and proposed new methods.", "startOffset": 2, "endOffset": 19}, {"referenceID": 1, "context": "Bojar et al. (2015). Specia et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Bojar et al. (2015). Specia et al. (2010) evaluated the qual-", "startOffset": 0, "endOffset": 42}, {"referenceID": 21, "context": "A strong baseline, QuEst, uses support vector regression trained over 17 features extracted from the output (Specia et al., 2013).", "startOffset": 108, "endOffset": 129}, {"referenceID": 17, "context": "Most closely related to our work, Parton et al. (2011) applied features from Educational Testing Service\u2019s e-rater R \u00a9 (Attali and Burstein, 2006) to evaluate MT output with a ranking SVM, without references, and improved performance by including features derived", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012).", "startOffset": 132, "endOffset": 178}, {"referenceID": 6, "context": "The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012).", "startOffset": 132, "endOffset": 178}, {"referenceID": 14, "context": "GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012).", "startOffset": 4, "endOffset": 38}, {"referenceID": 15, "context": "GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012).", "startOffset": 4, "endOffset": 38}, {"referenceID": 4, "context": ", 2014) were scored with the MaxMatch metric (M2), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012).", "startOffset": 142, "endOffset": 166}, {"referenceID": 4, "context": ", 2014) were scored with the MaxMatch metric (M2), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues.", "startOffset": 143, "endOffset": 194}, {"referenceID": 13, "context": "Unlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et al., 2015).", "startOffset": 115, "endOffset": 137}, {"referenceID": 16, "context": "Like BLEU (Papineni et al., 2002), GLEU captures both fluency and adequacy with n-gram overlap.", "startOffset": 10, "endOffset": 33}, {"referenceID": 19, "context": "shown that GLEU has the strongest correlation with human judgments compared to the GEC metrics described above (Sakaguchi et al., 2016).", "startOffset": 111, "endOffset": 135}, {"referenceID": 12, "context": "Two different tools are used to count errors: e-rater R \u00a9\u2019s grammatical error detection modules (ER) and Language Tool (Mi\u0142kowski, 2010) (LT).", "startOffset": 119, "endOffset": 136}, {"referenceID": 15, "context": "Shared Task on GEC (Ng et al., 2014).", "startOffset": 19, "endOffset": 36}, {"referenceID": 10, "context": "Recent works have evaluated RBMs by collecting human rankings of these system outputs and comparing them to the metric rankings (Grundkiewicz et al., 2015; Napoles et al., 2015).", "startOffset": 128, "endOffset": 177}, {"referenceID": 13, "context": "Recent works have evaluated RBMs by collecting human rankings of these system outputs and comparing them to the metric rankings (Grundkiewicz et al., 2015; Napoles et al., 2015).", "startOffset": 128, "endOffset": 177}, {"referenceID": 2, "context": "We use 20 references for scoring with RBMs: 2 original references, 10 references collected by Bryant and Ng (2015), and 8 references collected by Sakaguchi et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 2, "context": "We use 20 references for scoring with RBMs: 2 original references, 10 references collected by Bryant and Ng (2015), and 8 references collected by Sakaguchi et al. (2016). The motivations for using 20 references are twofold:", "startOffset": 94, "endOffset": 170}, {"referenceID": 19, "context": "the best GEC evaluation method uses these 20 references with the GLEU metric (Sakaguchi et al., 2016), and work in machine translation shows that more references are better for evaluation (Finch et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 8, "context": ", 2016), and work in machine translation shows that more references are better for evaluation (Finch et al., 2004).", "startOffset": 94, "endOffset": 114}], "year": 2016, "abstractText": "Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics.", "creator": "LaTeX with hyperref package"}}}