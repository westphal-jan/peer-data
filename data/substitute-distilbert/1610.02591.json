{"id": "1610.02591", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "Solving Marginal MAP Problems with NP Oracles and Parity Constraints", "abstract": "arising from many proposals at the intersection of rule making and software learning, marginal maximum a posteriori ( marginal filtering ) consistently unify the two main classes of inference, namely maximization ( optimization ) and marginal inference ( counting ), and are believed to have higher complexity than comparison of them. stakeholders propose xor _ mmap, a novel approach to solve the marginal map problem, theoretically represents the intractable counting requirement with algorithms to np oracles, subject to additional parity constraints. xor _ mmap provides a constant approximate approximation to the marginal filtering problem, by encoding it as a single optimization in polynomial size of the original problem. we evaluate our approach in computer machine learning and decision making applications, and show that our approach possesses most state - of - the - opinion marginal map solvers.", "histories": [["v1", "Sat, 8 Oct 2016 22:32:35 GMT  (1736kb,D)", "http://arxiv.org/abs/1610.02591v1", null], ["v2", "Tue, 29 Nov 2016 21:22:06 GMT  (3225kb,D)", "http://arxiv.org/abs/1610.02591v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yexiang xue", "zhiyuan li", "stefano ermon", "carla p gomes", "bart selman"], "accepted": true, "id": "1610.02591"}, "pdf": {"name": "1610.02591.pdf", "metadata": {"source": "CRF", "title": "Solving Marginal MAP Problems with NP Oracles and Parity Constraints", "authors": ["Yexiang Xue", "Zhiyuan Li", "Carla P. Gomes", "Bart Selman"], "emails": ["yexiang@cs.cornell.edu", "lizhiyuan13@mails.tsinghua.edu.cn", "ermon@cs.stanford.edu", "gomes@cs.cornell.edu", "selman@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Typical inference queries to make predictions and learn probabilistic models from data include the maximum a posteriori (MAP) inference task, which computes the most likely assignment to a set of variables, as well as the marginal inference task, which computes the probability of an event according to the model. Another common query is the Marginal MAP (MMAP) problem, which involves both maximization (optimization over a set of variables) and marginal inference (averaging over another set of variables).\nMarginal MAP Problems arise naturally in many machine learning applications. For example, learning latent variable models can be formulated as a MMAP inference problem, where the goal is to optimize over the model\u2019s parameters while marginalizing all the hidden variables. MMAP inference queries also arise naturally in the context of decision making under uncertainty, where the goal is to find a decision (optimization) that will perform well on average across multiple uncertain scenarios (averaging).\nThe Marginal MAP Problem is known to be NPPP-complete [13], which is commonly believed to be even harder than MAP inference (NP-hard) and marginal inference (#P-complete). As a supporting evidence, MMAP problems are NP-hard even on tree structured probabilistic graphical models [10]. Previous approaches in addressing the Marginal MAP problem fall into two categories in general.\n\u2217This research was done when Zhiyuan Li was an exchange student at Cornell University.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n02 59\n1v 1\n[ cs\n.A I]\nThe core idea in both categories of approaches is to approximate the intractable marginalization, which often involves averaging over an exponentially large number of possible scenarios. One class of approaches [10, 9, 12, 11, 14] use various variational forms to represent the intractable sum. Then the entire problem can be solved with massage passing algorithms, which correspond to searching for the best variational approximation in an iterative manner. As another family of approaches, Sample Average Approximation (SAA) [15, 16] uses a fixed set of samples to represent the intractable sum, which then transforms the entire problem into a restricted optimization, only considering a finite number of samples. Both approaches treat the optimization and marginalizing components separately. However, we will show that by solving these two tasks in an integrated manner, we can obtain significant computational benefits.\nErmon et. al. [6, 7] recently proposed an alternative approach to approximate intractable counting problems. Their key idea is a mechanism to transform a counting problem into a series of optimization problems, each corresponding to the original problem subject to randomly generated XOR constraints. Based on this mechanism, they developed an algorithm providing a constant-factor approximation to the counting (marginalization) problem.\nWe propose a novel algorithm, called XOR_MMAP, which approximates the intractable sum with a series of optimization problems, which in turn are folded into the global optimization task. Therefore, we effectively reduce the original MMAP inference query to a single joint optimization task of polynomial size of the original MMAP Problem.\nWe show that XOR_MMAP provides a constant factor approximation to the Marginal MAP Problem. Our approach also provides upper and lower bounds on the final result. The quality of the bounds can be improved incrementally with increased computational effort.\nWe evaluate our algorithm on unweighted SAT instances and on weighted Markov Random Field models, comparing our algorithm with variational methods, as well as sample average approximation. We also show the effectiveness of our algorithm on applications in computer vision with deep neural networks and in computational sustainability. Our sustainability application shows how Marginal MAP Problems are also found in scenarios for applying policy interventions to maximize the outcomes of machine learning models. In this setting, the maximization component represents a search over the decision variable to find an optimal intervention policy. As a specific example, we consider a network design application to maximize the spread of cascades [15]. In this application, natural processes, whether they are animal movements or the diffusion of news in social networks, are modeled as cascades. In our probabilistic decision model, the marginals give probabilities for a cascade to reach certain target states, and the overall network design problem is to make optimal policy interventions on the network structure to maximize the spread of the cascade. As a second example in crowdsourcing domain [17], probabilistic models are used to model people\u2019s behaviors. The designer would like to come up with an optimal mechanism, or use a limited amount of incentives to steer people\u2019s effort towards crucial tasks, taking into account the probabilistic behavioral model.\nWe show that XOR_MMAP is able to find better solutions than those found by previous methods, as well as provide tighter bounds."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Problem Definition", "text": "Let A = {0, 1}m be the set of all possible assignments to binary variables a1, . . . , am and X = {0, 1}n be the set of assignments to binary variables x1, . . . , xn. Let w(x, a) : X \u00d7A \u2192 R+ be a function that maps every assignment to a non-negative value. Typical queries over a probabilistic model include the maximization task, which require the computation of maxa\u2208A w(a), and the marginal inference task \u2211 x\u2208X w(x), which essentially sums over X .\nArising naturally from many machine learning applications, the following Marginal Maximum A Posteriori (Marginal MAP) Problem is a joint inference task, which combines the two aforementioned inference tasks:\nmax a\u2208A \u2211 x\u2208X w(x, a). (1)\nAlgorithm 1: XOR_Binary(w : A\u00d7X \u2192 {0, 1}, a0, k) Sample function hk : X \u2192 {0, 1}k from a pair-wise independent function family; Query an NP Oracle on whether W(a0, hk) = {x \u2208 X : w(a0, x) = 1, hk(x) = 0} is empty; Return true ifW(a0, hk) 6= \u2205, otherwise return false.\nWe consider the case where the counting problem \u2211 x\u2208X w(x, a), as well as the maximization problem maxa\u2208A#w(a) are defined over sets of exponential size. Both tasks are therefore intractable in general."}, {"heading": "2.2 Counting by Hashing and Optimization", "text": "Our approach is based on a recent theoretical result that transforms a counting problem to a series of optimization problems [6, 7, 2, 1]. A family of functionsH = {h : {0, 1}n \u2192 {0, 1}k} is said to be pairwise independent if the following two conditions hold for any function h randomly chosen from the familyH: (1) \u2200x \u2208 {0, 1}n, the random variable h(x) is uniformly distributed in {0, 1}k and (2) \u2200x1, x2 \u2208 {0, 1}n, x1 6= x2, the random variables h(x1) and h(x2) are independent. We sample matrices A \u2208 {0, 1}k\u00d7n and vector b \u2208 {0, 1}k uniformly at random to form the function family HA,b = {hA,b : hA,b(x) = Ax + b mod 2}. It\u2019s possible to show that HA,b is pairwise independent [6, 7]. hA,b maps a point in {0, 1}n to a point in {0, 1}k. Define Bh(g) = {x \u2208 {0, 1}n : hA,b(x) = g} to be a \u201cbucket\u201d that includes all elements in {0, 1}n whose mapped value is vector g (g \u2208 {0, 1}k). Notice that each function hA,b(x) = Ax+ b mod 2 corresponds to k parity constraints. Intuitively, if we sample functions hA,b from a pairwise independent family, we get the following: x \u2208 {0, 1}n has an equal probability to be in any bucket B(g), and the bucket locations of any two different elements x, y are independent."}, {"heading": "3 XOR_MMAP Algorithm", "text": ""}, {"heading": "3.1 Binary Case", "text": "We first solve the Marginal MAP Problem for the binary case, in which the function w : A\u00d7X \u2192 {0, 1} outputs either 0 or 1. We will extend the result to the weighted case in the next section. Due to the connection to decision making, where a \u2208 A often represent decision variables, we call a fixed assignment to vector a = a0 a \u201csolution strategy\u201d. To simplify the notation, we use W(a0) to represent the set {x \u2208 X : w(a0, x) = 1}, and use W(a0, hk) to represent the set {x \u2208 X : w(a0, x) = 1 and hk(x) = 0}, in which hk is sampled from a pairwise independent function family that maps X to {0, 1}k. We write #w(a0) as a shorthand for the count |{x \u2208 X : w(a0, x) = 1}| = \u2211 x\u2208X w(a0, x). Our algorithm depends on the following result: Theorem 3.1. (Ermon et. al.[6]) For a fixed solution strategy a0 \u2208 A,\n\u2022 Suppose #w(a0) \u2265 2k0 , then for any k \u2264 k0, with probability 1 \u2212 2 c\n(2c\u22121)2 , Algorithm XOR_Binary(w, a0, k \u2212 c)=true.\n\u2022 Suppose #w(a0) < 2k0 , then for any k \u2265 k0, with probability 1 \u2212 2 c\n(2c\u22121)2 , Algorithm XOR_Binary(w, a0, k + c)=false.\nTo understand Theorem 3.1 intuitively, we can think of hk as a function that maps every element inW(a0) into 2k buckets. Because hk comes from a pairwise independent function family, each element inW(a0) will have an equal probability to be in any one of the 2k buckets, and the buckets in which any two elements end up are mutually independent. Suppose the count of solutions for a fixed strategy #w(a0) is 2k0 , then with high probability, there will be at least one element located in a randomly selected bucket if the number of buckets 2k is less than 2k0 . Otherwise, with high probability there will be no element in a randomly selected bucket. Motivated by this, Ermon et. al.[6] developed a random algorithm that gives a constant factor approximation to counting problems, which corresponds to running XOR_Binary multiple times for each k, and selecting the largest k that XOR_Binary procedure returns true most of the time.\nAlgorithm 2: XOR_K(w : A\u00d7X \u2192 {0, 1}, k, T ) Sample T pair-wise independent hash functions\nh (1) k , h (2) k , . . . , h (T ) k : X \u2192 {0, 1}k;\nQuery Oracle\nmax a\u2208A,x(i)\u2208X T\u2211 i=1 w(a, x(i))\ns.t. h(i)k (x (i)) = 0, i = 1, . . . , T\n(2)\nReturn true if the max value is larger than dT/2e, otherwise return false.\nAlgorithm 3: XOR_MMAP(w : A \u00d7 X \u2192 {0, 1},n = log2 |X |,m = log2 |A|,T ) k = n; while k > 0 do\nif XOR_K(w, k, T ) then Return 2k; end k \u2190 k \u2212 1;\nend Return 1;\nThe insight of Theorem 3.1 is that one could get a rough count on #w(a0) via a series of tests on whetherW(a0, hk) is empty subject to extra parity functions hk. This transforms a counting problem to a series of NP queries, which can be also thought of as optimization queries. This transformation is also extremely helpful for the Marginal MAP Problem. As noted earlier, the main challenge for the marginal MAP Problem is the intractable sum embedded in the maximization. Nevertheless, the whole problem can be re-written as a single optimization if the intractable sum can be approximated well by solving an optimization problem over the same domain.\nWe therefore design Algorithm XOR_MMAP, which is able to provide a constant factor approximation to the Marginal MAP Problem. The whole algorithm is shown in Algorithm 3. In its main procedure XOR_K, the algorithm transforms the Marginal MAP Problem into an optimization over the sum of T replicates of the original problem2. Each replicate is subject to an independent set of parity constraints. Here, x(i) \u2208 X is a replicate of the original x, and w(a, x(i)) is the original w but takes x(i) as one of the inputs. All replicates share a common a. Theorem 3.2 states that XOR_MMAP provides a constant-factor approximation to the Marginal MAP Problem:\nTheorem 3.2. For T \u2265 m ln 2+ln(n/\u03b4)\u03b1\u2217(c) , with probability 1 \u2212 \u03b4, XOR_MMAP(w, log2 |X |, log2 |A|, T ) outputs a 22c-approximation to the Marginal MAP Problem: max\na\u2208A #w(a). \u03b1\u2217(c) is a constant.\nLet us first explain the theorem in an intuitive way. Denote the optimal objective value maxa\u2208A#w(a) as OPT. Denote a\u2217 as the solution that attains the optimum, ie, #w(a\u2217) = OPT. Suppose OPT = 2k0 , then according to Theorem 3.1, the setW(a\u2217, hk) has a high probability to be non-empty, for any function hk that contains k < k0 parity constraints. In this case, the optimization problem max\nx(i)\u2208X ,h(i)k (x(i))=0 w(a\u2217, x(i)) for one replicate x(i) and the fixed a\u2217 almost always returns 1. Because h(i)k (i = 1 . . . T ) are sampled independently, the sum \u2211T i=1 w(a\n\u2217, x(i)) is likely to be larger than dT/2e. Since XOR_K further maximizes this sum over all possible strategies a \u2208 A, the sum it finds will be at least as good as the one attained at a = a\u2217, which is over dT/2e. Therefore, we conclude that when k < k0, XOR_K will return true with high probability.\nOn the other hand, since 2k0 is the optimal objective value across all strategies a, we must have #w(a) \u2264 2k0 for any strategy a \u2208 A. Therefore, when k > k0, with high probability, the optimization problem max\nx(i)\u2208X ,h(i)k (x(i))=0 w(a0, x\n(i)) will return 0 for a fixed strategy a0 if h (i) k\ninvolves k > k0 parity constraints. Nevertheless, since we jointly maximize over a \u2208 A and x(i) \u2208 X in XOR_K, the failure probabilities (that one replicate does not return 0 as expected) will accumulate over different strategies a. We thus need to replicate the optimization problem T times, and apply a union bound on the total failure probability. As a counter-example, suppose function w(x, a) = 1 if and only if x = a, otherwise w(x, a) = 0 (m = n in this case). If we set the number of replicates T = 1, then XOR_K will almost always return 1 when k < n, which suggests that there are 2n solutions to the MMAP problem. Nevertheless, in this case the true optimal value of maxx#w(x, a) is 1, which is far away from 2n. This suggests at least more than one replicate is needed.\nLemma 3.3. For T \u2265 ln 2\u00b7m+ln(n/\u03b4)\u03b1\u2217(c) , procedure XOR_K(w,k) satisfies: 2Supplementary materials include the encoding of the sum of replicates when w(a, x) is in CNF.\n\u2022 Suppose \u2203a\u2217 \u2208 A, s.t. #w(a\u2217) \u2265 2k, then with probability 1\u2212 \u03b4n2m , XOR_K(w, k \u2212 c, T ) returns true.\n\u2022 Suppose \u2200a0 \u2208 A, s.t. #w(a0) < 2k, then with probability 1 \u2212 \u03b4n , XOR_K(w, k + c, T ) returns false.\nProof. Claim 1: If there exists such a\u2217 satisfying #w(a\u2217) \u2265 2k, pick a0 = a\u2217. Let X(i)(a0) = max\nx(i)\u2208X ,h(i)k\u2212c(x(i))=0 w(a0, x\n(i)), for i = 1 . . . , T . From Theorem 3.2, X(i)(a0) = 1 holds with\nprobability 1\u2212 2c(2c\u22121)2 . Let \u03b1\u2217(c) = D( 12\u2016 2 c (2c\u22121)2 ). By Chernoff Bound,\nPr [ max a\u2208A T\u2211 i=1 X(i)(a) \u2264 T/2 ] \u2264 Pr [ T\u2211 i=1 X(i)(a0) \u2264 T/2 ] \u2264 e\u2212D( 1 2\u2016 2c (2c\u22121)2 )T = e\u2212\u03b1 \u2217(c)T , (3)\nwhere\nD\n( 1\n2 \u2016 2\nc (2c \u2212 1)2 ) = 2 ln(2c \u2212 1)\u2212 ln 2\u2212 1 2 ln(2c)\u2212 1 2 ln((2c \u2212 1)2 \u2212 2c) \u2265 ( c 2 \u2212 2) ln 2.\nFor T \u2265 ln 2\u00b7m+ln(n/\u03b4)\u03b1\u2217(c) , we have e\u2212\u03b1 \u2217(c)T \u2264 \u03b4n2m . Thus, with probability 1 \u2212 \u03b4n2m , we have\u2211T\ni=1X (i)(a0) > T/2, which implies that XOR_K(w, k \u2212 c, T ) returns true.\nClaim 2: The proof is almost the same as Claim 1, except that we need to use a union bound to let the property hold for all a \u2208 A simultaneously. As a result, the success probability will be 1 \u2212 \u03b4n instead of 1 \u2212 \u03b4n2m . In detail, let Yi(a) = maxx(i)\u2208X ,h(i)k+c(x(i))=0 w(a0, x (i)) for i = 1, . . . , T . If for all a \u2208 A, #w(a) < 2k, then for any fixed a0,\nPr [ T\u2211 i=1 Yi(a0) > T 2 ] \u2264 e\u2212D ( 1 2\u2016 2c (2c\u22121)2 ) T \u2264 \u03b4 n2m . (4)\nUsing Union bound, we have:\nPr [ max a\u2208A T\u2211 i=1 Yi(a) > T 2 ] \u2264 Pr [ \u2203a \u2208 A, T\u2211 i=1 Yi(a) > T 2 ] \u2264 2m \u03b4 n2m = \u03b4 n . (5)\nTherefore, w.p. 1\u2212 \u03b4n , we have \u2200a, \u2211T i=1 Yi(a) \u2264 T2 , which implies that XOR_K returns false.\nProof. (Theorem 3.2) With probability 1 \u2212 n \u03b4n = 1 \u2212 \u03b4, the output of n calls of XOR_K(w, k, T ) (with different k = 1 . . . n) all satisfy the two claims in Lemma 3.3 simultaneously. Suppose max a\u2208A\n#w(a) \u2208 [2k0 , 2k0+1), we have (i) \u2200k \u2265 k0 + c + 1, XOR_K(w, k, T ) returns false, (ii) \u2200k \u2264 k0 \u2212 c, XOR_K(w, k, T ) returns true. Therefore, with probability 1 \u2212 \u03b4, the output of XOR_MMAP is guaranteed to be among 2k0\u2212c and 2k0+c.\nAcceleration using Binary Search and Reducing the Number of Replicates We can further use binary search to reduce the number of calls to XOR_K procedure. In this case, we only need to check the output of log2 n calls to XOR_K(w, k, T ). This can also reduce the number of replicates T to dm ln 2+ln log2 n+ln(1/\u03b4)\u03b1\u2217(c) e. We can further reduce this number by repeating the algorithm multiple times, and with more precise mathematical analysis."}, {"heading": "3.2 Extension to the Weighted Case", "text": "In this section, we study the more general case, where w(a, x) takes non-negative real numbers instead of integers in {0, 1}. Unlike in [6], we choose to build our proof from the unweighted case because it can effectively avoid modeling the median [5], which is difficult to encode in integer programming. Let w : A\u00d7X \u2192 R+, and M = maxa,x w(a, x).\nDefinition 3.4. We define the embedding Sa(w, l) of X in X \u00d7 {0, 1}l as:\nSa(w, l) = { (x, y)|\u22001 \u2264 i \u2264 l, w(a, x)\nM \u2264 2\ni\u22121 2l \u21d2 yi = 0\n} . (6)\nLemma 3.5. Let w\u2032l(a, x, y) be an indicator variable of whether (x, y) is in Sa(w, l) or not, ie, w\u2032l(a, x, y) = 1(x,y)\u2208Sa(w,l), we claim that\nmax a \u2211 x w(a, x) \u2264 M 2l max a \u2211 (x,y) w\u2032l(a, x, y) \u2264 2max a \u2211 x w(a, x) +M2n\u2212l.3 (7)\nProof. Define Sa(w, l, x0) as the set of (x, y) pairs within the set Sa(w, l) and x = x0, ie, Sa(w, l, x0) = {(x, y) \u2208 Sa(w, l) : x = x0}. It is not hard to see that \u2211 (x,y) w \u2032 l(a, x, y) =\u2211\nx |Sa(w, l, x)|. We are going to establish relationships between |Sa(w, l, x)| and w(a, x), and use that result to show the relationships between the sum \u2211 x |Sa(w, l, x)| and \u2211 x w(x, a).\nIf w(a, x) is sandwiched between two exponential levels M 2l 2i\u22121 < w(a, x) \u2264 M 2l 2i for an i \u2208 {0, 1, . . . , l}, according to our definition, for any (x, y) \u2208 Sa(w, l, x), we have yi+1 = yi+2 = . . . = yl = 0. This exactly renders |Sa(w, l, x)| = 2i, which further implies that\nM 2l \u00b7 |Sa(w, l, x)| 2 < w(a, x) \u2264 M 2l \u00b7 |Sa(w, l, x)|, (8)\nequivalently,\nw(a, x) \u2264 M 2l \u00b7 |Sa(w, l, x)| < 2w(a, x). (9)\nIn the remaining case, we have w(a, x) \u2264 M 2l+1 , in which case |Sa(w, l, x)| = 1. In other words,\n2w(a, x) \u2264 M 2l |Sa(w, l, x)| = M 2l . (10)\nTo sum up, the following bound holds in both cases,\nw(a, x) \u2264 M 2l |Sa(w, l, x)| \u2264 2w(a, x) +M2\u2212l. (11)\nThe lemma holds by summing up over X and maximizing over A.\nWith the result of Lemma 3.5, we are ready to prove the following approximation result:\nTheorem 3.6. Suppose there is an algorithm that gives a c-approximation to solve the unweighted problem: maxa \u2211 (x,y) w \u2032 l(a, x, y), then we have a 3c-approximation algorithm to solve the weighted\nMarginal MAP Problem maxa \u2211 x w(a, x). Proof. Pick l = n in Lemma 3.5. By definition M = maxa,x w(a, x) \u2264 maxa \u2211 x w(a, x), we have:\nmax a \u2211 x w(a, x) \u2264 M 2l max a \u2211 (x,y) w\u2032l(a, x, y) \u2264 2max a \u2211 x w(a, x) +M \u2264 3max a \u2211 x w(a, x).\nThis is equivalent to:\n1 3 \u00b7 M 2l max a \u2211 (x,y) w\u2032l(a, x, y) \u2264 max a \u2211 x w(a, x) \u2264 M 2l max a \u2211 (x,y) w\u2032l(a, x, y)."}, {"heading": "4 Experiments", "text": "We evaluate our proposed algorithm XOR_MMAP against two baselines \u2013 the Sample Average Approximation (SAA) [15] and the Mixed Loopy Belief Propagation (Mixed LBP) [10]. These two baselines are selected to represent the two most widely used classes of methods that approximate the embedded sum in MMAP Problems in two different ways. SAA approximates the intractable sum with a finite number of samples, while the Mixed LBP uses a variational approximation. We obtained the Mixed LBP implementation from the author of [10] and we use their default parameter settings. Since Marginal MAP Problems are in general very hard and there is currently no exact solver that scales to reasonably large instances, our main comparison is on the relative optimality gap: we first obtain the solution amethod for each approach. Then we compare the difference in objective function log \u2211 x\u2208X w(amethod, x)\u2212 log \u2211 x\u2208X w(abest, x), in which abest is the best solution among the 3 methods. Clearly a better algorithm will find a vector a which yields a larger objective function. The counting problem corresponding to a fixed action strategy a is solved using the exact solver ACE [4].\nOur first experiment is on unweighted random 2-SAT instances. Here, w(a, x) is an indicator variable on whether the 2-SAT instance is satisfiable. The SAT instances have 60 variables, 20 of which are randomly selected to form set A, and the remaining ones form set X . The number of clauses varies from 1 to 70. For a fixed number of clauses, we randomly generate 20 instances, and the left panel\n3 If w satisfy the property that mina,x w(a, x) \u2265 2\u2212l\u22121M , we don\u2019t have the M2n\u2212l term.\nof Figure 1 shows the median objective function\n\u2211\nx\u2208X w(amethod, x) for the 3 approaches. We tune the constants of our Algorithm XOR_MMAP so it gives a 210 = 1024-approximation (2\u22125 \u00b7 sol \u2264 OPT \u2264 25 \u00b7 sol, \u03b4 = 10\u22123). The upper and lower bounds are shown in dashed lines. SAA uses 10,000 samples. The optimal solution returned by XOR_MMAP is far better than the theoretical bound.\nAs we can see from the left panel of Figure 1, both Mixed LBP and SAA match the performance of our proposed XOR_MMAP on easy instances. However, as the number of clauses increases, their performance quickly deteriorates. In fact, for instances with more than 20 (or 60) clauses, in median case the a vectors returned by Mixed LBP (or SAA) cannot yield non-zero solution values. We therefore are not able to even plot their performance beyond the two points. At the same time, our algorithm XOR_MMAP can still find vector a yielding over 220 solutions on larger instances with more than 60 clauses.\nNext, we look at the performance of the 3 algorithms on weighted instances. Here, we set the number of replicates T = 3 for our algorithm XOR_MMAP, and we repeatedly start the algorithm with an increasing number of XOR constraints k, until it completes for all k or time out in an hour. For SAA, we use 1,000 samples, which is the largest we can use under memory limit. Despite the fact that the bound becomes too loose, in practice the solutions found by XOR_MMAP outperform the ones found by Mixed LBP and SAA. Figure 2 shows the performance of the 3 algorithms for 10-by-10 Ising models with mixed coupling strength, different field strengths and number of variables selected to form set A. All values in the figure are median values across 20 instances (log10 scale). In general, the difference in performance is bigger when the coupling strength increases and the problem instances are harder. The mixed LBP performs better than SAA when 50% of variables are selected for maximization. However, it becomes worse when the percentage of maximization variables increases to 80%. In all 4 cases, our algorithm XOR_MMAP is the best among the three algorithms.\nWe also apply the Marginal MAP solver to an image completion task. We first learn a two-layer deep belief network [3, 8] from a 14-by-14 MNIST dataset. Then for a binary image that only contains the upper part of a digit, we ask the solver to complete the lower part, based on the learned model. This is a Marginal MAP task, since one needs to integrate over all the states of the hidden variables, and query the most likely states of the lower part of the image. Figure 3 shows the result of a few digits. As we can see, SAA performs poorly. In most cases, it only manages to come up with a light dot for all 10 different digits. Mixed Loopy Belief Propagation and our proposed XOR_MMAP perform well. The good performance of Mixed LBP may be due to the fact that the learned deep belief network has relative non-discriminative weights on pairwise factors.\nFinally, we consider an application that applies decision making into machine learning models. This network design application maximizes the spread of cascades in networks, which is important in the domain of social networks and computational sustainability. In this application, we are given a stochastic graph, in which the source node at time t = 0 is affected. For a node v at time t, it will be affected if one of its ancestor nodes at time t \u2212 1 is affected, and the configuration of the edge connecting the two nodes is \u201con\u201d. An edge connecting node u and v has probability pu,v to be turned on. A node will not be affected if it is not purchased. Our goal is to purchase a set of nodes within a finite budget, so as to maximize the probability that the target node is affected. We refer the reader to [15] for more background knowledge. This application cannot be captured by graphical models due\nto global constraints. Therefore, we are not able to run mixed LBP on this problem. We consider a set of synthetic networks, and compare the performance of SAA and our XOR_MMAP with different budgets. As we can see from the right panel of Figure 3, the nodes that our XOR_MMAP decides to purchase result in higher probabilities of affecting the target node, compared to SAA. Each dot in the figure is the median value over 30 networks generated in a similar way."}, {"heading": "5 Conclusion", "text": "We propose XOR_MMAP, a novel constant approximation algorithm to solve the Marginal MAP Problem. Our approach represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. In our algorithm , the entire problem can be solved by a single optimization. We evaluate our approach on several machine learning and decision making applications. We are able to show that XOR_MMAP outperforms several state-of-the-art Marginal MAP solvers. XOR_MMAP provides a new angle to solving the Marginal MAP Problem, opening the door to new research directions and applications in real world domains."}, {"heading": "Acknowledgments", "text": "This research was supported by National Science Foundation (Award Number 0832782, 1522054, 1059284)."}], "references": [{"title": "Stochastic integration via error-correcting codes", "author": ["Dimitris Achlioptas", "Pei Jiang"], "venue": "In Proc. Uncertainty in Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Hashing-based approximate probabilistic inference in hybrid domains", "author": ["Vaishak Belle", "Guy Van den Broeck", "Andrea Passerini"], "venue": "In Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Compiling relational bayesian networks for exact inference", "author": ["Mark Chavira", "Adnan Darwiche", "Manfred Jaeger"], "venue": "Int. J. Approx. Reasoning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Embed and project: Discrete sampling with universal hashing", "author": ["Stefano Ermon", "Carla P. Gomes", "Ashish Sabharwal", "Bart Selman"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Taming the curse of dimensionality: Discrete integration by hashing and optimization", "author": ["Stefano Ermon", "Carla P. Gomes", "Ashish Sabharwal", "Bart Selman"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Low-density parity constraints for hashing-based discrete integration", "author": ["Stefano Ermon", "Carla P. Gomes", "Ashish Sabharwal", "Bart Selman"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey Hinton", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Message-passing for approximate MAP inference with latent variables", "author": ["Jiarong Jiang", "Piyush Rai", "Hal Daum\u00e9 III"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Variational algorithms for marginal MAP", "author": ["Qiang Liu", "Alexander T. Ihler"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "AND/OR search for marginal MAP", "author": ["Radu Marinescu", "Rina Dechter", "Alexander T. Ihler"], "venue": "In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Pushing forward marginal MAP with best-first search", "author": ["Radu Marinescu", "Rina Dechter", "Alexander T. Ihler"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Complexity results and approximation strategies for map explanations", "author": ["James D. Park", "Adnan Darwiche"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Decomposition bounds for marginal MAP", "author": ["Wei Ping", "Qiang Liu", "Alexander T. Ihler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Maximizing the spread of cascades using network design", "author": ["Daniel Sheldon", "Bistra N. Dilkina", "Adam N. Elmachtoub", "Ryan Finseth", "Ashish Sabharwal", "Jon Conrad", "Carla P. Gomes", "David B. Shmoys", "William Allen", "Ole Amundsen", "William Vaughan"], "venue": "In UAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Scheduling conservation designs for maximum flexibility via network cascade optimization", "author": ["Shan Xue", "Alan Fern", "Daniel Sheldon"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Avicaching: A two stage game for bias reduction in citizen science", "author": ["Yexiang Xue", "Ian Davies", "Daniel Fink", "Christopher Wood", "Carla P. Gomes"], "venue": "In Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "The Marginal MAP Problem is known to be NPPP-complete [13], which is commonly believed to be even harder than MAP inference (NP-hard) and marginal inference (#P-complete).", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "As a supporting evidence, MMAP problems are NP-hard even on tree structured probabilistic graphical models [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "One class of approaches [10, 9, 12, 11, 14] use various variational forms to represent the intractable sum.", "startOffset": 24, "endOffset": 43}, {"referenceID": 8, "context": "One class of approaches [10, 9, 12, 11, 14] use various variational forms to represent the intractable sum.", "startOffset": 24, "endOffset": 43}, {"referenceID": 11, "context": "One class of approaches [10, 9, 12, 11, 14] use various variational forms to represent the intractable sum.", "startOffset": 24, "endOffset": 43}, {"referenceID": 10, "context": "One class of approaches [10, 9, 12, 11, 14] use various variational forms to represent the intractable sum.", "startOffset": 24, "endOffset": 43}, {"referenceID": 13, "context": "One class of approaches [10, 9, 12, 11, 14] use various variational forms to represent the intractable sum.", "startOffset": 24, "endOffset": 43}, {"referenceID": 14, "context": "As another family of approaches, Sample Average Approximation (SAA) [15, 16] uses a fixed set of samples to represent the intractable sum, which then transforms the entire problem into a restricted optimization, only considering a finite number of samples.", "startOffset": 68, "endOffset": 76}, {"referenceID": 15, "context": "As another family of approaches, Sample Average Approximation (SAA) [15, 16] uses a fixed set of samples to represent the intractable sum, which then transforms the entire problem into a restricted optimization, only considering a finite number of samples.", "startOffset": 68, "endOffset": 76}, {"referenceID": 5, "context": "[6, 7] recently proposed an alternative approach to approximate intractable counting problems.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[6, 7] recently proposed an alternative approach to approximate intractable counting problems.", "startOffset": 0, "endOffset": 6}, {"referenceID": 14, "context": "As a specific example, we consider a network design application to maximize the spread of cascades [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "As a second example in crowdsourcing domain [17], probabilistic models are used to model people\u2019s behaviors.", "startOffset": 44, "endOffset": 48}, {"referenceID": 5, "context": "Our approach is based on a recent theoretical result that transforms a counting problem to a series of optimization problems [6, 7, 2, 1].", "startOffset": 125, "endOffset": 137}, {"referenceID": 6, "context": "Our approach is based on a recent theoretical result that transforms a counting problem to a series of optimization problems [6, 7, 2, 1].", "startOffset": 125, "endOffset": 137}, {"referenceID": 1, "context": "Our approach is based on a recent theoretical result that transforms a counting problem to a series of optimization problems [6, 7, 2, 1].", "startOffset": 125, "endOffset": 137}, {"referenceID": 0, "context": "Our approach is based on a recent theoretical result that transforms a counting problem to a series of optimization problems [6, 7, 2, 1].", "startOffset": 125, "endOffset": 137}, {"referenceID": 5, "context": "It\u2019s possible to show that HA,b is pairwise independent [6, 7].", "startOffset": 56, "endOffset": 62}, {"referenceID": 6, "context": "It\u2019s possible to show that HA,b is pairwise independent [6, 7].", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "[6]) For a fixed solution strategy a0 \u2208 A, \u2022 Suppose #w(a0) \u2265 20 , then for any k \u2264 k0, with probability 1 \u2212 2 c (2c\u22121)2 , Algorithm XOR_Binary(w, a0, k \u2212 c)=true.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] developed a random algorithm that gives a constant factor approximation to counting problems, which corresponds to running XOR_Binary multiple times for each k, and selecting the largest k that XOR_Binary procedure returns true most of the time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Unlike in [6], we choose to build our proof from the unweighted case because it can effectively avoid modeling the median [5], which is difficult to encode in integer programming.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Unlike in [6], we choose to build our proof from the unweighted case because it can effectively avoid modeling the median [5], which is difficult to encode in integer programming.", "startOffset": 122, "endOffset": 125}, {"referenceID": 14, "context": "We evaluate our proposed algorithm XOR_MMAP against two baselines \u2013 the Sample Average Approximation (SAA) [15] and the Mixed Loopy Belief Propagation (Mixed LBP) [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "We evaluate our proposed algorithm XOR_MMAP against two baselines \u2013 the Sample Average Approximation (SAA) [15] and the Mixed Loopy Belief Propagation (Mixed LBP) [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "We obtained the Mixed LBP implementation from the author of [10] and we use their default parameter settings.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "The counting problem corresponding to a fixed action strategy a is solved using the exact solver ACE [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "We first learn a two-layer deep belief network [3, 8] from a 14-by-14 MNIST dataset.", "startOffset": 47, "endOffset": 53}, {"referenceID": 7, "context": "We first learn a two-layer deep belief network [3, 8] from a 14-by-14 MNIST dataset.", "startOffset": 47, "endOffset": 53}, {"referenceID": 14, "context": "We refer the reader to [15] for more background knowledge.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "Arising from many applications at the intersection of decision making and machine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP Problem, which represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. XOR_MMAP provides a constant factor approximation to the Marginal MAP Problem, by encoding it as a single optimization in polynomial size of the original problem. We evaluate our approach in several machine learning and decision making applications, and show that our approach outperforms several state-of-the-art Marginal MAP solvers.", "creator": "LaTeX with hyperref package"}}}