{"id": "1610.03022", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Very Deep Convolutional Networks for End-to-End Speech Recognition", "abstract": "sequence - to - sequence models have shown trends in end - to - end texture recognition. generally these models have only used shallow acoustic relay networks. in prototype work, we successively train very deep convolutional networks to add more expressive power and better retrieval for end - to - end asr models. we apply network - intra - network principles, batch normalization, residual dynamics and convolutional lstms to build very deep recurrent and flexible structures. our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. his experiment visually his first asr task and exhibit 10. 5 \\ % word error rate without any dictionary or memory using a 15 layer deep network.", "histories": [["v1", "Mon, 10 Oct 2016 18:43:58 GMT  (267kb,D)", "http://arxiv.org/abs/1610.03022v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu zhang", "william chan", "navdeep jaitly"], "accepted": false, "id": "1610.03022"}, "pdf": {"name": "1610.03022.pdf", "metadata": {"source": "CRF", "title": "VERY DEEP CONVOLUTIONAL NETWORKS FOR END-TO-END SPEECH RECOGNITION", "authors": ["Yu Zhang", "William Chan", "Navdeep Jaitly"], "emails": ["yzhang87@mit.edu,", "williamchan@cmu.edu,", "ndjaitly@google.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 Automatic Speech Recognition, End-to-End Speech Recognition, Very Deep Convolutional Neural Networks\n1. INTRODUCTION\nThe sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6]. It is able to do this because it is not restricted by the classical independence assumptions of Hidden Markov Model (HMM) [7] and Connectionist Temporal Classification (CTC) [8] models. As a result, a single end-to-end model can jointly accomplish the ASR task within one single large neural network.\nThe foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs [4, 6] or GRUs [4]. However, their use of hierarchy in the encoders demonstrates that better encoder networks in the model should lead to better results. In this work we significantly extend the state of the art in this area by developing very deep hybrid convolutional and recurrent models, using recent developments in the vision community.\nConvolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12]. Unlike Deep Neural Networks (DNNs) [13], CNNs explicitly exploit structural locality in the spectral feature space. CNNs use shared weight filters and pooling to give the model better spectral and temporal invariance properties, thus typically yield better generalized and more robust models compared to DNNs [14]. Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters. Such a strategy can lead to more expressive models with better generalization.\nWhile very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been\n\u2217Work done as Google Brain interns.\nexplored in the speech community. We explore and apply some of these techniques in our end-to-end speech model:\n1. Network-in-Network (NiN) [19] increases network depth through the use of 1x1 convolutions. This allows us to increase the depth and expressive power of a network while reducing the total number of parameters that would have been needed otherwise to build such deeper models. NiN has seen great success in computer vision, building very deep models[18]. We show how to apply NiN principles in hierarchical Recurrent Neural Networks (RNNs) [20].\n2. Batch Normalization (BN) [21] normalizes each layer\u2019s inputs to reduce internal covariate shift. BN speeds up training and acts as an regularizer. BN has also seen success in endto-end CTC models [22]. The seq2seq attention mechanism [1] has high variance in the gradient (especially from random initialization); without BN we were unable to train the deeper seq2seq models we demonstrate in this paper. We extend on previous work and show how BN can be applied to seq2seq acoustic model encoders.\n3. Residual Networks (ResNets) [23] learns a residual function of the input through the usage of skip connections. ResNets allow us to train very deep networks without suffering from poor optimization or generalization which typically happen when the network is trapped at a local minima. We explore these skip connections to build deeper acoustic encoders.\n4. Convolutional LSTM (ConvLSTM) [24] use convolutions to replace the inner products within the LSTM unit. ConvLSTM allows us to maintain structural representations in our cell state and output. Additionally, it allows us to add more compute to the model while reducing the number of parameters for better generalization. We show how ConvLSTMs can be beneficial and replace LSTMs.\nWe are driven by same motivation that led to the success of very deep networks in vision [15, 18, 21, 23] \u2013 add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter. In this paper, we use very deep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models [4]. Our best model achieves a WER of 10.53% where our baseline acheives a WER of 14.76%. We present detailed analysis on how each technique improves the overall performance.\n2. MODEL\nIn this section, we will describe the details of each component of our model.\nar X\niv :1\n61 0.\n03 02\n2v 1\n[ cs\n.C L\n] 1\n0 O\nct 2\n01 6"}, {"heading": "2.1. Listen, Attend and Spell", "text": "Listen, Attend and Spell (LAS) [3] is an attention-based seq2seq model which learns to transcribe an audio sequence to a word sequence, one character at a time. Let x = (x1, . . . , xT ) be the input sequence of audio frames, and y = (y1, . . . , yS) be the output sequence of characters. The LAS models each character output yi using a conditional distribution over the previously emitted characters y<i and the input signal x. The probability of the entire output sequence is computed using the chain rule of probabilities:\nP (y|x) = \u220f i P (yi|x,y<i)\nThe LAS model consists of two sub-modules: the listener and the speller. The listener is an acoustic model encoder and the speller is an attention-based character decoder. The encoder (the Listen function) transforms the original signal x into a high level representation h = (h1, . . . , hU ) withU \u2264 T . The decoder (the AttendAndSpell function) consumes h and produces a probability distribution over character sequences:\nh = Listen(x) (1) P (y|x) = AttendAndSpell(h) (2)\nThe Listen is a stacked Bidirectional Long-Short Term Memory (BLSTM) [25] network with hierarchical subsampling as described in [3]. In our work, we replace Listen with a network of very deep CNNs and BLSTMs. The AttendAndSpell is an attention-based transducer [1], which generates one character yi at a time:\nsi = DecodeRNN([yi\u22121, ci\u22121], si\u22121) (3) ci = AttentionContext(si,h) (4)\np(yi|x,y<i) = TokenDistribution(si, ci) (5)\nThe DecodeRNN produces a transducer state si as a function of the previously emitted token yi\u22121, the previous attention context ci\u22121, and the previous transducer state si\u22121. In our implementation, DecodeRNN is a LSTM [26] function without peephole connections.\nThe AttentionContext function generates ci with a contentbased Multi-Layer Perceptron (MLP) attention network [1]."}, {"heading": "2.2. Network in Network", "text": "In our study, we add depth through NiN modules in the hierarchical subsampling connections between LSTM layers. We introduce a projected subsampling layer, wherein we simply concatenate two time frames to a single frame, project into a lower dimension and apply BN and ReLU non-linearity to replace the skip subsampling connections in [3]. Moreover, we further increase the depth of the network by adding more NiN 1 \u00d7 1 concolution modules inbetween each LSTM layer."}, {"heading": "2.3. Convolutional Layers", "text": "Unlike fully connected layers, Convolutional Neural Networks (CNNs) take into account the input topology, and are designed to reduce translational variance by using weight sharing with convolutional filters. CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.\nIn a hybrid system, convolutions require the addition of context window for each frame, or a way to treat the full utterance as a single\nsample [17]. One advantage of the seq2seq model is that the encoder can compute gradients over an entire utterance at once. Moreover, strided convolutions are an essential element of CNNs. For LAS applying striding is also a natural way to reduce temporal resolution."}, {"heading": "2.4. Batch Normalization", "text": "Batch normalization (BN) [21] is a technique to accelerate training and improve generalization, which is widely used in the computer vision community. Given a layer with output x, BN is implemented by normalizing each layer\u2019s inputs:\nBN(x) = \u03b3 x\u2212 E[x]\n(Var[x] + ) 1 2\n+ \u03b2 (6)\nwhere \u03b3 and \u03b2 are learnable parameters. The standard formulation of BN for CNNs can be readily applied to DNN acoustic models and cross-entropy training. For our seq2seq model, since we construct a minibatch containing multiple utterances, we follow the sequencewise normalization [22]. For each output channel, we compute the mean and variance statistics across all timesteps in the minibatch.\n2.5. Convolutional LSTM\nThe Convolutional LSTM (ConvLSTM) was first introduced in [24]. Although the fully connected LSTM layer has proven powerful for handling temporal correlations, it cannot maintain structural locality, and is more prone to overfitting. ConvLSTM is an extension of FC-LSTM which has convolutional strucutres in both the inputto-state and state-to-state transitions:\nit = \u03c3(Wxi \u2217 xt +Whi \u2217 ht\u22121 + bi) ft = \u03c3(Wxf \u2217 xt +Whf \u2217 ht\u22121 + bf ) ct = ft ct\u22121 + it tanh(Wxc \u2217 xt +Whc \u2217 ht\u22121 + bc) ot = \u03c3(Wxo \u2217 xt +Who \u2217 ht\u22121 + bo) ht = ot tanh(ct) (7)\niteratively from t = 1 to t = T , where \u03c3()\u0307 is the logistic sigmoid function, it, ft,ot, ct and ht are vectors to represent values of the input gate, forget gate, output gate, cell activation, and cell output at time t, respectively. denotes element-wise product of vectors. W\u2217 are the filter matrices connecting different gates, and b\u2217 are the corresponding bias vectors. The key difference is that \u2217 is now a convolution, while in a regular LSTM \u2217 is a matrix multiplication. Figure 1 shows the internal structure of a convolutional LSTM.\nThe state-to-state and input-to-state transitions can be achieved by a convolutional operation (here we ignore the multiple input/output channels). To ensure the attention mechanism can find the relation between encoder output and the test embedding, FC-LSTM is still necessary. However, we can use these ConvLSTMs to build deeper convolutional LSTM networks before the FC-LSTM layers. We expect this type of layer to learn better temporal representations compared to purely convolutional layers while being less prone to overfitting than FC-LSTM layers. We found bidirectional convolutional LSTMs to consistently perform better than unidirectional layers. All experiments reported in this paper used bidirectional models; here on we use convLSTM to mean bidirectional convLSTM."}, {"heading": "2.6. Residual Network", "text": "Deeper networks usually improve generalization and often outperform shallow networks. However, they tend to be harder to train and slower to converge when the model becomes very deep. Several architectures have been proposed recently to enable training of very deep networks [23, 27, 28, 29]. The idea behind these approaches is similar to the LSTM innovation \u2013 the introduction of linear or gated linear dependence between adjacent layers in the NN model to solve the vanishing gradient problem. In this study, we use a residual CNN/LSTM, to train deeper networks. Residual network [23] contains direct links between the lower layer outputs and the higher layer inputs. It defines a building block:\ny = F(x,Wi) + x (8)\nwhere x and y are the input and output vectors of the layers considered. The function F can be one or more convolutional or convLSTM layers. The residual block for different layers is illustrated in Figure 2. In our experiments, the convolutional based residual block always has a skip connection. However, for the LSTM layers we did not find skip connections necessary. All of the layers use the identity shortcut, and we did not find projection shortcuts to be helpful.\n3. EXPERIMENTS\nWe experimented with the Wall Street Journal (WSJ) ASR task. We used the standard configuration si284 dataset for training, dev93 for validation and eval92 for test evaluation. Our input features were 80 dimensional filterbanks computed every 10ms with delta and deltadelta acceleration normalized with per speaker mean and variance.\nThe baseline EncodeRNN function is a 3 layer BLSTM with 256 LSTM units per-direction (or 512 total) and 4 = 22 time factor reduction. The DecodeRNN is a 1 layer LSTM with 256 LSTM units. All the weight matrices were initialized with a uniform distribution U(\u22120.1, 0.1) and bias vectors to 0. For the convolutional model, all the filter matrices were initialized with a truncated normal distribution N (0, 0.1), and used 32 output channels. Gradient norm clipping to 1 was applied, together with Gaussian weight noiseN (0, 0.075) and L2 weight decay 1e\u22125 [30]. We used ADAM with the default hyperparameters described in [31], however we decayed the learning rate from 1e\u22123 to 1e\u22124 after it converged. We used 10 GPU workers for asynchronous SGD under the TensorFlow framework [32]. We monitor the dev93 Word Error Rate (WER) until convergence and report the corresponding eval92 WER. The models took O(5) days to converge."}, {"heading": "3.1. Acronyms for different type of layers", "text": "All the residual block follow the structure of Fig. 2. Here are the acronyms for each component we use in the following subsections:\nP / 2 subsampling projection layer.\nC (f \u00d7 t) convolutional layer with filter f and t under frequency and time axis.\nB batch normalization\nL bidirectional LSTM layer.\nResCNN residual block with convolutional layer inside.\nResConvLSTM residual block with convolutional LSTM layer inside."}, {"heading": "3.2. Network in Network for Hierarchical Connections", "text": "We first begin by investigating the acoustic encoder depth of the baseline model without using any convolutional layers. Our baseline model follows [4] using the skip connection technique in its time reduction. The baseline L \u00d7 3 or 3 layer BLSTM acoustic encoder, model achieves a 14.76% WER.\nWhen we simply increase the acoustic model encoder depth (i.e., to depth 8), the model does not converge well and we suspect the network to be trapped in poor local minimas. By using the projection subsampling layer as discussed in Section 2.2, we improves our WER to 13.61% WER or a 7.8% relative gain over the baseline.\nWe can further increase the depth of the network by adding more NiN 1 \u00d7 1 convolution modules inbetween each LSTM layer. This improves our model\u2019s performance further to 12.88% WER or 12.7% relative over the baseline. The BN layers were critical, and without them we found the model did not converge well. Table 1 summarizes the results of applying network-in-network modules in the hierarchical subsampling process."}, {"heading": "3.3. Going Deeper with Convolutions and Residual Connections", "text": "In this subsection, we extend on Section 3.2 and describe experiments in which we build deeper encoders by stacking convolutional layers and residual blocks in the acoustic encoder before the BLSTM. Unlike computer vision applications or truncated BPTT training in ASR, seq2seq models need to handle very long utterances (i.e., >2000 frames). If we simply stack a CNN before the BLSTMs, we quickly run out of GPU memory for deep models and also have excessive computation times. Our strategy to alleviate this problem is to apply striding in the first and second layer of the CNNs to reduce the time dimensionality and memory footprint.\nWe found no gains by simply stacking additional ResLSTM blocks even up to 8 layers. However, we do find gains if we use convolutions. If we stack 2 additional layers of 3 \u00d7 3 convolutions our model improves to 11.80% WER or 20% relative over the baseline. If we take this model and add 8 residual blocks (for a total of (2 + (8)2 + 5) = 23 layers in the encoder) our model further improves to 11.11% WER, or a 24.7% relative improvement over the baseline. We found that using 8 residual blocks a slightly outperform 4 residual blocks. Table 2 summarizes the results of these experiments."}, {"heading": "3.4. Convolutional LSTM", "text": "In this subsection, we investigate the effectiveness of the convolutional LSTM. Table 3 compares the effect of using convolutional LSTM layers. It can be observed that a pure ConvLSTM performs much worse than the baseline \u2014 we still need the fully connected LSTM 1. However, replacing the ResConv block with ResConvLSTM as shown in Figure 3 give us additional 7% relative gains. In our experiments, we always use 3\u00d71 filters for ConvLSTM because the recurrent structure captures temporal information while the convolutions capture spectral structure. We conjecture that the gain is because the convolutional recurrent state maintains spectral structure and reduces overfitting.\nTable 4 compares our WSJ results with other published end-toend models. To our knowledge, the previous best reported WER on WSJ without an LM was the seq2seq model with Task Loss Estimation achieving 18.0% WER in [5]. Our baseline, also a seq2seq model, achieved 14.76% WER. Our model is different from that of [5] in that we did not use location-based priors on the attention model and we used weight noise. Our best model, shown in Figure 3, achieves a WER of 10.53%.\n4. CONCLUSION\nWe explored very deep CNNs for end-to-end speech recognition. We applied Network-in-Network principles to add depth and nonlinearities to hierarchical RNNs. We also applied Batch Normalization and Residual connections to build very deep convolutional towers to process the acoustic features. Finally, we also explored Convolutional LSTMs, wherein we replaced the inner product of LSTMs with convolutions to maintain spectral structure in its representation. Together, we added more expressive capacity to build a very deep model without substantially increasing the number of parameters. On the WSJ ASR task, we obtained 10.5% WER without a language model, an 8.5% absolute improvement over published best result [4]. While we demonstrated our results only on the seq2seq task, we believe this architecture should also significantly help CTC and other recurrent acoustic models.\n1We only use 32 output channels thus it can be improve if we increase the channel size.\n5. REFERENCES\n[1] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by Jointly Learning to Align and Translate,\u201d in ICLR, 2015.\n[2] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, \u201cAttention-Based Models for Speech Recognition,\u201d in NIPS, 2015.\n[3] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition,\u201d in ICASSP, 2016.\n[4] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, \u201cEnd-to-end Attention-based Large Vocabulary Speech Recognition,\u201d in ICASSP, 2016.\n[5] D. Bahdanau, D. Serdyuk, P. Brakel, N. R. Ke, J. Chorowski, A. Courville, and Y. Bengio, \u201cTask Loss Estimation for Sequence Prediction,\u201d in ICLR Workshop, 2016.\n[6] W. Chan and I. Lane, \u201cOn Online Attention-based Speech Recognition and Joint Mandarin Character-Pinyin Training,\u201d in INTERSPEECH, 2016.\n[7] L. R. Rabiner, \u201cA tutorial on hidden Markov models and selected applications in speech recognition,\u201d Proceedings of the IEEE, vol. 77, pp. 257\u2013286, 2 1989.\n[8] A. Graves, \u201cSequence Transduction with Recurrent Neural Networks,\u201d in ICML: Representation Learning Workshop, 2012.\n[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradientbased learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 11 1998.\n[10] O. Abdel-Hamid, L. Deng, and D. Yu, \u201cExploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition,\u201d in INTERSPEECH, 2013.\n[11] T. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran, \u201cDeep Convolutional Neural Networks for LVCSR,\u201d in ICASSP, 2013.\n[12] W. Chan and I. Lane, \u201cDeep Convolutional Neural Networks for Acoustic Modeling in Low Resource Languages,\u201d in ICASSP, 2016.\n[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \u201cDeep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,\u201d IEEE Signal Processing Magazine, 2012.\n[14] T. Sainath, B. Kingsbury, A. Mohamed, G. E. Dahl, G. Saon, H. Soltau, T. Beran, A. Y. Aravkin, and B. Ramabhadran, \u201cImprovements to Deep Convolutional Neural Networks for LVCSR,\u201d in ASRU, 2013.\n[15] K. Simonyan and A. Zisserman, \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition,\u201d in ICLR, 2015.\n[16] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, \u201cVery deep multilingual convolutional neural networks for LVCSR,\u201d in ICASSP, 2016.\n[17] T. Sercu and V. Goel, \u201cAdvances in Very Deep Convolutional Neural Networks for LVCSR,\u201d in INTERSPEECH, 2016.\n[18] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing Deeper with Convolutions,\u201d in CVPR, 2015.\n[19] M. Lin, Q. Chen, and S. Yan, \u201cNetwork in network,\u201d in ICLR, 2013.\n[20] S. Hihi and Y. Bengio, \u201cHierarchical Recurrent Neural Networks for Long-Term Dependencies,\u201d in NIPS, 1996.\n[21] S. Ioffe and C. Szegedy, \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,\u201d in ICML, 2015.\n[22] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen, J. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley, L. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao, D. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep Speech 2: End-to-End Speech Recognition in English and Mandarin,\u201d in ICML, 2016.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep Residual Learning for Image Recognition,\u201d in CVPR, 2016.\n[24] X. Shi, Z. Chen, H. Wang, D. Y. Yeung, W. K. Wong, and W. C. Woo, \u201cConvolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,\u201d in NIPS, 2015.\n[25] A. Graves, N. Jaitly, and A. Mohamed, \u201cHybrid Speech Recognition with Bidirectional LSTM,\u201d in ASRU, 2013.\n[26] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[27] Y. Zhang, G. Chen, D. Yu, K. Yao, S. Khudanpur, and J. Glass, \u201cHighway Long Short-Term Memory RNNs for Distant Speech Recognition,\u201d in ICASSP, 2016.\n[28] N. Kalchbrenner, I. Danihelka, and A. Graves, \u201cGrid long short-term memory,\u201d in ICLR, 2016.\n[29] R. Srivastava, K. Greff, and J. Schmidhuber, \u201cTraining very deep networks,\u201d in NIPS, 2015.\n[30] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d in NIPS, 2011.\n[31] D. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Optimization,\u201d in ICLR, 2015.\n[32] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane\u0301, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vie\u0301gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, TensorFlow: large-scale machine learning on heterogeneous systems, Software available from tensorflow.org, 2015. [Online]. Available: http://tensorflow.org/.\n[33] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recognition with Recurrent Neural Networks,\u201d in ICML, 2014."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-Based Models for Speech Recognition", "author": ["J. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "NIPS, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "ICASSP, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end Attention-based Large Vocabulary Speech Recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "ICASSP, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Task Loss Estimation for Sequence Prediction", "author": ["D. Bahdanau", "D. Serdyuk", "P. Brakel", "N.R. Ke", "J. Chorowski", "A. Courville", "Y. Bengio"], "venue": "ICLR Workshop, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "On Online Attention-based Speech Recognition and Joint Mandarin Character-Pinyin Training", "author": ["W. Chan", "I. Lane"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, pp. 257\u2013286, 2 1989.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Sequence Transduction with Recurrent Neural Networks", "author": ["A. Graves"], "venue": "ICML: Representation Learning Workshop, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 11 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Convolutional Neural Networks for LVCSR", "author": ["T. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "ICASSP, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Convolutional Neural Networks for Acoustic Modeling in Low Resource Languages", "author": ["W. Chan", "I. Lane"], "venue": "ICASSP, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Improvements to Deep Convolutional Neural Networks for LVCSR", "author": ["T. Sainath", "B. Kingsbury", "A. Mohamed", "G.E. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A.Y. Aravkin", "B. Ramabhadran"], "venue": "ASRU, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "ICASSP, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Advances in Very Deep Convolutional Neural Networks for LVCSR", "author": ["T. Sercu", "V. Goel"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies", "author": ["S. Hihi", "Y. Bengio"], "venue": "NIPS, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": "ICML, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "D.Y. Yeung", "W.K. Wong", "W.C. Woo"], "venue": "NIPS, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid Speech Recognition with Bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A. Mohamed"], "venue": "ASRU, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yao", "S. Khudanpur", "J. Glass"], "venue": "ICASSP, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "ICLR, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Training very deep networks", "author": ["R. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "NIPS, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical Variational Inference for Neural Networks", "author": ["A. Graves"], "venue": "NIPS, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: large-scale machine learning on heterogeneous systems, Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Towards End-to-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "ICML, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 2, "context": "The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 3, "context": "The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 4, "context": "The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 5, "context": "The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 213}, {"referenceID": 6, "context": "It is able to do this because it is not restricted by the classical independence assumptions of Hidden Markov Model (HMM) [7] and Connectionist Temporal Classification (CTC) [8] models.", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "It is able to do this because it is not restricted by the classical independence assumptions of Hidden Markov Model (HMM) [7] and Connectionist Temporal Classification (CTC) [8] models.", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "The foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs [4, 6] or GRUs [4].", "startOffset": 147, "endOffset": 153}, {"referenceID": 5, "context": "The foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs [4, 6] or GRUs [4].", "startOffset": 147, "endOffset": 153}, {"referenceID": 3, "context": "The foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs [4, 6] or GRUs [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].", "startOffset": 90, "endOffset": 102}, {"referenceID": 10, "context": "Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].", "startOffset": 90, "endOffset": 102}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].", "startOffset": 90, "endOffset": 102}, {"referenceID": 12, "context": "Unlike Deep Neural Networks (DNNs) [13], CNNs explicitly exploit structural locality in the spectral feature space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "CNNs use shared weight filters and pooling to give the model better spectral and temporal invariance properties, thus typically yield better generalized and more robust models compared to DNNs [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 14, "context": "Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.", "startOffset": 89, "endOffset": 97}, {"referenceID": 16, "context": "Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.", "startOffset": 89, "endOffset": 97}, {"referenceID": 14, "context": "While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been", "startOffset": 157, "endOffset": 165}, {"referenceID": 17, "context": "While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been", "startOffset": 157, "endOffset": 165}, {"referenceID": 18, "context": "Network-in-Network (NiN) [19] increases network depth through the use of 1x1 convolutions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "NiN has seen great success in computer vision, building very deep models[18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "We show how to apply NiN principles in hierarchical Recurrent Neural Networks (RNNs) [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "Batch Normalization (BN) [21] normalizes each layer\u2019s inputs to reduce internal covariate shift.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "BN has also seen success in endto-end CTC models [22].", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "The seq2seq attention mechanism [1] has high variance in the gradient (especially from random initialization); without BN we were unable to train the deeper seq2seq models we demonstrate in this paper.", "startOffset": 32, "endOffset": 35}, {"referenceID": 22, "context": "Residual Networks (ResNets) [23] learns a residual function of the input through the usage of skip connections.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Convolutional LSTM (ConvLSTM) [24] use convolutions to replace the inner products within the LSTM unit.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "We are driven by same motivation that led to the success of very deep networks in vision [15, 18, 21, 23] \u2013 add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter.", "startOffset": 89, "endOffset": 105}, {"referenceID": 17, "context": "We are driven by same motivation that led to the success of very deep networks in vision [15, 18, 21, 23] \u2013 add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter.", "startOffset": 89, "endOffset": 105}, {"referenceID": 20, "context": "We are driven by same motivation that led to the success of very deep networks in vision [15, 18, 21, 23] \u2013 add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter.", "startOffset": 89, "endOffset": 105}, {"referenceID": 22, "context": "We are driven by same motivation that led to the success of very deep networks in vision [15, 18, 21, 23] \u2013 add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter.", "startOffset": 89, "endOffset": 105}, {"referenceID": 3, "context": "In this paper, we use very deep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "Listen, Attend and Spell (LAS) [3] is an attention-based seq2seq model which learns to transcribe an audio sequence to a word sequence, one character at a time.", "startOffset": 31, "endOffset": 34}, {"referenceID": 24, "context": "The Listen is a stacked Bidirectional Long-Short Term Memory (BLSTM) [25] network with hierarchical subsampling as described in [3].", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "The Listen is a stacked Bidirectional Long-Short Term Memory (BLSTM) [25] network with hierarchical subsampling as described in [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "The AttendAndSpell is an attention-based transducer [1], which generates one character yi at a time:", "startOffset": 52, "endOffset": 55}, {"referenceID": 25, "context": "In our implementation, DecodeRNN is a LSTM [26] function without peephole connections.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "The AttentionContext function generates ci with a contentbased Multi-Layer Perceptron (MLP) attention network [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "We introduce a projected subsampling layer, wherein we simply concatenate two time frames to a single frame, project into a lower dimension and apply BN and ReLU non-linearity to replace the skip subsampling connections in [3].", "startOffset": 223, "endOffset": 226}, {"referenceID": 13, "context": "CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.", "startOffset": 100, "endOffset": 108}, {"referenceID": 11, "context": "CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.", "startOffset": 100, "endOffset": 108}, {"referenceID": 16, "context": "In a hybrid system, convolutions require the addition of context window for each frame, or a way to treat the full utterance as a single sample [17].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "Batch normalization (BN) [21] is a technique to accelerate training and improve generalization, which is widely used in the computer vision community.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "For our seq2seq model, since we construct a minibatch containing multiple utterances, we follow the sequencewise normalization [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "The Convolutional LSTM (ConvLSTM) was first introduced in [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Several architectures have been proposed recently to enable training of very deep networks [23, 27, 28, 29].", "startOffset": 91, "endOffset": 107}, {"referenceID": 26, "context": "Several architectures have been proposed recently to enable training of very deep networks [23, 27, 28, 29].", "startOffset": 91, "endOffset": 107}, {"referenceID": 27, "context": "Several architectures have been proposed recently to enable training of very deep networks [23, 27, 28, 29].", "startOffset": 91, "endOffset": 107}, {"referenceID": 28, "context": "Several architectures have been proposed recently to enable training of very deep networks [23, 27, 28, 29].", "startOffset": 91, "endOffset": 107}, {"referenceID": 22, "context": "Residual network [23] contains direct links between the lower layer outputs and the higher layer inputs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "075) and L2 weight decay 1e\u22125 [30].", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "We used ADAM with the default hyperparameters described in [31], however we decayed the learning rate from 1e\u22123 to 1e\u22124 after it converged.", "startOffset": 59, "endOffset": 63}, {"referenceID": 31, "context": "We used 10 GPU workers for asynchronous SGD under the TensorFlow framework [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "Our baseline model follows [4] using the skip connection technique in its time reduction.", "startOffset": 27, "endOffset": 30}, {"referenceID": 32, "context": ", 2014) [33] 30.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": ", 2016) [5] 18.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "0% WER in [5].", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Our model is different from that of [5] in that we did not use location-based priors on the attention model and we used weight noise.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "5% absolute improvement over published best result [4].", "startOffset": 51, "endOffset": 54}], "year": 2016, "abstractText": "Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5% word error rate without any dictionary or language using a 15 layer deep network.", "creator": "LaTeX with hyperref package"}}}