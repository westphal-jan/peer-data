{"id": "1610.03165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "Long Short-Term Memory based Convolutional Recurrent Neural Networks for Large Vocabulary Speech Recognition", "abstract": "long short - term memory ( lstm ) natural neural networks ( rnns ) have been shown to give state - of - the - art performance or particular speech recognition tasks, as they became able to provide the learned dynamically changing contextual window of all sequence nodes. on the other hand, the convolutional neural capabilities ( cnns ) have brought significant improvements to deep feed - forward neural channels ( pots ), as they are able to rapidly reduce spectral variation in the input signal. in this paper, hybrid network architecture called interactive convolutional recurrent neural network ( crnn ) is made by combining the cnn and ml rnn. analyzing the xml crnns, each speech frame, without adjacent context frames, is organized as corresponding number of local feature patches along the frequency axis, and then a lstm path is performed on each feature sections along the time axis. we train and compare ffnns, lstm rnns and the proposed lstm crnns at various number of configurations. experimental results suggested that virtual lstm channels can exceed state - heavier - than - art speech recognition performance.", "histories": [["v1", "Tue, 11 Oct 2016 02:48:13 GMT  (596kb)", "http://arxiv.org/abs/1610.03165v1", "Published in INTERSPEECH 2015, September 6-10, 2015, Dresden, Germany"]], "COMMENTS": "Published in INTERSPEECH 2015, September 6-10, 2015, Dresden, Germany", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["xiangang li", "xihong wu"], "accepted": false, "id": "1610.03165"}, "pdf": {"name": "1610.03165.pdf", "metadata": {"source": "CRF", "title": "Long Short-Term Memory based Convolutional Recurrent Neural Networks for Large Vocabulary Speech Recognition", "authors": ["Xiangang Li", "Xihong Wu"], "emails": ["wxh}@cis.pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n03 16\n5v 1\n[ cs\n.C L\n] 1\n1 O\nct 2\n01 6"}, {"heading": "1. Introduction", "text": "Recently, the hybrid context dependent (CD) deep neural network (DNN) hidden Markov model (HMM) (CD-DNN-HMM) has become the dominant framework for acoustic modeling in speech recognition (e.g. [1][2][3]). The performance improvement over the conventional Gaussian mixture model (GMM)HMM is partially attributed to the powerful potential of DNN in modeling complex correlations in acoustic features.\nBased on the hybrid CD-DNN-HMM framework, many researches have been done from various aspects, such as the sequence discriminative training (e.g. [4][5][6]), the network architectures (e.g. [7][8][9]), and speaker adaptive methods (e.g. [10][11]), and have been shown to give significant performance improvements. In the researches of network architectures, two architectures have attracted lots of attentions: one is convolutional neural networks (CNNs), and the other is long short-term memory (LSTM) based recurrent neural networks (RNNs). In the seminal work, Ossama et al. [7] proposed to apply CNNs in the frequency domain to explicitly normalize speech spectral features to achieve frequency invariance and enforce locality of features, which have shown that further error rate reduction could be obtained comparing to the fully-connected DNNs on the phoneme recognition task. Subsequently, researchers have applied this idea on large vocabulary speech recognition tasks [12][13][14]. On the other hand, Graves et al.[8] proposed to\nuse stacked bidirectional LSTM network trained with connectionist temporal classification (CTC) [15] for phoneme recognition. Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].\nIn the literatures, most CNNs were applied on the frequency domain, and the variability along the time axis is handled by the fixed long time contextual window [7][12][13]. However, one of the original motivations for RNNs approach is to learn how much context to should be used for each prediction rather than fixed contextual window. Therefore, using recurrent connections in RNNs to improve CNNs is a natural choice. In this paper, an LSTM based convolutional recurrent neural network (CRNN) architecture is proposed by combining CNNs and LSTM RNNs. In the proposed approach, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each patch along the time axis. In other words, the proposed network architecture have convolutional operations to handle the variability along frequency axis, and recurrent operations to handle the variability along time axis. This proposed network architecture can be considered as introducing the recurrent operations in CNNs, or introducing the convolution operations in LSTM RNNs. Experiments are conducted on a large vocabulary conversational telephone speech recognition task, and results have shown that the proposed LSTM CRNNs can further improve the ASR performance."}, {"heading": "2. Review of LSTM RNNs and CNNs", "text": "In order to introduce the proposed network architecture, the conventional LSTM and CNN architectures for acoustic modeling are presented firstly in this section."}, {"heading": "2.1. LSTM RNNs for acoustic modeling", "text": "In modern feed-forward neural networks (FFNNs) based hybrid acoustic modeling, acoustic context windows of 11 to 31 frames are typically used as inputs. The cyclic connections in RNNs exploit a self-learnt amount of temporal context, which makes them in principle better suited for acoustic modeling. The RNN-HMM hybrids have been studied for almost twenty years (e.g.[21][22][23]), and have been shown to give the state-ofthe-art performance on many ASR tasks by introducing LSTM RNNs recently.\nGiven an input speech sequence x = (x1, x2, . . . , xT ), an conventional RNN computes the hidden vector sequence\nh = (h1, h2, . . . , hT ) and output vector sequence y = (y1, y2, . . . , yT ) from t = 1 to T as\nht = H(Wxhxt +Whhht\u22121 + bh) (1)\nyt = Whyht + by (2)\nwhere, W denotes weight matrices, b denotes bias vectors and H denotes hidden layer function. However, RNNs are hard to be trained properly due to the vanishing gradient and exploding gradient problems as described in [24]. To address these problems, long short-term memory (LSTM) is proposed [25].\nThe modern LSTM RNN architecture [25][26][27] is shown in Figure 1. In LSTM RNN, the recurrent hidden layer consists of a set of recurrently connected subnets known as \u201cmemory blocks\u201d. Each memory block contains one or more self-connected memory cells and three multiplicative gates to control the flow of information. Besides, there are peephole weights connecting gates to memory cell, which improve the LSTMs ability to learn precise timing and counting of internal states. The equations of LSTM memory blocks are as follows:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi) (3)\nft = \u03c3(Wxfxt +Whfht\u22121 +Wcf ct\u22121 + bf ) (4)\nat = \u03c4 (Wxcxt +Whcht\u22121 + bc) (5)\nct = ftct\u22121 + itat (6)\not = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo) (7)\nht = ot\u03b8(ct) (8)\nwhere, \u03c3 is the logistic sigmoid function, and i, f , o, a and c are respectively the input gate, forget gate, output gate, cell input activation, and cell state vectors, and all of which are the same size as the hidden vector h. Wci, Wcf , Wco are diagonal weight matrices for peephole connections. \u03c4 and \u03b8 are the cell input and cell output non-linear activation functions, generally in this paper tanh. Besides, the LSTM Projected (LSTMP) network is proposed in [9][18], which has a separate linear projection layer after the LSTM layer, and yield improved performance on a large vocabulary speech recognition task."}, {"heading": "2.2. CNNs for acoustic modeling", "text": "CNN is capable of modeling local frequency structures by applying linear convolutional filters on the local feature patches representing a limited bandwidth of the whole speech spectrum. In order to represent speech inputs in a frequency scale that can be divided into a number of local bands, CNN for acoustic modeling always use the filter-bank features as the inputs. Assuming the whole input feature is organized as J local patches, and each patch xj(j = 1, . . . , J) has s frequency bands, the equations\nof convolutional layer can be described as follow:\nhj = \u03b8(Wxj + b), (j = 1, . . . , J) (9)\nWhere, \u03b8(\u00b7) is the activation function, hj is the convolutional layers output vector of the jth feature patch. For each feature patch, the convolutional filter map the s input nodes into K output nodes, and the weights in convolutional filter are shared among all the feature patches.\nOn top of each convolutional layer, a pooling layer is added to compute a lower resolution representation of the convolutional layer activations through sub-sampling. Usually, the max pooling function can be used as the pooling strategy, and in literature [14], variants of pooling functions, such as the lp pooling [28], stochastic pooling [29] were also evaluated.\nBoth the weight sharing and pooling are important concepts in CNNs which helps to reduce the spectral variance in the input features. On top of stacked convolution-pooling layers, the standard fully connected layers are always added to combine the features of different bands."}, {"heading": "3. LSTM based convolutional recurrent neural networks", "text": "From the descriptions of the LSTM RNNs and CNNs, we can find that, the LSTM RNNs provide the dynamically changing contextual window, while the weight sharing and pooling in CNNs focus on the frequency shift invariance. Motivated from taking both advantages, in this paper a new network is proposed which attempts to combine these properties from CNNs and LSTM RNNs.\nThe convolutional layer in CNNs can be viewed as standard neural network layer operated on the local patches along the frequency axis re-organized from the input feature. In addition, a structure called \u201cNetwork In Network\u201d (NIN) is proposed in [30] to enhance model for local patches within the receptive field, which replace the filters in conventional CNNs with a \u201cmicro network\u201d, such as a multilayer perceptron consisting of multiple fully connected layers with nonlinear activation functions. Based on these understanding of CNNs, and in order to combining CNNs and RNNs, the proposed network is constructed by replacing the filters in conventional CNNs with recurrent networks, specifically LSTM networks, which leads to the architecture illustrated in Figure 2. This proposed network is called as convolutional LSTM (CLSTM) or LSTM based convolutional recurrent neural network (CRNN) in this paper.\nAs illustrated in Figure 2, the speech is represented with Mel-scale log-filterbank coefficients. Each speech frame, without context frames, is organized as local patches along the frequency axis, and adjacent patches have overlaps. Each patch represents a limited bandwidth of the whole speech spectrum, and a recurrent network is performed on each patch along the time axis. It means that, for each patch, a recurrent network receives the previous outputs and current inputs in the patch to make decisions. The equation of recurrent network based CRNNs can be written as:\nhj,t = H(Wxhxj,t +Whhhj,t\u22121 + bh), (j = 1, . . . , J) (10)\nwhere, hj,t denotes the outputs of the jth patch in time t. For the LSTM CRNNs, we only need to change the hidden layer function, just like from conventional RNNs to LSTM RNNs.\nIn the proposed CLSTM, like in the CNNs, the inputs of network are organized as a number of local feature patches. Meanwhile, same as in the LSTM RNNs, the input of the network only need current feature frame, without adjacent context frames. It is easy to find out that, there are convolution operations along the frequency axis, and recurrent operations along the time axis. The frequency shift invariance embodies in the convolutional part, while the dynamically changing contextual window embodies in the recurrent part.\nA similar network architecture to CLSTM is the multidimensional LSTM [31]. Through comparing these two architectures, it can be found out that, the CLSTM does not apply the recurrent operation along the adjacent frequency bands, while the multi-dimensional LSTM does. Another related work is introduced in [32] on biological sequence data analyzing, where the network architecture is a 1-dimensional convolutional layer followed by an LSTM layer, a fully connected layer and a final softmax layer, which can be understood as the stack of convolutional layer and LSTM layer."}, {"heading": "4. Experiments and discussion", "text": "We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs on a large vocabulary speech recognition task - the HKUST Mandarin Chinese conversational telephone speech recognition [33]. The corpus (LDC2005S15, LDC2005T32) is collected and transcribed by Hong Kong University of Science and Technology (HKUST), which contains 150-hour speech, and 873 calls in the training set and 24 calls in the development set, respectively. In our experiments, around 3-hour speech was randomly selected from the training set, used as the validate set for network training. The original development set in the corpus was used as ASR test set, which is not used in the training or the hyper-parameters determination processes."}, {"heading": "4.1. Experimental setup", "text": "The speech in the corpus is represented with 25ms frames of Mel-scale log-filterbank coefficients (including the energy value), along with their first- and second-order temporal derivatives. The FFNNs use concatenated features, which are constructed by concatenating the current frame with 5 frames in its left and right contexts. The inputs to the LSTM RNNs and LSTM CRNNs are only the current frames (no window of frames).\nA trigram language model estimated using all the acoustic model training transcriptions is used in all the experiments. The hybrid approach [2][34] is used, in which the neural networks\u2019\noutputs are converted as pseudo likelihood as the state output probability in hidden Markov model (HMM) framework. All the networks are trained based on the alignments generated by a well-trained GMM-HMM systems with 5529 senones (realignments by DNNs are not performed), and only the cross-entropy objective function is adopted.\nWe implement the RNN training on multi-GPU devices. In the training, the truncated back-propagation though time (BPTT) learning algorithm [35] is adopted. Each sentence in the training set is split into subsequences with equal length (15 frames in the experiments), and two adjacent subsequences have overlapping frames (5 frames in the experiments). For computational efficiency, one GPU operates in parallel on 20 subsequences from different utterances at a time. In order to train these networks on multi-GPU devices, asynchronous stochastic gradient descent [36][37] is adopted. The strategy introduced in [38] is applied to scale down the gradients. Since the information from the future frames helps making better decisions for current frame, we also delayed the output HMM state labels by 5 frames. In the experiments, the learning rate for training each network is decreased exponentially, and the initial and final learning rates are set specific to each network for stable convergence of training."}, {"heading": "4.2. Baseline systems", "text": "Firstly, the FFNNs and LSTM RNNs at various number of configurations are trained as the baseline, and results are summarized in Table 1 and Table 2. It is necessary to point out that, we found that, appropriate more senones would bring performance improvements. Thus, in this paper, we have 5529 senones against 3302 senones in [19][39], leading to slightly better experimental results than that in [19][39].\nIn Table 1, \u201c4\u00d7ReLU2000\u201d network has 4 hidden layers and each layer has 2000 rectified linear units (ReLU) [40][41], and \u201c4\u00d7Maxout800G3\u201d network has 4 hidden layers and each layer has 800 maxout units [42][43], where the group size is 3. The CNN, denoted by \u201c2\u00d7Conv+3\u00d7ReLU2000\u201d, has 2 convolution-pooling layers and 3 ReLU layers. In details, the convolutional layers has 256 units, and the pooling size is 3. It is expected that the CNN outperforms the other FFNNs.\nIn Table 2, \u201cLstm750\u201d network has only 1 LSTM layer with 750 LSTM cells, and \u201cLstm2000P750\u201d network has 1 LSTMP layer with 2000 LSTM cells projected to 750 nodes. Besides, based on the research in [19], LSTM based deep RNNs are constructed. LSTM RNNs yield better performance than FFNNs, and best performance among LSTM RNNs is obtained using \u201cLstm2000P750+3\u00d7ReLU2000\u201d network, which has an LSTMP layer followed by 3 ReLU layers. Besides, for comparison with the proposed LSTM CRNNs, we also trained networks construed by simply stacking convolutional layers and LSTM layers (the last two rows in Table 2), but unfortunately, these networks perform worse than \u201cLstm2000P750+3\u00d7ReLU2000\u201d."}, {"heading": "4.3. Results of CLSTMs", "text": "Since the pooling is a very importance concept in CNNs, we compared the models with and without pooling for the proposed CLSTMs, which shows no discernible performance difference. However, since the models with pooling layer have smaller number of parameters than that without pooling layer, the models in following experiments all have the pooling layers, and the pooling size is 3. Next, we explored the performance as a function of the number of LSTM cells for the convolutional recurrent layers. From Table 3, we can observe that as we increase the number of LSTM cells, the CER steadily decrease. We were able to obtain a comparable performance by using 384 LSTM cells for the convolutional recurrent layer over the best baseline performance.\nLiteratures [9][18] have proposed the LSTMP to make more effective use of model parameters to train acoustic models. Similarly, we explored the projection layer strategy in CLSTM networks. Results of LSTMP CRNNs are shown in Table 4. The projection layer strategy seems to provide no performance improvements. However, by introducing projection layers, the total number of parameters with the same LSTM cells can be reduced. More specifically, with same LSTM cells, the CLSTMP network has similar performance with the CLSTM one, but smaller number of parameters, for example, the \u201cCLstm384+Pooling+3\u00d7ReLU2000\u201d network and the \u201cCLstm384P128+Pooling+3\u00d7ReLU2000\u201d network. When we increase the LSTM cells from 256 to 512 for the CLSTMP networks, there are only small changes in the total number of parameters, but obvious CER reductions.\nIn the literatures, many studies have shown that performance can be improved by using multiple LSTM layers. Besides, multiple convolutional layers can also improve CNNs [13][14]. Thus, experiments were conducted to explore having two convolutional recurrent layers or another recurrent layers in the LSTM CRNNs. Results with different network structure configurations are shown in Table 5. The table shows that hav-\ning two convolutional recurrent layers also helps and yields a 3.8% relative improvement performance over the baseline systems. What is noteworthy is that the networks that have another LSTMP layers on the top of CLSTM layer can further reduce the CER to 31.43%, which is a 7.1% relative improvement.\nIn summary, the experimental results show that the proposed CLSTM network can exceed state-of-the-art ASR performance. The best performance can be obtained by the network which is constructed by one CLSTMP layer, one LSTMP layer and three ReLU layers."}, {"heading": "5. Conclusions", "text": "In this paper, an LSTM based convolutional recurrent neural network (CRNN) architecture is proposed for acoustic modeling by combining the CNNs and LSTM RNNs, which is constructed by replacing the filter in conventional CNNs with a recurrent filter, specifically a LSTM based filter. The proposed network can be considered as introducing the dynamically changing contextual window embedded in the LSTM network to the conventional CNNs, or introducing the frequency shift invariance embedded in the convolutional structure to LSTM RNNs. In other words, the proposed network contains convolutional operations along the frequency axis, and recurrent operations along the time axis.\nWe empirically evaluated the proposed network against FFNNs and LSTM networks on a large vocabulary speech recognition task. In the experiments, various configurations for constructing deep networks have been compared. The experimental results revealed that, the proposed LSTM CRNN can further improve the performance, delivering a 7% CER relative reduction significantly comparing to LSTM networks which have been shown to give state-of-the-art performance on some ASR tasks. However, we believe that this work is just a preliminary study. Future work includes training the CLTSM CRNNs using sequence discriminative training criterion [20] and experiments on a larger corpus."}, {"heading": "6. References", "text": "[1] A. Mohamed, G. Dahl, and G. Hinton, \u201cAcoustic modeling using\ndeep belief networks,\u201d IEEE Trans. Audio Speech Lang. Processing, vol. 20, pp. 14\u201322, 2012.\n[2] G. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-dependent pretrained deep neural networks for large-vocabulary speech recognition,\u201d IEEE Trans. Audio Speech Lang. Processing, vol. 20, pp. 30\u201342, 2012.\n[3] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: the shared views of four research groups,\u201d IEEE Signal Processing Mag., vol. 29, pp. 82\u201397, 2012.\n[4] B. Kingsbury, \u201cLattice-based optimization of sequence classification criteria for neural-network acoustic modeling,\u201d in ICASSP, 2009, pp. 3761\u20133764.\n[5] B. Kingsbury, T. Sainath, and H. Soltau, \u201cScalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization,\u201d in Interspeech, 2012, pp. 10\u201313.\n[6] K. Vesely\u0301, A. Ghoshal, L. Burget, and D. Povey, \u201cSequencediscriminative training of deep neural networks,\u201d in Interspeech, 2013, pp. 2345\u20132349.\n[7] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn, \u201cApplying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition,\u201d in ICASSP, 2012, pp. 4277\u20134280.\n[8] A. Graves, A. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in ICASSP, 2013, pp. 6645\u2013 6649.\n[9] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Interspeech, 2014, pp. 338\u2013342.\n[10] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, \u201cSpeaker adaptation of neural network acoustic models using i-vectors,\u201d in ASRU, 2013, pp. 55\u201359.\n[11] O. Abdel-Hamid and H. Jiang, \u201cFast speaker adaptation of hybrid nn/hmm model for speech recognition based on discriminative learning of speaker code,\u201d in ICASSP, 2013, pp. 7942\u20137946.\n[12] O. Abdel-Hamid, A. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, \u201cConvolutional neural networks for speech recognition,\u201d IEEE/ACM Trans. Audio Speech Lang. Processing, vol. 22, pp. 1533\u20131545, 2014.\n[13] T. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran, \u201cDeep convolutional neural networks for lvcsr,\u201d in ICASSP, 2013.\n[14] T. Sainath, B. Kingsbury, A. Mohamed, G. Dahl, G. Saon, H. Soltau, T. Beran, A. Aravkin, and B. Ramabhadran, \u201cImprovements to deep convolutional neural networks for lvcsr,\u201d 2013, arXiv:1309.1501.\n[15] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural network,\u201d in ICML, 2006, pp. 369\u2013376.\n[16] J. Geiger, Z. Zhang, F. Weninger, B. Schuller, and G. Rigoll, \u201cRobust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling,\u201d in Interspeech, 2014, pp. 631\u2013635.\n[17] A. Graves, N. Jaitly, and A. Mohamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in ASRU, 2013, pp. 273\u2013278.\n[18] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory based recurrent neural network architectures for large vocabulary speech recognition,\u201d 2014, arXiv:1402.1128.\n[19] X. Li and X. Wu, \u201cConstructing long short-term memory based deep recurrent neural network for large vocabulary speech recognition,\u201d in ICASSP, 2015.\n[20] H. Sak, O. Vinyals, G. Heigold, A. Senior, E. McDermott, R. Monga, and M. Mao, \u201cSequence discriminative distributed training of long short-term memory recurrent neural networks,\u201d in Interspeech, 2014, pp. 1209\u20131213.\n[21] A. Robinson, \u201cAn application of recurrent nets to phoneme probability estimation,\u201d IEEE Trans. Neural Networks, vol. 5, pp. 298\u2013 305, 1994.\n[22] O. Vinyals, S. Ravuri, and D. Povey, \u201cRevisiting recurrent neural networks for robust asr,\u201d in ICASSP, 2012, pp. 4085\u20134088.\n[23] L. Deng and J. Chen, \u201cSequence classification using the high-level features extracted from deep neural networks,\u201d in ICASSP, 2014, pp. 6844\u20136848.\n[24] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Trans. Neural Networks, vol. 5, pp. 157\u2013166, 1994.\n[25] S. Hochreiter and J. Schimidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, pp. 1735\u20131780, 1997.\n[26] F. Gers, J. Schmidhuber, and F. Cummins, \u201cLearning to forget: Continual prediction with lstm,\u201d Neural Computation, vol. 12, pp. 2451\u20132471, 2000.\n[27] F. Gers, N. Schraudolph, and J. Schmidhuber, \u201cLearning precise timing with lstm recurrent networks,\u201d Journal of Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.\n[28] P. Sermanet, S. Chintala, and Y. LeCun, \u201cConvolutional neural networks applied to house numbers digit classification,\u201d in ICPR, 2012, pp. 3288\u20133291.\n[29] M. Zeiler and R. Fergus, \u201cStochastic pooling for regularization of deep convolutional neural networks,\u201d in ICLR, 2013.\n[30] M. Lin, Q. Chen, and Y. S., \u201cNetwork in network,\u201d in ICLR, 2014.\n[31] A. Graves, S. Ferna\u0301ndez, and J. Schmidhuber, \u201cMulti-dimensional recurrent neural networks,\u201d in International Conference on Artificial Neural Networks, 2007.\n[32] S. Sonderby, C. Sonderby, H. Nielsen, and O. Winther, \u201cConvolutional lstm networks for subcellular localization of proteins,\u201d 2015, arXiv:1503.01919.\n[33] Y. Liu, P. Fung, Y. Yang, C. Cieri, S. Huang, and D. Graff, \u201cHkust/mts: A very large scale mandarin telephone speech corpus,\u201d in ISCSLP, 2006, pp. 724\u2013735.\n[34] H. Sak, A. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Interspeech, 2014, pp. 338\u2013342.\n[35] R. Williams and J. Peng, \u201cAn efficient gradient-based algorithm for online training of recurrent neural network trajectories,\u201d Neural Computation, vol. 2, pp. 490\u2013501, 1990.\n[36] R. Orma\u0301ndi, I. Hegedu\u0308s, and M. Jelasity, \u201cAsynchronous peer-topeer data mining with stochastic gradient descent,\u201d Lecture Notes in Computer Science, pp. 528\u2013540, 2011.\n[37] S. Zhang, C. Zhang, Z. You, R. Zheng, and B. Xu, \u201cAsynchronous stochastic gradient descent for dnn training,\u201d in ICASSP, 2013, pp. 6660\u20136663.\n[38] R. Pascanu and Y. Bengio, \u201cOn the difficulty of training recurrent neural networks,\u201d 2012, arXiv:1211.5063.\n[39] X. Li and X. Wu, \u201cImproving long short-term memory networks using maxout units for large vocabulary speech recognition,\u201d in ICASSP, 2015.\n[40] M. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q. Le, P. Nguyen, A. Senior, V. Vanhouche, J. Dean, and G. Hinton, \u201cOn rectified linear units for speech processing,\u201d in ICASSP, 2013, pp. 3517\u20133521.\n[41] G. Dahl, T. Sainath, and G. Hinton, \u201cImproving deep neural networks for lvcsr using rectified linear units and dropout,\u201d in ICASSP, 2013, pp. 8609\u20138613.\n[42] M. Cai, Y. Shi, and J. Liu, \u201cDeep maxout neural networks for speech recognition,\u201d in ASRU, 2013, pp. 291\u2013296.\n[43] Y. Miao, S. Rawat, and F. Metze, \u201cDeep maxout networks for low resource speech recognition,\u201d in ASRU, 2013, pp. 398\u2013403."}], "references": [{"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Trans. Audio Speech Lang. Processing, vol. 20, pp. 14\u201322, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio Speech Lang. Processing, vol. 20, pp. 30\u201342, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Mag., vol. 29, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B. Kingsbury"], "venue": "ICASSP, 2009, pp. 3761\u20133764.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization", "author": ["B. Kingsbury", "T. Sainath", "H. Soltau"], "venue": "Interspeech, 2012, pp. 10\u201313.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequencediscriminative training of deep neural networks", "author": ["K. Vesel\u00fd", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Interspeech, 2013, pp. 2345\u20132349.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "ICASSP, 2013, pp. 6645\u2013 6649.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "ASRU, 2013, pp. 55\u201359.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast speaker adaptation of hybrid nn/hmm model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "ICASSP, 2013, pp. 7942\u20137946.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Processing, vol. 22, pp. 1533\u20131545, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["T. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "ICASSP, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Improvements to deep convolutional neural networks for lvcsr", "author": ["T. Sainath", "B. Kingsbury", "A. Mohamed", "G. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A. Aravkin", "B. Ramabhadran"], "venue": "2013, arXiv:1309.1501.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural network", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "ICML, 2006, pp. 369\u2013376.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling", "author": ["J. Geiger", "Z. Zhang", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "Interspeech, 2014, pp. 631\u2013635.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["A. Graves", "N. Jaitly", "A. Mohamed"], "venue": "ASRU, 2013, pp. 273\u2013278.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "2014, arXiv:1402.1128.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Constructing long short-term memory based deep recurrent neural network for large vocabulary speech recognition", "author": ["X. Li", "X. Wu"], "venue": "ICASSP, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["H. Sak", "O. Vinyals", "G. Heigold", "A. Senior", "E. McDermott", "R. Monga", "M. Mao"], "venue": "Interspeech, 2014, pp. 1209\u20131213.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "An application of recurrent nets to phoneme probability estimation", "author": ["A. Robinson"], "venue": "IEEE Trans. Neural Networks, vol. 5, pp. 298\u2013 305, 1994.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Revisiting recurrent neural networks for robust asr", "author": ["O. Vinyals", "S. Ravuri", "D. Povey"], "venue": "ICASSP, 2012, pp. 4085\u20134088.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence classification using the high-level features extracted from deep neural networks", "author": ["L. Deng", "J. Chen"], "venue": "ICASSP, 2014, pp. 6844\u20136848.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Trans. Neural Networks, vol. 5, pp. 157\u2013166, 1994.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schimidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["F. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, pp. 2451\u20132471, 2000.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["F. Gers", "N. Schraudolph", "J. Schmidhuber"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "ICPR, 2012, pp. 3288\u20133291.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "ICLR, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "Y.S."], "venue": "ICLR, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-dimensional recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "International Conference on Artificial Neural Networks, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional lstm networks for subcellular localization of proteins", "author": ["S. Sonderby", "C. Sonderby", "H. Nielsen", "O. Winther"], "venue": "2015, arXiv:1503.01919.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Hkust/mts: A very large scale mandarin telephone speech corpus", "author": ["Y. Liu", "P. Fung", "Y. Yang", "C. Cieri", "S. Huang", "D. Graff"], "venue": "ISCSLP, 2006, pp. 724\u2013735.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "An efficient gradient-based algorithm for online training of recurrent neural network trajectories", "author": ["R. Williams", "J. Peng"], "venue": "Neural Computation, vol. 2, pp. 490\u2013501, 1990.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Asynchronous peer-topeer data mining with stochastic gradient descent", "author": ["R. Orm\u00e1ndi", "I. Heged\u00fcs", "M. Jelasity"], "venue": "Lecture Notes in Computer Science, pp. 528\u2013540, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Asynchronous stochastic gradient descent for dnn training", "author": ["S. Zhang", "C. Zhang", "Z. You", "R. Zheng", "B. Xu"], "venue": "ICASSP, 2013, pp. 6660\u20136663.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "2012, arXiv:1211.5063.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving long short-term memory networks using maxout units for large vocabulary speech recognition", "author": ["X. Li", "X. Wu"], "venue": "ICASSP, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "On rectified linear units for speech processing", "author": ["M. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q. Le", "P. Nguyen", "A. Senior", "V. Vanhouche", "J. Dean", "G. Hinton"], "venue": "ICASSP, 2013, pp. 3517\u20133521.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G. Dahl", "T. Sainath", "G. Hinton"], "venue": "ICASSP, 2013, pp. 8609\u20138613.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout neural networks for speech recognition", "author": ["M. Cai", "Y. Shi", "J. Liu"], "venue": "ASRU, 2013, pp. 291\u2013296.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep maxout networks for low resource speech recognition", "author": ["Y. Miao", "S. Rawat", "F. Metze"], "venue": "ASRU, 2013, pp. 398\u2013403.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "[1][2][3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1][2][3]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "[1][2][3]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "[4][5][6]), the network architectures (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4][5][6]), the network architectures (e.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[4][5][6]), the network architectures (e.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "[7][8][9]), and speaker adaptive methods (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7][8][9]), and speaker adaptive methods (e.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "[7][8][9]), and speaker adaptive methods (e.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "[10][11]), and have been shown to give significant performance improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10][11]), and have been shown to give significant performance improvements.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "[7] proposed to apply CNNs in the frequency domain to explicitly normalize speech spectral features to achieve frequency invariance and enforce locality of features, which have shown that further error rate reduction could be obtained comparing to the fully-connected DNNs on the phoneme recognition task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Subsequently, researchers have applied this idea on large vocabulary speech recognition tasks [12][13][14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Subsequently, researchers have applied this idea on large vocabulary speech recognition tasks [12][13][14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Subsequently, researchers have applied this idea on large vocabulary speech recognition tasks [12][13][14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 7, "context": "[8] proposed to use stacked bidirectional LSTM network trained with connectionist temporal classification (CTC) [15] for phoneme recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[8] proposed to use stacked bidirectional LSTM network trained with connectionist temporal classification (CTC) [15] for phoneme recognition.", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].", "startOffset": 193, "endOffset": 196}, {"referenceID": 16, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].", "startOffset": 200, "endOffset": 204}, {"referenceID": 18, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-the-art performance on robust speech recognition task [16], and many large vocabulary speech recognition tasks [9][17][18][19][20].", "startOffset": 208, "endOffset": 212}, {"referenceID": 6, "context": "In the literatures, most CNNs were applied on the frequency domain, and the variability along the time axis is handled by the fixed long time contextual window [7][12][13].", "startOffset": 160, "endOffset": 163}, {"referenceID": 11, "context": "In the literatures, most CNNs were applied on the frequency domain, and the variability along the time axis is handled by the fixed long time contextual window [7][12][13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "In the literatures, most CNNs were applied on the frequency domain, and the variability along the time axis is handled by the fixed long time contextual window [7][12][13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 20, "context": "[21][22][23]), and have been shown to give the state-ofthe-art performance on many ASR tasks by introducing LSTM RNNs recently.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[21][22][23]), and have been shown to give the state-ofthe-art performance on many ASR tasks by introducing LSTM RNNs recently.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "[21][22][23]), and have been shown to give the state-ofthe-art performance on many ASR tasks by introducing LSTM RNNs recently.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "However, RNNs are hard to be trained properly due to the vanishing gradient and exploding gradient problems as described in [24].", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "To address these problems, long short-term memory (LSTM) is proposed [25].", "startOffset": 69, "endOffset": 73}, {"referenceID": 24, "context": "The modern LSTM RNN architecture [25][26][27] is shown in Figure 1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "The modern LSTM RNN architecture [25][26][27] is shown in Figure 1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "The modern LSTM RNN architecture [25][26][27] is shown in Figure 1.", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "Besides, the LSTM Projected (LSTMP) network is proposed in [9][18], which has a separate linear projection layer after the LSTM layer, and yield improved performance on a large vocabulary speech recognition task.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": "Besides, the LSTM Projected (LSTMP) network is proposed in [9][18], which has a separate linear projection layer after the LSTM layer, and yield improved performance on a large vocabulary speech recognition task.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Usually, the max pooling function can be used as the pooling strategy, and in literature [14], variants of pooling functions, such as the lp pooling [28], stochastic pooling [29] were also evaluated.", "startOffset": 89, "endOffset": 93}, {"referenceID": 27, "context": "Usually, the max pooling function can be used as the pooling strategy, and in literature [14], variants of pooling functions, such as the lp pooling [28], stochastic pooling [29] were also evaluated.", "startOffset": 149, "endOffset": 153}, {"referenceID": 28, "context": "Usually, the max pooling function can be used as the pooling strategy, and in literature [14], variants of pooling functions, such as the lp pooling [28], stochastic pooling [29] were also evaluated.", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": "In addition, a structure called \u201cNetwork In Network\u201d (NIN) is proposed in [30] to enhance model for local patches within the receptive field, which replace the filters in conventional CNNs with a \u201cmicro network\u201d, such as a multilayer perceptron consisting of multiple fully connected layers with nonlinear activation functions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "A similar network architecture to CLSTM is the multidimensional LSTM [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 31, "context": "Another related work is introduced in [32] on biological sequence data analyzing, where the network architecture is a 1-dimensional convolutional layer followed by an LSTM layer, a fully connected layer and a final softmax layer, which can be understood as the stack of convolutional layer and LSTM layer.", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs on a large vocabulary speech recognition task the HKUST Mandarin Chinese conversational telephone speech recognition [33].", "startOffset": 183, "endOffset": 187}, {"referenceID": 1, "context": "The hybrid approach [2][34] is used, in which the neural networks\u2019 outputs are converted as pseudo likelihood as the state output probability in hidden Markov model (HMM) framework.", "startOffset": 20, "endOffset": 23}, {"referenceID": 33, "context": "The hybrid approach [2][34] is used, in which the neural networks\u2019 outputs are converted as pseudo likelihood as the state output probability in hidden Markov model (HMM) framework.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "In the training, the truncated back-propagation though time (BPTT) learning algorithm [35] is adopted.", "startOffset": 86, "endOffset": 90}, {"referenceID": 35, "context": "In order to train these networks on multi-GPU devices, asynchronous stochastic gradient descent [36][37] is adopted.", "startOffset": 96, "endOffset": 100}, {"referenceID": 36, "context": "In order to train these networks on multi-GPU devices, asynchronous stochastic gradient descent [36][37] is adopted.", "startOffset": 100, "endOffset": 104}, {"referenceID": 37, "context": "The strategy introduced in [38] is applied to scale down the gradients.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "Thus, in this paper, we have 5529 senones against 3302 senones in [19][39], leading to slightly better experimental results than that in [19][39].", "startOffset": 66, "endOffset": 70}, {"referenceID": 38, "context": "Thus, in this paper, we have 5529 senones against 3302 senones in [19][39], leading to slightly better experimental results than that in [19][39].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Thus, in this paper, we have 5529 senones against 3302 senones in [19][39], leading to slightly better experimental results than that in [19][39].", "startOffset": 137, "endOffset": 141}, {"referenceID": 38, "context": "Thus, in this paper, we have 5529 senones against 3302 senones in [19][39], leading to slightly better experimental results than that in [19][39].", "startOffset": 141, "endOffset": 145}, {"referenceID": 39, "context": "In Table 1, \u201c4\u00d7ReLU2000\u201d network has 4 hidden layers and each layer has 2000 rectified linear units (ReLU) [40][41], and \u201c4\u00d7Maxout800G3\u201d network has 4 hidden layers and each layer has 800 maxout units [42][43], where the group size is 3.", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "In Table 1, \u201c4\u00d7ReLU2000\u201d network has 4 hidden layers and each layer has 2000 rectified linear units (ReLU) [40][41], and \u201c4\u00d7Maxout800G3\u201d network has 4 hidden layers and each layer has 800 maxout units [42][43], where the group size is 3.", "startOffset": 111, "endOffset": 115}, {"referenceID": 41, "context": "In Table 1, \u201c4\u00d7ReLU2000\u201d network has 4 hidden layers and each layer has 2000 rectified linear units (ReLU) [40][41], and \u201c4\u00d7Maxout800G3\u201d network has 4 hidden layers and each layer has 800 maxout units [42][43], where the group size is 3.", "startOffset": 201, "endOffset": 205}, {"referenceID": 42, "context": "In Table 1, \u201c4\u00d7ReLU2000\u201d network has 4 hidden layers and each layer has 2000 rectified linear units (ReLU) [40][41], and \u201c4\u00d7Maxout800G3\u201d network has 4 hidden layers and each layer has 800 maxout units [42][43], where the group size is 3.", "startOffset": 205, "endOffset": 209}, {"referenceID": 18, "context": "Besides, based on the research in [19], LSTM based deep RNNs are constructed.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "Literatures [9][18] have proposed the LSTMP to make more effective use of model parameters to train acoustic models.", "startOffset": 12, "endOffset": 15}, {"referenceID": 17, "context": "Literatures [9][18] have proposed the LSTMP to make more effective use of model parameters to train acoustic models.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "Besides, multiple convolutional layers can also improve CNNs [13][14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "Besides, multiple convolutional layers can also improve CNNs [13][14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "Future work includes training the CLTSM CRNNs using sequence discriminative training criterion [20] and experiments on a larger corpus.", "startOffset": 95, "endOffset": 99}], "year": 2016, "abstractText": "Long short-term memory (LSTM) recurrent neural networks (RNNs) have been shown to give state-of-the-art performance on many speech recognition tasks, as they are able to provide the learned dynamically changing contextual window of all sequence history. On the other hand, the convolutional neural networks (CNNs) have brought significant improvements to deep feed-forward neural networks (FFNNs), as they are able to better reduce spectral variation in the input signal. In this paper, a network architecture called as convolutional recurrent neural network (CRNN) is proposed by combining the CNN and LSTM RNN. In the proposed CRNNs, each speech frame, without adjacent context frames, is organized as a number of local feature patches along the frequency axis, and then a LSTM network is performed on each feature patch along the time axis. We train and compare FFNNs, LSTM RNNs and the proposed LSTM CRNNs at various number of configurations. Experimental results show that the LSTM CRNNs can exceed stateof-the-art speech recognition performance.", "creator": "LaTeX with hyperref package"}}}