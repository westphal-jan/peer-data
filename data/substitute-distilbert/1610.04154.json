{"id": "1610.04154", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "An Information Theoretic Feature Selection Framework for Big Data under Apache Spark", "abstract": "with the advent of completely high profile datasets, dimensionality reduction techniques are becoming mandatory. among these techniques, feature selection claims been growing in interest as an important tool helped derive relevant features on huge datasets - - both utilizing choice of instances and features - -. the purpose of current work is to demonstrate that standard feature selection methods which be parallelized in big game platforms like apache spark, boosting both performance and accuracy. we thus make a distributed implementation of a generic feature selection construct which includes a wide group, well - known information theoretic authors. experimental results on a wide set of e - world datasets show that our distributed framework is capable of dealing with ultra - high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all current cases studied.", "histories": [["v1", "Thu, 13 Oct 2016 16:17:07 GMT  (417kb,D)", "https://arxiv.org/abs/1610.04154v1", null], ["v2", "Wed, 19 Oct 2016 16:46:28 GMT  (417kb,D)", "http://arxiv.org/abs/1610.04154v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC cs.LG", "authors": ["sergio ram\\'irez-gallego", "h\\'ector mouri\\~no-tal\\'in", "david mart\\'inez-rego", "ver\\'onica bol\\'on-canedo", "jos\\'e manuel ben\\'itez", "amparo alonso-betanzos", "francisco herrera"], "accepted": false, "id": "1610.04154"}, "pdf": {"name": "1610.04154.pdf", "metadata": {"source": "CRF", "title": "An Information Theoretic Feature Selection Framework for Big Data under Apache Spark", "authors": ["Sergio Ram\u0131\u0301rez-Gallego", "H\u00e9ctor Mouri\u00f1o-Ta\u013a\u0131n", "David Mart\u0301\u0131nez-Rego", "Ver\u00f3nica Bol\u00f3n-Canedo", "Jos\u00e9 Manuel Be\u0144\u0131tez", "Amparo Alonso-Betanzos", "Francisco Herrera"], "emails": ["sramirez@decsai.ugr.es,", "j.m.benitez@decsai.ugr.es,", "herrera@decsai.ugr.es", "h.mtalin@udc.es,", "dmartinez@udc.es,", "veronica.bolon@udc.es,", "ciamparo@udc.es"], "sections": [{"heading": null, "text": "Index terms\u2014 High-dimensional, Filtering methods, Feature selection, Apache Spark, Big Data."}, {"heading": "1 Introduction", "text": "During the last few decades, the dimensionality of datasets employed in Machine Learning (ML) or Data Mining tasks has increased significantly. This presents\nar X\niv :1\n61 0.\n04 15\n4v 2\nan unprecedented challenge for researchers in these areas, since the existing algorithms not always respond in an adequate time when dealing with this new extremely high dimension. In fact, if we analyze the datasets posted in the popular libSVM Database [9], we can observe that in the 1990s, the maximum dimensionality of the data was about 62 000; in the 2000s, this number increased to more than 16 million; and in the 2010s it further increased to more than 29 million. In this new scenario, it is common now to deal with millions of features, so the existing learning methods need to be adapted.\nWith the advent of extremely high dimensional datasets mentioned above, the identification of the relevant features has become paramount. Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6]. These dimensionality reduction techniques usually come in two flavors: feature selection (FS) and feature extraction. Both of them have their own merits. Feature extraction techniques combine the original features to yield a new set of features whereas feature selection techniques remove the irrelevant and redundant features. Due to the fact that FS maintains the original features, it is especially useful for applications where the original features are important for model interpretation and knowledge extraction [36, 7], and so this model will be the focus of this paper.\nOn the other hand, existing FS methods are not expected to scale well when dealing with Big Data due to the fact that their efficiency may significantly deteriorate or even become inapplicable [5]. Scalable distributed programming protocols and frameworks have been appearing in the last decade to manage the problem of Big Data. The first programming model was MapReduce [11] along with its open-source implementation Apache Hadoop [35, 1]. Recently, Apache Spark [20, 31], a new distributed framework, was presented as a fast and general engine for large-scale data processing, popular among machine learning researchers due to its suitability for iterative procedures.\nLikewise, several libraries for approaching ML task in Big Data environments have appeared in recent years. The first such library was Mahout [2], subsequently followed by MLlib [32] built on top of the Spark system [31]. Thanks to Spark\u2019s ability to do in-memory computation and so speed up iterative processes, algorithms developed for this kind of platform become pervasive in industry. Despite the fact that several golden standard algorithms for ML tasks have been redesigned with a distributed implementation for big data technologies already, it is not the case for FS algorithms yet. Only a simple approach based on Chi-Squared1, and an improvement to FS on Random Forest [33] have been proposed in the literature to deal with this problem.\nThis work aims at filling this gap. Our main purpose is to demonstrate that standard FS methods can be designed in these Big Data platforms and still can prove to be useful when dealing with big datasets, boosting both performance and accuracy. Here, we propose a new distributed design for a FS generic framework based on Information Theory [8], which has been implemented using\n1http://spark.apache.org/docs/latest/mllib-feature-extraction.html\nApache Spark paradigm. A wide variety of techniques from the distributed environment have been used to make feasible this adaptation: information caching, data partitioning and replication of relevant variables, among others. Notice that adapting this framework to Spark implies a deep restructuring of these classic algorithms, which presents a big challenge for researchers.\nLastly, to test the effectiveness of our framework, we have applied it to a complete set of real-world datasets (up to O(107) features and instances). The subsequent results have shown the competitive performance (in terms of generalization performance and efficiency) of our method when dealing with huge datasets \u2013both in number of features and instances\u2013. As an illustrative example, we have been able to select 100 features in a dataset with 29 \u00d7 106 features and 19\u00d7106 instances in less than 50 minutes (using a 432-core cluster).\nThe remainder of this paper is organized as follows: Section 2 provides some background information about FS, Big Data, MapReduce programming model and other frameworks. Section 3 describes the distributed framework proposed for FS in Big Data. Section 4 presents and discuss the experiments carried out. Finally, Section 5 concludes the paper."}, {"heading": "2 Background", "text": "In this section we give a brief introduction to FS, followed by a discussion on the advent of Big Data and its implication in this area. Finally, we outline the particularities of the MapReduce framework and the models derived from it."}, {"heading": "2.1 Feature Selection", "text": "FS is a dimensionality reduction technique that tries to remove irrelevant and redundant features from the original data. Its goal is to obtain a subset of features that describes properly the given problem with a minimum degradation of performance, in order to obtain simpler and more accurate schemes [17].\nFormally, we can define feature selection as: let ei be an instance ei = (ei1, . . . , ein, eiy), where eir corresponds to the r-th feature value of the i-th sample and eiy with the value of the output class Y . Let us assume a training set D withm examples, which instances ei are formed by a setX of n characteristics or features, and a test set Dt exist. Then let us define S\u03b8 \u2286 X as a subset of selected features yielded by an FS algorithm.\nFS methods can be broadly categorized as [4]:\n1. Wrapper methods, which use an evaluation function dependent on a learning algorithm [21]. They are aimed at optimizing a predictor as part of the learning process.\n2. Filtering methods, which use other selection techniques as separability measures or statistical dependences. They only consider the general characteristics of the dataset, being independent of any predictor [16].\n3. Embedded methods, which use a search procedure which is implicit in the classifier/regressor [30].\nFilter methods usually result in a better generalization due to its learning independence. Nevertheless, they usually select larger feature subsets, requiring sometimes a threshold to control them. Regarding complexity, filters are normally less expensive than wrappers. In those cases in which the number of features is large (especially for Big Data), it is indispensable to employ filtering methods as they are much faster than the other approaches."}, {"heading": "2.2 Big Data: a two-sided coin", "text": "Whereas the Internet continues generating quintillions bytes of data, the problem of handling large collections of those data is becoming more and more latent in our world. For example, in 2012, 2.5 exabytes of daily data were created. Collecting, transmitting and maintaining large data is no longer feasible. Exceptional technologies are needed to efficiently process these large quantities of data to obtain information, within tolerable elapsed times.\nExtracting valuable information from these collections of data has thus became one of the most important and complex challenges in data analytics research. This situation has caused that many knowledge extraction algorithms turn into obsolete methods when they face such vast amounts of data. As a result, the need for new methods, capable of managing such amount of data efficiently with similar performance, arises.\nBig Data is a popular term used to describe the exponential growth and availability of data nowadays, that becomes a problem for classical data analytics. Gartner [22] introduced the 3Vs concept by defining Big Data as high volume, velocity and variety information that require a new large-scale processing. Afterwards, this list was extended with 2 additional Vs. An under-explored but not less important topic is the \u201cBig Dimensionality\u201d in Big Data [38]. This phenomenon, also known as the \u201cCurse of Big Dimensionality\u201d, is boosted by the explosion of features and the combinatorial effects from new large incoming data where thousand or even millions of features are present.\nFrom the beginning, data scientists have generally focused on only one side of Big Data, which early days refers to the huge number of instances; paying less attention to the feature side. Big Dimensionality, though, calls for new FS strategies and methods that are able to deal with the feature explosion problem. It has been captured in many of the most famous dataset repositories in computational intelligence (like UCI or libSVM) [24, 9], where the majority of the new added datasets present a huge dimensionality [38] (e.g. almost 30 millions for the dataset KDD2010 ).\nNot only the amount of features but also the myriad of feature types and their combinations are becoming a standard in many real world applications. For instance, on the Internet, all multimedia content represents about 60% of total traffic [29], transmitted in thousands of different formats (audio, video, images, etc.). Another example is Natural Language Processing (NLP), where\nmultiple feature types such as words, n-gram templates, etc., are simultaneously employed so as to produce comprehensible and reliable models [26].\nDespite this myriad, not all the features in a problem contribute equally on the prediction models and results. FS is thus required by the learning and prediction processes for a fast and cost-effective performance, now more than ever. Isolating high value features from the raw set of features (potentially irrelevant, redundant and noisy), while maintaining the requirements in measurement and storage, is one of the most important tasks in Big Data research."}, {"heading": "2.3 MapReduce Programming Model and Frameworks: Hadoop and Spark", "text": "The MapReduce framework [11] was born in 2003 as a revolutionary tool in Big Data, designed by Google for processing and generating large-scale datasets. It was thought to automatically process data in a extremely distributed way through large clusters of computers. The framework is in charge of partitioning and managing data, recovering failure, job scheduling and communication; leaving to the programmers a transparent and scalable tool to easily execute tasks on distributed systems2.\nMapReduce is based on two phases of processing: Map and Reduce. First of all, users implement a Map function that processes key-value pairs which are transformed into a set of intermediate pairs, and a Reduce function that merges all intermediate pairs with a matching key. In this phase, the master node splits the data into chunks and distributes them across the nodes for an independent processing (in a divide-and-conquer fashion). Each node then executes the Map function on a given input subset and notifies its ending to the master node. After that, in the Reduce phase, the master node distributes the matching pairs across the nodes according to a key partitioning scheme, which combines these pairs using the Reduce function to form the final output.\nThe Map function takes <key, value> pairs as input and yields a list of intermediate <key, value> pairs as output. The Map function that internally process the data is defined by the user following a key-value scheme. Hence, the general scheme for a Map function is defined as:\nMap(< key1, val1 >)\u2192 list(< key2, val2 >) (1)\nIn the second phase, the master groups pairs by key and distributes the combined result to the Reduce functions started in each node. Here, a reduction function is applied to each associated list value and a new output value is yielded. This process can be schematized as follows:\nReduce(< key2, list(val2) >)\u2192< key3, val3 > (2) 2For a exhaustive review of MapReduce and others programming frameworks, please\ncheck [14].\nApache Hadoop [35, 1] is an open-source implementation of MapReduce for reliable, scalable, distributed computing. Despite being the most popular opensource implementation of MapReduce, Hadoop is not suitable in many cases, such as online and/or iterative computing, high inter-process communication paradigms or in-memory computing, among others [25].\nIn recent years, Apache Spark has been introduced in the Hadoop Ecosystem [20, 31]. This powerful framework is aimed at performing faster distributed computing on Big Data by using in-memory primitives that allows it to perform 100 times faster than Hadoop for certain applications. This platform allows user programs to load data into memory and query it repeatedly, making it a well suited tool for online and iterative processing (especially for machine learning algorithms). Additionally, it provides a wider range of primitives that ease the programming task.\nSpark is based on a distributed data structures called Resilient Distributed Datasets (RDDs). By using RDDs, we can implement several distributed programming models like Pregel or MapReduce, thanks to their generality capability. These parallel data structures also let programmers persist intermediate results in memory and manage the partitioning to optimize data placement.\nAs a subproject of Spark, a scalable machine learning library (MLlib) [32] was created. MLlib is formed by common learning algorithms and statistic utilities. Among its main functionalities includes: classification, regression, clustering, collaborative filtering, optimization, and dimensionality reduction (mostly feature extraction)."}, {"heading": "3 Filtering Feature Selection for Big Data", "text": "In [8], an Information Theoretic framework that includes many common FS filter algorithms was proposed. In their work the authors prove that algorithms like minimum Redundancy Maximum Relevance (mRMR) and others are special cases of Conditional Mutual Information when some specific independence assumptions are made about both the class and the features (see details below). Here, we demonstrate that these criteria are not only a sound theoretical formulation, but it also fits well in modern Big Data platforms and allows us to distribute several FS methods and their complexity across a cluster of machines.\nIn this work, we describe how we have redesigned this framework for a distributed paradigm. This version contains a generic implementation of several Information Theoretic FS methods such as: mRMR, Conditional Mutual Information Maximization (CMIM), or Joint Mutual Information (JMI), among others; that furthermore have been designed to be integrated in the MLlib Spark library. Additionally, the framework can be extended with other criteria provided by the user as long as they comply with the guidelines proposed by Brown et al.\nIn Section 3.1, we present this framework and explain the process to adapt it to the Big Data environment. Section 3.2 describes how the selection process and the underlying information theory operations have been implemented in a\ndistributed manner by using Spark primitives."}, {"heading": "3.1 Filter methods based on Information Theory", "text": "Information measures tell us how much information has been acquired by the receiver when he/she gets a message [27]. In predictive learning, we associate the message with the output feature in classification.\nA commonly used uncertainty function is Mutual Information (MI) [10], which measures the amount of information one random variable contains about another. This is, the reduction in the uncertainty of one random variable due to the knowledge of the other:\nI(A;B) = H(A)\u2212H(A|B) = \u2211 a\u2208A \u2211 b\u2208B p(a, b) log p(a, b) p(a)p(b) .\n(3)\nwhere A and B are two random variables with marginal probability mass functions p(a) and p(b), respectively; p(a, b) the joint mass function and H the entropy.\nIn the same way, MI can be conditioned to a third random variable. Thus, Conditional Mutual Information (CMI) is denoted as:\nI(A;B|C) = H(A|C)\u2212H(A|B,C) = \u2211 c\u2208C p(c) \u2211 a\u2208A \u2211 b\u2208B p(a, b, c) log p(a, b, c) p(a, c)p(b, c) .\n(4)\nwhere C is a third random variable with marginal probability mass function p(c); and p(a, c), p(b, c) and p(a, b, c) the joint mass functions.\nFiltering methods are based on a quantitative criterion or index, also known as relevance index or scoring. This index is aimed at measuring the usefulness of each feature for a specific classification problem. Through the relevance (self-interaction) of a feature with the class, we can rank the features and select the most relevant ones. However, the features can also be ranked using a more complex criterion such as if a feature is more redundant than another (multiinteraction). For instance, redundant features can be discarded (those variables that carry similar information) using the Mutual Information criterion [3]:\nJmifs(Xi) = I(Xi;Y )\u2212 \u03b2 \u2211 Xj\u2208S I(Xi;Xj),\nwhere S \u2286 S\u03b8 is the current set of selected features and \u03b2 is a weight factor. It considers the MI between each candidate Xi 6\u2208 S and the class, but also introduces a penalty proportional to its redundancy, calculated as the MI between the current set of selected features and each candidate.\nThere are a wide range of methods in the literature built on these information theoretic measures. To homogenize the use of all these criteria, Brown et\nal. [8] proposed a generic expression that allows to ensemble multiple information theoretic criteria into a unique FS framework. This framework is based on a greedy optimization process which assesses features based on a simple scoring criterion. Through some independence assumptions, it allows to transform many criteria as linear combinations of Shannon entropy terms: MI and CMI [10]. In some cases, it expresses more complex criteria as non-linear combinations of these terms (e.g. max or min). For a detailed description of the transformation processes, please see [8]. The generic formula proposed by Brown et al. [8] is:\nJ = I(Xi;Y )\u2212 \u03b2 \u2211 Xj\u2208S I(Xj ;Xi) + \u03b3 \u2211 Xj\u2208S I(Xj ;Xi|Y ), (5)\nwhere \u03b3 represents a weight factor for the conditional redundancy part. The formula can be divided into three parts: the first one represents the relevance of a feature Xi, the second one the redundancy between two features Xi and Xj , and the last one the conditional redundancy between two features Xi, Xj and the class Y . Through the aforementioned assumptions, many criteria were re-written by the authors to fit the generic formulation so that all these methods could be implemented with a slight variation in this formula. In Table 1, we show a comprehensive list of methods implemented in our proposal according to the adaptation proposed by Brown et al."}, {"heading": "3.2 Filter FS Framework for Big Data", "text": "Here, we present the proposed FS framework for Big Data using distributed operations. We outline the most important improvements carried out to adapt the classical approach to this new Big Data environment. Similarly, we analyze\nthe implications derived from the distributed implementation of Equation 5, as well as the complexity derived from the parallelization of the core operations of this expression: MI and CMI.\nBeyond the implementation on Spark, we have re-designed Brown\u2019s framework by adding some new important improvements to the performance of the classical approach, but also maintaining some features of this one:\n\u2022 Columnar transformation: The access pattern presented by most FS methods is thought to be feature-wise; in contrast to many other ML algorithms, which are used to work with rows (instance-wise). Despite being a simple detail, this can significantly degrade the performance since the natural way of computing relevance and redundancy in FS methods is normally thought to be performed by columns. This is specially important for distributed frameworks like Spark, where the partitioning scheme of data is quite influential in the performance.\n\u2022 Use of broadcasting: Once all features values are grouped and partitioned into different partitions, minimum data shift should occurred in order to avoid superfluous network and CPU usage. So if the MI process is performed locally in each partition, the overall algorithm will run efficiently (almost linearly) . We propose to minimize the data movement by replicating the output feature and the last selected feature in each iteration.\n\u2022 Caching pre-computed data: The first term that appears in the generic criterion of Equation 5 is relevance, which basically implies to calculate MI between all input features and the output (relevance). This operation is performed once at the start of our algorithm, then cached to be re-used in the next evaluations of Equation 5. Likewise, the subsequent marginal and joint proportions derived from these operations are also kept to omit some computations. This will also help to isolate the computation of redundancy per feature by replicating this permanent information in all nodes.\n\u2022 Greedy approach: Brown et al. proposed a greedy search process so that only one feature is selected in each iteration. This fact transform the quadratic complexity of typical FS algorithms into a more manageable complexity determined by the number of features to select.\nWe have also employed some complex operations from Spark API, which we present below. Spark primitives extend the idea of MapReduce to offer much more complex operations that ease code parallelization. Here, we outline those more relevant for our method3:\n\u2022 mapPartitions: Similar to Map, this runs a function independently on each partition. For each partition, an iterator of tuples is fetched and another of the same type is generated.\n3For a complete description of Spark\u2019s operations, please refer to Spark\u2019s API: https: //spark.apache.org/docs/latest/api/scala/index.html\n\u2022 groupByKey: This operation groups those tuples with the same key in a single vector of values (using a shuffle operation).\n\u2022 sortByKey: A distributed version of merge sort.\n\u2022 broadcast: This operation allows to keep a read-only copy of a given variable on each node rather than shipping a copy to each task. This is normally used for large permanent variables (such as big hash tables)."}, {"heading": "3.2.1 Main FS Algorithm", "text": "In Algorithm 1, the main algorithm for selecting features is presented. This procedure is in charge of deciding which feature to select in a sequential manner. Roughly, it calculates the initial relevances for all the features, and iterates over it selecting the best features according to Equation 5 and the underlying MI and CMI values.\nAlgorithm 1 Main FS Algorithm\nInput: D Dataset, an RDD of samples. Input: ns Number of features to select. Input: npart Number of partitions to set. Input: cindex Index of the output feature. Output: S\u03b8 Index list of selected features Dc \u2190 columnarTransformation(D,ns, npart) ni\u2190 D.nrows;nf \u2190 D.ncols REL\u2190 computeRelevances(Dc, cindex, ni) CRIT \u2190 initCriteria(REL) pbest \u2190 CRIT.max sfeat\u2190 Set(pbest) while |S| < |S\u03b8| do RED \u2190 computeRedundancies(Dc, pbest.index) CRIT \u2190 updateCriteria(CRIT,RED) pbest \u2190 CRIT.max sfeat\u2190 addTo(pbest, sfeat)\nend while return(sfeat)\nThe first step consists of transforming data into a columnar format as proposed in the list of improvements. Once data matrix is transformed, the algorithm obtains the relevance for each feature in X, initializing the criterion value (partial result according to Equation 5), and creates an initial ranking of the features. Relevance values are saved as part of the previous expression and re-used in next steps to update the criteria. Afterwards, the most relevant feature, pbest, is selected and added to the set sfeat, which is empty at first. The iterative phase begins by calculating MI and CMI between pbest, each candidate Xi, and Y . The subsequent values will serve to update the accumulated redundancies (simple and conditional) of the criteria. At each iteration, the most relevant candidate feature will be selected as the new pbest and added to\nsfeat. The loop ends when ns features (where ns = |S\u03b8|) have been selected, or there are no more features to select."}, {"heading": "3.2.2 Distributed Operations: Columnar Transformation and MI Computations", "text": "The estimation of MI and CMI are undoubtedly the costliest operations in Information Theoretic FS. When we face huge datasets, these operations are sequentially unfeasible to calculate as the number of combinations grow. This section describes how these calculations have been parallelized towards a set of distributed operations (explained in Section 2.3). For all the algorithms described below, RDD variables have been highlighted in uppercase in order to differentiate them from the ordinary variables.\nColumnar Transformation Columnar format is clearly much more manageable for filter FS methods than row-wise format, as mentioned before. Algorithm 2 explains this transformation, carried out in our algorithm as the the first step. The idea behind this transformation is to transpose the local data matrix provided by each partition. This partition operation will maintain the partitioning scheme without incurring in a high shuffling overhead. Additionally, once data are transformed, they can be cached and re-use in the subsequent loop. The result of this operation is a new matrix with one row per feature. It generates a tuple, where k represents the feature index, part.index the index of the partition (henceforth block index) and matrix(k) the local matrix for this feature-block.\nAlgorithm 2 Function that transform row-wise data into a columnar format (columnarTransformation)\nInput: D Dataset, an RDD of samples. Input: nf Number of features. Input: npart Number of partitions to set. Output: Column-wise data (RDD of feature vectors). 1: Dc \u2190 2: map partitions part \u2208 D 3: matrix\u2190 new Matrix(nf)(part.length) 4: for j = 0 until part.length do 5: for i = 0 until nf do 6: matrix(i)(j)\u2190 part(j)(i) 7: end for 8: end for 9: for k = 0 until nf do 10: EMIT < k, (part.index,matrix(k)) > 11: end for 12: end map 13: return(Dc.sortByKey(npart))\nIn order to benefit from data locality, the algorithm allocates all instances of the same feature in a determined set of partitions (if possible, only in one). To\ndo that, this sorts the new instances by key, limiting the number of partitions to npart. In next phases, the partitions will be mapped with the aim of generating a number of histograms per feature, which count the number of occurrences by combination.\nChoosing a proper number of partitions is important for the next steps. If npart is equal or less than the number of features, the number of total histograms per feature will be two at most. On the contrary, if this number is greater than the number of features, the total number of histograms generated per feature can be high since the same feature can be distributed across many partitions (more than two). We thus recommend setting this parameter to 2\u00d7 the number of features at most.\nFigure 1 details this process using a small example with eight instances and four features. In this figure, we can see how the algorithm generates a block for each feature in each partition. Then, all blocks are sorted by feature in order to gather them in the same partitions.\nComputing Relevance After transforming data, Algorithm 3 describes how to compute relevance (MI) between all the input features and Y (as expressed in Equation 3). This has been designed as an initialization method, so that all variables that appear in this function can be used in the subsequent algorithms. For example, the number of distinct values for each feature is first computed and saved as counter (to limit the size of histograms).\nThe main idea behind relevance and redundancy functions is to perform the calculations for each feature independently. This is done by distributing only the single variables (pbest and Y ) across the cluster, and leverage for the aforementioned data locality property. In this case, the first step consists of collecting all blocks of Y from the data, and putting all of them in a single vector to be broadcasted (bycol). Histograms for all the candidate features with\nAlgorithm 3 Compute mutual information between the set of features X and Y . (computeRelevances)\nInput: Dc RDD of tuples (index, (block, vector)). Input: yind Index of Y . Input: ni Number of instances. Output: MI values for all input features. 1: ycol\u2190 Dc.lookup(yind) 2: bycol\u2190 broadcast(ycol) 3: counter \u2190 broadcast(getMaxByFeature(Dc)) 4: H \u2190 getHistograms(Dc, yind, bycol, null, null) 5: joint\u2190 getProportions(H,ni) 6: marginal\u2190 getProportions(aggregateByRow(joint), ni) 7: return(computeMutualInfo(H, yind, null))\nrespect to Y are then calculated in getHistograms (explained below). This function is common to the relevance and redundancy phases. This computes 3-dimensional histograms between all non-selected features and two secondary variables (for redundancy) and between all non-selected features and one variable (for relevance)4. Joint and marginal proportions are generated from the resulting histograms using matrix operations: aggregating proportions by row (marginal), and computing proportions for joint (joint). Finally, using this information we can now obtain the MI value for each candidate feature.\nComputing Redundancy In this case, the computation of the simple and conditional redundancy is performed between pbest, each candidate feature Xi and Y . The conditional redundancy introduces a third conditional variable (Y ), following the formula: I(Xj ;Xi|Y ) (introduced in Equation 4).\nThis operation is repeated until we reach the number of selected features that is specified as a parameter. Algorithm 4 details this process, which is an extension of relevance computation (Algorithm 3). This obtains the blocks for pbest from the RDD, and broadcasts them to all the nodes. Then, the function getHistograms is called with two variables in order to obtain the histograms for all the candidate features with respect to pbest, and Y . Note that the vector for Y is already available from the redundancy phase. Finally, both types of redundancy are computed using the function that computes MI and CMI (computeMutualInfo).\nHistograms creation Algorithm 5 computes 3-dimensional histograms for the set of candidate features with respect to pbest and Y , which later will be used to compute MI and CMI. In case of not providing the conditional variable, this yields histograms whose third dimension is equal to one.\nThe first two lines return the dimensions for pbest and Y variables using counter (Algorithm 1). Then, a map operation on each partition is started on the dataset. This operation iterates over the blocks derived from the colum-\n4For relevance, the null value is used to represent the lack of the second variable\nAlgorithm 4 Compute CMI and MI between pbest, the set of candidate features, and Y . (computeRedundancies)\nInput: Dc RDD of tuples (index, (block, vector)). Input: jind Index of pbest. Output: CMI values for all input features. 1: jcol\u2190 Dc.lookup(jind) 2: bjcol\u2190 broadcast(jcol) 3: H \u2190 getHistograms(Dc, jind, bjcol, yind, bycol) 4: return(computeMutualInfo(H, jind, yind))\nAlgorithm 5 Function that computes 3-dimensional histograms between pbest, the set of candidate features, and Y for CMI; or between the set of candidate features, and Y for MI. (getHistograms)\nInput: Dc RDD of tuples (index, (block, vector)). Input: jind Index of Y or pbest. Input: yind Index of feature Y (can be empty). Input: jcol Values for Y or pbest, a broadcasted matrix. Input: ycol Values for Y , a broadcasted matrix (can be empty). Output: Columnar-wise dataset (RDD of feature vectors). 1: jsize\u2190 counter(jind) 2: ysize\u2190 counter(yind) 3: H \u2190 4: map partitions part \u2208 partitions 5: for (k, (block, v))\u2190 part do 6: isize\u2190 counter(k) 7: m\u2190 newMatrix(ysize)(isize)(jsize) 8: for e = 0 until v.size do 9: j \u2190 jcol(block)(e); y \u2190 ycol(block)(e); i\u2190 v(e) 10: m(y)(i)(j)+ = 1 11: end for 12: EMIT < k,m > 13: end for 14: end map 15: return(H.reduceByKey(sum))\nnar transformation. Each tuple is formed by a key (the index of a candidate feature), and a value with an index block and the corresponding feature array ((k, (block, v))). For each instance, a matrix is initialized to zero and then incremented by one depending on the value in each combination pbest, Xi, and Y . Single features (pbest and Y ) are broadcasted in form of matrices, whose first axis indicates the block index, and the second one the partial index of the value in this block. This updating operation is repeated until the end of the feature vector. After that, a new tuple is emitted with the feature index as key and the resulting matrix as value (< k,m >). The map operation continues with the next block until finishing the partition. Finally, final histograms are aggregated by summing them up. This aggregation process will remain simple as long as\nthe number of histograms per partition is small. This is normally true as a single partition usually contains all the blocks for the same feature. Therefore, it is important to reduce the number of histograms by adjusting the number of partitions, as explained above.\nFigure 2 details the histograms creation process using a simple example. In this figure, we can observe how the algorithm generates one histogram for each partition and feature. In this case, the number of partitions corresponds with the number of features, so only one histogram per feature is generated.\nMI and CMI computations Algorithm 6 details the process that unifies the computation of MI and CMI. The algorithm takes as input the indices of single variables (Y or pbest for MI, and both variables for CMI), and all previously computed histograms. Before starting, the algorithm broadcasts the marginal and joint matrices that correspond to these variables. All this information is sent to the nodes because this cannot be computed from the previous histograms independently, and is already computed.\nA map phase is then started on each histogram tuple, which consists of a given feature index as key and a 3-dimensional matrix as value. In this phase, the algorithm generates the MI and CMI values for all the combinations between the histograms and the single variables (see Equations 3 and 4). Before that, it is needed to compute the marginal proportions for the set of candidate features, and the joint proportions between each Xi and Y ; and, each Xi and pbest (using matrix operations as described in 3). Once all joint and marginal proportions are calculated, a loop starts over all combinations to compute the proportion pabc (which comes directly from the histogram), and finally, the final result by combination. All these results are then aggregated to get the overall MI and CMI values for each feature.\nFigure 3 details the MI process started once all histograms have been computed. Each partition generates a histogram for each feature contained in it. Histograms for the same feature are aggregated to obtain a single final histogram. Only those marginal and joint proportions that can not be computed\nAlgorithm 6 Calculate MI and CMI for the set of histograms with respect to Y or pbest for MI, and both variables for CMI. (computeMutualInfo)\nInput: H Histograms, an RDD of tuples (index, matrix). Input: bind Index of feature pbest. Input: cind Index of feature Y (can be empty). Output: MI and CMI values. 1: bprob\u2190 broadcast(marginal(bind)) 2: cprob\u2190 broadcast(marginal(cind)) 3: bcprob\u2190 broadcast(joint(bind)) 4: MINFO \u2190 5: map (k,m) \u2208 H 6: aprob\u2190 computeMarginal(m) 7: abprob\u2190 computeJoint(m, bind); acprob\u2190 computeJoint(m, cind) 8: for c = 0 until getSize(m) do 9: for b = 0 until getnRows(m(c)) do 10: for a = 0 until getnCols(m(c)) do 11: pc\u2190 cprob(c); pabc\u2190 (m(c)(a)(b)/ninstances)/pc 12: pac\u2190 acprob(c)(a); pbc\u2190 bcprob(c)(b) 13: cmi + = conditionalMutualInfo(pabc, pac, pbc, pc) 14: if c == 0 then 15: pa\u2190 xprob(a); pab\u2190 abprob(a)(b); pb\u2190 yprob(b) 16: mi + = mutualInfo(pa, pab, pb) 17: end if 18: end for 19: end for 20: end for 21: EMIT < k, (mi, cmi) > 22: end map 23: return(MINFO)\nindependently from each histograms are broadcasted. By using the histograms and these broadcasted variables, MI and CMI are computed per feature in an independent way."}, {"heading": "3.2.3 High-dimensional and sparse version", "text": "Previous version works well with \u201ctall and skinny\u201d data, formed by a small number of features and a large number of instances. However, when we face datasets with millions of features and with a high sparsity, this implies a new problem where the complexity grows in the opposite size (the horizontal one). It is therefore mandatory to re-adapt the previous algorithms in order to deal with such amount of characteristics, thus preventing an undermined performance. In this case, the algorithms highly affected by this change are detailed: the columnar transformation (Algorithm 2) and the histogram creation process (Algorithm 5). The rest of code remains unchanged except the structure of data, which is reduced to a single vector (in form of (index, vector)). The block index is therefore removed from this structure as only one histogram is generated per each feature.\nAs explained before, high-dimensional sparse data is usually characterized by a large number of features and a variable number of instances with an undefined number of non-zero indexed elements. Algorithm 7 presents a new model of processing that transposes data directly, generating single vectors for each feature. Sparsity is maintained on sparse vectors but employing the index of the original instance as key. The algorithm generates a tuple for each value so that the key is formed by the feature index, and the value is formed by the instance index and the value itself. All tuples are grouped by key to create a single sparse vector (formed by sorted key-value tuples).\nAlgorithm 7 Function that transforms row-wise data into columnar data (sparse version) (sparseColumnar)\nInput: D Dataset, an RDD of sparse samples. Input: npart Number of partitions to set. Output: Column-wise dataset (feature vectors). 1: Dc \u2190 2: map reg \u2208 D 3: ireg \u2190 reg.index 4: for i = 0 until reg.length do 5: EMIT < reg(i).key, (ireg, reg(i).value) > 6: end for 7: end map 8: Dc \u2190 Dc.groupByKey(npart).mapV alues(vectorize)\nRegarding the histogram creation, in this new version the map partition operation has been replaced by a map operation, which is applied to each feature vector previously generated. Algorithm 8 details this process. This is quite similar to Algorithm 5 with the caveat that only one histogram is yielded for\neach feature and, therefore, no reduce operation is needed.\nAlgorithm 8 Function that computes 3-dimensional histograms for the set of features Xi with respect to features pbest and Y (sparse version). (sparseHistograms)\nInput: Dc Dataset, an RDD of tuples (Int, (Int, Vector)). Input: jind Index of Y or pbest. Input: yind Index of feature Y (can be empty). Input: jcol Values for Y or pbest, a broadcasted matrix. Input: ycol Values for Y , a broadcasted Matrix (can be empty). Output: Columnar-wise dataset (RDD of feature vectors). 1: jsize\u2190 counter(jind); ysize\u2190 counter(yind) 2: jyhist\u2190 frequencyMap(jind, yind) 3: zhist\u2190 frequencyMap(yind) 4: H \u2190 5: map (k, v) \u2208 Dc 6: isize\u2190 counter(k) 7: m\u2190 newMatrix(ysize)(isize)(jsize) 8: for e = 0 until v.size do 9: j \u2190 jcol(e); y \u2190 ycol(e) 10: i\u2190 v(e) 11: if j <> 0 then 12: jyhist(j)(y) = jyhist(j)(y)\u2212 1 13: end if 14: m(y)(i)(j)+ = 1 15: end for 16: for ((j, y), q)\u2190 jyhist do 17: m(y)(0)(j)+ = q 18: end for 19: for (y, q)\u2190 yhist do 20: m(y)(0)(0)+ = yhist(y)\u2212 sum(mat(y)) 21: end for 22: EMIT < k,m > 23: end map 24: return(H)\nAs opposed to the dense version, the matrix generation process has be adapted to avoid visiting all possible combinations in the sparse features, using as much as possible accumulators to compute those combinations formed by zeros. As accumulators, the algorithm calculates the class histogram for the conditional variable and the joint class histogram for the parametric variables: jind and yind. Firstly, a loop is started for those combinations in which the first variable (i) is not equal to zero. The procedure is the same as in the dense version. Here, if the second variable (j) is equal to zero, the frequency counter (in the joint histogram) for the combination will be decreased by one. Secondly, for those cases in which j is not equal to zero, the algorithm completes the matrix with the frequencies in the joint histogram. And finally, for those cases in which both variables (j and y) are equal to zero, the matrix is updated with\nthe remaining occurrences for all classes5. The final result will be the feature index and the aforementioned matrix."}, {"heading": "3.2.4 Complexity of the algorithms", "text": "As we mentioned before, the FS algorithm performs a greedy search which stops when the condition defined as input is reached. Beyond that, this sequential algorithm is influenced by the set of distributed algorithms/operations presented in the previous section. The distributed primitives used in these algorithms need to be analyzed to check the complexity of the whole proposal. Note that the first operation (columnar transformation) is quite time-consuming as it makes a strong use of network and memory when shuffling all data (wide dependency). However, once data have a known partitioning, they can be re-used in the following phases (leveraging from the data locality property). Anyway, this transformation is performed once at the start, and can be omitted if data are already in a columnar format. The list of distributed operations involved in this algorithm are described below:\n\u2022 Algorithm 2: This algorithm starts with a mapPartitions operation that transposes the local matrix contained in the partition, and emits a tuple for each feature (linear distributed order). The total number of tuples is equal to multiplying n by the number of original partitions. Afterwards, these tuples are shuffled across the cluster, then a local sorting is launched on each subset (log-linear distributed order).\n\u2022 Algorithm 3-4: the first operation for both algorithms is the retrieval of a single column (feature) by using the lookup primitive (linear distributed order). As the data are already partitioned, the operation is done efficiently by only looking at the right partition. This variable is then broadcasted to all the nodes, which implies sending a single feature (m values) across the network. The next operations (histograms and MI computations) are described below.\n\u2022 Algorithm 5: this algorithm represents a simple mapReduce operation where the previously generated tuples are transformed to local histograms, and finally reduced to the final histograms by feature. This map operation consists of a linear function (O(m)) that fetches the data contained in each local matrix.\n\u2022 Algorithm 6: this operation starts by broadcasting three single values (proportion values). For each feature, three linear operations are launched to compute some extra probabilities. Finally, the mutual information values are computed by fetching the whole 3D-histogram (cubic linear order). Notice that the complexity of all these operations is bounded by the cardinality of the features included in the histogram.\n5Class vector is always dense"}, {"heading": "4 Experimental Framework and Analysis", "text": "This section describes the experiments carried out to evaluate the usefulness of FS over a set of real-world huge problems \u2013both in features and examples\u2013 using the proposed framework."}, {"heading": "4.1 Datasets and methods", "text": "Five classification datasets were used in order to measure the quality and usefulness of our framework implementation for Spark. We have classified these datasets into two groups: dense (large number of samples) and sparse (highdimensional datasets). For sparse datasets, the high-dimensional version presented in Section 3.2.3 is used.\nThe first dataset ECBDL14 was used as a reference at the International Conference GECCO-2014. This consists of 631 characteristics (including both numerical and categorical attributes) and 32 million instances. It is a binary classification problem where the class distribution is imbalanced: 98% of negative instances. For this imbalanced problem, the MapReduce version of the Random OverSampling (ROS) algorithm presented in [12] was applied (henceforth we will use ECBDL14 to refer to the ROS version). Another dataset used is dna, which consists of 50 000 000 instances with 201 discrete features. This dataset was created for the Pascal Large Scale Learning Challenge6 in 2008. In the experiments, it was only used the training set since the test set of dna does not contain the class labels, the training set was used to generate both subsets (using an 80/20 hold-out data split). As this problem also suffers imbalance between its classes, ROS technique was also applied (henceforth dna). The rest of datasets (epsilon, url and kddb) come from the LibSVM dataset repository [9]. These datasets and their descriptions can be found in the project\u2019s website7. Table 2 gives a summary of these datasets.\nAs an FS benchmark method, we have used mRMR algorithm [28] since it is one of the most relevant and cited selectors in the literature. Note that the FS algorithm chosen to test the performance does not affect the time results yielded by the framework since all criteria are computed in the same way.\n6http://largescale.ml.tu-berlin.de/summary/ 7http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\nIn order to carry out a comparison study, the following classifiers were chosen: Support Vector Machines (SVM) [18], and Naive Bayes [13]. For the experiments, we have employed the distributed versions of these algorithms implemented in the MLlib library [32]. The recommended parameters of the classifiers, according to their authors\u2019 specification [32], are shown in Table 3. For all executions, the datasets have been cached in memory as SVM and our method are iterative processes. The level of parallelism (number of partitions) has been set to 864, twice the total number of cores available in the cluster8.\nA Spark package associated to this work can be found in the third-party Spark\u2019s Repository: http: // spark-packages. org/ package/ sramirez/ sparkinfotheoretic-feature-selection . This software has been designed to be integrated as part of MLlib Library. This has associated a JIRA issue to discuss its integration in this library: https: // issues. apache. org/ jira/ browse/ SPARK-6531 .\nFor evaluation purposes, we use two common evaluation metrics to assess the quality of the subsequent FS schemes: Area under Receiver Operating Characteristic (AUROC, henceforth called only AUC) to evaluate the accuracy yielded by the classifier, and the modeling time in training to evaluate the performance of the FS process."}, {"heading": "4.2 Cluster configuration", "text": "For all the experiments we have used a cluster composed of eighteen computing nodes and one master node. The computing nodes hold the following characteristics: 2 processors x Intel Xeon CPU E5-2620, 6 cores per processor, 2.00 GHz, 15 MB cache, QDR InfiniBand Network (40 Gbps), 2 TB HDD, 64 GB RAM. Regarding the software, we have used the following configuration: Hadoop 2.5.0-cdh5.3.1 from Cloudera\u2019s open-source Apache Hadoop distribution9, HDFS replication factor: 2, HDFS default block size: 128 MB, Apache Spark and MLlib 1.2.0, 432 cores (24 cores/node), 864 RAM GB (48 GB/node).\nBoth HDFS and Spark master processes (the HDFS NameNode and the Spark Master) are hosted in the main node. The NameNode controls the HDFS, coordinating the slave machines by the means of their respective DataNode daemons whereas the Spark Master controls all the executors in each worker\n8The Spark creators recommend using 2-4 partitions per core: http://spark.apache.org/ docs/latest/programming-guide.html\n9http://www.cloudera.com/content/cloudera/en/documentation/cdh5/v5-0-0/CDH5homepage.html\nnode. Spark uses HDFS file system to load and save data in the same way Hadoop framework does."}, {"heading": "4.3 Analysis of selection results", "text": "Here, we evaluate the time employed by our implementation to rank the most relevant features. Table 4 presents the time results obtained by our algorithm using different ranking thresholds (number of features selected).\nAs can be seen in Table 4, our algorithm yields competitive results in all cases regardless of the number of iterations employed (represented by the threshold value). For those datasets with the highest volume of data: kddb (ultra-high dimensional data) and ECBDL14 (huge number of samples), it is important to note that our method is able to rank 100 features in less than one hour.\nFurthermore, a comparison study is performed between our distributed version and the sequential version developed by Brown\u2019s lab10. Samples from dna have been generated with different ratios of instances to study the scalability of our approach in counterpart to the sequential version11. The level of parallelism has been set to 200 in the distributed executions. This has been done to ease the comparison between the distributed version which uses one core per feature, and the sequential version that only uses one core.\nTable 5 and Figure 4 show the time results for this comparison. Regarding the sequential version, the last two values was estimated by using linear interpolation (highlighted in italics) since they could not be computed due to memory problems. As shown in Table 5, our distributed version outperforms the classical approach in all cases. This is specially remarkable for the largest dataset where the maximum speedup rate is achieved (29.83).\nFinally, an additionaly study of scalability has been performed by varying the number of cores used. For this study, ECBDL14 (the largest dense dataset) was used as a reference, with the same parameters as in the previous study. Figure 5 depicts the performance of our method varying the number of cores from ten to one hundred. The results show a logarithmic behaviour as the number of cores is increased. Note that in the first case, the number of nodes is only ten and the amount of memory available is not the fullest.\n10FEAST toolbox (python version): http://www.cs.man.ac.uk/~gbrown/software/ 11Sequential version has been executed in one node of our cluster with the aforementioned\ncharacteristics"}, {"heading": "4.4 Analysis of classification results", "text": "In this section, a study about the usefulness of our FS solution on large-scale classification is performed. Figures 6 and 7 show the accuracy results for SVM and Naive Bayes using different FS schemes. All datasets described in Table 2 have been used in this study except kddb because the aforementioned classifiers are not designed to work with such a big dimensionality.\nFigure 7 shows an important improvement on using FS over url and epsilon, whereas its application seems to have a negligible impact on AUC for dna and ECBDL14. This can be explained by the fact of their high imbalance ratio and/or their low number of features. Figure 6 presents similar results to the previous case. However, in this case the improvement in url dataset is much smaller.\nBeyond AUC, the time employed in creating a classification model is quite important in many large-scale problems. Figures 8 and 9 show the classification time employed in the training phase for different datasets and thresholds. The results demonstrate that the simplicity and the performance of the generated models is improved after applying FS in all the cases studied. It is specially important for SVM, which spend more time modeling than Naive Bayes.\nResults in performance demonstrate that our solution is capable of selecting features in a competitive time when it is applied to huge datasets \u2013both in number of instances and features\u2013. Results also demonstrate the benefit of using\nour approach against the sequential version in all cases, enabling the application of FS on the largest cases.\nFurthermore, using our selection schemes the classifiers show better classification results in most cases, obtaining similar results in the other cases. Note that in all the studied cases the resulting model is much simpler and faster in spite of using a small percentage of the original set of features."}, {"heading": "5 Conclusions", "text": "In this paper, we have discussed the problem of processing huge data, especially from the perspective of dimensionality. We have seen the effects of a correct identification of relevant features on these datasets as well as the difficulty of this task due to the combinatorial effects when incoming data grow \u2013both in number instances and features\u2013. In spite of the growing interest in the field of dimensionality reduction for Big Data, only few FS methods have been developed to deal with high-dimensional problems.\nThereby we have redesigned a generic FS framework for Big Data based on Information Theory, adapting a previous one proposed by Brown et al. in [8]. The framework contains implementations of many state-of-the-art FS algorithms like mRMR or JMI. However, the adaptation carried out has entailed a deep redesign of Brown et al.\u2019s framework so as to adapt it to the distributed paradigm. With this work we have also aimed at contributing by adding an FS module to the emerging Spark and MLlib platforms, where no complex FS algorithm has been included until now.\nThe experimental results show the usefulness of our FS solution over a wide set of large real-world problems. Our solution has thus revealed to perform well with two dimensions of Big Data (samples and features), obtaining competitive performance results when dealing with ultra-high dimensional datasets as well as those with a huge number of samples. Furthermore, our solution has outperformed the sequential version in all studied cases, enabling the resolution of problems that were not practical with the classical approach."}, {"heading": "Acknowledgment", "text": "This work is supported by the Spanish National Research Project TIN201237954, TIN2013-47210-P and TIN2014-57251-P, and the Andalusian Research Plan P10-TIC-6858, P11-TIC-7765 and P12-TIC-2958, and by the Xunta de Galicia through the research project GRC 2014/035 (all projects partially funded by FEDER funds of the European Union). S. Ram\u0131\u0301rez-Gallego holds a FPU scholarship from the Spanish Ministry of Education and Science (FPU13/00047). D. Mart\u0301\u0131nez-Rego and V. Bolo\u0301n-Canedo acknowledge support of the Xunta de Galicia under postdoctoral Grant codes POS-A/2013/196 and ED481B 2014/164- 0."}], "references": [{"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["Roberto Battiti"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Selection of relevant features and examples in machine learning", "author": ["Avrim L. Blum", "Pat Langley"], "venue": "Artif. Intell.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Recent advances and emerging challenges of feature selection in the context of big data", "author": ["V. Bol\u00f3n-Canedo", "N. S\u00e1nchez-Marono", "A. Alonso-Betanzos"], "venue": "Knowledge-Based Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Feature Selection for High-Dimensional Data. Artificial Intelligence: Foundations, Theory, and Algorithms", "author": ["Ver\u00f3nica Bol\u00f3n-Canedo", "Noelia S\u00e1nchez-Maro\u00f1o", "Amparo Alonso- Betanzos"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A review of microarray datasets and applied feature selection methods", "author": ["V. Bol\u00f3n-Canedo", "N. S\u00e1nchez-Maro\u00f1o", "A. Alonso-Betanzos", "J.M. Be\u0144\u0131tez", "F. Herrera"], "venue": "Information Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "OSDI", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "On the use of mapreduce for imbalanced big data using random forest", "author": ["Sara del \u0154\u0131o", "Victoria L\u00f3pez", "Jos\u00e9 Manuel Be\u0144\u0131tez", "Francisco Herrera"], "venue": "Information Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Pattern classification and scene analysis, volume", "author": ["Richard O. Duda", "Peter E. Hart"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1973}, {"title": "Big data with cloud computing: an insight on the computing environment, mapreduce, and programming frameworks", "author": ["Alberto Fern\u00e1ndez", "Sara del \u0154\u0131o", "Victoria L\u00f3pez", "Abdullah Bawakid", "Ma\u0155\u0131a Jos\u00e9 del Jes\u00fas", "Jos\u00e9 Manuel Be\u0144\u0131tez", "Francisco Herrera"], "venue": "Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["Fran\u00e7ois Fleuret"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing)", "author": ["Isabelle Guyon", "Steve Gunn", "Masoud Nikravesh", "Lotfi A. Zadeh"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Scholkopf. Support vector machines", "author": ["Marti A. Hearst", "Susan T. Dumais", "Edgar Osman", "John Platt", "Bernhard"], "venue": "Intelligent Systems and their Applications, IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Attribute Interactions in Machine Learning", "author": ["Aleks Jakulin"], "venue": "PhD thesis, University of Ljubljana,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Learning Spark: Lightning-Fast Big Data Analytics", "author": ["Holden Karau", "Andy Konwinski", "Patrick Wendell", "Matei Zaharia"], "venue": "O\u2019Reilly Media, Incorporated,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H. John"], "venue": "Artif. Intell.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "3d data management: Controlling data volume, velocity and variety", "author": ["Doug Laney"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Feature selection and feature extraction for text categorization", "author": ["David D. Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Mapreduce is good enough? if all you have is a hammer, throw away everything that\u2019s not a nail", "author": ["Jimmy Lin"], "venue": "CoRR, abs/1209.2191,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Efficient multitemplate learning for structured prediction", "author": ["Qi Mao", "Ivor Wai-Hung Tsang"], "venue": "IEEE Trans. Neural Netw. Learning Syst.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Experimental study of information measure and inter-intra class distance ratios on feature selection and orderings", "author": ["Mark Michael", "Wen-Chun Lin"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1973}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["Hanchuan Peng", "Fulmi Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "TR10: Peering into Video\u2019s Future", "author": ["Wade Roush"], "venue": "MIT Technology Review March", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A review of feature selection techniques in bioinformatics", "author": ["Yvan Saeys", "I\u00f1aki Inza", "Pedro Larra\u00f1aga"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "An improvement to feature selection of random forests on spark", "author": ["Ke Sun", "Wansheng Miao", "Xin Zhang", "Ruonan Rao"], "venue": "In IEEE 17th International Conference on Computational Science and Engineering (CSE),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Object recognition with informative features and linear classification", "author": ["Michel Vidal-Naquet", "Shimon Ullman"], "venue": "In ICCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Realistic human action recognition with multimodal feature selection and fusion. Systems, Man, and Cybernetics: Systems", "author": ["Qiuxia Wu", "Zhiyong Wang", "Feiqi Deng", "Zheru Chi", "D.D. Feng"], "venue": "IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["Howard Hua Yang", "John E Moody"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "The emerging \u201cbig dimensionality", "author": ["Yiteng Zhai", "Yew-Soon Ong", "Ivor W. Tsang"], "venue": "IEEE Comp. Int. Mag.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Spectral feature selection for data mining", "author": ["Zheng Alan Zhao", "Huan Liu"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "In fact, if we analyze the datasets posted in the popular libSVM Database [9], we can observe that in the 1990s, the maximum dimensionality of the data was about 62 000; in the 2000s, this number increased to more than 16 million; and in the 2010s it further increased to more than 29 million.", "startOffset": 74, "endOffset": 77}, {"referenceID": 13, "context": "Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6].", "startOffset": 142, "endOffset": 153}, {"referenceID": 31, "context": "Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6].", "startOffset": 142, "endOffset": 153}, {"referenceID": 3, "context": "Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6].", "startOffset": 142, "endOffset": 153}, {"referenceID": 28, "context": "Due to the fact that FS maintains the original features, it is especially useful for applications where the original features are important for model interpretation and knowledge extraction [36, 7], and so this model will be the focus of this paper.", "startOffset": 190, "endOffset": 197}, {"referenceID": 4, "context": "Due to the fact that FS maintains the original features, it is especially useful for applications where the original features are important for model interpretation and knowledge extraction [36, 7], and so this model will be the focus of this paper.", "startOffset": 190, "endOffset": 197}, {"referenceID": 2, "context": "On the other hand, existing FS methods are not expected to scale well when dealing with Big Data due to the fact that their efficiency may significantly deteriorate or even become inapplicable [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "The first programming model was MapReduce [11] along with its open-source implementation Apache Hadoop [35, 1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Recently, Apache Spark [20, 31], a new distributed framework, was presented as a fast and general engine for large-scale data processing, popular among machine learning researchers due to its suitability for iterative procedures.", "startOffset": 23, "endOffset": 31}, {"referenceID": 26, "context": "Only a simple approach based on Chi-Squared, and an improvement to FS on Random Forest [33] have been proposed in the literature to deal with this problem.", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "Here, we propose a new distributed design for a FS generic framework based on Information Theory [8], which has been implemented using", "startOffset": 97, "endOffset": 100}, {"referenceID": 13, "context": "Its goal is to obtain a subset of features that describes properly the given problem with a minimum degradation of performance, in order to obtain simpler and more accurate schemes [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 1, "context": "FS methods can be broadly categorized as [4]:", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "Wrapper methods, which use an evaluation function dependent on a learning algorithm [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "They only consider the general characteristics of the dataset, being independent of any predictor [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "Embedded methods, which use a search procedure which is implicit in the classifier/regressor [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Gartner [22] introduced the 3Vs concept by defining Big Data as high volume, velocity and variety information that require a new large-scale processing.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "An under-explored but not less important topic is the \u201cBig Dimensionality\u201d in Big Data [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "It has been captured in many of the most famous dataset repositories in computational intelligence (like UCI or libSVM) [24, 9], where the majority of the new added datasets present a huge dimensionality [38] (e.", "startOffset": 120, "endOffset": 127}, {"referenceID": 30, "context": "It has been captured in many of the most famous dataset repositories in computational intelligence (like UCI or libSVM) [24, 9], where the majority of the new added datasets present a huge dimensionality [38] (e.", "startOffset": 204, "endOffset": 208}, {"referenceID": 24, "context": "For instance, on the Internet, all multimedia content represents about 60% of total traffic [29], transmitted in thousands of different formats (audio, video, images, etc.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": ", are simultaneously employed so as to produce comprehensible and reliable models [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "The MapReduce framework [11] was born in 2003 as a revolutionary tool in Big Data, designed by Google for processing and generating large-scale datasets.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "2For a exhaustive review of MapReduce and others programming frameworks, please check [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Despite being the most popular opensource implementation of MapReduce, Hadoop is not suitable in many cases, such as online and/or iterative computing, high inter-process communication paradigms or in-memory computing, among others [25].", "startOffset": 232, "endOffset": 236}, {"referenceID": 16, "context": "In recent years, Apache Spark has been introduced in the Hadoop Ecosystem [20, 31].", "startOffset": 74, "endOffset": 82}, {"referenceID": 5, "context": "In [8], an Information Theoretic framework that includes many common FS filter algorithms was proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "Information measures tell us how much information has been acquired by the receiver when he/she gets a message [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "For instance, redundant features can be discarded (those variables that carry similar information) using the Mutual Information criterion [3]:", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "[8] proposed a generic expression that allows to ensemble multiple information theoretic criteria into a unique FS framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For a detailed description of the transformation processes, please see [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "[8] is:", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Mutual Information Maximisation (MIM) [23]", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "Mutual Information FS (MIFS) [3]", "startOffset": 29, "endOffset": 32}, {"referenceID": 29, "context": "Joint Mutual Information (JMI) [37]", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Minimum-Redundancy Maximum-Relevance (mRMR) [28]", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Conditional Mutual Information Maximization (CMIM) [15]", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "Informative Fragments (IF) [34] (equivalent to CMIM)", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Interaction Capping (ICAP) [19]", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "For this imbalanced problem, the MapReduce version of the Random OverSampling (ROS) algorithm presented in [12] was applied (henceforth we will use ECBDL14 to refer to the ROS version).", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "The rest of datasets (epsilon, url and kddb) come from the LibSVM dataset repository [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 23, "context": "As an FS benchmark method, we have used mRMR algorithm [28] since it is one of the most relevant and cited selectors in the literature.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "In order to carry out a comparison study, the following classifiers were chosen: Support Vector Machines (SVM) [18], and Naive Bayes [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "In order to carry out a comparison study, the following classifiers were chosen: Support Vector Machines (SVM) [18], and Naive Bayes [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "in [8].", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "With the advent of extremely high dimensional datasets, dimensionality reduction techniques are becoming mandatory. Among many techniques, feature selection has been growing in interest as an important tool to identify relevant features on huge datasets \u2013both in number of instances and features\u2013. The purpose of this work is to demonstrate that standard feature selection methods can be parallelized in Big Data platforms like Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of a generic feature selection framework which includes a wide group of well-known Information Theoretic methods. Experimental results on a wide set of real-world datasets show that our distributed framework is capable of dealing with ultra-high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all the cases studied.", "creator": "LaTeX with hyperref package"}}}