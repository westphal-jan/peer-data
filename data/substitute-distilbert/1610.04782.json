{"id": "1610.04782", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings", "abstract": "a new computationally distributed dependence measure, and an adaptive statistical test of independence, are discovered. the dependence measure is the difference via analytic embeddings of the joint distribution and the product across the marginals, evaluated at a finite set of locations ( squared ). these features are chosen so as to maximize reasonable lower bound on the relative power, resulting in a test that is data - efficient, and that runs in linear time ( with respect to the sample size n ). the optimized features can be interpreted as evidence to detect the unique hypothesis, indicating regions in the error domain where relative joint distribution and squared product of the marginals differ most. consistency of the independence test is established, for an appropriate choice of features. in real - grade benchmarks, subjective tests using the discrete features provide analogous to the state - of - the - art quadratic - geometric hsic coefficient, and outperform competing o ( n ) and o ( n \u2228 n ) tests.", "histories": [["v1", "Sat, 15 Oct 2016 20:19:48 GMT  (1678kb,D)", "http://arxiv.org/abs/1610.04782v1", "8 pages of main text"]], "COMMENTS": "8 pages of main text", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wittawat jitkrittum", "zolt\u00e1n szab\u00f3 0001", "arthur gretton"], "accepted": true, "id": "1610.04782"}, "pdf": {"name": "1610.04782.pdf", "metadata": {"source": "CRF", "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings", "authors": ["Wittawat Jitkrittum", "Zolt\u00e1n Szab\u00f3", "Arthur Gretton"], "emails": ["wittawat@gatsby.ucl.ac.uk", "zoltan.szabo@polytechnique.edu", "arthur.gretton@gmail.com"], "sections": [{"heading": null, "text": "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratictime HSIC test, and outperform competing O(n) and O(n log n) tests."}, {"heading": "1 Introduction", "text": "We consider the design of adaptive, nonparametric statistical tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals PxPy. While classical tests of dependence, such as Pearson\u2019s correlation and Kendall\u2019s \u03c4 , are able to detect monotonic relations between univariate variables, more modern tests can address complex interactions, for instance changes in variance of X with the value of Y . Key to many recent tests is to examine covariance or correlation between data features. These interactions become significantly harder to detect, and the features\n\u2217Zolt\u00e1n Szab\u00f3\u2019s ORCID ID: http://orcid.org/ 0000-0001-6183-7603.\nare more difficult to design when the data reside in high dimensions.\nA basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the Hilbert-Schmidt norm of the covariance operator between feature mappings of the random variables [Gretton et al., 2005, 2008]. Each random variable X and Y is mapped to a respective reproducing kernel Hilbert space Hk and Hl. For sufficiently rich mappings, the covariance operator norm is zero if and only if the variables are independent. A second basic nonlinear dependence measure is the smoothed difference between the characteristic function of the joint distribution, and that of the product of marginals. When a particular smoothing function is used, the statistic corresponds to the covariance between distances of X and Y variable pairs [Feuerverger, 1993, Sz\u00e9kely et al., 2007, Sz\u00e9kely and Rizzo, 2009], yielding a simple test statistic. It has been shown by Sejdinovic et al. [2013] that the distance covariance (and its generalization to semi-metrics) is an instance of HSIC for an appropriate choice of kernels. A disadvantage of these feature covariance statistics, however, is that they require quadratic time to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo and Sz\u00e9kely [2014] achieve an O(n log n) cost). Moreover, the feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of an expensive eigenvalue problem [e.g. Zhang et al., 2011] is required for consistent estimation of the quantiles. Several approaches were proposed by Zhang et al. [2016] to obtain faster tests along the lines of HSIC. These include computing HSIC on finite-dimensional feature mappings chosen as random Fourier features (RFFs) [Rahimi and Recht, 2008], a block-averaged statistic, and a Nystr\u00f6m approximation to the statistic. Key to each of these approaches is a more efficient computation of the statistic and its threshold un-\nar X\niv :1\n61 0.\n04 78\n2v 1\n[ st\nat .M\nL ]\n1 5\nO ct\nder the null distribution: for RFFs, the null distribution is a finite weighted sum of \u03c72 variables; for the blockaveraged statistic, the null distribution is asymptotically normal; for Nystr\u00f6m, either a permutation approach is employed, or the spectrum of the Nystr\u00f6m approximation to the kernel matrix is used in approximating the null distribution. Each of these methods costs significantly less than the O(n2) cost of the full HSIC (the cost is linear in n, but also depends quadratically on the number of features retained). A potential disadvantage of the Nystr\u00f6m and Fourier approaches is that the features are not optimized to maximize test power, but are chosen randomly. The block statistic performs worse than both, due to the large variance of the statistic under the null (which can be mitigated by observing more data).\nIn addition to feature covariances, correlation measures have also been developed in infinite dimensional feature spaces: in particular, Bach and Jordan [2002], Fukumizu et al. [2008] proposed statistics on the correlation operator in a reproducing kernel Hilbert space. While convergence has been established for certain of these statistics, their computational cost is high at O(n3), and test thresholds have relied on permutation. A number of much faster approaches to testing based on feature correlations have been proposed, however. For instance, Dauxois and Nkiet [1998] compute statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order B-splines. The cost of this approach is O(n). This idea was extended by Lopez-Paz et al. [2013], who computed the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they performed a copula transform on the inputs, with a total cost of O(n log n). Finally, space partitioning approaches have also been proposed, based on statistics such as the KL divergence, however these apply only to univariate variables [Heller et al., 2016], or to multivariate variables of low dimension [Gretton and Gy\u00f6rfi, 2010] (that said, these tests have other advantages of theoretical interest, notably distribution-independent test thresholds).\nThe approach we take is most closely related to HSIC on a finite set of features. Our simplest test statistic, the Finite Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each of X and Y . A normalized version of the statistic (NFSIC) yields a distribution-independent asymptotic test threshold. We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al. [2016], our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power. The design of features for independence testing turns out\nto be quite different to the case of two-sample testing, however: the task is to find correlated feature pairs on the respective marginal domains, rather than attempting to find a single, high-dimensional feature representation for the entire (x, y) (as we would need to do if we were comparing distributions Pxy and Qxy, rather than testing a specific property of Pxy). We demonstrate the performance of our tests on several challenging artificial and real-world datasets, including detection of dependence between music and its year of appearance, and between videos and captions. In these experiments, we outperform competing linear and O(n log n) time tests."}, {"heading": "2 Independence Criteria and Statistical Tests", "text": "We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle that dependence can be measured in terms of the covariance between data features. Next, we propose a normalized version of this statistic (NFSIC), with a simpler asymptotic distribution when Pxy = PxPy. We show how to select features for the latter statistic to maximize a lower bound on the power of its corresponding statistical test."}, {"heading": "2.1 The Finite Set Independence Criterion", "text": "We begin by introducing the Hilbert-Schmidt Independence Criterion (HSIC) as proposed in Gretton et al. [2005], since our unnormalized statistic is built along similar lines. Consider two random variables X \u2208 X \u2282 Rdx and Y \u2208 Y \u2282 Rdy . Denote by Pxy the joint distribution between X and Y ; Px and Py are the marginal distributions of X and Y . Let \u2297 denote the tensor product, such that (a\u2297 b) c = a \u3008b, c\u3009. Assume that k : X \u00d7 X \u2192 R and l : Y \u00d7Y \u2192 R are positive definite kernels associated with reproducing kernel Hilbert spaces (RKHS) Hk and Hl, respectively. Let \u2016 \u00b7 \u2016HS be the norm on the space of Hl \u2192 Hk Hilbert-Schmidt operators. Then, HSIC between X and Y is defined as\nHSIC(X,Y ) = \u2225\u2225\u00b5xy \u2212 \u00b5x \u2297 \u00b5y\u2225\u22252HS\n= E(x,y),(x\u2032,y\u2032) [k(x,x\u2032)l(y,y\u2032)] + ExEx\u2032 [k(x,x\u2032)]EyEy\u2032 [l(y,y\u2032)] \u2212 2E(x,y) [Ex\u2032 [k(x,x\u2032)]Ey\u2032 [l(y,y\u2032)]] , (1)\nwhere Ex := Ex\u223cPx , Ey := Ey\u223cPy , E(x,y) := E(x,y)\u223cPxy , and x\u2032 is an independent copy of x. The mean embedding of Pxy belongs to the space of HilbertSchmidt operators from Hl to Hk, \u00b5xy := \u222b X\u00d7Y k(x, \u00b7)\u2297 l(y, \u00b7) dPxy(x,y) \u2208 HS(Hl,Hk), and the marginal mean\nembeddings are \u00b5x := \u222b X k(x, \u00b7) dPx(x) \u2208 Hk and\n\u00b5y := \u222b Y l(y, \u00b7) dPy(y) \u2208 Hl [Smola et al., 2007]. Gretton et al. [2005, Theorem 4] show that if the kernels k and l are universal [Steinwart and Christmann, 2008] on compact domains X and Y, then HSIC(X,Y ) = 0 if and only if X and Y are independent. Alternatively, Gretton [2015] shows that it is sufficient for each of k and l to be characteristic to their respective domains (meaning that distribution embeddings are injective in each marginal domain: see Sriperumbudur et al. [2010]). Given a joint sample Zn = {(xi,yi)}ni=1 \u223c Pxy, an empirical estimator of HSIC can be computed in O(n2) time by replacing the population expectations in (1) with their corresponding empirical expectations based on Zn.\nWe now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC). Let X \u2282 Rdx and Y \u2282 Rdy be open sets. Define the empirical measure \u03bd := 1J \u2211J i=1 \u03b4(vi,wi) over J test locations VJ := {(vi,wi)}Ji=1 \u2282 X \u00d7 Y where \u03b4t denotes the Dirac measure centered on t, and (vi,wi) are realizations from an absolutely continuous distribution (wrt the Lebesgue measure). Write Exy for E(x,y)\u223cPxy . The idea is to see \u00b5xy(v,w) = Exy[k(x,v)l(y,w)], \u00b5x(v) = Ex[k(x,v)] and \u00b5y(w) = Ey[l(y,w)] as smooth functions, and consider an L2(X \u00d7 Y, \u03bd) distance between \u00b5xy and \u00b5x\u00b5y instead of a Hilbert-Schmidt distance as in HSIC [Gretton et al., 2005]. Let \u00b5x\u00b5y(x,y) := \u00b5x(x)\u00b5y(y). FSIC is defined as\nFSIC2(X,Y ) := \u2016\u00b5xy \u2212 \u00b5x\u00b5y\u20162L2(X\u00d7Y,\u03bd)\n= \u222b X \u222b Y (\u00b5xy(x,y)\u2212 \u00b5x(x)\u00b5y(y))2 d\u03bd(x,y)\n:= 1\nJ J\u2211 i=1 u(vi,wi) 2 = 1 J \u2016u\u201622, where\nu(v,w) := \u00b5xy(v,w)\u2212 \u00b5x(v)\u00b5y(w) = Exy[k(x,v)l(y,w)]\u2212 Ex[k(x,v)]Ey[l(y,w)], (2) = covxy[k(x,v), l(y,w)],\nand u := (u(v1,w1), . . . , u(vJ ,wJ))>. Our first result in Proposition 2 states that FSIC(X,Y ) almost surely defines a dependence measure for the random variables X and Y , provided that the product kernel on the joint space X \u00d7 Y is characteristic and analytic (see Definition 1).\nDefinition 1 (Analytic kernels [Chwialkowski et al., 2015]). Let X be an open set in Rd. A positive definite kernel k : X \u00d7 X \u2192 R is said to be analytic on its domain X \u00d7 X if for all v \u2208 X , f(x) := k(x,v) is an analytic function on X .\nAssumption A. The kernels k : X \u00d7 X \u2192 R and l : Y \u00d7 Y \u2192 R are bounded by Bk and Bl respectively, and\nthe product kernel g((x,y), (x\u2032,y\u2032)) := k(x,x\u2032)l(y,y\u2032) is characteristic [Sriperumbudur et al., 2010, Definition 6], and analytic (Definition 1) on (X \u00d7 Y)\u00d7 (X \u00d7 Y).\nProposition 2 (FSIC is a dependence measure). Assume that 1. Assumption A holds.\n2. The test locations VJ = {(vi,wi)}Ji=1 are drawn from an absolutely continuous distribution.\nThen, almost surely, FSIC(X,Y ) = 1\u221a J \u2016u\u20162 = 0 if and only if X and Y are independent.\nProof. Since g is characteristic, the mean embedding map \u03a0g : P 7\u2192 E(x,y)\u223cP [g((x,y), \u00b7)] is injective [Sriperumbudur et al., 2010, Section 3], where P is a probability distribution on X \u00d7 Y . Since g is analytic, by Lemma 10 (Appendix), \u00b5xy and \u00b5x\u00b5y are analytic functions. Thus, Lemma11 (Appendix, setting \u039b = \u03a0g) guarantees that FSIC(X,Y ) = 0 \u21d0\u21d2 Pxy = PxPy \u21d0\u21d2 X and Y are independent almost surely.\nFSIC uses \u00b5xy as a proxy for Pxy, and \u00b5x\u00b5y as a proxy for PxPy. Proposition 2 suggests that, to detect the dependence betweenX and Y , it is sufficient to evaluate at a finite number of locations (defined by VJ ) the difference of the population joint embedding \u00b5xy and the embedding of the product of the marginal distributions \u00b5x\u00b5y. A brief explanation to justify this property is as follows. If Pxy = PxPy, then \u03c1(v,w) := \u00b5xy(v,w)\u2212\u00b5x\u00b5y(v,w) is zero, and FSIC(X,Y ) = 0 for any VJ . If Pxy 6= PxPy, then \u03c1 will not be a zero function, since the mean embedding map is injective (require the product kernel to be characteristic). Using the same argument as in Chwialkowski et al. [2015], since k and l are analytic, \u03c1 is also analytic, and the set of roots R := {(v,w) | \u03c1(v,w) = 0} has Lebesgue measure zero. Thus, it is sufficient to draw (v,w) from an absolutely continuous distribution, as we are guaranteed that (v,w) /\u2208 R giving FSIC(X,Y ) > 0. For FSIC to be a dependence measure, the product kernel is required to be characteristic and analytic. We next show in Proposition 3 that Gaussian kernels k and l yield such a product kernel.\nProposition 3 (A product of Gaussian kernels is characteristic and analytic). Let k(x,x\u2032) = exp ( \u2212(x\u2212 x\u2032)>A(x\u2212 x\u2032) ) and\nl(y,y\u2032) = exp ( \u2212(y \u2212 y\u2032)>B(y \u2212 y\u2032) ) be Gaussian kernels on Rdx \u00d7 Rdx and Rdy \u00d7 Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x\u2032,y\u2032)) = k(x,x\u2032)l(y,y\u2032) is characteristic and analytic on (Rdx \u00d7 Rdy )\u00d7 (Rdx \u00d7 Rdy ). Proof (sketch). The main idea is to use the fact a Gaussian kernel is analytic, and a product of Gaussian kernels is a Gaussian kernel on the pair of variables. See the full proof in Appendix D.\nPlug-in Estimator We now give an empirical estimator of FSIC. Assume that we observe a joint sample Zn := {(xi,yi)}ni=1\ni.i.d.\u223c Pxy. Unbiased estimators of \u00b5xy(v,w) and \u00b5x\u00b5y(v,w) are \u00b5\u0302xy(v,w) := 1n \u2211n i=1 k(xi,v)l(yi,w)\nand \u00b5\u0302x\u00b5y(v,w) := 1n(n\u22121) \u2211n i=1 \u2211 j 6=i k(xi,v)l(yj ,w), respectively. A straightforward empirical estimator of FSIC2 is then given by\nF\u0302SIC2(Zn) = 1\nJ J\u2211 i=1 u\u0302(vi,wi) 2,\nu\u0302(v,w) := \u00b5\u0302xy(v,w)\u2212 \u00b5\u0302x\u00b5y(v,w) (3)\n= 2 n(n\u2212 1) \u2211 i<j h(v,w)((xi,yi), (xj ,yj)), (4)\nwhere h(v,w)((x,y), (x\u2032,y\u2032)) := 12 (k(x,v) \u2212 k(x\u2032,v))(l(y,w) \u2212 l(y\u2032,w)). For conciseness, we define u\u0302 := (u\u03021, . . . , u\u0302J)> \u2208 RJ where u\u0302i := u\u0302(vi,wi) so that F\u0302SIC2(Zn) = 1J u\u0302\n>u\u0302. F\u0302SIC2 can be efficiently computed in O((dx + dy)Jn) time [see (3)], assuming that the runtime complexity of evaluating k(x,v) is O(dx) and that of l(y,w) is O(dy). The unbiasedness of \u00b5\u0302x\u00b5y is necessary for (4) to be a Ustatistic. This fact and the rewriting of F\u0302SIC2 in terms of h(v,w)((x,y), (x\n\u2032,y\u2032)) will be exploited when the asymptotic distribution of u\u0302 is derived (Proposition 4). Since FSIC satisfies FSIC(X,Y ) = 0 \u21d0\u21d2 X \u22a5 Y , in principle its empirical estimator can be used as a test statistic for an independence test proposing a null hypothesis H0 : \u201cX and Y are independent\u201d against an alternative H1 : \u201cX and Y are dependent\u201d. The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however and depends on the unknown Pxy. This prompts us to consider a normalized version of FSIC whose asymptotic null distribution of a convenient form. We first derive the asymptotic distribution of u\u0302 in Proposition 4, which we use to derive the normalized test statistic in Theorem 5. As a shorthand, we write z := (x,y), and t := (v,w).\nProposition 4 (Asymptotic distribution of u\u0302). Define k\u0303(x,v) := k(x,v)\u2212Ex\u2032k(x\u2032,v), and l\u0303(y,w) := l(y,w)\u2212 Ey\u2032 l(y\u2032,w). Then, under both H0 and H1, for any fixed locations t and t\u2032,\ncovz[u\u0302(t), u\u0302(t \u2032)] n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 covz[k\u0303(x,v)l\u0303(y,w), k\u0303(x,v\u2032)l\u0303(y,w\u2032)] = Exy[ ( k\u0303(x,v)l\u0303(y,w)\u2212 u(t) )( k\u0303(x,v\u2032)l\u0303(y,w\u2032)\u2212 u(t\u2032) ) ],\nwhere u(v,w) is given in (2), and u\u0302(v,w) is defined in (4). Second, if 0 < covz[u\u0302(ti), u\u0302(ti)] < \u221e for i = 1, . . . , J , then \u221a n(u\u0302 \u2212 u) d\u2192 N (0,\u03a3) as n \u2192 \u221e, where \u03a3ij = cov[u\u0302(ti), u\u0302(tj)] and u := (u(t1), . . . , u(tJ))>.\nProof. We first note that for a fixed t = (v,w), u\u0302(v,w) is a one-sample second-order U-statistic [Serfling, 2009, Section 5.1.3] with a U-statistic kernel ht where ht(a,b) =\nht(b,a). Thus, by Kowalski and Tu [2008, Section 5.1, Theorem 1], it follows directly that cov[u\u0302(t), u\u0302(t\u2032)] = 4covz[Eaht(z,a),Ebht\u2032(z,b)]. Substituting ht with its definition yields the first claim, where we note that Exy[k\u0303(x,v)l\u0303(y,w)] = u(v,w). For the second claim, since u\u0302 is a multivariate onesample U-statistic, by Lehmann [1999, Theorem 6.1.6] and Kowalski and Tu [2008, Section 5.1, Theorem 1], it follows that \u221a n(u\u0302 \u2212 u) d\u2192 N (0,\u03a3) as n \u2192 \u221e, where \u03a3ij = cov[u\u0302(ti), u\u0302(tj)].\nRecall from Proposition 2 that u = 0 holds almost surely under H0. The asymptotic normality in the second claim of Proposition 4 implies that nF\u0302SIC2 = nJ u\u0302\n>u\u0302 converges in distribution to a sum of J dependent weighted \u03c72 random variables. The dependence comes from the fact that the coordinates u\u03021 . . . , u\u0302J of u\u0302 all depend on the sample Zn. This null distribution is not analytically tractable, and requires a large number of simulations to compute the rejection threshold T\u03b1 for a given significance value \u03b1."}, {"heading": "2.2 Normalized FSIC and Adaptive Test", "text": "For the purpose of an independence test, we will consider a normalized variant of F\u0302SIC2, which we call N\u0302FSIC2, whose tractable asymptotic null distribution is \u03c72(J), the chi-squared distribution with J degrees of freedom. We then show that the independence test defined by N\u0302FSIC2 is consistent. These results are given in Theorem 5.\nTheorem 5 (Independence test using N\u0302FSIC2 is consistent). Let \u03a3\u0302 be a consistent estimate of \u03a3 based on the joint sample Zn. The N\u0302FSIC2 statistic is defined as\n\u03bb\u0302n := nu\u0302 > ( \u03a3\u0302 + \u03b3nI )\u22121 u\u0302 where \u03b3n \u2265 0 is a regulariza-\ntion parameter. Assume that\n1. Assumption A holds."}, {"heading": "2. \u03a3 is invertible almost surely with respect to VJ =", "text": "{(vi,wi)}Ji=1 drawn from an absolutely continuous distribution.\n3. limn\u2192\u221e \u03b3n = 0."}, {"heading": "Then, for any k, l and VJ satisfying the assumptions,", "text": "1. Under H0, \u03bb\u0302n d\u2192 \u03c72(J) as n\u2192\u221e. 2. Under H1, for any r \u2208 R, limn\u2192\u221e P ( \u03bb\u0302n \u2265 r ) = 1\nalmost surely. That is, the independence test based on N\u0302FSIC2 is consistent.\nProof (sketch) . Under H0, nu\u0302>(\u03a3\u0302 + \u03b3nI)\u22121u\u0302 asymptotically follows \u03c72(J) because \u221a nu\u0302 is asymptotically normally distributed (see Proposition 4). Claim 2 builds on the result in Proposition 2 stating that u 6= 0 under H1;\nit follows using the convergence of u\u0302 to u. The full proof can be found in Appendix E.\nTheorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for any fixed threshold. Asymptotically the test threshold T\u03b1 is given by the (1\u2212\u03b1)-quantile of \u03c72(J) and is independent of n. The assumption on the consistency of \u03a3\u0302 is required to obtain the asymptotic chi-squared distribution. The regularization parameter \u03b3n is to ensure that (\u03a3\u0302 + \u03b3nI)\u22121 can be stably computed. In practice, \u03b3n requires no tuning, and can be set to be a very small constant. The next proposition states that the computational\ncomplexity of the N\u0302FSIC2 estimator is linear in both the input dimension and sample size, and that it can be expressed in terms of the K =[Kij ] = [k(vi,xj)] \u2208 RJ\u00d7n,L = [Lij ] = [l(wi,yj)] \u2208 RJ\u00d7n matrices.\nProposition 6 (An empirical estimator of N\u0302FSIC2). Let 1n := (1, . . . , 1)\n> \u2208 Rn. Denote by \u25e6 the element-wise matrix product. Then,\n1. u\u0302 = (K\u25e6L)1nn\u22121 \u2212 (K1n)\u25e6(L1n) n(n\u22121) .\n2. A consistent estimator for \u03a3 is \u03a3\u0302 = \u0393\u0393 >\nn where\n\u0393 := (K\u2212 n\u22121K1n1>n ) \u25e6 (L\u2212 n\u22121L1n1>n )\u2212 u\u0302b1>n , u\u0302b = n\u22121 (K \u25e6 L) 1n \u2212 n\u22122 (K1n) \u25e6 (L1n) .\nAssume that the complexity of the kernel evaluation is linear in the input dimension. Then the test statistic\n\u03bb\u0302n = nu\u0302 > ( \u03a3\u0302 + \u03b3nI )\u22121 u\u0302 can be computed in O(J3 +\nJ2n+ (dx + dy)Jn) time.\nProof (sketch). Claim 1 for u\u0302 is straightforward. The expression for \u03a3\u0302 in claim 2 follows directly from the asymptotic covariance expression in Proposition 4. The consistency of \u03a3\u0302 can be obtained by noting that the finite sample bound for P(\u2016\u03a3\u0302 \u2212 \u03a3\u2016F > t) decreases as n increases. This is implicitly shown in Appendix F.2.2 and its following sections.\nAlthough the dependency of the estimator on J is cubic, we empirically observe that only a small value of J is required (see Section 3). The number of test locations J relates to the number of regions in X \u00d7 Y of pxy and pxpy that differ (see Figure 1). In particular, J need not increase with n for test consistency. Our final theoretical result gives a lower bound on\nthe test power of N\u0302FSIC2 i.e., the probability of correctly rejectingH0. We will use this lower bound as the objective function to determine VJ and the kernel parameters. Let \u2016 \u00b7 \u2016F be the Frobenius norm.\nTheorem 7 (A lower bound on the test power). Let NFSIC2(X,Y ) := \u03bbn := nu\n>\u03a3\u22121u. Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that"}, {"heading": "1. There exist finite Bk and Bl such that", "text": "supk\u2208K supx,x\u2032\u2208X |k(x,x\u2032)| \u2264 Bk and supl\u2208L supy,y\u2032\u2208Y |l(y,y\u2032)| \u2264 Bl.\n2. c\u0303 := supk\u2208K supl\u2208L supVJ\u2208V \u2016\u03a3 \u22121\u2016F <\u221e.\nThen, for any k \u2208 K, l \u2208 L, VJ \u2208 V, and \u03bbn \u2265 r, the test power satisfies P ( \u03bb\u0302n \u2265 r ) \u2265 L(\u03bbn) where\nL(\u03bbn) = 1\u2212 62e\u2212\u03be1\u03b3 2 n(\u03bbn\u2212r) 2/n \u2212 2e\u2212b0.5nc(\u03bbn\u2212r) 2/[\u03be2n2]\n\u2212 2e\u2212[(\u03bbn\u2212r)\u03b3n(n\u22121)/3\u2212\u03be3n\u2212c3\u03b3 2 nn(n\u22121)] 2 /[\u03be4n2(n\u22121)],\nb\u00b7c is the floor function, \u03be1 := 132c21J2B\u2217 , \u03be2 := 72c 2 2JB 2, B := BkBl, \u03be3 := 8c1B2J , c3 := 4B2Jc\u03032, \u03be4 := 28B4J2c21, c1 := 4B2J \u221a Jc\u0303, c2 := 4B \u221a Jc\u0303, and B\u2217 is a constant depending on only Bk and Bl. Moreover, for sufficiently large fixed n, L(\u03bbn) is increasing in \u03bbn.\nWe provide the proof in Appendix F. To put Theorem 7 into perspective, let \u03b8x and \u03b8y be the parameters of the kernels k and l, respectively. We denote by \u03b8 = {\u03b8x, \u03b8y, VJ} the collection of all tuning parameters of the test. Assume that K = { (x,v) 7\u2192 exp ( \u2212\u2016x\u2212v\u2016 2\n2\u03c32x\n) | \u03c32x \u2208 [\u03c32x,l, \u03c32x,u] } =:\nKg for some 0 < \u03c32x,l < \u03c32x,u < \u221e and L ={ (y,w) 7\u2192 exp ( \u2212\u2016y\u2212w\u2016 2\n2\u03c32y\n) | \u03c32y \u2208 [\u03c32y,l, \u03c32y,u] } =: Lg for\nsome 0 < \u03c32y,l < \u03c3 2 y,u < \u221e are Gaussian kernel classes. Then, in Theorem 7, B = Bk = Bl = 1, and B\u2217 = 2. The assumption c\u0303 <\u221e is a technical condition to guarantee that the test power lower bound is finite for all \u03b8 defined by the feasible sets K,L, and V. Let V ,r :={ VJ | \u2016vi\u20162, \u2016wi\u20162 \u2264 r and \u2016vi \u2212 vj\u201622 + \u2016wi \u2212wj\u201622 \u2265 , for all i 6= j } . If we set K = Kg,L = Lg, and V = V ,r for some , r > 0, then c\u0303 < \u221e as Kg,Lg, and V ,r are compact. In practice, these conditions do not necessarily create restrictions as they almost always hold implicitly. We show in Appendix C that the objective function used to choose VJ will discourage any two locations to be in the same neighborhood. Parameter Tuning The test power lower bound L(\u03bbn) in Theorem 7 is a function of \u03bbn = nu>\u03a3\u22121u which is the population counterpart of the test statistic \u03bb\u0302n. As in FSIC, it can be shown that \u03bbn = 0 if and only if X are Y are independent (from Proposition 2). If X and Y are dependent, then \u03bbn > 0. According to Theorem 7, for a sufficiently large n, the test power lower bound is increasing in \u03bbn. One can therefore think of \u03bbn (a function of \u03b8) as representing how easily the test rejects H0 given a problem Pxy. The higher the \u03bbn, the\ngreater the lower bound on the test power, and thus the more likely it is that the test will reject H0 when it is false.\nIn light of this reasoning, we propose setting \u03b8 to \u03b8\u2217 = arg max\u03b8 \u03bbn. That this procedure is also valid under H0 can be seen as follows. Under H0, \u03b8\u2217 = arg max\u03b8 0 will be arbitrary. Since Theorem 7 guarantees that \u03bb\u0302n\nd\u2192 \u03c72(J) as n\u2192\u221e for any \u03b8, the asymptotic null distribution does not change by using \u03b8\u2217. In practice, \u03bbn is a population quantity which is unknown. We propose dividing the sample Zn into two disjoint sets: training and test sets. The training set is used to optimize for \u03b8\u2217, and the test set is used for the actual independence test with the optimized \u03b8\u2217. The splitting is to guarantee the independence of \u03b8\u2217 and the test sample, which is an assumption of Theorem 5.\nTo better under N\u0302FSIC2, we visualize \u00b5\u0302xy(v,w), \u00b5\u0302x\u00b5y(v,w) and \u03a3\u0302(v,w) as a function of one test location (v,w) on a simple toy problem. In this problem, Y = \u2212X +Z where Z \u223c N (0, 0.32). As we consider only one location (J = 1), \u03a3\u0302(v,w) is a scalar. The\nstatistic can be written as \u03bb\u0302n = n (\u00b5\u0302xy(v,w)\u2212\u00b5\u0302x\u00b5y(v,w))\n2\n\u03a3\u0302(v,w) .\nThese components are shown in Figure 1, where we use Gaussian kernels for both X and Y , and the horizontal and vertical axes correspond to v \u2208 R and w \u2208 R, respectively.\nIntuitively, u\u0302(v,w) = \u00b5\u0302xy(v,w)\u2212 \u00b5\u0302x\u00b5y(v,w) captures the difference of the joint distribution and the product of the marginals as a function of (v,w). Squaring u\u0302(v,w) and dividing it by the variance shown in Figure 1c gives the statistic (also the parameter tuning objective) shown in Figure 1d. The latter figure suggests that the parameter tuning objective function can be non-convex. However, we note that the non-convexity arises since there are multiple ways to detect the difference between the joint distribution and the product of the marginals. In this case, the lower left and upper right regions equally indicate the largest difference."}, {"heading": "3 Experiments", "text": "In this section, we empirically study the performance of the proposed method on both toy (Section 3.1) and real-life problems (Section 3.2). Our interest is in the performance of linear-time tests on challenging problems which require a large sample size to be able to accurately reveal the dependence. All the code is available at https: //github.com/wittawatj/fsic-test.\nWe compare the proposed NFSIC with optimization (NFSIC-opt) to five multivariate nonparametric tests. The N\u0302FSIC2 test without optimization (NFSIC-med) acts as a baseline, allowing the effect of parameter optimization to be clearly seen. For pedagogical reason, we consider the original HSIC test of Gretton et al. [2005] denoted by QHSIC, which is a quadratic-time test. Nystr\u00f6m HSIC (NyHSIC) uses a Nystr\u00f6m approximation to the kernel matrices of X and Y when computing the HSIC statistic. FHSIC is another variant of HSIC in which a random Fourier feature approximation [Rahimi and Recht, 2008] to the kernel is used. NyHSIC and FHSIC are studied in Zhang et al. [2016] and can be computed in O(n), with quadratic dependency on the number of inducing points in NyHSIC, and quadratic dependency in the number of random features in FHSIC. Finally, the Randomized Dependence Coefficient (RDC) proposed in Lopez-Paz et al. [2013] is also considered. The RDC can be seen as the primal form (with random Fourier features) of the kernel canonical correlation analysis of Bach and Jordan [2002] on copula-transformed data. We consider RDC as a linear-time test even though preprocessing by an empirical copula transform costs O((dx + dy)n log n). We use Gaussian kernel classes Kg and Lg for both X and Y in all the methods. Except NFSIC-opt, all other tests use full sample to conduct the independence test, where the Gaussian widths \u03c3x and \u03c3y are set according to the widely used median heuristic i.e., \u03c3x = median ({\u2016xi \u2212 xj\u20162 | 1 \u2264 i < j \u2264 n}), and \u03c3y is set in the same way using {yi}ni=1. The J locations for NFSIC-med are randomly drawn from the standard multivariate normal distribution in each trial. For a sample of size n, NFSIC-opt uses half the sample for parameter tuning, and the other disjoint half for the test. We permute the sample 300 times in RDC1 and HSIC to simulate from the null distribution and compute the test threshold. The null distributions for FHSIC and NyHSIC are given by a finite sum of weighted \u03c72(1) random variables given in Eq. 8 of Zhang et al. [2016]. Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1\u2212 \u03b1)-quantile of \u03c72(J). To provide a fair comparison, we set J = 10, use 10 inducing points in NyHSIC, and 10\n1We use a permutation test for RDC, following the authors\u2019 implementation (https://github.com/lopezpaz/randomized_ dependence_coefficient, referred commit: b0ac6c0).\nrandom Fourier features in FHSIC and RDC. Optimization of NFSIC-opt The parameters of NFSIC-opt are \u03c3x, \u03c3y, and J locations of size (dx + dy)J . We treat all the parameters as a long vector in R2+(dx+dy)J and use gradient ascent to optimize \u03bb\u0302n/2. We observe that initializing VJ by randomly picking J points from the training sample yields good performance. The regularization parameter \u03b3n in NFSIC is fixed to a small value, and is not optimized. It is worth emphasizing that the complexity of the optimization procedure is still linear in n. Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent only if both the number of features increases with n. By constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed. We refer the reader to Appendix C for a brief investigation of the test power vs. increasing J . The test power does not necessarily monotonically increase with J ."}, {"heading": "3.1 Toy Problems", "text": "We consider three toy problems: Same Gaussian (SG), Sinusoid (Sin), and Gaussian Sign (GSign). 1. Same Gaussian (SG). The two variables are independently drawn from the standard multivariate normal distribution i.e., X \u223c N (0, Idx) and Y \u223c N (0, Idy ) where Id is the d\u00d7 d identity matrix. This problem represents a case in which H0 holds. 2. Sinusoid (Sin). Let pxy be the probability density of Pxy. In the Sinusoid problem, the dependency of X and Y is characterized by (X,Y ) \u223c pxy(x, y) \u221d 1 + sin(\u03c9x) sin(\u03c9y), where the domains of X ,Y = (\u2212\u03c0, \u03c0) and \u03c9 is the frequency of the sinusoid. As the frequency \u03c9 increases, the drawn sample becomes more similar to a sample drawn from Uniform((\u2212\u03c0, \u03c0)2). That is, the higher \u03c9, the harder to detect the dependency between X and Y . This problem was studied in Sejdinovic et al. [2013]. Plots of the density for a few values of \u03c9 are shown in Figures 6 and 7 in the appendix. The main characteristic of interest in this problem is the local change in the density function. 3. Gaussian Sign (GSign). In this problem, Y =\n|Z|\n\u220fdx\ni=1 sgn(Xi), where X \u223c N (0, Idx), sgn(\u00b7) is the sign function, and Z \u223c N (0, 1) serves as a source of noise. The full interaction of X = (X1, . . . , Xdx) is what makes the problem challenging. That is, Y is dependent on X, yet it is independent of any proper subset of {X1, . . . , Xd}. Thus, simultaneous consideration of all the coordinates of X is required to successfully detect the dependency.\nWe fix n = 4000 and vary the problem parameters. Each problem is repeated for 300 trials, and the sample is redrawn each time. The significance level \u03b1 is set to 0.05. The results are shown in Figure 2. It can be seen that in the SG problem (Figure 2b) where H0 holds, all the tests achieve roughly correct type-I errors at \u03b1 = 0.05. In the Sin problem, NFSIC-opt achieves the highest test power for all considered \u03c9 = 1, . . . , 6, highlighting its strength in detecting local changes in the joint density. The performance of NFSIC-med is significantly lower than that of NFSIC-opt. This phenomenon clearly emphasizes the importance of the optimization to place the locations at the relevant regions in X \u00d7 Y . RDC has a remarkably high performance in both Sin and GSign (Figure 2c, 2d) despite no parameter tuning. Interestingly, both NFSICopt and RDC outperform the quadratic-time QHSIC in these two problems. The ability to simultaneously consider interacting features of NFSIC-opt is indicated by its superior test power in GSign, especially at the challenging settings of dx = 5, 6. An average trial runtime for each test in the SG problem is shown in Figure 2a. We observe that the runtime does not increase with dimension, as the complexity of all the tests is linear in the dimension of the input. All the tests are implemented in Python using a common SciPy Stack.\nTo investigate the sample efficiency of all the tests, we fix dx = dy = 250 in SG, \u03c9 = 4 in Sin, dx = 4 in GSign, and increase n. Figure 3 shows the results. The quadratic dependency on n in QHSIC makes it infeasible both in terms of memory and runtime to consider n larger than 6000 (Figure 3a). In constrast, although not the most time-efficient, NFSIC-opt has the highest sampleefficiency for GSign, and for Sin in the low-sample regime, significantly outperforming QHSIC. Despite the small additional overhead from the optimization, we are yet able to conduct an accurate test with n = 105, dx = dy =\n250 in less than 100 seconds. We observe in Figure 3b that the two NFSIC variants have correct type-I errors across all sample sizes, indicating that the asymptotic null distribution approximately holds by the time n reaches 1000. We recall from Theorem 5 that the NFSIC test with random test locations will asymptotically reject H0 if it is false. A demonstration of this property is given in Figure 3c, where the test power of NFSIC-med eventually reaches 1 with n higher than 105."}, {"heading": "3.2 Real Problems", "text": "We now examine the performance of our proposed test on real problems. Million Song Data (MSD) We consider a subset of the Million Song Data2 [Bertin-Mahieux et al., 2011], in which each song (X) out of 515,345 is represented by 90 features, of which 12 features are timbre average (over all segments) of the song, and 78 features are timbre covariance. Most of the songs are western commercial tracks from 1922 to 2011. The goal is to detect the dependency between each song and its year of release (Y ). We set \u03b1 = 0.01, and repeat for 300 trials where the full sample is randomly subsampled to n points in each trial. Other settings are the same as in the toy problems. To make sure that the type-I error is correct, we use the permutation approach in the NFSIC tests to compute the threshold. Figure 4b shows the test powers as n increases from 500 to 2000. To simulate the case where H0 holds in the problem, we permute the sample to break the dependency of X and Y . The results are shown in Figure 5 in the appendix.\nEvidently, NFSIC-opt has the highest test power among all the linear-time tests for all the sample sizes. Its test power is second to only QHSIC. We recall that NFSIC-opt uses half of the sample for parameter tuning. Thus, at n = 500, the actual sample for testing is 250, which is relatively small. The fact that there is a vast power gain from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that the optimization procedure can perform well even at a lower sample sizes.\n2Million Song Data subset: https://archive.ics.uci.edu/ml/ datasets/YearPredictionMSD.\nVideos and Captions Our last problem is based on the VideoStory46K3 dataset [Habibian et al., 2014]. The dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding text captions (Y ) uploaded by the users. Each video is represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors of Wang and Schmid [2013]. Each caption is represented as a bag of words with each feature being the frequency of one word. After filtering only words which occur in at least six video captions, we obtain dy = 1878 words. We examine the test powers as n increases from 2000 to 8000. The results are given in Figure 4. The problem is sufficiently challenging that all linear-time tests achieve a low power at n = 2000. QHSIC performs exceptionally well on this problem, achieving a maximum power throughout. NFSIC-opt has the highest sample efficiency among the linear-time tests, showing that the optimization procedure is also practical in a high dimensional setting."}, {"heading": "Acknowledgement", "text": "We thank the Gatsby Charitable Foundation for the financial support. The major part of this work was carried out while Zolt\u00e1n Szab\u00f3 was a research associate at the Gatsby Computational Neuroscience Unit, University College London.\n3VideoStory46K dataset: https://ivi.fnwi.uva.nl/isis/ mediamill/datasets/videostory.php."}, {"heading": "R. J. Serfling. Approximation Theorems of Mathematical", "text": "Statistics. John Wiley & Sons, 2009.\nA. Smola, A. Gretton, L. Song, and B. Sch\u00f6lkopf. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory (ALT), pages 13\u201331, 2007.\nB. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch\u00f6lkopf, and G. R. G. Lanckriet. Hilbert Space Embeddings and Metrics on Probability Measures. Journal of Machine Learning Research, 11:1517\u20131561, 2010.\nI. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media, 2008.\nG. J. Sz\u00e9kely and M. L. Rizzo. Brownian distance covariance. The Annals of Applied Statistics, 3(4):1236\u20131265, 2009.\nG. J. Sz\u00e9kely, M. L. Rizzo, and N. K. Bakirov. Measuring and testing dependence by correlation of distances. The Annals of Statistics, 35(6):2769\u20132794, 2007.\nA. W. v. d. Vaart. Asymptotic Statistics. Cambridge University Press, 2000.\nH. Wang and C. Schmid. Action recognition with improved trajectories. In IEEE International Conference on Computer Vision (ICCV), pages 3551\u20133558, 2013.\nK. Zhang, J. Peters, D. Janzing, B., and B. Sch\u00f6lkopf. Kernel-based conditional independence test and application in causal discovery. In Conference on Uncertainty in Artificial Intelligence (UAI), pages 804\u2013813, 2011.\nQ. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. LargeScale Kernel Methods for Independence Testing. 2016. URL http://arxiv.org/abs/1606.07892.\nAn Adaptive Test of Independence with Analytic Kernel Embeddings Supplementary Material"}, {"heading": "A Type-I Errors", "text": "In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true) in real problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0 holds. The results are shown in Figure 5."}, {"heading": "B Redundant Test Locations", "text": "Here, we provide a simple illustration to show that two locations t1 = (v1,w1) and t2 = (v2,w2) which are too close to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section 3.1 with \u03c9 = 1, and use J = 2 test locations. In Figure 6, t1 is fixed at the red star, while t2 is varied along the horizontal line. The objective value \u03bb\u0302n as a function of (t1, t2) is shown in the bottom figure. It can be seen that \u03bb\u0302n decreases sharply when t2 is in the neighborhood of t1. This property implies that two locations which are too close will not maximize the objective function (i.e., the second feature contains no additional information when it matches the first). For J > 2, the objective sharply decreases if any two locations are in the same neighborhood."}, {"heading": "C Test Power vs. J", "text": "It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we empirically show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1 with \u03c9 = 2 (also see the left figure of Figure 7). By construction, X and Y are dependent in this problem. We run\nNFSIC test with a sample size of n = 800, varying J from 1 to 600. For each value of J , the test is repeated for 500 times. In each trial, the sample is redrawn and the J test locations are drawn from Uniform((\u2212\u03c0, \u03c0)2). There is no optimization of the test locations. We use Gaussian kernels for both X and Y , and use the median heuristic to set the Gaussian widths to 1.8. Figure 7 shows the test power as J increases.\nWe observe that the test power does not monotonically increase as J increases. When J = 1, the difference of pxy and pxpy cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.8 at J = 10, and stays at the maximum until about J = 100. Then, the power starts to drop sharply when J is higher than 400 in this problem.\nUnlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles used to approximate an expectation. There is a tradeoff: if the test locations are in key regions (i.e., regions in which there is a big difference between pxy and pxpy), then they increase power; yet the statistic gains in variance (thus reducing test power) as J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can reveal the difference of pxy and pxpy. Using an unnecessarily high J not only makes the covariance matrix \u03a3\u0302 harder to estimate accurately, it also increases the computation as the complexity on J is O(J3).\nWe note that NFSIC is not intended to be used with a large J . In practice, it should be set to be large enough so as to capture the key regions as stated. As a practical guide, with optimization of the test locations, a good starting point is J = 5 or 10."}, {"heading": "D Proof of Proposition 3", "text": "Recall Proposition 3,\nProposition (A product of Gaussian kernels is characteristic and analytic). Let k(x,x\u2032) = exp ( \u2212(x\u2212 x\u2032)>A(x\u2212 x\u2032) ) and l(y,y\u2032) = exp ( \u2212(y \u2212 y\u2032)>B(y \u2212 y\u2032) ) be Gaussian kernels on Rdx \u00d7 Rdx and Rdy \u00d7 Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x\u2032,y\u2032)) = k(x,x\u2032)l(y,y\u2032) is characteristic and analytic on (Rdx \u00d7 Rdy )\u00d7 (Rdx \u00d7 Rdy ).\nProof. Let z := (x>,y>)> and z\u2032 := (x\u2032>,y\u2032>)> be vectors in Rdx+dy . We prove by reducing the product kernel to one Gaussian kernel with g(z, z\u2032) = exp ( \u2212(z\u2212 z\u2032)>C(z\u2212 z\u2032) ) where C := ( A 0 0 B ) . Write g(z, z\u2032) = \u03a8(z\u2212 z\u2032)\nwhere \u03a8(t) := exp ( \u2212t>Ct ) . Since C is positive definite, we see that the finite measure \u03b6 corresponding to \u03a8 as defined in Lemma 12 has support everywhere in Rdx+dy . Thus, Sriperumbudur et al. [2010, Theorem 9] implies that g is characteristic. To see that g is analytic, we observe that for each z\u2032 \u2208 Rdx+dy , z 7\u2192 \u2212(z \u2212 z\u2032)>C(z \u2212 z\u2032) is a multivariate polynomial in z, which is known to be analytic. Using the fact that t 7\u2192 exp(t) is analytic on R, and that a composition of analytic functions is analytic, we see that z 7\u2192 exp ( \u2212(z\u2212 z\u2032)>C(z\u2212 z\u2032) ) is analytic on Rdx+dy for each z\u2032."}, {"heading": "E Proof of Theorem 5", "text": "Recall Theorem 5,\nTheorem 5 (Independence test using N\u0302FSIC2 is consistent). Let \u03a3\u0302 be a consistent estimate of \u03a3 based on the joint sample Zn. The N\u0302FSIC2 statistic is defined as \u03bb\u0302n := nu\u0302> ( \u03a3\u0302 + \u03b3nI )\u22121 u\u0302 where \u03b3n \u2265 0 is a regularization\nparameter. Assume that\n1. Assumption A holds.\n2. \u03a3 is invertible almost surely with respect to VJ = {(vi,wi)}Ji=1 drawn from an absolutely continuous distribution.\n3. limn\u2192\u221e \u03b3n = 0.\nThen, for any k, l and VJ satisfying the assumptions,\n1. Under H0, \u03bb\u0302n d\u2192 \u03c72(J) as n\u2192\u221e. 2. Under H1, for any r \u2208 R, limn\u2192\u221e P ( \u03bb\u0302n \u2265 r ) = 1 almost surely. That is, the independence test based on\nN\u0302FSIC2 is consistent. Proof. Assume thatH0 holds. The consistency of \u03a3\u0302 and the continuous mapping theorem imply that ( \u03a3\u0302 + \u03b3nI )\u22121 p\u2192 \u03a3\u22121 which is a constant. Let a be a random vector in RJ following N (0,\u03a3). By Vaart [2000, Theorem 2.7 (v)], it\nfollows that [ \u221a nu\u0302, ( \u03a3\u0302 + \u03b3nI )\u22121] d\u2192 [a,\u03a3\u22121] where u = 0 almost surely by Proposition 2, and \u221anu\u0302 d\u2192 N (0,\u03a3) by Proposition 4. Since f(x,S) := x>Sx is continuous, f ( \u221a nu\u0302, ( \u03a3\u0302 + \u03b3nI\n)\u22121) d\u2192 f(a,\u03a3\u22121). Equivalently, nu\u0302> ( \u03a3\u0302 + \u03b3nI )\u22121 u\u0302\nd\u2192 a>\u03a3\u22121a \u223c \u03c72(J) by Anderson [2003, Theorem 3.3.3]. This proves the first claim. The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al. [2015]. Assume that H1 holds. Then, u 6= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that |ht(z, z\u2032)| \u2264 2BkBl for any z, z\u2032 (see (8)), and we have that u\u0302\na.s.\u2192 u by Serfling [2009, Section 5.4, Theorem A]. Thus, u\u0302> ( \u03a3\u0302 + \u03b3nI )\u22121 u\u0302\u2212 rn d\u2192 u>\u03a3\u22121u by the continuous mapping theorem, and the consistency of \u03a3\u0302. Consequently,\nlim n\u2192\u221e\nP ( \u03bb\u0302n \u2265 r ) = 1\u2212 lim n\u2192\u221e P ( u\u0302> ( \u03a3\u0302 + \u03b3nI )\u22121 u\u0302\u2212 r n < 0\n) (a) = 1\u2212 P ( u>\u03a3\u22121u < 0 ) (b) = 1,\nwhere at (a) we use the Portmanteau theorem [Vaart, 2000, Lemma 2.2 (i)] guaranteeing that xn d\u2192 x if and only if P(xn < t)\u2192 P(x < t) for all continuity points of t 7\u2192 P(x < t). Step (b) is justified by noting that the covariance matrix \u03a3 is positive definite so that u>\u03a3\u22121u > 0, and t 7\u2192 P(u>\u03a3\u22121u < t) (a step function) is continuous at 0."}, {"heading": "F Proof of Theorem 7", "text": "Recall Theorem 7,\nTheorem 7 (A lower bound on the test power). Let NFSIC2(X,Y ) := \u03bbn := nu>\u03a3\u22121u. Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that 1. There exist finite Bk and Bl such that supk\u2208K supx,x\u2032\u2208X |k(x,x\u2032)| \u2264 Bk and supl\u2208L supy,y\u2032\u2208Y |l(y,y\u2032)| \u2264 Bl.\n2. c\u0303 := supk\u2208K supl\u2208L supVJ\u2208V \u2016\u03a3 \u22121\u2016F <\u221e. Then, for any k \u2208 K, l \u2208 L, VJ \u2208 V, and \u03bbn \u2265 r, the test power satisfies P ( \u03bb\u0302n \u2265 r ) \u2265 L(\u03bbn) where\nL(\u03bbn) = 1\u2212 62e\u2212\u03be1\u03b3 2 n(\u03bbn\u2212r) 2/n \u2212 2e\u2212b0.5nc(\u03bbn\u2212r) 2/[\u03be2n2]\n\u2212 2e\u2212[(\u03bbn\u2212r)\u03b3n(n\u22121)/3\u2212\u03be3n\u2212c3\u03b3 2 nn(n\u22121)] 2 /[\u03be4n2(n\u22121)],\nb\u00b7c is the floor function, \u03be1 := 132c21J2B\u2217 , \u03be2 := 72c 2 2JB 2, B := BkBl, \u03be3 := 8c1B2J , c3 := 4B2Jc\u03032, \u03be4 := 28B4J2c21,\nc1 := 4B 2J \u221a Jc\u0303, c2 := 4B \u221a Jc\u0303, and B\u2217 is a constant depending on only Bk and Bl. Moreover, for sufficiently large fixed n, L(\u03bbn) is increasing in \u03bbn.\nOverview of the proof We first derive a probabilistic bound for |\u03bb\u0302n \u2212 \u03bbn|/n. The bound is in turn upper bounded by an expression involving \u2016u\u0302\u2212 u\u20162 and \u2016\u03a3\u0302\u2212\u03a3\u2016F . The difference \u2016u\u0302\u2212 u\u20162 can be bounded by applying the bound for U-statistics given in Serfling [2009, Theorem A, p. 201]. For \u2016\u03a3\u0302\u2212\u03a3\u2016F , we decompose it into a sum of smaller components, and bound each term with a product variant of the Hoeffding\u2019s inequality (Lemma 9). L(\u03bbn) is obtained by combining all the bounds with the union bound."}, {"heading": "F.1 Notations", "text": "Let \u3008A,B\u3009F := tr(A>B) denote the Frobenius inner product, and \u2016A\u2016F := \u221a\ntr(A>A) be the Frobenius norm. Write z := (x,y) to denote a pair of points from X \u00d7 Y. We write t := (v,w) to denote a pair of test locations from X \u00d7 Y. For brevity, an expectation over (x,y) (i.e., E(x,y)\u223cPxy) will be written as Ez or Exy. Define k\u0303(x,v) := k(x,v)\u2212 Ex\u2032k(x\u2032,v), and l\u0303(y,w) := l(y,w)\u2212 Ey\u2032 l(y\u2032,w). Let B2(r) := {x | \u2016x\u20162 \u2264 r} be a closed ball with radius r centered at the origin. Similarly, define BF (r) := {A | \u2016A\u2016F \u2264 r} to be a closed ball with radius r of J \u00d7 J matrices under the Frobenius norm. Denote the max operation by (x1, . . . , xm)+ = max(x1, . . . , xm). For a product of marginal mean embeddings \u00b5x(v)\u00b5y(w), we write \u00b5\u0302x\u00b5y(v,w) := 1\nn(n\u22121) \u2211n i=1 \u2211 j 6=i k(xi,v)l(yj ,w) to denote the unbiased plug-in estimator, and write \u00b5\u0302x(v)\u00b5\u0302y(w) := 1 n \u2211n i=1 k(xi,v) 1 n \u2211n j=1 l(yj ,w) which is a biased estimator. Define u\u0302\nb(v,w) := \u00b5\u0302xy(v,w) \u2212 \u00b5\u0302x(v)\u00b5\u0302y(w) so that u\u0302b := ( u\u0302b(t1), . . . , u\u0302 b(tJ) )> where the superscript b stands for \u201cbiased\u201d. To avoid confusing with a positive definite kernel, we will refer to a U-statistic kernel as a core."}, {"heading": "F.2 Proof", "text": "We will first derive a bound for P(|\u03bb\u0302n \u2212 \u03bbn| \u2265 t), which will then be reparametrized to get a bound for the target quantity P(\u03bb\u0302n \u2265 r). We closely follow the proof in Jitkrittum et al. [2016, Section C.1] up to (12), then we diverge. We start by considering |\u03bb\u0302n \u2212 \u03bbn|/n.\n|\u03bb\u0302n \u2212 \u03bbn|/n = \u2223\u2223\u2223u\u0302>(\u03a3\u0302 + \u03b3nI)\u22121u\u0302\u2212 u>\u03a3\u22121u\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223u\u0302> (\u03a3\u0302 + \u03b3nI)\u22121 u\u0302\u2212 u> (\u03a3 + \u03b3nI)\u22121 u + u> (\u03a3 + \u03b3nI)\u22121 u\u2212 u>\u03a3\u22121u\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223u\u0302> (\u03a3\u0302 + \u03b3nI)\u22121 u\u0302\u2212 u> (\u03a3 + \u03b3nI)\u22121 u\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223u> (\u03a3 + \u03b3nI)\u22121 u\u2212 u>\u03a3\u22121u\u2223\u2223\u2223\n:= (F)1 + (F)2 .\nWe next bound (F1) and (F2) separately.\n(F)1 = \u2223\u2223\u2223\u2223u\u0302> (\u03a3\u0302 + \u03b3nI)\u22121 u\u0302\u2212 u> (\u03a3 + \u03b3nI)\u22121 u\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223u\u0302> (\u03a3\u0302 + \u03b3nI)\u22121 u\u0302\u2212 u\u0302> (\u03a3 + \u03b3nI)\u22121 u\u0302 + u\u0302> (\u03a3 + \u03b3nI)\u22121 u\u0302\u2212 u> (\u03a3 + \u03b3nI)\u22121 u\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223u\u0302> (\u03a3\u0302 + \u03b3nI)\u22121 u\u0302\u2212 u\u0302> (\u03a3 + \u03b3nI)\u22121 u\u0302\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223u\u0302> (\u03a3 + \u03b3nI)\u22121 u\u0302\u2212 u> (\u03a3 + \u03b3nI)\u22121 u\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2329u\u0302u\u0302>,(\u03a3\u0302 + \u03b3nI)\u22121 \u2212 (\u03a3 + \u03b3nI)\u22121\u232a F \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2329u\u0302u\u0302> \u2212 uu>, (\u03a3 + \u03b3nI)\u22121\u232a F \u2223\u2223\u2223 \u2264 \u2016u\u0302u\u0302>\u2016F \u2016(\u03a3\u0302 + \u03b3nI)\u22121 \u2212 (\u03a3 + \u03b3nI)\u22121\u2016F + \u2016u\u0302u\u0302> \u2212 uu>\u2016F \u2016(\u03a3 + \u03b3nI)\u22121\u2016F = \u2016u\u0302u\u0302>\u2016F \u2016(\u03a3\u0302 + \u03b3nI)\u22121[(\u03a3 + \u03b3nI)\u2212 (\u03a3\u0302 + \u03b3nI)](\u03a3 + \u03b3nI)\u22121\u2016F + \u2016u\u0302u\u0302> \u2212 u\u0302u> + u\u0302u> \u2212 uu>\u2016F \u2016(\u03a3 + \u03b3nI)\u22121\u2016F\n(a) \u2264 \u2016u\u0302u\u0302>\u2016F \u2016(\u03a3\u0302 + \u03b3nI)\u22121\u2016F \u2016\u03a3\u2212 \u03a3\u0302\u2016F \u2016\u03a3\u22121\u2016F + \u2016u\u0302u\u0302> \u2212 u\u0302u> + u\u0302u> \u2212 uu>\u2016F \u2016\u03a3\u22121\u2016F (b)\n\u2264 \u221a J\n\u03b3n \u2016u\u0302\u201622\u2016\u03a3\u2212 \u03a3\u0302\u2016F \u2016\u03a3\u22121\u2016F +\n( \u2016u\u0302(u\u0302\u2212 u)>\u2016F + \u2016(u\u0302\u2212 u)u>\u2016F ) \u2016\u03a3\u22121\u2016F\n\u2264 \u221a J\n\u03b3n \u2016u\u0302\u201622\u2016\u03a3\u2212 \u03a3\u0302\u2016F \u2016\u03a3\u22121\u2016F + (\u2016u\u0302\u20162 + \u2016u\u20162) \u2016u\u0302\u2212 u\u20162\u2016\u03a3\u22121\u2016F , (5)\nwhere at (a) we used \u2016(\u03a3 + \u03b3nI)\u22121\u2016F \u2264 \u2016\u03a3\u22121\u2016F , at (b) we used \u2016(\u03a3\u0302 + \u03b3nI)\u22121\u2016F \u2264 \u221a J\u2016(\u03a3\u0302 + \u03b3nI)\u22121\u20162 \u2264 \u221a J/\u03b3n.\nFor (F)2, we have\n(F)2 = \u2223\u2223\u2223u> (\u03a3 + \u03b3nI)\u22121 u\u2212 u>\u03a3\u22121u\u2223\u2223\u2223\n= \u2223\u2223\u2329uu>, (\u03a3 + \u03b3nI)\u22121 \u2212\u03a3\u22121\u232aF \u2223\u2223 \u2264 \u2016uu>\u2016F \u2016(\u03a3 + \u03b3nI)\u22121 \u2212\u03a3\u22121\u2016F = \u2016u\u201622\u2016(\u03a3 + \u03b3nI)\u22121 [\u03a3\u2212 (\u03a3 + \u03b3nI)] \u03a3\u22121\u2016F \u2264 \u03b3n\u2016u\u201622\u2016(\u03a3 + \u03b3nI)\u22121\u2016F \u2016\u03a3\u22121\u2016F (a) \u2264 \u03b3n\u2016u\u201622\u2016\u03a3\u22121\u20162F , (6)\nwhere at (a) we used \u2016(\u03a3 + \u03b3nI)\u22121\u2016F \u2264 \u2016\u03a3\u22121\u2016F . Combining (5) and (6), we have\u2223\u2223\u2223u\u0302>(\u03a3\u0302 + \u03b3nI)\u22121u\u0302\u2212 u>\u03a3\u22121u\u2223\u2223\u2223\n\u2264 \u221a J\n\u03b3n \u2016u\u0302\u20162\u2016\u03a3\u2212 \u03a3\u0302\u2016F \u2016\u03a3\u22121\u2016F + (\u2016u\u0302\u20162 + \u2016u\u20162) \u2016u\u0302\u2212 u\u20162\u2016\u03a3\u22121\u2016F + \u03b3n\u2016u\u201622\u2016\u03a3\u22121\u20162F . (7)\nBounding \u2016u\u0302\u201622 and \u2016u\u201622 Here, we show that by the boundedness of the kernels k and l, it follows that \u2016u\u0302\u201622 is bounded. Recall that supx,x\u2032\u2208X |k(x,x\u2032)| \u2264 Bk, supy,y\u2032 |l(y,y\u2032)| \u2264 Bl, our notation t = (v,w) for the test locations, and zi := (xi,yi). We first show that the U-statistic core h is bounded.\n|ht((x,y), (x\u2032,y\u2032))| = \u2223\u2223\u2223\u222312(k(x,v)\u2212 k(x\u2032,v))(l(y,w)\u2212 l(y\u2032,w)) \u2223\u2223\u2223\u2223 \u2264 1\n2 (|k(x,v)|+ |k(x\u2032,v)|) (|l(y,w)|+ |l(y\u2032,w)|)\n\u2264 2BkBl := 2B, (8)\nwhere we define B := BkBl. It follows that\n\u2016u\u0302\u201622 = J\u2211\nm=1\n 2 n(n\u2212 1) \u2211 i<j htm(zi, zj) 2 \u2264 J\u2211 m=1 [2BkBl] 2 = 4B2J, (9)\n\u2016u\u201622 = J\u2211\nm=1\n[EzEz\u2032htm(z, z\u2032)] 2 \u2264 4B2J. (10)\nUsing the upper bounds on \u2016u\u0302\u201622, \u2016u\u201622 ,(7) and the definition of c\u0303, we have\u2223\u2223\u2223u\u0302>(\u03a3\u0302 + \u03b3nI)\u22121u\u0302\u2212 u>\u03a3\u22121u\u2223\u2223\u2223 \u2264 \u221a J\n\u03b3n 4B2Jc\u0303\u2016\u03a3\u2212 \u03a3\u0302\u2016F + 4B\n\u221a Jc\u0303\u2016u\u0302\u2212 u\u20162 + 4B2Jc\u03032\u03b3n\n=: c1 \u03b3n \u2016\u03a3\u2212 \u03a3\u0302\u2016F + c2\u2016u\u0302\u2212 u\u20162 + c3\u03b3n, (11)\nwhere we define c1 := 4B2J \u221a Jc\u0303, c2 := 4B \u221a Jc\u0303, and c3 := 4B2Jc\u03032. This upper bound implies that\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1 \u03b3n n\u2016\u03a3\u2212 \u03a3\u0302\u2016F + c2n\u2016u\u0302\u2212 u\u20162 + c3n\u03b3n. (12)\nWe will separately upper bound \u2016\u03a3\u2212 \u03a3\u0302\u2016F and \u2016u\u0302\u2212 u\u20162, and combine them with a union bound.\nF.2.1 Bounding \u2016u\u0302\u2212 u\u20162 Let t\u2217 = arg maxt\u2208{t1,...,tJ} |u\u0302(t)\u2212 u(t)|. Recall that u = (u(t1), . . . , u(tJ))> = (u1, . . . , uJ)>.\n\u2016u\u0302\u2212 u\u20162 = sup b\u2208B2(1) \u3008b, u\u0302\u2212 u\u30092 \u2264 sup b\u2208B2(1) J\u2211 j=1 |bj ||u\u0302(tj)\u2212 u(tj)|\n\u2264 |u\u0302(t\u2217)\u2212 u(t\u2217)| sup b\u2208B2(1) J\u2211 j=1 |bj |\n(a) \u2264 \u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)| sup\nb\u2208B2(1) \u2016b\u20162\n= \u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)|, (13)\nwhere at (a) we used \u2016a\u20161 \u2264 \u221a J\u2016a\u20162 for any a \u2208 RJ . From (13), it can be seen that bounding \u2016u\u0302\u2212u\u20162 amounts to bounding the difference of a U-statistic u\u0302(t\u2217) (see (4)) to its expectation u(t\u2217). Combining (13) and (12), we have\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1 \u03b3n n\u2016\u03a3\u2212 \u03a3\u0302\u2016F + c2n\n\u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)|+ c3n\u03b3n. (14)\nF.2.2 Bounding \u2016\u03a3\u0302\u2212\u03a3\u2016F\nThe plan is to write \u03a3\u0302 = S\u0302\u2212 u\u0302bu\u0302b>,\u03a3 = S\u2212 uu>, so that \u2016\u03a3\u0302\u2212\u03a3\u2016F \u2264 \u2016S\u0302\u2212 S\u2016F + \u2016u\u0302bu\u0302b> \u2212 uu>\u2016F and bound separately \u2016S\u0302\u2212 S\u2016F and \u2016u\u0302bu\u0302b> \u2212 uu>\u2016F .\nRecall that \u03a3ij = \u03b7(ti, tj), \u03b7(t, t\u2032) = Exy[ ( k\u0303(x,v)l\u0303(y,w)\u2212 u(v,w) )( k\u0303(x,v\u2032)l\u0303(y,w\u2032)\u2212 u(v\u2032,w\u2032) ) ] where k\u0303(x,v) = k(x,v)\u2212Ex\u2032k(x\u2032,v), and l\u0303(y,w) = l(y,w)\u2212Ey\u2032 l(y\u2032,w). Its empirical estimator (see Proposition 6) is \u03a3\u0302ij = \u03b7\u0302(ti, tj) where\n\u03b7\u0302(t, t\u2032) = 1\nn n\u2211 i=1 [ ( k(xi,v)l(yi,w)\u2212 u\u0302b(v,w) )( k(xi,v \u2032)l(yi,w \u2032)\u2212 u\u0302b(v\u2032,w\u2032) ) ]\n= 1\nn n\u2211 i=1 k(xi,v)l(yi,w)k(xi,v \u2032)l(yi,w \u2032)\u2212 u\u0302b(v,w)u\u0302b(v\u2032,w\u2032),\nk(x,v) := k(x,v) \u2212 1n \u2211n i=1 k(xi,v), and l(y,w) := l(y,w) \u2212 1 n \u2211n i=1 l(yi,w). We\nnote that 1n \u2211n i=1 k(xi,v)l(yi,w) = u\u0302\nb(v,w). We define S\u0302 \u2208 RJ\u00d7J such that S\u0302ij := 1 n \u2211n m=1 k(xm,vi)l(ym,wi)k(xm,vj)l(yi,wj), and define similarly its population counterpart S such that Sij := Exy[k\u0303(x,v)l\u0303(y,w)k\u0303(x,v\u2032)l\u0303(y,w\u2032)]. We have\n\u03a3\u0302 = S\u0302\u2212 u\u0302bu\u0302b>, \u03a3 = S\u2212 uu>,\n\u2016\u03a3\u0302\u2212\u03a3\u2016F = \u2016S\u0302\u2212 S\u2212 (u\u0302bu\u0302b> \u2212 uu>)\u2016F (15)\n\u2264 \u2016S\u0302\u2212 S\u2016F + \u2016u\u0302bu\u0302b> \u2212 uu>\u2016F . (16)\nWith (16), (14) becomes\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1n\n\u03b3n \u2016S\u0302\u2212 S\u2016F +\nc1n\n\u03b3n \u2016u\u0302bu\u0302b> \u2212 uu>\u2016F + c2n\n\u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)|+ c3n\u03b3n. (17)\nWe will further separately bound \u2016S\u0302\u2212 S\u2016F and \u2016u\u0302bu\u0302b> \u2212 uu>\u2016F .\nF.2.3 Bounding \u2016u\u0302bu\u0302b> \u2212 uu>\u2016F\n\u2016u\u0302bu\u0302b> \u2212 uu>\u2016F = \u2016u\u0302bu\u0302b> \u2212 u\u0302bu> + u\u0302bu> \u2212 uu>\u2016F \u2264 \u2016u\u0302b(u\u0302b \u2212 u)>\u2016F + \u2016(u\u0302b \u2212 u)u>\u2016F = \u2016u\u0302b\u20162\u2016u\u0302b \u2212 u\u20162 + \u2016u\u0302b \u2212 u\u20162\u2016u\u20162 \u2264 4B \u221a J\u2016u\u0302b \u2212 u\u20162,\nwhere we used (10) and the fact that \u2016u\u0302b\u20162 \u2264 2B \u221a J which can be shown similarly to (9) as\n\u2016u\u0302b\u201622 = J\u2211\nm=1\n[\u00b5\u0302xy(vm,wm)\u2212 \u00b5\u0302x(vm)\u00b5\u0302y(wm)]2 = J\u2211\nm=1\n 1 n2 n\u2211 i=1 n\u2211 j=1 htm(zi, zj) 2 \u2264 J\u2211 m=1 [2BkBl] 2 = 4B2J.\nLet (v\u0303, w\u0303) := t\u0303 = arg maxt\u2208{t1,...,tJ} |u\u0302b(t)\u2212 u(t)|. We bound \u2016u\u0302b \u2212 u\u20162 by\n\u2016u\u0302b \u2212 u\u20162 (a) \u2264 \u221a J |u\u0302b(t\u0303)\u2212 u(t\u0303)|\n= \u221a J \u2223\u2223\u00b5\u0302xy(t\u0303)\u2212 \u00b5\u0302x(v\u0303)\u00b5\u0302y(w\u0303)\u2212 u(t\u0303)\u2223\u2223\n= \u221a J \u2223\u2223\u00b5\u0302xy(t\u0303)\u2212 \u00b5\u0302x\u00b5y(t\u0303) + \u00b5\u0302x\u00b5y(t\u0303)\u2212 \u00b5\u0302x(v\u0303)\u00b5\u0302y(w\u0303)\u2212 u(t\u0303)\u2223\u2223\n\u2264 \u221a J \u2223\u2223\u00b5\u0302xy(t\u0303)\u2212 \u00b5\u0302x\u00b5y(t\u0303)\u2212 u(t\u0303)\u2223\u2223+\u221aJ \u2223\u2223\u00b5\u0302x\u00b5y(t\u0303)\u2212 \u00b5\u0302x(v\u0303)\u00b5\u0302y(w\u0303)\u2223\u2223\n= \u221a J \u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223+\u221aJ \u2223\u2223\u00b5\u0302x\u00b5y(t\u0303)\u2212 \u00b5\u0302x(v\u0303)\u00b5\u0302y(w\u0303)\u2223\u2223 , (18)\nwhere at (a) we used the same reasoning as in (13). The bias \u2223\u2223\u00b5\u0302x\u00b5y(t\u0303)\u2212 \u00b5\u0302x(v\u0303)\u00b5\u0302y(w\u0303)\u2223\u2223 in the second term can be bounded as\u2223\u2223\u00b5\u0302x\u00b5y(t\u0303)\u2212 \u00b5\u0302x(v\u0303)\u00b5\u0302y(w\u0303)\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2223 1n(n\u2212 1) n\u2211 i=1 \u2211 j 6=i k(xi, v\u0303)l(yj , w\u0303)\u2212 1 n2 n\u2211 i=1 n\u2211 j=1 k(xi, v\u0303)l(yj , w\u0303) \u2223\u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2223 1n(n\u2212 1) n\u2211 i=1 n\u2211 j=1 k(xi, v\u0303)l(yj , w\u0303)\u2212 1 n(n\u2212 1) n\u2211 i=1 k(xi, v\u0303)l(yi, w\u0303)\u2212 1 n2 n\u2211 i=1 n\u2211 j=1 k(xi, v\u0303)l(yj , w\u0303) \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 ( 1\u2212 n n\u2212 1 ) 1 n2 n\u2211 i=1 n\u2211 j=1 k(xi, v\u0303)l(yj , w\u0303) + 1 n(n\u2212 1) n\u2211 i=1 k(xi, v\u0303)l(yi, w\u0303)\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223 ( 1\u2212 n n\u2212 1 ) 1 n2 n\u2211 i=1 n\u2211 j=1 k(xi, v\u0303)l(yj , w\u0303) \u2223\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 1n(n\u2212 1) n\u2211 i=1 k(xi, v\u0303)l(yi, w\u0303)\n\u2223\u2223\u2223\u2223\u2223 \u2264 B n\u2212 1 + B n\u2212 1 = 2B n\u2212 1 .\nCombining this upper bound with (18), we have\n\u2016u\u0302bu\u0302b> \u2212 uu>\u2016F \u2264 4BJ \u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223+ 8B2J\nn\u2212 1 . (19)\nWith (19), (17) becomes\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1n\n\u03b3n \u2016S\u0302\u2212 S\u2016F +\n4BJc1n\n\u03b3n\n\u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223+ c1n \u03b3n 8B2J n\u2212 1 + c2n \u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)|+ c3n\u03b3n. (20)\nF.2.4 Bounding \u2016S\u0302\u2212 S\u2016F Recall that VJ = {t1, . . . , tJ}, S\u0302ij = S\u0302(ti, tj) = 1n \u2211n m=1 k(xm,vi)l(ym,wi)k(xm,vj)l(ym,wj), and Sij = S(ti, tj) = Exy[k\u0303(x,vi)l\u0303(y,wi)k\u0303(x,vj)l\u0303(y,wj)]. Let (t(1), t(2)) = arg max(s,t)\u2208VJ\u00d7VJ |S\u0302(s, t)\u2212 S(s, t)|.\n\u2016S\u0302\u2212 S\u2016F = sup B\u2208BF (1)\n\u2329 B, S\u0302\u2212 S \u232a F\n\u2264 sup B\u2208BF (1) J\u2211 i=1 J\u2211 j=1 |Bij ||S\u0302ij \u2212 Sij |\n\u2264 \u2223\u2223\u2223S\u0302(t(1), t(2))\u2212 S(t(1), t(2))\u2223\u2223\u2223 sup\nB\u2208BF (1) J\u2211 i=1 J\u2211 j=1 |Bij |\n(a) \u2264 J \u2223\u2223\u2223S\u0302(t(1), t(2))\u2212 S(t(1), t(2))\u2223\u2223\u2223 sup\nB\u2208BF (1) \u2016B\u2016F\n= J \u2223\u2223\u2223S\u0302(t(1), t(2))\u2212 S(t(1), t(2))\u2223\u2223\u2223 , (21)\nwhere at (a) we used \u2211J i=1 \u2211J j=1 |Aij | \u2264 J\u2016A\u2016F for any matrix A \u2208 RJ\u00d7J . We arrive at\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1Jn\n\u03b3n \u2223\u2223\u2223S\u0302(t(1), t(2))\u2212 S(t(1), t(2))\u2223\u2223\u2223+ 4BJc1n \u03b3n \u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223 + c1n\n\u03b3n\n8B2J n\u2212 1 + c2n \u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)|+ c3n\u03b3n. (22)"}, {"heading": "F.2.5 Bounding", "text": "\u2223\u2223\u2223S\u0302(t, t\u2032)\u2212 S(t, t\u2032)\u2223\u2223\u2223\nHaving an upper bound for \u2223\u2223\u2223S\u0302(t, t\u2032)\u2212 S(t, t\u2032)\u2223\u2223\u2223 will allow us to bound (22). To keep the notations uncluttered, we\nwill define the following shorthands.\nExpression Shorthand\nk(x,v) a\nk(x,v\u2032) a\u2032\nk(xi,v) ai\nk(xi,v \u2032) a\u2032i\nEx\u223cPxk(x,v) a\u0303 Ex\u223cPxk(x,v\u2032) a\u0303\u2032\n1 n \u2211n i=1 k(xi,v) a 1 n \u2211n i=1 k(xi,v \u2032) a\u2032\nExpression Shorthand\nl(y,w) b\nl(y,w\u2032) b\u2032\nl(yi,w) bi\nl(yi,w \u2032) b\u2032i\nEy\u223cPy l(y,w) b\u0303\nEy\u223cPy l(y,w\u2032) b\u0303\u2032\n1 n \u2211n i=1 l(yi,w) b\n1 n \u2211n i=1 l(yi,w \u2032) b \u2032\nWe will also use \u00b7 to denote a empirical expectation over x, or y, or (x,y). The argument under \u00b7 will determine the variable over which we take the expectation. For instance, aa\u2032 = 1n \u2211n i=1 k(xi,v)k(xi,v\n\u2032) and aba\u2032 = 1n \u2211n i=1 k(xi,v)l(yi,w)k(xi,v\n\u2032), and so on. We define in the same way for the population expectation using \u00b7\u0303 i.e., a\u0303a\u2032 = Ex [k(x,v)k(x,v\u2032)] and a\u0303ba\u2032 = Exy [k(x,v)l(y,w)k(x,v\u2032)].\nWith these shorthands, we can rewrite S\u0302(t, t\u2032) and S(t, t\u2032) as\nS\u0302(t, t\u2032) = 1\nn n\u2211 i=1 (ai \u2212 a)(bi \u2212 b)(a\u2032i \u2212 a\u2032)(b\u2032i \u2212 b \u2032 ),\nS(t, t\u2032) = Exy [ (a\u2212 a\u0303)(b\u2212 b\u0303)(a\u2032 \u2212 a\u0303\u2032)(b\u2032 \u2212 b\u0303\u2032) ] .\nBy expanding S(t, t\u2032), we have\nS(t, t\u2032) = Exy [ + aba\u2032b\u2032 \u2212 aba\u2032b\u0303\u2032 \u2212 aba\u0303\u2032b\u2032 + aba\u0303\u2032b\u0303\u2032\n\u2212 ab\u0303a\u2032b\u2032 + ab\u0303a\u2032b\u0303\u2032 + ab\u0303a\u0303\u2032b\u2032 \u2212 ab\u0303a\u0303\u2032b\u0303\u2032\n\u2212 a\u0303ba\u2032b\u2032 + a\u0303ba\u2032b\u0303\u2032 + a\u0303ba\u0303\u2032b\u2032 \u2212 a\u0303ba\u0303\u2032b\u0303\u2032 + a\u0303b\u0303a\u2032b\u2032 \u2212 a\u0303b\u0303a\u2032b\u0303\u2032 \u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032 + a\u0303b\u0303a\u0303\u2032b\u0303\u2032 ]\n= +a\u0303ba\u2032b\u2032 \u2212 a\u0303ba\u2032b\u0303\u2032 \u2212 a\u0303bb\u2032a\u0303\u2032 + a\u0303ba\u0303\u2032b\u0303\u2032\n\u2212 a\u0303a\u2032b\u2032b\u0303+ a\u0303a\u2032b\u0303b\u0303\u2032 + a\u0303b\u2032a\u0303\u2032b\u0303\u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032\n\u2212 a\u0303\u2032bb\u2032a\u0303+ a\u0303\u2032ba\u0303b\u0303\u2032 + a\u0303a\u0303\u2032b\u0303b\u2032 \u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032\n+ a\u0303\u2032b\u2032a\u0303b\u0303\u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032 \u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032 + a\u0303b\u0303a\u0303\u2032b\u0303\u2032\n= +a\u0303ba\u2032b\u2032 \u2212 a\u0303ba\u2032b\u0303\u2032 \u2212 a\u0303bb\u2032a\u0303\u2032 + a\u0303ba\u0303\u2032b\u0303\u2032\n\u2212 a\u0303a\u2032b\u2032b\u0303+ a\u0303a\u2032b\u0303b\u0303\u2032 + a\u0303b\u2032a\u0303\u2032b\u0303+ a\u0303\u2032b\u2032a\u0303b\u0303\n\u2212 a\u0303\u2032bb\u2032a\u0303+ a\u0303\u2032ba\u0303b\u0303\u2032 + a\u0303a\u0303\u2032b\u0303b\u2032 \u2212 3a\u0303b\u0303a\u0303\u2032b\u0303\u2032.\nThe expansion of S\u0302(t, t\u2032) can be done in the same way. By the triangle inequality, we have\u2223\u2223\u2223S\u0302(t, t\u2032)\u2212 S(t, t\u2032)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u2032b\u2032\u2223\u2223\u2223+ \u2223\u2223\u2223aba\u2032 b\u2032 \u2212 a\u0303ba\u2032b\u0303\u2032\u2223\u2223\u2223+ \u2223\u2223\u2223abb\u2032a\u2032 \u2212 a\u0303bb\u2032a\u0303\u2032\u2223\u2223\u2223+ \u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u0303\u2032b\u0303\u2032\u2223\u2223\u2223\u2223\u2223\u2223aa\u2032b\u2032 b\u2212 a\u0303a\u2032b\u2032b\u0303\u2223\u2223\u2223+ \u2223\u2223\u2223aa\u2032 b b\u2032 \u2212 a\u0303a\u2032b\u0303b\u0303\u2032\u2223\u2223\u2223+ \u2223\u2223\u2223ab\u2032a\u2032b\u2212 a\u0303b\u2032a\u0303\u2032b\u0303\u2223\u2223\u2223+ \u2223\u2223\u2223a\u2032b\u2032ab\u2212 a\u0303\u2032b\u2032a\u0303b\u0303\u2223\u2223\u2223\u2223\u2223\u2223a\u2032bb\u2032a\u2212 a\u0303\u2032bb\u2032a\u0303\u2223\u2223\u2223+ \u2223\u2223\u2223a\u2032bab\u2032 \u2212 a\u0303\u2032ba\u0303b\u0303\u2032\u2223\u2223\u2223+ \u2223\u2223\u2223a a\u2032bb\u2032 \u2212 a\u0303a\u0303\u2032b\u0303b\u2032\u2223\u2223\u2223+ 3 \u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032\u2223\u2223\u2223 . The first term\n\u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u2032b\u2032\u2223\u2223\u2223 can be bounded by applying the Hoeffding\u2019s inequality. Other terms can be bounded by applying Lemma 9. Recall that we write (x1, . . . , xm)+ for max(x1, . . . , xm)."}, {"heading": "Bounding", "text": "\u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u2032b\u2032\u2223\u2223\u2223 (1st term). Since \u2212B2 \u2264 aba\u2032b\u2032 \u2264 B2, by the Hoeffding\u2019s inequality (Lemma 14), we\nhave\nP (\u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u2032b\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 2 exp(\u2212 nt2\n2B4\n) ."}, {"heading": "Bounding", "text": "\u2223\u2223\u2223aba\u2032 b\u2032 \u2212 a\u0303ba\u2032b\u0303\u2032\u2223\u2223\u2223 (2nd term). Let f1(x,y) = aba\u2032 = k(x,v)l(y,w)k(x,v\u2032) and f2(y) = b\u2032 = l(y,w\u2032).\nWe note that |f1(x,y)| \u2264 (BBk, Bl)+ and |f2(y)| \u2264 (BBk, Bl)+. Thus, by Lemma 9 with E = 2, we have\nP (\u2223\u2223\u2223aba\u2032 b\u2032 \u2212 a\u0303ba\u2032b\u0303\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 4 exp(\u2212 nt2\n8(BBk, Bl)4+\n) ."}, {"heading": "Bounding", "text": "\u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u0303\u2032b\u0303\u2032\u2223\u2223\u2223 (4th term). Let f1(x,y) = ab = k(x,v)l(y,w), f2(x) = a\u2032 = k(x,v\u2032) and f3(y) = b\u2032 = l(y,w\u2032). We can see that |f1(x,y)|, |f2(x)|, |f3(y)| \u2264 (B,Bk, Bl)+. Thus, by Lemma 9 with E = 3, we have\nP (\u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303ba\u0303\u2032b\u0303\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 6 exp(\u2212 nt2\n18(B,Bk, Bl)6+\n) ."}, {"heading": "Bounding", "text": "\u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032\u2223\u2223\u2223 (last term). Let f1(x) = a = k(x,v), f2(y) = b = l(y,w), f3(x) = a\u2032 = k(x,v\u2032) and f4(y) = b \u2032 = l(y,w\u2032). It can be seen that |f1(x)|, |f2(y)|, |f3(x)|, |f4(y)| \u2264 (Bk, Bl)+. Thus, by Lemma 9 with E = 4, we have\nP ( 3 \u2223\u2223\u2223aba\u2032b\u2032 \u2212 a\u0303b\u0303a\u0303\u2032b\u0303\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 8 exp(\u2212 nt2\n32 \u00b7 32(Bk, Bl)8+\n) .\nBounds for other terms can be derived in a similar way to yield\n(3rd term) P (\u2223\u2223\u2223abb\u2032a\u2032 \u2212 a\u0303bb\u2032a\u0303\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 4 exp(\u2212 nt2\n8(BBl, Bk)4+\n) ,\n(5th term) P (\u2223\u2223\u2223aa\u2032b\u2032 b\u2212 a\u0303a\u2032b\u2032b\u0303\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 4 exp(\u2212 nt2\n8(BBk, Bl)4+\n) ,\n(6th term) P (\u2223\u2223\u2223aa\u2032 b b\u2032 \u2212 a\u0303a\u2032b\u0303b\u0303\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 6 exp(\u2212 nt2\n18(B2k, Bl) 6 +\n) ,\n(7th term) P (\u2223\u2223\u2223ab\u2032a\u2032b\u2212 a\u0303b\u2032a\u0303\u2032b\u0303\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 6 exp(\u2212 nt2\n18(B,Bk, Bl)6+\n) ,\n(8th term) P (\u2223\u2223\u2223a\u2032b\u2032ab\u2212 a\u0303\u2032b\u2032a\u0303b\u0303\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 6 exp(\u2212 nt2\n18(B,Bk, Bl)6+\n) ,\n(9th term) P (\u2223\u2223\u2223a\u2032bb\u2032a\u2212 a\u0303\u2032bb\u2032a\u0303\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 4 exp(\u2212 nt2\n8(BBl, Bk)4+\n) ,\n(10th term) P (\u2223\u2223\u2223a\u2032bab\u2032 \u2212 a\u0303\u2032ba\u0303b\u0303\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 6 exp(\u2212 nt2\n18(B,Bk, Bl)6+\n) ,\n(11th term) P (\u2223\u2223\u2223a a\u2032bb\u2032 \u2212 a\u0303a\u0303\u2032b\u0303b\u2032\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 6 exp(\u2212 nt2\n18(Bk, B2l ) 6 +\n) .\nBy the union bound, we have\nP (\u2223\u2223\u2223S\u0302(t, t\u2032)\u2212 S(t, t\u2032)\u2223\u2223\u2223 \u2264 12t) \u2265 1\u2212 [ 2 exp ( \u2212 nt 2\n2B4\n) + 4 exp ( \u2212 nt 2\n8(BBk, Bl)4+\n) + 4 exp ( \u2212 nt 2\n8(BBl, Bk)4+\n) + 6 exp ( \u2212 nt 2\n18(B,Bk, Bl)6+ ) 4 exp ( \u2212 nt 2\n8(BBk, Bl)4+\n) + 6 exp ( \u2212 nt 2\n18(B2k, Bl) 6 +\n) + 6 exp ( \u2212 nt 2\n18(B,Bk, Bl)6+\n) + 6 exp ( \u2212 nt 2\n18(B,Bk, Bl)6+ ) 4 exp ( \u2212 nt 2\n8(BBl, Bk)4+\n) + 6 exp ( \u2212 nt 2\n18(B,Bk, Bl)6+\n) + 6 exp ( \u2212 nt 2\n18(Bk, B2l ) 6 +\n) + 8 exp ( \u2212 nt 2\n32 \u00b7 32(Bk, Bl)8+ )] = 1\u2212 [ 2 exp ( \u2212 nt 2\n2B4\n) + 8 exp ( \u2212 nt 2\n8(BBk, Bl)4+\n) + 8 exp ( \u2212 nt 2\n8(BBl, Bk)4+\n) + 24 exp ( \u2212 nt 2\n18(B,Bk, Bl)6+ ) + 6 exp ( \u2212 nt 2\n18(B2k, Bl) 6 +\n) + 6 exp ( \u2212 nt 2\n18(Bk, B2l ) 6 +\n) + 8 exp ( \u2212 nt 2\n32 \u00b7 32(Bk, Bl)8+ )] \u2265 1\u2212 [ 2 exp ( \u221212 2nt2\nB\u2217\n) + 8 exp ( \u221212 2nt2\nB\u2217\n) + 8 exp ( \u221212 2nt2\nB\u2217\n) + 24 exp ( \u221212 2nt2\nB\u2217 ) + 6 exp ( \u221212 2nt2\nB\u2217\n) + 6 exp ( \u221212 2nt2\nB\u2217\n) + 8 exp ( \u221212 2nt2\nB\u2217 )] = 1\u2212 62 exp ( \u221212 2nt2\nB\u2217\n) ,\nwhere\nB\u2217 := 1\n122 max(2B4, 8(BBk, Bl) 4 +, 8(BBl, Bk) 4 +, 18(B,Bk, Bl) 6 +, 18(B 2 k, Bl) 6 +, 18(Bk, B 2 l ) 6 +, 32 \u00b7 32(Bk, Bl)8+).\nBy reparameterization, it follows that P ( c1Jn\n\u03b3n \u2223\u2223\u2223S\u0302(t, t\u2032)\u2212 S(t, t\u2032)\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 62 exp(\u2212 \u03b32nt2 c21J 2nB\u2217 ) . (23)"}, {"heading": "F.2.6 Union Bound for", "text": "\u2223\u2223\u2223\u03bb\u0302n \u2212 \u03bbn\u2223\u2223\u2223 and Final Lower Bound\nRecall from (22) that\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1Jn\n\u03b3n \u2223\u2223\u2223S\u0302(t(1), t(2))\u2212 S(t(1), t(2))\u2223\u2223\u2223+ 4BJc1n \u03b3n \u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223 + c1n\n\u03b3n\n8B2J n\u2212 1 + c2n \u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)|+ c3n\u03b3n.\nWe will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the U-statistic core h is bounded between \u22122B and 2B. Thus, by Lemma 13 (with m = 2), we have\nP ( c2n \u221a J |u\u0302(t\u2217)\u2212 u(t\u2217)| \u2264 t ) \u2265 1\u2212 2 exp ( \u2212 b0.5nct 2\n8c22n 2JB2\n) . (24)\nBounding c1n\u03b3n 8B2J n\u22121 + c3n\u03b3n + 4BJc1n \u03b3n \u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223. By Lemma 13 (with m = 2), it follows that P ( c1n\n\u03b3n\n8B2J n\u2212 1 + c3n\u03b3n + 4BJc1n\n\u03b3n\n\u2223\u2223u\u0302(t\u0303)\u2212 u(t\u0303)\u2223\u2223 \u2264 t)\n\u2265 1\u2212 2 exp \u2212b0.5nc\u03b32n [ t\u2212 c1n\u03b3n 8B2J n\u22121 \u2212 c3n\u03b3n ]2 27B4J2c21n 2  = 1\u2212 2 exp ( \u2212 b0.5nc [ t\u03b3n(n\u2212 1)\u2212 8c1B2nJ \u2212 c3n(n\u2212 1)\u03b32n\n]2 27B4J2c21n 2(n\u2212 1)2 ) (a) \u2265 1\u2212 2 exp ( \u2212 [ t\u03b3n(n\u2212 1)\u2212 8c1B2nJ \u2212 c3n(n\u2212 1)\u03b32n\n]2 28B4J2c21n 2(n\u2212 1) ) , (25)\nwhere at (a) we used b0.5nc \u2265 (n\u2212 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can bound (22) with\nP (\u2223\u2223\u2223\u03bb\u0302n \u2212 \u03bbn\u2223\u2223\u2223 \u2264 T) \u2265 1\u2212 62 exp(\u2212 \u03b32nT 2\n32c21J 2nB\u2217\n) \u2212 2 exp ( \u2212 b0.5ncT 2\n72c22n 2JB2 ) \u2212 2 exp ( \u2212 [ T\u03b3n(n\u2212 1)/3\u2212 8c1B2nJ \u2212 c3\u03b32nn(n\u2212 1)\n]2 28B4J2c21n 2(n\u2212 1) ) .\nSince \u2223\u2223\u2223\u03bb\u0302n \u2212 \u03bbn\u2223\u2223\u2223 \u2264 T implies \u03bb\u0302n \u2265 \u03bbn \u2212 T , a reparametrization with r = \u03bbn \u2212 T gives\nP ( \u03bb\u0302n \u2265 r ) \u2265 1\u2212 62 exp ( \u2212\u03b3 2 n(\u03bbn \u2212 r)2\n32c21J 2nB\u2217\n) \u2212 2 exp ( \u2212b0.5nc(\u03bbn \u2212 r) 2\n72c22n 2JB2 ) \u2212 2 exp ( \u2212 [ (\u03bbn \u2212 r)\u03b3n(n\u2212 1)/3\u2212 8c1B2nJ \u2212 c3\u03b32nn(n\u2212 1)\n]2 28B4J2c21n 2(n\u2212 1) ) := L(\u03bbn).\nGrouping constants into \u03be1, . . . \u03be5 gives the result.\nThe lower bound L(\u03bbn) takes the form\n1\u2212 62 exp ( \u2212C1(\u03bbn \u2212 T\u03b1)2 ) \u2212 2 exp ( \u2212C2(\u03bbn \u2212 T\u03b1)2 ) \u2212 2 exp ( \u2212 [(\u03bbn \u2212 T\u03b1)C3 \u2212 C4] 2\nC5\n) ,\nwhere C1, . . . , C5 are positive constants. For fixed large enough n such that \u03bbn > T\u03b1, and fixed significance level \u03b1, increasing \u03bbn will increase L(\u03bbn). Specifically, since n is fixed, increasing u>\u03a3\u22121u in \u03bbn = nu>\u03a3\u22121u will increase L(\u03bbn)."}, {"heading": "G Helper Lemmas", "text": "This section contains lemmas used to prove the main results in this work.\nLemma 8 (Product to sum). Assume that |ai| \u2264 B, |bi| \u2264 B for i = 1, . . . , E. Then \u2223\u2223\u2223\u220fEi=1 ai \u2212\u220fEi=1 bi\u2223\u2223\u2223 \u2264\nBE\u22121 \u2211E j=1 |aj \u2212 bj |.\nProof. \u2223\u2223\u2223\u2223\u2223\u2223 E\u220f i=1 ai \u2212 E\u220f j=1 bj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223 E\u220f i=1 ai \u2212 E\u22121\u220f i=1 aibE \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 E\u22121\u220f i=1 aibE \u2212 E\u22122\u220f i=1 aibE\u22121bE \u2223\u2223\u2223\u2223\u2223+ . . .+ \u2223\u2223\u2223\u2223\u2223\u2223a1 E\u220f j=2 bj \u2212 E\u220f j=1 bj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 |aE \u2212 bE |\n\u2223\u2223\u2223\u2223\u2223 E\u22121\u220f i=1 ai \u2223\u2223\u2223\u2223\u2223+ |aE\u22121 \u2212 bE\u22121| \u2223\u2223\u2223\u2223\u2223 ( E\u22122\u220f i=1 ai ) bE \u2223\u2223\u2223\u2223\u2223+ . . .+ |a1 \u2212 b1| \u2223\u2223\u2223\u2223\u2223\u2223 E\u220f j=2 bj \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 |aE \u2212 bE |BE\u22121 + |aE\u22121 \u2212 bE\u22121|BE\u22121 + . . .+ |a1 \u2212 b1|BE\u22121 = BE\u22121 E\u2211 j=1 |aj \u2212 bj |\napplying triangle inequality, and the boundedness of ai and bi-s.\nLemma 9 (Product variant of the Hoeffding\u2019s inequality). For i = 1, . . . , E, let {x(i)j } ni j=1 \u2282 Xi be an i.i.d. sample from a distribution Pi, and fi : Xi 7\u2192 R be a measurable function. Note that it is possible that P1 = P2 = \u00b7 \u00b7 \u00b7 = PE and {x(1)j } n1 j=1 = \u00b7 \u00b7 \u00b7 = {x (E) j } nE j=1. Assume that |fi(x)| \u2264 B <\u221e for all x \u2208 Xi and i = 1, . . . , E. Write P\u0302i to denote an empirical distribution based on the sample {x(i)j } ni j=1. Then,\nP (\u2223\u2223\u2223\u2223\u2223 [ E\u220f i=1 Ex(i)\u223cP\u0302ifi(x (i)) ] \u2212 [ E\u220f i=1 Ex(i)\u223cPifi(x (i)) ]\u2223\u2223\u2223\u2223\u2223 \u2264 T ) \u2265 1\u2212 2 E\u2211 i=1 exp ( \u2212 niT 2 2E2B2E ) .\nProof. By Lemma8, we have\u2223\u2223\u2223\u2223\u2223 [ E\u220f i=1 Ex(i)\u223cP\u0302ifi(x (i)) ] \u2212 [ E\u220f i=1 Ex(i)\u223cPifi(x (i)) ]\u2223\u2223\u2223\u2223\u2223 \u2264 BE\u22121 E\u2211 i=1 \u2223\u2223\u2223Ex(i)\u223cP\u0302ifi(x(i))\u2212 Ex(i)\u223cPifi(x(i))\u2223\u2223\u2223 . By applying the Hoeffding\u2019s inequality to each term in the sum, we have P (\u2223\u2223\u2223Ex(i)\u223cP\u0302ifi(x(i))\u2212 Ex(i)\u223cPifi(x(i))\u2223\u2223\u2223 \u2264 t) \u2265 1\u2212 2 exp(\u2212 2nit24B2 ) . The result is obtained with a union bound."}, {"heading": "H External Lemmas", "text": "In this section, we provide known results referred to in this work.\nLemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a bounded, analytic kernel (in the sense given in Definition 1) on Rd \u00d7 Rd, then all functions in the RKHS defined by k are analytic.\nLemma 11 (Chwialkowski et al. [2015, Lemma 3]). Let \u039b be an injective mapping from the space of probability measures into a space of analytic functions on Rd. Define\nd2VJ (P,Q) = J\u2211 j=1 |[\u039bP ](vj)\u2212 [\u039bQ](vj)|2 ,\nwhere VJ = {vi}Ji=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with respect to the Lebesgue measure. Then, dVJ (P,Q) is almost surely (w.r.t. VJ) a metric.\nLemma 12 (Bochner\u2019s theorem [Rudin, 2011]). A continuous function \u03a8 : Rd \u2192 R is positive definite if and only if it is the Fourier transform of a finite nonnegative Borel measure \u03b6 on Rd, that is, \u03a8(x) = \u222b Rd e \u2212ix>\u03c9 d\u03b6(\u03c9), x \u2208 Rd.\nLemma 13 (A bound for U-statistics [Serfling, 2009, Theorem A, p. 201]). Let h(x1, . . . ,xm) be a Ustatistic kernel for an m-order U-statistic such that h(x1, . . . ,xm) \u2208 [a, b] where a \u2264 b < \u221e. Let Un =( n m )\u22121\u2211 i1<\u00b7\u00b7\u00b7<im h(xi1 , . . . ,xim) be a U-statistic computed with a sample of size n, where the summation is over the(\nn m ) combinations of m distinct elements {i1, . . . , im} from {1, . . . , n}. Then, for t > 0 and n \u2265 m,\nP(Un \u2212 Eh(x1, . . . ,xm) \u2265 t) \u2264 exp ( \u22122bn/mct2/(b\u2212 a)2 ) ,\nP(|Un \u2212 Eh(x1, . . . ,xm)| \u2265 t) \u2264 2 exp ( \u22122bn/mct2/(b\u2212 a)2 ) ,\nwhere bxc denotes the greatest integer which is smaller than or equal to x. Hoeffind\u2019s inequality is a special case when m = 1.\nLemma 14 (Hoeffding\u2019s inequality). Let X1, . . . , Xn be i.i.d. random variables such that a \u2264 Xi \u2264 b almost surely. Define X := 1n \u2211n i=1Xi. Then,\nP (\u2223\u2223X \u2212 E[X]\u2223\u2223 \u2264 \u03b1) \u2265 1\u2212 2 exp(\u2212 2n\u03b12\n(b\u2212 a)2\n) ."}], "references": [{"title": "Fast Two-Sample Testing with Analytic Representations of Probability Measures", "author": ["K.P. Chwialkowski", "A. Ramdas", "D. Sejdinovic", "A. Gretton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chwialkowski et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 1981}, {"title": "Interpretable Distribution Features with Maximum Testing Power. 2016", "author": ["W. Jitkrittum", "Z. Szab\u00f3", "K. Chwialkowski", "A. Gretton"], "venue": "URL http://arxiv.org/abs/1605.06796", "citeRegEx": "Jitkrittum et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2016}, {"title": "Fourier analysis on groups", "author": ["W. Rudin"], "venue": null, "citeRegEx": "Rudin.,? \\Q2011\\E", "shortCiteRegEx": "Rudin.", "year": 2011}, {"title": "Approximation Theorems of Mathematical Statistics", "author": ["R.J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q2009\\E", "shortCiteRegEx": "Serfling.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al.", "startOffset": 131, "endOffset": 158}, {"referenceID": 0, "context": "We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al. [2016], our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power.", "startOffset": 131, "endOffset": 227}, {"referenceID": 0, "context": "Using the same argument as in Chwialkowski et al. [2015], since k and l are analytic, \u03c1 is also analytic, and the set of roots R := {(v,w) | \u03c1(v,w) = 0} has Lebesgue measure zero.", "startOffset": 30, "endOffset": 57}, {"referenceID": 0, "context": "The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al. [2015]. Assume that H1 holds.", "startOffset": 92, "endOffset": 119}, {"referenceID": 2, "context": "Lemma 12 (Bochner\u2019s theorem [Rudin, 2011]).", "startOffset": 28, "endOffset": 41}], "year": 2016, "abstractText": "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratictime HSIC test, and outperform competing O(n) and O(n log n) tests.", "creator": "LaTeX with hyperref package"}}}