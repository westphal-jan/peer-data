{"id": "1610.04841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2016", "title": "Translation Quality Estimation using Recurrent Neural Network", "abstract": "this paper describes our submission to another shared task on word / phrase level quality estimation ( tx ) in the first conference on statistical machine translation ( wmt16 ). the objective of the shared task was to predict if the given word / phrase is a correct / incorrect ( ok / bad ) message in making correct sentence. in this paper, parents propose a novel approach for word level quality estimation using recurrent tagged network language model ( rnn - lm ) architecture. rnn - lms have been found very effective in modeling natural language processing ( nlp ) applications. rnn - lm is mainly used for vector space language modeling experiencing different nlp problems. for this functionality, consumers control the architecture of ex - lm. the modified system predicts a label ( ok / bad ) in the slot rather towards predicting the word. the input, the system constitutes a word sequence, similar to the standard tx - lm. the approach encourages language independent and requires only the translated text for qe. to estimate the phrase level quality, we control audio output without the word level qe system.", "histories": [["v1", "Sun, 16 Oct 2016 10:54:23 GMT  (23kb)", "https://arxiv.org/abs/1610.04841v1", "7 pages, published at First Conference on Machine Translation"], ["v2", "Fri, 21 Oct 2016 07:01:05 GMT  (23kb,D)", "http://arxiv.org/abs/1610.04841v2", "7 pages, published at First Conference on Machine Translation"]], "COMMENTS": "7 pages, published at First Conference on Machine Translation", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raj nath patel", "sasikumar m"], "accepted": false, "id": "1610.04841"}, "pdf": {"name": "1610.04841.pdf", "metadata": {"source": "CRF", "title": "Translation Quality Estimation using Recurrent Neural Network", "authors": ["Raj Nath Patel"], "emails": ["rajnathp@cdac.in", "sasi@cdac.in"], "sections": [{"heading": "1 Introduction", "text": "Quality estimation is the process to predict the quality of translation without any reference translation (Blatz et al., 2004; Specia et al., 2009). Whereas, Machine Translation (MT) system evaluation does require references (human translation). QE could be done at word, phrase, sentence or document level. This paper describes the submission to the shared task on word and phrase level QE (Task 2) for English-German (en-de) MT.\nThe shared task has the trace of last five years\u2019 research in the field of QE (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015).\nIn recent years, RNN-LM has demonstrated exceptional performance in a variety of NLP applications (Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b; Socher et al., 2013a; Socher et al., 2013b). The RNN-LM represents each word as high-dimensional real-valued vectors, like the other continuous space language models such as feed forward neural network language models (Schwenk and Gauvain, 2002; Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007) and Hierarchical Log-Bi-linear language models (Mnih and Hinton, 2009).\nIn this paper, we have used a modified version of RNN-LM, which accepts the word sequence (context window) as input and predicts label at the output for the middle word. For example, let us consider the following input/output sample:\nEnglish (MT input): Layer effects are retained by default .\nGerman (MT output): \u201d Effekte sind standardmig beibehalten .\nGerman (Post-edited): Ebeneneffekte werden standardmig beibehalten .\nTags: BAD BAD BAD OK OK OK Now if we have to predict the output tag (BAD) for the word \u201csind\u201d in the MT output, our input sequence to the RNN-LM will be \u201cEffekte sind standardmig\u201d (if context window size is 3). Whereas, for standard RNN-LM model, \u201cEffekte standardmig\u201d would be the input to the network with \u201csind\u201d as the output. We add padding at the start and end of the sentence according to the context window. The detailed description of the model and its implementation is given in section 3.\nWe have used the data provided by the or-\nar X\niv :1\n61 0.\n04 84\n1v 2\n[ cs\n.C L\n] 2\n1 O\nct 2\n01 6\nganizers for the shared task on quality estimation (2016) which includes: (i) source sentence (ii) translated output (word/phrase level) (iii) word/phrase level tagging (OK/BAD) (iv) post edited translation (v) 22 baseline features (vi) word alignment. The goal of the task is to predict whether the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence.\nThe remainder of the paper is organised as follows. Section 2 describes the related work. Section 3 presents RNN models we use, and its implementation. In section 4, we discuss the data distribution, our approaches, and results. Discussion of our methodology and different models is covered in section 5 followed by concluding remarks in section 6."}, {"heading": "2 Related Work", "text": "For word level QE, supervised classification techniques are being used widely. Most of these approaches require manually designed features (Bojar et al., 2014), similar to the feature set provided by the organizers.\nLogacheva et al. (2015) modeled the word level QE using the CRF++ tool with data selection and data bootstrapping in which data selection filters out the sentences having the smallest proportion of erroneous tokens and are assumed to be less useful for the task. The bootstrapping technique creates additional data instances and boosts the importance of BAD labels occurring in the training data. Shang et al. (2015) tried to solve the problem of label imbalance with creating sub-labels like OK B (begin), OK I (intermediate), OK E (end). Shah et al. (2015) have used word embedding as an additional feature (+25 features) with SVM classifier. Bilingual Deep Neural Network (DNN) based model for word level QE was proposed by Kreutzer et al. (2015), in which word embedding was pre-trained and fine-tuned with other parameters of the network using stochastic gradient descent. de Souza et al. (2014) have used Bidirectional LSTM as a classifier for word level QE.\nThe architecture of RNN-LM has been used for Natural Language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) earlier. Our approach is quite similar to the Kreutzer et al. (2015), but we are using RNN instead of DNN. We have also tried to address the problem of label-imbalance, introducing sub-labels as suggested by Shang et\nal. (2015)."}, {"heading": "3 RNN Models for QE", "text": "For this task, we exploited RNN\u2019s extensions, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). LSTM and GRU have shown to perform better at modeling the long-range dependencies in the data than the simple RNN. Simple RNN also suffers from the problem of exploding and vanishing gradient (Bengio et al., 1994). LSTM and GRU tackle this problem by introducing a gating mechanism. LSTM includes input, output and forget gates with a memory cell, whereas GRU has reset and update gates only (no memory cell). The detailed description of each model is given in the following subsections."}, {"heading": "3.1 LSTM", "text": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015). We implemented the version of LSTM described by the following set of equations:\nit = sigm(Wxixt +Whiht\u22121 + bi)\not = sigm(Wxoxt +Whoht\u22121 + bo)\nft = sigm(Wxfxt +Whfht\u22121 + bf )\njt = tanh(Wxjxt +Whjht\u22121 + bj)\nct = ct\u22121 ft + it jt ht = tanh(ct) ot\nwhere sigm is the logistic sigmoid function and tanh is the hyperbolic tangent function to add non linearity in the network. is the elementwise multiplication of vectors. i, o, f are input, output, forget gates respectively, j is the new memory content whereas c is the updated memory content. In these equations, W\u2217 are the weight matrices and b\u2217 are the bias vectors."}, {"heading": "3.2 Deep LSTM", "text": "In this paper, we have used deep LSTM with two layers. Deep LSTM is created by stacking multiple LSTMs on the top of each other. We feed the output of the lower LSTM as the input to the upper LSTM. For example, if ht is the output of the lower LSTM, we apply a matrix transform to form the input xt for the upper LSTM. The matrix transformation allows having two consecutive LSTM layers of different sizes."}, {"heading": "3.3 GRU", "text": "GRU is an architecture, which is quite similar to the LSTM. Chung et al. (2014) found that GRU outperforms LSTM on a suit of tasks. GRU is defined by the following set of equations:\nrt = sigm(Wxrxt +Whrht\u22121 + br)\nzt = sigm(Wxzxt +Whzht\u22121 + bz)\nh\u0303t = tanh(Wxhxt +Whh(rt ht\u22121) + bh)\nht = zt ht\u22121 + (1\u2212 zt) h\u0303t\nIn the above equations, W\u2217 are the weight matrices and b\u2217 are the bias vectors. r and z are known as the reset and update gate respectively. GRU does not use any separate memory cell as used in LSTM. However, gated mechanism controls the flow of information in the unit."}, {"heading": "3.4 Implementation Details", "text": "We implemented all the models (LSTM, deep LSTM and GRU) with 1THEANO framework (Bergstra et al., 2010; Bastien et al., 2012) as described above. For all the models in the paper, the size of a hidden layer is 100, the word embedding dimensionality is 100 and the context word window size is 5.\nWe initialized all the square weight matrices as random orthogonal matrices. All the bias vectors were initialized to zero. Other weight matrices were sampled from a Gaussian distribution with mean 0 and variance 0.012.\nTo update the model parameters, we have used Truncated Back-Propagation-Through-Time (T-BPTT) (Werbos, 1990) (Werbos, 1990) with stochastic gradient descent. We fixed the depth of BPTT to 7 for all the models. We used Ada-delta (Zeiler, 2012) (Zeiler, 2012) to adapt the learning rate of each parameter automatically ( = 10\u22126 and \u03c1 = 0.95). We trained each model for 50 epochs."}, {"heading": "4 Experiments and Results", "text": "In this section, we describe the experiments carried out for the shared task and present the experimental results."}, {"heading": "4.1 Data distribution", "text": "We have used the corpus shared by the organizers for our experiments. The split for train-\n1http://deeplearning.net/software/ theano/#download\ning/development/testing is detailed in Table 1. Test1 split was used for evaluating the different experiments that we have carried out for the shared task. Evaluation scores displayed in the results section are against Test1 only. Organizers provided another set of test data (Test2), against which all the submitted systems were evaluated."}, {"heading": "4.2 Methodology", "text": "In the following subsections, we discuss our approaches for word/phrase level quality estimation."}, {"heading": "4.2.1 Word Level QE", "text": "Our experiments are mainly focused on the word level QE. We have used the output of the word level QE system for the estimation of the phrase level quality.\nAs mentioned above, we have used the modified RNN-LM architecture for the experiments. Baseline (LSTM) system was developed by training word embedding from scratch with other parameters of the model. In another set of experiments, we have pre-trained the word embedding with word2vec (Mikolov et al., 2013b), and further tuned with the training of the model parameters. For pretraining, we have used an additional corpus (2M sentences approx.) from EnglishGerman Europarl data (Koehn, 2005).\nFor bilingual models, we restructured the source sentence (English) according to the target (German) using word alignment provided by the organizers. For many-to-one mapping in the alignment (English-German), we chose the first alignment only. The \u2018NULL\u2019 token was assigned to the words where were not aligned with any word on the target side. The input of the model is constructed by concatenating context words of source and target. For example, consider the source word sequence s1s2s3, and the target word sequence t1t2t3, then the input to the network will be s1s2s3t1t2t3.\nIn the training data, the distribution of the labels (OK/BAD) is skewed (OK to BAD ratio is approx. 4:1). To handle the issue, we tried one of the strategies proposed by Shang et al. (2015),\nin which we replace \u2018OK\u2019 label with sub-labels to balance the distribution. The sub-labels are OK B, OK I, OK E, depending on the location of the token in the sentence."}, {"heading": "4.2.2 Phrase Level QE", "text": "For phrase level QE, we have not trained any explicit system. As it was mentioned by the organizers that a phrase is tagged as \u2018BAD\u2019, if any word in the phrase is an incorrect translation. So, We have taken the output of the word level QE system and tagged the phrase as \u2018BAD\u2019, if any word in the phrase boundary is tagged \u2018BAD\u2019. And other phrases (all words have the OK tag) are simply tagged as \u2018OK\u2019."}, {"heading": "4.3 Results", "text": "To develop a baseline system for word and phrase level QE, organizers have used the baseline features (22 features) to train a Conditional Random Field (CRF) model with CRFSuite tool. The results of the experiments against Test2 are displayed in Table 2 and 3.\nWe have evaluated our systems using the F1score. As \u2018OK\u2019 class is dominant in the data and a naive system tagging all the words \u2018OK\u2019 will score high. Hence, F1-score of the \u2018BAD\u2019 class has been used as a primary metric for the system evaluation. We have used the separate set of test and development corpus as shown in Table 2. The evaluation of all the experiments against Test1 corpus is displayed in Table 2 for word level QE. Results for\nphrase level QE are shown in Table 3. From the result tables, it is evident that GRU outperforms LSTM as reported by Cho et al. (2014) for this task as well. Pre-training is helpful in all the models. Also, the introduction of sub-labels is able to handle the problem of labelimbalance up to some extent. The results of Bilingual models are better than monolingual models, as reported by Kreutzer et al. (2015)."}, {"heading": "4.4 Submission to the shared task", "text": "We have participated in the Task-2, which includes word and phrase level quality estimation. The submitted system setting was: GRU + Pretrain + Sublabels, which is marked in the result tables (2 and 3) as well. Table 4 and 5 detail the 2results of the submission on Test2 corpus. The submission results were provided by the organizers.\n2http://www.quest.dcs.shef.ac.uk/ wmt16_files_qe/wmt16_task2_results.pdf"}, {"heading": "5 Discussion", "text": "The approach is language independent and it uses only context words\u2019 vector for predicting the tag for a word. In the other words, we check if any word fits (grammatically) in the given slot of words or not. We could use language specific features to enhance the classification accuracy, though. Experiments with bilingual models are similar to the concept of adding more features to any machine learning algorithm. In monolingual models, we use only target (German) words\u2019 vector as feature whereas, in bilingual models, we use source (English) words\u2019 vector also. A challenge which machine learning practitioners often face is, how to deal with skewed classes in classification problems. The distribution of classes (OK/BAD) is skewed in our case as well. To handle the issue, we tried to balance the distribution of classes by introducing the sub-labels.\nLSTM and GRU are quite similar models, except the gating mechanism. It is hard to say which model will perform better in what conditions or in general (Chung et al., 2014). In this paper and in general as well, this restricts us to conduct only the empirical comparison between the LSTM and the GRU units. Deep models generally perform better than the shallow models, which is opposite for this task where LSTM outperforms Deep LSTM. The reason could be the insufficient data for training the deep models."}, {"heading": "6 Conclusion and Future Work", "text": "We have developed a language independent word/phrase level Quality Estimation system using RNN. We have used RNN-LM architecture, with LSTM, deep LSTM, and GRU. We showed that these models benefit from pretraining and the introduction of sub-labels. Also, models with bilingual features outperform the monolingual models.\nWe can extend the work for sentence and document level quality estimation. Improving the word level quality estimation with data selection and bootstrapping (Logacheva et al., 2015), more effective ways to handle label-imbalance, training bigger models, using language specific features, other variations of LSTM architecture etc., are the other possibilities."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal of Machine Learning Reseach,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Frederic Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Confidence Estimation for Machine Translation", "author": ["Blatz et al.2004] John Blatz", "Erin Fitzgerald", "George Foster", "Simona Gandrabur", "Cyril Goutte", "Alex Kulesza", "Alberto Sanchis", "Nicola Ueffing"], "venue": "In Proceedings of the 20th international con-", "citeRegEx": "Blatz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blatz et al\\.", "year": 2004}, {"title": "Findings of the 2012 workshop on statistical machine translation", "author": ["Soricut", "Lucia Specia."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10\u201351, Montreal, Canada. Association for Computational Linguistics.", "citeRegEx": "Soricut and Specia.,? 2012", "shortCiteRegEx": "Soricut and Specia.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merrinboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task", "author": ["U Politecnica de Valencia", "Christian Buck", "Marco Turchi", "Matteo Negri"], "venue": "In Proceedings of the Ninth Workshop on Statisti-", "citeRegEx": "Souza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Souza et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks. arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation", "author": ["Shigehiko Schamoni", "Stefan Riezler"], "venue": "In Proceedings of the Tenth Workshop on Statistical Ma-", "citeRegEx": "Kreutzer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kreutzer et al\\.", "year": 2015}, {"title": "Data enhancement and selection strategies for the word-level Quality Estimation", "author": ["Chris Hokamp", "Lucia Specia"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Logacheva et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Logacheva et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "In CoRR,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Connectionist language modeling for large vocabulary continuous speech recognition", "author": ["Schwenk", "Gauvain2002] Holger Schwenk", "Jean-Luc Gauvain"], "venue": "In ICASSP. IEEE,", "citeRegEx": "Schwenk et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2002}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "In Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "SHEF-NN: Translation Quality Estimation with Neural Networks", "author": ["Shah et al.2015] Kashif Shah", "Varvara Logacheva", "G Paetzold", "Frederic Blain", "Daniel Beck", "Fethi Bougares", "Lucia Specia"], "venue": "In Proceedings of the Tenth Workshop", "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "Strategy-Based Technology for Estimating MT Quality", "author": ["Shang et al.2015] Liugang Shang", "Dongfeng Cai", "Duo Ji"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Parsing With Compositional Vector Grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jy Wu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Estimating the Sentence-Level Quality of Machine Translation Systems", "author": ["Specia et al.2009] Lucia Specia", "Marco Turchi", "Nicola Cancedda", "Marc Dymetman", "Nello Cristianini"], "venue": "In 13th Conference of the European Association for Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2009}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "In IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long shortterm memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Quality estimation is the process to predict the quality of translation without any reference translation (Blatz et al., 2004; Specia et al., 2009).", "startOffset": 106, "endOffset": 147}, {"referenceID": 26, "context": "Quality estimation is the process to predict the quality of translation without any reference translation (Blatz et al., 2004; Specia et al., 2009).", "startOffset": 106, "endOffset": 147}, {"referenceID": 15, "context": "tions (Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b; Socher et al., 2013a; Socher et al., 2013b).", "startOffset": 6, "endOffset": 118}, {"referenceID": 2, "context": "as feed forward neural network language models (Schwenk and Gauvain, 2002; Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007) and Hierarchical Log-Bi-linear language models (Mnih and Hinton, 2009).", "startOffset": 47, "endOffset": 134}, {"referenceID": 21, "context": "as feed forward neural network language models (Schwenk and Gauvain, 2002; Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007) and Hierarchical Log-Bi-linear language models (Mnih and Hinton, 2009).", "startOffset": 47, "endOffset": 134}, {"referenceID": 20, "context": "Shang et al. (2015) tried to solve the problem of label imbalance with creating sub-labels like OK B (begin), OK I (intermediate), OK E (end).", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "Shah et al. (2015) have used word embedding as an additional feature (+25 features) with SVM classifier.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Bilingual Deep Neural Network (DNN) based model for word level QE was proposed by Kreutzer et al. (2015), in which word embedding was pre-trained and fine-tuned with other parameters of the network using stochastic gradient descent.", "startOffset": 82, "endOffset": 105}, {"referenceID": 8, "context": "de Souza et al. (2014) have used Bidirectional LSTM as a classifier for word level QE.", "startOffset": 3, "endOffset": 23}, {"referenceID": 28, "context": "The architecture of RNN-LM has been used for Natural Language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) earlier.", "startOffset": 82, "endOffset": 118}, {"referenceID": 29, "context": "The architecture of RNN-LM has been used for Natural Language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) earlier.", "startOffset": 82, "endOffset": 118}, {"referenceID": 13, "context": "Our approach is quite similar to the Kreutzer et al. (2015), but we are using RNN instead of DNN.", "startOffset": 37, "endOffset": 60}, {"referenceID": 13, "context": "Our approach is quite similar to the Kreutzer et al. (2015), but we are using RNN instead of DNN. We have also tried to address the problem of label-imbalance, introducing sub-labels as suggested by Shang et al. (2015).", "startOffset": 37, "endOffset": 219}, {"referenceID": 6, "context": "For this task, we exploited RNN\u2019s extensions, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 142, "endOffset": 160}, {"referenceID": 1, "context": "Simple RNN also suffers from the problem of exploding and vanishing gradient (Bengio et al., 1994).", "startOffset": 77, "endOffset": 98}, {"referenceID": 9, "context": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015).", "startOffset": 59, "endOffset": 116}, {"referenceID": 29, "context": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015).", "startOffset": 59, "endOffset": 116}, {"referenceID": 11, "context": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015).", "startOffset": 59, "endOffset": 116}, {"referenceID": 7, "context": "Chung et al. (2014) found that GRU outperforms LSTM on a suit of tasks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "We implemented all the models (LSTM, deep LSTM and GRU) with 1THEANO framework (Bergstra et al., 2010; Bastien et al., 2012) as described above.", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "We implemented all the models (LSTM, deep LSTM and GRU) with 1THEANO framework (Bergstra et al., 2010; Bastien et al., 2012) as described above.", "startOffset": 79, "endOffset": 124}, {"referenceID": 27, "context": "To update the model parameters, we have used Truncated Back-Propagation-Through-Time (T-BPTT) (Werbos, 1990) (Werbos, 1990) with stochastic gradient descent.", "startOffset": 94, "endOffset": 108}, {"referenceID": 27, "context": "To update the model parameters, we have used Truncated Back-Propagation-Through-Time (T-BPTT) (Werbos, 1990) (Werbos, 1990) with stochastic gradient descent.", "startOffset": 109, "endOffset": 123}, {"referenceID": 30, "context": "We used Ada-delta (Zeiler, 2012) (Zeiler, 2012) to adapt the learning rate of each parameter automatically ( = 10\u22126 and \u03c1 = 0.", "startOffset": 18, "endOffset": 32}, {"referenceID": 30, "context": "We used Ada-delta (Zeiler, 2012) (Zeiler, 2012) to adapt the learning rate of each parameter automatically ( = 10\u22126 and \u03c1 = 0.", "startOffset": 33, "endOffset": 47}, {"referenceID": 12, "context": ") from EnglishGerman Europarl data (Koehn, 2005).", "startOffset": 35, "endOffset": 48}, {"referenceID": 12, "context": ") from EnglishGerman Europarl data (Koehn, 2005). For bilingual models, we restructured the source sentence (English) according to the target (German) using word alignment provided by the organizers. For many-to-one mapping in the alignment (English-German), we chose the first alignment only. The \u2018NULL\u2019 token was assigned to the words where were not aligned with any word on the target side. The input of the model is constructed by concatenating context words of source and target. For example, consider the source word sequence s1s2s3, and the target word sequence t1t2t3, then the input to the network will be s1s2s3t1t2t3. In the training data, the distribution of the labels (OK/BAD) is skewed (OK to BAD ratio is approx. 4:1). To handle the issue, we tried one of the strategies proposed by Shang et al. (2015),", "startOffset": 36, "endOffset": 819}, {"referenceID": 6, "context": "From the result tables, it is evident that GRU outperforms LSTM as reported by Cho et al. (2014) for this task as well.", "startOffset": 79, "endOffset": 97}, {"referenceID": 13, "context": "The results of Bilingual models are better than monolingual models, as reported by Kreutzer et al. (2015).", "startOffset": 83, "endOffset": 106}, {"referenceID": 7, "context": "It is hard to say which model will perform better in what conditions or in general (Chung et al., 2014).", "startOffset": 83, "endOffset": 103}], "year": 2016, "abstractText": "This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16). The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture. RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNNLM. The modified system predicts a label (OK/BAD) in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system.", "creator": "LaTeX with hyperref package"}}}