{"id": "1610.06067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Fairness as a Program Property", "abstract": "we explore the following result : is a decision - making program fair, defining some useful definition of fairness? first, we investigate but several algorithmic fairness questions can be phrased after program verification problems. second, we discuss linear automated verification technique for proving or disproving fairness when decision - making programs with respect to a probabilistic model of the population.", "histories": [["v1", "Wed, 19 Oct 2016 15:31:34 GMT  (2264kb,D)", "http://arxiv.org/abs/1610.06067v1", null]], "reviews": [], "SUBJECTS": "cs.PL cs.AI", "authors": ["aws albarghouthi", "loris d'antoni", "samuel drews", "aditya nori"], "accepted": false, "id": "1610.06067"}, "pdf": {"name": "1610.06067.pdf", "metadata": {"source": "CRF", "title": "Fairness as a Program Property", "authors": ["Aws Albarghouthi", "Loris D\u2019Antoni", "Samuel Drews", "Aditya Nori"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others. With the range and sensitivity of algorithmic decisions expanding by the day, the question of whether an algorithm is fair is a pressing one. Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].\nUltimately, algorithmic fairness is a question about programs and their properties: Is a given program P fair, under some definition of fairness? Or, how fair is P? In this paper, we describe a line of work that approaches the question of algorithmic fairness from a program-analytic perspective, in which our goal is to analyze a given decision-making program and construct a proof of its fairness or unfairness\u2014just as a traditional static program verifier would prove correctness of a program with respect to, for example, lack of divisions by zero, integer overflows, null-pointer derefrences, etc.\nWe start by analyzing what are the challenges and research questions in checking algorithmic fairness for decision making programs (Section 2). We then present\na simple case study and show how techniques for verifying probabilistic programs can be used to automatically prove or disprove global fairness for a class of programs that subsume a range of machine learning classifiers (Section 3). Finally, we lay a list of many challenging and interesting questions that the algorithms and programming languages communities need to answer to achieve the ultimate goal of building a fully automated system for verifying and guaranteeing algorithmic fairness in real-world applications (Section 4)."}, {"heading": "2. Proving Programs Fair", "text": "In this section, we describe the components of the fairness verification problem. Intuitively, our goal is to prove whether a certain program is fair with respect to the set of possible inputs over which it operates. Tackling the fairness-verification problem requires answering a number of challenging questions: \u2013 What class of decision-making programs should our\nprogram model capture?\nar X\niv :1\n61 0.\n06 06\n7v 1\n[ cs\n.P L\n] 1\n9 O\nct 2\n01 6\n\u2013 How can we define the set of possible inputs to the program and capture complex probability distributions that are useful and amenable to verification?\n\u2013 How can we describe what it means for the program to be fair?\n\u2013 How can we fully automate the verification process? Figure 1 provides a high-level picture of our proposed framework. As shown, the fairness verifier takes a (white-box) decision-making program P and a population model M . The verifier then proceeds to prove or disprove that P is fair for the given population defined by the model M . Here, the model M defines a joint probability distribution on the inputs of P . Existing definitions of fairness define programs as fair or unfair with respect to a given concrete dataset. While using a concrete dataset simplifies the verification problem, it also raises questions of whether the dataset is representative for the population for which we are trying to prove fairness. Our technique moves away from concrete datasets and replaces them with a probabilistic population model. We envision a future in which fairness verification is regulated.1 For instance, a governmental agency can publish a probabilistic population model (e.g., generated from census data). Any organization employing a decision-making algorithm with potentially significant consequences (e.g., hiring) must quantify fairness of their algorithmic process against the current picture of the population, as specified by the population model.\nDecision-making programs In the context of algorithmic fairness, a program P takes as input a vector of arguments v representing a set of input attributes (features), where one (or more) of the arguments vs in the vector v is sensitive\u2014e.g., gender or race. Evaluating P(v) may return a Boolean value indicating\u2014e.g., hire or not hire\u2014if the program is a binary or a numerical value\u2014e.g., a mortgage rate. The set of combinators, operations, and types used by the program can vastly affect the complexity of the verification procedures. For example, loops are the hardest type of programming construct to reason about, but most machine learning classifiers do not contain loops. Similarly, since classifiers typically operate over real values, we can limit the set of possible types allowed in our programs to only being reals or other types that can be desugared into\n1 The European Union (EU), for instance, has already begun regulating algorithmic decision-making [17].\nreals. All these decisions are crucial in the design of a verification procedure. Population model To be able to reason about the outcome of the program we need to specify what kind of input the program will operate on. For example, although a program that allocates mortgages might be \u201cfair\u201d with respect a certain set of applicants, it may become unfair when considering a different pool of people. In program verification, the \u201ckind of inputs\u201d over which the program operates is called the precondition and is typically stated as a formal logical property with the program inputs as free variables. An example of program precondition is\nvgender = f \u2192 vjob 6= priest\nwhich indicates that none of the program inputs is both a woman and a priest. Of course, there are many possible choices for what language we can use to describe the program\u2019s precondition. In particular, if we want to capture a certain probability distribution over the input of the program, our language will be a logic that can describe probabilities and random variables. For example, we might want to be able to specify that half of the inputs are female, Pr[vgender = f ] = 0.5, or that the age of the processed inputs has a particular distribution, vage \u223c gauss(18, 5). Again, the choice of the language allowed in the preconditions is crucial in the design of a verification procedure. From now on, we refer to the program precondition, Dpop, as the population model. Fairness properties The next step is to define a property stating that the program\u2019s outcome is fair with respect to the program\u2019s precondition. In program verification, this is called the postcondition of the program. As observed in the fairness literature, there are many ways to define when and why a program is fair or unfair.\nFor example, if we want to prove group fairness\u2014 i.e., that the algorithm is just as likely to hire a minority applicant (m) as it is for other, non-minority applicants\u2014our postcondition will be an expression of the form\nPr[P(v) = true | vs = m] Pr[P(v) = true | vs 6= m] > 1\u2212\nwhere true is the desired return value of the program, e.g., indicating hiring. On the other hand, if we want to prove individual fairness\u2014i.e., similar inputs should\nhave similar outcomes\u2014our postcondition will be an expression of the form\nPr[P(v) 6= P(v\u2032) | v \u223c v\u2032] <\nNotice that the last postcondition relates the outcomes of the program on different input values. As the two types of properties we described are radically different, they will also require different verification mechanisms. Proofs of (un)fairness The task of proving whether a program is fair boils down to statically checking whether, on inputs satisfying the precondition, the outcome of the program satisfies the post-condition. For simple definitions, such as group fairness, the verification problem reduces to computing the probability of a number of events with respect to the program and the population model. For more complex definitions, such as individual fairness, proving fairness requires more complex reasoning involving multiple runs of the programs (i.e., a hyperproperty [9]), a notoriously hard problem. In the case of a negative result, the verifier should provide the users with a proof of unfairness. Depending on the fairness definition, producing a humanreadable proof might be challenging as the argument might involve multiple and potentially infinite inputs. For example, in the case of group fairness it might be challenging to explain why the program outputs true on 40% of the minority inputs and on 70% of the majority inputs."}, {"heading": "3. Case Study", "text": "We now describe a simplified case study demonstrating how our fairness verification methodology can be used to prove or disprove fairness of a given decisionmaking program. A program and a population model Consider the following program dec, which is a decision-making program that takes a job applicant\u2019s college ranking and years of experience and decides whether they get hired or not (the fairness target). The program implements a decision tree, perhaps one generated by a machinelearning algorithm. A person is hired if they attended a top-5 college (colRank <= 5) or have lots of experience compared to their college\u2019s ranking (expRank > -5). Observe that dec does not access ethnicity.\ndefine dec(colRank, yExp) expRank \u2190 yExp - colRank if (colRank <= 5)\nhire \u2190 true elif (expRank > -5) hire \u2190 true else hire \u2190 false return hire\nNow, consider the program popModel, which is a probabilistic program describing a simple model of the population. Here, a member of the population has three attributes, all of which are real-valued: (i) ethnicity; (ii) colRank, the ranking of the college the person attended (lower is better); and (iii) yExp, the years of work experience a person has. We consider a person is a member of a protected group if ethnicity > 10; we call this the sensitive condition. The population model can be viewed as a generative model of records of individuals\u2014the more likely a combination is to occur in the population, the more likely it will be generated. For instance, the years of experience an individual has (line 4) follows a Gaussian distribution with mean 10 and standard deviation 5.\ndefine popModel() ethnicity ~ gauss(0,10) colRank ~ gauss(25,10) yExp ~ gauss(10,5) if (ethnicity > 10) colRank \u2190 colRank + 5\nreturn colRank, yExp\nA note on the program model Note that our program model, while admitting arbitrary programs, is rich enough to capture programs (classifiers) generated by standard machine learning algorithms. For example, linear support vector machines, decision trees, and neural networks, can be represented in our language simply using assignments with arithmetic expressions and conditionals. Similarly, the population model is a probabilistic program, where assignments can be made by drawing values from predefined distributions. Like other probabilistic programming languages, our programming model is rich enough to subsume graphical models like Bayesian networks [19].\nGroup fairness Suppose that our goal is to prove group fairness, following the definition of Feldman et al. [16]:\nPr[hire | min] Pr[hire | \u00acmin] > 1\u2212\nwhere min is shorthand for the sensitive condition ethnicity > 10.\nProbabilistic inference as volume computation To prove (un)fairness of the decision-making model with respect to the population, we need to compute the probabilities appearing in the group fairness ratio. For illustration, suppose we are computing the probability Pr[hire \u2227 \u00acmin]. We need to reason about the composition of the two programs, dec \u25e6 popModel. That is, we want to compute the probability that (i) popModel generates a non-minority applicant, and (ii) dec hires that applicant. To do so, we observe that every possible execution of the composition dec \u25e6 popModel is uniquely characterized by the set of the three probabilistic choices made by popModel. In other words, every execution is characterized by a vector v \u2208 R3.\nThus, our goal is to compute the probability that we draw a vector v that results in a minority applicant being hired. Probabilistic programming languages, e.g., Church [18], R2 [23], and Stan [7], employ approximate inference techniques, like MCMC, which converge in the limit but offer no guarantees on how far we are from the exact result. In our work, we consider exact inference, which has primarily received attention in the Bayesian network setting, and boils down to solving a #SAT instance [8]. In our setting, however, we are dealing with real-valued variables.\nUsing standard techniques from program analysis and verification, we can characterize the set of all such vectors as a formula \u03d5, which is comprised of Boolean combinations (conjunctions/disjunctions) of linear inequalities\u2014since our program only has linear expressions. Geometrically, the formula \u03d5 is a set of convex polyhedra in Rn. Therefore, the probability Pr[hire \u2227 \u00acmin] is the same as the probability of drawing a vector v that lies inside of \u03d5. In other words, we are interested in the volume of \u03d5, weighted by the probabilistic choices. Formally:\nPr[hire \u2227 \u00acmin] = \u222b \u03d5 pepypc dedydc\nwhere, e.g., pe is the probability density function of the distribution gauss(0,10)\u2014the distribution from which the value of ethnicity is drawn in line 2 of popModel.\nThe volume computation problem is a well-studied and hard problem [14, 20]. Indeed, even for a convex polytope, computing its volume is #P-hard. Leveraging the great developments in satisfiabiltiy modulo theories (SMT) solvers [4], we developed a procedure that reduces the volume compuation problem to a series of\ncalls to the SMT solver, viewed completely as an oracle. Specifically, our procedure uses the SMT solver to sample subregions of \u03d5 that are hyperrectangular. Intuitively, for hyperrectangular regions in Rn, evaluating the above integral is a matter of evaluating the CDFs of the various distributions. Thus, by systematically sampling more and more non-overlapping hyperrectangles in \u03d5, we maintain a lower bound on the probability of interest. Figure 2 pictorially illustrates \u03d5 and an under-approximation with 4 hyperrectangles. Similarly, to compute an upper bound on the probability, we can simply invoke our procedure on \u00ac\u03d5. Fairness certificates The fairness verification tool terminates when it has computed lower/upper bounds that prove or disprove the desired fairness criteria. The hyperrectangles sampled in the process of computing volumes can serve as proof certificates. That is, an external entity can take the hyperrectangles, compute their volumes, and ensure that they indeed lie in the expected regions in Rn."}, {"heading": "4. Experience and future Outlook", "text": "Experience We have built a fairness-verification tool, called FairSquare, that takes a decision-making program, a population model, and verifies fairness of the program with respect to the model. So far, we have focused on group fairness. The tool uses the popular Z3 SMT solver [12] for manipulating first-order formulas over arithmetic theories.\nWe have used FairSquare to prove or disprove fairness of a suite of population models and programs representing machine-learning classifiers that were automatically generated from real-world datasets used in other work on algorithmic fairness [11, 16, 29]. Specifically, we have considered linear SVMs, simple neural networks with rectified linear units, and decision trees. Future outlook Looking forward, we see a wide range of avenues for improvement and exploration. For in-\nstance, we are currently working on the problem of making an unfair program fair. That is, given a program P that is considered unfair, what is the smallest tweak that would make it fair. Our goal is to repair the program, making it fair, while ensuring that it is semantically close to the original program."}], "references": [{"title": "Hiring by algorithm: predicting and preventing disparate impact", "author": ["Ifeoma Ajunwa", "Sorelle Friedler", "Carlos E Scheidegger", "Suresh Venkatasubramanian"], "venue": "Available at SSRN", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Machine bias: There\u2019s software used across the country to predict future criminals. and it\u2019s biased against blacks", "author": ["Julia Angwin", "Jeff Larson", "Surya Mattu", "Lauren Kirchner"], "venue": "https://www.propublica.org/article/machinebias-risk-assessments-in-criminal-sentencing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Big data\u2019s disparate impact", "author": ["Solon Barocas", "Andrew D Selbst"], "venue": "Available at SSRN 2477899,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Satisfiability modulo theories", "author": ["Clark W. Barrett", "Roberto Sebastiani", "Sanjit A. Seshia", "Cesare Tinelli"], "venue": "In Handbook of Satisfiability,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Predicting crime, lapd-style", "author": ["Nate Berg"], "venue": "https://www.theguardian.com/cities/2014/jun/ 25/predicting-crime-lapd-los-angeles-policedata-analysis-algorithm-minority-report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Three naive bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Stan: a probabilistic programming language", "author": ["Bob Carpenter", "Andrew Gelman", "Matt Hoffman", "Daniel Lee", "Ben Goodrich", "Michael Betancourt", "Marcus A Brubaker", "Jiqiang Guo", "Peter Li", "Allen Riddell"], "venue": "Journal of Statistical Software,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "On probabilistic inference by weighted model counting", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Automated experiments on ad privacy settings", "author": ["Amit Datta", "Michael Carl Tschantz", "Anupam Datta"], "venue": "Proceedings on Privacy Enhancing Technologies,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Algorithmic transparency via quantitative input influence", "author": ["Anupam Datta", "Shayak Sen", "Yair Zick"], "venue": "In Proceedings of 37th IEEE Symposium on Security and Privacy,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "An efficient smt solver", "author": ["Leonardo De Moura", "Nikolaj Bj\u00f8rner. Z"], "venue": "In International conference on Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard S. Zemel"], "venue": "In Innovations in Theoretical Computer Science", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "On the complexity of computing the volume of a polyhedron", "author": ["Martin E. Dyer", "Alan M. Frieze"], "venue": "SIAM Journal on Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1988}, {"title": "The dangers of letting algorithms enforce policy. http://www.slate.com/articles/ technology/future_tense/2015/04/the_dangers_ of_letting_algorithms_enforce_policy.html", "author": ["Virginia Eubanks"], "venue": "(Accessed on 06/18/2016)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "EU regulations on algorithmic decision-making and a \u201cright to explanation", "author": ["B. Goodman", "S. Flaxman"], "venue": "ArXiv e-prints,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Church: a language for generative models", "author": ["Noah Goodman", "Vikash Mansinghka", "Daniel M Roy", "Keith Bonawitz", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1206.3255,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Probabilistic programming", "author": ["Andrew D Gordon", "Thomas A Henzinger", "Aditya V Nori", "Sriram K Rajamani"], "venue": "In Proceedings of the on Future of Software Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Complexity of polytope volume", "author": ["Leonid Khachiyan"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "Who do you blame when an algorithm gets you fired? http://www.wired.co.uk/article/ make-algorithms-accountable, January 2016", "author": ["Nicole Kobie"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Can an algorithm hire better than a human? http://www.nytimes.com/2015/06/26/ upshot/can-an-algorithm-hire-better-than-ahuman.html", "author": ["Claire Cain Miller"], "venue": "(Accessed on 06/18/2016)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "An efficient mcmc sampler for probabilistic programs", "author": ["Aditya V Nori", "Chung-Kil Hur", "Sriram K Rajamani", "Selva Samuel"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Predictive policing: The role of crime forecasting in law enforcement operations", "author": ["Walt L Perry"], "venue": "Rand Corporation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Discrimination in online ad delivery", "author": ["Latanya Sweeney"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "An fda for algorithms", "author": ["Andrew Tutt"], "venue": "Available at SSRN", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Websites vary prices, deals based on users", "author": ["Jennifer Valentino-Devries", "Jeremy Singer-Vine", "Ashkan Soltani"], "venue": "information. http://www.wsj.com/articles/ SB10001424127887323777204578189391813881534,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Learning fair representations", "author": ["Richard S. Zemel", "Yu Wu", "Kevin Swersky", "Toniann Pitassi", "Cynthia Dwork"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others.", "startOffset": 121, "endOffset": 129}, {"referenceID": 20, "context": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others.", "startOffset": 121, "endOffset": 129}, {"referenceID": 13, "context": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others.", "startOffset": 150, "endOffset": 154}, {"referenceID": 1, "context": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others.", "startOffset": 174, "endOffset": 177}, {"referenceID": 4, "context": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others.", "startOffset": 188, "endOffset": 195}, {"referenceID": 22, "context": "Algorithms have become powerful arbitrators of a range of significant decisions with far-reaching societal impact\u2014hiring [21, 22], welfare allocation [15], prison sentencing [2], policing [5, 25], amongst many others.", "startOffset": 188, "endOffset": 195}, {"referenceID": 5, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 142, "endOffset": 157}, {"referenceID": 11, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 142, "endOffset": 157}, {"referenceID": 14, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 142, "endOffset": 157}, {"referenceID": 26, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 142, "endOffset": 157}, {"referenceID": 1, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 209, "endOffset": 224}, {"referenceID": 8, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 209, "endOffset": 224}, {"referenceID": 23, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 209, "endOffset": 224}, {"referenceID": 25, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 209, "endOffset": 224}, {"referenceID": 0, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 261, "endOffset": 271}, {"referenceID": 2, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 261, "endOffset": 271}, {"referenceID": 24, "context": "Indeed, the notion of algorithmic fairness has captured the attention of a broad spectrum of experts: machine learning and theory researchers [6, 13, 16, 29]; privacy researchers and investigative journalists [2, 10, 26, 28]; law scholars and social scientists [1, 3, 27]; governmental agencies and NGOs [24].", "startOffset": 261, "endOffset": 271}, {"referenceID": 15, "context": "1 The European Union (EU), for instance, has already begun regulating algorithmic decision-making [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "Like other probabilistic programming languages, our programming model is rich enough to subsume graphical models like Bayesian networks [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "[16]: Pr[hire | min] Pr[hire | \u00acmin] > 1\u2212", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": ", Church [18], R2 [23], and Stan [7], employ approximate inference techniques, like MCMC, which converge in the limit but offer no guarantees on how far we are from the exact result.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": ", Church [18], R2 [23], and Stan [7], employ approximate inference techniques, like MCMC, which converge in the limit but offer no guarantees on how far we are from the exact result.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": ", Church [18], R2 [23], and Stan [7], employ approximate inference techniques, like MCMC, which converge in the limit but offer no guarantees on how far we are from the exact result.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "In our work, we consider exact inference, which has primarily received attention in the Bayesian network setting, and boils down to solving a #SAT instance [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 12, "context": "The volume computation problem is a well-studied and hard problem [14, 20].", "startOffset": 66, "endOffset": 74}, {"referenceID": 18, "context": "The volume computation problem is a well-studied and hard problem [14, 20].", "startOffset": 66, "endOffset": 74}, {"referenceID": 3, "context": "Leveraging the great developments in satisfiabiltiy modulo theories (SMT) solvers [4], we developed a procedure that reduces the volume compuation problem to a series of colRank", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "The tool uses the popular Z3 SMT solver [12] for manipulating first-order formulas over arithmetic theories.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "We have used FairSquare to prove or disprove fairness of a suite of population models and programs representing machine-learning classifiers that were automatically generated from real-world datasets used in other work on algorithmic fairness [11, 16, 29].", "startOffset": 243, "endOffset": 255}, {"referenceID": 14, "context": "We have used FairSquare to prove or disprove fairness of a suite of population models and programs representing machine-learning classifiers that were automatically generated from real-world datasets used in other work on algorithmic fairness [11, 16, 29].", "startOffset": 243, "endOffset": 255}, {"referenceID": 26, "context": "We have used FairSquare to prove or disprove fairness of a suite of population models and programs representing machine-learning classifiers that were automatically generated from real-world datasets used in other work on algorithmic fairness [11, 16, 29].", "startOffset": 243, "endOffset": 255}], "year": 2016, "abstractText": "We explore the following question: Is a decision-making program fair, for some useful definition of fairness? First, we describe how several algorithmic fairness questions can be phrased as program verification problems. Second, we discuss an automated verification technique for proving or disproving fairness of decision-making programs with respect to a model of the population.", "creator": "LaTeX with hyperref package"}}}