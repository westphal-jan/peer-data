{"id": "1610.07505", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition", "abstract": "we develop a bayesian model for decision - producing under time rules with endogenous information acquisition. in our model, the epoch maker decides when to observe ( costly ) information by sampling an underlying continuous - time stochastic process ( time series ) that conveys anxiety about the potential cancellation or probable - detection of an adverse event which or terminate it decision - making process. in her attempt to predict the occurrence of the adverse event, the decision - maker follows a direction that determines when players acquire information from the time series ( continuation ), and when to stop acquiring information and make a final prediction ( stopping ). we show that the optimal policy has appropriate rendezvous structure, i. e. a structure in which whenever a new information sample is gathered from the time series, the optimal \" date \" for acquiring the next sample becomes computable. the optimal plan comparing an information samples balances a trade - off like the decision planner's surprise, i. l. the drift in or posterior expenditure on observing new information, and conditional, i. e. the probability that the adverse event occurs in the time interval between two information samples. moreover, we characterize the continuation and stopping regions in the decision - maker's state - space, and show that they depend not only towards the result - maker's beliefs, but also on conditional context, i. e. the ultimate realization concludes the time series.", "histories": [["v1", "Mon, 24 Oct 2016 17:43:34 GMT  (478kb)", "http://arxiv.org/abs/1610.07505v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ahmed ibrahim", "mihaela van der schaar"], "accepted": true, "id": "1610.07505"}, "pdf": {"name": "1610.07505.pdf", "metadata": {"source": "CRF", "title": "Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition", "authors": ["Ahmed M. Alaa"], "emails": ["ahmedmalaa@ucla.edu", "mihaela.vanderschaar@eng.ox.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n07 50\n5v 1\n[ cs\n.A I]\n2 4\nO ct"}, {"heading": "1 Introduction", "text": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7]. A common setting that arises in all these domains is that a decision-maker, provided with sequential observations of a time series, needs to decide whether or not an adverse event (e.g. financial crisis, clinical acuity for ward patients, etc) will take place in the future. The decision-maker\u2019s recognition of a forthcoming adverse event needs to be timely, for that a delayed decision may hinder effective intervention (e.g. delayed admission of clinically acute patients to intensive care units can lead to mortality [5]). In the context of cognitive science, this decision-making task is known as the two-alternative forced choice (2AFC) task [15]. Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].\nIn this paper, we present a Bayesian decision-making model in which a decision-maker adaptively decides when to gather (costly) information from an underlying time series in order to accumulate evidence on the occurrence/non-occurrence of an adverse event. The decision-maker operates under time pressure: occurrence of the adverse event terminates the decision-making process. Our abstract model is motivated and inspired by many practical decision-making tasks such as: constructing temporal patterns for gathering sensory information in perceptual decision-making [1], scheduling lab\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\ntests for ward patients in order to predict clinical deterioration in a timely manner [3, 5], designing breast cancer screening programs for early tumor detection [7], etc.\nWe characterize the structure of the optimal decision-making policy that prescribes when should the decision-maker acquire new information, and when should she stop acquiring information and issue a final prediction. We show that the decision-maker\u2019s posterior belief process, based on which policies are prescribed, is a supermartingale that reflects the decision-maker\u2019s tendency to deny the occurrence of an adverse event in the future as she observes the survival of the time series for longer time periods. Moreover, the information acquisition policy has a \u201drendezvous\u201d structure; the optimal \u201ddate\u201d for acquiring the next information sample can be computed given the current sample. The optimal schedule for gathering information over time balances the information gain (surprise) obtained from acquiring new samples, and the probability of survival for the underlying stochastic process (suspense). Finally, we characterize the continuation and stopping regions in the decision-maker\u2019s state-space and show that, unlike previous models, they depend on the time series \u201dcontext\u201d and not just the decision-maker\u2019s beliefs.\nRelated Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17]. Most of these models use tools from sequential analysis developed by Wald [8] and Shiryaev [21, 22]. In [9,13,14], optimal decision-making policies for the 2AFC task were computed by modelling the decision-maker\u2019s sensory evidence using diffusion processes [20]. These models assume an infinite time horizon for the decision-making policy, and an exogenous supply of sensory information.\nThe assumption of an infinite time horizon was relaxed in [10] and [15], where decision-making is assumed to be performed under the pressure of a stochastic deadline; however, these deadlines were considered to be drawn from known distributions that are independent of the hypothesis and the realized sensory evidence, and the assumption of an exogenous information supply was maintained. In practical settings, the deadlines would naturally be dependent on the realized sensory information (e.g. patients\u2019 acuity events are correlated with their physiological information [5]), which induces more complex dynamics in the decision-making process. Context-based decision-making models were introduced in [17], but assuming an exogenous information supply and an infinite time horizon.\nThe notions of \u201csuspense\u201d and \u201csurprise\u201d in Bayesian decision-making have also been recently introduced in the economics literature (see [18] and the references therein). These models use measures for Bayesian surprise, originally introduced in the context of sensory neuroscience [19], in order to model the explicit preference of a decision-maker to non-instrumental information. The goal there is to design information disclosure policies that are suspense-optimal or surprise-optimal. Unlike our model, such models impose suspense (and/or surprise) as a (behavioral) preference of the decision-maker, and hence they do not emerge endogenously by virtue of rational decision making."}, {"heading": "2 Timely Decision Making with Endogenous Information Acquisition", "text": "Time Series Model The decision-maker has access to a time-series X(t) modeled as a continuoustime stochastic process that takes values in R, and is defined over the time domain t \u2208 R+, with an underlying filtered probability space (\u2126,F , {Ft}t\u2208R+ ,P). The process X(t) is naturally adapted to {Ft}t\u2208R+ , and hence the filtration Ft abstracts the information conveyed in the time series realization up to time t. The decision-maker extracts information from X(t) to guide her actions over time.\nWe assume that X(t) is a stationary Markov process1, with a stationary transition kernel P\u03b8 (X(t) \u2208 A|Fs) = P\u03b8 (X(t) \u2208 A|X(s)) , \u2200A \u2282 R, \u2200s < t \u2208 R+, where \u03b8 is a realization of a latent Bernoulli random variable \u0398 \u2208 {0, 1} (unobservable by the decision-maker), with P(\u0398 = 1) = p. The distributional properties of the paths of X(t) are determined by \u03b8, since the realization of \u03b8 decides which Markov kernel (Po or P1) generates X(t). If the realization \u03b8 is equal to 1, then an adverse event occurs almost surely at a (finite) random time \u03c4 , the distribution of which is dependent on the realization of the path (X(t))0\u2264t\u2264\u03c4 .\n1Most of the insights distilled from our results would hold for more general dependency structures. However, we keep this assumption to simplify the exposition and maintain the tractability and interpretability of the results.\nThe decision-maker\u2019s ultimate goal is to sequentially observe X(t), and infer \u03b8 before the adverse event happens; inference is obsolete if it is declared after \u03c4 . Since \u0398 is latent, the decision-maker is unaware whether the adverse event will occur or not, i.e. whether her access to X(t) is temporary (\u03c4 < \u221e for \u03b8 = 1) or permanent (\u03c4 = \u221e for \u03b8 = 0). In order to model the occurrence of the adverse event; we define \u03c4 as an F -stopping time for the process X(t), for which we assume the following:\n\u2022 The stopping time \u03c4 |\u0398 = 1 is finite almost surely, whereas \u03c4 |\u0398 = 0 is infinite almost surely, i.e. P (\u03c4 < \u221e|\u0398 = 1) = 1, and P (\u03c4 = \u221e|\u0398 = 0) = 1.\n\u2022 The stopping time \u03c4 |\u0398 = 1 is accessible2, with a Markovian dependency on history, i.e. P (\u03c4 < t| Fs) = P (\u03c4 < t|X(s)) , \u2200s < t, where P (\u03c4 < t|X(s)) is an injective map from R to [0, 1] and P (\u03c4 < t|X(s)) is non-decreasing in X(s).\nThus, unlike the stochastic deadline models in [10] and [15], the decision deadline in our model (i.e. occurrence of the adverse event) is context-dependent as it depends on the time series realization (i.e. P (\u03c4 < t|X(s)) is not independent of X(t) as in [15]). We use the notation X\u03c4 (t) = X(t\u2227\u03c4), where t \u2227 \u03c4 = min{t, \u03c4} to denote the stopped process to which the decision-maker has access. Throughout the paper, the measures Po and P1 assign probability measures to the paths X\u03c4 (t)|\u0398 = 0 and X\u03c4(t)|\u0398 = 1 respectively, and we assume that Po << P13.\nInformation The decision-maker can only observe a set of (costly) samples of X\u03c4 (t) rather than the full continuous path. The samples observed by the decision-maker are captured by partitioning X(t) over specific time intervals: we define Pt = {to, t1, . . ., tN(Pt)\u22121}, with 0 \u2264 to < t1 < . . . < tN(Pt)\u22121 \u2264 t, as a size-N(Pt) partition of X\n\u03c4 (t) over the interval [0, t], where N(Pt) is the total number of samples in the partition Pt. The decision-maker observes the values that X\u03c4 (t) takes at the time instances in Pt; thus the sequence of observations is given by the process X(Pt) = \u2211N(Pt)\u22121 i=0 X(ti)\u03b4ti , where \u03b4ti is the Dirac measure. The space of all partitions over the interval [0, t] is denoted by Pt = [0, t]N. We denote the probability measures for partitioned paths generated under \u0398 = 0 and 1 with a partition Pt as P\u0303o(Pt) and P\u03031(Pt) respectively.\nSince the decision-maker observes X\u03c4 (t) through the partition Pt, her information at time t is conveyed in the \u03c3-algebra \u03c3(X\u03c4 (Pt)) \u2282 Ft. The stopping event is observable by the decisionmaker even if \u03c4 /\u2208 P\u03c4 . We denote the \u03c3-algebra generated by the stopping event as St = \u03c3 ( 1{t\u2265\u03c4} ) .\nThus, the information that the decision-maker has at time t is expressed by the filtration F\u0303t = \u03c3(X\u03c4 (Pt)) \u2228 St, and it follows that any decision-making policy needs to be F\u0303t-measurable.\nFigure 1 depicts a Brownian path (a sample path of a Wiener process, which satisfies all the assumptions of our model)4, with an exemplary partition Pt over the time interval [0, 1]. The decision-maker observes the samples in X(Pt) sequentially, and reasons about the realization of the latent variable \u0398 based on these samples and the process survival, i.e. at t = 0.2, the decision-\n2Our analyses hold if the stopping time is totally inaccessible. 3The absolute continuity of Po with respect to P1 means that no sample path of X\u03c4 (t)|\u0398 = 0 should be\nfully revealing of the realization of \u0398. 4In Figure 1, the stopping event was simulated as a totally inaccessible first jump of a Poisson process.\nmaker\u2019s information resides in the \u03c3-algebra \u03c3(X(0), X(0.1), X(0.15)) generated by the samples in P0.2 = {0, 0.1, 0.15}, and the \u03c3-algebra generated by the process\u2019 survival S0.2 = \u03c3(1{\u03c4>0.2}).\nPolicies and Risks The decision-maker\u2019s goal is to come up with a (timely) decision \u03b8\u0302 \u2208 {0, 1}, that reflects her prediction for whether the actual realization \u03b8 is 0 or 1, before the process X\u03c4(t) potentially stops at the unknown time \u03c4 . The decision-maker follows a policy: a (continuous-time) mapping from the observations gathered up to every time instance t to two types of actions:\n\u2022 A sensing action \u03b4t \u2208 {0, 1}: if \u03b4t = 1, then the decision-maker decides to observe a new sample from the running process X\u03c4 (t) at time t.\n\u2022 A continuation/stopping action \u03b8\u0302t \u2208 {\u2205, 0, 1}: if \u03b8\u0302t \u2208 {0, 1}, then the decision-maker decides to stop gathering samples from X\u03c4 (t), and declares a final decision (estimate) for \u03b8. Whenever \u03b8\u0302t = \u2205, the decision-maker continues observing X\u03c4 (t) and postpones her declaration for the estimate of \u03b8.\nA policy \u03c0 = (\u03c0t)t\u2208R+ is a (F\u0303t-measurable) mapping rule that maps the information in F\u0303t to an action tuple \u03c0t = (\u03b4t, \u03b8\u0302t) at every time instance t. We assume that every single observation that the decision-maker draws from X\u03c4(t) entails a fixed cost, hence the process (\u03b4t)t\u2208R+ has to be a point process under any optimal policy5. We denote the space of all such policies by \u03a0.\nA policy \u03c0 generates the following random quantities as a function of the paths X\u03c4(t) on the probability space (\u2126,F , {Ft}t\u2208R+ ,P):\n1- A stopping time T\u03c0: The first time at which the decision-maker declares its estimate for \u03b8, i.e. T\u03c0 = inf{t \u2208 R+ : \u03b8\u0302t \u2208 {0, 1}}. 2- A decision (estimate of \u03b8) \u03b8\u0302\u03c0: Given by \u03b8\u0302\u03c0 = \u03b8\u0302T\u03c0\u2227\u03c4 . 3- A random partition P \u03c0T\u03c0 : A realization of the point process (\u03b4t)t\u2208R+ , comprising a finite set of strictly increasing F -stopping times at which the decision-maker decides to sample the path X\u03c4 (t).\nA loss function is associated with every realization of the policy \u03c0, representing the overall cost incurred when following that policy for a specific path X\u03c4(t). The loss function is given by\n\u2113 (\u03c0; \u0398) , (C1 1{\u03b8\u0302\u03c0=0,\u03b8=1} \ufe38 \ufe37\ufe37 \ufe38\nType I error\n+Co 1{\u03b8\u0302\u03c0=1,\u03b8=0} \ufe38 \ufe37\ufe37 \ufe38\nType II error\n+Cd T\u03c0 \ufe38 \ufe37\ufe37 \ufe38\nDelay\n)1{T\u03c0\u2264\u03c4}+ Cr 1{T\u03c0>\u03c4} \ufe38 \ufe37\ufe37 \ufe38\nDeadline missed\n+CsN(P \u03c0 T\u03c0\u2227\u03c4 )\n\ufe38 \ufe37\ufe37 \ufe38\nInformation\n,\n(1) where C1 is the cost of type I error (failure to anticipate the adverse event), Co is the cost of type II error (falsely predicting that an adverse event will occur), Cd is the cost of the delay in declaring the estimate \u03b8\u0302\u03c0, Cr is the cost incurred when the adverse event occurs before an estimate \u03b8\u0302\u03c0 is declared (cost of missing the deadline), and Cs is the cost of every observation sample (cost of information). The risk of each policy \u03c0 is defined as its expected loss\nR(\u03c0) , E [\u2113 (\u03c0; \u0398)] , (2)\nwhere the expectation is taken over the paths of X\u03c4 (t). In the next section, we characterize the structure of the optimal policy \u03c0\u2217 = arg inf\u03c0\u2208\u03a0R(\u03c0)."}, {"heading": "3 Structure of the Optimal Policy", "text": "Since the decision-maker\u2019s posterior belief at time t, defined as \u00b5t = P(\u0398 = 1| F\u0303t), is an important statistic for designing sequential policies [10, 21-22], we start our characterization for \u03c0\u2217 by investigating the belief process (\u00b5t)t\u2208R+ ."}, {"heading": "3.1 The Posterior Belief Process", "text": "Recall that the decision-maker distills information from two types of observations: the realization of the partitioned time series X\u03c4(Pt) (i.e. the information in \u03c3(X\u03c4 (Pt))), and 2) the survival of the\n5Note that the cost of observing any local continuous path is infinite, hence any optimal policy must have (\u03b4t)t\u2208R+ being a point process to keep the number of observed samples finite.\nprocess up to time t (i.e. the information in St). In the following Theorem, we study the evolution of the decision-maker\u2019s beliefs as she integrates these pieces of information over time6.\nTheorem 1 (Information and beliefs). Every posterior belief trajectory (\u00b5t)t\u2208R+ associated with a policy \u03c0 \u2208 \u03a0 that creates a partition P \u03c0t \u2208 Pt of X \u03c4 (t) is a ca\u0300dla\u0300g path given by\n\u00b5t =\n{ 1, for t \u2265 \u03c4 (\n1 + 1\u2212pp dP\u0303o(P\n\u03c0 t )\ndP\u03031(P\u03c0t )\n)\u22121\n, for 0 \u2264 t < \u03c4\nwhere dP\u0303o(P \u03c0 t )\ndP\u03031(P\u03c0t ) is the Radon-Nikodym derivative7 of the measure P\u0303o(P \u03c0t ) with respect to P\u03031(P \u03c0 t ),\nand is given by the following elementary predictable process\n1 dP\u0303o(P\u03c0t ) dP\u03031(P\u03c0t ) =\nN(P\u03c0 t )\u22121\n\u2211\nk=1\nP(X(P \u03c0t )|\u0398 = 1) P(X(P \u03c0t )|\u0398 = 0) \ufe38 \ufe37\ufe37 \ufe38\nLikelihood ratio\nP(\u03c4 > t|\u03c3(X(P \u03c0t ),\u0398 = 1) \ufe38 \ufe37\ufe37 \ufe38\nSurvival probability\n1{P\u03c0 t (k)\u2264t\u2264P\u03c0 t (k+1)},\nfor t \u2265 P \u03c0t (1), and pP(\u03c4 > t|\u0398 = 1) for t < P \u03c0 t (k). Moreover, the path (\u00b5t)t\u2208R+ has exactly N(P \u03c0T\u03c0\u2227\u03c4 ) + 1{\u03c4<\u221e} jumps at the time indexes in P \u03c0 t\u2227\u03c4 \u222a {\u03c4}.\nProof: The posterior belief process (\u00b5t)t\u2208R+ is given by\n\u00b5t = P(\u0398 = 1|F\u0303t) (a) = P(\u0398 = 1|\u03c3(X(P \u03c0t )),St)\n= 1{t\u2265\u03c4} \u00b7 P(\u0398 = 1|\u03c3(X(P \u03c0 t )), t \u2265 \u03c4) + 1{t<\u03c4} \u00b7 P(\u0398 = 1|\u03c3(X(P \u03c0 t )), t < \u03c4)\n(b) = 1{t\u2265\u03c4} + 1{t<\u03c4} \u00b7 P(\u0398 = 1|\u03c3(X(P \u03c0 t )), t < \u03c4), (3)\nwhere we have used the fact that F\u0303t = \u03c3(X(P \u03c0t )) \u2228 St in (a), and the fact that the event {t \u2265 \u03c4} is F\u0303t-measurable in (b), and hence P(\u0398 = 1|\u03c3(X(P \u03c0t )), t \u2265 \u03c4) = 1. Therefore, we can write the posterior belief process (\u00b5t)t\u2208R+ in the following form\n\u00b5t = { 1, for t \u2265 \u03c4 P(\u0398 = 1|\u03c3(X(P \u03c0t )), t < \u03c4), for0 \u2264 t < \u03c4.\n6All proofs are provided in the supplementary material 7Since we impose the condition Po << P1 and fix a partition Pt, then the Radon-Nikodym derivative exists.\nNow we focus on computing P(\u0398 = 1|\u03c3(X(P \u03c0t )), t < \u03c4). Note that using Bayes\u2019 rule, we have that\nP(\u0398 = 1|\u03c3(X(P \u03c0t )), t < \u03c4) = P(\u0398 = 1, \u03c3(X(P \u03c0t )), t < \u03c4)\nP(\u03c3(X(P \u03c0t )), t < \u03c4)\n= P(\u0398 = 1, \u03c3(X(P \u03c0t )), t < \u03c4) \u2211\n\u03b8\u2208{0,1} P(\u0398 = \u03b8, \u03c3(X(P \u03c0 t )), t < \u03c4)\n= dP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 1)P(\u0398 = 1) \u2211\n\u03b8\u2208{0,1} dP(\u03c3(X(P \u03c0 t )), t < \u03c4 |\u0398 = \u03b8)P(\u0398 = \u03b8)\n= dP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 1)P(\u0398 = 1)\ndP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 0)P(\u0398 = 0) + dP(\u03c3(X(P \u03c0 t )), t < \u03c4 |\u0398 = 1)P(\u0398 = 1)\n= p dP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 1)\n(1\u2212 p) dP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 0) + p dP(\u03c3(X(P \u03c0 t )), t < \u03c4 |\u0398 = 1)\n=\n(\n1 + 1\u2212 p p \u00b7 dP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 0)\ndP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 1)\n)\u22121\n=\n(\n1 + 1\u2212 p p \u00b7 dP\u0303o(P\n\u03c0 t )\ndP\u03031(P \u03c0t )\n)\u22121\n, (4)\nwhere the existence of the Radon-Nykodim derivative dP\u0303o(P \u03c0 t )\ndP\u03031(P\u03c0t ) follows from the fact that\nP\u0303o(P \u03c0 t ) << P\u03031(P \u03c0 t ). Hence, we have that\n\u00b5t =\n{ 1, for t \u2265 \u03c4 (\n1 + 1\u2212pp \u00b7 dP\u0303o(P\n\u03c0 t )\ndP\u03031(P\u03c0t )\n)\u22121\n, for 0 \u2264 t < \u03c4.\nNow we focus on evaluating dP\u0303o(P \u03c0 t )\ndP\u03031(P\u03c0t ) . Using a further application of Bayes\u2019 rule we have that\n(\ndP\u0303o(P \u03c0 t ) dP\u03031(P \u03c0t )\n)\u22121\n= dP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 1)\ndP(\u03c3(X(P \u03c0t )), t < \u03c4 |\u0398 = 0)\n= P(t < \u03c4 |X(P \u03c0t ),\u0398 = 1) \u00b7 dP(X(P \u03c0 t )|\u0398 = 1)\nP(t < \u03c4 |X(P \u03c0t ),\u0398 = 0) \u00b7 dP(X(P \u03c0 t )|\u0398 = 0)\n= dP(X(P \u03c0t )|\u0398 = 1)\ndP(X(P \u03c0t )|\u0398 = 0) \u00b7 P(t < \u03c4 |X(P \u03c0t ),\u0398 = 1), (5)\nwhere we have used the fact that P(t < \u03c4 |X(P \u03c0t ),\u0398 = 0) = 1. For any partition P \u03c0 t , the likelihood ratio dP(X(P \u03c0 t )|\u0398=1)\ndP(X(P\u03c0 t )|\u0398=0) is an elementary predictable process that takes an initial value that is equal to\nthe prior p (when no samples are initially observed), and then takes constant values of dP(X(P \u03c0 t )|\u0398=1)\ndP(X(P\u03c0 t )|\u0398=0)\nin the interval between any two samples in the partition (only when a new sample is observed, the likelihood is updated). Hence, we have that\ndP(X(P \u03c0t )|\u0398 = 1) dP(X(P \u03c0t )|\u0398 = 0) = p1{t=0} +\nN(P\u03c0 t )\u22121\n\u2211\nk=1\nP(X(P \u03c0t )|\u0398 = 1) P(X(P \u03c0t )|\u0398 = 0) 1{P\u03c0 t (k\u22121)\u2264t\u2264P\u03c0 t (k)}.\nThe process is predictable since the likelihood remains constant as long as no new samples are observed. Modulated by the survival probability, ( dP\u0303o(P \u03c0 t )\ndP\u03031(P\u03c0t )\n)\u22121\ncan be written as\npP(\u03c4 > t|\u0398 = 1)1{t<P\u03c0 t (k)}+\nN(P\u03c0 t )\u22121\n\u2211\nk=1\nP(X(P \u03c0t )|\u0398 = 1) P(X(P \u03c0t )|\u0398 = 0) P(\u03c4 > t|\u03c3(X(P \u03c0t ),\u0398 = 1) 1{P\u03c0t (k)\u2264t\u2264P\u03c0t (k+1)}.\nUnder usual regularity conditions on P(\u03c4 > t|\u03c3(X(P \u03c0t ),\u0398 = 1) it is easy to see that ( dP\u0303o(P \u03c0 t )\ndP\u03031(P\u03c0t )\n)\u22121\nwill have jumps only at the time instances in the partition P \u03c0t and at the stopping time \u03c4 , i.e. a total of N(P \u03c0T\u03c0\u2227\u03c4 ) + 1{\u03c4<\u221e} jumps at the time indexes in P \u03c0 t\u2227\u03c4 \u222a {\u03c4}.\nTheorem 1 says that every belief path is right-continuous with left limits, and has jumps at the time indexes in the partition P \u03c0t , whereas between each two jumps, the paths (\u00b5t)t\u2208[t1,t2), t1, t2 \u2208 P \u03c0 t are predictable (i.e. they are known ahead of time once we know the magnitudes of the jumps preceding them). This means that the decision-maker obtains \u201dactive\u201d information by probing the time series to observe new samples (i.e. the information in \u03c3(X\u03c4 (Pt))), inducing jumps that revive her beliefs, whereas the progression of time without witnessing a stopping event offers the decision-maker \u201dpassive information\u201d that is distilled just from the costless observation of the process\u2019 survival. Both sources of information manifest themselves in terms of the likelihood ratio, and the survival probability in the expression of dP\u0303o(P \u03c0 t )\ndP\u03031(P\u03c0t ) above.\nIn Figure 2, we plot the ca\u0300dla\u0300g belief paths for policies \u03c01 and \u03c02, where P \u03c01 \u2282 P \u03c02 (i.e. policy \u03c01 observe a subset of the samples observed by \u03c02). We also plot the (predictable) belief path of a wait-and-watch policy that observes no samples. We can see that \u03c02, which has more jumps of \u201dactive information\u201d, copes faster with the truthful belief over time. Between each two jumps, the belief process exhibits a non-increasing predictable path until fed with a new piece of information. The wait-and-watch policy has its belief drifting away from the prior p = 0.5 towards the wrong belief \u00b5t = 0 since it only distills information from the process survival, which favors the hypothesis \u0398 = 0. This discussion motivates the introduction of the following key quantities.\nInformation gain (surprise) It(\u2206t): The amount of drift in the decision-maker\u2019s belief at time t + \u2206t with respect to her belief at time t, given the information available up to time t, i.e. It(\u2206t) = (\u00b5t+\u2206t \u2212 \u00b5t) |F\u0303t.\nPosterior survival function (suspense) St(\u2206t): The probability that a process generated with \u0398 = 1 survives up to time t + \u2206t given the information observed up to time t, i.e. St(\u2206t) = P(\u03c4 > t + \u2206t|F\u0303t,\u0398 = 1). The function St(\u2206t) is a non-increasing function in \u2206t, i.e. \u2202St(\u2206t)\n\u2202\u2206t \u2264 0.\nThat is, the information gain is the amount of \u201csurprise\u201d that the decision-maker experiences in response to a new information sample expressed in terms of the change in here belief, i.e. the jumps in \u00b5t, whereas the survival probability (suspense) is her assessment for the risk of having the adverse event taking places in the next \u2206t time interval. As we will see in the next subsection, the optimal policy would balance the two quantities when scheduling the times to sense X\u03c4 (t).\nWe conclude our analysis for the process \u00b5t by noting that the lack of information samples creates bias towards the belief that \u0398 = 0 (e.g. see the belief path of the wait-and-watch policy in Figure 2). We formally express this behavior in the following Corollary.\nCorollary 1 (Leaning towards denial). For every policy \u03c0 \u2208 \u03a0, the posterior belief process \u00b5t is a supermartingale with respect to F\u0303t, where\nE[\u00b5t+\u2206t|F\u0303t] = \u00b5t \u2212 \u00b5 2 tSt(\u2206t)(1 \u2212 St(\u2206t)) \u2264 \u00b5t, \u2200\u2206t \u2208 R+.\nProof: Recall that from Theorem 1, we know that the posterior belief process can be written as\n\u00b5t = 1{t\u2265\u03c4} + 1{t<\u03c4}P(\u0398 = 1|F\u0303t).\nHence, the expected posterior belief at time t+\u2206t given the information in the filtration F\u0303t can be written as\nE\n[\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] = E [ 1{t+\u2206t\u2265\u03c4} + 1{t+\u2206t<\u03c4}P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t ]\n= E [\n1{t+\u2206t\u2265\u03c4} \u2223 \u2223 \u2223F\u0303t ] + E [ 1{t+\u2206t<\u03c4}P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t ]\n= P(\u0398 = 1, t+\u2206t \u2265 \u03c4 |F\u0303t) + P(t+\u2206t < \u03c4 |F\u0303t) \u00b7 E [ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ] ,\n(6)\nand hence E [\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] can be written as\nP(t+\u2206t \u2265 \u03c4 |F\u0303t,\u0398 = 1)\u00b7P(\u0398 = 1|F\u0303t)+P(t+\u2206t < \u03c4 |F\u0303t)\u00b7E [ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ] ,\nwhich is equivalent to\nE\n[\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] = (1\u2212 St(\u2206t)) \u00b7 \u00b5t + P(t+\u2206t < \u03c4 |F\u0303t) \u00b7 E [ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ] .\n(7)\nFurthermore, the term P(t+\u2206t < \u03c4 |F\u0303t) in the expression above can be expressed as\nP(t+\u2206t < \u03c4 |F\u0303t) = P(t+\u2206t < \u03c4 |F\u0303t,\u0398 = 1) \u00b7 P(\u0398 = 1|F\u0303t) + P(t+\u2206t < \u03c4 |F\u0303t,\u0398 = 0) \u00b7 P(\u0398 = 0|F\u0303t) (8)\n= St(\u2206t) \u00b7 \u00b5t + (1\u2212 \u00b5t).\nTherefore, E [\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] can be written as\nE\n[\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] = (1\u2212 St(\u2206t)) \u00b7 \u00b5t + (1\u2212 \u00b5t + St(\u2206t) \u00b7 \u00b5t) \u00b7 E [ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ] .\n(9)\nNow it remains to evaluate the term E [ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ] in order to find\nE\n[\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] . We first note that\nE\n[ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ] = E [ P(\u0398 = 1|\u03c3(X\u03c4 (P \u03c0t+\u2206t)), t+\u2206t < \u03c4) \u2223 \u2223 \u2223F\u0303t ] .\nWe start evaluating the above by first looking at the term P(\u0398 = 1|\u03c3(X\u03c4 (P \u03c0t+\u2206t)), t + \u2206t < \u03c4). Using Bayes\u2019 rule, we have that\nP(\u0398 = 1|X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) = P(\u0398 = 1, X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4)\nP(X\u03c4 (P \u03c0t+\u2206t), t+\u2206t < \u03c4) , (10)\nwhere P(\u0398 = 1, X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) can be expanded using successive applications of Bayes\u2019 rule as\nP(\u0398 = 1|X\u03c4(P \u03c0t ), t < \u03c4) \u00b7 P(X \u03c4 (P \u03c0t ), t < \u03c4) \u00b7 P(t+\u2206t < \u03c4 |\u0398 = 1, X \u03c4(P \u03c0t ), t < \u03c4)\n\u00b7dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4),\nwhich is equivalent to\nP(\u0398 = 1, X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) = \u00b5t \u00b7 St(\u2206t) \u00b7 P(X \u03c4 (P \u03c0t ), t < \u03c4) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4) (11)\nSimilarly, it is easy to see that\nP(\u0398 = 0, X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) = (1\u2212 \u00b5t) \u00b7 P(X \u03c4 (P \u03c0t ), t < \u03c4) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = 0, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4), (12)\nwhere again, we have used the fact that P(t + \u2206t < \u03c4 |\u0398 = 0, X\u03c4(P \u03c0t ), t < \u03c4) = 1. Now we re-formulate (10) using Bayes rule to arrive at the following\nP(\u0398 = 1|X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) = P(\u0398 = 1, X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) \u2211\n\u03b8\u2208{0,1} P(\u0398 = \u03b8,X \u03c4 (P \u03c0t+\u2206t), t+\u2206t < \u03c4)\n, (13)\nthen using (11) and (12), (13) can be further reduced to P(\u0398 = 1|X\u03c4(P \u03c0t+\u2206t), t+\u2206t < \u03c4) =\n\u00b5t \u00b7 St(\u2206t) \u00b7 dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4)\n\u00b5t \u00b7 St(\u2206t) \u00b7 dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4) + (1\u2212 \u00b5t) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = 0, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4)\n.\n(14)\nFinally, we use the expression in (14) to evaluate the term\nE\n[ P(\u0398 = 1|\u03c3(X\u03c4 (P \u03c0t+\u2206t)), t+\u2206t < \u03c4) \u2223 \u2223 \u2223F\u0303t ] as follows\nE\n[ P(\u0398 = 1|\u03c3(X\u03c4 (P \u03c0t+\u2206t)), t+\u2206t < \u03c4) \u2223 \u2223 \u2223F\u0303t ] =\n\u2211 \u03b8\u2208{0,1}\n\u222b\nP(\u0398 = 1|X\u03c4 (P \u03c0t+\u2206t), t+\u2206t < \u03c4) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = \u03b8,X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4),\nwhich, using (14), can be written as \u2211\n\u03b8\u2208{0,1}\n\u222b \u00b5t \u00b7 St(\u2206t) \u00b7 dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = \u03b8,X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4)\n\u00b5t \u00b7 St(\u2206t) \u00b7 dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4) + (1\u2212 \u00b5t) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = 0, X\u03c4(P \u03c0t ), t+\u2206t\nSince \u2211\n\u03b8\u2208{0,1}\ndP(X\u03c4 (t+\u2206t)|\u0398 = \u03b8,X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4) =\n\u00b5t \u00b7St(\u2206t) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4)+(1\u2212\u00b5t)\u00b7dP(X \u03c4 (t+\u2206t)|\u0398 = 0, X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4),\nthen the integral above reduces to \u222b\n\u00b5t \u00b7St(\u2206t) \u00b7dP(X \u03c4 (t+\u2206t)|\u0398 = \u03b8,X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4) = \u00b5t \u00b7St(\u2206t)\u00b7\n\u222b\ndP(X\u03c4 (t+\u2206t)|\u0398 = \u03b8,X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4),\nand since the conditional density integrates to 1, i.e. \u222b dP(X\u03c4 (t+\u2206t)|\u0398 = \u03b8,X\u03c4 (P \u03c0t ), t+\u2206t < \u03c4) = 1, then we have that\nE\n[ P(\u0398 = 1|\u03c3(X\u03c4 (P \u03c0t+\u2206t)), t+\u2206t < \u03c4) \u2223 \u2223 \u2223F\u0303t ] = \u00b5t \u00b7 St(\u2206t).\nBy substituting the above in (9), we arrive at\nE\n[\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] = (1\u2212 St(\u2206t)) \u00b7 \u00b5t + (1\u2212 \u00b5t + St(\u2206t) \u00b7 \u00b5t) \u00b7 E [ P(\u0398 = 1|F\u0303t+\u2206t) \u2223 \u2223 \u2223F\u0303t \u2228 {t+\u2206t < \u03c4} ]\n= (1\u2212 St(\u2206t)) \u00b7 \u00b5t + (1\u2212 \u00b5t + St(\u2206t) \u00b7 \u00b5t) \u00b7 \u00b5t \u00b7 St(\u2206t)\n= \u00b5t \u2212 \u00b5 2 tSt(\u2206t)(1 \u2212 St(\u2206t)). (15)\nSince St(\u2206t) \u2265 0, \u2200t,\u2206t \u2208 R+, then the term \u00b52tSt(\u2206t)(1 \u2212 St(\u2206t)) \u2265 0, and it follows that\nE\n[\n\u00b5t+\u2206t \u2223 \u2223 \u2223F\u0303t ] \u2264 \u00b5t, \u2200t,\u2206t \u2208 R+,\nand hence the posterior belief process (\u00b5t)t\u2208R+ is a supermartingale with respect to the filtration F\u0303t.\nThus, unlike classical Bayesian learning models with a belief martingale [18, 21-23], the belief process in our model is a supermartingale that leans toward decreasing over time. The reason for this is that in our model, time conveys information. That is, unlike [10] and [15] where the decision deadline is hypothesis-independent and is almost surely occurring in finite time for any path, in our model the occurrence of the adverse event is itself a hypothesis, hence observing the survival of the process is informative and contributes to the evolution of the belief. The informativeness of both the acquired information samples and process survival can be disentangled using Doob decomposition, by writing \u00b5t as \u00b5t = \u00b5\u0303t +A(\u00b5t, St(\u2206t)), where \u00b5\u0303t is a martingale, capturing the information gain from the acquired samples, and A(\u00b5t, St(\u2206t)) is a predictable compensator process [23], capturing information extracted from the process survival."}, {"heading": "3.2 The Optimal Policy", "text": "The optimal policy \u03c0\u2217 minimizes the expected risk as defined in (1) and (2) by generating the tuple of random processes (T\u03c0, \u03b8\u0302\u03c0, P \u03c0t ) in response to the paths of X\n\u03c4 (t) on (\u2126,F , {Ft}t\u2208R+ ,P) in a way that \u201dshapes\u201d a belief process \u00b5t that maximizes informativeness, maintains timeliness and controls cost. In the following, we introduce the notion of a \u201drendezvous policy\u201d, then in Theorem 2, we show that the optimal policy \u03c0\u2217 complies with this definition.\nRendezvous policies We say that a policy \u03c0 is a rendezvous policy, if the random partition P \u03c0T\u03c0 constructed by the sequence of sensing actions (\u03b4\u03c0t )t\u2208[0,T\u03c0], is a point process with predictable jumps, where for every two consecutive jumps at times t and t \u2032 , with t \u2032 > t and t, t \u2032\n\u2208 P \u03c0T\u03c0 , we\nhave that t \u2032\nis F\u0303t-measurable.\nThat is, a rendezvous policy is a policy that constructs a sensing schedule (\u03b4\u03c0t )t\u2208[0,T\u03c0 ], such that every time t \u2032\nat which the decision-maker acquires information is actually computable using the information available up to time t, the previous time instance at which information was gathered. Hence, the decision-maker can decide the next \u201ddate\u201d in which she will gather information directly after she senses a new information sample. This structure is a natural consequence of the information structure in Theorem 1, since the belief paths between every two jumps are predictable, then they convey no \u201dactionable\u201d information, i.e. if the decision-maker was to respond to a predictable belief path, say by sensing or making a stopping decision, then she should have taken that decision right before the predictable path starts, which leads her to better off by saving the delay cost Cd. We denote the space of all rendezvous policies by \u03a0r. In the following Theorem, we establish that the rendezvous structure is optimal.\nTheorem 2 (Rendezvous). The optimal policy \u03c0\u2217 is a rendezvous policy (\u03c0\u2217 \u2208 \u03a0r).\nProof: Assume a discrete-time version of the problem, where the decision (\u03b8\u0302\u03c0t , \u03b4 \u03c0 t ) are made in time steps {0,\u2206t, 2\u2206t, . . .}. Define a value function V : N \u00d7 [0, 1] \u2192 R+ as a map from the current history to the risk of the best policy given the history F\u0303t as follows:\nV (F\u0303t) , inf(\u03b8\u0302\u03c0,T\u03c0\u2265t,P\u03c0T\u03c0\u2283P \u03c0 t )E\n[ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303t ] ,\nand define the action-value function as the value function achieved by taking actions (\u03b8\u0302t, \u03b4t), andthen following the best policy thereafter. That is, when the decision is to continue (i.e. \u03b8\u0302t = \u2205), we have that\nQ(F\u0303t; (\u03b8\u0302t = \u2205, \u03b4t = 1)) , inf(\u03b8\u0302\u03c0,T\u03c0\u2265t,P\u03c0T\u03c0\u2283P \u03c0 t ,t\u2208P\u03c0 T\u03c0 )E\n[ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303t ] ,\nand Q(F\u0303t; (\u03b8\u0302t = \u2205, \u03b4t = 0)) , inf(\u03b8\u0302\u03c0,T\u03c0\u2265t,P\u03c0T\u03c0\u2283P \u03c0 t ,t/\u2208P\u03c0 T\u03c0 )E [ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303t ] .\nBased on Bellmans optimality principle [24], we know that the optimal policy has to satisfy the following in every time step, i.e.\n\u03b4\u03c0 \u2217\nt = arg inf\u03b4t\u2208{0,1}Q(F\u0303t; (\u03b8\u0302t = \u2205, \u03b4t)).\nNow let us look at the optimal partition on P \u03c0 \u2217\nT\u03c0\u2217 on the discrete time steps {0,\u2206t, 2\u2206t, . . .},\nand look at an arbitrary realization for P \u03c0 \u2217\nT\u03c0\u2217 . Then we pick two consecutive time indexes in\n{0,\u2206t, 2\u2206t, . . .}, say n1\u2206t and n2\u2206t, with n1 < n2, for which \u03b4\u03c0 \u2217 n1\u2206t = \u03b4\u03c0 \u2217 n2\u2206t = 1, and \u03b4\u03c0 \u2217\nn\u2206t = 0, \u2200n1 < n < n2. Since the policy is optimal, we know that\narg inf\u03b4n\u2206t\u2208{0,1}Q(F\u0303n\u2206t; (\u03b8\u0302n\u2206t = \u2205, \u03b4n\u2206t)) = 0, \u2200n1 < n < n2,\nand arg inf\u03b4n2\u2206t\u2208{0,1}Q(F\u0303n2\u2206t; (\u03b8\u0302n2\u2206t = \u2205, \u03b4n2\u2206t)) = 1,\nwhich is equivalent to\narg inf\u03b4n\u2206t\u2208{0,1}E [ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303n\u2206t ] = 0, \u2200n1 < n < n2,\nand arg inf\u03b4n2\u2206t\u2208{0,1}E [ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303n2\u2206t ] = 1,\nwhich can be further decomposed into\narg inf\u03b4n\u2206t\u2208{0,1}E [ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223\u03c3(X(P \u03c0 \u2217 n1\u2206t)) \u2228 Sn\u2206t ] = 0, \u2200n1 < n < n2,\nand arg inf\u03b4n2\u2206t\u2208{0,1}E [ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223\u03c3(X(P \u03c0 \u2217 n1\u2206t)) \u2228 Sn2\u2206t ] = 1, since both functions E [ \u2113(\u03c0; \u0398) \u2223 \u2223\u03c3(X(P \u03c0 \u2217\nn1\u2206t )) \u2228 Sn\u2206t\n] and E [ \u2113(\u03c0; \u0398) \u2223 \u2223\u03c3(X(P \u03c0 \u2217\nn1\u2206t )) \u2228 Sn2\u2206t\n]\nare F\u0303n1\u2206t-measurable, then the decision-maker can compute the optimal decision sequence {\u03b4n\u2206t}\nn2 n=n1+1 at time n1\u2206t. Since this holds for an arbitrary discretization step \u2206t, including\nan arbitrarily small step \u2206t \u2192 0, it follows that the sensing actions construct a predictable point process under the optimal policy, which concludes the Theorem.\nA direct implication of Theorem 2 is that the time variable can now be viewed as a state variable, whereas the problem is virtually solved in \u201ddiscrete-time\u201d since the decision-maker effectively jumps from one time instance to another in a discrete manner. Hence, we alter the definition of the action \u03b4t from an indicator variable that indicates sensing the time series at time t, to a \u201drendezvous action\u201d that takes real values, and specifies the time after which the decision-maker would sense a new sample, i.e. if \u03b4t = \u2206t, then the decision-maker gathers the new sample at t + \u2206t. This transformation restricts our policy design problem to the space of rendezvous policies \u03a0r, which we know from Theorem 2 that it contains the optimal policy (i.e. \u03c0\u2217 = arg inf\u03c0\u2208\u03a0rR(\u03c0)).\nHaving established the result in Theorem 2, in the following Theorem, we characterize the optimal policy \u03c0\u2217 in terms of the random process (T\u03c0\u2217 , \u03b8\u0302\u03c0\u2217 , P \u03c0 \u2217\nt ) using discrete-time Bellman optimality conditions [24].\nTheorem 3 (The optimal policy). The optimal policy \u03c0\u2217 is a sequence of actions (\u03b8\u0302\u03c0 \u2217 t , \u03b4 \u03c0\u2217 t )t\u2208R+ , resulting in a random process (\u03b8\u0302\u03c0\u2217 , T\u03c0\u2217 , P \u03c0 \u2217\nT\u03c0\u2217 ) with the following properties:\n(Continuation and stopping)\n1. The process (t, \u00b5t, X\u0304(P \u03c0 \u2217\nt ))t\u2208R+ is a Markov sufficient statistic for the distribution of (\u03b8\u0302\u03c0\u2217 , T\u03c0\u2217 , P \u03c0\u2217\nT\u03c0\u2217 ), where X\u0304(P \u03c0\n\u2217 t ) is the most recent sample in the partition P \u03c0\u2217 t , i.e.\nX\u0304(P \u03c0 \u2217 t ) = X(t \u2217), t\u2217 = maxP \u03c0 \u2217 t .\n2. The policy \u03c0\u2217 recommends continuation, i.e. \u03b8\u0302\u03c0 \u2217\nt = \u2205, as long as the belief \u00b5t \u2208 C(t, X\u0304(P \u03c0 \u2217\nt )), where C(t, X\u0304(P \u03c0\u2217\nt )), is a time and context-dependent continuation set with the following properties: C(t \u2032 , X) \u2282 C(t,X), \u2200t \u2032 > t, and C(t,X \u2032 ) \u2282 C(t,X), \u2200X \u2032 > X .\n(Rendezvous and decisions)\n1. Whenever \u00b5t \u2208 C(t, X\u0304(P \u03c0 \u2217 t )), and t \u2208 P \u03c0\u2217 T\u03c0\u2217 , then a rendezvous \u03b4\u03c0 \u2217 t is set as follows\n\u03b4\u03c0 \u2217\nt = arg inf\u03b4\u2208R+ ((C1 \u2212 Co)P(It(\u03b4) \u2265 \u03b7t) + C1) St(\u03b4) + Cr (1\u2212 St(\u03b4)),\nwhere \u03b7t = C1Co+C1 \u2212 \u00b5t.\n2. Whenever \u00b5t /\u2208 C(t, X\u0304(P \u03c0 \u2217 t )), then a decision \u03b8\u0302 \u03c0\u2217\nt = \u03b8\u0302\u03c0\u2217 \u2208 {0, 1} is issued, and is based on a belief threshold as follows: \u03b8\u0302\u03c0\u2217 = 1{\u00b5t\u2265 C1Co+C1 }. The stopping time is given by T\u03c0\u2217 = inf{t \u2208 R+ : \u00b5t /\u2208 C(t, X\u0304(P \u03c0 \u2217 t ))}.\nProof: We start by proving that the optimal decision rule is 1{ \u00b5t>\nC1 Co+C1\n}. Fix an optimal stopping\ntime T\u03c0\u2217 and an optimal partition P \u03c0 \u2217\nT\u03c0\u2217 . The optimal decision rule is given by\n\u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223P \u03c0 \u2217 T\u03c0\u2217 , T\u03c0\u2217 ] ,\nwhich is equivalent to \u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ (C1 1{\u03b8\u0302\u03c0=0,\u03b8=1} + Co 1{\u03b8\u0302\u03c0=1,\u03b8=0} + Cd T\u03c0\u2217)1{T\u03c0\u2217\u2264\u03c4} + Cr 1{T\u03c0\u2217>\u03c4} + CsN(P \u03c0\u2217 T\u03c0\u2217\u2227\u03c4 ) ] ,\nwhich by smoothing can be written as \u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ E [ (C1 1{\u03b8\u0302\u03c0=0,\u03b8=1} + Co 1{\u03b8\u0302\u03c0=1,\u03b8=0} + Cd T\u03c0\u2217)1{T\u03c0\u2217\u2264\u03c4} + Cr 1{T\u03c0\u2217>\u03c4} + CsN(P \u03c0\u2217 T\u03c0\u2217\u2227\u03c4 ) \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]] ,\nand hence we have that\n\u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ E [ (C1 1{\u03b8\u0302\u03c0=0,\u03b8=1} + Co 1{\u03b8\u0302\u03c0=1,\u03b8=0} + Cd T\u03c0\u2217)1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] +\nE\n[\nCr 1{T\u03c0\u2217>\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] + E [ CsN(P \u03c0\u2217 T\u03c0\u2217\u2227\u03c4 ) \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]] .\nSince the terms E [\nCr 1{T\u03c0\u2217>\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] , E [ Cd T\u03c0\u2217 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] , and\nE\n[\nCsN(P \u03c0\u2217 T\u03c0\u2217\u2227\u03c4 ) \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] are the information and delay costs, which do not depend on the\nchoice of \u03b8\u0302\u03c0, we have that\n\u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ E [ (C1 1{\u03b8\u0302\u03c0=0,\u03b8=1} + Co 1{\u03b8\u0302\u03c0=1,\u03b8=0})1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]] ,\nwhich can be reduced to the following\n\u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ E [ (C1 1{\u03b8\u0302\u03c0=0,\u03b8=1} + Co 1{\u03b8\u0302\u03c0=1,\u03b8=0})1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]]\n= arg inf\u03b8\u0302\u03c0E [ C1 \u00b7 E [ 1{\u03b8\u0302\u03c0=0,\u03b8=1} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] + Co \u00b7 E [ 1{\u03b8\u0302\u03c0=1,\u03b8=0} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]] = arg inf\u03b8\u0302\u03c0E [ C1 \u00b7 E [ 1{\u03b8\u0302\u03c0=0} \u00b7 1{\u03b8=1} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] + Co \u00b7 E [ 1{\u03b8\u0302\u03c0=1} \u00b7 1{\u03b8=0} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]] .\n(16)\nSince 1{\u03b8\u0302\u03c0=\u03b8} is an F\u0303T\u03c0\u2217 -measurable function, we have that\n\u03b8\u0302\u03c0\u2217 = arg inf\u03b8\u0302\u03c0E [ C1 \u00b7 E [ 1{\u03b8\u0302\u03c0=0} \u00b7 1{\u03b8=1} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] + C1 \u00b7 E [ 1{\u03b8\u0302\u03c0=1} \u00b7 1{\u03b8=0} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]]\n= arg inf\u03b8\u0302\u03c0E [ C1 \u00b7 1{\u03b8\u0302\u03c0=0} \u00b7 E [ 1{\u03b8=1} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ] + Co \u00b7 1{\u03b8\u0302\u03c0=1} \u00b7 E [ 1{\u03b8=0} \u00b7 1{T\u03c0\u2217\u2264\u03c4} \u2223 \u2223 \u2223F\u0303T\u03c0\u2217 ]] = arg inf\u03b8\u0302\u03c0E [ C1 \u00b7 1{\u03b8\u0302\u03c0=0} \u00b7 (1\u2212 \u00b5T\u03c0\u2217 ) + Co \u00b7 1{\u03b8\u0302\u03c0=1} \u00b7 \u00b5T\u03c0\u2217 ] = arg inf\u03b8\u0302\u03c0E [ C1 \u00b7 1{\u03b8\u0302\u03c0=0} \u00b7 (1\u2212 \u00b5T\u03c0\u2217 ) + Co \u00b7 1{\u03b8\u0302\u03c0=1} \u00b7 \u00b5T\u03c0\u2217 ] , (17)\nwhich is simply minimized by setting \u03b8\u0302\u03c0 = 1 whenever C1(1 \u2212 \u00b5T\u03c0\u2217 ) > Co\u00b5T\u03c0\u2217 , hence we have that \u03b8\u0302\u03c0 = 1{}.\nNow we resume by first defining the value and the action-value functions, and find the policy characteristics under Bellman optimality conditions.\nDefine a value function V : N\u00d7 [0, 1] \u2192 R+ as a map from the current history to the risk of the best policy given the history F\u0303t as follows:\nV (F\u0303t) , inf(\u03b8\u0302\u03c0,T\u03c0\u2265t,P\u03c0T\u03c0\u2283P \u03c0 t )E\n[ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303t ] ,\nand define the action-value function as the value function achieved by taking actions (\u03b8\u0302t, \u03b4t), and then following the best policy thereafter, i.e.\nQ(F\u0303t; (\u03b8\u0302t, \u03b4t)) , inf(\u03b8\u0302\u03c0,T\u03c0\u2265t+\u03b4t,P\u03c0T\u03c0\u2283P \u03c0 t \u222a{t+\u03b4t}) E\n[ \u2113(\u03c0; \u0398) \u2223 \u2223 \u2223F\u0303t ] .\nBellman optimality condition requires that at any time step t, we have\n(\u03b8\u0302\u03c0 \u2217 t , \u03b4 \u03c0\u2217 t ) = arg inf(\u03b8\u0302t,\u03b4t)\u2208{0,1}\u00d7R+Q(F\u0303t; (\u03b8\u0302t, \u03b4t)).\nRecall from the proof of Corollary 1 that the belief process follows the following dynamics\n\u00b5t+\u2206t =\n\u00b5t \u00b7 St(\u2206t) \u00b7 dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4)\n\u00b5t \u00b7 St(\u2206t) \u00b7 dP(X\u03c4 (t+\u2206t)|\u0398 = 1, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4) + (1\u2212 \u00b5t) \u00b7 dP(X \u03c4 (t+\u2206t)|\u0398 = 0, X\u03c4(P \u03c0t ), t+\u2206t < \u03c4)\n,\nwhich depends only on \u00b5t and the most recent sample realization in the partition P \u03c0t , which we denote as X\u0304\u03c4 (P \u03c0t ). Hence, the tuple (t, \u00b5t, X\u0304 \u03c4 (P \u03c0t )) is a Markov process since X \u03c4 (t) is Markovian, and the belief process follows the above Markovian dynamics, and time is deterministic. Since the survival probability depends only on X\u0304\u03c4 (P \u03c0t ), we can write the action-value function as\nQ(F\u0303t; (\u03b8\u0302t, \u03b4t)) , inf(\u03b8\u0302\u03c0,T\u03c0\u2265t+\u03b4t,P\u03c0T\u03c0\u2283P \u03c0 t \u222a{t+\u03b4t})\nE [ \u2113(\u03c0; \u0398) \u2223 \u2223\u00b5t, X\u0304 \u03c4 (P \u03c0t ) ] ,\nand consequently, the optimal actions at every time step t following Bellman conditions are given by\n(\u03b8\u0302\u03c0 \u2217 t , \u03b4 \u03c0\u2217\nt ) = arg inf(\u03b8\u0302t,\u03b4t)\u2208{0,1}\u00d7R+ inf(\u03b8\u0302\u03c0,T\u03c0\u2265t+\u03b4t,P\u03c0T\u03c0\u2283P \u03c0 t \u222a{t+\u03b4t})\nE [ \u2113(\u03c0; \u0398) \u2223 \u2223\u00b5t, X\u0304 \u03c4 (P \u03c0t ) ] .\nHence, at any time step t, we only need to know the tuple (t, \u00b5t, X\u0304\u03c4 (P \u03c0t )) in order to compute the optimal action-value function, and hence, on the path to the optimal policy, knowing only (t, \u00b5t, X\u0304 \u03c4 (P \u03c0t )) suffice to generate the random process (T\u03c0\u2217 , P \u03c0\u2217 T\u03c0\u2217 , \u03b8\u0302\u03c0\u2217). Hence, (t, \u00b5t, X\u0304\u03c4(P \u03c0t )) is a Markov sufficient statistic for (T\u03c0\u2217 , P \u03c0 \u2217\nT\u03c0\u2217 , \u03b8\u0302\u03c0\u2217).\nNote that our proof for the optimal decision rule \u03b8\u0302\u03c0\u2217 implies that the action-value function for stopping at time t, i.e. \u03b8\u0302\u03c0 \u2217\nt 6= \u2205 is\nQ(t, \u00b5t, X\u0304 \u03c4 (P \u03c0t ); (\u03b8\u0302t 6= \u2205, \u03b4t)) = Co\u00b5t \u2227 C1(1 \u2212 \u00b5t) + Cd t+ CsN(P \u03c0 t ),\nwhereas the continuation cost at any time step t is given by finding the optimal rendezvous time inf\u03b4t\u2208R+Q(t, \u00b5t, X\u0304\n\u03c4(P \u03c0t )); (\u03b8\u0302t = \u2205, \u03b4t)). Therefore, the optimal action-value at any time step t is given by\nQ\u2217(t, \u00b5t, X\u0304 \u03c4 (P \u03c0t ); (\u03b8\u0302t 6= \u2205, \u03b4t)) = min{Co\u00b5t\u2227C1(1\u2212\u00b5t)+Cd t+CsN(P \u03c0 t ), inf\u03b4t\u2208R+Q(t, \u00b5t, X\u0304 \u03c4 (P \u03c0t ); (\u03b8\u0302t = \u2205, \u03b4t))}.\nThe equation above determines the stopping and continuation conditions, and using the monotonicity of the survival function in both time t and the time series realizations X\u0304\u03c4 (P \u03c0t ), we can show the monotonicity of the continuation set C(t, X\u0304\u03c4(P \u03c0t )) using the same arguments of Theorem 1 in [15].\nThe optimal rendezvous can be found by optimizing the time interval such that the cost of stopping in the next time step is minimized. Hence, we have that\n\u03b4\u03c0 \u2217 t = inf\u03b4t\u2208R+Q(t, \u00b5t, X\u0304 \u03c4 (P \u03c0t ); (\u03b8\u0302t = \u2205, \u03b4t))\n= inf\u03b4t\u2208R+E [ (Co\u00b5t+\u03b4t \u2227 C1(1\u2212 \u00b5t+\u03b4t) + Cd t+ \u03b4t) 1{t+\u03b4t<\u03c4} + Cr1{t+\u03b4t\u2265\u03c4} + CsN(P \u03c0 t ) + 1 \u2223 \u2223 \u2223F\u0303t ]\n= inf\u03b4t\u2208R+\n(\n(C1 \u2212 Co)P(\u00b5t+\u2206t \u2265 C1\nCo + C1 ) + C1\n)\nSt(\u03b4t) + Cr(1 \u2212 St(\u03b4t)), (18)\nwhere P(\u00b5t+\u2206t \u2265 C1Co+C1 ) can be written as P(It(\u2206t) \u2265 C1 Co+C1 \u2212\u00b5t), where It(\u2206t) = \u00b5t+\u2206t\u2212\u00b5t is the information gain.\nTheorem 3 establishes the structure of the optimal policy and its prescribed actions in the decisionmaker\u2019s state-space. The first part of the Theorem says that in order to generate the random tuple (T\u03c0\u2217 , \u03b8\u0302\u03c0\u2217 , P \u03c0 \u2217\nt ) optimally, we only need to keep track of the realization of the process (t, \u00b5t, X\u0304(Pt))t\u2208R+ in every time instance. That is, an optimal policy maps the current belief, the current time, and the most recently observed realization of the time series to an action tuple (\u03b8\u0302\u03c0t , \u03b4 \u03c0 t ), i.e. a decision on whether to stop and declare an estimate for \u03b8 or sense a new sample. Hence, the process (t, \u00b5t, X\u0304(Pt))t\u2208R+ represents the \u201dstate\u201d of the decision-maker, and the decision-maker\u2019s actions can partially influence the state through the belief process, i.e. a decision on when to acquire the next sample affects the distributional properties of the posterior belief. The remaining state variables t and X(t) are beyond the decision-maker\u2019s control.\nWe note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent. That is, since the state is (t, \u00b5t, X\u0304(P \u03c0t )) and not just the time-belief tuple (t, \u00b5t), a policy \u03c0 can recommend different actions for the same belief and at the same time but for a different context. This is because, while \u00b5t captures what the decision-maker learned from the history, X\u0304(P \u03c0t ) captures her foresightedness into the future, i.e. it can be that the belief \u00b5t is not decisive (e.g. \u00b5t \u2248 p), but the context is \u201drisky\u201d (i.e. X\u0304(P \u03c0t ) is large), which means that a potential forthcoming adverse event is likely to happen in the near future, hence the decision-maker would be more eager to make a stopping decision and declare an estimate \u03b8\u0302\u03c0. This is manifested through the dependence of the continuation set C(t, X\u0304(P \u03c0t )) on both time and context; the continuation set is monotonically decreasing in time due to the deadline pressure, and is also monotonically decreasing in X\u0304(P \u03c0t ) due to the dependence of the deadline on the time series realization.\nThe context dependence of the optimal policy is pictorially depicted in Figure 3 where we show two exemplary trajectories for the decision-maker\u2019s state, and the actions recommended by a policy \u03c0 for the same time and belief, but a different context, i.e. a stopping action recommended when X(t) is large since it corresponds to a low survival probability, whereas for the same belief and time, a continuation action can be recommended if X(t) is low since it is safer to keep observing the process for that the survival probability is high. Such a prescription specifies optimal decisionmaking in context-driven settings such as clinical decision-making in critical care environment [3-5], where a combination of a patient\u2019s length of hospital stay (i.e. t), clinical risk score (i.e. \u00b5t) and current physiological test measurements (i.e. X\u0304(P \u03c0t )) determine the decision on whether or not a patient should be admitted to an intensive care unit.\nPolicy \u03c0: Stop and declare \u03b8\u0302\u03c0\nX(t) \u03b8\u0302\u03c0 = 1\nPolicy \u03c0: Continue sampling X\u03c4 (t)\nt\n\u00b5t\nSample path 1\nSample path 2\n\u00b5\u0304\nt\u0304\nFigure 3: Context-dependence of the decision-making policy \u03c0. For the same belief and time pair (\u00b5\u0304, t\u0304), different actions are recommended in different contexts (different sample paths).\nThe second part of Theorem 3 says that whenever the optimal policy decides to stop gathering information and issue a conclusive decision, it imposes a threshold on the posterior belief, based on which it issues the estimate \u03b8\u0302\u03c0\u2217 ; the threshold is C1Co+C1 , and hence weights the estimates by their respective risks. When the policy favors continuation, it issues a rendezvous action, i.e. the next time instance at which information will be gathered. This rendezvous balances surprise and suspense: the decision-maker prefers maximizing surprise in order to draw the maximum informativeness from the costly sample it will acquire; this is captured in terms of the tail distribution of the information gain P(It(\u03b4) \u2265 \u03b7t). Maximizing surprise may increase suspense, i.e. the probability of process termination, which is controlled by the survival function St(\u03b4), and hence it can be that harvesting the maximum informativeness entails a survival risk when Cr is high. Therefore, the optimal policy selects a rendezvous \u03b4\u03c0 \u2217\nt that optimizes a combination of the survival risk survival, captured by the cost Cr and the survival function St(\u2206t), and the value of information, captured by the costs Co, C1 and the information gain It(\u03b4).\nTo get a feel of the surprise-suspense trade-off, we assume that X\u03c4 (t)|\u0398 = 1 is a standard Brownian motion, and the prior on \u0398 = 1 is p = 0.5, whereas the stopping time is the hitting time of a target level \u03b7 = 30. When should the decision-maker set the date for the first rendezvous? In Figure 4, we plot the expected information gain from the first sample (E[ |Io(\u2206t)|| F\u0303o]) (solid line), and the corresponding survival function So(\u2206t) (dotted line). It can be seen that the expected information gain is maximum at t = 42, but with a 50% survival probability, hence depending on the costs Co, C1 and Cr, the optimal policy may favor an earlier rendezvous (i.e. \u03b4\u03c0 \u2217\no < 42) in order to keep the survival probability within a reasonable limit and at the same time attain a reasonable level of informativeness."}, {"heading": "4 Conclusions", "text": "We developed a model for decision-making with endogenous information acquisition under time pressure, where a decision-maker needs to issue a conclusive decision before an adverse event (potentially) takes place. We have shown that the optimal policy has a \u201drendezvous\u201d structure, i.e. the\noptimal policy sets a \u201ddate\u201d for gathering a new sample whenever the current information sample is observed. The optimal policy selects the time between two information samples such that it balances the information gain (surprise) with the survival probability (suspense). Moreover, we characterized the optimal policy\u2019s continuation and stopping conditions, and showed that they depend on the context and not just on beliefs. Our model can help understanding the nature of optimal decision-making in settings where timely risk assessment and information gathering is essential."}, {"heading": "5 Acknowledgements", "text": "This work was supported by the Office of Naval Research (ONR) and the NSF (Grant number: ECCS 1462245)."}], "references": [{"title": "Optimal temporal risk assessment", "author": ["F. Balci", "D. Freestone", "P. Simen", "L. de Souza", "J.D. Cohen", "P. Holmes"], "venue": "Frontiers in Integrative Neuroscience,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Data-efficient quickest change detection with onoff observation control", "author": ["T. Banerjee", "V.V. Veeravalli"], "venue": "Sequential Analysis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Patient risk stratification for hospital-associated c. diff as a time-series classification", "author": ["J. Wiens", "E. Horvitz", "J.V. Guttag"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-resolution Structure", "author": ["P. Schulam", "S. Saria"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Impact of delayed transfer of critically ill patients from the emergency department to the intensive care unit", "author": ["D.B. Chalfin", "S. Trzeciak", "A. Likourezos", "B.M. Baumann", "R.P. Dellinger"], "venue": "DELAY-ED study group", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Optimization of radiation therapy fractionation schedules in the presence of tumor repopulation", "author": ["T. Bortfeld", "J. Ramakrishnan", "J.N. Tsitsiklis", "J. Unkelbach"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Breast cancer screening programmes in 22 countries: current policies, administration and guidelines", "author": ["S Shapiro"], "venue": "International journal of epidemiology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "A Bayesian Framework for Modeling Confidence in Perceptual Decision Making, In Advances in neural information processing", "author": ["K. Khalvati", "R.P. Rao"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Reward-Rate Maximization in Sequential Identification under a Stochastic Deadline", "author": ["S. Dayanik", "J.Y. Angela"], "venue": "SIAM J. Control Optim.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Forgetful Bayes and myopic planning: Human learning and decisionmaking in a bandit setting, In Advances in neural information processing", "author": ["S. Zhang", "J.Y. Angela"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Strategic impatience in Go/NoGo versus forced-choice decision-making, In Advances in neural information processing", "author": ["P. Shenoy", "J.Y. Angela"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Optimal decision-making with time-varying evidence reliability, In Advances in neural information processing", "author": ["J. Drugowitsch", "R. Moreno-Bote", "A. Pouget"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Dynamics of attentional selection under conflict: toward a rational Bayesian account", "author": ["A.J. Yu", "P. Dayan", "J.D. Cohen"], "venue": "Journal of Experimental Psychology: Human Perception and Performance,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sequential hypothesis testing under stochastic deadlines", "author": ["P. Frazier", "J.Y. Angela"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "The cost of accumulating evidence in perceptual decision making", "author": ["J. Drugowitsch", "R. Moreno-Bote", "A.K. Churchland", "M.N. Shadlen", "A. Pouget"], "venue": "The Journal of Neuroscience,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A Theory of Decision Making Under Dynamic Context", "author": ["M. Shvartsman", "V. Srivastava", "Cohen J. D"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Bayesian Surprise Attracts Human Attention", "author": ["L. Itti", "P. Baldi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks", "author": ["R. Bogacz", "E. Brown", "J. Moehlis", "P.J. Holmes", "Cohen J. D"], "venue": "Psychological Review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Optimal stopping and free-boundary problems, Birkhuser Basel", "author": ["G. Peskir", "A. Shiryaev"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Optimal stopping rules (Vol", "author": ["A.N. Shiryaev"], "venue": "Science & Business Media", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Stochastic calculus for finance II: Continuous-time models (Vol", "author": ["Shreve", "Steven E"], "venue": "Science & Business Media,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Stochastic optimal control: The discrete time case (Vol", "author": ["D.P. Bertsekas", "S.E. Shreve"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 1, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 2, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 3, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 4, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 5, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 6, "context": "The problem of timely risk assessment and decision-making based on a sequentially observed time series is ubiquitous, with applications in finance, medicine, cognitive science and signal processing [1-7].", "startOffset": 198, "endOffset": 203}, {"referenceID": 4, "context": "delayed admission of clinically acute patients to intensive care units can lead to mortality [5]).", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "In the context of cognitive science, this decision-making task is known as the two-alternative forced choice (2AFC) task [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 8, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 9, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 10, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 11, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 12, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 13, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 14, "context": "Insightful structural solutions for the optimal Bayesian 2AFC decision-making policies have been derived in [9-16], most of which are inspired by the classical work of Wald on sequential probability ratio tests (SPRT) [8].", "startOffset": 108, "endOffset": 114}, {"referenceID": 0, "context": "Our abstract model is motivated and inspired by many practical decision-making tasks such as: constructing temporal patterns for gathering sensory information in perceptual decision-making [1], scheduling lab", "startOffset": 189, "endOffset": 192}, {"referenceID": 2, "context": "tests for ward patients in order to predict clinical deterioration in a timely manner [3, 5], designing breast cancer screening programs for early tumor detection [7], etc.", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "tests for ward patients in order to predict clinical deterioration in a timely manner [3, 5], designing breast cancer screening programs for early tumor detection [7], etc.", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "tests for ward patients in order to predict clinical deterioration in a timely manner [3, 5], designing breast cancer screening programs for early tumor detection [7], etc.", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 9, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 10, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 11, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 12, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 13, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 14, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 15, "context": "Related Works Mathematical models and analyses for perceptual decision-making based on sequential hypothesis testing have been developed in [9-17].", "startOffset": 140, "endOffset": 146}, {"referenceID": 18, "context": "Most of these models use tools from sequential analysis developed by Wald [8] and Shiryaev [21, 22].", "startOffset": 91, "endOffset": 99}, {"referenceID": 19, "context": "Most of these models use tools from sequential analysis developed by Wald [8] and Shiryaev [21, 22].", "startOffset": 91, "endOffset": 99}, {"referenceID": 7, "context": "In [9,13,14], optimal decision-making policies for the 2AFC task were computed by modelling the decision-maker\u2019s sensory evidence using diffusion processes [20].", "startOffset": 3, "endOffset": 12}, {"referenceID": 11, "context": "In [9,13,14], optimal decision-making policies for the 2AFC task were computed by modelling the decision-maker\u2019s sensory evidence using diffusion processes [20].", "startOffset": 3, "endOffset": 12}, {"referenceID": 12, "context": "In [9,13,14], optimal decision-making policies for the 2AFC task were computed by modelling the decision-maker\u2019s sensory evidence using diffusion processes [20].", "startOffset": 3, "endOffset": 12}, {"referenceID": 17, "context": "In [9,13,14], optimal decision-making policies for the 2AFC task were computed by modelling the decision-maker\u2019s sensory evidence using diffusion processes [20].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "The assumption of an infinite time horizon was relaxed in [10] and [15], where decision-making is assumed to be performed under the pressure of a stochastic deadline; however, these deadlines were considered to be drawn from known distributions that are independent of the hypothesis and the realized sensory evidence, and the assumption of an exogenous information supply was maintained.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The assumption of an infinite time horizon was relaxed in [10] and [15], where decision-making is assumed to be performed under the pressure of a stochastic deadline; however, these deadlines were considered to be drawn from known distributions that are independent of the hypothesis and the realized sensory evidence, and the assumption of an exogenous information supply was maintained.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "patients\u2019 acuity events are correlated with their physiological information [5]), which induces more complex dynamics in the decision-making process.", "startOffset": 76, "endOffset": 79}, {"referenceID": 15, "context": "Context-based decision-making models were introduced in [17], but assuming an exogenous information supply and an infinite time horizon.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "These models use measures for Bayesian surprise, originally introduced in the context of sensory neuroscience [19], in order to model the explicit preference of a decision-maker to non-instrumental information.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "P (\u03c4 < t| Fs) = P (\u03c4 < t|X(s)) , \u2200s < t, where P (\u03c4 < t|X(s)) is an injective map from R to [0, 1] and P (\u03c4 < t|X(s)) is non-decreasing in X(s).", "startOffset": 92, "endOffset": 98}, {"referenceID": 8, "context": "Thus, unlike the stochastic deadline models in [10] and [15], the decision deadline in our model (i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "Thus, unlike the stochastic deadline models in [10] and [15], the decision deadline in our model (i.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "P (\u03c4 < t|X(s)) is not independent of X(t) as in [15]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Figure 1 depicts a Brownian path (a sample path of a Wiener process, which satisfies all the assumptions of our model)4, with an exemplary partition Pt over the time interval [0, 1].", "startOffset": 175, "endOffset": 181}, {"referenceID": 8, "context": "Since the decision-maker\u2019s posterior belief at time t, defined as \u03bct = P(\u0398 = 1| F\u0303t), is an important statistic for designing sequential policies [10, 21-22], we start our characterization for \u03c0 by investigating the belief process (\u03bct)t\u2208R+ .", "startOffset": 146, "endOffset": 157}, {"referenceID": 18, "context": "Since the decision-maker\u2019s posterior belief at time t, defined as \u03bct = P(\u0398 = 1| F\u0303t), is an important statistic for designing sequential policies [10, 21-22], we start our characterization for \u03c0 by investigating the belief process (\u03bct)t\u2208R+ .", "startOffset": 146, "endOffset": 157}, {"referenceID": 19, "context": "Since the decision-maker\u2019s posterior belief at time t, defined as \u03bct = P(\u0398 = 1| F\u0303t), is an important statistic for designing sequential policies [10, 21-22], we start our characterization for \u03c0 by investigating the belief process (\u03bct)t\u2208R+ .", "startOffset": 146, "endOffset": 157}, {"referenceID": 18, "context": "Thus, unlike classical Bayesian learning models with a belief martingale [18, 21-23], the belief process in our model is a supermartingale that leans toward decreasing over time.", "startOffset": 73, "endOffset": 84}, {"referenceID": 19, "context": "Thus, unlike classical Bayesian learning models with a belief martingale [18, 21-23], the belief process in our model is a supermartingale that leans toward decreasing over time.", "startOffset": 73, "endOffset": 84}, {"referenceID": 20, "context": "Thus, unlike classical Bayesian learning models with a belief martingale [18, 21-23], the belief process in our model is a supermartingale that leans toward decreasing over time.", "startOffset": 73, "endOffset": 84}, {"referenceID": 8, "context": "That is, unlike [10] and [15] where the decision deadline is hypothesis-independent and is almost surely occurring in finite time for any path, in our model the occurrence of the adverse event is itself a hypothesis, hence observing the survival of the process is informative and contributes to the evolution of the belief.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "That is, unlike [10] and [15] where the decision deadline is hypothesis-independent and is almost surely occurring in finite time for any path, in our model the occurrence of the adverse event is itself a hypothesis, hence observing the survival of the process is informative and contributes to the evolution of the belief.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "The informativeness of both the acquired information samples and process survival can be disentangled using Doob decomposition, by writing \u03bct as \u03bct = \u03bc\u0303t +A(\u03bct, St(\u2206t)), where \u03bc\u0303t is a martingale, capturing the information gain from the acquired samples, and A(\u03bct, St(\u2206t)) is a predictable compensator process [23], capturing information extracted from the process survival.", "startOffset": 310, "endOffset": 314}, {"referenceID": 0, "context": "Define a value function V : N \u00d7 [0, 1] \u2192 R+ as a map from the current history to the risk of the best policy given the history F\u0303t as follows:", "startOffset": 32, "endOffset": 38}, {"referenceID": 21, "context": "Based on Bellmans optimality principle [24], we know that the optimal policy has to satisfy the following in every time step, i.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "Having established the result in Theorem 2, in the following Theorem, we characterize the optimal policy \u03c0 in terms of the random process (T\u03c0\u2217 , \u03b8\u0302\u03c0\u2217 , P \u03c0 \u2217 t ) using discrete-time Bellman optimality conditions [24].", "startOffset": 212, "endOffset": 216}, {"referenceID": 0, "context": "Define a value function V : N\u00d7 [0, 1] \u2192 R+ as a map from the current history to the risk of the best policy given the history F\u0303t as follows:", "startOffset": 31, "endOffset": 37}, {"referenceID": 13, "context": "The equation above determines the stopping and continuation conditions, and using the monotonicity of the survival function in both time t and the time series realizations X\u0304 (P \u03c0 t ), we can show the monotonicity of the continuation set C(t, X\u0304(P \u03c0 t )) using the same arguments of Theorem 1 in [15].", "startOffset": 296, "endOffset": 300}, {"referenceID": 7, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 8, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 9, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 10, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 11, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 12, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 13, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 14, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 43, "endOffset": 49}, {"referenceID": 15, "context": "We note that unlike the previous models in [9-16], with the exception of [17], a policy in our model is context-dependent.", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Such a prescription specifies optimal decisionmaking in context-driven settings such as clinical decision-making in critical care environment [3-5], where a combination of a patient\u2019s length of hospital stay (i.", "startOffset": 142, "endOffset": 147}, {"referenceID": 3, "context": "Such a prescription specifies optimal decisionmaking in context-driven settings such as clinical decision-making in critical care environment [3-5], where a combination of a patient\u2019s length of hospital stay (i.", "startOffset": 142, "endOffset": 147}, {"referenceID": 4, "context": "Such a prescription specifies optimal decisionmaking in context-driven settings such as clinical decision-making in critical care environment [3-5], where a combination of a patient\u2019s length of hospital stay (i.", "startOffset": 142, "endOffset": 147}], "year": 2016, "abstractText": "We develop a Bayesian model for decision-making under time pressure with endogenous information acquisition. In our model, the decision-maker decides when to observe (costly) information by sampling an underlying continuoustime stochastic process (time series) that conveys information about the potential occurrence/non-occurrence of an adverse event which will terminate the decisionmaking process. In her attempt to predict the occurrence of the adverse event, the decision-maker follows a policy that determines when to acquire information from the time series (continuation), and when to stop acquiring information and make a final prediction (stopping). We show that the optimal policy has a \u201drendezvous\u201d structure, i.e. a structure in which whenever a new information sample is gathered from the time series, the optimal \u201ddate\u201d for acquiring the next sample becomes computable. The optimal interval between two information samples balances a trade-off between the decision maker\u2019s \u201dsurprise\u201d, i.e. the drift in her posterior belief after observing new information, and \u201dsuspense\u201d, i.e. the probability that the adverse event occurs in the time interval between two information samples. Moreover, we characterize the continuation and stopping regions in the decisionmaker\u2019s state-space, and show that they depend not only on the decision-maker\u2019s beliefs, but also on the \u201dcontext\u201d, i.e. the current realization of the time series.", "creator": "LaTeX with hyperref package"}}}