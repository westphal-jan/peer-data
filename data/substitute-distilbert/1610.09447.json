{"id": "1610.09447", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction", "abstract": "in the big data era, both of the sample size and dimension could be separated at the same time. multiple parallel technology was recently proposed to handle the overall data. still, modern stochastic gradient descent algorithms the recently characterized to scale any sample size, and asynchronous stochastic coordinate search algorithms were proposed to scale the dimension. however, a few existing asynchronous parallel algorithms can scale jointly in sample size and dimension simultaneously. in this paper, we focus on a composite objective function consists of this smooth convex function f and a symmetric convex function g. we propose an asynchronous doubly stochastic proximal learning mechanism with variance reduction ( asydspovr ) to scale well with the sample size and measurements simultaneously. we prove that asydspovr achieves a linear convergence dependence when the function f arrives to the optimal strong convexity property, preferring a sublinear time when f is with the general convexity.", "histories": [["v1", "Sat, 29 Oct 2016 03:39:16 GMT  (17kb)", "https://arxiv.org/abs/1610.09447v1", null], ["v2", "Mon, 7 Nov 2016 20:05:31 GMT  (17kb)", "http://arxiv.org/abs/1610.09447v2", null], ["v3", "Mon, 14 Nov 2016 02:14:36 GMT  (19kb)", "http://arxiv.org/abs/1610.09447v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bin gu", "zhouyuan huo", "heng huang"], "accepted": false, "id": "1610.09447"}, "pdf": {"name": "1610.09447.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction", "authors": ["Bin Gu", "Zhouyuan Huo", "Heng Huang"], "emails": ["jsgubin@gmail.com", "zhouyuan.huo@mavs.uta.edu", "heng@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n09 44\n7v 3\n[ cs\n.L G\n] 1\nKeywords: stochastic optimization, block coordinate descent, parallel computing, lockfree\n1. Introduction\nStochastic optimization technologies in theory and practice are emerging recently due to the demand of handling large scale data. Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems. Also, stochastic coordinate descent (SCD) algorithms (Taka\u0301c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed. For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014). Basically, these algorithms are sequential algorithms which can not be directly used in parallel environment.\nTo scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richta\u0301rik and Taka\u0301c\u030c, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.\nAmong these asynchronous parallel implementations, the ones with lock-free are more efficient than the ones with writing or reading lock, because they can achieve near-linear speedup which is the ultimate goal of the parallel computation. In these paper, we focus on the asynchronous parallel implementations with lock-free.\nThere have been several asynchronous parallel implementations for the stochastic optimization algorithms which are totally free of reading and writing locks. For example, Zhao and Li (2016) proposed an asynchronous parallel algorithm for SVRG and proved the linear convergence. Liu and Wright (2015) proposed an asynchronous parallel algorithm for SCD and proved the linear convergence. Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence. Lian et al. (2016) proposed an asynchronous stochastic optimization algorithm with zeroth order and proved the convergence.\nIn this paper, we focus on a composite objective function as follows.\nmin x\u2208Rn F (x) = f(x) + g(x) (1)\nwhere f(x) = 1l \u2211l i=1 fi(x), fi : R n 7\u2192 R is a smooth convex function. g : Rn 7\u2192 R \u222a {\u221e} is a block separable, closed, convex, and extended real-valued function. Given a partition {G1, \u00b7 \u00b7 \u00b7 ,Gk} of n coordinates of x, we can write g(x) as g(x) = \u2211k j=1 gGj (xGj ). The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on. Note that, Liu and Wright (2015) consider the formulation (1) with the constraints that |Gj | = 1 for all j. Thus, the formulation in (Liu and Wright, 2015) is a special case of (1). Each iteration in (Liu and Wright, 2015) only modifies a single component of x which is an atomic operation in the parallel system with shared memory. However, we need to modify a block coordinate Gj of x with lockfree for each iteration, which are more complicated in the asynchronous parallel analysis than the atomic updating in (Liu and Wright, 2015). Due to the complication induced by the block representation of g(x), there have been no asynchronous stochastic block coordinate descent algorithm with lock-free proposed for handle formulation (1), especially on the theoretical analysis.\nIn this paper, we propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lockfree in the implementation and analysis. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.\nAsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. In the big data era, both of the sample size and dimension could be huge at the same time as modern data collection technologies evolve, which demands that the learning algorithms can process large scale datasets with large sample size and high dimension. AsySBCDVR is based on a doubly stochastic scheme which randomly\nchoose a set of samples and a block coordinate for each iteration. Thus, AsySBCDVR can process large scale datasets. Especially, the technology of variance reduction is used to accelerate AsySBCDVR such that AsySBCDVR has the linear or sublinear convergence in different conditions. Thus, AsySBCDVR can scale well with the sample size and dimension simultaneously.\nWe organize the rest of the paper as follows. In Section 2, we give some preliminaries. In section 3, we propose our AsySBCDVR algorithm. In Section 4, we prove the convergence rate for AsySBCDVR. Finally, we give some concluding remarks in Section 5.\n2. Preliminaries\nIn this section, we introduce the condition of optimal strong convexity and three different Lipschitz constants and give the corresponding assumptions, which are critical to the analysis of AsySBCDVR. Optimal Strong Convexity: Let F \u2217 denote the optimal value of (1), and let S denote the solution set of F such that F (x) = F \u2217, \u2200x \u2208 S. Firstly, we assume that S is nonempty (i.e., Assumption 1), which is reasonable to (1).\nAssumption 1 The solution set S of (1) is nonempty.\nBased on S, we define PS(x) = argminy\u2208S \u2016y \u2212 x\u20162 as the Euclidean-norm projection of a vector x onto S. Then, we assume that the convex function f is with the optimal strong convexity (i.e., Assumption 2).\nAssumption 2 (Optimal strong convexity) The convex function f has the condition of optimal strong convexity with parameter l > 0 with respect to the optimal set S, which means that, \u2203l such that, \u2200x, we have\nF (x)\u2212 F (PS(x)) \u2265 l\n2 \u2016x\u2212PS(x)\u20162 (2)\nAs mentioned in Liu and Wright (2015), the condition of optimal strong convexity is significantly weaker than the normal strong convexity condition. And several examples of optimally strongly convex functions that are not strongly convex are provided in (Liu and Wright, 2015). Lipschitz Smoothness: Let \u2206j denote the zero vector in R\nn except that the block coordinates indexed by the set Gj. We define the normal Lipschitz constant (Lnor), block restricted Lipschitz constant (Lres) and block coordinate Lipschitz constant (Lmax) as follows.\nDefinition 1 (Normal Lipschitz constant) Lnor is the normal Lipschitz constant for \u2207fi (\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , l}) in (1), such that, \u2200x and \u2200y, we have\n\u2016\u2207fi(x)\u2212\u2207fi(y)\u2016 \u2264 Lnor\u2016x\u2212 y\u2016 (3)\nDefinition 2 (Block Restricted Lipschitz constant) Lres is the block restricted Lipschitz constant for \u2207fi (\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , l}) in (1), such that, \u2200x, and \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , k}, we have\n\u2016\u2207fi(x+\u2206j)\u2212\u2207fi(x)\u2016 \u2264 Lres \u2225\u2225(\u2206j)Gj \u2225\u2225 (4)\nDefinition 3 (Block Coordinate Lipschitz constant) Lmax is the block coordinate Lipschitz constant for \u2207fi (\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , l}) in (1), such that, \u2200x, and \u2200j \u2208 {1, \u00b7 \u00b7 \u00b7 , k}, we have\nmax j=1,\u00b7\u00b7\u00b7 ,k\n\u2016\u2207fi(x+\u2206j)\u2212\u2207fi(x)\u2016 \u2264 Lmax \u2225\u2225(\u2206j)Gj \u2225\u2225 (5)\n(5) is equvilent to the formulation (6).\nfi(x+\u2206j) \u2264 fi(x) + \u3008\u2207Gjfi(x), (\u2206j)Gj \u3009+ Lmax\n2\n\u2225\u2225(\u2206j)Gj \u2225\u22252 (6)\nBased on Lnor Lres and Lmax as defined above, we assume that the function fi (\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , l} is Lipschitz smooth with Lnor Lres and Lmax (i.e., Assumption 3). In addition, we define \u039bres =\nLres Lmax , \u039bnor = Lnor Lmax .\nAssumption 3 (Lipschitz smoothness) The function fi (\u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , l} is Lipschitz smooth with the normal Lipschitz constant Lnor, block restricted Lipschitz constant Lres and block coordinate Lipschitz constant Lmax.\n3. Algorithm\nIn this section, we propose our AsySBCDVR. AsySBCDVR is designed for the parallel environment with shared memory, such as multi-core processors and GPU-accelerators, but it can also work in the parallel environment with distributed memory.\nIn the parallel environment with shared memory, all cores in CPU or GPU can read and write the vector x in the shared memory simultaneously without any lock. Besides randomly choosing a sample set and a block coordinate, AsySBCDVR is also accelerated by the variance reduction. Thus, AsySBCDVR has two-layer loops. The outer layer is to parallelly compute the full gradient \u2207f(xs) = 1l \u2211l i=1\u2207fi(xs), where the superscript s denotes the s-th outer loop. The inner layer is to parallelly and repeatedly update the vector x in the shared memory. Specifically, all cores repeat the following steps independently and concurrently without any lock:\n1. Read: Read the vector x from the shared memory to the local memory without reading lock. We use x\u0302s+1t to denote its value, where the subscript t denotes the t-th inner loop.\n2. Compute: Randomly choose a mini-batch B and a block coordinate j from {1, ..., k}, and locally compute v\u0302s+1Gj = 1 |B| \u2211 i\u2208B \u2207Gjfi(x\u0302s+1t )\u2212 1|B| \u2211 i\u2208B \u2207Gjfi(x\u0303s) +\u2207Gjf(x\u0303s).\n3. Update: Update the block j of the vector x in the shared memory as (xs+1t+1 )Gj \u2190 PGj , \u03b3Lmax gj ( (xs+1t )Gj \u2212 \u03b3Lmax v\u0302 s+1 Gj ) without writing lock.\nThe detailed description of AsySBCDVR is presented in Algorithm 1. Note that v\u0302s+1Gj computed locally is the approximation of \u2207Gjf(x\u0302s+1t ), and the expectation of v\u0302s+1t on B is equal to \u2207f(x\u0302s+1t ) as follows.\nEv\u0302s+1t = E\n( 1\n|B| \u2211\ni\u2208B\n\u2207fi(x\u0302s+1t )\u2212 1 |B| \u2211\ni\u2208B\n\u2207fi(x\u0303s) +\u2207f(x\u0303s) )\n(7)\n= \u2207f(x\u0302s+1t )\u2212\u2207f(x\u0303s) +\u2207f(x\u0303s) = \u2207f(x\u0302s+1t )\nThus, v\u0302s+1t is called a stochastic gradient of f(x) at x\u0302 s+1 t .\nBecause AsySBCDVR does not use the reading lock, the vector x\u0302s+1t read into the local memory may be inconsistent to the vector xs+1t in the shared memory, which means that some components of x\u0302s+1t are same with the ones in x s+1 t , but others are different to the ones in xs+1t . However, we can define a set K(t) of inner iterations, such that,\nxs+1t = x\u0302 s+1 t +\n\u2211\nt\u2032\u2208K(t)\nBs+1t\u2032 \u2206 s+1 t\u2032 (8)\nwhere t\u2032 \u2264 t \u2212 1, (\u2206s+1t\u2032 )Gj(t\u2032) = PGj(t\u2032), \u03b3Lmax gj(t\u2032) ( (xs+1t\u2032 )Gj(t\u2032) \u2212 \u03b3 Lmax v\u0302s+1t\u2032,Gj(t\u2032) ) \u2212 (xs+1t\u2032 )Gj(t\u2032) , (\u2206s+1t\u2032 )\\Gj(t\u2032) = 0, B s+1 t\u2032 is a diagonal matrix with diagonal entries either 1 or 0. It is reasonable to assume that Bst\u2032 6= 0, \u2200t\u2032 \u2208 K(t) (i.e., Assumption 4), and there exists an upper bound \u03c4 such that \u03c4 \u2265 t\u2212min{t\u2032|t\u2032 \u2208 K(t)} (i.e., Assumption 5).\nAssumption 4 (Non zero of Bst\u2032) For all inner iterations t in AsySBCDVR, \u2200t\u2032 \u2208 K(t), we have that Bst\u2032 6= 0.\nAssumption 5 (Bound of delay) There exists a upper bound \u03c4 such that \u03c4 \u2265 t\u2212min{t\u2032|t\u2032 \u2208 K(t)} for all inner iterations t in AsySBCDVR.\nAlgorithm 1 Asynchronous Stochastic Block Coordinate Descent with Variance Reduction (AsySBCDVR)\nInput: \u03b3, S, and m. Output: xS. 1: Initialize x0 \u2208 Rd, p threads. 2: for s = 0, 1, 2, S \u2212 1 do 3: x\u0303s \u2190 xs 4: All threads parallelly compute the full gradient \u2207f(x\u0303s) = 1l \u2211l i\u2207fi(x\u0303s)\n5: For each thread, do: 6: for t = 0, 1, 2,m \u2212 1 do 7: Randomly sample a mini-batch B from {1, ..., l} with equal probability. 8: Randomly choose a block j(t) from {1, ..., k} with equal probability. 9: Compute v\u0302s+1Gj(t) = 1 |B| \u2211 i\u2208B \u2207Gj(t)fi(x\u0302s+1t )\u2212 1|B| \u2211 i\u2208B \u2207Gj(t)fi(x\u0303s) +\u2207Gj(t)f(x\u0303s).\n10: (xs+1t+1 )Gj(t) \u2190 PGj(t), \u03b3Lmax gj(t) ( (xs+1t )Gj(t) \u2212 \u03b3Lmax v\u0302 s+1 t,Gj(t) ) . 11: (xs+1t+1 )\\Gj(t) \u2190 (xs+1t )\\Gj(t) . 12: end for 13: xs+1 \u2190 xs+1m 14: end for\n4. Convergence Analysis\nIn this section, we follow the analysis of (Liu and Wright, 2015) and prove the convergence rate of AsyDSCDVR (Theorem 8). Specifically, AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity.\nBefore providing the theoretical analysis, we give the definitions of x\u0302st,t\u2032+1, x s t+1 and the\nexplanation of xst used in the analysis as follows.\n1. x\u0302st,t\u2032 : Assume the indices in K(t) are sorted in the increasing order, we use K(t)t\u2032 to denote the t\u2032-th index in K(t). For t\u2032 = 0, 1, \u00b7 \u00b7 \u00b7 , |K(t)|, we define\nx\u0302st,t\u2032 = x\u0302 s t +\nt\u2032\u2211\nt\u2032\u2032=1\n( BsK(t)t\u2032\u2032 \u2206sK(t)t\u2032\u2032 ) = x\u0302st + t\u2032\u2211\nt\u2032\u2032=1\n( xK(t)t\u2032\u2032+1 \u2212 xK(t)t\u2032\u2032 ) ) (9)\nThus, we have that\nx\u0302st = x\u0302 s t,0 (10) xst = x\u0302 s t,|K(t)| (11)\nxst \u2212 x\u0302st = |K(t)|\u22121\u2211\nt\u2032\u2032=0\n( Bs+1K(t)t\u2032\u2032 \u2206s+1K(t)t\u2032\u2032 ) = |K(t)|\u22121\u2211\nt\u2032=0\n( x\u0302st,t\u2032+1 \u2212 x\u0302st,t\u2032 ) (12)\n\u2207f(xst)\u2212\u2207f(x\u0302st) = |K(t)|\u22121\u2211\nt\u2032=0\n( \u2207f(x\u0302st,t\u2032+1)\u2212\u2207f(x\u0302t,t\u2032) ) (13)\n2. xst+1: x s t+1 is defined as:\nxst+1 def = P \u03b3\nLmax g\n( xst \u2212 \u03b3\nLmax v\u0302st\n) (14)\nBased on (14), it is easy to verify that (xst+1)Gj(t) = (x s+1 t+1 )Gj(t) . Thus, we have Ej(t)(x s t+1 \u2212 xst ) = 1k ( xst+1 \u2212 xst ) . It means that xst+1 \u2212 xst captures the expectation of xst+1 \u2212 xst .\n3. xst : As mentioned previously, AsySBCDVR does not use any locks in the reading and writing. Thus, in the line 10 of Algorithm 1, xst (left side of \u2018\u2190\u2019) updated in the shared memory may be inconsistent with the idea one (right side of \u2018\u2190\u2019) computed by the proximal operator. In the analysis, we use xst to denote the idea one computed by the proximal operator. Same as mentioned in (Mania et al., 2015), there might not be an actual time the idea ones exist in the shared memory, except the first and last iterates for each outer loop. It is noted that, xs0 and x s m are exactly what is stored in\nshared memory. Thus, we only consider the idea xst in the analysis.\nThen, we give two inequalities in Lemma 4 and 5 respectively. Based on Lemma 4 and 5, we prove that E\u2016xst\u22121 \u2212 xst\u20162 \u2264 \u03c1E\u2016xst \u2212 xst+1\u20162 (Lemma 6), where \u03c1 > 1 is a user defined parameter. Then, we prove the monotonicity of the expectation of the objectives EF (xst+1) \u2264 EF (xst ) (Lemma 7). Note that the analyses only consider the case |B| = 1 without loss of generality. The case of |B| > 1 can be proved similarly.\nLemma 4 For \u2016\u2207f(xst )\u2212\u2207f(x\u0302st)\u2016 in each iteration of AsySBCDVR, we have its upper bound as\n\u2016\u2207f(xst)\u2212\u2207f(x\u0302st)\u2016 \u2264 Lres \u2211\nt\u2032\u2208K(t)\n\u2016\u2206st\u2032\u2016 (15)\nProof Based on , we have that\n\u2016\u2207f(xst)\u2212\u2207f(x\u0302st)\u2016 = \u2225\u2225\u2225\u2225\u2225\u2225 |K(t)|\u22121\u2211\nt\u2032=0\n\u2207f(x\u0302st,t\u2032+1)\u2212\u2207f(x\u0302st,t\u2032) \u2225\u2225\u2225\u2225\u2225\u2225 (16)\n\u2264 |K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225\u2207f(x\u0302st,t\u2032+1)\u2212\u2207f(x\u0302st,t\u2032) \u2225\u2225 \u2264 Lres |K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225x\u0302st,t\u2032+1 \u2212 x\u0302st,t\u2032 \u2225\u2225\n= Lres\n|K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225\u2225BsK(t)t\u2032\u2206 s K(t)t\u2032 \u2225\u2225\u2225 \u2264 Lres |K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225\u2225BsK(t)t\u2032 \u2225\u2225\u2225 \u2225\u2225\u2225\u2206sK(t)t\u2032 \u2225\u2225\u2225 \u2264 Lres \u2211\nt\u2032\u2208K(t)\n\u2016\u2206st\u2032\u2016\nThis completes the proof.\nLemma 5 In each iteration of AsySBCDVR, \u2200x, we have the following inequality. \u2329 (v\u0302st )Gj(t) + Lmax\n\u03b3 \u2206st , (x s t+1 \u2212 x)Gj(t)\n\u232a + gGj(t) ( (xst+1)Gj(t) ) \u2212 gGj(t) ( (x)Gj(t) ) \u2264 0 (17)\nProof The problem solved in lines 8 of Algorithm 1 is as follows\nxst+1 = argminx\n\u2329 (v\u0302st )Gj(t) , (x\u2212 xst)Gj(t) \u232a + Lmax\n2\u03b3\n\u2225\u2225\u2225(x\u2212 xst )Gj(t) \u2225\u2225\u2225 2\n(18)\n+gGj(t) ( (x)Gj(t) )\ns.t. x\\Gj(t) = (x s t )\\Gj(t)\nIf xst+1 is the solution of (18), the solution of optimization problem (19) is also x s t+1 according to the subdifferential version of Karush-Kuhn-Tucker (KKT) conditions (Ruszczyn\u0301ski., 2006).\nP (x) = min x\n\u2329 (v\u0302st )Gj(t) + Lmax\n\u03b3\n( xst+1 \u2212 xst ) Gj(t) , (x\u2212 xst)Gj(t) \u232a + gGj(t) ( (x)Gj(t) ) (19)\ns.t. x\\Gj(t) = (x s t )\\Gj(t)\nThus, we have that P (x) \u2265 P (xst+1), \u2200x, which leads to (17). This completes the proof.\nLemma 6 Let \u03c1 be a constant that satisfies \u03c1 > 1, and define the quantities \u03b81 = \u03c1 1 2\u2212\u03c1 \u03c4+1 2\n1\u2212\u03c1 1 2\nand \u03b82 = \u03c1 1 2 \u2212\u03c1 m 2\n1\u2212\u03c1 1 2\n. Suppose the nonnegative steplength parameter \u03b3 > 0 satisfies \u03b3 \u2264\nmin {\nk1/2(1\u2212\u03c1\u22121)\u22124 4(\u039bres(1+\u03b81)+\u039bnor(1+\u03b82)) , k 1/2\n1 2 k1/2+2\u039bnor\u03b82+\u039bres\u03b81\n} , we have\nE\u2016xst\u22121 \u2212 xst\u20162 \u2264 \u03c1E\u2016xst \u2212 xst+1\u20162 (20)\nProof According to (A.8) in Liu and Wright (2015), we have\n\u2016xst\u22121 \u2212 xst\u20162 \u2212 \u2016xst \u2212 xst+1\u20162 \u2264 2\u2016xst\u22121 \u2212 xst\u2016\u2016xst \u2212 xst+1 \u2212 xst\u22121 + xst\u2016 (21)\nThe second part in the right half side of (21) is bound as follows if B = {it} and J(t) = {j(t)}.\n\u2016xst \u2212 xst+1 \u2212 xst\u22121 + xst\u2016 (22)\n= \u2225\u2225\u2225\u2225x s t \u2212 P \u03b3Lmax g ( xst \u2212 \u03b3 Lmax v\u0302st ) \u2212 xst\u22121 + P \u03b3Lmax g ( xst\u22121 \u2212 \u03b3 Lmax v\u0302st\u22121 )\u2225\u2225\u2225\u2225 \u2264 \u2016xst \u2212 xst\u22121\u2016+ \u2225\u2225\u2225\u2225P \u03b3Lmax g ( xst \u2212 \u03b3 Lmax v\u0302st ) \u2212 P \u03b3 Lmax g ( xst\u22121 \u2212 \u03b3 Lmax v\u0302st\u22121 )\u2225\u2225\u2225\u2225 \u2264 2\u2016xst \u2212 xst\u22121\u2016+ \u03b3\nLmax\n\u2225\u2225v\u0302st \u2212 v\u0302st\u22121 \u2225\u2225\n= 2\u2016xst \u2212 xst\u22121\u2016+ \u03b3\nLmax\n\u2225\u2225\u2207fit(x\u0302st )\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s)\u2212\u2207fit\u22121(x\u0302st\u22121) +\u2207fit\u22121(x\u0303s)\u2212\u2207f(x\u0303s) \u2225\u2225\n= 2\u2016xst \u2212 xst\u22121\u2016+ \u03b3\nLmax\n\u2225\u2225\u2207fit(x\u0302st )\u2212\u2207fit(x\u0303s)\u2212\u2207fit\u22121(x\u0302st\u22121) +\u2207fit\u22121(x\u0303s) \u2225\u2225\n\u2264 2\u2016xst \u2212 xst\u22121\u2016+ \u03b3\nLmax \u2016\u2207fit(x\u0302st )\u2212\u2207fit(xst ) +\u2207fit(xst )\u2212\u2207fit(x\u0303s)\u2016\n+ \u03b3\nLmax\n\u2225\u2225\u2207fit\u22121(x\u0302st\u22121)\u2212\u2207fit\u22121(xst\u22121) +\u2207fit\u22121(xst\u22121)\u2212\u2207fit\u22121(x\u0303s) \u2225\u2225\n\u2264 2\u2016xst \u2212 xst\u22121\u2016+ \u03b3\nLmax \u2016\u2207fit(x\u0302st )\u2212\u2207fit(xst )\u2016+\n\u03b3\nLmax \u2016\u2207fit(xst )\u2212\u2207fit(x\u0303s)\u2016\n+ \u03b3\nLmax\n\u2225\u2225\u2207fit\u22121(x\u0302st\u22121)\u2212\u2207fit\u22121(xst\u22121) \u2225\u2225+ \u03b3\nLmax\n\u2225\u2225\u2207fit\u22121(xst\u22121)\u2212\u2207fit\u22121(x\u0303s) \u2225\u2225\n\u2264 2\u2016xst \u2212 xst\u22121\u2016+ \u03b3\u039bres\n  \u2211\nt\u2032\u2208K(t\u22121)\n\u2016\u2206st\u2032\u2016+ \u2211\nt\u2032\u2208K(t)\n\u2016\u2206st\u2032\u2016\n \n+\u03b3\u039bnor ( \u2016xst \u2212 x\u0303s\u2016+ \u2016xst\u22121 \u2212 x\u0303s\u2016 )\n\u2264 2\u2016xst \u2212 xst\u22121\u2016+ 2\u03b3 ( \u039bres t\u22121\u2211\nt\u2032=t\u22121\u2212\u03c4\n\u2016\u2206st\u2032\u2016+ \u039bnor t\u22121\u2211\nt\u2032=0\n\u2016\u2206st\u2032\u2016 )\nwhere the first inequality use the nonexpansive property of P \u03b3 Lmax g, the fifth inequality use\nA.7 of Liu and Wright (2015), the sixth inequality comes from \u2016xst \u2212 x\u0303s\u2016 = \u2016 \u2211t\u22121\nt\u2032=0\u2206 s t\u2032\u2016 \u2264\u2211t\u22121\nt\u2032=0 \u2016\u2206st\u2032\u2016. If t = 1, we have that K(0) = \u2205 and K(1) \u2286 {0}. Thus, according to (22), we have\n\u2016xs1 \u2212 xs2 \u2212 xs0 + xs1\u2016 \u2264 2\u2016xs1 \u2212 xs0\u2016+ 2\u03b3 (\u039bres + \u039bnor) \u2016\u2206s0\u2016 (23)\nSubstituting (23) into (21), and takeing expectations, we have\nE\u2016xs0 \u2212 xs1\u20162 \u2212 E\u2016xs1 \u2212 xs2\u20162 \u2264 2E (\u2016xs0 \u2212 xs1\u2016\u2016xs1 \u2212 xs2 \u2212 xs0 + xs1\u2016) (24) \u2264 4E (\u2016xs0 \u2212 xs1\u2016\u2016xs1 \u2212 xs0\u2016) + 4\u03b3 (\u039bres + \u039bnor)E (\u2016xs0 \u2212 xs1\u2016\u2016\u2206s0\u2016) \u2264 4k\u2212 12E ( \u2016xs0 \u2212 xs1\u20162 ) + 4\u03b3 (\u039bres + \u039bnor)E (\u2016xs0 \u2212 xs1\u2016\u2016\u2206s0\u2016)\nwhere the last inequality uses A.13 in (Liu and Wright, 2015). Further, we have the upper bound of E ( \u2016xst \u2212 xst+1\u2016\u2016\u2206st\u2016 ) as\nE ( \u2016xst \u2212 xst+1\u2016\u2016\u2206st\u2016 ) \u2264 1\n2 E\n( k\u2212 1 2 \u2016xst \u2212 xst+1\u20162 + k 1 2 \u2016\u2206st\u20162 ) (25)\n= 1\n2 E\n( k\u2212 1 2 \u2016xst \u2212 xst+1\u20162 + k 1 2Ej(t)\u2016\u2206st\u20162 ) = 1\n2 E\n( k\u2212 1 2\u2016xst \u2212 xst+1\u20162 + k\u2212 1 2E\u2016xst \u2212 xst+1\u20162 )\n= k\u2212 1 2E\u2016xst \u2212 xst+1\u20162\nSubstituting (25) into (24), we have\nE\u2016xs0 \u2212 xs1\u20162 \u2212 E\u2016xs1 \u2212 xs2\u20162 \u2264 k\u2212 1 2 (4 + 4\u03b3 (\u039bres + \u039bnor))E ( \u2016xs0 \u2212 xs1\u20162 ) (26)\nwhich implies that\nE\u2016xs0 \u2212 xs1\u20162 \u2264 ( 1\u2212 4 + 4\u03b3 (\u039bres + \u039bnor)\u221a\nk\n)\u22121 E\u2016xs1 \u2212 xs2\u20162 \u2264 \u03c1E\u2016xs1 \u2212 xs2\u20162 (27)\nwhere the last inequality follows from . Thus, we have (20) for t = 1.\n\u03c1\u22121 \u2264 1\u2212 4 + 4\u03b3 (\u039bres + \u039bnor)\u221a k \u21d4 \u03b3 \u2264 k 1/2(1\u2212 \u03c1\u22121)\u2212 4 4 (\u039bres + \u039bnor)\n(28)\nNext, we consider the cases for t > 1. For t\u2212 1\u2212 \u03c4 \u2264 t\u2032 \u2264 t\u2212 1 and any \u03b2 > 0, we have\nE ( \u2016xst \u2212 xst+1\u2016\u2016\u2206st\u2032\u2016 ) \u2264 1\n2 E\n( k\u2212\n1 2\u03b2\u22121\u2016xst \u2212 xst+1\u20162 + k 1 2\u03b2\u2016\u2206st\u2032\u20162\n) (29)\n= 1\n2 E\n( k\u2212 1 2\u03b2\u22121\u2016xst \u2212 xst+1\u20162 + k 1 2\u03b2Ej(t)\u2016\u2206st\u2032\u20162 )\n= 1\n2 E\n( k\u2212 1 2\u03b2\u22121\u2016xst \u2212 xst+1\u20162 + k\u2212 1 2\u03b2E\u2016xst\u2032 \u2212 xst\u2032+1\u20162 )\n\u2264 1 2 E\n( k\u2212\n1 2\u03b2\u22121\u2016xst \u2212 xst+1\u20162 + k\u2212 1 2 \u03c1t\u2212t \u2032 \u03b2E\u2016xst \u2212 xst+1\u20162 )\n\u03b2=\u03c1 t\u2032\u2212t 2\n\u2264 k\u2212 12\u03c1 t\u2212t \u2032\n2 E\u2016xst \u2212 xst+1\u20162\nWe assume that (20) holds \u2200t\u2032 < t. By substituting (22) into (21) and taking expectation on both sides of (21), we can have\nE ( \u2016xst\u22121 \u2212 xst\u20162 \u2212 \u2016xst \u2212 xst+1\u20162 ) (30)\n\u2264 2E ( \u2016xst\u22121 \u2212 xst\u2016\u2016xst \u2212 xt+1 \u2212 xst\u22121 + xst\u2016 ) \u2264 2E ( \u2016xst\u22121 \u2212 xst\u2016 ( 2\u2016xst \u2212 xst\u22121\u2016+ 2\u03b3 ( \u039bres t\u22121\u2211\nt\u2032=t\u22121\u2212\u03c4\n\u2016\u2206st\u2032\u2016+ \u039bnor t\u22121\u2211\nt\u2032=0\n\u2016\u2206st\u2032\u2016 )))\n= 4E ( \u2016xst\u22121 \u2212 xst\u2016\u2016xst\u22121 \u2212 xst\u2016 ) +\n4\u03b3E ( \u039bres t\u22121\u2211\nt\u2032=t\u22121\u2212\u03c4\n\u2016xst\u22121 \u2212 xst\u2016\u2016\u2206st\u2032\u2016+ \u039bnor t\u22121\u2211\nt\u2032=0\n\u2016xst\u22121 \u2212 xst\u2016\u2016\u2206st\u2032\u2016 )\n\u2264 4k\u22121/2E ( \u2016xst\u22121 \u2212 xst\u20162 ) +\n4\u03b3k\u22121/2E ( \u2016xst\u22121 \u2212 xst\u20162 ) \u00b7 ( \u039bres t\u22121\u2211\nt\u2032=t\u22121\u2212\u03c4\n\u03c1 t\u22121\u2212t\u2032\n2 + \u039bnor\nt\u22121\u2211\nt\u2032=0\n\u03c1 t\u22121\u2212t\u2032 2\n)\n= (4 + 4\u03b3 (\u039bres + \u039bnor))k \u22121/2 E ( \u2016xst\u22121 \u2212 xst\u20162 ) +\n4\u03b3k \u22121 2 E ( \u2016xst\u22121 \u2212 xst\u20162 ) ( \u039bres \u03c4\u2211\nt\u2032=1\n\u03c1 t\u2032\n2 + \u039bnor\nt\u22121\u2211\nt\u2032=1\n\u03c1 t\u2032 2\n)\n= k\u22121/2E ( \u2016xst\u22121 \u2212 xst\u20162 ) ( 4 + 4\u03b3\u039bres ( 1 +\n\u03c1 1 2 \u2212 \u03c1 \u03c4+12 1\u2212 \u03c1 12\n) + 4\u03b3\u039bnor ( 1 +\n\u03c1 1 2 \u2212 \u03c1m2 1\u2212 \u03c1 12\n))\n= k\u22121/2E ( \u2016xst\u22121 \u2212 xst\u20162 ) \u00b7 (4 + 4\u03b3 (\u039bres (1 + \u03b81) + \u039bnor (1 + \u03b82)))\nwhere the third inequality uses (29). Based on (30), we have that\nE ( \u2016xst\u22121 \u2212 xst\u20162 ) (31)\n\u2264 ( 1\u2212 k\u22121/2 (4 + 4\u03b3 (\u039bres (1 + \u03b81) + \u039bnor (1 + \u03b82))) )\u22121 \u00b7 E ( \u2016xst \u2212 xst+1\u20162 ) \u2264 \u03c1E ( \u2016xst \u2212 xst+1\u20162 )\nwhere the last inequality follows from\n\u03c1\u22121 \u2264 1\u2212 k\u22121/2 (4 + 4\u03b3 (\u039bres (1 + \u03b81) + \u039bnor (1 + \u03b82))) (32)\n\u21d4 \u03b3 \u2264 k 1/2(1\u2212 \u03c1\u22121)\u2212 4\n4 (\u039bres (1 + \u03b81) + \u039bnor (1 + \u03b82))\nThis completes the proof.\nLemma 7 Let \u03c1 be a constant that satisfies \u03c1 > 1, and define the quantities \u03b81 = \u03c1 1 2\u2212\u03c1 \u03c4+1 2\n1\u2212\u03c1 1 2\nand \u03b82 = \u03c1 1 2 \u2212\u03c1 m 2\n1\u2212\u03c1 1 2\n. Suppose the nonnegative steplength parameter \u03b3 > 0 satisfies \u03b3 \u2264\nmin {\nk1/2(1\u2212\u03c1\u22121)\u22124 4(\u039bres(1+\u03b81)+\u039bnor(1+\u03b82)) , k 1/2\n1 2 k1/2+2\u039bnor\u03b82+\u039bres\u03b81\n} . The expectation of the objective func-\ntion EF (xst ) is monotonically decreasing, i.e., EF (x s t+1) \u2264 EF (xst ).\nProof Take expectation F (xst+1) on j(t), we have that\nEj(t)F (x s t+1) = Ej(t) ( f(xst +\u2206 s t) + g(x s t+1) ) (33)\n\u2264 Ej(t) ( f(xst ) + \u2329 \u2207Gj(t)f(xst), (\u2206st )Gj(t) \u232a + Lmax\n2\n\u2225\u2225\u2225(\u2206st )Gj(t) \u2225\u2225\u2225 2\n+gGj(t) ( (xst+1)Gj(t) ) + \u2211\nj\u2032 6=j(t)\ngGj\u2032 ( (xst+1)Gj\u2032\n)  \n= f(xst) + k \u2212 1 k g(xst ) + Ej(t) (\u2329 \u2207Gj(t)f(xst), (\u2206st )Gj(t) \u232a + Lmax 2 \u2225\u2225\u2225(\u2206st )Gj(t) \u2225\u2225\u2225 2\n+gGj(t) ( (xst+1)Gj(t) ))\n= F (xst ) + Ej(t) (\u2329 (v\u0302st )Gj(t) , (\u2206 s t )Gj(t) \u232a + Lmax\n2\n\u2225\u2225\u2225(\u2206st )Gj(t) \u2225\u2225\u2225 2 + gGj(t) ( (xst+1)Gj(t) )\n\u2212gGj(t) ( (xst )Gj(t) ) + \u2329 \u2207Gj(t)f(xst )\u2212 (v\u0302st )Gj(t) , (\u2206st )Gj(t) \u232a)\n\u2264 F (xst ) + Ej(t) ( \u2212Lmax\n\u03b3\n\u2225\u2225\u2225(\u2206st )Gj(t) \u2225\u2225\u2225 2 + Lmax\n2\n\u2225\u2225\u2225(\u2206st )Gj(t) \u2225\u2225\u2225 2 +\n+ \u2329 \u2207Gj(t)f(xst )\u2212 (v\u0302st )Gj(t) , (\u2206st )Gj(t) \u232a)\n= F (xst ) + Ej(t)\n(( Lmax\n2 \u2212 Lmax \u03b3\n)\u2225\u2225\u2225(\u2206st )Gj(t) \u2225\u2225\u2225 2 ) + Ej(t) \u2329 \u2207Gj(t)f(xst)\u2212 (v\u0302st )Gj(t) , (\u2206st )Gj(t) \u232a\n= F (xst ) + Lmax\nk\n( 1\n2 \u2212 1 \u03b3\n) \u2016xst+1 \u2212 xst\u20162 + Ej(t) \u2329 \u2207Gj(t)f(xst)\u2212 (v\u0302st )Gj(t) , (\u2206st )Gj(t) \u232a\nwhere the first inequality uses (6), and the second inequality uses (17) in Lemma 5. Consider the expectation of the last term on the right-hand side of (33), we have\nE \u2329 \u2207Gj(t)f(xst )\u2212 (v\u0302st )Gj(t) , (\u2206st )Gj(t) \u232a (34)\n= E \u2329 \u2207Gj(t)f(xst )\u2212 (\u2207fit(x\u0302st )\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s))Gj(t) , (\u2206 s t )Gj(t) \u232a = E \u2329 \u2207Gj(t)f(xst )\u2212 (\u2207fit(x\u0302st )\u2212\u2207fit(xst ) +\u2207fit(xst )\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s))Gj(t) , (\u2206st )Gj(t) \u232a = E \u2329 \u2207Gj(t)f(xst )\u2212\u2207Gj(t)f(x\u0303s), (\u2206st )Gj(t) \u232a + E \u2329 \u2207Gj(t)fit(xst )\u2212\u2207Gj(t)fit(x\u0302st ), (\u2206st )Gj(t) \u232a\n+E \u2329 \u2207Gj(t)fit(x\u0303s)\u2212\u2207Gj(t)fit(xst ), (\u2206st )Gj(t) \u232a\n\u2264 E (\u2225\u2225\u2225\u2207Gj(t)f(xst )\u2212\u2207Gj(t)f(x\u0303s) \u2225\u2225\u2225 \u2016\u2206st\u2016 ) + E (\u2225\u2225\u2225\u2207Gj(t)fit(xst )\u2212\u2207Gj(t)fit(x\u0302st ) \u2225\u2225\u2225 \u2016\u2206st\u2016 )\n+E (\u2225\u2225\u2225\u2207Gj(t)fit(x\u0303s)\u2212\u2207Gj(t)fit(xst ) \u2225\u2225\u2225 \u2016\u2206st\u2016 )\n= 1 k E ( \u2016\u2207f(xst)\u2212\u2207f(x\u0303s)\u2016 \u2225\u2225xst+1 \u2212 xst \u2225\u2225)+ 1 k E ( \u2016\u2207fit(xst )\u2212\u2207fit(x\u0302st )\u2016 \u2225\u2225xst+1 \u2212 xst \u2225\u2225)\n+ 1 k E ( \u2016\u2207fit(x\u0303s)\u2212\u2207fit(xst )\u2016 \u2225\u2225xst+1 \u2212 xst \u2225\u2225)\n\u2264 1 k E\n 2Lnor\u2016xst \u2212 x\u0303s\u2016\u2016xst+1 \u2212 xst\u2016+ Lres \u2211\nt\u2032\u2208K(t)\n\u2016\u2206st\u2032\u2016 \u2016xst+1 \u2212 xst\u2016\n \n\u2264 1 k E\n( 2Lnor t\u22121\u2211\nt\u2032=0\n\u2016\u2206st\u2032\u2016\u2016xst+1 \u2212 xst\u2016+ Lres t\u22121\u2211\nt\u2032=t\u2212\u03c4\n\u2016\u2206st\u2032\u2016\u2016xst+1 \u2212 xst\u2016 )\n\u2264 2Lnor t\u22121\u2211\nt\u2032=0\n\u03c1 t\u2212t\u2032 2 k3/2 E\u2016xst+1 \u2212 xst\u20162 + Lres t\u22121\u2211\nt\u2032=t\u2212\u03c4\n\u03c1 t\u2212t\u2032 2 k3/2 E\u2016xst+1 \u2212 xst\u20162\n= k\u22123/2 ( 2Lnor\n\u03c1 1 2 \u2212 \u03c1m2 1\u2212 \u03c1 12 + Lres \u03c1 1 2 \u2212 \u03c1 \u03c4+12 1\u2212 \u03c1 12\n) \u00b7 E\u2016xst+1 \u2212 xst\u20162\n= k\u22123/2 (2Lnor\u03b82 + Lres\u03b81)E\u2016xst+1 \u2212 xst\u20162\nwhere the first inequality uses the Cauchy-Schwarz inequality (Callebaut, 1965), the third inequality uses (3) and (4), the sixth inequality uses (29).\nBy taking expectations on both sides of (33) and substituting (34), we have\nEF (xst+1) (35)\n\u2264 EF (xst ) + Lmax\nk\n( 1\n2 \u2212 1 \u03b3\n) E\u2016xst+1 \u2212 xst\u20162 + E \u2329 \u2207Gj(t)f(xst)\u2212 (v\u0302st )Gj(t) , (\u2206st )Gj(t) \u232a\n\u2264 EF (xst )\u2212 1 k \u00b7 ( Lmax ( 1 \u03b3 \u2212 1 2 ) \u2212 2Lnor\u03b82 + Lres\u03b81 k1/2 ) E\u2016xst+1 \u2212 xst\u20162\nwhere Lmax ( 1 \u03b3 \u2212 12 ) \u2212 2Lnor\u03b82+Lres\u03b81 k1/2 \u2265 0 because that \u03b3\u22121 \u2265 12 + 2\u039bnor\u03b82+\u039bres\u03b81k1/2 . This completes the proof.\nTheorem 8 Let \u03c1 be a constant that satisfies \u03c1 > 1, and define the quantity \u03b8\u2032 = \u03c1 \u03c4+1\u2212\u03c1 \u03c1\u22121 . Suppose the nonnegative steplength parameter \u03b3 > 0 satisfies 1\u2212\u039bnor\u03b3\u2212\u03b3\u03c4\u03b8 \u2032 n \u2212 2(\u039bres\u03b81+\u039bnor\u03b82)\u03b3 n1/2 \u2265 0. If the optimal strong convexity holds for f with l > 0, we have\nEF (xs)\u2212 F \u2217 \u2264 Lmax 2\u03b3\n( 1\n1 + 2m\u03b3l2k(l\u03b3+Lmax)\n)s \u00b7 ( \u2016x0 \u2212 PS(x0)\u20162 + 2\u03b3\nLmax\n( EF (x0)\u2212 F \u2217 )) (36)\nIf f is a general smooth convex function, we have\nEF (xs)\u2212 F \u2217 \u2264 nLmax\u2016x 0 \u2212 PS(x0)\u20162 + 2\u03b3k\n( F (x0)\u2212 F \u2217 )\n2\u03b3k + 2m\u03b3s (37)\nProof We have that\n\u2016xst+1 \u2212PS(xst+1)\u20162 \u2264 \u2016xst+1 \u2212 PS(xst )\u20162 = \u2016xst +\u2206st \u2212 PS(xst )\u20162 (38) = \u2016xst \u2212PS(xst )\u20162 \u2212 \u2016\u2206st\u20162 \u2212 2\u3008(PS(xst )\u2212 xst \u2212\u2206st )Gj(t) , (\u2206 s t )Gj(t)\u3009 = \u2016xst \u2212PS(xst )\u20162 \u2212 \u2016\u2206st\u20162 \u2212 2\u3008 ( PS(xst )\u2212 xst+1 ) Gj(t) , (\u2206st )Gj(t)\u3009 \u2264 \u2016xst \u2212PS(xst )\u20162 \u2212 \u2016\u2206st\u20162 + 2\u03b3\nLmax\n( \u3008 ( PS(xst )\u2212 xst+1 ) Gj(t) , (v\u0302st )Gj(t)\u3009 ) +\n2\u03b3\nLmax\n( gGj(t)(PS(xst )Gj(t))\u2212 gj(t)(xst+1)Gj(t)) )\n= \u2016xst \u2212PS(xst )\u20162 \u2212 \u2016\u2206st\u20162 + 2\u03b3\nLmax\n( \u3008(PS(xst )\u2212 xst )Gj(t) , (v\u0302 s t )Gj(t)\u3009 ) \ufe38 \ufe37\ufe37 \ufe38 T1 +\n2\u03b3\nLmax\n( \u3008(\u2206st)Gj(t) , (v\u0302 s t )Gj(t)\u3009 ) \ufe38 \ufe37\ufe37 \ufe38 T2 + 2\u03b3 Lmax ( gj(t)(PS(xst )Gj(t))\u2212 gGj(t)(xst+1)Gj(t)) )\nwhere the first inequality comes from the definition of function PS(x) = argminy\u2208S \u2016y\u2212x\u20162, and the second inequality uses (17) in Lemma 5. For the expectation of T1, we have\nE(T1) = E ( \u3008(PS(xst )\u2212 xst )Gj(t) , (v\u0302 s t )Gj(t)\u3009 ) (39)\n= 1\nk E\u3008PS(xst )\u2212 xst , v\u0302st \u3009\n= 1\nk E\u3008PS(xst )\u2212 xst ,\u2207fit(x\u0302st )\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s)\u3009\n= 1\nk E\u3008PS(xst )\u2212 xst ,\u2207fit(x\u0302st )\u3009+\n1 k \u3008E(PS(xst )\u2212 xst ),E(\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s))\u3009\n= 1\nk E\u3008PS(xst )\u2212 x\u0302st ,\u2207fit(x\u0302st )\u3009+\n1 k E\u3008x\u0302st \u2212 xst ,\u2207fit(x\u0302st )\u3009\n= 1\nk E\u3008PS(xst )\u2212 x\u0302st ,\u2207fit(x\u0302st )\u3009+\n1 k E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u3008x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1,\u2207fit(x\u0302st )\u3009\n= 1\nk E\u3008PS(xst )\u2212 x\u0302st ,\u2207fit(x\u0302st )\u3009+\n1 k E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u3008x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1,\u2207fit(x\u0302st,t\u2032)\u3009\n+ 1\nk E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u3008x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1,\u2207fit(x\u0302st )\u2212\u2207fit(x\u0302st,t\u2032)\u3009\n\u2264 1 k E (fit(PS(xst ))\u2212 fit(x\u0302st )) + 1 k E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u3008x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1,\u2207fit(x\u0302st )\u2212\u2207fit(x\u0302st,t\u2032)\u3009\n+ 1\nk E\n|K(t)|\u22121\u2211\nt\u2032=0\n( fit(x\u0302 s t,t\u2032)\u2212 fit(x\u0302st,t\u2032+1) + Lmax\n2 \u2016x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1\u20162\n)\n= 1\nk E (fit(PS(xst ))\u2212 fit(xst )) +\n1 k E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u3008x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1,\u2207fit(x\u0302st )\u2212\u2207fit(x\u0302st,t\u2032)\u3009\n+ Lmax\n2k E\n|K(t)|\u22121\u2211\nt\u2032=0\n( \u2016x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1\u20162 )\n= 1\nk E (fit(PS(xst ))\u2212 fit(xst )) +\nLmax\n2k E\n|K(t)|\u22121\u2211\nt\u2032=0\n( \u2016x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1\u20162 )\n+ 1\nk E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u3008x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1, t\u2032\u22121\u2211\nt\u2032\u2032=0\n\u2207fit(x\u0302st\u2032\u2032)\u2212\u2207fit(x\u0302st,t\u2032\u2032+1)\u3009\n\u2264 1 k E (fit(PS(xst ))\u2212 fit(xst )) + Lmax 2k E\n|K(t)|\u22121\u2211\nt\u2032=0\n( \u2016x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1\u20162 )\n+ Lmax\nk E\n|K(t)|\u22121\u2211\nt\u2032=0\n( \u2225\u2225x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1 \u2225\u2225 t\u2032\u22121\u2211\nt\u2032\u2032=0\n\u2225\u2225x\u0302st\u2032\u2032 \u2212 x\u0302st,t\u2032\u2032+1 \u2225\u2225 )\n= 1\nk E (fit(PS(xst ))\u2212 fit(xst )) +\nLmax\n2k E\n|K(t)|\u22121\u2211\nt\u2032=0\n( \u2016x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1\u20162 )\n+ Lmax\n2k E\n    |K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1 \u2225\u2225   2 \u2212 |K(t)|\u22121\u2211\nt\u2032=0\n( \u2016x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1\u20162\n)  \n= 1\nk E (fit(PS(xst ))\u2212 fit(xst )) +\nLmax\n2k E\n  |K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1 \u2225\u2225   2\n\u2264 1 k E (fit(PS(xst ))\u2212 fit(xst )) + Lmax\u03c4 2k E\n|K(t)|\u22121\u2211\nt\u2032=0\n\u2225\u2225x\u0302st,t\u2032 \u2212 x\u0302st,t\u2032+1 \u2225\u22252\n= 1\nk E (fit(PS(xst ))\u2212 fit(xst )) +\nLmax\u03c4\n2k\n|K(t)|\u22121\u2211\nt\u2032=0\nE \u2016Bst\u2032\u2206st\u2032\u20162\n\u2264 1 k E (fit(PS(xst ))\u2212 fit(xst )) + Lmax\u03c4 2k2\n|K(t)|\u22121\u2211\nt\u2032=0\nE \u2225\u2225xst\u2032+1 \u2212 xst\u2032 \u2225\u22252\n\u2264 1 k E (fit(PS(xst ))\u2212 fit(xst )) + Lmax\u03c4 2k2\n\u03c4\u2211\nt\u2032=1\n\u03c1t \u2032 E \u2225\u2225xst+1 \u2212 xst \u2225\u22252\n\u2264 1 k E (f(PS(xst ))\u2212 f(xst)) +\nLmax\u03c4\u03b8 \u2032\n2k2 E(\u2016xst \u2212 xst+1\u20162)\nwhere the fifth equality comes from that xst is independent to it, the sixth equality uses Lemma 4, the first inequality uses the convexity of fi and (6), the second inequality uses (5). For the expectation of T2, we have\nE(T2) = E\u3008(\u2206st )Gj(t) , (v\u0302 s t )Gj(t)\u3009 (40)\n= E\u3008(\u2206t)Gj(t) , (\u2207fit(x\u0302 s t )\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s))Gj(t)\u3009 = E\u3008(\u2206t)Gj(t) , (\u2207fit(x\u0302 s t )\u2212\u2207fit(xst ) +\u2207fit(xst )\u2212\u2207fit(x\u0303s) +\u2207f(x\u0303s))Gj(t)\u3009 = E\u3008(\u2206t)Gj(t) , (\u2207fit(x\u0302 s t )\u2212\u2207fit(xst ))Gj(t)\u3009+\nE\u3008(\u2206t)Gj(t) , (\u2207fit(x s t )\u2212\u2207fit(x\u0303s))Gj(t)\u3009+ E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n\u2264 1 k E ( \u2016xst+1 \u2212 xst\u2016 \u2016\u2207fit(x\u0302st )\u2212\u2207fit(xst )\u2016 ) +\n1 k E ( \u2016xst+1 \u2212 xst\u2016 \u2016\u2207fit(xst )\u2212\u2207fit(x\u0303s)\u2016 ) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n\u2264 Lres k\n  \u2211\nt\u2032\u2208K(t)\n\u2016xst+1 \u2212 xst\u2016\u2016\u2206st\u2032\u2016\n \nLnor k E ( \u2016xst+1 \u2212 xst\u2016\u2016xst \u2212 xs\u2016 ) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n\u2264 Lres k\n  \u2211\nt\u2032\u2208K(t)\n\u2016xst+1 \u2212 xst\u2016\u2016\u2206st\u2032\u2016\n \nLnor\nk E\n( t\u22121\u2211\nt\u2032=0\n\u2016xst+1 \u2212 xst\u2016\u2016\u2206st\u2032\u2016 ) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n\u2264 Lres k3/2\nt\u22121\u2211\nt\u2032=t\u2212\u03c4\n\u03c1(t\u2212t \u2032)/2 E(\u2016xst \u2212 xst+1\u20162)\n+ Lnor\nk3/2\nt\u22121\u2211\nt\u2032=0\n\u03c1(t\u2212t \u2032)/2 E(\u2016xst \u2212 xst+1\u20162) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n= 1\nk3/2 (Lres\u03b81 + Lnor\u03b82)E(\u2016xst \u2212 xst+1\u20162) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n\u2264 Lres\u03b81 + Lnor\u03b82 k3/2 E(\u2016xst \u2212 xst+1\u20162) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\nwhere the second inequality uses Lemma 4, the fourth inequality uses (29). By substituting the upper bounds from (39) and (40) into (38), we have\nE\u2016xst+1 \u2212 PS(xst+1)\u20162 (41)\n\u2264 E\u2016xst \u2212 PS(xst )\u20162 \u2212 1\nk E(\u2016xst \u2212 xst+1\u20162) +\n2\u03b3\nLmaxk E (f(PS(xst ))\u2212 f(xst)) +\n\u03b3\u03c4\u03b8\u2032\nk2 E(\u2016xst \u2212 xst+1\u20162)\n+ 2\u03b3\nLmax\n( Lres\u03b81 + Lnor\u03b82\nk3/2 \u03b8E(\u2016xst \u2212 xst+1\u20162) + E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n1 k Eg(PS(xst ))\u2212 Eg(xst+1) + k \u2212 1 k Eg(xst )\n)\n= E\u2016xst \u2212 PS(xst )\u20162 + 2\u03b3\nLmaxk E (f(PS(xst ))\u2212 f(xst ))\u2212\n1\nk\n( 1\u2212 \u03b3\u03c4\u03b8 \u2032\nk \u2212 2(Lres\u03b81 + Lnor\u03b82)\u03b3\nk1/2Lmax\n) E(\u2016xst \u2212 xst+1\u20162) + 2\u03b3\nLmax\n( E\u3008(\u2206t)Gj(t) ,\u2207Gj(t)f(x\u0303 s)\u3009\n1 k Eg(PS(xst ))\u2212 Eg(xst+1) + k \u2212 1 k Eg(xst )\n)\nWe consider a fixed stage s+ 1 such that xs+10 = x s m. By summing the the inequality (42) over t = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1, we obtain\nE\u2016xs+1 \u2212 PS(xs+1)\u20162 (42)\n\u2264 E\u2016xs \u2212 PS(xs)\u20162 + m\u22121\u2211\nt\u2032=0\n2\u03b3 Lmaxk E ( f(PS(xs+1t\u2032 ))\u2212 f(xs+1t\u2032 ) ) \u2212\nm\u22121\u2211\nt\u2032=0\n1\nk\n( 1\u2212 \u03b3\u03c4\u03b8 \u2032\nk \u2212 2(Lres\u03b81 + Lnor\u03b82)\u03b3\nk1/2Lmax\n) \u00b7 E(\u2016xs+1t\u2032 \u2212 xs+1t\u2032+1\u20162)\n+ 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\nE \u2329 (\u2206t\u2032)Gj(t\u2032) ,\u2207Gj(t\u2032)f(x\u0303 s) \u232a\n+ 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\n( 1\nk Eg(PS(xs+1t\u2032 ))\u2212 Eg(xs+1t\u2032+1) + n\u2212 1 k Eg(xs+1t\u2032 )\n)\n= E\u2016xs \u2212 PS(xs)\u20162 + m\u22121\u2211\nt\u2032=0\n2\u03b3 Lmaxk E ( f(PS(xs+1t\u2032 ))\u2212 f(xs+1t\u2032 ) ) \u2212\nm\u22121\u2211\nt\u2032=0\n1\nk\n( 1\u2212 \u03b3\u03c4\u03b8 \u2032\nk \u2212 2(Lres\u03b81 + Lnor\u03b82)\u03b3\nk1/2Lmax\n) \u00b7 E(\u2016xs+1t\u2032 \u2212 xs+1t\u2032+1\u20162)\n+ 2\u03b3 Lmax E \u2329 xs+1 \u2212 xs,\u2207f(x\u0303s) \u232a\n+ 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\n( 1\nk Eg(PS(xs+1t\u2032 ))\u2212 Eg(xs+1t\u2032+1) + k \u2212 1 k Eg(xs+1t\u2032 )\n)\n\u2264 E\u2016xs \u2212 PS(xs)\u20162 + m\u22121\u2211\nt\u2032=0\n2\u03b3 Lmaxk E ( f(PS(xs+1t\u2032 ))\u2212 f(xs+1t\u2032 ) ) \u2212\nm\u22121\u2211\nt\u2032=0\n1\nk\n( 1\u2212 \u03b3\u03c4\u03b8 \u2032\nk \u2212 2(Lres\u03b81 + Lnor\u03b82)\u03b3\nk1/2Lmax\n) \u00b7 E(\u2016xs+1t\u2032 \u2212 xs+1t\u2032+1\u20162)\n+ 2\u03b3\nLmax E\n( f(xs)\u2212 f(xs+1) + Lnor\n2 \u2016xs+1 \u2212 xs\u20162\n)\n+ 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\n( 1\nk Eg(PS(xs+1t\u2032 ))\u2212 Eg(xs+1t\u2032+1) + k \u2212 1 k Eg(xs+1t\u2032 )\n)\n= E\u2016xs \u2212 PS(xs)\u20162 + m\u22121\u2211\nt\u2032=0\n2\u03b3 Lmaxk E ( f(PS(xs+1t\u2032 ))\u2212 f(xs+1t\u2032 ) ) \u2212\nm\u22121\u2211\nt\u2032=0\n1\nk\n( 1\u2212 \u03b3\u03c4\u03b8 \u2032\nk \u2212 2(Lres\u03b81 + Lnor\u03b82)\u03b3\nk1/2Lmax\n) \u00b7 E(\u2016xs+1t\u2032 \u2212 xs+1t\u2032+1\u20162)\n+ 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\nE ( f(xs+1t\u2032 )\u2212 f(xs+1t+1 ) ) + Lnor\u03b3\nLmax E \u2225\u2225\u2225\u2225\u2225 m\u22121\u2211\nt\u2032=0\n( xs+1t\u2032 \u2212 xs+1t\u2032+1 ) \u2225\u2225\u2225\u2225\u2225 2\n+ 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\n( 1\nk Eg(PS(xs+1t\u2032 ))\u2212 Eg(xs+1t\u2032+1) + k \u2212 1 k Eg(xs+1t\u2032 )\n)\n\u2264 E\u2016xs \u2212 PS(xs)\u20162 + 2\u03b3\nLmaxk\nm\u22121\u2211\nt\u2032=0\n( F \u2217 \u2212 EF (xs+1t\u2032 ) ) + 2\u03b3\nLmax\nm\u22121\u2211\nt\u2032=0\n( EF (xs+1t\u2032 )\u2212 EF (xs+1t\u2032+1) )\n\u2212 m\u22121\u2211\nt\u2032=0\n1\nk\n( 1\u2212 \u039bnor\u03b3 \u2212 \u03b3\u03c4\u03b8\u2032\nn \u2212 2(\u039bres\u03b81 +\u039bnor\u03b82)\u03b3 k1/2\n) \u00b7 E(\u2016xs+1t\u2032 \u2212 xs+1t\u2032+1\u20162)\n\u2264 E\u2016xs \u2212 PS(xs)\u20162 + 2\u03b3\nLmaxk\nm\u22121\u2211\nt\u2032=0\n( F \u2217 \u2212 EF (xs+1t\u2032 ) ) + 2\u03b3\nLmax\n( EF (xs)\u2212 EF (xs+1) )\nwhere the second inequality uses (3), the final inequality comes from 1 \u2212 \u039bnor\u03b3 \u2212 \u03b3\u03c4\u03b8 \u2032 n \u2212 2(\u039bres\u03b81+\u039bnor\u03b82)\u03b3\nn1/2 \u2265 0. Define S(xs) = E\u2016xt \u2212PS(xs)\u20162 + 2\u03b3LmaxE (F (x s)\u2212 F \u2217). According to\n(42), we have\nS(xs+1) \u2264 S(xs)\u2212 2\u03b3 Lmaxk\nm\u22121\u2211\nt\u2032=0\nE ( F (xs+1t\u2032 )\u2212 F \u2217 ) \u2264 S(xs)\u2212 2m\u03b3 Lmaxk E ( F (xs+1)\u2212 F \u2217 ) (43)\nwhere the second inequality comes from the monotonicity of EF (xst ). According to (43), we have\nS(xs) \u2264 S(x0)\u2212 2m\u03b3s Lmaxk E (F (xs)\u2212 F \u2217) (44)\nThus, the sublinear convergence rate (37) for general smooth convex function f can be obtained from (44).\nIf the optimal strong convexity for the smooth convex function f holds with l > 0, we have (45) as proved in (A.28) of Liu and Wright (2015).\nE (F (xs)\u2212 F \u2217) \u2265 Lmaxl 2(l\u03b3 + Lmax) S(xs) (45)\nThus, substituting (46) into (43), we have\nS(xs+1) \u2264 S(xs)\u2212 2m\u03b3l 2k(l\u03b3 + Lmax) S(xs+1) (46)\nBased on (46), we have (47) by induction.\nS(xs) \u2264 (\n1\n1 + 2m\u03b3l2k(l\u03b3+Lmax)\n)s S(x0) (47)\nThus, the linear convergence rate (36) for the optimal strong convexity on f can be obtained from (47).\n5. Conclusion\nIn this paper, we propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lockfree in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.\nReferences\nHaim Avron, Alex Druinsky, and Anshul Gupta. Revisiting asynchronous linear solvers: Provable convergence rate through randomization. Journal of the ACM (JACM), 62(6): 51, 2015.\nMathieu Blondel, Kazuhiro Seki, and Kuniaki Uehara. Block coordinate descent algorithms for large-scale sparse multiclass classification. Machine learning, 93(1):31\u201352, 2013.\nDK Callebaut. Generalization of the cauchy-schwarz inequality. Journal of mathematical analysis and applications, 12(3):491\u2013494, 1965.\nSorathan Chaturapruek, John C Duchi, and Christopher Re\u0301. Asynchronous stochastic convex optimization: the noise is in the noise and sgd don\u2019t care. In Advances in Neural Information Processing Systems, pages 1531\u20131539, 2015.\nSaeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.\nCho-Jui Hsieh, Hsiang-Fu Yu, and Inderjit S Dhillon. Passcode: Parallel asynchronous stochastic dual co-ordinate descent. arXiv preprint, 2015.\nZhouyuan Huo and Heng Huang. Asynchronous stochastic gradient descent with variance reduction for non-convex optimization. arXiv preprint arXiv:1604.03584, 2016a.\nZhouyuan Huo and Heng Huang. Distributed asynchronous dual-free stochastic dual coordinate ascent. arXiv preprint arXiv:1605.09066, 2016b.\nZhouyuan Huo, Bin Gu, and Heng Huang. Decoupled asynchronous proximal stochastic gradient descent with variance reduction. arXiv preprint arXiv:1609.06804, 2016.\nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013 323, 2013.\nXiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2737\u20132745, 2015.\nXiangru Lian, Huan Zhang, Cho-Jui Hsieh, Yijun Huang, and Ji Liu. A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zerothorder to first-order. arXiv preprint arXiv:1606.00498, 2016.\nQihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances in Neural Information Processing Systems, pages 3059\u20133067, 2014.\nJi Liu and Stephen J Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization, 25(1):351\u2013376, 2015.\nJi Liu, Stephen J Wright, Christopher Re\u0301, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. Journal of Machine Learning Research, 16(285-322):1\u20135, 2015.\nZhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent methods. Mathematical Programming, 152(1-2):615\u2013642, 2015.\nHoria Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. arXiv preprint arXiv:1507.06970, 2015.\nAtsushi Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Advances in Neural Information Processing Systems, pages 1574\u20131582, 2014.\nBenjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693\u2013701, 2011.\nSashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex J Smola. On variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in Neural Information Processing Systems, pages 2647\u20132655, 2015.\nPeter Richta\u0301rik and Martin Taka\u0301c\u030c. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1-2):433\u2013484, 2016.\nVolker Roth and Bernd Fischer. The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms. In Proceedings of the 25th international conference on Machine learning, pages 848\u2013855. ACM, 2008.\nAndrzej P Ruszczyn\u0301ski. Nonlinear Optimization. Number 13 in Nonlinear optimization. Princeton University Press, 2006.\nMark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\nShai Shalev-Shwartz. SDCA without duality, regularization, and individual convexity. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 747\u2013754, 2016. URL http://jmlr.org/proceedings/papers/v48/shalev-shwartza16.html.\nShai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In ICML, pages 64\u201372, 2014.\nMartin Taka\u0301c. Randomized coordinate descent methods for big data optimization. 2014.\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996.\nLin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.\nTong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the twenty-first international conference on Machine learning, page 116. ACM, 2004.\nShen-Yi Zhao and Wu-Jun Li. Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\nTuo Zhao, Mo Yu, Yiming Wang, Raman Arora, and Han Liu. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems, pages 3329\u20133337, 2014."}], "references": [{"title": "Revisiting asynchronous linear solvers: Provable convergence rate through randomization", "author": ["Haim Avron", "Alex Druinsky", "Anshul Gupta"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Avron et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2015}, {"title": "Block coordinate descent algorithms for large-scale sparse multiclass classification", "author": ["Mathieu Blondel", "Kazuhiro Seki", "Kuniaki Uehara"], "venue": "Machine learning,", "citeRegEx": "Blondel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2013}, {"title": "Generalization of the cauchy-schwarz inequality", "author": ["DK Callebaut"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "Callebaut.,? \\Q1965\\E", "shortCiteRegEx": "Callebaut.", "year": 1965}, {"title": "Asynchronous stochastic convex optimization: the noise is in the noise and sgd don\u2019t care", "author": ["Sorathan Chaturapruek", "John C Duchi", "Christopher R\u00e9"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chaturapruek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chaturapruek et al\\.", "year": 2015}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi and Lan.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2013}, {"title": "Passcode: Parallel asynchronous stochastic dual co-ordinate descent", "author": ["Cho-Jui Hsieh", "Hsiang-Fu Yu", "Inderjit S Dhillon"], "venue": "arXiv preprint,", "citeRegEx": "Hsieh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2015}, {"title": "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization", "author": ["Zhouyuan Huo", "Heng Huang"], "venue": "arXiv preprint arXiv:1604.03584,", "citeRegEx": "Huo and Huang.,? \\Q2016\\E", "shortCiteRegEx": "Huo and Huang.", "year": 2016}, {"title": "Distributed asynchronous dual-free stochastic dual coordinate ascent", "author": ["Zhouyuan Huo", "Heng Huang"], "venue": "arXiv preprint arXiv:1605.09066,", "citeRegEx": "Huo and Huang.,? \\Q2016\\E", "shortCiteRegEx": "Huo and Huang.", "year": 2016}, {"title": "Decoupled asynchronous proximal stochastic gradient descent with variance reduction", "author": ["Zhouyuan Huo", "Bin Gu", "Heng Huang"], "venue": "arXiv preprint arXiv:1609.06804,", "citeRegEx": "Huo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huo et al\\.", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zerothorder to first-order", "author": ["Xiangru Lian", "Huan Zhang", "Cho-Jui Hsieh", "Yijun Huang", "Ji Liu"], "venue": "arXiv preprint arXiv:1606.00498,", "citeRegEx": "Lian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2016}, {"title": "An accelerated proximal coordinate gradient method", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["Ji Liu", "Stephen J Wright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Liu and Wright.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Wright.", "year": 2015}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Zhaosong Lu", "Lin Xiao"], "venue": "Mathematical Programming,", "citeRegEx": "Lu and Xiao.,? \\Q2015\\E", "shortCiteRegEx": "Lu and Xiao.", "year": 2015}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1507.06970,", "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["Atsushi Nitanda"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nitanda.,? \\Q2014\\E", "shortCiteRegEx": "Nitanda.", "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex J Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Reddi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2016\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2016}, {"title": "The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms", "author": ["Volker Roth", "Bernd Fischer"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Roth and Fischer.,? \\Q2008\\E", "shortCiteRegEx": "Roth and Fischer.", "year": 2008}, {"title": "Nonlinear Optimization. Number 13 in Nonlinear optimization", "author": ["Andrzej P Ruszczy\u0144ski"], "venue": null, "citeRegEx": "Ruszczy\u0144ski.,? \\Q2006\\E", "shortCiteRegEx": "Ruszczy\u0144ski.", "year": 2006}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "SDCA without duality, regularization, and individual convexity", "author": ["Shai Shalev-Shwartz"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2016}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2014}, {"title": "Randomized coordinate descent methods for big data optimization", "author": ["Martin Tak\u00e1c"], "venue": null, "citeRegEx": "Tak\u00e1c.,? \\Q2014\\E", "shortCiteRegEx": "Tak\u00e1c.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Zhang.", "year": 2014}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}, {"title": "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee", "author": ["Shen-Yi Zhao", "Wu-Jun Li"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhao and Li.,? \\Q2016\\E", "shortCiteRegEx": "Zhao and Li.", "year": 2016}, {"title": "Accelerated mini-batch randomized block coordinate descent method", "author": ["Tuo Zhao", "Mo Yu", "Yiming Wang", "Raman Arora", "Han Liu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 9, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 23, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 4, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 19, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 26, "context": "Also, stochastic coordinate descent (SCD) algorithms (Tak\u00e1c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 53, "endOffset": 104}, {"referenceID": 31, "context": "Also, stochastic coordinate descent (SCD) algorithms (Tak\u00e1c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 53, "endOffset": 104}, {"referenceID": 15, "context": "Also, stochastic coordinate descent (SCD) algorithms (Tak\u00e1c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 53, "endOffset": 104}, {"referenceID": 25, "context": ", 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 76, "endOffset": 130}, {"referenceID": 24, "context": ", 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 76, "endOffset": 130}, {"referenceID": 28, "context": "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).", "startOffset": 80, "endOffset": 135}, {"referenceID": 12, "context": "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).", "startOffset": 80, "endOffset": 135}, {"referenceID": 17, "context": "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).", "startOffset": 80, "endOffset": 135}, {"referenceID": 20, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 3, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 14, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 18, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 19, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 11, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 13, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 30, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 10, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 0, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 5, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 16, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 8, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 23, "context": "For example, Zhao and Li (2016) proposed an asynchronous parallel algorithm for SVRG and proved the linear convergence.", "startOffset": 13, "endOffset": 32}, {"referenceID": 8, "context": "Liu and Wright (2015) proposed an asynchronous parallel algorithm for SCD and proved the linear convergence.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG.", "startOffset": 0, "endOffset": 118}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence.", "startOffset": 0, "endOffset": 256}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence. Lian et al. (2016) proposed an asynchronous stochastic optimization algorithm with zeroth order and proved the convergence.", "startOffset": 0, "endOffset": 402}, {"referenceID": 27, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 21, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al.", "startOffset": 130, "endOffset": 154}, {"referenceID": 1, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on.", "startOffset": 189, "endOffset": 211}, {"referenceID": 13, "context": "Thus, the formulation in (Liu and Wright, 2015) is a special case of (1).", "startOffset": 25, "endOffset": 47}, {"referenceID": 13, "context": "Each iteration in (Liu and Wright, 2015) only modifies a single component of x which is an atomic operation in the parallel system with shared memory.", "startOffset": 18, "endOffset": 40}, {"referenceID": 13, "context": "However, we need to modify a block coordinate Gj of x with lockfree for each iteration, which are more complicated in the asynchronous parallel analysis than the atomic updating in (Liu and Wright, 2015).", "startOffset": 181, "endOffset": 203}, {"referenceID": 1, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on. Note that, Liu and Wright (2015) consider the formulation (1) with the constraints that |Gj | = 1 for all j.", "startOffset": 190, "endOffset": 257}, {"referenceID": 13, "context": "And several examples of optimally strongly convex functions that are not strongly convex are provided in (Liu and Wright, 2015).", "startOffset": 105, "endOffset": 127}, {"referenceID": 13, "context": "As mentioned in Liu and Wright (2015), the condition of optimal strong convexity is significantly weaker than the normal strong convexity condition.", "startOffset": 16, "endOffset": 38}, {"referenceID": 13, "context": "Convergence Analysis In this section, we follow the analysis of (Liu and Wright, 2015) and prove the convergence rate of AsyDSCDVR (Theorem 8).", "startOffset": 64, "endOffset": 86}, {"referenceID": 16, "context": "Same as mentioned in (Mania et al., 2015), there might not be an actual time the idea ones exist in the shared memory, except the first and last iterates for each outer loop.", "startOffset": 21, "endOffset": 41}, {"referenceID": 22, "context": "x\\Gj(t) = (x s t )\\Gj(t) If xt+1 is the solution of (18), the solution of optimization problem (19) is also x s t+1 according to the subdifferential version of Karush-Kuhn-Tucker (KKT) conditions (Ruszczy\u0144ski., 2006).", "startOffset": 196, "endOffset": 216}, {"referenceID": 13, "context": "8) in Liu and Wright (2015), we have \u2016xt\u22121 \u2212 xt\u2016 \u2212 \u2016xt \u2212 xt+1\u2016 \u2264 2\u2016xt\u22121 \u2212 xt\u2016\u2016xt \u2212 xt+1 \u2212 xt\u22121 + xt\u2016 (21) The second part in the right half side of (21) is bound as follows if B = {it} and J(t) = {j(t)}.", "startOffset": 6, "endOffset": 28}, {"referenceID": 13, "context": "7 of Liu and Wright (2015), the sixth inequality comes from \u2016xt \u2212 x\u0303s\u2016 = \u2016 \u2211t\u22121 t\u2032=0\u2206 s t\u2032\u2016 \u2264 \u2211t\u22121 t=0 \u2016\u2206t\u2032\u2016.", "startOffset": 5, "endOffset": 27}, {"referenceID": 13, "context": "13 in (Liu and Wright, 2015).", "startOffset": 6, "endOffset": 28}, {"referenceID": 2, "context": "where the first inequality uses the Cauchy-Schwarz inequality (Callebaut, 1965), the third inequality uses (3) and (4), the sixth inequality uses (29).", "startOffset": 62, "endOffset": 79}, {"referenceID": 13, "context": "28) of Liu and Wright (2015). E (F (x)\u2212 F ) \u2265 Lmaxl 2(l\u03b3 + Lmax) S(x) (45)", "startOffset": 7, "endOffset": 29}], "year": 2016, "abstractText": "Asynchronous parallel implementations for stochastic optimization have received huge successes in theory and practice recently. Asynchronous implementations with lock-free are more efficient than the one with writing or reading lock. In this paper, we focus on a composite objective function consisting of a smooth convex function f and a block separable convex function, which widely exists in machine learning and computer vision. We propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lock-free in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.", "creator": "LaTeX with hyperref package"}}}