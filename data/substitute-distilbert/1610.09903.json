{"id": "1610.09903", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Learning Runtime Parameters in Computer Systems with Delayed Experience Injection", "abstract": "learning effective configurations in computer systems and hand - crafting models for every parameter is a long - standing problem. this paper investigates increasing use of deep computational learning for runtime monitoring of cloud transactions under latency constraints. cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by collecting performance metrics. examining this work, we use continuous deep reinforcement learning to learn optimal cache expirations for http components in content delivery networks. achieving this end, we introduce stability metric for asynchronous project management called delayed experience injection, which finds delayed reward and next - state computation in concurrent environments where measurements are not immediately available. evaluation results show that our approach based on normalized logic constraints and asynchronous cpu - only requirements outperforms a statistical estimator.", "histories": [["v1", "Mon, 31 Oct 2016 12:57:25 GMT  (495kb,D)", "http://arxiv.org/abs/1610.09903v1", "Deep Reinforcement Learning Workshop, NIPS 2016"]], "COMMENTS": "Deep Reinforcement Learning Workshop, NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michael schaarschmidt", "felix gessert", "valentin dalibard", "eiko yoneki"], "accepted": false, "id": "1610.09903"}, "pdf": {"name": "1610.09903.pdf", "metadata": {"source": "CRF", "title": "Learning Runtime Parameters in Computer Systems with Delayed Experience Injection", "authors": ["Michael Schaarschmidt", "Felix Gessert"], "emails": ["michael.schaarschmidt@cl.cam.ac.uk", "gessert@informatik.uni-hamburg.de", "valentin.dalibard@cl.cam.ac.uk", "eiko.yoneki@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3]. Neural networks can capture structure in the environment from high-dimensional raw inputs and efficiently generalize over large state spaces. Deep reinforcement learning (DRL) techniques hence provide a powerful end-to-end task learning model from sensory inputs without prior knowledge of environment dynamics. However, training value functions for complex tasks requires significant training times and substantial computational resources.\nThere is another set of control problems in the domain of computer systems which are characterized by smaller problem dimensions and strong latency constraints. The problem this paper addresses is the utilization of DRL to provide real-time controllers for such problems. The key idea of this paper is that for comparatively small state dimensions (< 100) and tasks with weaker structure, no extended offline training is necessary to implement effective controllers.\nAs an example application, we consider cloud database services (database-as-a-service; DBaaS), which manage data and automate the operations of distributed database infrastructures. They typically employ a convention-over-configuration paradigm and do not adjust request-level parameters unless specified by developers. Nonetheless, many configuration parameters have significant performance impact for clients. In order to adjust them at runtime, one must address several challenges.\nFirst, the impact of individual actions taken in the system is difficult to measure due to serving many concurrent requests. Further, measuring system performance might only be possible after some time\nDeep Reinforcement Learning Workshop, NIPS 2016, Barcelona, Spain.\nar X\niv :1\n61 0.\n09 90\n3v 1\n[ cs\n.L G\n] 3\n1 O\nhas passed. Second, client-server architectures might prevent infrastructure providers from directly observing client performance. This work addresses the challenges of concurrent delayed credit assignment by introducing a mechanism for concurrent asynchronous experience management called delayed experience injection. Specifically, we modify normalized advantage functions [4], a recently introduced method for continuous deep reinforcement learning, to learn optimal cache expiration durations for dynamically changing query results. Results show that our controller outperforms a statistical estimator based on arrival processes."}, {"heading": "2 Background and related work", "text": ""}, {"heading": "2.1 Preliminaries", "text": "The parameter learning problem conforms to the setting of an infinite-horizon discounted Markov decision process where an agent interacts with an environment described by states s \u2208 S and aims to learn a policy \u03c0 that governs which action a \u2208 A to take in each state [5]. At each discrete time step t, the agent takes an action at according to its current policy \u03c0(a|s), transitions into a new state st+1 according to the (often stochastic) environment dynamics, and observes a reward rt. The goal of the agent is to maximize cumulative expected rewards R = E[ \u2211 t \u03b3\ntrt], where future rewards are discounted by \u03b3. This is often achieved by learning a Q-function Q\u03c0(st, at) which represents the expected return when starting from state st, taking action at with the highest Q-value and following \u03c0 thereafter [6]. Mnih et al. have demonstrated how deep neural networks can be used as value functions for a variety of complex tasks by utilising a replay memory of stored experiences, and using a second value function to stabilize learning (fixed Q-target) [2].\nIn this work, we utilize normalized advantage functions (NAFs), which have recently been suggested as an effective method for continuous DRL [4]. The key problem in continuous RL is to efficiently select the action maximising the Q-function, i.e. arg maxaQ(s, a) while avoiding to perform a costly numerical optimization at each step. Unlike other approaches in continuous DRL (e.g. deep deterministic policy gradients [7]), NAFs avoid the use of a second actor or policy network that needs to be trained separately. A single neural network Q(s, a|\u03b8Q) is used to output both a value function V (s|\u03b8V ):\nV \u03c0(st|\u03b8V ) = Eri\u2265t,si>t\u223cE,ai\u2265t\u223c\u03c0[Rt|st, at] (1)\nand an an advantage term A\u03c0(st, at):\nA\u03c0(st, at|\u03b8A) = Q\u03c0(st, at|\u03b8Q)\u2212 V \u03c0(st|\u03b8V ) (2)\nDecomposing Q into a state-value term V and an advantage term A is a technique for variance reduction often used in policy gradient methods [8, 9]. Gu et al. suggest using a quadratic advantage term:\nA(s, a|\u03b8A) = \u22121 2 (a\u2212 \u00b5(s|\u03b8\u00b5))P (s|\u03b8P )(a\u2212 \u00b5(s|\u03b8\u00b5)), (3)\nwhere P (s|\u03b8P ) is a positive-definite square matrix parametrized by a lower-triangular matrix L(s|\u03b8P ), which is given by a linear output of the network (P (s|\u03b8P ) = L(s|\u03b8P )L(s|\u03b8P )T ), with the diagonal entries exponentiated. Hence, the maximizing action is always given by \u00b5(s|\u03b8\u00b5). Updates are computed by minimizing the mini batch loss L = 1N \u2211 i(\u03b3i \u2212 Q(si, ai|\u03b8Q))2 and using a replay memory, as well as a target network Q\u2032 (as described by Mnih et al.) to compute yi = ri + \u03b3V \u2032(si+1|\u03b8Q \u2032 ). NAFs are especially appealing in our context because using a single network simplifies asynchronous update semantics."}, {"heading": "2.2 Related work", "text": "Our work is conceptually most similar to Tesauro et al.\u2019s work on resource allocation in data centres [10, 11, 12]. They utilized a perceptron with a single hidden layer to make server allocation decisions for different applications. Their method aims to maximize the expected sum of service level agreement payments while minimizing penalties for unmet service-level objectives. Their state comprised the mean arrival rate of HTTP requests and the number of currently allocated servers. The same approach has also been successfully applied to power management of web servers [13].\nNotably, their solution relies on a hybrid approach where initial values are improved by a parametric model. Our work similarly relies on arrival rates of certain events but shifts learning from global state and server-level decisions to per-request state and request-level decisions. RL has also been employed for auto-configuration of Xen virtual machines [14, 15].\nFor web caching, Candan et al. initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].\nPrior approaches on thread-parallel or distributed DRL such as A3C [19] or Gorila [20] accelerate training by having learners operate on separate copies of single-threaded environments (e.g. Atari simulator). Gu et al. have also recently applied distributed asynchronous NAFs to shared learning of 3D robot manipulation tasks [21]. In their work, distributed robot controllers asynchronously share their (sequentially) collected experiences with a central server. In contrast, our work considers thread-asynchronous training in a single node environment with a high degree of concurrency and delayed asynchronous reward assignment."}, {"heading": "3 Problem overview", "text": ""}, {"heading": "3.1 Estimating cache expirations", "text": "We consider the problem of learning parameters for cloud database services on a per-request level granularity. For each request, the database server can set response parameters affecting client performance. Multiple clients (e.g. mobile devices) can query and update the same entries in a single database. In this paper, we address the problem of estimating cache expiration times (time-to-live; TTL) for dynamically changing query results, which we now introduce.\nA query q issued by a client is executed by a database and yields a set of result records of varying cardinality n, identified by their unique keys k1, .., kn. Query results can be cached for a specified time interval t = TTL at server-controlled caches such as content delivery networks (CDNs) or reverse-proxy caches. If a key k is updated, all cached queries containing k become invalid and an invalidation request is sent to all caches. There are multiple reasons why estimating accurate TTLs for query results is critical.\nFirst, the server has to store all cached queries and their expiration times to determine which queries need to be invalidated. Using indiscriminately large TTLs for dynamically changing database content would thus both strain cache capacities as well as create too much overhead to determine invalidations, as every update needs to be compared against all cached queries. Further, every invalidation creates the potential for stale reads, as clients can retrieve stale cached results while the invalidation is propagated to all cache edges [22]. In contrast, small TTLs increase client latencies significantly if the database server is physically remote since web performance is primarily governed by round-trip latency [23]. In the following subsection, we will introduce a Monte Carlo framework designed to analyze web request flows."}, {"heading": "3.2 Simulation environment", "text": "We have implemented a Monte Carlo simulation inspired by the Yahoo! cloud serving benchmark (YCSB) [24, 25]. YCSB is a benchmark suite for cloud databases and defines a set of typical web workloads (e.g. read-dominant, scan-intensive, write-dominant). Custom workloads can be specified with properties such as request distribution, record count, operation count, and read/write/insert/scan/delete mixture. After running a workload, YCSB provides throughput and latency histograms. Our implementation provides the same workloads but instead of just providing a client interface and workloads, we stack together multiple layers (clients, caches, databases) and replicate web connection semantics. That is, YCSB operates on a synchronous thread-per-request model while web browsers typically use 6 HTTP connections and fetch multiple resources asynchronously.\nFigure 1 gives a schematic overview of the request flow for queries. Clients sample query or update requests from the workload mixture. In the simulation, each entry has a single field with a numerical value. Operations read or modify a single key k drawn from an access distribution. For easier result size control, queries are defined as range queries that request all objects for which the corresponding database entry satisfies the range predicate on the numerical field.\nWe assume the setting of a geographically remote database server hosted in the California Amazon EC2 region with a client located in Europe. Clients can drastically reduce request latencies if dynamically changing query results are present in a near cache such as content delivery network (CDN) edge. Multiple clients may query and write the same data, e.g. by commenting on a social media post or refreshing their news feed. If a client executes an update operation on a key against the DBaaS, it determines which queries need to be invalidated by re-evaluating a maintained index of cached queries against the update (e.g. through stream processing). Entries are removed from the index once the respective TTL expires. In the simulation, we pre-construct an index of queries and initial result keys and can thus cheaply determine invalidated queries by incrementally updating this index at runtime. The DBaaS then sends out asynchronous invalidation requests to the CDN. We regard a read operation as a special case of query with result size one. If another client requests a cached entry before an invalidation has been completed, a stale read occurs [22]. For TTL estimation, the server can utilize update rates on records as well as cache miss rates and invalidations on queries. In the following section, we will discuss different TTL estimation strategies and explain how our approach leverages these metrics."}, {"heading": "4 Estimating TTLs", "text": ""}, {"heading": "4.1 True TTL", "text": "We begin by considering a hypothetical optimal strategy. Ideally, TTLs are estimated to expire right before an update invalidates the respective cached result. We define the true TTL as the interval between serving the query and the query result being invalidated by a write w. In our simulation, we can hence capture the optimal action for every step after the respective query has been invalidated. Since this would not capture true TTLs for queries which expire from the cache without invalidation, we further measure the \"theoretical\" true TTL for queries which are currently not cached by evaluating which queries would have been invalidated if they had not expired."}, {"heading": "4.2 Baseline solution", "text": "We first introduce a baseline solution relying on the assumption of a Poisson process of incoming updates. For a Poisson process, the inter-arrival times of events have an exponential cumulative distribution function (CDF), i.e. each of the identically and independently distributed random variables Xi has the cumulative density F (x;\u03bb) = 1\u2212 e(\u2212\u03bbx) for x \u2265 0 and mean 1/\u03bb. For now, we make the impractical assumption that for each database record, there is an estimate of the rate of incoming writes \u03bbw over some time window.\nThe result set of a query of cardinality n can then be regarded as a set of independent exponentially distributed random variables Xi, . . . , Xn with different write-rates \u03bbw1, . . . , \u03bbwn. Estimating the\nTTL for the next update to any element of the result set requires a distribution that models the minimum time to the next write, i.e. Xmin = min{X1, . . . , Xn}, which is again exponentially distributed with \u03bbmin = \u03bbw1 + . . .+ \u03bbwn. We can hence obtain an estimate of the TTL by using the expected value until the next write on any record present in the result set, which would invalidate the cached result: TTLpoisson = E[Xmin] = 1/\u03bbmin. As we will show in the evaluation, the key problem of this approach is providing it with default write rates or default TTLs if no write-rate information is available."}, {"heading": "4.3 TTL estimation with NAFs", "text": "Motivation. TTL estimation is an appealing problem for reinforcement learning solutions as they provide a natural way to deal with time-dependent and noisy feedback loops in control problems. We proceed to model the TTL estimation problem using NAFs. First, the previous solution does not distinguish between queries that are read often and queries that are requested very rarely, i.e. it does not incorporate cache miss rates. Second, the baseline solution cannot deal well with sparse information in the state: For most objects, write rate information might not be available. Given no (or often partial) information an estimator needs to fall back to default values.\nState. We use individual record metrics to learn TTLs for query result sets. This is preferable to using an encoding of a query itself as the state, since many equivalent query strings lead to the same result. Using record-level metrics allows for an easier generalization when the result sets of seen and unseen queries overlap. Since update and query operations are generally independent, we also utilize query cache miss rates as part of the state to measure TTL impact by inputting the difference between current and last miss rate.\nQuery results can significantly vary in size but the contribution of records which are rarely updated to the TTL should be negligible. We hence set the number of inputs to the mean expected result size n and input a sorted vector of the top n available write rates to the network \u2013 other components in the case of card(result) > n are discarded.\nReward. The reward needs to encode as much information as possible from what the DBaaS can observe. From a service provider\u2019s perspective, rewards should allow to trade off invalidations against cache misses. The server cannot observe direct reward measures such as cache hit rates for clients or CDN cache utilisation. We note that we expect most queries not to be invalidated frequently (or at all) due to the power-law nature of web workloads [26]. Hence, using the expected invalidation rate as a TTL estimation strategy is unlikely to be successful as there will be no information for most queries.\nTo punish invalidated queries, the agent needs to know which actions cause a query to be invalidated by a later write. This means the server can only sensibly measure a reward after some delay td. The same problem exists for state measurements relying on cache miss rates. If there is an invalidation at time tinv before the expiration timestamp of a cached query texp, the reward can be computed as the difference between invalidation time and expiration time (in seconds), i.e. rt = tinv \u2212 texp. If there has been no invalidation, less informative metrics have to be used.\nNo invalidation before expiration means that the TTL could have been higher unless capacity constraints prohibit longer caching times. In this case, we hence use a static reward r and scale it by the current load ct (current cached queries divided by capacity) to encourage longer TTLs when fewer queries are cached and shorter TTLs when load is close to capacity (by using \u2212ct if larger than some threshold), i.e. rt = r \u00b7 (1 + ct). The intuition behind this approach is that only using invalidation timestamps would not allow to give a reward for 80\u221290% of queries (due to low invalidation rates, as shown in the evaluation), and would not give opportunity to globally up- or down-regulate estimates according to system-wide load. In the following section, we explain how we practically perform delayed reward and next-state measurements."}, {"heading": "4.4 Delayed experience injection", "text": "In standard RL semantics, the agent sequentially moves through a Markov decision process by taking steps and recording transitions of state, action, reward and next-state. When using a replay memory, learning is decoupled from current state and actions by sampling transitions from the memory to perform mini-batch gradient descent. Consequently, if the desired runtime measurements for rewards\nand next-states are not available immediately and the agent has to deal with many concurrent requests, the application needs to keep track of \"incomplete\" transitions and decide when to complete them.\nAlgorithm 1 Asynchronous NAF with delayed experience injection. Initialize empty replay memoryR \u2190 \u2205 Initialize Q-network Q(s, \u00b5|\u03b8Q) with random weights Initialize target network Q\u2019 with weight \u03b8Q\n\u2032 \u2190 \u03b8Q Initialize random process N for initial exploration for t = 1, T do\nSelect action at = \u00b5(st|\u03b8\u00b5) +Nt Create incomplete transition (st, at), Enqueue (st, at) in expiration queue with expt = now() + td At t = expt, asynchronously execute queue consumer:\nCompute rt and st+1 Insert complete transition (st, at, rt, st+1) intoR.\nSubmit asynchronous loss computation: Compute yi = ri + \u03b3V \u2032(si+1|\u03b8Q \u2032 ) Minimize L = 1N \u2211 i(\u03b3i \u2212Q(si, ai|\u03b8Q))2\nPeriodically update \u03b8Q \u2032 \u2190 \u03b8Q\nend for\nAlgorithm 1 shows the control flow in our model. The DBaaS server computes the state from write rate and cache miss metrics for an incoming query and creates an incomplete transition (st, at). This is then enqueued into an expiration queue data structure which triggers an asynchronous consumer after the specified delay td, which we set to at in our experiments (i.e. the TTL). The consumer computes the reward and the next state as described above by requesting the last invalidation timestamp and cache miss rate. It then inserts the completed transition (st, at, rt, st+1) into the replay memory R, a mechanism we call delayed experience injection (DEI). DEI decouples not only current state from learning (as a replay memory does) but also decouples future state and reward computation for specific queries from the sequence of incoming states. Hence, NAF-DEI also solves a different problem than the recently introduced distributed asynchronous NAF [21], where multiple controllers sequentially collect experiences without delay in the experience computation itself. Further, the difference between DEI and the standard delayed reward assignment problem [27] is that DEI deals with concurrent delayed credit assignment.\nUpdates are performed similar to standard NAF except that the update step is also computed asynchronously by another thread. This is necessary because blocking incoming decision queries on the update step would result in latency spikes."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Setup", "text": "The goal of the evaluation is to demonstrate the principal feasibility of using deep reinforcement learning for request-level parameter learning. We begin by describing the experimental setup. We have implemented our simulator in Java 8 (for YCSB compatibility) and utilized deeplearning4j [28] (0.5.0) for the NAF implementation. We set 10 clients with each 6 concurrent connections to execute a combined target throughput of 1,000 (asynchronous) operations per second. They accessed 10,000 documents with 1,000 distinct queries under varying workloads. Updates and queries were drawn from a Zipfian distribution (Zipf constant 0.6). Each workload was run for 30 minutes on a commodity 4 core desktop machine and results were averaged over five runs. Query result sizes were set to be between [1, 20] documents by sampling scan ranges fromN (10, 5) (resp. N (5, 2) in smaller experiments). Simulated round-trip latencies reflected a client in Europe, a CDN edge in Europe (4 ms round trip latency), and a server in the EC2 California region (150 ms round trip latency.)\nThe NAF agent used 10 inputs (resp. half the maximal result size in other experiments) for write rates w and 1 input for cache miss differences for 11 inputs in total, followed by 2 hidden layers with each 30 neurons using rectified linear unit activations. Updates were performed using an Adam [29]\nupdater with mini-batches of size 10 (all training was executed by the CPU due to the small model size). Learning was non-episodic and fully on-policy after an initial exploration period. The learning rate was set to \u03b1 = 0.0005 with gradients clipped at 30, allowing for aggressive updates."}, {"heading": "5.2 Results", "text": "Figure 2a compares results for different workload mixtures as the root-mean-squared per-step error (RMSE) against the optimal policy (truncated at the 99th percentile to remove outliers). The Poisson estimator is limited by its need to specify some default action if no write rate estimates are available. A feasible strategy is to set a maximum for TTLs and presume that the write rate on unknown objects corresponds to the inverse maximum. This is preferable to setting a single default estimate because such a default value would not account for different result sizes. We ran a number of configurations for the Poisson estimator and report the error for the best configuration (max TTL 300 s). The NAF agent (using DEI) outperformed the Poisson estimator, as it is not dependent on a maximum value and uses 0 as input if no write rate is available instead of default per-key estimates.\nFurther, we show the impact of running NAF without DEI (NAF-naive). NAF-naive instantly computes rewards and will thus rely on invalidations caused a by prior action, creating much larger error and larger standard deviations (for some configurations, learning from the wrong rewards leads to good accidental performance). NAF-DEI outperformed naive NAF by 38.5% on average. While the TTL estimation problem is a special case due to the action being a time period where the delay is set to the action value, delayed asynchronous reward computation is likely beneficial to various other problems with high degrees of concurrency. We also observe that the absolute error is large for all solutions and improves with larger write rates, as more per-key information becomes available and the CDF turns steeper. Figure 2b compares CDFs from traces of NAF-learned, optimal and Poisson-estimated policies. Both approaches produce an overly steep CDF as the long tail of the optimal policy is principally difficult to predict.\nIt is important to recognize hat the theoretical optimal TTL we use to compute errors is not an ideal performance measure. It assigns an error to queries which expire without invalidation by computing the error until a future point in time when this query would have been invalidated if it had still been cached. In contrast, the reward function specifies a complex trade-off between invalidations, cache misses and global load. We nonetheless report this error since it allows for a more neutral comparison of strategies with different objective functions. We also compared learning performance in different settings to a hypothetical default-value predictor which knows in advance which single default value would give the lowest error per workload mixture. Our results show that the learner can outperform the default-predictor by over 60% for individual queries and up to 20% for the top 20% of queries (sorted by observed cache misses) for some workloads, performing better with smaller result sizes. With increasing write rates or increasing result sizes, true TTL distributions become more narrow and outperforming a default value is more difficult. However, the learner\u2019s mean error roughly matches (sometimes outperforms) the default-predictor due to large errors from the long tail of the access distribution. Note that the NAF-agent\u2019s error includes the online training period, as we wanted to evaluate the feasibility of a controller without prior training.\nFigure 2c shows actual cache performance in terms of achieved cache hit rates (higher is better) and invalidation rates (lower is better). NAF-DEI accepts slightly higher invalidation rates to ensure high cache performance while the Poisson estimator tends to predict lower TTLs due to using default write rates. Assuming an equal weighting between cache hit rate and invalidation rate, NAF-DEI\noffers much better cache performance with mean cache hit rate 88.5% for w = 10% with 7.3% invalidations versus 79.5% cache hit rate and 6.8% invalidations for the Poisson estimator. For w = 30%, NAF-DEI achieved 77.7% cache hits and 21.4% invalidations versus 62.6% cache hits and 15.6% invalidations for the Poisson estimator. These results stress that simply estimating lower TTLs does not save many invalidations if estimates are imprecise on a per-query level.\nTo better understand learning behavior, we examine traces of individual queries through experiment runs. Figure 3a shows learned and optimal actions for every instance an individual query is observed throughout one experiment with w = 10%, illustrating both the online learning process and the noise in the optimal policy. Note that while learning seems fast, the plot does not show how much time (and hence learning from other queries) passes between each instance of the query. For a throughput of 1, 000 concurrent requests per second, most learning (by magnitude of error) took place in the first few minutes (about 20% of experiment duration). For w = 30%, the agent uses the additional information from both more writes and faster changing cache miss rates to make more specific guesses, as seen in figure 3b. While the learner seems to match the general shape of the series of true TTLs, it systematically overestimates true values by 5\u2212 10 s. This is because the reward was statically configured to always encourage high cache hit rates independent of higher write rates. In figure 3c, we adjusted rewards according to the workload mixture, i.e. we encouraged the load to stay below 1 \u2212 w. Consequently, TTLs for the query decrease over time once the cache fills up. The Poisson estimator could be similarly tuned by using not the expected value until seeing the next write but some other quantile, e.g. 75% to encourage higher cache hit rates. However, RL-based solutions offer a natural interface to incorporate additional performance metrics without having to translate them into an analytical model, i.e. by determining which Poisson parameters correspond to the desired performance.\nOur results allow some outlook on learning parameters at a much larger scale. Due to the Zipf nature of web workloads [26], most queries and updates will concentrate on a small sets of \"hot\" database records for which it might be feasible to track runtime information and use them for specific predictions. In conclusion, the combination of small model size and a high degree of concurrency allowed NAF-DEI to achieve an effective trade-off between avoiding invalidations and ensuring high cache hit rates without requiring prior training."}, {"heading": "6 Conclusion and future work", "text": "To the best of your knowledge, this work presents the first application of deep reinforcement learning in predicting request-level parameters in computer systems. We introduced the concept of delayed experience injection to capture asynchronous reward/next-state semantics in concurrent environments where relevant metrics are only available later. The key idea of our work is that instead of learning global parameters from global metrics, DRL can facilitate per-request decisions based on fine-grained metrics. Results show that our NAF-based approach can outperform a statistical estimator on the TTL estimation problem by leveraging available runtime information.\nIn future work, we will explore in more detail how to relate dynamic reward adjustments to specific service level objectives in non-stationary environments. We have also made the simplifying assumption of a single node backend receiving all incoming requests. Future work in this domain hence needs to investigate coordination and distributed learning in infrastructures where each node only observes part of the environment, as opposed to each node observing a separate copy of the problem."}, {"heading": "Acknowledgements", "text": "This work was supported by the EPSRC (grant reference EP/M508007/1) and a Computer Laboratory Premium Scholarship (Sansom scholarship)."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep Reinforcement Learning with Double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Utility-function-driven resource allocation in autonomic systems", "author": ["G. Tesauro", "R. Das", "W.E. Walsh", "J.O. Kephart"], "venue": "In Autonomic Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A hybrid reinforcement learning approach to autonomic resource allocation", "author": ["G. Tesauro", "N.K. Jong", "R. Das", "M.N. Bennani"], "venue": "In Proceedings of the 2006 IEEE International Conference on Autonomic Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Online resource allocation using decompositional reinforcement learning", "author": ["Gerald Tesauro"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Managing power consumption and performance of computing systems using reinforcement learning", "author": ["Gerald Tesauro", "Rajarshi Das", "Hoi Chan", "Jeffrey Kephart", "David Levine", "Freeman Rawson", "Charles Lefurgy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A distributed self-learning approach for elastic provisioning of virtualized cloud resources", "author": ["Jia Rao", "Xiangping Bu", "Cheng-Zhong Xu", "Kun Wang"], "venue": "In Modeling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Url: A unified reinforcement learning approach for autonomic cloud management", "author": ["Cheng-Zhong Xu", "Jia Rao", "Xiangping Bu"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Enabling dynamic content caching for database-driven web sites", "author": ["K. Sel\u00e7uk Candan", "Wen-Syan Li", "Qiong Luo", "Wang-Pin Hsiung", "Divyakant Agrawal"], "venue": "In SIGMOD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Experiences with coralcdn: A five-year operational view", "author": ["Michael J. Freedman"], "venue": "Proc NSDI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "An analysis of facebook photo caching", "author": ["Qi Huang", "Ken Birman", "Robbert van Renesse", "Wyatt Lloyd", "Sanjeev Kumar", "Harry C. Li"], "venue": "In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep Reinforcement Learning for Robotic Manipulation", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "ArXiv e-prints,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The cache sketch: Revisiting expiration-based caching in the age of cloud data management", "author": ["Felix Gessert", "Michael Schaarschmidt", "Wolfram Wingerath", "Steffen Friedrich", "Norbert Ritter"], "venue": "BTW \u201915,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "High performance browser networking", "author": ["Ilya Grigorik"], "venue": "O\u2019Reilly Media, [S.l.],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Benchmarking cloud serving systems with ycsb", "author": ["Brian F. Cooper", "Adam Silberstein", "Erwin Tam", "Raghu Ramakrishnan", "Russell Sears"], "venue": "In Proceedings of the 1st ACM Symposium on Cloud Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Ycsb++: Benchmarking and performance debugging advanced features in scalable table stores", "author": ["Swapnil Patil", "Milo Polte", "Kai Ren", "Wittawat Tantisiriroj", "Lin Xiao", "Julio L\u00f3pez", "Garth Gibson", "Adam Fuchs", "Billie Rinaldi"], "venue": "In Proceedings of the 2Nd ACM Symposium on Cloud Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Web caching and zipf-like distributions: evidence and implications", "author": ["L. Breslau", "Pei Cao", "Li Fan", "G. Phillips", "S. Shenker"], "venue": "In INFOCOM \u201999. Eighteenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Learning from Delayed Rewards", "author": [], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1989}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3].", "startOffset": 140, "endOffset": 149}, {"referenceID": 1, "context": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3].", "startOffset": 140, "endOffset": 149}, {"referenceID": 2, "context": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3].", "startOffset": 140, "endOffset": 149}, {"referenceID": 3, "context": "Specifically, we modify normalized advantage functions [4], a recently introduced method for continuous deep reinforcement learning, to learn optimal cache expiration durations for dynamically changing query results.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "The parameter learning problem conforms to the setting of an infinite-horizon discounted Markov decision process where an agent interacts with an environment described by states s \u2208 S and aims to learn a policy \u03c0 that governs which action a \u2208 A to take in each state [5].", "startOffset": 267, "endOffset": 270}, {"referenceID": 1, "context": "have demonstrated how deep neural networks can be used as value functions for a variety of complex tasks by utilising a replay memory of stored experiences, and using a second value function to stabilize learning (fixed Q-target) [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 3, "context": "In this work, we utilize normalized advantage functions (NAFs), which have recently been suggested as an effective method for continuous DRL [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "deep deterministic policy gradients [7]), NAFs avoid the use of a second actor or policy network that needs to be trained separately.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "\u2019s work on resource allocation in data centres [10, 11, 12].", "startOffset": 47, "endOffset": 59}, {"referenceID": 7, "context": "\u2019s work on resource allocation in data centres [10, 11, 12].", "startOffset": 47, "endOffset": 59}, {"referenceID": 8, "context": "\u2019s work on resource allocation in data centres [10, 11, 12].", "startOffset": 47, "endOffset": 59}, {"referenceID": 9, "context": "The same approach has also been successfully applied to power management of web servers [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "RL has also been employed for auto-configuration of Xen virtual machines [14, 15].", "startOffset": 73, "endOffset": 81}, {"referenceID": 11, "context": "RL has also been employed for auto-configuration of Xen virtual machines [14, 15].", "startOffset": 73, "endOffset": 81}, {"referenceID": 12, "context": "initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].", "startOffset": 171, "endOffset": 179}, {"referenceID": 14, "context": "initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].", "startOffset": 171, "endOffset": 179}, {"referenceID": 15, "context": "Prior approaches on thread-parallel or distributed DRL such as A3C [19] or Gorila [20] accelerate training by having learners operate on separate copies of single-threaded environments (e.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "Prior approaches on thread-parallel or distributed DRL such as A3C [19] or Gorila [20] accelerate training by having learners operate on separate copies of single-threaded environments (e.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "have also recently applied distributed asynchronous NAFs to shared learning of 3D robot manipulation tasks [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "Further, every invalidation creates the potential for stale reads, as clients can retrieve stale cached results while the invalidation is propagated to all cache edges [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "In contrast, small TTLs increase client latencies significantly if the database server is physically remote since web performance is primarily governed by round-trip latency [23].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "We have implemented a Monte Carlo simulation inspired by the Yahoo! cloud serving benchmark (YCSB) [24, 25].", "startOffset": 99, "endOffset": 107}, {"referenceID": 21, "context": "We have implemented a Monte Carlo simulation inspired by the Yahoo! cloud serving benchmark (YCSB) [24, 25].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "If another client requests a cached entry before an invalidation has been completed, a stale read occurs [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "We note that we expect most queries not to be invalidated frequently (or at all) due to the power-law nature of web workloads [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "Hence, NAF-DEI also solves a different problem than the recently introduced distributed asynchronous NAF [21], where multiple controllers sequentially collect experiences without delay in the experience computation itself.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "Further, the difference between DEI and the standard delayed reward assignment problem [27] is that DEI deals with concurrent delayed credit assignment.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Query result sizes were set to be between [1, 20] documents by sampling scan ranges fromN (10, 5) (resp.", "startOffset": 42, "endOffset": 49}, {"referenceID": 16, "context": "Query result sizes were set to be between [1, 20] documents by sampling scan ranges fromN (10, 5) (resp.", "startOffset": 42, "endOffset": 49}, {"referenceID": 24, "context": "Updates were performed using an Adam [29]", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "Due to the Zipf nature of web workloads [26], most queries and updates will concentrate on a small sets of \"hot\" database records for which it might be feasible to track runtime information and use them for specific predictions.", "startOffset": 40, "endOffset": 44}], "year": 2016, "abstractText": "Learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem. This paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints. Cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous deep reinforcement learning to learn optimal cache expirations for HTTP caching in content delivery networks. To this end, we introduce a technique for asynchronous experience management called delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Evaluation results show that our approach based on normalized advantage functions and asynchronous CPU-only training outperforms a statistical estimator.", "creator": "LaTeX with hyperref package"}}}