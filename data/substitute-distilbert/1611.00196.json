{"id": "1611.00196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Recurrent Neural Network Language Model Adaptation Derived Document Vector", "abstract": "in many natural language processing ( raw ) tasks, a document is commonly modeled since a bag of words using the term frequency - inverse document frequency ( tf - idf ) vector. one major variation atop the frequency - based tf - idf feature vector is namely it ignores word orders that carry syntactic and semantic relationships in the words in a document, and otherwise can be important in some nlp tasks such as genre classification. this paper shows a novel distributed vector representation through a document : a simple recurrent - neural - network language environment ( rnn - lm ) or a long short - term relational rnn language model ( lstm - lm ) is first created from all documents in a task ; some around the lm parameters are then processed by each document, until the adapted parameters are vectorized to represent the document. the new document vectors are labeled as dv - rnn matrices rr - aw respectively. we find that our new document vectors can capture more high - level sequential information in the documents, which no current document vectors struggle to capture. the new document vectors were evaluated in the default classification of documents in virtual corpora : the brown corpus, the bnc baby corpus with an artificially created penn treebank dataset. their classification performances are compared using static performance of tf - idf vector and the state - of - the - art distributed memory model of paragraph vector ( pv - dm ). the results show that dv - lstm significantly outperforms tf - cbn and pv - dm almost most cases, and combinations of the displaced document strings with tf - israel or pv - dm may further moderate compression.", "histories": [["v1", "Tue, 1 Nov 2016 12:14:02 GMT  (357kb,D)", "http://arxiv.org/abs/1611.00196v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wei li", "brian kan wing mak"], "accepted": false, "id": "1611.00196"}, "pdf": {"name": "1611.00196.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Network Language Model Adaptation Derived Document Vector", "authors": ["Wei Li", "Brian Kan Wing MAK"], "emails": ["wliax@cse.ust.hk", "mak@cse.ust.hk"], "sections": [{"heading": null, "text": "Index Terms: document genre classification, document embedding, recurrent neural network, long short-term memory network"}, {"heading": "1 Introduction", "text": "Document vectorization plays a vital role in many natural language processing (NLP) tasks. One of the most popular document vectors is the term frequency-inverse document frequency (TF-IDF) feature vector (Robertson and Jones, 1976). It can offer a robust baseline in many NLP tasks, including genre classification, albeit task specific design may improve the performance. (Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).\nTF-IDF Document vectorization has a major drawback(Cachopo, 2007; Le and Mikolov, 2014): it ignores word orders and other sequential information in a document. Moreover, TF-IDF and n-gram TF-IDF vectors cannot capture syntactic or semantic relationship/similarity between words, paragraphs, and documents. Besides TF-IDF, another notable document vectorization is the paragraph vector(Le and Mikolov, 2014). Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016). PV-DM has been shown to perform significantly better than the bag-of-words model in many NLP tasks. Moreover, Skip-Thought Vectors (Kiros et al., 2015) also show superior performances against bag-of-words model. ar X iv :1\n61 1.\n00 19\n6v 1\n[ cs\n.C L\n] 1\nN ov\nIn this paper, we would like to explore a new document vectorization method that produces a densely distributed representation of text documents while capturing some sequential information in the documents at the same time. Our approach is to adapt/retrain a simple recurrent neural network language model (RNN-LM) (Mikolov et al., 2010) or a long short-term memory RNN language model (LSTM-LM) (Sundermeyer et al., 2012) with a document, and then vectorize the retrained/adapted model parameters to obtain its document vector (labeled as DV-RNN and DV-LSTM) from RNN-LM and LSTM-LM respectively. The recurrent nature of the RNN-LM or LSTM-LM should capture some high-level and abstract sequential information from its training documents, and our extraction method will not suffer from the limitation imposed by a sliding context window such as in PV-DM. In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016). these models can also take abstract long-term information into consideration. However, these are semi-supervised or supervised methods, which prevent them from training document embeddings from unlabeled data. The effectiveness of the proposed document vectors was evaluated on genre classification of documents in three corpora, and their performance was compared with that of TF-IDF vector and PV-DM. Our results show that in most cases, our document vector significantly outperforms n-gram TF-IDF vector and PV-DM."}, {"heading": "2 Recurrent Neural Network Language Modeling (RNN-LM and LSTM-LM)", "text": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).\nFigure 1 shows the simple RNN with output layer factorized by classes. In this models, the input is the current word wt and the output layer is factorized by a (word) class layer. The input word is projected to a distributed representation by the input matrix U. The recurrent matrix H helps memorize the word\nhistory. Training the above simple RNN may suffer from the problem of exploding or vanishing gradients (Hochreiter and Schmidhuber, 1997; Sundermeyer et al., 2012). We also propose a network structure for DV-LSTM based on LSTM-LM (labeled as DVL100-LSTM100, which has a 100-unit hidden layer and a 100-unit sigmoid layer before the hidden layer. (Figure 2)."}, {"heading": "3 Document Vectorization by RNN-LM / LSTM-LM Adaptation", "text": "From here on we would refer to our approach as \u2019adaptation\u2019 for simplicity (it can also be considered as retraining). Below is the basic document vectorization procedure:\nSTEP 1 : Train a parent LM using all the documents in the training corpus, which serves as the initial model for adaptation.\nSTEP 2 : Adapt the parent LM with each document in the training corpus1\nSTEP 3 : Extract model parameters of interest from the adapted LM, and vectorize them to produce the DV for the adapting document.\n1Not all model parameters in the parent LM are necessarily adapted. Only the layers shown in green in Figure 1, Figure 2 are adapted in this paper."}, {"heading": "3.1 Derivation of DV-RNN from RNN-LM adaptation", "text": "As shown in Figure 1, an RNN-LM consists of 4 weight matrices: the input projection matrix U, the recurrent matrix H, the output matrix Q and the word class matrix K. We choose to adapt only H and K matrices for the derivation of DV-RNN of a document, while keeping the matrices U and Q intact during adaptation. After adaptation, H and/or K are vectorized to h = vec(H) and k = vec(K) respectively by enumerate the matrix parameters column by column. The two vectors are concatenated together to form the DV-RNN for the adapting document."}, {"heading": "3.2 Derivation of DV-LSTM from LSTM-LM adaptation", "text": "The structure of an LSTM cell is more complex than a simple RNN cell, To perform robust LSTMLM adaptation, we limit ourselves only to adapt the biases in the sigmoid layer bl (in DV-L100LSTM100), LSTM layer bm and the class layer bc. The LSTM biases bm is comprised of 4 bias subvectors: forget-gate biases bmf , input-gate biases bmi , output-gate biases bmo , and cell biases bmc .\nFor DV-L100-LSTM100, all four gates\u2019 bias is updated. Plus the bl and bc. The final DV-L100LSTM100 vector, dm100 , is the concatenation of the\nLSTM biases, the sigmoid layer biases and the class layer biases as follows:\ndm100 = [n(b \u2032 a100), n(b \u2032 c)] \u2032 . (1)\nand,\nba100 = [n(b \u2032 mf ), n(b\u2032mi), n(b \u2032 mo), n(b \u2032 mc), n(b \u2032 l)] \u2032 . (2)\nbm100 = [n(b \u2032 mf ), n(b\u2032mi), n(b \u2032 mo), n(b \u2032 mc)] \u2032 . (3)\nwhere bm100 is the four LSTM gate biases concatenated in DV-L100-LSTM100. ba100 is the concatenation of bm100 and bl. n() is the normalization operation which would normalize the vector to the unit norm so that the two sub-vectors of very different value range can be concatenated.\nThe three different biases in DV-LSTM are supposed to capture various types of information for a document: bl is to capture the abstract and distributed word embeddings from the sigmoid compression layer; bm is to capture the long-span sequential text information in a document, enabling by the recurrent structure and different gates in LSTM cell; bc is to capture the word-class information."}, {"heading": "4 Experimental Evaluation: Text Genre Classification", "text": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al., 1995). Performances are reported in weighted Fscores (weighted by the sample number in each category)."}, {"heading": "4.1 Corpora used for text genre classification", "text": "Table 1 summarizes some basic information of each testing corpus. The Penn Treebank dataset(PTB) was artificially extracted from the Penn Treebank Corpus by taking out the documents that have the available genre tags provided by (Webber, 2009; Plank, 2009). As the number of files in each genre\nis very unbalanced, we limited the number of documents in each genre to no more than 100 and remove the errata genre (as there are very few documents of that genre). We also removed short documents with fewer than 200 words. As a result, the dataset has a total of 239 documents in 4 genres (38 highlights, 95 essays, 42letters, and 64 news). For the Brown Corpus, the sub-genres under the fiction genre were merged so that the total number of genres was 10 (Wu et al., 2010)."}, {"heading": "4.2 Training of the document vector DV-LSTM", "text": "The RWTH Aachen University Neural Network Language Modeling Toolkit (RWTHLM) (Sundermeyer et al., 2015; Sundermeyer et al., 2014) was used for training all LSTM-LMs and adapting them to produce the DV-LSTMs.\nThe word-classes were determined by the Brown clustering method."}, {"heading": "4.3 Training of the paragraph vector PV-DM", "text": "The paragraph vectors (PV-DMs) (Le and Mikolov, 2014) were trained for each document in a corpus using the Gensim toolkit (vRehu\u030avrek and Sojka, 2010). The PV-DMs were trained with an initial learning rate of 0.025, with 20 epochs. It was found that PV-DMs of 500 dimensions provide consistent good performance on all three corpora; they\nare denoted as PV500. One-Tenth close-set data is randomly sampled for fine-tuning, and a grid search for the optimal hyperparameter combination is performed on the each task datasets. The combination of the hyperparameters is the permutation of each parameter value in Table 2. The respective optimal combinations of the hyperparameters are then chosen for each task to obtain better performance from PV-DM.\nTable 3 summarizes the dimensions of various feature vectors used for the experiments. z10005 and z5 are the TF-IDF feature vector of top 1000 and top 10000 5-gram terms respectively."}, {"heading": "4.4 Experimental Results", "text": "The genre classification results using different (combinations of) feature vectors over the three corpora are summarized in Table 4 in terms of weighted F-scores. The bold results represent the best results given by a single feature or a set of combined features. The baseline results labeled with * are quoted from (Tang and Cao, 2015; Wu et al., 2010).\nFrom the results, we have the following observations:\n\u2022 Our own 5-gram TF-IDF obtained results better than the quoted baseline results from existing works. Thus, we will use our 5-gram TF-IDF results as baselines in the ensuing discussion.\n\u2022 Both PV500 and our DVs show superior performances comparing to traditional N-gram TFIDF.\n\u2022 PV500\u2019s performance have shown significant improvements when we conduct the grid search on tuning hyperparameters. DV-LSTM and DV-RNN also have the same hyperparameters as in Table 2 except the window size. Due to the limitation of the current platform, we do not tune these parameters for our DV-LSTMs or DV-RNN. We could also expect a significant room for improvement on DV-LSTM and DV-RNN\u2019s performance if such hyperparameters are also fine-tuned like those of PV-DM\u2019s.\n\u2022 In most cases, our DV-LSTM\u2019s performance is better than PV-DM and PV-DM is better than the 5-gram TF-IDF\u2019s performance regarding Fscore. The difference is most significant in the PTB dataset and the BNC baby dataset. To evaluate the statistical significance of the performance difference between each vector, we also adopt the pair sample t-test (Dietterich, 1998). The confidence threshold is setting at 0.99. All the result highlighted with the bold font is statistically significant, comparing to the performance of 5-gram TF-IDF and PVDM. And the difference between DV-L100LSTM100 and PV-DM in the case of the Brown corpus is not statistically significant. Thus, our DV-LSTM show significant improvement over PV-DM and 5-gram vector in most cases, with the exception of DV-L100-LSTM100 in the Brown corpus\u2019s case (where the performance is at least as good as those of PV-DM and the 5-gram TF-IDF).\n\u2022 The concatenation of bm bc and bl form the final DV-LSTM vector dm gives good results than each sub-vectors used along in almost all the cases. Thus, we may conclude that the three bias vectors are complementary to each other for extracting information in genre clas-\nsification. The gate bias, the class layer bias and the sigmoid layer bias could capture different structural or semantic patterns in the documents.\n\u2022 In all three corpora, combining DV-RNN or DV-LSTM with the 5-gram TF-IDF or PV500 gives the best classification performance. This again shows that our proposed document vectors have extracted some information other than term frequency-based information.\n\u2022 The strength of various bias components of our DV-LSTM were investigated. It is interesting to see that the LSTM bias vector bm is outperformed by the class bias vector bc. There are two possible reasons: (a) the abstract form of term frequency, which is represented by the class bias parameter, is still the most import feature baseline. (b) DV-LSTM used the Brown clustering method to organize its vocabulary class, which may offer better performance. Thus we also test the performance of the stand along Brown clustering term frequency feature in c500 (the same 500 classes as the class bias vector in DV-LSTM), it shows that c500 is still much inferior comparing to the 5-gram TF-IDF and bc. Therefore, it could be inferred that it is the recurrent LSTM neural network structure and abstract feature extraction capability in the DV-LSTM makes the greater contribution to the performances.\n\u2022 In summary, in terms of weighted F-score, the best results are achieved by the concatenated feature of PV-DM or 5-gram TF-IDF with our DV-L100-LSTM100 vector. Vectors from DVL100-LSTM100 achieve the best (or at least equal) single vector performance."}, {"heading": "5 Conclusions and Future Works", "text": "This paper proposes a novel distributed representation of a document, which we call \u201cdocument vector\u201d (DV). In the current task, DV-LSTM shows good performance when used independently or in conjunction with TF-IDF or PV-DM, and performs better than DV-RNN. Given that DV-LSTM is more compact than DV-RNN and its better performance, DV-LSTM is preferred except that DV-RNNs is faster to train. In the future, we would investigate its effectiveness in other NLP problems such as topic classification and sentiment detection.\nFinally, we believe that the concept of our new DV can be applied to other sequential data (such as gene sequences and stock prices) of variable lengths by treating the sequential data as a document, from which we may create a DV of fixed length to represent them."}], "references": [{"title": "Analysis of the paragraph vector model for information retrieval", "author": ["Qingyao Ai", "Liu Yang", "Jiafeng Guo", "W Bruce Croft."], "venue": "Proceedings of the 2016 ACM on International Conference on the Theory of Information Retrieval, pages 133\u2013142. ACM.", "citeRegEx": "Ai et al\\.,? 2016", "shortCiteRegEx": "Ai et al\\.", "year": 2016}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Reference guide for BNC Baby", "author": ["Lou Burnard"], "venue": null, "citeRegEx": "Burnard.,? \\Q2003\\E", "shortCiteRegEx": "Burnard.", "year": 2003}, {"title": "Improving methods for single-label text categorization", "author": ["Ana Margarida de Jesus Cardoso Cachopo."], "venue": "Ph.D. thesis, Universidade T\u00e9cnica de Lisboa.", "citeRegEx": "Cachopo.,? 2007", "shortCiteRegEx": "Cachopo.", "year": 2007}, {"title": "Document embedding with paragraph vectors", "author": ["Andrew M Dai", "Christopher Olah", "Quoc V Le."], "venue": "arXiv preprint arXiv:1507.07998.", "citeRegEx": "Dai et al\\.,? 2015", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "The form is the substance: Classification of genres in text", "author": ["Nigel Dewdney", "Carol VanEss-Dykema", "Richard MacMillan."], "venue": "Proceedings of the Workshop on Human Language Technology and Knowledge Management, pages 1\u20138. Association for Computa-", "citeRegEx": "Dewdney et al\\.,? 2001", "shortCiteRegEx": "Dewdney et al\\.", "year": 2001}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["Thomas G Dietterich."], "venue": "Neural computation, 10(7):1895\u20131923.", "citeRegEx": "Dietterich.,? 1998", "shortCiteRegEx": "Dietterich.", "year": 1998}, {"title": "Brown corpus manual", "author": ["W. Nelson Francis", "Henry Kucera."], "venue": "Brown University.", "citeRegEx": "Francis and Kucera.,? 1979", "shortCiteRegEx": "Francis and Kucera.", "year": 1979}, {"title": "Towards genre classification for IR in the workplace", "author": ["Luanne Freund", "Charles L.A. Clarke", "Elaine G Toms."], "venue": "Proceedings of the 1st International Conference on Information Interaction in Context, pages 30\u201336. ACM.", "citeRegEx": "Freund et al\\.,? 2006", "shortCiteRegEx": "Freund et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Recognizing text genres with simple metrics using discriminant analysis", "author": ["Jussi Karlgren", "Douglass Cutting."], "venue": "Proceedings of the 15th Conference on Computational Linguistics, volume 2, pages 1071\u2013 1075. Association for Computational Linguistics.", "citeRegEx": "Karlgren and Cutting.,? 1994", "shortCiteRegEx": "Karlgren and Cutting.", "year": 1994}, {"title": "Automatic detection of text genre", "author": ["Brett Kessler", "Geoffrey Numberg", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computa-", "citeRegEx": "Kessler et al\\.,? 1997", "shortCiteRegEx": "Kessler et al\\.", "year": 1997}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "AAAI, pages 2267\u20132273.", "citeRegEx": "Lai et al\\.,? 2015", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Text genre classification with genre-revealing and subjectrevealing features", "author": ["Yong-Bae Lee", "Sung Hyon Myaeng."], "venue": "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 145\u2013", "citeRegEx": "Lee and Myaeng.,? 2002", "shortCiteRegEx": "Lee and Myaeng.", "year": 2002}, {"title": "Treebank 2", "author": ["M. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Marcus et al\\.,? 1995", "shortCiteRegEx": "Marcus et al\\.", "year": 1995}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1vs Mikolov", "Stefan Kombrink", "Luk\u00e1vs Burget", "Jan Honza vCernock\u1ef3", "Sanjeev Khudanpur."], "venue": "pages 5528\u20135531.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Assessing approaches to genre classification", "author": ["Philipp Petrenz."], "venue": "Ph.D. thesis, M. Sc. Thesis, School of Informatics, University of Edinburgh.", "citeRegEx": "Petrenz.,? 2009", "shortCiteRegEx": "Petrenz.", "year": 2009}, {"title": "PTB/PDTB files belonging to different genres", "author": ["Barbara Plank."], "venue": "http://www.let.rug. nl/ \u0303bplank/metadata/genre_files_ updated.html. Accessed: 2015-9-30.", "citeRegEx": "Plank.,? 2009", "shortCiteRegEx": "Plank.", "year": 2009}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim vReh\u016fvrek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/", "citeRegEx": "vReh\u016fvrek and Sojka.,? 2010", "shortCiteRegEx": "vReh\u016fvrek and Sojka.", "year": 2010}, {"title": "Relevance weighting of search terms", "author": ["Stephen E. Robertson", "K. Sparck Jones."], "venue": "Journal of the American Society for Information Science, 27(3):129\u2013146.", "citeRegEx": "Robertson and Jones.,? 1976", "shortCiteRegEx": "Robertson and Jones.", "year": 1976}, {"title": "Text genre detection using common word frequencies", "author": ["Efstathios Stamatatos", "Nikos Fakotakis", "George Kokkinakis."], "venue": "Proceedings of the 18th Conference on Computational Linguistics, volume 2, pages 808\u2013814. Association for Computational Lin-", "citeRegEx": "Stamatatos et al\\.,? 2000", "shortCiteRegEx": "Stamatatos et al\\.", "year": 2000}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "RWTHLM \u2014 the RWTH Aachen University neural network language modeling toolkit", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "pages 2093\u20132097.", "citeRegEx": "Sundermeyer et al\\.,? 2014", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schl\u00fcter."], "venue": "23(3):517\u2013529.", "citeRegEx": "Sundermeyer et al\\.,? 2015", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Automatic genre classification via n-grams of part-of-speech tags", "author": ["Xiaoyan Tang", "Jing Cao."], "venue": "Procedia-Social and Behavioral Sciences, 198:474\u2013 478.", "citeRegEx": "Tang and Cao.,? 2015", "shortCiteRegEx": "Tang and Cao.", "year": 2015}, {"title": "Genre distinctions for discourse in the Penn TreeBank", "author": ["Bonnie Webber."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, volume 2, pages", "citeRegEx": "Webber.,? 2009", "shortCiteRegEx": "Webber.", "year": 2009}, {"title": "Exploring the use of linguistic features in domain and genre classification", "author": ["Maria Wolters", "Mathias Kirsten."], "venue": "Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, pages 142\u2013149. Association for Computa-", "citeRegEx": "Wolters and Kirsten.,? 1999", "shortCiteRegEx": "Wolters and Kirsten.", "year": 1999}, {"title": "Finegrained genre classification using structural learning algorithms", "author": ["Zhili Wu", "Katja Markert", "Serge Sharoff."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 749\u2013759. Association for Computational Lin-", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Rui Zhang", "Honglak Lee", "Dragomir Radev."], "venue": "Proceedings of NAACL-HLT, pages 1512\u20131521.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "A c-lstm neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis Lau."], "venue": "arXiv preprint arXiv:1511.08630.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "One of the most popular document vectors is the term frequency-inverse document frequency (TF-IDF) feature vector (Robertson and Jones, 1976).", "startOffset": 114, "endOffset": 141}, {"referenceID": 10, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 11, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 30, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 23, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 5, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 15, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 8, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 19, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 29, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 3, "context": "TF-IDF Document vectorization has a major drawback(Cachopo, 2007; Le and Mikolov, 2014): it ignores word orders and other sequential information in a document.", "startOffset": 50, "endOffset": 87}, {"referenceID": 14, "context": "TF-IDF Document vectorization has a major drawback(Cachopo, 2007; Le and Mikolov, 2014): it ignores word orders and other sequential information in a document.", "startOffset": 50, "endOffset": 87}, {"referenceID": 14, "context": "Besides TF-IDF, another notable document vectorization is the paragraph vector(Le and Mikolov, 2014).", "startOffset": 78, "endOffset": 100}, {"referenceID": 14, "context": "Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016).", "startOffset": 139, "endOffset": 196}, {"referenceID": 4, "context": "Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016).", "startOffset": 139, "endOffset": 196}, {"referenceID": 0, "context": "Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016).", "startOffset": 139, "endOffset": 196}, {"referenceID": 12, "context": "Moreover, Skip-Thought Vectors (Kiros et al., 2015) also show superior performances against bag-of-words model.", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": "Our approach is to adapt/retrain a simple recurrent neural network language model (RNN-LM) (Mikolov et al., 2010) or a long short-term memory RNN language model (LSTM-LM) (Sundermeyer et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 24, "context": ", 2010) or a long short-term memory RNN language model (LSTM-LM) (Sundermeyer et al., 2012) with a document, and then vectorize the retrained/adapted model parameters to obtain its document vector (labeled as DV-RNN and DV-LSTM) from RNN-LM and LSTM-LM respectively.", "startOffset": 65, "endOffset": 91}, {"referenceID": 27, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 13, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 32, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 35, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 33, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 34, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 17, "context": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).", "startOffset": 140, "endOffset": 205}, {"referenceID": 18, "context": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).", "startOffset": 140, "endOffset": 205}, {"referenceID": 1, "context": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).", "startOffset": 140, "endOffset": 205}, {"referenceID": 9, "context": "Training the above simple RNN may suffer from the problem of exploding or vanishing gradients (Hochreiter and Schmidhuber, 1997; Sundermeyer et al., 2012).", "startOffset": 94, "endOffset": 154}, {"referenceID": 24, "context": "Training the above simple RNN may suffer from the problem of exploding or vanishing gradients (Hochreiter and Schmidhuber, 1997; Sundermeyer et al., 2012).", "startOffset": 94, "endOffset": 154}, {"referenceID": 7, "context": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al.", "startOffset": 123, "endOffset": 149}, {"referenceID": 2, "context": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al.", "startOffset": 198, "endOffset": 213}, {"referenceID": 16, "context": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al., 1995).", "startOffset": 270, "endOffset": 291}, {"referenceID": 29, "context": "The Penn Treebank dataset(PTB) was artificially extracted from the Penn Treebank Corpus by taking out the documents that have the available genre tags provided by (Webber, 2009; Plank, 2009).", "startOffset": 163, "endOffset": 190}, {"referenceID": 20, "context": "The Penn Treebank dataset(PTB) was artificially extracted from the Penn Treebank Corpus by taking out the documents that have the available genre tags provided by (Webber, 2009; Plank, 2009).", "startOffset": 163, "endOffset": 190}, {"referenceID": 31, "context": "For the Brown Corpus, the sub-genres under the fiction genre were merged so that the total number of genres was 10 (Wu et al., 2010).", "startOffset": 115, "endOffset": 132}, {"referenceID": 26, "context": "The RWTH Aachen University Neural Network Language Modeling Toolkit (RWTHLM) (Sundermeyer et al., 2015; Sundermeyer et al., 2014) was used for training all LSTM-LMs and adapting them to produce the DV-LSTMs.", "startOffset": 77, "endOffset": 129}, {"referenceID": 25, "context": "The RWTH Aachen University Neural Network Language Modeling Toolkit (RWTHLM) (Sundermeyer et al., 2015; Sundermeyer et al., 2014) was used for training all LSTM-LMs and adapting them to produce the DV-LSTMs.", "startOffset": 77, "endOffset": 129}, {"referenceID": 14, "context": "The paragraph vectors (PV-DMs) (Le and Mikolov, 2014) were trained for each document in a corpus using the Gensim toolkit (vReh\u016fvrek and Sojka, 2010).", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "The paragraph vectors (PV-DMs) (Le and Mikolov, 2014) were trained for each document in a corpus using the Gensim toolkit (vReh\u016fvrek and Sojka, 2010).", "startOffset": 122, "endOffset": 149}, {"referenceID": 28, "context": "The baseline results labeled with * are quoted from (Tang and Cao, 2015; Wu et al., 2010).", "startOffset": 52, "endOffset": 89}, {"referenceID": 31, "context": "The baseline results labeled with * are quoted from (Tang and Cao, 2015; Wu et al., 2010).", "startOffset": 52, "endOffset": 89}, {"referenceID": 6, "context": "To evaluate the statistical significance of the performance difference between each vector, we also adopt the pair sample t-test (Dietterich, 1998).", "startOffset": 129, "endOffset": 147}], "year": 2016, "abstractText": "In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequencyinverse document frequency (TF-IDF) vector. One major shortcoming of the frequencybased TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document, and they can be important in some NLP tasks such as genre classification. This paper proposes a novel distributed vector representation of a document: a simple recurrentneural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document. The new document vectors are labeled as DV-RNN and DV-LSTM respectively. We believe that our new document vectors can capture some high-level sequential information in the documents, which other current document representations fail to capture. The new document vectors were evaluated in the genre classification of documents in three corpora: the Brown Corpus, the BNC Baby Corpus and an artificially created Penn Treebank dataset. Their classification performances are compared with the performance of TF-IDF vector and the state-of-the-art distributed memory model of paragraph vector (PV-DM). The results show that DV-LSTM significantly outperforms TF-IDF and PV-DM in most cases, and combinations of the proposed document vectors with TF-IDF or PVDM may further improve performance.", "creator": "LaTeX with hyperref package"}}}