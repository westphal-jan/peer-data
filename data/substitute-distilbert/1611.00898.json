{"id": "1611.00898", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Low Rank Approximation with Entrywise $\\ell_1$-Norm Error", "abstract": "examples study the $ \\ ell _ 1 $ - low rank approximation problem, estimated for a given $ n \\ times d $. $ a $ or j factor $ \\ alpha \\ geq 1 $, the goal is to output a t - $ k $ matrix $ \\ widehat { a } $ for which", "histories": [["v1", "Thu, 3 Nov 2016 07:13:20 GMT  (146kb,D)", "http://arxiv.org/abs/1611.00898v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CC cs.LG", "authors": ["zhao song", "david p woodruff", "peilin zhong"], "accepted": false, "id": "1611.00898"}, "pdf": {"name": "1611.00898.pdf", "metadata": {"source": "CRF", "title": "Low Rank Approximation with Entrywise `1-Norm Error", "authors": ["Zhao Song", "David P. Woodruff", "Peilin Zhong"], "emails": ["zhaos@utexas.edu", "dpwoodru@us.ibm.com", "peilin.zhong@columbia.edu"], "sections": [{"heading": null, "text": "\u2016A\u2212 A\u0302\u20161 \u2264 \u03b1 \u00b7 min rank-k matrices A\u2032 \u2016A\u2212A\u2032\u20161,\nwhere for an n\u00d7 d matrix C, we let \u2016C\u20161 = \u2211n\ni=1 \u2211d j=1 |Ci,j |. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is indicated in models where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple places if there are any approximation algorithms.\nWe give the first provable approximation algorithms for `1-low rank approximation, showing that it is possible to achieve approximation factor \u03b1 = (log d)\u00b7poly(k) in nnz(A)+(n+d) poly(k) time, where nnz(A) denotes the number of non-zero entries of A. If k is constant, we further improve the approximation ratio toO(1) with a poly(nd)-time algorithm. Under the Exponential Time Hypothesis, we show there is no poly(nd)-time algorithm achieving a (1 + 1\nlog1+\u03b3(nd) )-\napproximation, for \u03b3 > 0 an arbitrarily small constant, even when k = 1. We give a number of additional results for `1-low rank approximation: nearly tight upper and lower bounds for column subset selection, CUR decompositions, extensions to low rank approximation with respect to `p-norms for 1 \u2264 p < 2 and earthmover distance, low-communication distributed protocols and low-memory streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. We also give a preliminary empirical evaluation.\n\u2217Work done while visiting IBM Almaden.\nar X\niv :1\n61 1.\n00 89\n8v 1\n[ cs\n.D S]\n3 N\nov 2\n01 6"}, {"heading": "1 Introduction", "text": "Two well-studied problems in numerical linear algebra are regression and low rank approximation. In regression, one is given an n\u00d7 d matrix A, and an n\u00d7 1 vector b, and one seeks an x \u2208 Rd which minimizes \u2016Ax \u2212 b\u2016 under some norm. For example, for least squares regression one minimizes \u2016Ax \u2212 b\u20162. In low rank approximation, one is given an n \u00d7 d matrix A, and one seeks a rank-k matrix A\u0302 which minimizes \u2016A \u2212 A\u0302\u2016 under some norm. For example, in Frobenius norm low rank\napproximation, one minimizes \u2016A \u2212 A\u0302\u2016F = (\u2211 i,j(Ai,j \u2212 A\u0302i,j)2 )1/2\n. Algorithms for regression are often used as subroutines for low rank approximation. Indeed, one of the main insights of [DMM06b, DMM06a, Sar06, DMM08, CW09] was to use results for generalized least squares regression for Frobenius norm low rank approximation. Algorithms for `1-regression, in which one minimizes \u2016Ax \u2212 b\u20161 = \u2211 i |(Ax)i \u2212 bi|, were also used [BD13, SW11] to fit a set of points to a hyperplane, which is a special case of entrywise `1-low rank approximation, the more general problem being to find a rank-k matrix A\u0302 minimizing \u2211 i,j |Ai,j \u2212 A\u0302i,j |.\nRandomization and approximation were introduced to significantly speed up algorithms for these problems, resulting in algorithms achieving relative error approximation with high probability. Such algorithms are based on sketching and sampling techniques; we refer to [Woo14b] for a survey. For least squares regression, a sequence of work [Sar06, CW13, MM13, NN13, LMP13, BDN15, Coh16] shows how to achieve algorithms running in nnz(A) + poly(d) time. For Frobenius norm low rank approximation, using the advances for regression this resulted in nnz(A) + (n + d) poly(k) time algorithms. For `1-regression, sketching and sampling-based methods [Cla05, SW11, CDMI+13, CW13, MM13, LMP13, WZ13, CW15b, CP15] led to an nnz(A) + poly(d) time algorithm.\nJust like Frobenius norm low rank approximation is the analogue of least squares regression, entrywise `1-low rank approximation is the analogue of `1-regression. Despite this analogy, no non-trivial upper bounds with provable guarantees are known for `1-low rank approximation. Unlike Frobenius norm low rank approximation, which can be solved exactly using the singular value decomposition, no such algorithm or closed-form solution is known for `1-low rank approximation. Moreover, the problem was recently shown to be NP-hard [GV15]. A major open question is whether there exist approximation algorithms, sketching-based or otherwise, for `1-low rank approximation. Indeed, the question of obtaining betters algorithms was posed in section 6 of [GV15], in [Exc13], and as the second part of open question 2 in [Woo14b], among other places. The earlier question of NP-hardness was posed in Section 1.4 of [KV09], for which the question of obtaining approximation algorithms is a natural followup. The goal of our work is to answer this question.\nWe now formally define the `1-low rank approximation problem: we are given an n\u00d7 d matrix A and approximation factor \u03b1 \u2265 1, and we would like, with large constant probability, to output a rank-k matrix A\u0302 for which\n\u2016A\u2212 A\u0302\u20161 \u2264 \u03b1 \u00b7 min rank-k matrices A\u2032\n\u2016A\u2212A\u2032\u20161, (1)\nwhere for an n \u00d7 d matrix C, we let \u2016C\u20161 = \u2211n\ni=1 \u2211d j=1 |Ci,j |. This notion of low rank approx-\nimation has been proposed as a more robust alternative to Frobenius norm low rank approximation [KK03, KK05, KLC+15, Kwa08, ZLS+12, BJ12, BD13, BDB13, MXZZ13, MKP13, MKP14, MKCP16, PK16], and is sometimes referred to as `1-matrix factorization or robust PCA. `1-low rank approximation gives improved results over Frobenius norm low rank approximation since outliers are less exaggerated, as one does not square their contribution in the objective. The outlier values are often erroneous values that are far away from the nominal data, appear only a few times in the data matrix, and would not appear again under normal system operation. These works also argue `1-low rank approximation can better handle missing data, is appropriate in noise models for which\nthe noise is not Gaussian, e.g., it produces the maximum likelihood estimator for Laplacian noise [Gao08, KAC+08, VT01], and can be used in image processing to prevent image occlusion [YZD12].\nTo see that `1-low rank approximation and Frobenius norm low rank approximation can give very different results, consider the n \u00d7 n matrix A = [ n 0 0 B ] , where B is any (n \u2212 1) \u00d7 (n \u2212 1) matrix with \u2016B\u2016F < n. The best rank-1 approximation with Frobenius norm error is given by A\u0302 = n \u00b7 e1e>1 , where e1 is the first standard unit vector. Here A\u0302 ignores all but the first row and column of A, which may be undesirable in the case that this row and column represent an outlier. Note \u2016A \u2212 A\u0302\u20161 = \u2016B\u20161. If, for example, B is the all 1s matrix, then A\u0302 = [0, 0; 0, B] is a rank-1 approximation for which \u2016A\u2212 A\u0302\u20161 = n, and therefore this solution is a much better solution to the `1-low rank approximation problem than n \u00b7 e1e>1 , for which \u2016A\u2212 n \u00b7 e1e>1 \u20161 = (n\u2212 1)2.\nDespite the advantages of `1-low rank approximation, its main disadvantage is its computationally intractability. It is not rotationally invariant and most tools for Frobenius low rank approximation do not apply. To the best of our knowledge, all previous works only provide heuristics. We provide hard instances for previous work in Section L, showing these algorithms at best give a poly(nd)-approximation (though even this is not shown in these works). We also mention why a related objective function, robust PCA [WGR+09, CLMW11, NNS+14, NYH14, CHD16, ZZL15], does not give a provable approximation factor for `1-low rank approximation. Using that for an n\u00d7d matrix C, \u2016C\u2016F \u2264 \u2016C\u20161 \u2264 \u221a nd\u2016C\u2016F , a Frobenius norm low rank approximation gives a \u221a nd approximation for `1-low rank approximation. A bit better is to use algorithms for low rank approximation with respect to the sum of distances, i.e., to find a rank-k matrix A\u0302 minimizing \u2016A\u2212 A\u0302\u20161,2, where for an n \u00d7 d matrix C, \u2016C\u20161,2 = \u2211n i=1 \u2016Ci\u20162, where Ci is the i-th row of C. A sequence of work [DV07, FMSW10, FL11, SV12, CW15a] shows how to obtain an O(1)-approximation to this problem in nnz(A) + (n + d) poly(k) + exp(k) time, and using that \u2016C\u20161,2 \u2264 \u2016C\u20161 \u2264 \u221a d\u2016C\u20161,2\nresults in an O( \u221a d)-approximation.\nThere are also many variants of Frobenius norm low rank approximation for which nothing is known for `1-low rank approximation, such as column subset selection and CUR decompositions, distributed and streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. Other interesting questions include low rank approximation for related norms, such as `p-low rank approximation in which one seeks a rank-k matrix A\u0302 minimizing \u2211n i=1 \u2211d j=1(Ai,j \u2212 A\u0302i,j)p. Note for 1 \u2264 p < 2 these are also more robust than the SVD."}, {"heading": "1.1 Our Results", "text": "We give the first efficient algorithms for `1-low rank approximation with provable approximation guarantees. By symmetry of the problem, we can assume d \u2264 n. We first give an algorithm which runs in O(nnz(A)) + n \u00b7 poly(k) time and solves the `1-low rank approximation problem with approximation factor (log d) \u00b7 poly(k). This is an exponential improvement over the previous approximation factor of O( \u221a d), provided k is not too large, and is polynomial time for every k. Moreover, provided nnz(A) \u2265 n \u00b7poly(k), our time is optimal up to a constant factor as any relative error algorithm must spend nnz(A) time. We also give a hard instance for our algorithm ruling out log d k log k + k\n1/2\u2212\u03b3 approximation for arbitrarily small constant \u03b3 > 0, and hard instances for a general class of algorithms based on linear sketches, ruling out k1/2\u2212\u03b3 approximation.\nVia a different algorithm, we show how to achieve an O\u0303(k)-approximation factor in poly(n)dO\u0303(k)2O\u0303(k\n2) time. This is useful for constant k, for which it gives an O(1)-approximation in poly(n) time, improving the O(log d)-approximation for constant k of our earlier algorithm. The approximation ratio of this algorithm, although O(1) for constant k, depends on k. We also show one can find a rank-2k matrix A\u0302 in poly(n) time for constant k for which \u2016A \u2212 A\u0302\u20161 \u2264\nC minrank-k matrices A\u2032 \u2016A \u2212 A\u2032\u20161, where C > 1 is an absolute constant independent of k. We refer to this as a bicriteria algorithm. Finally, one can output a rank-k matrix A\u0302, instead of a rank2k matrix A\u0302, in poly(n) time with the same absolute constant C approximation factor, under an additional assumption that the entries of A\u0302 are integers in the range {\u2212b,\u2212b+1, . . . , b} for an integer b \u2264 poly(n). Unlike our previous algorithms, this last algorithm has a bit complexity assumption.\nUnder the Exponential Time Hypothesis (ETH), we show there is no poly(n)-time algorithm achieving a (1 + 1\nlog1+\u03b3(n) )-approximation, for \u03b3 > 0 an arbitrarily small constant, even when k = 1.\nThe latter strengthens the NP-hardness result of [GV15]. We also give a number of results for variants of `1-low rank approximation which are studied for Frobenius norm low rank approxiation; prior to our work nothing was known about these problems. Column Subset Selection and CUR Decomposition: In the column subset selection problem, one seeks a small subset C of columns of A for which there is a matrix X for which \u2016CX \u2212 A\u2016 is small, under some norm. The matrix CX provides a low rank approximation to A which is often more interpretable, since it stores actual columns of A, preserves sparsity, etc. These have been extensively studied when the norm is the Frobenius or operator norm (see, e.g., [BMD09, DR10, BDM11] and the references therein). We initiate the study of this problem with respect to the `1-norm. We first prove an existence result, namely, that there exist matrices A for which any subset C of poly(k) columns satisfies minX \u2016CX\u2212A\u20161 \u2265 k1/2\u2212\u03b3 \u00b7minrank-k matrices A\u2032 \u2016A\u2212A\u2032\u20161, where \u03b3 > 0 is an arbitrarily small constant. This result is in stark contrast to the Frobenius norm for which for every matrix there exist O(k ) columns for which the approximation factor is 1 + . We also show that our bound is nearly optimal in this regime, by showing for every matrix there exists a subset of O(k log k) columns providing an O( \u221a k log k)-approximation. One can find such columns in poly(n)dO(k log k) time by enumerating and evaluating the cost of each subset. Although this is exponential in k, we show it is possible to find O(k log k) columns providing an O( \u221a k log k log d)-approximation in polynomial time for every k. We extend these results to the CUR decomposition problem (see, e.g., [DMM08, BW14]), in which one seeks a factorization CUR for which C is a subset of columns of A, R is a subset of rows of A, and \u2016CUR\u2212A\u2016 is as small as possible. In the case of Frobenius norm, one can choose O(k/ ) columns and rows, have rank(U) = k, have \u2016CUR\u2212A\u2016F be at most (1 + ) times the optimal cost, and find the factorization in nnz(A) log n + n \u00b7 poly(k/ ) time [BW14]. Using our column subset selection results, we give an nnz(A) + n \u00b7 poly(k) time algorithm choosing O(k log k) columns and rows, for which rank(U) = k, and for which \u2016CUR \u2212 A\u20161 is poly(k) log d times the cost of any rank-k approximation to A.\n`p-Low Rank Approximation and EMD-Low Rank Approximation: We also give the first algorithms with provable approximation guarantees for the `p-low rank approximation problem, 1 \u2264 p < 2, in which we are given an n \u00d7 d matrix A and approximation factor \u03b1 \u2265 1, and would like, with large constant probability, to output a rank-k matrix A\u0302 for which\n\u2016A\u2212 A\u0302\u2016pp \u2264 \u03b1 \u00b7 minrank-k matrices A\u2032 \u2016A\u2212A \u2032\u2016pp, (2)\nwhere for an n \u00d7 d matrix C, \u2016C\u2016pp = \u2211n\ni=1 \u2211d j=1 |Ci,j |p. We obtain similar algorithms for this\nproblem as for `1-low rank approximation. For instance, we obtain an nnz(A) + n \u00b7 poly(k) time algorithm with approximation ratio (log d) \u00b7 poly(k). We also provide the first low rank approximation with respect to sum of earthmover distances (of the n rows of A and A\u0302) with a (log2 d) poly(k) approximation factor. This low rank error measure was used, e.g., in [SL09]. Sometimes such applications also require a non-negative factorization, which we do not provide.\nDistributed/Streaming Algorithms, and Algorithms with Limited Randomness: There is a growing body of work on low rank approximation in the distributed (see, e.g., [TD99, QOSG02,\nBCL05, BRB08, MBZ10, FEGK13, PMvdG+13, KVW14, BKLW14, BLS+16, BWZ16, WZ16]) and streaming models (see, e.g., [McG06, CW09, KL11, GP13, Lib13, KLM+14, Woo14a]), though almost exclusively for the Frobenius norm. One distributed model is the arbitrary partition model [KVW14] in which there are s servers, each holding an n\u00d7dmatrix Ai, and they would like to output a k\u00d7 d matrix V > for which minU \u2016UV >\u2212A\u2016 is as small as possible (or, a centralized coordinator may want to output this). We give O\u0303(snk)-communication algorithms achieving a poly(k, log(n))approximation for `1-low rank approximation in the arbitrary partition model, which is optimal for this approximation factor (see [BW14] where lower bounds for Frobenius norm approximation with poly(n) multiplicative approximation were shown - such lower bounds apply to `1 low rank approximation). We also consider the turnstile streaming model [Mut05] in which we receive positive or negative updates to its entries and wish to output a rank-k factorization at the end of the stream. We give an algorithm using O\u0303(nk)+poly(k) space to achieve a poly(k, log(n))-approximation, which is space-optimal for this approximation factor, up to the degree of the poly(k) factor. To obtain these results, we show our algorithms can be implemented using O\u0303(dk) random bits.\nWe stress for all of our results, we do not make assumptions on A such as low coherence or condition number; our results hold for any n\u00d7 d input matrix A.\nWe report a promising preliminary empirical evaluation of our algorithms in Section L.\nRemark 1.1. We were just informed of the concurrent and independent work [CGK+16], which also obtains approximation algorithms for `1-low rank approximation. That paper obtains a 2O(k) log dapproximation in (log d)k poly(nd) time. Their algorithm is not polynomial time once k = \u2126\u0303(log d), whereas we obtain a polynomial time algorithm for every k (in fact nnz(A) + (n+ d) poly(k) time). Our approximation factor is also poly(k) log d, which is an exponential improvement over theirs in terms of k. In [CGK+16] they also obtain a 2k-approximation in poly(nd)dO(k) time. In contrast, we obtain an O\u0303(k)-approximation in poly(nd)dO\u0303(k)2O\u0303(k2) time. The dependence in [CGK+16] on k in the approximation ratio is exponential, whereas ours is polynomial."}, {"heading": "1.2 Technical Overview", "text": "Initial Algorithm and Optimizations: Let A\u2217 be a rank-k matrix for which \u2016A \u2212 A\u2217\u20161 = minrank-k matrices A\u2032 \u2016A \u2212 A\u2032\u20161. Let A\u2217 = U\u2217V \u2217 be a factorization for which U\u2217 is n \u00d7 k and V \u2217 is k \u00d7 d. Suppose we somehow knew U\u2217 and consider the multi-response `1-regression problem minV \u2016U\u2217V \u2212 A\u20161 = minV \u2211d i=1 \u2016U\u2217Vi \u2212 Ai\u20161, where Vi, Ai denote the i-th columns of V and A, respectively. We could solve this with linear programming though this is not helpful for our argument here.\nInstead, inspired by recent advances in sketching for linear algebra (see, e.g., [Woo14b] for a survey), we could choose a random matrix S and solve minV \u2016SU\u2217V \u2212SA\u20161 = minV \u2211d i=1 \u2016(SU\u2217)Vi\u2212 SAi\u20161. If V is an approximate minimizer of the latter problem, we could hope V is an approximate minimizer of the former problem. If also S has a small number t of rows, then we could instead solve minV \u2211d i=1 \u2016(SU\u2217)Vi \u2212 SAi\u20162, that is, minimize the sum of Euclidean norms rather than the sum of `1-norms. Since t\u22121/2\u2016(SU\u2217)Vi\u2212SAi\u20161 \u2264 \u2016(SU\u2217)Vi\u2212SAi\u20162 \u2264 \u2016(SU\u2217)Vi\u2212SAi\u20161, we would obtain a \u221a t-approximation to the problem minV \u2016SU\u2217V \u2212 SA\u20161. A crucial observation is that the\nsolution to minV \u2211d\ni=1 \u2016(SU\u2217)Vi\u2212SAi\u20162 is given by V = (SU\u2217)\u2020SA, which implies that V is in the row span of SA. If also S were oblivious to U\u2217, then we could compute SA without ever knowing U\u2217. Having a low-dimensional space containing a good solution in its span is our starting point.\nFor this to work, we need a distribution on oblivious matrices S with a small number of rows, for which an approximate minimizer V to minV \u2016SU\u2217V \u2212 SA\u20161 is also an approximate minimizer to minV \u2016U\u2217V \u2212 A\u20161. It is unknown if there exists a distribution on S with this property. What\nis known is that if S has O(d log d) rows, then the Lewis weights (see, e.g., [CP15] and references therein) of the concatenated matrix [U\u2217, A] give a distribution for which the optimal V for the latter problem is a (1+ )-approximation to the former problem; see also earlier work on `1-leverage scores [Cla05, DDH+09] which have poly(d) rows and the same (1 + )-approximation guarantee. Such distributions are not helpful here as (1) they are not oblivious, and (2) the number O(d log d) of rows gives an O( \u221a d log d) approximation factor, which is much larger than what we want.\nThere are a few oblivious distributions S which are useful for single-response `1-regression min \u2016U\u2217v \u2212 a\u20161 for column vectors v, a \u2208 Rk [SW11, CDMI+13, WZ13]. In particular, if S is an O(k log k)\u00d7n matrix of i.i.d. Cauchy random variables, then the solution v to min \u2016SU\u2217v\u2212Sa\u20161 is an O(k log k)-approximation to min \u2016U\u2217v\u2212a\u20161 [SW11]. The important property of Cauchy random variables is that if X and Y are independent Cauchy random variables, then \u03b1X+\u03b2Y is distributed as a Cauchy random variable times |\u03b1|+ |\u03b2|, for any scalars \u03b1, \u03b2 \u2208 R. The O(k log k) approximation arises because all possible regression solutions are in the column span of [U\u2217, a] which is (k + 1)- dimensional, and the sketch S gives an approximation factor of O(k log k) to preserve every vector norm in this subspace. If we instead had a multi-response regression problem min \u2016SU\u2217V \u2217 \u2212 SA\u20161 the dimension of the column span of [U\u2217, A] would be d + k, and this approach would give an O(d log d)-approximation. Unlike Frobenius norm multi-response regression min \u2016SU\u2217V \u2217 \u2212 SA\u2016F , which can be bounded if S is a subspace embedding for U\u2217 and satisfies an approximate matrix product theorem [Sar06], there is no convenient linear-algebraic analogue for the `1-norm.\nWe first note that since regression is a minimization problem, to obtain an O(\u03b1)-approximation by solving the sketched version of the problem, it suffices that (1) for the optimal V \u2217, we have \u2016SU\u2217V \u2217\u2212SA\u20161 \u2264 O(\u03b1)\u2016U\u2217V \u2217\u2212A\u20161, and (2) for all V , we have \u2016SU\u2217V \u2212SA\u20161 \u2265 \u2126(1)\u00b7\u2016U\u2217V \u2212A\u20161.\nWe show (1) holds for \u03b1 = O(log d) and any number of rows of S. Our analysis follows by truncating the Cauchy random variables (SU\u2217V \u2217j \u2212 SAj)i for i \u2208 [O(k log k)] and j \u2208 [d], so that their expectation exists, and applying linearity of expectation across the d columns. This is inspired from an argument of Indyk [Ind06] for embedding a vector into a lower-dimensional vector while preserving its `1-norm; for single-response regression this is the statement that \u2016SU\u2217v\u2217 \u2212 Sa\u20161 = \u0398(1)\u2016U\u2217v \u2212 a\u20161, implied by [Ind06]. However, for multi-response regression we have to work entirely with expectations, rather than the tail bounds in [Ind06], since the Cauchy random variables (SU\u2217Vj \u2212 SAj)i, while independent across i, are dependent across j. Moreover, our O(log d)-approximation factor is not an artifact of our analysis - we show in Section G that there is an n \u00d7 d input matrix A for which with probability 1 \u2212 1/ poly(k), there is no k-dimensional space in the span of SA achieving a ( log d t log t + k 1/2\u2212\u03b3 ) -approximation, for S a Cauchy matrix with t rows, where \u03b3 > 0 is an arbitrarily small constant. This shows (k log d)\u2126(1)-inapproximability. Thus, the fact that we achieve O(log d)-approximation instead of O(1) is fundamental for a matrix S of Cauchy random variables or any scaling of it.\nWhile we cannot show (2), we instead show for all V , \u2016SU\u2217V \u2212 SA\u20161 \u2265 \u2016U\u2217V \u2212 A\u20161/2 \u2212 O(log d)\u2016U\u2217V \u2217\u2212A\u20161 if S has O(k log k) rows. This suffices for regression, since the only matrices V for which the cost is much smaller in the sketch space are those providing an O(log d) approximation in the original space. The guarantee follows from the triangle inequality: \u2016SU\u2217V \u2212SA\u20161 \u2265 \u2016SU\u2217V \u2212 SU\u2217V \u2217\u20161\u2212\u2016SU\u2217V \u2217\u2212SA\u20161 and the fact that S is known to not contract any vector in the column span of U\u2217 if S has O(k log k) rows [SW11]. Because of this, we have \u2016SU\u2217V \u2212 SU\u2217V \u2217\u20161 = \u2126(1)\u2016U\u2217V \u2212U\u2217V \u2217\u20161 = \u2126(1)(\u2016U\u2217V \u2212A\u20161\u2212\u2016U\u2217V \u2217\u2212A\u20161), where we again use the triangle inequality. We also bound the additive term \u2016SU\u2217V \u2217 \u2212 SA\u20161 by O(log d)\u2016U\u2217V \u2217 \u2212A\u20161 using (1) above.\nGiven that SA contains a good rank-k approximation in its row span, our algorithm with a slightly worse poly(n) time and poly(k log(n))-approximation can be completely described here. Let S and T1 be independent O(k log k)\u00d7 n matrices of i.i.d. Cauchy random variables, and let R\nand T2 be independent d\u00d7O(k log k) matrices of i.i.d. Cauchy random variables. Let\nX = (T1AR) \u2020((T1AR)(T1AR) \u2020(T1AT2)(SAT2)(SAT2) \u2020)k(SAT2) \u2020,\nwhich is the rank-k matrix minimizing \u2016T1ARXSAT2 \u2212 T1AT2\u2016F , where for a matrix C, Ck is its best rank-k approximation in Frobenius norm. Output A\u0302 = ARXSA as the solution to `1-low rank approximation of A. We show with constant probability that A\u0302 is a poly(k log(n))-approximation.\nTo improve the approximation factor, after computing SA, we `1-project each of the rows of A onto SA using linear programming or fast algorithms for `1-regression [CW13, MM13], obtaining an n\u00d7 d matrix B of rank O(k log k). We then apply the algorithm in the previous paragraph with A replaced by B. This ultimately leads to a log d \u00b7 poly(k)-approximation.\nTo improve the running time from poly(n) to nnz(A) + n \u00b7 poly(k), we show a similar analysis holds for the sparse Cauchy matrices of [MM13]; see also the matrices in [WZ13].\nCUR Decompositions: To obtain a CUR decomposition, we first find a log d\u00b7poly(k)-approximate rank-k approximation A\u0302 as above. Let B1 be an n\u00d7 k matrix whose columns span those of A\u0302, and consider the regression minV \u2016B1V \u2212 A\u20161. Unlike the problem minV \u2016U\u2217V \u2212 A\u20161 where U\u2217 was unknown, we know B1 so can compute its Lewis weights efficiently, sample by them, and obtain a regression problem minV \u2016D1(B1V \u2212A)\u20161 where D1 is a sampling and rescaling matrix. Since\n\u2016D1(B1V \u2212A)\u20161 \u2264 \u2016D1(B1V \u2212B1V \u2217)\u20161 + \u2016D1(B1V \u2217 \u2212A)\u20161,\nwhere V \u2217 = argminV \u2016B1V \u2212 A\u20161, we can bound the first term by O(\u2016B1V \u2212 B1V \u2217\u20161) using that D1 is a subspace embedding if it has O(k log k) rows, while the second term is O(1)\u2016B1V \u2217 \u2212 A\u20161 by a Markov bound. Note that \u2016B1V \u2217 \u2212 A\u20161 \u2264 (log d) \u00b7 poly(k) minrank-k matrices A\u2032 \u2016A \u2212 A\u2032\u20161. By switching to `2 as before, we see that V\u0302 = (D1B1)\u2020D1A contains a (log d) poly(k)-approximation in its span. Here D1A is an actual subset of rows of A, as required in a CUR decomposition. Moreover the subset size is O(k log k). We can sample by the Lewis weights of V\u0302 to obtain a subset C of O(k log k) rescaled columns of A, together with a rank-k matrix U for which \u2016CUR\u2212A\u20161 \u2264 (log d) poly(k) minrank-k matrices A\u2032 \u2016A\u2212A\u2032\u20161.\nAlgorithm for Small k: Our CUR decomposition shows how we might obtain anO(1)-approximation for constant k in poly(n) time. If we knew the Lewis weights of U\u2217, an \u03b1-approximate solution to the problem minV \u2016D1(U\u2217V \u2212 A)\u20161 would be an O(\u03b1)-approximate solution to the problem minV \u2016U\u2217V \u2212 A\u20161, where D1 is a sampling and rescaling matrix of O(k log k) rows of A. Moreover, an O( \u221a k log k)-approximate solution to minV \u2016D1(U\u2217V \u2212A)\u20161 is given by V = (D1U\u2217)\u2020D1A, which implies the O(k log k) rows of D1A contain an O( \u221a k log k)-approximation. For small k, we can guess every subset of O(k log k) rows of A in nO(k log k) time (if d n, by taking transposes at the beginning one can replace this with dO(k log k) time). For each guess, we set up the problem minrank-k U \u2016U(D1A)\u2212A\u20161. If D2 is a sampling and rescaling matrix according to the Lewis weights of D1A, then by a similar triangle inequality argument as for our CUR decomposition, minimizing \u2016U(D1A)D2\u2212AD2\u20161 gives an O( \u221a k log k) approximation. By switching to `2, this implies there is an O(k log k)-approximation of the form AD2WD1A, whereW is an O(k log2 k)\u00d7O(k log k) matrix of rank k. By setting up the problem minrank-k W \u2016AD2WD1A\u2212 A\u20161, one can sample from Lewis weights on the left and right to reduce this to a problem independent of n and d, after which one can use polynomial optimization to solve it in exp(poly(k)) time. One of our guesses D1A will be correct, and for this guess we obtain an O\u0303(k)-approximation. For each guess we can compute its cost and take the best one found. This gives an O(1)-approximation for constant k, removing the O(log d)-factor from the approximation of our earlier algorithm.\nExistential Results for Subset Selection: In our algorithm for small k, the first step was to show there exist O(k log k) rows of A which contain a rank-k space which is an O( \u221a k log k)approximation. While for Frobenius norm one can find O(k) rows with an O(1)-approximation in their span, one of our main negative results for `1-low rank approximation is that this is impossible, showing that the best approximation one can obtain with poly(k) rows is k1/2\u2212\u03b3 for an arbitrarily small constant \u03b3 > 0. Our hard instance is an r\u00d7 (r+ k) matrix A in which the first k columns are i.i.d. Gaussian, and the remaining r columns are an identity matrix. Here, r can be twice the number of rows one is choosing. The optimal `1-low rank approximation has cost at most r, obtained by choosing the first k columns.\nLet R \u2208 Rr/2\u00d7k denote the first k entries of the r/2 chosen rows, and let y denote the first k entries of an unchosen row. For r/2 > k, there exist many solutions x \u2208 Rr/2 for which x>R = y. However, we can show the following tradeoff:\nwhenever \u2016x>R\u2212 y\u20161 < \u221a k poly(log k) , then \u2016x\u20161 > \u221a k poly(log k) .\nThen no matter which linear combination x> of the rows of R one chooses to approximate y by, either one incurs a \u221a k\npoly(log k) cost on the first k coordinates, or since A contains an identity matrix,\none incurs cost \u2016x\u20161 > \u221a k poly(log k) on the last r coordinates of x >R.\nTo show the tradeoff, consider an x \u2208 Rr/2. We decompose x = x0 + \u2211\nj\u22651 x j , where xj agrees\nwith x on coordinates which have absolute value in the range 1\u221a k logc k \u00b7 [2\u2212j , 2\u2212j+1], and is zero otherwise. Here, c > 0 is a constant, and x0 denotes the restriction of x to all coordinates of absolute value at least 1\u221a\nk logc k . Then \u2016x\u20161 <\n\u221a k\nlogc k , as otherwise we are done. Hence, x 0 has\nsmall support. Thus, one can build a small net for all x0 vectors by choosing the support, then placing a net on it. For xj for j > 0, the support sizes are increasing so the net size needed for all xj vectors is larger. However, since xj has all coordinates of roughly the same magnitude on its support, its `2-norm is decreasing in j. Since (xj)>R \u223c N(0, \u2016xj\u201622Ik), this makes it much less likely that individual coordinates of (xj)>R can be large. Since this probability goes down rapidly, we can afford to union bound over the larger net size. What we show is that for any sum of the form \u2211 j\u22651 x j , at most k10 of its coordinates are at least 1 log k in magnitude.\nFor \u2016x>R \u2212 y\u20161 to be at most \u221a k logc k , for at least k 2 coordinates i, we must have |(x\n>R \u2212 y)i| < 2\u221ak logc k . With probability 1 \u2212 2 \u2212\u2126(k), |yi| \u2265 1100 on at least 2k 3 coordinates. From the previous paragraph, it follows there are at least k2 \u2212 k 10 \u2212 k 3 = \u2126(k) coordinates i of x for which (1)\n|(x>R \u2212 y)i| < 2\u221ak logc k , (2) | \u2211 j\u22651 x j i | < 1 log k , and (3) |yi| \u2265 1 100 . On these i, (x 0)>Ri must be in an interval of width 1log k at distance at least 1 100 from the origin. Since (x 0)>R \u223c N(0, \u2016x0\u201622Ik), for any value of \u2016x0\u201622 the probability this happens on \u2126(k) coordinates is at most 2\u2212\u0398(k). Since the net size for x0 is small, we can union bound over every sequence x0, x1, . . . , coming from our nets.\nSome care is needed to union bound over all possible subsets R of rows which can be chosen. We handle this by conditioning on a few events of A itself, which imply corresponding events for every subset of rows. These events are such that if R is the chosen set of half the rows, and S the remaining set of rows of A, then the event that a constant fraction of rows in S are close to the row span of R is 2\u2212\u0398(kr), which is small enough to union bound over all choices of R.\nCuriously, we also show there are some matrices A \u2208 Rn\u00d7d for which any `1 rank-k approximation in the entire row span of A cannot achieve better than a (2\u2212\u0398(1/d))-approximation.\nBicriteria Algorithm: Our algorithm for small k gives an O(1)-approximation in poly(n) time for constant k, but the approximation factor depends on k. We show how one can find a rank2k matrix A\u0302 for which \u2016A \u2212 A\u0302\u20161 \u2264 C \u00b7 OPT, where C is an absolute constant, and OPT = minrank-k matrices A\u2032 \u2016A\u2212A\u2032\u20161. We first find a rank-k matrix B1 for which \u2016A\u2212B1\u20161 \u2264 p \u00b7OPT for a factor 1 \u2264 p \u2264 poly(n). We can use any of our algorithms above for this.\nNext consider the problem minV \u2208Rk\u00d7d \u2016U\u2217V \u2212 (A\u2212B1)\u20161, and let U\u2217V \u2217 be a best `1-low rank approximation to A\u2212 B1; we later explain why we look at this problem. We can assume V \u2217 is an `1 well-conditioned basis [Cla05, DDH+09], since we can replace U\u2217 with U\u2217R\u22121 and V \u2217 with RV \u2217 for any invertible linear transformation R. For any vector x we then have \u2016x\u20161f \u2264 \u2016x >V \u2217\u20161 \u2264 e\u2016x\u20161, where 1 \u2264 e, f \u2264 poly(k). This implies all entries of U\u2217 are at most 2f\u2016A \u2212 B\u20161, as otherwise one could replace U\u2217 with 0n\u00d7k and reduce the cost. Also, any entry of U\u2217 smaller than \u2016A\u2212B\u20161100enkp can be replaced with 0 as this incurs additive error OPT100 . If we round the entries of U\n\u2217 to integer multiples of \u2016A\u2212B\u20161100enkp , then we only have O(enkpf) possibilities for each entry of U\n\u2217, and still obtain an O(1)-approximation. We refer to the rounded U\u2217 as U\u2217, abusing notation.\nLet D be a sampling and rescaling matrix with O(k log k) non-zero diagonal entries, corresponding to sampling by the Lewis weights of U\u2217. We do not know D, but handle this below. By the triangle inequality, for any V ,\n\u2016D(U\u2217V \u2212 (A\u2212B1))\u20161 = \u2016D(U\u2217V \u2212 U\u2217V \u2217)\u20161 \u00b1 \u2016D(U\u2217V \u2217 \u2212 (A\u2212B1))\u20161 = \u0398(1)\u2016U\u2217V \u2212 U\u2217V \u2217\u20161 \u00b1O(1)\u2016U\u2217V \u2217 \u2212 (A\u2212B1)\u20161,\nwhere the Lewis weights give \u2016D(U\u2217V \u2212U\u2217V \u2217)\u20161 = \u0398(1)\u2016U\u2217V \u2212U\u2217V \u2217\u20161 and a Markov bound gives \u2016D(U\u2217V \u2217 \u2212 (A \u2212 B1))\u20161 = O(1)\u2016U\u2217V \u2217 \u2212 (A \u2212 B1)\u20161. Thus, minimizing \u2016DU\u2217V \u2212D(A \u2212 B1)\u20161 gives a fixed constant factor approximation to the problem minV \u2208Rk\u00d7d \u2016U\u2217V \u2212 (A \u2212 B1)\u20161. The non-zero diagonal entries of D can be assumed to be integers between 1 and n2.\nWe guess the entries ofDU\u2217 and note for each entry there are only O(enkpf log(n2)) possibilities. One of our guesses corresponds to Lewis weight sampling by U\u2217. We solve for V and by the guarantees of Lewis weights, the row span of this V provides an O(1)-approximation. We can find the corresponding U via linear programming. As mentioned above, we do not know D, but can enumerate over all D and all possible DU\u2217. The total time is npoly(k).\nAfter finding U , which has k columns, we output the rank-2k space formed by the column span of [U,B1]. By including the column span of B1, we ensure our original transformation of the problem minV \u2208Rk\u00d7d \u2016U\u2217 \u00b7V \u2212A\u20161 to the problem minV \u2208Rk\u00d7d \u2016U\u2217 \u00b7V \u2212 (A\u2212B1)\u20161 is valid, since we can first use the column span of B1 to replace A with A \u2212 B1. Replacing A with A \u2212 B1 ultimately results in a rank-2k output. Had we used A instead of A \u2212 B1 our output would have been rank k but would have additive error \u2016A\u20161poly(k/ ) . If we assume the entries of A are in {\u2212b,\u2212b+ 1, . . . , b}, then we can lower bound the cost \u2016U\u2217V \u2212A\u20161, given that it is non-zero, by (ndb)\u2212O(k) (if it is zero then we output A) using Lemma 4.1 in [CW09] and relating entrywise `1-norm to Frobenius norm. We can go through the same arguments above with A\u2212B replaced by A and our running time will now be (ndb)poly(k).\nHard Instances for Cauchy Matrices and More General Sketches: We consider a d \u00d7 d matrix A = Id+(log d)e>1 e, where e1 = (1, 0, . . . , 0) and e = (1, 1, . . . , 1) and Id is the d\u00d7d identity. For an O(k log k)\u00d7d matrix S of i.i.d. Cauchy random variables, SA = S+ (log d)S>1 e, where S1 is the first column of S. For a typical column of SA, all entries are at most poly(k) log d in magnitude. Thus, in order to approximate the first row of A, which is (log d)e, by x>SA for an x \u2208 Rk log k, we\nneed \u2016x\u20161 \u2265 1poly(k) . Also \u2016x >S\u20161 = \u2126(\u2016x\u20161d log d) with 1 \u2212 exp(\u2212k log k) probability, for d large enough, so by a net argument \u2016x\u20161 \u2264 poly(k) for all x. However, there are entries of SA that are very large, i.e., about one which is r = \u0398(dk log k) in magnitude, and in general about 2i entries about r2\u2212i in magnitude. These entries typically occur in columns Cj of SA for which all other entries in the column are bounded by poly(k) in magnitude. Thus, |x>Cj | \u2248 r2\u2212i for about 2i columns j. For each such column, if r2\u2212i log d, then we incur cost r2 \u2212i\npoly(k) in approximating the first row of A. In total the cost is r log r poly(k) = d log d poly(k) , but the optimal\ncost is at most d, giving a log dpoly(k) lower bound. We optimize this to a log d k log2 k lower bound.\nWhen k is large this bound deteriorates, but we also show a k1/2\u2212\u03b3 lower bound for arbitrarily small constant \u03b3 > 0. This bound applies to any oblivious sketching matrix. The idea is similar to our row subset selection lower bound. Let A be as in our row subset selection lower bound, consider SA, and write S = U\u03a3V > in its full SVD. Then SA is in the row span of the top O(k log k) rows of V >A, since \u03a3 only has O(k log k) non-zero singular values. Since the first k columns of A are rotationally invariant, V >A has first k columns i.i.d. Gaussian and remaining columns equal to V >. Call the first O(k log k) rows of V >A the matrix B. We now try to approximate a row of A by a vector in the row span of B. There are two issues that make this setting different from row subset selection: (1) B no longer contains an identity submatrix, and (2) the rows of B depend on the rows of A. We handle the first issue by building nets for subsets of coordinates of x>V > rather than x as before; since \u2016x>V >\u20162 = \u2016x\u20162 similar arguments can be applied. We handle the second issue by observing that if the number of rows of B is considerably smaller than that of A, then the distribution of B had we replaced a random row of A with zeros would be statistically close to i.i.d. Gaussian. Hence, typical rows of A can be regarded as being independent of B.\nLimited Independence, Distributed, and Streaming Algorithms: We show for an n \u00d7 d matrix A, if we left-multiply by an O(k log k)\u00d7n matrix S in which each row is an independent vector of O\u0303(d)-wise independent Cauchy random variables, SA contains a poly(k) log d-approximation in its span. This allows players in a distributed model to share a common S by exchanging O\u0303(kd) bits, independent of n. We use Lemma 2.2 of [KNW10] which shows for a reasonably smooth approximation f to an indicator function, E[f(X)] = E[f(Y )] + O( ), where X = \u2211 i aiXi, Y = \u2211 i aiYi, a \u2208 Rn is fixed, X is a vector of i.i.d. Cauchy random variables, and Y is a vector of O\u0303(1/ )-wise independent random variables.\nTo show the row span of SA contains a good rank-k approximation, we argue \u2016Sy\u20161 = \u2126(\u2016y\u20161) for a fixed y \u2208 Rn with 1\u2212exp(\u2212k log k) probability. We apply the above lemma with = \u0398(1). We also need for an n\u00d7d matrix A with unit-`1 columns, that \u2016SA\u20161 = O\u0303(kd). We fool the expectation of a truncated Cauchy by taking a weighted sum of O(log(dk)) indicator functions and applying the above lemma with = \u0398(1/d). An issue is there are \u0398\u0303(kd) Cauchy random variables corresponding to the entries of SA, some of which can be as large as \u0398\u0303(kd), so to fool their expectation (after truncation) we need = \u0398\u0303(1/(dk)), resulting in O\u0303(dk2) seed length and ruining our optimal O\u0303(dk) communication. We show we can instead pay a factor of k in our approximation and maintain O\u0303(dk)-wise independence. The distributed and streaming algorithms, given this, follow algorithms for Frobenius norm low rank approximation in [KVW14, BWZ16].\nHardness Assuming Exponential Time Hypothesis: By inspecting the proof of NP-hardness of [GV15], it at best gives a (1 + 1n\u03b3 )-inapproximability for an arbitrarily small constant \u03b3 > 0. We considerably strengthen this to (1 + 1\nlog1+\u03b3 n )-inapproximability by taking a modified version of the\nn \u00d7 n hard instance of [GV15] and planting it in a 2o(n) \u00d7 2o(n) matrix padded with tiny values.\nUnder the ETH, the maximum cut problem that [GV15] and that we rely on cannot be solved in 2o(n) time, so our transformation is efficient. Although we use the maximum cut problem as in [GV15] for our n\u00d7 n hard instance, in order to achieve our inapproximability we need to use that under the ETH this problem is hard to approximate even if the input graph is sparse and even up to a constant factor; such additional conditions were not needed in [GV15].\n`p-Low Rank Approximation and EMD-Low Rank Approximation: Our algorithms for entrywise `p-Norm Error are similar to our algorithms for `1. We use p-stable random variables in place of Cauchy random variables, and note that the p-th power of a p-stable random variable has similar tails to that of a Cauchy, so many of the same arguments apply. Our algorithm for EMD low rank approximation immediately follows by embedding EMD into `1.\nCounterexamples to Heuristics: Let A = diag(n2+\u03b3 , n1.5+ , B,B) \u2208 R(2n+2)\u00d7(2n+2) where \u2208 (0, .5), \u03b3 > 0, and B is the n \u00d7 n all 1s matrix. For this A we show the four heuristic algorithms [KK05, DZHZ06, Kwa08, BDB13] cannot achieve an nmin(\u03b3,0.5\u2212 ) approximation ratio when the rank parameter k = 3."}, {"heading": "1.3 Several Theorem Statements, an Algorithm, and a Roadmap", "text": "Algorithm 1 Main Meta-Algorithm 1: procedure L1LowRankApprox(A,n, d, k) . Theorem 1.2 2: Choose sketching matrix S (a Cauchy matrix or a sparse Cauchy matrix.) 3: Compute SA, form C by Ci \u2190 arg minx \u2016xSA\u2212Ai\u20161. Form B = C \u00b7 SA. 4: Choose sketching matrices T1, R,D, T2 (Cauchy matrices or sparse Cauchy matrices.) 5: Solve minX,Y \u2016T1BRXYDBT2 \u2212 T1BT2\u2016F . 6: return BRX,Y DB. 7: end procedure\nTheorem 1.2 (Informal Version of Theorem C.6). Given A \u2208 Rn\u00d7d, there is an algorithm which in nnz(A) + (n + d) \u00b7 poly(k) time, outputs a (factorization of a) rank-k matrix A\u2032 such that with probability 9/10, \u2016A\u2032 \u2212A\u20161 \u2264 (log d) poly(k) min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161.\nTheorem 1.3 (Informal Version of Theorem C.7). Given A \u2208 Rn\u00d7d, there is an algorithm that takes poly(n)dO\u0303(k)2O\u0303(k2) time and outputs a rank-k matrix A\u2032 such that, with probability 9/10, \u2016A\u2032 \u2212A\u20161 \u2264 O\u0303(k) min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161. In addition, A\u2032 is a CUR decomposition.\nTheorem 1.4 (Informal Version of Theorem G.28). For any k \u2265 1, and any constant c \u2265 1, let n = kc. There exists a matrix A such that for any matrix A\u2032 in the span of n/2 rows of A, \u2016A\u2032 \u2212A\u20161 = \u2126(k0.5\u2212\u03b1) min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161, where \u03b1 > 0 is an arbitrarily small constant.\nRoad map Section A introduces some notation and definitions. Section B includes several useful tools. We provide several `1-low rank approximation algorithms in Section C. Section D contains the no contraction and no dilation analysis for our main algorithm. The results for `p and earth mover distance are presented in Section E and F. We provide our existential hardness results for Cauchy matrices, row subset selection and oblivious subspace embeddings in Section G. We provide our computational hardness results in Section H. We analyze limited independent random Cauchy variables in Section I. Section K presents the results for the distributed setting. Section J presents the results for the streaming setting. Section L contains the experimental results of our algorithm and several heuristic algorithms, as well as counterexamples to heuristic algorithms."}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 1", "text": "1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Technical Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Several Theorem Statements, an Algorithm, and a Roadmap . . . . . . . . . . . . . . 10"}, {"heading": "A Notation 22", "text": ""}, {"heading": "B Preliminaries 22", "text": "B.1 Polynomial system verifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.2 Cauchy and p-stable transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.3 Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.4 Frobenious norm and `2 relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.5 Converting entry-wise `1 and `p objective functions into polynomials . . . . . . . . . 25 B.6 Converting entry-wise `1 objective function into a linear program . . . . . . . . . . . 25\nC `1-Low Rank Approximation 26 C.1 Existence results via dense Cauchy transforms, sparse Cauchy transforms, Lewis weights 26 C.2 Input sparsity time, poly(k, log n, log d)-approximation for an arbitrary matrix A . . 28 C.3 poly(k, log d)-approximation for an arbitrary matrix A . . . . . . . . . . . . . . . . . 31 C.4 O\u0303(k)-approximation for an arbitrary matrix A . . . . . . . . . . . . . . . . . . . . . . 32 C.5 Rank-2k and O(1)-approximation algorithm for an arbitrary matrix A . . . . . . . . 35 C.6 CUR decomposition for an arbitrary matrix A . . . . . . . . . . . . . . . . . . . . . . 37 C.7 Rank-r matrix B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nC.7.1 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 C.7.2 poly(k, r)-approximation for rank-r matrix B . . . . . . . . . . . . . . . . . . 41"}, {"heading": "D Contraction and Dilation Bound for `1 44", "text": "D.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 D.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 D.3 Cauchy embeddings, no dilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 D.4 Cauchy embeddings, no contraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 D.5 Cauchy embeddings, k-dimensional subspace . . . . . . . . . . . . . . . . . . . . . . . 51 D.6 Sparse Cauchy transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 D.7 `1-Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nE `p-Low Rank Approximation 57 E.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 E.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 E.3 Tools and inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 E.4 Dense p-stable transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 E.5 Sparse p-stable transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 E.6 `p-Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\nF EMD-Low Rank Approximation 66 F.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 F.2 Analysis of no contraction and no dilation bound . . . . . . . . . . . . . . . . . . . . 67"}, {"heading": "G Hardness results for Cauchy matrices, row subset selection, OSE 68", "text": "G.1 Hard instance for Cauchy matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 G.2 Hard instance for row subset selection . . . . . . . . . . . . . . . . . . . . . . . . . . 72 G.3 Hard instance for oblivious subspace embedding and more row subset selection . . . 77\nG.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 G.3.2 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79"}, {"heading": "H Hardness 91", "text": "H.1 Previous results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 H.2 Extension to multiplicative error `1-low rank approximation . . . . . . . . . . . . . . 92 H.3 Usin the ETH assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 H.4 Extension to the rank-k case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97"}, {"heading": "I Limited independent Cauchy random variables 101", "text": "I.1 Notations and Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 I.2 Analysis of limited independent random Cauchy variables . . . . . . . . . . . . . . . 101"}, {"heading": "J Streaming Setting 104", "text": "J.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 J.2 Turnstile model, poly(k, log(d), log(n)) approximation . . . . . . . . . . . . . . . . . 105 J.3 Row-update model, poly(k) log d approximation . . . . . . . . . . . . . . . . . . . . . 106"}, {"heading": "K Distributed Setting 108", "text": "K.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 K.2 Arbitrary-partition model, subspace, poly(k, log(d), log(n)) approximation . . . . . . 110 K.3 Arbitrary-partition model, decomposition, poly(k, log(d), log(n)) approximation . . . 112 K.4 Row-partition model, subspace, poly(k) log d approximation . . . . . . . . . . . . . . 112 K.5 Row-partition model, decomposition, poly(k) log d approximation . . . . . . . . . . . 114"}, {"heading": "L Experiments and Discussions 115", "text": "L.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 L.2 Counterexample for [DZHZ06] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 L.3 Counterexample for [BDB13] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 L.4 Counterexample for [Kwa08] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 L.5 Counterexample for [KK05] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 L.6 Counterexample for all . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 L.7 Discussion for Robust PCA [CLMW11] . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nM Acknowledgments 122"}, {"heading": "A Notation", "text": "Let N+ denote the set of positive integers. For any n \u2208 N+, let [n] denote the set {1, 2, \u00b7 \u00b7 \u00b7 , n}. For any p \u2208 [1, 2], the `p-norm of a vector x \u2208 Rd is defined as\n\u2016x\u2016p = ( d\u2211 i=1 |xi|p )1/p .\nFor any p \u2208 [1, 2), the `p-norm of a matrix A \u2208 Rn\u00d7d is defined as\n\u2016A\u2016p = ( n\u2211 i=1 d\u2211 j=1 |Aij |p )1/p .\nLet \u2016A\u2016F denote the Frobenius norm of matrix A. Let nnz(A) denote the number of nonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let A> denote the transpose of A. Let A\u2020 denote the Moore-Penrose pseudoinverse of A. Let A\u22121 denote the inverse of a full rank square matrix. We use Aj to denote the jth column of A, and Ai to denote the ith row of A. For an n\u00d7 d matrix A, for S a subset of [n] and T a subset of [d], we let AS denote the |S| \u00d7 d submatrix of A with rows indexed by S, while AT denotes the n \u00d7 |T | submatrix of A with columns indexed by T , and AST denote the |S| \u00d7 |T | submatrix A with rows in S and columns in T .\nFor any function f , we define O\u0303(f) to be f \u00b7 logO(1)(f). In addition to O(\u00b7) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f \u2264 Cg (resp. \u2265) for an absolute constant C. We use f h g to mean cf \u2264 g \u2264 Cf for constants c, C. We use OPT to denote minrank\u2212k Ak \u2016Ak \u2212A\u20161, unless otherwise specified."}, {"heading": "B Preliminaries", "text": ""}, {"heading": "B.1 Polynomial system verifier", "text": "Renegar [Ren92a, Ren92b] and Basu et al. [BPR96] independently provided an algorithm for the decision problem for the existential theory of the reals, which is to decide the truth or falsity of a sentence (x1, \u00b7 \u00b7 \u00b7 , xv)F (f1, \u00b7 \u00b7 \u00b7 , fm) where F is a quantifier-free Boolean formula with atoms of the form sign(fi) = \u03c3 with \u03c3 \u2208 {0, 1,\u22121}. Note that this problem is equivalent to deciding if a given semi-algebraic set is empty or not. Here we formally state that theorem. For a full discussion of algorithms in real algebraic geometry, we refer the reader to [BPR05] and [Bas14].\nTheorem B.1 (Decision Problem [Ren92a, Ren92b, BPR96]). Given a real polynomial system P (x1, x2, \u00b7 \u00b7 \u00b7 , xv) having v variables and m polynomial constraints fi(x1, x2, \u00b7 \u00b7 \u00b7 , xv)\u2206i0, \u2200i \u2208 [m], where \u2206i is any of the \u201cstandard relations\u201d: {>,\u2265,=, 6=,\u2264, <}, let d denote the maximum degree of all the polynomial constraints and let H denote the maximum bitsize of the coefficients of all the polynomial constraints. Then in\n(md)O(v) poly(H),\ntime one can determine if there exists a solution to the polynomial system P .\nRecently, this technique has been used to solve a number of low-rank approximation and matrix factorization problems [AGKM12, Moi13, CW15a, BDL16, RSW16].\nB.2 Cauchy and p-stable transform\nDefinition B.2 (Dense Cauchy transform). Let S = \u03c3 \u00b7 C \u2208 Rm\u00d7n where \u03c3 is a scalar, and each entry of C \u2208 Rm\u00d7n is chosen independently from the standard Cauchy distribution. For any matrix A \u2208 Rn\u00d7d, SA can be computed in O(m \u00b7 nnz(A)) time.\nDefinition B.3 (Sparse Cauchy transform). Let \u03a0 = \u03c3 \u00b7SC \u2208 Rm\u00d7n, where \u03c3 is a scalar, S \u2208 Rm\u00d7n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C \u2208 Rn\u00d7n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. For any matrix A \u2208 Rn\u00d7d, \u03a0A can be computed in O(nnz(A)) time.\nDefinition B.4 (Dense p-stable transform). Let p \u2208 (1, 2). Let S = \u03c3 \u00b7 C \u2208 Rm\u00d7n where \u03c3 is a scalar, and each entry of C \u2208 Rm\u00d7n is chosen independently from the standard p-stable distribution. For any matrix A \u2208 Rn\u00d7d, SA can be computed in O(m nnz(A)) time.\nDefinition B.5 (Sparse p-stable transform). Let p \u2208 (1, 2). Let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where \u03c3 is a scalar, S \u2208 Rm\u00d7n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C \u2208 Rn\u00d7n is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. For any matrix A \u2208 Rn\u00d7d, \u03a0A can be computed in O(nnz(A)) time."}, {"heading": "B.3 Lewis weights", "text": "We follow the exposition of Lewis weights from [CP15].\nDefinition B.6. For a matrix A, let ai denote ith row of A, and ai(= (Ai)>) is a column vector. The statistical leverage score of a row ai is\n\u03c4i(A) def = a>i (A >A)\u22121ai = \u2016(A>A)\u22121/2ai\u201622.\nFor a matrix A and norm p, the `p Lewis weights w are the unique weights such that for each row i we have\nwi = \u03c4i(W 1/2\u22121/pA).\nor equivalently\na>i (A >W 1\u22122/pA)\u22121ai = w 2/p i .\nLemma B.7 (Lemma 2.4 of [CP15] and Lemma 7 of [CLM+15]). Given a matrix A \u2208 Rn\u00d7d, n \u2265 d, for any constant C > 0, 4 > p \u2265 1, there is an algorithm which can compute C-approximate `p Lewis weights for every row i of A in O((nnz(A) + d\u03c9 log d) log n) time, where \u03c9 < 2.373 is the matrix multiplication exponent[Str69, CW87, Wil12].\nLemma B.8 (Theorem 7.1 of [CP15]). Given matrix A \u2208 Rn\u00d7d (n \u2265 d) with `p (4 > p \u2265 1) Lewis weights w, for any set of sampling probabilities pi, \u2211 i pi = N ,\npi \u2265 f(d, p)wi,\nif S \u2208 RN\u00d7n has each row chosen independently as the ith standard basis vector, times 1/p1/pi , with probability pi/N , then with probability at least 0.999,\n\u2200x \u2208 Rd, 1 2 \u2016Ax\u20161 \u2264 \u2016SAx\u20161 \u2264 2\u2016Ax\u20161\nFurthermore, if p = 1, N = O(d log d). If 1 < p < 2, N = O(d log d log log d). If 2 \u2264 p < 4, N = O(dp/2 log d)."}, {"heading": "B.4 Frobenious norm and `2 relaxation", "text": "Theorem B.9 (Generalized rank-constrained matrix approximations, Theorem 2 in [FT07]). Given matrices A \u2208 Rn\u00d7d, B \u2208 Rn\u00d7p, and C \u2208 Rq\u00d7d, let the SVD of B be B = UB\u03a3BV >B and the SVD of C be C = UC\u03a3CV >C . Then,\nB\u2020(UBU > BAVCC > C )kC \u2020 = arg min rank\u2212k X\u2208Rp\u00d7q \u2016A\u2212BXC\u2016F ,\nwhere (UBU>BAVCV > C )k \u2208 Rp\u00d7q is of rank at most k and denotes the best rank-k approximation to UBU > BAVCV > C \u2208 Rp\u00d7d in Frobenious norm.\nClaim B.10 (`2 relaxation of `p-regression). Let p \u2208 [1, 2). For any A \u2208 Rn\u00d7d and b \u2208 Rn, define x\u2217 = arg min\nx\u2208Rd \u2016Ax\u2212 b\u2016p and x\u2032 = arg min x\u2208Rd \u2016Ax\u2212 b\u20162. Then,\n\u2016Ax\u2217 \u2212 b\u2016p \u2264 \u2016Ax\u2032 \u2212 b\u2016p \u2264 n1/p\u22121/2 \u00b7 \u2016Ax\u2217 \u2212 b\u2016p.\nProof. The lower bound trivially holds by definition; we will focus on proving the upper bound. Because Ax\u2212 b is an n-dimensional vector, \u2200x,\n1\nn1/p\u22121/2 \u2016Ax\u2212 b\u2016p \u2264 \u2016Ax\u2212 b\u20162 \u2264 \u2016Ax\u2212 b\u2016p. (3)\nThen,\n\u2016Ax\u2032 \u2212 b\u2016p \u2264 \u221a n\u2016Ax\u2032 \u2212 b\u20162 by LHS of Equation (3) \u2264 \u221a n\u2016Ax\u2217 \u2212 b\u20162 by x\u2032 = arg min\nx \u2016Ax\u2212 b\u20162\n\u2264 \u221a n\u2016Ax\u2217 \u2212 b\u2016p by RHS of Equation (3)\nThis completes the proof.\nClaim B.11 (Frobenius norm relaxation of `p-low rank approximation). Let p \u2208 [1, 2) and for any matrix A \u2208 Rn\u00d7d, define A\u2217 = arg min\nrank\u2212k B\u2208Rn\u00d7d \u2016B \u2212A\u2016p and A\u2032 = arg min rank\u2212k B\u2208Rn\u00d7d \u2016B \u2212A\u2016F . Then\n\u2016A\u2217 \u2212A\u2016p \u2264 \u2016A\u2032 \u2212A\u2016p \u2264 (nd)1/p\u22121/2\u2016A\u2217 \u2212A\u2016p. (4)\nProof. The lower bound of \u2016A\u2032 \u2212 A\u2016p trivially holds by definition. We show an upper bound of \u2016A\u2032 \u2212A\u2016p in the rest of the proof. For any A\u2032 \u2212A \u2208 Rn\u00d7d, we have\n1\n(nd)1/p\u22121/2 \u2016A\u2032 \u2212A\u2016p \u2264 \u2016A\u2032 \u2212A\u2016F \u2264 \u2016A\u2032 \u2212A\u2016p. (5)\nThen,\n\u2016A\u2032 \u2212A\u2016p \u2264 (nd)1/p\u22121/2\u2016A\u2032 \u2212A\u2016F by LHS of Equation (5) \u2264 (nd)1/p\u22121/2\u2016A\u2217 \u2212A\u2016F by A\u2032 = arg min\nrank\u2212k B \u2016B \u2212A\u2016p\n\u2264 (nd)1/p\u22121/2\u2016A\u2217 \u2212A\u2016p. by RHS of Equation (5)\nB.5 Converting entry-wise `1 and `p objective functions into polynomials\nClaim B.12 (Converting absolute value constraints into variables). Givenm polynomials f1(x), f2(x), \u00b7 \u00b7 \u00b7 , fm(x) where x \u2208 Rv, solving the problem\nmin x\u2208Rv m\u2211 i=1 |fi(x)|, (6)\nis equivalent to solving another minimization problem with O(m) extra constraints and m extra variables,\nmin x\u2208Rv ,\u03c3\u2208Rm m\u2211 i=1 \u03c3ifi(x)\ns.t. \u03c32i = 1,\u2200i \u2208 [m] fi(x)\u03c3i \u2265 0, \u2200i \u2208 [m].\nClaim B.13. (Handling `p) Given m polynomials f1(x), f2(x), \u00b7 \u00b7 \u00b7 , fm(x) where x \u2208 Rv and p = a/b for positive integers a and b, solving the problem\nmin x\u2208Rv m\u2211 i=1 |fi(x)|p, (7)\nis equivalent to solving another minimization problem with O(m) extra constraints and O(m) extra variables,\nmin x\u2208Rv ,\u03c3\u2208Rm m\u2211 i=1 yi\ns.t. \u03c32i = 1,\u2200i \u2208 [m] fi(x)\u03c3i \u2265 0, \u2200i \u2208 [m] (\u03c3ifi(x))\na = ybi , \u2200i \u2208 [m] yi \u2265 0, \u2200i \u2208 [m].\nB.6 Converting entry-wise `1 objective function into a linear program\nClaim B.14. Given any matrix A \u2208 Rn\u00d7d and matrix B \u2208 Rk\u00d7d, the problem minU\u2208Rn\u00d7k \u2016UB\u2212A\u20161 can be solved by solving the following linear program,\nmin U\u2208Rn\u00d7k,x\u2208Rn\u00d7d n\u2211 i=1 m\u2211 j=1 xi,j\nUiB j \u2212Ai,j \u2264 xi,j , \u2200i \u2208 [n], j \u2208 [d]\nUiB j \u2212Ai,j \u2265 \u2212xi,j ,\u2200i \u2208 [n], j \u2208 [d]\nxi,j \u2265 0, \u2200i \u2208 [n], j \u2208 [d],\nwhere the number of constraints is O(nd) and the number of variables is O(nd).\nC `1-Low Rank Approximation\nThis section presents our main `1-low rank approximation algorithms. Section C.1 provides our three existence results. Section C.2 shows an input sparsity algorithm with poly(k) log2 d log napproximation ratio. Section C.3 improves the approximation ratio to poly(k) log d. Section C.4 explains how to obtain O\u0303(k) approximation ratio. Section C.5 improves the approximation ratio to O(1) by outputting a rank-2k solution. Section C.6 presents our algorithm for CUR decomposition. Section C.7 includes some useful properties. Our `1-low rank approximation algorithm for a rank-r (where k \u2264 r \u2264 (n, d) ) matrix is used as a black box (by setting r = poly(k)) in several other algorithms.\nC.1 Existence results via dense Cauchy transforms, sparse Cauchy transforms, Lewis weights\nThe goal of this section is to present the existence results in Corollary C.2. We first provide some bicriteria algorithms in Theorem C.1 which can be viewed as a \u201cwarmup\u201d. Then the proof of our bicriteria algorithm actually implies the existence results.\nTheorem C.1. Given matrix A \u2208 Rn\u00d7d, for any k \u2265 1, there exist bicriteria algorithms with running time T (specified below), which output two matrices U \u2208 Rn\u00d7m, V \u2208 Rm\u00d7d such that, with probability 9/10,\n\u2016UV \u2212A\u20161 \u2264 \u03b1 min rank\u2212k Ak \u2016Ak \u2212A\u20161.\n(I). Using a dense Cauchy transform, T = poly(n, d, k), m = O(k log k), \u03b1 = O( \u221a k log k log d).\n(II). Using a sparse Cauchy transform, T = poly(n, d, k),m = O(k5 log5 k), \u03b1 = O(k4.5 log4.5 k log d).\n(III). Sampling by Lewis weights, T = (nd)O\u0303(k), m = O(k log k), \u03b1 = O( \u221a k log k).\nThe matrices in (I), (II), (III) here, are the same as those in (I), (II), (III), (IV) of Lemma D.11. Thus, they have the properties shown in Section D.2.\nProof. We define\nOPT := min rank\u2212k Ak\n\u2016Ak \u2212A\u20161.\nWe define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be the optimal solution such that \u2016U\u2217V \u2217 \u2212A\u20161 = OPT . Part (I). Apply the dense Cauchy transform S \u2208 Rm\u00d7n with m = O(k log k) rows, and \u03b2 = O(log d). Part (II). Apply the sparse Cauchy transform S(= \u03a0 \u2208 Rm\u00d7n) with m = O(k5 log5 k) rows, and \u03b2 = O(\u03c3 log d) = O(k2 log2 k log d). Part (III). Use S (= D \u2208 Rn\u00d7k) to denote an n \u00d7 n matrix which is a sampling and rescaling diagonal matrix according to the Lewis weights of matrix U\u2217. It has m = O(k log k) rows, and \u03b2 = O(1). Sometimes we abuse notation, and should regard D as a matrix which has size m \u00d7 n, where m = O(k log k).\nWe can just replace M in Lemma D.11 with U\u2217V \u2217\u2212A, replace U in Lemma D.11 with U\u2217, and replace c1c2 with O(\u03b2). So, we can apply Lemma D.11 for S. Then we can plug it in Lemma D.8,\nwe have: with constant probability, for any c \u2265 1, for any V \u2032 \u2208 Rk\u00d7d which satisfies\n\u2016SU\u2217V \u2032 \u2212 SA\u20161 \u2264 c \u00b7 min V \u2208Rk\u00d7d \u2016SU\u2217V \u2212 SA\u20161, (8)\nit has\n\u2016U\u2217V \u2032 \u2212A\u20161 \u2264 c \u00b7O(\u03b2)\u2016U\u2217V \u2217 \u2212A\u20161. (9)\nDefine V\u0302i = arg min Vi\u2208Rk\n\u2016SU\u2217Vi \u2212 SAi\u20162 for each i \u2208 [d]. By using Claim B.10 with n = m and\nd = k, it shows\n\u2016SU\u2217V\u0302 \u2212 SA\u20161 = d\u2211 i=1 \u2016SU\u2217V\u0302i \u2212 SAi\u20161 \u2264 d\u2211 i=1 \u221a m\u2016SU\u2217V\u0303i \u2212 SAi\u20161 = \u221a m min V \u2208Rk\u00d7d \u2016SU\u2217V \u2212 SA\u20161.\nwhich means V\u0302 is a \u221a m-approximation solution to problem, min\nV \u2208Rk\u00d7d \u2016SU\u2217V \u2212 SA\u20161.\nNow, let us look into Equation (8) and Equation (9), we can obtain that\n\u2016U\u2217V\u0302 \u2212A\u20161 \u2264 \u221a mO(\u03b2) OPT .\nBecause V\u0302i is the optimal solution of the `2 regression problem, we have\nV\u0302i = (SU \u2217)\u2020SAi \u2208 Rk,\u2200i \u2208 [d], which means V\u0302 = (SU\u2217)\u2020SA \u2208 Rk\u00d7d.\nPlugging V\u0302 into original problem, we obtain\n\u2016U\u2217(SU\u2217)\u2020 \u00b7 SA\u2212A\u20161 \u2264 \u221a mO(\u03b2) OPT .\nIt means\nmin rank\u2212k X\u2208Rn\u00d7m\n\u2016XSA\u2212A\u20161 \u2264 \u221a mO(\u03b2) OPT . (10)\nIf we ignore the constraint on the rank of X, we can get a bicriteria solution: For part (I), notice that X is an n \u00d7m matrix which can be found by using a linear program, because matrices SA \u2208 Rm\u00d7d and A \u2208 Rn\u00d7d are known. For part (II), notice that X is an n\u00d7m matrix which can be found by using a linear program, because matrices SA \u2208 Rm\u00d7d and A \u2208 Rn\u00d7d are known. For part (III), notice that X is an n\u00d7m matrix which can be found by using a linear program, when the span of rows of DA \u2208 Rm\u00d7d is known. We assume that D is known in all the above discussions. But D is actually unknown. So we need to try all the possible choices of the row span of DA. Since D samples at most m = O(k log k) rows of A, then the total number of choices of selecting m rows from n rows is ( n m ) = nO(k log k). This completes the proof.\nEquation (10) in the proof of our bicriteria solution implies the following result,\nCorollary C.2. Given A \u2208 Rn\u00d7d, there exists a rank-k matrix A\u2032 \u2208 Rn\u00d7d such that A\u2032 \u2208 rowspan(S\u2032A) \u2286 rowspan(A) and \u2016A\u2032 \u2212 A\u20161 \u2264 \u03b1 \u00b7 min\nrank\u2212k Ak \u2016A \u2212 Ak\u20161, where S\u2032 \u2208 Rm\u00d7n is a\nsketching matrix. If S\u2032\n(I). indicates the dense Cauchy transform, then \u03b1 = O( \u221a k log k log d). (II). indicates the sparse Cauchy transform, then \u03b1 = O(k4.5 log4.5 k log d). (III). indicates sampling by Lewis weights, then \u03b1 = O( \u221a k log k).\nProof. Define OPT = min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161.\nProof of (I). Choose S to be a dense Cauchy transform matrix with m rows, then\nmin U\u2208Rn\u00d7k,Z\u2208Rk\u00d7m\n\u2016UZSA\u2212A\u20161 \u2264 O( \u221a m log d) OPT,\nwhere m = O(k log k). Choosing A\u2032 = UZSA completes the proof. Proof of (II). Choose \u03a0 = SD \u2208 Rm\u00d7n where S \u2208 Rm\u00d7n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and where D is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution, then\nmin U\u2208Rn\u00d7k,Z\u2208Rk\u00d7m\n\u2016UZ\u03a0A\u2212A\u20161 \u2264 O( \u221a m\u03c3 log d) OPT,\nwhere m = O(k5 log5 k) and \u03c3 = O(k2 log2 k). Choosing A\u2032 = UZ\u03a0A completes the proof. Proof of (III). Choose D to be the sampling and rescaling matrix corresponding to the Lewis weights of U\u2217, and let it have m = O(k log k) nonzero entries on the diagonal, then\nmin U\u2208Rn\u00d7k,Z\u2208Rk\u00d7m\n\u2016UZDA\u2212A\u20161 \u2264 O( \u221a m) OPT .\nChoosing A\u2032 = UZDA completes the proof.\nC.2 Input sparsity time, poly(k, log n, log d)-approximation for an arbitrary matrix A\nThe algorithm described in this section is actually worse than the algorithm described in the next section. But this algorithm is easy to extend to the distributed and streaming settings (See Section K and Section J).\nAlgorithm 2 Input Sparsity Time Algorithm 1: procedure L1LowRankApproxInputSparsity(A,n, d, k) . Theorem C.3 2: Set s\u2190 r \u2190 t1 \u2190 O\u0303(k5), t2 \u2190 O\u0303(k). 3: Choose sparse Cauchy matrices S \u2208 Rs\u00d7n, R \u2208 Rd\u00d7r, T1 \u2208 Rt1\u00d7n. 4: Choose dense Cauchy matrices T2 \u2208 Rd\u00d7t2 . 5: Compute S \u00b7A, A \u00b7R and T1 \u00b7A \u00b7 T2. 6: Compute XY = arg minX,Y \u2016T1ARXY SAT2 \u2212 T1AT2\u2016F . 7: return ARX,Y SA. 8: end procedure\nTheorem C.3. Given matrix A \u2208 Rn\u00d7d, for any k \u2265 1, there exists an algorithm which takes O(nnz(A)) + (n+ d) \u00b7 poly(k) time and outputs two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d such that\n\u2016UV \u2212A\u20161 \u2264 O(poly(k) log n log2 d) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nProof. Choose a Cauchy matrix S \u2208 Rs\u00d7n (notice that S can be either a dense Cauchy transform matrix or a sparse Cauchy transform matrix). Using Corollary C.2, we have\nmin U\u2208Rn\u00d7k,Z\u2208Rk\u00d7s\n\u2016UZSA\u2212A\u20161 \u2264 \u03b1s OPT,\nwhere \u03b1s is the approximation by using matrix S. If S is a dense Cauchy transform matrix, then due to Part (I) of Corollary C.2, \u03b1s = O( \u221a k log k log d), s = O(k log k), and computing SA takes O(snnz(A)) time. If S is a sparse Cauchy transform matrix, then due to Part II of Corollary C.2, \u03b1s = O\u0303(k\n4.5 log d), s = O\u0303(k5), and computing SA takes nnz(A) time. We define U\u2217, Z\u2217 = arg minU,Z \u2016UZSA\u2212A\u20161. For the fixed Z\u2217 \u2208 Rk\u00d7s, choose a Cauchy matrix R \u2208 Rd\u00d7r (note that R can be either a dense Cauchy transform matrix or a sparse Cauchy transform matrix) and sketch on the right of (UZSA \u2212 A). If R is a dense Cauchy transform matrix, then \u03b1r = O(log n), r = O(k log k), computing AR takes O(r \u00b7 nnz(A)) time. If R is a sparse Cauchy transform matrix, then \u03b1r = O\u0303(k2) log n, r = O\u0303(k5), computing AR takes O(nnz(A)) time.\nDefine a row vector U\u0302 j = AjR((Z\u2217SA)R)\u2020 \u2208 Rk. Then\n\u2200j \u2208 [n], \u2016U\u0302 jZ\u2217SAR\u2212AjR\u20162 = min x\u2208Rk \u2016x>Z\u2217SAR\u2212AjR\u20162.\nRecall that r is the number of columns of R. Due to Claim B.10,\nn\u2211 j=1 \u2016AjR((Z\u2217SA)R)\u2020Z\u2217SAR\u2212AjR\u20161 \u2264 O( \u221a r) n\u2211 j=1 min Uj\u2208Rk \u2016U jZ\u2217SAR\u2212AjR\u20161,\nwhich is equivalent to\n\u2016AR((Z\u2217SA)R)\u2020Z\u2217SAR\u2212AR\u20161 \u2264 O( \u221a r) min\nU\u2208Rn\u00d7k \u2016UZ\u2217SAR\u2212AR\u20161,\nwhere AR is an n\u00d7 r matrix and SA is an s\u00d7 d matrix. Using Lemma D.8, we obtain,\n\u2016AR((Z\u2217SA)R)\u2020Z\u2217SA\u2212A\u20161 \u2264 O( \u221a r\u03b1r) min\nU\u2208Rn\u00d7k \u2016UZ\u2217SA\u2212A\u20161.\nWe define X\u2217 \u2208 Rr\u00d7k, Y \u2217 \u2208 Rk\u00d7s,\nX\u2217, Y \u2217 = arg min X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s \u2016ARXY SA\u2212A\u20161.\nThen,\n\u2016ARX\u2217Y \u2217SA\u2212A\u20161 \u2264 \u2016AR((Z\u2217SA)R)\u2020Z\u2217SA\u2212A\u20161 \u2264 O( \u221a r\u03b1r) min\nU\u2208Rn\u00d7k \u2016UZ\u2217SA\u2212A\u20161\n= O( \u221a r\u03b1r) min\nU\u2208Rn\u00d7k,Z\u2208Rk\u00d7s \u2016UZSA\u2212A\u20161\n\u2264 O( \u221a r\u03b1r\u03b1s) OPT .\nIt means that ARX\u2217, Y \u2217SA gives an O(\u03b1r\u03b1s \u221a r)-approximation to the original problem.\nThus it suffices to use Lemma C.4 to solve\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s\n\u2016ARXY SA\u2212A\u20161,\nby losing an extra poly(k) log d factor in the approximation ratio. By using a sparse Cauchy transform (for the place discussing the two options), combining the approximation ratios and running times all together, we can get poly(k) log(n) log2(d)-approximation ratio with O(nnz(A)) + (n+ d) poly(k) running time. This completes the proof.\nLemma C.4. Given matrices A \u2208 Rn\u00d7d, SA \u2208 Rs\u00d7n,RA \u2208 Rr\u00d7d where S \u2208 Rs\u00d7n, R \u2208 Rd\u00d7r with min(n, d) \u2265 max(r, s). For any 1 \u2264 k \u2264 min(r, s), there exists an algorithm that takes O(nnz(A)) + (n+ d) poly(s, r, k) time to output two matrices X \u2032 \u2208 Rr\u00d7k, Y \u2032 \u2208 Rk\u00d7s such that\n\u2016ARX \u2032 \u00b7 Y \u2032SA\u2212A\u20161 \u2264 poly(r, s) log(d) min X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s \u2016ARXY SA\u2212A\u20161\nholds with probability at least .999.\nProof. Choose sketching matrices T1 \u2208 Rt1\u00d7n to sketch on the left of (ARXY SA \u2212 A) (note that S can be either a dense Cauchy transform matrix or a sparse Cauchy transform matrix). If T1 is a dense Cauchy transform matrix, then t1 = O(r log r), \u03b1t1 = O(log d), and computing T1A takes O(t1 \u00b7 nnz(A)) time. If T1 is a sparse Cauchy transform matrix, then t1 = O\u0303(r5), \u03b1t1 = O\u0303(r2) log d, and computing T1A takes nnz(A) time.\nChoose dense Cauchy matrices T>2 \u2208 Rt2\u00d7d to sketch on the right of T1(ARXY SA \u2212 A) with t2 = O((t1 + s) log(t1 + s)). We get the following minimization problem,\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s\n\u2016T1ARXY SAT2 \u2212 T1AT2\u20161. (11)\nDefine X \u2032, Y \u2032 to be the optimal solution of\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s\n\u2016T1ARXY SAT2 \u2212 T1AT2\u2016F .\nDue to Claim B.11,\n\u2016T1ARX \u2032Y \u2032SAT2 \u2212 T1AT2\u20161 \u2264 \u221a t1t2 min\nX\u2208Rr\u00d7k,Y \u2208Rk\u00d7s \u2016T1ARXY SAT2 \u2212 T1AT2\u20161.\nDue to Lemma D.10\n\u2016ARX \u2032Y \u2032SA\u2212A\u20161 \u2264 \u221a t1t2\u03b1t1 log t1 min\nX\u2208Rr\u00d7k,Y \u2208Rk\u00d7s \u2016ARXY SA\u2212A\u20161.\nIt remains to solve\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s\n\u2016T1ARXY SAT2 \u2212 T1AT2\u2016F .\nBy using Theorem B.9 and choosing T1 to be a sparse Cauchy transform matrix, we have that the optimal rank-k solution X \u2032Y \u2032 is (T1AR)\u2020(UBU>B (T1AT2)VCV > C )k(SAT2) which can be computed in O(nnz(A)) + (n + d) poly(s, r, k) time. Here, UB are the left singular vectors of T1AR. VC are the right singular vectors of SAT2.\nAn alternative way of solving Equation (11) is using a polynomial system verifier. Note that a polynomial system verifier does not allow absolute value constraints. Using Claim B.12, we are able to remove these absolute value constraints by introducing new constraints and variables. Thus, we can get a better approximation ratio but by spending exponential running time in k. In the previous step, we should always use a dense Cauchy transform to optimize the approximation ratio.\nCorollary C.5. Given A \u2208 Rn\u00d7d, there exists an algorithm which takes nd \u00b7poly(k)+(n+d) \u00b72O\u0303(k2) time and outputs two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d such that\n\u2016UV \u2212A\u20161 \u2264 O(poly(k) log n log2 d) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nThe poly(k) factor in the above corollary is much smaller than that in Theorem C.3.\nC.3 poly(k, log d)-approximation for an arbitrary matrix A\nIn this section, we explain how to get an O(log d) \u00b7 poly(k) approximation.\nAlgorithm 3 poly(k) log d-approximation Algorithm\n1: procedure L1LowRankApproxPolykLogd(A,n, d, k) . Theorem C.6 2: Set s\u2190 O\u0303(k5). 3: Choose sparse Cauchy matrices S \u2208 Rs\u00d7n and compute S \u00b7A. 4: Implicitly obtain B = UBVB by finding VB = SA \u2208 Rs\u00d7d and UB \u2208 Rn\u00d7s where \u2200i \u2208 [n],\nrow vector (UB)i gives an O(1) approximation to minx\u2208R1\u00d7s \u2016xSA\u2212Ai\u20161. 5: U, V \u2190L1LowRankApproxB(UB, VB, n, d, k, s). . Theorem C.19 6: return U, V . 7: end procedure\nIntuitively, our algorithm has two stages. In the first stage, we just want to find a low rank matrix B which is a good approximation to A. Then, we can try to find a rank-k approximation to B. Since now B is a low rank matrix, it is much easier to find a rank-k approximation to B. The procedure L1LowRankApproxB(UB, VB, n, d, k, s) corresponds to Theorem C.19.\nTheorem C.6. Given matrix A \u2208 Rn\u00d7d, for any k \u2265 1, there exists an algorithm which takes nnz(A) + (n+ d) \u00b7 poly(k) time to output two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d such that\n\u2016UV \u2212A\u20161 \u2264 poly(k) log d min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nProof. We define\nOPT := min rank\u2212k Ak\n\u2016Ak \u2212A\u20161.\nThe main idea is to replace the given n \u00d7 d matrix A with another low rank matrix B which also has size n \u00d7 d. Choose S \u2208 Rs\u00d7n to be a Cauchy matrix, where s \u2264 poly(k) (note that, if S is a dense Cauchy transform matrix, computing SA takes O(snnz(A)) time, while if S is a sparse Cauchy transform matrix, computing SA takes O(nnz(A)) time). Then B is obtained by taking each row of A and replacing it with its closest point (in `1-distance) in the row span of SA. By using Part II of Corollary C.2, we have,\nmin U\u2208Rn\u00d7k,Z\u2208Rk\u00d7s\n\u2016UZSA\u2212A\u20161 \u2264 O( \u221a spoly(k) log d) OPT .\nWe define B to be the product of two matrices UB \u2208 Rn\u00d7s and VB \u2208 Rs\u00d7d. We define VB to be SA and UB to be such that for any i \u2208 [n], (UB)i gives an O(1)-approximation to problem\nmin x\u2208R1\u00d7s \u2016xSA \u2212 Ai\u20161. This can be solved using an `1-regression solver in nnz(A) + (n + d) poly(s) time.\nBy the definition of B, it is an n\u00d7d matrix. Na\u00efvely we can write down B after finding UB and VB. The time for writing down B is O(nd). To avoid this, we can just keep a factorization UB and VB. We are still able to run algorithm L1LowRankApproxB. Because s = poly(k), the running time of algorithm L1LowRankApproxB is still O(nnz(A)) + (n+ d) poly(k).\nBy the definition of B, we have that B has rank at most s. Suppose we then solve `1-low rank approximation problem for rank-s matrix B, finding a rank-k g-approximation matrix UV . Due to Lemma C.15, we have that if B is an f -approximation solution to A, then UV is also an O(fg)-approximation solution to A,\n\u2016UV \u2212A\u20161 \u2264 O(log d) \u00b7 poly(k) \u00b7 gOPT .\nUsing Theorem C.19 we have that g = poly(s), which completes the proof."}, {"heading": "C.4 O\u0303(k)-approximation for an arbitrary matrix A", "text": "Algorithm 4 O\u0303(k)-approximation Algorithm\n1: procedure L1LowRankApproxK(A,n, d, k) . Theorem C.7 2: r \u2190 O(k log k),m\u2190 t1 \u2190 O(r log r), t2 \u2190 O(m logm). 3: Guess a diagonal matrix R \u2208 Rd\u00d7d with only r 1s. . R selects r columns of A \u2208 Rn\u00d7d. 4: Compute a sampling and rescaling matrix D \u2208 Rn\u00d7n, T1 \u2208 Rn\u00d7n corresponding to the Lewis\nweights of AR, and let them have m, t1 nonzero entries on the diagonals, respectively. 5: Compute a sampling and rescaling matrix T>2 \u2208 Rd\u00d7d according to the Lewis weights of\n(DA)>, and let it have t2 nonzero entries on the diagonal. 6: Solve minX,Y \u2016T1ARXYDAT2 \u2212 T1AT2\u20161. 7: Take the best solution X,Y over all guesses of R. 8: return ARX, Y DA. 9: end procedure\nTheorem C.7. Given matrix A \u2208 Rn\u00d7d, there exists an algorithm that takes poly(n) \u00b7 dO\u0303(k) \u00b7 2O\u0303(k2) time and outputs two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d such that\n|UV \u2212A\u20161 \u2264 O\u0303(k) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nProof. We define\nOPT := min rank\u2212k Ak\n\u2016Ak \u2212A\u20161."}, {"heading": "Let U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d satisfy", "text": "\u2016U\u2217V \u2217 \u2212A\u20161 = OPT .\nLet S> \u2208 Rd\u00d7d denote the sampling and rescaling matrix corresponding to the Lewis weights of (V \u2217)>, where the number of nonzero entries on the diagonal of S is s = r = O(k log k). Let\nR> \u2208 Rd\u00d7d denote a diagonal matrix such that \u2200i \u2208 [d], if Si,i 6= 0, then Ri,i = 1, and if Si,i = 0, then Ri,i = 0. Since rowspan(R>A>) = rowspan(S>A>),\nmin Z\u2208Rm\u00d7k,V \u2208Rk\u00d7d \u2016ASZV \u2212A\u20161 = min Z\u2208Rm\u00d7k,V \u2208Rk\u00d7d \u2016ARZV \u2212A\u20161.\nCombining with Part III of Corollay C.2, there exists a rank-k solution in the column span of AR, which means,\nmin Z\u2208Rm\u00d7k,V \u2208Rk\u00d7d\n\u2016ARZV \u2212A\u20161 \u2264 O( \u221a r) OPT . (12)\nBecause the number of 1s of R is r, and the size of the matrix is d\u00d7d, there are ( d r ) = dO\u0303(k) different choices for locations of 1s on the diagonal of R. We cannot compute R directly, but we can guess all the choices of locations of 1s. Regarding R as selecting r columns of A, then there are dO\u0303(k) choices. There must exist a \u201ccorrect\u201d way of selecting a subset of columns over all all choices. After trying all of them, we will have chosen the right one.\nFor a fixed guess R, we can compute D \u2208 Rn\u00d7n, which is a sampling and rescaling matrix corresponding to the Lewis weights of AR, and let m = O(k log2 k) be the number of nonzero entries on the diagonal of D.\nBy Equation (12), there exists a W \u2208 Rr\u00d7k such that,\nmin V \u2208Rk\u00d7d\n\u2016ARWV \u2212A\u20161 \u2264 O( \u221a r) OPT . (13)\nWe define V\u0302i = arg min Vi\u2208Rk\u00d7d\n\u2016DARWVi\u2212DAi\u20162, \u2200i \u2208 [d], which means V\u0302i = (DARW )\u2020DAi \u2208 Rk. Then\nV\u0302 = (DARW )\u2020DA \u2208 Rk\u00d7d. We define V \u2217 = arg min V \u2208Rk\u00d7d \u2016ARWV \u2212A\u20161. Then, by Claim B.10, it has\n\u2016DARWV\u0302 \u2212DA\u20161 \u2264 O( \u221a m) min\nV \u2208Rk\u00d7d \u2016DARWV \u2212DA\u20161.\nBy applying Lemma D.11, Lemma D.8 and Equation (13), we can show\n\u2016ARWV\u0302 \u2212A\u20161 \u2264 O( \u221a m)\u2016ARWV \u2217 \u2212A\u20161 \u2264 O( \u221a mr) OPT \u2264 O\u0303(k) OPT .\nPlugging V\u0302 = (DARW )\u2020DA into \u2016ARWV\u0302 \u2212A\u20161, we obtain that\n\u2016ARW (DARW )\u2020DA\u2212A\u20161 \u2264 O\u0303(k) OPT .\nand it is clear that,\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7m\n\u2016ARXYDA\u2212A\u20161 \u2264 \u2016ARW (DARW )\u2020DA\u2212A\u20161 \u2264 O\u0303(k) OPT .\nRecall that we guessed R, so it is known. We can compute T1 \u2208 Rn\u00d7n, which is a sampling and rescaling diagonal matrix corresponding to the Lewis weights of AR, and t1 = O(r log r) is the number of nonzero entries on the diagonal of T1.\nAlso, DA is known, and the number of nonzero entries in D ism = O(k log2 k). We can compute T>2 \u2208 Rd\u00d7d, which is a sampling and rescaling matrix corresponding to the Lewis weights of (DA)>, and t2 = O(m logm) is the number of nonzero entries on the diagonal of T2.\nDefine X\u2217, Y \u2217 = arg min X\u2208Rr\u00d7k,Y \u2208Rk\u00d7m \u2016T1(AR)XY (DA)T2 \u2212 T1AT2\u20161. Thus, using Lemma D.10, we\nhave\n\u2016(AR)X\u2217Y \u2217(DA)\u2212A\u20161 \u2264 O\u0303(k) OPT .\nTo find X\u2217, Y \u2217, we need to solve this minimization problem\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7m\n\u2016T1(AR)XY (DA)T2 \u2212 T1AT2\u20161,\nwhich can be solved by a polynomial system verifier (see more discussion in Section B.1 and B.5). In the next paragraphs, we explain how to solve the above problem by using a polynomial system verifier. Notice that T1(AR) is known and (DA)T2 is also known. First, we create r \u00d7 k variables for the matrix X, i.e., one variable for each entry of X. Second, we create k \u00d7 m variables for matrix Y , i.e., one variable for each entry of Y . Putting it all together and creating t1\u00d7 t2 variables \u03c3i,j , \u2200i \u2208 [t1], j \u2208 [t2] for handling the unknown signs, we write down the following optimization poblem\nmin X,Y t1\u2211 i=1 t2\u2211 j=1 \u03c3i,j(T1(AR)XY (DA)T2)i,j\ns.t \u03c32i,j = 1, \u2200i \u2208 [t1], j \u2208 [t2] \u03c3i,j \u00b7 (T1(AR)XY (DA)T2)i,j \u2265 0, \u2200i \u2208 [t1], j \u2208 [t2].\nNotice that the number of constraints is O(t1t2) = O\u0303(k2), the maximum degree is O(1), and the number of variables O(t1t2 + km+ rk) = O\u0303(k2). Thus the running time is,\n(# constraints \u00b7# degree)O(# variables) = 2O\u0303(k2).\nTo use a polynomial system verifier, we need to discuss the bit complexity. Suppose that all entries are multiples of \u03b4, and the maximum is \u2206, i.e., each entry \u2208 {\u2212\u2206, \u00b7 \u00b7 \u00b7 ,\u22122\u03b4,\u2212\u03b4, 0, \u03b4, 2\u03b4, \u00b7 \u00b7 \u00b7 ,\u2206}, and \u2206/\u03b4 = 2poly(nd). Then the running time is O(poly(\u2206/\u03b4)) \u00b7 2O\u0303(k2) = poly(nd)2O\u0303(k2).\nAlso, a polynomial system verifier is able to tell us whether there exists a solution in a semialgebraic set. In order to find the solution, we need to do a binary search over the cost C. In each step of the binary search we use a polynomial system verifier to determine if there exists a solution in,\nt1\u2211 i=1 t2\u2211 j=1 \u03c3i,j(T1(AR)XY (DA)T2)i,j \u2264 C\n\u03c32i,j = 1, \u2200i \u2208 [t1], j \u2208 [t2] \u03c3i,j \u00b7 (T1(AR)XY (DA)T2)i,j \u2265 0, \u2200i \u2208 [t1], j \u2208 [t2].\nIn order to do binary search over the cost, we need to know an upper bound on the cost and also a lower bound on the minimum nonzero cost. The upper bound on the cost is Cmax = O(nd\u2206), and the minimum nonzero cost is Cmin = 2\u2212\u2126(poly(nd)). Thus, the total number of steps for binary search is O(log(Cmax/Cmin)). Overall, the running time is\nndO\u0303(k) \u00b7 2O\u0303(k2) \u00b7 log(\u2206/\u03b4) \u00b7 log(Cmax/Cmin) = poly(n)dO\u0303(k)2O\u0303(k 2).\nThis completes the proof.\nInstead of solving an `1 problem at the last step of L1LowRankApproxK by using a polyonomial system verifier, we can just solve a Frobenious norm minimization problem. This slighly improves the running time and pays an extra poly(k) factor in the approximation ratio. Thus, we obtain the following corollary,\nCorollary C.8. Given matrix A \u2208 Rn\u00d7d, there exists an algorithm that takes poly(n) \u00b7 dO\u0303(k) time which outputs two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d such that\n|UV \u2212A\u20161 \u2264 poly(k) min rank\u2212k Ak \u2016Ak \u2212A\u20161\nholds with probability 9/10.\nC.5 Rank-2k and O(1)-approximation algorithm for an arbitrary matrix A\nIn this section, we show how to output a rank-2k solution that is able to achieve anO(1)-approximation.\nAlgorithm 5 Bicriteria O(1)-approximation Algorithm\n1: procedure L1LowRankApproxBicriteria(A,n, d, k) . Theorem C.9 2: UB, VB \u2190 min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u2016F .\n3: r \u2190 O(k log k). 4: Guess a diagonal matrix D \u2208 Rn\u00d7n with r nonzero entries. 5: Guess matrix DU \u2208 Rr\u00d7k. 6: Find VA by solving minV \u2016DUV \u2212D(A\u2212B)\u20161. 7: Find UA by solving minU \u2016UVA \u2212 (A\u2212B)\u20161.\n8: Take the best solution [ UA UB ] , [ VA VB ] over all guesses.\n9: return [ UA UB ] , [ VA VB ] .\n10: end procedure\nTheorem C.9. Given matrix A \u2208 Rn\u00d7d, for any k \u2265 1, there exists an algorithm which takes (nd)O\u0303(k 2) time to output two matrices U \u2208 Rn\u00d72k, V \u2208 R2k\u00d7d,\n\u2016UV \u2212A\u20161 . min rank\u2212k Ak \u2016Ak \u2212A\u20161,\nholds with probability 9/10.\nProof. We define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be the optimal solution, i.e.,\nU\u2217V \u2217 = arg min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161,\nand define OPT = \u2016U\u2217V \u2217 \u2212A\u20161. Solving the Frobenious norm problem, we can find a factorization of a rank-k matrix B = UBVB\nwhere UB \u2208 Rn\u00d7k, VB \u2208 Rk\u00d7d, and B satisfies \u2016A\u2212B\u20161 \u2264 \u03b1B OPT for an \u03b1B = \u221a nd.\nLet D be a sampling and rescaling diagonal matrix corresponding to the Lewis weights of U\u2217, and let the number of nonzero entries on the diagonal be t = O(k log k).\nBy Lemma D.11 and Lemma D.8, the solution of min V \u2208Rk\u00d7d\n\u2016DU\u2217V \u2212D(A \u2212 B)\u20161 together with\nU\u2217 gives an O(1)-approximation to A\u2212B. In order to compute D we need to know U\u2217. Although\nwe do not know U\u2217, there still exists a way to figure out the Lewis weights. The idea is the same as in the previous discussion Lemma C.10 \u201cGuessing Lewis weights\u201d. By Claim C.11 and Claim C.12, the total number of possible D is nO(t).\nLemma C.10 tries to find a rank-k solution when all the entries in A are integers at most poly(nd). Here we focus on a bicriteria algorithm that outputs a rank-2k matrix without such a strong bit complexity assumption. We can show a better claim C.13, which is that the total numer of possible DU\u2217 is N O\u0303(k2) where N = poly(n) is the number of choices for a single entry in U\u2217.\nWe explain how to obtain an upper bound on N . Consider the optimum \u2016U\u2217V \u2217 \u2212 (A \u2212 B)\u20161. We can always change the basis so assume V \u2217 is an Auerbach basis (i.e., an `1-well-conditioned basis discussed in Section 1), so e\u2016x\u20161 \u2265 \u2016xV \u2217\u20161 \u2265 \u2016x\u20161/f , where e, f = poly(k). Then no entry of U\u2217 is larger than 2f\u2016A\u2212B\u20161, otherwise we could replace U\u2217 with 0 and get a better solution. Also any entry smaller than \u2016A \u2212 B\u20161/(enk\u03b1B100) can be replaced with 0 as this will incur additive error at most OPT /100. So if we round to integer multiples of \u2016A\u2212B\u20161/(enk\u03b1B100) we only have O(enk\u03b1Bf) possibilities for each entry of U\u2217 and still have an O(1)-approximation. We will just refer to this rounded U\u2217 as U\u2217, abusing notation.\nLet U denote the set of all the matrices U that we guess. From the above discussion, we conclude that, there exists a U \u2208 U such that \u2016UV \u2217 \u2212 (A\u2212B)\u20161 \u2264 O(OPT).\nFor each guess of DU\u2217 and D, we find VA, UA in the following way. We find VA by using a linear program to solve,\nmin V \u2208Rk\u00d7d\n\u2016DU\u2217V \u2212D(A\u2212B)\u20161.\nGiven VA and A, we write down a linear program to solve this problem,\nmin U\u2208Rn\u00d7k\n\u2016UVA \u2212 (A\u2212B)\u20161,\nwhich takes poly(ndk) time. Then we obtain UA. Recall that VB, UB are the two factors of B and it is a rank-k, \u03b1B-approximation solution to min U,V \u2016UV \u2212A\u20161. Then we have\n\u2225\u2225\u2225\u2225[UA UB] [VAVB ] \u2212A \u2225\u2225\u2225\u2225 1 = \u2225\u2225\u2225\u2225UAVA \u2212 (A\u2212 UBVB)\u2225\u2225\u2225\u2225 1 = \u2225\u2225\u2225\u2225UAVA \u2212 (A\u2212B)\u2225\u2225\u2225\u2225 1 .\nBecause there must exist a pair UA, VA satifying \u2016UAVA\u2212 (A\u2212B)\u20161 \u2264 O(OPT), it follows that by taking the best solution [ UA UB ] [VA VB ] over all guesses, we obtain an O(1)-approximation solution.\nOverall, the running time is (nd)O\u0303(k2).\nLemma C.10. Given an n \u00d7 d matrix A with integers bounded by poly(n), for any k \u2265 1, there exists an algorithm which takes (nd)O\u0303(k3) time to output two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d, such that\n\u2016UV \u2212A\u20161 . min rank\u2212k Ak \u2016Ak \u2212A\u20161,\nProof. We define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be the optimal solution, i.e.,\nU\u2217, V \u2217 = arg min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161,\nand define OPT = \u2016U\u2217V \u2217 \u2212A\u20161. Let D denote a sampling and rescaling diagonal matrix corresponding to the Lewis weights of U\u2217, and let the number of nonzero entries on the diagonal be t = O(k log k). By Lemma D.11 and Lemma D.8, the solution to min\nV \u2208Rk\u00d7d \u2016DU\u2217V \u2212DA\u20161 together with U\u2217 gives\nan O(1)-approximation to A. In order to compute D, we need to know U\u2217. Although we do not know U\u2217, there still exists a way to figure out the Lewis weights. We call the idea \u201cGuessing Lewis weights\u201d. We will explain this idea in the next few paragraphs.\nFirst, we can guess the nonzero entries on the diagonal, because the number of choices is small.\nClaim C.11. The number of possibile choice of supp(D) is at most nO(t).\nProof. The matrix has dimension n \u00d7 n and the number of nonzero entries is t. Thus the number of possible choices is at most \u2211t i=1 ( n t ) = nO(t).\nSecond, we can guess the value of each probability. For each probability, it is trivially at most 1. If the probability is less than 1/(poly(n)k log k), then we will never sample that row with high probability. It means that we can truncate the probability if it is below that threshold. We can also round each probability to 2\u2212i which only loses another constant factor in the approximation ratio. Thus, we have:\nClaim C.12. The total number of possible D is nO(t) = nO\u0303(k).\nSince \u2206/\u03b4 \u2264 poly(n) and the entries of A are in {\u2212\u2206,\u2212\u2206+\u03b4, . . . ,\u22122\u03b4,\u2212\u03b4, 0, \u03b4, 2\u03b4, \u00b7 \u00b7 \u00b7 ,\u2206\u2212\u03b4,\u2206}, we can lower bound the cost of \u2016U\u2217V \u2212A\u20161 given that it is non-zero by (nd\u2206/\u03b4)\u2212O(k) (if it is zero then A has rank at most k and we output A) using Lemma 4.1 in [CW09] and relating entrywise `1-norm to Frobenius norm. We can assume V is an `1 well-conditioned basis, since we can replace U\u2217 with U\u2217R\u22121 and V with RV for any invertible linear transformation R. By properties of such basis, we can discretize the entries of U\u2217 to integer multiples of (nd\u2206/\u03b4)\u2212O(k) while preserving relative error. Hence we can correctly guess each entry of DU\u2217 in ( nO(k) ) time.\nClaim C.13. The total numer of possible DU\u2217 is nO\u0303(k3).\nIn the following, let DU denote a guess of DU\u2217. Now the problem remaining is to solve min\nV \u2208Rk\u00d7d \u2016DUV \u2212 DA\u20161. Since we already know DA can be computed, and we know DU , we can solve this multiple regression problem by running linear programming. Thus the running time of this step is in poly(nd). After we get such a solution V , we use a linear program to solve\nmin U\u2208Rn\u00d7k\n\u2016DUV \u2212DA\u20161. Then we can get U .\nAfter we guess all the choices of D and DU\u2217, we must find a solution U, V which gives an O(1) approximation. The total running time is nO\u0303(k) \u00b7 nO\u0303(k3) \u00b7 poly(nd) = (nd)O\u0303(k3)."}, {"heading": "C.6 CUR decomposition for an arbitrary matrix A", "text": "Theorem C.14. Given matrix A \u2208 Rn\u00d7d, for any k \u2265 1, there exists an algorithm which takes O(nnz(A))+(n+d) poly(k) time to output three matrices C \u2208 Rn\u00d7c with columns from A, U \u2208 Rc\u00d7r, and R \u2208 Rr\u00d7d with rows from A, such that rank(CUR) = k, c = O(k log k), r = O(k log k), and\n\u2016CUR\u2212A\u20161 \u2264 poly(k) log d min rank\u2212k Ak \u2016Ak \u2212A\u20161,\nholds with probability 9/10.\nAlgorithm 6 CUR Decomposition Algorithm 1: procedure L1LowRankApproxCUR(A,n, d, k) . Theorem C.14 2: UB, VB \u2190L1LowRankApproxPolykLogd(A,n, d, k). 3: Let D1 \u2208 Rn\u00d7n be the sampling and rescaling diagonal matrix coresponding to the Lewis\nweights of B1 = UB \u2208 Rn\u00d7k, and let D1 have d1 = O(k log k) nonzero entries. 4: Let D>2 \u2208 Rd\u00d7d be the sampling and rescaling diagonal matrix coresponding to the Lewis\nweights of B>2 = ( (D1B1) \u2020D1A )> \u2208 Rd\u00d7k, and let D2 have d2 = O(k log k) nonzero entries.\n5: C \u2190 AD2, U \u2190 (B2D2)\u2020(D1B1)\u2020, and R\u2190 D1A. 6: return C,U,R. 7: end procedure\nProof. We define\nOPT := min rank\u2212k Ak\n\u2016Ak \u2212A\u20161.\nDue to Theorem C.6, we can output two matrices UB \u2208 Rn\u00d7k, VB \u2208 Rk\u00d7d such that UBVB gives a rank-k, and poly(k) log d-approximation solution to A, i.e.,\n\u2016UBVB \u2212A\u20161 \u2264 poly(k) log dOPT . (14)\nBy Section B.3, we can compute D1 \u2208 Rn\u00d7n which is a sampling and rescaling matrix corresponding to the Lewis weights of B1 = UB in O(n poly(k)) time, and there are d1 = O(k log k) nonzero entries on the diagonal of D1.\nDefine V \u2217 \u2208 Rk\u00d7d to be the optimal solution of min V \u2208Rk\u00d7d \u2016B1V \u2212A\u20161, V\u0302 = (D1B1)\u2020D1A \u2208 Rk\u00d7d,\nU1 \u2208 Rn\u00d7k to be the optimal solution of min U\u2208Rn\u00d7k \u2016UV\u0302 \u2212 A\u20161, and V \u2032 to be the optimal solution of\nmin V \u2208Rk\u00d7d \u2016D1A\u2212D1B1V \u20161. By Claim B.10, we have\n\u2016D1B1V\u0302 \u2212D1A\u20161 \u2264 \u221a d1\u2016D1B1V \u2032 \u2212D1A\u20161.\nDue to Lemma D.11 and Lemma D.8, with constant probability, we have \u2016B1V\u0302 \u2212A\u20161 \u2264 \u221a d1\u03b1D1\u2016B1V \u2217 \u2212A\u20161,\nwhere \u03b1D1 = O(1). Now, we can show,\n\u2016U1V\u0302 \u2212A\u20161 \u2264 \u2016B1V\u0302 \u2212A\u20161 by U1 = arg min U\u2208Rn\u00d7k \u2016UV\u0302 \u2212A\u20161\n. \u221a d1\u2016B1V \u2217 \u2212A\u20161\n\u2264 \u221a d1\u2016UBVB \u2212A\u20161\n\u2264 poly(k) log dOPT . by Equation (14) (15)\nWe define B2 = V\u0302 , then we replace V\u0302 by B2 \u2208 Rk\u00d7d and look at this objective function,\nmin U\u2208Rn\u00d7k\n\u2016UB2 \u2212A\u20161,\nwhere U\u2217 denotes the optimal solution. We use a sketching matrix to sketch the RHS of matrix UB2 \u2212 A \u2208 Rn\u00d7d. Let D>2 \u2208 Rd\u00d7d denote a sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>2 \u2208 Rd\u00d7k, and let the number of nonzero entries on the diagonal of D2 be d2 = O(k log k). We define U\u0302 = AD2(B2D2)\u2020 \u2208 Rn\u00d7k, U \u2032 \u2208 Rn\u00d7k to be the optimal solution of\nmin U\u2208Rn\u00d7k \u2016(UB2 \u2212A)D2\u20161. Recall that U1 \u2208 Rn\u00d7k is the optimal of min U\u2208Rn\u00d7k \u2016UB2 \u2212A\u20161. By Claim B.10, we have\n\u2016U\u0302B2D2 \u2212AD2\u20161 \u2264 \u221a d2\u2016U \u2032B2D2 \u2212AD2\u20161.\nAccording to Lemma D.8 and Lemma D.11, with constant probability, \u2016U\u0302B2 \u2212A\u20161 \u2264 \u03b1D2 \u221a d2\u2016U1B2 \u2212A\u20161,\nwhere \u03b1B2 = O(1). We have\n\u2016U\u0302B2 \u2212A\u20161 \u2264 \u221a d2\u03b1D2\u2016U1B2 \u2212A\u20161\n= \u221a d2\u03b1D2\u2016U1V\u0302 \u2212A\u20161 by B2 = V\u0302 \u2264 poly(k) log(d) OPT . by Equation (15)"}, {"heading": "Notice that U\u0302B2 = AD2(B2D2)\u2020(D1B1)\u2020D1A. Setting", "text": "C = AD2 \u2208 Rn\u00d7d2 , U = (B2D2)\u2020(D1B1)\u2020 \u2208 Rd2\u00d7d1 , and R = D1A \u2208 Rd1\u00d7d,\nwe get the desired CUR decomposition,\n\u2016AD2\ufe38\ufe37\ufe37\ufe38 C \u00b7 (B2D2)\u2020(D1B1)\u2020\ufe38 \ufe37\ufe37 \ufe38 U \u00b7D1A\ufe38\ufe37\ufe37\ufe38 R \u2212A\u20161 \u2264 poly(k) log(d) OPT .\nwith rank(CUR) = k. Overall, the running time is O(nnz(A)) + (n+ d) poly(k).\nC.7 Rank-r matrix B"}, {"heading": "C.7.1 Properties", "text": "Lemma C.15. Given matrix A \u2208 Rn\u00d7d, let OPT = min rank\u2212k Ak \u2016A\u2212Ak\u20161. For any r \u2265 k, if rank-r matrix B \u2208 Rn\u00d7d is an f -approximation to A, i.e.,\n\u2016B \u2212A\u20161 \u2264 f \u00b7OPT,\nand U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d is a g-approximation to B, i.e.,\n\u2016UV \u2212B\u20161 \u2264 g \u00b7 min rank\u2212k Bk \u2016Bk \u2212B\u20161,\nthen, \u2016UV \u2212A\u20161 . gf \u00b7OPT .\nProof. We define U\u0303 \u2208 Rn\u00d7k, V\u0303 \u2208 Rk\u00d7d to be two matrices, such that\n\u2016U\u0303 V\u0303 \u2212B\u20161 \u2264 g min rank\u2212k Bk \u2016Bk \u2212B\u20161,\nand also define,\nU\u0302 , V\u0302 = arg min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212B\u20161 and U\u2217, V \u2217 = arg min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u20161,\nThen,\n\u2016U\u0303 V\u0303 \u2212A\u20161 \u2264 \u2016U\u0303 V\u0303 \u2212B\u20161 + \u2016B \u2212A\u20161 by triangle inequality\n\u2264 g\u2016U\u0302 V\u0302 \u2212B\u20161 + \u2016B \u2212A\u20161 by definition \u2264 g\u2016U\u2217V \u2217 \u2212B\u20161 + \u2016B \u2212A\u20161 by \u2016U\u0302 V\u0302 \u2212B\u20161 \u2264 \u2016U\u2217V \u2217 \u2212B\u20161 \u2264 g\u2016U\u2217V \u2217 \u2212A\u20161 + g\u2016B \u2212A\u20161 + \u2016B \u2212A\u20161 by triangle inequality = gOPT +(g + 1)\u2016B \u2212A\u20161 by definition of OPT \u2264 gOPT +(g + 1)f \u00b7OPT by B is f -approximation to A . gf OPT .\nThis completes the proof.\nLemma C.16. Given a matrix B \u2208 Rn\u00d7d with rank r, for any 1 \u2264 k < r, for any fixed U\u2217 \u2208 Rn\u00d7k, choose a Cauchy matrix S with m = O(r log r) rows and rescaled by \u0398(1/m). With probability .999 for all V \u2208 Rk\u00d7d, we have\n\u2016SU\u2217V \u2212 SB\u20161 \u2265 \u2016U\u2217V \u2212B\u20161.\nProof. This follows by definitions in Section D and Lemma D.23.\nLemma C.17. Given a matrix B \u2208 Rn\u00d7d with rank r, for any 1 \u2264 k < r, for any fixed U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d, choose a Cauchy matrix S with m rows and rescaled by \u0398(1/m). We have\n\u2016SU\u2217V \u2217 \u2212 SB\u20161 \u2264 O(r log r)\u2016U\u2217V \u2217 \u2212B\u20161,\nwith probabiliaty .999.\nProof. Let U denote a well-conditioned basis of [U\u2217V \u2217, B], then U has d\u0303 = O(r) columns. We have\n\u2016SU\u20161 = d\u0303\u2211 i=1 m\u2211 j=1 |(SUi)j |\n= d\u0303\u2211 i=1 m\u2211 j=1 | 1 m n\u2211 l=1 Sj,lUl,i| by Sj,l \u223c C(0, 1)\n= 1\nm d\u0303\u2211 i=1 m\u2211 j=1 |ci,j | by ci,j \u223c C(0, \u2016Ui\u20161)\n= 1\nm d\u0303\u2211 i=1 m\u2211 j=1 \u2016Ui\u20161 \u00b7 wi+(j\u22121)d, by wi,j \u223c |C(0, 1)|\nwhere the last step follows since each wi can be thought of as a clipped half-Cauchy random variable. Define d\u2032 = md\u0303. Define event \u03bei to be the situation when wi < D (we will choose D later), and define event \u03be = \u03be1 \u2229 \u03be2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 \u03bed\u2032 . Using a similar proof as Lemma D.12, which is also similar to previous work [Ind06, SW11, CDMI+13], we obtain that\nPr [ m\u2211 i=1 \u2016SUi\u20161 \u2265 m\u2211 i=1 \u2016Ui\u20161t ] . log d\u2032 t + d\u2032 D .\nChoosing t = \u0398(log d\u2032) and D = \u0398(d\u2032), we have\nPr [ m\u2211 i=1 \u2016SUi\u20161 \u2265 m\u2211 i=1 \u2016Ui\u20161O(log d\u2032) ] \u2264 1 C ,\nfor a constant C. Condition on the above event. Let y = Ux, for some x \u2208 Rd. Then for any y,\n\u2016Sy\u20161 = \u2016SUx\u20161\n\u2264 d\u2032\u2211 j=1 \u2016SUjxj\u20161 by triangle inequality = d\u2032\u2211 j=1 |xj | \u00b7 \u2016SUj\u20161 . \u2016x\u2016\u221e log(d\u2032) d\u2032\u2211 j=1 \u2016Uj\u20161\n. r log r\u2016y\u20161,\nwhere the last step follows by \u2211d\u2032\nj=1 \u2016Uj\u20161 \u2264 d\u2032 and \u2016x\u2016\u221e \u2264 \u2016Ux\u20161 = \u2016y\u20161. Chossing C = 1000 completes the proof.\nLemma C.18. Given a matrix M \u2208 Rn\u00d7d with rank O(r), choose a random matrix S \u2208 Rm\u00d7n with each entry drawn from a standard Cauchy distribution and scaled by \u0398(1/m). We have that\n\u2016SM\u20161 \u2264 O(r log r)\u2016M\u20161,\nholds with probability .999.\nProof. Let U \u2208 RO(r) be the well-condtioned basis of M . Then each column of M can be expressed by Ux for some x. We then follow the same proof as that of Lemma C.17.\nC.7.2 poly(k, r)-approximation for rank-r matrix B\nTheorem C.19. Given a factorization of a rank-r matrix B = UBVB \u2208 Rn\u00d7d, where UB \u2208 Rn\u00d7r, VB \u2208 Rr\u00d7d, for any 1 \u2264 k \u2264 r there exists an algorithm which takes (n + d) \u00b7 poly(k) time to output two matrices U \u2208 Rn\u00d7k, V \u2208 Rk\u00d7d such that\n\u2016UV \u2212B\u20161 \u2264 poly(r) min rank\u2212k Bk \u2016Bk \u2212B\u20161,\nholds with probability 9/10.\nAlgorithm 7 poly(r, k)-approximation Algorithm for Rank-r matrix B\n1: procedure L1LowRankApproxB(UB, VB, n, d, k, r) . Theorem C.19 2: Set s\u2190 O\u0303(r), r\u2032 \u2190 O\u0303(r), t1 \u2190 O\u0303(r), t2 \u2190 O\u0303(r). 3: Choose dense Cauchy matrices S \u2208 Rs\u00d7n, R \u2208 Rd\u00d7r\u2032 , T1 \u2208 Rt1\u00d7n, T2 \u2208 Rd\u00d7t2 . 4: Compute S \u00b7 UB \u00b7 VB, UB \u00b7 VB \u00b7R and T1 \u00b7 UB \u00b7 VB \u00b7 T2. 5: Compute XY = arg minX,Y \u2016T1UBVBRXY SUBVBT2 \u2212 T1UBVBT2\u2016F . 6: return UBVBRX,Y SUBVB. 7: end procedure\nProof. We define\nOPT = min rank\u2212k Bk\n\u2016Bk \u2212B\u20161.\nChoose S \u2208 Rs\u00d7n to be a dense Cauchy transform matrix with s = O(r log r). Using Lemma C.18, Lemma D.23, and combining with Equation (10), we have\nmin U\u2208Rn\u00d7k,Z\u2208Rk\u00d7s\n\u2016UZSB \u2212B\u20161 \u2264 \u221a sO(r log r) OPT = O(r1.5 log1.5 r) OPT .\nLet \u03b1s = O(r1.5 log1.5 r). We define U\u2217, Z\u2217 = arg min\nU\u2208Rn\u00d7k,Z\u2208Rk\u00d7d \u2016UZSB \u2212 B\u20161. For the fixed Z\u2217 \u2208 Rk\u00d7s, choose a dense\nCauchy transform matrix R \u2208 Rd\u00d7r\u2032 with r\u2032 = O(r log r) and sketch on the right of (UZSB \u2212 B). We obtain the minimization problem, min\nU\u2208Rn\u00d7k \u2016UZ\u2217SBR\u2212BR\u20161.\nDefine U\u0302 j = BjR((Z\u2217SB)R)\u2020 \u2208 Rk,\u2200j \u2208 [n]. Then U\u0302 = BR((Z\u2217SB)R)\u2020 \u2208 Rn\u00d7k. Due to Claim B.10,\nn\u2211 j=1 \u2016BjR((Z\u2217SB)R)\u2020Z\u2217SBR\u2212BjR\u20161 \u2264 O( \u221a r\u2032) n\u2211 j=1 min Uj\u2208Rk \u2016U jZ\u2217SBR\u2212BjR\u20161,\nwhich is equivalent to\n\u2016BR((Z\u2217SB)R)\u2020Z\u2217SBR\u2212BR\u20161 \u2264 O( \u221a r\u2032) min\nU\u2208Rn\u00d7k \u2016UZ\u2217SBR\u2212BR\u20161,\nwhere BR is an n \u00d7 r\u2032 matrix and SB is an s \u00d7 d matrix. Both of them can be computed in (n+ d) poly(r) time.\nUsing Lemma D.8, Lemma D.23, Lemma C.18, we obtain,\n\u2016BR((Z\u2217SB)R)\u2020Z\u2217SB \u2212B\u20161 \u2264 O( \u221a r\u2032\u03b1r\u2032) min\nU\u2208Rn\u00d7k \u2016UZ\u2217SB \u2212B\u20161\nwhere \u03b1r\u2032 = O\u0303(r). We define X\u2217 \u2208 Rr\u00d7k, Y \u2217 \u2208 Rk\u00d7s,\nX\u2217, Y \u2217 = arg min X\u2208Rr\u2032\u00d7k,Y \u2208Rk\u00d7s \u2016BRXY SB \u2212B\u20161.\nThen,\n\u2016BRX\u2217Y \u2217SB \u2212B\u20161 \u2264 \u2016BR((Z\u2217SB)R)\u2020Z\u2217SB \u2212B\u20161 \u2264 O( \u221a r\u2032\u03b1r\u2032) min\nU\u2208Rn\u00d7k \u2016UZ\u2217SB \u2212B\u20161\n= O( \u221a r\u2032\u03b1r\u2032) min\nU\u2208Rn\u00d7k,Z\u2208Rk\u00d7s \u2016UZSB \u2212B\u20161\n\u2264 O( \u221a r\u2032\u03b1r\u2032\u03b1s) OPT .\nIt means that BRX, Y SB gives an O(\u03b1r\u2032\u03b1s \u221a r\u2032)-approximation to the original problem.\nThus it suffices to use Lemma C.20 to solve\nmin X\u2208Rr\u00d7k,Y \u2208Rk\u00d7s\n\u2016BRXY SB \u2212B\u20161,\nby losing an extra poly(r) approximation ratio. Therefore, we finish the proof.\nLemma C.20. Suppose we are given S \u2208 Rs\u00d7n, R \u2208 Rd\u00d7r\u2032, and a factorization of a rank-r matrix B = UBVB \u2208 Rn\u00d7d, where UB \u2208 Rn\u00d7r, VB \u2208 Rr\u00d7d. Then for any 1 \u2264 k \u2264 r, there exists an algorithm which takes (n+ d) poly(r, r\u2032, s) time to output two matrices X \u2032 \u2208 Rr\u2032\u00d7k, Y \u2032 \u2208 Rk\u00d7s such that\n\u2016BRX \u2032 \u00b7 Y \u2032SB \u2212B\u20161 \u2264 poly(r) min X\u2208Rr\u2032\u00d7k,Y \u2208Rk\u00d7s \u2016BRXY SB \u2212B\u20161\nholds with probability at least .999.\nProof. Choosing dense Cauchy matrices T1 \u2208 Rt1\u00d7n, T>2 \u2208 Rt2\u00d7d to sketch on both sides, we get the problem\nmin X\u2208Rr\u2032\u00d7k,Y \u2208Rk\u00d7s\n\u2016T1BRXY SBT2 \u2212 T1BT2\u20161, (16)\nwhere t1 = O\u0303(r) and t2 = O\u0303(r). Define X \u2032, Y \u2032 to be the optimal solution of\nmin X\u2208Rr\u2032\u00d7k,Y \u2208Rk\u00d7s\n\u2016T1BRXY SBT2 \u2212 T1BT2\u2016F .\nDefine X\u0303, Y\u0303 to the be the optimal solution of\nmin X\u2208Rr\u2032\u00d7k,Y \u2208Rk\u00d7s\n\u2016BRXY SB \u2212B\u20161.\nBy Claim B.11,\n\u2016T1BRX \u2032Y \u2032SBT2 \u2212 T1BT2\u20161 \u2264 \u221a t1t2 min\nX\u2208Rr\u2032\u00d7k,Y \u2208Rk\u00d7s \u2016T1BRXY SBT2 \u2212 T1BT2\u20161.\nBy Lemma D.10, Lemma C.18 and Lemma D.23, we have\n\u2016BRX \u2032 \u00b7 Y \u2032SB \u2212B\u20161 \u2264 \u221a t1t2 \u00b7 O\u0303(r2)\u2016BRX\u0303 \u00b7 Y\u0303 SB \u2212B\u20161.\nThis completes the proof."}, {"heading": "D Contraction and Dilation Bound for `1", "text": "This section presents the essential lemmas for `1-low rank approximation. Section D.1 gives some basic definitions. Section D.2 shows some properties implied by contraction and dilation bounds. Section D.3 presents the no dilation lemma for a dense Cauchy transform. Section D.4 and D.5 presents the no contraction lemma for dense Cauchy transforms. Section D.6 and D.7 contains the results for sparse Cauchy transforms and Lewis weights."}, {"heading": "D.1 Definitions", "text": "Definition D.1. Given a matrix M \u2208 Rn\u00d7d, if matrix S \u2208 Rm\u00d7n satisfies\n\u2016SM\u20161 \u2264 c1\u2016M\u20161,\nthen S has at most c1-dilation on M .\nDefinition D.2. Given a matrix U \u2208 Rn\u00d7k, if matrix S \u2208 Rm\u00d7n satisfies\n\u2200x \u2208 Rk, \u2016SUx\u20161 \u2265 1\nc2 \u2016Ux\u20161,\nthen S has at most c2-contraction on U .\nDefinition D.3. Given matrices U \u2208 Rn\u00d7k, A \u2208 Rn\u00d7d, let V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u20161. If matrix S \u2208 Rm\u00d7n satisfies\n\u2200V \u2208 Rk\u00d7d, \u2016SUV \u2212 SA\u20161 \u2265 1 c3 \u2016UV \u2212A\u20161 \u2212 c4\u2016UV \u2217 \u2212A\u20161,\nthen S has at most (c3, c4)-contraction on (U,A).\nDefinition D.4. A (c5, c6) `1-subspace embedding for the column space of an n\u00d7 k matrix U is a matrix S \u2208 Rm\u00d7n for which all x \u2208 Rk\n1 c5 \u2016Ux\u20161 \u2264 \u2016SUx\u20161 \u2264 c6\u2016Ux\u20161.\nDefinition D.5. Given matrices U \u2208 Rn\u00d7k, A \u2208 Rn\u00d7d, let V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u20161. Let S \u2208 Rm\u00d7n. If for all c \u2265 1, and if for any V\u0302 \u2208 Rk\u00d7d which satisfies\n\u2016SUV\u0302 \u2212 SA\u20161 \u2264 c \u00b7 min V \u2208Rk\u00d7d \u2016SUV \u2212 SA\u20161,\nit holds that\n\u2016UV\u0302 \u2212A\u20161 \u2264 c \u00b7 c7 \u00b7 \u2016UV \u2217 \u2212A\u20161,\nthen S provides a c7-multiple-regression-cost preserving sketch of (U,A).\nDefinition D.6. Given matrices L \u2208 Rn\u00d7m1 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, k \u2265 1, let\nX\u2217 = arg min rank\u2212k X \u2016LXN \u2212A\u20161.\nLet S \u2208 Rm\u00d7n. If for all c \u2265 1, and if for any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016SLX\u0302N \u2212 SA\u20161 \u2264 c \u00b7 min rank\u2212k X \u2016SLXN \u2212 SA\u20161,\nit holds that\n\u2016LX\u0302N \u2212A\u20161 \u2264 c \u00b7 c8 \u00b7 \u2016LX\u2217N \u2212A\u20161,\nthen S provides a c8-restricted-multiple-regression-cost preserving sketch of (L,N,A, k)."}, {"heading": "D.2 Properties", "text": "Lemma D.7. Given matrices A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, let V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u20161. If S \u2208 Rm\u00d7n has at most c1-dilation on UV \u2217 \u2212A, i.e.,\n\u2016S(UV \u2217 \u2212A)\u20161 \u2264 c1\u2016UV \u2217 \u2212A\u20161,\nand it has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rk, \u2016SUx\u20161 \u2265 1\nc2 \u2016Ux\u20161,\nthen S has at most (c2, c1 + 1c2 )-contraction on (U,A), i.e.,\n\u2200V \u2208 Rk\u00d7d, \u2016SUV \u2212 SA\u20161 \u2265 1\nc2 \u2016UV \u2212A\u20161 \u2212 (c1 +\n1\nc2 )\u2016UV \u2217 \u2212A\u20161,\nProof. Let A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, S \u2208 Rm\u00d7n be the same as that described in the lemma. Then \u2200V \u2208 Rk\u00d7d\n\u2016SUV \u2212 SA\u20161 \u2265 \u2016SUV \u2212 SUV \u2217\u20161 \u2212 \u2016SUV \u2217 \u2212 SA\u20161 \u2265 \u2016SUV \u2212 SUV \u2217\u20161 \u2212 c1\u2016UV \u2217 \u2212A\u20161 = \u2016SU(V \u2212 V \u2217)\u20161 \u2212 c1\u2016UV \u2217 \u2212A\u20161\n= d\u2211 j=1 \u2016SU(V \u2212 V \u2217)j\u20161 \u2212 c1\u2016UV \u2217 \u2212A\u20161 \u2265 d\u2211 j=1 1 c2 \u2016U(V \u2212 V \u2217)j\u20161 \u2212 c1\u2016UV \u2217 \u2212A\u20161\n= 1\nc2 \u2016UV \u2212 UV \u2217\u20161 \u2212 c1\u2016UV \u2217 \u2212A\u20161\n\u2265 1 c2 \u2016UV \u2212A\u20161 \u2212 1 c2 \u2016UV \u2217 \u2212A\u20161 \u2212 c1\u2016UV \u2217 \u2212A\u20161 = 1\nc2 \u2016UV \u2212A\u20161 \u2212\n( ( 1\nc2 + c1)\u2016UV \u2217 \u2212A\u20161\n) .\nThe first inequality follows by the triangle inequality. The second inequality follows since S has at most c1 dilation on UV \u2217 \u2212 A. The third inequality follows since S has at most c2 contraction on U . The fourth inequality follows by the triangle inequality.\nLemma D.8. Given matrices A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, let V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u20161. If S \u2208 Rm\u00d7n has at most c1-dilation on UV \u2217 \u2212A, i.e.,\n\u2016S(UV \u2217 \u2212A)\u20161 \u2264 c1\u2016UV \u2217 \u2212A\u20161,\nand has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rk, \u2016SUx\u20161 \u2265 1\nc2 \u2016Ux\u20161,\nthen S provides a (2c1c2 + 1)-multiple-regression-cost preserving sketch of (U,A), i.e., for all c \u2265 1, for any V\u0302 \u2208 Rk\u00d7d which satisfies\n\u2016SUV\u0302 \u2212 SA\u20161 \u2264 c \u00b7 min V \u2208Rk\u00d7d \u2016SUV \u2212 SA\u20161,\nit has\n\u2016UV\u0302 \u2212A\u20161 \u2264 c \u00b7 (2c1c2 + 1) \u00b7 \u2016UV \u2217 \u2212A\u20161,\nProof. Let S \u2208 Rm\u00d7n, A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, V \u2217, V\u0302 \u2208 Rk\u00d7d, and c be the same as stated in the lemma.\n\u2016UV\u0302 \u2212A\u20161 \u2264 c2\u2016SUV\u0302 \u2212 SA\u20161 + (1 + c1c2)\u2016UV \u2217 \u2212A\u20161 \u2264 c2c min\nV \u2208Rk\u00d7d \u2016SUV \u2212 SA\u20161 + (1 + c1c2)\u2016UV \u2217 \u2212A\u20161\n\u2264 c2c\u2016SUV \u2217 \u2212 SA\u20161 + (1 + c1c2)\u2016UV \u2217 \u2212A\u20161 \u2264 c1c2c\u2016UV \u2217 \u2212A\u20161 + (1 + c1c2)\u2016UV \u2217 \u2212A\u20161 \u2264 c \u00b7 (1 + 2c1c2)\u2016UV \u2217 \u2212A\u20161.\nThe first inequality follows by Lemma D.7. The second inequality follows by the guarantee of V\u0302 . The fourth inequality follows since S has at most c1-dilation on UV \u2217 \u2212 A. The fifth inequality follows since c \u2265 1.\nLemma D.9. Given matrices L \u2208 Rn\u00d7m1 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, k \u2265 1, let\nX\u2217 = arg min rank\u2212k X \u2016LXN \u2212A\u20161.\nIf S \u2208 Rm\u00d7n has at most c1-dilation on LX\u2217N \u2212A, i.e.,\n\u2016S(LX\u2217N \u2212A)\u20161 \u2264 c1\u2016LX\u2217N \u2212A\u20161,\nand has at most c2-contraction on L, i.e.,\n\u2200x \u2208 Rm1\u2016SLx\u20161 \u2265 \u2016Lx\u20161,\nthen S provides a (2c1c2 + 1)-restricted-multiple-regression-cost preserving sketch of (L,N,A, k), i.e., for all c \u2265 1, for any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016SLX\u0302N \u2212 SA\u20161 \u2264 c \u00b7 min rank\u2212k X \u2016SLXN \u2212 SA\u20161,\nit holds that\n\u2016LX\u0302N \u2212A\u20161 \u2264 c \u00b7 (2c1c2 + 1) \u00b7 \u2016LX\u2217N \u2212A\u20161.\nProof. Let S \u2208 Rm\u00d7n, L \u2208 Rn\u00d7m1 , X\u0302 \u2208 Rm1\u00d7m2 , X\u2217 \u2208 Rm1\u00d7m2 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, and c \u2265 1 be the same as stated in the lemma.\n\u2016SLX\u0302N \u2212 SA\u20161 \u2265 \u2016SLX\u0302N \u2212 SLX\u2217N\u20161 \u2212 \u2016SLX\u2217N \u2212 SA\u20161\n\u2265 1 c2 \u2016L(X\u0302N \u2212X\u2217N)\u20161 \u2212 c1\u2016LX\u2217N \u2212A\u20161 \u2265 1 c2 \u2016LX\u0302N \u2212A\u20161 \u2212 1 c2 \u2016LX\u2217N \u2212A\u20161 \u2212 c1\u2016LX\u2217N \u2212A\u20161 = 1\nc2 \u2016LX\u0302N \u2212A\u20161 \u2212 (\n1 c2 + c1)\u2016LX\u2217N \u2212A\u20161.\nThe inequality follows by the triangle inequality. The second inequality follows since S has at most c2-contraction on L, and it has at most c1-dilation on LX\u2217N \u2212 A. The third inequality follows by the triangle inequality.\nIt follows that\n\u2016LX\u0302N \u2212A\u20161 \u2264 c2\u2016SLX\u0302N \u2212 SA\u20161 + (1 + c1c2)\u2016LX\u2217N \u2212A\u20161 \u2264 c2c \u00b7 min\nrank\u2212k X \u2016SLXN \u2212 SA\u20161 + (1 + c1c2)\u2016LX\u2217N \u2212A\u20161\n\u2264 c2c \u00b7 \u2016SLX\u2217N \u2212 SA\u20161 + (1 + c1c2)\u2016LX\u2217N \u2212A\u20161 \u2264 cc1c2 \u00b7 \u2016LX\u2217N \u2212A\u20161 + (1 + c1c2)\u2016LX\u2217N \u2212A\u20161 \u2264 c \u00b7 (1 + 2c1c2)\u2016LX\u2217N \u2212A\u20161.\nThe first inequality directly follows from the previous one. The second inequality follows from the guarantee of X\u0302. The fourth inequality follows since S has at most c1 dilation on LX\u2217N \u2212 A. The fifth inequality follows since c \u2265 1.\nLemma D.10. Given matrices L \u2208 Rn\u00d7m1 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, k \u2265 1, let\nX\u2217 = arg min rank\u2212k X \u2016LXN \u2212A\u20161.\nLet T1 \u2208 Rt1\u00d7n have at most c1-dilation on LX\u2217N \u2212A, and at most c2-contraction on L. Let\nX\u0303 = arg min rank\u2212k X\n\u2016T1LXN \u2212 T1A\u20161.\nLet T>2 \u2208 Rt2\u00d7d have at most c\u20321-dilation on (T1LX\u0303N \u2212 T1A)>, and at most c\u20322-contraction on N>. Then, for all c \u2265 1, for any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016T1(LX\u0302N \u2212 SA)T2\u20161 \u2264 c \u00b7 min rank\u2212k X \u2016T1(LXN \u2212A)T2\u20161,\nit has\n\u2016LX\u0302N \u2212A\u20161 \u2264 c \u00b7 (2c1c2 + 1)(2c\u20321c\u20322 + 1) \u00b7 \u2016LX\u2217N \u2212A\u20161.\nProof. Apply Lemma D.9 for sketch matrix T2. Then for any c \u2265 1, any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016T1(LX\u0302N \u2212A)T2\u20161 \u2264 c \u00b7 min rank\u2212k X \u2016T1(LXN \u2212A)T2\u20161,\nhas\n\u2016T1(LX\u0302N \u2212A)\u20161 \u2264 c \u00b7 (2c\u20321c\u20322 + 1) \u00b7 \u2016T1(LX\u0303N \u2212A)\u20161.\nApply Lemma D.9 for sketch matrix T1. Then for any c \u2265 1, any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016T1(LX\u0302N \u2212A)\u20161 \u2264 c(2c\u20321c\u20322 + 1) \u00b7 min rank\u2212k X \u2016T1(LX\u0303N \u2212A)\u20161,\nhas\n\u2016LX\u0302N \u2212A\u20161 \u2264 c \u00b7 (2c1c2 + 1)(2c\u20321c\u20322 + 1) \u00b7 \u2016LX\u2217N \u2212A\u20161.\nLemma D.11. Given matrices M \u2208 Rn\u00d7d, U \u2208 Rn\u00d7t, d \u2265 t = rank(U), n \u2265 d \u2265 r = rank(M), if sketching matrix S \u2208 Rm\u00d7n is drawn from any of the following probability distributions of matrices, with .99 probability S has at most c1-dilation on M , i.e.,\n\u2016SM\u20161 \u2264 c1\u2016M\u20161,\nand S has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rt, \u2016SUx\u20161 \u2265 1\nc2 \u2016Ux\u20161,\nwhere c1, c2 are parameters depend on the distribution over S.\n(I) S \u2208 Rm\u00d7n is a dense Cauchy matrix: a matrix with i.i.d. entries from the standard Cauchy distribution. If m = O(t log t), then c1c2 = O(log d). If m = O((t + r) log(t + r)), then c1c2 = O(min(log d, r log r)).\n(II) S \u2208 Rm\u00d7n is a sparse Cauchy matrix: S = TD, where T \u2208 Rm\u00d7n has each column i.i.d. from the uniform distribution on standard basis vectors of Rm, and D \u2208 Rn\u00d7n is a diagonal matrix with i.i.d. diagonal entries following a standard Cauchy distribution. If m = O(t5 log5 t), then c1c2 = O(t2 log2 t log d). If m = O((t + r)5 log5(t + r)), then c1c2 = O(min(t2 log2 t log d, r3 log3 r)).\n(III) S \u2208 Rm\u00d7n is a sampling and rescaling matrix (notation S \u2208 Rn\u00d7n denotes a diagonal sampling and rescaling matrix with m non-zero entries): If S samples and reweights m = O(t log t) rows of U , selecting each with probability proportional to the ith row\u2019s `1 Lewis weight and reweighting by the inverse probability, then c1c2 = O(1).\n(IV) S \u2208 Rm\u00d7n is a dense Cauchy matrix with limited independence: S is a matrix with each entry drawn from a standard Cauchy distribution. Entries from different rows of S are fully independent, and entries from the same row of S are W -wise independent. If m = O(t log t), and W = O\u0303(d), then c1c2 = O(t log d). If m = O(t log t), and W = O\u0303(td), then c1c2 = O(log d).\nIn the above, if we replace S with \u03c3 \u00b7 S where \u03c3 \u2208 R\\{0} is any scalar, then the relation between m and c1c2 can be preserved.\nFor (I), if m = O(t log t), then c1c2 = O(log d) is implied by Lemma D.23 and Lemma D.13. If m = O((t+ r) log(t+ r)), c1c2 = O(r log r) is implied by [SW11].\nFor (II), if m = O(t5 log5 t), then c1c2 = O(t2 log2 t log d) is implied by Corollary D.27 and Lemma D.25. If m = O((t+ r)5 log5(t+ r)), c1c2 = O(r3 log3 r) is implied by [MM13].\nFor (III), it is implied by [CP15] and Lemma D.29. For (IV), it is implied by Lemma I.4, Corollary I.5."}, {"heading": "D.3 Cauchy embeddings, no dilation", "text": "Lemma D.12. Define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be the optimal solution of min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u20161. Choose a Cauchy matrix S with m rows and rescaled by \u0398(1/m). We have that\n\u2016SU\u2217V \u2217 \u2212 SA\u20161 \u2264 O(log d)\u2016U\u2217V \u2217 \u2212A\u20161\nholds with probability at least 99/100.\nProof. The proof technique has been used in [Ind06] and [CDMI+13]. Fix the optimal U\u2217 and V \u2217, then\n\u2016SU\u2217V \u2217 \u2212 SA\u20161 = d\u2211 i=1 \u2016S(U\u2217V \u2217i \u2212Ai)\u20161\n= d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 1 m Sj,l \u00b7 (U\u2217V \u2217i \u2212Ai)l| where Sj,l \u223c C(0, 1)\n= d\u2211 i=1 m\u2211 j=1 1 m |ci,j | where ci,j \u223c C(0, \u2016U\u2217V \u2217i \u2212Ai\u20161)\n= 1\nm d\u2211 i=1 m\u2211 j=1 \u2016U\u2217V \u2217i \u2212Ai\u20161 \u00b7 wi+d(j\u22121). where wi+d(j\u22121) \u223c |C(0, 1)| (17)\nwhere the last step follows since each wi can be thought of as a clipped half-Cauchy random variable. Define d\u2032 = md. Define event \u03bei to be the situation in which wi < D (we will decide upon D later), and define event \u03be = \u03be1 \u2229 \u03be2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 \u03bed\u2032 . Then it is clear that \u03be \u2229 \u03bei = \u03be, \u2200i \u2208 [d\u2032].\nUsing the probability density function (pdf) of a Cauchy and because tan\u22121 x \u2264 x, we can lower bound Pr[event \u03bei holds] in the following sense,\nPr[\u03bei] = 2 \u03c0 tan\u22121(D) = 1\u2212 2 \u03c0 tan\u22121(1/D) \u2265 1\u2212 2 \u03c0D .\nBy a union bound over all i \u2208 [d\u2032], we can lower bound Pr[event \u03be holds],\nPr[\u03be] \u2265 1\u2212 d\u2032\u2211 i=1 Pr[\u03bei] \u2265 1\u2212 2d\u2032 \u03c0D . (18)"}, {"heading": "By Bayes rule and \u03be = \u03be \u2229 \u03bei, Pr[\u03be|\u03bei] Pr[\u03bei] = Pr[\u03be \u2229 \u03bei] = Pr[\u03be], which implies that Pr[\u03be|\u03bei] =", "text": "Pr[\u03be]/Pr[\u03bei]. First, we can lower bound E[wi|\u03bei],\nE[wi|\u03bei] = E[wi|\u03bei \u2229 \u03be] Pr[\u03be|\u03bei] + E[wi|\u03be \u2229 \u03be] Pr[\u03be|\u03bei] \u2265 E[wi|\u03bei \u2229 \u03be] Pr[\u03be|\u03bei] by wi \u2265 0 and Pr[] \u2265 0 = E[wi|\u03be] Pr[\u03be|\u03bei] by \u03be = \u03be \u2229 \u03bei.\nThe above equation implies that\nE[wi|\u03be] \u2264 E[wi|\u03bei] Pr[\u03be|\u03bei]\n= E[wi|\u03bei] Pr[\u03bei]\nPr[\u03be \u2229 \u03bei] by Bayes rule Pr[\u03be|\u03bei] Pr[\u03bei] = Pr[\u03be \u2229 \u03bei]\n= E[wi|\u03bei] Pr[\u03bei]\nPr[\u03be] by \u03be = \u03be \u2229 \u03bei.\nUsing the pdf of a Cauchy, E[wi|\u03bei] = 1\u03c0 log(1 +D 2)/Pr[\u03bei] and plugging it into the lower bound\nof E[wi|\u03be],\nE[wi|\u03be] \u2264 E[wi|\u03bei] Pr[\u03bei]\nPr[\u03be] =\n1 \u03c0 log(1 +D 2)\nPr[\u03be] \u2264\n1 \u03c0 log(1 +D 2)\n1\u2212 2d\u03c0D . log(D),\nwhere the third step follows since Pr[\u03be] \u2265 1\u2212 2d\u2032\u03c0D and the last step follows by choosing D = \u0398(d \u2032).\nWe can conclude\nE[\u2016SU\u2217V \u2217 \u2212 SA\u20161|\u03be] = 1\nm d\u2211 i=1 m\u2211 j=1 \u2016U\u2217V \u2217i \u2212Ai\u20161 \u00b7E[wi+d(j\u22121)|\u03be] . (log d\u2032) \u00b7 \u2016U\u2217V \u2217 \u2212A\u20161. (19)\nFor simplicity, define X = \u2016SU\u2217V \u2217 \u2212 SA\u20161 and \u03b3 = \u2016U\u2217V \u2217 \u2212 A\u20161. By Markov\u2019s inequality and because Pr[X \u2265 \u03b3t|\u03be] \u2264 1, we have\nPr[X \u2265 \u03b3t] = Pr[X \u2265 \u03b3t|\u03be] Pr[\u03be] + Pr[X \u2265 \u03b3t|\u03be] Pr[\u03be] \u2264 Pr[X \u2265 \u03b3t|\u03be] + Pr[\u03be]\n\u2264 E[X|\u03be] \u03b3t + Pr[\u03be] by Markov\u2019s inequality \u2264 E[X|\u03be] \u03b3t + 2d\u2032 \u03c0D by Equation (18) . log d\u2032\nt +\n2d\u2032 \u03c0D by Equation (19)\n\u2264 .01,\nwhere choosing t = \u0398(log d\u2032) and D = \u0398(d\u2032). Since k \u2264 d and m = poly(k), we have t = \u0398(log d), which completes the proof.\nLemma D.13. Given any matrix M \u2208 Rn\u00d7d, if matrix S \u2208 Rm\u00d7n has each entry drawn from an i.i.d. standard Cauchy distribution and is rescalsed by \u0398(1/m), then\n\u2016SM\u20161 \u2264 O(log d)\u2016M\u20161\nholds with probability at least 99/100.\nProof. Just replace the matrix U\u2217V \u2217 \u2212 A in the proof of Lemma D.12 with M . Then we can get the result directly."}, {"heading": "D.4 Cauchy embeddings, no contraction", "text": "We prove that if we choose a Cauchy matrix S, then for a fixed optimal solution U\u2217 of minU,V \u2016UV \u2212 A\u20161, and for all V , we have that with high probability \u2016SU\u2217V \u2212SA\u20161 is lower bouned by \u2016U\u2217V \u2212A\u20161 up to some constant.\nLemma D.14. Define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be the optimal solution of min U\u2208Rn\u00d7kV \u2208Rk\u00d7d \u2016UV\u2212A\u20161, where A \u2208 Rn\u00d7d. Let m = O(k log k), S \u2208 Rm\u00d7n be a random matrix with each entry an i.i.d. standard Cauchy random variable, scaled by \u0398(1/m). Then with probability at least 0.95,\n\u2200V \u2208 Rk\u00d7d, \u2016U\u2217V \u2212A\u20161 . \u2016SU\u2217V \u2212 SA\u20161 +O(log(d))\u2016U\u2217V \u2217 \u2212A\u20161.\nProof. This follows by Lemmas D.12, D.23, D.7.\nD.5 Cauchy embeddings, k-dimensional subspace\nThe goal of this section is to prove Lemma D.23. Before getting into the details, we first give the formal definition of an (\u03b1, \u03b2) `1 well-conditioned basis and -net.\nDefinition D.15 (`1 Well-conditioned basis). [DDH+09] A basis U for the range of A is (\u03b1, \u03b2)condtioned if \u2016U\u20161 \u2264 \u03b1 and for all x \u2208 Rk, \u2016x\u2016\u221e \u2264 \u03b2\u2016Ux\u20161. We will say U is well-conditioned if \u03b1 and \u03b2 are low-degree polynomials in k, independent of n.\nNote that a well-conditoned basis implies the following result.\nFact D.16. There exist \u03b1, \u03b2 \u2265 1 such that,\n\u2200x \u2208 Rk, 1 k\u03b2 \u2016x\u20161 \u2264 \u2016Ux\u20161 \u2264 \u03b1\u2016x\u20161.\nProof. The lower bound can be proved in the following sense,\n\u2016Ux\u20161 \u2265 1\n\u03b2 \u2016x\u2016\u221e \u2265\n1\n\u03b2\n1 k \u2016x\u20161,\nwhere the first step follows by the properties of a well-conditioned basis, and the second step follows since k\u2016x\u2016\u221e \u2265 \u2016x\u20161. Then we can show an upper bound,\n\u2016Ux\u20161 \u2264 \u2016U\u20161 \u00b7 \u2016x\u20161 \u2264 \u03b1\u2016x\u20161,\nwhere the first step follows by \u2016Ux\u20161 \u2264 \u2016U\u20161\u2016x\u20161, and the second step follows using \u2016U\u20161 \u2264 \u03b1.\nDefinition D.17 ( -net). Define N to an -net where, for all the x \u2208 Rk that \u2016x\u20161 = 1, for any vectors two x, x\u2032 \u2208 N , \u2016x \u2212 x\u2032\u20161 \u2265 , for any vector y /\u2208 N , there exists an x \u2208 N such that \u2016x\u2212 y\u20161 \u2264 . Then the size of N is (1 ) O(k) = 2O(k log(1/ )).\nLemma D.18 (Lemma 6 in [SW11]). There is a constant c0 > 0 such that for any t \u2265 1 and any constant c > c0, if S is a t \u00d7 n matrix whose entries are i.i.d. standard Cauchy random variables scaled by c/t, then, for any fixed y \u2208 Rn,\nPr[\u2016Sy\u20161 < \u2016y\u20161] \u2264 1/2t\nLemma D.19. Suppose we are given a well-conditioned basis U \u2208 Rn\u00d7k. If S is a t \u00d7 n matrix whose entries are i.i.d. standard Cauchy random variables scaled by \u0398(1/t), then with probability 1\u2212 2\u2212\u2126(t), for all vectors x \u2208 N we have that \u2016SUx\u20161 \u2265 \u2016Ux\u20161.\nProof. First, using Lemma D.18, we have for any fixed vector y \u2208 Rn, Pr[\u2016Sy\u2016 < \u2016y\u20161] \u2264 1/2t. Second, we can rewrite y = Ux. Then for any fixed x \u2208 N , Pr[\u2016SUx\u2016 < \u2016Ux\u20161] \u2264 1/2t. Third, choosing t & k log(1/ ) and taking a union bound over all the vectors in the -net N completes the proof.\nLemma D.20 (Lemma 7 in [SW11]). Let S be a t\u00d7n matrix whose entries are i.i.d standard Cauchy random variables, scaled by c/t for a constant c, and t \u2265 1. Then there is a constant c\u2032 = c\u2032(c) > 0 such that for any fixed set of {y1, y2, \u00b7 \u00b7 \u00b7 , yk} of d vectors in {y \u2208 Rn : x \u2208 Rk, Ux = y},\nPr[ k\u2211 i=1 \u2016Syi\u20161 \u2265 c\u2032 log(tk) k\u2211 i=1 \u2016yi\u20161] \u2264 1 1000 .\nUsing Lemma D.20 and the definition of an (\u03b1, \u03b2) well-conditioned basis, we can show the following corollary.\nCorollary D.21. Suppose we are given an (\u03b1, \u03b2) `1 well-conditioned basis U \u2208 Rn\u00d7k. Choose S to be an i.i.d. Cauchy matrix with t = O(k log k) rows, and rescale each entry by \u0398(1/t). Then with probability 99/100, for all vectors x \u2208 Rk,\n\u2016SUx\u20161 \u2264 O(\u03b1\u03b2 log(k)) \u00b7 \u2016Ux\u20161,\nProof. Define event E to be the situation when k\u2211 i=1 \u2016SUi\u20161 < c\u2032 log(tk) k\u2211 i=1 \u2016Ui\u20161\nholds, and U is a well-conditioned basis. Using Lemma D.20, we can show that event E holds with probability 999/1000. We condition on Event E holding. Then for any y = Ux for an x \u2208 Rk, we have\n\u2016Sy\u20161 = \u2016SUx\u20161\n\u2264 k\u2211 j=1 \u2016SUjxj\u20161 by triangle inequality = k\u2211 j=1 |xj | \u00b7 \u2016SUj\u20161 \u2264 \u2016x\u2016\u221ec\u2032 log(tk) k\u2211 j=1 \u2016Uj\u20161 by Lemma D.20 = \u2016x\u2016\u221ec\u2032 log(tk)\u03b1 by \u2016U\u20161 = k\u2211 j=1 \u2016Uj\u20161 \u2264 \u03b1 \u2264 \u03b2\u2016Ux\u20161c\u2032 log(tk)\u03b1 by \u2016x\u2016\u221e \u2264 \u03b2\u2016Ux\u20161 \u2264 \u03b2\u2016y\u20161c\u2032 log(tk)\u03b1 by Ux = y.\nThis completes the proof.\nLemma D.22. Given an (\u03b1, \u03b2) `1 well-conditioned basis, condition on the following two events, 1. For all x \u2208 N , \u2016SUx\u20161 \u2265 \u2016Ux\u20161. (Lemma D.18) 2. For all x \u2208 Rk, \u2016SUx\u20161 \u2264 O(\u03b1\u03b2 log k)\u2016Ux\u20161. (Corollary D.21) Then, for all w \u2208 Rk, \u2016SUw\u20161 & \u2016Uw\u20161.\nProof. For any w \u2208 Rk we can write it as w = ` \u00b7 z where ` is some scalar and z has \u2016z\u20161 = 1. Define y = arg min\ny\u2032\u2208N \u2016y\u2032 \u2212 z\u20161.\nWe first show that if U is an (\u03b1, \u03b2) well-conditioned basis for `1, then \u2016U(y\u2212z)\u20161 \u2264 \u03b1\u03b2k \u2016Uy\u20161,\n\u2016U(y \u2212 z)\u20161 \u2264 \u03b1\u2016y \u2212 z\u20161 by \u2016U(y \u2212 z)\u20161 \u2264 \u03b1\u2016y \u2212 z\u20161 \u2264 \u03b1 by \u2016y \u2212 z\u20161 \u2264 = \u03b1 \u2016y\u20161 by \u2016y\u20161 = 1\n\u2264 \u03b1 \u2016Uy\u20161\u03b2k by \u2016Uy\u20161 \u2265 1\n\u03b2k \u2016y\u20161.\nBecause < 1/(\u03b1\u03b2kc+1), we have\n\u2016U(y \u2212 z)\u20161 \u2264 1\nkc \u2016Uy\u20161. (20)\nUsing the triangle inequality, we can lower bound \u2016Uy\u20161 by \u2016Uz\u20161 up to some constant,\n\u2016Uy\u20161 \u2265 \u2016Uz\u20161 \u2212 \u2016U(y \u2212 z)\u20161 \u2265 \u2016Uz\u20161 \u2212 1\nkc \u2016Uy\u20161, (21)\nwhich implies \u2016Uy\u20161 \u2265 .99\u2016Uz\u20161. (22)\nThus,\n\u2016SUz\u20161 \u2265 \u2016SUy\u20161 \u2212 \u2016SU(z \u2212 y)\u20161 by triangle inequality \u2265 \u2016Uy\u20161 \u2212 \u2016SU(z \u2212 y)\u20161 by Lemma D.18 \u2265 \u2016Uy\u20161 \u2212 \u03b1\u03b2 log(k) \u00b7 \u2016U(z \u2212 y)\u20161 by Corollary D.21\n\u2265 \u2016Uy\u20161 \u2212 \u03b1\u03b2 log(k) \u00b7 1\nkc \u2016Uy\u20161 by Equation (20)\n& \u2016Uy\u20161 by kc & \u03b1\u03b2 log(k) & \u2016Uz\u20161, by Equation (22)\nby rescaling z to w, we complete the proof.\nLemma D.23. Given matrix U \u2208 Rn\u00d7k, let t = O(k log k), and let S \u2208 Rt\u00d7n be a random matrix with entries drawn i.i.d. from a standard Cauchy distribution, where each entry is rescaled by \u0398(1/t). With probability .99,\n\u2200x \u2208 Rk, \u2016SUx\u20161 & \u2016Ux\u20161.\nProof. We can compute a well-conditioned basis for U , and denote it U \u2032. Then, \u2200x \u2208 Rk, there exists y \u2208 Rk such that Ux = U \u2032y. Due to Lemma D.22, with probability .99, we have\n\u2200y \u2208 Rk, \u2016SU \u2032y\u20161 & \u2016U \u2032y\u20161."}, {"heading": "D.6 Sparse Cauchy transform", "text": "This section presents the proof of two lemmas related to the sparse Cauchy transform. We first prove the no dilation result in Lemma D.24. Then we show how to get the no contraction result in Lemma D.26.\nLemma D.24. Given matrix A \u2208 Rn\u00d7d, let U\u2217, V \u2217 be the optimal solutions of minU,V \u2016UV \u2212A\u20161. Let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where S \u2208 Rm\u00d7n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, where C is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution, and \u03c3 is a scalar. Then\n\u2016\u03a0U\u2217V \u2217 \u2212\u03a0A\u20161 . \u03c3 \u00b7 log(md) \u00b7 \u2016U\u2217V \u2217 \u2212A\u20161\nholds with probability at least .999.\nProof. We define \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, as in the statement of the lemma. Then,\n\u2016\u03a0(U\u2217V \u2217 \u2212A)\u20161\n= d\u2211 i=1 \u2016SD(U\u2217V \u2217i \u2212Ai)\u20161 = d\u2211 i=1 \u2225\u2225\u2225\u2225  S11 S12 \u00b7 \u00b7 \u00b7 S1n S21 S22 \u00b7 \u00b7 \u00b7 S2n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Sm1 Sm2 \u00b7 \u00b7 \u00b7 Smn  \u00b7  c1 0 0 0 0 c2 0 0 0 0 \u00b7 \u00b7 \u00b7 0 0 0 0 cn  \u00b7 (U\u2217V \u2217i \u2212Ai)\u2225\u2225\u2225\u2225 1\n= d\u2211 i=1 \u2225\u2225\u2225\u2225  c1S11 c2S12 \u00b7 \u00b7 \u00b7 cnS1n c1S21 c2S22 \u00b7 \u00b7 \u00b7 cnS2n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 c1Sm1 c2Sm2 \u00b7 \u00b7 \u00b7 cnSmn  \u00b7 (U\u2217V \u2217i \u2212Ai)\u2225\u2225\u2225\u2225 1\n= d\u2211 i=1 \u2225\u2225\u2225\u2225 n\u2211 l=1 clS1l \u00b7 (U\u2217V \u2217i \u2212Ai)l, n\u2211 l=1 clS2l \u00b7 (U\u2217V \u2217i \u2212Ai)l, \u00b7 \u00b7 \u00b7 , n\u2211 l=1 clSml \u00b7 (U\u2217V \u2217i \u2212Ai)l \u2225\u2225\u2225\u2225 1\n= d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 clSjl \u00b7 (U\u2217V \u2217i \u2212Ai)l|\n= d\u2211 i=1 m\u2211 j=1 |w\u0303ij \u00b7 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l|| where w\u0303ij \u223c C(0, 1)\n= d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l| \u00b7 |w\u0303ij |\n= d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l| \u00b7 wi+(j\u22121)d where wi+(j\u22121)d \u223c |C(0, 1)|,\nwhere the last step follows since each wi can be thought of as a clipped half-Cauchy random variable. Define d\u2032 = md. Define event \u03bei to be the situation when wi < D (we will decide upon D later). Define event \u03be = \u03be1 \u2229 \u03be2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 \u03bed\u2032 . By choosing D = \u0398(d\u2032), we can conclude that,\nE[\u2016\u03a0U\u2217V \u2217 \u2212\u03a0A\u20161|\u03be] = d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l| \u00b7E[wi+(j\u22121)d|\u03be]\n. d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l| \u00b7 log(d\u2032)\n= d\u2211 i=1 n\u2211 l=1 |(U\u2217V \u2217i \u2212Ai)l|1 \u00b7 log(d\u2032) by m\u2211 j=1 |Sjl| = 1, \u2200l \u2208 [n] = log(d\u2032) \u00b7 \u2016U\u2217V \u2217 \u2212A\u20161.\nThus, we can show that\nPr[\u2016\u03a0U\u2217V \u2217 \u2212\u03a0A\u20161 . log(d\u2032)\u2016U\u2217V \u2217 \u2212A\u20161] \u2265 0.999.\nLemma D.25. Given any matrix M \u2208 Rn\u00d7d. Let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where S \u2208 Rm\u00d7n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, where C is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution, and \u03c3 is a scalar. Then\n\u2016\u03a0M\u20161 . \u03c3 \u00b7 log(md) \u00b7 \u2016M\u20161\nholds with probability at least .999.\nProof. Just replace the matrix U\u2217V \u2217 \u2212 A in the proof of Lemma D.24 with M . Then we can get the result directly.\nWe already provided the proof of Lemma D.24. It remains to prove Lemma D.26.\nLemma D.26. Given matrix A \u2208 Rn\u00d7d, let U\u2217, V \u2217 be the optimal solution of minU,V \u2016UV \u2212A\u20161. Let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where S \u2208 Rm\u00d7n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, and where C is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. Then with probability at least .999, for all V \u2208 Rk\u00d7d,\n\u2016\u03a0U\u2217V \u2212\u03a0A\u20161 \u2265 \u2016U\u2217V \u2212A\u20161 \u2212O(\u03c3 \u00b7 log(md))\u2016U\u2217V \u2217 \u2212A\u20161.\nNotice that m = O(k5 log5 k) and \u03c3 = O(k2 log2 k) according to Theorem 2 in [MM13].\nWe start by using Theorem 2 in [MM13] to generate the following Corollary.\nCorollary D.27. Given U \u2208 Rn\u00d7k with full column rank, let \u03a0 = \u03c3 \u00b7SC \u2208 Rm\u00d7n where S \u2208 Rm\u00d7n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and where C \u2208 Rn\u00d7n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. Then with probability .999, for all x \u2208 Rk, we have\n\u2016\u03a0Ux\u20161 \u2265 \u2016Ux\u20161.\nNotice that m = O(k5 log5 k) and \u03c3 = O(k2 log2 k) according to Theorem 2 in [MM13].\nWe give the proof of Lemma D.26.\nProof. Follows by Corollary D.27, Lemma D.24, Lemma D.7.\nD.7 `1-Lewis weights\nIn this section we show how to use Lewis weights to get a better dilation bound. The goal of this section is to prove Lemma D.32 and Lemma D.31. Notice that our algorithms in Section C use Lewis weights in several different ways. The first way is using Lewis weights to show an existence result, which means we only need an existential result for Lewis weights. The second way is only guessing the nonzero locations on the diagonal of a sampling and rescaling matrix according to the Lewis weights. The third way is guessing the values on the diagonal of a sampling and rescaling matrix according to the Lewis weights. The fourth way is computing the Lewis weights for a known low dimensional matrix(n\u00d7poly(k) or poly(k)\u00d7d). We usually do not need to optimize the running time of computing Lewis weights for a low-rank matrix to have input-sparsity time.\nClaim D.28. Given matrix A \u2208 Rn\u00d7d, let B = U\u2217V \u2217\u2212A. For any distribution p = (p1, p2, . . . , pn) define random variable X such that X = \u2016Bi\u20161/pi with probability pi. Then take any m independent samples X1, X2, . . . , Xm, let Y = 1m \u2211m j=1X j. We have\nPr[Y \u2264 1000\u2016B\u20161] \u2265 .999\nProof. We can compute the expectation of Xj , for any j \u2208 [m]\nE[Xj ] = n\u2211 i=1 \u2016Bi\u20161 pi \u00b7 pi = \u2016B\u20161.\nThen, E[Y ] = 1m \u2211m j=1 E[X j ] = \u2016B\u20161. Using Markov\u2019s inequality, we have\nPr[Y \u2265 1000\u2016B\u20161] \u2264 .001\nLemma D.29. Given matrix M \u2208 Rn\u00d7d, let S \u2208 Rn\u00d7n be any sampling and rescaling diagonal matrix. Then with probability at least .999,\n\u2016SM\u20161 . \u2016M\u20161.\nProof. Just replace the matrix B in the proof of Claim D.28 with M . Then we can get the result directly.\nUsing Theorem 1.1 of [CP15], we have the following result,\nClaim D.30. Given matrix A \u2208 Rn\u00d7d, for any fixed U\u2217 \u2208 Rn\u00d7k and V \u2217 \u2208 Rk\u00d7d, choose D \u2208 Rn\u00d7n to be the sampling and rescaling diagonal matrix with m = O(k log k) nonzeros according to the Lewis weights of U\u2217. Then with probability .999, for all V ,\n\u2016U\u2217V \u2217 \u2212 U\u2217V \u20161 \u2264 \u2016DU\u2217V \u2217 \u2212DU\u2217V \u20161 . \u2016U\u2217V \u2217 \u2212 U\u2217V \u20161.\nLemma D.31. Given matrix A \u2208 Rn\u00d7d, U\u2217 \u2208 Rn\u00d7k, define V \u2217 \u2208 Rk\u00d7d to be the optimal solution of min\nV \u2208Rk\u00d7d \u2016U\u2217V \u2212A\u20161. Choose a sampling and rescaling diagonal matrix D \u2208 Rn\u00d7n with m = O(k log k) non-zero entries according to the Lewis weights of U\u2217. Then with probability at least .99, we have: for all V \u2208 Rk\u00d7d,\n\u2016DU\u2217V \u2212DA\u20161 . \u2016U\u2217V \u2217 \u2212 U\u2217V \u20161 +O(1)\u2016U\u2217V \u2217 \u2212A\u20161 . \u2016U\u2217V \u2212A\u20161,\nholds with probability at least .99.\nProof. Using the above two claims, we have with probability at least .99, for all V \u2208 Rk\u00d7d,\n\u2016DU\u2217V \u2212DA\u20161 \u2264 \u2016DU\u2217V \u2212DU\u2217V \u2217\u20161 + \u2016DU\u2217V \u2217 \u2212DA\u20161 by triangle inequality . \u2016DU\u2217V \u2212DU\u2217V \u2217\u20161 +O(1)\u2016U\u2217V \u2217 \u2212A\u20161 by Claim D.28 . \u2016U\u2217V \u2212 U\u2217V \u2217\u20161 +O(1)\u2016U\u2217V \u2217 \u2212A\u20161 by Claim D.30 \u2264 \u2016U\u2217V \u2212A\u20161 + \u2016U\u2217V \u2217 \u2212A\u20161 +O(1)\u2016U\u2217V \u2217 \u2212A\u20161 by triangle inequality . \u2016U\u2217V \u2212A\u20161.\nLemma D.32. Given matrix A \u2208 Rn\u00d7d, define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be the optimal solution of min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u20161. Choose a sampling and rescaling diagonal matrix D \u2208 Rn\u00d7n with\nm = O(k log k) non-zero entries according to the Lewis weights of U\u2217. For all V \u2208 Rk\u00d7d we have\n\u2016U\u2217V \u2212A\u20161 . \u2016DU\u2217V \u2212DA\u20161 +O(1)\u2016U\u2217V \u2217 \u2212A\u20161,\nholds with probability at least .99.\nProof. Follows by Claim D.28, Lemma D.31, Lemma D.7.\nE `p-Low Rank Approximation\nThis section presents some fundamental lemmas for `p-low rank approximation problems. Using these lemmas, all the algorithms described for `1-low rank approximation problems can be extended to `p-low rank approximation directly. We only state the important Lemmas in this section, due to most of the proofs in this section being identical to the proofs in Section D."}, {"heading": "E.1 Definitions", "text": "This section is just a generalization of Section D.1 to the `p setting when 1 < p < 2.\nDefinition E.1. Given a matrix M \u2208 Rn\u00d7d, if matrix S \u2208 Rm\u00d7n satisfies\n\u2016SM\u2016pp \u2264 c1\u2016M\u2016pp,\nthen S has at most c1-dilation on M .\nDefinition E.2. Given a matrix U \u2208 Rn\u00d7k, if matrix S \u2208 Rm\u00d7n satisfies\n\u2200x \u2208 Rk, \u2016SUx\u2016pp \u2265 1\nc2 \u2016Ux\u2016pp,\nthen S has at most c2-contraction on U .\nDefinition E.3. Given matrices U \u2208 Rn\u00d7k, A \u2208 Rn\u00d7d, denote V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212A\u2016 p p. If matrix S \u2208 Rm\u00d7n satisfies\n\u2200V \u2208 Rk\u00d7d, \u2016SUV \u2212 SA\u2016pp \u2265 1 c3 \u2016UV \u2212A\u2016pp \u2212 c4\u2016UV \u2217 \u2212A\u2016pp,\nthen S has at most (c3, c4)-contraction on (U,A).\nDefinition E.4. A (c5, c6) `p-subspace embedding for the column space of an n \u00d7 k matrix U is a matrix S \u2208 Rm\u00d7n for which all x \u2208 Rk\n1 c5 \u2016Ux\u2016pp \u2264 \u2016SUx\u2016pp \u2264 c6\u2016Ux\u2016pp.\nDefinition E.5. Given matrices U \u2208 Rn\u00d7k, A \u2208 Rn\u00d7d, denote V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u2016 p p. Let S \u2208 Rm\u00d7n. If for all c \u2265 1, and if for any V\u0302 \u2208 Rk\u00d7d which satisfies\n\u2016SUV\u0302 \u2212 SA\u2016pp \u2264 c \u00b7 min V \u2208Rk\u00d7d \u2016SUV \u2212 SA\u2016pp,\nit holds that\n\u2016UV\u0302 \u2212A\u2016pp \u2264 c \u00b7 c7 \u00b7 \u2016UV \u2217 \u2212A\u2016pp,\nthen S provides a c7-multiple-regression-cost preserving sketch of (U,A).\nDefinition E.6. Given matrices L \u2208 Rn\u00d7m1 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, k \u2265 1, let\nX\u2217 = arg min rank\u2212k X \u2016LXN \u2212A\u2016pp.\nLet S \u2208 Rm\u00d7n. If for all c \u2265 1, and if for any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016SLX\u0302N \u2212 SA\u2016pp \u2264 c \u00b7 min rank\u2212k X \u2016SLXN \u2212 SA\u2016pp,\nit holds that\n\u2016LX\u0302N \u2212A\u2016pp \u2264 c \u00b7 c8 \u00b7 \u2016LX\u2217N \u2212A\u2016pp,\nthen S provides a c8-restricted-multiple-regression-cost preserving sketch of (L,N,A, k)."}, {"heading": "E.2 Properties", "text": "Lemma E.7. Given matrices A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, let V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u2016 p p. If S \u2208 Rm\u00d7n has at most c1-dilation on UV \u2217 \u2212A, i.e.,\n\u2016S(UV \u2217 \u2212A)\u2016pp \u2264 c1\u2016UV \u2217 \u2212A\u2016pp,\nand it has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rk, \u2016SUx\u2016pp \u2265 1\nc2 \u2016Ux\u2016pp,\nthen S has at most (22p\u22122c2, c1 + 21\u2212p 1c2 )-contraction on (U,A), i.e.,\n\u2200V \u2208 Rk\u00d7d, \u2016SUV \u2212 SA\u2016pp \u2265 1\nc2 \u2016UV \u2212A\u2016pp \u2212 (c1 +\n1 c2 )\u2016UV \u2217 \u2212A\u2016pp,\nProof. Let A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, and S \u2208 Rm\u00d7n be the same as that described in the lemma. Then\n\u2200V \u2208 Rk\u00d7d\n\u2016SUV \u2212 SA\u2016pp \u2265 21\u2212p\u2016SUV \u2212 SUV \u2217\u2016pp \u2212 \u2016SUV \u2217 \u2212 SA\u2016pp \u2265 21\u2212p\u2016SUV \u2212 SUV \u2217\u2016pp \u2212 c1\u2016UV \u2217 \u2212A\u2016pp = 21\u2212p\u2016SU(V \u2212 V \u2217)\u2016pp \u2212 c1\u2016UV \u2217 \u2212A\u2016pp\n= 21\u2212p d\u2211 j=1 \u2016SU(V \u2212 V \u2217)j\u2016pp \u2212 c1\u2016UV \u2217 \u2212A\u2016pp \u2265 21\u2212p d\u2211 j=1 1 c2 \u2016U(V \u2212 V \u2217)j\u2016pp \u2212 c1\u2016UV \u2217 \u2212A\u2016pp\n= 21\u2212p 1\nc2 \u2016UV \u2212 UV \u2217\u2016pp \u2212 c1\u2016UV \u2217 \u2212A\u2016pp\n\u2265 22\u22122p 1 c2 \u2016UV \u2212A\u2016pp \u2212 21\u2212p 1 c2 \u2016UV \u2217 \u2212A\u2016pp \u2212 c1\u2016UV \u2217 \u2212A\u2016pp = 22\u22122p 1\nc2 \u2016UV \u2212A\u2016pp \u2212\n( (21\u2212p 1\nc2 + c1)\u2016UV \u2217 \u2212A\u2016pp\n) .\nThe first inequality follows by Fact E.14. The second inequality follows since S has at most c1 dilation on UV \u2217 \u2212 A. The third inequality follows since S has at most c2 contraction on U . The fourth inequality follows by Fact E.14.\nLemma E.8. Given matrices A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, let V \u2217 = arg minV \u2208Rk\u00d7d \u2016UV \u2212 A\u2016 p p. If S \u2208 Rm\u00d7n has at most c1-dilation on UV \u2217 \u2212A, i.e.,\n\u2016S(UV \u2217 \u2212A)\u2016pp \u2264 c1\u2016UV \u2217 \u2212A\u2016pp,\nand has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rk, \u2016SUx\u2016pp \u2265 1\nc2 \u2016Ux\u2016pp,\nthen S provides a 2p\u22121(2c1c2 + 1)-multiple-regression-cost preserving sketch of (U,A), i.e., for all c \u2265 1, for any V\u0302 \u2208 Rk\u00d7d which satisfies\n\u2016SUV\u0302 \u2212 SA\u2016pp \u2264 c \u00b7 min V \u2208Rk\u00d7d \u2016SUV \u2212 SA\u2016pp,\nit has\n\u2016UV\u0302 \u2212A\u2016pp \u2264 c \u00b7 2p\u22121(2c1c2 + 1) \u00b7 \u2016UV \u2217 \u2212A\u2016pp,\nProof. Let S \u2208 Rm\u00d7n, A \u2208 Rn\u00d7d, U \u2208 Rn\u00d7k, V \u2217, V\u0302 \u2208 Rk\u00d7d, and c be the same as stated in the lemma.\n\u2016UV\u0302 \u2212A\u2016pp \u2264 22p\u22122c2\u2016SUV\u0302 \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016UV \u2217 \u2212A\u2016pp \u2264 22p\u22122c2c min\nV \u2208Rk\u00d7d \u2016SUV \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016UV \u2217 \u2212A\u2016pp\n\u2264 22p\u22122c2c\u2016SUV \u2217 \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016UV \u2217 \u2212A\u2016pp \u2264 22p\u22122c1c2c\u2016UV \u2217 \u2212A\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016UV \u2217 \u2212A\u2016pp \u2264 c \u00b7 2p\u22121(1 + 2c1c2)\u2016UV \u2217 \u2212A\u2016pp.\nThe first inequality follows by Lemma E.7. The second inequality follows by the guarantee of V\u0302 . The fourth inequality follows since S has at most c1-dilation on UV \u2217 \u2212 A. The fifth inequality follows since c \u2265 1.\nLemma E.9. Given matrices L \u2208 Rn\u00d7m1 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, k \u2265 1, let\nX\u2217 = arg min rank\u2212k X \u2016LXN \u2212A\u2016pp.\nIf S \u2208 Rm\u00d7n has at most c1-dilation on LX\u2217N \u2212A, i.e.,\n\u2016S(LX\u2217N \u2212A)\u2016pp \u2264 c1\u2016LX\u2217N \u2212A\u2016pp,\nand has at most c2-contraction on L, i.e.,\n\u2200x \u2208 Rm1\u2016SLx\u2016pp \u2265 \u2016Lx\u2016pp,\nthen S provides a 2p\u22121(2c1c2+1)-restricted-multiple-regression-cost preserving sketch of (L,N,A, k), i.e., for all c \u2265 1, for any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016SLX\u0302N \u2212 SA\u2016pp \u2264 c \u00b7 min rank\u2212k X \u2016SLXN \u2212 SA\u2016pp,\nit has\n\u2016LX\u0302N \u2212A\u2016pp \u2264 c \u00b7 2p\u22121(2c1c2 + 1) \u00b7 \u2016LX\u2217N \u2212A\u2016pp.\nProof. Let S \u2208 Rm\u00d7n, L \u2208 Rn\u00d7m1 , X\u0302 \u2208 Rm1\u00d7m2 , X\u2217 \u2208 Rm1\u00d7m2 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d and c \u2265 1 be the same as stated in the lemma.\n\u2016SLX\u0302N \u2212 SA\u2016pp \u2265 21\u2212p\u2016SLX\u0302N \u2212 SLX\u2217N\u2016pp \u2212 \u2016SLX\u2217N \u2212 SA\u2016pp\n\u2265 21\u2212p 1 c2 \u2016L(X\u0302N \u2212X\u2217N)\u2016pp \u2212 c1\u2016LX\u2217N \u2212A\u2016pp \u2265 22\u22122p 1 c2 \u2016LX\u0302N \u2212A\u2016pp \u2212 21\u2212p 1 c2 \u2016LX\u2217N \u2212A\u20161 \u2212 c1\u2016LX\u2217N \u2212A\u2016pp = 22\u22122p 1\nc2 \u2016LX\u0302N \u2212A\u2016pp \u2212 (21\u2212p\n1\nc2 + c1)\u2016LX\u2217N \u2212A\u2016pp.\nThe inequality follows from the Fact E.14. The second inequality follows since S has at most c2contraction on L, and it has at most c1-dilation on LX\u2217N \u2212 A. The third inequality follows by Fact E.14.\nIt follows that\n\u2016LX\u0302N \u2212A\u2016pp \u2264 22p\u22122c2\u2016SLX\u0302N \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016LX\u2217N \u2212A\u2016pp \u2264 22p\u22122c2c \u00b7 min\nrank\u2212k X \u2016SLXN \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016LX\u2217N \u2212A\u2016pp\n\u2264 22p\u22122c2c \u00b7 \u2016SLX\u2217N \u2212 SA\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016LX\u2217N \u2212A\u2016pp \u2264 22p\u22122cc1c2 \u00b7 \u2016LX\u2217N \u2212A\u2016pp + (2p\u22121 + 22p\u22122c1c2)\u2016LX\u2217N \u2212A\u2016pp \u2264 c \u00b7 2p\u22121(1 + 2c1c2)\u2016LX\u2217N \u2212A\u2016pp.\nThe first inequality directly follows from the previous one. The second inequality follows from the guarantee of X\u0302. The fourth inequality follows since S has at most c1 dilation on LX\u2217N \u2212 A. The fifth inequality follows since c \u2265 1.\nLemma E.10. Given matrices L \u2208 Rn\u00d7m1 , N \u2208 Rm2\u00d7d, A \u2208 Rn\u00d7d, k \u2265 1, let\nX\u2217 = arg min rank\u2212k X \u2016LXN \u2212A\u2016pp.\nLet T1 \u2208 Rt1\u00d7n have at most c1-dilation on LX\u2217N \u2212A, and have at most c2-contraction on L. Let\nX\u0303 = arg min rank\u2212k X\n\u2016T1LXN \u2212 T1A\u2016pp.\nLet T>2 \u2208 Rt2\u00d7d have at most c\u20321-dilation on (T1LX\u0303N \u2212 T1A)>, and at most c\u20322-contraction on N>. Then, for all c \u2265 1, for any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016T1(LX\u0302N \u2212 SA)T2\u2016pp \u2264 c \u00b7 min rank\u2212k X \u2016T1(LXN \u2212A)T2\u2016pp,\nit holds that\n\u2016LX\u0302N \u2212A\u2016pp \u2264 c \u00b7 22p\u22122(2c1c2 + 1)(2c\u20321c\u20322 + 1) \u00b7 \u2016LX\u2217N \u2212A\u2016pp.\nProof. Apply Lemma D.9 for sketching matrix T2. Then for any c \u2265 1, any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016T1(LX\u0302N \u2212A)T2\u2016pp \u2264 c \u00b7 min rank\u2212k X \u2016T1(LXN \u2212A)T2\u2016pp,\nit has\n\u2016T1(LX\u0302N \u2212A)\u2016pp \u2264 c \u00b7 2p\u22121(2c\u20321c\u20322 + 1) \u00b7 \u2016T1(LX\u0303N \u2212A)\u2016pp.\nApply Lemma D.9 for sketch matrix T1. Then for any c \u2265 1, any rank\u2212k X\u0302 \u2208 Rm1\u00d7m2 which satisfies\n\u2016T1(LX\u0302N \u2212A)\u2016pp \u2264 c2p\u22121(2c\u20321c\u20322 + 1) \u00b7 min rank\u2212k X \u2016T1(LX\u0303N \u2212A)\u2016pp,\nit has\n\u2016LX\u0302N \u2212A\u2016pp \u2264 c \u00b7 22p\u22122(2c1c2 + 1)(2c\u20321c\u20322 + 1) \u00b7 \u2016LX\u2217N \u2212A\u2016pp.\nLemma E.11. Given matrices M \u2208 Rn\u00d7d, U \u2208 Rn\u00d7t, d \u2265 t = rank(U), n \u2265 d \u2265 r = rank(M). If sketching matrix S \u2208 Rm\u00d7n is drawn from any of the following probability distributions on matrices, with .99 probability, S has at most c1-dilation on M , i.e.,\n\u2016SM\u2016pp \u2264 c1\u2016M\u2016pp,\nand S has at most c2-contraction on U , i.e.,\n\u2200x \u2208 Rt, \u2016SUx\u2016pp \u2265 1\nc2 \u2016Ux\u2016pp,\nwhere c1, c2 are parameters depend on the distribution over S.\n(I) S \u2208 Rm\u00d7n is a dense matrix with entries drawn from a p-stable distribution: a matrix with i.i.d. standard p-stable random variables. If m = O(t log t), then c1c2 = O(log d).\n(II) S \u2208 Rm\u00d7n is a sparse matrix with some entries drawn from a p-stable distribution: S = TD, where T \u2208 Rm\u00d7n has each column drawn i.i.d. from the uniform distribution over standard basis vectors of Rm, and D \u2208 Rn\u00d7n is a diagonal matrix with each diagonal entry drawn from i.i.d. from the standard p-stable distribution. If m = O(t5 log5 t), then c1c2 = O(t2/p log2/p t log d). If m = O((t+ r)5 log5(t+ r)), then c1c2 = O(min(t 2/p log2/p t log d, r3/p log3/p r)).\n(III) S \u2208 Rm\u00d7n is a sampling and rescaling matrix (notation S \u2208 Rn\u00d7n denotes a diagonal sampling and rescaling matrix with m non-zero entries): If S samples and reweights m = O(t log t log log t) rows of U , selecting each with probability proportional to the ith row\u2019s `p Lewis weight and reweighting by the inverse probability, then c1c2 = O(1).\nIn the above, if we replace S with \u03c3 \u00b7 S where \u03c3 \u2208 R\\{0} is any scalar, then the relation between m and c1c2 can be preserved.\nFor (I), it is implied by Lemma E.17, Lemma E.19. Also see from [SW11]1. For (II), if m = O(t5 log5 t), then c1c2 = O(t2/p log2/p t log d) is implied by Corollary D.27 and Lemma E.20 and Theorem 4 in [MM13]. If m = O((t + r)5 log5(t + r)), c1c2 = O(r3/p log3/p r) is implied by [MM13].\nFor (III), it is implied by [CP15] and Lemma D.29."}, {"heading": "E.3 Tools and inequalities", "text": "Lemma E.12 (Lemma 9 in [MM13], Upper Tail Inequality for p-stable Distributions). Let p \u2208 (1, 2) and m \u2265 3. \u2200i \u2208 [m], let Xi be m(not necessarily independent) random variables sampled from Dp, and let \u03b3i > 0 with \u03b3 = \u2211m i=1 \u03b3i. Let X = \u2211m i=1 \u03b3i|Xi|p. Then for any t \u2265 1,\nPr[X \u2265 t\u03b1p\u03b3] \u2264 2 log(mt)\nt .\nWe first review some facts about the p-norm and q-norm,\nFact E.13. For any p \u2265 q > 0 and any x \u2208 Rk,\n\u2016x\u2016p \u2264 \u2016x\u2016q \u2264 k 1 q \u2212 1 p \u2016x\u2016p.\nWe provide the triangle inequality for the p-norm,\nFact E.14. For any p \u2208 (1, 2), for any x, y \u2208 Rk,\n\u2016x+ y\u2016p \u2264 \u2016x\u2016p + \u2016y\u2016p, and \u2016x+ y\u2016pp \u2264 2p\u22121(\u2016x\u2016pp + \u2016y\u2016pp).\nFact E.15 (H\u00f6lder\u2019s inequality). For any x, y \u2208 Rk, if 1p + 1 q = 1, then |x >y| \u2264 \u2016x\u2016p\u2016y\u2016q.\nWe give the definition of a well-conditioned basis for `p,\nDefinition E.16. Let p \u2208 (1, 2). A basis U for the range of A is (\u03b1, \u03b2, p)-conditioned if \u2016U\u2016p \u2264 \u03b1 and for all x \u2208 Rk, \u2016x\u2016q \u2264 \u03b2\u2016Ux\u2016p. We will say U is well-conditioned if \u03b1 and \u03b2 are low-degree polynomials in k, independent of n.\n1Full version.\nProof. We first show an upper bound,\n\u2016Ux\u2016p \u2264 \u2016U\u2016p \u00b7 \u2016x\u2016p \u2264 \u03b1\u2016x\u2016p\nThen we show a lower bound,\n\u2016Ux\u2016p \u2265 1\n\u03b2 \u2016x\u2016q\nFor any p and q with 1/p+ 1/q = 1, by H\u00f6lder\u2019s inequality we have\n|x>y| \u2264 \u2016x\u2016p \u00b7 \u2016y\u2016q\nchoosing y to be the vector that has 1 everywhere, \u2016x\u20161 \u2264 \u2016x\u2016pk1/q\nE.4 Dense p-stable transform\nThis section states the main tools for the dense p-stable transform. The proof is identical to that for the dense Cauchy transform.\nLemma E.17. Given matrix A \u2208 Rn\u00d7d and p \u2208 (1, 2), define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be an optimal solution of min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u2016p. Choose a p-stable distribution matrix S \u2208 Rm\u00d7n,\nrescaled by \u0398(1/m1/p). Then we have\n\u2016SU\u2217V \u2217 \u2212 SA\u2016pp . log(md)\u2016U\u2217V \u2217 \u2212A\u2016pp\nwith probability at least 99/100.\nProof. Let P (0, 1) denote the p-stable distribution. Then,\n\u2016SU\u2217V \u2217 \u2212 SA\u2016pp \u2264 d\u2211 i=1 \u2016S(U\u2217V \u2217i \u2212Ai)\u2016pp\n= d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 1 m Sj,l(U \u2217V \u2217i \u2212Ai)l|p where Sj,l \u223c P (0, 1)\n= 1\nm d\u2211 i=1 m\u2211 j=1 |w\u0303ij( n\u2211 l=1 |(U\u2217V \u2217i \u2212Ai)l|p)1/p|p where w\u0303ij \u223c P (0, 1)\n= 1\nm d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |(U\u2217V \u2217i \u2212Ai)l|p \u00b7 |w\u0303ij |p\n= 1\nm d\u2211 i=1 m\u2211 j=1 \u2016U\u2217V \u2217i \u2212Ai\u2016pp \u00b7 w p i+(j\u22121)d, where wi+(j\u22121)d \u223c |P (0, 1)|\nwhere the last step follows since each wi can be thought of as a clipped half-p-stable random variable. Define X to be \u2211d i=1 \u2211m j=1 \u2016U\u2217V \u2217i \u2212Ai\u2016 p p \u00b7 wpi+(j\u22121)d and \u03b3 to be \u2211d i=1 \u2211m j=1 \u2016U\u2217V \u2217i \u2212Ai\u2016 p p. Then applying Lemma E.12,\nPr[X \u2265 t\u03b1p\u03b3] \u2264 2 log(mdt)\nt .\nChoosing t = \u0398(log(md)), we have with probability .999,\nX . log(md)\u03b1p\u03b3 = log(md)\u03b1p d\u2211 i=1 \u2016U\u2217V \u2217i \u2212Ai\u2016pp,\nwhere the last steps follows by definition of \u03b3. Thus, we can conclude that with probability .999, \u2016\u03a0(U\u2217V \u2217 \u2212A)\u2016pp . log(md)\u2016U\u2217V \u2217 \u2212A\u2016pp.\nLemma E.18. Given matrix A \u2208 Rn\u00d7d and p \u2208 (1, 2), define U\u2217 \u2208 Rn\u00d7k to be the optimal solution of min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u2016p. Choose a matrix of i.i.d. p-stable random variables S \u2208 Rm\u00d7n. Then\nfor all V \u2208 Rk\u00d7n, we have\n\u2016SU\u2217V \u2212 SA\u2016pp & \u2016U\u2217V \u2212A\u2016pp \u2212O(log(md))\u2016U\u2217V \u2217 \u2212A\u2016pp.\nLemma E.19. Let p \u2208 (1, 2). Given an (\u03b1, \u03b2) `p well-conditioned basis, condition on the following two events,\n1. For all x \u2208 N , \u2016SUx\u2016p & \u2016Ux\u2016p. 2. For all x \u2208 Rk, \u2016SUx\u2016p \u2264 poly(k)\u2016Ux\u2016p. Then for all x \u2208 Rk, \u2016SUx\u2016p & \u2016Ux\u2016p.\nProof. The proof is identical to Lemma D.22 in Section D.\nE.5 Sparse p-stable transform\nThis section states the main tools for the sparse p-stable transform. The proof is identical to that of the sparse Cauchy transform.\nLemma E.20. Let p \u2208 (1, 2). Given matrix A \u2208 Rn\u00d7d with U\u2217, V \u2217 an optimal solution of minU,V \u2016UV \u2212 A\u2016p, let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where S \u2208 Rm\u00d7n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, where C is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution, and \u03c3 is a scalar. Then\n\u2016\u03a0U\u2217V \u2217 \u2212\u03a0A\u2016pp . \u03c3 \u00b7 log(md) \u00b7 \u2016U\u2217V \u2217 \u2212A\u2016pp\nholds with probability at least .999.\nProof. We define \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n as in the statement of the lemma. Then by the definition of\n\u03a0, we have,\n\u2016\u03a0(U\u2217V \u2217 \u2212A)\u2016pp\n= d\u2211 i=1 \u2016SC(U\u2217V \u2217i \u2212Ai)\u2016pp\n= d\u2211 i=1 \u2225\u2225\u2225\u2225  S11 S12 \u00b7 \u00b7 \u00b7 S1n S21 S22 \u00b7 \u00b7 \u00b7 S2n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Sm1 Sm2 \u00b7 \u00b7 \u00b7 Smn  \u00b7  c1 0 0 0 0 c2 0 0 0 0 \u00b7 \u00b7 \u00b7 0 0 0 0 cn  \u00b7 (U\u2217V \u2217i \u2212Ai)\u2225\u2225\u2225\u2225p p = d\u2211 i=1 \u2225\u2225\u2225\u2225  c1S11 c2S12 \u00b7 \u00b7 \u00b7 cnS1n c1S21 c2S22 \u00b7 \u00b7 \u00b7 cnS2n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 c1Sm1 c2Sm2 \u00b7 \u00b7 \u00b7 cnSmn  \u00b7 (U\u2217V \u2217i \u2212Ai)\u2225\u2225\u2225\u2225p p\n= d\u2211 i=1 \u2225\u2225\u2225\u2225 n\u2211 l=1 clS1l \u00b7 (U\u2217V \u2217i \u2212Ai)l, n\u2211 l=1 clS2l \u00b7 (U\u2217V \u2217i \u2212Ai)l, \u00b7 \u00b7 \u00b7 , n\u2211 l=1 clSml \u00b7 (U\u2217V \u2217i \u2212Ai)l \u2225\u2225\u2225\u2225p p\n= d\u2211 i=1 m\u2211 j=1 \u2223\u2223\u2223\u2223 n\u2211 l=1 clSjl \u00b7 (U\u2217V \u2217i \u2212Ai)l \u2223\u2223\u2223\u2223p by aX + bY and (|a|p + |b|p)1/pZ are indentically distributed\n= d\u2211 i=1 m\u2211 j=1 \u2223\u2223\u2223\u2223w\u0303ij \u00b7 ( n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l|p)1/p \u2223\u2223\u2223\u2223p where w\u0303ij \u223c P (0, 1)\n= d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l|p \u00b7 |w\u0303ij |p\n= d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l|p \u00b7 w p i+(j\u22121)d, where wi+(j\u22121)d \u223c |P (0, 1)|\nwhere the last step follows since each wi can be thought of as a clipped half-p-stable random variable. DefineX to be \u2211d i=1 \u2211m j=1 \u2211n l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l|p\u00b7w p i+(j\u22121)d and \u03b3 to be \u2211d i=1 \u2211m j=1 \u2211n l=1 |Sjl(U\u2217V \u2217i \u2212 Ai)l|p. Then applying Lemma E.12,\nPr[X \u2265 t\u03b1p\u03b3] \u2264 2 log(mdt)\nt .\nChoosing t = \u0398(log(md)), we have with probability .999,\nX . log(md)\u03b1p\u03b3 = log(md)\u03b1p d\u2211 i=1 n\u2211 l=1 |(U\u2217V \u2217i \u2212Ai)l|p,\nwhere the last steps follows by\n\u03b3 = d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl(U\u2217V \u2217i \u2212Ai)l|p = d\u2211 i=1 m\u2211 j=1 n\u2211 l=1 |Sjl|p|(U\u2217V \u2217i \u2212Ai)l|p = d\u2211 i=1 n\u2211 l=1 |(U\u2217V \u2217i \u2212Ai)l|p.\nThus, we can conclude that with probability .999, \u2016\u03a0(U\u2217V \u2217 \u2212A)\u2016pp . log(md)\u2016U\u2217V \u2217 \u2212A\u2016pp.\nLemma E.21. Given matrix A \u2208 Rn\u00d7d with U\u2217, V \u2217 an optimal solution of minU,V \u2016UV \u2212A\u2016p, let \u03a0 = \u03c3 \u00b7 SC \u2208 Rm\u00d7n, where S \u2208 Rm\u00d7n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, and where C is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. Then with probability at least .999, for all V \u2208 Rk\u00d7d,\n\u2016\u03a0U\u2217V \u2212\u03a0A\u2016pp \u2265 \u2016U\u2217V \u2212A\u2016pp \u2212O(\u03c3 log(md))\u2016U\u2217V \u2217 \u2212A\u2016pp.\nNotice that m = O(k5 log5 k) and \u03c3 = O((k log k)2/p) according to Theorem 4 in [MM13].\nE.6 `p-Lewis weights\nThis section states the main tools for `p-Lewis weights. The proof is identical to `1-Lewis weights.\nLemma E.22. For any p \u2208 (1, 2). Given matrix A \u2208 Rn\u00d7d, define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be an optimal solution of min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u2016p. Choose a diagonal matrix D \u2208 Rn\u00d7n according to\nthe Lewis weights of U\u2217. We have that\n\u2016DU\u2217V \u2217 \u2212DA\u2016pp . \u2016U\u2217V \u2217 \u2212A\u2016pp,\nholds with probability at least .99.\nLemma E.23. Let p \u2208 (1, 2). Given matrix A \u2208 Rn\u00d7d, define U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d to be an optimal solution of min\nU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u2016p. Choose a sampling and rescaling matrix D \u2208 Rn\u00d7n\naccording to the Lewis weights of U\u2217. For all V \u2208 Rk\u00d7d we have\n\u2016DU\u2217V \u2212DA\u2016pp & \u2016U\u2217V \u2212A\u2016pp \u2212O(1)\u2016U\u2217V \u2217 \u2212A\u2016pp,\nholds with probability at least .99.\nF EMD-Low Rank Approximation\nIn this section we explain how to embed EMD to `1. For more detailed background on the EarthMover Distance(EMD) problem, we refer the reader to [IT03, AIK08, ABIW09, IP11, BIRW16] and [SL09, LOG16]. Section F.1 introduces some necessary notation and definitions for Earth-Mover Distance. Section F.2 presents the main result for the Earth-Mover distance low rank approximation problem."}, {"heading": "F.1 Definitions", "text": "Consider any two non-negative vectors x, y \u2208 R[\u2206] 2\n+ such that \u2016x\u20161 = \u2016y\u20161. Let \u0393(x, y) be the set of functions \u03b3 : [\u2206]2 \u00d7 [\u2206]2 \u2192 R+, such that for any i, j \u2208 [\u2206]2 we have \u2211 l \u03b3(i, l) = xi and\u2211\nl \u03b3(l, j) = yj ; that is, \u0393 is the set of possible \u201cflows\u201d from x to y. Then we define\nEMD(x, y) = min \u03b3\u2208\u0393 \u2211 i,j\u2208[\u2206]2 \u03b3(i, j)\u2016i\u2212 j\u20161\nto be the min cost flow from x to y, where the cost of an edge is its `1 distance.\nUsing the EMD(\u00b7, \u00b7) metric, for general vectors w, we define \u2016 \u00b7 \u2016EEMD distance (which is the same as [SL09]),\n\u2016w\u2016EEMD = min x\u2212y+z=w \u2016x\u20161=\u2016y\u20161 x,y\u22650 EMD(x, y) + 2\u2206\u2016z\u20161.\nUsing \u2016 \u00b7 \u2016EEMD distance, for general matrices X \u2208 Rn\u00d7d, we define the \u2016 \u00b7 \u20161,EEMD distance,\n\u2016X\u20161,EEMD = d\u2211 i=1 \u2016Xi\u2016EEMD,\nwhere Xi denotes the j-th column of matrix X.\nF.2 Analysis of no contraction and no dilation bound\nLemma F.1. Given matrix A \u2208 Rn\u00d7d and U\u2217, V \u2217 = arg min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u20161,EEMD, there exist sketching matrices S \u2208 Rm\u00d7n such that, with probability .999, for all V \u2208 Rk\u00d7d,\n\u2016S(U\u2217V \u2212A)\u20161 \u2265 \u2016U\u2217V \u2212A\u20161,EEMD holds.\nProof. Using Lemma 1 in [IT03], there exists a constant C > 0 such that for all i \u2208 [d],\nC\u2016SU\u2217Vi \u2212 SAi\u20161 \u2265 \u2016U\u2217Vi \u2212Ai\u2016EEMD. (23)\nThen taking a summation over all d terms and rescaling the matrix S, we obtain, d\u2211 i=1 \u2016S(U\u2217Vi \u2212Ai)\u20161 \u2265 \u2016U\u2217Vi \u2212Ai\u2016EEMD\nwhich completes the proof.\nLemma F.2. Given matrix A \u2208 Rn\u00d7d and U\u2217, V \u2217 = arg min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u20161,EEMD, there exist sketching matrices S \u2208 Rm\u00d7n such that\n\u2016S(U\u2217V \u2217 \u2212A)\u20161 \u2264 O(log n)\u2016U\u2217V \u2217 \u2212A\u20161,EEMD holds with probability at least .999.\nProof. Using Lemma 2 in [IT03], we have for any i \u2208 [d],\nE[\u2016SU\u2217V \u2217i \u2212 SAi\u20161] \u2264 O(log n)\u2016U\u2217V \u2217i \u2212Ai\u2016EEMD. (24)\nThen using that the expectation is linear, we have\nE[\u2016SU\u2217V \u2217 \u2212 SA\u20161] = E[ d\u2211 i=1 \u2016SU\u2217V \u2217i \u2212 SAi\u20161]\n= d\u2211 i=1 E[\u2016SU\u2217V \u2217i \u2212 SAi\u20161]\n\u2264 d\u2211 i=1 O(log n)\u2016U\u2217V \u2217i \u2212Ai\u2016EEMD by Equation (24) = O(log n)\u2016U\u2217V \u2217 \u2212A\u20161,EEMD.\nUsing Markov\u2019s inequality, we can complete the proof.\nTheorem F.3. Given a matrix A \u2208 Rn\u00d7d, there exists an algorithm running in poly(k, n, d) time that is able to output U \u2208 Rn\u00d7k and V \u2208 Rk\u00d7d such that\n\u2016UV \u2212A\u20161,EEMD \u2264 poly(k) \u00b7 log d \u00b7 log n min rank\u2212k Ak \u2016Ak \u2212A\u20161,EEMD\nholds with probability .99.\nProof. First using Lemma F.1 and Lemma F.2, we can reduce the original problem into an `1- low rank approximation problem by choosing m = poly(n). Second, we can use our `1-low rank approximation algorithm to solve it. Notice that all of our `1-low rank approximation algorithms can be applied here. If we apply Theorem C.6, we complete the proof.\nOur current \u2016 \u00b7 \u20161,EEMD is column-based. We can also define it to be row-based. Then we get a slightly better result by applying the `1-low rank algorithm.\nCorollary F.4. Given a matrix A \u2208 Rn\u00d7d, there exists an algorithm running in poly(k, n, d) time that is able to output U \u2208 Rn\u00d7k and V \u2208 Rk\u00d7d such that\n\u2016UV \u2212A\u20161,EEMD \u2264 poly(k) \u00b7 log2 d min rank\u2212k Ak \u2016Ak \u2212A\u20161,EEMD\nholds with probability .99."}, {"heading": "G Hardness results for Cauchy matrices, row subset selection, OSE", "text": "Section G.1 presents some inapproximability results by using random Cauchy matrices. Section G.2 is a warmup for inapproximability results for row subset selection problems. Section G.3 shows the inapproximability results by using any linear oblivious subspace embedding(OSE), and also shows inapproximability results for row subset selection.\nG.1 Hard instance for Cauchy matrices\nThe goal of this section is to prove Theorem G.2. Before stating the result, we first introduce some useful tools in our analysis.\nLemma G.1 (Cauchy Upper Tail Inequality, Lemma 3 of [CDMI+13]). For i \u2208 [m], let Ci be m random Cauchy variables from C(0, 1) (not necessarily independent), and \u03b3i > 0 with \u03b3 = \u2211 i\u2208[m] \u03b3i.\nLet X = \u2211\ni\u2208[m] \u03b3i|Ci|. Then, for any t \u2265 1,\nPr[X > \u03b3t] \u2264 O(log(mt)/t).\nTheorem G.2. Let k \u2265 1. There exist matrices A \u2208 Rd\u00d7d such that for any o(log d) \u2265 t \u2265 1, where c can be any constant smaller than 1/3, for random Cauchy matrices S \u2208 Rt\u00d7d where each entry is sampled from an i.i.d. Cauchy distribution C(0, \u03b3) where \u03b3 is an arbitrary real number, with probability .99 we have\nmin U\u2208Rd\u00d7t \u2016USA\u2212A\u20161 \u2265 \u2126(log d/(t log t)) min rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20161.\nProof. We define matrix A \u2208 Rd\u00d7d\nA = B + I = \u03b1 \u00b7  1 1 1 \u00b7 \u00b7 \u00b7 1 0 0 0 \u00b7 \u00b7 \u00b7 0 0 0 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 0 +  1 0 0 \u00b7 \u00b7 \u00b7 0 0 1 0 \u00b7 \u00b7 \u00b7 0 0 0 1 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 1  , where \u03b1 = \u0398(log d).So, if we only fit the first row of A, we can get approximation cost at most O(d).\nFor t > 0, let S \u2208 Rt\u00d7d denote a random Cauchy matrix where Si,j denotes the entry in the ith row and jth column. Then SA is\nSA = SB + SI = \u03b1 \u00b7  S1,1 S1,1 S1,1 \u00b7 \u00b7 \u00b7 S1,1 S2,1 S2,1 S2,1 \u00b7 \u00b7 \u00b7 S2,1 S3,1 S3,1 S3,1 \u00b7 \u00b7 \u00b7 S3,1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 St,1 St,1 St,1 \u00b7 \u00b7 \u00b7 St,1 +  S1,1 S1,2 S1,3 \u00b7 \u00b7 \u00b7 S1,d S2,1 S2,2 S2,3 \u00b7 \u00b7 \u00b7 S2,d S3,1 S3,2 S3,3 \u00b7 \u00b7 \u00b7 S3,d \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 St,1 St,2 St,3 \u00b7 \u00b7 \u00b7 St,d  . Since \u2200\u03b3,\nmin U\u2208Rd\u00d7t \u2016USA\u2212A\u20161 = min U\u2208Rd\u00d7t \u2016\u03b3USA\u2212A\u20161,\nwithout loss of generality, we can let Si,j \u223c C(0, 1). Then we want to argue that, if we want to use SA to fit the first row of A, with high probability, the cost will be \u2126(d log d).\nLet b \u2208 Rd denote the first row of A. Then, we want to use SA to fit the first row of A, which is a d-dimensional vector that has entry \u0398(log d) on each position. The problem is equiavlent to\nmin x\u2208Rt \u2016(SA)>x\u2212 b\u20161.\nFirst, we want to show that for any x in Rt, if x>SA fits the first row of A very well, then the `1 and `2 norm of vector x must have reasonable size.\nClaim G.3. Define A1 to be the first row of matrix A. With probability .999, for any column vector x \u2208 Rt\u00d71, if \u2016x>SA\u2212A1\u20161 \u2264 o(d log d), then\nProperty (I) : \u2016x\u20162 \u2265 \u2126(1/t log t); Property (II) : \u2016x\u20161 \u2264 O(log d).\nProof. Consider the absolute value of the i-th coordinate of x>SA. We can rewrite |\u3008(SA)i, x\u3009| in the following sense,\n|\u3008(SA)i, x\u3009| = |\u3008(SB)i, x\u3009+ \u3008(SI)i, x\u3009| = |\u3008(SB)1, x\u3009+ \u3008(SI)i, x\u3009| = |\u3008(SB)1, x\u3009+ \u3008Si, x\u3009|, (25)\nwhere the second step follows because (SB)i = (SB)1, \u2200i \u2208 [n], and the last step follows because (SI)i = Si,\u2200i \u2208 [n].\nWe start by proving Property I. Using the triangle inequality and Equation (25),\n|\u3008(SA)i, x\u3009| \u2264 |\u3008(SB)1, x\u3009|+ |\u3008Si, x\u3009| \u2264 \u2016(SB)1\u20162\u2016x\u20162 + \u2016Si\u20162\u2016x\u20162 by Cauchy-Schwarz inequality \u2264 \u2016(SB)1\u20161\u2016x\u20162 + \u2016Si\u20161\u2016x\u20162. by \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u20161\nThen, according to Lemma G.1, with probability .99999, we have \u2016(SB)1\u20161 \u2264 O(t log t log d) and for a fixed i \u2208 [d], \u2016Si\u20161 \u2264 O(t log t). Applying the Chernoff bound, with probability 1 \u2212 2\u2212\u2126(d), there are a constant fraction of i such that \u2016Si\u20161 = O(t log t). Taking the union bound, with probability .9999, there exists a constant fraction of i such that |\u3008(SA)i, x\u3009| \u2264 O(t log t log d)\u2016x\u20162. Because A1,i \u2265 \u03b1, \u2200i \u2208 [d] where \u03b1 = \u0398(log d), we need \u2016x\u20162 \u2265 \u2126(1/t log t). Otherwise, the total cost on this constant fraction of coordinates will be at least \u2126(d log d).\nFor Property II, for a fixed x \u2208 Rt, we have\n|\u3008(SA)i, x\u3009| =|\u3008(SB)1, x\u3009+ \u3008Si, x\u3009| by Equation (25)\n= \u2223\u2223\u2223\u2223\u0398(log d)\u2016x\u20161w\u20321(x) + \u2016x\u20161w\u2032i(x)\u2223\u2223\u2223\u2223. where w\u2032i(x) \u223c C(0, 1), and for different x, w\u2032i(x) are different. Then for a fixed x \u2208 Rt with probability at least 0.9, |w\u2032i(x)| = \u2126(1), and with probability 0.5, w\u20321(x) and w\u2032i(x) have the same sign. Since these two events are independent, with probability at least 0.45, we have\n|\u3008(SA)i, x\u3009| \u2265 \u2016x\u20161 \u00b7 \u2126(1).\nApplying the Chernoff bound, with probability at least 1 \u2212 2\u0398(d), there exists a 3/10 fraction of i such that |\u3008(SA)i, x\u3009| \u2265 \u2016x\u20161\u2126(1).\nWe build an -net N for x \u2208 Rt on an `1-norm unit ball, where = 1/(t2 log2 d). Thus the size of the net is |N | = 2\u0398\u0303(t). Consider y to be an arbitrary vector, let y/\u2016y\u20161 = x + \u03b4, where x is the closest point to y/\u2016y\u20161 and x \u2208 N . For any \u03b4 \u2208 Rt,\n|\u3008(SA)i, \u03b4\u3009| = |\u3008(SB)1, \u03b4\u3009+ \u3008(SI)i, \u03b4\u3009| \u2264 |\u3008(SB)1, \u03b4\u3009|+ |\u3008(SI)i, \u03b4\u3009| by triangle inequality \u2264 \u2016(SB)1\u20162\u2016\u03b4\u20162 + \u2016Si\u20162\u2016\u03b4\u20162 by Cauchy-Schwarz inequality \u2264 \u2016(SB)1\u20162\u2016\u03b4\u20161 + \u2016Si\u20162\u2016\u03b4\u20161. by \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u20161\nAs we argued before, With probability .99999, we have\n\u2016(SB)1\u20162 \u2264 \u2016(SB)1\u20161 \u2264 O(t log t log d), (26)\nand with probability 1 \u2212 2\u2212\u0398(d), there is a 9/10 fraction of i \u2208 [d], \u2016Si\u20162 \u2264 \u2016Si\u20161 = O(t log t). Therefore, with probability .999, for any \u03b4 \u2208 Rt, there exists a 9/10 fraction of i such that |\u3008(SA)i, \u03b4\u3009| \u2264 \u0398(t log t log d)\u2016\u03b4\u20161.\nTherefore, with probability .99, \u2200y \u2208 Rt, due to the pigeonhole principle, there is a 3/10 + 9/10\u2212 1 = 1/5 fraction of i such that\n|\u3008(SA)i, y\u3009| = \u2016y\u20161 \u00b7 |\u3008(SA)i, y/\u2016y\u20161\u3009| = \u2016y\u20161 \u00b7 |\u3008(SA)i, x+ \u03b4\u3009| by y/\u2016y\u20161 = x+ \u03b4 \u2265 \u2016y\u20161 \u00b7 ( |\u3008(SB)i, x\u3009+ \u3008(SI)i, x\u3009| \u2212 |\u3008(SB)1, \u03b4\u3009+ \u3008(SI)i, \u03b4\u3009| ) by triangle inequality\n\u2265 \u2016y\u20161 ( \u2126(1)\u2212 O(t log t log d) ) \u2265 \u2016y\u20161\u2126(1).\nSo \u2016y\u20161 should be O(log d). Otherwise, the total cost on this 1/5 fraction of coordinates is at least \u2126(d log d).\nCombining Property (I) and (II) completes the proof.\nNext, we need to show the following claim is true,\nClaim G.4. For any d independent Cauchy random variables x1, x2, \u00b7 \u00b7 \u00b7 , xd from C(0, 1), with probability 1 \u2212 1/ poly(t log d), for any j \u2208 [1, 2, \u00b7 \u00b7 \u00b7 , log d \u2212 \u0398(log log(t log d))], there are \u2126(d/2j) variables belonging to (2j , 2j+1].\nProof. For each Cauchy random variable xi, we have for any j \u2208 [1, 2, \u00b7 \u00b7 \u00b7 ,\u0398(log d)],\nPr [ |xi| \u2208 (2j , 2j+1] ] = \u0398(1/2j).\nWe define the indicator random variable zi,j\nzi,j = { 1 if |xi| \u2208 (2j , 2j+1], 0 otherwise.\nWe define zj = \u2211d i=1 zi,j . It is clear that E[zi,j ] = \u0398(1/2 j). We use a Chernoff bound,\nPr[X < (1\u2212 \u03b4)\u00b5] < (\ne\u2212\u03b4\n(1\u2212 \u03b4)1\u2212\u03b4\n)\u00b5 ,\nand set X = zj , \u03b4 = 1/2, \u00b5 = E[zj ]. Then, this probability is at most 2\u2212\u2126(\u00b5) = 2\u2212\u2126(E[zj ]). For any j \u2208 [1, log d \u2212 \u0398(log log(t log d))], we have E[zj ] = dE[zij ] = \u2126(d/2j) = \u2126(log(t log d)). Thus, this probability is at most 1/ poly(t log d). Overall, we have\nPr [ zj & d/2 j ] \u2265 1\u2212 1/ poly(t log d).\nTaking a union bound over \u0398(log d) such j, we complete the proof.\nClaim G.5. Let 1 > c1 > c2 > 1/3 > 0 be three arbitrary constants. We fix the first column of (SI)> \u2208 Rd\u00d7t. All the rows are grouped together according to the value in the first column. Let Rj be the set of rows for which the entry in the first column is \u2208 (2j , 2j+1]. With probability at least 1 \u2212 O(1/ poly(t log(d))), for any j \u2208 [c2 log d, c1 log d], the following event holds. There exist \u2126(d/2j) rows such that the first coordinate \u2208 (2j , 2j+1] and all the other coordinates are at most O(d1/3).\nProof. Let Rj be a subset of rows of S>, such that for any row in Rj , the first coordinate is \u2208 (2j , 2j+1]. By Claim G.4, we have that with probability 1 \u2212 1/ poly(t log d), for any j \u2208 [c2 log d, c1 log d], |Rj | \u2265 \u2126(d/2j). We want to show that for any j \u2208 [c2 log d, c1 log d], there exists a constant fraction of rows in Rj such that, the remaining coordinates are at most O(d1/3).\nFor a fixed row in Rj and a fixed coordinate in that row, the probability that the absolute value of this entry is at least \u2126(d1/3) is at most O(1/d1/3). By taking a union bound, with probability at least 1\u2212O(t/d1/3), the row has every coordinate of absolute value at most O(d1/3).\nBy applying a Chernoff bound, for a fixed subset Rj of rows, the probability that there is a constant fraction of these rows for which every coordinate except the first one in absolute value is at most O(d1/3), is at least 1\u2212 2\u2212\u0398(|Rj |) \u2265 1\u2212 2\u2212\u0398(d1\u2212c1 ) \u2265 1\u2212O(1/poly(t log(d))).\nAfter taking a union over all the j, we complete the proof.\nClaim G.6. With probability at least 1 \u2212 O(1/t), the absolute value of any coordinate in column vector (SB)1 \u2208 Rt\u00d71 is at most O(t2 log d).\nProof. Because each entry is sampled from a Cauchy distribution C(0, 1) scaled by O(log d), then with probability at most O(1/t2), the absolute value of one coordinate is at least \u2126(t2 log d). Taking a union over all the coordinates, we complete the proof.\nBecause \u2016x\u20161 \u2264 O(log d), \u2016x\u20162 \u2265 \u2126(1/t log t), there exists one coordinate i such that the absolute value of it is at least \u2126(1/t log t) and all the other coordinates are most O(log d). We can assume that i = 1 for now. (To remove this assumption, we can take a union bound over all the possibilities for i \u2208 [t].) According to Claim G.5 and G.6, let R\u0302j denote a subset of Rj , which is a \u201cgood\u201d constant fraction of Rj . Considering the `1-norm of all coordinates l \u2208 R\u0302j \u2282 Rj \u2282 [d], we have\u2211\nl\u2208R\u0302j\n|((SA)>x)l \u2212O(log d)|\n\u2265 \u2211 l\u2208R\u0302j |\u3008(SA)l, x\u3009 \u2212O(log d)| \u2265 \u2211 l\u2208R\u0302j |(SI)1,l \u00b7 x1| \u2212 t\u2211 j=2 |(SI)j,l \u00b7 xj | \u2212 |\u3008(SB)l, x\u3009| \u2212O(log d)\n \u2265 \u2211 l\u2208R\u0302j ( \u2126( 2j t log t )\u2212O(td1/3 log d)\u2212 |\u3008(SB)1, x\u3009| \u2212O(log d) )\n\u2265 \u2211 l\u2208R\u0302j ( \u2126( 2j t log t )\u2212O(td1/3 log d)\u2212 \u2016(SB)1\u20161\u2016x\u20161 \u2212O(log d) )\n\u2265 \u2211 l\u2208R\u0302j ( \u2126( 2j t log t )\u2212O(td1/3 log d)\u2212O(t log t log2 d)\u2212O(log d) ) & \u2211 l\u2208R\u0302j 2j/(t log t)\n&d/2j \u00b7 2j/(t log t) &d/(t log t).\nThe second inequality follows by the triangle inequality. The third inequality follows by (SB)1 = (SB)l, |x1| = \u2126(1/t log t), (SI)1,l \u2208 [2j , 2j+1), and \u2200j 6= 1, |xj | < O(log d), (SI)j,l \u2264 O(d1/3). The fourth inequality follows by Cauchy-Schwarz and \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u20161. The fifth inequality follows by Equation (26) and Claim G.3. The sixth inequality follows by t = o(log d) where c is a constant smaller than 1/3 and 2j \u2265 dc2 > poly(t). The seventh inequality follows from |R\u0302j | \u2265 \u2126(d/2j).\nSince there are c1 \u2212 c2 different j, the total cost is \u2126(d log d/(t log t)). The gap then is \u2126(log d/(t log t)). This completes the proof of Theorem G.2.\nG.2 Hard instance for row subset selection\nTheorem G.7. Let \u2208 (0, 1). There exists a value k \u2265 1 and matrix A \u2208 R(d\u22121)\u00d7d such that, for any subset R of rows of A, letting B be the `1-projection of each row of A onto R, we have\n\u2016A\u2212B\u20161 > (2\u2212 ) min rank\u2212k A\u2032\n\u2016A\u2032 \u2212A\u20161,\nunless |R| & d.\nProof. We construct the (d\u2212 1)\u00d7 d matrix A in the following way. The first column is all 1s, and then the remaining (d\u2212 1)\u00d7 (d\u2212 1) matrix is the identity matrix.\nA =  1 1 0 0 \u00b7 \u00b7 \u00b7 0 1 0 1 0 \u00b7 \u00b7 \u00b7 0 1 0 0 1 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1 0 0 0 \u00b7 \u00b7 \u00b7 1  .\nNote that min rank\u22121 A\u2032 \u2016A \u2212 A\u2032\u20161 = OPT \u2264 d \u2212 1, since one could choose A\u2032 to have the first column all 1s and remaining entries 0. On the other hand, consider any subset of r rows. We can permute the columns and preserve the entrywise `1-norm so w.l.o.g., we can take the first r rows for R.\nBecause we are taking the first r rows, we do not pay any cost on the first rows. To minimize the cost on the i-th row, where i \u2208 {r + 1, r + 2, . . . , d}, let vi denote the row vector we use for the i-th row. Then vi can be written as a linear combination of the first r rows {A1, A2, . . . , Ar},\nvi = r\u2211 j=1 \u03b1i,jA j .\nThen the cost of using vi to approximate the i-th row of A is:\n\u2016vi \u2212Ai\u20161 = (cost in 1st col) + ( cost in 2nd,3rd, . . . , r + 1th cols) + ( cost in i+ 1th col)\n= | r\u2211 j=1 \u03b1i,j \u2212 1|+ r\u2211 j=1 |\u03b1i,j |+ 1\n\u2265 | r\u2211 j=1 \u03b1i,j \u2212 1\u2212 r\u2211 j=1 \u03b1i,j |+ 1 by triangle inequality\n= 2.\nHence, the cost of using vi to approximate the i-th row of A is at least 2. So in total, across these (d\u2212 1\u2212 r) rows, the algorithm pays at least 2(d\u2212 1\u2212 r) cost, which needs to be at most C(d\u2212 1), and therefore r \u2265 (d\u2212 1)(1\u2212 C/2) = \u2126( d). Choosing C = 2\u2212 completes the proof.\nTheorem G.8. There exists a value k \u2265 1 and matrix A \u2208 R(d\u22121)\u00d7d such that, there is no algorithm that is able to output a rank\u2212k matrix B in the row span of A satisfying\n\u2016A\u2212B\u20161 < 2(1\u2212\u0398( 1\nd )) min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161.\nProof. We use the same matrix A as in the previous theorem.\nA =  1 1 0 0 \u00b7 \u00b7 \u00b7 0 1 0 1 0 \u00b7 \u00b7 \u00b7 0 1 0 0 1 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1 0 0 0 \u00b7 \u00b7 \u00b7 1  .\nLet vector v be ( \u2211d\u22121\ni=1 \u03b2i, \u03b21, . . . , \u03b2d\u22121). For the i-th row of A, we use \u03b1j \u00b7 v to cancel the cost of that row, where \u03b1j is a scalar. Then for any \u03b21, . . . , \u03b2d\u22121, \u03b11, . . . , \u03b1d\u22121, we can compute the entire residual cost,\nf(\u03b1, \u03b2) = d\u22121\u2211 j=1 \u2016Aj \u2212 v\u03b1j\u20161 = d\u22121\u2211 j=1 ( |1\u2212 \u03b1j( d\u22121\u2211 i=1 \u03b2i)|+ |1\u2212 \u03b1j\u03b2j |+ \u2211 i 6=j |\u03b1j\u03b2i| ) .\nIn the next few paragraphs, we will show that optimizing f(\u03b1, \u03b2) over some extra constraints for \u03b1, \u03b2 does not change the optimality. Without loss of generality, we can assume that \u2211d\u22121 i=1 \u03b2i = 1. If not we can just rescale all the \u03b1i. Consider a fixed index j. All the terms related to \u03b1j and \u03b2j are,\n|1\u2212 \u03b1j( d\u22121\u2211 i=1 \u03b2i)|+ |1\u2212 \u03b1j\u03b2j |+ \u2211 i 6=j |\u03b1j\u03b2i|+ |\u03b1i\u03b2j |.\nWe first show a simple proof by assuming \u03b2j \u2265 0. Later, we will prove the general version which does not have any assumptions.\nHandling a special case We can optimize f(\u03b1, \u03b2) in the following sense,\nmin f(\u03b1, \u03b2)\ns.t. d\u22121\u2211 j=1 \u03b2j = 1\n\u03b1j \u2265 0, \u03b2j \u2265 0, \u2200j \u2208 [d\u2212 1].\nFor each j, we consider three cases. Case I, if \u03b1j \u2265 1\u03b2j , then\n|1\u2212 \u03b1j |+ |1\u2212 \u03b1j\u03b2j |+ \u2211 i 6=j \u03b1j\u03b2i = \u03b1j \u2212 1 + \u03b1j\u03b2j \u2212 1 + \u2211 i 6=j \u03b1j\u03b2i = 2\u03b1j \u2212 2 \u2265 2(1/\u03b2j \u2212 1) \u2265 2(1\u2212 \u03b2j),\nwhere the last step follows by \u03b2j + 1/\u03b2j \u2265 2. Case II, if 1 \u2264 \u03b1j < 1/\u03b2j , then\n|1\u2212 \u03b1j |+ |1\u2212 \u03b1j\u03b2j |+ \u2211 i 6=j \u03b1j\u03b2i = \u03b1j \u2212 1 + 1\u2212 \u03b1j\u03b2j + \u2211 i 6=j \u03b1j\u03b2i = 2\u03b1j(1\u2212 \u03b2j) \u2265 2(1\u2212 \u03b2j).\nCase III, if \u03b1j < 1, then\n|1\u2212 \u03b1j |+ |1\u2212 \u03b1j\u03b2j |+ \u2211 i 6=j \u03b1j\u03b2i = 1\u2212 \u03b1j + 1\u2212 \u03b1j\u03b2j + \u2211 i 6=j \u03b1j\u03b2i = 2(1\u2212 \u03b1j\u03b2j) \u2265 2(1\u2212 \u03b2j).\nPutting it all together, we have\nf(\u03b1, \u03b2) \u2265 d\u22121\u2211 j=1 2(1\u2212 \u03b2j) = 2(d\u2212 1)\u2212 2 d\u22121\u2211 j=1 \u03b2j = 2(d\u2212 2).\nTo handle the case where \u03b2i can be negative Without loss of generality, we can assume that\u2211d\u22121 i=1 \u03b2i = 1. Notice that we can also assume \u03b2i 6= 0, otherwise it means we do not choose that row. We split all \u03b2i into two disjoint sets S and S. For any i \u2208 S, \u03b2i > 0 and for any i \u2208 S, \u03b2i < 0. As a first step, we discuss the case when all the j are in set S. Case I, if 1\u2212\u03b1j \u2211d\u22121 i=1 \u03b2i < 0 and\n1\u2212 \u03b1j\u03b2j < 0, then it means \u03b1j \u2265 max( 1\u2211d\u22121 i=1 \u03b2i , 1\u03b2 j ). The cost of that row is,\n= \u03b1j d\u22121\u2211 i=1 \u03b2i \u2212 1 + \u03b1j\u03b2j \u2212 1 + \u2211 i 6=j |\u03b1j\u03b2i|\n= 2\u03b1j\u03b2j + 2\u03b1j \u2211 i\u2208S\\j \u03b2i \u2212 2.\nIf \u03b2j \u2265 1, then \u03b1j \u2265 1. The cost is at least 2 \u2211\ni\u2208S\\j \u03b2i. If \u03b2j < 1, then \u03b1j \u2265 1/\u03b2j , and the cost is at least 2 1\u03b2j \u2211 i\u2208S\\j \u03b2i. If there are C such j in Case I, then the total cost of Case I is at least 0 if C = 1, and 2(C \u2212 1) if C \u2265 2. Case II, if 1\u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i < 0 and 1\u2212 \u03b1j\u03b2j > 0, then it means 1 < \u03b1j < 1/\u03b2j . The cost of that row is,\n= \u03b1j d\u22121\u2211 i=1 \u03b2i \u2212 1 + 1\u2212 \u03b1j\u03b2j + |\u03b1j | \u2211 i 6=j |\u03b2j |\n= 2\u03b1j \u2211 i\u2208S\\j \u03b2i\n\u2265 2 \u2211 i\u2208S\\j \u03b2i.\nSimilarly to before, if there are C such j in Case II, then the total cost of Case II is at least 0 if C = 1, and 2(C \u2212 1) if C \u2265 2.\nCase III, if 1\u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i > 0 and 1\u2212 \u03b1j\u03b2j < 0, then it means 1/\u03b2j < \u03b1j < 1\u2211d\u22121\ni=1 \u03b2i . The cost\nof the row is,\n= 1\u2212 \u03b1j d\u22121\u2211 i=1 \u03b2i + \u03b1j\u03b2j \u2212 1 + |\u03b1j | \u2211 i 6=j |\u03b2j |\n= 2|\u03b1j | \u2211 i\u2208S |\u03b2i|\n= 2|\u03b1j |( \u2211 i\u2208S |\u03b2i| \u2212 1) \u2265 2 1 \u03b2j \u2211 i\u2208S\\j \u03b2i.\nIf there are C such j in Case III, then the total cost of Case III is at least 0 if C = 1, and 2(C \u00b7(C\u22121)) if C \u2265 2.\nCase IV, if 1 \u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i > 0 and 1 \u2212 \u03b1j\u03b2j > 0, then it means \u03b1j \u2264 min(1/\u03b2j , 1\u2211d\u22121\ni=1 \u03b2i ). The\ncost for the case \u03b1j < 0 is larger than the cost for case \u03b1 > 0. Thus we can ignore the case \u03b1j < 0.\nThe cost of the row is,\n= 1\u2212 \u03b1j d\u22121\u2211 i=1 \u03b2i + 1\u2212 \u03b1j\u03b2j + |\u03b1j | \u2211 i 6=j |\u03b2j |\n= 2\u2212 2\u03b1j\u03b2j + 2\u03b1j \u2211 i\u2208S |\u03b2i|.\nIf \u03b2j < \u2211\ni\u2208S |\u03b2i|, we know that the cost is at least 2. Otherwise, using \u03b1j \u2264 1, we have the cost is at least 2\u2212 2\u03b2j + \u2211 i\u2208S |\u03b2i|. Let T denote the set of those j with \u03b2j \u2265 \u2211 i\u2208S |\u03b2i|. If |T | = 1, we know that cost is at least 0. If |T | \u2265 2, then the cost is at least\n= \u2211 j\u2208T (2\u2212 2\u03b2j + 2 \u2211 i\u2208S |\u03b2i|)\n\u2265 2|T | \u2212 2 \u2211 j\u2208S \u03b2j + 2|T | \u2211 i\u2208S |\u03b2i|\n\u2265 2|T | \u2212 2 + 2(|T | \u2212 1) \u2211 i\u2208S |\u03b2i|\n\u2265 2(C \u2212 1).\nNow we discuss the case where j \u2208 S, which means \u03b2j < 0. Case V if 1\u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i < 0 and 1\u2212 \u03b1j\u03b2j < 0, then it means \u03b1j > 1/ \u2211d\u22121 i=1 \u03b2i and \u03b1j < 1/\u03b2j .\nNotice that this case will never happen, because \u2211d\u22121\ni=1 \u03b2i = 1 and \u03b1j < 0. Case VI if 1 \u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i < 0 and 1 \u2212 \u03b1j\u03b2j < 0, then it means \u03b1j \u2265 max(1/ \u2211d\u22121 i=1 \u03b2i, 1/\u03b2j).\nBecause of \u03b2j < 0, then \u03b1j \u2265 1. The cost of that is,\n= \u03b1j d\u22121\u2211 i=1 \u03b2i \u2212 1 + 1\u2212 \u03b1j\u03b2j + |\u03b1j | \u2211 i 6=j |\u03b2i|\n= 2\u03b1j \u2211 i\u2208S |\u03b2i|\n\u2265 2. by \u03b1j \u2265 1 and \u2211 i\u2208S |\u03b2i| \u2265 1.\nCase VII if 1 \u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i > 0 and 1 \u2212 \u03b1j\u03b2j < 0, then it means \u03b1j < min(1/\u03b2j , 1/ \u2211d\u22121\ni=1 \u03b2i). Because \u03b2j < 0 and \u2211d\u22121 i=1 \u03b2i = 1, thus \u03b1j < 1/\u03b2j . The cost of that row is,\n= 1\u2212 \u03b1j d\u22121\u2211 i=1 \u03b2i + \u03b1j\u03b2j \u2212 1 + |\u03b1j | \u2211 i 6=j |\u03b2i|\n= 2|\u03b1j | \u2211 i\u2208S\\j |\u03b2i|\n\u2265 2 1 |\u03b2j | \u2211 i\u2208S\\j |\u03b2i|.\nIf there are C such j in Case VII, then the total cost of Case VII is at least 0 if C = 1, and 2(C \u00b7 (C \u2212 1)) if C \u2265 2.\nCase VIII if 1\u2212 \u03b1j \u2211d\u22121 i=1 \u03b2i > 0 and 1\u2212 \u03b1j\u03b2j > 0, then it means 1/\u03b2j < \u03b1j < 1/ \u2211d\u22121\ni=1 \u03b2i. The cost of that row,\n= 1\u2212 \u03b1j d\u22121\u2211 i=1 \u03b2i + 1\u2212 \u03b1j\u03b2j + |\u03b1j | \u2211 i 6=j |\u03b2i|\n= 2\u2212 2\u03b1j\u03b2j + 2|\u03b1j | \u2211 i\u2208S\\j |\u03b2i|.\nIf \u03b1j > 0, then the cost is always at least 2. If \u03b1j < 0, the cost is at least,\n2\u2212 2|\u03b1j\u03b2j |+ 2|\u03b1j | \u2211 i\u2208S\\j |\u03b2i|.\nIf \u2211\ni\u2208S\\j |\u03b2i| > |\u03b2j |, we also have cost at least 2. Otherwise, we have\n2\u2212 2|\u03b1j |(|\u03b2j | \u2212 \u2211 i\u2208S\\j |\u03b2i|) =\n{ 2\u2212 2|\u03b2j |+ 2 \u2211 i\u2208S\\j |\u03b2i| if 1/|\u03b2j | \u2264 1,\n2 1|\u03b2j | \u2211 i\u2208S\\j |\u03b2i| if 1/|\u03b2j | > 1.\nLet C denote the number of such j with 1/|\u03b2j | \u2264 1. If C = 1, the cost is at least 0. If C \u2265 2, the cost is at least 2C. Let C \u2032 denote the number of such j with 1/|\u03b2j | > 1. If C \u2032 = 1, the cost is at least 0, if C \u2032 \u2265 2, the cost is at least 2(C \u2032(C \u2032\u2212 1)). Overall, putting all the eight cases together, we complete the proof.\nG.3 Hard instance for oblivious subspace embedding and more row subset selection\nThe goal in this section is to prove Theorem G.31. By applying Yao\u2019s minmax principle, it suffices to prove Theorem G.27."}, {"heading": "G.3.1 Definitions", "text": "We first give the definition of total variation distance and Kullback-Leibler divergence.\nDefinition G.9. [LPW09, Ver14] The total variation distance between two probability measures P and Q on the measurable space (X , F) is defined as,\nDTV(P,Q) = sup A\u2208F |P (A)\u2212Q(A)|.\nThe Kullback-Leibler( KL) divergence of P and Q is defined as,\nDKL(P ||Q) = E P\n( log dP\ndQ\n) = \u222b X ( log dP dQ ) dP.\nLemma G.10. [Pin60, Tsy09, CK11] Pinsker\u2019s inequality states that, if P and Q are two probability distributions on a measurable (X , F), then\nDTV(P,Q) \u2264 \u221a 1\n2 DKL(P ||Q).\nLemma G.11. For any Gaussian random variable x \u223c N(0, \u03c32), we have, for any a > 0\nPr[|x| < a] . a \u03c3 .\nProof.\nPr[|x| < a] = erf( a \u03c3 \u221a 2 )\n\u2264 (1\u2212 e\u2212 \u22124a2 2\u03c32\u03c0 ) 1 2 by 1\u2212 exp(\u22124x2/\u03c0) \u2265 erf(x)2\n\u2264 ( 4a 2\n2\u03c32\u03c0 ) 1 2 by 1\u2212 e\u2212x \u2264 x\n. a\n\u03c3 .\nLemma G.12. Let V \u2208 Rn\u00d7m be a matrix with orthonormal columns. Let H\u2032 be a distribution over V >A, where A \u2208 Rn\u00d7k is a random matrix with each entry i.i.d. Gaussian N(0, 1). Denote H as a distribution over H \u2208 Rm\u00d7k, where each entry of H is drawn from i.i.d. Gaussian N(0, 1). Then, H and H\u2032 are the same distribution.\nProof. It is clear that each entry of V >A is a random Gaussian variable from N(0, 1), so our goal is to prove the entries of V >A are fully independent. Since the the jth column of V >A only depends on Aj , the variables from different columns are fully independent. Now, we look at one column of x = V >Aj . The density function is\nf(x) = 1\u221a\n(2\u03c0)m|V >V | exp\n( \u22121\n2 x>V >V x\n) =\n1\u221a (2\u03c0)m exp\n( \u22121\n2 x>x\n) ,\nwhich is exactly the density function of N(0, Im). Thus x \u2208 N(0, Im). Therefore, all the entries of V >A are fully independent.\nLemma G.13 (Matrix Determinant Lemma). Suppose A \u2208 Rn\u00d7n is an invertible matrix, and u, v \u2208 Rn. Then,\n|A+ uv>| = (1 + v>A\u22121u)|A|.\nLemma G.14 (KL divergence between two multivariate Gaussians [PP+08]2). Given two d-dimensional multivariate Gaussian distribution N(\u00b51,\u03a31) and N(\u00b52,\u03a32), then\nDKL(N(\u00b51||\u03a31), N(\u00b52,\u03a32)) = 1\n2 ( log |\u03a32| |\u03a31| \u2212 d+ tr(\u03a3\u221212 \u03a31) + (\u00b52 \u2212 \u00b51) >\u03a3\u221212 (\u00b52 \u2212 \u00b51) ) .\nLemma G.15 (Sherman-Morrison formula). Suppose A \u2208 Rn\u00d7n is an invertible matrix and u, v \u2208 Rn. Suppose furthermore that 1 + v>A\u22121u 6= 0. Then the Sherman-Morrison formula states that\n(A+ uv>)\u22121 = A\u22121 \u2212 A \u22121uv>A\u22121\n1 + v>A\u22121u . (27)\n2http://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians"}, {"heading": "G.3.2 Main results", "text": "Lemma G.16. Let V \u2208 Rn\u00d7m be a matrix with orthonormal columns, and let A \u2208 Rn\u00d7k be a random matrix with each entry drawn from i.i.d. Gaussian N(0, 1). We denote the distribution Di over Di \u2208 R(m+1)\u00d7k where\nDi = [ V >A Ai ] ."}, {"heading": "If \u2016(V >)i\u201622 < 12 , then", "text": "DTV(Di,G) \u2264 O(k\u2016V i\u20162) + 2\u2212\u0398(k),\nwhere G is a distribution over G \u2208 R(m+1)\u00d7k, where each entry of G is drawn from the i.i.d. Gaussian N(0, 1).\nProof. Let H be a distribution over H \u2208 Rm\u00d7k, where each entry of H is an i.i.d. Gaussian N(0, 1). Let H\u2032 be a distribution over V >A. According to Lemma G.12, H and H\u2032 are the same distribution.\nWe define matrix V \u00aci \u2208 Rn\u00d7m as\nV \u00aci = [ (V 1)> (V 2)> \u00b7 \u00b7 \u00b7 (V i\u22121)> 0 (V i+1)> \u00b7 \u00b7 \u00b7 (V n)> ]> ,\nwhere V i denotes the ith row of V \u2208 Rn\u00d7m, \u2200i \u2208 [n]. Let G be a distribution over G \u2208 R(m+1)\u00d7k, where each entry of G is an i.i.d. Gaussian N(0, 1). Let Pi be a distribution over Pi \u2208 R(m+1)\u00d7k, where\nPi =\n[ (V \u00aci)>A\nAi\n] .\nLet P\u0302i be a distribution over P\u0302i \u2208 Rm\u00d7k, where P\u0302i = (V \u00aci)>A. Then we have:\nDTV(Pi,G) = DTV(P\u0302i,H\u2032) = DTV(P\u0302i,H). (28)\nThe first equality is because (V \u00aci)>A is independent from Ai. The second equality follows the Lemma G.12.\nClaim G.17. If \u2016V i\u201622 \u2264 12 ,\nDKL(P\u0302i||H) = \u2212 k\n2\n( log(1\u2212 \u2016V i\u201622) + \u2016V i\u201622 ) \u2264 k\u2016V i\u201622.\nProof. Let P\u0302i \u223c P\u0302i, H \u223c H. Notice that different columns of P\u0302i are i.i.d, and all entries of H are fully independent. We can look at one column of P\u0302i and H. Since P\u0302i = (V \u00aci)>A, it is easy to see that its column is drawn from N(0,\u03a31) where \u03a31 = Im \u2212 (V i)>V i. Since H is fully independent, each column of H is drawn from N(0,\u03a32) where \u03a32 = Im. Let p(x) be the pdf of the column of P\u0302i,\nand let q(x) be the pdf of the column of H. We have the following calculation [PP+08]3,\nDKL(P\u0302i||H) = k\n2 ( log |\u03a32| |\u03a31| \u2212m+ tr(\u03a3\u221212 \u03a31) )\n= k\n2\n( log\n|Im| |Im \u2212 (V i)>V i|\n\u2212m+ tr(Im \u2212 (V i)>V i) )\n= k\n2\n( \u2212 log |Im \u2212 (V i)>V i| \u2212m+m\u2212 \u2016V i\u201622 ) = k\n2\n( \u2212 log(1\u2212 \u2016V i\u201622)\u2212m+m\u2212 \u2016V i\u201622 ) = \u2212k\n2\n( log(1\u2212 \u2016V i\u201622) + \u2016V i\u201622 ) \u2264 \u2212k\n2 \u00b7 2\u2016V i\u201622\n= k\u2016V i\u201622.\nThe first equality is due to Lemma G.14. The sixth equality follows by \u03a32 = Im and \u03a31 = Im \u2212 (V i)>V i. The eighth equality follows by Lemma G.13. The first inequality follows by log(1\u2212x)+x \u2265 \u22122x, when 0 < x < 1/2.\nAccording to Lemma G.10, we have\nDTV(P\u0302i,H) \u2264 \u221a 1\n2 DKL(P\u0302i||H) \u2264\n\u221a k\u2016V i\u20162. (29)\nNow, we want to argue that DTV(Di,Pi) is small, where Di is a distribution over Di \u2208 R(m+1)\u00d7k that\nDi = [ V >A Ai ] .\nFor a fixed x \u2208 Rk, let D\u0302i(x) be a distribution over D\u0302i(x) \u2208 Rm\u00d7k, where D\u0302i(x) = (V \u00aci)>A+(V i)>x. Let p(x) be the pdf of (Ai)>, then\nDTV(Di,Pi) = \u222b DTV ( D\u0302i(x), P\u0302i ) p(x)dx. (30)\nNow we look at the jth column of D\u0302i(x) and the jth column of P\u0302i. The distribution of the previous one is over N((V i)>xj ,\u03a32), and the latter distribution as we said before is N(0,\u03a32),\n3http://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians\nwhere \u03a32 = Im \u2212 (V i)>V i. Now we can argue that the KL divergence between them is bounded:\nDKL(N((V i)>xj ,\u03a32)||N(0,\u03a32))\n= 1\n2 ( log |\u03a32| |\u03a32| \u2212m+ tr(\u03a3\u221212 \u03a32) + x 2 jV i\u03a3\u221212 (V i)> )\n= 1\n2\n( \u2212m+ tr(Im) + x2jV i\u03a3\u221212 (V i)> )\n= 1\n2 x2jV i\u03a3\u221212 (V i)>\n= 1\n2 x2jV i(Im \u2212 (V i)>V i)\u22121(V i)>\n= 1\n2 x2jV i\n( Im + (V i)>V i\n1\u2212 V i(V i)>\n) (V i)>\n= 1\n2 x2j\n( \u2016V i\u201622 +\n\u2016V i\u201622 1\u2212 \u2016V i\u201622 ) = 1\n2 x2j \u2016V i\u201622\n1\u2212 \u2016V i\u201622 .\nThe first equality is due to Lemma G.14. The fourth equality follows by \u03a32 = Im \u2212 (V i)>V i. The fifth equality follows by Lemma G.15.\nBy summing the KL divergence on all the columns up,\nDKL(D\u0302i(x)||P\u0302i) = 1\n2 \u2016x\u201622 \u2016V i\u201622 1\u2212 \u2016V i\u201622 .\nApplying Lemma G.10 again, we get DTV(D\u0302i(x), P\u0302i) \u2264 \u221a 1\n2 DKL(D\u0302i(x)||P\u0302i) = \u2016x\u20162\u2016V i\u20162 2 \u221a 1\u2212 \u2016V i\u201622 .\nPlugging it into Equation (30), we get DTV(Di,Pi) = \u222b DTV ( D\u0302i(x), P\u0302i ) p(x)dx\n= \u222b \u2016x\u20162\u226410k DTV ( D\u0302i(x), P\u0302i ) p(x)dx+ \u222b \u2016x\u20162>10k DTV ( D\u0302i(x), P\u0302i ) p(x)dx\n\u2264 \u222b \u2016x\u20162\u226410k DTV ( D\u0302i(x), P\u0302i ) p(x)dx+ \u222b \u2016x\u20162>10k p(x)dx\n\u2264 \u222b \u2016x\u20162\u226410k \u2016x\u20162\u2016V i\u20162 2 \u221a 1\u2212 \u2016V i\u201622 p(x)dx+ \u222b \u2016x\u20162>10k p(x)dx \u2264 10k\u2016V i\u20162\n2 \u221a 1\u2212 \u2016V i\u201622 + \u222b \u2016x\u20162>10k p(x)dx\n\u2264 10k\u2016V i\u20162 2 \u221a 1\u2212 \u2016V i\u201622 + 2\u2212\u0398(k).\nThe first inequality just follows from the fact that total variation distance is smaller than 1. The second inequality is what we plugged in. The last inequality follows from the fact that x \u223c N(0, Ik) and from the tail bounds of a Gaussian.\nTogether with Equation (28) and Equation (29), we can get\nDTV(Di,G) \u2264 DTV(Di,Pi) +DTV(G,Pi)\n= DTV(Di,Pi) +DTV(P\u0302i,H)\n\u2264 10k\u2016V i\u20162 2 \u221a 1\u2212 \u2016V i\u201622 + 2\u2212\u0398(k) + \u221a k\u2016V i\u20162 \u2264 10k\u2016V i\u20162 + 2\u2212\u0398(k) + \u221a k\u2016V i\u20162.\nThe last inequality follows by \u2016V i\u201622 \u2264 12 . Then, we have completed the proof.\nLemma G.18. A \u2208 Rr\u00d7k (r \u2265 k) is a random matrix for which each entry is i.i.d. N(0, 1). With probability at least 1\u2212 e\u2212\u0398(r), the maximum singular value \u2016A\u20162 is at most O( \u221a r).\nProof. Since A \u2208 Rk\u00d7r is a random matrix with each entry i.i.d. N(0, 1), this follows by standard arguments (Proposition 2.4 in [RV10]). Since r \u2265 k, with probability at least 1\u2212 e\u2212\u0398(r), \u2016A\u20162 is at most O( \u221a r).\nDefinition G.19. Let V \u2208 Rn\u00d7r be a matrix with orthonormal columns, and let each entry of A \u2208 Rk\u00d7r be a random variable drawn from an i.i.d. Gaussian N(0, 1). Define event E\u0302(A, V, \u03b2, \u03b3) to be: \u2200y \u2208 Rn, \u2016y\u20161 \u2264 O(k\u03b3) and each coordinate of y has absolute value at most 1/k\u03b2, and also AV >y has at most O(k/ log k) coordinates with absolute value at least \u2126(1/ log k), and \u2016A\u20162 \u2264 O( \u221a r).\nLemma G.20. For any k \u2265 1, and any constants c2 \u2265 c1 \u2265 1, let k \u2264 r = O(kc1), r \u2264 n = O(kc2), let V \u2208 Rn\u00d7r be a matrix with orthonormal columns, and let each entry of A \u2208 Rk\u00d7r be a random variable drawn from i.i.d. Gaussian N(0, 1). Furthermore, if \u03b2 and \u03b3 are two constants which satisfy \u03b2 > \u03b3 > 0 and \u03b2 + \u03b3 < 1,\nPr [ E\u0302(A, V, \u03b2, \u03b3) ] \u2265 1\u2212 2\u2212\u0398(k).\nProof. Due to Lemma G.18, with probability at least 1\u2212 2\u2212\u0398(r), \u2016A\u20162 \u2264 O( \u221a r), so we can restrict attention to \u2016A\u20162 \u2264 O( \u221a r) in the following proof.\nTake any y \u2208 Rn which has each non-zero coordinate with absolute value at most 1/k\u03b2 , and write it as y = \u2211+\u221e j=j0\nyj , where the coordinates in yj have absolute value in the range [2\u2212j\u22121, 2\u2212j), and the supports of different yj are disjoint. Since each coordinate of y is at most 1/k\u03b2 , 2\u2212j0 = 1/k\u03b2 . Since \u2016y\u20161 \u2264 O(k\u03b3), the support size of yj \u2208 Rn is at most sj \u2264 O(2j+1k\u03b3), so it follows that\n\u2016yj\u20162 \u2264 \u221a sj \u00b7 2\u22122j \u2264 \u221a 2\u2212j+1k\u03b3 .\nWe also know \u2016yj\u20162 \u2265 \u221a sj \u00b7 2\u22122j\u22122. Then we can conclude that yj has 2-norm \u0398(2\u2212j \u221a sj). Now we state an \u03b5-net for all the possible yj : let \u03b5 = O(1/(nrk3)) = O(1/(kc1+c2+3)). Let Nj \u2282 Rn be the following:\nNj = { p \u2208 Rn | \u2203q \u2208 Zn, s.t. p = \u03b5q, \u2016p\u20161 \u2264 O(k\u03b3),\u2200i \u2208 [n], either |pi| \u2208 [2\u2212j\u22121, 2\u2212j) or pi = 0 } .\nObviously, for any yj , there exists p \u2208 Nj such that \u2016yj \u2212 p\u2016\u221e \u2264 \u03b5 = O(1/(kc1+c2+3)), since n \u2264 O(kc2), \u2016yj \u2212 p\u20162 \u2264 \u2016yj \u2212 p\u20161 \u2264 n\u2016yj \u2212 p\u2016\u221e \u2264 O(1/kc1+3). Now let us consider the size of Nj . If p \u2208 Nj , the choice of one coordinate of p is at most 2/\u03b5 + 1. And since \u2016p\u20161 \u2264 O(k\u03b3) and each\ncoordinate of p has absolute value at least 2\u2212j\u22121, the number of supports of p is at most O(2j+1k\u03b3). Therefore,\n|Nj | \u2264 (n+ 1)O(2 j+1k\u03b3) \u00b7 (2/\u03b5+ 1)O(2j+1k\u03b3)\n\u2264 2O(2j+1k\u03b3(logn+log(2/\u03b5+1))\n\u2264 2O(2j+1k\u03b3 log k).\nThe last inequality follows from n \u2264 O(kc2), r \u2264 O(kc1), \u03b5 = O(1/(rnk3)). We define an event E(yj) to be: AV >yj has k/ log2 k coordinates which are each at least 2/ log2 k in absolute value. Now, we want to show,\nClaim G.21. For any j \u2265 j0, for a fixed yj \u2208 Rn,\nPr [ E(yj) happens ] \u2264 2ke\u2212\u0398(k/(\u2016yj\u201622 log 6 k)) \u2264 e\u2212\u0398(2j\u22121k1\u2212\u03b3/ log 6 k).\nProof. We let p be the probability that the absolute value of a single coordinate of AV >yj is at least 1/ log2 k. Notice that each coordinate of AV >yj is i.i.d. Gaussian N(0, \u2016V >yj\u201622) and because for any Gaussian random variable g, Pr[|g| \u2265 t] \u2264 exp(\u2212\u0398(t2/\u03c32)), then p \u2264 exp(\u22121/(\u2016V >yj\u201622 log4 k)), by plugging \u03c32 = \u2016V >yj\u201622 and t = 1/ log2 k. So the probability AV >yj has k/ log2 k coordinates which are each at least 1/ log2 k in absolute value is,\nk\u2211 i=k/ log2 k pi(1\u2212 p)k\u2212i ( k i ) \u2264 k\u2211 i=k/ log2 k pi ( k k/2 ) \u2264 2pk/ log 2 k2k\n\u2264 e\u2212\u0398(kt2/(\u03c32 log 2 k))2k by p \u2264 exp(\u2212\u0398(t2/\u03c32))\n\u2264 e\u2212\u0398(k/(\u2016V >yj\u201622 log 6 k))2k\n\u2264 e\u2212\u0398(k/(\u2016yj\u201622 log 6 k))2k by \u2016V >yj\u201622 \u2264 \u2016V \u201622\u2016yj\u201622 \u2264 \u2016yj\u201622 \u2264 e\u2212\u0398(k/(2\u22122jsj log 6 k))2k by \u2016yj\u201622 \u2264 2\u22122jsj \u2264 e\u2212\u0398(k/(2\u2212j+1k\u03b3 log 6 k))2k by sj \u2264 O(2j+1k\u03b3) = e\u2212\u0398(k/(2 \u2212j+1k\u03b3 log6 k))\n\u2264 e\u2212\u0398(2j\u22121k1\u2212\u03b3/ log 6 k).\nThe first equality follows from 2\u2212j+1k\u03b3 log6 k \u2264 2\u2212j0+1k\u03b3 log6 k \u2264 2k\u03b3\u2212\u03b2 log6 k = o(1).\nFor j1 = d100(c1 + c2 + 1) log ke = \u0398(log k), consider j \u2208 [j1,\u221e). We have \u2016 \u2211\u221e\nj=j1 yj\u20162 is at\nmost \u0398(2\u2212j1 \u221a n) \u2264 1/k100(1+c1), and so\n\u2016AV > \u221e\u2211 j=j1 yj\u20161 \u2264 \u221a k\u2016AV > \u221e\u2211 j=j1 yj\u20162 \u2264 \u221a k\u2016A\u20162\u2016V \u20162\u2016 \u221e\u2211 j=j1 yj\u20162 \u2264 \u221a k \u00b7 \u221a r \u00b7 1/k100(1+c1) \u2264 1/k50.\nThe last inequality follows from r = O(kc1). So the contribution of yj to \u2016AV >yj\u20161 for all j \u2265 j1 is at most 1/k50. Thus, if we only consider those j which contribute, i.e., j0 \u2264 j \u2264 j1, we have\nO(log k) values of j. Then we can only construct O(log k) nets Nj0 ,Nj0+1, \u00b7 \u00b7 \u00b7 ,Nj1 . Since the size of net Nj is 2\u0398(2 j+1k\u03b3 log k), by combining Claim G.21 and taking a union bound, we have\nPr \u2203yj \u2208 j1\u22c3 j=j0 Nj , E(yj) happens  \u2264 j1\u2211 j=j0 2\u0398(2 j+1k\u03b3 log k) \u00b7 e\u2212\u0398(2j\u22121k1\u2212\u03b3/ log 6 k)\n\u2264 e\u2212\u0398(2j0\u22121k1\u2212\u03b3/ log 6 k)\n\u2264 e\u2212\u0398(k1+\u03b2\u2212\u03b3/ log 6 k)\n\u2264 2\u2212\u0398(k).\nThe second inequality follows since k\u03b3 = o(k1\u2212\u03b3). The third inequality follows since 2j0 \u2265 k\u03b2 . The fourth inequality follows since 1 + \u03b2 \u2212 \u03b3 > 1.\nThen, \u2200j0 \u2264 j \u2264 j1, \u2200y\u0303j 6\u2208 Nj , there exists a vector y\u0302j in Nj , such that \u2016y\u0302j \u2212 y\u0303j\u20162 \u2264 1/k3+c1 . We can upper bound the `\u221e norm of AV >y\u0302j \u2212AV >y\u0303j in the following sense,\n\u2016AV >y\u0302j \u2212AV >y\u0303j\u2016\u221e \u2264 \u2016AV >y\u0302j \u2212AV >y\u0303j\u20162 by \u2016 \u00b7 \u2016\u221e \u2264 \u2016 \u00b7 \u20162 \u2264 \u2016A\u20162 \u00b7 \u2016y\u0302j \u2212 y\u0303j\u20162 \u2264 \u221a r/k3+c1 by Claim G.18 and \u2016y\u0302j \u2212 y\u0303j\u20162 \u2264 1/k3+c1\n= 1/k2 by r \u2264 O(kc1).\nWe let Y = {y \u2208 Rn | \u2016y\u20161 \u2264 O(k\u03b3) and each coordinate of y \u2264 1/k\u03b2}. Since 1/k2 < 1/ log2 k we can conclude that,\nPr [ \u2203y \u2208 Y, j \u2265 j0, E(yj) happens ] \u2264 2\u2212\u0398(k). (31)\nRecalling y = \u2211\nj y j , by Equation (31), for any y, with probability at most 2\u2212\u0398(k), there are at most O(log k) \u00b7 k/ log2 k \u2264 O(k/ log k) coordinates for which AV > \u2211j1\nj=j0 yj (the same statement also holds for AV > \u2211\u221e\nj=j0 yj = AV >y since we argued that there is negligible \u201ccontribution\u201d for\nthose j > j1) is at least O(log k)/ log2 k = O(1/ log k) on that coordinate. Summarizing,\nPr [ E\u0302(A, V, \u03b2, \u03b3) ] \u2265 1\u2212 2\u2212\u0398(k).\nLemma G.22. For any t, k \u2265 1, and any constants c2 \u2265 c1 \u2265 1, let k \u2264 r = O(kc1), r \u2264 n = O(kc2), let V \u2208 Rn\u00d7r be a matrix with orthonormal columns, and let each entry of A \u2208 Rk\u00d7r, v1, v2, \u00b7 \u00b7 \u00b7 , vt \u2208 Rk be an i.i.d. Gaussian N(0, 1) random variable. For a constant \u03b1 \u2208 (0, 0.5) which can be arbitrarily small, if E\u0302(A, V, 0.5+\u03b1/2, 0.5\u2212\u03b1) happens, then with probability at least 1\u22122\u2212\u0398(tk), there are at least dt/10e such j \u2208 [t] that \u2200x \u2208 Rr either \u2016Ax\u2212 vj\u20161 \u2265 \u2126(k0.5\u2212\u03b1) or \u2016V x\u20161 \u2265 \u2126(k0.5\u2212\u03b1) holds.\nProof. For convenience, we define \u03b3 = 0.5 \u2212 \u03b1 which can be an arbitrary constant in (0, 0.5). We let constant \u03b2 = 0.5 + \u03b1/2. Then we have \u03b2 + \u03b3 < 1 and \u03b2 > \u03b3. Let v \u2208 Rk be a random vector with each entry drawn from i.i.d. Gaussian N(0, 1). Suppose E\u0302(A, V, \u03b2, \u03b3) happens.\nFor any x \u2208 Rr, we can find a y \u2208 Rn such that y = V x. Since V \u2208 Rn\u00d7r has orthonormal columns, x = V >V x = V >y. Then our goal is to argue that with high probability if \u2016AV >y\u2212v\u20161 \u2264 O(k\u03b3), then \u2016y\u20161 > \u2126(k\u03b3). Take any y \u2208 Rn which can be expressed as V x, and decompose it as y = y0 +y1, where y0 \u2208 Rn and y1 \u2208 Rn have disjoint supports, y0 has each coordinate with absolute value greater than 1/k\u03b2 , and y1 has each coordinate with absolute value at most 1/k\u03b2 .\nNow we create an \u03b5-net for y0: let \u03b5 = O(1/(rnk3)) = O(1/kc1+c2+3), we denote N \u2282 Rn as follows:\nN = { p \u2208 Rn | \u2203q \u2208 Zn, s.t. p = \u03b5q, \u2016p\u20161 \u2264 O(k\u03b3), \u2200i \u2208 [n], either |pi| > 1/k\u03b2 or pi = 0 } .\nObviously, for any y0, there exists p \u2208 N such that \u2016y0 \u2212 p\u2016\u221e \u2264 \u03b5 = O(1/(kc1+c2+3)), since n \u2264 O(kc2), \u2016y0 \u2212 p\u20162 \u2264 \u2016y0 \u2212 p\u20161 \u2264 n\u2016y0 \u2212 p\u2016\u221e \u2264 O(1/kc1+3). Now let us consider the size of N . If p \u2208 N , the number of choices of one coordinate of p is at most O(k\u03b3/\u03b5). And since \u2016p\u20161 \u2264 O(k\u03b3) and each coordinate of p has absolute value at least 1/k\u03b2 , the number of supports of p is at most O(k\u03b3+\u03b2). Therefore,\n|N | \u2264 (n+ 1)O(k\u03b3+\u03b2) \u00b7O(k\u03b3/\u03b5)O(k\u03b3+\u03b2)\n\u2264 2O(k\u03b3+\u03b2 log k).\nThe last inequality follows from n \u2264 O(kc2), r \u2264 O(kc1), \u03b5 = O(1/(rnk3)). For y0 \u2208 Rn, we define event E1(y0) as: \u2203 valid y1 \u2208 Rn, \u2016AV >y\u2212v\u20161 \u2264 O(k\u03b3), where y = y0+y1 is the decomposition of a possible y \u2208 Rn. Here y1 is valid means that there exists y \u2208 Rn such that \u2016y\u20161 \u2264 O(k\u03b3) and the decomposition of y is y = y0 + y1. We define event E2(y0) as: \u2203 valid y1, the absolute value of AV >y \u2212 v is at most O(1/k\u03b3) for at least k \u2212 O(k2\u03b3) coordinates i in [k] . We define event E3(y0) as: at least k\u2212O(k2\u03b3)\u2212O(k/ log k) coordinates of Ay0\u2212 v have absolute value at most O(1/ log k).\nClaim G.23. For y0 \u2208 Rn,\nPr[E3(y0) happens ] \u2265 Pr[E2(y0) happens ] \u2265 Pr[E1(y0) happens ].\nProof. If E1(y0) happens, then there exists a valid y1 \u2208 Rn such that y = y0 +y1 and \u2016AV >y\u2212v\u20161 \u2264 O(k\u03b3). For this y, there are at least k \u2212O(1/k2\u03b3) coordinates of AV >y \u2212 v with absolute value at most O(1/k\u03b3). Otherwise, \u2016AV >y \u2212 v\u20161 > \u2126(k\u03b3). Thus, E1(y0) implies E2(y0).\nNow we want to show E2(y0) implies E3(y0). We suppose E2(y0) happens. Then there is a valid y1 such that there are at least k \u2212O(1/k2\u03b3) coordinates of AV >y \u2212 v with absolute value at most O(1/k\u03b3), where y = y0 + y1. Recall that the event E\u0302(A, V, \u03b2, \u03b3) happens, for any valid y1 there are at most O(k/ log k) coordinates of AV >y1 are at least \u2126(1/ log k). Therefore,\nAV >y0 \u2212 v = AV >y \u2212 v\ufe38 \ufe37\ufe37 \ufe38 \u2265k\u2212O(k2\u03b3) coordinates\neach \u2264O(1/k\u03b3)\n\u2212 AV >y1\ufe38 \ufe37\ufe37 \ufe38 \u2264O(k/ log k) coordinates\neach\u2265\u2126(1/ log k)\n.\nTherefore, at least k \u2212 O(k2\u03b3) \u2212 O(k/ log k) coordinates of Ay0 \u2212 v in absolute value is at most O(1/ log k) +O(1/k\u03b3) = O(1/ log k)\nClaim G.24. Define event F1 to be the situation for which there exists 1/2 of the coordinates of v \u2208 Rk that are at least 1/100. The probability this event F1 holds is at least 1\u2212 2\u2212\u0398(k).\nProof. Note that F1 means there exist k/2 of the coordinates of v \u2208 Rk which are at most 1/100. Using Lemma G.11, for each single coordinate the probability it is smaller than 1/100 is 1/200. Then the probability more than half the coordinates are no more than 1/100 is\nk\u2211 i=k/2 pi(1\u2212 p)k\u2212i ( k i ) \u2264 k\u2211 i=k/2 pi ( k k/2 ) \u2264 2pk/22k \u2264 (1/200)\u0398(k) = 2\u2212\u0398(k).\nConditioned on F1 happening, if E3(y0) happens, then due to the pigeonhole principle, there are at least k2 \u2212 O(k\n2\u03b3) \u2212 O(k/ log k) coordinates of AV >y0 \u2212 v that are at most O(1/ log k) and the corresponding coordinate of v is larger than 1/100. Now, let us look at the probability on a single coordinate of AV >y0 \u2212 v.\nClaim G.25. If the ith coordinate of v \u2208 Rk is at least 1/100. Then Pr[|(AV >y0)i \u2212 (v)i| \u2264 O(1/ log k)] \u2264 O(1/ log k).\nProof. Since |(v)i| > 1/100 and v is independent from A \u2208 Rk\u00d7r, for 0 < \u03b7 < 1/100, Pr[|(AV >y0)i\u2212 (v)i| \u2264 \u03b7] is always upper bounded by Pr[(Ax0)i \u2208 [1/100\u2212 \u03b7, 1/100 + \u03b7]. Thus, it suffices to prove an upper bound for Pr[(AV >x0)i \u2208 [1/100\u2212 \u03b7, 1/100 + \u03b7]. Let f(x) be the pdf of N(0, \u03c32), where \u03c32 = \u2016V >y0\u201622, V \u2208 Rn\u00d7r, y0 \u2208 Rn. Then\nPr[(AV >y0)i \u2208 [1/100\u2212 \u03b7, 1/100 + \u03b7]]\n= \u222b 1/100+\u03b7 1/100\u2212\u03b7 f(x)dx\n\u2264 f(1/200) \u222b 1/100+\u03b7\n1/100\u2212\u03b7 dx\n\u2264O(\u03b7).\nwhere the last step follows since f(1/200) \u2264 200. We set \u03b7 = O(1/ log k), then we get the statement.\nClaim G.26. Conditioned on F1, for a fixed y0 \u2208 Rn, with probability at most 2\u2212\u0398(k), there are at least k10 coordinates of AV\n>y0 \u2212 v \u2208 Rk which are at most O(1/ log k) and the corresponding coordinate of v is larger than 1/100.\nProof. We look at the coordinate i \u2208 [k] which has |(v)i| > 1/100. The probability \u2016(AV >y0)i \u2212 (v)i\u2016 \u2264 O(1/ log k) is at most O(1/ log k). Due to the independence between different coordinates of v, since there are at least k/2 coordinates of v satisfying that they have absolute value greater than 1/100, with probability at most 2\u2212\u0398(k), there are at least 15 \u00b7 k 2 = k/10 coordinates of AV\n>y0 \u2212 v which are at most O(1/ log k).\nBecause E3(y0) implies the event described in the above claim when conditioning on F1, for a fixed y0, the probability that E3(y0) holds is at most 2\u2212\u0398(k). Since \u03b3 + \u03b2 < 1, the |N | \u2264 2k\no(1) , we can take a union bound over the N :\nPr[\u2203y0 \u2208 N , E1(y0) happens ] \u2264 2\u2212\u0398(k).\nIt means that with probability at least 1\u22122\u2212\u0398(k), for any y = y0 +y1 with y0 \u2208 N , \u2016AV >y\u2212v\u20161 > \u2126(k\u03b3). Let y\u0303 = y\u03030 + \u2211 y\u0303j , where y\u03030 6\u2208 N . We can find y\u03020 \u2208 N which is the closest to y\u03030. Denote\ny\u0302 = y\u0302 + \u2211 y\u0303j . Then,\n\u2016AV >y\u0302 \u2212 v\u20161 \u2264 \u2016AV >y\u0303 \u2212 v\u20161 + \u2016AV >y\u0303 \u2212AV >y\u0302\u20161 by triangle inequality\n\u2264 \u2016AV >y\u0303 \u2212 v\u20161 + \u221a k\u2016AV >y\u0303 \u2212AV >y\u0302\u20162 by \u2016 \u00b7 \u20161 \u2264 \u221a dim\u2016 \u00b7 \u20162 \u2264 \u2016AV >y\u0303 \u2212 v\u20161 + \u221a k\u2016A\u20162\u2016y\u0303 \u2212 y\u0302\u20162 \u2264 \u2016AV >y\u0303 \u2212 v\u20161 + \u221a k \u00b7 \u221a r \u00b7 \u2016y\u03030 \u2212 y\u03020\u20162 by \u2016A\u20162 \u2264 \u221a r \u2264 \u2016AV >y\u0303 \u2212 v\u20161 + \u221a k \u00b7 \u221a r \u00b7 \u03b5 by \u2016y\u03030 \u2212 y\u03020\u20162 \u2264 \u03b5 = \u2016AV >y\u0303 \u2212 v\u20161 + \u221a k \u00b7 \u221a r \u00b7 1/kc1+3 by \u03b5\u2032 = 1/kc1+3 = \u2016AV >y\u0303 \u2212 v\u20161 + 1/k. by r = O(kc1)\nand so if \u2016AV >y\u0303 \u2212 v\u20161 is at most O(k\u03b3) then \u2016Ay\u0302 \u2212 v\u20161 is at most O(k\u03b3). For j \u2208 [t], we now use notation E4(vj) to denote the event: \u2203y0 \u2208 Rn with \u2016y0\u20161 \u2264 O(k\u03b3) and each non-zero coordinate of y0 is greater than 1/k\u03b2 , at least k\u2212O(k2\u03b3)\u2212O(k/ log k) coordinates of AV >y0\u2212vj in absolute value are at most O(1/ log k). Based on the previous argument, conditioned on E\u0302(A, V, \u03b2, \u03b3),\nPr[E4(vj)] \u2264 2\u2212\u0398(k).\nAlso notice that, conditioned on E\u0302(A, V, \u03b2, \u03b3), \u2200j \u2208 [t], E4(vj) are independent. Thus, conditioned on E\u0302(A, V, \u03b2, \u03b3), due to the Chernoff bound, the probability that there are dt/10e such j that E4(vj) happens is at most 2\u2212\u0398(kt). We define E5(vj) to be the event: \u2203y \u2208 Rn, \u2016AV >y \u2212 vj\u2016 \u2264 O(k\u03b3) with \u2016y\u20161 \u2264 O(k\u03b3). Similar to the proof of Claim G.23, conditioned on E\u0302(A, V, \u03b2, \u03b3), E5(vj) implies E4(vj). Thus, conditioned on E\u0302(A, V, \u03b2, \u03b3), the probability that there are dt/10e such j that E5(vj) happens is at most 2\u2212\u0398(tk). Then, we complete the proof.\nTheorem G.27. For any k \u2265 1, and any constants c1, c2 which satisfy c2 \u2212 2 > c1 > 1, let r = \u0398(kc1), n = \u0398(kc2), and let A(k, n) denote a distribution over n\u00d7 (k + n) matrices where each entry of the first n \u00d7 k matrix is i.i.d. Gaussian N(0, 1) and the next n \u00d7 n matrix is an identity matrix. For any fixed r \u00d7 n matrix S and a random matrix A\u0302 \u223c A(k, n), with probability at least 1\u2212O(k1+ c1\u2212c2 2 )\u2212 2\u2212\u0398(k), there is no algorithm that is able to output a matrix B \u2208 Rn\u00d7r such that\n\u2016BSA\u0302\u2212 A\u0302\u20161 \u2264 O(k0.5\u2212\u03b5) min rank\u2212k A\u2032 \u2016A\u2032 \u2212 A\u0302\u20161,\nwhere \u03b5 > 0 is a constant which can be arbitrarily small.\nProof. For convenience, we let \u03b3 = 0.5\u2212 \u03b5 be a constant which can be arbitrarily close to 0.5. Since the last n columns of A\u0302 is an identity matrix, we can fit the first k columns of A\u0302, so we have\nmin rank\u2212k A\u2032\n\u2016A\u2032 \u2212 A\u0302\u20161 \u2264 n.\nNow, we want to argue that, for a fixed S, with high probability, for any rank-k n \u00d7 r matrix B, the cost\n\u2016BSA\u0302\u2212 A\u0302\u20161 \u2265 \u2126(n \u00b7 k\u03b3).\nThus, the approximation gap will be at least \u2126(k\u03b3). We denote the SVD of S = US\u03a3SV >S where US \u2208 Rr\u00d7r,\u03a3S \u2208 Rr\u00d7r, VS \u2208 Rn\u00d7r. Then, we can rewrite\n\u2016BSA\u0302\u2212 A\u0302\u20161 = \u2016BUS\u03a3SV >S A\u0302\u2212 A\u0302\u20161\n= n\u2211 l=1 \u2016(BUS\u03a3S)l(V >S A\u0302)\u2212 A\u0302l\u20161\n\u2265 \u2211\nl:\u2016V lS\u2016 2 2\u22642r/n\n\u2016(BUS\u03a3S)l(V >S A\u0302)\u2212 A\u0302l\u20161. (32)\nThe first equality follows from the SVD of S. The second equality follows from the fact that the `1-norm of a matrix is the sum of `1-norms of rows. The third inequality follows since we just look at the cost on a part of the rows.\nWe use \u03b2l to denote (BUS\u03a3S)l. We look at a fixed row l, then the cost on this row is:\n\u2016\u03b2l(V >S A\u0302)\u2212 A\u0302l\u20161 =\u2016\u03b2l(V >S A\u0302)[1:k] \u2212 (A\u0302l)[1:k]\u20161 + \u2016\u03b2l(V >S A\u0302)[k+1:k+n] \u2212 (A\u0302l)[k+1:n+k]\u20161 \u2265\u2016\u03b2l(V >S A\u0302)[1:k] \u2212 (A\u0302l)[1:k]\u20161 + \u2016\u03b2l(V >S A\u0302)[k+1:k+n]\u20161 \u2212 \u2016(A\u0302l)[k+1:n+k]\u20161 \u2265\u2016\u03b2l(V >S A\u0302)[1:k] \u2212 (A\u0302l)[1:k]\u20161 + \u2016\u03b2l(V >S A\u0302)[k+1:k+n]\u20161 \u2212 1 \u2265\u2016\u03b2l(V >S A\u0302)[1:k] \u2212 (A\u0302l)[1:k]\u20161 + \u2016\u03b2lV >S \u20161 \u2212 1. (33)\nwhere (V >S A\u0302)[1:k] denotes the first k columns of (V > S A\u0302), and similarly, (V > S A\u0302)[k+1:k+n] denotes the last n columns of (V >S A\u0302). The first equality is because we can compute the sum of `1 norms on the first k coordinates and `1 norm on the last n coordinates. The first inequality follows from the triangle inequality. The second inequality follows since the last n columns of A\u0302 form an identity, so there is exactly one 1 on the last n columns in each row. The third inequality follows since the last n columns of A\u0302 form an identity. Let Dl be a distribution over Dl \u2208 R(r+1)\u00d7k, where\nDl =\n[ (V >S A\u0302)[1:k]\n(A\u0302l)[1:k]\n] .\nLet G be a distribution over G \u2208 R(r+1)\u00d7k where each entry of G is drawn from i.i.d. N(0, 1). According to Lemma G.16, we have\nDTV(Dl,G) \u2264 O(k\u2016(VS)l\u20162) + 2\u2212\u0398(k). (34)\nLet A = G[1:r], v = Gr+1. Due to Lemma G.20, with probability at least 1\u22122\u2212\u0398(k), E\u0302(A>, VS , 0.75\u2212 \u03b3/2, \u03b3) happens. Then conditioned on E\u0302(A>, VS , 0.75\u2212\u03b3/2, \u03b3), due to Lemma G.22, with probability at most 2\u2212\u0398(k), there exists \u03b2l such that\n\u2016\u03b2lA\u2212 v\u20161 + \u2016\u03b2lV >S \u20161 = o(k\u03b3).\nCombined with Equation (34), we can get that for a fixed l, with probability at most O(k\u2016(VS)l\u20162)+ 2\u2212\u0398(k), there exists \u03b2l such that\n\u2016\u03b2l(V >S A\u0302)[1:k] \u2212 (A\u0302l)[1:k]\u20161 + \u2016\u03b2lV >S \u20161 = o(k\u03b3).\nWhen \u2016(VS)l\u201622 \u2264 2r/n = \u0398(kc1\u2212c2), this probability is at most \u0398(k1+(c1\u2212c2)/2) + 2\u2212\u0398(k). Since\u2211n l=1 \u2016(VS)l\u201622 = r, there are at most n/2 such l that \u2016(VS)l\u201622 > 2r/n which means that there are at least n/2 such l that \u2016(VS)l\u201622 \u2264 2r/n. Let s be the number of l such that \u2016(VS)l\u201622 \u2264 2r/n, then s > n/2. Let t be a random variable which denotes that the number of l which satisfies \u2016(VS)l\u201622 \u2264 2r/n and achieve\n\u2016\u03b2l(V >S A\u0302)[1:k] \u2212 (A\u0302l)[1:k]\u20161 + \u2016\u03b2lV >S \u20161 = o(k\u03b3),\nat the same time. Then, E[t] \u2264 (O(k\u2016(VS)l\u20162) + 2\u2212\u0398(k))s = (O(k \u221a 2r/n) + 2\u2212\u0398(k))s.\nDue to a Markov inequality, Pr[t > s/2 > n/4] \u2264 O(k \u221a 2r/n) + 2\u2212\u0398(k) = O(k1+(c1\u2212c2)/2) + 2\u2212\u0398(k).\nThe equality follows since r = \u0398(kc1), n = \u0398(kc2). Plugging it into Equation (32), now we can conclude, with probability at least 1\u2212O(k1+(c1\u2212c2)/2)\u2212 2\u2212\u0398(k), \u2200B \u2208 Rn\u00d7r\n\u2016BSA\u0302\u2212 A\u0302\u20161 \u2265 \u2211\nl:\u2016V lS\u2016 2 2\u22642r/n\n\u2016(BUS\u03a3S)l(V >S A\u0302)\u2212 A\u0302l\u20161 \u2265 n/4 \u00b7 \u2126(k\u03b3) = \u2126(n \u00b7 k\u03b3).\nTheorem G.28 (Hardness for row subset selection). For any k \u2265 1, any constant c \u2265 1, let n = O(kc), and let A(k, n) denote the same distribution stated in Theorem G.27. For matrix A\u0302 \u223c A(k, n), with positive probability, there is no algorithm that is able to output B \u2208 Rn\u00d7(n+k) in the row span of any r = n/2 rows of A\u0302 such that\n\u2016A\u0302\u2212B\u20161 \u2264 O(k0.5\u2212\u03b1) min rank\u2212k A\u2032 \u2016A\u2032 \u2212 A\u0302\u20161,\nwhere \u03b1 \u2208 (0, 0.5) is a constant which can be arbitrarily small.\nProof. For convenience, we define \u03b3 = 0.5\u2212 \u03b1 which can be an arbitrary constant in (0, 0.5). Since the last n columns of A\u0302 is an identity matrix, we can fit the first k columns of A\u0302, so we have\nmin rank\u2212k A\u2032\n\u2016A\u2032 \u2212 A\u0302\u20161 \u2264 n.\nWe want to argue that \u2200B \u2208 Rn\u00d7(k+n) in the row span of any r = n/2 rows of A\u0302,\n\u2016A\u0302\u2212B\u20161 \u2265 \u2126(n \u00b7 k\u03b3).\nLet A> \u2208 Rn\u00d7k be the first k columns of A\u0302, and let S \u2282 [n] be a set of indices of chosen rows of A\u0302 with k \u2264 |S| \u2264 r. Let MS \u2208 Rk\u00d7n with the ith column MSi = Ai if i \u2208 S and MSi = 0 otherwise. We use M\u0302S \u2208 Rk\u00d7r to be MS without those columns of zeros, so it is a random matrix with each entry i.i.d. Gaussian N(0, 1). Then the minimum cost of using a matrix in the span of rows of A\u0302 with index in S to fit A\u0302 is at least:\u2211\nl 6\u2208S min xl\u2208Rn\n( \u2016MSxl \u2212Al\u20161 + \u2016xl\u20161 \u2212 1 ) .\nThe part of \u2016MSxl \u2212 Al\u20161 is just the cost on the lth row of the first k columns of A\u0302, and the part of \u2016xl\u20161 \u2212 1 is just the lower bound of the cost on the lth row of the last n columns of A\u0302.\nClaim G.29. A, M\u0302S \u2208 Rk\u00d7n, \u03b3 \u2208 (0, 0.5),\nPr [ E\u0302(M\u0302S , Ir, 0.75\u2212 \u03b3/2, \u03b3) \u2223\u2223\u2223\u2223 E\u0302(A, In, 0.75\u2212 \u03b3/2, \u03b3)] = 1. Proof. Suppose E\u0302(A, In, 0.75 \u2212 \u03b3/2, \u03b3) happens. Since MS has just a subset of columns of A, \u2016MS\u20162 \u2264 \u2016A\u20162 \u2264 \u221a n. Notice that \u2200x \u2208 Rn with \u2016x\u20161 \u2264 O(k\u03b3) and each non-zero coordinate of x is at most O(1/k0.75\u2212\u03b3/2), MSx \u2261 MSxS \u2261 AxS , where xS \u2208 Rn has xSi = xi if i \u2208 S and xSi = 0 otherwise. Because \u2016xS\u20161 \u2264 O(k\u03b3) and xS has each coordinate in absolute value at most O(1/k0.75\u2212\u03b3/2), AxS has at most O(k/ log k) coordinates in absolute value at least \u2126(1/ log k). So, MSx = AxS has at most O(k/ log k) coordinates in absolute value at least \u2126(1/ log k).\nWe denote cost(S, l) = minxl\u2208Rn ( \u2016MSxl \u2212Al\u20161 + \u2016xl\u20161 \u2212 1 ) . Since \u2200l 6\u2208 S, Al are independent,\nand they are independent from MS , due to Lemma G.22,\nPr \u2211 l 6\u2208S cost(S, l) \u2264 O(n \u00b7 k\u03b3) \u2223\u2223\u2223\u2223 E\u0302(M\u0302S , Ir, 0.75\u2212 \u03b3/2, \u03b3)  \u2264 2\u2212\u0398(rk). (35) Now we just want to upper bound the following:\nPr \u2203S \u2282 [n], |S| \u2264 r,\u2211 l 6\u2208S cost(S, l) \u2264 O(n \u00b7 k\u03b3)  \u2264 Pr\n\u2203S \u2282 [n], |S| \u2264 r,\u2211 l 6\u2208S cost(S, l) \u2264 O(n \u00b7 k\u03b3) \u2223\u2223\u2223\u2223 E\u0302(A, In, 0.75\u2212 \u03b3/2, \u03b3)  + Pr [ \u00acE\u0302(A, In, 0.75\u2212 \u03b3/2, \u03b3)\n] \u2264\n\u2211 S\u2282[n],|S|\u2264r Pr \u2211 l 6\u2208S cost(S, l) \u2264 O(n \u00b7 k\u03b3) \u2223\u2223\u2223\u2223 E\u0302(A, In, 0.75\u2212 \u03b3/2, \u03b3)  + Pr [ \u00acE\u0302(A, In, 0.75\u2212 \u03b3/2, \u03b3)\n] \u2264\n\u2211 S\u2282[n],|S|\u2264r Pr \u2211 l 6\u2208S cost(S, l) \u2264 O(n \u00b7 k\u03b3) \u2223\u2223\u2223\u2223 E\u0302(A, In, 0.75\u2212 \u03b3/2, \u03b3) + 2\u2212\u0398(k) \u2264\n\u2211 S\u2282[n],|S|\u2264r Pr \u2211 l 6\u2208S cost(S, l) \u2264 O(n \u00b7 k\u03b3) \u2223\u2223\u2223\u2223 E\u0302(M\u0302S , Ir, 0.75\u2212 \u03b3/2, \u03b3) + 2\u2212\u0398(k) \u2264 (n+ 1)r2\u2212\u0398(rk) + 2\u2212\u0398(k)\n\u2264 2\u2212\u0398(rk) + 2\u2212\u0398(k)\n\u2264 2\u2212\u0398(k).\nThe second inequality follows by a union bound. The third inequality follows by Lemma G.20. The fourth inequality follows by Claim G.29. The fifth inequality is due to Equation (35). The sixth inequality follows by n \u2264 O(kc), r = n/2. Thus, with probability at least 1\u22122\u2212\u0398(k), \u2200B \u2208 Rn\u00d7(n+k) which is in the span of any r \u2264 n/2 rows of A\u0302,\n\u2016B \u2212 A\u0302\u20161 \u2265 \u2126(n \u00b7 k\u03b3).\nThen, we have completed the proof.\nDefinition G.30. Given a matrix A \u2208 Rn\u00d7d, a matrix S \u2208 Rr\u00d7n, k \u2265 1 and \u03b3 \u2208 (0, 12), we say that an algorithmM(A,S, k, \u03b3) which outputs a matrix B \u2208 Rn\u00d7r \u201csucceeds\u201d, if\n\u2016BSA\u2212A\u20161 \u2264 k\u03b3 \u00b7 min rank\u2212k A\u2032 \u2016A\u2032 \u2212A\u20161,\nholds.\nTheorem G.31 (Hardness for oblivious embedding). Let \u03a0 denote a distribution over matrices S \u2208 Rr\u00d7n. For any k \u2265 1, any constant \u03b3 \u2208 (0, 12), arbitrary constants c1, c2 > 0 and min(n, d) \u2265 \u2126(kc2), if for all A \u2208 Rn\u00d7d, it holds that\nPr S\u223c\u03a0\n[M(A,S, k, \u03b3) succeeds ] \u2265 \u2126(1/kc1).\nThen r must be at least \u2126(kc2\u22122c1\u22122).\nProof. We borrow the idea from [NN14, PSW16]. We use Yao\u2019s minimax principle [Yao77] here. Let D be an arbitrary distribution over Rn\u00d7d, then\nPr A\u223cD,S\u223c\u03a0\n[M(A,S, k, \u03b3)] \u2265 1\u2212 \u03b4.\nIt means that there is a fixed S0 such that\nPr A\u223cD\n[M(A,S0, k, \u03b3)] \u2265 1\u2212 \u03b4.\nTherefore, we want to find a hard distribution Dhard that if\nPr A\u223cDhard\n[M(A,S0, k, \u03b3)] \u2265 1\u2212 \u03b4,\nS0 must have at least some larger poly(k) rows. Here, we just use the distribution A(k,\u2126(kc2)) described in Theorem G.27 as our hard distribution. We can just fill zeros to expand the size of matrix to n\u00d7 d. We can complete the proof by using Theorem G.27.\nRemark G.32. Actually, in Lemma G.20 and Lemma G.22, the reason we need \u03b2 > \u03b3 > 0 is that we want k\u03b2\u2212\u03b3 = \u03c9(poly(log k)), and the reason we need \u03b2+\u03b3 < 1 is that we want k\u03b2+\u03b3 poly(log k) = o(k). Thus we can replace all the k\u03b3 by \u221a k/ poly(log k), e.g., \u221a k/ log20 k, and replace all the k\u03b2 by \u221a k poly(log k) with a smaller poly(log k), e.g., \u221a k log10 k. Our proofs still work. Therefore, if we replace the approximation ratio in Theorem G.27, Theorem G.28, and Theorem G.31 to be\u221a k/ logc k where c is a sufficiently large constant, the statements are still correct."}, {"heading": "H Hardness", "text": "This section presents our hardness results. Section H.1 states several useful tools from literature. Section H.2 shows that, it is NP-hard to get some multiplicative error. Assuming ETH is true, we provide a stronger hardness result in Section H.3. Section H.4 extends the result from the rank-1 case to the rank-k case."}, {"heading": "H.1 Previous results", "text": "Definition H.1 (\u2016A\u2016\u221e\u21921,[GV15]). Given matrix A \u2208 Rn\u00d7d,\n\u2016A\u2016\u221e\u21921 = min x\u2208{\u22121,+1}n,y\u2208{\u22121,+1}d\nx>Ay.\nThe following lemma says that computing \u2016A\u2016\u221e\u21921 for matrix A with entries in {\u22121,+1} is equivalent to computing a best {\u22121,+1} matrix which is an `1 norm rank-1 approximation to A.\nLemma H.2 (Lemma 3 of [GV15]). Given matrix A \u2208 {\u22121,+1}n\u00d7d,\n\u2016A\u2016\u221e\u21921 + min x\u2208{\u22121,+1}n,y\u2208{\u22121,+1}d\n\u2016A\u2212 xy>\u20161 = nd.\nLemma H.3 (Theorem 2 of [GV15]). Given A \u2208 {\u22121,+1}n\u00d7d, we have\nmin x\u2208{\u22121,+1}n,y\u2208{\u22121,+1}d \u2016A\u2212 xy>\u20161 = min x\u2208Rn,y\u2208Rd \u2016A\u2212 xy>\u20161.\nCombining with Lemma H.2 and Lemma H.3, it implies that computing \u2016A\u2016\u221e\u21921 for A \u2208 {\u22121,+1}n\u00d7d is equivalent to computing the best `1 norm rank-1 approximation to the matrix A:\n\u2016A\u2016\u221e\u21921 + min x\u2208Rn,y\u2208Rd\n\u2016A\u2212 xy>\u20161 = nd.\nTheorem H.4 (NP-hard result, Theorem 1 of [GV15]). Computing \u2016A\u2016\u221e\u21921 for matrix A \u2208 {\u22121,+1}n\u00d7d is NP-hard.\nThe proof of the above theorem in [GV15] is based on the reduction from MAX-CUT problem. The above theorem implies that computing minx\u2208Rn,y\u2208Rd \u2016A\u2212 xy>\u20161 is also NP-hard.\nH.2 Extension to multiplicative error `1-low rank approximation\nThe previous result only shows that solving the exact problem is NP-hard. This section presents a stronger hardness result, which says that, it is still NP-hard even if the goal is to find a solution that is able to achieve some multiplicative error. The proof in this section and the next section are based on the reduction from the MAX-CUT problem. For recent progress on MAX-CUT problem, we refer the readers to [GW95, BGS98, TSSW00, H\u00e5s01, KKMO07, FLP15].\nTheorem H.5. Given A \u2208 {\u22121,+1}n\u00d7d, computing an x\u0302 \u2208 Rn, y\u0302 \u2208 Rd s.t.\n\u2016A\u2212 x\u0302>y\u0302\u20161 \u2264 (1 + 1\nnd ) min x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161\nis NP-hard.\nMAX-CUT decision problem: Given a positive integer c\u2217 and an unweighted graph G = (V,E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G has at least c\u2217 edges.\nLemma H.6. MAX-CUT decision problem is NP-hard.\nWe give the definition for the Hadamard matrix,\nDefinition H.7. The Hadamard matrix Hp of size p \u00d7 p is defined recursively : [ Hp/2 Hp/2 Hp/2 \u2212Hp/2 ] with H2 = [ +1 +1 +1 \u22121 ] .\nFor simplicity, we use H to denote Hp in the rest of the proof. Recall the reduction shown in [GV15] which is from MAX-CUT to computing \u2016 \u00b7 \u2016\u221e\u21921 for {\u22121,+1} matrices. We do the same thing: for a given graph G = (V,E), we construct a matrix A \u2208 {\u22121,+1}n\u00d7d where n = p|E| and d = p|V |. Notice that p = poly(|E|, |V |) is a parameter which will be determined later, and also p is a power of 2.\nWe divide the matrix A into |E| \u00d7 |V | blocks, and each block has size p\u00d7 p. For e \u2208 [|E|], if the eth edge has endpoints i \u2208 [|V |], j \u2208 [|V |] and i < j, let all the p\u00d7 p elements of (e, i) block of A be 1, all the p\u00d7 p elements of (e, j) block of A be \u22121, and all the (e, l) block of A be p\u00d7 p Hadamard matrix H for l 6= i, j.\nClaim H.8 (Lower bound of \u2016A\u2016\u221e\u21921, proof of Theorem 1 of [GV15]). If there is a cut of G with cut size at least c,\n\u2016A\u2016\u221e\u21921 \u2265 2p2c\u2212 |E||V |p3/2.\nClaim H.9 (Upper bound of \u2016A\u2016\u221e\u21921, proof of Theorem 1 of [GV15]). If the max cut of G has fewer than c edges,\n\u2016A\u2016\u221e\u21921 \u2264 2p2(c\u2212 1) + |E||V |p3/2.\nRemark H.10. In [GV15], they set p as a power of 2 and p > |E|2|V |2. This implies\n\u2200c \u2208 [|E|], 2p2(c\u2212 1) + |E||V |p3/2 < 2p2c\u2212 |E||V |p3/2.\nTherefore, according to Claim H.8 and Claim H.9, if we can know the precise value of \u2016A\u2016\u221e\u21921, we can decide whether G has a cut with cut size at least c\u2217.\nFor convenience, we use T \u2217 to denote \u2016A\u2016\u221e\u21921 and use L\u2217 to denote\nmin x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161.\nAlso, we use L to denote a (1 + 1nd) relative error approximation to L \u2217, which means:\nL\u2217 \u2264 L \u2264 (1 + 1 nd )L\u2217.\nWe denote T as nd\u2212 L.\nProof of Theorem H.5. Because L\u2217 \u2264 L \u2264 (1 + 1nd)L \u2217, we have:\nnd\u2212 L\u2217 \u2265 nd\u2212 L \u2265 nd\u2212 (1 + 1 nd )L\u2217.\nDue to Lemma H.2 and the definition of T , it has:\nT \u2217 \u2265 T \u2265 T \u2217 \u2212 1 nd L\u2217.\nNotice that A is a {\u22121,+1} matrix, we have\nL\u2217 \u2264 \u2016A\u20161 \u2264 2nd.\nThus,\nT \u2217 \u2265 T \u2265 T \u2217 \u2212 2.\nIt means\nT + 2 \u2265 T \u2217 \u2265 T.\nAccording to Claim H.16, if G has a cut with cut size at least c, we have:\nT + 2 \u2265 T \u2217 \u2265 2p2c\u2212 |E||V |p3/2.\nThat is\nT \u2265 2p2c\u2212 |E||V |p3/2 \u2212 2.\nAccording to Claim H.17, if the max cut of G has fewer than c edges,\nT \u2264 T \u2217 \u2264 2p2(c\u2212 1) + |E||V |p3/2.\nLet p be a power of 2 and p > |E|3|V |3, we have\n2p2(c\u2212 1) + |E||V |p3/2 < 2p2c\u2212 |E||V |p3/2 \u2212 2.\nTherefore, we can decide whether G has a cut with size at least c based on the value of T . Thus, if we can compute x\u0302 \u2208 Rn, y\u0302 \u2208 Rd s.t.\n\u2016A\u2212 x\u0302>y\u0302\u20161 \u2264 (1 + 1\nnd ) min x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161,\nin polynomial time, it means we can compute L and T in polynomial time, and we can solve MAX-CUT decision problem via the value of T , which leads to a contradiction.\nH.3 Usin the ETH assumption\nThe goal of this section is to prove Theorem H.13. We first introduce the definition of 3-SAT and Exponential Time Hypothesis(ETH). For the details and background of 3-SAT problem, we refer the readers to [AB09].\nDefinition H.11 (3-SAT problem). Given an r variables and m clauses conjunctive normal form CNF formula with size of each clause at most 3, the goal is to decide whether there exists an assignment for the r boolean variables to make the CNF formula be satisfied.\nHypothesis H.12 (Exponential Time Hypothesis (ETH) [IPZ98]). There is a \u03b4 > 0 such that 3-SAT problem defined in Definition H.11 cannot be solved in O(2\u03b4r) running time.\nThe main lower bound is stated as follows:\nTheorem H.13. Unless ETH(see Hypothesis H.12) fails, for arbitrarily small constant \u03b3 > 0, given some matrix A \u2208 {\u22121,+1}n\u00d7d, there is no algorithm can compute x\u0302 \u2208 Rn, y\u0302 \u2208 Rd s.t.\n\u2016A\u2212 x\u0302>y\u0302\u20161 \u2264 (1 + 1\nlog1+\u03b3 nd ) min x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161,\nin (nd)O(1) running time.\nBefore we prove our lower bound, we introduce the following theorem which is used in our proof.\nDefinition H.14 (MAX-CUT decision problem). Given a positive integer c\u2217 and an unweighted graph G = (V,E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G has at least c\u2217 edges.\nTheorem H.15 (Theorem 6.1 in [FLP15]). There exist constants a, b \u2208 (0, 1) and a > b, such that, for a given MAX-CUT (see Definition H.14) instance graph G = (E, V ) which is an n-vertices 5-regular graph, if there is an algorithm in time 2o(n) which can distinguish the following two cases:\n1. At least one cut of the instance has at least a|E| edges,\n2. All cuts of the instance have at most b|E| edges,\nthen ETH(see Hypothesis H.12) fails.\nProof of Theorem H.13. We prove it by contradiction. We assume, for any given A \u2208 {\u22121,+1}n\u00d7d, there is an algorithm can compute x\u0302 \u2208 Rn, y\u0302 \u2208 Rd s.t.\n\u2016A\u2212 x\u0302>y\u0302\u20161 \u2264 (1 + 1\nW ) min x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161,\nin time poly(nd), where W = log1+\u03b3 d for arbitrarily small constant \u03b3 > 0. Then, we show the following. There exist constants a, b \u2208 [0, 1], a > b, for a given MAX-CUT instance G = (V,E) with |E| = O(|V |), such that we can distinguish whether G has a cut with size at least a|E| or all the cuts of G have size at most b|E| in 2o(|V |) time, which leads to a contradiction to Theorem H.15.\nRecall the reduction shown in [GV15] which is from MAX-CUT to computing \u2016 \u00b7 \u2016\u221e\u21921 for {\u22121,+1} matrices. We do similar things here: for a given graph G = (V,E) where |E| = O(|V |), we construct a matrix A \u2208 {\u22121,+1}n\u00d7d where n = p|E| and d = p|V |. Notice that p is a parameter which will be determined later, and also p is a power of 2.\nWe divide the matrix A into |E| \u00d7 |V | blocks, and each block has size p\u00d7 p. For e \u2208 [|E|], if the eth edge has endpoints i \u2208 [|V |], j \u2208 [|V |] and i < j, let all the p\u00d7 p elements of (e, i) block of A be 1, all the p\u00d7 p elements of (e, j) block of A be \u22121, and all the (e, l) block of A be p\u00d7 p Hadamard matrix H for l 6= i, j.\nWe can construct the matrix in nd time, which is p2|E||V |. We choose p to be the smallest number of power of 2 which is larger than 2 2 a\u2212b |V |\n1\u2212 \u03b310 for some \u03b3 > 0. Thus, the time for construction of the matrix A is O(nd) = 2O(|V |\n1\u2212 \u03b310 ). We will show, if we can compute a (1 + 1/W )-approximation to A, we can decide whether G has a cut with size at least a|E| or has no cut with size larger than b|E|. For convenience, we use T \u2217 to denote \u2016A\u2016\u221e\u21921 and use L\u2217 to denote\nmin x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161.\nAlso, we use L to denote a (1 + 1W ) relative error approximation to L \u2217, which means:\nL\u2217 \u2264 L \u2264 (1 + 1 W )L\u2217.\nWe denote T as nd\u2212 L. Because L\u2217 \u2264 L \u2264 (1 + 1W )L \u2217, we have:\nnd\u2212 L\u2217 \u2265 nd\u2212 L \u2265 nd\u2212 (1 + 1 W )L\u2217.\nDue to Lemma H.2 and the definition of T , it has:\nT \u2217 \u2265 T \u2265 T \u2217 \u2212 1 W L\u2217.\nNotice that A is a {\u22121,+1} matrix, we have\nL\u2217 \u2264 \u2016A\u20161 \u2264 2nd.\nThus,\nT \u2217 \u2265 T \u2265 T \u2217 \u2212 2nd/W.\nIt means\nT + 2nd/W \u2265 T \u2217 \u2265 T.\nClaim H.16 (Lower bound of \u2016A\u2016\u221e\u21921, proof of Theorem 1 of [GV15]). If there is a cut of G with cut size at least c,\n\u2016A\u2016\u221e\u21921 \u2265 2p2c\u2212 |E||V |p3/2.\nClaim H.17 (Upper bound of \u2016A\u2016\u221e\u21921, proof of Theorem 1 of [GV15]). If the max cut of G has fewer than c edges,\n\u2016A\u2016\u221e\u21921 \u2264 2p2(c\u2212 1) + |E||V |p3/2.\nAccording to Claim H.16, if G has a cut with cut size at least a|E|, we have:\nT + 2nd/W \u2265 T \u2217 \u2265 2p2a|E| \u2212 |E||V |p3/2.\nThat is\nT \u2265 2p2a|E| \u2212 |E||V |p3/2 \u2212 2nd/W. (36)\nAccording to Claim H.17, if the max cut of G has fewer than b|E| edges,\nT \u2264 T \u2217 \u2264 2p2b|E|+ |E||V |p3/2. (37)\nUsing these conditions p \u2265 2 2 a\u2212b |V | 1\u2212 \u03b310 , d = p|V |,W \u2265 log1+\u03b3 d, we can lower bound |W | by |V |\nup to some constant,\nW \u2265 log1+\u03b3 d by W \u2265 log1+\u03b3 d = log1+\u03b3(p|V |) by d = p|V | = (log |V |+ log p)1+\u03b3\n\u2265 (log |V |+ 2 a\u2212 b |V |1\u2212 \u03b3 10 )1+\u03b3 by p \u2265 2 2 a\u2212b |V |\n1\u2212 \u03b310\n\u2265 2 a\u2212 b |V |. by (1\u2212 \u03b3/10)(1 + \u03b3) > 1 for \u03b3 small enough (38)\nThus, we can upper bound 1/W in the following sense,\n1 W \u2264 a\u2212 b 2|V | \u2264 a\u2212 b |V | \u2212 p\u2212 1 2 , (39)\nwhere the first inequality follows by Equation (38) and the second inequality follows by p \u2265 2 2 a\u2212b |V |\n1\u2212 \u03b310 , \u03b3 is sufficient small, and |V | is large enough. Now, we can conclude,\n1 W \u2264 a\u2212 b |V | \u2212 p\u2212 1 2\n\u21d0\u21d2 p2|E||V |/W \u2264 (a\u2212 b)p2|E| \u2212 |E||V |p 3 2\nby multiplying p2|E||V | on both sides\n\u21d0\u21d2 2nd/W \u2264 2(a\u2212 b)p2|E| \u2212 2|E||V |p 3 2\nby multiplying 2 on both sides and p2|E||V | = nd \u21d0\u21d2 2p2b|E|+ |E||V |p3/2 < 2p2a|E| \u2212 |E||V |p3/2 \u2212 2nd/W, (40)\nby adding 2p2b|E|+ |E||V |p3/2 \u2212 2nd/W on both sides\nwhich implies that Equation (40) is equivalent to Equation (39). Notice that the LHS of (40) is exactly the RHS of (37) and the RHS of (40) is exactly the RHS of (36). Therefore, we can decide whether G has a cut with size larger than a|E| or has no cut with size larger than b|E|.\nThus, if we can compute x\u0302 \u2208 Rn, y\u0302 \u2208 Rd s.t.\n\u2016A\u2212 x\u0302>y\u0302\u20161 \u2264 (1 + 1\nlog1+\u03b3 d ) min x\u2208Rn,y\u2208Rd\n\u2016A\u2212 x>y\u20161,\nin poly(nd) time, which means we can compute L and T in poly(nd) time. Notice that nd = p2|E||V |, |E| = O(|V |), p \u2265 2 2 a\u2212b |V | 1\u2212 \u03b310 , it means poly(nd) = 2O(|V | 1\u2212 \u03b310 ). Because we decide whether G has a cut with size larger than a|E| or has no cut with size larger than b|E| via the value of T , we can solve it in 2O(|V | 1\u2212 \u03b310 ) time which leads to a contradiction to Theorem H.15.\nH.4 Extension to the rank-k case\nThis section presents a way of reducing the rank-k case to the rank-1 case. Thus, we can obtain a lower bound for general k \u2265 1 under ETH.\nTheorem H.18. For any constants c1 > 0, c2 > 0 and c3 > 0, and any constant c4 \u2265 10(c1 + c2 + c3 + 1), given any matrix A \u2208 Rn\u00d7n with absolute value of each entry bounded by nc1, we define a block diagonal matrix A\u0303 \u2208 R(n+k\u22121)\u00d7(n+k\u22121) as\nA\u0303 =  A 0 0 \u00b7 \u00b7 \u00b7 0 0 B 0 \u00b7 \u00b7 \u00b7 0 0 0 B \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 B  ,\nwhere B = nc4. If A\u0302 is an `1-norm rank-k C-approximation solution to A\u0303, i.e.,\n\u2016A\u0302\u2212 A\u0303\u20161 \u2264 C \u00b7 min rank\u2212k A\u0302\u2032\n\u2016A\u0302\u2032 \u2212 A\u0303\u20161,\nwhere C \u2208 [1, nc3 ], then there must exist j\u2217 \u2208 [n] such that\nmin v\u2208Rn\n\u2016A\u0302[1:n]j\u2217 v > \u2212A\u20161 \u2264 C \u00b7 min u,v\u2208Rn \u2016uv> \u2212A\u20161 + 1/nc2 ,\ni.e., the first n coordinates of the column j\u2217 of A\u0302 can give an `1-norm rank-1 C-approximation to A.\nProof. The first observation is that because we can use a rank-1 matrix to fit A and use a rank-(k\u22121) matrix to fit other Bs, we have\nmin rank\u2212k A\u0302\u2032 \u2016A\u0302\u2032 \u2212 A\u0303\u20161 \u2264 min u,v\u2208Rn \u2016uv> \u2212A\u20161 \u2264 \u2016A\u20161. (41)\nClaim H.19. Let A\u0302 denote the rank-k C-approximate solution to A\u0303. Let Z \u2208 R(n+k\u22121)\u00d7(k\u22121) denote the rightmost k \u2212 1 columns of A\u0302, then, rank(Z) = k \u2212 1.\nProof. Consider the (k \u2212 1) \u00d7 (k \u2212 1) submatrix Z [n+1:n+k\u22121] of Z. Each element on the diagonal of this submatrix should be at least B \u2212 C\u2016A\u20161, and each element not on the diagonal of the submatrix should be at most C\u2016A\u20161. Otherwise \u2016A\u0302 \u2212 A\u0303\u20161 > C\u2016A\u20161 which will lead to a contradiction. Since B = nc4 is sufficiently large, Z [n+1:n+k\u22121] is diagonally dominant. Thus rank(Z) \u2265 rank(Z [n+1:n+k\u22121]) = k \u2212 1. Because Z only has k \u2212 1 columns, rank(Z) = k \u2212 1.\nClaim H.20. \u2200x \u2208 Rk\u22121, i \u2208 [n], \u2203j \u2208 {n+ 1, n+ 2, \u00b7 \u00b7 \u00b7 , n+ k \u2212 1} such that,\n|(Zx)j | |(Zx)i| \u2265 B 2(k \u2212 1)C\u2016A\u20161 .\nProof. Without loss of generality, we can let \u2016x\u20161 = 1. Thus, there exists j such that |xj | \u2265 1k\u22121 . So we have\n|(Zx)n+j | = | k\u22121\u2211 i=1 Zn+j,ixi|\n\u2265 |Zn+j,jxj | \u2212 \u2211 i 6=j |Zn+j,ixi|\n\u2265 (B \u2212 C\u2016A\u20161)|xj | \u2212 \u2211 i 6=j |xi|C\u2016A\u20161\n\u2265 (B \u2212 C\u2016A\u20161)/(k \u2212 1)\u2212 C\u2016A\u20161\n\u2265 B 2(k \u2212 1) .\nThe second inequality follows because |Zn+j,j | \u2265 B\u2212C\u2016A\u20161 and \u2200i 6= j, |Zn+j,i| \u2264 C\u2016A\u20161 (otherwise \u2016A\u0302\u2212A\u0303\u20161 > C\u2016A\u20161 which leads to a contradiction.) The third inequality follows from |xj | > 1/(k\u22121) and \u2016x\u20161 = 1. The fourth inequality follows since B is large enough such that B2(k\u22121) \u2265 C\u2016A\u20161 k\u22121 + C\u2016A\u20161. Now we consider any q \u2208 [n]. We have\n|(Zx)q| = k\u22121\u2211 i=1 |Zq,ixi| \u2264 max i\u2208[k\u22121] |Zq,i| \u00b7 k\u22121\u2211 i=1 |xi| \u2264 C\u2016A\u20161.\nThe last inequality follows that \u2016x\u20161 = 1 and \u2200i \u2208 [k \u2212 1], Zq,i \u2264 C\u2016A\u20161. Otherwise, \u2016A\u0302 \u2212 A\u0303\u20161 > C\u2016A\u20161 which will lead to a contradiction.\nLook at |(Zx)n+j |/|(Zx)q|, it is greater than B2(k\u22121)C\u2016A\u20161 .\nWe now look at the submatrix A\u0302[1:n][1:n], we choose i \u2217, j\u2217 \u2208 [n] such that\n|A\u0302i\u2217,j\u2217 | \u2265 1/nc2\u22122.\nIf there is no such (i\u2217, j\u2217), it means that we already found a good rank-1 approximation to A\n\u20160\u2212A\u20161 \u2264 \u20160\u2212 A\u0302[1:n][1:n]\u20161 + \u2016A\u0302 [1:n] [1:n] \u2212A\u20161\n\u2264 \u20160\u2212 A\u0302[1:n][1:n]\u20161 + \u2016A\u0302\u2212 A\u0303\u20161\n\u2264 1/nc2 + \u2016A\u0302\u2212 A\u0303\u20161 \u2264 1/nc2 + C min\nrank\u2212k A\u0302\u2032 \u2016A\u0302\u2032 \u2212 A\u0303\u20161\n\u2264 1/nc2 + C min u,v\u2208Rn \u2016uv> \u2212A\u20161,\nwhere 0 is an n\u00d7 n all zeros matrix. The second inequality follows since A\u0302[1:n][1:n] \u2212 A is a submatrix of A\u0302\u2212 A\u0303. The third inequality follows since each entry of A\u0302[1:n][1:n] should be no greater than 1/n c2\u22122 (otherwise, we can find (i\u2217, j\u2217)). The last inequality follows from equation 41.\nClaim H.21. A\u0302j\u2217 is not in the column span of Z, i.e.,\n\u2200x \u2208 Rk\u22121, A\u0302j\u2217 6= Zx.\nProof. If there is an x such that A\u0302j\u2217 = Zx, it means (Zx)i\u2217 = A\u0302i\u2217,j\u2217 \u2265 1/nc2\u22122. Due to Claim H.20, there must exist i\u2032 \u2208 {n+ 1, n+ 2, \u00b7 \u00b7 \u00b7 , n+ k \u2212 1} such that (Zx)i\u2032 \u2265 B2(k\u22121)C\u2016A\u20161nc2\u22122 . Since B is sufficiently large, A\u0302i\u2032,j\u2217 = (Zx)i\u2032 > C\u2016A\u20161 which implies that \u2016A\u0302\u2212 A\u0303\u20161 > C\u2016A\u20161, and so leads to a contradiction.\nDue to Claim H.19 and Claim H.21, the dimension of the subspace spanned by A\u0302j\u2217 and the column space of Z is k. Since A\u0302 has rank at most k, it means that each column of A\u0302 can be written as a linear combination of A\u0302j\u2217 and the columns of Z.\nNow consider the jth column A\u0302j of A\u0302 for j \u2208 [n]. We write it as\nA\u0302j = \u03b1j \u00b7 A\u0302j\u2217 + Zxj .\nClaim H.22. \u2200j \u2208 [n], \u03b1j \u2264 2C\u2016A\u20161nc2+2.\nProof. Otherwise, suppose \u03b1j > 2C\u2016A\u20161nc2+2. We have\n|(Zxj)i\u2217 | \u2265 \u03b1j \u00b7 |A\u0302i\u2217,j\u2217 | \u2212 |A\u0302i\u2217,j |\n\u2265 \u03b1j \u00b7 1\nnc2\u22122 \u2212 |A\u0302i\u2217,j |\n\u2265 1 2 \u03b1j \u00b7 1 nc2\u22122 .\nThe second inequality follows from |A\u0302i\u2217,j\u2217 | \u2265 1/nc2\u22122. The third inequality follows from |A\u0302i\u2217,j | \u2264 \u2016A\u20161 and 12\u03b1j \u00b7 1 nc2\u22122\n\u2265 C\u2016A\u20161 \u2265 \u2016A\u20161. Due to Claim H.20, there exists i \u2208 {n+1, n+2, \u00b7 \u00b7 \u00b7 , n+k\u22121} such that |(Zxj)i| \u2265 B2(k\u22121)C\u2016A\u20161 \u00b7\n1 2\u03b1j \u00b7 1 nc2\u22122 . For sufficiently large B, we can have |(Zxj)i| \u2265 \u03b1jB1/2. Then we look at\n|A\u0302i,j | \u2265 |(Zxj)i| \u2212 \u03b1j |A\u0302i,j\u2217 | \u2265 \u03b1j(B1/2 \u2212 C\u2016A\u20161)\n\u2265 \u03b1j 1\n2 B1/2.\nThe second inequality follows by |A\u0302i,j\u2217 | \u2264 C\u2016A\u20161, otherwise \u2016A\u0302\u2212 A\u0303\u20161 > C\u2016A\u20161 which will lead to a contradiction. The third inequality follows that B is sufficient large that 12B\n1/2 > C\u2016A\u20161. Since |A\u0302i,j | \u2265 \u03b1j 12B\n1/2 > C\u2016A\u20161, it contradicts to the fact \u2016A\u0302\u2212 A\u0303\u20161 \u2264 C\u2016A\u20161. Therefore, \u2200j \u2208 [n], \u03b1j \u2264 2C\u2016A\u20161nc2+2.\nClaim H.23. \u2200j \u2208 [n], i \u2208 {n+ 1, n+ 2, \u00b7 \u00b7 \u00b7 , n+ k \u2212 1}, |(Zxj)i| \u2264 4C2\u2016A\u201621nc2+2\nProof. Consider j \u2208 [n], i \u2208 {n+ 1, n+ 2, \u00b7 \u00b7 \u00b7 , n+ k \u2212 1}, we have\n|(Zxj)i| \u2264 |A\u0302i,j |+ \u03b1j |A\u0302i,j\u2217 | \u2264 C\u2016A\u20161 + \u03b1j \u00b7 C\u2016A\u20161 \u2264 4C2\u2016A\u201621nc2+2.\nThe second inequality follows by |A\u0302i,j | \u2264 C\u2016A\u20161 and |A\u0302i,j\u2217 | \u2264 C\u2016A\u20161, otherwise the \u2016A\u0302\u2212 A\u0303\u20161 will be too large and leads to a contradiction. The third inequality is due to \u03b1j + 1 \u2264 4C\u2016A\u20161nc2+2 via Claim H.22.\nClaim H.24. \u2200j \u2208 [n], \u2016A\u0302[1:n]j \u2212 \u03b1j \u00b7 A\u03021:nj\u2217 \u2016\u221e \u2264 1/nc2\u22122\nProof. Due to Claim H.23 and Claim H.20, \u2200i, j \u2208 [n], we have\n|(Zxj)i| \u2264 4C2\u2016A\u201621nc2+2\nB/(2(k \u2212 1)C\u2016A\u20161) \u2264 1/B1/2.\nThe second inequality follows for a large enough B. Therefore, \u2200i, j \u2208 [n],\n|A\u0302i,j \u2212 \u03b1j \u00b7 A\u0302i,j\u2217 | \u2264 |(Zxj)i| \u2264 1/B1/2 \u2264 1/nc2\u22122.\nThe last inequality follows since B is large enough.\nNow, let us show that A\u03021:nj\u2217 can provide a good rank-1 approximation to A:\n\u2016A\u03021:nj\u2217 \u03b1> \u2212A\u20161 \u2264 \u2016A\u03021:nj\u2217 \u03b1> \u2212 A\u0302 [1:n] [1:n]\u20161 + \u2016A\u0302 [1:n] [1:n] \u2212A\u20161\n= n\u2211 j=1 \u2016\u03b1jA\u0302j\u2217 \u2212 A\u0302[1:n]j \u20161 + \u2016A\u0302 [1:n] [1:n] \u2212A\u20161\n\u2264 n2 \u00b7 1/nc2\u22122 + \u2016A\u0302[1:n][1:n] \u2212A\u20161\n\u2264 1/nc2 + \u2016A\u0302\u2212 A\u0303\u20161 \u2264 1/nc2 + C min\nu,v\u2208Rn \u2016uv> \u2212A\u20161.\nThe first inequality follows by triangle inequality. The first equality is due to the linearity of `1 norm. The second inequality is due to Claim H.24. The third inequality follows since A\u0302[1:n][1:n] \u2212 A is a submatrix of A\u0302\u2212 A\u0303. The fourth inequality is due to the equation 41."}, {"heading": "I Limited independent Cauchy random variables", "text": "This section presents the fundamental lemmas with limited independent Cauchy variables, which will be used in Section J and K. In Section I.1, we provide some notation, definitions and tools from previous work. Section I.2 includes the main result."}, {"heading": "I.1 Notations and Tools", "text": "For a function f : R \u2192 R and nonnegative integer `, f (`) denotes the `th derivative of f , with f (0) = f . We also often use x \u2248 y to state that |x\u2212y| = O( ). We use I[a,b] to denote the indicator function of the interval [a, b].\nTo optimize the communication complexity of our ditributed algorithm, we show that instead of using fully independent Cauchy variables, poly(k, d)-wise independent Cauchy variables suffice.\nWe start by stating two useful Lemmas from previous work [KNW10].\nLemma I.1 (Lemma 2.2 in [KNW10]). There exists an 0 > 0 such that the following holds. Let n be a positive integer and 0 < < 0, 0 < p < 2 be given. Let f : R\u2192 R satisfy \u2016f (`)\u2016\u221e = O(\u03b1`) for all ` \u2265 0, for some \u03b1 satisfying \u03b1p \u2265 log(1/ ). Let k = \u03b1p. Let a \u2208 Rn satisfy \u2016a\u2016p = O(1). Let Xi be a 3Ck-independent family of p-stable random variables. Let X = \u2211 i aiXi and Y = \u2211 i aiYi. Then E[f(x)] = E[f(Y )] +O( ).\nLemma I.2 (Lemma 2.5 in [KNW10]). There exist constants c\u2032, 0 > 0 such that for all c > 0 and 0 < < 0, and for all [a, b] \u2286 R, there exists a function Jc[a,b] : R\u2192 R satisfying:\ni. \u2016(Jc[a,b]) (`)\u2016\u221e = O(c`) for all ` \u2265 0.\nii. For all x such that a, b /\u2208 [x\u2212 , x+ ], and as long as c > c\u2032 \u22121 log3(1/ ), |Jc[a,b](x)\u2212I[a,b](x)| < .\nI.2 Analysis of limited independent random Cauchy variables\nLemma I.3. Given a vector y \u2208 Rn, choose Z to be the t \u00d7 n random Cauchy matrices with 1/t rescaling and t = O(k log k). The variables from different rows are fully independent, and the variables from the same rows are O(1)-wise independent. Then, we have\n\u2016Zy\u20161 & \u2016y\u20161\nholds with probability at least 1\u2212 2\u2212\u2126(t).\nProof. Let S denote the original fully independent matrix and Z denote the matrix for which the entries in the same row are w-wise independent, and the entries from different rows are fully independent. Notice we define the random matrices without rescaling by 1/t and it will be added back at the end. (We will decide w later)\nWe define random variable X such that X = 1 if |(Zy)i| \u2264 150 and X = 0 otherwise. We also define random variable Y such that Y = 1 if |(Sy)i| \u2264 150 and Y = 0 otherwise. Then, we have\nE[X] = Pr [ |(Zy)i| \u2264 1\n50\n] = E [ I[\u2212 1\n50 , 1 50\n]((Zy)i) ] E[Y ] = Pr [ |(Sy)i| \u2264 1\n50\n] = E [ I[\u2212 1\n50 , 1 50\n]((Sy)i) ] The goal is to show that E[X] \u2248 E[Y ]. Following the same idea from [KNW10], we need to argue this chain of inequalities,\nE[I[a,b](X)] \u2248 E[Jc[a,b](X)] \u2248 E[J c [a,b](Y )] \u2248 E[I[a,b](Y )]\nUsing Lemma 2.2 and Lemma 2.5 from [KNW10], choosing sufficiently small constant (which implies w = O(1)), it follows that for each i \u2208 [t], we still have\nPr [ |(Zy)i| > 1\n50 \u2016y\u20161\n] & Pr [ |(Sy)i| > 1\n50 \u2016y\u20161\n] \u2265 0.9\nBecause all rows of Z are fully independent, using the Chernoff bound we can get that\nPr [ \u2016Zy\u20161 . t\u2016y\u20161 ] \u2264 exp(\u2212\u2126(t))\nas we needed for the \u201cno contraction\u201d part of the net argument.\nFor the no dilation, we need to argue that\nLemma I.4. Given a set of vectors {y1, y2, . . . , yd} where yi \u2208 Rn, \u2200i \u2208 [d], choose Z to be the t\u00d7n random Cauchy matrices with 1/t rescaling and t = O(k log k), where the variables from different rows are fully independent, and the variables from the same rows are w-wise independent.\nI. If w = O\u0303(dk), we have d\u2211 i=1 \u2016Zyi\u20161 \u2264 O(log d) d\u2211 i=1 \u2016yi\u20161\nholds with probability at least .999. II. If If w = O\u0303(d), we have\nd\u2211 i=1 \u2016Zyi\u20161 \u2264 O(k log d) d\u2211 i=1 \u2016yi\u20161\nholds with probability at least .999.\nProof. Let m = t. Let S \u2208 Rm\u00d7n denote the original fully independent matrix and Z denote the matrix that for each entry in the same row are w-wise independent, where the entries from different rows are fully independent. (We will decide onw later)\nApplying matrix S to those fixed set of vectors, we have\nd\u2211 i=1 \u2016Syi\u20161 = d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 1 m Sj,l \u00b7 (yi)l| = 1 m d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 Sj,l \u00b7 (yi)l|\nApplying matrix Z to those fixed set of vectors, we have a similar thing,\nd\u2211 i=1 \u2016Zyi\u20161 = d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 1 m Zj,l \u00b7 (yi)l| = 1 m d\u2211 i=1 m\u2211 j=1 | n\u2211 l=1 Zj,l \u00b7 (yi)l|\nThe goal is to argue that, for any i \u2208 [d], j \u2208 [m],\nE [ | n\u2211 l=1 Zj,l \u00b7 (yi)l| \u2223\u2223\u2223\u2223\u03be] . E[| n\u2211 l=1 Sj,l \u00b7 (yi)l| \u2223\u2223\u2223\u2223\u03be]+ \u03b4\nAs long as \u03b4 is small enough, we are in a good shape. Let X = 1\u2016yi\u20161 \u2211n l=1 Zj,l(yi)l, and Y = 1 \u2016yi\u20161 \u2211m l=1 Sj,l(yi)l. Let D be the truncating threshold of each Cauchy random variable. Define T = O(logD). On one hand, we have\nE[|X||\u03be] \u2264 2 Pr[X \u2208 [0, 1]|\u03be] \u00b7 1 + T\u2211 j=0 Pr[X \u2208 (2j , 2j+1]|\u03be] \u00b7 2j+1 \n\u2264 2 1 + T\u2211 j=0 Pr[I(2j ,2j+1)(X) = 1|\u03be] \u00b7 2j+1 \n= 2 1 + T\u2211 j=0 E[I(2j ,2j+1)(X)|\u03be] \u00b7 2j+1  (42)\nOn the other hand, we can show\nE[|Y ||\u03be] \u2265 2 Pr[X \u2208 [0, 1]|\u03be] \u00b7 0 + T\u2211 j=0 Pr[Y \u2208 (2j , 2j+1]|\u03be] \u00b7 2j \n\u2265 2 T\u2211 j=0 Pr[I(2j ,2j+1](Y ) = 1|\u03be] \u00b7 2j \u2265 2 T\u2211 j=0 E[I(2j ,2j+1](Y )|\u03be] \u00b7 2j (43)\nThus, we need to show that, for each j,\nE[I(2j ,2j+1)(X)|\u03be] \u2248 E[I(2j ,2j+1](Y )|\u03be] (44)\nFollowing the same idea from [KNW10], we need to argue this chain of inequalities,\nE[I[a,b](X)] \u2248 E[Jc[a,b](X)] \u2248 E[J c [a,b](Y )] \u2248 E[I[a,b](Y )]\nWe first show E[I[a,b](X)] \u2248 E[Jc[a,b](X)]. Notice that I[a,b] and J[a,b] are within everywhere except for two intervals of length O( ). Also the Cauchy distribution is anticoncentrated (any length-O( ) interval contains O( ) probability mass) and \u2016I[a,b]\u2016\u221e, \u2016Jc[a,b]\u2016\u221e = O(1), these intervals contribute O( ) to the difference.\nSecond, we show E[Jc[a,b](X)] \u2248 E[J c [a,b](Y )]. This directly follows by Lemma I.1 by choosing\n\u03b1 = O( \u22121 log3(1/ )).\nThird, we show E[Jc[a,b](Y )] \u2248 E[I[a,b](Y )]. The argument is similar as the first step, but we need to show anticoncentration of Y . Suppose for any t \u2208 R we had a nonnegative function f ,t : R\u2192 R symmetric about t satisfying:\nI. \u2016f (`)t, \u2016\u221e = O(\u03b1`) for all ` \u2265 0, with \u03b1 = O(1/ ) II. E[ft, (z)] = O( ) for z \u223c D1 III. ft, (t+ ) = \u2126(1) IV. ft, (x) is strictly decreasing as |x\u2212 t| \u2192 \u221e\nBy I, II and Lemma I.1 we could have E[f ,t(Y )] \u2248 E[ft, (z)] = O( ). Then, E[ft, (Y )] \u2265 ft, (t+ ) \u00b7 Pr[Y \u2208 [t\u2212 , t+ ]] = \u2126(Pr[Y \u2208 [t\u2212 , t+ ]]) by III and IV, implying anticoncentration in [t\u2212 , t+ ] as desired. For the details of function ft, , we refer the readers to Section A.4 in [KNW10].\nNow, combining Equation (42), (43) and (44) gives\nE [ | n\u2211 l=1 Zj,l \u00b7 (yi)l| \u2223\u2223\u2223\u2223\u03be] . E[| n\u2211 l=1 Sj,l \u00b7 (yi)l| \u2223\u2223\u2223\u2223\u03be]+ T\u2211 j=0 2j \u00b7 \u00b7 \u2016yi\u20161\n. E [ | n\u2211 l=1 Sj,l \u00b7 (yi)l| \u2223\u2223\u2223\u2223\u03be]+D \u00b7 \u00b7 \u2016yi\u20161\nOverall, for the fixed j, Sj,l is O\u0303(1/ )-independent family of Cauchy random variable. Choosing D = O(dk) and = O(1/D), we can show\n1\nm d\u2211 i=1 m\u2211 j=1 E [ | n\u2211 l=1 Sj,l \u00b7 (yi)l| \u2223\u2223\u2223\u2223\u03be] \u2264 O(log d) d\u2211 i=1 \u2016yi\u20161\nas before. Notice that D \u2016yi\u20161 = O(\u2016yi\u20161). Thus, we complete the proof of the first result. Choosing D = O(dk) and = O(1/d), the dominant term becomes D \u2016yi\u20161 = O(k\u2016yi\u20161). Thus, we complete the proof of second result.\nCorollary I.5. Given U \u2208 Rn\u00d7k, let Z \u2208 Rt\u00d7n be the same as the matrix stated in the Lemma I.3, then with probability at least .95,\n\u2200x \u2208 Rk, \u2016ZUx\u20161 & \u2016Ux\u20161.\nThe proof is very similar to the proof of Lemma D.22. Without loss of generality, we can suppose U is a well-conditioned basis. Due to Lemma I.4, with arbitrarily high constant probability \u2016ZU\u20161 is bounded by poly(t, k). By simply applying the net argument and using Lemma I.3 to take a union bound over net points, we can get the above corollary."}, {"heading": "J Streaming Setting", "text": "Section J.1 provides some notation and definitions about row-update streaming model and the turnstile streaming model. For some recent developments of row-update streaming and turnstile streaming models, we refer the readers to [CW09, KL11, GP13, Lib13, KLM+14, BWZ16] and the references therein. Section J.2 presents our turnstile streaming algorithm. Section J.3 presents our row-update streaming algorithm."}, {"heading": "J.1 Definitions", "text": "Definition J.1 (Row-update model). Let matrix A \u2208 Rn\u00d7d be a set of rows A1, \u00b7 \u00b7 \u00b7 , An. In the row-update streaming model, each row of A will occur in the stream exactly once. But the rows can be in arbitrary order. An algorithm in this model is only allowed a single pass over these rows. At the end of the stream, the algorithm stores some information of A. The space of the algorithm is the total number of words required to store this information during the stream. Here, each word is O(log(nd)) bits.\nDefinition J.2 (Turnstile model). At the beginning, let matrix A \u2208 Rn\u00d7d be a zero matrix. In the turnstile streaming model, there is a stream of update operations, and the ith operation has the form (xi, yi, ci) which means that Axi,yi should be incremented by ci. An algorithm in this model is only allowed a single pass over the stream. At the end of the stream, the algorithm stores some information of A. The space complexity of the algorithm is the total number of words required to store this information during the stream. Here, each word is O(log(nd)) bits.\nJ.2 Turnstile model, poly(k, log(d), log(n)) approximation\nDefinition J.3 (Turnstile model `1-low rank approximation - rank-k subspace version). Given matrix A \u2208 Rn\u00d7d and k \u2208 N+, the goal is to propose an algorithm in the streaming model of Definition J.2 such that\n1. Upon termination, the algorithm outputs a matrix V \u2217 \u2208 Rk\u00d7d.\n2. V \u2217 satisfies that\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The space complexity is as small as possible\nTheorem J.4. Suppose A \u2208 Rn\u00d7d is given in the turnstile streaming model (See Definition J.2), there is an algorithm(in Algorithm 8 without decomposition) which solves the problem in Definition J.3 with constant probability. Further, the space complexity of the algorithm is poly(k) + O\u0303(kd) words.\nProof. Correctness. The correctness is implied by (IV) of Lemma D.11, and the proof of Theorem C.3. Notice that L = T1AR,N = SAT2,M = T1AT2, so X\u0302 \u2208 RO(k log k)\u00d7O(k log k) minimizes\nmin rank\u2212k X\n\u2016T1ARXSAT2 \u2212 T1AT2\u2016F .\nAccording to the proof of Theorem C.3, ARX\u0302SA gives a `1 rank-k poly(k, log(d), log(n))-approximation to A. Because X\u0302 = U\u0302 \u03a3\u0302V\u0302 >, V \u2217 = \u03a3\u0302V\u0302 >SA satisfies:\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\nSpace complexity. Generating O\u0303(kd)-wise independent random Cauchy variables needs O\u0303(kd) bits. The size of L,N and M are k2 log2 k, k2 log3 k and k2 log3 k words separately. So the space of maintaining them is O(k2 log3 k) words. The size of D is O(k log k)\u00d7 d, so maintaining it needs O(kd log k) words. Therefore, the total space complexity of the algorithm is poly(k)+ O\u0303(kd) words.\nIt is easy to extend our algorithm to output a decomposition. The formal definition of the decomposition problem is as the following:\nDefinition J.5 (Turnstile model `1-low rank approximation - rank-k decomposition version). Given matrix A \u2208 Rn\u00d7d and k \u2208 N+, the goal is to propose an algorithm in the streaming model of Definition J.2 such that\n1. Upon termination, the algorithm outputs a matrix U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d.\n2. U\u2217, V \u2217 satisfies\n\u2016A\u2212 U\u2217V \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The space complexity is as small as possible\nTheorem J.6. Suppose A \u2208 Rn\u00d7d is given by the turnstile streaming model (See Definition J.2). There is an algorithm( in Algorithm 8 with decomposition) which solves the problem in Definition J.5 with constant probability. Further, the space complexity of the algorithm is poly(k) + O\u0303(k(d + n)) words.\nProof. Correctness. The only difference from the Algorithm 8 (without decomposition) is that the algorithm maintains C. Thus, finally it can compute U\u2217 = ARU\u0302 . Notice that U\u2217V \u2217 = ARX\u0302SA, according to the proof of Theorem C.3, U\u2217V \u2217 gives a `1 rank-k poly(k, log(d), log(n))-approximation to A.\nSpace complexity. Since the size of C is O(nk log k) words, the total space is poly(k)+O\u0303(k(d+ n)) words.\nJ.3 Row-update model, poly(k) log d approximation\nDefinition J.7 (Row-update model `1-low rank approximation - rank-k subspace version). Given matrix A \u2208 Rn\u00d7d and k \u2208 N+, the goal is to propose an algorithm in the streaming model of Definition J.1 such that\n1. Upon termination, the algorithm outputs a matrix V \u2217 \u2208 Rk\u00d7d.\n2. V \u2217 satisfies that\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k) log(d) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The space complexity is as small as possible\nTheorem J.8. Suppose A \u2208 Rn\u00d7d is given by the row-update streaming model (See Definition J.1), there is an algorithm( in Algorithm 9 without decomposition ) which solves the problem in Definition J.7 with constant probability. Further, the space complexity of the algorithm is poly(k) + O\u0303(kd) words.\nAlgorithm 8 Turnstile Streaming Algorithm 1: procedure TurnstileStreaming(k,S) 2: Construct sketching matrices S \u2208 RO(k log k)\u00d7n, R \u2208 Rd\u00d7O(k log k), T1 \u2208 RO(k log k)\u00d7n, T2 \u2208\nRd\u00d7O(k log2 k) where R, T2 are fully independent random Cauchy matrices, and S, T1 are random Cauchy matrices with fully independent variables across different rows and O\u0303(d)-wise independent variables from the same row.\n3: Initialize matrices: 4: L\u2190 {0}O(k log k)\u00d7O(k log k), N \u2190 {0}O(k log k)\u00d7O(k log2 k). 5: M \u2190 {0}O(k log k)\u00d7O(k log2 k), D \u2190 {0}O(k log k)\u00d7d. 6: if need decomposition then 7: C \u2190 {0}n\u00d7O(k log k). 8: end if 9: for i \u2208 [l] do\n10: Receive update operation (xi, yi, ci) from the data stream S. 11: for r = 1\u2192 O(k log k), s = 1\u2192 O(k log k) do 12: Lr,s \u2190 Lr,s + T1r,xi \u00b7 ci \u00b7Ryi,s. 13: end for 14: for r = 1\u2192 O(k log k), s = 1\u2192 O(k log2 k) do 15: Nr,s \u2190 Nr,s + Sr,xi \u00b7 ci \u00b7 T2yi,s. 16: end for 17: for r = 1\u2192 O(k log k), s = 1\u2192 O(k log2 k) do 18: Mr,s \u2190Mr,s + T1r,xi \u00b7 ci \u00b7 T2yi,s. 19: end for 20: for r = 1\u2192 O(k log k) do 21: Dr,yi \u2190 Dr,s + Sr,xi \u00b7 ci. 22: end for 23: if need decomposition then 24: for s = 1\u2192 O(k log k) do 25: Cxi,s \u2190 Cxi,s + ci \u00b7Ryi,s. 26: end for 27: end if 28: end for 29: Compute the SVD of L = UL\u03a3LV >L . 30: Compute the SVD of N = UN\u03a3NV >N . 31: Compute X\u0302 = L\u2020(ULU>LMVNV > N )kN\n\u2020. 32: Compute the SVD of X\u0302 = U\u0302 \u03a3\u0302V\u0302 >. 33: if need decomposition then 34: return V \u2217 = \u03a3\u0302V\u0302 >D,U\u2217 = CU\u0302 . 35: else 36: return V \u2217 = \u03a3\u0302V\u0302 >D. 37: end if 38: end procedure\nProof. Correctness. Notice that L = T1BR,N = SBT2,M = T1BT2. Thus, X\u0302 \u2208 RO(k log k)\u00d7O(k log k) actually minimizes\nmin rank\u2212k X\n\u2016T1BRXSBT2 \u2212 T1BT2\u2016F .\nAlso notice that B is just taking each row of A and replacing it with its nearest point in the row span of S\u2032A. According to the proof of Theorem C.6 and (IV) of Lemma D.11 BRX\u0302SB gives a poly(k) log d `1 norm rank-k approximation to A. Since X\u0302 = U\u0302 \u03a3\u0302V\u0302 >, V \u2217 = \u03a3\u0302V\u0302 >SB satisfies:\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\nSpace complexity. Constructing sketching matrices needs O\u0303(kd) bits to store random seeds. Maintaining L,N,M needs O(k2 log3 k) words. The cost of storing X\u0302 is also O(k2 log2 k) words. Maintaining D needs O(kd log k) words. Therefore, the total space cost of the algorithm is poly(k)+ O\u0303(kd) words.\nIt is easy to extend our algorithm to output a decomposition. The formal definition of the decomposition problem is as the following:\nDefinition J.9 (Row-update model `1-low rank approximation - rank-k decomposition version). Given matrix A \u2208 Rn\u00d7d and k \u2208 N+, the goal is to propose an algorithm in the streaming model of Definition J.1 such that\n1. Upon termination, the algorithm outputs matrices U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d.\n2. U\u2217, V \u2217 satisfies that\n\u2016A\u2212 U\u2217V \u2217\u20161 \u2264 poly(k) log d \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The space complexity is as small as possible.\nTheorem J.10. Suppose A \u2208 Rn\u00d7d is given by the row-update streaming model (See Definition J.1), there is an algorithm(in Algorithm 9 with decomposition ) which solves the problem in Definition J.9 with constant probability. Further, the space complexity of the algorithm is poly(k) + O\u0303(k(n + d)) words.\nProof. Correctness. The only difference is that the above algorithm maintains C. Thus, We can compute U\u2217 = CU\u0302 in the end. Notice that U\u2217V \u2217 = BRX\u0302SB, according to the proof of Theorem C.6, U\u2217V \u2217 gives a poly(k) log d `1 norm rank-k approximation to A.\nSpace complexity. Since the size of C is nk log k words, the total space is poly(k)+O\u0303(k(n+d)) words."}, {"heading": "K Distributed Setting", "text": "Section K.1 provides some notation and definitions for the Row-partition distributed model and the Arbitrary-partition model. These two models were recently studied in a line of works such as [TD99, QOSG02, BCL05, BRB08, MBZ10, FEGK13, PMvdG+13, KVW14, BKLW14, BLS+16, BWZ16, WZ16]. Section K.2 and K.3 presents our distributed protocols for the Arbitrary-partition distributed model. Section K.4 and K.5 presents our distributed protocols for the Row-partition distributed model.\nAlgorithm 9 Row Update Streaming Algorithm 1: procedure RowUpdateStreaming(k,S) 2: Construct sketching matrices S\u2032 \u2208 RO(k log k)\u00d7n, S \u2208 RO(k log k)\u00d7n, R \u2208 Rd\u00d7O(k log k), T1 \u2208\nRO(k log k)\u00d7n, T2 \u2208 Rd\u00d7O(k log 2 k) where R, T2 are fully independent random Cauchy variables, and S , S\u2032 , T1 are random Cauchy matrices with fully independent random variables from different rows and O\u0303(d)-wise independent in the same row.\n3: Initialize matrices: 4: L\u2190 {0}O(k log k)\u00d7O(k log k), N \u2190 {0}O(k log k)\u00d7O(k log2 k). 5: M \u2190 {0}O(k log k)\u00d7O(k log2 k), D \u2190 {0}O(k log k)\u00d7d. 6: if need decomposition then 7: C \u2190 {0}n\u00d7O(k log k). 8: end if 9: for i \u2208 [n] do\n10: Recieve a row update (i, Ai) from the data stream S. 11: Compute Y \u2217i \u2208 R1\u00d7O(k log k) which minimizes minY \u2208R1\u00d7O(k log k) \u2016Y S\u2032:,iAi \u2212Ai\u20161. 12: Compute Bi = Y \u2217i S \u2032 :,iAi. 13: for r = 1\u2192 O(k log k), s = 1\u2192 O(k log k), j = 1\u2192 d do 14: Lr,s \u2190 Lr,s + T1r,i \u00b7Bi,j \u00b7Rj,s. 15: end for 16: for r = 1\u2192 O(k log k), s = 1\u2192 O(k log2 k), j = 1\u2192 d do 17: Nr,s \u2190 Nr,s + Sr,i \u00b7Bi,j \u00b7 T2j,s. 18: end for 19: for r = 1\u2192 O(k log k), s = 1\u2192 O(k log2 k), j = 1\u2192 d do 20: Mr,s \u2190Mr,s + T1r,i \u00b7Bi,j \u00b7 T2j,s. 21: end for 22: for r = 1\u2192 O(k log k), j = 1\u2192 d do 23: Dr,j \u2190 Dr,j + Sr,i \u00b7Bi,j . 24: end for 25: if need decomposition then 26: for s = 1\u2192 O(k log k), j = 1\u2192 d do 27: Ci,s := Ci,s +Bi,j \u00b7Rj,s. 28: end for 29: end if 30: end for 31: Compute the SVD of L = UL\u03a3LV >L . 32: Compute the SVD of N = UN\u03a3NV >N . 33: Compute X\u0302 = L\u2020(ULU>LMVNV > N )kN\n\u2020. 34: Compute the SVD of X\u0302 = U\u0302 \u03a3\u0302V\u0302 >. 35: if need decomposition then 36: return V \u2217 = \u03a3\u0302V\u0302 >D,U\u2217 = CU\u0302 . 37: else 38: return V \u2217 = \u03a3\u0302V\u0302 >D. 39: end if 40: end procedure"}, {"heading": "K.1 Definitions", "text": "Definition K.1 (Row-partition model [BWZ16]). There are s machines, and the ith machine has a matrix Ai \u2208 Rni\u00d7d as input. Suppose n = \u2211s i=1 ni, and the global data matrix A \u2208 Rn\u00d7d is denoted\nas  A1 A2 \u00b7 \u00b7 \u00b7 As  , we say A is row-partitioned into these s matrices distributed in s machines respectively. Furthermore, there is a machine which is a coordinator. The model only allows communication between the machines and the coordinator. The communication cost in this model is the total number of words transferred between machines and the coordinator. Each word is O(log(snd)) bits.\nDefinition K.2 (Arbitrary-partition model [BWZ16]). There are s machines, and the ith machine has a matrix Ai \u2208 Rn\u00d7d as input. Suppose the global data matrix A \u2208 Rn\u00d7d is denoted as A = \u2211s i=1Ai. We say A is arbitrarily partitioned into these s matrices distributed in s machines respectively. Furthermore, there is a machine which is a coordinator. The model only allows communication between the machines and the coordinator. The communication cost in this model is the total number of words transferred between machines and the coordinator. Each word is O(log(snd)) bits.\nK.2 Arbitrary-partition model, subspace, poly(k, log(d), log(n)) approximation\nDefinition K.3 (Arbitrary-partition model `1-low rank approximation - rank-k subspace version). Given matrix A \u2208 Rn\u00d7d arbitrarily partitioned into s matrices A1, A2, \u00b7 \u00b7 \u00b7 , As distributed in s machines respectively, and k \u2208 N+, the goal is to propose a protocol in the model of Definition K.2 such that\n1. Upon termination, the protocol leaves a matrix V \u2217 \u2208 Rk\u00d7d on the coordinator.\n2. V \u2217 satisfies that\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The communication cost is as small as possible\nTheorem K.4. Suppose A \u2208 Rn\u00d7d is partitioned in the arbitrary partition model (See Definition K.2). There is a protocol(in Algorithm 10) which solves the problem in Definition K.3 with constant probability. Further, the communication complexity of the protocol is s(poly(k) + O\u0303(kd)) words.\nProof. Correctness. The correctness is shown by the proof of Theorem C.3 and (IV) of Lemma D.11. Notice that X\u0302 \u2208 RO(k log k)\u00d7O(k log k) minimizes\nmin rank\u2212k X\n\u2016LXN \u2212M\u2016F .\nwhich is min\nrank\u2212k X \u2016T1ARXSAT2 \u2212 T1AT2\u2016F .\nAlgorithm 10 Arbitrary Partition Distributed Protocol 1: procedure ArbitraryPartitionDistributedProtocol(k,s,A) 2: A \u2208 Rn\u00d7d was arbitrarily partitioned into s matrices A1, \u00b7 \u00b7 \u00b7 , As \u2208 Rn\u00d7d distributed in s\nmachines. 3: Coordinator Machines i 4: Chooses a random seed. 5: Sends it to all machines. 6: \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 > 7: Agrees on R, T2 which are fully 8: independent random Cauchy matrices. 9: Agrees on S, T1 which are random Cauchy 10: matrices with fully independent entries 11: from different rows, and O\u0303(d)-wise indepen12: dent variables from the same row. 13: Computes Li = T1AiR,Ni = SAiT2. 14: Computes Mi = T1AiT2. 15: Sends Li, Ni,Mi to the coordinator. 16: < \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 17: Computes L =\ns\u2211 i=1 Li, N = s\u2211 i=1 Ni.\n18: Computes M = s\u2211 i=1 Mi. 19: Computes the SVD of L = UL\u03a3LV >L . 20: Computes the SVD of N = UN\u03a3NV >N . 21: Computes X\u0302 = L\u2020(ULU>LMVNV > N )kN\n\u2020. 22: Sends X\u0302 to machines. 23: \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 > 24: Computes the SVD of X\u0302 = U\u0302 \u03a3\u0302V\u0302 >. 25: Computes V \u2217i = \u03a3\u0302V\u0302\n>SAi. 26: If need decomposition 27: U\u2217i = AiRU\u0302 . 28: Sends U\u2217i , V \u2217 i to the coordinator. 29: Else 30: Sends V \u2217i to the coordinator. 31: Endif 32: < \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 33: If need decomposition, 34: return V \u2217 = \u2211s i=1 V \u2217 i , U \u2217 = \u2211s i=1 U \u2217 i . 35: Else 36: return V \u2217 = \u2211s i=1 V \u2217 i . 37: Endif 38: end procedure\nAccording to the proof of Theorem C.3, ARX\u0302SA gives an `1 rank-k poly(k, log(d), log(n))-approximation to A. Because X\u0302 = U\u0302 \u03a3\u0302V\u0302 >, V \u2217 = \u03a3\u0302V\u0302 >SA satisfies:\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\nCommunication complexity. Since the random seed generates O\u0303(kd)-wise independent random Cauchy variables, the cost of line 5 is O\u0303(skd) bits. The size of Li, Ni andMi are k2 log2 k, k2 log3 k and k2 log3 k words separately. So the cost of line 15 is O(sk2 log3 k) words. Because the size of X\u0302 is O(k2 log2 k), the cost of line 22 is O(sk2 log2 k) words. line 30 needs skd words of communication. Therefore, the total communication of the protocol is s(poly(k) + O\u0303(kd)) words.\nK.3 Arbitrary-partition model, decomposition, poly(k, log(d), log(n)) approximation\nDefinition K.5 (Arbitrary-partition model `1-low rank approximation - rank-k decomposition version). Given matrix A \u2208 Rn\u00d7d arbitrarily partitioned into s matrices A1, A2, \u00b7 \u00b7 \u00b7 , As distributed in s machines respectively, and k \u2208 N+, the goal is to propose a protocol in the model of Definition K.2 such that\n1. Upon termination, the protocol leave matrices U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d on the coordinator.\n2. U\u2217, V \u2217 satisfies that\n\u2016A\u2212 U\u2217V \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The communication cost is as small as possible.\nTheorem K.6. Suppose A \u2208 Rn\u00d7d is partitioned in the arbitrary partition model (See Definition K.2). There is a protocol(in Algorithm 10 with decomposition) which solves the problem in Definition K.5 with constant probability. Further, the communication complexity of the protocol is s(poly(k) + O\u0303(k(d+ n))) words.\nProof. Correctness. The only difference from the protocol (without decomposition) in Section K.2 is that the protocol sends Ui. Thus, the coordinator can compute U\u2217 = ARU\u0302 . Notice that U\u2217V \u2217 = ARX\u0302SA. According to the proof of Theorem C.3, U\u2217V \u2217 gives a `1 rank-k poly(k, log(d), log(n))approximation to A.\nCommunication complexity. Since the size of Ui is kn words, the total communication is s(poly(k) + O\u0303(k(d+ n))) words.\nK.4 Row-partition model, subspace, poly(k) log d approximation\nDefinition K.7 (Row-partition model `1-low rank approximation - rank-k subspace version). Given matrix A \u2208 Rn\u00d7d row-partitioned into s matrices A1, A2, \u00b7 \u00b7 \u00b7 , As distributed in s machines respectively, and k \u2208 N+, the goal is to propose a protocol in the model of Definition K.1 such that\n1. Upon termination, the protocol leaves a matrix V \u2217 \u2208 Rk\u00d7d on the coordinator.\n2. V \u2217 satisfies that\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k) log d \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The communication cost is as small as possible\nTheorem K.8. Suppose A \u2208 Rn\u00d7d is partitioned in the row partition model (See Definition K.1). There is a protocol(in Algorithm 11 without decomposition) which solves the problem in Definition K.7 with constant probability. Further, the communication complexity of the protocol is s(poly(k)+ O\u0303(kd)) words.\nAlgorithm 11 Row Partition Distributed Protocol 1: procedure RowPartitionDistributedProtocol(k,s,A) 2: A \u2208 Rn\u00d7d was row partitioned into s matrices A1 \u2208 Rn1\u00d7d, \u00b7 \u00b7 \u00b7 , As \u2208 Rns\u00d7d distributed in s\nmachines. 3: Coordinator Machines i 4: Chooses a random seed. 5: Sends it to all machines. 6: \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 > 7: Agrees on R, T2 which are fully 8: independent random Cauchy variables. 9: Generates random Cauchy matrices\n10: S\u2032i \u2208 RO(k log k)\u00d7ni , Si, T1i \u2208 RO(k log 2 k)\u00d7ni . 11: Computes Y \u2217i = arg min Y \u2208Rni\u00d7O(k log k) \u2016Y S\u2032iAi \u2212Ai\u20161. 12: Computes Bi = Y \u2217i S \u2032 iAi. 13: Computes Li = T1iBiR,Ni = SiBiT2. 14: Computes Mi = T1iBiT2. 15: Sends Li, Ni,Mi to the coordinator. 16: < \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 17: Computes L = \u2211s i=1 Li, N = \u2211s i=1Ni.\n18: Computes M = \u2211s\ni=1Mi. 19: Computes the SVD of L = UL\u03a3LV >L . 20: Computes the SVD of N = UN\u03a3NV >N . 21: Computes X\u0302 = L\u2020(ULU>LMVNV > N )kN\n\u2020. 22: Sends X\u0302 to machines. 23: \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 > 24: Computes the SVD of X\u0302 = U\u0302 \u03a3\u0302V\u0302 >. 25: Computes V \u2217i = \u03a3\u0302V\u0302\n>SiBi. 26: If need decomposition 27: Computes U\u2217i = BiRU\u0302 . 28: Sends U\u2217i , V \u2217 i to the coordinator. 29: Else 30: Sends V \u2217i to the coordinator. 31: Endif 32: < \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 33: If need decomposition 34: return V \u2217 = \u2211s i=1 V \u2217 i , U \u2217 = \u2211s i=1 U \u2217 i . 35: Else 36: return V \u2217 = \u2211s i=1 V \u2217 i . 37: Endif 38: end procedure\nProof. Correctness. For convenience, we denote matrices B \u2208 Rn\u00d7d, S \u2208 RO(k log2 k)\u00d7n and T1 \u2208 RO(k log2 k)\u00d7n as\nB =  B1 B2 \u00b7 \u00b7 \u00b7 Bs  S = ( S1 S2 \u00b7 \u00b7 \u00b7 Ss ) T1 = ( T11 T12 \u00b7 \u00b7 \u00b7 T1s ) . Notice that L = T1BR,N = SBT2,M = T1BT2. Thus, X\u0302 \u2208 RO(k log k)\u00d7O(k log k) actually minimizes\nmin rank\u2212k X\n\u2016T1BRXSBT2 \u2212 T1BT2\u2016F .\nAlso notice that B is just taking each row of A and replacing it with its nearest point in the row span of S\u2032A. According to the proof of Theorem C.6, BRX\u0302SB gives a poly(k) log d `1 norm rank-k approximation to A. Since X\u0302 = U\u0302 \u03a3\u0302V\u0302 >, V \u2217 = \u03a3\u0302V\u0302 >SB satisfies:\nmin U\u2208Rn\u00d7k \u2016A\u2212 UV \u2217\u20161 \u2264 poly(k, log(d), log(n)) \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\nCommunication complexity. Since the R and T2 are O\u0303(kd)-wise independent, line 6 needs O(sW ) bits of communication. Line 16 needsO(sk2 log3 k) words. The cost of line 23 isO(sk2 log2 k) words. Line 32 needs skd words of communication. Therefore, the total communication of the protocol is s(poly(k) + O\u0303(kd)) words.\nK.5 Row-partition model, decomposition, poly(k) log d approximation\nDefinition K.9 (Row-partition model `1-low rank approximation - rank-k decomposition version). Given matrix A \u2208 Rn\u00d7d row partitioned into s matrices A1, A2, \u00b7 \u00b7 \u00b7 , As distributed in s machines respectively, and a positive integer k < rank(A), the goal is to propose a protocol in the model of Definition K.1 such that\n1. Upon termination, the protocol leaves matrices U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d on the coordinator.\n2. U\u2217, V \u2217 satisfies that\n\u2016A\u2212 U\u2217V \u2217\u20161 \u2264 poly(k) log d \u00b7 min U\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016A\u2212 UV \u20161.\n3. The communication cost is as small as possible.\nTheorem K.10. Suppose A \u2208 Rn\u00d7d is partitioned in the row partition model (See Definition K.1). There is a protocol(in Algorithm 11 with decomposition) which solves the problem in Definition K.9 with constant probability. Further, the communication complexity of the protocol is s(poly(k) + O\u0303(k(n+ d))) words.\nProof. Correctness. The only difference is that the above protocol sends Ui. Thus, the coordinator can compute U\u2217 = BRU\u0302 . Notice that U\u2217V \u2217 = BRX\u0302SB, according to the proof of Theorem C.6, U\u2217V \u2217 gives a poly(k) log d `1 norm rank-k approximation to A.\nCommunication complexity. Since the size of Ui is kn words, the total communication is s(poly(k) + O\u0303(k(n+ d))) words."}, {"heading": "L Experiments and Discussions", "text": "In this section, we provide some counterexamples for the other heuristic algorithms such that, for those examples, the heuristic algorithms can output a solution with a very \u201cbad\u201d approximation ratio, i.e., nc, where c > 0 and the input matrix has size n\u00d7 n. We not only observe that heuristic algorithms sometimes have very bad performance in practice, but also give a proof in theory."}, {"heading": "L.1 Setup", "text": "We provide some details of our experimental setup. We obtained the R package of [KK05, Kwa08, BDB13] from https://cran.r-project.org/web/packages/pcaL1/index.html. We also implemented our algorithm and the r1-pca algorithm [DZHZ06] using the R language. The version of the R language is 3.0.2. We ran experiments on a machine with Intel X5550@2.67GHz CPU and 24G memory. The operating system of that machine is Linux Ubuntu 14.04.5 LTS. All the experiments were done in single-threaded mode."}, {"heading": "L.2 Counterexample for [DZHZ06]", "text": "The goal is to find a rank k = 1 approximation for matrix A. For any \u2208 [0, 0.5), we define A \u2208 Rn\u00d7n as\nA =\n[ n1.5+ 0\n0 0\n] + [ 0 0 0 B ] , (45)\nwhere B \u2208 R(n\u22121)\u00d7(n\u22121) is all 1s matrix. It is immediate that the optimal cost is at most n1.5+ . However, using the algorithm in [DZHZ06], the cost is at least \u2126(n2). Thus, we can conclude,\nusing algorithm [DZHZ06] to solve `1 low rank approximation problem on A cannot achieve an approximation ratio better than n0.5\u2212 ."}, {"heading": "L.3 Counterexample for [BDB13]", "text": "The goal is to find a rank k = 1 approximation for matrix A. The input matrix A \u2208 Rd\u00d7d for algorithm [BDB13] is defined to be,\nA =\n[ n1.5 0\n0 0\n] + [ 0 0 0 B ] , (46)\nwhere B \u2208 R(n\u22121)\u00d7(n\u22121) is an all 1s matrix. It is immediate that the optimal cost is at most n1.5. Now, let us look at the procedure of [BDB13]. Basically, the algorithm described in [BDB13] is that they first find a rank n \u2212 1 approximation via a best `1-fit hyperplane algorithm, then they rotate it based on the right singular vectors of the rank n\u2212 1 approximation matrix, and next they recursively do the same thing for the rotated matrix which has only n\u2212 1 columns.\nWhen running their algorithm on A, they will fit an arbitrary column except the first column of A. Without loss of generality, it just fits the last column of A. After the rotation, the matrix will be an n\u00d7 (n\u2212 1) matrix:  n1.5 0 0 \u00b7 \u00b7 \u00b7 0 0 \u221a n\u2212 2 0 \u00b7 \u00b7 \u00b7 0 0 \u221a n\u2212 2 0 \u00b7 \u00b7 \u00b7 0\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u221a n\u2212 2 0 \u00b7 \u00b7 \u00b7 0\n .\nThen, after the tth iteration for t < (n\u2212 1), they will get an n\u00d7 (n\u2212 t) matrix: n1.5 0 0 \u00b7 \u00b7 \u00b7 0 0 \u221a n\u2212 2 0 \u00b7 \u00b7 \u00b7 0 0 \u221a n\u2212 2 0 \u00b7 \u00b7 \u00b7 0\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u221a n\u2212 2 0 \u00b7 \u00b7 \u00b7 0  . This means that their algorithm will run on an n\u00d7 2 matrix: n1.5 0 0 \u221a n\u2212 2 0 \u221a n\u2212 2\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u221a n\u2212 2\n ,\nin the last iteration. Notice that n \u00d7 \u221a n\u2212 2 < n1.5. This means that their algorithm will fit the first column which implies that their algorithm will output a rank-1 approximation to A by just fitting the first column of A. But the cost of this rank-1 solution is (n\u2212 1)2. Since the optimal cost is at most n1.5, their algorithm cannot achieve an approximation ratio better than n0.5."}, {"heading": "L.4 Counterexample for [Kwa08]", "text": "We show that the algorithm [Kwa08] cannot achieve an approximation ratio better than \u0398(n) on the matrix A \u2208 R(2n+1)\u00d7(2n+1) defined as,\nA = n1.5 0 00 B 0 0 0 B  , (47) where B is an n\u00d7n matrix that contains all 1s. We consider the rank-2 approximation problem for this input matrix A. The optimal cost is at most n1.5.\nWe run their algorithm. Let x0 denote the initial random unit vector. Consider the sign vector s \u2208 {\u00b11}2n+1 where each entry is the sign of the inner product between x0 and each column of A. There are only three possibilities,\ns =  (1, {1}n, {1}n) (1, {1}n, {\u22121}n) (1, {\u22121}n, {1}n) .\nCase I, s = (1, {1}n, {1}n). Define u\u0302 = \u22112n+1\ni=1 si \u00b7 Ai = (n1.5, n, \u00b7 \u00b7 \u00b7 , n). Let u = u\u0302/\u2016u\u20162 = u\u0302/( \u221a 3n1.5) = (1/ \u221a 3, 1/ \u221a 3n, \u00b7 \u00b7 \u00b7 , 1/ \u221a\n3n). Define matrix D to be A\u2212uu>A. Then, we can compute D,\nD = A\u2212 uu>A\n= A\u2212  1/3 1 3 \u221a n 1> 1 3 \u221a n 1> 1 3 \u221a n 1 13nB 1 3nB\n1 3 \u221a n 1 13nB\n1 3nB A =\nn1.5 0 00 B 0 0 0 B \u2212 n1.53 \u221a n 3 1 > \u221a n 3 1 > n 31 1 3B 1 3B\nn 31\n1 3B\n1 3B  = 2n1.5/3 \u2212 \u221a n 3 1 > \u2212 \u221a n 3 1 >\n\u2212n31 2 3B \u2212 1 3B \u2212n31 \u2212 1 3B 2 3B  . Now we need to take the linear combination of columns D = A \u2212 uu>A. Let w denote another sign vector {\u22121,+1}2n+1. Then let v denote the basis vector, v = \u22112n+1 i=1 Di. There are three possibilities, Case I(a), if w = (1, {1}n, {1}n), then v is the all 0 vector. Using vector v to interpolate each column of D, the cost we obtain is at least 2n2. Case I(b), if w = (1, {1}n, {\u22121}n), then v = (2n1.5/3, {2/3}n, {\u22124/3}n). We also obtain at least 2n2 cost if we use that v to interpolate each column of D. Case I(c), if w = (1, {\u22121}n, {\u22121}n), then v = (0, {\u22122/3}n, {\u22122/3}n). The cost is also at least 2n2. Case II, s = (1, {1}n, {\u22121}n). Define 1 to be a length n all 1s column vector. Define u\u0302 =\u22112n+1 i=1 si\u00b7Ai = (n1.5, {n}n, {\u2212n}n). Let u = u\u0302/\u2016u\u20162 = u\u0302/( \u221a 3n1.5) = (1/ \u221a 3, {1/ \u221a 3n}n, {\u22121/ \u221a 3n}n). Define (2n+ 1)\u00d7 (2n+ 1) matrix D to be A\u2212 uu>A. Then, we can compute D,\nD = A\u2212 uu>A\n= A\u2212  1/3 1 3 \u221a n 1> \u2212 1 3 \u221a n 1> 1 3 \u221a n 1 13nB \u2212 1 3nB\n\u2212 1 3 \u221a n 1 \u2212 13nB\n1 3nB A =\nn1.5 0 00 B 0 0 0 B \u2212 n1.5/3 \u221a n 3 1 > \u2212 \u221a n 3 1 > n 31 1 3B \u2212 1 3B\n\u2212n31 \u2212 1 3B\n1 3B  = 2n1.5/3 \u2212 \u221a n 3 1 > \u221a n 3 1 >\n\u2212n31 2 3B\n1 3B\nn 31\n1 3B\n2 3B  . Similarly to the previous case, we can also discuss three cases. Case III, s = (1, {1}n, {\u22121}n). Define 1 to be a length n all 1s column vector. Define u\u0302 =\u22112n+1 i=1 si\u00b7Ai = (n1.5, {\u2212n}n, {\u2212n}n). Let u = u\u0302/\u2016u\u20162 = u\u0302/( \u221a 3n1.5) = (1/ \u221a 3, {\u22121/ \u221a 3n}n, {\u22121/ \u221a 3n}n). Define matrix D to be A\u2212 uu>A. Then, we can compute D,\nD = A\u2212 uu>A\n= A\u2212  1/3 \u2212 1 3 \u221a n )1> \u2212 1 3 \u221a n 1> \u2212 1 3 \u221a n 1 13nB 1 3nB\n\u2212 1 3 \u221a n 1 13nB\n1 3nB A =\nn1.5 0 00 B 0 0 0 B \u2212 n1.5/3 \u2212 \u221a n 3 1 > \u2212 \u221a n 3 1 > \u2212n31 1 3B 1 3B\n\u2212n31 1 3B\n1 3B  = 2n1.5/3 \u221a n 3 1 > \u221a n 3 1 >\nn 31\n2 3B\n1 3B\nn 31\n1 3B\n2 3B  . Similarly to the previous case, we can also discuss three cases."}, {"heading": "L.5 Counterexample for [KK05]", "text": "We show that there exist matrices such that the algorithm of [KK05] cannot achieve an approximation ratio better than \u0398(n). Their algorithm has two different ways of initialization. We provide counterexamples for each of the initialization separately.\nRandom vector initialization We provide a counterexample matrix A \u2208 Rn\u00d7n defined as,\nA =  nc 0 \u00b7 \u00b7 \u00b7 0 0 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 0 \u00b7 \u00b7 \u00b7 0 + I, (48) where c \u2265 2. Consider the rank-1 approximation problem for this matrix A. The optimal cost is at most n\u2212 1.\nRun their algorithm. The starting vectors are u(0) \u223c N(0, I) and v(0) \u223c N(0, 1). We define two properties for a given vector y \u2208 Rn. Property I is for all i \u2208 [n], |yi| \u2264 n/8, and Property II is there exist half of the i such that |yi| \u2265 1/2. We can show that with probability 1 \u2212 2\u2212\u2126(n), both u(0) and v(0) satisfy Property I and II. After 1 iteration, we can show that u(1)1 = v(1)1 = 0\nNow let us use column vector u(0) \u2208 Rn to interpolate the first column of matrix A. We simplify u(0) to be u. Let ui denote the i-th coordinate of vector u, \u2200i \u2208 [n]. Let A1 denote the first column of matrix A. We define \u03b1 = v(1)1 = arg min\u03b1 \u2016\u03b1u(0)\u2212A1\u20161. For any scalar \u03b1, the cost we pay on the first column of matrix A is,\n|\u03b1 \u00b7 u1 \u2212 nc|+ n\u2211 i=2 |\u03b1ui| \u2265 |\u03b1 \u00b7 u1 \u2212 nc|+ n 2 |\u03b1 \u00b7 1 2 | \u2265 |\u03b1 \u00b7 u1 \u2212 nc|+ n 4 |\u03b1|. (49)\nNotice that c \u2265 2. Then |\u03b1 \u00b7 u1 \u2212 nc| + n4 |\u03b1| \u2265 |\u03b1 \u00b7 n 8 \u2212 n c| + n4 |\u03b1|. If \u03b1 \u2264 0, then the cost is minimized when \u03b1 = 0. If \u03b1 \u2208 [0, 8nc\u22121], the cost is minimized when \u03b1 = 0. If \u03b1 \u2265 8nc\u22121, the cost is minimized when \u03b1 = 8nc\u22121. Putting it all together, to achieve the minimum cost, there is only one choice for \u03b1, which is \u03b1 = 0. The optimal cost is at least nc.\nThen after T iterations(for any T \u2265 1), u(T )1 = v(T )1 = 0. Thus, we always pay at least nc cost on the first entry.\nTherefore, their algorithm cannot achieve any approximation ratio better than nc\u22121. Because c \u2265 2, we complete the proof.\nTop singular vector initialization The counterexample input matrix A \u2208 Rn\u00d7n is defined as,\nA = [ n 0 0 B ] , (50)\nwhere matrix B \u2208 R(n\u22121)\u00d7(n\u22121) contains all 1s. Consider the rank-1 approximation problem for this matrix A. The optimal cost is at most n. Run their algorithm. The starting vectors u(0) and v(0) will be set to (1, 0, \u00b7 \u00b7 \u00b7 , 0) \u2208 Rn. After T iterations(for any T > 0), the support of u(T )(resp. v(T ))\nis the same as u(0)(resp. v(T )). Thus, the cost is at least \u2016B\u20161 = (n \u2212 1)2. Therefore, we can conclude that their algorithm cannot achieve any approximation ratio better than (n\u22121)2/n = \u0398(n)."}, {"heading": "L.6 Counterexample for all", "text": "For any \u2208 (0, 0.5) and \u03b3 > 0, we construct the input matrix A \u2208 R(2n+2)\u00d7(2n+2) as follows\nA =  n2+\u03b3 0 0 0\n0 n1.5+ 0 0 0 0 B 0 0 0 0 B  , where B is n\u00d7n all 1s matrix. We want to find a rank k = 3 solution for A. Then any of those four heuristic algorithms [KK05, DZHZ06, Kwa08, BDB13] is not able to achieve better than nmin(\u03b3,0.5\u2212 ) approximation ratio. We present our main experimental results in Figure 5. Both [KK05] and [Kwa08] have two different ways of initialization. In Figure 5 we use KK05r(resp. Kwak08r) to denote the way that uses random vector as initialization, and use KK05s(resp. Kwak08s) to denote the way that uses top singular vector as initialization. Figure 5(a) shows the performance of all the algorithms and Figure 5(b) presents the running time. The `1 residual cost of all the other algorithm is growing much faster than our algorithm. Most of the algorithms (including ours) are pretty efficient, i.e., the running time is always below 3 seconds. The running time of [BDB13, KK05] is increasing very fast when the matrix dimension n is growing.\nIn Figure 5(a), the cost of KK05r at {82, \u00b7 \u00b7 \u00b7 , 142} is in [105, 106], at {162, \u00b7 \u00b7 \u00b7 , 302} is in [106, 107], and at {322, \u00b7 \u00b7 \u00b7 , 402} is in [107, 108]. In Figure 5(b), the time of KK05r at {382, 482} is 64s and 160s. The running time of BDB13 at {82, \u00b7 \u00b7 \u00b7 , 222} is between 1 minute and 1 hour. The running time of BDB13 at {242, \u00b7 \u00b7 \u00b7 , 322} is between 1 hour and 20 hours. The running time of BDB13 at {342, \u00b7 \u00b7 \u00b7 , 402} is more than 20 hours."}, {"heading": "L.7 Discussion for Robust PCA [CLMW11]", "text": "A popular method is robust PCA [CLMW11], which given a matrix A, tries to find a matrix L for which \u03bb\u2016A\u2212 L\u20161 + \u2016L\u2016\u2217 is minimized, where \u03bb > 0 is a tuning parameter and \u2016L\u2016\u2217 is the nuclear norm of L. This is a convex program, but it need not return a low rank matrix L with relative error. As a simple example, suppose \u03bb = 1, and the n \u00d7 n matrix A is a block-diagonal matrix of rank k and n = k2 (b + 1). Further, the first k/2 blocks are b \u00d7 b matrices of all 1s, while the next k/2 blocks are just a single value b on the diagonal.\nThen the solution to the above problem may return L to be the first k/2 blocks of A. The total cost of \u03bb\u2016A\u2212 L\u20161 + \u2016L\u2016\u2217is (k/2)b+ (k/2)b = kb.\nAlso, the solution to the above problem may return L to be A, which has cost 0 + \u2016A\u2016\u2217 = kb. Because this solution has the same cost, it means that their algorithm might output a rank-k solution, and also might output a rank-k/2 solution.\nTherefore, the relative error of the output matrix may be arbitrarily bad for `1-low rank approximation.\nWe also consider the following example. Suppose \u03bb = 1/ \u221a n, and let n\u00d7 n matrix A denote the Hadamard matrix Hn. Recall that the Hadamard matrix Hp of size p \u00d7 p is defined recursively :[ Hp/2 Hp/2 Hp/2 \u2212Hp/2 ] with H2 = [ +1 +1 +1 \u22121 ] . Notice that every singular values of A is \u221a n. We consider the objective function \u03bb\u2016A\u2212 L\u20161 + \u2016L\u2016\u2217.\nThen the solution to the above problem may return L to be the first n/2 rows of A. The total cost of \u03bb\u2016A\u2212L\u20161 +\u2016L\u2016\u2217 is (1/ \u221a n)n2/2 + (n/2) \u221a n = n1.5. Also, the solution to the above problem may return L to be A, which has cost 0 + \u2016A\u2016\u2217 = n \u221a n = n1.5. For any i, if the solution takes i rows of A, the cost is (1/ \u221a n)(n \u2212 i)n + i \u221a n = n1.5. Because this solution has the same cost, it means that their algorithm might output a rank-n solution, and also might output a rank-n/2 solution. Therefore, the relative error of the output matrix may be arbitrarily bad for `1-low rank approximation."}, {"heading": "M Acknowledgments", "text": "The authors would like to thank Alexandr Andoni, Saugata Basu, Cho-Jui Hsieh, Daniel Hsu, Chi Jin, Fu Li, Ankur Moitra, Cameron Musco, Richard Peng, Eric Price, Govind Ramnarayan, James Renegar, and Clifford Stein for useful discussions. The authors also thank Jiyan Yang, Yinlam Chow, Christopher R\u00e9, and Michael Mahoney for sharing the code."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study the `1-low rank approximation problem, where for a given n \u00d7 d matrix A and<lb>approximation factor \u03b1 \u2265 1, the goal is to output a rank-k matrix \u00c2 for which<lb>\u2016A\u2212 \u00c2\u20161 \u2264 \u03b1 \u00b7 min<lb>rank-k matrices A\u2032 \u2016A\u2212A\u20161,<lb>where for an n\u00d7 d matrix C, we let<lb>\u2016C\u20161 =<lb>\u2211n<lb>i=1<lb>\u2211d<lb>j=1 |Ci,j |. This error measure is known to<lb>be more robust than the Frobenius norm in the presence of outliers and is indicated in models<lb>where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard<lb>by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple<lb>places if there are any approximation algorithms.<lb>We give the first provable approximation algorithms for `1-low rank approximation, showing<lb>that it is possible to achieve approximation factor \u03b1 = (log d)\u00b7poly(k) in nnz(A)+(n+d) poly(k)<lb>time, where nnz(A) denotes the number of non-zero entries of A. If k is constant, we further<lb>improve the approximation ratio toO(1) with a poly(nd)-time algorithm. Under the Exponential<lb>Time Hypothesis, we show there is no poly(nd)-time algorithm achieving a (1 + 1<lb>log1+\u03b3(nd) )-<lb>approximation, for \u03b3 > 0 an arbitrarily small constant, even when k = 1.<lb>We give a number of additional results for `1-low rank approximation: nearly tight upper and<lb>lower bounds for column subset selection, CUR decompositions, extensions to low rank approx-<lb>imation with respect to `p-norms for 1 \u2264 p < 2 and earthmover distance, low-communication<lb>distributed protocols and low-memory streaming algorithms, algorithms with limited random-<lb>ness, and bicriteria algorithms. We also give a preliminary empirical evaluation. \u2217Work done while visiting IBM Almaden.<lb>ar<lb>X<lb>iv<lb>:1<lb>61<lb>1.<lb>00<lb>89<lb>8v<lb>1<lb>[<lb>cs<lb>.D<lb>S]<lb>3<lb>N<lb>ov<lb>2<lb>01<lb>6", "creator": "LaTeX with hyperref package"}}}