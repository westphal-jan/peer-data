{"id": "1611.02120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Neural Networks Designing Neural Networks: Multi-Objective Hyper-Parameter Optimization", "abstract": "artificial neural networks have gone through a recent rise in popularity, achieving state - of - the - art results in various fields, including frequency classification, speech recognition, adaptive automated control. both the capability and innovation complexity of such models are heavily dependant on the design of characteristic hyper - parameters ( e. g., number of voice layers, nodes per process, or choice of activation functions ), which have traditionally been updated manually. with machine learning penetrating low - performance mobile and embedded areas, the need to optimize not equally for performance ( accuracy ), but also for implementation complexity, becomes paramount. in her work, we present a multi - objective design space exploration method that reduces the number of solution networks trained and evaluated or response surface modelling. given algorithms which can easily exceed 1020 solutions, manually designing a near - optimal machine is assessed as unnecessary to reduce network complexity, while maintaining performance, may be useless. this problem is exacerbated by the expectation that hyper - parameters which perform well on specific datasets may yield sub - par results on others, and increasingly therefore be designed on a per - task basis. utilizing our work, machine learning includes leveraged by training an artificial neural network to achieve the performance of future candidate platforms. faster method needs evaluated than the mnist node cifar - 10 image datasets, optimizing for both recognition accuracy and computational complexity. experimental results demonstrate that such proposed method can closely approximate the pareto - optimal front, simultaneously only filling a vast fraction of the design space.", "histories": [["v1", "Mon, 7 Nov 2016 15:38:39 GMT  (635kb,D)", "http://arxiv.org/abs/1611.02120v1", "To appear in ICCAD'16. The authoritative version will appear in the ACM Digital Library"]], "COMMENTS": "To appear in ICCAD'16. The authoritative version will appear in the ACM Digital Library", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sean c smithson", "guang yang", "warren j gross", "brett h meyer"], "accepted": false, "id": "1611.02120"}, "pdf": {"name": "1611.02120.pdf", "metadata": {"source": "CRF", "title": "Neural Networks Designing Neural Networks: Multi-Objective Hyper-Parameter Optimization", "authors": ["Sean C. Smithson", "Guang Yang", "Warren J. Gross", "Brett H. Meyer"], "emails": ["guang.yang3}@mail.mcgill.ca,", "brett.meyer}@mcgill.ca", "permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "Artificial Neural Network (ANN) models have become widely adopted as means to implement many machine learning algorithms and represent the state-of-the-art for many image and speech recognition applications [16]. As the application space for ANNs evolves beyond workstations and data cen-\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICCAD \u201916, November 07 - 10, 2016, Austin, TX, USA c\u00a9 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-4466-1/16/11. . . $15.00 DOI: http://dx.doi.org/10.1145/2966986.2967058\ntres towards low-power mobile and embedded platforms, so too must their design methodologies. Mobile voice recognition systems, such as Apple\u2019s Siri, currently remain too computationally demanding to execute locally on a handset. Instead, such applications are processed remotely and, depending on network conditions, are subject to variations in performance and delay [1]. ANNs are also finding application in other emerging areas, such as autonomous vehicle localization and control, where meeting power and cost requirements is paramount [11].\nANNs, which replace manually engineered computer algorithms, must be trained instead of programmed. This training involves an optimization process where network weights are adjusted with the objective of minimizing the output error. These adjustments often involve a variation of the Stochastic Gradient Descent (SGD) method [16]. While these training methods have been automated, much of the design process and choice of network hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions) has been historically relegated to manual optimization. This relies on human intuition and expert knowledge of the target application in conjunction with extensive trial and error [7, 22]. This process is difficult, considering the vast network hyper-parameter space which includes: the number of convolutional or hidden layers, the quantity of nodes in each layer, the type of nonlinear activation functions, and many others which depend on the system in-hand. In addition, the problem with manual hyper-parameter tuning is that there is no guarantee that the process will result in optimal configurations. Not only does the diversity of possible hyper-parameters create extremely large design spaces, but time intensive training phases on comprehensive data sets must also be performed prior to evaluating candidate solutions. This significant computational overhead renders exhaustive searches intractable, and necessitates the use of automated Design Space Exploration (DSE) tools to intelligently explore the solution space while limiting the number of candidate models that must be trained and evaluated.\nWith the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21]. These added constraints transform hyper-parameter design into a multi-objective optimization problem where no single optimal solution exists. Instead, the set of points which are not dominated by any other solution forms a Pareto-optimal front. Simply put, this set includes all solutions for which\nThis is the authors\u2019 final version. The authoritative version is to appear in the ACM Digital Library.\nar X\niv :1\n61 1.\n02 12\n0v 1\n[ cs\n.N E\n] 7\nN ov\n2 01\n6\nno other is objectively superior in all criteria. Formally, a solution vector (a1, b1) (where a and b are two objectives for optimization) is said to dominate another point (a2, b2) if a1 < a2 and b1 \u2264 b2, or b1 < b2 and a1 \u2264 a2; the set of points which are not dominated by any other solution constitutes the Pareto-optimal front [19].\nThis paper presents an automated DSE method that effectively trains a neural network to design other neural networks, optimizing for both performance and implementation cost. This meta-heuristic ANN exploits machine learning to predict the performance of candidate solutions (modelling the response surface); the algorithm learns which points to explore and avoids the lengthy computations involved in evaluating solutions which are predicted to be unfit. This leveraging of Response Surface Modelling (RSM) techniques dramatically reduces the proposed algorithm run-time, which can ultimately result in the reduction of product design time, application time-to-market, and overall Nonrecurring Engineering (NRE) costs."}, {"heading": "1.1 Motivational Example", "text": "While there are many different ANN models, the MultiLayer Perceptron (MLP) is a well-known form, which rose in popularity with the advent of the back-propagation training algorithm [5]. Characterized by a series of cascaded hidden layers, MLPs have a single feed-forward path from the input to output layers. MLP layers are also fully-connected; each node has a connection to each and every node in adjacent layers. When illustrated as a directed graph (shown in Figure 1), graph connections are representative of signals being multiplied by corresponding weights, and graph nodes the summation of all inputs followed by non-linear activation functions. The evaluation of such elements, commonly Rectified Linear Units (ReLUs) or sigmoid functions, is comparatively simple. Therefore, multiply-accumulate operations and memory accesses remain the dominant tasks in terms of cost [9, 12].\nWhile structurally simple, determining an optimal MLP configuration is a difficult task due to the large number of pa-\nrameters inherent to even the simplest designs. For example, given the simple MLP shown in Figure 1(a), with i inputs, a single hidden layer with j nodes, and k nodes in the output layer, i\u00d7j+j\u00d7k operations must be evaluated at each inference pass and an equal number of weights must be accessed in memory. Starting from this initial configuration, a designer may then choose to include a second hidden layer, as illustrated in Figure 1(b); the first will then act as a feature extractor and the newly added layer will then process only the limited number of features generated by the first. This alteration allows for the reduction in dimension of the first hidden layer, which reduces the total number of connections to the input layer. However, this also increases the network depth and results in a cost penalty (in terms of requiring additional memory accesses and arithmetic operations associated with the newly added layer). The key problem demonstrated is that even for these two simple configurations, there is no systematic way to determine which design yields superior performance without having trained and evaluated both. Even when the designer has a priori knowledge of the application, determining the optimal hyper-parameters is non-intuitive, especially for deep networks.\nA concrete example of the described problem would be the design of an embedded system to recognize handwritten numerical characters (such as those contained in the MNIST dataset). If the implementation goal is throughput of categorized digits, and a penalty is incurred when a character is misclassified (perhaps requiring manual intervention), then a smaller network that requires fewer clock cycles to evaluate may still result in an overall throughput exceeding those of more accurate alternatives. In such a scenario, the engineer would require knowledge of the cost and performance of all Pareto-optimal solutions in order to meet all requirements with the lowest implementation costs.\nEven more complex structures are those of Convolutional Neural Network (CNN), which have demonstrated state-ofthe-art results in image recognition [15]. The use of convolutional layers further complicates the design process as they introduce a separation between the memory and pro-\nThis is the authors\u2019 final version. The authoritative version is to appear in the ACM Digital Library.\ncessing requirements. Illustrated in Figure 2, convolutional layers are composed of trainable filters (five 3-by-3 kernels are shown). CNNs have the advantage of reduced memory requirements due to each convolutional filter reusing the same kernel weights for all input values. However, this is at the expense of increased processing demands, the result of each convolutional filter requiring N2 multiplication operations for each input value (where the convolutional kernels are sized N-by-N ). In a CNN, each filter produces a processed copy of the input data (as illustrated in Figure 2); and without any form of down-sampling, this greatly increases the computational complexity of the following layers. An example of a possible efficient down-sampling method is the inclusion of max-pooling (or mean-pooling) after convolutional layers [15]. Illustrated in Figure 2, max-pooling refers to partitioning the filtered images into non-overlapping Kby-K regions, with each outputting the maximum pixel value within (alternatively, mean-pooling would output the mean value). All of these additional CNN parameters further increase the design space dimensionality, forcing a designer to not only choose how many filters to use in each layer, but also the kernel and pooling sizes. This greatly affects both the performance and computational complexity of the resulting networks."}, {"heading": "1.2 Summary of Contributions", "text": "The key contributions of this work are presented as follows:\n\u2022 We introduce a DSE method, which employs RSM techniques to predict classification accuracy, to automate the design of ANN hyper-parameters. This method is then validated with MLP and CNN designs targeting the CIFAR-10 and MNIST image recognition datasets [14, 17].\n\u2022 We demonstrate that multi-objective hyper-parameter optimization can successfully be used as a method to reduce ANN implementation cost (computational complexity).\n2. RELATED WORK\nThis work exists at the intersection of two fields: automated hyper-parameter optimization, and reduction of ANN computational complexity. To our knowledge, this work is the first that has applied automated hyper-parameter optimization as a method of reducing ANN computational complexity."}, {"heading": "2.1 Hyper-Parameter Optimization", "text": "The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22]. However, these works have been developed with the sole purpose of optimizing performance, with little regard to the resulting computational resource requirements.\nTwo types of sequential hyper-parameter optimization algorithms were presented in [4]; in both cases experimental results compared positively to human designed alternatives. These findings were echoed in [3], where it was demonstrated that a random search of a large design space, which contains areas that may be considered less promising, can at times exceed the performance of manually tuned hyper-parameters.\nPositive results were also presented in [7] where similar algorithms were extended using a method to extrapolate the shape of learning curves during training, so as to evaluate fewer epochs for unfit solutions and reduce design time. Since parameters which perform favourably for a large network may not be optimal for a smaller alternative, and the most important hyper-parameters (with the greatest impact on resulting performance) may vary for different data sets, the need for multi-objective optimization (especially where low power platforms are targeted) is clear [3, 22].\nAdditionally, in [20] an automated DSE method was applied to the multi-objective optimization problem of applicationspecific MPSoC design, a field which also consists of highdimensionality solution spaces. It was demonstrated that\nthrough RSM the presented DSE method could efficiently identify Pareto-optimal architectures."}, {"heading": "2.2 Weight Quantization and Pruning", "text": "There also exists a body of work attempting to reduce the computational complexity of ANN models through weight quantization or the removal of extraneous node connections (pruning). The research in [6] and [9] are examples of two methods that construct networks which reduce the need for multiplications, or modify already trained networks in order to minimize the number of non-zero weights.\nIn [6], the authors attempted to reduce the computational resources required when evaluating fully-connected, as well as convolutional, networks through representing all weights as binary values of +1 or\u22121. Doing so reduces the number of multiplication operations performed each forward pass; however the requirement to store floating-point weights during training remains. The work in [6] also compared trained binary weighted networks to traditional equivalents with equal layer dimensions and similar performance was demonstrated. However, [6] only considered very large network dimensions; further benefits may be obtained from smaller optimized architectures.\nInstead of restricting weights to specific values, in [9] a pruning method was presented in which a network can be trained while reducing the number of non-zero weights. The resulting compressed networks have lower bandwidth requirements and require fewer multiplications due to most weights being zero. The results in [9] demonstrated up to 70% reductions in the numbers of floating-point operations required for various networks, with little to no reduction in performance. However, such pruning methods are not mutually exclusive to the use of DSE tools and could very well be implemented in conjunction with the presented methodology in order to compress an already optimized network configuration."}, {"heading": "3. ANN SELF-DESIGN METHODOLOGY", "text": "This work presents a DSE approach that searches for Paretooptimal hyper-parameter configurations and has been applied to both MLP and CNN topologies. The design space is confined to: the numbers of Fully-Connected (FC) and convolutional layers, the number of nodes or filters in each layer, the convolution kernel sizes, the max-pooling sizes, the type of activation function, and network training rate. These degrees of freedom constitute vast design spaces and all strongly influence the resulting networks\u2019 cost and performance.\nFor design spaces of such size, performing an exhaustive search is intractable (designs with over 1010 to 1020 possible solutions are not uncommon), therefore we propose to model the response surface using an ANN for regression where the set of explored solution points is used as a training set. The presented meta-heuristic ANN is then used to predict the performance of candidate networks; only points which are expected not to be Pareto-dominated are explored."}, {"heading": "3.1 Main DSE Algorithm Overview", "text": "A flowchart describing the DSE implementation is shown in Figure 3. The general steps performed during the design space exploration can be broken down into:\n1. Sample the next candidate point from a Gaussian distribution centred around the previously explored solution (or sample a random point for the first iteration).\n2. Predict the candidate solution performance using the RSM neural network, and calculate the cost as:\ncost = (# of weights)\u00d7 (weight unit cost) + (# of multiplications)\u00d7 (multiplication unit cost)\n3. Compare the predicted results to the current Paretooptimal front. If the candidate is predicted to be Paretodominated, it is accepted with probability \u03b1, otherwise it is accepted with probability 1\u2212 \u03b1.\n4. If rejected, the previously explored solution is rolled back and the algorithm returns to Step 1.\n5. If accepted, the candidate model is trained, tested, and the evaluated results are added to the training set of the RSM ANN (which is then retrained).\n6. Finally, if the training set size exceeds the maximum number of desired iterations, the process ends. Otherwise, the algorithm returns to Step 1 and a new solution is sampled."}, {"heading": "3.2 Candidate Solution Sampling", "text": "The sampling strategy proposed is an adaptation of the Metropolis-Hastings algorithm [18]. In each iteration a new candidate is sampled from a Gaussian distribution centred around the previously explored solution point. Performing this random walk limits the number of samples chosen from areas of the design space that are known to contain unfit solutions, thereby reducing wasted exploration effort. However, exploring an inferior solution may eventually lead to those of superior performance, therefore the probability of accepting such a solution (\u03b1) must remain greater than zero; this also ensures that the training set for the RSM ANN remains varied. All experimental results in Section 5 were obtained with \u03b1 = 10\u22124."}, {"heading": "3.3 Predictive Neural Network Design", "text": "We choose to model the response surface using a MLP model with an input set representative of network hyper-parameters and a single output trained to predict the error of corresponding networks. This RSM ANN is composed of two hidden ReLU layers and a linear output layer. Experimental results demonstrated that sizing the hidden layers with 25\u00d7 to 30\u00d7 the number of input nodes provided the best performance.\nThe RSM network inputs are formed as arrays characterizing all explored dimensions. Integer input parameters (such as number of nodes in a hidden layer, or size of the convolutional kernels) are scaled by the maximum possible value of the respective parameter, resulting in normalized variables between 0 and 1. For each parameter that represents a choice where the options have no numerical relation to each other (such as whether ReLU or sigmoid functions are used) an input mode is added and the node that represents the chosen option is given an input value of 1, all others \u22121. For example, a solution with two hidden layers with 20 nodes each (assuming a maximum of 100), using ReLUs (with the other option being sigmoid functions) and with a learning rate of 0.5 would be presented as input values: [0.2, 0.2, 1,\u22121, 0.5].\nThe RSM model was trained using SGD, where 100 training epochs were performed on the set of explored solutions each time the next is evaluated (and in turn, added to the training set). The learning rate was kept constant, with a value of 0.1, in order to train the network quickly during early exploration, when the set of evaluated solutions is limited."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "We evaluated our DSE strategy on ANN applications targeting the standard MNIST and CIFAR-10 image recognition datasets. In the case of designing MLP models targeting the simpler MNIST problem, the results generated by the DSE algorithm were compared to the true Pareto-optimal front obtained from an exhaustive search performed on a constrained solution space. Such a limitation of the design space was required in order to render the exhaustive search tractable. The DSE algorithm was also evaluated on much larger design spaces for both MLP and CNN models targeting MNIST as well as an even larger one for the design of CNNs targeting CIFAR-10. In all cases only a few iterations were required in order to converge to approximated Pareto-optimal fronts.\nIn order to perform the DSE, all ANN models were trained and tested using the Theano framework in order to take advantage of GPGPU optimizations [2]. This allowed the entire DSE and evaluation to be performed using a Nvidia Tesla K20c GPU. Upon completion, all explored network data and simulation results were stored to disk, allowing future design re-use.\nTo model cost, we assumed normalized memory access and multiply-accumulate operation costs. These can be quantified at either the software or hardware levels (depending on the target application). In both cases the costs (such as power, area, or time) are implementation specific and the exact values used are transparent to the DSE algorithm.\nNormalized costs assumed for all the experimental results are shown in Table 1, which are based on the energy costs for 32-bit floating-point operations from [9].\nWe experimentally validated the DSE algorithm on two separate image recognition databases; future work is planned to address a broader range of benchmarks. The first, MNIST, contains 28 \u00d7 28 pixel grayscale images of handwritten numeric characters (from 0 to 9). The second, CIFAR-10, is a much more complicated dataset composed of 32 \u00d7 32 RGB colour images, each belonging to one of ten object categories."}, {"heading": "5. RESULTS", "text": "The network hyper-parameters composing the design spaces explored are outlined in Table 2. In addition, several parameters were kept constant for all experiments: output layers were composed of softmax units (with jth output defined\nas einj \u00f7 \u2211K\nk=1 e ink for a layer with K nodes) and all net-\nwork training was performed with categorical cross-entropy (\u2212 \u2211\n[targets\u00d7 log (predictions)]) as loss function [8, 12]. Finally, for all except the reduced MNIST design problem, batch normalization was included after each network layer in order to smooth the response surfaces [10]."}, {"heading": "5.1 Exhaustive Search Comparison", "text": "In order to evaluate the efficiency with which the method approximates the true Pareto-optimal front, we first compare experimental results to those of an exhaustive search targeting the design of MLP models for the MNIST dataset. In order to make an exhaustive search tractable, we limited the design space to only the values outlined in the first section of Table 2. This resulted in a moderate design space of 104 solutions, all of which were trained and tested.\nThe results of executing the DSE algorithm for 200 iterations (each iteration represents a design training, evaluation, and model update pass) are plotted alongside those of the exhaustive search in Figure 4. These results demonstrate that the true Pareto-optimal front is very closely approximated by the presented method, while requiring very few solutions to be evaluated. However, it should be noted that SGD is by nature non-deterministic and training the same network twice may yield different performances. Consequently, the DSE generated results (shown in Figure 4) dominate those of the exhaustive search at several points. This figure also demonstrates that the majority of solutions evaluated by the DSE algorithm remain within a close vicinity of the true Pareto-front, successfully avoiding Paretodominated areas. Since the true Pareto-optimal front is\nknown for this restricted case, we use Average Distance to Reference Set (ADRS) as the metric of evaluation; quantifying how closely the approximated set differs from the exact [20]. ADRS is defined in Eq. (1), where \u039b and \u03a0 are the approximate and exact Pareto-optimal fronts, and the \u03b4 (xR, xA) function represents the normalized distance between solutions xR and xA.\nADRS(\u03a0,\u039b) = 1 |\u03a0| \u2211\nxR\u2208\u03a0\nmin xA\u2208\u039b\n( \u03b4 (xR, xA) ) (1)\nThe evolution of the approximated Pareto-optimal set discovered by the DSE algorithm, as a function of iterations completed is plotted in Figure 5(a). Evident from this figure is that the optimal front obtained from the DSE algorithm progressively approaches the true Pareto-optimal, while evaluating only a comparatively small number of iterations. The proposed method identifies the optimal solutions with high accuracy, achieving low ADRS values of 7.1% after completing only 50 iterations, 5.0% after 100, and 3.6% after 200. The results in Table 3 also demonstrate fast convergence, with the largest changes occurring over the first 30 iterations. Further execution yields more gradual changes as the approximated Pareto-front asymptotically approaches the exact. Decreasing design time (both in terms of computation and man-hours), these results also demon-\nstrate that by exploring less than 1% of the design space, the DSE method closely predict which hyper-parameters are required for optimal solutions. In addition, the generated set of Pareto-optimal configurations also exposes the trade-offs application designers may want to make for cost-constrained systems, allowing for more informed design choices to be made."}, {"heading": "5.2 Evaluation on Expanded Design Spaces", "text": "In order to evaluate performance of the heuristic method for much larger designs, the algorithm was run on the remaining spaces described in Table 2 for both MLP and CNN design problems. The total execution times (on an Intel Xeon E51603 CPU with 8GB of RAM and a Nvidia Tesla K20c GPU) for the design examples are listed in Table 4 and the corresponding Pareto-optimal results (plotted as functions of the number of iterations completed) are shown in Figure 5. In these plots, the colour scheme presents solutions with low error in blue, and low cost in red. Even though the DSE outputs cannot be directly compared to the true results from an exhaustive search (the CIFAR-10 example design space exceeds 1010 solutions), the trends discussed in Section 5.1 are mirrored by all plots in Figure 5.\nBecause of the intractable nature of such exhaustive searches, the DSE generated results are not expected to always predict the true Pareto-optimal fronts. Instead, automated explo-\nration must only match, or exceed, the fitness of manually designed networks in order to provide NRE cost reductions. In comparison with manually designed architectures in literature, the Pareto-optimal results in Figure 5(c) include points that offer equivalent performance to the CNN designs in [12] and [17], with implementation costs as low as 25% of their manually designed counterparts (when weighted with the same cost model detailed in Table 1). This demonstrates that the proposed DSE method can design ANNs with vastly reduced cost requirements and same levels of performance when compared to manual design approaches.\nFurthermore, the Pareto-optimal results can also find data points with substantial cost savings penalized only by a small decrease in performance. Given that increasing accuracy by only 0.01% requires a doubling in implementation cost (for MNIST CNN designs shown in Figure 5(c)), these additional points are invaluable for application platforms with extremely stringent cost budgets."}, {"heading": "5.3 RSM Prediction Accuracy", "text": "In order to validate the assumption that a neural network can be trained through regression to model the response surface with sufficient accuracy, the RSM ANN prediction error is plotted in Figure 6. This graph plots the absolute value of the error (% difference between the predicted and the evaluated performance of each explored solution) for each of the 500 DSE algorithm iterations performed during the MNIST CNN design example (with results in Figure 5(c)). The narrow error spikes, which are expected, occur at points where the DSE algorithm encounters previously unexplored areas. As these solutions are added to the training set, the prediction error decreases as the response surface model is updated, and the spikes begin to occur less frequently. Outside of these sparse peaks, the prediction accuracy is exceptionally high; the mean error over the last 95 iterations (all points after the last spike) is only 0.35%."}, {"heading": "6. CONCLUSION", "text": "A DSE method to automate the multi-objective optimization of neural network hyper-parameters, reducing both algorithm error and computational complexity, was presented. When compared to the results of an exhaustive search on a restricted design space targeting the MNIST dataset, the presented method was shown to dramatically reduce computation time required for convergence to a Pareto-optimal solution set. A low ADRS of 5% was achieved after only 100 iterations; in practice, fewer solution evaluations may be required, with corresponding execution times less than those in Table 4. Furthermore, scalability of the method was demonstrated on larger design spaces for both MLP and CNN models targeting the CIFAR-10 dataset as well.\nEven when evaluated on massive design spaces, the presented DSE method was found to still efficiently converge to a diverse Pareto-optimal front. Not only was the automation of ANN hyper-parameter design process demonstrated to be both feasible and time efficient, but when compared to manually designed networks from literature, the automated DSE technique produced results with near identical performance while reducing the associated costs by a factor of 3\u00d7. As applications for ANNs make further inroads in mobile and embedded market segments, the need to reduce timeto-market and NRE costs will necessitate the use of such automated design methods."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This work was supported in part by a Postgraduate Scholarship - Doctoral (PGS-D) scholarship from the Natural Sciences and Engineering Research Council of Canada (NSERC), as well as equipment donations from Nvidia Corporation."}, {"heading": "8. REFERENCES", "text": "[1] M. Assefi, M. Wittie, and A. Knight. Impact of\nnetwork performance on cloud speech recognition. In Computer Communication and Networks (ICCCN), 2015 24th Int. Conf., pages 1\u20136, Aug 2015.\n[2] F. Bastien et al. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.\n[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. J. of Machine Learning Research, 13(1):281\u2013305, Feb 2012.\n[4] J. S. Bergstra et al. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24, pages 2546\u20132554. Curran Associates, Inc., 2011.\n[5] R. Collobert and S. Bengio. Links between perceptrons, MLPs and SVMs. In Proc. of the Twenty-first Int. Conf. on Machine Learning, ICML \u201904, pages 23\u201330, New York, NY, USA, 2004. ACM.\n[6] M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems 28, pages 3123\u20133131. Curran Associates, Inc., 2015.\n[7] T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Proc. of the Twenty-Fourth Int. Joint Conf. on Artificial Intelligence, IJCAI, pages 3460\u20133468, July 2015.\n[8] A. Graves, A. R. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pages 6645\u20136649, May 2013.\n[9] S. Han et al. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems 28, pages 1135\u20131143. Curran Associates, Inc., 2015.\n[10] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015.\n[11] S. Ishibushi et al. Statistical localization exploiting convolutional neural network for an autonomous vehicle. In Industrial Electronics Society, IECON 2015 - 41st Annual Conf. of the IEEE, pages 1369\u20131375, Nov 2015.\n[12] K. Jarrett et al. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th Int. Conf., pages 2146\u20132153, Sept 2009.\n[13] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.\n[14] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc., 2012.\n[16] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, May 2015.\n[17] Y. LeCun et al. Gradient-based learning applied to document recognition. Proc. of the IEEE, 86(11):2278\u20132324, Nov 1998.\n[18] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by fast computing machines. Journal of Chemical Physics, 21:1087\u20131092, Jun 1953.\n[19] T. Okabe, Y. Jin, and B. Sendhoff. A critical survey of performance indices for multi-objective optimisation. In Evolutionary Computation, 2003. CEC \u201903. The 2003 Congress on, volume 2, pages 878\u2013885, Dec 2003.\n[20] G. Palermo, C. Silvano, and V. Zaccaria. ReSPIR: A response surface-based Pareto iterative refinement for application-specific design space exploration. Computer-Aided Design of Integrated Circuits and Systems, IEEE Trans., 28(12):1816\u20131829, Dec 2009.\n[21] E. Park et al. Big/little deep neural network for ultra low power inference. In Hardware/Software Codesign and System Synthesis (CODES+ISSS), 2015 Int. Conf., pages 124\u2013132, Oct 2015.\n[22] S. R. Young et al. Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proc. of the Workshop on Machine Learning in High-Performance Computing Environments, MLHPC \u201915, pages 4:1\u20134:5, New York, NY, USA, 2015. ACM."}], "references": [{"title": "Impact of network performance on cloud speech recognition", "author": ["M. Assefi", "M. Wittie", "A. Knight"], "venue": "Computer Communication and Networks (ICCCN), 2015 24th Int. Conf., pages 1\u20136, Aug", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "J. of Machine Learning Research, 13(1):281\u2013305, Feb", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J.S. Bergstra"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Links between perceptrons, MLPs and SVMs", "author": ["R. Collobert", "S. Bengio"], "venue": "Proc. of the Twenty-first Int. Conf. on Machine Learning, ICML \u201904, pages 23\u201330, New York, NY, USA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems 28, pages 3123\u20133131. Curran Associates, Inc.,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves", "author": ["T. Domhan", "J.T. Springenberg", "F. Hutter"], "venue": "Proc. of the Twenty-Fourth Int. Joint Conf. on Artificial Intelligence, IJCAI, pages 3460\u20133468, July", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": "2013 IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pages 6645\u20136649, May", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, abs/1502.03167,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical localization exploiting convolutional neural network for an autonomous vehicle", "author": ["S. Ishibushi"], "venue": "In Industrial Electronics Society, IECON 2015 - 41st Annual Conf. of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, May", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun"], "venue": "Proc. of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "Journal of Chemical Physics, 21:1087\u20131092, Jun", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1953}, {"title": "A critical survey of performance indices for multi-objective optimisation", "author": ["T. Okabe", "Y. Jin", "B. Sendhoff"], "venue": "Evolutionary Computation, 2003. CEC \u201903. The 2003 Congress on, volume 2, pages 878\u2013885, Dec", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "ReSPIR: A response surface-based Pareto iterative refinement for application-specific design space exploration", "author": ["G. Palermo", "C. Silvano", "V. Zaccaria"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Trans., 28(12):1816\u20131829, Dec", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Big/little deep neural network for ultra low power inference", "author": ["E. Park"], "venue": "In Hardware/Software Codesign and System Synthesis (CODES+ISSS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Optimizing deep learning hyper-parameters through an evolutionary algorithm", "author": ["S.R. Young"], "venue": "In Proc. of the Workshop on Machine Learning in High-Performance Computing Environments,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Artificial Neural Network (ANN) models have become widely adopted as means to implement many machine learning algorithms and represent the state-of-the-art for many image and speech recognition applications [16].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Instead, such applications are processed remotely and, depending on network conditions, are subject to variations in performance and delay [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 10, "context": "ANNs are also finding application in other emerging areas, such as autonomous vehicle localization and control, where meeting power and cost requirements is paramount [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "These adjustments often involve a variation of the Stochastic Gradient Descent (SGD) method [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "This relies on human intuition and expert knowledge of the target application in conjunction with extensive trial and error [7, 22].", "startOffset": 124, "endOffset": 131}, {"referenceID": 21, "context": "This relies on human intuition and expert knowledge of the target application in conjunction with extensive trial and error [7, 22].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "With the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21].", "startOffset": 158, "endOffset": 169}, {"referenceID": 15, "context": "With the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21].", "startOffset": 158, "endOffset": 169}, {"referenceID": 20, "context": "With the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21].", "startOffset": 158, "endOffset": 169}, {"referenceID": 18, "context": "Formally, a solution vector (a1, b1) (where a and b are two objectives for optimization) is said to dominate another point (a2, b2) if a1 < a2 and b1 \u2264 b2, or b1 < b2 and a1 \u2264 a2; the set of points which are not dominated by any other solution constitutes the Pareto-optimal front [19].", "startOffset": 281, "endOffset": 285}, {"referenceID": 4, "context": "1 Motivational Example While there are many different ANN models, the MultiLayer Perceptron (MLP) is a well-known form, which rose in popularity with the advent of the back-propagation training algorithm [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 8, "context": "Therefore, multiply-accumulate operations and memory accesses remain the dominant tasks in terms of cost [9, 12].", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "Therefore, multiply-accumulate operations and memory accesses remain the dominant tasks in terms of cost [9, 12].", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "Even more complex structures are those of Convolutional Neural Network (CNN), which have demonstrated state-ofthe-art results in image recognition [15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "An example of a possible efficient down-sampling method is the inclusion of max-pooling (or mean-pooling) after convolutional layers [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "This method is then validated with MLP and CNN designs targeting the CIFAR-10 and MNIST image recognition datasets [14, 17].", "startOffset": 115, "endOffset": 123}, {"referenceID": 16, "context": "This method is then validated with MLP and CNN designs targeting the CIFAR-10 and MNIST image recognition datasets [14, 17].", "startOffset": 115, "endOffset": 123}, {"referenceID": 2, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 229, "endOffset": 232}, {"referenceID": 6, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 234, "endOffset": 237}, {"referenceID": 21, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 243, "endOffset": 247}, {"referenceID": 3, "context": "Two types of sequential hyper-parameter optimization algorithms were presented in [4]; in both cases experimental results compared positively to human designed alternatives.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "These findings were echoed in [3], where it was demonstrated that a random search of a large design space, which contains areas that may be considered less promising, can at times exceed the performance of manually tuned hyper-parameters.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Positive results were also presented in [7] where similar algorithms were extended using a method to extrapolate the shape of learning curves during training, so as to evaluate fewer epochs for unfit solutions and reduce design time.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "Since parameters which perform favourably for a large network may not be optimal for a smaller alternative, and the most important hyper-parameters (with the greatest impact on resulting performance) may vary for different data sets, the need for multi-objective optimization (especially where low power platforms are targeted) is clear [3, 22].", "startOffset": 337, "endOffset": 344}, {"referenceID": 21, "context": "Since parameters which perform favourably for a large network may not be optimal for a smaller alternative, and the most important hyper-parameters (with the greatest impact on resulting performance) may vary for different data sets, the need for multi-objective optimization (especially where low power platforms are targeted) is clear [3, 22].", "startOffset": 337, "endOffset": 344}, {"referenceID": 19, "context": "Additionally, in [20] an automated DSE method was applied to the multi-objective optimization problem of applicationspecific MPSoC design, a field which also consists of highdimensionality solution spaces.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "The research in [6] and [9] are examples of two methods that construct networks which reduce the need for multiplications, or modify already trained networks in order to minimize the number of non-zero weights.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "The research in [6] and [9] are examples of two methods that construct networks which reduce the need for multiplications, or modify already trained networks in order to minimize the number of non-zero weights.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "In [6], the authors attempted to reduce the computational resources required when evaluating fully-connected, as well as convolutional, networks through representing all weights as binary values of +1 or\u22121.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The work in [6] also compared trained binary weighted networks to traditional equivalents with equal layer dimensions and similar performance was demonstrated.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "However, [6] only considered very large network dimensions; further benefits may be obtained from smaller optimized architectures.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Instead of restricting weights to specific values, in [9] a pruning method was presented in which a network can be trained while reducing the number of non-zero weights.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "The results in [9] demonstrated up to 70% reductions in the numbers of floating-point operations required for various networks, with little to no reduction in performance.", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "2 Candidate Solution Sampling The sampling strategy proposed is an adaptation of the Metropolis-Hastings algorithm [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 1, "context": "In order to perform the DSE, all ANN models were trained and tested using the Theano framework in order to take advantage of GPGPU optimizations [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Operation Energy Cost from [9] Normalized Cost", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "Normalized costs assumed for all the experimental results are shown in Table 1, which are based on the energy costs for 32-bit floating-point operations from [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "In addition, several parameters were kept constant for all experiments: output layers were composed of softmax units (with jth output defined as ej \u00f7 \u2211K k=1 e ink for a layer with K nodes) and all network training was performed with categorical cross-entropy (\u2212 \u2211 [targets\u00d7 log (predictions)]) as loss function [8, 12].", "startOffset": 311, "endOffset": 318}, {"referenceID": 11, "context": "In addition, several parameters were kept constant for all experiments: output layers were composed of softmax units (with jth output defined as ej \u00f7 \u2211K k=1 e ink for a layer with K nodes) and all network training was performed with categorical cross-entropy (\u2212 \u2211 [targets\u00d7 log (predictions)]) as loss function [8, 12].", "startOffset": 311, "endOffset": 318}, {"referenceID": 9, "context": "Finally, for all except the reduced MNIST design problem, batch normalization was included after each network layer in order to smooth the response surfaces [10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "1 16 Log Filter kernel size 3x3 to 9x9 4 Linear Max-pool size 2x2 to 3x3 2 Linear Activation function ReLU 1 N/A Training algorithm Adam [13] 1 N/A Batch sizes 200 1 N/A Training epochs 20 1 N/A", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "known for this restricted case, we use Average Distance to Reference Set (ADRS) as the metric of evaluation; quantifying how closely the approximated set differs from the exact [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "In comparison with manually designed architectures in literature, the Pareto-optimal results in Figure 5(c) include points that offer equivalent performance to the CNN designs in [12] and [17], with implementation costs as low as 25% of their manually designed counterparts (when weighted with the same cost model detailed in Table 1).", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "In comparison with manually designed architectures in literature, the Pareto-optimal results in Figure 5(c) include points that offer equivalent performance to the CNN designs in [12] and [17], with implementation costs as low as 25% of their manually designed counterparts (when weighted with the same cost model detailed in Table 1).", "startOffset": 188, "endOffset": 192}], "year": 2016, "abstractText": "Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 10 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.", "creator": "LaTeX with hyperref package"}}}