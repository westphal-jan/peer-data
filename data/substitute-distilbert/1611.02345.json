{"id": "1611.02345", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "abstract": "modern convolutional calculations, incorporating rectifiers and max - pooling, are neither smooth nor convex. standard guarantees therefore don't apply. nevertheless, methods from convex optimization such as gradient descent and adam are widely used as building blocks for deep learning algorithms. another paper provides the first convergence guarantee applicable to spatial convnets. last solution matches a lower bound for convex nonsmooth functions. the key technical tool is general neural spectral approximation - - a straightforward application of taylor expansions to dimensional domains - - and the associated taylor loss. experiments on a range of optimizers, layers, and tasks provide evidence that the analysis thus captures recent attitudes towards neural optimization.", "histories": [["v1", "Mon, 7 Nov 2016 23:47:05 GMT  (1608kb,D)", "https://arxiv.org/abs/1611.02345v1", "13 pages, 6 figures"], ["v2", "Fri, 24 Feb 2017 02:26:15 GMT  (1609kb,D)", "http://arxiv.org/abs/1611.02345v2", "11 pages, 6 figures"]], "COMMENTS": "13 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["david balduzzi", "brian mcwilliams", "tony butler-yeoman"], "accepted": true, "id": "1611.02345"}, "pdf": {"name": "1611.02345.pdf", "metadata": {"source": "CRF", "title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks", "authors": ["David Balduzzi", "Brian McWilliams", "Tony Butler-Yeoman"], "emails": ["<dbalduzzi@gmail.com>."], "sections": [{"heading": null, "text": "This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation \u2013 a straightforward application of Taylor expansions to neural networks \u2013 and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization.\nThe second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions."}, {"heading": "1. Introduction", "text": "Deep learning has achieved impressive performance on a range of tasks (LeCun et al., 2015). The workhorse underlying deep learning is gradient descent or backprop. Gradient descent has convergence guarantees in settings that are smooth, convex or both. However, modern convnets are neither smooth nor convex. Every winner of the ImageNet classification challenge since 2012 has used rectifiers which are not smooth (Krizhevsky et al., 2012; Zeiler\n1Victoria University of Wellington, New Zealand 2Disney Research, Zu\u0308rich, Switzerland. Correspondence to: David Balduzzi <dbalduzzi@gmail.com>.\nFig. 1: Shattered gradients in a PL-function.\n& Fergus, 2014; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2015). Even in convex settings, convergence for nonsmooth functions is lower-bounded by 1/ \u221a N (Bubeck, 2015).\nThe paper\u2019s main contribution is the first convergence result for modern convnets, Theorem 2. The idea is simple: backprop constructs linear snapshots (gradients) of a neural net\u2019s landscape; section 2 introduces neural Taylor approximations which are used to construct Taylor losses as convex snapshots closely related to backprop. We then use the online convex optimization framework (Zinkevich, 2003) to show 1/ \u221a N convergence to the Taylor optimum, matching the lower bound in (Bubeck, 2015). Section 2.4 investigates the Taylor optimum and regret terms empirically. We observe that convergence to the Taylor optimum occurs at 1/ \u221a N in practice. The theorem applies to any neural net with a loss convex in the output of the net (for example, the cross-entropy loss is convex in the output but not the parameters of a neural net).\nThe nonsmoothness of rectifier nets is perhaps underappreciated. Fig. 1 shows a piecewise-linear (PL) function and its gradient. The gradient is discontinuous or shattered. Shattering is problematic for accelerated and Hessian-based methods which speed up convergence by exploiting the relationship between gradients at nearby points (Sutskever et al., 2013). The success of these methods on rectifier networks requires explanation since gradients at (nearby) points on different sides of a kink are not related. Further, the number of kinks grows exponentially with network depth (Pascanu et al., 2014; Telgarsky, 2016).\nSection 3 addresses the success of adaptive optimizers\nar X\niv :1\n61 1.\n02 34\n5v 2\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n17\nin rectifier nets.1 Adaptive optimizers normalize gradients by their root-mean-square; e.g. AdaGrad, RMSProp, Adam and RadaGrad (Duchi et al., 2011; Hinton et al., 2012; Kingma & Ba, 2015; Krummenacher et al., 2016). Dauphin et al. (2015) argue that RMSProp is successful because it approximates the equilibriation matrix\u221a\ndiag(H2) which approximates the absolute Hessian |H| (Dauphin et al., 2014). However, the argument is at best part of the story when gradients are shattered.\nThe only way an optimizer can estimate gradients of a shattered function is to compute them directly. Effective optimizers must therefore explore the space of smooth regions \u2013 the bound in theorem 2 is only as good as the optimum over the Taylor losses encountered during backprop. Observations 1 and 2 relate smooth regions in rectifier nets and the Taylor losses to configurations of active neurons. We hypothesize that root-mean-square normalization increases exploration through the set of smooth regions in a rectifier net\u2019s landscape. Experiments in section 3.3 confirm the hypothesis."}, {"heading": "1.1. Comparison with related work", "text": "Researchers have applied convex techniques to neural networks. Bengio et al. (2006) show that choosing the number of hidden units converts neural optimization into a convex problem, see also Bach (2014). A convex multi-layer architectures are developed in Aslan et al. (2014); Zhang et al. (2016). However, these approaches have not achieved the practical success of convnets. In this work, we analyze convnets as they are rather than proposing a more tractable, but potentially less useful, model. A Taylor decomposition for neural networks was proposed in Montavon et al. (2015). They treat inputs as variable instead of weights and study interpretability instead of convergence.\nOur results are closely related to Balduzzi (2016), which uses game-theoretic techniques to prove convergence in rectifier nets. The approach taken here is more direct and holds in greater generality."}, {"heading": "2. Convergence of Neural Networks", "text": "Before presenting the main result, we highlight some issues that arise when studying convergence in rectifier nets. Many optimization methods have guarantees that hold in convex or smooth settings. However, none of the guarantees extend to rectifier nets. For example, the literature provides no rigorous account of when or why Adam\n1For simplicity, we restrict to fully connected rectifier (ReLU) nets. The results also apply to convolutions, max-pooling, dropout, dropconnect, maxout, PReLUs and CReLUs (Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013; He et al., 2015; Shang et al., 2016).\nor Adagrad converges faster on rectifier nets than vanilla gradient descent. Instead, we currently have only intuition, empirics and an analogy with convex or smooth settings.\nGradient-based optimization on neural nets can converge on local optima that are substantially worse than the global optimum. Fortunately, \u201cbad\u201d local optima are rare in practice. A partial explanation for the prevalence of \u201cgood enough\u201d local optima is Choromanska et al. (2015). Nevertheless, it is important to acknowledge that neural nets can and do converge to bad local optima. It is therefore impossible to prove (non-stochastic) bounds relative to the global optimum. Such a result may be provable under further assumptions. However, since the result would contradict empirical evidence, the assumptions would necessarily be unrealistic."}, {"heading": "2.1. What kind of guarantee is possible?", "text": "The landscape of a rectifier net decomposes into smooth regions separated by kinks (Pascanu et al., 2014; Telgarsky, 2016), see figure 2. Gradients on different sides of a kink are unrelated since the derivative is discontinuous. Gradient-based optimizers cannot \u201cpeer around\u201d the kinks in rectifier nets.\nGradient-based optimization on rectifier nets thus factorizes conceptually into two components. The first is steepest descent in a smooth region; the second moves between smooth regions. The first component is vanilla optimization whereas the second involves an element of exploration: what the optimizer encounters when it crosses a kink cannot be predicted.\nThe convergence guarantee in theorem 2 takes both components of the factorization into account in different ways. It is formulated in the adversarial setting of online convex optimization. Intuitively, the adversary is the nonsmooth geometry of the landscape, which generates what may-aswell-be a new loss whenever backprop enters a different smooth region.\nBackprop searches a vast nonconvex landscape with a linear flashlight (the Taylor losses are a more sharply focused convex flashlight, see A4). The adversary is the landscape: from backprop\u2019s perspective its geometry \u2013 especially across kinks \u2013 is an unpredictable external force.\nThe Taylor losses are a series of convex problems that backprop de facto optimizes \u2013 the gradients of the actual and Taylor losses are identical. The Taylor optimum improves when, stepping over a kink, backprop shines its light on a new (better) region of the landscape (fig. 2). Regret quantifies the gap between the Taylor optimal loss and the losses incurred during training."}, {"heading": "2.2. Online Convex Optimization", "text": "In online convex optimization (Zinkevich, 2003), a learner is given convex loss functions `1, . . . `N . On the nth round, the learner predicts Wn prior to observing `n, and then incurs loss `n(Wn). Since the losses are not known in advance, the performance of the learner is evaluated post hoc via the regret, the difference between the incurred losses and the optimal loss in hindsight:\nRegret(N) := N\u2211\nn=1\n[ `n(Wn)\ufe38 \ufe37\ufe37 \ufe38\nlosses incurred \u2212 `n(V\u2217)\ufe38 \ufe37\ufe37 \ufe38 optimal-in-hindsight\n]\nwhere V\u2217 := argminV\u2208H [\u2211N n=1 ` n(V) ] . An algorithm has no-regret if limN\u2192\u221e Regret(N)/N = 0 for any sequence of convex losses with bounded gradients. For example, Kingma & Ba (2015) prove:\nTheorem 1 (Adam has no-regret). Suppose the convex losses `n have bounded gradients \u2016\u2207W `n(W)\u20162 \u2264 G and \u2016\u2207W `n(W)\u2016\u221e \u2264 G for all W \u2208 H and suppose that the weights chosen by the algorithm satisfy \u2016Wm \u2212Wn\u20162 \u2264 D and \u2016Wm \u2212Wn\u2016\u221e \u2264 D for all m,n \u2208 {1, . . . , N}. Then Adam satisfies\nRegret(N)/N \u2264 O ( 1/ \u221a N ) for all N \u2265 1. (1)\nThe regret of gradient descent, AdaGrad (Duchi et al., 2011), mirror descent and a variety of related algorithms satisfy (1), albeit with different constant terms that are hidden in the big-O notation. Finally, the 1/ \u221a N rate is also lower-bound. It cannot be improved without additional assumptions."}, {"heading": "2.3. Neural Taylor Approximation", "text": "Consider a network with L \u2212 1 hidden layers and weight matrices W := {W1, . . . ,WL}. Let x0 denote the input. For hidden layers l \u2208 {1, . . . , L \u2212 1}, set al = Wl \u00b7 xl\u22121\nand xl = s(al) where s(\u00b7) is applied coordinatewise. The last layer outputs xL = aL = WL \u00b7 xL\u22121. Let pl denote the size of the lth layer; p0 is the size of the input and pL is the size of the output. Suppose the loss `(f , y) is smooth and convex in the first argument. The training data is (xd, yd)Dd=1. The network is trained on stochastic samples from the training data on a series of rounds n = 1, . . . , N . For simplicity we assume minibatch size 1; the results generalize without difficulty.\nWe recall backprop using notation from Martens et al. (2012). Let Jab denote the Jacobian matrix of the vector a with respect to the vector b. By the chain rule the gradient decomposes as\n\u2207Wl ` ( fW(x0), y) = J\nE L \u00b7 JLL\u22121 \u00b7 \u00b7 \u00b7Jl+1l\ufe38 \ufe37\ufe37 \ufe38\n\u03b4l\n\u2297xl\u22121 (2)\n= JEL\ufe38\ufe37\ufe37\ufe38 \u2207f `(f ,y) \u00b7 JLl \u2297 xl\u22121\ufe38 \ufe37\ufe37 \ufe38 \u2207Wl fW(x0)=:Gl\nwhere \u03b4l = JEl is the backpropagated error computed recursively via \u03b4l = \u03b4l+1 \u00b7 Jl+1l .2 The middle expression in (2) is the standard representation of backpropagated gradients. The expression on the right factorizes the backpropagated error \u03b4l = JEL \u00b7JLl into the gradient of the loss JEL and the Jacobian JLl between layers, which describes gradient flow within the network.\nThe first-order Taylor approximation to a differentiable function f : R\u2192 R near a is Ta(x) = f(a)+f \u2032(a)\u00b7(x\u2212a). The neural Taylor approximation for a fully connected network is as follows.\nDefinition 1. The Jacobian tensor of layer l, Gl := JLl \u2297 xl\u22121, is the gradient of the output of the neural network with respect to the weights of layer l. It is the outer product of a (pL \u00d7 pl)-matrix with a pl\u22121-vector, and so is a (pL, pl, pl\u22121)-tensor.\nGiven Gl and (pl \u00d7 pl\u22121)-matrix V, the expression \u3008Gl,V\u3009 := JLl \u00b7 V \u00b7 xl\u22121 is the pL-vector computed via matrix-matrix-vector multiplication. The neural Taylor approximation to f in a neighborhood of Wn, given input xn0 , is the first-order Taylor expansion\nTn(V) := fWn(x n 0 ) +\nL\u2211\nl=1\n\u2329 Gl,Vl \u2212Wnl \u232a \u2248 fV(xn0 ).\nFinally, the Taylor loss of the network on round n is T n(V) = `(Tn(V), yn).\nThe Taylor approximation to layer l is\nTnl (Vl) := fWn(x n 0 ) + \u2329 Gl,Vl \u2212Wnl \u232a .\n2Note: we suppress the dependence of the Jacobians on the round n to simplify notation.\nWe can also construct the Taylor approximation to neuron \u03b1 in layer l. Let the pL-vector JL\u03b1 := J L l [ :, \u03b1] denote the Jacobian with respect to neuron \u03b1 and let the (pL \u00d7 pl\u22121)matrix G\u03b1 := JL\u03b1 \u2297 xl\u22121 denote the Jacobian with respect to the weights of neuron \u03b1. The Taylor approximation to neuron \u03b1 is\nTn\u03b1(V\u03b1) := fWn(x n 0 ) + \u2329 G\u03b1,V\u03b1 \u2212Wn\u03b1 \u232a .\nThe Taylor losses are the simplest non-trivial (i.e. nonaffine) convex functions encoding the information generated by backprop, see section A4.\nThe following theorem provides convergence guarantees at mutiple spatial scales: network-wise, layer-wise and neuronal. See sections A2 for a proof of the theorem. It is not currently clear which scale provides the tightest bound.\nTheorem 2 (no-regret relative to Taylor optimum). Suppose, as in Theorem 1, the Taylor losses have bounded gradients and the weights of the neural network have bounded diameter during training.\nSuppose the neural net is optimized by an algorithm with Regret(N) \u2264 O( \u221a N) such as gradient descent, AdaGrad, Adam or mirror descent.\n\u2022 Network guarantee: The running average of the training error of the neural network satisfies\n1\nN\nN\u2211\nn=1\n` (fWn(x n 0 ), y n)\n\ufe38 \ufe37\ufe37 \ufe38 running average of training errors\n\u2264 min V\n{ 1\nN\nN\u2211\nn=1\nT n(V) }\n\ufe38 \ufe37\ufe37 \ufe38 Taylor optimum\n(3)\n+ O ( 1\u221a N )\n\ufe38 \ufe37\ufe37 \ufe38 Regret(N)/N\n.\n\u2022 Layer-wise / Neuron-wise guarantee: The Taylor loss of [layer-l / neuron-\u03b1] is T nl/\u03b1(Vl/\u03b1) := `(Tnl/\u03b1(Vl/\u03b1), yn). Then,\n1\nN\nN\u2211\nn=1\n` (fWn(x n 0 ), y n)\n\ufe38 \ufe37\ufe37 \ufe38 running average of training errors\n\u2264 min Vl/\u03b1\n{ 1\nN\nN\u2211\nn=1\nT nl/\u03b1(Vl/\u03b1) }\n\ufe38 \ufe37\ufe37 \ufe38 layer-wise/neuronal Taylor optimum\n(4)\n+ O ( 1\u221a N )\n\ufe38 \ufe37\ufe37 \ufe38 Regret(N)/N\n.\nThe running average of errors during training (or cumulative loss) is the same quantity that arises in the analyses\nof Adam and Adagrad (Kingma & Ba, 2015; Duchi et al., 2011).\nImplications of the theorem. The global optima of neural nets are not computationally accessible. Theorem 2 sidesteps the problem by providing a guarantee relative to the Taylor optimum. The bound is path-dependent; it depends on the convex snapshots encountered during backprop.\nPath-dependency is a feature not a bug. It is a simple matter to construct a deep fully connected network (> 100 layers) that fails to learn because gradients do not propagate through the network (He et al., 2016). A convergence theorem for neural nets must also be applicable in pathological cases. Theorem 2 still holds because the failure of gradients to propagate through the network results in Taylor losses with poor solutions.\nAlthough the bound in theorem 2 is path-dependent, it is nevertheless meaningful. The right-hand-side is given by the Taylor optimum, which is the optimal solution to the best convex approximations to the actual losses; best in the sense that they have the same value and have the same gradient for the encountered weights. The theorem replaces a seemingly intractable problem \u2013 neither smooth nor convex \u2013 with a sequence of convex problems.\nEmpirically, see below, we find that the Taylor optimum is a tough target on a range of datasets and settings: MNIST and CIFAR10; supervised and unsupervised learning; convolutional and fully-connected architectures; under a variety of optimizers (Adam, SGD, RMSProp), and for individual neurons as well as entire layers.\nFinally, see section 3, the conceptual factorization into vanilla optimization and exploration components suggests to investigate the exploratory behavior of different optimizers \u2013 with the theorem providing concrete tools to do so."}, {"heading": "2.4. Empirical Analysis of Online Neural Optimization", "text": "This section empirically investigates the Taylor optimum and regret terms in theorem 2 on two tasks:\nAutoencoder trained on MNIST. Dense layers with architecture 784 \u2192 50 \u2192 30 \u2192 20 \u2192 30 \u2192 50 \u2192 784 and ReLU non-linearities. Trained with MSE loss using minibatches of 64.\nConvnet trained on CIFAR-10. Three convolutional layers with stack size 64 and 5\u00d75 receptive fields, ReLU nonlinearities and 2 \u00d7 2 max-pooling. Followed by a 192 unit fully-connected layer with ReLU before a ten-dimensional fully-connected output layer. Trained with cross-entropy loss using minibatches of 128.\nFor both tasks we compare the optimization performance of\nAdam, RMSProp and SGD (figure 6). Learning rates were tuned for optimal performance. Additional parameters for Adam and RMSProp were left at default. For the convnet all three methods perform equally well: achieving a small loss and an accuracy of \u2265 99% on the training set. However, SGD exhibits slightly more variance. For the autoencoder, although it is an extremely simple model, SGD with the best (fixed) learning rate performs significantly worse than the adaptive optimizers.\nThe neuronal and layer-wise regret are evaluated for each model. At every iteration we record the training error \u2013 the left-hand-side of eq. (4). To evaluate the Taylor loss, we record the input to the neuron/layer, it\u2019s weights, the output of the network and the gradient tensor Gl. After training, we minimize the Taylor loss with respect to V to find the Taylor optimum at each round. The regret is the difference between the observed training loss and the optimal Taylor loss.\nThe figures show cumulative losses and regret. For illustrative purposes we normalize by 1/ \u221a N : quantities growing\nat \u221a N therefore flatten out. Figure 3(a) compares the average regret incurred by neurons in each convolutional layer of the convnet. Shaded regions show one standard deviation. Dashed lines are the regret of individual neurons \u2013 importantly the regret behaviour of neurons holds both on average and individually. Figs 3(b) and 3(c) show the regret, cumulative loss incurred by the network, the average loss incurred and the Taylor optimal loss for neurons in layers 1 and 2 respectively.\nFig. 4 compares Adam, RMSProp and SGD. Figure 4(a) shows the layer-wise regret on the convnet. The regret of all of the optimizers scales as \u221a N for both models, matching the bound in Theorem 2. The additional variance exhibited by SGD explains the difference in regret magnitude. Similar behaviour was observed in the other layers of the\nnetworks and also for convnets trained on MNIST.\nFigure 4(b) shows the same plot for the autoencoder. The regret of all methods scales as \u221a N (this also holds for the other layers in the network). The gap in performance can be further explained by examining the difference between the observed loss and Taylor optimal loss. Figure 4(c) compares these quantities for each method on the autoencoder. The adaptive optimizers incur lower losses than SGD. Further, the gap between the actually incurred and optimal loss is smaller for adaptive optimizers. This is possibly because adaptive optimizers find better activation configurations of the network, see discussion in section 3.\nRemarkably, figures 3 and 4 confirm that regret scales as\u221a N for a variety of optimizers, datasets, models, neurons and layers \u2013 verifying the multi-scale guarantee of Theorem 2. A possible explanation for why optimizers match the worst-case (1/ \u221a N ) regret is that the adversary (that is, the landscape) keeps revealing Taylor losses with better solutions. The optimizer struggles to keep up with the hindsight optimum on the constantly changing Taylor losses."}, {"heading": "3. Optimization and Exploration in Rectifier Neural Networks", "text": "Poor optima in rectifier nets are related to shattered gradients: backprop cannot estimate gradients in nearby smooth regions without directly computing them; the flashlight does not shine across kinks. Two recent papers have shown that noise improves the local optima found during training: Neelakantan et al. (2016) introduce noise into gradients whereas Gulcehre et al. (2016) use noisy activations to extract gradient information from across kinks. Intuitively, noise is a mechanism to \u201cpeer around kinks\u201d in shattered landscapes.\nIntroducing noise is not the only way to find better optima.\nNot only do adaptive optimizers often converge faster than vanilla gradient descent, they often also converge to better local minima.\nThis section investigates how adaptive optimizers explore shattered landscapes. Section 3.1 shows that smooth regions in rectifier nets correspond to configurations of active neurons and that neural Taylor approximations clamp the activation configuration \u2013 i.e. the convex flashlight does not shine across kinks in the landscape. Section 3.2 observes that adaptive optimizers incorporate an exploration bias and hypothesizes that the success of adaptive optimizers derives from exploring the set of smooth regions more extensively than SGD. Section 3.3 evaluates the hypothesis empirically."}, {"heading": "3.1. The Role of Activation Configurations in Optimization", "text": "We describe how configurations of active neurons relate to smooth regions of rectifier networks and to neural Taylor approximations. Recall that the loss of a neural net on its training data is\n\u02c6\u0300(W) = 1\nD\nD\u2211\nd=1\n` ( fW(x d), yd ) .\nDefinition 2. Enumerate the data as [D] = {1, . . . , D} and neurons as [M ]. The activation configuration A(W) is a (D\u00d7M) binary matrix representing the active neurons for each input. The set of all possible activation configurations corresponds to the set of all (D\u00d7M) binary matrices. Observation 1 (activation configurations correspond to smooth regions in rectifier networks). A parameter value exhibits a kink in \u02c6\u0300 iff an infinitesimal change alters the of activation configuration, i.e. \u02c6\u0300 is not smooth at W iff there is a V s.t. A(W) 6= A(W + \u03b4V) for all \u03b4 > 0.\nThe neural Taylor approximation to a rectifier net admits a natural description in terms activation configurations.\nObservation 2 (the Taylor approximation clamps activation configurations in rectifier networks). Suppose datapoint d is sampled on round n. Let 1k := A(Wn)[d,layer k] be the pk-vector given by entries of row d of A(Wn) corresponding to neurons in layer k of a rectifier network. The Taylor approximation Tnl is\nTnl (Vl) = 1l \u00b7\n \u220f\nk 6=l Wnk \u00b7 diag(1k)\n \n\ufe38 \ufe37\ufe37 \ufe38 clamped\n\u00b7 (Vl \u2212Wnl )\ufe38 \ufe37\ufe37 \ufe38 free\nwhich clamps the activation configuration, and weights of all layers excluding l.\nObservations 1 and 2 connect shattered gradients in rectifier nets to activation configurations and the Taylor loss. The main implication is to conceptually factorize neural optimization into hard (finding \u201cgood\u201d smooth regions) and easy (optimizing within a smooth region) subproblems that correspond, roughly, to finding \u201cgood\u201d Taylor losses and optimizing them respectively."}, {"heading": "3.2. RMS-Normalization encourages Exploration", "text": "Adaptive optimizers based on root-mean-square normalization exhibit an up-to-exponential improvement over nonadaptive methods when gradients are sparse (Duchi et al., 2013) or low-rank (Krummenacher et al., 2016) in convex settings. We propose an alternate explanation for the performance of adaptive optimizers in nonconvex nonsmooth settings. Let \u2207` := 1D \u2211D d=1\u2207 `d denote the average gradient over a dataset. RProp replaces the average gradient with its coordinatewise sign (Riedmiller & Braun, 1993). Hinton and Tieleman observed that normalizing by the root-\nmean-square recovers sign(\u2207`) = \u2211D d=1\u2207 `d\u221a\u2211D d=1(\u2207 `d)2 , where (\u2207 `d)2 is the square taken coordinatewise. An alternate characterization of the signed-gradient is\nObservation 3 (signed-gradient is a maximizer). Suppose none of the coordinates in \u2207` are zero. The signed-gradient satisfies\nsign(\u2207`) = argmax x\u2208Bp\u221e\n{ \u2016x\u20161 : \u2329 x,\u2207` \u232a \u2265 0 } ,\nwhere Bp\u221e = {x \u2208 Rp : max i=1,...,p |xi| \u2264 1}.\nThe signed-gradient therefore has two key properties. Firstly, small weight updates using the signed-gradient decrease the loss since \u3008\u2207`, sign(\u2207`)\u3009 > 0. Secondly, the signed-gradient is the update that, subject to an `\u221e constraint, has the largest impact on the most coordinates. Viewing the signed-gradient as changing weights \u2013 or exploring \u2013 maximally suggests the following hypothesis:\nHypothesis 1 (RMS-normalization encourages exploration over activation configurations). Gradient descent with RMS-normalized updates (or running average of RMS) performs a broader search through the space of activation configurations than vanilla gradient descent."}, {"heading": "3.3. Empirical Analysis of Exploration by Adaptive Optimizers", "text": "Motivated by hypothesis 1, we investigate how RMSProp and SGD explore the space of activation configurations on the tasks from section 2.4; see A6 for details.\nFor a fixed parameters W, the activation configuration of a neural net with M neurons and D datapoints is represented as a (D \u00d7M) binary matrix, recall definition 2. The set of activation configurations encountered by a network over N rounds of training is represented by an (N,D,M) binary tensor denoted A where An := A[n, :, :] := A(Wn). Figure 5 quantifies exploration in the space of activation configurations in three ways:\n5(a): Hamming distance plots mink<n \u2016An \u2212Ak\u20162F , the minimum Hamming distance between the current activation configuration and all previous configurations. It indicates the novelty of the current activation configuration. 5(b): Activation-state switches plots 1tot \u2211N\u22121 n=1\n\u2225\u2225An[d, : ]\u2212An\u22121[d, :] \u2225\u22252 F\n, the total number of times each data point (sorted) switches its activation state across all neurons and epochs as a proportion of possible switches. It indicates the variability of the network response.\n5(c): Log-product of singular values. The matrix A[ :, : ,m] specifies the rounds and datapoints that activate neuron\nm. The right column plots the log-product of A[ :, :,m]\u2019s first 50 singular values for each neuron (sorted).3 It indicates the (log-)volume of configuration space covered by each neuron. Note that values reaching the bottom of the plot indicate singular values near 0.\nWe observe the following.\nRMSProp explores the space of activation configurations far more than SGD. The result holds on both tasks, across all three measures, and for multiple learning rates for SGD (including the optimally tuned rate). The finding provides evidence for hypothesis 1.\nRMSProp converges to a significantly better local optimum on the autoencoder, see Fig. 6. We observe no difference on CIFAR-10. We hypothesize that RMSProp finds a better optimum through more extensive exploration through the space of activation configurations. CIFAR is an easier problem and possibly requires less exploration.\nAdam performs targeted exploration. Adam achieves the best performance on the autoencoder. Surprisingly, it explores substantially less than RMSProp according to the Hamming distance and activation-switches, although still more than SGD. The singular values provide a higherresolution analysis: the \u00b140 most exploratory neurons match the behavior of RMSProp, with a sharp dropoff from neuron 60 onwards. A possible explanation is that momentum encourages targeted exploration by rapidly discarding avenues that are not promising."}, {"heading": "4. Discussion", "text": "Rectifier convnets are the dominant technology in computer vision and a host of related applications. Our main contribution is the first convergence result applicable to convnets as they are used in practice, including rectifier nets, max-pooling, dropout and related methods. The key analytical tool is the neural Taylor approximation, the firstorder approximation to the output of a neural net. The Taylor loss \u2013 the loss on the neural Taylor approximation \u2013 is a convex approximation to the loss of the network. Remarkably, the convergence rate matches known lower bounds on convex nonsmooth functions (Bubeck, 2015). Experiments in section 2.4 show the regret matches the theoretical analysis under a wide range of settings.\nThe bound in theorem 2 contains an easy term to optimize (the regret) and a hard term (finding \u201cgood\u201d Taylor losses). Section 3.1 observes that the Taylor losses speak directly to the fundamental difficulty of optimizing nonsmooth func-\n3The time-average is subtracted from each column of A[ :, : ,m]. If the response of neuron m to datapoint d is constant over all rounds, then column A[ :, d,m] maps to (0, . . . , 0) and does not contribute to the volume.\ntions: that gradients are shattered \u2013 the gradient at a point is not a reliable estimate of nearby gradients.\nSmooth regions of rectifier nets correspond to activation configurations. Gradients in one smooth region cannot be used to estimate gradients in another. Exploring the set of activation configurations may therefore be crucial for optimizers to find better local minima in shattered landscapes. Empirical results in section 3.3 suggest that the improved performance of RMSProp over SGD can be explained in part by a carefully tuned exploration bias.\nFinally, the paper raises several questions:\n1. To what extent is exploration necessary for good performance?\n2. Can exploration/exploitation tradeoffs in nonsmooth neural nets be quantified?\n3. There are exponentially more kinks in early layers (near the input) compared to later layers. Should optimizers explore more aggressively in early layers?\n4. Can exploring activation configurations help design better optimizers?\nThe Taylor decomposition provides a useful tool for sep-\narating the convex and nonconvex aspects of neural optimization, and may also prove useful when tackling exploration in neural nets."}, {"heading": "A1. Background on convex optimization", "text": "A continuous function f is smooth if there exists a \u03b2 > 0 such that \u2016\u2207 f(x)\u2212\u2207 f(y)\u20162 \u2264 \u03b2 \u00b7\u2016x\u2212y\u20162 for all x and y in the domain. Rectifiers are not smooth for any value of \u03b2.\nNonsmooth convex functions. Let X \u2282 Rp be a convex set contained in a ball of radius R. Let ` : X \u2192 R be a convex function. Section 3.1 of Bubeck (2015) shows that projected gradient descent has convergence guarantee\n`\n( 1\nN\nN\u2211\nn=1\nwn ) \u2212 `(w\u2217) \u2264 O ( 1\u221a N )\nwhere wn are generated by gradient descent and w\u2217 := argminw\u2208X `(w) is the minimizer of `. It is also shown, section 3.5, that\nmin 1\u2264n\u2264N\n`(wn)\u2212 `(w\u2217) \u2265 \u2126 (\n1\u221a N\n)\nwhere the weights are in the span of the previously observed gradients: wn \u2208 span{\u2207 `(w1), . . . ,\u2207 `(wn\u22121)} for all n \u2208 {1, . . . , N}. The gradient of a convex function increases monotonically. That is \u2329\n\u2207 `(w)\u2212\u2207 `(v),w \u2212 v \u232a \u2265 0\nfor all points w,v where the gradient exists. Gradients at one point of a nonsmooth convex function therefore do contain information about other points, although not as much information as in the smooth case. In contrast, the gradients of nonsmooth nonconvex functions can vary wildly as shown in Fig. 1.\nSmooth convex functions. In the smooth setting, gradient descent converges at rate 1N . The lower bound for convergence is even better, 1N2 . The lower bound is achieved by Nesterov\u2019s accelerated gradient descent method."}, {"heading": "A2. Proof of Theorem 2", "text": "Proof. We prove the network case; the others are similar. The Taylor loss has three key properties by construction:\nT1. The Taylor loss T n coincides with the loss at Wn: `(fWn(x n 0 ), y\nn) = T n(V)|V=Wn T2. The Taylor loss gradient T n coincides with the\nloss gradient at Wn:\n\u2207W `(fWn(xn0 ), yn) = \u2207V T n(V)|V=Wn T3. The Taylor losses are convex functions of V be-\ncause `(f , y) is convex in its first argument and convexity is invariant under affine maps. If ` is a convex function, then so is g(x) = `(Ax + b), where A \u2208 Rm\u00d7n and b \u2208 Rm.\nBy T1, the training loss, i.e. the left-hand side of (3), exactly coincides with the Taylor losses. By T2, the gradients of the Taylor losses exactly coincide with the errors computed by backpropagation on the training losses. That is, the training loss over n rounds is indistinguishable from the Taylor losses to the first order:\nlosses: 1\nN\nN\u2211\nn=1\n` (fWn(x n 0 ), y n)\n= 1\nN\nN\u2211\nn=1\nT n (Wn)\ngradients: \u2207W ( 1\nN\nN\u2211\nn=1\n` (fWn(x n 0 ), y n)\n)\n= \u2207W ( 1\nN\nN\u2211\nn=1\nT n (Wn) )\nWe can therefore substitute the Taylor losses in place of the training loss (fW(x0), y) without altering either the losses incurred during training or the dynamics of backpropagation (or any first-order method).\nSince the Taylor losses are convex, the bound holds for any no-regret optimizer following Zinkevich (2003)."}, {"heading": "A3. Proof of Observations in Section 3", "text": "Proof of observation 1.\nProof. The loss `(f , y) is a smooth function of the network\u2019s output f by assumption. Kinks in \u02c6\u0300(W) = 1 D \u2211D d=1 ` ( fW(x d), yd )\ncan therefore only arise when a rectifier changes its activation for at least one element of the training data.\nProof of observation 2. Note that the rectifier is \u03c1(a) = max(0, a) with derivative \u03c1\u2032(a) = 1 if a > 0 and \u03c1\u2032(a) = 0 if a < 0.\nProof. Recall that the Taylor approximation to layer l is\nTnl (Vl) := fWn(x n 0 ) + \u2329 Gl,Vl \u2212Wnl \u232a\n= fWn(x n 0 ) + J L l \u00b7 (Vl \u2212Wnl ) \u00b7 xnl\u22121\n= fWn(x n 0 ) +\n( l+1\u220f\nk=L\nJkk\u22121 ) \u00b7 (Vl \u2212Wnl ) \u00b7 xnl\u22121\nThe Jacobian of layer k is the function Jk+1k (ak) = Wk+1\u00b7 diag ( s\u2032(ak) ) which in general varies nonlinearly with ak. The Taylor approximation clamps the Jacobian by setting it as constant.\nFor a layer of rectifiers, s(\u00b7) = \u03c1(\u00b7), the Jacobian Jk+1k = Wk+1 \u00b7 diag ( \u03c1\u2032(ak) ) is constructed by zeroing out the rows of Wk+1 corresponding to inactive neurons in layer k. It follows that the Taylor loss can be written as\nTnl (Vl) =\n( l+1\u220f\nk=L\nWnk \u00b7 diag(1k\u22121) ) \u00b7(Vl\u2212Wnl )\u00b7xnl\u22121\nFinally, observe that\nxnl\u22121 =\n( 1\u220f\nk=l\u22121 diag(1k) \u00b7Wnk\n) \u00b7 xn0\nsince diag(1k) \u00b7Wnk \u00b7xnk\u22121 = diag ( \u03c1\u2032(ak) ) \u00b7Wnkxnk\u22121 = \u03c1(Wnk \u00b7 xnk\u22121).\nProof of observation 3.\nProof. Immediate."}, {"heading": "A4. Comparison of Taylor loss with Taylor approximation to loss", "text": "It is instructive to compare the Taylor loss in definition 1 with the Taylor approximation to the loss. The Taylor loss is\n` ( Tn(V), yn ) = ` ( fWn(x n 0 ) + L\u2211\nl=1\n\u2329 Gl,Vl \u2212Wnl \u232a , yn\n)\nIn contrast, the Taylor approximation to the loss is\nTn` (V) = ` ( fWn(x n 0 ), y n )\n\ufe38 \ufe37\ufe37 \ufe38 loss incurred on round n\n+ L\u2211\nl=1\n\u2329 JEL \u00b7Gl\ufe38 \ufe37\ufe37 \ufe38\n\u03b4l\n,Vl \u2212Wnl \u232a\n= `n +\nL\u2211\nl=1\n\u2329 \u2207Wl `,Vl \u2212Wnl \u232a .\nThe constant term is the loss incurred on round n; the linear coefficients are the backpropagated errors.\nIt is easy to see that the two expressions have the same gradient. Why not work directly with the Taylor approximation to the loss? The problem is that the Taylor approximation to the loss is affine, and so decreases without bound. Upgrading to a second order Taylor approximation is no help since it is not convex."}, {"heading": "A5. Details on experiments on regret", "text": "See section 2.4 for the architecture of the autoencoder and convnet used. The hyperparameters used for different optimizers are as follows: the autoencoder uses learning rate \u03b7 = 0.001 for RMSprop and \u03b7 = 0.01 for Adam, while the convnet uses learning rate \u03b7 = 0.0005 for RMSprop and\n\u03b7 = 0.0002 for Adam. All other hyperparameters are kept at their literature-standard values.\nFig. 6 shows the training losses obtained by the convnet on CIFAR-10 and the autoencoder on MNIST.\nThe gradient tensor Gl is not computed explicitly by TensorFlow. Instead, it is necessary to compute the gradient of each component of the output layer (e.g. 10 in total for a network trained on CIFAR-10, 784 for an autoencoder trained on MNIST) with respect to Wl and then assemble the gradients into a tensor. When the loss is the squared error, the Taylor optimal at round n can be computed in closed form. Otherwise we use SGD."}, {"heading": "A6. Details on experiments on exploration", "text": "Given matrix or vector A or a, the squared Frobenius norm is\n\u2016A\u20162F = M,N\u2211\nm,n=1\nA2m,n and \u2016a\u20162F = N\u2211\nn=1\na2n.\nThe Hamming distance between two binary vectors a and b can be computed as \u2016a\u2212 b\u20162F . For tractability in the convnet, we only record activations for 1% of the CIFAR dataset, and at most 10000 units of each convolutional layer. We record the full network state on all inputs for the autoencoder. The singular value plots in figure 5 are calculated only on the first 50 epochs."}], "references": [{"title": "Convex Deep Learning via Normalized Kernels", "author": ["O Aslan", "X Zhang", "Schuurmans", "Dale"], "venue": "In NIPS,", "citeRegEx": "Aslan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Aslan et al\\.", "year": 2014}, {"title": "Breaking the Curse of Dimensionality with Convex Neural Networks", "author": ["Bach", "Francis"], "venue": "In arXiv:1412.8690,", "citeRegEx": "Bach and Francis.,? \\Q2014\\E", "shortCiteRegEx": "Bach and Francis.", "year": 2014}, {"title": "Deep Online Convex Optimization with Gated Games", "author": ["Balduzzi", "David"], "venue": "In arXiv:1604.01952,", "citeRegEx": "Balduzzi and David.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and David.", "year": 2016}, {"title": "Convex Neural Networks", "author": ["Bengio", "Yoshua", "Roux", "Nicolas Le", "Vincent", "Pascal", "Delalleau", "Olivier", "Marcotte", "Patrice"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Convex Optimization: Algorithms and Complexity", "author": ["Bubeck", "S\u00e9bastien"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and S\u00e9bastien.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck and S\u00e9bastien.", "year": 2015}, {"title": "The loss surface of multilayer networks", "author": ["A Choromanska", "M Henaff", "M Mathieu", "G B Arous", "Y. LeCun"], "venue": "In Journal of Machine Learning Research: Workshop and Conference Proceeedings,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Dauphin", "Yann", "Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Ganguli", "Surya", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Dauphin", "Yann", "de Vries", "Harm", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Estimation, optimization, and parallelism when data is sparse", "author": ["Duchi", "John", "Jordan", "Michael I", "McMahan", "Brendan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Noisy Activation Functions", "author": ["Gulcehre", "Caglar", "Moczulski", "Marcin", "Denil", "Misha", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Lecture 6a: Overview of minibatch gradient descent", "author": ["G Hinton", "Srivastava", "Nitish", "Swersky", "Kevin"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "Hinton", "G E"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Scalable adaptive stochastic optimization using random projections", "author": ["Krummenacher", "Gabriel", "McWilliams", "Brian", "Kilcher", "Yannic", "Buhmann", "Joachim M", "Meinshausen", "Nicolai"], "venue": null, "citeRegEx": "Krummenacher et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krummenacher et al\\.", "year": 2016}, {"title": "Estimating the Hessian by Backpropagating Curvature", "author": ["Martens", "James", "Sutskever", "Ilya", "Swersky", "Kevin"], "venue": "In ICML,", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition", "author": ["G Montavon", "S Bach", "A Binder", "W Samek", "K. M\u00fcller"], "venue": null, "citeRegEx": "Montavon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Montavon et al\\.", "year": 2015}, {"title": "Adding Gradient Noise Improves Learning for Very Deep Networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Kurach", "Karol", "Martens", "James"], "venue": "In ICLR,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["Riedmiller", "Martin", "H. Braun"], "venue": "In IEEE Int Conf on Neural Networks, pp", "citeRegEx": "Riedmiller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 1993}, {"title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "author": ["Shang", "Wenling", "Sohn", "Kihyuk", "Almeida", "Diogo", "Lee", "Honglak"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1958}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going Deeper With Convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["Telgarsky", "Matus"], "venue": "In COLT,", "citeRegEx": "Telgarsky and Matus.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2016}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["Zeiler", "Matthew", "Fergus", "Rob"], "venue": "In ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 25, "context": "Shattering is problematic for accelerated and Hessian-based methods which speed up convergence by exploiting the relationship between gradients at nearby points (Sutskever et al., 2013).", "startOffset": 161, "endOffset": 185}, {"referenceID": 20, "context": "Further, the number of kinks grows exponentially with network depth (Pascanu et al., 2014; Telgarsky, 2016).", "startOffset": 68, "endOffset": 107}, {"referenceID": 8, "context": "AdaGrad, RMSProp, Adam and RadaGrad (Duchi et al., 2011; Hinton et al., 2012; Kingma & Ba, 2015; Krummenacher et al., 2016).", "startOffset": 36, "endOffset": 123}, {"referenceID": 13, "context": "AdaGrad, RMSProp, Adam and RadaGrad (Duchi et al., 2011; Hinton et al., 2012; Kingma & Ba, 2015; Krummenacher et al., 2016).", "startOffset": 36, "endOffset": 123}, {"referenceID": 16, "context": "AdaGrad, RMSProp, Adam and RadaGrad (Duchi et al., 2011; Hinton et al., 2012; Kingma & Ba, 2015; Krummenacher et al., 2016).", "startOffset": 36, "endOffset": 123}, {"referenceID": 6, "context": "(2015) argue that RMSProp is successful because it approximates the equilibriation matrix \u221a diag(H2) which approximates the absolute Hessian |H| (Dauphin et al., 2014).", "startOffset": 145, "endOffset": 167}, {"referenceID": 6, "context": "Dauphin et al. (2015) argue that RMSProp is successful because it approximates the equilibriation matrix \u221a diag(H2) which approximates the absolute Hessian |H| (Dauphin et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Bengio et al. (2006) show that choosing the number of hidden units converts neural optimization into a convex problem, see also Bach (2014).", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bengio et al. (2006) show that choosing the number of hidden units converts neural optimization into a convex problem, see also Bach (2014). A convex multi-layer architectures are developed in Aslan et al.", "startOffset": 0, "endOffset": 140}, {"referenceID": 0, "context": "A convex multi-layer architectures are developed in Aslan et al. (2014); Zhang et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 0, "context": "A convex multi-layer architectures are developed in Aslan et al. (2014); Zhang et al. (2016). However, these approaches have not achieved the practical success of convnets.", "startOffset": 52, "endOffset": 93}, {"referenceID": 0, "context": "A convex multi-layer architectures are developed in Aslan et al. (2014); Zhang et al. (2016). However, these approaches have not achieved the practical success of convnets. In this work, we analyze convnets as they are rather than proposing a more tractable, but potentially less useful, model. A Taylor decomposition for neural networks was proposed in Montavon et al. (2015). They treat inputs as variable instead of weights and study interpretability instead of convergence.", "startOffset": 52, "endOffset": 377}, {"referenceID": 0, "context": "A convex multi-layer architectures are developed in Aslan et al. (2014); Zhang et al. (2016). However, these approaches have not achieved the practical success of convnets. In this work, we analyze convnets as they are rather than proposing a more tractable, but potentially less useful, model. A Taylor decomposition for neural networks was proposed in Montavon et al. (2015). They treat inputs as variable instead of weights and study interpretability instead of convergence. Our results are closely related to Balduzzi (2016), which uses game-theoretic techniques to prove convergence in rectifier nets.", "startOffset": 52, "endOffset": 529}, {"referenceID": 28, "context": "The results also apply to convolutions, max-pooling, dropout, dropconnect, maxout, PReLUs and CReLUs (Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013; He et al., 2015; Shang et al., 2016).", "startOffset": 101, "endOffset": 206}, {"referenceID": 11, "context": "The results also apply to convolutions, max-pooling, dropout, dropconnect, maxout, PReLUs and CReLUs (Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013; He et al., 2015; Shang et al., 2016).", "startOffset": 101, "endOffset": 206}, {"referenceID": 22, "context": "The results also apply to convolutions, max-pooling, dropout, dropconnect, maxout, PReLUs and CReLUs (Srivastava et al., 2014; Wan et al., 2013; Goodfellow et al., 2013; He et al., 2015; Shang et al., 2016).", "startOffset": 101, "endOffset": 206}, {"referenceID": 5, "context": "A partial explanation for the prevalence of \u201cgood enough\u201d local optima is Choromanska et al. (2015). Nevertheless, it is important to acknowledge that neural nets can and do converge to bad local optima.", "startOffset": 74, "endOffset": 100}, {"referenceID": 20, "context": "The landscape of a rectifier net decomposes into smooth regions separated by kinks (Pascanu et al., 2014; Telgarsky, 2016), see figure 2.", "startOffset": 83, "endOffset": 122}, {"referenceID": 8, "context": "The regret of gradient descent, AdaGrad (Duchi et al., 2011), mirror descent and a variety of related algorithms satisfy (1), albeit with different constant terms that are hidden in the big-O notation.", "startOffset": 40, "endOffset": 60}, {"referenceID": 17, "context": "We recall backprop using notation from Martens et al. (2012). Let Jb denote the Jacobian matrix of the vector a with respect to the vector b.", "startOffset": 39, "endOffset": 61}, {"referenceID": 8, "context": "The running average of errors during training (or cumulative loss) is the same quantity that arises in the analyses of Adam and Adagrad (Kingma & Ba, 2015; Duchi et al., 2011).", "startOffset": 136, "endOffset": 175}, {"referenceID": 12, "context": "It is a simple matter to construct a deep fully connected network (> 100 layers) that fails to learn because gradients do not propagate through the network (He et al., 2016).", "startOffset": 156, "endOffset": 173}, {"referenceID": 18, "context": "Two recent papers have shown that noise improves the local optima found during training: Neelakantan et al. (2016) introduce noise into gradients whereas Gulcehre et al.", "startOffset": 89, "endOffset": 115}, {"referenceID": 10, "context": "(2016) introduce noise into gradients whereas Gulcehre et al. (2016) use noisy activations to extract gradient information from across kinks.", "startOffset": 46, "endOffset": 69}, {"referenceID": 9, "context": "Adaptive optimizers based on root-mean-square normalization exhibit an up-to-exponential improvement over nonadaptive methods when gradients are sparse (Duchi et al., 2013) or low-rank (Krummenacher et al.", "startOffset": 152, "endOffset": 172}, {"referenceID": 16, "context": ", 2013) or low-rank (Krummenacher et al., 2016) in convex settings.", "startOffset": 20, "endOffset": 47}], "year": 2017, "abstractText": "Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation \u2013 a straightforward application of Taylor expansions to neural networks \u2013 and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.", "creator": "LaTeX with hyperref package"}}}