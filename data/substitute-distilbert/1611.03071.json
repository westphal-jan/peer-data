{"id": "1611.03071", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Fairness in Reinforcement Learning", "abstract": "we initiate the study of intrinsic learning in markovian settings, where the actions of a learning algorithm may include its environment and future rewards. working in the model formulation reinforcement learning, we define a fairness constraint requiring that learn algorithm never prefers conditional action over punishment if the long - term ( discounted ) reward of choosing the latter action is higher.", "histories": [["v1", "Wed, 9 Nov 2016 20:19:45 GMT  (415kb,D)", "http://arxiv.org/abs/1611.03071v1", null], ["v2", "Thu, 17 Nov 2016 17:46:00 GMT  (411kb,D)", "http://arxiv.org/abs/1611.03071v2", null], ["v3", "Wed, 1 Mar 2017 16:35:53 GMT  (409kb,D)", "http://arxiv.org/abs/1611.03071v3", null], ["v4", "Sun, 6 Aug 2017 00:12:49 GMT  (426kb,D)", "http://arxiv.org/abs/1611.03071v4", "The short version of this paper appears in the proceedings of ICML-17"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shahin jabbari", "matthew joseph", "michael kearns", "jamie morgenstern", "aaron roth"], "accepted": true, "id": "1611.03071"}, "pdf": {"name": "1611.03071.pdf", "metadata": {"source": "CRF", "title": "Fair Learning in Markovian Environments", "authors": ["Shahin Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "emails": ["aaroth}@cis.upenn.edu"], "sections": [{"heading": null, "text": "Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take exponentially many rounds in the number of states to achieve non-trivial approximation to the optimal policy. Our main result is a polynomial time algorithm that is provably fair under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness."}, {"heading": "1 Introduction", "text": "The growing use of machine learning for automated decision-making has raised concerns about the potential for unfairness and discrimination in learning algorithms and models. In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].\nThis paper initiates the study of fair learning algorithms in Markovian environments, where an algorithm\u2019s choices may influence the state of the world and future rewards. Previous work on fairness in machine learning has focused on myopic settings where such influence is absent, as in i.i.d. or no-regret models (see e.g. [6, 7, 12, 14]). Myopic notions of fairness may be undesirable in the contexts outlined above, where policies favoring certain actions will influence future state and rewards. For example, increased policing of a particular neighborhood may lead to more arrests without an actual increase in crime, and in turn may incubate increased tension and frayed relationships with police. As another example, an oft-cited rationale for affirmative action policies is that in the long-term they improve not just the prospects but the qualifications and abilities of individuals in historically disadvantaged groups. Such examples explicitly acknowledge that policies do not operate in a vacuum, but must account for their influence on the evolution of state.\nWe conduct our study in the standard model of reinforcement learning, in which an algorithm seeks to maximize its discounted sum of rewards in a Markovian decision process (MDP). A state s \u2208 S encodes the current environment. From state s, a chosen action a \u2208 A will lead to an observed transition to a new state s\u2032 with probability p(s, a, s\u2032) and an observed reward r(s\u2032). We now define fairness of an algorithm with respect to a measure Q\u2217 of the long-run quality of an action a from a state s.\nDefinition 1. [Fairness, Informal] A reinforcement learning algorithm L is fair if for any input \u03b4 > 0, the following condition holds with probability at least 1 \u2212 \u03b4 (over the randomization of the\nar X\niv :1\n61 1.\n03 07\n1v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\n6\nMDP and the algorithm): for all rounds t, all states s \u2208 S and all actions a, a\u2032 \u2208 A\nQ\u2217(s, a) \u2265 Q\u2217(s, a\u2032)\u21d2 L(s, a, ht\u22121) \u2265 L(s, a\u2032, ht\u22121)\nwhere Q\u2217(s, a) denotes the (expected) infinite discounted reward when taking action a from state s and following the optimal discounted policy thereafter, and L(s, a, ht\u22121) is the probability that L chooses action a from state s given the observed history ht\u22121.\nIn other words, we ask that if from some state s, action a is at least as good action a\u2032 (with respect to the long-term discounted reward achievable following these actions), an algorithm must choose a with at least the probability it chooses a\u2032. This definition of fairness is adapted from Joseph et al. [14], who study fairness in the contextual bandit framework. Their definition and ours differ from previous work by requiring that an algorithm be fair throughout the learning process itself rather than merely outputting a fair policy or model after a period of (possibly unfair) learning. Joseph et al. [14] define fairness with respect to one-step rewards, which is appropriate for the contextual bandit setting where the learner\u2019s actions do not affect future state. In our Markovian setting, we define fairness with respect to the expected long-term discounted reward of an action.\nUnfortunately, our first result shows an exponential separation in expected performance between the best unfair algorithm and any algorithm satisfying Definition 1. This motivates our study of a natural relaxation of (exact) fairness, for which we provide a polynomial time learning algorithm, thus establishing an exponential separation between exact and approximately fair learning in MDPs.\nThroughout, the reader should interpret the actions available to a learning algorithm in an MDP as corresponding to choices affecting individuals \u2014 such as the choice of which applicants to admit to college, or which applicants to grant loans to. The rewards associated with each action should be viewed as the short-term payoff of making the corresponding decision (the 4-year graduation rate of the admitted class, or the default rate on the loans given). The actions taken by the algorithm affect the underlying state of the system (the high school graduation rate of students in different populations, or home ownership rate of different populations) \u2014 which in turn can affect the actions and payoffs available to the algorithm in the future. Direct use of the fairness definition from Joseph et al. [14] in terms of myopic rewards would be in conflict with the optimal policy in an underlying MDP if it was necessary to make sub-optimal decisions in the short run (e.g. admit students with less high school preparation) in order to obtain high rewards in the long-term (e.g. by improving the overall applicant pool). Instead, we propose using an action\u2019s potential long-term reward as its measured quality, and so (in the college admissions example) would permit \u201caffirmative action\u201d in the short run so long as it actually improves long-term reward."}, {"heading": "1.1 Our Contributions", "text": "The contributions of this paper can be divided into three parts. First, in Section 4, we propose several definitions of fairness for learning in MDPs. The first (which formalizes Definition 1) requires that with high probability an algorithm never favors a worse action over a better one, where the quality of an action is measured in terms of its potential long-term discounted reward. We then relax this definition in two ways. The first relaxation, which we call approximate-choice fairness, requires that an algorithm never chooses a worse action with probability substantially higher than better actions. The second relaxation of fairness, approximate-action fairness, requires that an algorithm never favor an action of substantially lower quality than that of a better action.\nSecond, in Section 5, we analyze the consequences of these definitions, providing a lower bound on the time required for a fair learning algorithm to achieve near-optimal performance.\nTheorem (Informal statement of Theorems 4, 5, and 6). All fair and approximate-choice fair learning algorithms must take a number of rounds exponential in n before achieving -optimal performance for constant . All fair and approximate-action fair learning algorithms require a number of rounds exponential in 1/ \u221a 1\u2212 \u03b3, where \u03b3 is the discount factor.\nThird, we present an approximate-action fair algorithm (Fair-E3) in Section 6. We prove a polynomial upper bound on the time Fair-E3 requires to achieve near-optimal performance in Section 6.4.\nTheorem (Informal statement of Theorem 7). For any MDP M satisfying standard assumptions, Fair-E3 is an approximate-action fair algorithm achieving -optimal performance for in a number of rounds that is exponential in 1/(1\u2212 \u03b3), and polynomial in all other parameters.\nNote that Theorem 6 shows that it is impossible for any approximate-action fair algorithm to have a running time that has a polynomial dependence on 1/(1\u2212 \u03b3). More generally, in the same vein as the contextual bandit results in Joseph et al. [14], our results establish rigorous trade-offs between fairness and performance in reinforcement learning algorithms."}, {"heading": "1.2 Our Techniques", "text": "We now briefly describe the tools we use to prove the theorems described above. The lower bounds follow from constructing two similar MDPs and arguing that fair and approximate-choice fair algorithms must behave identically for both until receiving some observation distinguishing the two, which will take an exponential number of steps in the size of state space with constant probability.\nWe then study a different relaxation of fairness, called approximate-action fairness, which allows an algorithm to prefer slightly worse actions to slightly better ones. We adapt E3 [19], a wellknown algorithm for MDP learning, to satisfy approximate-action fairness. E3 transitions between exploration and exploitation: if some state has not been sufficiently explored, E3 takes the least explored action in that state. The pigeonhole principle then implies E3 will eventually explore some state often enough to learn accurate estimates of its reward and transition functions, after which E3 treats the state as \u201cknown\u201d. To exploit, E3 treats the set of known states as if they are the only states of the MDP and follows a near-optimal policy if such a policy exists in the MDP restricted to the known states. Otherwise, E3 plans to escape to the set of \u201cunknown\u201d states in hopes of achieving better long-term rewards.\nDeveloping a fair version of E3 requires a number of nontrivial modifications to both the algorithm and its analysis. The first main one has to do with the definition of a known state. The original E3 uses a weak definition that is sufficient for one-step lookahead. Since fairness demands that an algorithm never prefer a much worse action, we require a much stronger definition of known state for Fair-E3 that permits accurate estimation of Q\u2217 values (long-term discounted rewards) rather than just next-state transitions.\nThe second major modification arises from the fact that when attempting to escape to unknown states, E3 may badly violate fairness, since it is ignoring rewards entirely in an attempt to reach rarely visited states. Fair-E3 tries to find an escape policy subject to our fairness constraint. A \u201cwin-win\u201d analysis proves that whenever Fair-E3 cannot execute an \u201cexploit\u201d policy, then there must exist a fair escape policy, and so our algorithm is always either able to exploit or fairly escape when it encounters a known state."}, {"heading": "2 Related Work", "text": "There is an enormous literature on reinforcement learning, and the most relevant parts focus on constructing learning algorithms with provable performance guarantees. E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesva\u0301ri [28] and references within).\nSeparately, there is a growing literature studying the problem of fairness in machine learning (see e.g. [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]). One line of work aims to give algorithms for batch classification which achieve notions of group fairness, and in particular, statistical parity (see e.g. [1, 5, 7, 8, 18, 22]). While statistical parity is sometimes a desirable goal, as observed by Dwork et al. [6], it suffers from two problems. First, if different populations have different statistical properties, then it can be at odds with accuracy. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level \u2013 see Dwork et al. [6] for a catalog of ways in which statistical parity fails as a definition of fairness. In the context of classification and regression, Hardt et al. [12] give a definition aiming to capture the idea of equality of opportunity that overcomes many of these limitations of notions of group fairness. Their definition is a \u201cgroup fairness\u201d definition in that it imposes a constraint that binds on average outcomes, rather than on individuals. In contrast, we study a definition of fairness that binds at the individual level.\nDwork et al. [6] propose and explore the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d. Specifically, their work presupposes the existence and knowledge of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. The work of Joseph et al. [14] and the present work suggest a natural metric for the tasks of regret minimization in a bandit setting and long-term reward maximization in an MDP. The primary conceptual difference between these works and that of Dwork et al. [6] is that the latter operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions."}, {"heading": "3 Preliminaries", "text": "In this paper we study reinforcement learning in Markov Decision Processes (MDPs):\nDefinition 2. A Markov Decision Process (MDP) is a tuple M = (SM ,AM , PM , RM , T, \u03b3) where SM = {s1, . . . , sn} is a set of n states, AM = {a1, . . . , ak} is a set of k actions, T is a horizon of a (possibly infinite) number of rounds of activity in M , and \u03b3 is a discount factor, where\n\u2022 PM : SM \u00d7 AM \u00d7 SM \u2192 [0, 1] is a transition probability distribution mapping (s, a, s\u2032) 7\u2192 p where p is the probability of arriving in state s\u2032 after taking action a from state s.\n\u2022 For any state s, RM (s) denote the reward distribution at state s. We assume the support of the reward distribution is [0, 1] for all s. R\u0304M : SM \u2192 [0, 1] denote the mean of reward distribution at state s. We remark that R\u0304M \u2264 1 and Var(RM ) \u2264 1 for all states s. 1\n1The assumption that the rewards are bounded can be relaxed to the assumption that the support of the reward\nFor the remainder of this paper, any MDP M is defined with the SM ,AM , PM , RM , T, and \u03b3 as given above. A policy will dictate choices of actions from each state. A deterministic policy \u03c0 \u2208 \u03a0: SM \u2192 AM in M maps from states to actions. A non-deterministic policy \u03c0 : SM \u2192 4(AM ) maps from states to probability distributions on actions. We make the following unichain assumption for all M . It is important to note that the unichain assumption does not imply that every policy will eventually visit every state, or even that there exists a single policy that will do so quickly; thus, the exploitation/exploration dilemma remains in unichain MDPs.\nAssumption 1. The stationary distribution of any policy in M is independent of its start state.\nA few ancillary concepts will be useful for reasoning about MDPs, most of which are standard (see, e.g. Kearns and Singh [19]). First, we quantify the value of states and actions based on the rewards, both present and future. In the discounted case, the contribution of the future rewards to the value of a state is scaled down by the discount factor \u03b3.\nDefinition 3. The T -step expected reward of \u03c0 from s, and the limit of this for infinite T , and its optimum over policies are defined as\nV \u03c0M (s, T ) = \u2211 p P[p] T\u2211 i=1 \u03b3i\u22121R\u0304(si) and V \u03c0 M (s) = lim T\u2192\u221e V \u03c0M (s, T ) and V \u2217 M (s, T ) = max \u03c0\u2208\u03a0 V \u03c0M (s, T )\nwhere p = (s, s1, . . . , sT ) is a sequence of T states starting from s, P[p] denotes the probability of taking p following policy \u03c0 and suming over all sequences p. Given action a, the T -step expected reward from s after taking action a and following \u03c0 thereafter and its optimum as Q\u03c0M (s, a, T ) = \u2211 pa P[pa] T\u2211 i=1 \u03b3i\u22121R\u0304(si), Q \u03c0 M (s, a) = lim T\u2192\u221e Q\u03c0M (s, a, T ) and Q \u2217 M (s, a, T ) = max \u03c0\u2208\u03a0 Q\u03c0M (s, a, T )\nfor pa = (s, s1, . . . , sT ) a sequence of T states starting from s, P[pa] the probability of pa after taking action a from s and then following policy \u03c0, and summing over all sequences pa.\nAny policy achieving the V \u2217M (s) values for all states s is an optimal policy: fix one and denote it as \u03c0\u2217 for the remainder of the paper. Finally, note that V \u2217M (s) = V \u03c0\u2217 M (s) and Q \u2217 M (s, a) = Q \u03c0\u2217 M (s, a).\nLet \u00b5\u03c0 denote the stationary distribution of \u03c0. We now define the -mixing time of a policy.\nDefinition 4. Fix > 0. The -mixing time of \u03c0 is the smallest number of steps T \u03c0 such that for any T \u2265 T \u03c0 and any state s, the distribution \u00b5\u0302\u03c0T of \u03c0 on states of M after following \u03c0 for T steps from s satisfies \u03a3ni=1|\u00b5\u03c0(si)\u2212 \u00b5\u0302\u03c0T (si)| \u2264 . Let T \u2217 denote the -mixing time of \u03c0 \u2217. Lemma 1 relates the -mixing time of any \u03c0 to the number of rounds until the values V \u03c0M of the visited states are close to the expected V \u03c0 M (under the stationary distribution \u00b5\u03c0). We defer the proof of Lemma 1 to Section B.1.\nLemma 1. Fix > 0. For any state s, following \u03c0 for T \u2265 T \u03c0 steps from s satisfies\nEs\u223c\u00b5\u03c0 [V \u03c0M (s)]\u2212 E\n[ 1\nT T\u2211 t=1 V \u03c0M (st)\n] \u2264\n1\u2212 \u03b3 ,\nwhere st is the state visited at time t when following \u03c0 from s and the expectation in the second term is over the transition function and the randomization of \u03c0.\ndistribution is over a possibly unbounded region but the mean and the variance of the reward distribution is bounded in all states using standard techniques (see Kearns and Singh [19] for more details). Furthermore, assuming that the support of the reward distributions is between [0, 1] can be made without loss of generality up to scaling.\nOur results can be stated for a weaker notion of mixing time called the -reward mixing time, the smallest number of time steps T such that the average undiscounted rewards of the states visited by the policy in T steps is close to the average undiscounted rewards of the states visited under the stationary distribution of the policy. -reward mixing time is always linearly bounded by the -mixing time but can be much smaller than the -mixing time in certain cases (see Kearns and Singh [19] for a discussion).\nThe horizon time H\u03b3 of an MDP captures the number of steps an approximately optimal policy must optimize over. In the discounted rewards setting, the expected discounted reward of any policy after H\u03b3 = log ( (1\u2212 \u03b3)) / log(\u03b3) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]). If T \u2265 H\u03b3 = log( (1\u2212\u03b3))log(\u03b3) , then for all s \u2208 S, V \u03c0M (s, T )\u2212 \u2264 V \u03c0M (s) \u2264 V \u03c0M (s, T ) + . A learning algorithm L is a non-stationary2 policy that at each round takes the entire history (the sequence of triples (state, action, reward) observed so far) and outputs a distribution over actions. Formally, at any fixed time t, define the history at time t to be the sequence of (state, action, reward) triples ht\u22121 = {(st\u2032 , at\u2032 , rt\u2032)}t\u22121t\u2032=1 observed before round t. Then L : SM \u00d7 ht\u22121 \u2192 \u2206(AM ). A stationary policy as defined above is an algorithm whose mapping is independent of the observed history, i.e. \u03c0(st) = \u03c0(st, ht\u22121).\nWe conclude this section by defining a performance measure for learning algorithms.\nDefinition 5. Let > 0 and \u03b4 \u2208 (0, 1/2). L achieves -optimality in T steps if for any T \u2265 T with probability at least 1\u2212 \u03b4,\nEs\u223c\u00b5\u2217V \u2217M (s)\u2212 1\nT E T\u2211 t=1 V \u2217M (st) \u2264 2 1\u2212 \u03b3 , (1)\nfor st the state L reaches at time t, where the expectation is taken over the transitions and the randomization of L, for any MDP M .\nWe pause to justify this notion of optimality. First, rewards are discounted, so the contribution of the discounted sum of rewards of any algorithm or policy after H\u03b3 steps is less than : it is therefore unreasonable to expect the discounted sum of rewards of any algorithm to compete favorably with the discounted sum of rewards of an optimal policy (especially if the -mixing time of an optimal policy is much more than H\u03b3 ). So, we instead consider the average V \u2217M values of the states visited by our algorithm and compare this value to the average V \u2217M values of the states visited under the stationary distribution of an optimal policy. Lemma 3 further justifies this definition; it shows that the average V \u03c0M values of states visited under the stationary distribution of \u03c0 (and in particular an optimal policy) is closely related to the average undiscounted rewards achieved under the stationary distribution of that policy.\nLemma 3 (Singh [26]). Let R\u0304M be the vector of mean rewards in states of M and V \u03c0 M the vector of discounted rewards in states under \u03c0. Then \u00b5\u03c0 \u00b7 R\u0304M = (1\u2212 \u03b3)\u00b5\u03c0 \u00b7V\u03c0M .\nOur goal will be to find some algorithm for which T , the number of transitions after which our policy has average one-step rewards within of an optimal policy as in Definition 5, is small (ideally polynomial in the parameters of M). In Sections 5 and 6, we will prove lower and upper bounds on T based on various notions of fairness and also compare these bounds with standard reinforcement learning bounds for algorithms which do not satisfy these definitions of fairness.\n2A non-stationary policy allows the action choice from a state to depend on the time of arrival at that state and the previously observed rewards: an algorithm choosing actions based upon its observations will be non-stationary."}, {"heading": "4 Notions of Fairness", "text": "We now turn to quantitative notions of fairness. We begin by considering Joseph et al. [14]\u2019s definition for learning in a contextual bandit setting: namely, Definition 1 when the Q\u2217(s, a) values are replaced with the expected payoff of choosing a given arm a. Translated to our setting, Joseph et al. [14] define the quality of an action to be Q\u2217M (s, a, 1), the expected one-step reward for choosing a from s. This definition would require that, with high probability, an algorithm in state s never play a with higher probability than a\u2032 if a has lower expected one-step reward than a\u2032.\nThis naive translation, however, does not adequately capture the structural differences between the bandit and MDP settings: in the bandit setting, present rewards are independent of past choices, while in MDPs they are not. In particular, defining fairness in terms of one-step rewards would prohibit any policy sacrificing short-term rewards in favor of long-term rewards. This is undesirable, as it is precisely long-term rewards that matter in reinforcement learning, and optimizing for longterm rewards often necessitates short-term sacrifices (see Figure 1 for an illustration).\nIn an MDP, the relevant quality of an action encompasses not only its immediate reward but also the future actions and rewards the action enables. We will therefore define fairness using the long-term discounted rewards, namely Q\u2217M (s, a).\nDefinition 6 (Fairness). L is fair if for any input \u03b4 > 0, for all MDPs M , all rounds t, all states s \u2208 SM and actions a, a\u2032 \u2208 AM\nQ\u2217M (s, a) \u2265 Q\u2217M (s, a\u2032)\u21d2 L(s, a, ht\u22121) \u2265 L(s, a\u2032, ht\u22121)\nwith probability at least 1\u2212 \u03b4 over histories ht\u22121.\nIn Section 5, we show this requirement can prove extremely restrictive (see Theorem 4). Intuitively, L must play uniformly at random until the algorithm has a high degree of confidence about the Q\u2217M values, in some cases taking exponential time to achieve near-optimality. This motivates relaxing our definition. The first relaxation we introduce requires that an algorithm not substantially favor a worse action over a better one.\nDefinition 7 (Approximate-choice Fairness). L is \u03b1-choice fair if for any inputs \u03b4 > 0 and \u03b1 > 0: for all MDPs M , all rounds t, all states s \u2208 SM and actions a and a\u2032 \u2208 AM\nQ\u2217M (s, a) \u2265 Q\u2217M (s, a\u2032)\u21d2 L(s, a, ht\u22121) \u2265 L(s, a\u2032, ht\u22121)\u2212 \u03b1,\nwith probability of at least 1\u2212 \u03b4 over histories ht\u22121. If L is \u03b1-choice fair for any input \u03b1 > 0, we call L approximate-choice fair.\nA slight modification of the lower bound for exact fairness shows that algorithms satisfying approximate-choice fairness can also require exponential time to achieve near-optimality. Therefore, we propose an alternative relaxation.\nDefinition 8 (Approximate-action Fairness). L is \u03b1-action fair if for any input \u03b4 > 0, for all MDPs M , all rounds t, all states s \u2208 SM and actions a and a\u2032 \u2208 AM\nQ\u2217M (s, a) > Q \u2217 M (s, a \u2032) + \u03b1\u21d2 L(s, a, ht\u22121) \u2265 L(s, a\u2032ht\u22121)\nwith probability of at least 1\u2212 \u03b4 over histories ht\u22121. If L is \u03b1-action fair for any input \u03b1 > 0, we call L approximate-action fair.\nNote that \u03b1-action fair algorithms can make arbitrary choices between two actions which are \u03b1close in quality, but cannot place higher probability on a than a\u2032 if a\u2032\u2019s long-term expected reward is at least \u03b1 greater than a\u2019s. As a result, approximate-choice fairness prevents equally good actions from being chosen at very different rates, while approximate-action fairness only prevents very poor actions from being chosen over very good ones. Amongst actions of nearly identical quality, the algorithm is permitted to break ties arbitrarily without violating approximate-action fairness. For example, this constraint is consistent with a college making admissions decisions based on diversity considerations when deciding between two applicants whose relevant qualities appear to be identical. Hence, approximate-action fairness should be viewed as a weaker constraint: rather than safeguarding against every violation of \u201cfairness\u201d, it instead restricts how egregious these violations can be. Approximate-action fairness is a particularly attractive relaxation because it allows us to give algorithms circumventing the exponential hardness we prove for fairness and approximate-choice fairness. Thus, after proving lower bounds on the performance of all of our definitions of fairness in Section 5, approximate-action fairness will form our primary focus.\nWe conclude this section by making several useful observations. We defer all formal statements and their proofs to Appendix A. There always exists an optimal policy which is fair, but it might require randomization (Observation 1); conversely, any optimal policy (deterministic or randomized) is approximate-action fair (Observation 2), as is the uniformly random policy (Observation 3).\nFinally, we consider a restriction of the actions in an MDP M to nearly-optimal actions (as measured by Q\u2217M values). Optimal policies need only select amongst these actions, and fair policies can only select amongst these actions.\nDefinition 9. The \u03b1-restricted MDP of M , denoted M\u03b1, is identical to M except that in each state s, the set of available actions are restricted to {a : Q\u2217M (s, a) \u2265 maxa\u2032\u2208AM Q\u2217M (s, a\u2032)\u2212 \u03b1 | a \u2208 AM}.\nM\u03b1 has the following two properties: (i) any policy in M\u03b1 is \u03b1-action fair in M (Observation 4) and (ii) the optimal policy in M\u03b1 is also optimal in M (Observation 5). Observations 4 and 5 aid our design of an approximate-action fair algorithm: we construct M\u03b1 from estimates of the Q\u2217M values (see Section 6.3 for more details)."}, {"heading": "5 Lower Bounds", "text": "In this section, we demonstrate a stark separation between the performance of algorithms for reinforcement learning with and without fairness. First, we show that neither fair nor approximate-choice fair algorithms achieve near-optimality (see Definition 5) unless the number of time steps T is at least \u2126(kn), exponential in the size of the state space. We then show that any approximate-action fair algorithm requires a number of time steps T that is at least \u2126(k1/ \u221a 1\u2212\u03b3) to achieve nearoptimality. We start by proving a lower bound for fair algorithms.\nTheorem 4. For \u03b4 < 1/4, \u03b3 > 0.5, and < 1/8, no fair algorithm can be -optimal in T = O(kn) steps. 3\n3We have not optimized for the upper bound constants on the parameters in the statement of Theorem 4 as well as Theorems 5 and 6. Hence, these values are only chosen for convenience.\nStandard algorithms like E3 (absent a fairness constraint) learn an -optimal policy in a number of steps polynomial in n and 1/ ; Theorem 4 therefore shows a steep cost of imposing fairness on reinforcement learning algorithms. We sketch the high level idea of the proof of Theorem 4 and defer all the proofs to Section B.2. For intuition, first consider the special case when the number of actions k = 2. We introduce the MDPs witnessing the claim in Theorem 4 for this special case.\nDefinition 10. Let M(x) = (SM ,AM , PM , RM , T, \u03b3, x) be an MDP where AM = {L,R}, \u2022 for all i \u2208 [n], PM (si, L, s1) = PM (si, R, sj) = 1 where j = min{i+ 1, n} and is 0 otherwise. \u2022 for i \u2208 [n\u2212 1], RM (si) = 0.5, and RM (sn) = x.\nFigure 2 illustrates the MDP from Definition 10. Note that all the transitions and rewards in M are deterministic. However, the reward at state sn can be either 1 or 1/2. Any algorithm (fair or otherwise) cannot determine whether the Q\u2217M values of all the states are the same or not until it reaches sn (the final state in the chain). But until then, fairness requires that the algorithm play all the actions uniformly at random. Thus, any fair algorithm will need to take exponential time in the number of states to reach sn. While we specialized this exposition to the case of k = 2, one can easily modify it such that, from each state si, k \u2212 1 of the actions from state si (deterministically) reach state s1 and only one action (deterministically) reaches states smin{i+1,n}. This gives a lower bound of kn time steps before any fair algorithm can stop playing uniformly at random (which is necessary for near-optimality). The same example, with a slightly modified analysis, also provides a lower bound of \u2126((k/(1 + k\u03b1)n) time steps for approximate-choice fair algorithms as stated in Theorem 5.\nTheorem 5. For \u03b4 < 1/4, \u03b1 < 1/4, \u03b3 > 0.5, and < 1/8, no \u03b1-choice fair algorithm can be -optimal for T = O ( ( k1+k\u03b1) n ) steps.\nTheorem 5 suggests that fairness and approximate-choice fairness are both extremely costly constraints on reinforcement learning algorithms, ruling out polynomial time solutions. Hence, we focus on approximate-action fair learning. First, we note that the time complexity of approximate-action fair algorithms will still suffer from a super-polynomial dependence on 1/(1\u2212 \u03b3). Theorem 6. For \u03b4 < 1/4, \u03b1 < 1/4, \u03b3 > 0.9 and < 1/32, no \u03b1-action fair algorithm can be\n-optimal for T = O ( k1/ \u221a 1\u2212\u03b3 ) steps.\nThe MDPs used in proving Theorem 4 can also witness the claim of Theorem 6, by choosing n = blog(1/2\u03b1)/(1\u2212\u03b3)c. We think of the discount factor \u03b3 as a constant, and so in most interesting cases, 1/ \u221a 1\u2212 \u03b3 n, meaning that this lower bound is substantially less stringent compared to the lower bounds proven for fairness and approximate-choice fairness. Therefore, in the remainder of the paper, we focus on approximate-action fairness. We aim to obtain algorithms that have learning rates that are polynomial in every parameter except for 1/(1\u2212 \u03b3), since we show that the learning rate of any approximate-action algorithm has a super-polynomial dependence on 1/(1\u2212\u03b3)."}, {"heading": "6 A Provably Fair and Efficient Learning Algorithm", "text": "We now present an approximate-action fair learning algorithm, Fair-E3, which achieves the performance guarantees stated below.\nTheorem 7. Given > 0, \u03b1 > 0, \u03b4 \u2208 (0, 1/2) and \u03b3 \u2208 [0, 1) as inputs, Fair-E3 is an \u03b1-action fair algorithm which achieves -optimality after\nT = O (\nn5T \u2217\nmin{\u03b14, 4} 2 (1\u2212 \u03b3)12 k\n1 1\u2212\u03b3+5 log (n \u03b4 ) log ( k \u03b4 ) log ( 1 \u03b4 ) log9 ( 1 (1\u2212 \u03b3) )) (2)\nsteps.\nThe running time of Fair-E3 is polynomial in all MDP parameters except 1/(1\u2212\u03b3). Moreover, Theorem 6 implies that no approximate-action fair algorithm can have a running time that is polynomial in 1/(1 \u2212 \u03b3). The remainder of this section is structured as follows. A high-level description of Fair-E3 can be found in Section 6.1; the analysis of its behavior begins in Sections 6.2 and 6.3; the final proof of Theorem 7 follows in Section 6.4. We discuss extensions and relaxations in Section 6.5.\n6.1 Informal Description of Fair-E3\nLike E3, Fair-E3 relies heavily on a notion of \u201cknown\u201d states. For both algorithms, a state s is defined to be known only after all actions have been played from s a sufficient number of times. For E3, \u201csufficient\u201d is defined as a quantity such that with high probability, it is possible to estimate the reward distribution and transition probabilities for each action. For Fair-E3, on the other hand, \u201csufficient\u201d is defined such that with high probability, for every action a at state s, it is possible to estimate Q\u2217M (s, a), which is a more exacting condition (see Section 6.2). Similarly, when in an unknown state s, E3 engages in a process of \u201cbalanced exploration\u201d by choosing the least-explored action from s, in an attempt to hasten state s becoming known; Fair-E3, constrained by fairness, cannot do this. Instead, it chooses a trajectory of actions uniformly at random from s. Fair-E3 uses these random trajectories to estimate the Q\u2217M values once the state becomes known. From a known state, E3 performs an offline computation to determine the policy to follow in a prespecified number of future time steps. Fair-E3 also engages in an offline planning computation in known states, but modified to ensure that it does not violate the approximate-action fairness (see Section 6.3). In particular, because of fairness, it is necessary to prove that whenever FairE3 cannot find an exploitation policy amongst the set of known states, there must exist a fair exploration policy which quickly escapes to unknown states.\n6.2 Known States in Fair-E3\nWe now give a formal definition of known states for Fair-E3. If a state s is known by Fair-E3, then we will have (i) highly accurate estimates of all transition probabilities from s, and (ii) highly accurate estimates of Q\u2217M (s, a) for all a.\nDefinition 11 (Q-known State). Let\nm1 = O\n( kH \u03b3 +3n ( 1\n(1\u2212 \u03b3)\u03b1\n)2 log ( k\n\u03b4\n)) and m2 = O (( n\nmin{ , \u03b1}\n)4 H\u03b3 8 log ( 1\n\u03b4\n)) .\nA state s becomes Q-known after taking\nmQ := k \u00b7max{m1,m2} (3)\nlength-H\u03b3 random trajectories from s.\nWe will show that both conditions (i) and (ii) hold for a Q-known state; informally, m1 many random trajectories suffice to ensure that we have accurate estimates of all Q\u2217M (s, a) values, and m2 random trajectories suffice to ensure accurate estimates of the transition probabilities and rewards.\nTo make the first condition formal, we rely on Theorem 8 adapted from Kearns et al. [20], connecting the number of random walks taken from s to the accuracy of the empirical V \u2217M estimates.\nTheorem 8 (Theorem 5.5, Kearns et al. [20]). For any state s and \u03b1 > 0, after\nm = O ( kH \u03b3 +3 ( 1\n(1\u2212 \u03b3)\u03b1\n)2 log ( |\u03a0| \u03b4 ))\nrandom walks of length H\u03b3 from s, with probability of at least 1\u2212 \u03b4, we can compute estimates V\u0302 \u03c0M such that |V \u03c0M (s)\u2212 V\u0302 \u03c0M (s) | \u2264 \u03b1, simultaneously for all \u03c0 \u2208 \u03a0.\nTheorem 8 enables us to translate between the number of trajectories taken from a state and the uncertainty about its V \u03c0M values for all policies (including \u03c0\n\u2217 and hence V \u2217M ). Note that since |\u03a0| = kn, we re-write log (|\u03a0|) = n log (k). To estimate Q\u2217M (s, a) values using the V \u2217M (s) values we increase the number of necessary length-H\u03b3 random trajectories by a factor of k.\nFor the second condition, we adapt the analysis of E3 in which Kearns and Singh [19] show that if each action in a state s is taken m2 times, then the transition probabilities and reward in state s can be estimated accurately (see Section 6.4).\n6.3 Planning in Fair-E3\nWe now formalize the planning steps in Fair-E3 from Q-known states. For the remainder of this section and also throughout our analysis in Section 6.4, we make Assumption 2 which we remove in Section 6.5 entirely.\nAssumption 2. T \u2217 is known.\nWe describe the offline planning step of Fair-E3 and why it is guaranteed to satisfy approximate-action fairness. Consider the following MDPs with deterministic rewards constructed from M .\n\u2022 M\u0393 (the exploitation MDP) is the MDP in which the unknown states of M are condensed into a single absorbing state s0 with no reward. In the known states \u0393, the transitions are kept intact and the rewards are deterministically set to their mean value.\n\u2022 M[n]\\\u0393 (the exploration MDP) is identical to M\u0393 except for the rewards. The rewards in the known states \u0393 are set to 0 and the reward in s0 is set to 1.\nSee the middle (right) panel of Figure 3 for an illustration of M\u0393 (M[n]\\\u0393), and Appendix C for formal definitions. M[n]\\\u0393 effectively inverts M\u0393: M[n]\\\u0393 apportions all rewards to the unknown states of M , while M\u0393 apportions all rewards to the known states of M .\nFair-E3 plans according to the following natural idea: when in a Q-known state, Fair-E3 constructs M\u0302\u0393 and M\u0302[n]\\\u0393, based on the estimated transition and rewards observed so far. Then it computes the restricted MDPs M\u0302\u03b1\u0393 and M\u0302 \u03b1 [n]\\\u0393 from M\u0302\u0393 and M\u0302[n]\\\u0393. Next, it finds the optimal policies in M\u0302\u03b1\u0393 and M\u0302 \u03b1 [n]\\\u0393. If the optimal policy in M\u0302 \u03b1 [n]\\\u0393 escapes to the absorbing state of M\u0393 with high enough probability (to be specified shortly) in 2T \u2217 steps, then Fair-E 3 explores by following that policy. Otherwise, Fair-E3 exploits by following the optimal policy in M\u0302\u03b1\u0393 for T \u2217 steps. Note that while following any of these policies, if Fair-E3 encounters an unknown state, it stops following the policy and proceeds by taking a length-H\u03b3 random trajectory.\n6.4 Analysis of Fair-E3\nIn this section we formally analyze Fair-E3 and prove Theorem 7. All the omitted proofs from this section can be found in Appendix B.3. Intuitively, M\u0393 distills the useful knowledge of FairE3 about M : from known states, Fair-E3 has enough information to play (nearly) optimally (and therefore fairly); from unknown states, Fair-E3 does not. We begin by showing that there either exists an exploitation policy achieving high reward in M\u03b1\u0393 or there exists an exploration policy in M\u03b1\u0393 which quickly reaches the unknown states of M .\nLemma 9 (Exploit or Explore Lemma). For any state s \u2208 \u0393, \u03b2 \u2208 (0, 1) and any T > 0 at least one of the statements below holds:\n\u2022 there exists an exploitation policy \u03c0 in M\u03b1\u0393 such that\n1 T max \u03c0\u0304\u2208\u03a0 E T\u2211 t=1 V \u03c0\u0304M ( \u03c0\u0304t(s), T ) \u2212 1 T E T\u2211 t=1 V \u03c0M\u0393 ( \u03c0t(s), T ) \u2264 \u03b2\nwhere the random variables \u03c0t(s) and \u03c0\u0304t(s) denote the states reached from s after following \u03c0 and \u03c0\u0304 for t steps, respectively.\n\u2022 there exists an exploration policy \u03c0 in M\u03b1\u0393 such that the probability that a walk of 2T -steps from s following \u03c0 will terminate in s0 exceeds \u03b2/T .\nNote that the optimal policy in M is approximate-action fair by Observation 2. If the optimal policy only stays in the known states of M (represented by M\u0393) then following the optimal policy in M\u0393 (and more specifically in M \u03b1 \u0393 ) results in both optimal and approximate-action fair play. However, the optimal policy in M might escape to the unknown states of M quickly, and in this case, it may be that no policy in M\u03b1\u0393 can compete with the optimal policy. Ignoring fairness, one natural way of computing an \u201cescape\u201d policy is to compute the optimal policy in M[n]\\\u0393 (as is done by E3). However, this approach will violate approximate-action fairness. Instead, we compute an escape policy in M\u03b1[n]\\\u0393 and show that if no near-optimal exploitation policy exists, then the optimal policy in M\u03b1[n]\\\u0393 (which is fair by construction) quickly escapes to the unknown states of M .\nFinally, Fair-E3 only has access to the empirically estimated MDPs M\u0302\u03b1\u0393 and M\u0302 \u03b1 [n]\\\u0393. We show that the behavior of any policy in M\u0302\u03b1\u0393 (and M\u0302 \u03b1 [n]\\\u0393) is similar to the behavior of the same policy\nin M\u03b1\u0393 (and M \u03b1 [n]\\\u0393). To do so, we prove the stronger claim that the behavior of any policy in M\u0302\u0393 (and M\u0302[n]\\\u0393) is similar to the behavior of the same policy in M\u0393 (and M[n]\\\u0393).\nLemma 10. Let \u0393 be the set of known states and M\u0302\u0393 the approximation to M\u0393. Then for any state s \u2208 \u0393, any action a and any policy \u03c0\n1. V \u03c0M\u0393(s)\u2212min{\u03b1/2, } \u2264 V \u03c0 M\u0302\u0393 (s) \u2264 V \u03c0M\u0393(s) + min{\u03b1/2, },\n2. Q\u03c0M\u0393 (s, a)\u2212min{\u03b1/2, } \u2264 Q \u03c0 M\u0302\u0393 (s, a) \u2264 Q\u03c0M\u0393 (s, a) + min{\u03b1/2, },\nwith probability at least 1\u2212 \u03b4.\nNote that an analog of Lemma 10 can be also stated for M\u0302[n]\\\u0393 and M[n]\\\u0393. Moreover, as mentioned earlier, when Fair-E3 computes the optimal policy in M\u03b1\u0393 and M \u03b1 [n]\\\u0393, it first checks to see that whether the optimal policy in M\u03b1[n]\\\u0393 quickly reaches the absorbing state of M\u0393 with significant probability (as specified in Lemma 9). This can be easily checked by simulating the execution of the optimal policy of M\u03b1[n]\\\u0393 for 2T \u2217 steps from the known state s in M \u03b1 \u0393 several times, counting the ratio of runs which end up in s0 and applying Chernoff bound. Note that this is the only place that Assumption 2 is used by Fair-E3.\nWhen there is no exploration policy then Lemma 10 implies that an exploitation policy in M\u03b1\u0393 exists. By setting T \u2265 T \u2217 in Lemma 9 and applying Lemmas 1 and 10 we can prove Corollary 11 regarding this exploitation policy.\nCorollary 11. For any state s \u2208 \u0393 and T \u2265 T \u2217 if there exists an exploitation policy \u03c0 in M\u03b1\u0393 then\u2223\u2223\u2223\u2223\u2223 1T E T\u2211 t=1 V \u03c0M ( \u03c0t(s), T ) \u2212 Es\u223c\u00b5\u2217V \u2217M (s) \u2223\u2223\u2223\u2223\u2223 \u2264 1\u2212 \u03b3 . We finally have all the background needed to prove Theorem 7.\nProof of Theorem 7. We divide the analysis into separate parts: the performance guarantee of Fair-E3 and its approximate-action fairness. We defer the analysis of the probability of failure of Fair-E3 to Appendix B.3.\nWe start with the performance guarantee. We first show that when Fair-E3 follows the exploitation policy the average V \u2217M values of the visited states is very close to Es\u223c\u00b5\u2217V \u2217M (s). However, when following the exploration policy or when taking random trajectories the V \u2217M values of the visited states might be potentially low. We then show that the number of these exploratory steps is bounded by the parameters of the MDP, and so they have only a small effect on overall performance\nObserve that in each T \u2217 -step exploitation phase of Fair-E 3, the expectation of the average V \u2217M values of the visited states is at least Es\u223c\u00b5\u2217V \u2217M (s)\u2212 /(1\u2212\u03b3)\u2212 /2 by Lemmas 1, 9 and Observation 5. By a Chernoff bound, the probability that the actual average V \u2217M values of the visited states is less than Es\u223c\u00b5\u2217V \u2217M (s)\u2212 /(1\u2212 \u03b3)\u2212 3 /4 is smaller than \u03b4/4 if the number of exploitation phases is at least (1/ 2 log(1/\u03b4)).\nThe total number of exploratory steps of Fair-E3 is bounded by\nT1 = O ( nmQH \u03b3 + nmQ T \u2217 log (n \u03b4 )) ,\nwhere mQ is defined in Equation 3 of Definition 11. The first term in T1 bounds the total number of steps needed in all the possible H\u03b3 -length random trajectories taken by Fair-E3 before all the\nstates become Q-known in the worst case. There are n states and in each state mQ trajectories are sufficient for the state to become Q-known. The second term in T1 bounds the total number of steps needed in all the possible 2T \u2217 -steps of following the exploration policy before all the states become Q-known in the worst case. Each exploration policy will end up in an unknown state with probability of at least /T \u2217 according to Lemma 9. So after O(T \u2217 log(\nn \u03b4 )/ ) times of following the\nexploration policy Fair-E3 reaches an unknown state. Finally, each of the unknown states might need to be visited mQ times before becoming Q-known where mQ is again defined in Equation 3.\nFinally, to make up for the potentially poor performance in exploration, the number of T \u2217 -step exploitation phases needed is at least\nT2 = O\n( T1(1\u2212 \u03b3) ) .\nTherefore, after T = T1 + T2 steps we have\nEs\u223c\u00b5\u2217V \u2217M (s)\u2212 1 T E T\u2211 t=1 V \u2217M (st) \u2264 2 1\u2212 \u03b3 ,\nas claimed in Equation 2. The running time of Fair-E3 is O(nT 3/ ): the additional nT 2/ factor comes from offline computation of the optimal policies in M\u0302\u03b1\u0393 and M\u0302 \u03b1 [n]\\\u0393.\nWe now prove Fair-E3 satisfies approximate-action fairness. Observe that when taking H\u03b3 - step random trajectories, Fair-E3 never violates the fairness (and hence the approximate-action fairness) constraint because randomly taking actions is \u03b1-action fair for any \u03b1 \u2265 0 by Observation 3.\nWith probability at least 1\u2212 \u03b4, all the estimates of the Q\u2217M values in the known states \u0393 have error of at most \u03b1/2. Hence, Observation 4 implies that taking any policy in either of the restricted MDPs M\u03b1\u0393 or M \u03b1 [n]\\\u0393 is \u03b1/2-action fair in M . However, Fair-E 3 computes a policy in the empirically estimated MDPs M\u0302\u03b1\u0393 and M\u0302 \u03b1 [n]\\\u0393 instead of M\u0393 and M[n]\\\u0393. Lemma 10 then shows that the Q \u2217 M values of every state/action pair in M\u0302\u03b1\u0393 and M\u0302 \u03b1 [n]\\\u0393 are within \u03b1/2 of the corresponding Q \u2217 M values in M\u03b1\u0393 and M \u03b1 [n]\\\u0393. Hence, Fair-E 3 is \u03b1-action fair."}, {"heading": "6.5 Relaxations and Discussion", "text": "Throughout Sections 6.3 and 6.4 we assumed that T \u2217 , the -mixing time of the optimal policy \u03c0 \u2217, was known (see Assumption 2). Although Fair-E3 uses the knowledge of T \u2217 to decide whether to follow the exploration or exploitation policy, Lemma 9 continues to hold even without this assumption. Note that Fair-E3 is parameterized by T \u2217 and for any input T \u2217 runs in time poly(T \u2217 ). Thus if T \u2217 is unknown, we simply run Fair-E 3 for T \u2217 = 1, 2, . . . sequentially and the running time and sample complexity will still be poly(T \u2217 ). Similar to the analysis of Fair-E 3 when T \u2217 is known we have to run the new algorithm for sufficiently many steps so that the possibly low V \u2217M values of the visited states in the early stages are dominated by the near-optimal V \u2217M values of the visited states for large enough guessed values of T \u2217 .\nFinally, our work leaves open several interesting questions. For example, we give an algorithm that has an undesirable exponential dependence on 1/(1 \u2212 \u03b3), but we also show that no approximate-action fair algorithm can run in time polynomial in 1/(1 \u2212 \u03b3). Without fairness, near-optimality in learning can be achieved in time that is polynomial in all of the parameters of the underlying MDP. So, we can ask: does there exist a meaningful fairness notion that enables reinforcement learning in time polynomial in all parameters?"}, {"heading": "A Observations on Optimality and Fairness", "text": "Observation 1. For any MDP M , there exists an optimal policy \u03c0\u2217 such that \u03c0\u2217 is fair.\nProof. In time t, let state st denote the state from which \u03c0 chooses an action. Let a \u2217 = argmaxaQ \u2217 M (st, a) and A\u2217(st) = {a \u2208 A | Q\u2217M (st, a) = Q\u2217M (st, a\u2217)}. The policy of playing an action uniformly at random from A\u2217(st) in state st for all t, is fair and optimal.\nApproximate-action fairness, conversely, can be satisfied by any optimal policy, even a deterministic one.\nObservation 2. Let \u03c0\u2217 be an optimal policy in MDP M . Then \u03c0\u2217 is approximate-action fair.\nProof. Assume that \u03c0\u2217 is not approximate-action fair. Given state s, the action that \u03c0\u2217 takes from s is uniquely determined since \u03c0\u2217 is deterministic we may denote it by a\u2217. Then there exists a time step in which \u03c0\u2217 is in state s and chooses action a\u2217(s) such that there exists another action a with\nQ\u2217M (s, a) > Q \u2217 M (s, a \u2217(s)) + \u03b1,\na contradiction of the optimality of \u03c0\u2217.\nObservations 1 and 2 state that policies with optimal performance are fair; we now state that playing an action uniformly at random is also fair.\nObservation 3. An algorithm that, in every state, plays each action uniformly at random (regardless of the history) is fair.\nProof. Let L denote an algorithm that in every state plays uniformly at random between all available actions. Then L(s, ht\u22121)a = L(s, ht\u22121)a\u2032 regardless of state s, (available) action a, or history ht\u22121. Q \u2217 M (s, a) > Q \u2217 M (s, a\n\u2032) + \u03b1 \u21d2 L(s, ht\u22121)a \u2265 L(s, ht\u22121)a\u2032 then follows immediately, which guarantees both fairness and approximate-action fairness.\nObservation 4. Let M be an MDP and M\u03b1 the \u03b1-restricted MDP of M . Let \u03c0 be a policy in M\u03b1. Then \u03c0 is \u03b1-action fair.\nProof. Assume \u03c0 is not \u03b1-action fair. Then there must exist round t, state s, and action a such that Q\u2217M (s, a) > Q \u2217 M (s, a\n\u2032) + \u03b1 and L(s, ht\u22121)a < L(s, ht\u22121)a\u2032 . Therefore L(s, ht\u22121)a\u2032 > 0, so M\u03b1 must include action a\u2032 from state s. But this is a contradiction, as in state s M\u03b1 only includes actions a\u2032 such that Q\u2217M (s, a \u2032) + \u03b1 \u2265 Q\u2217M (s, a). \u03c0 is therefore \u03b1-action fair.\nObservation 5. Let M be an MDP and M\u03b1 the \u03b1-restricted MDP of M . Let \u03c0\u2217 be an optimal policy in M\u03b1. Then \u03c0\u2217 is also optimal in M .\nProof. If \u03c0\u2217 is not optimal in M , then there exists a state s and action a such that Q\u2217M (s, a) > Ea\u2217(s)\u223c\u03c0\u2217(s)Q\u2217M (s, a\u2217(s)) where a\u2217(s) is drawn from \u03c0\u2217(s) and the expectation is taken over choices of a\u2217(s). This is a contradiction because action a is available from state s in M\u03b1 by Definition 9."}, {"heading": "B Omitted Proofs", "text": "B.1 Omitted Proofs for Section 3\nProof of Lemma 1. Let \u00b5\u0302\u03c0T denote the distribution of \u03c0 on states of M after following \u03c0 for T steps starting from s. Then we know,\nEs\u223c\u00b5\u03c0V \u03c0M (s)\u2212 1\nT E T\u2211 t=1 V \u03c0M (st) = n\u2211 i=1 (\u00b5\u03c0(si)\u2212 \u00b5\u0302\u03c0T (si))V \u03c0M (si) \u2264 n\u2211 i=1 |\u00b5\u03c0(si)\u2212 \u00b5\u0302\u03c0T (si)|V \u03c0M (si) \u2264 1\u2212 \u03b3 .\nThe last inequality is due to the following observations: (i) V \u03c0M (si) \u2264 1/(1 \u2212 \u03b3) as rewards are in [0, 1] and (ii) \u03a3ni=1 |\u00b5\u03c0(si)\u2212 \u00b5\u0302\u03c0T (si)| \u2264 since T is at least the -mixing time of \u03c0.\nB.2 Omitted Proofs for Section 5\nWe first state the following useful Lemma about M .\nLemma 12. Let M be the MDP in Definition 10. Then for any i \u2208 {1, . . . , n}, V \u2217M (si) < 1+2\u03b3n\u2212i+1 2(1\u2212\u03b3) .\nProof.\nV \u2217M (si) = discounted reward before reaching state n+ discounted reward from staying at state n\n< [ n\u2212i\u22121\u2211 t=1 \u03b3t 2 ] + \u03b3n\u2212i+1 1\u2212 \u03b3\n=\n[ 1\n2\n( 1\n1\u2212 \u03b3 \u2212 \u03b3\nn\u2212i\n1\u2212 \u03b3\n)] + \u03b3n\u2212i+1\n1\u2212 \u03b3\n= 1\u2212 \u03b3n\u2212i 2(1\u2212 \u03b3) + \u03b3n\u2212i+1 1\u2212 \u03b3 = 1 + \u03b3n\u2212i(2\u03b3 \u2212 1)\n1\u2212 \u03b3\n< 1 + 2\u03b3n\u2212i+1\n2(1\u2212 \u03b3) ,\nvia two applications of the summation formula for geometric series.\nProof of Theorem 4. We prove Theorem 4 for the special case of k = 2 first. Consider coupling the run of a fair algorithm L on both M(0.5) and M(1). To achieve this, we can fix the randomness of L up front, and use the same randomness on both MDPs. The set of observations and hence the actions taken on both MDPs are identical, until L reaches state sn. Until then, with probability at least 1 \u2212 \u03b4, L must play L and R with equal probability in order to satisfy fairness (since, for M(0.5), the only fair policy is to play both actions with equal probability at each time step).\nLet f\u03b3 = d 11\u2212 3\u221a\u03b3 e and T = 2 n\u22122f\u03b3 for n \u2265 100(f\u03b3)2. First observe that the probability of reaching a fixed state si for any i \u2265 n\u2212 f\u03b3 from a random walk of length T is upper bounded by the probability that the random walk takes i \u2265 n\u2212 f\u03b3 consecutive steps to the right in the first T steps. This probability is at most p = 2n\u22122f\u03b3 (12)\nn\u2212f\u03b3 = 2\u2212f\u03b3 for any fixed i. Then the probability that the T -step random walk arrives in any state si for i \u2265 n\u2212 f\u03b3 is also upper bounded by p.\nNext, we observe that the V \u2217M (si) is a nondecreasing function of i, for both MDPs. Then the average V \u2217M values of the visited states of any fair policy can be broken into two pieces: the average\nconditioned on the 1 \u2212 \u03b4 fairness and never reaching a state beyond sn\u2212f\u03b3 , and the average when fairness might be violated (i.e. choices are not uniform random) or the uniform random walk of length T reaches a state beyond sn\u2212f\u03b3 . So, we have that\n1 T E T\u2211 t=1 V \u2217M (st) \u2264 (1\u2212 p\u2212 \u03b4)V \u2217M (sn\u2212f\u03b3 ) + (p+ \u03b4) 1 1\u2212 \u03b3\n\u2264 (1\u2212 p\u2212 \u03b4) 1 + 2\u03b3 f\u03b3+1\n2(1\u2212 \u03b3) + (p+ \u03b4)\n1\n1\u2212 \u03b3 .\nThe first inequality follows from the fact that V \u2217M (si) \u2264 1/(1\u2212 \u03b3) for all i, and the second from Lemma 12 along with V \u2217M values being nondecreasing in i. Putting it all together,\nEs\u223c\u00b5\u2217V \u2217M (s)\u2212 1 T E T\u2211 t=1 V \u2217M (st) \u2265 1 1\u2212 \u03b3 \u2212 [ (1\u2212 p\u2212 \u03b4) 1 + 2\u03b3 f\u03b3+1 2(1\u2212 \u03b3) + (p+ \u03b4) 1 1\u2212 \u03b3 ] =\n1\u2212 p\u2212 \u03b4 1\u2212 \u03b3\n[ 1\u2212 1 + 2\u03b3 f\u03b3+1\n2\n] .\nSo -optimality requires 2\n1\u2212 \u03b3 \u2265 1\u2212 p\u2212 \u03b4 1\u2212 \u03b3\n[ 1\u2212 1 + 2\u03b3 f\u03b3+1\n2\n] . (4)\nHowever, if < 1/8 we get\n2\n1\u2212 \u03b3 < 1\u2212 0.04\u2212 1/4 1\u2212 \u03b3\n[ 1\u2212 1 + 2\u00d7 e \u22123\n2 ] <\n1\u2212 2\u2212f\u03b3 \u2212 \u03b4 1\u2212 \u03b3\n[ 1\u2212 1 + 2\u03b3 f\u03b3+1\n2\n] ,\nwhere the third inequality follows when \u03b4 < 1/4 and \u03b3 > 0.5. This means < 1/8 makes - optimality impossible, as desired.\nThroughout we considered the special case of k = 2 and proved a lower bound of \u2126(2n) time steps for any fair algorithm satisfying the -optimality condition. However, it is easy to see that MDP M in Definition 10 can be easily modified in a way that k \u2212 1 of the actions from state si reach state s1 and only one action in each state si reaches states smin{i+1,n}. Hence, a lower bound of \u2126(kn) time steps can be similarly proved.\nProof of Theorem 5. We mimic the argument used to prove Theorem 4 with the difference that, until visiting sn, L may not play R with probability more than 1/2 + \u03b1 (as opposed to 1/2 in Theorem 6). Let f\u03b3 = d 11\u2212 3\u221a\u03b3 e and T = ( 2 1+2\u03b1)\nn\u22122f\u03b3 for n \u2265 100(f\u03b3)2. By a similar process as in Theorem 6, the probability of reaching state si for any i \u2265 n \u2212 f\u03b3 from a random walk of length T is bounded by p = ( 21+2\u03b1)\nf\u03b3 , and so the probability that the T -step random walk arrives in any state si for i \u2265 n \u2212 f\u03b3 is bounded by p. Carrying out the same process used to prove Theorem 6 then once more implies that -optimality requires Equation 5 to hold when \u03b4 < 1/4, \u03b1 < 1/4 and \u03b3 > 0.5. Hence, < 1/8 violates this condition as desired.\nFinally, throughout we considered the special case of k = 2. The same trick as in the proof of Theorem 4 can be used to prove the lower bound of \u2126(( k1+k\u03b1)\nn) time steps for any fair algorithm satisfying the -optimality condition.\nProof of Theorem 6. We also prove Theorem 6 for the special case of k = 2 first. Again consider the MDP in Definition 10. We set the size of the state space in M to be n = blog(1/(2\u03b1))/(1\u2212\u03b3)c. Then given the parameter ranges, for any i, Q\u2217M (si, R) \u2212 Q\u2217M (si, L) > \u03b1 in M(1). Therefore, any approximate-action fair algorithm should play actions R and L with equal probability.\nLet T = 2n/2 = \u2126(21/ \u221a\n1\u2212\u03b3). First observe that the probability of reaching a fixed state si for any i \u2265 3n/4 from a random walk of length T is upper bounded by the probability that the random walk takes i \u2265 3n/4 consecutive steps to the right in the first T steps. This probability is at most p = 2n/2(12)\n3n/4 = 2\u2212n/4 for any fixed i. Then the probability that the T -step random walk arrives in any state si for i \u2265 3n/4 is also upper bounded by p.\nNext, we observe that the V \u2217M (si) is a nondecreasing function of i, for both MDPs. Then the average V \u2217M values of the visited states of any fair policy can be broken into two pieces: the average conditioned on the 1 \u2212 \u03b4 fairness and never reaching a state beyond s3n/4, and the average when fairness might be violated or the uniform random walk of length T reaches a state beyond s3n/4. So, we have that\n1 T E T\u2211 t=1 V \u2217M (st) \u2264 (1\u2212 p\u2212 \u03b4)V \u2217M (s3n/4) + (p+ \u03b4) 1 1\u2212 \u03b3\n\u2264 (1\u2212 p\u2212 \u03b4) 1 + (2\u03b3 \u2212 1)\u03b3 n/4\n2(1\u2212 \u03b3) + (p+ \u03b4)\n1\n1\u2212 \u03b3 .\nThe first inequality follows from the fact that V \u2217M (si) \u2264 1/(1\u2212 \u03b3) for all i, and the second from Lemma 12 along with V \u2217M values being nondecreasing in i. Putting it all together,\nEs\u223c\u00b5\u2217V \u2217M (s)\u2212 1 T E T\u2211 t=1 V \u2217M (st) \u2265 1 1\u2212 \u03b3 \u2212\n[ (1\u2212 p\u2212 \u03b4) 1 + (2\u03b3 \u2212 1)\u03b3 n/4\n2(1\u2212 \u03b3) + (p+ \u03b4)\n1\n1\u2212 \u03b3\n]\n= 1\u2212 p\u2212 \u03b4\n1\u2212 \u03b3\n[ 1\u2212 1 + (2\u03b3 \u2212 1)\u03b3 n/4\n2\n]\n= 1\u2212 p\u2212 \u03b4\n1\u2212 \u03b3\n[ 1\n2 \u2212 (2\u03b3 \u2212 1)\u03b3 n/4 2\n] .\nSo -optimality requires\n2 1\u2212 \u03b3 \u2265 1\u2212 p\u2212 \u03b4 1\u2212 \u03b3\n[ 1\n2 \u2212 (2\u03b3 \u2212 1)\u03b3 n/4 2\n] . (5)\nHowever, if < 1/32 we get\n2\n1\u2212 \u03b3 < 1\u2212 0.22\u2212 1/4 1\u2212 \u03b3\n[ 1\n2 \u2212 0.4 \u00b7 0.99/4 ] <\n1\u2212 2\u2212n/4 \u2212 \u03b4 1\u2212 \u03b3\n[ 1\n2 \u2212 (2\u03b3 \u2212 1)\u03b3 n/4 2\n] ,\nwhere the second inequality follows from \u03b4 < 1/4, \u03b1 < 1/4, \u03b3 > 0.9, and n \u2265 log(1/2\u03b1)1\u2212\u03b3 \u2212 1 \u2265 9. This means < 1/32 makes -optimality impossible, as desired.\nFinally, the same trick as in the proof of Theorem 4 can be used to prove the \u2126(k1/ \u221a\n1\u2212\u03b3) lower bound for k > 2 actions.\nB.3 Omitted Proofs for Section 6\nProof of Lemma 9. We first show that either\n\u2022 there exists an exploitation policy \u03c0 in M\u0393 such that\n1 T max \u03c0\u0304\u2208\u03a0 E T\u2211 t=1 V \u03c0\u0304M ( \u03c0\u0304t(s), T ) \u2212 1 T E T\u2211 t=1 V \u03c0M\u0393 ( \u03c0t(s), T ) \u2264 \u03b2\nwhere the random variable \u03c0t(s) and \u03c0\u0304t(s) denote the states reached from s after following \u03c0 and \u03c0\u0304 for t steps, respectively.\n\u2022 there exists an exploration policy \u03c0 in M\u0393 such that the probability of that a walk of 2T -steps from s following \u03c0 will terminate in s0 exceeds \u03b2/T .\nLet \u03c0 be a policy in M satisfying\n1 T E T\u2211 t=1 V \u03c0M (\u03c0 t(s), T ) = 1 T max \u03c0\u0304\u2208\u03a0 E T\u2211 t=1 V \u03c0 \u2032 M (\u03c0\u0304 t(s), T ) := V\u0303 .\nFor any state s\u2032 let p(s\u2032) denote all the T -paths in M that start in s\u2032, q(s\u2032) denote all the T -paths in M that start in s\u2032 such that all the states in every T -path in q(s\u2032) are in \u0393 and r(s\u2032) all the T -paths in M that start in s\u2032 such that at least one state in every T -paths in r(s\u2032) is not in \u0393. Suppose\n1 T E T\u2211 t=1 V \u03c0M\u0393(\u03c0 t(s)) < V\u0303 \u2212 \u03b2.\nOtherwise, \u03c0 already witnesses the claim. We show that a walk of 2T -steps from s following \u03c0 will terminate in s0 with probability of at least \u03b2/T .\nFirst,\nE T\u2211 t=1 V \u03c0M (\u03c0 t(s), T ) = E T\u2211 t=1 \u2211 p(\u03c0t(s)) P[p(\u03c0t(s))]VM (p(\u03c0t(s)))\n= E T\u2211 t=1 \u2211 q(\u03c0t(s)) P[q(\u03c0t(s))]VM (q(\u03c0t(s))) + E T\u2211 t=1 \u2211 r(\u03c0t(s)) P[r(\u03c0t(s))]VM (r(\u03c0t(s)))\nsince p(\u03c0t(s)) = q(\u03c0t(s)) \u222a r(\u03c0t(s)), which is a disjoint union. Next, E T\u2211 t=1 \u2211 q(\u03c0t(s)) P[q(\u03c0t(s))]VM (q(\u03c0t(s))) = E T\u2211 t=1 \u2211 q(\u03c0t(s)) P\u03c0M\u0393 [q(\u03c0 t(s))]VM\u0393(q(\u03c0 t(s))) \u2264 E T\u2211 t=1 V \u03c0M\u0393(\u03c0 t(s), T ),\nwhere the equality is due to Definition 13 and the definition of q, and the inequality follows because V \u03c0M\u0393(\u03c0\nt(s), T ) is the sum over all the T -paths in M\u0393, not just those that avoid the absorbing state s0. Therefore by our original assumption on \u03c0,\nE T\u2211 t=1 \u2211 q(\u03c0t(s)) P[q(\u03c0t(s))]VM (q(\u03c0t(s))) \u2264 E T\u2211 t=1 V \u03c0M\u0393(\u03c0 t(s), T ) < TV\u0303 \u2212 T\u03b2.\nThis implies\nE T\u2211 t=1 \u2211 r(\u03c0t(s)) P[r(\u03c0t(s))]VM (r(\u03c0t(s))) = E T\u2211 t=1 V \u03c0M (\u03c0 t(s), T )\u2212 E T\u2211 t=1 \u2211 q(\u03c0t(s)) P[q(\u03c0t(s))]VM (q(\u03c0t(s)))\n= T V\u0303 \u2212 E T\u2211 t=1 \u2211 q(\u03c0t(s)) P[q(\u03c0t(s))]VM (q(\u03c0t(s))) \u2265 T\u03b2,\nwhere the last step is the result of applying the previous inequality. However,\nE T\u2211 t=1 \u2211 r(\u03c0t(s)) P[r(\u03c0t(s))]VM (r(\u03c0t(s))) \u2264 TE T\u2211 t=1 \u2211 r(\u03c0t(s)) P[r(\u03c0t(s))],\nbecause trivially VM (r(\u03c0 t(s))) \u2264 T for all \u03c0t(s). So T\u03b2 \u2264 TE \u2211T t=1 \u2211 r(\u03c0t(s)) P[r(\u03c0t(s))]. Finally, if we let P\u03c02T denote the probability that a walk of 2T steps following \u03c0 terminates in s0, i.e. the probability that \u03c0 escapes to an unknown state within 2T steps then for each t \u2208 [T ], E \u2211 r(\u03c0t(s)) \u2264 TP\u03c02T . It follows that T\u03b2 \u2264 T 2P\u03c02T\nand rearranging yields P\u03c02T \u2265 \u03b2/T as desired. Next, note that the exploitation policy (if it exists) can be derived by computing the optimal policy in M\u0393. Moreover, the exploration policy (if it exists) in the exploitation MDP M\u0393 can indeed be derived by computing the optimal policy in the exploration MDP M[n]\\\u0393 as observed by [19]. Finally, by Observation 5, any optimal policy in M\u0302\u03b1\u0393 (M\u0302 \u03b1 [n]\\\u0393) is an optimal policy in M\u0302\u0393 (M\u0302[n]\\\u0393)\nTo prove Lemma 10, we need some useful background adapted from Kearns and Singh [19].\nDefinition 12 (Definition 7, Kearns and Singh [19]). Let M and M\u0302 be two MDPs with the same set of states and actions. We say M\u0302 is a \u03b2-approximation of M if\n\u2022 For any state s, R\u0304M (s)\u2212 \u03b2 \u2264 R\u0304M\u0302 (s) \u2264 R\u0304M (s) + \u03b2.\n\u2022 For any states s and s\u2032 and action a,\nPM (s, a, s \u2032)\u2212 \u03b2 \u2264 PM\u0302 (s, a, s \u2032) \u2264 PM (s, a, s\u2032) + \u03b2.\nLemma 13 (Lemma 5, Kearns and Singh [19]). Let M be an MDP and \u0393 the set of known states of M . For any s, s\u2032 \u2208 \u0393 and action a \u2208 A, let P\u0302M (s, a, s\u2032) denote the empirical probability transition estimates obtained from the visits to s. Moreover, for any state s \u2208 \u0393 let \u00af\u0302R(s) denote the empirical estimates of the average reward obtained from visits to s. Then with probability at least 1\u2212 \u03b4,\n|P\u0302M (s, a, s\u2032)\u2212 PM (s, a, s\u2032)| = O ( min{ , \u03b1}2\nn2H\u03b3 4\n) , and | \u00af\u0302RM (s)\u2212 R\u0304M (s)| = O ( min{ , \u03b1}2\nn2H\u03b3 4\n) .\nLemma 13 shows that M\u0302\u0393 and M\u0302[n]\\\u0393 are O(min{ , \u03b1}2/(n2H \u03b3 4 ))-approximation MDPs for M\u0393\nand M[n]\\\u0393, respectively.\nLemma 14 (Lemma 4, Kearns and Singh [19]). Let M be an MDP and M\u0302 its O(min{ , \u03b1}2/(n2H\u03b3 4))- approximation. Then for any policy \u03c0 \u2208 \u03a0 and any state s and action a\nV \u03c0M (s)\u2212min{ , \u03b1} \u2264 V \u03c0M\u0302 (s) \u2264 V \u03c0 M (s) + min{ , \u03b1},\nand Q\u03c0M (s, a)\u2212min{\u03b1/4, } \u2264 Q\u03c0M\u0302 (s, a) \u2264 Q \u03c0 M (s, a) + min{\u03b1/4, }.\nProof of Lemma 10. By Definition 11 and Lemma 13, M\u0302\u0393 is aO(min{ , \u03b1}2/(n2H\u03b3 4))-approximation of M\u0393. Then the statement directly follows by applying Lemma 14.\nRest of the Proof of Theorem 7. The only remaining part of the proof of Theorem 7 is the analysis of the probability of failure of Fair-E3. To do so, we break down the probability of failure of Fair-E3, by considering the following (exhaustive) list of possible failures:\n1. At some known state the algorithm has a poor approximation of the next step, causing M\u0302\u0393 to not be a O(min{ , \u03b1}2/(n2H\u03b3 4))-approximation of M\u0393.\n2. At some known state the algorithm has a poor approximation of the Q\u2217M values for one of the actions.\n3. Following the exploration policy for 2T \u2217 -step fails to yield enough visits to unknown states.\n4. At some known state, the approximation values of that state in M\u0302\u0393 is not an accurate estimate for the value of the state in M\u0393.\nWe allocate \u03b4/4 of our total probability of failure to each of these sources:\n1. Set \u03b4\u2032 = \u03b4/(4n) in Lemma 10.\n2. Set \u03b4\u2032 = \u03b4/(4nk) in Theorem 8.\n3. By Lemma 9, each attempted exploration is a Bernoulli trial with probability of success of at least /(4T \u2217 ). In the worst case we might need to make every state known before exploiting, leading to the nmQ trajectories (mQ as Equation 3 in Definition 11) of length H \u03b3 . Therefore,\nthe probability of taking fewer than nmQ trajectories of length H \u03b3 would be bounded by \u03b4/4 if the number of 2T \u2217 -steps explorations is at least\nmexp = O\n( T \u2217 nmQ\nlog(\nn \u03b4 )\n) . (6)\n4. Set \u03b4\u2032 = \u03b4/(4mexp) (mexp as defined in Equation 6) in Lemma 10, as Fair-E 3 might make\n2T \u2217 -steps explorations up to mexp times.\nC Omitted Details of Fair-E3\nWe first formally define the exploitation MDP M\u0393 and the exploration MDP M[n]\\\u0393:\nDefinition 13 (Definition 9, Kearns and Singh [19]). Let M = (SM ,AM , PM , RM , T, \u03b3) be an MDP with state space SM and let \u0393 \u2282 SM . We define the exploration MDP M\u0393 = (SM\u0393 ,AM , PM\u0393 , RM\u0393 , T, \u03b3) on \u0393 where\n\u2022 SM\u0393 = \u0393 \u222a {s0}.\n\u2022 For any state s \u2208 \u0393, R\u0304M\u0393(s) = R\u0304M (s), rewards in M\u0393 are deterministic, and R\u0304M\u0393(s0) = 0.\n\u2022 For any action a, PM\u0393(s0, a, s0) = 1. Hence, s0 is an absorbing state.\n\u2022 For any states s1, s2 \u2208 \u0393 and any action a, PM\u0393(s1, a, s2) = PM (s1, a, s2), i.e. transitions between states in \u0393 are preserved in M\u0393.\n\u2022 For any state s1 \u2208 \u0393 and any action a, PM\u0393(s1, a, s0) = \u03a3s2 /\u2208\u0393PM (s1, a, s2). Therefore, all the transitions between a state in \u0393 and states not in \u0393 are directed to s0 in M\u0393.\nDefinition 14 (Implicit, Kearns and Singh [19]). Given MDP M and set of known states \u0393, the exploration MDP M[n]\\\u0393 on \u0393 is identical to the exploitation MDP M\u0393 except for its reward function. Specifically, rewards in M[n]\\\u0393 are deterministic as in M\u0393, but for any state s \u2208 \u0393, R\u0304M[n]\\\u0393(s) = 0, and R\u0304M[n]\\\u0393(s0) = 1.\nWe next define the approximation MDPs M\u0302\u0393 and M\u0302[n]\\\u0393 which are defined over the same set of states and actions as in M\u0393 and M[n]\\\u0393, respectively.\nLet M be an MDP and \u0393 the set of known states of M . For any s, s\u2032 \u2208 \u0393 and action a \u2208 A, let P\u0302M\u0393(s, a, s \u2032) denote the empirical probability transition estimates obtained from the visits to s. Moreover, for any state s \u2208 \u0393 let \u00af\u0302RM\u0393(s) denote the empirical estimates of the average reward obtained from visits to s. Then M\u0302\u0393 is identical to M\u0393 except that:\n\u2022 in any known state s \u2208 \u0393, R\u0302M\u0302\u0393(s) = \u00af\u0302 RM\u0393(s).\n\u2022 for any s, s\u2032 \u2208 \u0393 and action a \u2208 A, PM\u0302\u0393(s, a, s \u2032) = P\u0302M\u0393(s, a, s \u2032).\nAlso M\u0302[n]\\\u0393 is identical to M[n]\\\u0393 except that:\n\u2022 for any s, s\u2032 \u2208 \u0393 and action a \u2208 A, PM\u0302[n]\\\u0393(s, a, s \u2032) = P\u0302M[n]\\\u0393(s, a, s \u2032)."}], "references": [{"title": "Auditing black-box models by obscuring features", "author": ["Philip Adler", "Casey Falk", "Sorelle Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "The new science of sentencing", "author": ["Anna Barry-Jester", "Ben Casselman", "Dana Goldstein"], "venue": "The Marshall Project, August", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Artificial intolerance. MIT Technology Review, March 28 2016", "author": ["Nanette Byrnes"], "venue": "URL https: //www.technologyreview.com/s/600996/artificial-intolerance/", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Three naive Bayes approaches for discrimination-free classification", "author": ["Toon Calders", "Sicco Verwer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Certifying and removing disparate impact", "author": ["Michael Feldman", "Sorelle Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A confidence-based approach for balancing fairness and accuracy", "author": ["Benjamin Fish", "Jeremy Kun", "\u00c1d\u00e1m D\u00e1niel Lelkes"], "venue": "In Proceedings of the 2016 SIAM International Conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "A methodology for direct and indirect discrimination prevention in data mining", "author": ["Sara Hajian", "Josep Domingo-Ferrer"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Martinez-Balleste. Rule protection for indirect discrimination prevention in data mining", "author": ["Sara Hajian", "Josep Domingo-Ferrer", "Antoni"], "venue": "In Modeling Decision for Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Equality of opportunity in supervised learning", "author": ["Moritz Hardt", "Eric Price", "Nathan Srebro"], "venue": "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Rawlsian fairness for machine learning", "author": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Fairness in learning: Classic and contextual bandits", "author": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "venue": "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Data preprocessing techniques for classification without discrimination", "author": ["Faisal Kamiran", "Toon Calders"], "venue": "Knowledge and Information Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Discrimination aware decision tree learning", "author": ["Faisal Kamiran", "Toon Calders", "Mykola Pechenizkiy"], "venue": "In Proceedings of the 10th IEEE International Conference on Data Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Decision theory for discrimination-aware classification", "author": ["Faisal Kamiran", "Asim Karim", "Xiangliang Zhang"], "venue": "In Proceedings of the 12th IEEE International Conference on Data Mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Fairness-aware classifier with prejudice remover regularizer", "author": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "venue": "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Approximate planning in large POMDPs via reusable trajectories", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Ng"], "venue": "In Proceedings of the 13th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Inherent trade-offs in the fair determination of risk", "author": ["Jon Kleinberg", "Sendhil Mullainathan", "Manish Raghavan"], "venue": "scores. CoRR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "k-NN as an implementation of situation testing for discrimination discovery and prevention", "author": ["Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Can an algorithm hire better than a human?  The New York", "author": ["Clair Miller"], "venue": "Times, June", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Discrimination-aware data mining", "author": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Predictive policing using machine learning to detect patterns of crime", "author": ["Cynthia Rudin"], "venue": "URL http://www.wired.com/insights/2013/08/ predictive-policing-using-machine-learning-to-detect-patterns-of-crime/", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Personal Communication, June 2016", "author": ["Satinder Singh"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Discrimination in online ad delivery", "author": ["Latanya Sweeney"], "venue": "Communications of the ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u2019an Szita", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 47, "endOffset": 50}, {"referenceID": 22, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 91, "endOffset": 94}, {"referenceID": 24, "context": "In contexts as diverse as hiring [23], lending [4], policing [25], and criminal sentencing [3], mounting empirical evidence suggests these concerns are not merely hypothetical [2, 27].", "startOffset": 176, "endOffset": 183}, {"referenceID": 4, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[6, 7, 12, 14]).", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[14], who study fairness in the contextual bandit framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] define fairness with respect to one-step rewards, which is appropriate for the contextual bandit setting where the learner\u2019s actions do not affect future state.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] in terms of myopic rewards would be in conflict with the optimal policy in an underlying MDP if it was necessary to make sub-optimal decisions in the short run (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14], our results establish rigorous trade-offs between fairness and performance in reinforcement learning algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We adapt E3 [19], a wellknown algorithm for MDP learning, to satisfy approximate-action fairness.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesv\u00e1ri [28] and references within).", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "E3 [19] was the first learning algorithm with a polynomial learning rate, and a subsequent literature gave a sequence of improved bounds (see Szita and Szepesv\u00e1ri [28] and references within).", "startOffset": 163, "endOffset": 167}, {"referenceID": 7, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 8, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 9, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 10, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 11, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 12, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 13, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 14, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 15, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 18, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 19, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 21, "context": "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 29]).", "startOffset": 0, "endOffset": 55}, {"referenceID": 0, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "[1, 5, 7, 8, 18, 22]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "[6], it suffers from two problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] for a catalog of ways in which statistical parity fails as a definition of fairness.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[12] give a definition aiming to capture the idea of equality of opportunity that overcomes many of these limitations of notions of group fairness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] propose and explore the basic properties of a technical definition of individual fairness formalizing the idea that \u201csimilar individuals should be treated similarly\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[14] and the present work suggest a natural metric for the tasks of regret minimization in a bandit setting and long-term reward maximization in an MDP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] is that the latter operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "\u2022 PM : SM \u00d7 AM \u00d7 SM \u2192 [0, 1] is a transition probability distribution mapping (s, a, s\u2032) 7\u2192 p where p is the probability of arriving in state s\u2032 after taking action a from state s.", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "We assume the support of the reward distribution is [0, 1] for all s.", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "R\u0304M : SM \u2192 [0, 1] denote the mean of reward distribution at state s.", "startOffset": 11, "endOffset": 17}, {"referenceID": 16, "context": "Kearns and Singh [19]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "distribution is over a possibly unbounded region but the mean and the variance of the reward distribution is bounded in all states using standard techniques (see Kearns and Singh [19] for more details).", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "Furthermore, assuming that the support of the reward distributions is between [0, 1] can be made without loss of generality up to scaling.", "startOffset": 78, "endOffset": 84}, {"referenceID": 16, "context": "-reward mixing time is always linearly bounded by the -mixing time but can be much smaller than the -mixing time in certain cases (see Kearns and Singh [19] for a discussion).", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "In the discounted rewards setting, the expected discounted reward of any policy after H = log ( (1\u2212 \u03b3)) / log(\u03b3) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "In the discounted rewards setting, the expected discounted reward of any policy after H = log ( (1\u2212 \u03b3)) / log(\u03b3) (see Kearns and Singh [19], Lemma 2) steps approaches the expected asymptotic discounted reward: Lemma 2 (Lemma 2, Kearns and Singh [19]).", "startOffset": 245, "endOffset": 249}, {"referenceID": 23, "context": "Lemma 3 (Singh [26]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[14]\u2019s definition for learning in a contextual bandit setting: namely, Definition 1 when the Q\u2217(s, a) values are replaced with the expected payoff of choosing a given arm a.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] define the quality of an action to be QM (s, a, 1), the expected one-step reward for choosing a from s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20], connecting the number of random walks taken from s to the accuracy of the empirical V \u2217 M estimates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "For the second condition, we adapt the analysis of E3 in which Kearns and Singh [19] show that if each action in a state s is taken m2 times, then the transition probabilities and reward in state s can be estimated accurately (see Section 6.", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "We initiate the study of fair learning in Markovian settings, where the actions of a learning algorithm may affect its environment and future rewards. Working in the model of reinforcement learning, we define a fairness constraint requiring that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take exponentially many rounds in the number of states to achieve non-trivial approximation to the optimal policy. Our main result is a polynomial time algorithm that is provably fair under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.", "creator": "LaTeX with hyperref package"}}}