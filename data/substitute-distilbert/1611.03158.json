{"id": "1611.03158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Using Neural Networks to Compute Approximate and Guaranteed Feasible Hamilton-Jacobi-Bellman PDE Solutions", "abstract": "hamilton - jacobi ( hj ) reachability is a powerful tool that asserts performance enhanced safety guarantees for dynamical systems. unfortunately, using the most - of - the - art dynamic algorithms - based approaches, hj reachability is impossible for systems with more than five dimensions because underlying computational complexity scales exponentially with system dimension. to sidestep the curse of dimensionality, we propose than algorithm explicitly leverages downstream neural network to approximate the same time - to - reach function to synthesize controls. we see that our neural network generates near optimal controls which are guaranteed to successfully drive the system to a target state. our framework is not dependent on intermediate dimensional discretization, leading until a significant reduction into computation time and space complexity in comparison though dynamic grid - based approaches. using this grid - free approach vastly enables us accurate plan over longer time horizons with relatively little additional physical overhead. unlike many previous neural network reachability formulations, our approximation is invalid only hence any trajectories we generate will be strictly feasible. for demonstration, we specialize our new general framework underlying the dubins car model and discuss how the general proof can be applied to other models with higher - dimensional state spaces.", "histories": [["v1", "Thu, 10 Nov 2016 01:48:39 GMT  (3022kb,D)", "http://arxiv.org/abs/1611.03158v1", "Submitted to HSCC 2017"], ["v2", "Mon, 27 Mar 2017 05:21:42 GMT  (5055kb,D)", "http://arxiv.org/abs/1611.03158v2", "Submitted to IEEE Conference on Decision and Control, 2017"]], "COMMENTS": "Submitted to HSCC 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["frank jiang", "glen chou", "mo chen", "claire j tomlin"], "accepted": false, "id": "1611.03158"}, "pdf": {"name": "1611.03158.pdf", "metadata": {"source": "CRF", "title": "Using Neural Networks for Fast Reachable Set Computations", "authors": ["Frank Jiang", "Glen Chou", "Mo Chen", "Claire J. Tomlin"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "In recent years, rapid progress in robotics and artificial intelligence have accelerated the need for efficient path-planning algorithms in high-dimensional spaces. In particular, there has been vast interest in the development of autonomous cars and unmanned aerial vehicles (UAVs) for civilian purposes [3,4,7,21,23,25]. As such systems grow in complexity, development of algorithms that can tractably control them in high-dimensional state spaces will become necessary.\nHamilton-Jacobi (HJ) reachability is a theoretically important and practically powerful tool for analyzing a large\n\u2217* All three authors contributed equally to this work.\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nrange of systems. It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30]. The core idea of reachability analysis is to compute the reachable set, which represents the set of states from which the system can be driven into a target set. From the reachable set, the optimal control for reaching the target set can be synthesized. Unfortunately, current state-of-the-art numerical methods for computing reachable sets discretize the state space, resulting in space and time complexities that scale exponentially with the system dimension.\nIn an effort to overcome the curse of dimensionality, many methods which heavily restrict the system at hand have been proposed. For example, one method requires the system dynamics to be polynomial [18, 26], while another method requires the Hamiltonian to be a function of only the controlled variable [15]. There are also less restrictive methods using projections, approximate dynamic programming, and approximate systems decoupling [11, 28, 31], each with its own limitations in flexibility, scalability, and degree of conservatism.\nOne of the main drawbacks of reachability analysis and other dynamic programming-based approaches is the need to compute a value function over a large portion of the state space. This is computationally wasteful since the value function, from which the optimal controller is derived, is only needed along a trajectory from the system\u2019s initial state to the target set. A more efficient approach would be to only compute the value function local to the trajectory from the initial state to the target set. However, there is no way of knowing where such a trajectory will lie before thoroughly computing the value function.\nMethods that exploit machine learning have great potential because they are state discretization-free and do not depend on the dynamic programming principle. Unfortunately, many machine learning techniques cannot make the guarantees provided by reachability analysis. For instance, [8] and [24] use neural networks (NNs) as nonlinear optimizers to synthesize trajectories which may not be dynamically feasible. The authors in [2] propose a supervised learningbased algorithm that depends heavily on feature tuning and design, making its application to high-dimensional problems cumbersome. In [17] and [32], the authors successfully use NNs for approximating the value function, but the approximation is not guaranteed to be conservative.\nIn this paper, we attempt to combine the best features of both reachability and machine learning using an NN-based algorithm. Our proposed grid-free method conservatively\nar X\niv :1\n61 1.\n03 15\n8v 1\n[ cs\n.L G\n] 1\n0 N\nov 2\n01 6\napproximates the time-to-reach (TTR) representation of a reachable set in only a region around a feasible trajectory. Unlike previous machine learning techniques, our technique guarantees a direction of conservatism, and unlike previous dynamic programming-based methods, our approach involves an NN that effectively figures out the relevant region in which a value function is required. Our contributions will be presented as follows:\n\u2022 In Section 2, we summarize HJ reachability as well as the formalisms used for this work.\n\u2022 In Sections 3 and 4, we present the two phases of our proposed algorithm and the underlying design choices. In particular, we show the guaranteed feasibility and conservatism properties of our framework.\n\u2022 In Section 5, we develop, apply, and validate our method on the Dubins car model and discuss our numerical results. Here, we will illustrate our guaranteed-conservative approximation of the TTR function, and the resulting near-optimal trajectories.\n\u2022 Finally, in Section 6, we discuss future directions, including applying the method to higher-dimensional systems."}, {"heading": "2. PROBLEM FORMULATION", "text": "In this section, we will first provide an overview of HJ reachability theory as well as some definitions essential to express our main results. Afterwards, we will briefly discuss the goals of this paper."}, {"heading": "2.1 Background", "text": "2.1.1 System Dynamics Consider a dynamical system governed by the following\nordinary differential equation (ODE):\nx\u0307 = f(x, u), u \u2208 U (1)\nHere, x \u2208 Rn is the state of the system and the control function u(\u00b7) is assumed to be drawn from the set of measurable functions1. Let us further assume that the system dynamics f : Rn \u00d7 U \u2192 Rn are uniformly continuous, bounded, and Lipschitz continuous in x for fixed u. Denote the function space from which f is drawn as F.\nWith these assumptions, given some initial state x, initial time t0, and control function u(\u00b7) \u2208 U, there exists a unique trajectory solving (1). We refer to trajectories of (1) starting from state x and time t0 as \u03be(t;x, t0, u(\u00b7)). Trajectories satisfy an initial condition and (1) almost everywhere:\nd dt \u03bef (t;x, t0, u(\u00b7)) = f(\u03bef (t;x, t0, u(\u00b7)), u(t))\n\u03bef (t0;x, t0, u(\u00b7)) = x (2)\nThroughout the paper, we will extensively use the backwardtime dynamics x\u0307 = \u2212f(x, u), whose trajectories will be denoted \u03be\u2212f (t;x, t0, u(\u00b7)). Note that 1A function f : X \u2192 Y between two measurable spaces (X,\u03a3X) and (Y,\u03a3Y ) is said to be measurable if the preimage of a measurable set in Y is a measurable set in X, that is: \u2200V \u2208 \u03a3Y , f1(V ) \u2208 \u03a3X , with \u03a3X ,\u03a3Y \u03c3-algebras on X,Y .\nxf = \u03be f (tf ;x0, t0, u(\u00b7))\u21d4 x0 = \u03be\u2212f (tf ;xf , t0, u(\u00b7)) (3)\n2.1.2 Minimum Time-To-Reach In this paper we consider a particular formulation of the\nreachability problem, which we will refer to as the time-toreach (TTR) problem. In this context, we would like to find the minimum time the system needs to reach a target set L from a starting point x. Without loss of generality (WLOG), we assume the starting time is t0 = 0, and define the TTR function as follows:\nDefinition 1. Time-to-reach function.\n\u03c6(x) = min u(\u00b7)\u2208U min t {\u03bef (t;x, 0, u(\u00b7)) \u2208 L} (4)\nThere are many HJ formulations for obtaining the TTR function \u03c6(x) [6, 10, 30]. For example, \u03c6(x) is the viscosity solution [13] of the following HJ PDE [37]:\nmin u\u2208U {\u2212\u2207\u03c6(x) \u00b7 f(x, u)\u2212 1} = 0 \u2200x /\u2208 L\n\u03c6(x) = 0 \u2200\u03c6 \u2208 L (5)\nAfter obtaining the solution \u03c6(x), the optimal control, which drives the system to the set L in minimum, can be extracted as follows:\nu\u2217(x) = arg min u\u2208U \u2207\u03c6(x) \u00b7 f(x, u) (6)\nIn general, numerical methods for finding \u03c6(x) such as the one outlined in [37], as well as many others [29, 33, 34], involve representing the state space on a grid, leading to an exponentially scaling computation complexity with respect to the system dimension."}, {"heading": "2.2 Goal", "text": "In this paper, we seek to overcome the exponentially scaling computation complexity. Our approach is inspired by two inherent challenges that dynamic programming-based methods face.\nFirst, since only relatively mild assumptions are placed on the system dynamics (1), optimal trajectories are a priori unknown and could essentially trace out any arbitrary path in the state space. Dynamic programming ignores this issue by considering all possible behaviors of the trajectory. Numerical methods are needed to obtain dynamic programming solutions, leading to the necessity of discretizing the state space.\nSecond, in a practical setting, the system starts at some particular state x. Thus, the optimal control, and in particular\u2207\u03c6(x) in (6), is needed only along the optimal trajectory. However, again, since the optimal trajectory is a priori unknown, dynamic programming-based approaches resort to computing \u03c6(x) over a very large portion of the state space so that the \u2207\u03c6(x) is available regardless of where the optimal trajectory happens to be.\nIn this paper, we propose a method that, in contrast to dynamic programming-based methods,\n1. has a sub-exponential complexity scaling with repsect to the state space dimension;\n2. is applicable to higher-order or more complex systems without changing the NN architecture or dynamic training procedure;\n3. generates an approximate TTR function from which a controller that drives the system to the target can be synthesized;\n4. guarantees a direction of conservatism, despite the use of an NN.\nWe enforce 1) by avoiding operations that exhaustively search the state space. As seen in Sections 3.2 and 3.3, the training and final data sets are either generated randomly or outputted by the NN, both constant time operations. Furthermore, we rely on NNs being universal function approximators [14] to make our method general to the system dynamics, thus satisfying 2). While our method does have some limitations, as discussed at the end of Section 3.1, we do not believe that these limitations are restrictive in the context of reachability analysis. Our post processing of the NN outputs outlined in Section 4 satisfies 3). Finally, we use the dynamics of the system to ensure our final output satisfies 4); this is detailed in Section 3.3.\nOur method overcomes the challenges faced by dynamic programming-based in two phases: the NN training phase, which allows the NN to learn the inverse backward system dynamics, and the controller synthesis phase, which uses the NN to obtain TTR approximations that can be used to synthesize a controller."}, {"heading": "3. NEURAL NETWORK TRAINING PHASE", "text": "Given a target state xL and assuming the system starts at some initial state x\u0304, we want to design an NN that can produce a control function u(\u00b7) that drives the system from x\u0304 to xL. Equivalently, u(\u00b7) should drive the backward system from xL to x\u0304. Since the NN is not perfect, it can only be expected to produce approximate control functions u\u0302i(\u00b7) that drive the backward system from xL to x\u0302i. If the NN is well-trained, then each x\u0302i would be very close to x\u0304.\nMore concretely, consider the inverse backward dynamics of the system, denoted g : Rn \u00d7 F\u2192 U, and defined to be\ng(x\u0304, xL;\u2212f(\u00b7, \u00b7)) = u\u2217(\u00b7) (7)\nwhere the optimal control u\u2217(\u00b7) is defined in the time interval [0, \u03c6(x\u0304)]. Given the u\u2217(\u00b7), we have x\u0304 = \u03be\u2212f (\u03c6(x\u0304);xL, 0, u\u2217(\u00b7)).\nOur NN is an approximation to g, and we will denote the NN as g\u0302. Let u\u0302(\u00b7) be the control produced by g\u0302, and let the time interval for which u\u0302(\u00b7) is defined be denoted [0, T\u0302 ] . In general, applying u\u0302(\u00b7) brings the backward system from xL to some x\u0302 6= x\u0304. Regardless of what x\u0302 happens to be, T\u0302 is guaranteed to be an over-approximation of \u03c6(x\u0302): \u03c6\u0302(x\u0302) = T\u0302 \u2265 \u03c6(x\u0302). This is because the point x\u0302 is generated from simulation, and therefore the corresponding TTR T\u0302 cannot possibly be smaller than the optimal TTR \u03c6(x\u0302). If\nwe can obtain the \u03c6\u0302(x) for many values of x near x\u0304, then we would be able to synthesize a suboptimal control via (6).\nDenote the maximum value of \u03c6 we would like to approximate as T\u0304 . To reduce the space in which the NN needs to look for candidate control functions, we assume that u\u0302(\u00b7) returned by the NN is composed of two finite sequences {uj}Kj=1, {\u03c4 j}Kj=1 called the sequence of control primitives\nx\nu PRIMITIVE DURATION\n0\nn\nN\nPLANT\nx U[N]\nPRIMITIVE DURATION\n0\nn\nN\nPLANT\nOUTPUT\nn\nP [n] D [n]\nn\nP [n] ~\nn\nFigure 1: Four-layer RRNN which takes input states and outputs discretized control.\nand the sequence of time durations respectively. Mathematically, the control function u\u0302(\u00b7) is of the form\nu\u0302(t) =  u1, t \u2208 [0, \u03c41] u2, t \u2208 [\u03c41, \u03c41 + \u03c42] \u00b7 \u00b7 \u00b7 uK , t \u2208 [ \u2211N\u22121 j=1 \u03c4 j , \u2211N j=1 \u03c4 j ]\n(8)\nNote that \u2211K j=1 \u03c4\nj = T\u0302 \u2264 T\u0304 . Since we will only use the control to obtain the approximation \u03c6\u0302(\u00b7) and not for actually controlling the system, we do not need the generated controls to be extremely accurate or continuous, as we will explain in Sections 3.3 and 4."}, {"heading": "3.1 Neural Network Architecture", "text": "We propose a rectified linear unit recurrent NN (RRNN) with the following structure:\nPrimitive Layer: P [n] = \u03c8(WP \u00b7X [n] + bP )\nDuration Layer: D[n] = \u03c8(WD1 \u00b7 P [n] +WD2 \u00b7X [n] + bD)\nControl Layer: U [n] = WL \u00b7D[n] + bL Plant Layer: X [n+1] = \u03c8(WX \u00b7 U [n] + bX)\nwhere \u03c8 is the positive rectifying function defined as \u03c8(a) = max(0, a) and n = 0, 1, . . . , N . The input of the RRNN is x\u0304 (X [0] = x\u0304), and the output is some u\u0302(\u00b7) (U [N ] = u\u0302(\u00b7)), that approximately brings the system from x\u0304 to xL. The parameters we learn through training are the weights WP , WD1 , WD2 , WL, WX and biases bP , bD, bL, bX . All weights and biases will be collectively denoted W. The training is performed with the mean squared error as the cost function.\nAs already mentioned, the primitive layer takes a state as input and computes a control primitive. The duration layer takes in the primitive layer\u2019s output and the same input state, and outputs a time duration. This time duration is then passed through the control (also called output) linear layer, which outputs the sequences {uj}, {\u03c4 j} representing the control function u\u0302(\u00b7). Afterwards, the control function is fed into the plant layer, which attempst to encode the backward dynamics \u2212f . The plant\u2019s output state, X [n+1], is then fed back to the primitive layer.\nIn order to ensure WX , or the plant layer weights, are accurate, they are initialized by an isolated supervised train-\ning process. The training set we use to perform the isolated training is detailed in 3.2. Intuitively, by initializing WX , we are giving the RRNN an initial idea of the backward dynamics before learning the inverse backward dynamics. Explicitly, the RRNN can be written as\ng\u0302(x\u0304, xL;\u2212f(\u00b7, \u00b7),W) = u\u0302(\u00b7) = U [N ] (9) where u\u0302(\u00b7) is given in the form of the sequences {uj}, {\u03c4 j}, from which we can obtain T\u0302 = \u2211N j=1 \u03c4\nj . In the next section, we discuss a method for generating\ntraining data for the RRNN."}, {"heading": "3.2 Initial Training Data Generation", "text": "We first train the RRNN without knowledge of x\u0304 to initialize the dynamic training algorithm described in the next section. To do this, we require training examples in the form of {(x\u0302i, u\u0302i(\u00b7))} that sufficiently capture the behavior of the system dynamics. {x\u0302i} is randomly generated using an accept-reject algorithm similar to the one in [17] and described in Alg. 1, which outputs a boolean variable A indicating whether to accept some state x. A state x, an accept region R, and a decay rate \u03bb are required as inputs.\nIn Alg. 1, we first compute xproj, the Euclidean projection of the state x onto the set R as follows:\nxproj = min x\u2032 \u2016x\u2032 \u2212 x\u20162 : x\u2032 \u2208 R (10)\nUsing the accept-reject algorithm, we generate two training sets. The larger data set D1 is used for supervised training of the plant layer (the weights WX), and is generated with a large accept region until a certain number of points are accepted. The smaller dataset D2 is used to initialize the dynamic training algorithm, and is generated with a smaller accept region.\nAlgorithm 1 Exponential Filter\n1: Result: A 2: Inputs: x, R, \u03bb 3: compute xproj via (10) 4: generate \u03b2 uniformly from [0, \u03bb] 5: if x \u2208 R then 6: A = true 7: else 8: if \u03b2 \u2264 \u03bbe\u2212\u03bb\u2016x\u2212xproj\u20162 then 9: A = true\n10: else 11: A = false 12: end if 13: end if"}, {"heading": "3.3 Dynamic Training", "text": "The dynamic training procedure is designed to selectively prune the RRNN\u2019s inputs in order to bias its resulting controls u\u0302(\u00b7) towards bringing the backward system from xL to x\u0304. The result of dynamic training is a set of states X which contains many points near x\u0304, and the corresponding set of controls V that drives the system from the states X to xL. The procedure is shown in detail in Alg. 2, and can bebroken down into the following steps:\n1. Initialization (lines 3-5)\n2. Train RRNN by updating the weights W (line 8)\n3. Filter RRNN outputs Xnew in current iteration (lines 9-24)\n4. Filter RRNN training set X from previous iterations (line 25-32)\n5. Filter data from initial training set X \u2229 D2, and then update the training sets X ,V. (line 33-47)\n6. Repeat steps 2-5 until either a maximum iteration limit is reached or enough states in X are within a distance of to x\u0304.\n3.3.1 Initialization Before the initialization of Alg. 2, the RRNN\u2019s plant layer\nhas been trained on D1 in isolation. Dynamic training initialization requires the following inputs: the dataset D2, the state x\u0304, the desired accept region radius , the number of desired samples M in that region, the iteration iterps after which we want to activate pruning of the D2, the maximum desired number of states Mmax near x\u0304, and the maximum number of training iterations itermax. In addition, three different accept regions, denoted RO,RPS,RITS, are required. Suggestions on choosing RO,RPS,RITS for the Dubins car example are given in Section 5.4.1.\nWe first initialize X to be X = D2. Next, we uniformly sample M states within a distance of to x\u0304 to produce a set of states denoted E .\n3.3.2 Neural Network and Output Filtering We now input the states in E to the RRNN to produce M\ncontrols, {u\u0302i(\u00b7)}M i=1, from which we compute the resulting set of states {x\u0302i}M i=1 according to the dynamics (1). Next, we add {x\u0302i}M i=1 to a temporary set denoted Xnew.\nTo avoid training on data points far from the target, the exponential accept-reject filter (Alg. 1) is applied to the states in Xnew, with the accept region RO. RO depends on the system dynamics and is generally tuned; usually, it should contains states that could lie on a feasible path from x\u0304 to xL. For general high-dimensional systems with complicated dynamics, it may be difficult to estimate RO, and RO can be tuned based on the performance of RRNN.\n3.3.3 Previous State Filtering This part of the algorithm filters out states from previous\ntraining iterations using the accept region RPS. Such a filter reduces the likelihood that old data from many iterations ago affect the performance of the RRNN adversely.\n3.3.4 Initial Training Set Filtering In early iterations of training, states from the initial train-\ning set D2 provide the general information about the system dynamics needed for the RRNN to explore the state space. However, after sufficient exploration, the RRNN gains information about the system dynamics specific for reaching x\u0304; thus, the data in D2 becomes less important. Hence, once the number of training iterations exceeds a threshold iterps, we prune states from the D2 using the accept region denoted by RITS, chosen to remove states in D2 that are far from xL.\nAt this point in the algorithm, we also update the set X by adding to it the states in Xnew. We do the same for the corresponding set of controls V. We then feed the updated X and V back into the RRNN and the algorithm begins a new iteration."}, {"heading": "4. CONTROLLER SYNTHESIS PHASE", "text": "After the dynamic training procedure completes, we obtain the set of states X near x\u0304 and corresponding controls V. From each state x\u0302i \u2208 X and control function u\u0302i(\u00b7) \u2208 V, we directly have the approximate TTR values: \u03c6\u0302(xi) = T\u0302i. These approximate TTR values can be used to synthesize a control at x\u0304 according to (6). However, in order to bring the system from x\u0304 all the way to xL, a few more post-processing steps are needed."}, {"heading": "4.1 Trajectory Tracing", "text": "In order to drive the system from x\u0304 to xL, gradient information is necessary at points between x\u0304 and xL along a dynamically feasible trajectory. Fortunately, this information can be computed from X and V. Specifically, each x\u0302i \u2208 X and u\u0302i(\u00b7) \u2208 V produce a trajectory \u03befi (t; x\u0302i, 0, u\u0302i(\u00b7)), t \u2208 [0, T\u0302i]. From the trajectories, we can obtain MT states on the trajectory by discretizing the time t into MT time points. We denote these states x(i,j), where the index i comes from the index of x\u0302i \u2208 X , and the index j \u2208 {0, . . . ,MT \u22121} indicates that the state is computed from the jth time point on the trajectory \u03befi . Mathematically, x\u0302(i,j) is given as follows:\nx\u0302(i,j) = \u03bei(tj ; x\u0302i, 0, u\u0302i(\u00b7)),\ntj = jT\u0302i\nMT \u2212 1 , j = 0, 1, . . . ,MT \u2212 1\n(11)\nThe trajectory tracing process is summarized in Alg. 3, where we have denoted the set {x(i,j)} as X\u0304 ."}, {"heading": "4.2 Imposing a Grid", "text": "While the source of the exponential time and space complexity problem lies in discretization of the state space, imposing a grid can still be useful as long as the number of grid points is small. Thus, we organize the approximate TTR values \u03c6\u0302(x), x \u2208 X\u0304 by imposing a small grid G(x) around a state x at which the a control needs to be synthesized, starting with x = x\u0304. As the system moves from x\u0304 to xL, we move the grid G(x) along so that it is always centered at the current system state x(t) and has a resolution suitable for the local density of points near x in X\u0304 . This is computationally feasible since the grid does not cover a large part of the state space.\nAfter the local grid G(x) is created, each state x\u0302i that lies within the bounds of G(x) gets mapped to the nearest grid point. Depending on the resolution of G(x) and the\nAlgorithm 2 Dynamic Training\n1: Results: states near x\u0304, X ; corresponding controls, V 2: Inputs: x\u0304, , M , itermax, Mmax, iterps, D2, RO, RPS, RITS 3: Initialization: 4: X = D2, iter = 0 5: E \u2190M uniformly sampled states from -ball around x\u0304 6: while iter < itermax do 7: iter\u2190 iter + 1 8: W\u2190 trainNN(X ,V) 9: NN output filtering:\n10: Vnew,Xnew \u2190 \u2205 11: for xi \u2208 E do 12: u\u0302(\u00b7)\u2190 g\u0302(xi, xL;\u2212f(\u00b7, \u00b7),W) 13: Vnew \u2190 Vnew \u222a {(u\u0302(\u00b7)} 14: end for 15: for u\u0302i(\u00b7) \u2208 Vnew do 16: Xnew \u2190 Xnew \u222a {\u03be(T\u0302 ;xL, 0; u\u0302i(\u00b7))} 17: end for 18: for xi \u2208 Xnew do 19: A = Filter(xi,RO) 20: if !A then 21: Xnew \u2190 Xnew \\ xi 22: Vnew \u2190 Vnew \\ ui (corresponding control) 23: end if 24: end for 25: Previous state filtering: 26: for xi \u2208 X do 27: A = Filter(xi,RPS) 28: if !A then 29: X \u2190 X \\ xi 30: V \u2190 V \\ ui (corresponding control) 31: end if 32: end for 33: Initial training set filtering: 34: if iter > iterps then 35: for xi \u2208 X \u2229 D2 do 36: A = Filter(xi,RITS) 37: if !A then 38: X \u2190 X \\ xi 39: V \u2190 V \\ ui (corresponding control) 40: end if 41: end for 42: end if 43: X \u2190 X \u222a Xnew 44: V \u2190 V \u222a Vnew 45: if |{x \u2208 X : \u2016x\u2212 xL\u20162 \u2264 }| \u2265Mmax then 46: return 47: end if 48: end while\nAlgorithm 3 Trajectory Tracing\n1: Result: Points in region around dynamically feasible trajectory, X\u0304 2: Inputs: X , V, MT 3: Initialization: X\u0304 \u2190 \u2205 4: Algorithm: 5: for x\u0302i \u2208 X do 6: for j \u2208 {0, 1, . . . ,MT \u2212 1} do 7: compute x\u0302(i,j) using (11) 8: X\u0304 \u2190 X\u0304 \u222a {x\u0302(i,j)} 9: end for\n10: end for\ndensity of X\u0304 around x, multiple states {x(i,j)} \u2282 X\u0304 may be mapped to the same grid point, which for convenience we will denote xG . When this occurs, we assign \u03c6\u0302(xG) to be the minimum out of the corresponding TTR approximations \u03c6\u0302(x\u0302(i,j)). If no points are mapped to some grid point xG , then \u03c6\u0302(xG) =\u221e for that point. Fig. 3 illustrates the process of imposing the grid. Blue dots correspond to states x\u0302(i,j) \u2208 X\u0304 , and numbers adjacent to the blue dots represent \u03c6\u0302(x\u0302(i,j)). Red dots correspond to\ngrid points, and the adjacent number represents \u03c6\u0302(x\u0302(i,j)). The shaded region surrounding each grid point corresponds to the locations in which states are mapped to that particular grid point."}, {"heading": "4.3 Smart Gradient Approximation", "text": "The next step for driving the system to xL is computing the gradients of the approximate TTR function, \u2207\u03c6\u0302(x), so that (6) can be used. Since we have the grid G(x) and approximate TTR values \u03c6\u0302(x) on the grid, standard numerical methods for computing the gradient can in principle be used. However, depending on the set X\u0304 , in general not every grid point of G(x) can be assigned a \u03c6\u0302(x).\nAlg. 4 provides a heuristic for approximating the gradient on a grid whose grid points do not all contain a function value. This is done by selecting grid points near x carefully based on the availability of function value, and then approximating a component of the gradient by taking a first-order forward, backward, or central difference. When needed, function values are extrapolated.\nIn the algorithm, we assume that the gradient in kth component, \u2207k\u03c6\u0302(x) is computed. The functions in the algorithm are named according to their output and are simple to implement. These are listed below:\n\u2022 data above, data below: returns all available points on G respectively \u201cabove\u201d and \u201cbelow\u201d x in the kth component.\n\u2022 closest above, closest below: returns the closest point on G respectively \u201cabove\u201d and \u201cbelow\u201d x in the kth component.\n\u2022 second closest above, second closest below: returns the second closest point on G respectively \u201cabove\u201dand\u201cbelow\u201d x in the kth component."}, {"heading": "4.4 Control Computation", "text": "Repeatedly using Alg. 4 for all the components of the system dimension, we can obtain \u2207\u03c6\u0302(x), from which a suboptimal control can be computed via (6). In practice, depending on how the control appears in the system dynamics, some components of the gradients may not be needed to derive a control. In this case, only the gradient components necessary for computing the control according to (6) are needed.\nAlgorithm 4 Gradient Approximation\n1: Result: \u2207k\u03c6\u0302(x) 2: Inputs: G, {\u03c6\u0302(x\u0302(i,j))}, X\u0304 3: Initialization: 4: above \u2190 data above(G, X\u0304 ) 5: below \u2190 data below(G, X\u0304 ) 6: if |above|! = 0 and |below|! = 0 then 7: x+ \u2190 closest above(G, X\u0304 ) 8: x\u2212 \u2190 closest below(G, X\u0304 ) 9: else if |above| == 0 and |below|! = 0 then 10: if |below| < 2 then 11: \u2207k\u03c6\u0302(x)\u2190\u221e 12: return 13: else 14: x+ \u2190 closest below(G, X\u0304 ) 15: x\u2212 \u2190 second closest below(G, X\u0304 ) 16: end if 17: else if |below| == 0 and |above|! = 0 then 18: if |above| < 2 then 19: \u2207k\u03c6\u0302(x)\u2190 \u2212\u221e 20: return 21: else 22: x+ \u2190 second closest above(G, X\u0304 ) 23: x\u2212 \u2190 closest above(G, X\u0304 ) 24: end if 25: else 26: \u2207k\u03c6\u0302(x) cannot be determined 27: end if 28: \u2207k\u03c6\u0302(x)\u2190 \u03c6\u0302(x+)\u2212\u03c6\u0302(x\u2212)x+\u2212x\u2212"}, {"heading": "5. DUBINS CAR EXAMPLE", "text": ""}, {"heading": "5.1 Vehicle Dynamics", "text": "Consider the Dubins Car, with state x = (px, py, \u03b8). (px, py) are the x and y positions of the vehicle, and \u03b8 is the heading\nof the vehicle. The system dynamics, assuming unit longitudinal speed, are\np\u0307x = cos \u03b8\np\u0307y = sin \u03b8\n\u03b8\u0307 = u, |u| \u2264 1 (12)\nThe control of the Dubins car is denoted u, and is constrained to lie in the interval [\u22121, 1], the interpretation of which is that the vehicle has a maximum turn rate of 1 rad/s. We choose the Dubins car to illustrate our method because of the simple structure of the optimal controls. In addition, since the model is only 3D, we are able to verify our results by comparing them to the those obtained via HJ reachability.\nFor our example, we have chosen many different initial states x\u0304 for the system. The target state xL is chosen as the origin."}, {"heading": "5.2 Control Primitives", "text": "In [19], the author shows that all optimal trajectories of the Dubins car utilize controls that represent going straight or turning maximally left or right. Thus, the set of controls that are valid for generating optimal trajectories can be reduced down to three motion primitives, {\u2018L\u2019, \u2018S\u2019, \u2018R\u2019}, encoding the controls u = 1 (max left), u = 0 (straight), and u = \u22121 (max right) respectively. Furthermore, [19] also shows that for any given x\u0304 and xL, the optimal control is a sequence of at most three of these motion primitives. In other words, the optimal control for any given target state is simply a permutation of {\u2018L\u2019, \u2018S\u2019, \u2018R\u2019} with an appropriate\ntime duration associated with each primitive. Since these motion primitives correspond to arcs and line segments, the optimal trajectories for Dubins car can be geometrically interpreted as concatenated sequences of arcs and line segments.\nFollowing our notation in Section 3, any optimal control sequence can be written as {u1, u2, u3}, {\u03c41, \u03c42, \u03c43} where ui denotes the ith control primitive and \u03c4 i denotes the duration of ith control primitive. We choose ui from the three possible optimal values {\u22121, 0, 1}. The initial dataset D2 for dynamic training is composed of control primitives with duration randomly sampled from the interval [0, T ], T = 2\u03c0. With our choice of T , the maximum time horizon considered is then 3T = 6\u03c0."}, {"heading": "5.3 Neural Network", "text": "Using the RRNN architecture described in Section 3.1, we let N = 3, since we only need at most three control primitives. Since the controls and dynamics of Dubins car are simple, we have chosen the hidden sizes of P,D,U,X to be [10, 10, 6, 75], respectively. The NN is trained with the training functionality of the MATLAB Neural Network ToolBox 2016. The training function used for the full NN is resilient back-propagation and the performance function used is mean squared error."}, {"heading": "5.4 Neural Network Training", "text": "Running the dynamic training algorithm on even the lowdimensional Dubins Car results in significant memory usage savings compared to using dynamic programming-based methods. As the NN learns the inverse backward system dynamics g, a dynamically feasible \u201ccorridor\u201d between the initial state x\u0304 and target state xL is picked out, as shown in Fig. 4. Without our proposed method, if one chooses some arbitrary region enclosing x\u0304 and xL, and naively computes the TTR function \u03c6(x) using level set methods, it is unlikely that the a solution can be found throughout the entire corridor, since the region may not contain dynamically feasible trajectories.\nBefore dynamic training, we first initialize the weights WX . This is done using the dataset D1, which is generated by random sampling. Exploiting the fact that there are only six optimal control primitives for the Dubins car, we sample the u = \u00b11 primitives 25 times less frequently than the u = 0 primitive. For the possible durations of each primitive, \u03c4 i, we randomly sampled in the interval [0, T ], T = 100 according to a linearly increasing probability distribution on [0, T ]. This distribution is chosen so that starting from xL, the backward dynamics drive the system to a relatively uniform distribution of states x\u0304.\nFor generating the dataset D2, we follow a similar procedure to generating D1, except we sample time durations \u03c4 i in the interval [0, T ], T = 2\u03c0.\nTo initialize the dynamic training process, we chose = 1, and M = 500, itermax = 50, Mmax = 10000, iterps = 5.\n5.4.1 Filtering Algorithms For the exponential filtering process, RO is chosen to be\nthe cone of minimum size that contains the epsilon ball, with the tip of the cone located at x\u0304. The choice of using a conical filter is based on the hypothesis that a trajectory taking the system from x\u0304 to xL is likely to stay in the cone RO. RPS was chosen to be a sphere around x\u0304 to serve as a\nmore stringent filter than the cone filter. RITS was chosen to be a spherical target filter as well, since the initial training set is supposed to be pruned only when the NN has already learned to drive the system to x\u0304 reasonably well. The conical and spherical target filters are shown in Fig. 5a and Fig. 5b respectively.\n5.4.2 Filter Decay Rate: Setting and Timing Although RO,RPS,RITS are chosen before the dynamic\ntraining process, the filtering of the training set X can still be adjusted while training. This is done by varying the parameters \u03bbO, \u03bbPS, \u03bbITS. In early training iterations, we want to decrease \u03bbO slightly to ensure that we are not filtering out states needed for the NN to explore the state space. Once the NN has gained a better understanding of how to reach x\u0304, we increase \u03bbO and \u03bbITS slightly to further encourage the NN to drive states near x\u0304. When the dataset is mostly near x\u0304, we increase \u03bbPS and \u03bbITS significantly."}, {"heading": "5.5 Dubins Car Results", "text": "5.5.1 Training Process In Fig. 6a, 6b, 6c, the process by which the training set X\nchanges from the initial training set D2 to an -ball around x\u0304 = (15, 12, 3) is shown. Here, the red states represent the set E and the black states represent the states in X . In the\nearly iterations (Fig. 6a), the NN explores outward from the initial training set, frequently making mistakes, resulting in the states in X being very far away from x\u0304. As the iteration number increases, the training set D2 is filtered out, and eventually the NN tends to output controls u\u0302i(\u00b7) that produces states x\u0302i in an arc. This can be seen in Fig. 6b. By the end of the training process, the RO conical target filter prunes the states outside of the -ball. This can be seen in Fig. 6c.\n5.5.2 TTR Value Comparison The true TTR values \u03c6(x) and the approximate TTR val-\nues \u03c6\u0302(x) determined by the NN output of several states are compared in Table 1.\nThe difference between \u03c6(x) and \u03c6\u0302(x) is not very large. In\nfact, \u03c6\u0302(x) is sometimes even smaller than \u03c6(x). Our method\nguarantees that \u03c6\u0302(x) is a conservative approximation \u2013 an over-approximation. The larger \u03c6(x) values are likely due to numerical error in the level set methods used. In our particular example, the target set is a single state, making a level set representation of the target set very difficult. However, a singleton target set does not limit our NN-based approach in any way.\n5.5.3 Computation Complexity Synthesizing control using our NN-based approach allows\nfor large time complexity improvements in comparison to using level set methods. On a 2012 MacBook Pro laptop, data generation requires approximately 3 minutes, and controller synthesis from this data and simulation requires approximately 23 minutes. Since the region of the state space we are considering is quite large, and the target set is quite small (a singleton), the level set methods approach is intractable on this laptop, and requires 4 days on a desktop computer with a Core i7-5820K processor and 128 GB of RAM.\nThere are also large spatial savings by using the NN. For example, X and \u03c6\u0302(x), x \u2208 X for one particular corridor computed between (10,\u22121,\u22123) and (0, 0, 0) requires only 179 MB of RAM, while a reachable set computed over that horizon on a very low resolution grid requires approximately 7 GB of RAM.\nAs can be seen from this and the previous sections, using level set methods not only is more time-consuming compared to using our NN-based approach, but also does not guarantee a more shorter trajectory due to discretization error."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "Our NN-based grid-free method computes an upper bound of the optimal TTR function in a region of the state space that contains the initial state, the target set, and a feasible trajectory. By combining the strengths of dynamic programming-based and machine learning-based approaches,\nwe greatly alleviate the curse of dimensionality while maintaining a desired direction of conservatism, effectively avoiding the shortcomings of both types of approaches.\nUsing a numerical example, we demonstrate that our approach can successfully generate near-optimal TTR values in multiple test cases for the Dubins car. Due to our method being grid-free, in some cases the estimated TTR values are even smaller than the those computed by level set methods, which suffer from discretization error. In addition, we are able to approximate TTR values in regions that are very far from the target set, a very computationally expensive task for dynamic programming-based approaches. Our approximate TTR function is able to drive the Dubins car from many different initial conditions to the target set.\nAlthough our current results are promising, much more investigation is still needed to make our approach more practical and applicable to more scenarios. For example, better intuition for the choice of accept regions in the filtering process is needed to extend our approach to other systems. We currently plan to investigate applying our method to the 6D engine-out plane [1] as well as a 12D quadrotor model. In addition to path planning, we also hope to extend our theory to provide safety guarantees and robustness against disturbances. Such extensions are non-trivial due to the different roles that the control and disturbance inputs play in the system dynamics."}, {"heading": "7. REFERENCES", "text": "[1] Adler, A., Bar-Gill, A., and Shimkin, N. Optimal\nflight paths for engine-out emergency landing. In Proceedings of the 2012 24th Chinese Control and Decision Conference, CCDC 2012 (may 2012), IEEE, pp. 2908\u20132915.\n[2] Allen, R. E., Clark, A. A., Starek, J. A., and Pavone, M. A Machine Learning Approach for Real-Time Reachability Analysis Ross. In International Conference on Intelligent Robots and Systems (sep 2014), no. Iros, IEEE, pp. 2202\u20132208.\n[3] Amazon. Amazon Prime Air. http://www.amazon.com/b?node=8037720011. Accessed: 2016-10-09.\n[4] AUVSI News. UAS Aid in South Carolina Tornado Investigation. http://www.auvsi.org/blogs/ auvsi-news/2016/01/29/tornado. Accessed: 2016-10-09.\n[5] Barron, E. N. Differential games with maximum cost. Nonlinear Analysis 14, 11 (jun 1990), 971\u2013989.\n[6] Barron, E. N., and Ishii, H. The Bellman equation for minimizing the maximum cost. Nonlinear Analysis 13, 9 (1989), 1067\u20131090.\n[7] BBC News. Google plans drone delivery service for 2017. http://www.bbc.co.uk/news/technology-34704868, 2015. Accessed: 2016-10-09.\n[8] Becerikli, Y., Konar, A. F., and Samad, T. Intelligent optimal control with dynamic neural networks. Neural networks : the official journal of the International Neural Network Society 16, 2 (mar 2003), 251\u20139.\n[9] Bokanowski, O., Forcadel, N., and Zidani, H. Reachability and Minimal Times for State Constrained Nonlinear Problems without Any Controllability Assumption. SIAM Journal on Control and Optimization 48, 7 (jan 2010), 4292\u20134316.\n[10] Bokanowski, O., and Zidani, H. Minimal time problems with moving targets and obstacles. IFAC Proceedings Volumes (IFAC-PapersOnline) 18, PART 1 (2011), 2589\u20132593.\n[11] Chen*, M., Herbert*, S., and Tomlin, C. J. Fast Reachable Set Approximations via State Decoupling Disturbances. In 55th IEEE Conference on Decision and Control (to appear) (2016).\n[12] Chen, M., Hu, Q., Mackin, C., Fisac, J. F., and Tomlin, C. J. Safe platooning of unmanned aerial vehicles via reachability. In 2015 54th IEEE Conference on Decision and Control (CDC) (dec 2015), vol. 2016-Febru, IEEE, pp. 4695\u20134701.\n[13] Crandall, M. G., and Lions, P.-l. Viscosity solutions of Hamilton-Jacobi equations. Transactions of the American Mathematical Society 277, 1 (jan 1983), 1\u20131.\n[14] Cybenko, G. Approximation by Superposition of a Sigmoidal Function.\n[15] Darbon, J., and Osher, S. Algorithms for overcoming the curse of dimensionality for certain Hamiltona\u0302A\u0306S\u0327Jacobi equations arising in control theory and elsewhere. Research in the Mathematical Sciences 3, 1 (dec 2016), 19.\n[16] Ding, J., Sprinkle, J., Sastry, S. S., and Tomlin, C. J. Reachability calculations for automated aerial refueling. In Proceedings of the IEEE Conference on Decision and Control (Cancun, Mexico, 2008), pp. 3706\u20133712.\n[17] Djeridane, B., and Lygeros, J. Neural approximation of PDE solutions: An application to reachability computations. In Proceedings of the 45th IEEE Conference on Decision and Control (2006), IEEE, pp. 3034\u20133039.\n[18] Dreossi, T., Dang, T., and Piazza, C. Parallelotope Bundles for Polynomial Reachability. In\nProceedings of the 19th International Conference on Hybrid Systems: Computation and Control - HSCC \u201916 (New York, New York, USA, 2016), ACM Press, pp. 297\u2013306.\n[19] Dubins, L. E. On Curves of Minimal Length with a Constraint on Average Curvature, and with Prescribed Initial and Terminal Positions and Tangents. American Journal of Mathematics 79, 3 (jul 1957), 497\u2013516.\n[20] Evans, L. C., and Souganidis, P. E. Differential games and representation formulas for solutions of {Hamilton-Jacobi-Isaacs} equations. Indiana Univ. Math. J. 33, 5 (1984), 773\u2013797.\n[21] Fehrenbacher, K. Feds Say Safety Is the Key to the Future of Autonomous Cars. http://fortune.com/ 2016/07/19/safety-feds-autonomous-cars/. Accessed: 2016-10-09.\n[22] Fisac, J. F., Chen, M., Tomlin, C. J., and Sastry, S. S. Reach-avoid problems with time-varying dynamics, targets and constraints. In Proceedings of the 18th International Conference on Hybrid Systems Computation and Control - HSCC \u201915 (New York, New York, USA, 2015), ACM Press, pp. 11\u201320.\n[23] Joint Planning and Development Office. Unmanned Aircraft Systems (UAS) Comprehensive Plan: A Report on the Nation\u2019s UAS Path Forward. Tech. rep., 2013.\n[24] Kim, B. S., and Calise, A. J. Nonlinear Flight Control Using Neural Networks. Journal of Guidance, Control, and Dynamics 20, 1 (jan 1997), 26\u201333.\n[25] Kopardekar, P., Rios, J., Prevot, T., Johnson, M., Jung, J., and Iii, J. E. R. Unmanned Aircraft System Traffic Management ( UTM ) Concept of Operations. 16th AIAA Aviation Technology, Integration, and Operations Conference, 1\u201316.\n[26] Majumdar, A., Vasudevan, R., Tobenkin, M. M., and Tedrake, R. Convex optimization of nonlinear feedback controllers via occupation measures. The International Journal of Robotics Research 33, 9 (aug 2014), 1209\u20131230.\n[27] Margellos, K., and Lygeros, J. Hamilton-Jacobi Formulation for Reach-Avoid Differential Games. IEEE Transactions on Automatic Control 56, 8 (aug 2011), 1849\u20131861.\n[28] McGrew, J., Bush, L., How, J., Roy, N., and Williams, B. Air Combat Strategy Using Approximate Dynamic Programming. AIAA Guidance, Navigation and Control Conference and Exhibit (aug 2008), 1\u201333.\n[29] Mitchell, I. A toolbox of level set methods. Tech. rep., 2007.\n[30] Mitchell, I. M., Bayen, A. M., and Tomlin, C. J. A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games. IEEE Transactions on Automatic Control 50, 7 (2005), 947\u2013957.\n[31] Mitchell, I. M., and Tomlin, C. J. Overapproximating Reachable Sets by Hamilton-Jacobi Projections. Journal of Scientific Computing 19, 1-3 (2003), 323\u2013346.\n[32] Niarchos, K. N., and Lygeros, J. A Neural Approximation to Continuous Time Reachability Computations. In Proceedings of the 45th IEEE Conference on Decision and Control (2006), IEEE, pp. 6313\u20136318.\n[33] Osher, S., and Fedkiw, R. Level Set Methods and Dynamic Implicit Surfaces. Springer-Verlag, 2006.\n[34] Sethian, J. A. A fast marching level set method for monotonically advancing fronts. Pnas 93, 4 (1996), 1591\u20131595.\n[35] Tomlin, C. J., Lygeros, J., and Sastry, S. A Game Theoretic Approach to Controller Design for Hybrid Systems. Proceedings of IEEE 88, 7 (jul 2000), 949\u2013969.\n[36] Varaiya, P. On the existence of solutions to a differential game. SIAM Journal on Control 5, 1 (1967), 153\u2013162.\n[37] Yang, I., Becker-Weimann, S., Bissell, M. J., and Tomlin, C. J. One-shot computation of reachable sets for differential games. In Proceedings of the 16th international conference on Hybrid systems: computation and control - HSCC \u201913 (New York, New York, USA, 2013), ACM Press, p. 183."}], "references": [{"title": "Optimal flight paths for engine-out emergency landing", "author": ["A. Adler", "A. Bar-Gill", "N. Shimkin"], "venue": "In Proceedings of the 2012 24th Chinese Control and Decision Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A Machine Learning Approach for Real-Time Reachability Analysis Ross", "author": ["R.E. Allen", "A.A. Clark", "J.A. Starek", "M. Pavone"], "venue": "In International Conference on Intelligent Robots and Systems (sep", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Differential games with maximum cost", "author": ["E.N. Barron"], "venue": "Nonlinear Analysis", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "The Bellman equation for minimizing the maximum cost", "author": ["E.N. Barron", "H. Ishii"], "venue": "Nonlinear Analysis 13,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Google plans drone delivery service for 2017", "author": ["BBC News"], "venue": "http://www.bbc.co.uk/news/technology-34704868", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Intelligent optimal control with dynamic neural networks", "author": ["Y. Becerikli", "A.F. Konar", "T. Samad"], "venue": "Neural networks : the official journal of the International Neural Network Society", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Reachability and Minimal Times for State Constrained Nonlinear Problems without Any Controllability Assumption", "author": ["O. Bokanowski", "N. Forcadel", "H. Zidani"], "venue": "SIAM Journal on Control and Optimization", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Minimal time problems with moving targets and obstacles", "author": ["O. Bokanowski", "H. Zidani"], "venue": "IFAC Proceedings Volumes (IFAC-PapersOnline) 18, PART", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Fast Reachable Set Approximations via State Decoupling Disturbances", "author": ["M. Chen", "S. Herbert", "C.J. Tomlin"], "venue": "In 55th IEEE Conference on Decision and Control (to appear)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Safe platooning of unmanned aerial vehicles via reachability", "author": ["M. Chen", "Q. Hu", "C. Mackin", "J.F. Fisac", "C.J. Tomlin"], "venue": "In 2015 54th IEEE Conference on Decision and Control (CDC) (dec 2015), vol. 2016-Febru,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Viscosity solutions of Hamilton-Jacobi equations", "author": ["M.G. Crandall", "Lions", "P.-l"], "venue": "Transactions of the American Mathematical Society 277,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1983}, {"title": "Algorithms for overcoming the curse of dimensionality for certain Hamilton\u00e2\u0102\u015eJacobi equations arising in control theory and elsewhere", "author": ["J. Darbon", "S. Osher"], "venue": "Research in the Mathematical Sciences", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Reachability calculations for automated aerial refueling", "author": ["J. Ding", "J. Sprinkle", "S.S. Sastry", "C.J. Tomlin"], "venue": "In Proceedings of the IEEE Conference on Decision and Control (Cancun, Mexico,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Neural approximation of PDE solutions: An application to reachability computations", "author": ["B. Djeridane", "J. Lygeros"], "venue": "In Proceedings of the 45th IEEE Conference on Decision and Control (2006),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Parallelotope Bundles for Polynomial Reachability", "author": ["T. Dreossi", "T. Dang", "C. Piazza"], "venue": "Proceedings of the 19th International Conference on Hybrid Systems: Computation and Control - HSCC \u201916", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "On Curves of Minimal Length with a Constraint on Average Curvature, and with Prescribed Initial and Terminal Positions and Tangents", "author": ["L.E. Dubins"], "venue": "American Journal of Mathematics 79,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1957}, {"title": "Differential games and representation formulas for solutions of {Hamilton-Jacobi-Isaacs} equations", "author": ["L.C. Evans", "P.E. Souganidis"], "venue": "Indiana Univ. Math. J. 33,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1984}, {"title": "Feds Say Safety Is the Key to the Future of Autonomous Cars", "author": ["K. Fehrenbacher"], "venue": "http://fortune.com/ 2016/07/19/safety-feds-autonomous-cars/", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Reach-avoid problems with time-varying dynamics, targets and constraints", "author": ["J.F. Fisac", "M. Chen", "C.J. Tomlin", "S.S. Sastry"], "venue": "In Proceedings of the 18th International Conference on Hybrid Systems Computation and Control - HSCC", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Nonlinear Flight Control Using Neural Networks", "author": ["B.S. Kim", "A.J. Calise"], "venue": "Journal of Guidance, Control, and Dynamics 20,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Convex optimization of nonlinear feedback controllers via occupation measures", "author": ["A. Majumdar", "R. Vasudevan", "M.M. Tobenkin", "R. Tedrake"], "venue": "The International Journal of Robotics Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Hamilton-Jacobi Formulation for Reach-Avoid Differential Games", "author": ["K. Margellos", "J. Lygeros"], "venue": "IEEE Transactions on Automatic Control", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Air Combat Strategy Using Approximate Dynamic Programming. AIAA Guidance, Navigation and Control Conference and Exhibit (aug", "author": ["J. McGrew", "L. Bush", "J. How", "N. Roy", "B. Williams"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "A toolbox of level set methods", "author": ["I. Mitchell"], "venue": "Tech. rep.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "A time-dependent Hamilton-Jacobi formulation of reachable sets for continuous dynamic games", "author": ["I.M. Mitchell", "A.M. Bayen", "C.J. Tomlin"], "venue": "IEEE Transactions on Automatic Control 50,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Overapproximating Reachable Sets by Hamilton-Jacobi Projections", "author": ["I.M. Mitchell", "C.J. Tomlin"], "venue": "Journal of Scientific Computing 19,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "A Neural Approximation to Continuous Time Reachability Computations", "author": ["K.N. Niarchos", "J. Lygeros"], "venue": "In Proceedings of the 45th IEEE Conference on Decision and Control (2006),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A fast marching level set method for monotonically advancing fronts", "author": ["J.A. Sethian"], "venue": "Pnas 93,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1996}, {"title": "A Game Theoretic Approach to Controller Design for Hybrid Systems", "author": ["C.J. Tomlin", "J. Lygeros", "S. Sastry"], "venue": "Proceedings of IEEE 88,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "On the existence of solutions to a differential game", "author": ["P. Varaiya"], "venue": "SIAM Journal on Control", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1967}, {"title": "One-shot computation of reachable sets for differential games. In Proceedings of the 16th international conference on Hybrid systems: computation and control - HSCC", "author": ["I. Yang", "S. Becker-Weimann", "M.J. Bissell", "C.J. Tomlin"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "In particular, there has been vast interest in the development of autonomous cars and unmanned aerial vehicles (UAVs) for civilian purposes [3,4,7,21,23,25].", "startOffset": 140, "endOffset": 156}, {"referenceID": 17, "context": "In particular, there has been vast interest in the development of autonomous cars and unmanned aerial vehicles (UAVs) for civilian purposes [3,4,7,21,23,25].", "startOffset": 140, "endOffset": 156}, {"referenceID": 2, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 6, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 16, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 18, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 21, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 24, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 28, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 29, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 35, "endOffset": 65}, {"referenceID": 9, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 134, "endOffset": 144}, {"referenceID": 12, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 134, "endOffset": 144}, {"referenceID": 24, "context": "It has been extensively studied in [5, 9, 20, 22, 27, 30, 35, 36], and successfully applied to many lowdimensional real world systems [12,16,30].", "startOffset": 134, "endOffset": 144}, {"referenceID": 14, "context": "For example, one method requires the system dynamics to be polynomial [18, 26], while another method requires the Hamiltonian to be a function of only the controlled variable [15].", "startOffset": 70, "endOffset": 78}, {"referenceID": 20, "context": "For example, one method requires the system dynamics to be polynomial [18, 26], while another method requires the Hamiltonian to be a function of only the controlled variable [15].", "startOffset": 70, "endOffset": 78}, {"referenceID": 11, "context": "For example, one method requires the system dynamics to be polynomial [18, 26], while another method requires the Hamiltonian to be a function of only the controlled variable [15].", "startOffset": 175, "endOffset": 179}, {"referenceID": 8, "context": "There are also less restrictive methods using projections, approximate dynamic programming, and approximate systems decoupling [11, 28, 31], each with its own limitations in flexibility, scalability, and degree of conservatism.", "startOffset": 127, "endOffset": 139}, {"referenceID": 22, "context": "There are also less restrictive methods using projections, approximate dynamic programming, and approximate systems decoupling [11, 28, 31], each with its own limitations in flexibility, scalability, and degree of conservatism.", "startOffset": 127, "endOffset": 139}, {"referenceID": 25, "context": "There are also less restrictive methods using projections, approximate dynamic programming, and approximate systems decoupling [11, 28, 31], each with its own limitations in flexibility, scalability, and degree of conservatism.", "startOffset": 127, "endOffset": 139}, {"referenceID": 5, "context": "For instance, [8] and [24] use neural networks (NNs) as nonlinear optimizers to synthesize trajectories which may not be dynamically feasible.", "startOffset": 14, "endOffset": 17}, {"referenceID": 19, "context": "For instance, [8] and [24] use neural networks (NNs) as nonlinear optimizers to synthesize trajectories which may not be dynamically feasible.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "The authors in [2] propose a supervised learningbased algorithm that depends heavily on feature tuning and design, making its application to high-dimensional problems cumbersome.", "startOffset": 15, "endOffset": 18}, {"referenceID": 13, "context": "In [17] and [32], the authors successfully use NNs for approximating the value function, but the approximation is not guaranteed to be conservative.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [17] and [32], the authors successfully use NNs for approximating the value function, but the approximation is not guaranteed to be conservative.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "There are many HJ formulations for obtaining the TTR function \u03c6(x) [6, 10, 30].", "startOffset": 67, "endOffset": 78}, {"referenceID": 7, "context": "There are many HJ formulations for obtaining the TTR function \u03c6(x) [6, 10, 30].", "startOffset": 67, "endOffset": 78}, {"referenceID": 24, "context": "There are many HJ formulations for obtaining the TTR function \u03c6(x) [6, 10, 30].", "startOffset": 67, "endOffset": 78}, {"referenceID": 10, "context": "For example, \u03c6(x) is the viscosity solution [13] of the following HJ PDE [37]:", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "For example, \u03c6(x) is the viscosity solution [13] of the following HJ PDE [37]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "In general, numerical methods for finding \u03c6(x) such as the one outlined in [37], as well as many others [29, 33, 34], involve representing the state space on a grid, leading to an exponentially scaling computation complexity with respect to the system dimension.", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "In general, numerical methods for finding \u03c6(x) such as the one outlined in [37], as well as many others [29, 33, 34], involve representing the state space on a grid, leading to an exponentially scaling computation complexity with respect to the system dimension.", "startOffset": 104, "endOffset": 116}, {"referenceID": 27, "context": "In general, numerical methods for finding \u03c6(x) such as the one outlined in [37], as well as many others [29, 33, 34], involve representing the state space on a grid, leading to an exponentially scaling computation complexity with respect to the system dimension.", "startOffset": 104, "endOffset": 116}, {"referenceID": 13, "context": "{x\u0302i} is randomly generated using an accept-reject algorithm similar to the one in [17] and described in Alg.", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "In [19], the author shows that all optimal trajectories of the Dubins car utilize controls that represent going straight or turning maximally left or right.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Furthermore, [19] also shows that for any given x\u0304 and xL, the optimal control is a sequence of at most three of these motion primitives.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "Since the controls and dynamics of Dubins car are simple, we have chosen the hidden sizes of P,D,U,X to be [10, 10, 6, 75], respectively.", "startOffset": 107, "endOffset": 122}, {"referenceID": 7, "context": "Since the controls and dynamics of Dubins car are simple, we have chosen the hidden sizes of P,D,U,X to be [10, 10, 6, 75], respectively.", "startOffset": 107, "endOffset": 122}, {"referenceID": 3, "context": "Since the controls and dynamics of Dubins car are simple, we have chosen the hidden sizes of P,D,U,X to be [10, 10, 6, 75], respectively.", "startOffset": 107, "endOffset": 122}, {"referenceID": 0, "context": "We currently plan to investigate applying our method to the 6D engine-out plane [1] as well as a 12D quadrotor model.", "startOffset": 80, "endOffset": 83}], "year": 2017, "abstractText": "Hamilton-Jacobi (HJ) reachability is a powerful tool that provides performance and safety guarantees for dynamical systems. Unfortunately, using the state-of-the-art dynamic programming-based approaches, HJ reachability is intractable for systems with more than five dimensions because its computational complexity scales exponentially with system dimension. To sidestep the curse of dimensionality, we propose an algorithm that leverages a neural network to approximate the minimum time-to-reach function to synthesize controls. We show that our neural network generates near optimal controls which are guaranteed to successfully drive the system to a target state. Our framework is not dependent on state space discretization, leading to a significant reduction in computation time and space complexity in comparison with dynamic programming-based approaches. Using this grid-free approach also enables us to plan over longer time horizons with relatively little additional computation overhead. Unlike many previous neural network reachability formulations, our approximation is conservative and hence any trajectories we generate will be strictly feasible. For demonstration, we specialize our new general framework to the Dubins car model and discuss how the general framework can be applied to other models with higher-dimensional state spaces.", "creator": "LaTeX with hyperref package"}}}