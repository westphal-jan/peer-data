{"id": "1611.03382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "abstract": "encoder - decoder models have been informally used to solve sequence to sequence prediction tasks. lately, approaches suffer from two shortcomings. first, the mice compute a representation of different word taking into account only the history of the words it has contained so far, yielding suboptimal conclusions. second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow reconstruction times. in this paper we address modeling procedures. towards this goal, we first introduce a simple mechanism that first reads selected input sequence before committing to a representation of sampled word. furthermore, we propose a strong copy mechanism that is able towards exploit very small vocabularies and handle out - of - vocabulary words. ways demonstrate the effectiveness of our approach covering the image dataset and duc competition when the state - of - the - art.", "histories": [["v1", "Thu, 10 Nov 2016 16:23:04 GMT  (1087kb,D)", "http://arxiv.org/abs/1611.03382v1", "11 pages, 4 figures, 5 tables"]], "COMMENTS": "11 pages, 4 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenyuan zeng", "wenjie luo", "sanja fidler", "raquel urtasun"], "accepted": false, "id": "1611.03382"}, "pdf": {"name": "1611.03382.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun"], "emails": ["cengwy13@mails.tsinghua.edu.cn", "urtasun}@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs.\nIn this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al. (2016)). Despite their success, it is commonly believed that the intermediate feature vectors are limited as they are created by only looking at previous words. This is particularly detrimental when dealing with large input sequences. Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope.\nThe decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al. (2016) proposed to use a copy mechanism that dynamically copy the words from the input sequence while decoding. However, they lack the ability to extract proper embeddings of out-of-vocabulary words from the input context. Bahdanau et al. (2014) proposed to use an attention mechanism to emphasize specific parts of the input sentence when generating each word. However the encoder problem still remains in this approach.\nIn this work, we propose two simple mechanisms to deal with both encoder and decoder problems. We borrowed intuition from human readers which read the text multiple times before generating summaries. We thus propose a \u2018Read-Again\u2019 model that first reads the input sequence before committing to a representation of each word. The first read representation then biases the second read\nar X\niv :1\n61 1.\n03 38\n2v 1\n[ cs\n.C L\n] 1\n0 N\nov 2\n01 6\nrepresentation and thus allows the intermediate hidden vectors to capture the meaning appropriate for the input text. We show that this idea can be applied to both LSTM and GRU models. Our second contribution is a copy mechanism which allows us to use much smaller decoder vocabulary sizes resulting in much faster decoding. Our copy mechanism also allows us to construct a better representation of out-of-vocabulary words. We demonstrate the effectiveness of our approach in the challenging Gigaword dataset and DUC competition showing state-of-the-art performance."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 SUMMARIZATION", "text": "In the past few years, there has been a lot of work on extractive summarization, where a summary is created by composing words or sentences from the source text. Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al. (2008), Filippova & Altun (2013) and Colmenares et al. (2015). As a consequence of their extractive nature the summary is restricted to words (sentences) in the source text.\nAbstractive summarization, on the contrary, aims at generating consistent summaries based on understanding the input text. Although there has been much less work on abstractive methods, they can in principle produce much richer summaries. Abstractive summarization is standardized by the DUC2003 and DUC2004 competitions (Over et al. (2007)). Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods.\nVery recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al. (2015)\u2019s work with an RNN decoder, and Nallapati et al. (2016) proposed an RNN encoder-decoder architecture for summarization. Both techniques are currently the state-of-the-art on the DUC competition. However, the encoders exploited in these methods lack the ability to encode each word condition on the whole text, as an RNN encodes a word into a hidden vector by taking into account only the words up to that time step.\nIn contrast, in this work we propose a \u2018Read-Again\u2019 encoder-decoder architecture, which enables the encoder to understand each input word after reading the whole sentence. Our encoder first reads the text, and the results from the first read help represent the text in the second pass over the source text. Our second contribution is a simple copy mechanism that allows us to significantly reduce the decoder vocabulary size resulting in much faster inference times. Furthermore our copy mechanism allows us to handle out-of-vocabulary words in a principled manner. Finally our experiments show state-of-the-art performance on the DUC competition."}, {"heading": "2.2 NEURAL MACHINE TRANSLATION", "text": "Our work is also closely related to recent work on neural machine translation, where neural encoderdecoder models have shown promising results (Kalchbrenner & Blunsom (2013); Cho et al. (2014); Sutskever et al. (2014)). Bahdanau et al. (2014) further developed an attention mechanism in the decoder in order to pay attention to a specific part of the input at every generating time-step. Our approach also exploits an attention mechanism during decoding."}, {"heading": "2.3 OUT-OF-VOCABULARY AND COPY MECHANISM", "text": "Dealing with Out-Of-Vocabulary words (OOVs) is an important issue in sequence to sequence approaches as we cannot enumerate all possible words and learn their embeddings since they might not be part of our training set. Luong et al. (2014) address this issue by annotating words on the source, and aligning OOVs in the target with those source words. Recently, Vinyals et al. (2015) propose Pointer Networks, which calculate a probability distribution over the input sequence instead\nof predicting a token from a pre-defined dictionary. Cheng & Lapata (2016) develop a neural-based extractive summarization model, which predicts the targets from the input sequences. Gulcehre et al. (2016); Nallapati et al. (2016) add a hard gate to allow the model to decide wether to generate a target word from the fixed-size dictionary or from the input sequence. Gu et al. (2016) use a softmax operation instead of the hard gating. This softmax pointer mechanism is similar to our decoder. However, our decoder can also extract different OOVs\u2019 embedding from the input text instead of using a single <UNK> embedding to represent all OOVs. This further enhances the model\u2019s ability to handle OOVs."}, {"heading": "3 THE READ AGAIN MODEL", "text": "Text summarization can be formulated as a sequence to sequence prediction task, where the input is a longer text and the output is a summary of that text. In this paper we develop an encoder-decoder approach to summarization. The encoder is used to represent the input text with a set of continuous vectors, and the decoder is used to generate a summary word by word.\nIn the following, we first introduce our \u2018Read-Again\u2019 model for encoding sentences. The idea behind our approach is very intuitive and is inspired by how humans do this task. When we create summaries, we first read the text and then we do a second read where we pay special attention to the words that are relevant to generate the summary. Our \u2018Read-Again\u2019 model implements this idea by reading the input text twice and using the information acquired from the first read to bias the second read. This idea can be seamlessly plugged into LSTM and GRU models. Our second contribution is a copy mechanism used in the decoder. It allows us to reduce the decoder vocabulary size dramatically and can be used to extract a better embedding for OOVs. Fig. 1(a) gives an overview of our model."}, {"heading": "3.1 ENCODER", "text": "We first review the typical encoder used in machine translation (e.g., Sutskever et al. (2014); Bahdanau et al. (2014)). Let x = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} be the input sequence of words. An encoder sequentially reads each word and creates the hidden representation hi by exploting a recurrent neural network (RNN)\nhi = RNN(xi, hi\u22121), (1)\nwhere xi is the word embedding of xi. The hidden vectors h = {h1, h2, \u00b7 \u00b7 \u00b7 , hn} are then treated as the feature representations for the whole input sentence and can be used by another RNN to decode and generate a target sentence. Although RNNs have been shown to be useful in modeling sequences, one of the major drawback is that hi depends only on past information i.e., {x1, \u00b7 \u00b7 \u00b7 , xi}. However, it is hard (even for humans) to have a proper representation of a word without reading the whole input sentence.\nFollowing this intuition, we propose our \u2018Read-Again\u2019 model where the encoder reads the input sentence twice. In particular, the first read is used to bias the second more attentive read. We apply this idea to two popular RNN architectures, i.e. GRU and LSTM, resulting in better encodings of the\ninput text. Note that although other alternatives, such as bidirectional RNN exist, the hidden states from the forward RNN lack direct interactions with the backward RNN, and thus forward/backward hidden states still cannot utilize the whole sequence. Besides, although we only use our model in a uni-directional manner, it can also be easily adapted to the bidirectional case. We now describe the two variants of our model."}, {"heading": "3.1.1 GRU READ-AGAIN", "text": "We read the input sentence {x1, x2, \u00b7 \u00b7 \u00b7 , xn} for the first-time using a standard GRU h1i = GRU 1(xi, h 1 i\u22121), (2)\nwhere the function GRU1 is defined as,\nzi = \u03c3(Wz[xi, h 1 i\u22121]) (3)\nri = \u03c3(Wr[xi, h 1 i\u22121])\nh\u03031i = tanh(Wh[xi, ri h1i\u22121]) h1i = (1\u2212 zi) h1i\u22121 + zi h\u03031i\nIt consists of two gatings zi, ri, controlling whether the current hidden state h1i should be directly copied from h1i\u22121 or should pass through a more complex path h\u0303 1 i .\nGiven the sentence feature vector h1n, we then compute an importance weight vector \u03b1i of each word for the second reading. We put the importance weight \u03b1i on the skip-connections as shown in Fig. 2(a) to bias the two information flows: If the current word xi has a very small weight \u03b1i, then the second read hidden state h2i will mostly take the information directly from the previous state h2i\u22121, ignoring the influence of the current word. If \u03b1i is close to 1 then it will be similar to a standard GRU, which is only influenced from the current word. Thus the second reading has the following update rule\nh2i = (1\u2212 \u03b1i) h2i\u22121 + \u03b1i GRU2(xi, h2i\u22121), (4) where means element-wise product. We compute the importance weights by attending h1i with h1n as follows\n\u03b1i = tanh(Weh 1 i + Ueh 1 n + Vexi), (5)\nwhere We, Ue, Ve are learnable parameters. Note that \u03b1i is a vector representing the importance of each dimension in the word embedding. Empirically, we find that using a vector is better than a single value. We hypothesize that this is because different dimensions represent different semantic meanings, and a single value lacks the ability to model the variances among these dimensions.\nCombining this with the standard GRU update rule\nGRU2(xi, h2i\u22121) = (1\u2212 zi) h2i\u22121 + zi h\u03032i ,\nwe can simplify the updating rule Eq. (4) to get\nh2i = (1\u2212 \u03b1i zi) h2i\u22121 + (\u03b1i zi) h\u03032i (6)\nThis equations shows that our \u2018read-again\u2019 model on GRU is equivalent to replace the GRU cell with a more general gating mechanism that also depends on the feature representation of the whole sentence computed from the first reading pass. We argue that adding this global information could help direct the information flow for the forward pass resulting in a better encoder."}, {"heading": "3.1.2 LSTM READ-AGAIN", "text": "We now apply the \u2018Read-Again\u2019 idea to the LSTM architecture as shown in Fig. 2(b). Our first reading is performed by an LSTM1 defined as\nfi = \u03c3(Wf [xi, hi\u22121]) (7) ii = \u03c3(Wi[xi, hi\u22121])\noi = \u03c3(Wo[xi, hi\u22121])\nC\u0303i = tanh(WC [xi, hi\u22121])\nCi = ft Ci\u22121 + ii C\u0303i hi = oi tanh(Ci)\nDifferent from the GRU architecture, LSTM calculates the hidden state by applying a non-linear activation function to the cell state Ci, instead of a linear combination of two paths used in the GRU. Thus for our second read, instead of using skip-connections, we make the gating functions explicitly depend on the whole sentence vector computed from the first reading pass. We argue that this helps the encoding of the second reading LSTM2, as all gating and updating increments are also conditioned on the whole sequence feature vector (h1i , h 1 n). Thus\nh2i = LSTM 2([xi, h 1 i , h 1 n], h 2 i\u22121), (8)"}, {"heading": "3.1.3 READING MULTIPLE SENTENCES", "text": "In this section we extend our \u2018Read-Again\u2019 model to the case where the input sequence has more than one sentence. Towards this goal, we propose to use a hierarchical representation, where each sentence has its own feature vector from the first reading pass. We then combine them into a single vector to bias the second reading pass. We illustrate this in the context of two input sentences, but it is easy to generalize to more sentences. Let {x1, x2, \u00b7 \u00b7 \u00b7 , xn} and {x\u20321, \u00b7 \u00b7 \u00b7 , x\u2032m} be the two input sentences. The first RNN reads these two sentences independently to get two sentence feature vectors h1n and h \u20321 m respectively.\nHere we investigate two different ways to handle multiple sentences. Our first option is to simply concatenate the two feature vectors to bias our second reading pass:\nh2i = RNN 2([xi, h 1 i , h 1 n, h \u20321 m], h 2 i\u22121) (9)\nh\u20322i = RNN 2([x\u2032i, h \u20321 i , h 1 n, h \u20321 m], h \u20322 i\u22121)\nwhere h20 and h \u20322 0 are initial zero vectors. Feeding h 1 n, h \u20321 m into the second RNN provides more global information explicitly and helps acquire long term dependencies.\nThe second option we explored is shown in Fig. 3.1.2. In particular, we use a non-linear transformation to get a single feature vector hglobal from both sentence feature vectors:\nhglobal = tanh(Wrh 1 n + Urh \u20321 m + vr) (10)\nThe second reading pass is then\nh\u03032i = RNN 2([xi, h 1 i , h 1 n, hglobal], h 2 i\u22121) (11)\nh\u0303\u20322i = RNN 2([x\u2032i, h \u20321 i , h \u20321 m, hglobal], h \u20322 i\u22121)\nNote that this is more easily scalable to more sentences. In our experiments both approaches perform similarly."}, {"heading": "3.2 DECODER WITH COPY MECHANISM", "text": "In this paper we argue that only a small number of common words are needed for generating a summary in addition to the words that are present in the source text. We can consider this as a hybrid approach which combines extractive and abstractive summarization. This has two benefits: first it allow us to use a very small vocabulary size, speeding up inference. Furthermore, we can create summaries which contain OOVs if they are present in the source text.\nOur decoder reads the vector representations of the input text using an attention mechanism, and generates the target summary word by word. We use an LSTM as our decoder, with a fixed-size vocabulary dictionary Y and learnable word embeddings Y \u2208 R|Y |\u00d7dim. At time-step t the LSTM generates a summary word yt by first computing the current hidden state st from the previous hidden state st\u22121, previous summary word yt\u22121 and current context vector ct\nst = LSTM([yt\u22121, ct], st\u22121), (12)\nwhere the context vector ct is computed with an attention mechanism on the encoder hidden states:\nct = n\u2211 i=1 \u03b2ith 2 i . (13)\nThe attention score \u03b2it at time-step t on the i-th word is computed via a soft-max over oit, where\noit = att(st\u22121, h 2 i ) = v T a tanh(Wast\u22121 + Uah 2 i ), (14)\nwith va, Wa, Ua learnable parameters.\nA typical way to treat OOVs is to encode them with a single shared embedding. However, different OOVs can have very different meanings, and thus using a single embedding for all OOVs will confuse the model. This is particularly detrimental when using small vocabulary sizes. Here we address this issue by deriving the representations of OOVs from their corresponding context in the input text. Towards this goal, we change the update rule of yt\u22121. In particular, if yt\u22121 belongs to a word that is in our decoder vocabulary we take its representation from the word embedding, otherwise if it appears in the input sentence as xi we use\nyt\u22121 = pi = tanh(Wch 2 i + bc) (15)\nwhere Wc and bc are learnable parameters. Since h2i encodes useful context information of the source word xi, pi can be interpreted as the semantics of this word extracted from the input sentence. Furthermore, if yt\u22121 does not appear in the input text, nor in Y , then we represent yt\u22121 using the <UNK> embedding.\nGiven the current decoder\u2019s hidden state st, we can generate the target summary word yt. As shown in Fig. 1(b), at each time step during decoding, the decoder outputs a distribution over generating words from Y , as well as over copying a specific word xi from the source sentence."}, {"heading": "3.3 LEARNING", "text": "We jointly learn our encoder and decoder by maximizing the likelihood of decoding the correct word at each time step. We refer the reader to the experimental evaluation for more details."}, {"heading": "4 EXPERIMENTAL EVALALUATION", "text": "In this section, we show results of abstractive summarization on Gigaword (Graff & Cieri (2003); Napoles et al. (2012)) and DUC2004 (Over et al. (2007)) datasets. Our model can learn a meaningful re-reading weight distribution for each word in the input text, putting more emphasis on important verb and nous, while ignoring common words such as prepositions. As for the decoder, we demonstrate that our copy mechanism can successfully reduce the typical vocabulary size by a factor 5 while achieving much better performance than the state-of-the-art, and by a factor of 30 while maintaining the same level of performance. In addition, we provide an analysis and examples of which words are copied during decoding.\nDataset and Evaluation Metric: We use the Gigaword corpus to train and evaluate our models. Gigaword is a news corpus where the title is employed as a proxy for the summary of the article. We follow the same pre-processing steps of Rush et al. (2015), which include filtering, PTB tokenization, lower-casing, replacing digit characters with #, replacing low-frequency words with UNK and extracting the first sentence in each article. This results in a training set of 3.8M articles, a validation set and a test set each containing 400K articles. The average sentence length is 31.3 words for the source, and 8.3 words for the summaries. Following the standard protocol we evaluate ROUGE score on 2000 random samples from the test set. As for evaluation metric, we use full-length F1 score on Rouge-1, Rouge-2 and Rouge-L, following Chopra et al. (2016) and Nallapati et al. (2016), since these metrics are less bias to the outputs\u2019 length than full-length recall scores.\nImplemetation Details: We implement our model in Tensorflow and conduct all experiments on a NVIDIA Titan X GPU. Our models converged after 2-3 days of training, depending on model size. Our RNN cells in all models have 1 layer, 512-dimensional hidden states, and 512-dimensional word embeddings. We use dropout rate of 0.2 in all activation layers. All parameters, except the biases are initialized uniformly with a range of \u221a 3/d, where d is the dimension of the hidden state (Sussillo & Abbott (2014)). The biases are initialized to 0.1. We use plain SGD to train the model with gradient clipped at 10. We start with an initial learning rate of 2, and halve it every epoch after first 5 epochs. Our max epoch for training is 10. We use a mini-batch size of 64, which is shuffled during training."}, {"heading": "4.1 QUANTITATIVE EVALUATION", "text": "Results on Gigaword: We compare the performances of different architectures and report ROUGE scores in Tab. 1. Our baselines include the ABS model of Rush et al. (2015) with its proposed\nvocabulary size as well as an attention encoder-decoder model with uni-directional GRU encoder. We allow the decoder to generate variable length summaries. As shown in Tab. 1 our Read-Again models outperform the baselines on all ROUGE scores, when using both 15K and 69K sized vocabularies. We also observe that adding the copy mechanism further helps to improve performance: Even though the decoder vocabulary size of our approach with copy (15K) is much smaller than ABS (69K) and GRU (69K), it achieves a higher ROUGE score. Besides, our Multiple-Sentences model achieves the best performance.\nEvaluation on DUC2004: DUC 2004 (Over et al. (2007)) is a commonly used benchmark on summarization task consisting of 500 news articles. Each article is paired with 4 different humangenerated reference summaries, capped at 75 characters. This dataset is evaluation-only. Similar to Rush et al. (2015), we train our neural model on the Gigaword training set, and show the models\u2019 performances on DUC2004. Following the convention, we also use ROUGE limited-length recall as our evaluation metric, and set the capping length to 75 characters. We generate summaries with 15 words using beam-size of 10. As shown in Table 2, our method outperforms all previous methods on Rouge-1 and Rouge-L, and is comparable on Rouge-2. Furthermore, our model only uses 15k decoder vocabulary, while previous methods use 69k or 200k.\nImportance Weight Visualization: As we described in the section before, \u03b1i is a high-dimension vector representing the importance of each word xi. While the importance of a word is different over each dimension, by averaging we can still look at general trends of which word is more relevant. Fig. 4 depicts sample sentences with the importance weight \u03b1i over input words. Words such as the, a, \u2019s, have small \u03b1i, while words such as aeronautics, resettled, impediments, which carry more information have higher values. This shows that our read-again technique indeed extracts useful information from the first reading to help bias the second reading results.\nthe sh utt le sp ac e\ndo wn tou ch ed co lum bia at su nd ay ken ne dythe ce nte r ca na ve ral ca pein sp ac e flo rid a,\nae ron\nau tic s na tio na l the sp ac e, ad mi nis tra tio n an d sa id . <e os >\nindonesia has moved #.# million people and resettled them in #,### villages in a national transmigration scheme over the\npast ## years , president suharto said here monday .\nind on\nes ia #.#ha s an d\nmi llio\nn\nmo ve\nd\nres ett\nled\npe op\nle\nvill ag\nes the m in\ntra ns\nmi gra\ntio n\nna tio\nna la #.# ##\nsc he\nme pa st ov er the he re##\npre sid\nen t .\n<e os\n> ye ars ,\nmo nd\nay\nsu ha\nrto sa idin\nindo esia 's state- wned domestic carrier merp ti nusantara airlines will lease ## aircraft from the united states , the official\nantara news agency reported here thursday .\nind on\nes ia sta teow ne d\u2018s me rpa ti ca rrie r do me sti c air lin es nu sa nta ra lea sewil l air cra ft un ite dthefro m## the sta tes\nthu rsd\nay\noffi cia\nl\nhe re\nag en\ncy, ne ws an tar a\nrep ort\ned . <e os > tar iffs ba rrie rsan d\nim pe\ndim en\nts rem ainoth er to se rio us an d tra de in reg ion\nas ia-\npa cifi\nc\nthe\nbu sin\nes s \u201c\nde sp\nite\npro gre\nss\nsu bs\ntan tia\nl the las t\u201d rep ortov er se ve n . <e os >\ntariffs and other barriers remain serious impediments to trade and business in the asia-pacific region despite `` substantial ''\nprogress over the last seven years , a leading economic council said in a report .\nye ars , a lea din g\nec on\nom ic\nco un\ncilsa id in a\nFigure 4: Weight Visualization"}, {"heading": "4.2 DECODER VOCABULARY SIZE", "text": "Table 3 shows the effect on our model of decreasing the decoder vocabulary size. We can see that when using the copy mechanism, we are able to reduce the decoder vocabulary size from 69K to 2K, with only 2-3 points drop on ROUGE score. This contrasts the models that do not use the copy mechanism. This is possibly due to two reasons. First, when faced with OOVs during decoding\ntime, our model can extract their meanings from the input text. Second, equipped with a copy mechanism, our model can generate OOVs as summary words, maintaining its expressive ability even with a small decoder vocabulary size. Tab. 4 shows the decoding time as a function of vocabulary size. As computing the soft-max is usually the bottleneck for decoding, reducing vocabulary size dramatically reduces the decoding time from 0.38 second per sentence to 0.08 second.\nTab. 5 provides some examples of visualization of the copy mechanism. Note that we are able to copy key words from source sentences to improve the summary. From these examples we can see that our model is able to copy different types of rare words, such as special entities\u2019 names in case 1 and 2, rare nouns in case 3 and 4, adjectives in case 5 and 6, and even rare verbs in the last example. Note that in the third example, when the copy model\u2019s decoder uses the embedding of headmaster as its first input, which is extracted from the source sentence, it generates the same following sentence as the no-copy model. This probably means that the extracted embedding of headmaster is closely related to the learned embedding of teacher."}, {"heading": "5 CONCLUSION", "text": "In this paper we have proposed two simple mechanisms to alleviate the problems of current encoderdecoder models. Our first contribution is a \u2018Read-Again\u2019 model which does not form a representation of the input word until the whole sentence is read. Our second contribution is a copy mechanism that can handle out-of-vocabulary words in a principled manner allowing us to reduce the decoder vocabulary size and significantly speed up inference. We have demonstrated the effectiveness of our approach in the context of summarization and shown state-of-the-art performance. In the future, we plan to tackle summarization problems with large input text. We also plan to exploit our findings in other tasks such as machine translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu O Mittal", "Michael J Witbrock"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Banko et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1603.07252,", "citeRegEx": "Cheng and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M Rush", "SEAS Harvard"], "venue": "arXiv preprint arXiv:1602.06023,", "citeRegEx": "Chopra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Sentence compression beyond word deletion", "author": ["Trevor Cohn", "Mirella Lapata"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Cohn and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Cohn and Lapata.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Heads: Headline generation as sequence prediction using an abstract feature-rich", "author": ["Carlos A Colmenares", "Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri"], "venue": null, "citeRegEx": "Colmenares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Colmenares et al\\.", "year": 2015}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev.,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Katja Filippova", "Yasemin Altun"], "venue": "In EMNLP,", "citeRegEx": "Filippova and Altun.,? \\Q2013\\E", "shortCiteRegEx": "Filippova and Altun.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li"], "venue": "arXiv preprint arXiv:1603.06393,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "arXiv preprint arXiv:1506.05865,", "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a glar Gul\u00e7ehre", "Bing Xiang"], "venue": null, "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pp. 95\u2013100", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Automatic text summarization using a machine learning approach", "author": ["Joel Larocca Neto", "Alex A Freitas", "Celso AA Kaestner"], "venue": "In Brazilian Symposium on Artificial Intelligence,", "citeRegEx": "Neto et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Neto et al\\.", "year": 2002}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["David Sussillo", "LF Abbott"], "venue": "arXiv preprint arXiv:1412.6558,", "citeRegEx": "Sussillo and Abbott.,? \\Q2014\\E", "shortCiteRegEx": "Sussillo and Abbott.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Extractive summarization using supervised and semisupervised learning", "author": ["Kam-Fai Wong", "Mingli Wu", "Wenjie Li"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Wong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2008}, {"title": "Generation with quasi-synchronous grammar", "author": ["Kristian Woodsend", "Yansong Feng", "Mirella Lapata"], "venue": "In Proceedings of the 2010 conference on empirical methods in natural language processing,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["David Zajic", "Bonnie Dorr", "Richard Schwartz"], "venue": "In Proceedings of the HLT-NAACL", "citeRegEx": "Zajic et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al.", "startOffset": 104, "endOffset": 122}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)).", "startOffset": 104, "endOffset": 147}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al.", "startOffset": 104, "endOffset": 645}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al.", "startOffset": 104, "endOffset": 663}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al. (2016)).", "startOffset": 104, "endOffset": 685}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al. (2016)). Despite their success, it is commonly believed that the intermediate feature vectors are limited as they are created by only looking at previous words. This is particularly detrimental when dealing with large input sequences. Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al.", "startOffset": 104, "endOffset": 960}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left.", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al.", "startOffset": 48, "endOffset": 795}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al.", "startOffset": 48, "endOffset": 820}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al. (2016) proposed to use a copy mechanism that dynamically copy the words from the input sequence while decoding.", "startOffset": 48, "endOffset": 841}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al. (2016) proposed to use a copy mechanism that dynamically copy the words from the input sequence while decoding. However, they lack the ability to extract proper embeddings of out-of-vocabulary words from the input context. Bahdanau et al. (2014) proposed to use an attention mechanism to emphasize specific parts of the input sentence when generating each word.", "startOffset": 48, "endOffset": 1080}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al.", "startOffset": 21, "endOffset": 62}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al. (2008), Filippova & Altun (2013) and Colmenares et al.", "startOffset": 21, "endOffset": 82}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al. (2008), Filippova & Altun (2013) and Colmenares et al.", "startOffset": 21, "endOffset": 108}, {"referenceID": 4, "context": "(2008), Filippova & Altun (2013) and Colmenares et al. (2015). As a consequence of their extractive nature the summary is restricted to words (sentences) in the source text.", "startOffset": 37, "endOffset": 62}, {"referenceID": 4, "context": "(2008), Filippova & Altun (2013) and Colmenares et al. (2015). As a consequence of their extractive nature the summary is restricted to words (sentences) in the source text. Abstractive summarization, on the contrary, aims at generating consistent summaries based on understanding the input text. Although there has been much less work on abstractive methods, they can in principle produce much richer summaries. Abstractive summarization is standardized by the DUC2003 and DUC2004 competitions (Over et al. (2007)).", "startOffset": 37, "endOffset": 515}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al.", "startOffset": 55, "endOffset": 96}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al.", "startOffset": 55, "endOffset": 118}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al.", "startOffset": 55, "endOffset": 145}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods.", "startOffset": 55, "endOffset": 198}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization .", "startOffset": 55, "endOffset": 393}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task.", "startOffset": 55, "endOffset": 466}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder.", "startOffset": 55, "endOffset": 565}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al.", "startOffset": 55, "endOffset": 727}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al. (2015)\u2019s work with an RNN decoder, and Nallapati et al.", "startOffset": 55, "endOffset": 755}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al. (2015)\u2019s work with an RNN decoder, and Nallapati et al. (2016) proposed an RNN encoder-decoder architecture for summarization.", "startOffset": 55, "endOffset": 811}, {"referenceID": 2, "context": "Our work is also closely related to recent work on neural machine translation, where neural encoderdecoder models have shown promising results (Kalchbrenner & Blunsom (2013); Cho et al. (2014); Sutskever et al.", "startOffset": 175, "endOffset": 193}, {"referenceID": 2, "context": "Our work is also closely related to recent work on neural machine translation, where neural encoderdecoder models have shown promising results (Kalchbrenner & Blunsom (2013); Cho et al. (2014); Sutskever et al. (2014)).", "startOffset": 175, "endOffset": 218}, {"referenceID": 0, "context": "Bahdanau et al. (2014) further developed an attention mechanism in the decoder in order to pay attention to a specific part of the input at every generating time-step.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Luong et al. (2014) address this issue by annotating words on the source, and aligning OOVs in the target with those source words.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Luong et al. (2014) address this issue by annotating words on the source, and aligning OOVs in the target with those source words. Recently, Vinyals et al. (2015) propose Pointer Networks, which calculate a probability distribution over the input sequence instead", "startOffset": 0, "endOffset": 163}, {"referenceID": 10, "context": "Gulcehre et al. (2016); Nallapati et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "Gulcehre et al. (2016); Nallapati et al. (2016) add a hard gate to allow the model to decide wether to generate a target word from the fixed-size dictionary or from the input sequence.", "startOffset": 0, "endOffset": 48}, {"referenceID": 10, "context": "Gu et al. (2016) use a softmax operation instead of the hard gating.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": ", Sutskever et al. (2014); Bahdanau et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2014)).", "startOffset": 8, "endOffset": 31}, {"referenceID": 16, "context": "In this section, we show results of abstractive summarization on Gigaword (Graff & Cieri (2003); Napoles et al. (2012)) and DUC2004 (Over et al.", "startOffset": 97, "endOffset": 119}, {"referenceID": 16, "context": "In this section, we show results of abstractive summarization on Gigaword (Graff & Cieri (2003); Napoles et al. (2012)) and DUC2004 (Over et al. (2007)) datasets.", "startOffset": 97, "endOffset": 152}, {"referenceID": 16, "context": "We follow the same pre-processing steps of Rush et al. (2015), which include filtering, PTB tokenization, lower-casing, replacing digit characters with #, replacing low-frequency words with UNK and extracting the first sentence in each article.", "startOffset": 43, "endOffset": 62}, {"referenceID": 4, "context": "As for evaluation metric, we use full-length F1 score on Rouge-1, Rouge-2 and Rouge-L, following Chopra et al. (2016) and Nallapati et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 4, "context": "As for evaluation metric, we use full-length F1 score on Rouge-1, Rouge-2 and Rouge-L, following Chopra et al. (2016) and Nallapati et al. (2016), since these metrics are less bias to the outputs\u2019 length than full-length recall scores.", "startOffset": 97, "endOffset": 146}, {"referenceID": 18, "context": "Our baselines include the ABS model of Rush et al. (2015) with its proposed", "startOffset": 39, "endOffset": 58}, {"referenceID": 21, "context": "Models Size Rouge-1 Rouge-2 Rouge-L ZOPIARY (Zajic et al. (2004)) - 25.", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "12 ABS (Rush et al. (2015)) 69K 26.", "startOffset": 8, "endOffset": 27}, {"referenceID": 16, "context": "12 ABS (Rush et al. (2015)) 69K 26.55 7.06 23.49 ABS+ (Rush et al. (2015)) 69K 28.", "startOffset": 8, "endOffset": 74}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.", "startOffset": 13, "endOffset": 34}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.41 7.69 23.06 RAS-Elman (Chopra et al. (2016)) 69K 28.", "startOffset": 13, "endOffset": 88}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.41 7.69 23.06 RAS-Elman (Chopra et al. (2016)) 69K 28.97 8.26 24.06 big-words-lvt2k-1sent (Nallapati et al. (2016)) 69K 28.", "startOffset": 13, "endOffset": 157}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.41 7.69 23.06 RAS-Elman (Chopra et al. (2016)) 69K 28.97 8.26 24.06 big-words-lvt2k-1sent (Nallapati et al. (2016)) 69K 28.35 9.46 24.59 big-words-lvt5k-1sent (Nallapati et al. (2016)) 200K 28.", "startOffset": 13, "endOffset": 226}, {"referenceID": 18, "context": "Similar to Rush et al. (2015), we train our neural model on the Gigaword training set, and show the models\u2019 performances on DUC2004.", "startOffset": 11, "endOffset": 30}], "year": 2016, "abstractText": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}