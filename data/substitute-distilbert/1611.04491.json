{"id": "1611.04491", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Ranking medical jargon in electronic health record notes by adapted distant supervision", "abstract": "objective : allowing patients to access their own electronic health record ( ehr ) published through the patient portals has the potential to improve patient - centered care. however, medical jargon, which abounds in ehr notes, has been spotted to be a barrier for patient ehr comprehension. geographic knowledge bases that link medical jargon to lay terms or definitions adopt an important role through remedy this problem but have low coverage of medical descriptions below ehrs. we developed a data - driven approach that utilizes ehrs to identify uniquely rank medical language based on its contribution to patients, to support the building of ehr - centric lay language resources.", "histories": [["v1", "Mon, 14 Nov 2016 17:36:05 GMT  (1162kb)", "http://arxiv.org/abs/1611.04491v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jinying chen", "abhyuday n jagannatha", "samah j jarad", "hong yu"], "accepted": false, "id": "1611.04491"}, "pdf": {"name": "1611.04491.pdf", "metadata": {"source": "CRF", "title": "Ranking medical jargon in electronic health record notes by adapted distant supervision", "authors": ["Jinying Chen", "Abhyuday N. Jagannatha", "Samah J. Jarad", "Hong Yu"], "emails": ["hong.yu}@umassmed.edu,", "abhyuday@cs.umass.edu,", "samah.fodeh@yale.edu"], "sections": [{"heading": null, "text": "Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources.\nMethods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations (word embeddings)\nlearned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms.\nResults: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TF*IDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified over 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives.\nConclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay terms/definitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request."}, {"heading": "INTRODUCTION", "text": "Patient portals, including My HealtheVet [1], have been embraced by many healthcare organizations for patient-clinician communication. Allowing patients to access their EHR notes helps improve their disease understanding, self-management and outcomes [1,2]. However, studies have shown that patients often have difficulty in comprehending medical jargon [3\u20137] (here \u201cmedical jargon\u201d is defined as \u201ctechnical terminology or special words that are used by medical professions and are difficult for others to understand\u201d), limiting their ability to understand their clinical status [5,6]. Figure 1 shows a sample text found in a typical clinical note. The medical terms that may hinder patients\u2019 comprehension are italicized. In addition, those medical jargon terms judged by physicians to be important for patient understanding are also underlined.\nFigure 1. Illustration of medical jargon in a clinical note\nTo reduce the communication gap between patients and clinicians, there have been decades of research efforts in creating medical resource for lay people [8]. Natural language processing methods have also been developed to automatically substitute medical jargon with lay terms [9\u2013 11] or to link them to consumer-oriented definitions [12]. These approaches require high-quality lexical resources of medical jargon and lay terms/definitions. The open-access collaborative consumer health vocabulary (CHV) is one such resource [13]. It has been incorporated into the Unified Medical Language System and has also been used in EHR simplification [9,10].\nResearch in CHV has been motivated by the vocabulary discrepancies between lay people and health care professionals [14\u201317]. CHV incorporates terms extracted from various consumer health sites, such as queries submitted to MedLinePlus, a consumer-oriented online knowledge resource maintained by the National Library of Medicine, and from postings in health-focused online discussion forums [18,19]. CHV contained 152,338 terms, most of which are consumer health terms [18\u201320]. Zeng et al. [18] mapped these consumer health terms to the Unified Medical Language System by a semi-automatic approach. As the result of this work, CHV encompasses lay terms as well as corresponding medical jargon.\nFrom our current work, we found that CHV alone is not sufficient as a lexical resource for comprehending EHR notes, as many medical jargon terms in EHRs do not exist in CHV, and many\nothers exist in CHV but with lay terms identical to the jargon terms themselves. For example, in CHV, the respective lay terms of 19,674 jargon terms (e.g., \u201cneurocytoma\u201d, \u201clymphangiomatosis\u201d, and \u201claryngeal carcinoma\u201d) are themselves. Although CHV provides lay definitions to some of these terms, 18,823 (96%) terms remain to be unannotated (i.e., have neither appropriate lay terms nor lay definitions).\nThe goal of this study is to identify medical jargon from EHRs for the purpose of creating new lexical entries to link medical jargon to lay terms/definitions. Since the size of medical jargon is large (tens of thousands of terms), we will rank them based on how important they are to lay people, and therefore prioritize the annotation effort of lexical entries on those important terms. Specifically, we proposed and developed an adapted distant-supervision (ADS) model to rank terms in EHRs to prioritize those important medical jargon terms for lay language annotation. We made a novel use of a non-EHR-centric resource (i.e., CHV) for distant supervision and showed promising results from using this approach. Our work is different from previous work in building biomedical lexical resources. Previous work either uses unsupervised automatic term recognition methods [21\u201323] or uses supervised learning (when human annotations are available) [21] to prioritize terms.\nOur contributions are twofold. Firstly, we develop and evaluate the ADS model, a new model that mines and ranks medical terms from large EHR corpora based on terms\u2019 importance to patients. Secondly, we apply our ADS model to rank over 100K EHR terms to prioritize 10K important terms for lay language annotation.\nIn addition, the ranking methods we developed have a great potential to be applied to other clinical natural language processing tasks, including generating features for keyphrase extraction, information retrieval, summarization, and question answering."}, {"heading": "MATERIALS AND METHODS", "text": ""}, {"heading": "EHR Corpora and Candidate Terms", "text": "In this study, we utilized two EHR corpora: EHR-Pittsburg and EHR-UMass.\nEHR-Pittsburg 1 contains 7,839 discharge summary notes with 5.4M words. We applied the linguistic filter of the automatic term recognition toolkit Jate [24] to this corpus and extracted 106,108 candidate terms (see Step 1 in Figure 2). These candidate terms were further used to identify and rank medical jargon terms.\nEHR-UMass contains 90 de-identified EHR notes from the UMass Memorial Hospital outpatient clinics. To maximize the representativeness, we selected notes from patients with six common primary clinical diagnoses: cancer, COPD, diabetes, heart failure, hypertension, and liver failure. We de-identified the notes and then asked physicians to identify, for each note, terms important to patients [25]. Specifically, physicians were asked to identify EHR terms which the patients need to know to comprehend the notes for the most important aspects medically relevant to their health and treatment course. For each note, we obtained annotations from two physicians. Three physicians did the annotation and annotated 48, 68, and 64 notes respectively. We used this expert-annotated corpus to create a dataset for evaluating ADS (details in the subsection Evaluation Using the EHRUMass Dataset)."}, {"heading": "Medical Jargon in CHV", "text": "In this study, we evaluated the coverage of CHV for medical jargon in EHR. In addition, we used medical jargon in CHV to create training data for the ADS model. We followed [9] (i.e., CHV familiarity score \u2264 0.6) to identify medical jargon terms in CHV."}, {"heading": "Baseline Systems", "text": "We evaluated two baseline systems for ranking medical terms: Corpus-level TF*IDF [24] and CValue [26]. Both baselines are state-of-the-art automatic term recognition methods that have proved to be effective in identifying domain-specific biomedical terms [24,26]. EHR notes are abundant in medical jargon. The two methods, by their definitions described below, are expected to be able to identify these medical terms and rank them by their importance to the EHR corpora. When applying the two methods to our task, we made an assumption that medical terms salient in a large EHR corpus are important to patients (based on the fact that EHR notes are documents that record patients\u2019 medical and treatment course) and therefore should be prioritized for annotation.\nCorpus-level TF*IDF: TF*IDF [27] is a widely used metric for measuring the importance of a term to a document d in a corpus D. The more frequently the term appears in the document and the less frequently it appears in other documents, the more important it is to this document. We used Corpus-level TF*IDF (abbreviated as TF*IDF in the rest of the paper) to measure the importance of a candidate term t to a corpus D by summing up a term\u2019s TF*IDF per document over the corpus, as defined in Equation (1):\n\ud835\udc47\ud835\udc39 \u2217 \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc61, \ud835\udc37) = \u2211 (\ud835\udc61\ud835\udc53(\ud835\udc61, \ud835\udc51)\u00d7\ud835\udc56\ud835\udc51\ud835\udc53(\ud835\udc61, \ud835\udc37)\ud835\udc51\u2208\ud835\udc37 ) (1)\nwhere d is any document in D; tf(t,d) is term frequency of t in d; and idf(t,D) is inverse document frequency of t in D, which indicates the importance of t across the corpus D and is defined as \ud835\udc59\ud835\udc5c\ud835\udc54 |\ud835\udc37| |{\ud835\udc51|\ud835\udc61\u2208\ud835\udc51}| .\nC-Value: C-Value is a widely used method for extracting terminology from text corpora. It has been used to prioritize health and biomedical terms for developing lexical resources, including CHV [22,23]. It measures the importance of a term t in a corpus D by its frequency in the corpus tf(t,D). If a term is nested in other longer terms, C-Value penalizes it, as defined in Equation (2):\n\ud835\udc36 \u2212 \ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 (\ud835\udc61, \ud835\udc37) = { \ud835\udc59\ud835\udc5c\ud835\udc542|\ud835\udc61|\u00d7\ud835\udc61\ud835\udc53(\ud835\udc61, \ud835\udc37), \ud835\udc4e \ud835\udc56\ud835\udc60 \ud835\udc5b\ud835\udc5c\ud835\udc61 \ud835\udc5b\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc51\n\ud835\udc59\ud835\udc5c\ud835\udc542|\ud835\udc61|\u00d7\ud835\udc61\ud835\udc53(\ud835\udc61, \ud835\udc37) \u2212 1\n|\ud835\udc47\ud835\udc61| \u2211 \ud835\udc61\ud835\udc53(\ud835\udc4f, \ud835\udc37), \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52\ud835\udc4f\u2208\ud835\udc47\ud835\udc61\n(2)\nwhere |t| is the number of words contained in t; Tt is the set of all long candidate terms (phrases) b that contain t; and |Tt| is the number of terms in Tt."}, {"heading": "The ADS Model", "text": "Our ADS model is a case of learning from positive and unlabeled data [28\u201332]. In particular, Elkan and Noto [32] have shown that a binary classifier that outputs probability confidence scores on examples can be trained using positive examples and unlabeled examples (treated as \u201cnegative\u201d) and its confidence scores on new examples can then be used to rank those new examples.\nWe used CHV to select and label positive examples to train ADS (see Step 2 in Figure 2). Our approach is based on an assumption that medical terms important to patients must be used by patients. Specifically, we assume that medical terms that occur in both EHRs and CHV are important to patients because they are medical synonyms of terms initially identified from queries and postings generated by patients in online health forums. Based on this assumption, we used medical jargon terms in both EHRs and CHV (called EHR-CHV medical jargon terms) as positive examples because our goal is to prioritize important medical jargon terms from EHRs. In addition to the positive examples labeled by CHV, some unlabeled EHR terms may be also important and therefore should also be ranked high. We achieved this goal by learning from positive and unlabeled data. Figure 2 illustrates data extraction, ADS and its evaluation.\nOur ADS model has two major components: the support vector machine classifier and the features used for classification.\nThe Support Vector Machine Classifier\nPrevious work has shown that support vector machines [33] are effective in learning from positive and unlabeled data [29,32,34], our ADS is therefore built upon support vector machines. We employed the widely used and reproducible LibSVM package [35]. We chose the RBF-kernel\nsupport vector machine as we found it performed better than support vector machines using linear and polynomial kernels in our preliminary experiments. We assigned a rank score to each term using LibSVM\u2019s probabilistic outputs [36,37]. We used these probabilistic rank scores to merge the rankings from 10-fold runs to obtain the global rank of a term.\nFeatures\nWe developed three types of learning features: (1) confidence scores as computed by automatic\nterm recognition (ATR) (2) Unified Medical Language System semantic types (Sem), and (3) distributed word representation or word embedding (WE).\nConfidence scores from automatic term recognition: we used the confidence scores from TF*IDF and C-value.\nUnified Medical Language System semantic types: We mapped candidate terms to Unified Medical Language System concepts and included semantic types for those concepts that had an exact match or a head-noun match as features. Each semantic type is a 0-1 binary feature. This type of feature has been used to identify domain-specific medical terms [12,22]. In this work, we made an assumption that different semantic types contribute differently to a term\u2019s importance to patients. We relied on the support vector machine classifier to learn the weight/contribution of each semantic type.\nDistributed word representation (word embedding): Word embeddings are distributed vector representations of words. Each dimension of a word vector has a real value ranged between 0 and 1. We treated each dimension as a feature.\nBecause word embedding vectors are learned from large text corpora and incorporate syntactic and semantic properties of words, words sharing similar semantics and context are expected to be close in their word vector space [38,39]. In this work, we made an assumption that medical terms that are important to patients share similar semantics and context. Therefore, word embeddings are likely to be useful features for learning important medical terms.\nWe trained a neural language model to learn word embeddings. Specifically, we used Word2Vec software, which supports efficient computations on large datasets, to create the Skip Gram word embeddings [38,39]. We trained Word2Vec using a combined text corpus (over 3G words) of English Wikipedia, articles from PubMed Open Access and 99K EHR notes from EHR-Pittsburg. We set the training parameters based on the study of Pyysalo et al. [40]. We represented multiword terms with the mean of individual word vectors."}, {"heading": "Training and Evaluation Datasets", "text": "We used two datasets in our study. The EHR-Pittsburg dataset was used for training, evaluation, and generating the global ranking of candidate terms, while the EHR-UMass dataset was used for evaluation. The statistics of these two datasets are summarized in Table 1.\nTraining Using the EHR-Pittsburg Dataset\nThe numbers of terms used as positive and unlabeled data were 6,959 and 99,149, respectively (see Step 2 in Figure 2). For training, we divided the data into 10 folds. We used 9 folds to train the ADS model and applied it to the remaining fold to obtain the rank scores of the candidate terms. We produced a total of 10 ranking outputs, one for each fold. We then merged the 10 outputs to produce a global ranking, which we evaluated using a metrics that measures system performance for learning from positive and unlabeled data (details in the subsection Evaluation Metrics).\nEvaluation Using the EHR-UMass Dataset\nBecause the EHR-UMass corpus was annotated by physicians in such a way that terms that are important to patients were labeled as \u201cpositive\u201d, we utilized their annotations to create a data set with both positive and negative examples for evaluation (see Step 4 in Figure 2). Specifically, we applied Jate to extract 6,280 candidate terms from the EHR-UMass corpus. Candidate terms exactly matching the physician-annotated important terms were labeled as positive. The remaining candidate terms were labeled as negative. In total, we obtained 1,018 positive and 5,262 negative examples, which we included in the EHR-UMass dataset for evaluating ADS. We compute the CValue and TF*IDF scores for the terms in this dataset by using a large EHR corpus that contains 6K notes (including the 90 EHR-UMass notes) collected using the same six diagnoses used for collecting the 90 notes."}, {"heading": "Post-processing", "text": "As we found that the performances of TF*IDF and C-Value were negatively affected by highfrequency, common terms (e.g., \u201cpatient\u201d and \u201cpain\u201d) in EHRs, we added a post-processing procedure that used a stopwords list to filter out common terms from the models\u2019 outputs. This list contains 100 high-frequency non-medical terms in the EHR-UMass corpus. In addition, we used\nregular expressions to rank low compound terms that contain \u201cnot\u201d \u201cno\u201d, \u201cand\u201d, or \u201cor\u201d. In our evaluation using the EHR-UMass dataset, we report both conditions: with and without postprocessing."}, {"heading": "Evaluation Metrics", "text": "Receiver Operating Characteristic (ROC) and Area Under ROC Curve (ROC-AUC): ROC curve is a metrics widely used for evaluating ranking outputs. It plots the true positive rate (y-coordinate) against the false positive rate (x-coordinate) at various threshold settings. We report both ROC and ROC-AUC by using the R library pROC [41].\nMetrics for learning from Positive and Unlabeled data (PU Metrics): Evaluating systems that learn from positive and unlabeled data is challenging because the data include unlabeled examples, and thus we can\u2019t calculate recall and precision. For evaluation, Lee and Liu [42] introduced PU metrics, r2/Pr[system positive], where r is recall and Pr[system positive] is the probability of positive examples predicted by the system. Recall can be estimated as the total number of positive predictions divided by the total number of labeled positive examples, as explained in [42]. We plotted r2/Pr[system positive] as a function of the rank k."}, {"heading": "RESULTS", "text": ""}, {"heading": "CHV Coverage of Medical Jargon in EHRs", "text": "From the EHR-Pittsburg corpus, we extracted 106,108 candidate terms, on which we applied MetaMap [43] to select medical terms. A total of 19,503 (18%) of the candidate terms were successfully mapped to Unified Medical Language System concepts by MetaMap. However, 4,680 (24%) of these medical terms do not appear in CHV. We manually examined those terms and found\na majority of them were medical jargon terms, such as \u201cBruton agammaglobulinemia\u201d, \u201cmolecular diagnostics\u201d, \u201cmotor symptom\u201d, and \u201creactive lymphocytosis\u201d. The remaining 86,605 (82%) candidate terms also contained medical jargon terms, such as \u201canticardiolipin\u201d, \u201cBGM\u201d, 2 \u201cdemargination\u201d, \u201cheptoglobin\u201d, \u201chypoalimentation\u201d, and \u201chypobilirubinemia\u201d."}, {"heading": "ADS Ranking Performance on EHR-Pittsburg Dataset", "text": "Figure 3 plots the PU metrics as a function of rank k for the ADS model and two baseline systems on the EHR-Pittsburg dataset. The PU metrics curve of ADS rapidly reaches to the peak at k = 9,229, and then declines sharply. In contrast, the two baseline systems\u2019 PU metrics are relatively stable. Overall, ADS was consistently better than the two baselines for all k."}, {"heading": "ADS Ranking Performance on EHR-UMass Dataset", "text": "Figure 4 plots the ROC curves of ADS, TF*IDF and C-Value in ranking EHR-UMass terms, without and with post-processing. ADS achieved the best performance. Table 2 shows the ROCAUC, where ADS outperformed TF*IDF and C-Value by large margins (>15 points, absolute gains). Although post-processing improved performance of TF*IDF and C-Value substantially, ADS still exhibited better performance.\nFigure 4. ROC plots of different methods in ranking the EHR-UMass terms: (4a) without postprocessing and (4b) with post-processing"}, {"heading": "DISCUSSION", "text": ""}, {"heading": "Principle Results", "text": "We find that CHV has incomplete coverage of medical jargon in EHRs. We therefore developed the ADS model to rank 100K candidate terms from the EHR-Pittsburg corpus and prioritized our annotation of lay terms/definitions for top-ranked terms. ADS ranks EHR terms based on the assumption that medical terms that occur in both EHRs and CHV are important to patients. ADS achieved 0.810 ROC-AUC on the EHR-UMass dataset (Table 2). This level of performance is adequate, especially considering that ADS does not use any human-annotated training data. This result indirectly verifies the validity of our assumption. It also suggests that we can use ADS to prioritize EHR terms for annotation."}, {"heading": "Interpretation of the PU Metrics Curves", "text": "In Figure 3, the performance of ADS on the EHR-Pittsburg dataset reaches a peak at rank k = 9,229, with a sharp drop afterwards. At its peak point, ADS is able to identify 5,248 (75%) of the total\n(4a) (4b)\n6,959 labeled positive terms (i.e., EHR-CHV medical jargon terms). Most of them are important medical jargon, including \u201cpremature atrial contraction\u201d, \u201cpolycythemia\u201d, \u201cT-cell lymphoma\u201d, and \u201cthallium stress test\u201d. The non-CHV terms that were ranked by ADS in top-10K also contain many important medical jargon terms, such as \u201cchronic respiratory insufficiency\u201d, \u201cnasogastric decompression\u201d, and \u201cpreoperative chemotherapy\u201d. At lower ranks, ADS is less effective in identifying EHR-CHV jargon terms, finding more common terms such as \u201cnose\u201d, \u201cactivate\u201d, \u201ctraining\u201d, and \u201cdust\u201d. This result suggests that we can use the rank 10K as a threshold to divide the ranked terms into high-quality and low-quality groups. Terms in the high-quality group are being used to support annotation of lay terms/definitions.\nThe curves of TF*IDF and C-Value are relatively flat because term ranking is based on statistical values and therefore insensitive to CHV terms, as opposed to ADS, which is supervised by CHV."}, {"heading": "ADS and the Baselines TF*IDF and C-Value", "text": "Figure 4 and Table 2 show that ADS outperforms the two baselines on the EHR-UMass dataset. Our result analysis shows that the three models have similar performance at top ranks (top-n lists where n<30) and ADS has much better performance at lower ranks. We manually checked the top10 erroneous terms identified from the EHR-UMass dataset (with post-processing), shown in Table 4 where the EHR-CHV medical jargon terms are bolded. As shown in Table 4, the top-10 errors identified by ADS are all EHR-CHV medical jargon terms (bolded). Because the physicians only identified few (15 per note, on average) important medical terms from each EHR note, they did not mark many CHV terms as important. However, this type of error may not be critical for our annotation task. For example, some CHV terms not marked by physicians are still worth annotating with lay definitions/terms for EHR comprehension, e.g., \u201cRaynaud\u201d(Raynaud\u2019s disease), \u201cpyelonephristis\u201d, \u201conychomycosis\u201d, and \u201ccholestasis\u201d.\nAlthough post-processing boosted TF*IDF and C-Value performances, they still ranked some common terms high (e.g., \u201csurgery\u201d, \u201csleep\u201d, \u201cliver\u201d, \u201cabdominal pain\u201d, and \u201cblood sugar\u201d). In addition, C-Value ranks many multi-word terms high because it favors long phrases by its definition.\nThe false-negative terms predicted by the three systems (i.e., important medical jargon terms that were ranked low) are also different. ADS often missed medical terms that contain easy words, such as \u201cpancreatic enzyme replacement\u201d and \u201cchronic lower extremity edema\u201d. One reason for this error is that these terms have similar word embedding and/or semantic type features as lay (negative) terms (e.g., \u201creplacement\u201d, \u201cchronic\u201d, \u201clower\u201d, and \u201cextremity\u201d). Using advanced phrase embedding techniques may alleviate this problem, which we may explore in future. Different from ADS, TF*IDF and C-Value often missed terms that are important but occur infrequently in the 6K EHR-UMass notes, such as \u201cneurodermatitis\u201d, \u201cdiabetic renal disease\u201d, \u201cautonomic neuropathy\u201d, and \u201cPML\u201d (progressive multifocal leukoencephalopathy)."}, {"heading": "CONCLUSION", "text": "We report an ADS model for prioritizing medical terms that are important for patient EHR comprehension. Our experiments have shown that ADS is more effective than TF*IDF and CValue, two methods that are widely used to mine and prioritize terms from large text corpora for building domain-specific lexical resources. The EHR terms prioritized by our model are being used to enrich a comprehensive medical jargon\u2013lay term/definition knowledge resource for EHR simplification. Our top-10K-ranked EHR terms are available upon request."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Investigator Initiated Research (1I01HX001457-01) from the Health Services Research and Development Program of the United States Department of Veterans Affairs. The content is solely the responsibility of the authors and does not represent the views of the United States Department of Veterans Affairs or the United States Government.\nWe thank Weisong Liu for technical support in collecting the EHR notes and the UMassMed annotation team, including Elaine Freund, Victoria Wang, Andrew Hsu, Barinder Hansra and Sonali Harchandani, for creating the UMass-EHR corpus."}], "references": [{"title": "Medical communication: do our patients", "author": ["EB Lerner", "DV Jehle", "DM Janicke"], "venue": null, "citeRegEx": "Lerner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lerner et al\\.", "year": 2012}, {"title": "Lay understanding of terms used in cancer consultations", "author": ["K Chapman", "C Abraham", "V Jenkins"], "venue": "Psychooncology 2003;12:557\u201366", "citeRegEx": "Chapman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2003}, {"title": "Patients\u2019 experiences when accessing their on-line electronic patient records in primary care", "author": ["C Pyper", "J Amery", "M Watson"], "venue": "Br J Gen Pr 2004;54:38\u201343", "citeRegEx": "Pyper et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pyper et al\\.", "year": 2004}, {"title": "Towards consumer-friendly PHRs: patients\u2019 experience with reviewing their health records", "author": ["A Keselman", "L Slaughter", "CA Smith"], "venue": "Proceedings of AMIA Annual Symposium", "citeRegEx": "Keselman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Keselman et al\\.", "year": 2007}, {"title": "Lay understanding of common medical terminology in oncology", "author": ["AH Pieterse", "NA Jager", "EMA Smets"], "venue": "Psychooncology 2013;22:1186\u201391", "citeRegEx": "Pieterse et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pieterse et al\\.", "year": 2013}, {"title": "Promoting health literacy", "author": ["McCray AT"], "venue": "J Am Med Inform Assoc 2005;12:152\u2013163", "citeRegEx": "AT.,? \\Q2005\\E", "shortCiteRegEx": "AT.", "year": 2005}, {"title": "Making texts in electronic health records comprehensible to consumers: a prototype translator", "author": ["Q Zeng-Treitler", "S Goryachev", "H Kim"], "venue": "Proceedings of AMIA Annual Symposium", "citeRegEx": "Zeng.Treitler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zeng.Treitler et al\\.", "year": 2007}, {"title": "A semantic and syntactic text simplification tool for health content", "author": ["S Kandula", "D Curtis", "Q. Zeng-Treitler"], "venue": "Proceedings of AMIA Annual Symposium", "citeRegEx": "Kandula et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kandula et al\\.", "year": 2010}, {"title": "Medical text simplification using synonym replacement: Adapting assessment of word difficulty to a compounding language. In: Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations at EACL", "author": ["E Abrahamsson", "T Forni", "M Skeppstedt"], "venue": "Association for Computational Linguistics", "citeRegEx": "Abrahamsson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abrahamsson et al\\.", "year": 2014}, {"title": "Improving Patients\u2019 Electronic Health Record Comprehension with NoteAid", "author": ["B Polepalli Ramesh", "T Houston", "C Brandt"], "venue": "Stud Health Technol Inform 2013;192:714\u20138", "citeRegEx": "Ramesh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ramesh et al\\.", "year": 2013}, {"title": "Exploring and developing consumer health vocabularies", "author": ["QT Zeng", "T. Tse"], "venue": "J Am Med Inform Assoc 2006;13:24", "citeRegEx": "Zeng and Tse,? \\Q2006\\E", "shortCiteRegEx": "Zeng and Tse", "year": 2006}, {"title": "Terminology issues in user access to Web-based medical information", "author": ["AT McCray", "RF Loane", "AC Browne"], "venue": "Proc AMIA Symp 1999;:107\u201311", "citeRegEx": "McCray et al\\.,? \\Q1999\\E", "shortCiteRegEx": "McCray et al\\.", "year": 1999}, {"title": "Patient and clinician vocabulary: how different are they", "author": ["Q Zeng", "S Kogan", "N Ash"], "venue": "Stud Health Technol Inform 2001;84:399\u2013403", "citeRegEx": "Zeng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2001}, {"title": "Evaluation of Controlled Vocabulary Resources for Development of a Consumer Entry Vocabulary for Diabetes", "author": ["TB Patrick", "HK Monga", "MC Sievert"], "venue": "J Med Internet Res 2001;3:e24", "citeRegEx": "Patrick et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Patrick et al\\.", "year": 2001}, {"title": "Characteristics of consumer terminology for health information retrieval", "author": ["Q Zeng", "S Kogan", "N Ash"], "venue": "Methods Inf Med", "citeRegEx": "Zeng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2002}, {"title": "Term identification methods for consumer health", "author": ["Q Zeng", "T Tse", "G Divita"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2003}, {"title": "Facilitating the development", "author": ["I Spasi\u0107", "D Schober", "S-A Sansone"], "venue": "J Med Internet Res 2007;9:e4", "citeRegEx": "Spasi\u0107 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Spasi\u0107 et al\\.", "year": 2007}, {"title": "A Comparative Evaluation of Term", "author": ["Z Zhang", "J Iria", "C Brewster"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Language Resources and Evaluation (LREC)", "author": ["J Chen", "J Zheng", "H. Yu"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "A statistical interpretation of term specificity and its application", "author": ["K. Sparck Jones"], "venue": "Int J Digit Libr 2000;3:115\u2013130", "citeRegEx": "Jones,? \\Q2000\\E", "shortCiteRegEx": "Jones", "year": 2000}, {"title": "Text classification from positive and unlabeled", "author": ["F Denis", "R Gilleron", "M. Tommasi"], "venue": "J Doc 1972;28:11\u201321", "citeRegEx": "Denis et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Denis et al\\.", "year": 1972}, {"title": "Building text classifiers using positive and unlabeled", "author": ["B Liu", "Y Dai", "X Li"], "venue": null, "citeRegEx": "2002", "shortCiteRegEx": "2002", "year": 1934}, {"title": "A simple probabilistic approach to learning from positive and unlabeled examples", "author": ["Zhang D", "Lee WS"], "venue": "Proceedings of the 5th Annual UK Workshop on Computational Intelligence (UKCI). Citeseer", "citeRegEx": "D and WS.,? \\Q2005\\E", "shortCiteRegEx": "D and WS.", "year": 2005}, {"title": "Text classification without negative examples revisit", "author": ["GPC Fung", "JX Yu", "H Lu"], "venue": "Knowl Data Eng IEEE Trans On 2006;18:6\u201320", "citeRegEx": "Fung et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2006}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C Elkan", "K. Noto"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "Elkan and Noto,? \\Q2008\\E", "shortCiteRegEx": "Elkan and Noto", "year": 2008}, {"title": "A bagging SVM to learn from positive and unlabeled examples", "author": ["Mordelet F", "Vert J-P"], "venue": "Pattern Recognit Lett 2014;37:201\u2013209", "citeRegEx": "F and J.P.,? \\Q2014\\E", "shortCiteRegEx": "F and J.P.", "year": 2014}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang C-C", "Lin C-J"], "venue": "ACM Trans Intell Syst Technol TIST 2011;2:27:1-27:27", "citeRegEx": "C.C and C.J.,? \\Q2011\\E", "shortCiteRegEx": "C.C and C.J.", "year": 2011}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J. Platt"], "venue": "Adv Large Margin Classif", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "A note on Platt\u2019s probabilistic outputs for support vector machines. Mach Learn 2007;68:267\u2013276", "author": ["Lin H-T", "Lin C-J", "Weng RC"], "venue": null, "citeRegEx": "H.T et al\\.,? \\Q2007\\E", "shortCiteRegEx": "H.T et al\\.", "year": 2007}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T Mikolov", "K Chen", "G Corrado"], "venue": "Cs Published Online First:", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "I Sutskever", "K Chen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["S Pyysalo", "F Ginter", "H Moen"], "venue": "The 5th International Symposium on Languages in Biology and Medicine", "citeRegEx": "Pyysalo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pyysalo et al\\.", "year": 2013}, {"title": "pROC: an open-source package for R and S+ to analyze and compare ROC curves", "author": ["X Robin", "N Turck", "A Hainard"], "venue": "BMC Bioinformatics", "citeRegEx": "Robin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Robin et al\\.", "year": 2011}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["WS Lee", "B. Liu"], "venue": "ICML", "citeRegEx": "Lee and Liu,? \\Q2003\\E", "shortCiteRegEx": "Lee and Liu", "year": 2003}, {"title": "An overview of MetaMap: historical perspective and recent advances", "author": ["Aronson AR", "Lang F-M"], "venue": "J Am Med Inform Assoc 2010;17:229\u201336", "citeRegEx": "AR and F.M.,? \\Q2010\\E", "shortCiteRegEx": "AR and F.M.", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources. Methods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations (word embeddings) learned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms. Results: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TF*IDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified over 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives. Conclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay terms/definitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request.", "creator": "Microsoft\u00ae Word 2016"}}}