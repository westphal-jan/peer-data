{"id": "1611.04686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Robust Matrix Regression", "abstract": "modern technologies are producing datasets with complex intrinsic structures, and they effectively be naturally represented as matrices instead of vectors. to preserve the latent data structures during processing, modern regression approaches incorporate the low - rank index to the analysis and achieve satisfactory performance for certain applications. stability features all assume that both predictors and labels for each pair of data within the training set are accurate. however, in real - world applications, it arises common to see the structural data contaminated by noises, which can affect the implementation of these hierarchical regression methods. in this paper, we address this issue by rendering just novel robust matrix regression method. we also derive efficient proximal algorithms for regression training. to evaluate sufficient performance of our modeling, we attach it to real world applications with comparative studies. our strategy achieves the state - of - the - ensemble performance, which shows the effectiveness and the practical competence of our method.", "histories": [["v1", "Tue, 15 Nov 2016 03:15:46 GMT  (284kb,D)", "http://arxiv.org/abs/1611.04686v1", "8 pages, 4 tables"]], "COMMENTS": "8 pages, 4 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hang zhang", "fengyuan zhu", "shixin li"], "accepted": false, "id": "1611.04686"}, "pdf": {"name": "1611.04686.pdf", "metadata": {"source": "CRF", "title": "Robust Matrix Regression", "authors": ["Hang Zhang", "Fengyuan Zhu", "Shixin Li"], "emails": ["hzhang@cse.cuhk.edu.hk,", "fyzhu@cse.cuhk.edu.hk,", "leept416@gmail.com"], "sections": [{"heading": "Introduction", "text": "Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form. However, with the development of modern technology, it is common to meet datasets with sample unit not in vector form but instead in matrix form. Examples include the two-dimensional digital images, with quantized values of different colors at certain rows and columns of pixels; and electroencephalography (EEG) data with voltage fluctuations at multiple channels over a period of time. When using traditional regression methods to process these data, we have to reshape them into vectors, which may destroy the latent topological structural information, such as the correlation between different channels for EEG data (Zhou and Li, 2014), and the spatial relation within an image (Wolf, Jhuang, and Hazan, 2007).\nTo tackle this issue, several methods have been proposed to perform regression on data in matrix form directly. One such model is the regularized matrix regression (RGLM) (Zhou and Li, 2014). Given a dataset {Xi, yi}Ni=1, where N is the sample size, Xi \u2208 Rp\u00d7q denotes the ith data matrix as predictor, and yi \u2208 R is the corresponding output, the R-GLM model aims to learn a function f : Rp\u00d7q \u2192 R to\nidentify the output given a newly observed data matrix with\nyi = tr(WTXi) + b+ , (1)\nwhere tr(\u00b7) represents the trace of a matrix, W is the regression matrix with low-rank property to preserve structural information of each data matrix, and b denotes the offset. is zero-mean Gaussian noise to model the small uncertainty of the output. With this setting, the R-GLM has achieved satisfying results in several applications. However, there still exist certain issues that should be further addressed. Firstly, RGLM uses the Gaussian noise for model fitting, and take all deviations of predicted values from labels into account. This setting can be reasonable in certain cases, but may not make sense for particular applications. As an example, consider the problem of head pose estimation (Sherrah and Gong, 2001), where for each data pair, the predictor is a two dimensional digital image for the head of a person, while the output denotes the angle of his head. Because the real angle of head cannot be measured precisely, there should exist certain deviations of provided labels from the real ones empirically. In this case, the regression model should be able to tolerant such small deviations instead of taking them all into account. Another important issue is that, the predictors are also assumed to be noise free, which can be irrational in certain applications. Practically, it is common to see signals corrupted by noise, such as image signals with occlusion, specular reflections or noise (Huang, Cabral, and De la Torre, 2016), and financial data with noise (Magdon-Ismail, Nicholson, and Abu-Mostafa, 1998). Thus, it is important for a regression model to be tolerant of noise on predictors and labels to enhance its robustness empirically.\nIn this paper, we introduce two novel matrix regression methods to tackle the above mentioned issues. We first propose a \u201cRobust Matrix Regression\u201d (RMR) to tackle the noisy label problem, by introducing hinge loss to model the uncertainty of regression labels. In this way, our method only considers error larger than a pre-specified value, and can tolerate error around each labeled output within a small range. This approach is also favored for other advantages in certain scenarios. As an example, in applications like financial time-series prediction, it is common to require not to lose more than money when dealing with data like exchange rates, and this issue can be well addressed with our setting. Even though the hinge loss error has been used in\nar X\niv :1\n61 1.\n04 68\n6v 1\n[ cs\n.L G\n] 1\n5 N\nov 2\n01 6\nthe support vector regression model (Smola and Scho\u0308lkopf, 2004), it is an algorithm based on vector-form data, which can ruin the latent structure for matrix regression problem. We then propose efficient ADMM method to solve the optimization problem iteratively.\nTo further enhance the robustness of RMR with noisy predictors, we propose a generalized RMR (G-RMR) by decomposing each data matrix as latent clean signal plus sparse outliers. For model training, we also derive a proximal algorithm to estimate both the regression matrix and latent clean signals iteratively. To evaluate the performance of our methods, we conduct extensive experiments on both approaches with comparison of state-of-the-art ones. Our methods achieve superior performance consistently, which shows their efficiency in real world problems.\nNotations: We present the scalar values with lower case letters (e.g., x); vectors by bold lower case letters (e.g., x); and matrix by bold upper case letters (e.g., X). For a matrix X, its (i, j)-entity is represented as Xi,j . tr(\u00b7) denotes the trace of a matrix, and {a}+ = max(0, a). We further set ||X||F and ||X||\u2217 as the Frobenius norm and nuclear norm of a matrix X respectively."}, {"heading": "Robust Matrix Regression", "text": "We first introduce the RMR model to address the noisy label problem, with an ADMM algorithm for model training."}, {"heading": "Model", "text": "For matrix regression, classical techniques need to reshape each matrix Xi into a vector xi, which will destroy its intrinsic structures, resulting in the loss of information. The R-GLM approach (Zhou and Li, 2014) addresses this issue by enforcing the regression matrix W to be low-rank representable with nuclear norm penalty. However, this method is based on the Gaussian loss, which may affect the robustness with existence of noisy labels.\nTo tackle this issue, an intuitive idea is to ignore noises within a small margin {\u2212 , } around each label for robust model fitting. Motivated by this idea, we propose our RMR, by introducing the hinge loss for model fitting, where the residual corresponding to each data Xi is defined as follows\nhi(W, b) = (|tr(W>Xi) + b\u2212 yi| \u2212 )+. (2) With the above formulation of residuals, when learning the regression model, our approach only takes residuals larger than into account, thus, the labels contaminated by noise within a small margin is tolerable accordingly. Similar residual modeling approach has also been used in the method of support vector regression (Smola and Scho\u0308lkopf, 2004). However, this approach is proposed for vector data regression and cannot capture the latent structure within each data matrix. Differently, our method can capture such latent structure by incorporating the spectral elastic net penalty (Luo et al., 2015) into the regression matrix W, which can model the correlation of each data matrix effectively. And the corresponding optimization problem is defined as follows\nargmin W,b\nH(W, b) + \u03c4 ||W||\u2217 (3)\nwhere\nH(W, b) = 1\n2 tr(W>W)\n+ C N\u2211 i {\u2212 \u2212 tr(W>Xi)\u2212 b+ yi}+\n+ C N\u2211 i {\u2212 + tr(W>Xi) + b\u2212 yi}+, .\nwith 1\n2 tr(W>W) + \u03c4 ||W||\u2217 as the spectral elastic net\npenalty, we incorporate low-rank property into W and consider the group effect of the eigenvalues, to capture the latent structures among data matrices (Luo et al., 2015)."}, {"heading": "Solver", "text": "As the object function contains both hinge loss and nuclear norm, the Nesterov method used in R-GLM (Zhou and Li, 2014) is no longer available because the derivative of our loss function is not Lipschitz-continuous. Nevertheless, since our model is convex with respect to both W and b, we here derive an efficient learning algorithm based on Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011) with the restart rule (Goldstein et al., 2014) to solve the optimization problem. The optimization problem defined in Eq. (3) can be equivalently written as follows:\narg min W,b,S H(W, b) +G(S) (4)\ns.t. S\u2212W = 0\nwhere S is an auxiliary variable and G(S) = \u03c4 ||S||\u2217. In this way, the original optimization has been split into two subproblems with respect to {W, b} and S respectively. In this way, we can develop efficient ADMM method to solve Eq. (4) by using the Augmented Lagrangian approach as follows:\nL(W, b,S,\u039b) =H(W, b) +G(S) + tr[\u039b>(S\u2212W)]\n+ \u03c1\n2 ||S\u2212W||2F (5)\nwhere \u03c1 > 0 is a hyper parameter.\nOptimization for Auxiliary Variable We first derive the optimization method for solving the auxiliary variable S. The first subproblem for solving S is\nargmin S\n\u03c4 ||\u03c1S||\u2217 + 1\n2 ||(\u03c1W \u2212\u039b)\u2212 \u03c1S||2F . (6)\nThen we can get S(k) in the kth iteration by solving the problem in Eq. (6) and get the analytical solution as follows.\nS(k) = 1\n\u03c1 UD\u03bb(\u03c1W(k) \u2212\u039b(k))V>, (7)\nwhere U\u03a3V> = \u03c1W(k)\u2212\u039b(k) andD\u03bb(\u03a3)ii = max(\u03a3ii\u2212 \u03bb, 0).\nOptimization for Regression Matrix To solve the regression matrix, we have\nargmin W,b H(W, b)\u2212 tr(\u039b>W) + \u03c1 2 ||W \u2212 S||2F . (8)\nAnd one solution of this optimization problem is\nW\u2217 = 1\n1 + \u03c1 ( N\u2211 i=1 (\u03b1i \u2212 \u03b1\u2217i )Xi + \u039b + \u03c1S), (9)\nb\u2217 = 1 |I\u2217| \u2211 i\u2208I\u2217 {yi \u2212 sign(\u03b2\u2217i ) \u00b7 \u2212 tr[(W\u2217)>Xi]},\nwhere I\u2217 = {i : 0 < \u03b2\u2217i < C}, \u03b2\u2217 \u2208 Rn and \u03b2 = \u03b1\u2212 \u03b1 \u2217. \u03b1 and \u03b1\u2217 can be constructed from the solution of the following box constrained quadratic programming problem:\nargmin x\n1 2 x>Hx + c>x (10)\ns.t 0 \u2264 x \u2264 C12n n\u2211 i=1 (\u03b1i \u2212 \u03b1\u2217i ) = 0.\nHere we have\nx = [ \u03b1 \u03b1\u2217 ] , c = [ p p\u2217 ] , H = [ K \u2212K \u2212K K ] . (11)\nK = [Kij ] \u2208 RN\u00d7N and q \u2208 RN are independent of \u03b1 with,\nKij = tr(X>i Xj) \u03c1+ 1 ,\npi = \u2212 yi + tr[(\u039b + \u03c1S)>Xi]\n\u03c1+ 1\np\u2217i = + yi \u2212 tr[(\u039b + \u03c1S)>Xi]\n\u03c1+ 1\nIn this way, we can get W(k) and b(k) in each iteration with sequential minimization optimization algorithm (Keerthi and Gilbert, 2002; Platt and others, 1998).\nThe algorithm is summarized in Algorithm 1. And for its convergence, we have the following theorem.\nTheorem 1 Suppose the optimal solution of Problem (4) is (W\u0303, b\u0303, S\u0303). Then\nW\u0303 = S\u0303 = \u039b\u0303 + \u2211\n(\u03b1\u0303i \u2212 \u03b1\u0303\u2217i )Xi. (12)\nBecause the hinge loss and nuclear norm are weakly convex, the convergence property of Algorithm 1 can be proved immediately based on the result in (Goldstein et al., 2014; He and Yuan, 2015). That is, we have\nTheorem 2 For any \u03c1 > 0 and \u03b7 \u2208 (0, 1), the iteration sequence given by Algorithm 1 converges to the optimal solution of Problem (4).\nAlgorithm 1 ADMM for Subproblem 1 in Eq. (4) 1: Initialize S(\u22121) = S\u0302(0) \u2208 Rp\u00d7q, \u039b(\u22121) = \u039b\u0302 \u2208 Rp\u00d7q, \u03c1 >\n0, t(1) = 1, \u03b7 \u2208 (0, 1). 2: for k = 0, 1, 2, 3... do 3: (W(k), b(k)) = argmin\nW,b H(W, b) \u2212 tr(\u039b\u0302(k)>W) +\n\u03c1 2 ||W \u2212 S\u0302(k)||2F\n4: S(k) = argmin S\nG(S) + tr(\u039b\u0302(k)>S) + \u03c1\n2 ||W(k) \u2212 S||2F\n5: \u039b(k) = \u039b\u0302(k) \u2212 \u03c1(W(k) \u2212 S(k)) 6: c(k) = \u03c1\u22121||\u039b(k) \u2212 \u039b\u0302(k)||2F + \u03c1||S(k) \u2212 S\u0302(k)||2F 7: if c(k) < \u03b7c(k\u22121) then\n8: t(k+1) = 1 +\n\u221a 1 + 4(t)2\n2\n9: S\u0302(k+1) = S(k) + t(k)\u22121\nt(k+1) (S(k) \u2212 S(k)\u22121)\n10: \u039b\u0302(k+1) = \u039b(k) + t(k)\u22121\nt(k+1) (\u039b(k) \u2212\u039b(k)\u22121)\n11: else 12: t(k+1) = 1 13: S\u0302(k+1) = S(k\u22121) 14: \u039b\u0302(k+1) = \u039b(k\u22121) 15: c(k) = \u03b7\u22121c(k\u22121) 16: end if 17: end for"}, {"heading": "Theoretical Analysis", "text": "We further theoretically analyze the excess risk of our RMR model. Following the framework of (Wimalawarne, Tomioka, and Sugiyama, 2016), we assume each entity of a data matrix follows standard Gaussian distribution. Then, the optimization problem of our RMR can be rewritten as\nargmin W,b N\u2211 i=1 l(W, b,Xi, yi) (13)\ns.t.||W||\u2217 \u2264 C1, ||W||F \u2264 C2, where C1 and C2 are certain constants, and l(W, b,Xi, yi) is the hinge loss. Based on the relation between W and b in Eq. ((9)), the loss function l(W, b,Xi, yi) can be simplified as\nl\u0302(W, X\u0303i, yi) = N\u2211 i {\u2212tr(W>X\u0303i) + c3}+ (14)\n+ N\u2211 i {tr(W>X\u0303i) + c4}+,\nwhich is a L-Lipschitz continuous function, and X\u0303i = Xi \u2212 1\nN\n\u2211N j=1 Xj , with 1\nN\n\u2211N j=1 Xj to be the empirical mean\nof data, which tends to be 0 when N is large. Thus, the X\u0303i can also be considered as standard Gaussian distributed.\nLet R(W) and R\u0302(W) be the empirical risk and expected risk respectively (Maurer and Pontil, 2013). Also, we set Wo be the optimal solution to"}, {"heading": "Wo = argmin", "text": "W R(W), s.t.||W||\u2217 \u2264 C1, ||W||F \u2264 C2,\n(15)\nand W\u0303 be the optimal solution of\nW\u0303 = argmin W R\u0302(W), s.t.||W||\u2217 \u2264 C1, ||W||F \u2264 C2. (16) Then, we provide the upper bound of the excess risk of RMR in the following theorem, with proof in supplementary. Theorem 3 With probability at least 1\u2212\u03b4, let r be the rank of Wo (Wo,W\u0303 \u2208 Rp\u00d7q), and the excess risk of RMR is bounded with\nR(W\u0303)\u2212R(Wo) \u2264 2Lmax{C1,\n1\u221a r C2}\n\u221a N \u00b7 (\u221ap+\u221aq) + \u221a ln(1/\u03b4)\n2N Proof. To prove the above theorem, We can first reformu-\nlate the excess risk with respect to Wo and W\u0302 as follows\nR(W\u0303)\u2212R(Wo) = [R(W\u0303)\u2212 R\u0302(W\u0303)] + [R\u0302(W\u0303)\u2212 R\u0302(Wo)] + [R\u0302(Wo)\u2212R(Wo)]\n(17) Here, the second term is negative naturally. And following the Hoeffding\u2019s inequality, the third one can be bounded as\u221a\nln(1/\u03b4)/2N , with probability 1\u2212 \u03b4/2. For the first term, it is easy to obtain that\nR(W\u0303)\u2212 R\u0302(W\u0303) \u2264 sup ||W||\u2217\u2264C1,||W||F\u2264C2 [R(W)\u2212 R\u0302(W)]. (18) Further using the McDiarmid\u2019s inequality, we can simply obtain the Rademacher complexity with probability 1 \u2212 \u03b4, with\nR = 2 N E sup ||W||\u2217\u2264C1,||W||F\u2264C2 N\u2211 i=1 \u03c3i l\u0302(W, X\u0303i, yi), (19) where \u03c3i \u2208 {\u22121, 1} represents the Rademacher variables. Let M\u0303 = \u2211N i=1 \u03c3iX\u0303i, we can obtain the upper bound of R(W\u0303)\u2212 R\u0302(W\u0303) as follows R(W\u0303)\u2212 R\u0302(W\u0303) \u2264 R\n\u2264 2L N E sup ||W||\u2217\u2264C1,||W||F\u2264C2 N\u2211 i=1 \u03c3itr(WX\u0303i)\n= 2L\nN E sup ||W||\u2217\u2264C1,||W||F\u2264C2 tr(WM\u0303).\n(20) Further applying the Ho\u0308lder\u2019s inequality, we have\nR(W\u0303)\u2212 R\u0302(W\u0303) \u2264 2L N E sup ||W||\u2217\u2264C1,||W||F\u2264C2 ||W||\u2217||M\u0303||\u2217\u2217\n\u2264 2L N Emax( sup ||W||\u2217\u2264C1 ||W||\u2217, sup ||W||F\u2264C2 ||W||\u2217)\n\u00b7 ||M\u0303||\u2217\u2217\n=\n2Lmax(C1, 1\u221a r C2)\nN E||M\u0303||\u2217\u2217\n(21)\nwhere ||M\u0303||\u2217\u2217 is the dual norm of nuclear norm ||M\u0303||\u2217 and\nsup ||W||\u2217 s.t. ||W||F \u2264 C2,\n(22)\nis equivalent to\nsup r\u2211 i \u03c3i(W)\ns.t. r\u2211 i \u03c3i(W) 2 \u2264 C22 ,\n(23)\nwhose optimal solution is 1\u221a r C2.\nSince M\u0303 is the sum of random variables, its entries should also be considered as Gaussian distributed, with variance \u03c9 = N . Thus, following Maurer and Pontil (2013), with the Gordan\u2019s theorem, we have\nE||M\u0303||\u2217\u2217 \u2264 \u221a \u03c9( \u221a p+ \u221a q)\n= \u221a N( \u221a p+ \u221a q)\n(24)\nCombining all the above together, we can obtain the upper bound of the excess risk with probability at least 1 \u2212 \u03b4 as follows\nR(W\u0303)\u2212R(Wo) \u2264 2Lmax{C1,\n1\u221a r C2}\n\u221a N \u00b7 (\u221ap+\u221aq) + \u221a ln(1/\u03b4)\n2N\n(25)"}, {"heading": "Generalized Robust Matrix Regression", "text": "The RMR can tolerate label noises for matrix regression. However, it cannot handle noise or outliers on each predictor empirically. To address this issue, we further introduce a generalize RMR (G-RMR) which assumes each noisy data matrix can be decomposed as a latent clean signal plus outliers. The clean signals can be recovered from each noisy ones when learning the regression model."}, {"heading": "Model", "text": "As discussed in (Cande\u0300s et al., 2011), it is common for natural signals to contain correlation empirically. Thus, when stacking each vectorized latent clean data matrix, the resulting matrix should be low-rank representable. We also introduce the sparsity feature with the L1 norm to the outliers for robust modeling. With these settings, the optimization problem of G-RMR can be defined as follows\nargmin W,b\nH(W, b) + \u03c4 ||W||\u2217 + \u03b3||X||\u2217 + \u03bb||E||1 (26)\ns.t. D = X + E.\nHere, Di denotes the ith noisy input matrix, and we assume that it can be decomposed as Di = Xi + Ei, with Xi as the latent clean matrix signal, and Ei as the outliers. D is a\nmatrix with the ith row as the vector form of Di, X denotes the matrix with the ith row as the vector form of Xi, which is assumed to a low-rank representable with a nuclear norm penalty; and E contains the outliers for each data matrix, with its ith row as the vector form of Ei and is encouraged to be sparse with the L1 norm. It can be noticed that the decomposition form of each Di is the same as that in Robust Principal Component Analysis, which is effective in data recovery. But our approach is different because we update the regression matrix and recover the clean signals simultaneously within one optimization problem, which can benefit both tasks empirically."}, {"heading": "Solver", "text": "The optimization problem for G-RMR contain three additional non-smooth term. Fortunately, the object function in Eq. (26) is still bi-convex, which means that it is convex with respect to W with X and E fixed, and convex with respect to X and E with W fixed. Therefore, we derive an iterative ADMM method to solve this problem, where the optimization problem in Eq. (26) is divided into two subproblems and each subproblem can be solved by ADMM individually.\nThe first subproblem is to solve W with X and E fixed, which is equivalent to solve the problem in Eq. (3). The second subproblem is to solve X and E with W fixed. Different from the first subproblem, an L1 norm is introduced to the object function. And the subproblem can be written as\nargmin X,E\nH(W, b) + \u03b3||X||\u2217 + \u03bb||E||1, (27)\ns.t. D = X + E. Since H(W, b) contains hinge loss function and other two non-smooth terms, the optimization problem is hard to solve. Thus, we relax the hinge loss to squared one and develop another ADMM method to solve this subproblem in with Augmented Lagrangian approach as follows: L(X,E,\u0393) = C||Xw + b\u2212 y||22 + \u03b3||X||\u2217 + \u03bb||E||1\n+ tr[\u0393>(D\u2212X\u2212E)]\n+ \u00b5\n2 ||D\u2212X\u2212E||2F , (28)\nwhere w is the coefficient variable in vector form, b = bem w = vec(W) and \u00b5 is a hyper parameter.\nWe summarized our method to solve the problem in Eq. (28) in Algorithm 2. The key steps include the computations of X(k) and E(k), where the derivation of both X(k) and E(k) are based on proximal gradient method.\nWe first solve E with X fixed, and the optimization problem is as follows,\nargmin E\n\u03bb||E||1 + tr[\u0393>(D\u2212X\u2212E)] (29)\n+ \u00b5\n2 ||D\u2212X\u2212E||2F ,\nwhich can be further written as argmin\nE g1(E) + h1(E), (30)\nwhere g1(E) = \u00b5 2 ||D \u2212 X \u2212 1 \u00b5 \u0393 \u2212 E||2F and h1(E) = \u03bb||E||1. Both of them are convex and their derivatives are\nLipschitz continuous. Thus, we use the proximal gradient method to update E with,\nE(k) = Proxtkh(E\u0302 (k) \u2212 tk 5 g(E\u0302(k))) (31)\nwhere Proxtkh1(X) = argmin U\n(tkh(U)+ 1\n2 ||U\u2212X||2F ) and\ntk is the size of a gradient step. After solving E, we proceed to solve X with E fixed. Similarly, we re-write the this subproblem as follows:\nargmin E g2(X) + h2(X), (32)\nwhere g2(X) = C||Xw+b\u2212y||22+ \u00b5 2 ||D\u2212X\u2212E\u2212 1 \u00b5 \u0393||2F and h2(X) = \u03b3||X||\u2217. Since both g2(X) and h2(X) are convex, g2(X) is smooth and h2(X) is non-smooth and their derivatives are Lipschitz continuous, we can update X by\nX(k) = Proxtkh2(X\u0302 (k) \u2212 tk 5 g(X\u0302(k))). (33)\nAlgorithm 2 ADMM for Subproblem 2 in Eq. (27) 1: Initialize X(\u22121) = X\u0302(0) \u2208 Rn\u00d7pq, E(\u22121) = E\u0302(0) \u2208\nRn\u00d7pq, \u0393(\u22121) = \u0393\u0302 \u2208 Rn\u00d7pq, \u03c1 > 0, t(0) = 1, \u03b7 \u2208 (0, 1), tk \u2208 [0, 1L ], L is Lipschits constant.\n2: for k = 0, 1, 2, 3... do 3: E(k) = Proxtkh1(E\u0302\n(k) \u2212 tk 5 g(E\u0302(k))) 4: X(k) = Proxtkh2(X\u0302\n(k) \u2212 tk 5 g(X\u0302(k))) 5: \u0393(k) = \u0393\u0302(k) \u2212 \u00b5(D\u2212X(k) \u2212E(k)) 6: c(k) = \u00b5\u22121||\u0393(k) \u2212 \u0393\u0302(k)||2F + \u00b5||X(k) \u2212 X\u0302(k)||2F 7: if c(k) < \u03b7c(k\u22121) then\n8: t(k+1) = 1 +\n\u221a 1 + 4(t)2\n2\n9: X\u0302(k+1) = X(k) + t(k)\u22121\nt(k+1) (X(k) \u2212X(k)\u22121)\n10: \u0393\u0302(k+1) = \u0393(k) + t(k)\u22121\nt(k+1) (\u0393(k) \u2212 \u0393(k)\u22121)\n11: else 12: t(k+1) = 1 13: X\u0302(k+1) = X(k\u22121) 14: \u0393\u0302(k+1) = \u0393(k\u22121) 15: c(k) = \u03b7\u22121c(k\u22121) 16: end if; 17: end for\nWe summarize our method for solving G-RMR in Algorithm 3, based on Algorithm 1 and Algorithm 2, .\nAlgorithm 3 Solve Robust Matrix Regression 1: Input: Input training data (X1, y1), ..., (Xn, yn) and related\nparameters for the solver. 2: repeat 3: Update (W, b) using Algorithm. 1; 4: Update X and E using Algorithm. 2; 5: until Convergence or maximum iteration number; 6: Output: The estimation of (W, b)."}, {"heading": "Experiments", "text": "In this section, we conduct extensive experiments with comparative studies. The proposed method is implemented by Matlab R2015a in a machine with four-core 3.7GHz CPU and 16GB memory. We investigate the performance of our RMR and G-RMR with comparison of two state-of-the-art methods: 1) The classical Support Vector Regression (SVR) (Smola and Scho\u0308lkopf, 2004); 2) Regularized Matrix Regression (R-GLM) (Zhou and Li, 2014).\nTo evaluate and compare the performance of these algorithms, we apply them on three empirical tasks. Firstly, we elaborate on the illustrative examples by examining different geometric and natural shapes on the regression matrix. Secondly, we apply them on real-world finical time series data, where each sample can be represented as a matrix. At last, we apply RMR and G-RMR on the application of human head pose estimation (Sherrah and Gong, 2001).\nTo evaluate the performance of each algorithm, we use the Relative Absolute Error (RAE) that measures error between true labels y and estimated labels y\u0302 with RAEy = ||y\u0302 \u2212 y||/||y||2. For compared method, SVR, RMR and G-RMR, we fix the coefficients C = 1\u00d7 103 and = 1\u00d7 10\u22122,. And all the hyper-parameters are selected via cross validation."}, {"heading": "Shape Recovery", "text": "We first conduct the illustrative examples by examining various signal shapes, where each of them represents a 64\u00d7 64 regression matrix. We use the regression matrix to generate each sample by the following equation:\nyi = tr(W>Xi) + b+ i, (34)\nwhere W is the regression matrix illustrated by a signal shape, (Xi, yi) is a randomly generated sample, b is a bias term and i is the noise term on label yi, which is sampled from Laplacian distribution (The probability distribu-\ntion function is P (x|\u00b5, \u03c3) = 1 2\u03c3 exp(\u2212|x\u2212 \u00b5| \u03c3\n) ). In the experiment, we randomly generate 1000 samples for 10 rounds. In each round, half of the samples are used for model training and the rest are for testing. Then we compute the mean and standard deviation of RAE error on classifier matrix W for each approach. And detailed comparison of our RMR and G-RMR with other methods are shown in Table 1, where column \u201cshape\u201d denotes the type of signals. The illustration of true signal shapes followed by the estimation results from the four methods can be found in the Fig. 1.\nIt can be clearly seen from the illustrative examples that our method outperforms R-GLM in recovering lower rank\nsignals, such as square, cross and T shape. Although they yield comparable results in recovering the high rank signal, it can be seen from Table 1 that our approaches still outperform R-GLM in terms of the RAE error on W quantitatively. It clearly shows that the hinge loss in our approaches for model fitting is more robust empirically. Besides, it can be observed that our RMR and G-RMR methods substantially outperforms the traditional SVR method for all illustrative examples, because the SVR fails to capture the correlation among each data matrix. Specifically, we further use the square shaped signal to display the RAE error along the solution path of the nuclear norm for matrix regression, which is shown in Fig. 2. It can be seen that, when the weight is larger than 0, the performance is better, which shows the effectiveness of incorporating the nuclear norm as penalty."}, {"heading": "Financial Time Series Data Analysis", "text": "We further evaluate our RMR on the financial time series data. We use daily price data (details can be found in the Table 2) from (Akbilgic, Bozdogan, and Balaban, 2014), where the prices are converted to returns. In this data set, there are 536 daily returns from January 5, 2009 to Feburary 22,\n2011. Particularly, the days are excluded when the Turkish stock exchange was closed.\nIntuitively, the value of an index in a certain date may be related to others as well as previous values. Thus, it is natural to process data in matrix form instead of a vector to preserve the latent topological structure of data. Besides, there exist many fluctuations in different indices due to the complicated stock market and many entries of the data are contaminated accordingly. Therefore, it is imperative to tackle the above issues with the proposed methods.\nBesides using the RAE for evaluation, we further use 2 extra criterions to evaluate our results on the financial data set, i.e., percentage of correctly predicted (PCP) days, which can interpret our results in a simple and logical manner, and the Dollar 100 (D100) criterion, which gives us the theoretical future value of $100 invested at the beginning of predicted term and traded accordingly. We use the first 30% days\u2019 data to train our model and predict the left 70% days\u2019 index values for evaluation.\nAs we can see from Table 3, our proposed methods outperform other state-of-the-art methods in terms of all 3 different criterions. Our approaches achieve better results than previous R-GLM, because there exist many fluctuations in everyday\u2019s stock exchange rates due to the complicated situations, and allowing a range of error for the returns can be more robust than directly applying the squared error for model fitting. Also, the SVR ignores the latent topological structure among different indices and historical data, and is beaten by our proposed methods. It can be observed that our G-RMR achieves a significant improvement over the RMR. This is because financial data always contain serious noise problems (Magdon-Ismail, Nicholson, and AbuMostafa, 1998) not only in the label but also in the data matrix entries. It shows that considering noise on predictors in G-RMR is also effective in certain real world situations."}, {"heading": "Head Pose Data Analysis", "text": "We further test the performance of our methods on the application of head pose estimation with dataset used in (Sherrah and Gong, 2001) . The dataset arises from a study to estimate the human pose via digital images captured from cameras, with 37 people in gray scale of 100 \u00d7 100 image size, which can be represented as data in matrix form naturally. Each person has 133 facial images covering a view sphere of \u00b190 degrees in yaw and \u00b130 degrees in tilt at 10 degrees increment. Several example images can be found in the Fig. 3. As discussed before, it is difficult to measure the real angle of human head accurately, thus, it can be expected that the label of each image may contain several small errors, resulting in the noisy label problem.\nFor each person, we keep the degree in tilt fixed and use the degree in yaw angle [0\u25e6, 10\u25e6, ...170\u25e6, 180\u25e6] as our label. We then set tilting degree of the face to 90\u25e6, which denotes the frontal face. Each image is cropped around the face and resized to 32\u00d7 32.\nThe numerical performance of each algorithm is shown in Table. 4, where column \u201cTrn#\u201d lists the corresponding\nnumber of training samples and column \u201cG-RMR (C)\u201d denotes that data used here is corrupted by adding white square blocks (10% samples are corrupted). On one hand, as shown in Table 4, our methods outperform all competitive cones in terms of RAE error on the head angle, because our methods take both the correlation within each data and the noisy label issue into consideration, resulting in more robust estimation of the model. On the other hand, our G-RMR also achieves competitive result on corrupted data compared with the result on normal data, which shows the robustness of our model on noisy entries."}, {"heading": "Conclusions", "text": "In this paper, we addressed the robust matrix regression issue with two novel methods proposed, i.e., RMR and GRMR. For RMR, we introduced the hinge loss for model fitting, to enhance the robustness of matrix regression methods against the problem of noisy labels. An ADMM algorithm was further derived for model training. As an extension of RMR, the G-RMR was proposed to take noisy predictors into consideration by clean matrix signal recovery during model training procedure. We also conducted extensive empirical studies to evaluate the performance of RMR and G-RMR, and our methods achieve state-of-the-art performance. It shows that our approaches can address realworld problems effectively."}], "references": [{"title": "A novel hybrid rbf neural networks model as a forecaster", "author": ["O. Akbilgic", "H. Bozdogan", "M.E. Balaban"], "venue": "Statistics and Computing 24(3):365\u2013375.", "citeRegEx": "Akbilgic et al\\.,? 2014", "shortCiteRegEx": "Akbilgic et al\\.", "year": 2014}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R", "citeRegEx": "Boyd et al\\.,? 2011", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM)", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Fast alternating direction optimization methods", "author": ["T. Goldstein", "B. O\u2019Donoghue", "S. Setzer", "R. Baraniuk"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "Goldstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 2014}, {"title": "On non-ergodic convergence rate of douglas\u2013rachford alternating direction method of multipliers", "author": ["B. He", "X. Yuan"], "venue": "Numerische Mathematik 130(3):567\u2013577.", "citeRegEx": "He and Yuan,? 2015", "shortCiteRegEx": "He and Yuan", "year": 2015}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics 12(1):55\u201367.", "citeRegEx": "Hoerl and Kennard,? 1970", "shortCiteRegEx": "Hoerl and Kennard", "year": 1970}, {"title": "Robust regression", "author": ["D. Huang", "R. Cabral", "F. De la Torre"], "venue": "IEEE transactions on pattern analysis and machine intelligence 38(2):363\u2013375.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Convergence of a generalized smo algorithm for svm classifier design", "author": ["S.S. Keerthi", "E.G. Gilbert"], "venue": "Machine Learning 46(1-3):351\u2013360.", "citeRegEx": "Keerthi and Gilbert,? 2002", "shortCiteRegEx": "Keerthi and Gilbert", "year": 2002}, {"title": "Support matrix machines", "author": ["L. Luo", "Y. Xie", "Z. Zhang", "W.-J. Li"], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Financial markets: very noisy information processing", "author": ["M. Magdon-Ismail", "A. Nicholson", "Y.S. Abu-Mostafa"], "venue": "Proceedings of the IEEE 86(11):2184\u20132195.", "citeRegEx": "Magdon.Ismail et al\\.,? 1998", "shortCiteRegEx": "Magdon.Ismail et al\\.", "year": 1998}, {"title": "Excess risk bounds for multitask learning with trace norm regularization", "author": ["A. Maurer", "M. Pontil"], "venue": "Conference on Learning Theory (COLT), volume 30, 55\u201376.", "citeRegEx": "Maurer and Pontil,? 2013", "shortCiteRegEx": "Maurer and Pontil", "year": 2013}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J Platt"], "venue": null, "citeRegEx": "Platt,? \\Q1998\\E", "shortCiteRegEx": "Platt", "year": 1998}, {"title": "Fusion of perceptual cues for robust tracking of head pose and position", "author": ["J. Sherrah", "S. Gong"], "venue": "Pattern Recognition 34(8):1565\u20131572.", "citeRegEx": "Sherrah and Gong,? 2001", "shortCiteRegEx": "Sherrah and Gong", "year": 2001}, {"title": "A tutorial on support vector regression", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "Statistics and computing 14(3):199\u2013222.", "citeRegEx": "Smola and Sch\u00f6lkopf,? 2004", "shortCiteRegEx": "Smola and Sch\u00f6lkopf", "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 267\u2013288.", "citeRegEx": "Tibshirani,? 1996", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Theoretical and experimental analyses of tensor-based regression and classification", "author": ["K. Wimalawarne", "R. Tomioka", "M. Sugiyama"], "venue": "Neural computation 28(4):686\u2013715.", "citeRegEx": "Wimalawarne et al\\.,? 2016", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2016}, {"title": "Modeling appearances with low-rank svm", "author": ["L. Wolf", "H. Jhuang", "T. Hazan"], "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition, 1\u20136. IEEE.", "citeRegEx": "Wolf et al\\.,? 2007", "shortCiteRegEx": "Wolf et al\\.", "year": 2007}, {"title": "Regularized matrix regression", "author": ["H. Zhou", "L. Li"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76(2):463\u2013483.", "citeRegEx": "Zhou and Li,? 2014", "shortCiteRegEx": "Zhou and Li", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form.", "startOffset": 68, "endOffset": 93}, {"referenceID": 14, "context": "Introduction Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form.", "startOffset": 104, "endOffset": 122}, {"referenceID": 17, "context": "When using traditional regression methods to process these data, we have to reshape them into vectors, which may destroy the latent topological structural information, such as the correlation between different channels for EEG data (Zhou and Li, 2014), and the spatial relation within an image (Wolf, Jhuang, and Hazan, 2007).", "startOffset": 232, "endOffset": 251}, {"referenceID": 17, "context": "One such model is the regularized matrix regression (RGLM) (Zhou and Li, 2014).", "startOffset": 59, "endOffset": 78}, {"referenceID": 12, "context": "As an example, consider the problem of head pose estimation (Sherrah and Gong, 2001), where for each data pair, the predictor is a two dimensional digital image for the head of a person, while the output denotes the angle of his head.", "startOffset": 60, "endOffset": 84}, {"referenceID": 13, "context": "the support vector regression model (Smola and Sch\u00f6lkopf, 2004), it is an algorithm based on vector-form data, which can ruin the latent structure for matrix regression problem.", "startOffset": 36, "endOffset": 63}, {"referenceID": 17, "context": "The R-GLM approach (Zhou and Li, 2014) addresses this issue by enforcing the regression matrix W to be low-rank representable with nuclear norm penalty.", "startOffset": 19, "endOffset": 38}, {"referenceID": 13, "context": "Similar residual modeling approach has also been used in the method of support vector regression (Smola and Sch\u00f6lkopf, 2004).", "startOffset": 97, "endOffset": 124}, {"referenceID": 8, "context": "Differently, our method can capture such latent structure by incorporating the spectral elastic net penalty (Luo et al., 2015) into the regression matrix W, which can model the correlation of each data matrix effectively.", "startOffset": 108, "endOffset": 126}, {"referenceID": 8, "context": "with 1 2 tr(W>W) + \u03c4 ||W||\u2217 as the spectral elastic net penalty, we incorporate low-rank property into W and consider the group effect of the eigenvalues, to capture the latent structures among data matrices (Luo et al., 2015).", "startOffset": 208, "endOffset": 226}, {"referenceID": 17, "context": "Solver As the object function contains both hinge loss and nuclear norm, the Nesterov method used in R-GLM (Zhou and Li, 2014) is no longer available because the derivative of our loss function is not Lipschitz-continuous.", "startOffset": 107, "endOffset": 126}, {"referenceID": 1, "context": "Nevertheless, since our model is convex with respect to both W and b, we here derive an efficient learning algorithm based on Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011) with the restart rule (Goldstein et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 3, "context": ", 2011) with the restart rule (Goldstein et al., 2014) to solve the optimization problem.", "startOffset": 30, "endOffset": 54}, {"referenceID": 7, "context": "In this way, we can get W and b in each iteration with sequential minimization optimization algorithm (Keerthi and Gilbert, 2002; Platt and others, 1998).", "startOffset": 102, "endOffset": 153}, {"referenceID": 3, "context": "Because the hinge loss and nuclear norm are weakly convex, the convergence property of Algorithm 1 can be proved immediately based on the result in (Goldstein et al., 2014; He and Yuan, 2015).", "startOffset": 148, "endOffset": 191}, {"referenceID": 4, "context": "Because the hinge loss and nuclear norm are weakly convex, the convergence property of Algorithm 1 can be proved immediately based on the result in (Goldstein et al., 2014; He and Yuan, 2015).", "startOffset": 148, "endOffset": 191}, {"referenceID": 10, "context": "Let R(W) and R\u0302(W) be the empirical risk and expected risk respectively (Maurer and Pontil, 2013).", "startOffset": 72, "endOffset": 97}, {"referenceID": 10, "context": "Thus, following Maurer and Pontil (2013), with the Gordan\u2019s theorem, we have E||M\u0303||\u2217 \u2264 \u221a \u03c9( \u221a p+ \u221a q)", "startOffset": 16, "endOffset": 41}, {"referenceID": 2, "context": "Model As discussed in (Cand\u00e8s et al., 2011), it is common for natural signals to contain correlation empirically.", "startOffset": 22, "endOffset": 43}, {"referenceID": 13, "context": "We investigate the performance of our RMR and G-RMR with comparison of two state-of-the-art methods: 1) The classical Support Vector Regression (SVR) (Smola and Sch\u00f6lkopf, 2004); 2) Regularized Matrix Regression (R-GLM) (Zhou and Li, 2014).", "startOffset": 150, "endOffset": 177}, {"referenceID": 17, "context": "We investigate the performance of our RMR and G-RMR with comparison of two state-of-the-art methods: 1) The classical Support Vector Regression (SVR) (Smola and Sch\u00f6lkopf, 2004); 2) Regularized Matrix Regression (R-GLM) (Zhou and Li, 2014).", "startOffset": 220, "endOffset": 239}, {"referenceID": 12, "context": "At last, we apply RMR and G-RMR on the application of human head pose estimation (Sherrah and Gong, 2001).", "startOffset": 81, "endOffset": 105}, {"referenceID": 12, "context": "We further test the performance of our methods on the application of head pose estimation with dataset used in (Sherrah and Gong, 2001) .", "startOffset": 111, "endOffset": 135}], "year": 2016, "abstractText": "Modern technologies are producing datasets with complex intrinsic structures, and they can be naturally represented as matrices instead of vectors. To preserve the latent data structures during processing, modern regression approaches incorporate the low-rank property to the model, and achieve satisfactory performance for certain applications. These approaches all assume that both predictors and labels for each pair of data within the training set are accurate. However, in real world applications, it is common to see the training data contaminated by noises, which can affect the robustness of these matrix regression methods. In this paper, we address this issue by introducing a novel robust matrix regression method. We also derive efficient proximal algorithms for model training. To evaluate the performance of our methods, we apply it on real world applications with comparative studies. Our method achieves the state-of-the-art performance, which shows the effectiveness and the practical value of our method. Introduction Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form. However, with the development of modern technology, it is common to meet datasets with sample unit not in vector form but instead in matrix form. Examples include the two-dimensional digital images, with quantized values of different colors at certain rows and columns of pixels; and electroencephalography (EEG) data with voltage fluctuations at multiple channels over a period of time. When using traditional regression methods to process these data, we have to reshape them into vectors, which may destroy the latent topological structural information, such as the correlation between different channels for EEG data (Zhou and Li, 2014), and the spatial relation within an image (Wolf, Jhuang, and Hazan, 2007). To tackle this issue, several methods have been proposed to perform regression on data in matrix form directly. One such model is the regularized matrix regression (RGLM) (Zhou and Li, 2014). Given a dataset {Xi, yi}i=1, where N is the sample size, Xi \u2208 Rp\u00d7q denotes the ith data matrix as predictor, and yi \u2208 R is the corresponding output, the R-GLM model aims to learn a function f : Rp\u00d7q \u2192 R to identify the output given a newly observed data matrix with yi = tr(WXi) + b+ , (1) where tr(\u00b7) represents the trace of a matrix, W is the regression matrix with low-rank property to preserve structural information of each data matrix, and b denotes the offset. is zero-mean Gaussian noise to model the small uncertainty of the output. With this setting, the R-GLM has achieved satisfying results in several applications. However, there still exist certain issues that should be further addressed. Firstly, RGLM uses the Gaussian noise for model fitting, and take all deviations of predicted values from labels into account. This setting can be reasonable in certain cases, but may not make sense for particular applications. As an example, consider the problem of head pose estimation (Sherrah and Gong, 2001), where for each data pair, the predictor is a two dimensional digital image for the head of a person, while the output denotes the angle of his head. Because the real angle of head cannot be measured precisely, there should exist certain deviations of provided labels from the real ones empirically. In this case, the regression model should be able to tolerant such small deviations instead of taking them all into account. Another important issue is that, the predictors are also assumed to be noise free, which can be irrational in certain applications. Practically, it is common to see signals corrupted by noise, such as image signals with occlusion, specular reflections or noise (Huang, Cabral, and De la Torre, 2016), and financial data with noise (Magdon-Ismail, Nicholson, and Abu-Mostafa, 1998). Thus, it is important for a regression model to be tolerant of noise on predictors and labels to enhance its robustness empirically. In this paper, we introduce two novel matrix regression methods to tackle the above mentioned issues. We first propose a \u201cRobust Matrix Regression\u201d (RMR) to tackle the noisy label problem, by introducing hinge loss to model the uncertainty of regression labels. In this way, our method only considers error larger than a pre-specified value, and can tolerate error around each labeled output within a small range. This approach is also favored for other advantages in certain scenarios. As an example, in applications like financial time-series prediction, it is common to require not to lose more than money when dealing with data like exchange rates, and this issue can be well addressed with our setting. Even though the hinge loss error has been used in ar X iv :1 61 1. 04 68 6v 1 [ cs .L G ] 1 5 N ov 2 01 6 the support vector regression model (Smola and Sch\u00f6lkopf, 2004), it is an algorithm based on vector-form data, which can ruin the latent structure for matrix regression problem. We then propose efficient ADMM method to solve the optimization problem iteratively. To further enhance the robustness of RMR with noisy predictors, we propose a generalized RMR (G-RMR) by decomposing each data matrix as latent clean signal plus sparse outliers. For model training, we also derive a proximal algorithm to estimate both the regression matrix and latent clean signals iteratively. To evaluate the performance of our methods, we conduct extensive experiments on both approaches with comparison of state-of-the-art ones. Our methods achieve superior performance consistently, which shows their efficiency in real world problems. Notations: We present the scalar values with lower case letters (e.g., x); vectors by bold lower case letters (e.g., x); and matrix by bold upper case letters (e.g., X). For a matrix X, its (i, j)-entity is represented as Xi,j . tr(\u00b7) denotes the trace of a matrix, and {a}+ = max(0, a). We further set ||X||F and ||X||\u2217 as the Frobenius norm and nuclear norm of a matrix X respectively. Robust Matrix Regression We first introduce the RMR model to address the noisy label problem, with an ADMM algorithm for model training. Model For matrix regression, classical techniques need to reshape each matrix Xi into a vector xi, which will destroy its intrinsic structures, resulting in the loss of information. The R-GLM approach (Zhou and Li, 2014) addresses this issue by enforcing the regression matrix W to be low-rank representable with nuclear norm penalty. However, this method is based on the Gaussian loss, which may affect the robustness with existence of noisy labels. To tackle this issue, an intuitive idea is to ignore noises within a small margin {\u2212 , } around each label for robust model fitting. Motivated by this idea, we propose our RMR, by introducing the hinge loss for model fitting, where the residual corresponding to each data Xi is defined as follows hi(W, b) = (|tr(WXi) + b\u2212 yi| \u2212 )+. (2) With the above formulation of residuals, when learning the regression model, our approach only takes residuals larger than into account, thus, the labels contaminated by noise within a small margin is tolerable accordingly. Similar residual modeling approach has also been used in the method of support vector regression (Smola and Sch\u00f6lkopf, 2004). However, this approach is proposed for vector data regression and cannot capture the latent structure within each data matrix. Differently, our method can capture such latent structure by incorporating the spectral elastic net penalty (Luo et al., 2015) into the regression matrix W, which can model the correlation of each data matrix effectively. And the corresponding optimization problem is defined as follows argmin W,b H(W, b) + \u03c4 ||W||\u2217 (3) where H(W, b) = 1 2 tr(W>W)", "creator": "LaTeX with hyperref package"}}}