{"id": "1611.04847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "The Power of Side-information in Subgraph Detection", "abstract": "we illustrate an erdos - renyi graph with $ n $ nodes and edge probability $ q $ that is embedded with a random subgraph of size $ k $ with edge probabilities $ \u03c0 $ such that $ exists & returns ; q $. implementations address the problem of detecting the subgraph nodes when only the graph edges are activated, along with some extra knowledge explaining a small fraction of subgraph nodes, called cued vertices or cues. we employ a local posterior seasonal detector called belief propagation ( bp ). recent models on subgraph detection without cues have asserted that global item likelihood ( ml ) detection strictly measures bp in terms of asymptotic error rate, namely, there assumes a threshold condition that the subgraph constraint should satisfy means which bp fails in achieving asymptotically zero parameters, but ml succeeds. in contrast, we show sensitivity when the fraction of cues is strictly bounded away from zero, i. e., when there exists non - trivial side - feedback, bp achieves zero asymptotic error even below this threshold, thus computing the minimum of ml detection.", "histories": [["v1", "Thu, 10 Nov 2016 10:13:10 GMT  (932kb)", "http://arxiv.org/abs/1611.04847v1", null], ["v2", "Wed, 23 Nov 2016 09:45:20 GMT  (864kb)", "http://arxiv.org/abs/1611.04847v2", null], ["v3", "Mon, 6 Mar 2017 13:26:53 GMT  (811kb)", "http://arxiv.org/abs/1611.04847v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["arun kadavankandy", "konstantin avrachenkov", "laura cottatellucci", "rajesh sundaresan"], "accepted": false, "id": "1611.04847"}, "pdf": {"name": "1611.04847.pdf", "metadata": {"source": "CRF", "title": "Subgraph Detection with cues using Belief Propagation", "authors": ["Rajesh Sundaresan", "A.Kadavankandy", "K. Avrachenkov", "L. Cottatellucci"], "emails": ["arun.kadavankandy@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 84\n7v 1\n[ cs\n.L G\n] 1\n0 N\nov 2\n01 6\nIS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- X\nX X\n-- F\nR +\nE N\nG\nRESEARCH REPORT\nN\u00b0 XXX Oct 2016\nProject-Team Maestro\nSubgraph Detection with cues using Belief Propagation A.Kadavankandy, K. Avrachenkov, L. Cottatellucci, and Rajesh Sundaresan.\nRESEARCH CENTRE SOPHIA ANTIPOLIS \u2013 M\u00c9DITERRAN\u00c9E\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nSubgraph Detection with cues using Belief\nPropagation\nA.Kadavankandy\u2217, K. Avrachenkov,\nL. Cottatellucci, and Rajesh Sundaresan.\nProject-Team Maestro\nResearch Report n\u00b0 XXX \u2014 Oct 2016 \u2014 17 pages\nAbstract: We consider an Erd\u0151s-R\u00e9nyi graph with n nodes and edge probability q that is embedded with a random subgraph of size K with edge probabilities p such that p > q. We address the problem of detecting the subgraph nodes when only the graph edges are observed, along with some extra knowledge of a small fraction of subgraph nodes, called cued vertices or cues. We employ a local and distributed algorithm called belief propagation (BP). Recent works on subgraph detection without cues have shown that global maximum likelihood (ML) detection strictly outperforms BP in terms of asymptotic error rate, namely, there is a threshold condition that the subgraph parameters should satisfy below which BP fails in achieving asymptotically zero error, but ML succeeds. In contrast, we show that when the fraction of cues is strictly bounded away from zero, i.e., when there exists non-trivial side-information, BP achieves zero asymptotic error even below this threshold, thus approaching the performance of ML detection.\nKey-words: Belief Propagation, Subgraph Detection, Semisupervised Learning, Random Graphs\n\u2217 Corresponding author, arun.kadavankandy@inria.fr\nLa Detection de Sousgraphes en presence des indices gr\u00e2ce\nau Belief Propagation\nR\u00e9sum\u00e9 : Nous consid\u00e9rons un graphe Erd\u0151s-R\u00e9nyi qui a n sommets dont q est la probabilit\u00e9 d\u2019arr\u00eates. La dessus il y un sousgraphe plac\u00e9 sur leurs m sommets selectionn\u00e9s al\u00e9atoirement et leur probabilit\u00e9 d\u2019arr\u00eates est p, en sorte que p > q. Nous proposons un algorithme distribu\u00e9 aux calculs locales \u00e0 chaque sommet, tir\u00e9 du \u201cBelief Propagation\u201d (BP), qui d\u00e9tecte les sommets du sousgraphe, quand on connait une fraction de sommets du sousgraphe en tant qu\u2019indices. Des recherches r\u00e9centes ont prouv\u00e9 que la prestation du BP dans l\u2019absence des indices est strictement inf\u00e9rior par rapport \u00e0 la detection globale du maximum de vraisemblance (DMV). A l\u2019oppos\u00e9, ici on prouve qu\u2019en presence des indices, la prestation du BP est \u00e0 l\u2019hauteur de celle de DMV, dans la sens o\u00f9 le premier reussie \u00e0 detecter la sousgraphe avec une erreur qui tend a z\u00e9ro, \u00e0 chaque fois le dernier peut le faire, dans la limite o\u00f9 le nombre de sommets du graph tend l\u2019infinit\u00e9.\nMots-cl\u00e9s : Belief Propagation, Detection de Sousgraphes, Semisupervised Learning, Graphes Al\u00e9atoires\nSubgraph Detection with cues using Belief Propagation 3\n1 Introduction\nDetecting a small community of highly connected nodes in a sparse network is an important problem in data mining, machine learning, and theoretical computer science. This problem is linked to threat detection, anomaly detection, fault detection etc. in a network. Please see [1] for a survey. The hidden subgraph model with both the subgraph and the background modelled as ER graphs with different edge densities was proposed in [10] to study anomalous transactions in a computer network.\nIn this model the background graph is ER with n nodes and edge probability q. A random subset of K < n/2 vertices has the edge probabilities within it changed to p > q, without affecting any other edge. This graph, denoted G(K,n, p, q), can model a network with a hidden community [10]. See [8,9] for works on detecting the presence of such a subgraph in a given graph and detecting the subgraph nodes. A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]). When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).\nWe use the framework of analysis developed in [7] and [11]. In [11] the author considers the problem of detecting the hidden community in G(K,n, p, q) under the assumption that p = a/n, q = b/n and K = \u03ba/n, where a, b, \u03ba are constants. A parameter \u03bb (defined later) is introduced to characterise the \u201cstrength\u201d of the subgraph. They develop a local BP algorithm and show it achieves zero asymptotic error when \u03bb > 1/exp(1), whereas if \u03bb \u2264 1/exp(1), BP does not do better than random guessing. In contrast, the ML detector achieves zero error asymptotically for any \u03bb > 0. In [7], the authors consider a more general setting of sparse graphs and prove that BP succeeds when \u03bb > 1/e. In [6] global ML detection is shown to achieve zero error rate asymptotically if \u03bb = \u0398((K/n) log(K/n)).\nThe optimality of BP with side information is shown in [12] and [4] for community detection on SBM with symmetric communities, but to the best of our knowledge no theoretical studies of BP have been made when cued vertices are available for detecting a small subgraph in G(K,n, p, q).\nOur contributions: In our work we consider the subgraph detection problem where some side information in the form of cued nodes is available. This fits within the framework of semisupervised learning. We develop a BP algorithm that detects the nodes of the subgraph in the presence of cues and prove that when the graph is dilute, with p, q = \u0398(1/n), the fraction of miss-classified nodes approaches zero for any \u03bb > 0 when there is a strictly positive fraction of cues. In other words we show that BP with cues succeeds in the entire regime where ML succeeds [11, Proposition 4.1].\nThe paper is organised as follows: In section 2 we describe our graph model and the problem. In section 3, we present our algorithm and its derivation. In section 4 we derive the asymptotic distribution of BP messages. In section 5 we prove our main result on the asymptotic error rate of our algorithm. In section 6 we present some simulation results to back up the theory.\n2 Model and Problem Definition\nLet G = (V,E) be a realisation of G(K,n, p, q). Let S be the set of subgraph nodes and C be the set of cued nodes. The latter is chosen from S by independent Bernoulli sampling with probability (w.p.) \u03b1 < 1. Let p = a/n and q = b/n, where a and b are constants independent of n. Such graphs, with average degree O(1) are called dilute graphs. The results in this paper presuppose that \u03ba = K/n is a constant independent of n. Our aim is to propose a candidate set S\u0302 given G and C, assuming p, q,K and \u03b1 are known, using local and recursive updates provided\nRR n\u00b0 XXX\n4 Arun et al\nby BP. Note that this problem is identical to detecting the hidden labels \u03c3i of the graph nodes assigned such that \u03c3i = 1 if i \u2208 S and \u03c3i = 0 otherwise.\nNotation and Nomenclature: A graph node is denoted by a lower case letter such as i. The graph distance between two nodes i and j is the length of the shortest sequence of edges to go from i to j. The neighbourhood of a node i, denoted by \u03b4i is the set of one-hop neighbours of i, i.e., nodes that are at a graph distance of one. Similarly, we also work with t\u2212 hop neighbours of i, which are the set of nodes at a distance of t from i. We use the following symbols to denote set operations: C = A\\B is the set of elements that belong to A and not B when B \u2282 A, and \u2206 denotes the set difference, i.e., A\u2206B = (A\u222aB)\\(A\u2229B). The symbol \u223c denotes the distribution of a random variable (rv), for example X \u223c Poi(\u03bb) means that X is a Poisson distributed rv with rate \u03bb. Also, N (\u00b5, \u03c32) denotes the Gaussian distribution with mean \u00b5 and variance \u03c32. The symbol D\u2212\u2192 denotes convergence in distribution [3].\n3 Belief Propagation Algorithm for Detection in the Pres-\nence of Cues\nIn this section we describe the local and distributed BP algorithm (1), which performs detection in the presence of side-information available in the form of cued nodes. The algorithm has two stages: message passing (2), and belief updation (1). At step t of Algorithm 1, each node u \u2208 V \\C updates its own log-likelihood ratio:\nRtu = log\n( P(Gtu, C t u|\u03c3u = 1)\nP(Gtu, C t u|\u03c3u = 0)\n) ,\nwhere Gtu denotes the subgraph induced by the t-neighbourhood of u and C t u is the set of cues in Gtu. This computation is local, because it uses only messages transmitted to u by its neighbours i \u2208 \u03b4u, given by\nRti\u2192u = log\n( P(Gti, C t i |\u03c3i = 1)\nP(Gti, C t i |\u03c3i = 0)\n) ,\nwhere Gti and C t i are defined as done for u. It can be checked that the total computation time for tf steps of BP is O(tf |E|). Recall that the optimum detector that minimises the expected number of misclassified nodes is the ML detector [6] given as:\n\u03c3\u0302i = 1{Ri>log(n\u2212K/K(1\u2212\u03b1))},\nwhere\nRi = log P(G,C|\u03c3i = 1) P(G,C|\u03c3i = 0) .\nThe output set size here may not be exactly equal to K, but this can be mitigated by some post-processing (for example, if the size is larger than K, we can simply pick the top K values of the likelihood function). This detector however requires the observation of the whole graph, and cannot be implemented in a distributed fashion. In addition, it is not computationally feasible, since it requires marginalising over 2n pair-wise dependent random variables over a large graph. We would like algorithms that, for a decision at a node u of the graph, rely only on the observation of the t-neighbourhood Gtu of u, BP being one of them.\nA small neighbourhood of a large sparse graph can be approximated by a tree. This is formalised in Lemma 1. In the following we present a Poisson random tree, which can be coupled with Gtu.\nInria\nSubgraph Detection with cues using Belief Propagation 5\nAlgorithm 1 BP with cues\n1: Initialize: Set R0i\u2192j to 0, for all (i, j) \u2208 E. Let tf < log(n)log(np) + 1. Set t = 0. 2: For all directed pairs (i, u) \u2208 E, such that u /\u2208 C:\nRt+1i\u2192u = \u2212K(p\u2212 q) + \u2211\nl\u2208Ci,l 6=u log\np q +\n\u2211\nl\u2208\u03b4i\\Ci,l 6=u log exp(Rtl\u2192i \u2212 \u03bd)(p/q)(1 \u2212 \u03b1) + 1 exp(Rtl\u2192i \u2212 \u03bd)(1 \u2212 \u03b1) + 1\n(1)\n3: If t < tf \u2212 1 go back to 2, else go to 4 4: Compute R tf u for every u \u2208 V \\C as follows:\nRt+1u = \u2212K(p\u2212 q) + \u2211\nl\u2208Cu log\np q +\n\u2211\nl\u2208\u03b4u\\Cu log exp(Rtl\u2192u \u2212 \u03bd)(p/q)(1 \u2212 \u03b1) + 1 exp(Rtl\u2192u \u2212 \u03bd)(1 \u2212 \u03b1) + 1\n(2)\n5: Output S\u0302 as the union of C and the K \u2212 |C| set of nodes in V \\C with the largest values of R tf u .\nLet T tu be a labelled Galton-Watson (G-W) tree of depth t rooted at node u constructed as follows (as in [7]): The label \u03c4u at node u is chosen at random in the following way:\nP{\u03c4u = 1} = K\nn P{\u03c4u = 0} = n\u2212K n .\nThe number of children Nu of the root u is Poisson-distributed with mean d1 = Kp+(n\u2212K)q if \u03c4u = 1 and mean d0 = nq if \u03c4u = 0. Each child is also assigned a label. The number of children i with label \u03c4i = 1 is Poisson distributed with mean Kp if \u03c4u = 1 and mean Kq if \u03c4i = 0. The number of children with label \u03c4i = 0 is Poisson distributed with mean (n\u2212K)q for both \u03c4u = 0 and \u03c4u = 1. By the independent splitting property of Poisson rvs, this is equivalent to assigning the label \u03c4i = 1 to each child i by sampling a Bernoulli rv with probability (w.p.) Kp/d1 if \u03c4u = 1 and Kq/d0 if \u03c4u = 0. Similarly \u03c4i = 0 w.p. (n \u2212K)q/d1 and (n \u2212K)q/d0 for \u03c4u = 0, 1 and 1 respectively. Namely, if i is a child of u,\nP(\u03c4i = 1|\u03c4u = 1) = Kp\nd1 , P(\u03c4i = 1|\u03c4u = 0) =\nKq\nd0 . (3)\nWe then assign the cue indicator function c such that ci = 1 w.p. \u03b1 if \u03c4i = 1 and ci = 0 if \u03c4i = 0. The process is repeated up to depth t giving us C t u, the set of cued neighbours.\nConsider the problem of estimating the label \u03c4u of node u /\u2208 C based on an observation of T tu and Ctu. The optimal ML detector is given as\n\u03c4\u0302u = 1{\u039btu>log( (n\u2212K) K(1\u2212\u03b1) )},\nwhere \u039btu = log(P(T t u, C t u|\u03c4u = 1)/P(T tu, Ctu|\u03c4u = 0)). By the following coupling lemma established in [7], the detection of label \u03c3u based on G t u is statistically identical to the detection of \u03c4u based on T tu :\nRR n\u00b0 XXX\n6 Arun et al\nLemma 1 [7] For t such that (np)t = no(1), there exists a coupling such that (Gtu, \u03c3 t) = (T tu, \u03c4 t) with probability 1\u2212 n\u22121+o(1). In our case since p = a/n, any t = o(log(n)) satisfies the condition of the above lemma.\nConsequently, the likelihood ratios in a small neighbourhood Gtu of u are statistically identical to the likelihoods derived on the corresponding G-W tree, which are the BP messages. Hence we proceed by deriving BP recursions in Algorithm 1 for node u assuming Gtu is a tree. Consider a node u \u2208 V \\C. We can express the likelihood ratio at u based on an observation of T t+1u , Ct+1u as\n\u039bt+1u = log P(T t+1u , C t+1 u |\u03c4u = 1)\nP(T t+1u , C t+1 u |\u03c4u = 0)\n= log P{Nu|\u03c4u = 1} P{Nu|\u03c4u = 0} + \u2211\ni\u2208\u03b4u log\nP(T ti , ci, C t i |\u03c4u = 1) P(T ti , ci, C t i |\u03c4u = 0) , (4)\nby independence of the children of u given \u03c4u. Moreover, P(Nu|\u03c4u = 1) = dNu1 e\u2212d1/Nu! , and similarly for P(Nu|\u03c4u = 0). Therefore we have\nlog P{Nu|\u03c4u = 1} P{Nu|\u03c4u = 0} = Nu log d1 d0 \u2212 (d1 \u2212 d0)\n= Nu log d1 d0 \u2212K(p\u2212 q). (5)\nNext we look at the second term in (4). We analyse separately the cued neighbours of u and the non-cue neighbours.\nCase 1 ( ci = 1): We have\nlog P(T ti , ci, C t i |\u03c4u = 1)\nP(T ti , ci, C t i |\u03c4u = 0)\n(6)\n= log   P(T ti ,ci,C t i ,\u03c4i=1|\u03c4u=1)+ 0 P(T ti ,ci,C t i ,\u03c4i=0|\u03c4u=1)\nP(T ti ,ci,C t i ,\u03c4i=1|\u03c4u=0)+ 0 P(T ti ,ci,C t i ,\u03c4i=0|\u03c4u=0)\n \n(a) = log\n( P(T ti , ci, C t i , \u03c4i = 1|\u03c4u = 1)\nP(T ti , ci, C t i , \u03c4i = 1|\u03c4u = 0)\n)\n= log\n( P(T ti , ci, C t i |\u03c4i = 1)P(\u03c4i = 1|\u03c4u = 1)\nP(T ti , ci, C t i |\u03c4i = 1)P(\u03c4i = 1|\u03c4u = 0)\n)\n(b) = log Kp/d1 Kq/d0 , (7)\nwhere in step (a) we applied the fact that P(ci = 1, \u03c4i = 0) = 0 and in (b) we used (3). Case 2 (ci = 0): Observe that P(ci = 0|\u03c4i = 1) = 1\u2212 \u03b1 and P(ci = 0|\u03c4i = 0) = 1. Note that\nP(T ti , ci, C t i |\u03c4u = 1) (8)\n= P(T ti , C t i |\u03c4i = 1)P(ci|\u03c4i = 1)P(\u03c4i = 1|\u03c4u = 1)\n+P(T ti , C t i |\u03c4i = 0)P(ci|\u03c4i = 0)P(\u03c4i = 0|\u03c4u = 1)\n= P(T ti , C t i |\u03c4i = 1)(1\u2212 \u03b1)\nKp\nd1 + P(T ti , C t i |\u03c4i = 0) (n\u2212K)q d1 .\nInria\nSubgraph Detection with cues using Belief Propagation 7\nSimilarly, we can show\nP(T ti , ci, C t i |\u03c4u = 0)\n= P(T ti , C t i |\u03c4i = 1)(1\u2212 \u03b1)\nKq\nd0\n+P(T ti , C t i |\u03c4i = 0) (n\u2212K)q d0 .\nLet us define\n\u039bti\u2192u \u2261 log ( P(T ti , C t i |\u03c4i = 1)\nP(T ti , C t i |\u03c4i = 0)\n) ,\nthe message that i sends to u at step t. We plug this into (8). Finally combining (5), (7) and (8) and replacing \u039btu with R t u and \u039b t i\u2192u with R t i\u2192u, we arrive at (2). The recursive equation (1) can be derived in exactly the same way by looking at the children of i \u2208 \u03b4u.\n4 Asymptotic Error Analysis\nWe analyse the distributions of BP messages \u039bti given \u03c4i = 1 and \u03c4i = 0. This will help us to bound the error rate on a tree. This equals the error rate on G asymptotically since by the coupling Lemma 1 the two are the same with a probability that tends to 1. Notice that since we only focus on non-cued vertices the prior distribution after the observation of cues changes. Therefore P{\u03c4i = 1|ci = 0} = K(1\u2212\u03b1)/(n\u2212K\u03b1) and P{\u03c4i = 0|ci = 0} = (n\u2212K)/(n\u2212K\u03b1) are the prior probabilities of the uncued vertices. For convenience we put a line over the symbols for expectation and probability to denote conditioning w.r.t {ci = 0} when considering the posterior distributions (eg: E,P). Define \u03c5 = log ( n\u2212K\nK(1\u2212\u03b1)\n) .\nInstead of studying the distribution of \u039bti, i \u2208 V \\C, we look at the log of the ratio of the a-posteriori probabilities of \u03c4i given as\n\u039b\u0303ti = log ( P(\u03c4i = 1|T ti , Cti , ci = 0) P(\u03c4i = 0|T ti , Cti , ci = 0) ) .\nThis is just a matter of choice since by Bayes rule it holds that \u039b\u0303ti = \u039b t i \u2212 \u03c5. Let \u03bet+10 , \u03bet+11 be the random variables with the same distribution as the messages \u039b\u0303t+1i given \u03c4i = 0 and \u03c4i = 1 respectively, conditioned on {ci = 0}, in the limit as n \u2192 \u221e. In view of the coupling formulation, it is then straightforward to show that they satisfy the following two recursive distributional evolutionary equations with initial conditions \u03be00 = \u03be 0 1 = log \u03ba(1\u2212 \u03b1)/(1\u2212 \u03ba):\n\u03be (t+1) 0 D = h+\nL0c\u2211\ni=1\nlog p\nq +\nL00\u2211\ni=1\nf(\u03be (t) 0,i) +\nL01\u2211\ni=1\nf(\u03be (t) 1,i) (9)\n\u03be (t+1) 1 D = h+\nL1c\u2211\ni=1\nlog p\nq +\nL10\u2211\ni=1\nf(\u03be (t) 0,i) +\nL11\u2211\ni=1\nf(\u03be (t) 1,i), (10)\nwhere, D = means that the L.H.S has the same distribution as the R.H.S. and h = \u2212K(p\u2212 q) \u2212 log( n\u2212KK(1\u2212\u03b1) ) = \u2212\u03ba(a\u2212 b)\u2212 log( 1\u2212\u03ba\u03ba(1\u2212\u03b1)) and the function f is defined as\nf(\u00b7) \u2261 log ( exp(\u00b7)(p/q) + 1\nexp(\u00b7) + 1\n) .\nRR n\u00b0 XXX\n8 Arun et al\nThe rvs \u03bet0,i are independent and identically distributed (iid) and identically distributed to \u03be t 0, and \u03bet1,i are iid with the same distribution as \u03be t 1. Furthermore, L00 \u223c Poi((n\u2212K)q) is the rv that equals the number of children of u with label 0 if \u03c4u = 0, and L01 \u223c Poi(Kq(1\u2212\u03b1)), the number of children with label 1 when \u03c4u = 0. Similarly L10 \u223c Poi((n\u2212K)q) and L11 \u223c Poi(Kp(1\u2212 \u03b1)) denote the number of children of u with label 0 and 1 respectively when \u03c4u = 1. Lastly, L0c and L1c are the number of cued children of u when \u03c4u = 0 and \u03c4u = 1 respectively with L0c \u223c Poi(Kq\u03b1) and L1c \u223c Poi(Kp\u03b1). We define the parameter \u03bb, interpreted as an effective SNR [11] of the detection problem, as\n\u03bb = K2(p\u2212 q)2 (n\u2212K)q (11)\n= \u03ba2(a\u2212 b)2 (1\u2212 \u03ba)b = \u03ba2b(a/b\u2212 1)2 (1\u2212 \u03ba) . (12)\nIf P0 and P1 are the probability measures of \u03be t 0 and \u03be t 1 respectively, then they are related as\nfollows.\nLemma 2\ndP0 dP1 (\u03be) = \u03ba(1\u2212 \u03b1) 1\u2212 \u03ba exp(\u2212\u03be).\nIn other words for any integrable function g(\u00b7)\nE[g(\u039b\u0303tu)|\u03c4u = 0] = \u03ba(1\u2212 \u03b1) 1\u2212 \u03ba E[g(\u039b\u0303 t u)e \u2212\u039b\u0303tu |\u03c4u = 1].\nProof: Following the logic in [11], we show this result for g(\u039b\u0303tu) = 1{\u039b\u0303u\u2208A}, A being some measurable set . The result for general g then follows because any integrable function can be obtained as the limit of a sequence of such rvs [3]. Let Y = (T tu, C t u), the observed rv. Therefore\nE[1{\u039b\u0303tu\u2208A}|\u03c4u = 0] = P[\u039b\u0303 t u \u2208 A|\u03c4u = 0]\n= P(\u039b\u0303tu \u2208 A, \u03c4u = 0)\nP{\u03c4u = 0}\n= EY [P{\u039b\u0303tu \u2208 A, \u03c4u = 0|Y }]\nP{\u03c4u = 0}\n= EY\n[ 1{\u039b\u0303tu \u2208 A}P(\u03c4u = 0|Y )\nP(\u03c4u = 0)\n]\n(a) = EY\n[ 1{\u039b\u0303tu \u2208 A}e\u2212\u039b\u0303 t uP(\u03c4u = 1|Y )\nP(\u03c4u = 0)\n]\n= P(\u03c4u = 1)\nP(\u03c4u = 0) E1[1(\u039b\u0303\nt u \u2208 A)e\u2212\u039b\u0303 t u ]\n= \u03ba(1\u2212 \u03b1) 1\u2212 \u03ba E1[1(\u039b\u0303 t u \u2208 A)e\u2212\u039b\u0303 t u ],\nwhere in (a) we used the fact that P{\u03c4u=0|Y } P{\u03c4u=1|Y } = exp(\u2212\u039b\u0303tu), and E1 denotes expectation conditioned on the event {\u03c4u = 1}.\nInria\nSubgraph Detection with cues using Belief Propagation 9\nNote that the distributional equations (9) and (10) give the asymptotic distributions of the messages on the graph G as n \u2192 \u221e. These equations do not depend on n because of the choice of p, q and K. For ease of analysis we will presently study the distributions in the limit where a, b \u2192 \u221e. This limit is taken after n \u2192 \u221e. Ideally, one would like to analyse the distributions for finite a and b, but this is left for future work. We have the following result on the Gaussianity of the asymptotic messages in the limit where a, b \u2192 \u221e, after n \u2192 \u221e.\nProposition 1 In the regime where \u03bb and \u03ba are held fixed and a, b \u2192 \u221e, we have\n\u03bet+10 D\u2212\u2192 N (\u2212 log 1\u2212 \u03ba \u03ba(1\u2212 \u03b1) \u2212 1 2 \u00b5(t+1), \u00b5(t+1))\n\u03bet+11 D\u2212\u2192 N (\u2212 log 1\u2212 \u03ba \u03ba(1\u2212 \u03b1) + 1 2 \u00b5(t+1), \u00b5(t+1)),\nwhere \u00b5(t) satisfies the following recursion with initial condition \u00b50 = 0 :\n\u00b5(t+1) = \u03bb\u03b1 1\u2212\u03ba\u03ba + \u03bbE\n( (1\u2212\u03b1)2(1\u2212\u03ba)\n\u03ba(1\u2212\u03b1)+(1\u2212\u03ba) exp(\u2212\u00b5(t)/2\u2212 \u221a \u00b5(t)Z)\n) , (13)\nwhere the expectation is w.r.t Z \u223c N (0, 1). Remark : When \u03b1 = 0 (13) reduces to the recursion given in [11] as expected. Proof: Since \u03bb is fixed and b \u2192 \u221e, we have\n\u03c1 \u2261 a/b = 1 + \u221a\n\u03bb(1 \u2212 \u03ba) \u03ba2b = 1 +O(b\u22121/2), (14)\nby (12) since \u03bb and \u03ba are fixed. In the proof we use Berry-Essen inequality for Poisson sums [7, Lemma 11]\nLemma 3 Let S\u03bb = X1 +X2 + . . .XN\u03bb , where Xi : i \u2265 1 are independent, identically distributed random variables with mean \u00b5, variance \u03c32 and E[|X3i |] \u2264 g3, and for some \u03bb > 0, N\u03bb is a Poi(\u03bb) random variable independent of (Xi : i \u2265 1). Then\nsupx \u2223\u2223\u2223\u2223\u2223P { S\u03bb \u2212 \u03bb\u00b5\u221a \u03bb(\u00b52 + \u03c32) } \u2212 \u03a6(x) \u2223\u2223\u2223\u2223\u2223 \u2264 CBEg 3 \u221a \u03bb(\u00b52 + \u03c32)3 ,\nwhere CBE = 0.3041.\nFollowing [11], we prove the result by induction on t. First let us verify the result holds when t = 0, for the initial condition that \u03be00 = \u03be 0 1 = \u2212\u03c5. We only do this for \u03bet0 since for \u03bet1 the steps are similar. Observe that\nf(\u2212\u03c5)\n= log\n \npK(1\u2212\u03b1) q(n\u2212K) + 1 K(1\u2212\u03b1) (n\u2212K) + 1\n \n= log ( 1 + (\u03c1\u2212 1)\u03ba(1\u2212 \u03b1)\n1\u2212 \u03ba\u03b1\n)\n(a) = (\u03c1\u2212 1)\u03ba(1\u2212 \u03b1) 1\u2212 \u03ba\u03b1 \u2212 (\u03c1\u2212 1)2 2 \u03ba2(1\u2212 \u03b1)2 (1\u2212 \u03ba\u03b1)2 +\nO(b\u22123/2), (15)\nRR n\u00b0 XXX\n10 Arun et al\nwhere (a) follows from (14), and Taylor\u2019s expansion around \u03c1 = 1. Similarly,\nf2(\u2212\u03c5) = (\u03c1\u2212 1)2\u03ba 2(1 \u2212 \u03b1)2 (1\u2212 \u03ba\u03b1)2 +O(b \u22123/2), (16)\nlog(\u03c1) = log(1 + (\u03c1\u2212 1)) = \u221a\n\u03bb(1\u2212 \u03ba) \u03ba2b \u2212 \u03bb(1\u2212 \u03ba) 2\u03ba2b +\nO(b\u22123/2), (17)\nand\nlog2(\u03c1) = \u03bb(1\u2212 \u03ba)\n\u03ba2b +O(b\u22123/2) (18)\nLet us verify the induction result for t = 0. Using the recursion (9) with \u03be00 = log \u03ba(1\u2212\u03b1) 1\u2212\u03ba = \u2212\u03c5, we can express E\u03be10 as\nE\u03be10 = \u2212\u03bab(\u03c1\u2212 1)\u2212 \u03c5 + \u03bab\u03b1 log(\u03c1) + b(1\u2212 \u03ba\u03b1)f(\u2212\u03c5).\nNow using (15) and (17) we obtain\nE\u03be10 = \u2212\u03ba \u221a\n\u03bbb(1 \u2212 \u03ba) \u03ba2\n\u2212 \u03c5 + \u03ba\u03b1 \u221a\n\u03bb(1\u2212 \u03ba)b \u03ba2 \u2212 \u03bb(1\u2212 \u03ba)\u03b1 2\u03ba\n+\n\u221a \u03bb(1\u2212 \u03ba)b\n\u03ba2 \u03ba(1\u2212 \u03b1)\u2212 (1 \u2212 \u03b1) 2 2(1\u2212 \u03ba\u03b1)\u03bb(1\u2212 \u03ba) (19)\n+O(b\u22121/2)\n= \u2212\u03c5 \u2212 \u03bb(1\u2212 \u03ba) 2\u03ba \u03b1\u2212 (1 \u2212 \u03b1) 2 2(1\u2212 \u03ba\u03b1)\u03bb(1\u2212 \u03ba) +O(b \u22121/2), (20)\nand\nVar\u03be10 = log 2(\u03c1)\u03bab\u03b1+ f2(\u2212\u03c5)(1\u2212 \u03ba)b+ f2(\u2212\u03c5)\u03bab(1\u2212 \u03b1)\n(a) = \u03bb\u03b1(1 \u2212 \u03ba) \u03ba + (1\u2212 \u03b1)2(1\u2212 \u03ba)\u03bb 1\u2212 \u03ba\u03b1 , (21)\nwhere in (a) we used (18) and (16). Comparing (20) and (21) with \u00b5(1) in (13) using \u00b5(0) = 0, we can verify the mean and variance recursions. Next we use Lemma3 to prove gaussianity. Note that we can express \u03be10 \u2212 h as the Poisson sum of iid mixture random variables as follows\n\u03be10 \u2212 h = L0\u2211\ni=1\nXi,\nwhere L0 \u223c Poi(nq) = Poi(b), and L(Xi) = \u03ba\u03b1L(p/q)+(1\u2212\u03ba)bL(f(\u2212\u03c5))+(\u03bab(1\u2212\u03b1))L(f(\u2212\u03c5)), keeping in mind the independent splitting property of Poissons, where L denotes the law of a random variable. Then by comparing with the form in Lemma 3, \u03bb = b, and the term \u03bb(\u00b52 + \u03c32) = Var\u03be10 = b(\u00b5 2 + \u03c32), which is finite. Next we calculate E|Xi|3. It is easy to see that\nE|Xi|3 = \u03ba\u03b1 log3(b) + ((1\u2212 \u03ba) + \u03ba(1\u2212 \u03b1))f3(\u2212\u03c5) (22) = O(b\u22123/2). (23)\nInria\nSubgraph Detection with cues using Belief Propagation 11\nHence bE|Xi|3= O(b\u22121/2). Therefore the RHS of Lemma (3) becomes\nCBEE|Xi|3\u221a \u03bb(\u00b52 + \u03c32)3 = CBEE|Xi|3\u221a b3/b2(\u00b52 + \u03c32)3\n= CBEbE|Xi|3\u221a (b(\u00b52 + \u03c32))3 = O(b\u22121/2).\nHaving shown the induction hypothesis for t = 0, we now assume it holds for some t. Notice that f(x) = (\u03c1\u2212 1) ex1+ex \u2212 12 (\u03c1\u2212 1)2( e x 1+ex ) 2 +O(b\u22123/2), by Taylor\u2019s expansion, and using (14). Then by using dominated convergence theorem [3] and Lemma 2 we obtain\nEf(\u03bet0) = (\u03c1\u2212 1) \u03ba(1\u2212 \u03b1) 1\u2212 \u03ba E 1 1 + e\u03be t 1 \u2212\n(\u03c1\u2212 1)2\u03ba(1 \u2212 \u03b1) 2(1\u2212 \u03ba) E\ne\u03be t 1\n(1 + e\u03be t 1)2\n+O(b\u22123/2)\n(24)\nand\nEf(\u03bet1) = (\u03c1\u2212 1)E e\u03be\nt 1\n1 + e\u03be t 1\n\u2212 (\u03c1\u2212 1) 2\n2 E\ne2\u03be t 1\n(1 + e\u03be t 1)2\n+O(b\u22123/2). (25)\nNow we take the expectation of both sides of (9) and (10). Here we use the fact that E \u2211L\ni=1 Xi = EXiEL if L \u223c Poi and Xi are independent and identically distributed (iid) random variables, hence obtaining\nE\u03bet+10 = h+ log( p\nq )\u03bab\u03b1+ Ef(\u03bet0)(1 \u2212 \u03ba)b+ Ef(\u03bet1)\u03bab(1\u2212 \u03b1) (26)\nand\nE\u03bet+11 = h+ log( p\nq )\u03baa\u03b1+ Ef(\u03bet0)(1\u2212 \u03ba)b+ Ef(\u03bet1)\u03baa(1\u2212 \u03b1). (27)\nWe now substitute (24) and (25) in (26) to get:\nE\u03bet+10 = h+ \u03bab\u03b1 log(\u03c1) + (1 \u2212 \u03ba)b [ (\u03c1\u2212 1)\u03ba(1\u2212 \u03b1)\n1\u2212 \u03ba E 1 1 + e\u03be t 1\n\u2212 (\u03c1\u2212 1) 2\u03ba(1\u2212 \u03b1) 2(1\u2212 \u03ba) E e\u03be t 1 (1 + e\u03be t 1)2 +O(b\u22123/2)\n] +\n\u03bab(1\u2212 \u03b1) [ (\u03c1\u2212 1)E e \u03bet1\n1 + e\u03be t 1\n\u2212\n(\u03c1\u2212 1)2 2 E e2\u03be t 1 (1 + e\u03be t 1)2 +O(b\u22123/2)\n] ,\nwhich on simplifying and grouping like terms becomes\nE\u03bet+10 = h+ \u03bab\u03b1 log(\u03c1) + \u03ba(a\u2212 b)(1 \u2212 \u03b1)\u2212 \u03bb(1 \u2212 \u03ba)(1\u2212 \u03b1)\n2\u03ba E\ne\u03be t 1\n1 + e\u03be t 1\n.\nRR n\u00b0 XXX\n12 Arun et al\nSince h = \u2212\u03ba(a\u2212 b)\u2212 log (\n1\u2212\u03ba \u03ba(1\u2212\u03b1)\n)\nE\u03bet+10 = \u2212 log ( 1\u2212 \u03ba \u03ba(1\u2212 \u03b1) ) \u2212 \u03b1\u03ba(a\u2212 b) + \u03bab\u03b1 log(\u03c1)\u2212\n\u03bb(1 \u2212 \u03ba)(1\u2212 \u03b1) 2\u03ba E e\u03be t 1 1 + e\u03be t 1 .\nUsing (17) we get\n\u2212\u03b1\u03ba(a\u2212 b) + \u03bab\u03b1 log(\u03c1) = b\u03b1\u03ba(log(\u03c1)\u2212 (\u03c1\u2212 1))\n= b\u03b1\u03ba(\u2212\u03bb(1\u2212 \u03ba) 2\u03ba2b +O(b\u22123/2)) = \u2212\u03bb\u03b1(1 \u2212 \u03ba) 2\u03ba +O(b\u22121/2).\nFinally we obtain\nE\u03bet+10 = \u2212 log( 1\u2212 \u03ba \u03ba(1 \u2212 \u03b1) )\u2212 \u03b1\u03bb(1 \u2212 \u03ba) 2\u03ba \u2212\n\u03bb (1 \u2212 \u03ba)(1\u2212 \u03b1)\n2\u03ba E(\ne\u03be t 1\n1 + e\u03be t 1\n) +O(b\u22121/2).\n(28)\nUsing exactly the same simplifications we can get\nE\u03bet+11 = \u2212 log( 1\u2212 \u03ba \u03ba(1 \u2212 \u03b1) ) + \u03b1\u03bb(1 \u2212 \u03ba) 2\u03ba +\n\u03bb (1 \u2212 \u03ba)(1\u2212 \u03b1)\n2\u03ba E(\ne\u03be t 1\n1 + e\u03be t 1\n) +O(b\u22121/2).\n(29)\nObserve that f2(x) = (\u03c1\u2212 1)2 ( ex\n1+ex\n)2 +O(b\u22123/2). Therefore\nEf2(\u03bet0) = (\u03c1\u2212 1)2E e2\u03be\nt 0\n(1 + e\u03be t 0)2\n+O(b\u22123/2),\nand using Lemma 2 the above becomes\nEf2(\u03bet0) = (\u03c1\u2212 1)2 \u03ba(1\u2212 \u03b1) 1\u2212 \u03ba E\ne\u03be t 1\n(1 + e\u03be t 1)2\n+O(b\u22123/2). (30)\nSimilarly,\nEf2(\u03bet1) = (\u03c1\u2212 1)2E e2\u03be\nt 1\n(1 + e\u03be t 1)2\n+O(b\u22123/2). (31)\nNow we use the formula for the variance of Poisson sums Var \u2211L\ni=1 Xi = EX 2 i EL, to get\nVar[\u03bet+10 ] = log 2(\u03c1)\u03bab\u03b1+ (1\u2212 \u03ba)bEf2(\u03bet0)+ \u03bab(1\u2212 \u03b1)Ef2(\u03bet1) Var[\u03bet+11 ] = log\n2(\u03c1)\u03baa\u03b1+ (1 \u2212 \u03ba)bEf2(\u03bet0)+ \u03baa(1\u2212 \u03b1)Ef2(\u03bet1).\nInria\nSubgraph Detection with cues using Belief Propagation 13\nSubstituting (30) and (31) into the above equations we get\nVar\u03bet+11 = Var\u03be t+1 0 = \u03bb\u03b1(1 \u2212 \u03ba) \u03ba + \u03bb(1 \u2212 \u03ba)(1\u2212 \u03b1) \u03ba\nE exp \u03bet1\n1 + exp(\u03bet1) .\n(32)\nLet us use \u00b5(t+1) to denote Var\u03be (t+1) 1 = Var\u03be (t+1) 0 . Then\nE\u03bet+10 = \u2212 log ( (1\u2212 \u03ba) \u03ba(1\u2212 \u03b1) ) \u2212 1 2 \u00b5(t+1) +O(b\u22121/2) E\u03bet+11 = \u2212 log ( (1\u2212 \u03ba) \u03ba(1\u2212 \u03b1) ) + 1 2 \u00b5(t+1) +O(b\u22121/2). (33)\nNow we use the fact the induction assumption that \u03bet1 \u2192 N (E\u03bet1, \u00b5(t)). Since the function 1/(1+ e\u2212\u03be t 1) is bounded, by Bounded Convergence Theorem this means E[1/(1 + e\u2212\u03be t 1)] \u2192 E[1/(1 + e\u2212N (E\u03be t 1,\u00b5 (t)))]. We can write N (E\u03bet1, \u00b5(t)) = \u221a \u00b5(t)Z+E\u03bet1, where Z \u223c N (0, 1). Therefore we can write and using (33) we obtain\nE 1\n1 + e\u2212\u03be t 1\n= E 1\n1 + e\u2212 \u221a\n\u00b5(t)Z (1\u2212\u03ba) \u03ba(1\u2212\u03b1)e \u2212\u00b5(t)2\n= E \u03ba(1\u2212 \u03b1)\n\u03ba(1 \u2212 \u03b1) + (1\u2212 \u03ba)e(\u2212 \u221a \u00b5tZ\u2212\u00b5(t)2 ) .\nSubstituting the above into (32) gives us the recursion for \u00b5(t+1) given in (13). Next we prove Gaussianity. Consider\n\u03bet+10 \u2212 E\u03bet+10\n= log\n( p\nq\n) (L0c \u2212 EL0c) + L00\u2211\ni=1\n(f(\u03bet0,i)\u2212 Ef(\u03bet0)) +\nL01\u2211\ni=1\n(f(\u03bet1,i)\u2212 Ef(\u03bet1)) + (L00 \u2212 EL00)Ef(\u03bet0) +\n(L01 \u2212 EL01)Ef(\u03bet1). (34)\nLet us look at the second term. Let Xi = f(\u03be t 0,i) \u2212 Ef(\u03bet0,i). Then it can be shown that\nEX2i = O(1/b). Let D \u2261 \u2211L00 i=1 Xi \u2212 \u2211 EL00 i=1 Xi. Here the summation is taken up to i \u2264 EL00.\nThen ED2 = |\u2211\u03b4i=1 Xi|2, where \u03b4 \u2264 |L00 \u2212 EL00|+1, where the extra 1 is because EL00 may not be an integer. Therefore ED2 = E\u03b4E|X1|2\u2264 (C/b)((1 \u2212 \u03ba)b + 1)1/2 = O(1/ \u221a b). Thus, we can replace the Poisson upper limits of the summations in the second and third terms of (34) by their means, leading to\n\u03bet+10 \u2212 E\u03bet+10 = log ( p\nq\n) (L0c \u2212 EL0c) + EL00\u2211\ni=1\n(f(\u03bet0,i)\u2212 Ef(\u03bet0))\n+\nEL01\u2211\ni=1\n(f(\u03bet1,i)\u2212 Ef(\u03bet1)) + (L00 \u2212 EL00)Ef(\u03bet0)+\n(L01 \u2212 EL01)Ef(\u03bet1) + op(1),\n(35)\nRR n\u00b0 XXX\n14 Arun et al\nwhere op(1) indicates a random variable that goes to zero in probability in the limit. The variance of the above term is \u00b5t+1, defined in (13), and it is finite for a fixed t. Now since we have an infinite sum of independent random variables as a, b \u2192 \u221e, with zero mean and finite variance, from standard CLT we can conclude that the distribution tends N (0, \u00b5t+1).\n5 Detection Method\nIt is shown [7] that asymptotically the tests\nS\u03020 = {i : Rti > log 1\u2212 \u03ba\n\u03ba(1\u2212 \u03b1)},\nand S\u0302, the output of Algorithm 1, have the same fraction of miss-classified nodes. So we now go on to show that S\u03020 weakly recovers S, i.e., the expected fraction of missclassified nodes approaches 0, and the result for S\u0302 follows. By Lemma 1 we work with \u039bi instead of Ri. Consider the estimator on the tree:\n\u03c4\u0302i = { 1 if \u039bti \u2265 log 1\u2212\u03ba\u03ba(1\u2212\u03b1) , 0 otherwise.\nAlternatively, \u03c4\u0302i = 1{\u039b\u0303ti\u22650} . The above estimator minimises the following error probability:\npe = P{\u03c4i = 1}P(\u03c4\u0302i = 0|i \u2208 S) + P(\u03c4\u0302i = 1|i 6\u2208 S)P{\u03c4i = 0}.\nIn the following proposition, we state and prove the main result of our paper. We show that the expected fraction of miss-classified nodes goes to zero for an infinitesimally small subgraph size, for any \u03bb > 0. This implies that BP with cue beats BP without cues, which requires \u03bb > 1/e for zero asymptotic error rate ( [7, 11]).\nProposition 2 In the regime where a, b \u2192 \u221e we have\nlim \u03ba\u21920\nES\u2206S\u0302\nK(1\u2212 \u03b1) \u2192 0,\nfor any \u03bb > 0, i.e., the expected fraction of miss-classified nodes tends to zero, as long as \u03b1 is strictly positive.\nProof: We upperbound the error rate of S\u03020 and the result for S\u0302 follows based on the explanation in Section 5. By Lemma 1, the \u039btu and R t u have the same distributions on an event whose probability goes to 1. Therefore it is sufficient to bound the error for the tree, as follows:\nES\u2206S\u03020 K(1\u2212 \u03b1) = ( n\u2212K\u03b1 K \u2212K\u03b1 ) pe\n(a) = ( (1\u2212 \u03ba) \u03ba(1\u2212 \u03b1) ) P t0(\u03be > 0) + P t 1(\u03be < 0) (36)\nwhere in (a) P t0 and P t 1 denote probabilities w.r.t. the distributions of \u03be t 0, \u03be t 1 respectively. We now analyse the asymptotic value of each term in (36) in the limit as \u03ba \u2192 0 with \u03b1 fixed. By Proposition 1 we have that in the limit where a \u2192 \u221e and b \u2192 \u221e,\nP t1(\u03be < 0) = Q ( 1\u221a \u00b5(t) ( \u00b5(t) 2 \u2212 log( (1\u2212 \u03ba) \u03ba(1\u2212 \u03b1) ) ))\nInria\nSubgraph Detection with cues using Belief Propagation 15\nwhere Q(\u00b7) denotes the standardQ function. Notice that by (13) we have that \u00b5(t) \u2265 \u03bb\u03b1(1\u2212\u03ba)/\u03ba, since F (\u00b5) \u2261 E 1\u2212\u03ba\u03ba(1\u2212\u03b1)+(1\u2212\u03ba) exp(\u2212\u00b5/2\u2212\u221a\u00b5Z) \u2265 0. In addition, by (32), \u00b5(t) \u2264 \u03bb(1\u2212\u03ba) \u03ba . Therefore \u00b5(t) = \u0398( 1\u03ba ). Note that the lower bound on \u00b5 (t) is not useful when \u03b1 = 0. Consequently lim\u03ba\u21920 1\u00b5(t) log( (1\u2212\u03ba) \u03ba(1\u2212\u03b1) ) = 0. Therefore:\nP t1(\u03be < 0) = 1\n\u03ba Q(\n\u221a \u00b5(t)(1 +O(\u03ba)))\n\u2264 exp(\u2212\u0398(1/\u03ba)) \u2192 0.\nSimilarly we have\n1 \u03ba P t0(\u03be > 0) (37)\n= 1\n\u03ba Q(\nlog( (1\u2212\u03ba)\u03ba(1\u2212\u03b1) ) + \u00b5 2\u221a\n\u00b5(t) ) (38)\n\u2264 1 \u03ba exp(\u2212\u0398(1 \u03ba )) (39) \u2192 0. (40)\nSubstituting these back in (36) the result then follows.\n6 Numerical Experiments\nIn this section we provide simulation results to corroborate our theoretical findings and also to demonstrate the performance improvement of Algorithm 1 in the presence of side-information. We fix n = 104, b = 100, and \u03ba = 0.005, giving K = 50. Next we sweep over different values of \u03bb in the range [0.1, 0.8] and average over 1000 graph realisations to find the fraction of missclassified subgraph nodes for each value of \u03bb. In Figure 1 we have the ratio between the number of subgraph nodes wrongly classified by the algorithm and the number of unlabelled subgraph nodes on the y-axis and \u03bb on the x-axis. This demonstrates that there is a marked improvement in the performance of BP with the introduction of cues.\nIn Figure 2 we plot the theoretical error of Algorithm 1 given in (36) against \u03ba for the two cases of \u03b1 = 0 (no cues) and \u03b1 = 0.1 (10% cues) for \u03bb = 12e . We have chosen this value of \u03bb in order to be below the detectability threshold of \u03bb = 1e of BP without cues. We can observe that contrary to when \u03b1 = 0, with \u03b1 = 0.1 the error decreases as \u03ba decreases, as proved in our analysis. We also observed this in our simulations where we obtained an error rate of 73.86% for \u03ba = 4\u00d7 10\u22124 (n = 5\u00d7 104) with \u03b1 = 0.1, whereas it was 0.995 when \u03b1 = 0.\n7 Conclusions and Future Extensions\nIn this work we developed a local distributed BP algorithm that takes advantage of sideinformation to detect a dense subgraph embedded in a sparse graph. We obtained theoretical results based on density evolution on trees to show that it achieves zero asymptotic error regardless of the SNR parameter \u03bb, unlike BP without cues, where there is a non-zero detectability threshold. We also obtained some simulation results on synthetic graphs to demonstrate the improvement in error rates in the presence of cues. In the future, we would like to investigate non-asymptotic properties of the algorithm for finite a and b and when K = o(n).\nRR n\u00b0 XXX\n16 Arun et al\nInria\nSubgraph Detection with cues using Belief Propagation 17\nReferences\n[1] L. Akoglu, H. Tong, and D. Koutra, \u201cGraph based anomaly detection and description: a survey,\u201d Data Mining and Knowledge Discovery, vol. 29, no. 3, pp. 626\u2013688, 2015.\n[2] N. Alon, M. Krivelevich, and B. Sudakov, \u201cFinding a large hidden clique in a random graph,\u201d Random Structures and Algorithms, vol. 13, no. 3-4, pp. 457\u2013466, 1998.\n[3] P. Billingsley, Probability and measure. John Wiley & Sons, 2008.\n[4] T. T. Cai, T. Liang, and A. Rakhlin, \u201cInference via message passing on partially labeled stochastic block models,\u201d arXiv preprint arXiv:1603.06923, 2016. [5] Y. Deshpande and A. Montanari, \u201cFinding hidden cliques of size \u221a N/e in nearly linear\ntime,\u201d Foundations of Computational Mathematics, vol. 15, no. 4, pp. 1069\u20131128, 2015.\n[6] B. Hajek, Y. Wu, and J. Xu, \u201cInformation limits for recovering a hidden community,\u201d arXiv preprint arXiv:1509.07859, 2015.\n[7] \u2014\u2014, \u201cRecovering a Hidden Community Beyond the Spectral Limit in O(|E|log\u2217|V |) Time,\u201d arXiv Prepr. arXiv1510.02786, 2015.\n[8] A. Kadavankandy, L. Cottatellucci, and K. Avrachenkov, \u201cCharacterization of L1-norm statistic for Anomaly Detection in Erd\u00f6s R\u00e9nyi Graphs,\u201d in CDC. IEEE, 2016. [Online]. Available: http://www.eurecom.fr/en/publication/list/author/cottatellucci-laura\n[9] A. Martinsson, \u201cLovasz \u03b8 function , SVMs and Finding Dense Subgraphs,\u201d J. Mach. Learn. Res., vol. 14, pp. 3495\u20133536, 2013.\n[10] T. Mifflin, C. Boner, G. Godfrey, and J. Skokan, \u201cA random graph model for terrorist transactions,\u201d in 2004 IEEE Aerosp. Conf. Proc. (IEEE Cat. No.04TH8720), vol. 5. IEEE, 2004, pp. 3258\u20133264.\n[11] A. Montanari, \u201cFinding one community in a sparse graph,\u201d Journal of Statistical Physics, vol. 161, no. 2, pp. 273\u2013299, 2015.\n[12] E. Mossel and J. Xu, \u201cLocal Algorithms for Block Models with Side Information,\u201d in Proc. 2016 ACM Conf. Innov. Theor. Comput. Sci. - ITCS \u201916. New York, New York, USA: ACM Press, jan 2016, pp. 71\u201380.\nContents\n1 Introduction 3\n2 Model and Problem Definition 3\n3 Belief Propagation Algorithm for Detection in the Presence of Cues 4\n4 Asymptotic Error Analysis 7\n5 Detection Method 14\n6 Numerical Experiments 15\n7 Conclusions and Future Extensions 15\nRR n\u00b0 XXX\nRESEARCH CENTRE SOPHIA ANTIPOLIS \u2013 M\u00c9DITERRAN\u00c9E\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399\nThis figure \"logo-inria.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"logo-inria.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"pagei.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"pagei.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"rrpage1.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/1611.04847v1\nThis figure \"rrpage1.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1611.04847v1"}], "references": [{"title": "Graph based anomaly detection and description: a survey", "author": ["L. Akoglu", "H. Tong", "D. Koutra"], "venue": "Data Mining and Knowledge Discovery, vol. 29, no. 3, pp. 626\u2013688, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding a large hidden clique in a random graph", "author": ["N. Alon", "M. Krivelevich", "B. Sudakov"], "venue": "Random Structures and Algorithms, vol. 13, no. 3-4, pp. 457\u2013466, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Probability and measure", "author": ["P. Billingsley"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Inference via message passing on partially labeled stochastic block models", "author": ["T.T. Cai", "T. Liang", "A. Rakhlin"], "venue": "arXiv preprint arXiv:1603.06923, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding hidden cliques of size  \u221a N/e in nearly linear time", "author": ["Y. Deshpande", "A. Montanari"], "venue": "Foundations of Computational Mathematics, vol. 15, no. 4, pp. 1069\u20131128, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Information limits for recovering a hidden community", "author": ["B. Hajek", "Y. Wu", "J. Xu"], "venue": "arXiv preprint arXiv:1509.07859, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Recovering a Hidden Community Beyond the Spectral Limit in O(|E|log\u2217|V |) Time", "author": ["\u2014\u2014"], "venue": "arXiv Prepr. arXiv1510.02786, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Characterization of L-norm statistic for Anomaly Detection in Erd\u00f6s R\u00e9nyi Graphs", "author": ["A. Kadavankandy", "L. Cottatellucci", "K. Avrachenkov"], "venue": "CDC. IEEE, 2016. [Online]. Available: http://www.eurecom.fr/en/publication/list/author/cottatellucci-laura", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Lovasz \u03b8 function , SVMs and Finding Dense Subgraphs", "author": ["A. Martinsson"], "venue": "J. Mach. Learn. Res., vol. 14, pp. 3495\u20133536, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A random graph model for terrorist transactions", "author": ["T. Mifflin", "C. Boner", "G. Godfrey", "J. Skokan"], "venue": "2004 IEEE Aerosp. Conf. Proc. (IEEE Cat. No.04TH8720), vol. 5. IEEE, 2004, pp. 3258\u20133264.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Finding one community in a sparse graph", "author": ["A. Montanari"], "venue": "Journal of Statistical Physics, vol. 161, no. 2, pp. 273\u2013299, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Please see [1] for a survey.", "startOffset": 11, "endOffset": 14}, {"referenceID": 9, "context": "The hidden subgraph model with both the subgraph and the background modelled as ER graphs with different edge densities was proposed in [10] to study anomalous transactions in a computer network.", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "This graph, denoted G(K,n, p, q), can model a network with a hidden community [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "See [8,9] for works on detecting the presence of such a subgraph in a given graph and detecting the subgraph nodes.", "startOffset": 4, "endOffset": 9}, {"referenceID": 8, "context": "See [8,9] for works on detecting the presence of such a subgraph in a given graph and detecting the subgraph nodes.", "startOffset": 4, "endOffset": 9}, {"referenceID": 1, "context": "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).", "startOffset": 216, "endOffset": 226}, {"referenceID": 6, "context": "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).", "startOffset": 216, "endOffset": 226}, {"referenceID": 8, "context": "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).", "startOffset": 216, "endOffset": 226}, {"referenceID": 10, "context": "A stream of recent works in this area suggests that there is a subgraph size below which detection is impossible by means of nearly linear-time algorithms, and above which efficient algorithms have been identified ( [2,7,9,11]).", "startOffset": 216, "endOffset": 226}, {"referenceID": 1, "context": "When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "When q = 1/2 and p = 1, the subgraph detection problem reduces to the well-known clique detection problem ( [2], [9], [5]).", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "We use the framework of analysis developed in [7] and [11].", "startOffset": 46, "endOffset": 49}, {"referenceID": 10, "context": "We use the framework of analysis developed in [7] and [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "In [11] the author considers the problem of detecting the hidden community in G(K,n, p, q) under the assumption that p = a/n, q = b/n and K = \u03ba/n, where a, b, \u03ba are constants.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "In [7], the authors consider a more general setting of sparse graphs and prove that BP succeeds when \u03bb > 1/e.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [6] global ML detection is shown to achieve zero error rate asymptotically if \u03bb = \u0398((K/n) log(K/n)).", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "The optimality of BP with side information is shown in [12] and [4] for community detection on SBM with symmetric communities, but to the best of our knowledge no theoretical studies of BP have been made when cued vertices are available for detecting a small subgraph in G(K,n, p, q).", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "The symbol D \u2212\u2192 denotes convergence in distribution [3].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Recall that the optimum detector that minimises the expected number of misclassified nodes is the ML detector [6] given as: \u03c3\u0302i = 1{Ri>log(n\u2212K/K(1\u2212\u03b1))}, where Ri = log P(G,C|\u03c3i = 1) P(G,C|\u03c3i = 0) .", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Let T t u be a labelled Galton-Watson (G-W) tree of depth t rooted at node u constructed as follows (as in [7]): The label \u03c4u at node u is chosen at random in the following way: P{\u03c4u = 1} = K n P{\u03c4u = 0} = n\u2212K n .", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "By the following coupling lemma established in [7], the detection of label \u03c3u based on G t u is statistically identical to the detection of \u03c4u based on T t u :", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "Lemma 1 [7] For t such that (np) = n, there exists a coupling such that (Gu, \u03c3 ) = (T t u, \u03c4 ) with probability 1\u2212 n\u22121+o(1).", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "We define the parameter \u03bb, interpreted as an effective SNR [11] of the detection problem, as", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "Proof: Following the logic in [11], we show this result for g(\u039b\u0303u) = 1{\u039b\u0303u\u2208A}, A being some measurable set .", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "The result for general g then follows because any integrable function can be obtained as the limit of a sequence of such rvs [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Remark : When \u03b1 = 0 (13) reduces to the recursion given in [11] as expected.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "Following [11], we prove the result by induction on t.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "Then by using dominated convergence theorem [3] and Lemma 2 we obtain", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "5 Detection Method It is shown [7] that asymptotically the tests", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "This implies that BP with cue beats BP without cues, which requires \u03bb > 1/e for zero asymptotic error rate ( [7, 11]).", "startOffset": 109, "endOffset": 116}, {"referenceID": 10, "context": "This implies that BP with cue beats BP without cues, which requires \u03bb > 1/e for zero asymptotic error rate ( [7, 11]).", "startOffset": 109, "endOffset": 116}], "year": 2017, "abstractText": "We consider an Erd\u0151s-R\u00e9nyi graph with n nodes and edge probability q that is embedded with a random subgraph of size K with edge probabilities p such that p > q. We address the problem of detecting the subgraph nodes when only the graph edges are observed, along with some extra knowledge of a small fraction of subgraph nodes, called cued vertices or cues. We employ a local and distributed algorithm called belief propagation (BP). Recent works on subgraph detection without cues have shown that global maximum likelihood (ML) detection strictly outperforms BP in terms of asymptotic error rate, namely, there is a threshold condition that the subgraph parameters should satisfy below which BP fails in achieving asymptotically zero error, but ML succeeds. In contrast, we show that when the fraction of cues is strictly bounded away from zero, i.e., when there exists non-trivial side-information, BP achieves zero asymptotic error even below this threshold, thus approaching the performance of ML detection. Key-words: Belief Propagation, Subgraph Detection, Semisupervised Learning, Random Graphs \u2217 Corresponding author, arun.kadavankandy@inria.fr La Detection de Sousgraphes en presence des indices gr\u00e2ce au Belief Propagation R\u00e9sum\u00e9 : Nous consid\u00e9rons un graphe Erd\u0151s-R\u00e9nyi qui a n sommets dont q est la probabilit\u00e9 d\u2019arr\u00eates. La dessus il y un sousgraphe plac\u00e9 sur leurs m sommets selectionn\u00e9s al\u00e9atoirement et leur probabilit\u00e9 d\u2019arr\u00eates est p, en sorte que p > q. Nous proposons un algorithme distribu\u00e9 aux calculs locales \u00e0 chaque sommet, tir\u00e9 du \u201cBelief Propagation\u201d (BP), qui d\u00e9tecte les sommets du sousgraphe, quand on connait une fraction de sommets du sousgraphe en tant qu\u2019indices. Des recherches r\u00e9centes ont prouv\u00e9 que la prestation du BP dans l\u2019absence des indices est strictement inf\u00e9rior par rapport \u00e0 la detection globale du maximum de vraisemblance (DMV). A l\u2019oppos\u00e9, ici on prouve qu\u2019en presence des indices, la prestation du BP est \u00e0 l\u2019hauteur de celle de DMV, dans la sens o\u00f9 le premier reussie \u00e0 detecter la sousgraphe avec une erreur qui tend a z\u00e9ro, \u00e0 chaque fois le dernier peut le faire, dans la limite o\u00f9 le nombre de sommets du graph tend l\u2019infinit\u00e9. Mots-cl\u00e9s : Belief Propagation, Detection de Sousgraphes, Semisupervised Learning, Graphes Al\u00e9atoires Subgraph Detection with cues using Belief Propagation 3", "creator": "LaTeX with hyperref package"}}}