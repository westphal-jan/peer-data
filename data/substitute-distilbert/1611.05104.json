{"id": "1611.05104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "lstms have become a basic building block for many deep nlp technologies. these recent years, many improvements and variations have been proposed for deep sequence models in general, and systems in particular. we propose and analyze a measure of adaptive modifications designing lstm networks calling in improved performance for text classification datasets. we encounter substantial improvements on traditional lstms using monte carlo test - time model averaging, deep vector averaging ( dva ), and residual connections, along several four other suggested modifications. our analysis proposes a simple, reliable, and high quality baseline model.", "histories": [["v1", "Wed, 16 Nov 2016 00:53:01 GMT  (591kb,D)", "http://arxiv.org/abs/1611.05104v1", null], ["v2", "Sat, 17 Dec 2016 06:47:05 GMT  (591kb,D)", "http://arxiv.org/abs/1611.05104v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shayne longpre", "sabeek pradhan", "caiming xiong", "richard socher"], "accepted": false, "id": "1611.05104"}, "pdf": {"name": "1611.05104.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shayne Longpre", "Sabeek Pradhan"], "emails": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "When exploring a new problem, having a simple yet competitive off-the-shelf baseline is fundamental to new research. For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks. For computer vision, off-the-shelf convolutional neural networks (CNNs) have earned their reputation as a strong baseline (Sharif Razavian et al., 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016). For natural language processing (NLP) and other sequential modeling tasks, recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, with a linear projection layer at the end have begun to attain a similar status. However, the standard LSTM is in many ways lacking as a baseline. Zaremba (2015), Gal (2015), and others show that large improvements are possible using a forget bias, inverted dropout regularization or bidirectionality. We add three major additions with similar improvements to off-the-shelf LSTMs: Monte Carlo model averaging, deep vector averaging, and residual connections. We analyze these and other more common improvements."}, {"heading": "2 LSTM NETWORK", "text": "LSTM networks are among the most commonly used models for tasks involving variable-length sequences of data, such as text classification. The basic LSTM layer consists of six equations:\nit = tanh (Wixt +Riht\u22121 + bi) (1) jt = \u03c3 (Wjxt +Rjht\u22121 + bj) (2) ft = \u03c3 (Wfxt +Rfht\u22121 + bf ) (3) ot = tanh (Woxt +Roht\u22121 + bo) (4) ct = it jt + ft ct\u22121 (5) ht = ot tanh (ct) (6)\nar X\niv :1\n61 1.\n05 10\n4v 1\n[ cs\n.C L\n] 1\n6 N\nov 2\nWhere \u03c3 is the sigmoid function, is element-wise multiplication, and vt is the value of variable v at timestep t. Each layer receives xt from the layer that came before it and ht\u22121 and ct\u22121 from the previous timestep, and it outputs ht to the layer that comes after it and ht and ct to the next timestep. The c and h values jointly constitute the recurrent state of the LSTM that is passed from one timestep to the next. Since the h value completely updates at each timestep while the c value maintains part of its own value through multiplication by the forget gate f , h and c complement each other very well, with h forming a \u201cfast\u201d state that can quickly adapt to new information and c forming a \u201cslow\u201d state that allows information to be retained over longer periods of time (Zaremba, 2015). While various papers have tried to systematically experiment with the 6 core equations constituting an LSTM (Greff et al., 2015; Zaremba, 2015), in general the basic LSTM equations have proven extremely resilient and, if not optimal, at least a local maximum."}, {"heading": "3 MONTE CARLO MODEL AVERAGING", "text": "It is common practice when applying dropout in neural networks to scale the weights up at train time (inverted dropout). This ensures that the expected magnitude of the inputs to any given layer are equivalent between train and test, allowing for an efficient computation of test-time predictions. However, for a model trained with dropout, test-time predictions generated without dropout merely approximate the ensemble of smaller models that dropout is meant to provide. A higher fidelity method requires that test-time dropout be conducted in a manner consistent with how the model was trained. To achieve this, we sample k neural nets with dropout applied for each test example and average the predictions. With sufficiently large k this Monte Carlo average should approach the true model average (Srivastava et al., 2014). We show in Figure 1 that this technique can yield more accurate predictions on test-time data than the standard practice. This is demonstrated over a number of datasets, suggesting its applicability to many types of sequential architectures. While running multiple Monte Carlo samples is more computationally expensive, the overall increase is minimal as the process is only run on test-time forward passes and is highly parallelizable. We show that higher performance can be achieved with relatively few Monte Carlo samples, and that this number of samples is similar across different NLP datasets and tasks.\nWe encountered one ambiguity of Monte Carlo model averaging that to our knowledge remains unaddressed in prior literature: there is relatively little exploration as to where and how the model averaging is most appropriately handled. We investigated averaging over the output of the final recurrent layer (just before the projection layer), over the output of the projection layer (the presoftmax unnormalized logits), and the post-softmax normalized probabilities, which is the approach taken by Gal (2015) for language modeling. We saw no discernible difference in performance between averaging the pre-projection and post-projection outputs. Averaging over the post-softmax probabilities showed marginal improvements over these two methods, but interestingly only for bidirectional models. We also explored using majority voting among the sampled models. This\ninvolves tallying the maximum post-softmax probabilities and selecting the class that received the most votes. This method differs from averaging the post-softmax probabilities in the same way max-margin differs from maximum likelihood estimation (MLE), de-emphasizing the points well inside the decision boundary or the models that predicted a class with extremely high probability. With sufficiently large k, this voting method seemed to work best of the averaging methods we tried, and thus all of our displayed models use this technique. However, for classification problems with more classes, more Monte Carlo samples might be necessary to guarantee a meaningful plurality of class predictions. We conclude that the majority-vote Monte Carlo averaging method is preferable in the case where the ratio of Monte Carlo samples to number of classification labels is large (k/output size).\nThe Monte Carlo model averaging experiments, shown in Figure 1, were conducted as follows. We drew k = 400 separate test samples for each example, differentiated by their dropout masks. For each sample size p (whose values, plotted on the x-axis, were in the range from 2 to 200 with step-size 2) we selected p of our k samples randomly without replacement and performed the relevant Monte Carlo averaging technique for that task, as discussed above. We do this m = 20 times for each point, to establish the mean and variance for that number of Monte Carlo iterations/samples p. The variance is used to visualize the 90% confidence interval in blue, while the red line denotes the test accuracy computed using the traditional approximation method (inverted dropout at train-time, and no dropout at test-time)."}, {"heading": "4 DEEP VECTOR AVERAGING", "text": "Reliably retaining long-range information is a well documented weakness of LSTM networks (Karpathy et al., 2015). This is especially the case for very long sequences like the IMDB sentiment dataset (Maas et al., 2011),captio where deep sequential models fail to capture uni- and bi-gram occurrences over long sequences. This is likely why n-gram based models, such as a bi-gram NBSVM (Wang and Manning, 2012), outperform RNN models on such datasetes. It was shown by Iyyer et al. (2015) and others that for general NLP classification tasks, the use of a deep, unordered composition (or bag-of-words) of a sequence can yield strong results. Their solution, the deep averaging network (DAN), combines the observed effectiveness of depth, with the unreasonable effectiveness of unordered representations of long sequences.\nWe suspect that the primary advantage of DANs is their ability to keep track of information that would have otherwise been forgotten by a sequential model, such as information early in the sequence for a unidirectional RNN or information in the middle of the sequence for a bidirectional RNN. Our Deep Vector Averaging (DVA) supplements the bidirectional RNN with the information from a DAN at a relatively negligible computational cost.\nAs shown in Figure 2, DVA works by averaging the sequence of word vectors and passing this average through an MLP. The output of this MLP is concatenated to the final output of the RNN, and the combined vector is then passed into the projection and softmax layer. We apply the same dropout mask to the word vectors when passing them to the RNN as when averaging them, and we apply a different dropout mask on the output of the MLP. We experimented with applying the MLP before rather than after averaging the word vectors but found the latter to be most effective."}, {"heading": "5 RESIDUAL CONNECTIONS", "text": "For feed-forward convolutional neural networks used in computer vision tasks, residual networks, or ResNets, have obtained state of the art results (He et al., 2015). Rather than having each layer learn a wholly new representation of the data, as is customary for neural networks, ResNets have each layer (or group of layers) learn a residual which is added to the layer\u2019s input and then passed on to the next layer. More formally, if the input to a layer (or group of layers) is x and the output of that layer (or group of layers) is F (x), then the input to the next layer (or group of layers) is x+ F (x), whereas it would be F (x) in a conventional neural network. This architecture allows the training of far deeper models. He et al. (2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al., 2015), and won the 2015 ImageNet Challenge. Since then, various papers have tried to build upon the ResNet paradigm (Huang et al., 2016; Szegedy et al., 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al., 2016).\nWe explored many different ways to incorporate residual connections in an RNN. The two most successful ones, which we call Res-V1 and Res-V2 are depicted in Figure 6. Res-V1 incorporates only vertical residuals, while Res-V2 incorporates both vertical and lateral residuals. With vertical residual connections, the input to a layer is added to its output and then passed to the next layer, as is done in feed-forward ResNets. Thus, whereas the input to a layer is normally the ht from the previous layer, with vertical residuals the input becomes the ht + xt from the previous layer. This maintains many of the attractive properties of ResNets (e.g. unimpeded gradient flow across layers, adding/averaging the contributions of each layer) and thus lends itself naturally to deeper networks. However, it can interact unpredictably with the LSTM architecture, as the \u201cfast\u201d state of the LSTM no longer reflects the network\u2019s full representation of the data at that point. To mitigate this unpredictability, Res-V2 also includes lateral residual connections. With lateral residual connections, the input to a layer is added to its output and then passed to the next timestep as the fast state of the LSTM. It is equivalent to replacing equation 6 with ht = ot tanh (ct) + xt. Thus, applying both vertical and lateral residuals ensures that the same value is passed both to the next layer as input and to the next timestep as the \u201cfast\u201d state.\nIn addition to these two, we explored various other, ultimately less successful, ways of adding residual connections to an LSTM, the primary one being horizontal residual connections. In this architecture, rather than adding the input from the previous layer to a layer\u2019s output, we added the fast state from the previous timestep. The hope was that adding residual connections across timesteps would allow information to flow more effectively across timesteps and thus improve the performance of RNNs that are deep across timesteps, much as ResNets do for networks that are deep across layers. Thus, we believed horizontal residual connections could solve the problem of LSTMs not learning long-term dependencies, the same problem we also hoped to mitigate with DVA. Unfortunately, horizontal residuals failed, possibly because they blurred the distinction between the LSTM\u2019s \u201cfast\u201d state and \u201cslow\u201d state and thus prevented the LSTM from quickly adapting to new data. Alternate combinations of horizontal, vertical, and lateral residual connections were also experimented with but yielded poor results."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "6.1 DATASETS", "text": "We chose two commonly used benchmark datasets for our experiments: the Stanford Sentiment Treebank (SST) (Socher et al., 2013) and the IMDB sentiment dataset (Maas et al., 2011). This allowed us to compare the performance of our models to existing work and review the flexibility of\nLSTM\nLSTM\nLSTM\nSoftmax\nh(1)t\nh(2)t\nh(3)t\nxt\u2026 \u2026 xt+1xt 1\nh(1)t 1 h (1) t\nh(2)t 1\nh(3)t 1\nh(2)t\nh(3)t\n(a) Res-V1: An illustration of vertical residual connections\nLSTM\nLSTM\nLSTM\nSoftmax\n\u2026 \u2026xtxt 1 xt+1\nh(1)t 1 h (1) t\nh(1)t\nh(2)t 1\nh(2)t\nh(2)t\nh(3)t 1\nh(3)t\nh(3)t\n(b) Res-V2: An illustration of vertical and lateral residual connections\nFigure 3: An illustration of vertical (ResV) and lateral residual (ResL) connections added to a 3-layer RNN. A model with only vertical residuals is denoted Res-V1, whereas a model with vertical and lateral residuals is denoted \u201cRes-V2\u201d.\nour proposed model extensions across fairly disparate types of classification datasets. SST contains relatively well curated, short sequence sentences, in contrast to IMDB\u2019s comparatively colloquial and lengthy sequences (some up to 2, 000 tokens). To further differentiate the classification tasks we chose to experiment with fine-grained, five-class sentiment on SST, while IMDB only offered binary labels. For IMDB, we randomly split the training set of 25, 000 examples into training and validation sets containing 22, 500 and 2, 500 examples respectively, as done in Maas et al. (2011)."}, {"heading": "6.2 METHODOLOGY", "text": "Our objective is to show a series of compounding extensions to the standard LSTM baseline that enhance accuracy. To ensure scientific reliability, the addition of each feature is the only change from the previous model (see Figures 4 and 5). The baseline model is a 2-layer stacked LSTM with hidden size 170 for SST and 120 for IMDB, as used in Tai et al. (2015). All models in this paper used publicly available 300 dimensional word vectors, pre-trained using Glove on 840 million tokens of Common Crawl Data (Pennington et al., 2014), and both the word vectors and the subsequent weight matrices were trained using Adam with a learning rate of 10\u22124.\nThe first set of basic feature additions were adding a forget bias and using dropout. Adding a bias of 1.0 to the forget gate (i.e. adding 1.0 to the inside of the sigmoid function in equation 3) improves results across NLP tasks, especially for learning long-range dependencies (Zaremba, 2015). Dropout (Srivastava et al., 2014) is a highly effective regularizer for deep models. For SST and IMDB we used grid search to select dropout probabilities of 0.5 and 0.7 respectively, applied to the input of each layer, including the projection/softmax layer. While forget bias appears to hurt performance in Figure 5, the combination of dropout and forget bias yielded better results in all cases than dropout without forget bias. Our last two basic optimizations were increasing the hidden sizes and then adding sharedweight bidirectionality to the RNN. The hidden sizes for SST and IMDB were increased to 800 and\n360 respectively; we found significantly diminishing returns to performance from increases beyond this. We chose shared-weight bidirectionality to ensure the model size did not increase any further. Specifically, the forward and backward weights are shared, and the input to the projection/softmax layer is a concatenation of the forward and backward passes\u2019 final hidden states.\nAll of our subsequent proposed model extensions are described at length in their own sections. For both datasets we chose 60 as the number of Monte Carlo samples and 300 as the output dimension of the Deep Vector Averaging MLP. Note that although the MLP weights increased the size of their respective models, this increase is negligible (equivalent to increasing the hidden size for SST from 800 to 804 or the hidden size of IMDB from 360 to 369), and we found that such a size increase had no discernible effect on accuracy when done outside the DVA framework."}, {"heading": "6.3 RESULTS", "text": "Since each of our proposed modifications operate independently, they are well suited to use in combination as well as in isolation. In Figures 4 and 5 we compound these features on top of the more traditional enhancements. Due to the expensiveness of bidirectional models, Figure 4 also shows these compounding features on SST with and without bidirectionality. The validation accuracy distributions show that each augmentation usually provides some small but noticeable improvement on the previous model, as measured by consistent improvements in mean and median accuracy.\nWe originally suspected that MC would provide marginal yet consistent improvements across datasets, while DVA would especially excel for long sequences like in IMDB, where n-gram based models and deep unordered compositions have benefited from their ability to retain information from disparate parts of the text. The former hypothesis was largely confirmed. However, while DVA was generally performance-enhancing, the performance boost it yielded for IMDB was not significantly larger than the one it yielded for SST, though that may have been because the other enhancements already encompassed most of the advantages provided by deep unordered compositions.\nThe only evident exceptions to the positive trend are the variations of residual connections. Which of Res-V1 (vertical only) and Res-V2 (vertical and residual) outperformed the other depended on the dataset and whether the network was bidirectional. The Res-V2 architecture dominated in experiments 4b and 5 while the Res-V1 (only vertical residuals) architecture is most performant in Figure 4a. This suggests for short sequences, bidirectionality and lateral residuals conflict. Further analysis of the effect of residual connections and model depth can be found in Figure 6. In that figure, the number of parameters, and hence model size, are kept uniform by modifying the hidden size as the layer depth changed. The hidden sizes used for 1, 2, 4, 6, and 8 layer models were 250, 170, 120, 100, and 85 respectively, maintaining \u2248 550, 000 total parameters for all models. As the graph demonstrates,\nnormal LSTMs (\u201cVanilla\u201d) perform drastically worse as they become deeper and narrower, while Res-V1 and Res-V2 both see their performance stay much steadier or even briefly rise. While depth wound up being far from a panacea for the datasets we experimented on, the ability of an LSTM with residual connections to maintain its performance as it gets deeper holds promise for other domains where the extra expressive power provided by depth might prove more crucial.\nSelecting the best results for each model, we see results competitive with state-of-the-art performance for both IMDB1 and SST, even though many state-of-the-art models use either parse-tree information (Tai et al., 2015), multiple passes through the data (Kumar et al., 2016) or tremendous train and test-time computational and memory expenses (Le and Mikolov, 2014). To our knowledge, our models constitute the best performance of purely sequential, single-pass, and computationally feasible models, precisely the desired features of a solid out-of-the-box baseline. Furthermore, for SST, the compounding enhancement model without bidirectionality, the final model shown in Figure 4b, greatly exceeded the performance of the large bidirectional model (51.6% vs 50.9%), with significantly less training time (Table 1). This suggests our enhancements could provide a similarly reasonable and efficient alternative to shared-weight bidirectionality for other such datasets."}, {"heading": "7 CONCLUSION", "text": "We explore several easy to implement enhancements to the basic LSTM network that positively impact performance. These include both fairly well established extensions (biasing the forget gate, dropout, increasing the model size, bidirectionality) and several more novel ones (Monte Carlo model averaging, Deep Vector Averaging, residual connections). We find that these enhancements improve the performance of the LSTM in classification tasks, both in conjunction or isolation, with an accuracy close to state of the art despite being more lightweight and using less information than the current state of the art architectures. Our results suggest that these extensions should be incorporated into LSTM baselines.\n1For IMDB, we benchmark only against results obtained from training exclusively on the labeled training set. Thus, we omit results from unsupervised models that leveraged the additional 50, 000 unlabeled examples, such as Miyato et al. (2016)."}], "references": [{"title": "An empirical evaluation of supervised learning in high dimensions", "author": ["Rich Caruana", "Nikos Karampatziakis", "Ainur Yessenalina"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Caruana et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2008}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Modeling compositionality with multiplicative recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "arXiv preprint arXiv:1412.6577,", "citeRegEx": "Irsoy and Cardie.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "CoRR, abs/1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Qianli Liao", "Tomaso A. Poggio"], "venue": "CoRR, abs/1604.03640,", "citeRegEx": "Liao and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Liao and Poggio.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Virtual adversarial training for semi-supervised text classification", "author": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07725,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "CoRR, abs/1607.04492,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Cnn features off-the-shelf: An astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "venue": "CoRR, abs/1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Residual networks are exponential ensembles of relatively shallow networks", "author": ["Andreas Veit", "Michael J. Wilber", "Serge J. Belongie"], "venue": "CoRR, abs/1605.06431,", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba"], "venue": null, "citeRegEx": "Zaremba.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": ", 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016).", "startOffset": 88, "endOffset": 108}, {"referenceID": 0, "context": "For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks.", "startOffset": 14, "endOffset": 36}, {"referenceID": 0, "context": "For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks. For computer vision, off-the-shelf convolutional neural networks (CNNs) have earned their reputation as a strong baseline (Sharif Razavian et al., 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016). For natural language processing (NLP) and other sequential modeling tasks, recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, with a linear projection layer at the end have begun to attain a similar status. However, the standard LSTM is in many ways lacking as a baseline. Zaremba (2015), Gal (2015), and others show that large improvements are possible using a forget bias, inverted dropout regularization or bidirectionality.", "startOffset": 14, "endOffset": 719}, {"referenceID": 0, "context": "For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks. For computer vision, off-the-shelf convolutional neural networks (CNNs) have earned their reputation as a strong baseline (Sharif Razavian et al., 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016). For natural language processing (NLP) and other sequential modeling tasks, recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, with a linear projection layer at the end have begun to attain a similar status. However, the standard LSTM is in many ways lacking as a baseline. Zaremba (2015), Gal (2015), and others show that large improvements are possible using a forget bias, inverted dropout regularization or bidirectionality.", "startOffset": 14, "endOffset": 731}, {"referenceID": 27, "context": "Since the h value completely updates at each timestep while the c value maintains part of its own value through multiplication by the forget gate f , h and c complement each other very well, with h forming a \u201cfast\u201d state that can quickly adapt to new information and c forming a \u201cslow\u201d state that allows information to be retained over longer periods of time (Zaremba, 2015).", "startOffset": 359, "endOffset": 374}, {"referenceID": 2, "context": "While various papers have tried to systematically experiment with the 6 core equations constituting an LSTM (Greff et al., 2015; Zaremba, 2015), in general the basic LSTM equations have proven extremely resilient and, if not optimal, at least a local maximum.", "startOffset": 108, "endOffset": 143}, {"referenceID": 27, "context": "While various papers have tried to systematically experiment with the 6 core equations constituting an LSTM (Greff et al., 2015; Zaremba, 2015), in general the basic LSTM equations have proven extremely resilient and, if not optimal, at least a local maximum.", "startOffset": 108, "endOffset": 143}, {"referenceID": 1, "context": "We investigated averaging over the output of the final recurrent layer (just before the projection layer), over the output of the projection layer (the presoftmax unnormalized logits), and the post-softmax normalized probabilities, which is the approach taken by Gal (2015) for language modeling.", "startOffset": 263, "endOffset": 274}, {"referenceID": 7, "context": "Reliably retaining long-range information is a well documented weakness of LSTM networks (Karpathy et al., 2015).", "startOffset": 89, "endOffset": 112}, {"referenceID": 12, "context": "This is especially the case for very long sequences like the IMDB sentiment dataset (Maas et al., 2011),captio where deep sequential models fail to capture uni- and bi-gram occurrences over long sequences.", "startOffset": 84, "endOffset": 103}, {"referenceID": 25, "context": "This is likely why n-gram based models, such as a bi-gram NBSVM (Wang and Manning, 2012), outperform RNN models on such datasetes.", "startOffset": 64, "endOffset": 88}, {"referenceID": 6, "context": "It was shown by Iyyer et al. (2015) and others that for general NLP classification tasks, the use of a deep, unordered composition (or bag-of-words) of a sequence can yield strong results.", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "For feed-forward convolutional neural networks used in computer vision tasks, residual networks, or ResNets, have obtained state of the art results (He et al., 2015).", "startOffset": 148, "endOffset": 165}, {"referenceID": 18, "context": "(2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al.", "startOffset": 106, "endOffset": 136}, {"referenceID": 21, "context": "(2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al., 2015), and won the 2015 ImageNet Challenge.", "startOffset": 168, "endOffset": 190}, {"referenceID": 4, "context": "Since then, various papers have tried to build upon the ResNet paradigm (Huang et al., 2016; Szegedy et al., 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 22, "context": "Since then, various papers have tried to build upon the ResNet paradigm (Huang et al., 2016; Szegedy et al., 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 11, "context": ", 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 24, "context": ", 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 3, "context": "For feed-forward convolutional neural networks used in computer vision tasks, residual networks, or ResNets, have obtained state of the art results (He et al., 2015). Rather than having each layer learn a wholly new representation of the data, as is customary for neural networks, ResNets have each layer (or group of layers) learn a residual which is added to the layer\u2019s input and then passed on to the next layer. More formally, if the input to a layer (or group of layers) is x and the output of that layer (or group of layers) is F (x), then the input to the next layer (or group of layers) is x+ F (x), whereas it would be F (x) in a conventional neural network. This architecture allows the training of far deeper models. He et al. (2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al.", "startOffset": 149, "endOffset": 746}, {"referenceID": 19, "context": "We chose two commonly used benchmark datasets for our experiments: the Stanford Sentiment Treebank (SST) (Socher et al., 2013) and the IMDB sentiment dataset (Maas et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 12, "context": ", 2013) and the IMDB sentiment dataset (Maas et al., 2011).", "startOffset": 39, "endOffset": 58}, {"referenceID": 12, "context": "For IMDB, we randomly split the training set of 25, 000 examples into training and validation sets containing 22, 500 and 2, 500 examples respectively, as done in Maas et al. (2011).", "startOffset": 163, "endOffset": 182}, {"referenceID": 16, "context": "All models in this paper used publicly available 300 dimensional word vectors, pre-trained using Glove on 840 million tokens of Common Crawl Data (Pennington et al., 2014), and both the word vectors and the subsequent weight matrices were trained using Adam with a learning rate of 10\u22124.", "startOffset": 146, "endOffset": 171}, {"referenceID": 27, "context": "0 to the inside of the sigmoid function in equation 3) improves results across NLP tasks, especially for learning long-range dependencies (Zaremba, 2015).", "startOffset": 138, "endOffset": 153}, {"referenceID": 21, "context": "The baseline model is a 2-layer stacked LSTM with hidden size 170 for SST and 120 for IMDB, as used in Tai et al. (2015). All models in this paper used publicly available 300 dimensional word vectors, pre-trained using Glove on 840 million tokens of Common Crawl Data (Pennington et al.", "startOffset": 103, "endOffset": 121}, {"referenceID": 19, "context": "Model # Params (M) Train Time / Epoch (sec) Test Acc (%) RNTN (Socher et al., 2013) \u2212 \u2212 45.", "startOffset": 62, "endOffset": 83}, {"referenceID": 8, "context": "7 CNN-MC (Kim, 2014) \u2212 \u2212 47.", "startOffset": 9, "endOffset": 20}, {"referenceID": 5, "context": "4 DRNN (Irsoy and Cardie, 2014) \u2212 \u2212 49.", "startOffset": 7, "endOffset": 31}, {"referenceID": 23, "context": "8 CT-LSTM (Tai et al., 2015) 0.", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "0 DMN (Kumar et al., 2016) \u2212 \u2212 52.", "startOffset": 6, "endOffset": 26}, {"referenceID": 15, "context": "1 NTI-SLSTM-LSTM (Munkhdalai and Yu, 2016) \u2212 \u2212 53.", "startOffset": 17, "endOffset": 42}, {"referenceID": 25, "context": "Model # Params (M) Train Time / Epoch (sec) Test Acc (%) SVM-bi (Wang and Manning, 2012) \u2212 \u2212 89.", "startOffset": 64, "endOffset": 88}, {"referenceID": 6, "context": "2 DAN-RAND (Iyyer et al., 2015) \u2212 \u2212 88.", "startOffset": 11, "endOffset": 31}, {"referenceID": 6, "context": "8 DAN (Iyyer et al., 2015) \u2212 \u2212 89.", "startOffset": 6, "endOffset": 26}, {"referenceID": 25, "context": "4 NBSVM-bi (Wang and Manning, 2012) \u2212 \u2212 91.", "startOffset": 11, "endOffset": 35}, {"referenceID": 13, "context": "2 NBSVM-tri, RNN, Sentence-Vec Ensemble (Mesnil et al., 2014) \u2212 \u2212 92.", "startOffset": 40, "endOffset": 61}, {"referenceID": 23, "context": "Selecting the best results for each model, we see results competitive with state-of-the-art performance for both IMDB1 and SST, even though many state-of-the-art models use either parse-tree information (Tai et al., 2015), multiple passes through the data (Kumar et al.", "startOffset": 203, "endOffset": 221}, {"referenceID": 9, "context": ", 2015), multiple passes through the data (Kumar et al., 2016) or tremendous train and test-time computational and memory expenses (Le and Mikolov, 2014).", "startOffset": 42, "endOffset": 62}, {"referenceID": 10, "context": ", 2016) or tremendous train and test-time computational and memory expenses (Le and Mikolov, 2014).", "startOffset": 76, "endOffset": 98}, {"referenceID": 14, "context": "Thus, we omit results from unsupervised models that leveraged the additional 50, 000 unlabeled examples, such as Miyato et al. (2016).", "startOffset": 113, "endOffset": 134}], "year": 2017, "abstractText": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "creator": "LaTeX with hyperref package"}}}