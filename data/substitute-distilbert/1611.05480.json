{"id": "1611.05480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Solving Cold-Start Problem in Large-scale Recommendation Engines: A Deep Learning Approach", "abstract": "collaborative filtering ( cf ) is much used in large - scale recommendation engines because of its efficiency, accuracy and scalability. however, in practice, common fact that recommendation engines based on cf require learning between users upon items before listing recommendations, make it inappropriate for new items which haven't been exposed to the core users to bargain with. this is such as the cold - start problem. getting this paper we introduce a novel approach which employs deep learning to tackle this problem in any cf based recommendation engine. one of among most iconic features of the proposed technique are the fact that it can be applied on top of any existing cn based recommendation engine without changing the cf core. sap successfully applied this technique to overcome redundant item cold - start problem in careerbuilder's process based compilation procedure. our experiments show that the proposed technique is very efficient to resolve the cold - start problem while maintaining high accuracy of the cf recommendations.", "histories": [["v1", "Wed, 16 Nov 2016 22:03:04 GMT  (488kb,D)", "http://arxiv.org/abs/1611.05480v1", "in Big Data, IEEE International Conference on, 2016"]], "COMMENTS": "in Big Data, IEEE International Conference on, 2016", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["jianbo yuan", "walid shalaby", "mohammed korayem", "david lin", "khalifeh aljadda", "jiebo luo"], "accepted": false, "id": "1611.05480"}, "pdf": {"name": "1611.05480.pdf", "metadata": {"source": "CRF", "title": "Solving Cold-Start Problem in Large-scale Recommendation Engines: A Deep Learning Approach", "authors": ["Jianbo Yuan", "Walid Shalaby", "Mohammed Korayem", "David Lin", "Khalifeh AlJadda", "Jiebo Luo"], "emails": ["jluo@cs.rochester.edu", "wshalaby@uncc.edu", "khalifeh.aljadda@careerbuilder.com"], "sections": [{"heading": null, "text": "Keywords-Deep Learning; Cold-Start; Recommendation System; Document Similarity; Job Search\nI. INTRODUCTION\nRecommendation Systems (RSs) utilize knowledge discovery and data mining techniques in order to predict items of interest to individual users and subsequently suggest these items to them as recommendations [1], [2]. Over the years, techniques and applications of RSs have evolved in both academia and industry due to the exponential increase in the number of both on-line information and on-line users. Such volumes of big data generated at very high velocities pose many challenges when it comes to developing and deploying scalable accurate RSs. Applications domains of RSs include: \u201ce-government, e-business, e-commerce/eshopping, e-library, e-learning, e-tourism, e-resource services and e-group activities\u201d [2]. On the other hand, new RSs deployment platforms emerged over years to include, besides the classical Web-based platform, other modern platforms like mobile, TV, and radio [2]. Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain\nand cross-platform scalability and quality challenges [1], [3], [4].\nApproaches for building RSs can be broadly clustered into three main categories: Content-Based (CB), Collaborative Filtering (CF), and hybrid techniques. CB techniques work by measuring or predicting the similarity between profiles of the items (attributes/descriptions) and profiles of the users\u2019 (attributes/descriptions of past preferred items) [5], [6]. Typically, the similarity is either measured using a suitable function (e.g., cosine similarity), or predicted using statistical or machine learning models (e.g., regression). CB systems are therefore suitable in domains where items\u2019 descriptions or their metadata can be easily obtained (e.g., books, news articles, jobs, TV shows, etc.). They are also suitable when the lifetime of items is short and/or the change in the item recommendation pool is frequent due to many new items entering the pool over short periods. These constraints limit the number of ratings those items can receive over their lifetime. Many challenges arise when it comes to CB systems. For example, items\u2019 metadata might be very limited or does not really contribute to generating user\u2019s interest. Item description, on the other hand, is typically textual which makes the similarity scoring more challenging due to language ambiguity raising the need for semanticaware CB systems [7].\nCF is perhaps the most prominent and successful techniques of modern recommendation systems. Unlike CB methods which recommend items that are similar to what target user liked in the past, CF methods leverage preferences of other similar users in order to make recommendations to the target user [4], [5]. In other words, CF tries to predict good recommendations by analyzing the correlations between all user behaviors rather than analyzing correlations between items content. CF is therefore suitable in domains where obtaining meta-features or descriptions of items is infeasible. Another motivation for CF is to introduce more diversity and serendipity to user\u2019s experience by leveraging the wisdom of the crowd, i.e., recommending items that\nar X\niv :1\n61 1.\n05 48\n0v 1\n[ cs\n.I R\n] 1\n6 N\nov 2\nthe target user might not have thought about or consumed before but rather liked by other similar users. One of the major challenges of CF based recommendation systems is the cold-start problem which occurs when there is a lack of information linking new users or items. In such cases, the RS is unable to determine how those users or items are related and is therefore unable to provide useful recommendations. Hybrid RSs try to achieve better performance by combining two or more techniques. The main theme of hybrid RSs is to leverage the advantages of both CB and CF, and to alleviate their shortcomings at the same time through hybridization.\nIn this work, we propose a deep learning based solution to the cold-start problem. Our solution utilizes a state-of-the-art deep learning document embedding algorithms (also known as doc2vec) [8]. Our main contributions are as follows: \u2022 We propose a deep learning based matching algorithm\nto solve cold-start and sparsity problems in CF based recommendation systems without major changes in the existing system as shown in figure 1. \u2022 We improve the performance of the deep learning matcher by incorporating contextual meta-data which boost the accuracy significantly. \u2022 The proposed algorithm is capable of being extended to solve similar cold-start problems in different real application scenarios such as relevancy and ranking in search engines.\nThe remainder of the paper is organized as follows: in Section II we will introduce the related work in recommendation systems and the document embedding; Section III concludes CF related techniques; we discuss the problem description and proposed algorithm in Section IV; Section V will explain the experiments, case study, and discussion. Finally we conclude in Section VI."}, {"heading": "II. RELATED WORK", "text": "The main objective for the Recommendation Systems (RSs) is providing a user with content he/she would like by estimating the relevancy or the rating of these contents based on the information about users and the items [1], [9]. The cold-start problem is one of the major challenges in RSs design and deployment. Cold-start occurs when either new users or new items are introduced into the system. In this situation there would be no behavioral data (ratings, purchases, clicks, etc.) for CF to work properly. Addressing cold-start is inevitable in modern RSs for two reasons [10]: 1) the item and user pools change on daily basis, and 2) CF is considered state-of-the-art recommendation technique but it requires significant behavioral data in order to work properly. Therefore, it is important to promote new items as quickly as possible in order to establish links between them and users to improve CF performance subsequently.\nAnother related problem to CF is the data sparsity which appears due to insufficient ratings per user and/or item in\nthe rating matrix challenging the ability of CF to predict accurate user preferences and item similarities.\nSeveral methods have been introduced to address the coldstart and the data sparsity problems [10]\u2013[14]. In this work, we focus on the problem of cold-start in item-based CF. Most of the proposed methods to address item cold-start adopt content-based approach; they utilize the content of new items in order to identify similar user profiles and subsequently recommend these new items to them. Saveski and Mantrach [10] proposed Local Collective Embeddings (LCE) as a solution for item cold-start. LCE utilizes descriptions of the new items (i.e., the term-document matrix) and project it collectively with the user-item rating matrix into a common low-dimensional space using non-negative matrix factorization. Soboroff and Nicholas [14] utilized Latent Semantic Indexing (LSI) [15] in order to create topical representations of user profiles in the latent space. New items are then projected into the same latent space and compared to user profiles then recommended to the most similar profiles. Similar to [14], Schein et al. [11] proposed an approach that creates a joint distribution of items and users through an aspect model in the latent space by combining items\u2019 content and users\u2019 preferences. Our approach to solve cold-start problem relies on pairing the new items with existing items that have been exposed to the users and gained enough ratings to be considered by\nCF. Thus, our approach differs from prior ones which try to utilize content-based techniques to pair the new items with users by matching the content of these items with the content of users\u2019 profile."}, {"heading": "III. BACKGROUND", "text": "Collaborative Filtering is one of the most successful techniques to building RSs due to their independence from the content of items being recommended, which make them easy to create and use across many application domains [4].\nTypically, CF methods utilize the user-item rating matrix R \u2208 R|U |\u00d7|I| which contains past ratings of items made by users. Rows in R represent users U = {u1, u2, ..., u|U |}, while columns represent items I = {i1, i2, ..., i|I|}. Each entry rui in R represents the rating user u gave to item i. The role of the CF algorithm is to perform matrix completion filling empty entries in R by analyzing existing entries.\nThe early generation of CF algorithms are the memorybased (a.k.a neighborhood-based) techniques [16], [17]. These techniques work by measuring the similarities or correlations between either users (rows) or items (columns) in the user-item rating matrix R. After finding these similar neighbors, recommendations are generated by choosing the top-K items similar to a given item in case of itembased recommendations, or by aggregating the correlation scores of items liked by similar users in case of user-based recommendations [18]\u2013[20].\nThe choice of the similarity or correlation metric has a major contribution to the quality of recommendations [18]. Pearson-r correlation is a very popular metric used in CF systems, and is used to estimate how well two variables are linearly related. Pearson-r correlation corri,j between two items or two users i, j is computed as:\ncorri,j = \u2211 w\u2208W (rwi \u2212 r\u0304i)(rwj \u2212 r\u0304j)\u221a\u2211\nw\u2208W (rwi \u2212 r\u0304i)2 \u221a\u2211\nw\u2208W (rwj \u2212 r\u0304j)2 (1)\nwhere, in case of user-based recommendation, w \u2208 W denotes items rated by both users i, j. rwi & rwj are ratings of item w by users i, j respectively. r\u0304i, r\u0304j are average ratings of users i, j respectively. In case of item-based recommendation, w \u2208W denotes users who rated items i, j. rwi & rwj are ratings of user w for items i, j respectively. r\u0304i, r\u0304j are average ratings of items i, j respectively. Another commonly used similarity metric is the cosine similarity which is computed as:\ncosi,j = ~Ri . ~Rj\n|| ~Ri|| || ~Rj || (2)\nwhere ~Ri & ~Rj are the rating vectors of items/users i, j in the rating matrix R in case of item-based/user-based recommendation respectively.\nAnother category of CF techniques are the model-based approaches which are more scalable than memory-based\ntechniques as they offline build a model of item-item or user-user similarities and then use it at real-time to generate recommendations. Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].\nAnother important aspect to understand this work is how to represent documents in the vector space to measure similarity [33]. Lexical features are commonly used for extracting vectors from documents including Bag-of-Words (BoW), n-grams (typically bigram and trigram), and term frequency-inverse document frequency (tf-idf ). Topic models such as Latent Dirichlet Allocation (LDA) are also used as features in document classification problems such as sentiment analysis and have shown promising results [34], [35]. Moreover, the application of deep learning to natural language processing field has shown a great success.\nTerm Frequency-Inverse Document Frequency: Common lexical features for document embedding include Bagof-Words, n-gram and tf-dif. Both Bag-of-Words and n-gram model draw much attention on frequent words, which may not be the best way to measure the importance of a word in a document. On the other hand, tf-idf can be considered as a weighted form of BoW for evaluating the importance of a word to the document. Let tf(w; d) be the number of times word w appears in document d from a collection D, idf(w;D) indicates inverse document frequency of word w in set D, then tf-idf is defined in Equation 3 and 4.\ntf \u2212 idf = tf(w, d)\u00d7 idf(w,D) (3)\nidf(w,D) = log N\n1 + |{d \u2208 D : w \u2208 d}| (4)\nLatent Dirichlet Allocation: LDA is a probabilistic model which learns P (z|w), distribution of a latent topic variable z given a word w [34]. Compared with lexical features (BoW and tf-idf ) mentioned above, representations learned by LDA focus more on the semantic meanings of each word, and have a feature space that is in low dimensions. Topic vectors learned represent the weights of words for each topic, and after normalizing each word vector from a sentence or a document, we obtain the vector of the sentence or document for all topics and thus embed the target document into a vector representation based on LDA model."}, {"heading": "IV. METHODOLOGY", "text": ""}, {"heading": "A. Problem Description", "text": "In dynamic domains like job boards, new items appear frequently which makes CF not feasible due to the coldstart problem. However, due to many shortcomings of the CB recommendations, most domains still prefer CF over CB due to the accuracy and diversity of results CF can provide\nwhich CB can not. That said, the cold-start problem continue to impact substantial number of new items. Recommendation engine is considered one of the major channels of engaging users with items and continuing to re-engage them when they leave a website via recommendation emails. Therefore, items which have no chance to be recommended due to the coldstart problem, will lose that major channel of exposure to end users. Thus, we propose a novel technique that helps CF to be considered in dynamic domains with frequent presence of new items by leveraging a deep learning matcher (DLM). The DLM is used to pair each new item with an existing item that has behavioral data (like ratings). This pairing will allow the new items to be considered in CF even before any users interact with these new items."}, {"heading": "B. Proposed Framework", "text": "In conventional CF based recommendation engines (Figure 2-a) the only items to be considered for recommendations are those with behavioral data (i.e. users interact with those items by rating, purchasing, clicking...etc), however, all new items without such behavioral data can not be part of the recommendations generated by CF. Our proposed system which is depicted in Figure 2-b adds a new module which can be thought of as a plug-in that will match each new item ix with an item that has behavioral data iy , we call this process pairing. Once this pairing is done, each pair (ix, iy) will be considered as one item, so when item iy is selected for recommendation by CF, item ix (the new item with no behavioral data) will appear with that recommendation as well. Therefore, the accuracy of the pairing is very important since pairing irrelevant items will introduce noise to the recommendations."}, {"heading": "C. Document Embedding and Matching", "text": "Given the importance of accurate pairs (similar ones) for the overall quality of the recommendations to be generated by CF afterwards, we tried the standard document similarity techniques like Term Frequency-Inverse Document Frequency (tf-idf ) and Latent Dirichlet Allocation (LDA) but the results were not promising. Therefore we built a Deep Learning Matcher (DLM) which outperforms all the other techniques significantly. Our DLM was built utilizing doc2vec which is considered the current state-of-the-art deep learning algorithm for document embedding and matching. Contrary to lexical features, the information extracted from each word is distributed all along a word window in distributed representations (as known as word2vec and doc2vec feature) as shown in Figure 3 [8]. For a word vector learning, given a sequence of T words {w1, w2, ..., wT } and a window\nsize c, the objective function is as follows:\n1\nT T\u2212c\u2211 i=c log p(wi|wi\u2212c, ..., wi+c) (5)\nIn order to maximize the objective function in Equation 5, the probability of wi is calculated based on the softmax function shown in Equation 6 where the word vectors are concatenated or averaged for predicting the next word in the content. Similarly to learning the word vectors, the processing of learning the paragraph vector (i.e. the doc2vec model) is maximizing the averaged log probability with the softmax function by combining the word vectors with the paragraph vector pi in a concatenated or averaged fashion as shown in Figure 3. For new documents input in the model, the paragraph vector is learned by holding the softmax parameters and gradient descending on the new vector entry.\np(wi|wi\u2212c, ..., wi+c) = eywi\u2211\nj\u2208(1,...,T ) e ywj\n(6)"}, {"heading": "D. Contextual Features Enrichment", "text": "Simply using documents that are in a large scale into the models is hardly good enough for learning a good representation of the documents [36]. doc2vec model can learn very good representations of the documents semantically. However, in some domains like job boards, many documents can share major overlapping content such as company or benefit descriptors, while the important context including the distinguished information such as requirements or qualifications are overshadowed. For example, in job boards domain 90% of a job description may be dedicated to describe the company and its values and culture, while only 10% describes the job requirements. In such scenario, doc2vec will generate almost same representation for two jobs posted by the same company, however they are totally different by the job requirements. To overcome this problem, we enrich each document with contextual features including the document classification, and location. Additionally, in our domain since not all the parts of a document (job posting) is equally important to measure similarity, we utilized our in-house NLP document parser to extract the important content such as job requirements and skills. These extracted information is injected into the original document N times (we choose N to be 3 empirically) to guide doc2vec to a better representation by improving the content distribution."}, {"heading": "V. EXPERIMENT AND RESULTS", "text": "To validate and evaluate the proposed technique, we applied our approach on top of Careerbuilder\u2019s CF-based recommendation engine. CareerBuilder operates one of the largest job boards in the world and has an extensive and growing global presence, with millions of job postings, more than 60 million actively-searchable resumes, over one billion searchable documents, and more than a million searches per\nhour. In their recommendation engine, Careerbuilder strives to recommend the right job to the right person at the right time. However, the cold-start problem is very serious at Careerbuilder given that every day thousands of new jobs are posted and it is important for the employers who have posted those jobs to start receiving applications from job seekers within a short period of time. Recommendation is a major channel of job exposure, therefore new jobs will lose a chance to be exposed through this important channel due to the cold-start problem. In this section we will discuss the different experiments we run to evaluate solving the coldstart problem using our technique. All experiments are done in a machine with Intel(R) Xeon (R) E5-2667 series CPUs (32 cores in total) and 264GB memory. The runtime of each model is also evaluated since our application scenario has a requirement of running the whole workflow on a large dataset (more than one million documents) on a daily basis."}, {"heading": "A. Job-to-Job (J2J) Matcher", "text": "All experiments used a sample of 1,147,725 classified jobs from our dataset [37]. The distribution of job classification is shown in Figure 4. For experiments we use Gensim [38] to generate tf-idf model and train LDA and doc2vec models. For the doc2vec model, we train our model with one iteration only due to speed limitation in order to run the whole process on a daily basis, and chose 100 dimensions for the embedded vectors. The reason is that, according to our experiments adding more dimensions for the vectors barely improved the performance if any, but increased the runtime notably. We choose the number of job categories which have at least 100 jobs included in our testing dataset to be the number of topics used for generating LDA model, so that one topic is expected to indicate one job category (java developer, registered nurse, etc). Eventually we have 805 topics in total. We use the fine-grained categories instead of the general ones since the number is small and can hardly learn a very good representation of the documents. We chose cosine similarity as the similarity metric to evaluate\nsimilarity between documents with threshold 0.5 to indicate if two documents are similar or not.\nIn this section we test the embedding algorithms for learning document level similarity, among which the tf-idf and LDA models are set as baselines. We evaluated the performance of each model both with and without contextual features enrichment. Since the contextual feature enrichment is proposed to improve the performance for certain specific circumstances, we don\u2019t add it in this experiments section. Our objective is to evaluate the job-to-job matching performance for our model rather than to evaluate the results for the whole recommendations, i.e., to evaluate the similarity we learned from different models.\nWe started our evaluation by selecting 100 randomly selected jobs as test samples and generated the top 10 most similar jobs for each test sample based on the doc2vec model with contextual features to achieve the best performance and evaluated the result manually as good/bad matches. The good matches are then considered as ground truth for evaluating other models. The overall precision for this set-up is 91.8%. To evaluate the other models, we use the similar jobs which have been labeled as good matches as ground truth and calculate the recall value (namely, among all the correct matches, how many of them have been returned as matches by the other models) from top 10 to 50 most similar jobs generated by the other models, as well as the precision based on the top 10 most similar jobs returned from each model (namely, among the top 10 most similar jobs returned from the other models, how many of them are labeled as correct match in our evaluation). The results are shown in Table I and Table II.\nAccording to the results shown in Table II, our proposed doc2vec model with contextual features yields the best performance for job-to-job matching task. On the other hand,\nthe LDA model obtains the lowest. The LDA model learns a good representation of the documents for classifying them under different topics, however for job recommendation and job-to-job pairing tasks it does not perform good enough. Previous works have shown that tf-idf model itself is capable of learning a good document embedding for documents which are long enough, and according to our results the tfidf model outperforms the LDA model by about 10%. Since the documents in our dataset are mainly job descriptions with contextual features including job requirements, job title, job classifications, etc., which are not expected to be long enough for tf-idf to learn a very good representation. The doc2vec outperforms the tf-idf and LDA models significantly and achieves the highest precision of 91.8%. By including the contextual features we achieve a 9% gain in the precision of the top-10 most similar jobs generated by the doc2vec model. For LDA and tf-idf the improvements are not comparable because the two baseline models are not capable of learning a good embedding for the documents and adding more information does not help. However, on the other hand the doc2vec model can learn a decent document embedding semantically and by adding the extra information we are able to obtain that significant improvement on performance. Table I shows the recall value of different models from top 10 to 50 most similar jobs. Since the manually labeled results based on doc2vec with contextual features are established as ground truth, we did not calculate the recall value for it. Based on our results the doc2vec model without contextual features still performs better than the other models, followed by the tf-idf model and then the LDA model. The doc2vec model without contextual features is able to return about 90% of the correct job matches for top 10 most similar jobs and almost all of the correct matches with the top 50, while the tf-idf model reaches at about 40% and the LDA model\nreaches at about 30%. By adding the context we still obtain an improvement on the recall values which is consistent with the results on precision.\nAs introduced in Section IV, we have thousands of new jobs added to the system on a daily basis and about the same number of jobs expiring. Therefore it is best to train our model and generate similar jobs for those jobs which suffer from the cold-start problem on a daily basis. This would require the runtime for the whole process including training and inferring to be in an hourly-scale. The runtime for each model is shown in Table III. We can conclude that by using multi-process computing (all models are using 24 cores for multi-process computing) we can run all the models on a daily basis except the LDA model. contextual features adds more text contents and more computation in the process, but with multi-process computing the extra runtime is reasonable compared with the significant improvement on the performance. The runtime for tf-idf model is the shortest,\nwhile doc2vec model with and without contextual features is still comparable which can be completed within two hours. For a daily based workflow it is acceptable and we choose to run it at midnight every day for our job recommendation systems."}, {"heading": "B. Case Study", "text": "The job-to-job matcher did admirably matching documents which are similar to one another, however, we encountered challenges with documents comprised of approximately 70% or higher of exact same content due to company descriptors like background, benefits and values. As the example shown in Table IV, compared with the source job on the left column, HVAC Technician was matched to Banquet Houseperson in the middle with completely different skill sets and requirements. This is due to the document being so similar as stated previously. In this example 25% of the document is actually different which still allows poor recommendations which are highly similar to prevail and\nbe recommended. To counter, we enrich contextual features which helped alleviate and push down similarity scores for these edge cased documents. Referring to the right column, HVAC Mechanic was returned instead when we emphasized on promoting the differences within the document. In our domain, the contextual features proved successful in filtering out highly similar scoring documents with different roles and functions."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "Collaborative Filtering is widely used in recommendation systems for real world applications, however, it suffers from the cold-start problem where new entities cannot be recommended or receive recommendations including the users and items. To tackle the cold-start problem we build an item-to-item deep learning matcher based on the document similarities learned from the state-of-the-art document embedding model doc2vec. The performance of document level similarities learned by the doc2vec model with contextual features outperforms the baseline models significantly and can run on a daily basis which fits the requirement of our practical application. Our approach can be integrated with any existing CF-based recommendation engine with no need to modify the CF core. To proof the efficiency, scalability, and accuracy of the proposed technique we apply it on top of Careerbuilder\u2019s CF-based recommendation engine which is used to recommend jobs to job seekers. After testing this model on more than 1 million documents we prove its efficiency in resolving the cold-start problem in large scale while maintaining high level of accuracy. We are working\non a multimodal document embedding model for learning user-to-user and user-to-job similarity whose initial results are very promising to solve the cold-start problem for userbased CF as shown in Table V."}], "references": [{"title": "Recommender systems survey", "author": ["Jes\u00fas Bobadilla", "Fernando Ortega", "Antonio Hernando", "Abraham Guti\u00e9rrez"], "venue": "Knowledge-Based Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Recommender system application developments: a survey", "author": ["Jie Lu", "Dianshuang Wu", "Mingsong Mao", "Wei Wang", "Guangquan Zhang"], "venue": "Decision Support Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["Pasquale Lops", "Marco De Gemmis", "Giovanni Semeraro"], "venue": "In Recommender systems handbook,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "A survey of collaborative filtering techniques", "author": ["Xiaoyuan Su", "Taghi M Khoshgoftaar"], "venue": "Advances in artificial intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Content-based recommender systems", "author": ["Charu C Aggarwal"], "venue": "In Recommender Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Content-based recommendation systems. In The adaptive web, pages 325\u2013341", "author": ["Michael J Pazzani", "Daniel Billsus"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Semantics-aware contentbased recommender systems", "author": ["Marco de Gemmis", "Pasquale Lops", "Cataldo Musto", "Fedelucio Narducci", "Giovanni Semeraro"], "venue": "In Recommender Systems Handbook,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Item cold-start recommendations: learning local collective embeddings", "author": ["Martin Saveski", "Amin Mantrach"], "venue": "In Proceedings of the 8th ACM Conference on Recommender systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["Andrew I Schein", "Alexandrin Popescul", "Lyle H Ungar", "David M Pennock"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Integrating trust and similarity to ameliorate the data sparsity and cold start for recommender systems", "author": ["Guibing Guo"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A collaborative filtering approach to mitigate the new user cold start problem", "author": ["Jes\u00faS Bobadilla", "Fernando Ortega", "Antonio Hernando", "Jes\u00faS Bernal"], "venue": "Knowledge-Based Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Combining content and collaboration in text filtering", "author": ["Ian Soboroff", "Charles Nicholas"], "venue": "In Proceedings of the IJCAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Paul Resnick", "Neophytos Iacovou", "Mitesh Suchak", "Peter Bergstrom", "John Riedl"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Grouplens: applying collaborative filtering to usenet news", "author": ["Joseph A Konstan", "Bradley N Miller", "David Maltz", "Jonathan L Herlocker", "Lee R Gordon", "John Riedl"], "venue": "Communications of the ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Badrul Sarwar", "George Karypis", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Item-based top-n recommendation algorithms", "author": ["Mukund Deshpande", "George Karypis"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Amazon. com recommendations: Item-to-item collaborative filtering", "author": ["Greg Linden", "Brent Smith", "Jeremy York"], "venue": "IEEE Internet computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["John S Breese", "David Heckerman", "Carl Kadie"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Collaborative filtering with the simple bayesian classifier", "author": ["Koji Miyahara", "Michael J Pazzani"], "venue": "In Pacific Rim International conference on artificial intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Collaborative filtering for multi-class data using belief nets algorithms", "author": ["Xiaoyuan Su", "Taghi M Khoshgoftaar"], "venue": "In 2006 18th IEEE International Conference on Tools with Artificial Intelligence", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Clustering methods for collaborative filtering", "author": ["Lyle H Ungar", "Dean P Foster"], "venue": "In AAAI workshop on recommendation systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Rectree: An efficient collaborative filtering method", "author": ["Sonny Han Seng Chee", "Jiawei Han", "Ke Wang"], "venue": "In International Conference on Data Warehousing and Knowledge Discovery,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Latent semantic models for collaborative filtering", "author": ["Thomas Hofmann"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Learning collaborative information filters", "author": ["Daniel Billsus", "Michael J Pazzani"], "venue": "In Icml,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Large-scale parallel collaborative filtering for the netflix prize", "author": ["Yunhong Zhou", "Dennis Wilkinson", "Robert Schreiber", "Rong Pan"], "venue": "In International Conference on Algorithmic Applications in Management,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Collaborative filtering using a regression-based approach", "author": ["Slobodan Vucetic", "Zoran Obradovic"], "venue": "Knowledge and Information Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "An mdp-based recommender system", "author": ["Guy Shani", "David Heckerman", "Ronen I Brafman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Model-based collaborative filtering", "author": ["Charu C Aggarwal"], "venue": "In Recommender Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "A neural knowledge language model", "author": ["Sunging Ahn", "Heeyoul Choi", "Tanel P\u00e4rnamaa", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1608.00318,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Query sense disambiguation leveraging large scale user behavioral data", "author": ["Mohammed Korayem", "Camilo Ortiz", "Khalifeh AlJadda", "Trey Grainger"], "venue": "In IEEE International Conference on Big Data (Big Data", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "of interest to individual users and subsequently suggest these items to them as recommendations [1], [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "of interest to individual users and subsequently suggest these items to them as recommendations [1], [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Applications domains of RSs include: \u201ce-government, e-business, e-commerce/eshopping, e-library, e-learning, e-tourism, e-resource services and e-group activities\u201d [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "deployment platforms emerged over years to include, besides the classical Web-based platform, other modern platforms like mobile, TV, and radio [2].", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain and cross-platform scalability and quality challenges [1], [3], [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 2, "context": "Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain and cross-platform scalability and quality challenges [1], [3], [4].", "startOffset": 200, "endOffset": 203}, {"referenceID": 3, "context": "Depending on the application domain and the deployment platform, dozens of techniques and methods have been proposed to address cross-domain and cross-platform scalability and quality challenges [1], [3], [4].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "CB techniques work by measuring or predicting the similarity between profiles of the items (attributes/descriptions) and profiles of the users\u2019 (attributes/descriptions of past preferred items) [5], [6].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "CB techniques work by measuring or predicting the similarity between profiles of the items (attributes/descriptions) and profiles of the users\u2019 (attributes/descriptions of past preferred items) [5], [6].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "Item description, on the other hand, is typically textual which makes the similarity scoring more challenging due to language ambiguity raising the need for semanticaware CB systems [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "Unlike CB methods which recommend items that are similar to what target user liked in the past, CF methods leverage preferences of other similar users in order to make recommendations to the target user [4], [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Unlike CB methods which recommend items that are similar to what target user liked in the past, CF methods leverage preferences of other similar users in order to make recommendations to the target user [4], [5].", "startOffset": 208, "endOffset": 211}, {"referenceID": 7, "context": "Our solution utilizes a state-of-the-art deep learning document embedding algorithms (also known as doc2vec) [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "The main objective for the Recommendation Systems (RSs) is providing a user with content he/she would like by estimating the relevancy or the rating of these contents based on the information about users and the items [1], [9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "The main objective for the Recommendation Systems (RSs) is providing a user with content he/she would like by estimating the relevancy or the rating of these contents based on the information about users and the items [1], [9].", "startOffset": 223, "endOffset": 226}, {"referenceID": 9, "context": "Addressing cold-start is inevitable in modern RSs for two reasons [10]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Several methods have been introduced to address the coldstart and the data sparsity problems [10]\u2013[14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Several methods have been introduced to address the coldstart and the data sparsity problems [10]\u2013[14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "Saveski and Mantrach [10] proposed Local Collective Embeddings", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Soboroff and Nicholas [14] utilized", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "Latent Semantic Indexing (LSI) [15] in order to create topical representations of user profiles in the latent space.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Similar to [14], Schein et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "[11] proposed an approach that creates a joint distribution of", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Collaborative Filtering is one of the most successful techniques to building RSs due to their independence from the content of items being recommended, which make them easy to create and use across many application domains [4].", "startOffset": 223, "endOffset": 226}, {"referenceID": 15, "context": "a neighborhood-based) techniques [16], [17].", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "a neighborhood-based) techniques [16], [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "After finding these similar neighbors, recommendations are generated by choosing the top-K items similar to a given item in case of itembased recommendations, or by aggregating the correlation scores of items liked by similar users in case of user-based recommendations [18]\u2013[20].", "startOffset": 270, "endOffset": 274}, {"referenceID": 19, "context": "After finding these similar neighbors, recommendations are generated by choosing the top-K items similar to a given item in case of itembased recommendations, or by aggregating the correlation scores of items liked by similar users in case of user-based recommendations [18]\u2013[20].", "startOffset": 275, "endOffset": 279}, {"referenceID": 17, "context": "The choice of the similarity or correlation metric has a major contribution to the quality of recommendations [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 107, "endOffset": 111}, {"referenceID": 24, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 25, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 144, "endOffset": 148}, {"referenceID": 26, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 185, "endOffset": 189}, {"referenceID": 27, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 223, "endOffset": 227}, {"referenceID": 28, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 229, "endOffset": 233}, {"referenceID": 29, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 252, "endOffset": 256}, {"referenceID": 30, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 291, "endOffset": 295}, {"referenceID": 31, "context": "Several algorithms were proposed to generate such models including Bayesian networks [21]\u2013[23], clustering [24], [25], latent semantic analysis [26], Singular Value Decomposition (SVD) [27], Alternating Least Squares (ALS) [28], [29] regression models [30], Markov Decision Processes (MDPs) [31], and others [32].", "startOffset": 308, "endOffset": 312}, {"referenceID": 32, "context": "Another important aspect to understand this work is how to represent documents in the vector space to measure similarity [33].", "startOffset": 121, "endOffset": 125}, {"referenceID": 33, "context": "Topic models such as Latent Dirichlet Allocation (LDA) are also used as features in document classification problems such as sentiment analysis and have shown promising results [34], [35].", "startOffset": 177, "endOffset": 181}, {"referenceID": 34, "context": "Topic models such as Latent Dirichlet Allocation (LDA) are also used as features in document classification problems such as sentiment analysis and have shown promising results [34], [35].", "startOffset": 183, "endOffset": 187}, {"referenceID": 33, "context": "Latent Dirichlet Allocation: LDA is a probabilistic model which learns P (z|w), distribution of a latent topic variable z given a word w [34].", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "each word is distributed all along a word window in distributed representations (as known as word2vec and doc2vec feature) as shown in Figure 3 [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 35, "context": "Simply using documents that are in a large scale into the models is hardly good enough for learning a good representation of the documents [36].", "startOffset": 139, "endOffset": 143}, {"referenceID": 36, "context": "All experiments used a sample of 1,147,725 classified jobs from our dataset [37].", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "[38] to generate tf-idf model and train LDA and doc2vec models.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Collaborative Filtering (CF) is widely used in large-scale recommendation engines because of its efficiency, accuracy and scalability. However, in practice, the fact that recommendation engines based on CF require interactions between users and items before making recommendations, make it inappropriate for new items which haven\u2019t been exposed to the end users to interact with. This is known as the cold-start problem. In this paper we introduce a novel approach which employs deep learning to tackle this problem in any CF based recommendation engine. One of the most important features of the proposed technique is the fact that it can be applied on top of any existing CF based recommendation engine without changing the CF core. We successfully applied this technique to overcome the item cold-start problem in Careerbuilder\u2019s CF based recommendation engine. Our experiments show that the proposed technique is very efficient to resolve the coldstart problem while maintaining high accuracy of the CF recommendations. Keywords-Deep Learning; Cold-Start; Recommendation System; Document Similarity; Job Search", "creator": "TeX"}}}