{"id": "1611.08737", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Structural Correspondence Learning for Cross-Lingual Sentiment Classification with One-to-Many Mappings", "abstract": "structural correspondence learning ( scl ) is an effective method for cross - lingual sentiment classification. this system uses unlabeled expressions along with enhanced word translation oracle : automatically induce task specific, cross - lingual correspondences. it encourages knowledge through each important features, i. e., pivot role. for presentation, however, it assumes that the word translation oracle maps each critical feature during source language to exactly only one word in target language. establishing one - to - one mapping between words in different languages is perfectly strict. also the context is not considered at all. starting this paper, we propose a cross - lingual scl based on descriptive representation of words ; it can learn meaningful one - to - many options for pivot roles using large amounts of monolingual data and potentially small dictionary. we conduct experiments on nlp \\ & amp ; cc 2013 cross - lingual sentiment analysis dataset, employing english as general linguistic, and chinese functional target language. second method does not rely on eliminating parallel corpora and the experimental results show that our approach is more competitive matching the state - of - the - art methods in cross - lingual sentiment classification.", "histories": [["v1", "Sat, 26 Nov 2016 20:11:00 GMT  (111kb,D)", "http://arxiv.org/abs/1611.08737v1", "To appear in AAAI 2017. arXiv admin note: text overlap witharXiv:1008.0716by other authors"]], "COMMENTS": "To appear in AAAI 2017. arXiv admin note: text overlap witharXiv:1008.0716by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.ML", "authors": ["nana li", "shuangfei zhai", "zhongfei zhang", "boying liu"], "accepted": true, "id": "1611.08737"}, "pdf": {"name": "1611.08737.pdf", "metadata": {"source": "CRF", "title": "Structural Correspondence Learning for Cross-lingual Sentiment Classification with One-to-many Mappings", "authors": ["Nana Li", "Shuangfei Zhai", "Zhongfei Zhang", "Boying Liu"], "emails": ["lin@binghamton.edu", "szhai2@binghamton.edu,", "zhongfei@cs.binghamton.edu", "lby@hebut.edu.cn"], "sections": [{"heading": "Introduction", "text": "Sentiment classification is the task to predict the sentiment polarity of a given document such as a product review or commentary essay. Its goal is to develop automated approaches that can classify sentiment polarity in text as positive, neutral, or negative. To obtain a satisfactory classification performance, most methods require lots of labeled data, which can be costly in terms of both time and human efforts. While there have been lots of resources available in English, including labeled corpora and sentiment lexicons, for other languages, such resources are often insufficient. Thus, it is expected to make use of the knowledge learned from those resource-rich languages to perform sentiment classification in other languages, which can substantially reduce human efforts. This problem is called cross-lingual sentiment classification (CLSC), which we address in this paper.\nCLSC uses annotated sentiment corpora in one language as the training data, to predict the sentiment polarity of the data in another language. Domain adaptation focuses on solving this problem by transferring knowledge from the\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ntypically sufficient training samples in a resource-rich language to target, resource-scarce language. As a result, domain adaptation has been proposed to address this problem ((Blitzer, McDonald, and Pereira 2006); (Banea et al. 2008); (Prettenhofer and Stein 2011)). There has been a lot of work in domain adaptation, and one effective method for CLSC is based on structural correspondence learning (SCL), named as cross-lingual structural correspondence learning (CLSCL) proposed by Prettenhofer and Stein (Prettenhofer and Stein 2010). Its key idea is to identify a low-dimensional representation that captures the correspondence between features from both domains by modeling their correlations with some special pivot features. From these correspondences a cross-lingual representation is created that enables the transfer of classification knowledge from the source to the target language. This approach is a good fit for CLSC as it transfers knowledge through identifying important features. However, for simplicity, CL-SCL assumes that the word translation oracle maps each pivot word in source language to exactly only one word in target language. As we all know, machine translation performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation. Furthermore, this one-to-one translation between words in different languages is too strict.\nIn 2013, Mikolov et al.(Mikolov, Le, and Sutskever 2013) proposed a method for exploiting similarities among languages. It used the distributed representation of words and learned a linear mapping between vector spaces that represent the corresponding languages, respectively. It translated word and phrase entries by learning language structures based on large monolingual data and mappings between languages from small amount of bilingual data. Despite its simplicity, the results showed that their method was surprisingly effective, especially for the translation between languages that are substantially different (such as English to Chinese). In this paper we introduce the distributed representation of words into the CL-SCL, and propose a novel structural correspondence learning method with oneto-many mappings (SCL-OM). This method aims at building up the one-to-many mappings between the pivot features in source language and those in the target language. Evaluations on NLP&CC 2013 datasets show that our algorithm outperforms the state-of-the-art methods. ar X iv :1\n61 1.\n08 73\n7v 1\n[ cs\n.L G\n] 2\n6 N\nov 2\n01 6"}, {"heading": "Related Work", "text": ""}, {"heading": "Sentiment Classification", "text": "Sentiment classification is usually formulated as a two-class classification problem, positive and negative. Training and testing data are normally product reviews. It has emerged and become a very active research area since the year 2000 (Liu 2012). In general, sentiment classification has been investigated mainly at three levels: document level, sentence level, and aspect level. In this paper, we only focus on document level. Document level sentiment classification aims to classify an opinion document as expressing a positive or negative opinion. The approaches are generally based on two kinds of resources: sentiment lexicons and corpora. Lexiconbased approaches predict the sentiment polarities by creating and using sentiment lexicons, while corpora-based approaches generally treat the sentiment classification problem as a machine learning task. Most of the existing approaches focus on extracting various features from text and then applying supervised learning techniques to learn classifiers (Hu and Liu 2004);(Mullen and Collier 2004). Pang (Pang, Lee, and Vaithyanathan 2002) was the first to take supervised learning to classify movie reviews using unigrams as features in classification with standard machine learning techniques (Naive Bayes, maximum entropy classification, and support vector machines). In the subsequent research, many more features and learning algorithms were developed by a number of researchers. Like other supervised machine learning applications, the key factor for sentiment classification is a set of effective features. There are also some unsupervised methods. Turney et al. (Turney 2002) were the first to apply an unsupervised learning technique based on the mutual information between document phrases and predict the sentiment orientation by the average scores of the phrases given in the document. Zhai et al. (Zhai and Zhang 2016) proposed an autoencoder based semisupervised learning method to learn representations with both labeled and unlabeled data. However, supervised approaches are the mainstream methods for sentiment classification. Moreover, most of the existing supervised and semi-supervised approaches typically require high-quality labeled data to train classifiers with a good accuracy."}, {"heading": "Cross-lingual Sentiment Classification", "text": "The traditional CLSC approaches employ machine translation systems to bridge the gap between the source language and target language. Wan et al. (Wan 2009) proposed a cotraining approach to address this problem. The labeled English reviews and unlabeled Chinese reviews were translated into labeled Chinese reviews and unlabeled English reviews separately. Each review thus had the two views. support vector machines (SVM) was then applied to learn two classifiers. Finally, the two classifiers were combined into a single classifier. Pan et al. (Pan et al. 2011) designed a bi-view nonnegative matrix tri-factorization model using machine translation. It learned previously unseen sentiment words from the large parallel dataset. Li et al. (Li et al. 2011) studied semi-supervised learning for imbalanced sentiment classification by using a dynamic co-training approach. Gui et\nal. (Gui et al. 2013) compared several of these approaches, and then they incorporated class noise detection into transductive transfer learning to reduce negative transfers in the process of transfer learning (Gui et al. 2014). However, machine translation is far from perfect. The translated text can potentially mislead the classifier. Consequently, many researchers use domain adaptation to solve this problem. Various domain adaptation techniques are explored (Blitzer et al. 2007); (Banea et al. 2008); (Prettenhofer and Stein 2011); (Meng et al. 2012). Prettenhofer and Stein (Prettenhofer and Stein 2010) proposed a representative domain adaptation approach CL-SCL which was effective for cross-lingual sentiment classification. They found a set of pivot features shared by both source language and target language, and then learned the correlations between pivot and non-pivot features and generated a projection matrix to build a bridge between the two languages. Meng et al. (Meng et al. 2012) proposed a cross-lingual mixture model (CLMM) to leverage unlabeled bilingual parallel data. Zhang et al. (Zhang, Chao, and Wang 2015) proposed a semi-supervised learning approach with an adjusted method to train an initial classifier to predict the labels for target instances and then to obtain a new label space and a large-scale, labeled target language dataset. It selected the confident instances and trained a new classifier. This method needs to learn three classifiers and is very time-consuming.\nWith the development of deep learning, shared deep representations are employed for CLSC. Some researchers apply deep learning techniques to learn bilingual representations (Zhou et al. 2015);(Mogadala and Rettinger );(Zhou, Wan, and Xiao 2016). Paired sentences from parallel corpora are used to learn word embeddings across languages, eliminating the need of machine learning. Zhou et al. (Zhou et al. 2015) proposed an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC, and it incorporated sentiment information of text into bilingual embeddings. Zhou et al. (Zhou, Wan, and Xiao 2016) proposed a cross-lingual representation learning model which simultaneously learned both the word and document representations in both languages. However, high-quality bilingual embeddings rely on the large-scale task-related parallel corpora, which are also a scarce resource."}, {"heading": "CL-SCL with One-to-many Mappings", "text": ""}, {"heading": "Problem Definition", "text": "We have a set of labeled training documents DS = {(xi, yi)}ni=1 written in source language S;XS is the source language feature space, XT is the target language feature space, and Y is the set of class labels. Let X = Xs \u22c3 XT denote the feature space. For simplicity, and without loss of generality, we consider the binary classification problem, i.e., Y = {+1,\u22121}. In addition, VS denotes the source vocabulary, VT denotes the target vocabulary, and V = VS \u22c3 VT . Besides the labeled training documents DS , we have unlabeled documents DS,U and DT,U from the source language S and the target language T , respectively. Let DU denote DS,U \u22c3 DT,U . The goal is to create a classifier for documents written in a different language T , which can pre-\ndict the labels of new, previously unseen reviews in T ."}, {"heading": "Pivot Sets Based on CL-SCL with One-to-many Mappings", "text": "Our approach is based on CL-SCL, which is an approach to cross-lingual text classification that builds on structural correspondence learning. In order to induce task-specific, crosslingual word correspondences, this approach used unlabeled documents, along with a word translation oracle. The advantage of this method is resource efficiency because parallel corpora are not recquired. The first step of CL-SCL is to define a set of pivot features on the unlabeled data from both languages. Then these pivot features are used to learn a mapping \u03b8 from the original feature spaces of both languages to a shared, low-dimensional feature space. Once such a mapping is found, the cross-lingual sentiment classification problem is reduced to a standard classification problem in the cross-lingual space.\nPivot features are selected to induce correspondences among the words from both languages, and they play a very important role in CL-SCL. A pivot is a pair of words, {wS , wT }, separately from the source language S and the target language T , which possess the similar semantics. wT is wS\u2019s translation in the target vocabulary VT by querying the translation oracle. However, for simplicity, CL-SCL assumes that the word translation oracle maps each pivot in source language into exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. In addition it does not consider the context either when translating the pivot features. In this step we propose a cross-lingual SCL based on distributed representation of words; it learns meaningful one-to-many translations for words using large amounts of monolingual data and a small dictionary. The framework of our approach to generate pivot features is demonstrated in Figure 1.\nFirst, select pivots wS in the source language according\nto the mutual information with respect to the class labels in the labeled training documents in source language. CLSCL used a word translation oracle (e.g., a domain expert) to map words in the source vocabulary VS to their corresponding translations in the target vocabulary VT . Unlike CL-SCL, it is more reasonable to build one-to-many mappings between words in the two languages. Thus, instead of using translator or domain expert, we build a one-to-many mapping by learning from bilingual word pairs using distributed representation of words. We incorporate the method Word2Vec recently proposed in (Mikolov, Le, and Sutskever 2013) into CL-SCL for the translation. We use Word2Vec\u2019s CBOW model to learn the representations of languages. This model learns word representations using the neural network architecture that aims to predict the neighbors of a word. The process is as follows:\n\u2022 Step 1, Build monolingual models of languages (i.e., distributed representation of words in source language and target language) using large amounts of documents DS,U and DT,U . Suppose that there are a set of word pairs D = {si, ti}(i = 1, 2, \u00b7 \u00b7 \u00b7 , num), si \u2208 VS , ti \u2208 VT , where ti is the translation of si. We obtain their associated vector representations {xi, zi},where xi is the distributed representation of word si in the source language S, and zi is the vector representation of ti in the target language T .\n\u2022 Step 2, Use {xi, zi} to learn a linear projection between the languages. The goal is to find a translation matrix W such that Wxi approximates zi. We learn W by the following optimization problem:\nmin W num\u2211 i=1 \u2016Wxi \u2212 zi\u201622 (1)\n\u2022 Step 3, Translate wS into wT by projecting their vector representations from the source language space to the target language space. Suppose that the vector representa-\ntion of word a (a \u2208 ws) is xas. We map it to the target language space by computing b = Wxas. Then we obtain a set \u03a8 of pn words that are the top pn closest to b in the target language space, using cosine similarity as the distance metric. For simplicity, we set pn = 3. \u03a8 = {(word1, d1), (word2, d2), (word3, d3)}, where di denotes the cosine distance between b and wordi, and they can be automatically obtained from Word2Vec. We define a threshold \u03c6. If d1 \u2212 d2 and d2 \u2212 d3 are both smaller than \u03c6, we take {word1, word2, word3} as the translation of a; if d1 \u2212 d2 is smaller than \u03c6 but d2 \u2212 d3 is larger than \u03c6, we take {word1, word2} as the translation; otherwise, we take {word1} as the translation of a. For example, we consider the pivot words \u201cexcellent\u201d and \u201crecommend\u201d as follows: \u03a8excellent = {(\u68d2/excellent,0.628),(\u592a\u597d\u4e86/very good, 0.613), (\u51fa \u8272/outstanding, 0.603)} ; and \u03a8recommend = {(\u63a8 \u8350/recommend, 0.835), (\u5efa\u8bae/advise, 0.695), (\u8d2d\u4e70/buy, 0.581)}. The English word after \u201c/\u201d is the translation from chinese to english for non-chinese readers. Supposing that \u03c6 is 0.05, the translations of \u201cexcellent\u201d and \u201crecommend\u201d respectively are : {\u68d2,\u592a\u597d\u4e86,\u51fa\u8272} and {\u63a8\u8350}. Finally, we eliminate the candidate pivots {wS , wT } where the document frequency of wS or wT is smaller than a threshold \u03b4."}, {"heading": "Framework of the Proposed Method", "text": "First, as described above, we generate pivot features P (|P | = m) which are pairs of words {wS , wT }, where wS is the pivot word in source language and wT is the pivot word in target language. From wS to wT , it is a one-tomany mapping by learning from bilingual word pairs using distributed representation of words. The details are described in the above Section. Second, similar to CL-SCL, we build the connection from unlabeled documents in both source and target languages and obtain the low-dimensional hypothesis space \u03b8. For each pivot pl \u2208 P , a linear classifier is trained to model the correlations between the pivot {wS , wT } and all other words w /\u2208 {wS , wT }. Each linear classifier is characterized by the parameter vector wl. Thus, a |V |\u00d7m dimensional parameter matrixW can be obtained, W = [w1w2 \u00b7 \u00b7 \u00b7wm]. Correlations across pivots are identified by computing the singular value decomposition ofW to find a low dimensional representation.\nU\u03a3V T = SV D(W ) (2)\nChoosing the columns of U associated with the largest singular values yields those substructures that capture most of the correlation in W . Define \u03b8 as those columns of U that are associated with the k largest singular values:\n\u03b8 = UT[1:k,1:|V |] (3)\nApply the projection \u03b8 to each input instance x. The vector v that minimizes the regularized training error for DS in the projected space is defined as follows:\nv\u2217 = argmin v\u2208Rk \u2211 (x,y)\u2208DS L(y, vT \u03b8x) + \u03bb 2 \u2016v\u20162 (4)\nThe final classifier fST (x) is defined as follows:\nfST (x) = sign(v \u2217T\u03b8x) (5)"}, {"heading": "Experiments", "text": "In this section, we evaluate the effectiveness and efficiency of the algorithm SCL-OM proposed in this paper. We use English as source language and Chinese as target language for the task of cross-lingual sentiment classification."}, {"heading": "Dataset and Preprocessing", "text": "We evaluate the proposed approach on an open cross-lingual sentiment analysis task in NLP&CC 2013. The dataset includes product reviews of three product categories from Amazon (Books, DVD, and Music). Each category contains 4,000 labeled English reviews as the training data, 4,000 Chinese reviews as the test data, and over ten thousands of Chinese product reviews without label. Furthermore, since training the monolingual language model needs a large amount of text data, we use the unlabeled English reviews from (Prettenhofer and Stein 2011) to learn the representations of English words. See Table 1 for details.\nEach English or Chinese review includes summary, text and category; we extract the content of summary and text and combine them as one review document d , which is expressed as a feature vector x using unigram bag-of-words model. In addition, we only select those words as the features with the frequency frew larger than 5. We summarize the vocabulary size of the datasets in Table 2.\nFirst, the monolingual word vectors are trained using CBOW model with negative sampling of window size 5. To generate a bilingual dictionary between languages, we use the most frequent 500 words from the monolingual source datasets, and translate these words using online Google Translate. In addition, in our experiments, the Chinese word segmentation tool is Jieba, and the monolingual sentiment classifier is SVM."}, {"heading": "Methods", "text": "\u2022 Train CHN: The labeled English reviews are translated to\nChinese by Google Translate with correspondence labels. A Chinese SVM classifier is learned with the translated reviews. The Chinese testing dataset is used for test.\n\u2022 Train ENG: using labeled English reviews as training data, an English SVM classifier is learned. Then the Chinese test reviews are translated to English by Google Translate. The translated testing dataset is used for test.\n\u2022 Basic co-training (Co-Train): The co-training method proposed in (Pan et al. 2011) is implemented. It is a bidirectional transfer learning.\n\u2022 CL-SCL: This method is proposed by Prettenhofer et al. Google Translate is used to do the translation which only returns one word for each pivot feature. Also we fix the same parameter values as those in our method m = 300, \u03d5 = 30, k = 120.\n\u2022 Best result in NLP&CC 2013 : This is the best result reported in NLP&CC 2013. Unfortunately, the specification of the method is not available.\n\u2022 NTD (Best): Gui et al. (Gui et al. 2013) proposed a mixed CLSC model by combining co-training and transfer learning strategies. They (Gui et al. 2015) further improved the accuracy by removing the noise from the transferred samples to avoid negative transfers (NTD).\n\u2022 BSWE (Best): Zhou et al. (Zhou et al. 2015) proposed a method that learned bilingual sentiment word embedding (BSWE) for English-Chinese CLSC. The proposed BSWE incorporated sentiment information of text into bilingual embedding. The baseline methods described above are categorized into two classes: the first four are preliminary methods; the last three are state-of-the-art models for CLSC. All the methods use SVM as basic classifier and use unigram+bigram features to train the basic classifiers except that CL-SCL and our approach use unigram features."}, {"heading": "Performance Results", "text": "Recall that SCL-OM has six parameters as input: the number of pivots m, the dimensionality of the cross-lingual representation k, the minimum support \u03b4 of a pivot, word similarity distance threshold \u03c6, the dimensionality of English word vectors and the dimensionality of Chinese word vectors. We use fixed values of m = 300, k = 120, \u03b4 = 30, and \u03c6 = 0.1. As for the dimensionality of word vectors, Mikolov et al. (Mikolov, Le, and Sutskever 2013) showed that the dimensionality of the vectors trained in the source language should be several times (around 2 to 4 times) larger than that of the vectors trained in the target language for the best performance. Thus, we set the dimensionality of English word vectors as 200 and that of Chinese word vectors as 50.\nTable 3 documents the comparisons of the performances between our approach and the competing methods on NLP&CC 2013 CLSC dataset. From the results, we see that\nthe best performance of co-training is better than that of CLSCL, but it requires parallel dataset both in training and testing processes. Gui et al. combined co-training and transfer learning strategies. Their method achieved the highest accuracy of 80.1% in NLP&CC CLSC task (Gui et al. 2014). They further improved the accuracy to 80.8% (Gui et al. 2015) by removing the noise from the transferred samples to avoid negative transfers. Zhou et al. (Zhou et al. 2015) built denoising auto-encoders in two independent views to enhance the robustness to translation errors in the inputs. It integrated the bilingual embedding learning into a unified process, and achieved 80.7% accuracy. Our approach reaches up to 81.4% average accuracy with the fixed parameters. In Table 3, the last row shows the best results of our approach. For Books and Music categories, SCL-OM achieves the best accuracy when k = 120, m = 300, while for DVD, it is when k = 110, m = 200. The experimental results show that our approach is competitive with the state-of-theart in cross-language sentiment classification."}, {"heading": "Sensitivity Analysis", "text": "In this section, we analyze the sensitivity of the two important parameters while keeping the others fixed: the number of pivots m and the dimensionality of the cross-lingual representation k.\nNumber of Pivots m: Figure 2 shows the influence of the number of pivots m on the performance of SCL-OM. The plots show that a small number of pivots can capture a significant amount of the correspondence between S and T .\nDimensionality of the Cross-Lingual Representation k: Figure 3 shows the influence of the dimensionality of the cross-lingual representation k on the performance of SCLOM. we evaluate SCL-OM when parameter k varies from 50 to 300. As shown in Figure 3, the average accuracies generally move upward as k increases. When k \u2208 (100, 150), the accuracy reaches the peak value in all three categories, and then the accuracy declines with the increase of k.\nFurthermore, to gain more insight of the results, we visualize a small part of pivots learned by SCL-OM shown in Table 4. In table 4, the first and the third columns are some examples of pivot features in source language S, and the Chinese characters in column two and column four are their corresponding mappings in target language T obtained by our approach. The English word after \u201c/\u201d is the translation for non-chinese readers. From this table, we can see that the one-to-many mappings based on the distributed representation of words is more reasonable than the one-to-one mapping by machine translation."}, {"heading": "Conclusion", "text": "In this paper, we propose a novel structural correspondence learning method for cross-lingual sentiment classification with one-to-many mappings. This method employs distributed representation of words to build one-to-many mappings between the pivot features in source language and those in target language. It does not rely on the parallel corpora. This method is evaluated on the NLP&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. The experimental results show that our approach is competitive with the state-of-the-art methods in cross-lingual sentiment classification. However, our approach ignores polysemy in the one-to-many mappings. In the future, we will explore the method for learning sense-specific word embedding."}, {"heading": "Acknowledgments", "text": "This work is supported in part by Tianjin National Natural Science Foundation for Young Scholars (13JCQNJC00200)."}], "references": [{"title": "Multilingual subjectivity analysis using machine translation", "author": ["Banea"], "venue": "In Proceedings of the Conference", "citeRegEx": "Banea,? \\Q2008\\E", "shortCiteRegEx": "Banea", "year": 2008}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer"], "venue": "In ACL,", "citeRegEx": "Blitzer,? \\Q2007\\E", "shortCiteRegEx": "Blitzer", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["McDonald Blitzer", "J. Pereira 2006] Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 conference on empirical methods in natural language processing,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "A mixed model for cross lingual opinion analysis", "author": ["Gui"], "venue": "In Natural Language Processing and Chinese Computing", "citeRegEx": "Gui,? \\Q2013\\E", "shortCiteRegEx": "Gui", "year": 2013}, {"title": "Cross-lingual opinion analysis via negative transfer detection", "author": ["Gui"], "venue": "In ACL", "citeRegEx": "Gui,? \\Q2014\\E", "shortCiteRegEx": "Gui", "year": 2014}, {"title": "Improving transfer learning in cross lingual opinion analysis through negative transfer detection", "author": ["Gui"], "venue": "In International Conference on Knowledge Science, Engineering and Management,", "citeRegEx": "Gui,? \\Q2015\\E", "shortCiteRegEx": "Gui", "year": 2015}, {"title": "and Liu", "author": ["M. Hu"], "venue": "B.", "citeRegEx": "Hu and Liu 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "2011", "author": ["S. Li", "Z. Wang", "G. Zhou", "S.Y. M Lee"], "venue": "Semi-supervised learning for imbalanced sentiment classification. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22,", "citeRegEx": "Li et al. 2011", "shortCiteRegEx": null, "year": 1826}, {"title": "Sentiment analysis and opinion mining. Synthesis lectures on human language technologies 5(1):1\u2013167", "author": ["B. Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2012\\E", "shortCiteRegEx": "Liu", "year": 2012}, {"title": "Cross-lingual mixture model for sentiment classification", "author": ["Meng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Meng,? \\Q2012\\E", "shortCiteRegEx": "Meng", "year": 2012}, {"title": "Q", "author": ["Mikolov, T.", "Le"], "venue": "V.; and Sutskever, I.", "citeRegEx": "Mikolov. Le. and Sutskever 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Collier", "author": ["T. Mullen"], "venue": "N.", "citeRegEx": "Mullen and Collier 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Cross-lingual sentiment classification via bi-view non-negative matrix tri-factorization", "author": ["Pan"], "venue": null, "citeRegEx": "Pan,? \\Q2011\\E", "shortCiteRegEx": "Pan", "year": 2011}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Lee Pang", "B. Vaithyanathan 2002] Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "and Stein", "author": ["P. Prettenhofer"], "venue": "B.", "citeRegEx": "Prettenhofer and Stein 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Stein", "author": ["P. Prettenhofer"], "venue": "B.", "citeRegEx": "Prettenhofer and Stein 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Zhang", "author": ["S. Zhai"], "venue": "Z.", "citeRegEx": "Zhai and Zhang 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning on cross-lingual sentiment analysis with space transfer", "author": ["Chao Zhang", "H. Wang 2015] Zhang", "W. Chao", "D. Wang"], "venue": "In Big Data Computing Service and Applications (BigDataService),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Learning bilingual sentiment word embeddings for cross-language sentiment classification", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2015\\E", "shortCiteRegEx": "Zhou", "year": 2015}, {"title": "Cross-lingual sentiment classification with bilingual document representation learning", "author": ["Wan Zhou", "X. Xiao 2016] Zhou", "X. Wan", "J. Xiao"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Structural correspondence learning (SCL) is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification.", "creator": "LaTeX with hyperref package"}}}