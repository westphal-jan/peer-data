{"id": "1611.09100", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Learning to Compose Words into Sentences with Reinforcement Learning", "abstract": "we use reinforcement learning platforms learn pre - structured neural networks for computing representations encoding natural language sentences. in contrast with prior tests on case - structured models in which the trees are either provided as linguistic statements predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream boundary. experiments demonstrate extended benefit of learning task - specific composition orders, utilizing both sequential encoders and recursive encoders based on treebank annotations. we analyze the induced trees by show that while agents discover some linguistically intuitive structures ( ab. g., noun phrases, singular verb phrases ), they are different than conventional english syntactic structures.", "histories": [["v1", "Mon, 28 Nov 2016 12:57:07 GMT  (90kb,D)", "http://arxiv.org/abs/1611.09100v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dani yogatama", "phil blunsom", "chris dyer", "edward grefenstette", "wang ling"], "accepted": true, "id": "1611.09100"}, "pdf": {"name": "1611.09100.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["REINFORCEMENT LEARNING", "Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "emails": ["dyogatama@google.com", "pblunsom@google.com", "cdyer@google.com", "etg@google.com", "lingwang@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Languages encode meaning in terms of hierarchical, nested structures on sequences of words (Chomsky, 1957). However, the degree to which neural network architectures that compute representations of the meaning of sentences for practical applications should explicitly reflect such structures is a matter for debate.\nThere are three predominant approaches for constructing vector representations of sentences from a sequence of words. The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015). In such models, there is no explicit hierarchical organization imposed on the words, and the RNN\u2019s dynamics must learn to simulate it. The second approach uses tree-structured networks to recursively compose representations of words and phrases to form representations of larger phrases and, finally, the complete sentence. In contrast to sequential models, these models\u2019 architectures are organized according to each sentence\u2019s syntactic structure, that is, the hierarchical organization of words into nested phrases that characterizes human intuitions about how words combine to form grammatical sentences. Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016; Dyer et al., 2016). The last approach for constructing sentence representations uses convolutional neural networks to produce the representation in a bottom up manner, either with syntactic information (Ma et al., 2015) or without (Kim, 2014; Kalchbrenner et al., 2014).\nOur work can be understood as a compromise between the first two approaches. Rather than using explicit supervision of tree structure, we use reinforcement learning to learn tree structures (and thus, sentence-specific compositional architectures), taking performance on a downstream task that uses the computed sentence representation as the reward signal. In contrast to sequential RNNs, which ignore tree structure, our model still generates a latent tree for each sentence and uses it to structure the composition. Our hypothesis is that encouraging the model to learn tree-structured compositions will bias the model toward better generalizations about how words compose to form sentence meanings, leading to better performance on downstream tasks.\nThis work is related to unsupervised grammar induction (Klein & Manning, 2004; Blunsom & Cohn, 2010; Spitkovsky et al., 2011, inter alia), which seeks to infer a generative grammar of an infinite language from a finite sample of strings from the language\u2014but without any semantic feedback.\nar X\niv :1\n61 1.\n09 10\n0v 1\n[ cs\n.C L\n] 2\n8 N\nov 2\n01 6\nPrevious work on unsupervised grammar induction that incorporates semantic supervision involves designing complex models for Combinatory Categorial Grammars (Zettlemoyer & Collins, 2005). Since semantic feedback has been proposed as crucial for the acquisition of syntax (Pinker, 1984), our model offers a simpler alternative.1 However, our primary focus is on improving performance on the downstream model, so the learner may settle on a different solution than conventional English syntax. We thus also explore what kind of syntactic structures are derivable from shallow semantics.\nExperiments on various tasks (i.e., sentiment analysis, semantic relatedness, natural language inference, and sentence generation) show that reinforcement learning is a promising direction to discover hierarchical structures of sentences. Notably, representations learned this way outperformed both conventional left-to-right models and tree-structured models based on linguistic syntax in downstream applications. This is in line with prior work showing the value of learning tree structures in statistical machine translation models (Chiang, 2007). Although the induced tree structures manifested a number of linguistically intuitive structures (e.g., noun phrases, simple verb phrases), there are a number of marked differences to conventional analyses of English sentences (e.g., an overall left-branching structure)."}, {"heading": "2 MODEL", "text": "Our model consists of two components: a sentence representation model and a reinforcement learning algorithm to learn the tree structure that is used by the sentence representation model."}, {"heading": "2.1 TREE LSTM", "text": "Our sentence representation model follows the Stack-augmented Parser-Interpreter Neural Network (SPINN; Bowman et al., 2016), SPINN is a shift-reduce parser that uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) as its composition function. Given an input sentence of N words x = {x1, x2, . . . , xN}, we represent each word by its embedding vector xi \u2208 RD. The parser maintains an index pointer p starting from the leftmost word (p = 1) and a stack. To parse the sentence, it performs a sequence of operations a = {a1, a2, . . . , a2N\u22121}, where at \u2208 {SHIFT, REDUCE}. A SHIFT operation pushes xp to the stack and moves the pointer to the next word (p++); while a REDUCE operation pops two elements from the stack, composes them to a single element, and pushes it back to the stack. SPINN uses Tree LSTM (Tai et al., 2015) as the REDUCE composition function, which we follow. In Tree LSTM, each element of the stack is represented by two vectors, a hidden state representation h and a memory representation c. Two elements of the stack (hi, ci) and (hj , cj) are composed as:\ni = \u03c3(WI [hi,hj ] + bI)\nfL = \u03c3(WFL [hi,hj ] + bFL)\nfR = \u03c3(WFR [hi,hj ] + bFR)\no = \u03c3(WO[hi,hj ] + bI)\ng = tanh(WG[hi,hj ] + bG) c = fL ci + fR cj + i g h = o c\n(1)\nwhere [hi,hj ] denotes concatenation of hi and hj , and \u03c3 is the sigmoid activation function.\nA unique sequence of {SHIFT, REDUCE} operations corresponds to a unique binary parse tree of the sentence. A SHIFT operation introduces a new leaf node in the parse tree, while a REDUCE operation combines two nodes by merging them into a constituent. See Figure 1 for an example. We note that for a sentence of lengthN , there are exactlyN SHIFT operations andN\u22121 REDUCE operations that are needed to produce a binary parse tree of the sentence. The final sentence representation produced by the Tree LSTM is the hidden state of the final element of the stack hN\u22121 (i.e., the topmost node of the tree).\nTracking LSTM SPINN optionally augments Tree LSTM with another LSTM that incorporates contextual information in sequential order called tracking LSTM, which has been shown to improve\n1Our model only produces an interpretation grammar that parses language instead of a generative grammar.\nperformance for textual entailment. It is a standard recurrent LSTM network that takes as input the hidden states of the top two elements of the stack and the embedding vector of the word indexed by the pointer at timestep t. Every time a REDUCE operation is performed, the output of the tracking LSTM e is included as an additional input in Eq. 1 (i.e., the input to the REDUCE composition function is [hi,hj , e] instead of [hi,hj ])."}, {"heading": "2.2 REINFORCEMENT LEARNING", "text": "In previous work (Tai et al., 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.e., a is observed and provided as an input). Formally, each training data is a triplet {x,a,y}. Tai et al. (2015) consider models where a is also given at test time, whereas Bowman et al. (2016) explore models where a can be either observed or not at test time. When it is only observed during training, a policy is trained to predict a at test time. Note that in this case the policy is trained to match explicit human annotations (i.e., Penn TreeBank annotations), so the model learns to optimize representations according to structures that follows human intuitions. They found that models that observe a at both training and test time are better than models that only observe a during training.\nOur main idea is to use reinforcement learning (policy gradient methods) to discover the best tree structures for the task that we are interested in. We do not place any kind of restrictions when learning these structures other than that they have to be valid binary parse trees, so it may result in tree structures that match human linguistic intuition, heavily right or left branching, or other solutions if they improve performance on the downstream task.\nWe parameterize each action a \u2208 {SHIFT, REDUCE} by a policy network \u03c0(a | s;WR), where s is a representation of the current state and WR is the parameter of the network. Specifically, we use a two-layer feedforward network that takes the hidden states of the top two elements of the stack hi and hj and the embedding vector of the word indexed by the pointer xp as its input:\ns = ReLU(W1R[hi,hj,xp] + b 1 R)\n\u03c0(a | s;WR) \u221d exp(w2>R s+ b2R) where [hi,hj ,xp] denotes concatenation of vectors inside the brackets.\nIf a is given as part of the training data, the policy network can be trained\u2014in a supervised training regime\u2014to predict actions that result in trees that match human intuitions. Our training data, on the other hand, is a tuple {x,y}. We use REINFORCE (Williams, 1992), which is an instance of a broader class of algorithms called policy gradient methods, to learn WR such that the sequence of actions a = {a1, . . . , aT } maximizes:\nR(W) = E\u03c0(a,s;WR) [ T\u2211 t=1 rtat ] ,\nwhere rt is the reward at timestep t. We use performance on a downstream task as the reward function. For example, if we are interested in using the learned sentence representations in a classification\ntask, our reward function is the probability of predicting the correct label using a sentence representation composed in the order given by the sequence of actions sampled from the policy network, so R(W) = log p(y | T-LSTM(x);W), where we use W to denote all model parameters (Tree LSTM, policy network, and classifier parameters), y is the correct label for input sentence x, and x is represented by the Tree LSTM structure in \u00a72.1. For a natural language generation task where the goal is to predict the next sentence given the current sentence, we can use the probability of predicting words in the next sentence as the reward function, so R(W) = log p(xs+1 | T-LSTM(xs);W). Note that in our setup we do not immediately receive a reward after performing an action at timestep t. The reward is only observed at the end after we finish creating a representation for the current sentence with Tree LSTM and use the resulting representation for the downstream task. At each timestep t, we sample a valid action according to \u03c0(a | s;WR). We add two simple constraints to make the sequence of actions result in a valid tree: REDUCE is forbidden if there are fewer than two elements on the stack, and SHIFT is forbidden if there are no more words to read from the sentence. After reaching timestep 2N \u2212 1, we construct the final representation and receive a reward that is used to update our model parameters.\nWe experiment with two learning methods: unsupervised structures and semi-supervised structures. Suppose that we are interested in a classification task. In the unsupervised case, the objective function that we maximize is log p(y | T-LSTM(x);W). In the semi-supervised case, the objective function for the first E epochs also includes a reward term for predicting the correct SHIFT or REDUCE actions obtained from an external parser\u2014in addition to performance on the downstream task, so we maximize log p(y | T-LSTM(x);W) + log \u03c0(a | s;WR). The motivation behind this model is to first guide the model to discover tree structures that match human intuitions, before letting it explore other structures close to these ones. After epochE, we remove the second term from our objective function and continue maximizing the first term. Note that unsupervised and semi-supervised here refer to the tree structures, not the nature of the downstream task."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 BASELINES", "text": "The goal of our experiments is to evaluate our hypothesis that we can discover useful task-specific tree structures (composition orders) with reinforcement learning. We compare the following composition methods (the last two are unique to our work):\n\u2022 Right to left: words are composed from right to left.2\n\u2022 Left to right: words are composed from left to right. This is the standard recurrent neural network composition order.\n\u2022 Bidirectional: A bidirectional right to left and left to right models, where the final sentence embedding is an average of sentence embeddings produced by each of these models.\n\u2022 Supervised syntax: words are composed according to a predefined parse tree of the sentence. When parse tree information is not included in the dataset, we use Stanford parser (Klein & Manning, 2003) to parse the corpus.\n\u2022 Semi-supervised syntax: a variant of our reinforcement learning method, where for the first E epochs we include rewards for predicting predefined parse trees given in the super-\n2We choose to include right to left as a baseline since a right-branching tree structure\u2014which is the output of a right to left composition order\u2014has been shown to be a reliable baseline for unsupervised grammar induction. (Klein & Manning, 2004)\nvised model, before letting the model explore other kind of tree structures at later epochs (i.e., semi-supervised structures in \u00a72.2). \u2022 Latent syntax: another variant of our reinforcement learning method where there is no\npredefined structures given to the model at all (i.e., unsupervised structures in \u00a72.2).\nFor learning, we use stochastic gradient descent with minibatches of size 1 and `2 regularization constant tune on development data from {10\u22124, 10\u22125, 10\u22126, 0}. We use performance on development data to choose the best model and decide when to stop training."}, {"heading": "3.2 TASKS", "text": "We evaluate our method on four sentence representation tasks: sentiment classification, semantic relatedness, natural language inference (entailment), and sentence generation. We show statistics of the datasets in Table 1 and describe each task in details in the followings.\nStanford Sentiment Treebank We evaluate our model on a sentiment classification task from the Stanford Sentiment Treebank (Socher et al., 2013). We use the binary classification task where the goal is to predict whether a sentence is a positive or a negative movie review.\nWe set the word embedding size to 100 and initialize them with Glove vectors (Pennington et al., 2014)3. For each sentence, we create a 100-dimensional sentence representation s \u2208 R100 with Tree LSTM, project it to a 200-dimensional vector and apply ReLU: q = ReLU(Wps + bp), and compute p(y\u0302 = c | q;wq) \u221d exp(wq,cq+ bq). We run each model 3 times (corresponding to three different initialization points) and use the development data to pick the best model. We show the results in Table 2. Our results agree with prior work that have shown the benefits of using syntactic parse tree information on this dataset (i.e., supervised recursive model is generally better than sequential models). The best model is the latent syntax model, which is also competitive with results from other work on this dataset. Both the latent and semi-supervised syntax models outperform models with predefined structures, demonstrating the benefit of learning task-specific composition orders.\nSemantic relatedness The second task is to predict the degree of relatedness of two sentences from the Sentences Involving Compositional Knowledge corpus (SICK; Marelli et al., 2014) . In this dataset, each pair of sentences are given a relatedness score on a 5-point rating scale. For each sentence, we use Tree LSTM to create its representations. Denote the final representations\n3http://nlp.stanford.edu/projects/glove/\nby {s1, s2} \u2208 R100. We construct our prediction by computing: u = (s2 \u2212 s1)2, v = s1 s2, q = ReLU(Wp[u,v] + bp), and y\u0302 = w>q q + bq , where Wp \u2208 R200\u00d7200,bp \u2208 R200,wq \u2208 R200, bq \u2208 R1 are model parameters, and [u,v] denotes concatenation of vectors inside the brackets. We learn the model to minimize mean squared error.\nWe run each model 5 times and use the development data to pick the best model. Our results are shown in Table 3. Similar to the previous task, they clearly demonstrate that learning the tree structures yield to better performance.\nWe also provide results from other work on this dataset for comparisons. Some of these models (Lai & Hockenmaier, 2014; Jimenez et al., 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task. Our Tree LSTM implementation performs competitively with most models in terms of mean squared error. Our best model\u2014semi-supervised syntax\u2014is better than most models except LSTM models of Tai et al. (2015) which were trained with a different objective function.4 Nonetheless, we observe the same trends with their results that show the benefit of using syntactic information on this dataset.\nStanford Natural Language Inference We next evaluate our model for natural language inference (i.e., recognizing textual entailment) using the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015) . Natural language inference aims to predict whether two sentences are entailment, contradiction, or neutral, which can be formulated as a three-way classiciation problem. Given a pair of sentences, similar to the previous task, we use Tree LSTM to create sentence representations {s1, s2} \u2208 R100 for each of the sentences. Following Bowman et al. (2016), we construct our prediction by computing: u = (s2\u2212s1)2, v = s1 s2, q = ReLU(Wp[u,v, s1, s2]+bp), and p(y\u0302 = c | q;wq) \u221d exp(wq,cq+ bq), where Wp \u2208 R200\u00d7400,bp \u2208 R200,wq \u2208 R200, bq \u2208 R1 are model parameters. The objective function that we maximize is the log likelihood of the correct label under the models.\nWe show the results in Table 4. The latent syntax method performs the best. Interestingly, the sequential left to right model is better than the supervised recursive model in our experiments, which contradicts results from Bowman et al. (2016) that show 300D-LSTM is worse than 300D-SPINN. A possible explanation is that our left to right model has identical number of parameters with the supervised model due to the inclusion of the tracking LSTM even in the left to right model (the only difference is in the composition order), whereas the models in Bowman et al. (2016) have different number of parameters. Due to the poor performance of the supervised model, semi-supervised training does not help on this dataset, although it does significantly close the gap. Our models underperform state-of-the-art models on this dataset that have almost four times the number of parameters. We only experiment with smaller models since tree-based models with dynamic structures (e.g., our semi-supervised and latent syntax models) take longer to train. See \u00a74 for details and discussions about training time.\n4Our experiments with the regularized KL-divergence objective function (Tai et al., 2015) do not result in significant improvements, so we choose to report results with the simpler mean squared error objective function.\nSentence generation The last task that we consider is natural language generation. Given a sentence, the goal is to maximize the probability of generating words in the following sentence. This is a similar setup to the Skip Thought objective (Kiros et al., 2015), except that we do not generate the previous sentence as well. Given a sentence, we encode it with Tree LSTM to obtain s \u2208 R100. We use a bag-of-words model as our decoder, so p(wi | s;V) \u221d exp(v>i s), where V \u2208 R100\u00d729,209 and vi \u2208 R100 is the i-th column of V. Using a bag-of-words decoder as opposed to a recurrent neural network decoder increases the importance of producing a better representation of the current sentence, since the model cannot rely on a sophisticated decoder with a language model component to predict better. This also greatly speeds up our training time.\nWe use IMDB movie review corpus (Diao et al., 2014) for this experiment, The corpus consists of 280,593, 33,793, and 34,029 reviews in training, development, and test sets respectively. We construct our data using the development and test sets of this corpus. For training, we process 33,793 reviews from the original development set to get 441,617 pairs of sentences. For testing, we use 34,029 reviews in the test set (446,471 pairs of sentences). Half of these pairs is used as our development set to tune hyperparamaters, and the remaining half is used as our final test set. Our results in Table 5 further demonstrate that methods that learn tree structures perform better than methods that have fixed structures."}, {"heading": "4 DISCUSSION", "text": "Learned Structures Our results in \u00a73 show that our proposed method outperforms competing methods with predefined composition order on all tasks. The right to left model tends to perform worse than the left to right model. This suggests that the left to right composition order, similar to how human reads in practice, is better for neural network models. Our latent syntax method is able to discover tree structures that work reasonably well on all tasks, regardless of whether the task is better suited for a left to right or supervised syntax composition order.\nWe inspect what kind of structures the latent syntax model learned and how closely they match human intuitions. We first compute unlabeled bracketing F1 scores5 for the learned structures and\n5We use evalb toolkit from http://nlp.cs.nyu.edu/evalb/.\nparses given by Stanford parser on SNLI and Stanford Sentiment Treebank. In the SNLI dataset, there are 10,000 pairs of test sentences (20,000 sentences in total), while the Stanford Sentiment Treebank test set contains 1,821 test sentences. The F1 scores for the two datasets are 41.73 and 40.51 respectively. For comparisons, F1 scores of a right (left) branching tree are 19.94 (41.37) for SNLI and 12.96 (38.56) for SST.\nWe also manually inspect the learned structures. We observe that in SNLI, the trees exhibit overall left-branching structure, which explains why the F1 scores are closer to a left branching tree structure. Note that in our experiments on this corpus, the supervised syntax model does not perform as well as the left-to-right model, which suggests why the latent syntax model tends to converge towards the left-to-right model. We handpicked two examples of trees learned by our model and show them in Figure 2. We can see that in some cases the model is able to discover concepts such as noun phrases (e.g., a boy, his sleds) and simple verb phrases (e.g., wearing sunglasses, is frowning). Of course, the model sometimes settles on structures that make little sense to humans. We show two such examples in Figure 3, where the model chooses to compose playing frisbee in and outside a as phrases.\nTraining Time A major limitation of our proposed model is that it takes much longer to train compared to models with predefined structures. We observe that our models only outperforms models with fixed structures after several training epochs; and on some datasets such as SNLI or IMDB, an epoch could take a 5-7 hours (we use batch size 1 since the computation graph needs to be reconstructed for every example at every iteration depending on the samples from the policy network). This is also the main reason that we could only use smaller 100-dimensional Tree LSTM models in all our experiments. While for smaller datasets such as SICK the overall training time is approximately 6 hours, for SNLI or IMDB it takes 3-4 days for the model to reach convergence. In general, the latent syntax model and semi-supervised syntax models take about two or three times longer to converge compared to models with predefined structures."}, {"heading": "5 CONCLUSION", "text": "We presented a reinforcement learning method to learn hierarchical structures of natural language sentences. We demonstrated the benefit of learning task-specific composition order on four tasks: sentiment analysis, semantic relatedness, natural language inference, and sentence generation. We qualitatively and quantitatively analyzed the induced trees and showed that they both incorporate some linguistically intuitive structures (e.g., noun phrases, simple verb phrases) and are different than conventional English syntactic structures."}], "references": [{"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johannes Bjerva", "Johan Bos", "Rob van der Goot", "Malvina Nissim"], "venue": "In Proc. of SemEval,", "citeRegEx": "Bjerva et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Blunsom and Cohn.,? \\Q2010\\E", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "venue": "In Proc. of ACL,", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2007\\E", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A compositional distributional model of meaning", "author": ["Stephen Clark", "Bob Coecke", "Mehrnoosh Sadrzadeh"], "venue": "In Proc. of the Second Symposium on Quantum Interaction,", "citeRegEx": "Clark et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)", "author": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J. Smola", "Jing Jiang", "Chong Wang"], "venue": "In Proc. of KDD,", "citeRegEx": "Diao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diao et al\\.", "year": 2014}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Grefenstette and Sadrzadeh.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios Batiz", "Av Mendizabal"], "venue": "In Proc. of SemEval,", "citeRegEx": "Jimenez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Prof. of ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proc. of NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Corpus-based induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Klein and Manning.,? \\Q2004\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier"], "venue": "In Proc. of SemEval,", "citeRegEx": "Lai and Hockenmaier.,? \\Q2014\\E", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "venue": "In Proc. ACL,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": "In Proc. of SemEval,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In Proc. of ACL,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Language Learnability and Language Development", "author": ["Steven Pinker"], "venue": "Harvard,", "citeRegEx": "Pinker.,? \\Q1984\\E", "shortCiteRegEx": "Pinker.", "year": 1984}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher Manning", "Andrew Ng", "Christopher Potts"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Unsupervised dependency parsing without gold part-of-speech tags", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Angel X. Chang", "Daniel Jurafsky"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Spitkovsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "In Proc. of ICLR,", "citeRegEx": "Vendrov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins"], "venue": "In Proc. of UAI,", "citeRegEx": "Zettlemoyer and Collins.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 5, "context": "The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015).", "startOffset": 148, "endOffset": 210}, {"referenceID": 29, "context": "The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015).", "startOffset": 148, "endOffset": 210}, {"referenceID": 14, "context": "The first composes words sequentially using a recurrent neural network, treating the RNN\u2019s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015).", "startOffset": 148, "endOffset": 210}, {"referenceID": 6, "context": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al.", "startOffset": 114, "endOffset": 211}, {"referenceID": 25, "context": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al.", "startOffset": 114, "endOffset": 211}, {"referenceID": 30, "context": "Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2011; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al.", "startOffset": 114, "endOffset": 211}, {"referenceID": 3, "context": ", 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016; Dyer et al., 2016).", "startOffset": 107, "endOffset": 147}, {"referenceID": 8, "context": ", 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016; Dyer et al., 2016).", "startOffset": 107, "endOffset": 147}, {"referenceID": 18, "context": "The last approach for constructing sentence representations uses convolutional neural networks to produce the representation in a bottom up manner, either with syntactic information (Ma et al., 2015) or without (Kim, 2014; Kalchbrenner et al.", "startOffset": 182, "endOffset": 199}, {"referenceID": 13, "context": ", 2015) or without (Kim, 2014; Kalchbrenner et al., 2014).", "startOffset": 19, "endOffset": 57}, {"referenceID": 12, "context": ", 2015) or without (Kim, 2014; Kalchbrenner et al., 2014).", "startOffset": 19, "endOffset": 57}, {"referenceID": 24, "context": "Since semantic feedback has been proposed as crucial for the acquisition of syntax (Pinker, 1984), our model offers a simpler alternative.", "startOffset": 83, "endOffset": 97}, {"referenceID": 4, "context": "This is in line with prior work showing the value of learning tree structures in statistical machine translation models (Chiang, 2007).", "startOffset": 120, "endOffset": 134}, {"referenceID": 3, "context": "Our sentence representation model follows the Stack-augmented Parser-Interpreter Neural Network (SPINN; Bowman et al., 2016), SPINN is a shift-reduce parser that uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) as its composition function.", "startOffset": 96, "endOffset": 124}, {"referenceID": 10, "context": ", 2016), SPINN is a shift-reduce parser that uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) as its composition function.", "startOffset": 73, "endOffset": 113}, {"referenceID": 30, "context": "SPINN uses Tree LSTM (Tai et al., 2015) as the REDUCE composition function, which we follow.", "startOffset": 21, "endOffset": 39}, {"referenceID": 30, "context": "In previous work (Tai et al., 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.", "startOffset": 17, "endOffset": 56}, {"referenceID": 3, "context": "In previous work (Tai et al., 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.", "startOffset": 17, "endOffset": 56}, {"referenceID": 32, "context": "We use REINFORCE (Williams, 1992), which is an instance of a broader class of algorithms called policy gradient methods, to learn WR such that the sequence of actions a = {a1, .", "startOffset": 17, "endOffset": 33}, {"referenceID": 2, "context": ", 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.e., a is observed and provided as an input). Formally, each training data is a triplet {x,a,y}. Tai et al. (2015) consider models where a is also given at test time, whereas Bowman et al.", "startOffset": 8, "endOffset": 246}, {"referenceID": 2, "context": ", 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.e., a is observed and provided as an input). Formally, each training data is a triplet {x,a,y}. Tai et al. (2015) consider models where a is also given at test time, whereas Bowman et al. (2016) explore models where a can be either observed or not at test time.", "startOffset": 8, "endOffset": 327}, {"referenceID": 26, "context": "Stanford Sentiment Treebank We evaluate our model on a sentiment classification task from the Stanford Sentiment Treebank (Socher et al., 2013).", "startOffset": 122, "endOffset": 143}, {"referenceID": 23, "context": "We set the word embedding size to 100 and initialize them with Glove vectors (Pennington et al., 2014)3.", "startOffset": 77, "endOffset": 102}, {"referenceID": 26, "context": "2m RNTN (Socher et al., 2013) 85.", "startOffset": 8, "endOffset": 29}, {"referenceID": 12, "context": "4 DCNN (Kalchbrenner et al., 2014) 86.", "startOffset": 7, "endOffset": 34}, {"referenceID": 13, "context": "8 CNN-random(Kim, 2014) 82.", "startOffset": 12, "endOffset": 23}, {"referenceID": 13, "context": "7 CNN-word2vec (Kim, 2014) 87.", "startOffset": 15, "endOffset": 26}, {"referenceID": 13, "context": "2 CNN-multichannel (Kim, 2014) 88.", "startOffset": 19, "endOffset": 30}, {"referenceID": 30, "context": "8m Left to Right LSTM (Tai et al., 2015) 84.", "startOffset": 22, "endOffset": 40}, {"referenceID": 30, "context": "8m Bidirectional LSTM (Tai et al., 2015) 87.", "startOffset": 22, "endOffset": 40}, {"referenceID": 30, "context": "8m Constituency Tree\u2013LSTM\u2013random (Tai et al., 2015) 82.", "startOffset": 33, "endOffset": 51}, {"referenceID": 30, "context": "8m Constituency Tree\u2013LSTM\u2013GloVe (Tai et al., 2015) 88.", "startOffset": 32, "endOffset": 50}, {"referenceID": 30, "context": "8m Dependency Tree-LSTM (Tai et al., 2015) 85.", "startOffset": 24, "endOffset": 42}, {"referenceID": 19, "context": "Semantic relatedness The second task is to predict the degree of relatedness of two sentences from the Sentences Involving Compositional Knowledge corpus (SICK; Marelli et al., 2014) .", "startOffset": 154, "endOffset": 182}, {"referenceID": 11, "context": "Some of these models (Lai & Hockenmaier, 2014; Jimenez et al., 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task.", "startOffset": 21, "endOffset": 89}, {"referenceID": 0, "context": "Some of these models (Lai & Hockenmaier, 2014; Jimenez et al., 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task.", "startOffset": 21, "endOffset": 89}, {"referenceID": 0, "context": ", 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task. Our Tree LSTM implementation performs competitively with most models in terms of mean squared error. Our best model\u2014semi-supervised syntax\u2014is better than most models except LSTM models of Tai et al. (2015) which were trained with a different objective function.", "startOffset": 8, "endOffset": 308}, {"referenceID": 11, "context": "369 UNAL-NLP(Jimenez et al., 2014) 0.", "startOffset": 12, "endOffset": 34}, {"referenceID": 0, "context": "356 Meaning Factory (Bjerva et al., 2014) 0.", "startOffset": 20, "endOffset": 41}, {"referenceID": 27, "context": "322 DT-RNN (Socher et al., 2014) 0.", "startOffset": 11, "endOffset": 32}, {"referenceID": 30, "context": "382 Mean Vectors (Tai et al., 2015) 0.", "startOffset": 17, "endOffset": 35}, {"referenceID": 30, "context": "456 650k Left to Right LSTM (Tai et al., 2015) 0.", "startOffset": 28, "endOffset": 46}, {"referenceID": 30, "context": "0m Bidirectional LSTM (Tai et al., 2015) 0.", "startOffset": 22, "endOffset": 40}, {"referenceID": 30, "context": "0m Constituency Tree-LSTM (Tai et al., 2015) 0.", "startOffset": 26, "endOffset": 44}, {"referenceID": 30, "context": "0m Dependency Tree-LSTM (Tai et al., 2015) 0.", "startOffset": 24, "endOffset": 42}, {"referenceID": 2, "context": ", recognizing textual entailment) using the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015) .", "startOffset": 87, "endOffset": 114}, {"referenceID": 2, "context": ", recognizing textual entailment) using the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015) . Natural language inference aims to predict whether two sentences are entailment, contradiction, or neutral, which can be formulated as a three-way classiciation problem. Given a pair of sentences, similar to the previous task, we use Tree LSTM to create sentence representations {s1, s2} \u2208 R for each of the sentences. Following Bowman et al. (2016), we construct our prediction by computing: u = (s2\u2212s1), v = s1 s2, q = ReLU(Wp[u,v, s1, s2]+bp), and p(\u0177 = c | q;wq) \u221d exp(wq,cq+ bq), where Wp \u2208 R,bp \u2208 R,wq \u2208 R, bq \u2208 R are model parameters.", "startOffset": 94, "endOffset": 467}, {"referenceID": 30, "context": "Our experiments with the regularized KL-divergence objective function (Tai et al., 2015) do not result in significant improvements, so we choose to report results with the simpler mean squared error objective function.", "startOffset": 70, "endOffset": 88}, {"referenceID": 2, "context": "Interestingly, the sequential left to right model is better than the supervised recursive model in our experiments, which contradicts results from Bowman et al. (2016) that show 300D-LSTM is worse than 300D-SPINN.", "startOffset": 147, "endOffset": 168}, {"referenceID": 2, "context": "Interestingly, the sequential left to right model is better than the supervised recursive model in our experiments, which contradicts results from Bowman et al. (2016) that show 300D-LSTM is worse than 300D-SPINN. A possible explanation is that our left to right model has identical number of parameters with the supervised model due to the inclusion of the tracking LSTM even in the left to right model (the only difference is in the composition order), whereas the models in Bowman et al. (2016) have different number of parameters.", "startOffset": 147, "endOffset": 498}, {"referenceID": 2, "context": "3m 100D-LSTM (Bowman et al., 2015) 77.", "startOffset": 13, "endOffset": 34}, {"referenceID": 3, "context": "7m 300D-LSTM (Bowman et al., 2016) 80.", "startOffset": 13, "endOffset": 34}, {"referenceID": 3, "context": "5m 300D-SPINN (Bowman et al., 2016) 83.", "startOffset": 14, "endOffset": 35}, {"referenceID": 31, "context": "2m 1024D-GRU (Vendrov et al., 2016) 81.", "startOffset": 13, "endOffset": 35}, {"referenceID": 20, "context": "0m 300D-CNN (Mou et al., 2016) 82.", "startOffset": 12, "endOffset": 30}, {"referenceID": 14, "context": "This is a similar setup to the Skip Thought objective (Kiros et al., 2015), except that we do not generate the previous sentence as well.", "startOffset": 54, "endOffset": 74}, {"referenceID": 7, "context": "We use IMDB movie review corpus (Diao et al., 2014) for this experiment, The corpus consists of 280,593, 33,793, and 34,029 reviews in training, development, and test sets respectively.", "startOffset": 32, "endOffset": 51}], "year": 2016, "abstractText": "We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.", "creator": "LaTeX with hyperref package"}}}