{"id": "1611.09235", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Joint Copying and Restricted Generation for Paraphrase", "abstract": "many natural language generation skills, such such abstractive summarization without text simplification, are paraphrase - resistant. in these tasks, copying and rewriting are two main writing modes. older previous sequence - to - sequence ( seq2seq ) models use a single decoder and announce this fact. in this paper, we develop a novel seq2seq model to fuse a copying decoder and a restricted generative database. hence copying decoder finds the position will be deleted based on a typical attention model. the generative decoder produces words limited in the source - specific vocabulary. to combine the two decoders and determine the final output, tools develop a predictor approximately modify the mode of copying automatic rewriting. additional predictor can be guided by the actual writing mode in the training data. we conduct extensive experiments among two different paraphrase datasets. the result shows that our intuition outperforms the state - of - the - art approaches in programs improving collaborative informativeness and language knowledge.", "histories": [["v1", "Mon, 28 Nov 2016 16:49:37 GMT  (79kb,D)", "http://arxiv.org/abs/1611.09235v1", "7 pages, 1 figure, AAAI-17"]], "COMMENTS": "7 pages, 1 figure, AAAI-17", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ziqiang cao", "chuwei luo", "wenjie li", "sujian li"], "accepted": true, "id": "1611.09235"}, "pdf": {"name": "1611.09235.pdf", "metadata": {"source": "CRF", "title": "Joint Copying and Restricted Generation for Paraphrase", "authors": ["Ziqiang Cao", "Chuwei Luo", "Wenjie Li", "Sujian Li"], "emails": ["cswjli}@comp.polyu.edu.hk", "luochuwei@whu.edu.cn", "lisujian@pku.edu.cn"], "sections": [{"heading": "Introduction", "text": "Paraphrase is a restatement of the meaning of a text using other words. Many natural language generation tasks are paraphrase-orientated. For example, abstractive summarization is to use a condensed description to summarize the main idea of a document, while text simplification is to simplify the grammar and vocabulary of a document. In paraphrase, copying and rewriting are two main writing modes. Recently, the encoder-decoder structure (aka. SeqsSeq model) has become more and more popular in many language generation tasks (Bahdanau, Cho, and Bengio 2014; Shang, Lu, and Li 2015). In such a structure, the source text is encoded by the encoder as a context vector. Then, a decoder decodes the semantic information in the vector and outputs the target text. Studies such as (Rush, Chopra, and Weston 2015; Hu, Chen, and Zhu 2015) have applied the popular SeqsSeq model initially used in machine translation (Bahdanau, Cho, and Bengio 2014) to the paraphrase task. Despite the competitive performance, these models seldom take into account the two major writing modes of paraphrase.\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nOn the one hand, due to the nature of the task, keywords of the source text are usually reserved in the target text. However, with only one decoder generating over the entire vocabulary, a typical Seq2Seq model fails to reflect the copying mode. As a result, many keywords provided in the source text may be overlooked in the target text. In addition, certain keywords like named entities are often rare words and masked as unknown (UNK) tags in Seq2Seq models, which unavoidably causes the decoder to generate a number of UNK tags. Although not aiming to explicitly explore the copying mechanism, the work of (Rush, Chopra, and Weston 2015) finds that it largely improves the performance to add the input-related hand-crafted features to guide the generation.\nOn the other hand, rewriting also plays a significant role in paraphrase. In this writing mode, although the target words are not the same as the source words, there are semantic associations between them. For example, \u201cseabird\u201d is possibly generalized as \u201cwildlife\u201d in summarization, and \u201cthe visually impaired\u201d can be converted into \u201cpeople who can not see\u201d in text simplification. The decoders in most previous work generate words by simply picking the likely target words that fit the contexts out of a large vocabulary. This common practice suffers from two problems. First, the computation complexity is linear to the vocabulary size. In order to cover enough target words, the vocabulary size usually reaches 104 or even 105. Consequently, the decoding process becomes quite time-consuming. Moreover, the decoder sometimes produces the named entities or numbers which are common but do not exist in or even irrelevant to the input text, and in turn the meaningless paraphrases.\nIn this paper, we develop a novel Seq2Seq model called CoRe, which captures the two core writing modes in paraphrase, i.e., Copying and Rewriting. CoRe fuses a copying decoder and a restricted generative decoder. Inspired by (Vinyals, Fortunato, and Jaitly 2015), the copying decoder finds the position to be copied based on the existing attention mechanism. Therefore, the weights learned by the attention mechanism have the explicit meanings in the copying mode. Meanwhile, the generative decoder produces the words restricted in the source-specific vocabulary. This vocabulary is composed of a source-target word alignment table and a small set of frequent words. The alignment table is trained in advance, and many frequent rewriting patterns are included\nar X\niv :1\n61 1.\n09 23\n5v 1\n[ cs\n.C L\n] 2\n8 N\nov 2\n01 6\nin it. It seems better to update the alignment table according to the learned attention weights. However, with the supplement of a few frequent words, experiments (see Table:2) show that more than 95% of the target words have already been covered by our decoders. While the output dimension of our generative decoder is just one tenth of the output dimension used by the common Seq2Seq models, it is able to generate highly relevant words concerning rewriting. To combine the two decoders and determine the final output, we develop a predictor to predict the writing mode of copying or rewriting. Since we know the actual mode at each output position in a training instance, we introduce a binary sequence labeling task to guide the learning of this predictor, which takes advantages of the supervision derived from the writing modes.\nTo the best of our knowledge, the work most relevant to ours is the COPYNET (Gu et al. 2016) which also explores the copying mechanism. However, COPYNET adds an additional attention-like layer to predict the copying weight distribution. This layer then competes with the output of the generative decoder. Therefore, it is not easy for COPYNET to explain the contributions of copying and generation. Compared with our model, COPYNET introduces a lot of extra parameters and ignores the supervision derived from the writing modes. Moreover, the generative decoder of COPYNET is only allowed to produce frequent words. As a result, the rewriting patterns are discarded to a large extent.\nWe conduct extensive experiments on two different paraphrase tasks, i.e., abstractive summarization and text simplification. The result shows that both informativeness and sentence quality of our model outperform the state-of-theart Seq2Seq models as well as the statistical machine translation approaches.\nThe contributions of our work are as follows:\n\u2022 We develop two different decoders to simulate major human writing behaviors in paraphrase.\n\u2022 We introduce a binary sequence labeling task to predict the current writing mode, which utilizes additional supervision.\n\u2022 We add restrictions to the generative decoder in order to produce highly relevant content efficiently."}, {"heading": "Background: Seq2Seq Models and Attention Mechanism", "text": "Seq2Seq models have been successfully applied to a series of natural language generation tasks, such as machine translation (Bahdanau, Cho, and Bengio 2014), response generation (Shang, Lu, and Li 2015) and abstractive summarization (Rush, Chopra, and Weston 2015). With these models, the source sequence X = [x1, \u00b7 \u00b7 \u00b7 , xn] is converted into a fixed length context vector c, usually by a Recurrent Neural Network (RNN) encoder, i.e.,\nh\u03c4 = f(x\u03c4 ,h\u03c4\u22121) (1) c = \u03c6(h1, \u00b7 \u00b7 \u00b7hn) (2)\nwhere {h\u03c4} are the RNN states, f is the dynamics function, and \u03c6 summarizes the hidden states, e.g., choosing the last state hn.\nThe decoder unfolds the context vector c into the target RNN state st through the similar dynamics in the encoder:\nst = f(yt\u22121, st\u22121, c) (3)\nThen, the predictor is followed to generate the final sequence, usually using a softmax classifier:\np(yt|y<t,X) = exp(wt\u03c8(yt\u22121, st, ct))\u2211\nyt\u2032\u2208V exp(wt\u2032\u03c8(yt\u22121, st, ct))\n(4)\nwhere yt is the predicted target word at the state t, wt is the corresponding weight vector, and \u03c8 is an affine transformation. V is the target vocabulary, and it is usually as large as 104 or even 105.\nTo release the burden of summarizing the entire source into a single context vector, the attention mechanism (Bahdanau, Cho, and Bengio 2014) uses a dynamically changing context ct to replace c in Eq. 3. A common practice is to represent ct as the weighted sum of the source hidden states:\n\u03b1t\u03c4 = e\u03b7(st\u22121,h\u03c4 )\u2211n\n\u03c4 \u2032=1 e \u03b7(st\u22121,h\u03c4\u2032 )\n,\u2200\u03c4 \u2208 [1, n] (5)\nct = \u2211n\n\u03c4=1 \u03b1t\u03c4h\u03c4 (6)\nwhere \u03b1t\u03c4 reflects the alignments between source and target words, and \u03b7 is the function that shows the correspondence strength for attention, usually approximated with a deep neural network."}, {"heading": "Method", "text": "As illustrated in Figure 1, CoRe is based on the encoderdecoder structure. The source sequence is transformed by a RNN Encoder into the context representation, which is then read by another RNN Decoder to generate the target sequence."}, {"heading": "Encoder", "text": "We follow the work of (Bahdanau, Cho, and Bengio 2014) to build the encoder. Specifically, we use the Gated Recurrent Unit (GRU) as the recurrent unit, which often performs much better than the vanilla RNN. The bi-directional RNN is introduced to make the hidden state h\u03c4 aware of the contextual information from both ends. Then, we use the attention mechanism to build the context vector as Eq. 2. However, unlike most previous work, we re-use the learned alignments (Eq. 5) in the decoders."}, {"heading": "Decoder", "text": "Instead of using the canonical RNN-decoder like (Bahdanau, Cho, and Bengio 2014), we develop two distinct decoders to simulate the copying and rewriting behaviors, respectively.\nThe Copying Decoder (C) picks the words from the source text. In paraphrase, most keywords from the original document will be reserved in the output. This decoder captures this fact. Since the attention mechanism is supposed to\nprovide the focus of source text during generation, for the copying behavior, the weights learned by Eq. 5 can be interpreted as the copying probability distribution. Therefore, the output of the copying decoder is as follows:\npC(yt|y<t,X) = { \u03b1t\u03c4 , if yt = x\u03c4 0, otherwise (7)\nMost previous work uses the attention mechanism as a module to build the context vector. In contrast, our copying decoder provides the explicit meanings (i.e., the copying probability distribution) to the learned alignment. Notice that, this decoder only generates words in the source, i.e., the vocabulary for this decoder is VC = X. We observe that quite a number of low-frequency words in the actual target text are extracted from the source text. Therefore, the copying decoder largely reduces the chance to produce the unknown (UNK) tags.\nThe Restricted Generative Decoder (G), on the other hand, restricts the output in a small yet highly relevant vocabulary according to the source text. Specifically, we train a rough alignment table A based on the IBM Model (Dyer, Chahuneau, and Smith 2013) beforehand. This table is able to capture many representative rewriting patterns, such as \u201csustain\u2192 injury\u201d, and \u201cseabird\u2192 wildlife\u201d. Our pilot experimental results show that the alignment table covers most target words which are not extracted from the source. To further increase the coverage, we supplement an additional frequent word table U1. Putting together, the final vocabulary for this decoder is limited to:\nVG = A(X) \u222aU (8) In our experiments, we retain 10 most reliable alignments for a source word, and set |U| = 2000. As a result, VG is only one tenth of the vocabulary V used by the common canonical RNN-decoder in size. The output of this decoder is formulated by Eq. 9 which is similar to Eq. 4, except for the reduced vocabulary:\npG(yt|y<t,X) = exp(wTt \u03c8(yt\u22121, st, ct))\u2211\nyt\u2032\u2208VG exp(wTt\u2032\u03c8(yt\u22121, st, ct))\n(9)\n1We add the UNK tag in this table\nCompared with the generation on the large vocabulary V, the restricted decoder not only runs much faster but also produces more relevant words.\nTo combine the two decoders, we introduce a binary sequence labeling task to decide whether the current target word should come from copying or rewriting. Specifically, for each hidden state st, we compute a predictor \u03bbt to represent the probability of copying at the current generation position:\n\u03bbt = \u03c3(wCst) (10)\nwhere \u03c3 is the sigmoid function and wC is the weight parameters. \u03bbt measures the contributions of the two decoders, and the final combined prediction probability is:\np(yt|y<t,X) = \u03bbtpC + (1\u2212 \u03bbt)pG (11)\nIt is noted that \u03bbt has the following actual supervision in the training set:\n\u03bb\u2217t = { 1, if target word at t exists in the source 0, otherwise (12)\nTherefore, we can utilize this supervision to guide the writing mode prediction.\nThe common canonical RNN-decoder outputs the probability distribution over the reserved target word vocabulary V. Since the computation complex of a Seq2Seq model is linear to the output dimension of the decoder, a large amount of infrequent target words have to be discarded to ensure a reasonable vocabulary size. As a result, a target sentence may contain many UNK tags, and thus unreadable. By contrast, the output dimension of our generative decoder is totally independent on the reserved target word vocabulary. Therefore, we opt to reserve all the target words in the training set. Experiments demonstrate that our model runs efficiently and rarely generates the UNK tags."}, {"heading": "Learning", "text": "The cost function in our model is the sum of two parts, i.e.,\n= 1 + 2 (13)\nThe first one 1 is the difference between the output {yt} and the actual target sequence {y\u2217t }. As the common practice, we use Cross Entropy (CE) to measure the difference of probability distributions:\n\u03b51 = \u2212 \u2211\nt ln(p(y\u2217t |y<t,X)) (14)\nIn most existing Seq2Seq models, 1 is the final cost function. However, in our model, we include another cost function 2 derived from the prediction of writing modes. As shown in Eq. 10, a binary sequence labeling process in our model predicts whether or not the current target word is copied. 2 measures the performance in this task,\n\u03b52 = \u2212( \u2211\nt (\u03bb\u2217t ln(\u03bbt) + (1\u2212 \u03bb\u2217t ) ln(1\u2212 \u03bbt))) (15)\n2 utilizes the additional supervision of the training data. The experiments show that this cost function accurately balances the proportion of the words derived from copying and generation.\nGiven the cost function , we use the RmsProp (Tieleman and Hinton 2012) optimizer with mini-batches to tune the model weights. RmsProp is a popular method to train recurrent neural networks.\nExperiments"}, {"heading": "Dataset", "text": "We test our model on the following two paraphraseorientated tasks, 1. One-sentence abstractive summarization 2. Text simplification One-sentence summarization is to use a condensed sentence (aka. highlight) to describe the main idea of a document. This task facilitates efficient reading. Text simplification modifies a document in such a way that the grammar and vocabulary is greatly simplified, while the underlying meaning remains the same. It is able to make the scientific documents easily understandable for outsiders. We build datasets for both tasks based on the existing work.\nOne-sentence Abstractive Summarization: For this task, we need a corpus that consists of <document,highlight (one-sentence summary)> pairs. We modify an existing corpus that has been used for the task of passage-based question answering (Hermann et al. 2015). In this work, a collection of news documents and the corresponding highlights are downloaded from CNN and Daily Mail websites. For each highlight, we reserve the original sentences that have at least one word overlap with the source text. Therefore, if a document holds multiple highlights, the source text for each highlight can be different.\nText Simplification: Simple English Wikipedia2 articles represent a simplified version of traditional English Wikipedia articles. (Kauchak 2013) built a<Wikipedia text, Simple English Wikipedia text> corpus according to the aligned articles. We eliminate the non-English words in the corpus, and remove the pairs where the source and the target are exactly the same.\n2http://simple.wikipedia.org\nThe basic information of the two datasets are presented in Table 1. As can be seen, each dataset has a large vocabulary size. In the summarization dataset, the target length is much shorter than the source length, while in the simplification dataset, their lengths are similar.\nIn addition, we compute the target word coverage ratio based on different vocabulary sets, as shown in Table 2. It appears that both datasets hold a high copying ratio. When we restrict the generative decoder to produce the source alignments, more than 85% target words can be covered. When combined with 2000 frequent words, the coverage ratio of our model is already close to that using the vocabulary of 30000 words."}, {"heading": "Implementation", "text": "Turned on the validation dataset, we set the dimension of word embeddings to 256, and the dimension of hidden states to 512. The initial learning rate is 0.05 and the batch size is 32. Our implementation is based on the standard Seq2Seq model dl4mt3 under the Theano framework4. We leverage the popular tool Fast Align (Dyer, Chahuneau, and Smith 2013) to construct the source-target word alignment table A. The vocabulary of our generative decoder is restricted in the top 10 alignments of the source words plus 2000 frequent words. Although our model is more complex than the standard attentive Seq2Seq model, it only spends two thirds of the time in both training and test."}, {"heading": "Evaluation Method", "text": "Informativeness is evaluated using ROUGE5 (Lin 2004), which has been regarded as a standard automatic summarization evaluation metric. ROUGE counts the overlapping units such as the n-grams, word sequences and word pairs between the candidate text Y and the actual target text T. As the common practice, we take ROUGE-1 and ROUGE-2\n3https://github.com/nyu-dl/dl4mt-multi 4http://deeplearning.net/software/theano/ 5ROUGE-1.5.5 with options: -n 2 -m -u -c 95 -x -r 1000 -f A -p\n0.5 -t 0.\nscores as the main metrics. They measure the uni-gram and bi-gram similarities, respectively. For example, the f-score of ROUGE-2 is computed as follows:\nROUGE\u2212 2f - score = 2\u00d7 \u2211 b\u2208Y min{NY(b), NT(b)}\u2211\nb\u2208YNY(b) + \u2211 b\u2208TNT(b)\n(16)\nwhere b stands for a bi-gram. NY(b), NT(b) are the numbers of the bi-gram b in the candidate text and target text, respectively. Since we do not try to control the length of the generated sentences, we use the f-score rather than the recall for comparison.\nWe conduct text quality evaluation from several points of view. We use SRILM6 to train a 3-gram language model on the entire target datasets, and compute the perplexity (PPL) of the generated text. The lower PPL usually means higher readability. We also perform the statistical analysis on the average length of the target text, UNK ratio and copy ratio. We assume that the good sentences ought to have the similar length and copying ratio to the answers (refer to Table 1 and 2), and their UNK ratio should be low."}, {"heading": "Baselines", "text": "We compare the proposed model CoRe with various typical methods. At first, we introduce the standard baseline called \u201cLEAD\u201d. It simply selects the \u201cleading\u201d words from the source as the output. According to the averaged target length in Table 1, we choose the first 20 words for summarization and 25 for simplification. We also introduce the state-of-theart statistical machine translation system Moses (Koehn et al. 2007) and the Seq2Seq model ABS (Rush, Chopra, and Weston 2015). Moses is the dominant statistical approach to machine translation. It takes in the parallel data and uses co-occurrence of words and phrases to infer translation correspondences. For fair comparison, when implementing Moses, we also employ the alignment tool Fast Align and the language model tool SRILM. ABS is a Seq2Seq model with the attention mechanism. It is similar to the neural machine translation model proposed in (Bahdanau, Cho, and Bengio 2014). ABS has achieved promising performance on another one-sentence summarization benchmark.\nNote that, we would like to but fail to take COPYNET (Gu et al. 2016) into comparison. Its source code is not publicly available."}, {"heading": "Performance", "text": "The results of different approaches are presented in Table 3. In this table, the metrics that measure informativeness and text quality are separated. Let\u2019s look at the informativeness performance first. As can be seen, CoRe achieves the highest ROUGE scores on both summarization and text simplification. In contrast, the standard attentive Seq2Seq model ABS is slightly inferior to Moses. It even performs worse than the simple baseline LEAD in terms of ROUGE-2 in summarization. Apparently, introducing the copying and restricted generation mechanisms is critical for the paraphraseoriented tasks.\n6http://www.speech.sri.com/projects/srilm/\nThen, we check the quality of the generated sentences. According to PPL, the sentences produced by CoRe resemble the target language the most. It is interesting that LEAD extracts human-written text in the source. Nevertheless, its PPL is considerably higher than CoRe on both datasets. It seems that CoRe indeed captures some characteristics of the target language, such as the diction. We also find that the PPL of Moses is the largest, and its generated length reaches the length of the source text. Moses seems to conduct word-to-word translation. This practice is acceptable in text simplification, but totally offends the summarization requirement. Although not manually controlled, the lengths of the outputs in ABS and CoRe are both similar to the actual one, which demonstrates the learning ability of Seq2Seq models. In addition, Table 3 shows that compared to ABS, CoRe generates far fewer UNK tags and its copying ratio is closer to the actual one. The former verifies the power of our two decoders, while the latter may be attributed to the supplement of the supervision of writing modes."}, {"heading": "Case Study", "text": "In addition to the automatic sentence quality measurements, we manually inspect what our model actually generates. In text simplification, we observe that the paraphrase rules of Simple Wikipedia are relatively fixed. For example, no matter how the article in Wikipedia illustrates, Simple Wikipedia usually adopts the following pattern to describe a commune:\n#NAME is a commune . it is found in #LOCATION .\nCoRe grasps many frequent paraphrase rules, and there are more than 130 cases where the generation results of CoRe exactly hit the actual target sentences. Therefore, we focus more on the analysis of the summarization results next. In summarization, although most target words come from the copying decoder, we find CoRe tends to pick keywords from different parts of the source document. By contrast, the standard attentive Seq2Seq model often extracts a large part of continuous source words. Meanwhile, the restricted generation decoder usually plays the role to \u201cconnect\u201d these keywords, such as to change the tenses, or to supplement article words. Its behavior resembles a human summarizer to a large extent. Table 4 gives some examples generated by different models. We find that the sentence generated by CoRe is fluent and satisfies the need of summarization. The only difference from the actual target is that CoRe does not assume \u201ctold @entity3\u201d is important enough and simplifies it to \u201csaid\u201d. It is the common way that human summarizes. Notably, CoRe changes the starting word from \u201canother\u201d to \u201ca\u201d, which is actually more preferred for an independent highlight. Looking at other models, Moses almost repeats the content of the source text. As a result, it is the longest one and fails to catch the main idea. ABS indeed compresses the source text. It however focuses on the wrong place, i.e., the attributive clause. Therefore, its output does not even form a complete sentence.\ncommon words ."}, {"heading": "Related Work", "text": "The Seq2Seq model is a newly emerging approach. It was initially proposed by (Kalchbrenner and Blunsom 2013; Sutskever, Vinyals, and Le 2014; Cho et al. 2014) for machine translation. Compared with the traditional statistical machine translation approaches (e.g., (Koehn et al. 2007)), Seq2Seq models require less human efforts. Later, (Bahdanau, Cho, and Bengio 2014) developed the attention mechanism which largely promoted the applications of the Seq2Seq models. In addition to machine translation, Seq2Seq models achieved the state-of-the-art performance in many other tasks such as response generation (Shang, Lu, and Li 2015) Some researches (e.g., (Rush, Chopra, and Weston 2015; Hu, Chen, and Zhu 2015)) have directly applied the general Seq2Seq model (Bahdanau, Cho, and Bengio 2014) to the paraphrase-oriented task. However, the experiments of (Rush, Chopra, and Weston 2015) demonstrated that the introduction of hand-crafted features significantly improved the performance of the original model. Consequently, the general Seq2Seq model used for machine translation seemed not suitable for the paraphrase task which involves both copying and rewriting.\nLimited work has explored the copying mechanism. (Vinyals, Fortunato, and Jaitly 2015) proposed a pointer mechanism to predict the output sequence directly from the input. In addition to the different applications, their model cannot generate items outside of the set of input sequence. Later, (Allamanis, Peng, and Sutton 2016) developed a convolutional attention network to generate the function name of the source code. Since there are many out-of-vocabulary (OOV) words in the source code, they used another attention model in the decoder to directly copy a code token. In Seq2Seq generation, the most relevant work we find is COPYNET (Gu et al. 2016) which has been explained in the\nintroduction. Some existing work has tried to modify the output dimension of the decoder to speed up the training process. In training, (Cho, Memisevic, and Bengio 2015) restricted the decoder to generate the words from the actual target words together with a sampled word set. (Nallapati et al. ) supplemented the 1-nearest-neighbors of words in the source text, as measured by the similarity in the word embedding space. Notice that, these models still decoded on the full vocabulary during test. In comparison, our restricted generative decoder always produces the words in a small yet highly relevant vocabulary."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we develop a novel Seq2Seq model called CoRe to simulate the two core writing modes in paraphrase, i.e., copying and rewriting. CoRe fuses a copying decoder and a restricted generative decoder. To combine the two decoders and determine the final output, we train a predictor to predict the writing modes. We conduct extensive experiments on two different paraphrase-oriented datasets. The result shows that our model outperforms the state-of-the-art approaches in terms of both informativeness and language quality. At present, our model focuses on producing a single sentence. We plan to extend it to generate multi-sentence documents.\nAcknowledgments The work described in this paper was supported by Research Grants Council of Hong Kong (PolyU 152094/14E), National Natural Science Foundation of China (61272291, 61672445) and The Hong Kong Polytechnic University (GYBP6, 4-BCB5, B-Q46C). The correspondence authors of this paper are Wenjie Li and Sujian Li."}], "references": [{"title": "A convolutional attention network for extreme summarization of source code. arXiv preprint arXiv:1602.03001", "author": ["Peng Allamanis", "M. Sutton 2016] Allamanis", "H. Peng", "C. Sutton"], "venue": null, "citeRegEx": "Allamanis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Memisevic Cho", "S.J.K. Bengio 2015] Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "A simple, fast, and effective reparameterization of ibm model 2. Association for Computational Linguistics", "author": ["Chahuneau Dyer", "C. Smith 2013] Dyer", "V. Chahuneau", "N.A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu"], "venue": "arXiv preprint arXiv:1603.06393", "citeRegEx": "Gu,? \\Q2016\\E", "shortCiteRegEx": "Gu", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann,? \\Q2015\\E", "shortCiteRegEx": "Hermann", "year": 2015}, {"title": "Lcsts: A large scale chinese short text summarization dataset. arXiv preprint arXiv:1506.05865", "author": ["Chen Hu", "B. Zhu 2015] Hu", "Q. Chen", "F. Zhu"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "N. Blunsom 2013] Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive", "citeRegEx": "Koehn,? \\Q2007\\E", "shortCiteRegEx": "Koehn", "year": 2007}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y"], "venue": "In Proceedings of the ACL Workshop,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Chopra Rush", "A.M. Weston 2015] Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lu Shang", "L. Li 2015] Shang", "Z. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1503.02364", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104\u20133112", "author": ["Vinyals Sutskever", "I. Le 2014] Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["Tieleman", "T. Hinton 2012] Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Many natural language generation tasks, such as abstractive summarization and text simplification, are paraphrase-orientated. In these tasks, copying and rewriting are two main writing modes. Most previous sequence-to-sequence (Seq2Seq) models use a single decoder and neglect this fact. In this paper, we develop a novel Seq2Seq model to fuse a copying decoder and a restricted generative decoder. The copying decoder finds the position to be copied based on a typical attention model. The generative decoder produces words limited in the source-specific vocabulary. To combine the two decoders and determine the final output, we develop a predictor to predict the mode of copying or rewriting. This predictor can be guided by the actual writing mode in the training data. We conduct extensive experiments on two different paraphrase datasets. The result shows that our model outperforms the stateof-the-art approaches in terms of both informativeness and language quality.", "creator": "LaTeX with hyperref package"}}}