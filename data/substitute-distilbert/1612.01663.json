{"id": "1612.01663", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "Efficient Non-Oblivious Randomized Reduction for Risk Minimization with Improved Excess Risk Guarantee", "abstract": "in this paper, we address some problems for high dimensional simulations. previously, oblivious random projection numerical approaches that project high dimensional features onto a random subspace have been used in practice for facing high - dimensionality challenge in machine learning. recently, various non - oblivious randomized reduction methods have significantly developed and by considering solving many numerical problems such as matrix product approximation, low - rank matrix elimination, etc. however, they are recently explored for practical machine learning tasks, e. g., classification. more seriously, efficient theoretical analysis of excess risk bounds displaying risk minimization, an important measure of generalization performance, has not been established for non - oblivious randomized reduction methods. mortality therefore remains an open problem what is the benefit of using techniques over previous oblivious random projection based approaches. to tackle these challenges, we propose an algorithmic framework for employing non - oblivious randomized improvement tasks offering general empirical risk minimizing in machine learning tasks, where the greatest high - dimensional features are projected onto a random subspace that is derived from the algorithm with a small matrix approximation error. we then derive the first excess risk bound toward the proposed non - oblivious randomized reduction approach without requiring strong collaboration on the training materials. the established excess risk bound state that the model approach provides much better generalization ability until it also sheds improved insights about different randomized reduction approaches. finally, we conduct extensive experiments concerning both synthetic and real - used benchmark datasets, whose dimension rises to $ 500 ( 10 ^ 7 ) $, to demonstrate the efficacy of our proposed approach.", "histories": [["v1", "Tue, 6 Dec 2016 04:58:45 GMT  (44kb)", "http://arxiv.org/abs/1612.01663v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yi xu", "haiqin yang", "lijun zhang 0005", "tianbao yang"], "accepted": true, "id": "1612.01663"}, "pdf": {"name": "1612.01663.pdf", "metadata": {"source": "CRF", "title": "Efficient Non-oblivious Randomized Reduction for Risk Minimization with Improved Excess Risk Guarantee", "authors": ["Yi Xu", "Haiqin Yang", "Lijun Zhang", "Tianbao Yang"], "emails": ["tianbao-yang}@uiowa.edu,", "hqyang@ieee.org,", "zhanglj@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n01 66\n3v 1\n[ cs\n.L G\n] 6\nD ec\n2 01"}, {"heading": "Introduction", "text": "Recently, the scale and dimensionality of data associated with machine learning and data mining applications have seen unprecedented growth, spurring the BIG DATA research and development. Learning from largescale ultrahigh-dimensional data remains a computationally challenging problem. The big size of data not only increases the memory footprint but also increases the computational costs pertaining to optimization. A popular approach for addressing the high-dimensionality challenge is\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nto perform dimensionality reduction. Nowadays, randomized reduction methods are emerging to be attractive for dimensionality reduction. Compared with traditional dimensionality reduction methods (e.g., PCA and LDA), randomized reduction methods (i) can lead to simpler algorithms that are easier to analyze (Mahoney 2011); (ii) can often be organized to exploit modern computational architectures better than classical dimensional reduction methods (Halko, Martinsson, and Tropp 2011); (iii) can be more efficient without loss in efficacy (Paul et al. 2013).\nGenerally, randomized reduction methods can be cast into two types: the first type of methods reduces a set of high-dimensional vectors into a low dimensional space independent of each other. These methods usually sample a random matrix independent of the data and then use it to reduce the dimensionality of the data. The second type of methods projects a set of vectors (in the form of a matrix) onto a subspace such that the original matrix can be well reconstructed from the projected matrix and the subspace. Therefore, the subspace to which the data is projected depends on the original data. These methods have been deployed for solving many numerical problems related to matrices, e.g., matrix product approximation, low-rank matrix approximation, approximate singular value decomposition (Boutsidis and Gittens 2013a; Halko, Martinsson, and Tropp 2011). To differentiate these two types of randomized reduction methods, we refer to the first type as oblivious randomized reduction, and refer to the second type as non-oblivious randomized reduction. We note that in literature oblivious and non-oblivious are used interchangeably with data-independent and data-dependent. Here, we use the terminology commonly appearing in matrix analysis and numerical linear algebra due to that the general excess risk bound depends on the matrix approximation error.\nHowever, we have not seen any comprehensive study on the statistical property (in particular the excess risk bound) of these randomized reduction methods applied to risk minimization in machine learning. The excess risk bound measures the generalization performance of a learned model compared to the optimal model from a class that has the best generalization performance. The excess risk bounds facilitate a better understanding of different learning algorithms and have the potential to guide us to design bet-\nter algorithms (Kukliansky and Shamir 2015). It is worth noting that several studies have been devoted to understanding the theoretical properties of oblivious randomized reduction methods applied to classification and regression problems. For example, (Blum 2005; Shi et al. 2012; Paul et al. 2013) analyzed the preservation of the margin of SVM based classification methods with randomized dimension reduction. (Zhang et al. 2014; Yang et al. 2015; Pilanci and Wainwright 2015) studied the problem from the perspective of optimization. Nonetheless, these results are limited in the sense that (i) they focus on only oblivious randomized reduction where the data is projected onto a random subspace independent of the data; (ii) they depend heavily on strong assumptions of the training data or the problem, e.g., low-rank of the data matrix, linear separability of training examples, or the sparsity of optimal solution, and (iii) some of these results do not directly carry over to the excess risk bounds.\nTo tackle the above challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction (NOR) method to project the original high-dimensional features onto a random subspace that is derived from the original data. We study and establish the excess risk bound of the presented randomized algorithms for risk minimization. Different from previous results for oblivious randomized reduction methods, our theoretical analysis does not require assumptions of the training data or the problem, such as low-rank of the data matrix, linear separability of training examples, and the sparsity of optimal solution. When the data matrix is of low-rank or has a fast spectral decay, the excess risk bound of NOR is much better than that of oblivious randomized reduction based methods. Empirical studies on synthetic and real data sets corroborate the theoretical results and demonstrate the effectiveness of the proposed methods."}, {"heading": "Related Work", "text": "In literature, tremendous studies are devoted to nonoblivious randomized reduction in matrix applications. The focus of these studies is to establish matrix approximation error or the recovery error of the solution (e.g., in leastsquares regression). Few studies have examined their properties for risk minimization in machine learning. For oblivious randomized reduction methods, there exist some theoretical work trying to understand their impact on prediction performance (Blum 2005; Shi et al. 2012; Paul et al. 2013). This work differentiates from these studies in that we focus on the statistical property (the generalization property) of non-oblivious randomized reduction for expected risk minimization.\nWe employ tools in statistical learning theory to study the excess risk bounds of randomized reduction and results from randomized matrix theory to understand the order of the excess risk bound. A popular method for expected risk minimization is regularized empirical risk minimization (Vapnik 1998). The excess risk bounds of regularized empirical risk minimization have been well understood. In general, given a sample of size n it can achieve a risk bound of O(1/ \u221a n). Under some special conditions\n(e.g., low noise condition) this bound can be further improved (Bousquet, Boucheron, and Lugosi 2003). However, it is still not entirely clear what is the order of excess risk for learning from randomized dimensionality reduced data. The recovery result from (Zhang et al. 2014; Yang et al. 2015) could end up with an order of O(1/ \u221a m) excess risk for oblivious randomized reduction, where m is the reduced dimensionality. However, it relies on strong assumptions of the data. (Durrant and Kaban 2013) proved the generalization error of the linear classifier trained on randomly projected data by oblivious randomized reduction, which is upper bounded by the training error of the classifier learned in the original feature space by empirical risk minimization plus the VC-complexity in the projection space (proportional to O(1/ \u221a m) and plus terms depending on the average flipping probabilities on the training points defined as (1/n) \u2211n i=1 Pr(sign(w T nA\n\u22baAxi) 6= sign(w\u22banxi)), where wn is a model learned from the original data by empirical risk minimization. However, the order of the average flipping probabilities is generally unknown.\nRandom sampling (in particular uniform sampling) has been used in the Nystro\u0308m method for approximating a big kernel matrix. There are some related work focusing on the statistical properties of the Nystro\u0308m based kernel method (Yang et al. 2012; Bach 2013; Jin et al. 2013; Alaoui and Mahoney 2015). We note that the presented empirical risk minimization with non-oblivious randomized reduction using random sampling is similar to using the Nystro\u0308m approximation on the linear kernel. However, in the present work besides random sampling, we also study other efficient randomized reduction methods using different random matrices. By leveraging recent results of these randomized reduction methods we are able to obtain better performance than using random sampling."}, {"heading": "Preliminaries", "text": "Let (x, y) denote a feature vector and a label that follow a distribution P = P(x, y), where x \u2208 X \u2282 Rd and y \u2208 Y . In the sequel, we will focus on Y = {+1,\u22121} and Y = R. However, we emphasize that the results are applicable to other problems (e.g., multi-class and multi-label classification). We denote by \u2113(z, y) a non-negative loss function that measures the inconsistency between a prediction z and the label y. Let w \u2208 Rd, then by assuming a linear model z = w\u22bax for prediction, the risk minimization problem in machine learning is to solve following problem:\nw\u2217 = arg min w\u2208Rd EP [\u2113(w \u22ba x, y)] (1)\nwhere EP [\u00b7] denotes the expectation over (x, y) \u223c P . Let A be an algorithm that learns an approximate solution wn from a sample of size n, i.e., {(x1, y1), . . . , (xn, yn)}. The excess risk of wn is defined as the difference between the expected risk of the solution wn and that of the optimal solution w\u2217:\nER(wn,w\u2217) = EP [\u2113(w\u22banx, y)]\u2212 EP [\u2113(w\u22ba\u2217x, y)] (2) A popular method for learning an approximate solution wn is based on regularized empirical risk minimization (ERM),\ni.e.,\nwn = arg min w\u2208Rd\n1\nn\nn\u2211\ni=1\n\u2113(w\u22baxi, yi) + \u03bb\n2 \u2016w\u201622 (3)\nThe ERM problem is sometimes solved by solving its dual problem:\n\u03b1\u2217 = arg max \u03b1\u2208Rn \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1\n2\u03bbn2 \u03b1\u22baX\u22baX\u03b1 (4)\nwhere \u03b1 is usually called dual variable, \u2113\u2217i (\u03b1) = maxz \u03b1z\u2212 \u2113(z, yi) is the conjugate dual of the loss function, and X = (x1, . . . ,xn) \u2208 Rd\u00d7n is the data matrix. With \u03b1\u2217, we have wn = \u2212 1\u03bbnX\u03b1\u2217."}, {"heading": "Oblivious Randomized Reduction", "text": "In this section, we present an excess risk bound of oblivious randomized reduction building on previous theoretical results to facilitate the comparison with our result of non-oblivious randomized reduction. The idea of oblivious randomized reduction is to reduce a high-dimensional feature vector x \u2208 Rd to a low dimensional vector by x\u0302 = Ax \u2208 Rm, where A \u2208 Rm\u00d7d is a random matrix that is independent of the data. A traditional approach is to use a Gaussian matrix with each entry independently sampled from a normal distribution with mean zero and variance 1/m (Dasgupta and Gupta 2003). Recently, many other types of random matrix A are proposed that lead to much more efficient computation of reduction, including subsampled randomized Hadamard transform (SRHT) (Boutsidis and Gittens 2013a) and random hashing (RH) (Kane and Nelson 2014). The key property of A that plays an important role in the analysis is that it should preserve the Euclidean length of a high-dimensional vector with a high probability, which is stated formally in JohnsonLindenstrauss (JL) lemma below.\nLemma 1 (JL Lemma). For any 0 < \u01eb, \u03b4 < 1/2, there exists a probability distribution on matrices A \u2208 Rm\u00d7d such that there exists a small universal constant c > 0 and for any fixed x \u2208 Rd, with a probability at least 1\u2212 \u03b4, we have\n\u2223\u2223\u2016Ax\u201622 \u2212 \u2016x\u201622 \u2223\u2223 \u2264 c\n\u221a log(1/\u03b4)\nm \u2016x\u201622\nThe key consequence of the JL lemma is that we can reduce a set of d-dimensional vectors into a low dimensional space with a reduced dimensionality independent of d such that the pairwise distance between any two points can be well preserved.\nGiven the JL transform A \u2208 Rm\u00d7d, the problem can be imposed as,\nmin v\u2208Rm EP [\u2113(v \u22baAx, y)] (5)\nPrevious studies have focused on using the ERM of the above problem\nmin v\u2208Rm\n1\nn\nn\u2211\ni=1\n\u2113(v\u22bax\u0302i, yi) + \u03bb\n2 \u2016v\u201622 (6)\nto learn a model in the reduced feature space or using its dual solution to recover a model in the original high-dimensional space:\n\u03b1\u0302 = arg max \u03b1\u2208Rn \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1\n2\u03bbn2 \u03b1\u22baX\u0302\u22baX\u0302\u03b1 (7)\nwhere X\u0302 = (x\u03021, . . . , x\u0302n) \u2208 Rm\u00d7n. For example, (Zhang et al. 2014) proposed a dual recovery approach to recover a model in the original high-dimensional space that is close to the optimal solution wn in (3). The dual recovery approach consists of two steps (i) the first step obtains an approximate dual solution \u03b1\u0302 \u2208 Rn by solving the dual problem in (7), and (ii) the second step recovers a highdimensional model by w\u0302n = \u2212 1\u03bbnX\u03b1\u0302. By making a lowrank assumption of the data matrix, they established a recovery error \u2016wn\u2212 w\u0302n\u20162 in the order of O( \u221a r/m\u2016wn\u20162), where r represents the rank of the data matrix. The theory has been generalized to full rank data matrix but with an additional assumption that the optimal primal solution wn or the optimal dual solution \u03b1\u2217 is sparse (Zhang et al. 2014; Yang et al. 2015). A similar order O( \u221a r/m\u2016wn\u20162) of recovery error was established, where r represents the number of non-zero elements in the optimal solution. The proposition below exhibits the excess risk bound building on the recovery error.\nProposition 1. Suppose \u2113(z, y) is Lipschitz continuous and A is a JL transform. Let w\u0302n denote a recovered model by an ERM approach such that \u2016wn\u2212w\u0302n\u20162 \u2264 O( \u221a r/m\u2016wn\u20162)."}, {"heading": "Then", "text": "ER(w\u0302n,w\u2217) , EP [\u2113(w\u0302 \u22ba nx, y)]\u2212 EP [\u2113(w\u22ba\u2217x, y)] \u2264 O( \u221a r/m\u2016wn\u20162 + 1/ \u221a n)\nRemark: In the above bound, we omit dependence on upper bound of the data norm \u2016x\u2016 \u2264 R. Although the synthesis of the proposition and previous recovery error analysis can give us guarantee on the excess risk bound, it relies on certain assumptions of the data, which may not hold in practice."}, {"heading": "Non-Oblivious Randomized Reduction", "text": "The key idea of non-oblivious randomized reduction is to compute a subspace U\u0302 \u2208 Rd\u00d7m from the data matrix X \u2208 Rd\u00d7n such that the projection of the data matrix to the subspace is close to the data matrix. To compute the subspace U\u0302 , we first sample a random matrix \u2126 \u2208 Rn\u00d7m and compute Y = X\u2126 \u2208 Rd\u00d7m. Then let U\u0302 be the left singular vector matrix of Y . This technique has been used in low-rank matrix approximation, matrix product approximation and approximate singular value decomposition (SVD) of a large matrix (Halko, Martinsson, and Tropp 2011). Various random matrices \u2126 can be used as long as the matrix approximation error defined below can be well bounded.\n\u2016X \u2212 U\u0302 U\u0302\u22baX\u20162 = \u2016X \u2212 PY X\u20162 (8)\nwhere PY = U\u0302 U\u0302\u22ba denotes the projection to the subspace U\u0302 . We defer more discussions on different random matrices and\nAlgorithm 1 ERM with Non-Oblivious Randomized Reduction (NOR)\n1: Compute Y = X\u2126 \u2208 Rd\u00d7m, where \u2126 \u2208 Rn\u00d7m is a random subspace embedding matrix 2: Compute SVD of Y = U\u0302\u03a3\u0302V\u0302 \u22ba, where U\u0302 \u2208 Rd\u00d7m 3: Compute the reduced data by X\u0302 = U\u0302\u22baX \u2208 Rm\u00d7n 4: Solve the reduced problem in Eqn. (9) 5: Output v\u0302n\ntheir impact on the excess risk bound to subsection \u201cMatrix Approximation Error\u201d.\nNext, we focus on the non-oblivious reduction defined by U\u0302 for risk minimization. Let x\u0302i = U\u0302\u22baxi denote the reduced feature vector and X\u0302 = U\u0302\u22baX \u2208 Rm\u00d7n be the reduced data matrix. We propose to solve the following ERM problem:\nv\u0302n = arg min v\u2208Rm\n1\nn\nn\u2211\ni=1\n\u2113(v\u22bax\u0302i, yi) + \u03bb\n2 \u2016v\u201622. (9)\nTo understand the non-oblivious randomized reduction for ERM, we first see that the problem above is equivalent to\nmin w=U\u0302v,v\u2208Rm\n1\nn\nn\u2211\ni=1\n\u2113(w\u22baxi, yi) + \u03bb\n2 \u2016w\u201622.\ndue to that \u2016U\u0302v\u20162 = \u2016v\u20162. Compared to (3), we can see that the ERM with non-oblivious randomized reduction is restricting the model w to be U\u0302v. Since U\u0302 can capture the top column space of X due to the way it is constructed, and therefore the resulting model w\u0302n = U\u0302 v\u0302n is close to the top column space of X . Thus, we expect w\u0302n to be close to wn. The procedure is described in details in Algorithm 1. We note that the SVD in step 2 can be computed efficiently for sparse data. We defer the details into the appendix."}, {"heading": "Excess Risk Bound", "text": "Here, we show an excess risk bound of the proposed ERM with non-oblivious randomized reduction. The logic of the analysis is to first derive the optimization error of the approximate model w\u0302n = U\u0302 v\u0302n and then explore the statistical learning theory to bound the excess risk. In particular, we will show that the optimization error and consequentially the excess risk is bounded by the matrix approximation error in (8). To simplify the presentation, we introduce some notations:\nF (w) = 1\nn\nn\u2211\ni=1\n\u2113(w\u22baxi, yi) + \u03bb\n2 \u2016w\u20162 (10)\nF\u0304 (w) = EP [\u2113(w \u22ba x, y)] +\n\u03bb 2 \u2016w\u201622 (11)\nNext, we derive the optimization error of w\u0302n = U\u0302 v\u0302n.\nLemma 2. Suppose the loss function is G-Lipschitz continuous. Let w\u0302n = U\u0302 v\u0302n. We have\nF (w\u0302n) \u2264 F (wn) + G2\n2\u03bbn \u2016X \u2212 PY X\u201622\nThe lemma below bounds the excess risk by the optimization error.\nLemma 3 (Theorem 1 (Sridharan et al. 2008)). Assume the loss function is G-Lipschitz continuous and \u2016x\u20162 \u2264 R. Then, for any \u03b4 > 0 and any a > 0, with probability at least 1\u2212 \u03b4, we have that for any w\u2217 \u2208 Rd\nF\u0304 (w\u0302n)\u2212 F\u0304 (w\u2217) \u2264 (1 + a)(F (w\u0302n)\u2212 F (wn))\n+ 8(1 + 1/a)G2R2(32 + log(1/\u03b4))\n\u03bbn Using the lemma above and the the result in Lemma 2, we have the following theorem about the excess risk bound. Theorem 1. Assume the loss function is G-Lipschitz continuous and \u2016x\u20162 \u2264 R. Then, for any \u03b4 > 0 and any a > 0, with probability at least 1\u2212 \u03b4, we have that for any w\u2217 such that \u2016w\u2217\u20162 \u2264 B\nER(w\u0302n,w\u2217) \u2264 \u03bbB2\n2 +\nG2(1 + a)\n2\u03bbn \u2016X \u2212 PY X\u201622\n+ 8(1 + 1/a)G2R2(32 + log(1/\u03b4))\n\u03bbn In particular, if we optimize \u03bb over the R.H.S., we obtain\nER(w\u0302n,w\u2217) \u2264 GB \u221a (1 + a)\u221a n \u2016X \u2212 PY X\u20162\n+ 4GRB \u221a (1 + 1/a)(32 + log(1/\u03b4))\u221a\nn\nRemark: Note that the above theorem bounds the excess risk by the matrix approximation error. Thus, we can leverage state-of-the-art results on the matrix approximation to study the excess risk bound. Importantly, future results about matrix approximation can be directly plugged into the excess risk bound. When the data matrix is of low rank r, then if m \u2265 \u2126(r log r) the matrix approximation error can be made zero (see below). As a result, the excess risk bound of w\u0302n is O(1/ \u221a n), the same to that of wn. In contrast, the excess risk bound in Proposition 1 of oblivious randomized reduction for ERM is O( \u221a r/m) for the dual recovery approach under the low rank assumption."}, {"heading": "Matrix Approximation Error", "text": "In this subsection, we will present some recent results on the matrix approximation error of four commonly used randomized reduction operators \u2126 \u2208 Rn\u00d7m, i.e., random sampling (RS), random Gaussian (RG), subsampled randomized Hadamard transform (SRHT), and random hashing (RH), and discuss their impact on the excess risk bound. More details of these four randomized reduction operators can be found in (Yang et al. 2015). We first introduce some notations used in matrix approximation analysis. Let r \u2264 min(n, d) denote the rank of X and k \u2208 N+ such that 1 \u2264 k \u2264 r. We write the SVD of X \u2208 Rd\u00d7n as X = U1\u03a31V \u22ba1 + U2\u03a32V \u22ba\n2 , where \u03a31 \u2208 Rk\u00d7k, \u03a32 \u2208 R(r\u2212k)\u00d7(r\u2212k), U1 \u2208 R\nd\u00d7k, U2 \u2208 Rd\u00d7(r\u2212k), V1 \u2208 Rn\u00d7k and V2 \u2208 Rn\u00d7(r\u2212k). We use \u03c31, \u03c32, . . . , \u03c3r to denote the singular values of X in the descending order. Let \u00b5k denote the coherence measure of V1 defined as \u00b5k = nk max1\u2264i\u2264n \u2211k j=1[V1] 2 ij .\nTheorem 2 (RS (Gittens 2011)). Let \u2126 \u2208 Rn\u00d7m be a random sampling matrix corresponding to sampling the columns of X uniformly at random with or without replacement. If for any \u01eb \u2208 (0, 1) and \u03b4 > 0, m satisfies m \u2265 2\u00b5k(1\u2212\u01eb)2k log k\u03b4 , then with a probability at least 1\u2212 \u03b4,\n\u2016X \u2212 PY X\u20162 \u2264 \u221a 1 + n\n\u01ebm \u03c3k+1\nRemark: The matrix approximation error using RS implies the excess risk bound of RS for risk minimization is dominated by O(\u03c3k+1\u221a\nm ) provided m \u2265 \u2126(\u00b5kk log k),\nwhich is in the same order in terms of m to that in Proposition 1 of oblivious randomized reduction. However, random sampling is not guaranteed to work in oblivious randomized reduction since it does not satisfy the JL lemma in general (Yang et al. 2015) as required in Proposition 1. Moreover, if the data matrix is low rank such that m \u2265 \u2126(\u00b5rr log r), then the excess risk bound of NOR with RS is O(1/ \u221a n), the same order to that of wn learned from the original high-dimensional features.\nTheorem 3 (RG (Gittens and Mahoney 2013)). Let \u2126 \u2208 R\nn\u00d7m be a random Gaussian matrix. If for any \u01eb \u2208 (0, 1) and k > 4, m satisfies m \u2265 2\u01eb\u22122k log k, then with a probability at least 1\u2212 2k\u22121 \u2212 4k\u2212k/\u01eb2 ,\n\u2016X \u2212 PY X\u20162 \u2264O(\u03c3k+1) + O (\n\u01eb\u221a k log k )\u221a\u2211 j>k \u03c32j\nRemark: We are interested in comparing the error bound of RG with that of RS. In the worse case, when the tail singular values are flat, then \u2016X \u2212 PY X\u20162 \u2264 O( \u221a n m\u03c3k+1), which is in the same order to that of RS. However, if the tail\neigen-values decay fast such that \u221a\u2211\nj>k \u03c3 2 j \u226a \u221a n\u03c3k+1,\nthen the matrix approximation error could be much better than O( \u221a n m\u03c3k+1), and consequentially the excess risk\nbound could be much better than O(\u03c3k+1/ \u221a m) that is suffered by RS.\nTheorem 4 (SRHT (Boutsidis and Gittens 2013a)). Let \u2126 = \u221a n mDHP \u2208 Rn\u00d7m be a SRHT with P \u2208 Rn\u00d7m being a random sampling matrix, D \u2208 Rn\u00d7n is a diagonal matrix with each entry sampled from {1,\u22121} with equal probabilities and H \u2208 Rn\u00d7n is a normalized Hadamard transform. If for any 0 < \u01eb < 1/3, 2 \u2264 k \u2264 r and \u03b4 \u2208 (0, 1), m satisfies\n6C2\u01eb\u22121[ \u221a k + \u221a 8 log(n/\u03b4)]2 log(k/\u03b4) \u2264 m \u2264 n,\nthen with a probability at least 1\u2212 5\u03b4,\n\u2016X \u2212 PY X\u20162 \u2264 ( 4 + \u221a 3 log(n/\u03b4) log(r/\u03b4)\nm\n) \u03c3k+1\n+\n\u221a 3 log(r/\u03b4)\nm\n\u221a\u2211 j>k \u03c32j\nwhere C is a universal constant.\nRemark: The order of the matrix approximation error of SRHT is similar to that of RG up to a logarithmic factor.\nFinally, we summarize the matrix approximation error of the RH matrix \u2126. This has been studied in (Cohen, Nelson, and Woodruff 2015), in which RH is also referred to as sparse subspace embedding. We first describe the construction of random hashing matrix \u2126. Let hk(i) : [n] \u2192 [m/s], k = 1, . . . , s denote s independent random hashing functions and let \u2126 = ((H1D1) \u22ba, (H2D2) \u22ba, . . . , (HsDs)\n\u22ba)\u22ba \u2208 Rm\u00d7n be a random matrix with a block of s random hashing matrices, where Dk \u2208 Rn\u00d7n is a diagonal matrix with each entry sampled from {\u22121,+1} with equal probabilities, and Hk \u2208 Rm/s,n with [Hk]j,i = \u03b4j,hk(i). The following theorem below summarizes the matrix approximation error using such a random matrix \u2126.\nTheorem 5 (RH (Cohen, Nelson, and Woodruff 2015)). For any \u03b4 \u2208 (0, 1) and \u01eb \u2208 (0, 1). If s = 1 and m = O(k/(\u01eb\u03b4)) or s = O(log3(k/\u03b4)/ \u221a \u01eb) and m = O(k log6(k/\u03b4)/\u01eb), then with a probability 1\u2212 \u03b4\n\u2016X \u2212 PY X\u20162 \u2264 (1 + \u221a \u01eb)\u03c3k+1 +\n\u221a \u01eb\nk \u221a\u2211 j>k \u03c32j\nRemark: With the second choice of s and m, the order of the matrix approximation error of RH is similar to that of SRHT up to a logarithmic factor.\nTo conclude this section, we can see that the excess risk bound of the ERM with non-oblivious randomized reduction is dominated by O(1/ \u221a m) in the worst case, and could be much better than RG, SRHT and RH if the tail singular values decay fast."}, {"heading": "Experiments", "text": "In this section, we provide empirical evaluations in support of the proposed algorithms and the theoretical analysis. We implement and compare the following algorithms: (i) NOR: ERM with non-oblivious randomized reduction; (ii) previous ERM approaches with oblivious randomized reduction, including two dual recovery approaches, namely random projection with dual recovery (RPDR) (Zhang et al. 2014), and dual-sparse regularized randomized (DSRR) approach (Yang et al. 2015), and the pure random projection (RP) (Paul et al. 2013). We also implement and compare three randomized reduction operators for these different approaches, i.e., RH, RG and RS 1. For RH, we use only one block of random hashing matrix (i.e., s = 1). A similar result to Therorem 5 can be established for one-block of random hashing but with a constant success probability (Nelson and Nguyen 2012). The loss function for the binary classification problem is the hinge loss and for the multi-class classification problem is the softmax loss.\nExperiments are conducted on four real-world datasets and three synthetic datasets. The four real-world datasets are described in Table 1. To generate synthetic data, we first draw a random standard Gaussian matrix M \u2208 Rd\u00d7n and\n1We do not report the performance of SRHT because it has similar performance to RH but it is less efficient than RH.\ncompute its SVD M = USV \u22ba. Then we construct singular values following three different decay: an exponential decay (exp-\u03c4 ) with \u03c3i = e\u2212i\u03c4 , (\u03c4 = 1) and polynomial decay (poly-\u03c4 ) with \u03c3i = i\u2212\u03c4 , (\u03c4 = 0.5, 1). This will generate 3 synthetic datasets. We compute a base data matrix by Xb = \u221a nU\u03a3V \u22ba, where \u03a3 = diag{\u03c31, . . . , \u03c3d}. Then the binary labels are computed by y = sign(X\u22bab w), where w \u2208 Rd is a standard Gaussian random vector. To increase the difficulty of the problem, we add some Gaussian random features to each data in Xb and form a full data matrix X \u2208 R(d+t)\u00d7n. We use the first 90% examples as training data and the remaining 10% examples as testing data. In particular, we generate the synthetic datasets with d = 1000, n = 105, t = 10. We note that the synthetic data is not high-dimensional and it is solely for verifying the proposed approach and analysis. We perform data reduction to reduce features to the dimensionality of m = 100.\nWe first compare the performance of NOR, RPDR and DSRR with different randomized reduction operators on the synthetic datasets in order to verify the excess risk bounds established in Proposition 1 and subsection \u201cMatrix Approximation Error\u201d (MAE). The results are shown in Figure 1, where we also include the performance of SVM on the orig-\ninal features (denoted by Org). From the results, we can observe that (i) when the singular values follow an exponential decay (which yields almost low-rank data matrices), NOR performs almost the same to SVM on the original data; however the two recovery approaches RPDR and DSRR perform much worse than SVM, verifying our theoretical analysis in Proposition 1 and subsection \u201cMAE\u201d; (ii) The performance of NOR decreases gradually as the decay of singular values becomes slower, which is consistent with the theoretical results in subsection \u201cMAE\u201d; (iii) for NOR, RS is comparable to RH and RG when the decay of singular-values is fast, but is slightly worse when the decay of singular values becomes slower. This is also expected according to the discussions in subsection \u201cMAE\u201d; (iv) NOR always performs better than RPDR and DSRR. One reason that RPDR and DSRR do not perform well on these synthetic data sets is that the recovered dual solution is not accurate because that the data has noise. In contrast, NOR is much more robust to noise.\nSecondly, we present some experimental results on two binary classification real datasets, namely Reuters Text Categorization both for binary version (RCV1.b) and Splice Site Recognition (Splice), which are tested in previous studies (Sonnenburg and Franc 2010). For Splice dataset, we evaluate different algorithms by computing the same mea-\nsure, namely area under precision recall curve (auPRC), as in (Sonnenburg and Franc 2010). We compare NOR with RPDR, DSRR and RP. The results are shown in Figure 2. We can see that when m increases, the testing error/auPRC is monotonically decreasing/increasing. Comparing with other three algorithms, NOR has the best performance. In addition, RS does not work well for oblivious randomized reduction approaches (RP, RPDR and DSRR), but performs similarly to other randomized operators for NOR, which is consistent with our analysis (i.e., RS is not a JL transform as required in Proposition 1 for RPDR and DSRR; however, RS provides guarantee on the matrix approximation error that renders NOR work).\nFinally, we compare NOR with RP on RCV.m and News20 datasets for multi-class classification. The results are shown in Figure 3 (upper panel). We can see that NOR clearly outperforms RP. In addition, running time results 2 are reported in Figure 3 (lower panel). The running time consists of the reducation time and the optimization time in the reduced feture space. The results show that (i) NOR is more efficient than RP; (ii) RH and/or RS are much more efficient than RG for a certain approach. It is interesting to note that the total running time of NOR is less than RP. The reason is that the optimization of NOR is more efficient than RP 3 due to that the new data of NOR is better suited for classification (higher prediction performance), making the optimization easier, though NOR has slightly higher data reduction time than RP."}, {"heading": "Conclusions", "text": "In this paper, we have established the excess risk bound of non-oblivious randomized reduction method for risk minimization problems. More importantly, the new excess risk bound does not require stringent assumptions of the data and the loss functions, which is nontrivial and significant theoretical results. The empirical studies on synthetic datasets and real datasets validate our theoretical analysis and also demonstrate the effectiveness of the proposed non-oblivious randomized reduction approach."}, {"heading": "Acknowlegements", "text": "We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). L. Zhang is partially supported by NSFC (61603177) and JiangsuSF (BK20160658)."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of Lemma 2", "text": "Since the loss function is G-Lipschitz continuous, i.e., |\u2113\u2032(z, y)| \u2264 G, therefore, max\u03b1\u2208\u2126 |\u03b1| \u2264 G. Our proof is built on the dual formulation. First, we have\nF (wn) = max \u03b1\u2208\u2126n \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1\u22a4X\u22a4X\u03b1\nF (w\u0302n) = max \u03b1\u2208\u2126n \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1\u22a4X\u22a4U\u0302 U\u0302\u22a4X\u03b1\nThen\nF (w\u0302n) = max \u03b1\u2208\u2126n \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1\u22a4X\u22a4U\u0302 U\u0302\u22a4X\u03b1\n= max \u03b1\u2208\u2126n \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1\u22a4X\u22a4X\u03b1\n+ 1\n2\u03bbn2 \u03b1\u22a4(X\u22a4X \u2212X\u22a4U\u0302 U\u0302\u22a4X)\u03b1\n\u2264 max \u03b1\u2208\u2126n \u2212 1 n\nn\u2211\ni=1\n\u2113\u2217i (\u03b1i)\u2212 1 2\u03bbn2 \u03b1\u22a4X\u22a4X\u03b1\n+ max \u03b1\u2208\u2126n\n1\n2\u03bbn2 \u03b1\u22a4(X\u22a4X \u2212X\u22a4U\u0302 U\u0302\u22a4X)\u03b1\n\u2264 F (wn)\n+ 1\n2\u03bbn2 max \u03b1\u2208\u2126n\n\u2016\u03b1\u201622\u2016X\u22a4X \u2212X\u22a4U\u0302 U\u0302\u22a4X\u20162\n\u2264 F (wn) + G2\n2\u03bbn \u2016X\u22a4X \u2212X\u22a4U\u0302U\u0302\u22a4X\u20162\nOn the other hand, since PY = U\u0302U\u0302\u22a4 is the projection to the column space of Y = X\u2126, we have\n\u2016X\u22a4X \u2212X\u22a4U\u0302U\u0302\u22a4X\u20162 = \u2016X\u22a4X \u2212X\u22a4PY X\u20162 = \u2016X\u22a4(I \u2212 PY )X\u20162 = \u2016X\u22a4(I \u2212 PY )2X\u20162 (by the property of projection)\n= \u2016X \u2212 PY X\u201622 Combing above results together, we complete the proof."}, {"heading": "Proof of Theorem 1", "text": ""}, {"heading": "Proof.", "text": "ER(w\u0302n,w\u2217) = EP [\u2113(w\u0302\u22a4n x, y)]\u2212 EP [\u2113(w\u22a4\u2217 x, y)]\n= [ F\u0304 (w\u0302n)\u2212 \u03bb\n2 \u2016w\u0302n\u201622\n] \u2212 [ F\u0304 (w\u2217)\u2212 \u03bb\n2 \u2016w\u2217\u201622\n]\n= F\u0304 (w\u0302n)\u2212 F\u0304 (w\u2217) + \u03bb\n2 \u2016w\u2217\u201622 \u2212\n\u03bb 2 \u2016w\u0302n\u201622\n\u2264 F\u0304 (w\u0302n)\u2212 F\u0304 (w\u2217) + \u03bb\n2 \u2016w\u2217\u201622\n\u2264 (1 + a)(F (w\u0302n)\u2212 F (wn))\n+ 8(1 + 1/a)G2R2(32 + log(1/\u03b4))\n\u03bbn +\n\u03bb 2 \u2016w\u2217\u201622\n(by Lemma 3)\n\u2264 G 2(1 + a)\n2\u03bbn \u2016X \u2212 PY X\u201622\n+ 8(1 + 1/a)G2R2(32 + log(1/\u03b4))\n\u03bbn +\n\u03bb 2 \u2016w\u2217\u201622\n(by Lemma 2)\n\u2264 G 2(1 + a)\n2\u03bbn \u2016X \u2212 PY X\u201622\n+ 8(1 + 1/a)G2R2(32 + log(1/\u03b4))\n\u03bbn +\n\u03bbB2\n2\nThe last inequality is held because of \u2016w\u2217\u201622 \u2264 B2."}, {"heading": "Proof of Theorem 2", "text": "Proof. Recall that X = U1\u03a31V \u22a41 +U2\u03a32V \u22a4 2 , and we have Y = X\u2126 = U1\u03a31V \u22a4 1 \u2126 + U2\u03a32V \u22a4 2 \u2126. Let\u2019s denote \u21261 = V \u22a41 \u2126 and \u21262 = V \u22a4 2 \u2126, then\nY = X\u2126 = U1\u03a31\u21261 + U2\u03a32\u21262\nBased on the results from Theorem 9.1 of (Halko, Martinsson, and Tropp 2011), we have\n\u2016X \u2212 PY X\u201622 \u2264 \u2016\u03a32\u201622 + \u2016\u03a32\u21262\u2126\u20201\u201622 (12) with a probability at least 1 \u2212 \u03b4. In (Gittens 2011), Lemma 1 showed that\n\u2016\u2126\u20201\u201622 \u2264 n\n\u03b5m\nif assume that \u21261 has full row rank. It is easy to show that \u2016\u21262\u201622 \u2264 \u2016V \u22a42 \u201622\u2016\u2126\u201622 \u2264 1. Then we have\n\u2016\u03a32\u21262\u2126\u20201\u201622 \u2264 n\n\u03b5m \u2016\u03a32\u201622\nCombining this result with equation (12), we know that\n\u2016X \u2212 PY X\u201622 \u2264 (1 + n\n\u03b5m )\u2016\u03a32\u201622\ni.e.\n\u2016X \u2212 PY X\u20162 \u2264 \u221a (1 + n\n\u03b5m )\u03c3k+1\nwith a probability at least 1\u2212 \u03b4."}, {"heading": "Proof of Theorem 3", "text": "Proof. Our proof is modified from (Gittens and Mahoney 2013). In Section 10 of (Halko, Martinsson, and Tropp 2011), it is showed that if m = k + p with p > 4 and u, t \u2265 1, and \u03a32 is a diagonal matrix, then we have\n\u2016\u03a32\u21262\u2126\u20201\u20162 \u2264 \u2016\u03a32\u20162 (\u221a 3k\np+ 1 t+\ne \u221a m p+ 1 tu\n)\n+ \u2016\u03a32\u2016F e \u221a m\np+ 1 t (13)\nwith probability at least 1 \u2212 2t\u2212p \u2212 e\u2212u2/s. Since m \u2265 2\u01eb\u22122k log k and m = k + p, we have that p \u2265 \u01eb\u22122k log k. Then, the following inequalities hold:\n\u221a 3k\np+ 1 \u2264\n\u221a 3k\np \u2264\n\u221a 3\nlog k \u01eb\n\u221a m\np+ 1 \u2264\n\u221a k + p\np \u2264\n\u221a \u01eb4\nk log2 k +\n\u01eb2\nk log k \u2264\n\u221a 2\nk log k \u01eb\nApply these inequalities and set t = e and u = \u221a 2 log k in (13), we can obtain\n\u2016\u03a32\u21262\u2126\u20201\u20162 \u2264 \u2016\u03a32\u20162 ( e \u221a 3k\np+ 1 +\ne2 \u221a 2m log k\np+ 1\n)\n+ \u2016\u03a32\u2016F e2 \u221a m\np+ 1\n\u2264 ( e \u221a 3\nlog k + 2e2\n\u221a 1\nk\n) \u01eb\u2016\u03a32\u20162\n+ e2 \u221a 2\nk log k \u01eb\u2016\u03a32\u2016F\nCombining this results into equation (12), we have\n\u2016X \u2212 PY X\u201622 \u2264 \u2016\u03a32\u201622 + \u2016\u03a32\u21262\u2126\u20201\u201622\n\u2264 \u2016\u03a32\u201622 + ( e \u221a 3\nlog k + 2e2\n\u221a 1\nk\n)2 \u01eb2\u2016\u03a32\u201622\n+ e4 2\nk log k \u01eb2\u2016\u03a32\u20162F\n\u2264  1 + ( e \u221a 3\nlog k + 2e2\n\u221a 1\nk\n)2 \u01eb2  \u03c32k+1\n+ e4 2 k log k \u01eb2 \u2211\nj>k\n\u03c32j\nThus\n\u2016X \u2212 PY X\u20162 \u2264 \u221a\u221a\u221a\u221a1 + ( e \u221a 3\nlog k + 2e2\n\u221a 1\nk\n)2 \u01eb2\u03c3k+1\n+ e2 \u221a 2\nk log k \u01eb\n\u221a\u2211\nj>k\n\u03c32j\n\u2264 O(\u03c3k+1) +O (\n\u01eb\u221a k log k\n)\u221a\u2211\nj>k\n\u03c32j\nwith a probabilty at least 1\u2212 2k\u22121 \u2212 4k\u2212k/\u01eb2 ."}, {"heading": "Proof of Theorem 4", "text": "Proof. In (Boutsidis and Gittens 2013b), Lemma 4.1 showed that\n\u2016\u2126\u20201\u201622 \u2264 (1\u2212 \u221a \u01eb)\u22121\nwith probability at least 1 \u2212 3\u03b4. Comsequently, \u21261 has full row rank, and by applying Lemma 5.4 of (Boutsidis and Gittens 2013b) with the same probability, we obtain\n\u2016X \u2212 PY X\u201622 \u2264 \u2016\u03a32\u201622 + (1\u2212 \u221a \u01eb)\u22121\u2016\u03a32V \u22a42 \u2126\u201622 (14)\nFrom Lemma 4.8 of (Boutsidis and Gittens 2013b) we have\n(1\u2212 \u221a \u01eb)\u22121\u2016\u03a32V \u22a42 \u2126\u201622 \u2264\n5\n1\u2212\u221a\u01eb\u2016\u03a32V \u22a4 2 \u201622\n+ log(r/\u03b4)\n(1\u2212\u221a\u01eb)m (\u2016\u03a32V \u22a4 2 \u2016F +\n\u221a 8 log(n/\u03b4)\u2016\u03a32V \u22a42 \u20162)2\nwith probability at least 1\u22125\u03b4. Since 0 < \u01eb < 1/3, then (1\u2212\u221a \u01eb)\u22121 < 3. Also \u2016\u03a32V \u22a42 \u20162 = \u2016\u03a32\u20162 and \u2016\u03a32V \u22a42 \u2016F = \u2016\u03a32\u2016F . Thus, (1 \u2212 \u221a \u01eb)\u22121\u2016\u03a32V \u22a42 \u2126\u201622 \u2264 15\u2016\u03a32\u201622\n+ 3 log(r/\u03b4)\nm (\u2016\u03a32\u2016F +\n\u221a 8 log(n/\u03b4)\u2016\u03a32\u20162)2\nPlugging this equation into (14),\n\u2016X \u2212 PY X\u201622 \u2264 16\u2016\u03a32\u201622+ 3 log(r/\u03b4)\nm (\u2016\u03a32\u2016F +\n\u221a 8 log(n/\u03b4)\u2016\u03a32\u20162)2\nUse the subadditivity of the square-root function to obtain that\n\u2016X \u2212 PY X\u20162 \u2264 ( 4 + \u221a 3 log(n/\u03b4) log(r/\u03b4)\nm\n) \u03c3k+1\n+\n\u221a 3 log(r/\u03b4)\nm\n\u221a\u2211\nj>k\n\u03c32j\nwith probability at least 1\u2212 5\u03b4."}, {"heading": "Proof of Theorem 5", "text": "Proof. Recall that X = U1\u03a31V \u22a41 + U2\u03a32V \u22a4 2 and Y = X\u2126 = U\u0302 \u03a3\u0302V\u0302 \u22a4. Applying the SVD of PY X = U\u0302k\u03a3\u0302kV\u0302 \u22a4k + U\u0302k\u0304\u03a3\u0302k\u0304V\u0302 \u22a4 k\u0304\nto Theorem 4 of (Cohen, Nelson, and Woodruff 2015), we have\n\u2016X \u2212 U\u0302k\u03a3\u0302kV\u0302 \u22a4k \u201622 \u2264 (1 + \u01eb)\u2016X \u2212 U1\u03a31V \u22a41 \u201622 + \u01eb\nk \u2016X \u2212 U1\u03a31V \u22a41 \u20162F\n= (1 + \u01eb)\u2016\u03a32\u201622 + \u01eb\nk \u2016\u03a32\u20162F\nwith probability at least 1\u2212 \u03b4. Then we have\n\u2016X \u2212 PY X\u201622 = \u2016X \u2212 U\u0302k\u03a3\u0302kV\u0302 \u22a4k \u2212 U\u0302k\u0304\u03a3\u0302k\u0304V\u0302 \u22a4k\u0304 \u201622 \u2264 \u2016X \u2212 U\u0302k\u03a3\u0302kV\u0302 \u22a4k \u201622 + \u2016U\u0302k\u0304\u03a3\u0302k\u0304V\u0302 \u22a4k\u0304 \u201622 \u2264 \u2016X \u2212 U\u0302k\u03a3\u0302kV\u0302 \u22a4k \u201622 \u2264 (1 + \u01eb)\u2016\u03a32\u201622 + \u01eb\nk \u2016\u03a32\u20162F\nWe complete the proof by using the subadditivity of the square-root function, i.e. we have\n\u2016X \u2212 PY X\u20162 \u2264 \u221a (1 + \u01eb)\u2016\u03a32\u20162 +\n\u221a \u01eb\nk \u2016\u03a32\u2016F\n\u2264 (1 + \u221a \u01eb)\u03c3k+1 +\n\u221a \u01eb\nk\n\u221a\u2211\nj>k\n\u03c32j\nwith probability at least 1\u2212 \u03b4."}, {"heading": "An Efficient Implementation of Computing U\u0302 in NOR for Sparse Data", "text": "In this section, we present an efficient implementation of NOR for sparse data. We note that when data is sparse, the time complexity of calculating U\u0302\u22a4X is O(mN), where N \u226a dn is the number of non-zero elements in X . Consequently, computing the left singular vectors of Y could become a significant component of overall computation in Algorithm 2. To harness data sparsity, next we present a fast implementation of U\u0302 computation. Let Y = U\u0302 \u03a3\u0302V\u0302 \u22a4 be the SVD of Y , then V\u0302 \u03a3\u03022V\u0302 \u22a4 is the singular value decomposition of Km = Y \u22a4Y \u2208 Rm\u00d7m. Then, the projection matrix U\u0302\u22a4 can be computed by\nU\u0302\u22a4 = \u03a3\u0302\u22121V\u0302 \u22a4Y \u22a4 (15)\nAlgorithm 2 Fast Projection in NOR\n1: Compute Km = Y \u22a4Y \u2208 Rm\u00d7m 2: Compute eigen-values \u03bbi and corresponding eigen-vectors\nVi(i = 1, . . . ,m) of the small matrix Km 3: Let \u03a3\u0302 = diag( \u221a \u03bb1, . . . , \u221a \u03bbm) 4: Compute the projection matrix U\u0302\u22a4 by Eq. (15)\nTherefore, we can efficiently implement the projection matrix by first computing singular values and corresponding left singular vectors of the small matrix Km = Y \u22a4Y \u2208 Rm\u00d7m and then compute the projection matrix by Eq. (15). We assume the number of non-zero entries in X is N and the number of non-zero entries in Y is Nm. The time complexity consists of (i) O(mNm) for computing Km, (ii) O(m2 logm) for computing left singualr value decomposition of Km by randomized algorithms (Halko, Martinsson, and Tropp 2011), (iii) O(m2 + mNm) for computing \u03a3\u0302\u22121V\u0302 \u22a4Y \u22a4, yeilding an overall time complexity ofO(m2 logm+m2+2mNm). Compared to the overall time complexity of O(md logm) by directly computing the left singular vectors of Y , the efficient implementation could be much faster especially when Nm,m \u226a d. We present the detailed steps in Algorithm 2 for computing U\u0302\u22a4."}], "references": [{"title": "Fast randomized kernel ridge regression with", "author": ["W. M"], "venue": null, "citeRegEx": "M.,? \\Q2015\\E", "shortCiteRegEx": "M.", "year": 2015}, {"title": "Introduction to statistical learning theory", "author": ["Boucheron Bousquet", "O. Lugosi 2003] Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "In Advanced Lectures on Machine Learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2003}, {"title": "Improved matrix algorithms via the subsampled randomized hadamard transform", "author": ["Boutsidis", "C. Gittens 2013a] Boutsidis", "A. Gittens"], "venue": "SIAM J. Matrix Analysis Applications 34(3):1301\u20131340", "citeRegEx": "Boutsidis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2013}, {"title": "Improved matrix algorithms via the subsampled randomized hadamard transform", "author": ["Boutsidis", "C. Gittens 2013b] Boutsidis", "A. Gittens"], "venue": "SIAM Journal on Matrix Analysis and Applications 34(3):1301\u20131340", "citeRegEx": "Boutsidis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2013}, {"title": "D", "author": ["M.B. Cohen", "J. Nelson", "Woodruff"], "venue": "P.", "citeRegEx": "Cohen. Nelson. and Woodruff 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Gupta", "author": ["S. Dasgupta"], "venue": "A.", "citeRegEx": "Dasgupta and Gupta 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Kaban", "author": ["R.J. Durrant"], "venue": "A.", "citeRegEx": "Durrant and Kaban 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Mahoney", "author": ["A. Gittens"], "venue": "M.", "citeRegEx": "Gittens and Mahoney 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["N. Halko", "P.-G. Martinsson", "Tropp"], "venue": "A.", "citeRegEx": "Halko. Martinsson. and Tropp 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved bounds for the nystr\u00f6m method with application to kernel classification", "author": ["Jin"], "venue": "IEEE Transactions on Information Theory 59(10):6939\u20136949", "citeRegEx": "Jin,? \\Q2013\\E", "shortCiteRegEx": "Jin", "year": 2013}, {"title": "and Nelson", "author": ["D.M. Kane"], "venue": "J.", "citeRegEx": "Kane and Nelson 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Shamir", "author": ["D. Kukliansky"], "venue": "O.", "citeRegEx": "Kukliansky and Shamir 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "M", "author": ["Mahoney"], "venue": "W.", "citeRegEx": "Mahoney 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "H", "author": ["J. Nelson", "Nguyen"], "venue": "L.", "citeRegEx": "Nelson and Nguyen 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Random projections for support vector machines", "author": ["Paul"], "venue": null, "citeRegEx": "Paul,? \\Q2013\\E", "shortCiteRegEx": "Paul", "year": 2013}, {"title": "M", "author": ["M. Pilanci", "Wainwright"], "venue": "J.", "citeRegEx": "Pilanci and Wainwright 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Is margin preserved after random projection", "author": ["Shi"], "venue": null, "citeRegEx": "Shi,? \\Q2012\\E", "shortCiteRegEx": "Shi", "year": 2012}, {"title": "and Franc", "author": ["S. Sonnenburg"], "venue": "V.", "citeRegEx": "Sonnenburg and Franc 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan"], "venue": null, "citeRegEx": "Sridharan,? \\Q2008\\E", "shortCiteRegEx": "Sridharan", "year": 2008}, {"title": "V", "author": ["Vapnik"], "venue": "N.", "citeRegEx": "Vapnik 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2012\\E", "shortCiteRegEx": "Yang", "year": 2012}, {"title": "Theory of dual-sparse regularized randomized reduction", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Random projections for classification: A recovery approach", "author": ["Zhang"], "venue": "IEEE Transactions on Information Theory 60(11):7300\u20137316", "citeRegEx": "Zhang,? \\Q2014\\E", "shortCiteRegEx": "Zhang", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we address learning problems for high dimensional data. Previously, oblivious random projection based approaches that project high dimensional features onto a random subspace have been used in practice for tackling highdimensionality challenge in machine learning. Recently, various non-oblivious randomized reduction methods have been developed and deployed for solving many numerical problems such as matrix product approximation, low-rank matrix approximation, etc. However, they are less explored for the machine learning tasks, e.g., classification. More seriously, the theoretical analysis of excess risk bounds for risk minimization, an important measure of generalization performance, has not been established for non-oblivious randomized reduction methods. It therefore remains an open problem what is the benefit of using them over previous oblivious random projection based approaches. To tackle these challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction method for general empirical risk minimizing in machine learning tasks, where the original high-dimensional features are projected onto a random subspace that is derived from the data with a small matrix approximation error. We then derive the first excess risk bound for the proposed non-oblivious randomized reduction approach without requiring strong assumptions on the training data. The established excess risk bound exhibits that the proposed approach provides much better generalization performance and it also sheds more insights about different randomized reduction approaches. Finally, we conduct extensive experiments on both synthetic and real-world benchmark datasets, whose dimension scales to O(10), to demonstrate the efficacy of our proposed approach. Introduction Recently, the scale and dimensionality of data associated with machine learning and data mining applications have seen unprecedented growth, spurring the BIG DATA research and development. Learning from largescale ultrahigh-dimensional data remains a computationally challenging problem. The big size of data not only increases the memory footprint but also increases the computational costs pertaining to optimization. A popular approach for addressing the high-dimensionality challenge is Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. to perform dimensionality reduction. Nowadays, randomized reduction methods are emerging to be attractive for dimensionality reduction. Compared with traditional dimensionality reduction methods (e.g., PCA and LDA), randomized reduction methods (i) can lead to simpler algorithms that are easier to analyze (Mahoney 2011); (ii) can often be organized to exploit modern computational architectures better than classical dimensional reduction methods (Halko, Martinsson, and Tropp 2011); (iii) can be more efficient without loss in efficacy (Paul et al. 2013). Generally, randomized reduction methods can be cast into two types: the first type of methods reduces a set of high-dimensional vectors into a low dimensional space independent of each other. These methods usually sample a random matrix independent of the data and then use it to reduce the dimensionality of the data. The second type of methods projects a set of vectors (in the form of a matrix) onto a subspace such that the original matrix can be well reconstructed from the projected matrix and the subspace. Therefore, the subspace to which the data is projected depends on the original data. These methods have been deployed for solving many numerical problems related to matrices, e.g., matrix product approximation, low-rank matrix approximation, approximate singular value decomposition (Boutsidis and Gittens 2013a; Halko, Martinsson, and Tropp 2011). To differentiate these two types of randomized reduction methods, we refer to the first type as oblivious randomized reduction, and refer to the second type as non-oblivious randomized reduction. We note that in literature oblivious and non-oblivious are used interchangeably with data-independent and data-dependent. Here, we use the terminology commonly appearing in matrix analysis and numerical linear algebra due to that the general excess risk bound depends on the matrix approximation error. However, we have not seen any comprehensive study on the statistical property (in particular the excess risk bound) of these randomized reduction methods applied to risk minimization in machine learning. The excess risk bound measures the generalization performance of a learned model compared to the optimal model from a class that has the best generalization performance. The excess risk bounds facilitate a better understanding of different learning algorithms and have the potential to guide us to design better algorithms (Kukliansky and Shamir 2015). It is worth noting that several studies have been devoted to understanding the theoretical properties of oblivious randomized reduction methods applied to classification and regression problems. For example, (Blum 2005; Shi et al. 2012; Paul et al. 2013) analyzed the preservation of the margin of SVM based classification methods with randomized dimension reduction. (Zhang et al. 2014; Yang et al. 2015; Pilanci and Wainwright 2015) studied the problem from the perspective of optimization. Nonetheless, these results are limited in the sense that (i) they focus on only oblivious randomized reduction where the data is projected onto a random subspace independent of the data; (ii) they depend heavily on strong assumptions of the training data or the problem, e.g., low-rank of the data matrix, linear separability of training examples, or the sparsity of optimal solution, and (iii) some of these results do not directly carry over to the excess risk bounds. To tackle the above challenges, we propose an algorithmic framework for employing non-oblivious randomized reduction (NOR) method to project the original high-dimensional features onto a random subspace that is derived from the original data. We study and establish the excess risk bound of the presented randomized algorithms for risk minimization. Different from previous results for oblivious randomized reduction methods, our theoretical analysis does not require assumptions of the training data or the problem, such as low-rank of the data matrix, linear separability of training examples, and the sparsity of optimal solution. When the data matrix is of low-rank or has a fast spectral decay, the excess risk bound of NOR is much better than that of oblivious randomized reduction based methods. Empirical studies on synthetic and real data sets corroborate the theoretical results and demonstrate the effectiveness of the proposed methods.", "creator": "LaTeX with hyperref package"}}}