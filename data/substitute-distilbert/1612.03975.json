{"id": "1612.03975", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2016", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "abstract": "machine learning about language can be improved by supplying it with limited knowledge and sources of external complexity. we present here comprehensive new version of the linked open data resource conceptnet that is particularly well suited to be used with modern nlp techniques such as sparse embeddings.", "histories": [["v1", "Mon, 12 Dec 2016 23:54:52 GMT  (154kb,D)", "http://arxiv.org/abs/1612.03975v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert speer", "joshua chin", "catherine havasi"], "accepted": true, "id": "1612.03975"}, "pdf": {"name": "1612.03975.pdf", "metadata": {"source": "CRF", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "authors": ["Robert Speer", "Joshua Chin", "Catherine Havasi"], "emails": [], "sections": [{"heading": null, "text": "ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expertcreated resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.\nWhen ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies."}, {"heading": "Introduction", "text": "ConceptNet is a knowledge graph that connects words and phrases of natural language (terms) with labeled, weighted edges (assertions). The original release of ConceptNet (Liu and Singh 2004) was intended as a parsed representation of Open Mind Common Sense (Singh 2002), a crowd-sourced knowledge project. This paper describes the release of ConceptNet 5.5, which has expanded to include lexical and world knowledge from many different sources in many languages.\nConceptNet represents relations between words such as:\n\u2022 A net is used for catching fish. \u2022 \u201cLeaves\u201d is a form of the word \u201cleaf \u201d. \u2022 The word cold in English is studeny\u0301 in Czech. \u2022 O alimento e\u0301 usado para comer [Food is used for eating]. Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIn this paper, we will concisely represent assertions such as the above as triples of their start node, relation label, and end node: the assertion that \u201ca dog has a tail\u201d can be represented as (dog, HasA, tail).\nConceptNet also represents links between knowledge resources. In addition to its own knowledge about the English term astronomy, for example, ConceptNet contains links to URLs that define astronomy in WordNet, Wiktionary, OpenCyc, and DBPedia.\nThe graph-structured knowledge in ConceptNet can be particularly useful to NLP learning algorithms, particularly those based on word embeddings, such as (Mikolov et al. 2013). We can use ConceptNet to build semantic spaces that are more effective than distributional semantics alone.\nThe most effective semantic space is one that learns from both distributional semantics and ConceptNet, using a generalization of the \u201cretrofitting\u201d method (Faruqui et al. 2015). We call this hybrid semantic space \u201cConceptNet Numberbatch\u201d, to clarify that it is a separate artifact from ConceptNet itself.\nConceptNet Numberbatch performs significantly better than other systems across many evaluations of word relatedness, and this increase in performance translates to improvements on downstream tasks such as analogies. On a corpus of SAT-style analogy questions (Turney 2006), its accuracy of 56.1% outperforms other systems based on word embeddings and ties the previous best overall system, Turney\u2019s LRA. This level of accuracy is only slightly lower than the performance of the average human test-taker.\nBuilding word embeddings is not the only application of ConceptNet, but it is a way to apply ConceptNet that achieves clear benefits and is compatible with ongoing research in distributional semantics.\nAfter introducing related work, we will begin by describing ConceptNet 5.5 and its features, show how to use ConceptNet alone as a semantic space and a measure of word relatedness, and then proceed to describe and evaluate the hybrid system ConceptNet Numberbatch on these various semantic tasks."}, {"heading": "Related Work", "text": "ConceptNet is the knowledge graph version of the Open Mind Common Sense project (Singh 2002), a common sense\nar X\niv :1\n61 2.\n03 97\n5v 1\n[ cs\n.C L\n] 1\n2 D\nec 2\n01 6\nknowledge base of the most basic things a person knows. It was last published as version 5.2 (Speer and Havasi 2013).\nMany projects strive to create lexical resources of general knowledge. Cyc (Lenat and Guha 1989) has built an ontology of common-sense knowledge in predicate logic form over the decades. DBPedia (Auer et al. 2007) extracts knowledge from Wikipedia infoboxes, providing a large number of facts, largely focused on named entities that have Wikipedia articles. The Google Knowledge Graph (Singhal 2012) is perhaps the largest and most general knowledge graph, though its content is not freely available. It focuses largely on named entities that can be disambiguated, with a motto of \u201cthings, not strings\u201d.\nConceptNet\u2019s role compared to these other resources is to provide a sufficiently large, free knowledge graph that focuses on the common-sense meanings of words (not named entities) as they are used in natural language. This focus on words makes it particularly compatible with the idea of representing word meanings as vectors.\nWord embeddings represent words as dense unit vectors of real numbers, where vectors that are close together are semantically related. This representation is appealing because it represents meaning as a continuous space, where similarity and relatedness can be treated as a metric. Word embeddings are often produced as a side-effect of a machine learning task, such as predicting a word in a sentence from its neighbors. This approach to machine learning about semantics is sometimes referred to as distributional semantics or distributed word representations, and it contrasts with the knowledge-driven approach of semantic networks or knowledge graphs.\nTwo prominent matrices of embeddings are the word2vec embeddings trained on 100 billion words of Google News using skip-grams with negative sampling (Mikolov et al. 2013), and the GloVe 1.2 embeddings trained on 840 billion words of the Common Crawl (Pennington, Socher, and Manning 2014). These matrices are downloadable, and we will be using them both as a point of comparison and as inputs to an ensemble. Levy, Goldberg, and Dagan (2015) evaluated multiple embedding techniques and the effects of various explicit and implicit hyperparameters, produced their own performant word embeddings using a truncated SVD of words and their contexts, and provided recommendations for the engineering of word embeddings.\nHolographic embeddings (Nickel, Rosasco, and Poggio 2016) are embeddings learned from a labeled knowledge graph, under the constraint that a circular correlation of these embeddings gives a vector representing a relation. This representation seems extremely relevant to ConceptNet. In our attempt to implement it on ConceptNet so far, it has converged too slowly to experiment with, but this could be overcome eventually with some optimization and additional computing power."}, {"heading": "Structure of ConceptNet", "text": ""}, {"heading": "Knowledge Sources", "text": "ConceptNet 5.5 is built from the following sources:\n\u2022 Facts acquired from Open Mind Common Sense (OMCS) (Singh 2002) and sister projects in other languages (Anacleto et al. 2006)\n\u2022 Information extracted from parsing Wiktionary, in multiple languages, with a custom parser (\u201cWikiparsec\u201d)\n\u2022 \u201cGames with a purpose\u201d designed to collect common knowledge (von Ahn, Kedia, and Blum 2006) (Nakahara and Yamada 2011) (Kuo et al. 2009)\n\u2022 Open Multilingual WordNet (Bond and Foster 2013), a linked-data representation of WordNet (Miller et al. 1998) and its parallel projects in multiple languages\n\u2022 JMDict (Breen 2004), a Japanese-multilingual dictionary \u2022 OpenCyc, a hierarchy of hypernyms provided by Cyc\n(Lenat and Guha 1989), a system that represents common sense knowledge in predicate logic\n\u2022 A subset of DBPedia (Auer et al. 2007), a network of facts extracted from Wikipedia infoboxes\nWith the combination of these sources, ConceptNet contains over 21 million edges and over 8 million nodes. Its English vocabulary contains approximately 1,500,000 nodes, and there are 83 languages in which it contains at least 10,000 nodes.\nThe largest source of input for ConceptNet is Wiktionary, which provides 18.1 million edges and is mostly responsible for its large multilingual vocabulary. However, much of the character of ConceptNet comes from OMCS and the various games with a purpose, which express many different kinds of relations between terms, such as PartOf (\u201ca wheel is part of a car\u201d) and UsedFor (\u201ca car is used for driving\u201d)."}, {"heading": "Relations", "text": "ConceptNet uses a closed class of selected relations such as IsA, UsedFor, and CapableOf, intended to represent a relationship independently of the language or the source of the terms it connects.\nConceptNet 5.5 aims to align its knowledge resources on its core set of 36 relations. These generalized relations are similar in purpose to WordNet\u2019s relations such as hyponym and meronym, as well as to the qualia of the Generative Lexicon theory (Pustejovsky 1991). ConceptNet\u2019s edges are directed, but as a new feature in ConceptNet 5.5, some relations are designated as being symmetric, such as SimilarTo. The directionality of these edges is unimportant.\nThe core relations are:\n\u2022 Symmetric relations: Antonym, DistinctFrom, EtymologicallyRelatedTo, LocatedNear, RelatedTo, SimilarTo, and Synonym\n\u2022 Asymmetric relations: AtLocation, CapableOf, Causes, CausesDesire, CreatedBy, DefinedAs, DerivedFrom, Desires, Entails, ExternalURL, FormOf, HasA, HasContext, HasFirstSubevent, HasLastSubevent, HasPrerequisite, HasProperty, InstanceOf, IsA, MadeOf, MannerOf, MotivatedByGoal, ObstructedBy, PartOf, ReceivesAction, SenseOf, SymbolOf, and UsedFor\nDefinitions and examples of these relations appear in a page of the ConceptNet 5.5 documentation1.\nRelations with specific semantics, such as UsedFor and HasPrerequisite, tend to connect common words and phrases, while rarer words are connected by more general relations such as Synonym and RelatedTo.\nAn example of edges in ConceptNet, in a browsable interface that groups them by their relation expressed in natural English, appears in Figure 1."}, {"heading": "Term Representation", "text": "ConceptNet represents terms in a standardized form. The text is Unicode-normalized in NFKC form2 using Python\u2019s unicodedata implementation, lowercased, and split into non-punctuation tokens using the tokenizer in the Python package wordfreq (Speer et al. 2016), which builds on the standard Unicode word segmentation algorithm. The tokens are joined with underscores, and this text is prepended with the URI /c/lang, where lang is the BCP 47 language code3 for the language the term is in. As an example, the English term \u201cUnited States\u201d becomes /c/en/united states.\nRelations have a separate namespace of URIs prefixed with /r, such as /r/PartOf. These relations are given artificial names in English, but apply to all languages. The statement that was obtained in Portuguese as \u201cO alimento e\u0301 usado para comer\u201d is still represented with the relation /r/UsedFor.\nThe most significant change from ConceptNet 5.4 and earlier is in the representation of terms. ConceptNet 5.4 required terms in English to be in lemmatized form, so that, for example, \u201cUnited States\u201d had to be represented as /c/en/unite state. In this representation, \u201cdrive\u201d and \u201cdriving\u201d were the same term, allowing the assertions (car, UsedFor, driving) and (drive, HasPrerequisite, have license) to be connected. ConceptNet 5.5 removes the lemmatizer, and instead relates inflections of words using the FormOf relation. The two assertions above are now linked by the third assertion (driving, FormOf, drive), and both \u201cdriving\u201d and \u201cdrive\u201d can be looked up in ConceptNet.\n1https://github.com/commonsense/ conceptnet5/wiki/Relations\n2http://unicode.org/reports/tr15/ 3https://tools.ietf.org/html/bcp47"}, {"heading": "Vocabulary", "text": "When building a knowledge graph, the decision of what a node should represent has significant effects on how the graph is used. It also has implications that can make linking and importing other resources non-trivial, because different resources make different decisions about their representation.\nIn ConceptNet, a node is a word or phrase of a natural language, often a common word in its undisambiguated form. The word \u201clead\u201d in English is a term in ConceptNet, represented by the URI /c/en/lead, even though it has multiple meanings. The advantage of ambiguous terms is that they can be extracted easily from natural language, which is also ambiguous. This ambiguous representation is equivalent to that used by systems that learn distributional semantics from text.\nConceptNet\u2019s representation allows for more specific, disambiguated versions of a term. The URI /c/en/lead/n refers to noun senses of the word \u201clead\u201d, and is effectively included within /c/en/lead when searching or traversing ConceptNet, and linked to it with the implicit relation SenseOf. Many data sources provide information about parts of speech, allowing us to use this as a common representation that provides a small amount of disambiguation. Further disambiguation is allowed by the URI structure, but not currently used."}, {"heading": "Linked Data", "text": "ConceptNet imports knowledge from some other systems, such as WordNet, into its own representation. These other systems have their own target vocabularies that need to be aligned with ConceptNet, which is usually an underspecified, many-to-many alignment.\nA term that is imported from another knowledge graph will be connected to ConceptNet nodes via the relation ExternalURL, pointing to an absolute URL that represents that term in that external resource. This newly-introduced relation preserves the provenance of the data and enables looking up what the untransformed data was. ConceptNet terms can also be represented as absolute URLs, so this allows ConceptNet to connect bidirectionally to the broader ecosystem of Linked Open Data."}, {"heading": "Applying ConceptNet to Word Embeddings", "text": ""}, {"heading": "Computing ConceptNet Embeddings Using PPMI", "text": "We can represent the ConceptNet graph as a sparse, symmetric term-term matrix. Each cell contains the sum of the weights of all edges that connect the two corresponding terms. For performance reasons, when building this matrix, we prune the ConceptNet graph by discarding terms connected to fewer than three edges.\nWe consider this matrix to represent terms and their contexts. In a corpus of text, the context of a term would be the terms that appear nearby in the text; here, the context is the other nodes it is connected to in ConceptNet. We can calculate word embeddings directly from this sparse matrix by following the practical recommendations of Levy, Goldberg, and Dagan (2015).\nAs in Levy et al., we determine the pointwise mutual information of the matrix entries with context distributional smoothing, clip the negative values to yield positive pointwise mutual information (PPMI), reduce the dimensionality of the result to 300 dimensions with truncated SVD, and combine the terms and contexts symmetrically into a single matrix of word embeddings.\nThis gives a matrix of word embeddings we call ConceptNet-PPMI. These embeddings implicitly represent the overall graph structure of ConceptNet, and allow us to compute the approximate connectedness of any pair of nodes.\nWe can expand ConceptNet-PPMI to restore the nodes that we pruned away, assigning them vectors that are the average of their neighboring nodes."}, {"heading": "Combining ConceptNet with Distributional Word Embeddings", "text": "Having created embeddings from ConceptNet alone, we would now like to create a more robust set of embeddings that represents both ConceptNet and distributional word embeddings learned from text.\nRetrofitting (Faruqui et al. 2015) is a process that adjusts an existing matrix of word embeddings using a knowledge graph. Retrofitting infers new vectors qi with the objective of being close to their original values, q\u0302i, and also close to their neighbors in the graph with edges E, by minimizing this objective function:\n\u03a8(Q) = n\u2211 i=1 \u03b1i\u2016qi \u2212 q\u0302i\u20162 + \u2211 (i,j)\u2208E \u03b2ij\u2016qi \u2212 qj\u20162 \nFaruqui et al. give a simple iterative process to minimize this function over the vocabulary of the original embeddings.\nThe process of \u201cexpanded retrofitting\u201d (Speer and Chin 2016) can optimize this objective over a larger vocabulary, including terms from the knowledge graph that do not appear in the vocabulary of the word embeddings. This effectively sets \u03b1i = 0 for terms whose original values are undefined. We set \u03b2ij according to the weights of the edges in ConceptNet.\nThe particular benefit of expanded retrofitting to ConceptNet is that it can benefit from the multilingual connections in ConceptNet. It learns more about English words via their translations in other languages, and also gives these foreignlanguage terms useful embeddings in the same space as the English terms. The effect is similar to the work of Xiao and Guo (2014), who also propagate multilingual embeddings using crowd-sourced Wiktionary entries.\nWe add one more step to retrofitting, which is to subtract the mean of the vectors that result from retrofitting, then renormalize them to unit vectors. Retrofitting has a tendency to move all vectors closer to the vectors for highly-connected terms such as \u201cperson\u201d. Subtracting the mean helps to ensure that terms remain distinguishable from each other."}, {"heading": "Combining Multiple Sources of Embeddings", "text": "Retrofitting can be applied to any existing matrix of word embeddings, without needing access to the data that was\nused to train them. This is particularly useful because it allows building on publicly-released matrices of embeddings whose input data is unavailable or difficult to acquire.\nAs described in the \u201cRelated Work\u201d section, word2vec and GloVe both provide recommended pre-trained matrices. These matrices represent somewhat different domains of text and have complementary strengths, and the way that we can benefit from them the most is by taking both of them as input.\nTo do this, we apply retrofitting to both matrices, then find a globally linear projection that aligns the results on their common vocabulary. This process was inspired by Zhao, Hassan, and Auli (2015). We find the projection by concatenating the columns of the matrices and reducing them to 300 dimensions using truncated SVD. We then use this alignment to infer compatible embeddings for terms that are missing from one of the vocabularies.\nIn ongoing work, we are experimenting with additionally including distributional word embeddings from corpora of non-English text in this merger. Preliminary results show that this improves the multilingual performance of the embeddings.\nAfter retrofitting and merging, we have a labeled matrix of word embeddings whose vocabulary is derived from word2vec, GloVe, and the pruned ConceptNet graph. As in ConceptNet-PPMI, we re-introduce all the nodes from ConceptNet by looking up and averaging their neighboring nodes."}, {"heading": "Evaluation", "text": "To compare the performance of fully-built systems of word embeddings, we will first compare their results on intrinsic evaluations of word relatedness, then apply the word embeddings to the downstream tasks of solving proportional analogies and choosing the sensible ending to a story, to evaluate whether better embeddings translate to better performance on semantic tasks.\nThe hybrid system described above is the system we name ConceptNet Numberbatch, with the version number 16.09 indicating that it was built in September 2016. We now compare results from ConceptNet Numberbatch 16.09 to other systems that make their word embeddings available, both those that were used in building ConceptNet Numberbatch and a recently-released system, LexVec, that was not. The systems we evaluate are:\n\u2022 word2vec SGNS (Mikolov et al. 2013), trained on Google News text\n\u2022 GloVe 1.2 (Pennington, Socher, and Manning 2014), trained on the Common Crawl\n\u2022 LexVec (Salle, Idiart, and Villavicencio 2016), trained on the English Wikipedia and NewsCrawl 2014\n\u2022 ConceptNet-PPMI, described here and trained on ConceptNet 5.5 alone\n\u2022 ConceptNet Numberbatch 16.09, the hybrid of ConceptNet 5.5, word2vec, and GloVe described here"}, {"heading": "Evaluations of Word Relatedness", "text": "One way to evaluate the intrinsic performance of a semantic space is to ask it to rank the relatedness of pairs of words, and compare its judgments to human judgments.4 If one word in a pair is out-of-vocabulary, the pair is assumed to have a relatedness of 0. A good semantic space will provide a ranking of relatedness that is highly correlated with the human gold-standard ranking, as measured by its Spearman correlation (\u03c1).\nMany gold standards of word relatedness are in common use. Here, we focus on MEN-3000 (Bruni, Tran, and Baroni 2014), a large crowd-sourced ranking of common words; RW (Luong, Socher, and Manning 2013), a ranking of rare words; WordSim-353 (Finkelstein et al. 2001), a smaller evaluation that has been used as a benchmark for many methods; and MTurk-771 (Halawi et al. 2012), another crowd-sourced evaluation of a variety of words.\nTo avoid manually overfitting by designing our semantic space around a particular evaluation, we experimented using smaller development sets, holding out some test data until it was time to include results in this paper:\n\u2022 MEN-3000 is already divided into a 2000-item development set and a 1000-item test set. We use the results from the test set as the final results.\n\u2022 RW has no standard dev/test breakdown. We sampled 2/3 of its items as a development set and held out the other 1/3 (every third line of the file, starting with the third).\n\u2022 We used all of WordSim-353 in development. We examine its results both in English and in its Spanish translation (Hassan and Mihalcea 2009).\n\u2022 We did not use MTurk-771 in development, holding out the entire set as a final test, showing that ConceptNet Numberbatch performs well on a previously-unseen evaluation.\nWe use the Spanish WordSim-353 as an example of a prominent non-English evaluation, indicating that expanded retrofitting is sufficient to learn vectors for non-English languages, even when all the distributional semantics takes place in English. However, a thorough multilingual evaluation is beyond the scope of this paper; the systems we compare to have only made English vectors available, and it would add considerable complexity to the evaluation to reproduce other systems of multilingual embeddings, accounting for their various ways of handling morphology and OOV words."}, {"heading": "Solving SAT-style Analogies", "text": "Proportional analogies are statements of the form \u201ca1 is to b1 as a2 is to b2\u201d. The task of filling in missing values of a proportional analogy was common until recently on standardized tests such as the SAT. Now, it is popular as a way to show that a semantic space can approximate relationships\n4It is sometimes important to distinguish similarity from relatedness. For example, the term \u201ccoffee\u201d is related to \u201cmug\u201d, but coffee is not similar to a mug. What a machine can learn from the connectivity of ConceptNet is focused on relatedness.\nbetween words, even without taking explicit relationships into account.\nMuch of the groundwork for evaluating systems\u2019 ability to solve proportional analogies was laid by Peter Turney, including his method of Latent Relational Analysis (Turney 2006), which was quite effective at solving proportional analogies by repeatedly searching the Web for the words involved in them. A newer method called SuperSim (Turney 2013) does not require Web searching. These methods are evaluated on a dataset of 374 SAT questions that Turney and his collaborators have collected.\nMany of the best results on this evaluation have been achieved by Turney in his own work. One interesting system not by Turney is BagPack (Herdag\u030cdelen and Baroni 2009), which could learn about analogies either from unstructured text or from ConceptNet 4.\nSolving analogies over word embeddings is often described as comparing the difference b2 \u2212 a2 to b1 \u2212 a1 (Mikolov et al. 2013), but for the task of filling in the best pair for a2 and b2, it helps to take advantage of more of the structure of the question to provide more constraint than this single comparison.\nIn a sensible analogy, the words on the right side of the analogy will be related in some way to the words on the left side, so we should aim for some amount of relatedness between a1 and a2, and between b1 and b2, regardless of what the other terms are. Also, in many cases, a satisfying analogy will still make sense when it is transposed to \u201ca1 is to a2 as b1 is to b2\u201d. The analogy \u201cfire : hot :: ice : cold\u201d, for example, can be transposed to \u201cfire : ice :: hot : cold\u201d. Recognizing this structure helps in picking the best answer to difficult analogy questions.\nThis gives us three components that we can weigh to evaluate whether a pair (a2, b2) completes an analogy: their separate similarity to a1 and b1, the dot product of differences between the pairs, and the dot product of differences between the transposed pairs. The total weight does not matter, so we can put these together into a vector equation with two free parameters:\ns = a1 \u00b7 a2 + b1 \u00b7 b2 + w1(b2 \u2212 a2) \u00b7 (b1 \u2212 a1) + w2(b2 \u2212 b1) \u00b7 (a2 \u2212 a1)\nThe appropriate values of w1 and w2 depend on the nature of the relationships in the analogy questions, and also on how these relationships appear in the vector space. We optimize these parameters separately for each system we test, using grid search over a number of possible values so that each system can achieve its best performance. The grid search is performed on odd-numbered questions, holding out the even-numbered questions as a test set.\nThe weights found for ConceptNet Numberbatch 16.09 were w1 = 0.2 and w2 = 0.6. This indicates, surprisingly, that the comparisons being made by the transposed form of the analogy were often more important than the directly stated form of the analogy for choosing the best answer pair."}, {"heading": "An Evaluation of Common-Sense Stories", "text": "The Story Cloze Test (Mostafazadeh et al. 2016) is a recent evaluation of semantic understanding that tests whether a"}, {"heading": "Evaluation Dev Test Final", "text": "method can choose the sensible ending to a simple story. Prompts consist of four sentences that tell a story, and two choices are provided for a fifth sentence that concludes the story, only one of which makes sense.\nThis task is distinguished by being very challenging for computers but very easy for humans, because of the extent that it relies on implicit, common sense knowledge. Most systems that have been evaluated on the Story Cloze Test score only marginally above the random baseline of 50%, while human agreement is near 100%.\nOur preliminary attempt to apply ConceptNet Numberbatch to the Story Cloze Test is to use a very simple \u201cbag-ofvectors\u201d model, by averaging the embeddings of the words in the sentence and choosing the ending whose average is closest. This allows us to compare directly to one of the original results presented by Mostafazadeh et al., in which a bag of vectors using GenSim\u2019s implementation of word2vec scores 53.9% on the test set.\nThis bag-of-vectors model uses no knowledge of how one event might sensibly follow from another, only which words are related in context. Improving the score of this model should not be portrayed as actual \u201cstory understanding\u201d, but it recognizes that sensible stories do not suddenly change topic."}, {"heading": "Results and Discussion", "text": ""}, {"heading": "Word Relatedness", "text": "Figure 2 compares the performance of the systems we compared across all evaluations. For word-relatedness evaluations, the Y-axis represents the Spearman correlation (\u03c1), using the Fisher transformation to compute a 95% confidence interval that assumes the given word pairs are sampled from\nan unobservable larger set (Bonett and Wright 2000). For the analogy and story evaluations, the Y-axis is simply the proportion of questions answered correctly, with 95% confidence intervals calculated using the binomial exact test.\nThe scores of our system on all these evaluations appear in Table 1, including a development/test breakdown that shows no apparent overfitting. The \u201cFinal\u201d column is meant for comparisons to other papers and used in the graph. It uses the standard test set that other publications use, if it exists (which is the case for MEN-3000 and Story Cloze), or all of the data otherwise.\nOn all of the four word-relatedness evaluations, ConceptNet Numberbatch 16.09 (the complete system described in this paper) is state of the art, performing better than all other systems evaluated to an extent that exceeds the confidence interval of the choice of questions. Its high scores on both the Rare Words dataset and the crowd-sourced MEN-3000 and MTurk-771 datasets, exceeding the performance of other embeddings with high confidence, shows both the breadth and the depth of its understanding of words."}, {"heading": "SAT Analogies", "text": "ConceptNet Numberbatch performed the best among the word-embedding systems at SAT analogies, getting 56.1% of the questions correct (58.8% on the half that was held out for final testing). These analogy results outperform analogies based on other word embeddings, when evaluated in the same framework, as shown by Figure 2.\nThe analogy results also tie or slightly outperform the performance of best-in-class systems on this evaluation5. Table 2 compares our results to the other systems introduced in the \u201cSolving SAT-Style Analogies\u201d section: BagPack (Herdag\u030cdelen and Baroni 2009), the previous use of ConceptNet on this evaluation; LRA (Turney 2006), the system whose record has stood for a decade, which spends nine days searching the Web during its evaluation; and SuperSim (Turney 2013), the more recent system that held the record among self-contained systems. We also include the optimized results we found for word2vec (Mikolov et al. 2013), which scored best among other word-embedding systems on this task.\nThe results of three systems \u2013 SuperSim, LRA, and our ConceptNet Numberbatch \u2013 are all within each other\u2019s 95%\n5See http://www.aclweb.org/aclwiki/index. php?title=SAT_Analogy_Questions for a thorough list of results.\nconfidence intervals, indicating that the ranking of the results could easily change with a different selection of questions. Our score of 56.1% is also within the confidence interval of the performance of the average human college applicant on these questions, said to be 57.0% (Turney 2006).\nWe have shown that knowledge-informed word embeddings are up to the challenge of real SAT analogies; they perform the same as or slightly better than non-wordembedding systems on the same evaluation, when other word embeddings perform worse. In practice, recent word embeddings have instead been evaluated on simpler, synthetic analogy data sets (Mikolov et al. 2013), and have not usually been compared to existing non-embedding-based methods of solving analogies.\nWe achieve this performance even though the system, like other systems that form analogies from word embeddings, is only adding and subtracting values that measure the relatedness of terms; it uses no particular representation of what the relationships between these terms actually are. There is likely a way to take ConceptNet\u2019s relation labels into account and perform even better at analogies."}, {"heading": "Story Cloze Test", "text": "The performance of our system on the Story Cloze Test was acceptable but unremarkable. ConceptNet Numberbatch chose the correct ending 59.4% of the time, which is in fact slightly better than any results reported by Mostafazadeh et al. (2016), including neural nets trained on the task. However, we could also achieve a similar score by using the same bag-of-vectors approach on other word embeddings. The best score of 59.9% was achieved by LexVec, with ConceptNet Numberbatch, GloVe, and word2vec all within its confidence interval.\nThis result should perhaps be comforting to those who aim to improve the computational understanding of stories. A bag-of-vectors approach may be marginally more successful at choosing the correct ending to a story than other approaches, but the performance of this approach has likely reached a plateau. It seems that any sufficiently high-quality word embeddings can choose the correct ending about 59% of the time, based on nothing but the assumption that the end of a story should be similar to the rest of it. Consider this a baseline: any representation designed to usefully represent the events in stories should get more than 59% correct."}, {"heading": "Conclusion", "text": "We have compared word embeddings that represent only distributional semantics (word2vec, GloVe, and LexVec), word embeddings that represent only relational knowledge (ConceptNet PPMI), and the combination of the two (ConceptNet Numberbatch), and we have shown that the whole is more than the sum of its parts.\nConceptNet continues to be important in a field that has come to focus on word embeddings, because word embeddings can benefit from what ConceptNet knows. ConceptNet can make word embeddings more robust and more correlated with human judgments, as shown by the state-of-theart results that ConceptNet Numberbatch achieves at matching human annotators on multiple evaluations.\nAny technique built on word embeddings should consider including a source of relational knowledge, or starting from a pre-trained set of word embeddings that has taken relational knowledge into account. One of the many goals of ConceptNet is to provide this knowledge in a convenient form that can be applied across many domains and many languages."}, {"heading": "Availability of the Code and Data", "text": "The code and documentation of ConceptNet 5.5 can be found on GitHub at https://github.com/ commonsense/conceptnet5, and the knowledge graph can be browsed at http://conceptnet.io. The full build process, as well as the evaluation graph, can be reproduced using the instructions included in the README file for using Snakemake, a build system for data science (Ko\u0308ster and Rahmann 2012), and optionally using Docker Compose to reproduce the system environment. The version of the repository as of the submission of this paper has been tagged as aaai2017.\nThe ConceptNet Numberbatch word embeddings that resulted from this build process in September 2016 are the ones evaluated in this paper; they can be downloaded as pre-built embeddings from https://github.com/ commonsense/conceptnet-numberbatch, tag 16.09."}, {"heading": "Acknowledgments", "text": "We would like to thank the tens of thousands of volunteers who provided the crowd-sourced knowledge that makes ConceptNet possible. This includes contributors to Open Mind Common Sense and its related projects, as well as contributors to Wikipedia and Wiktionary, who are improving the state of knowledge for humans and computers alike."}], "references": [{"title": "Can common sense uncover cultural differences in computer applications? In Artificial intelligence in theory and practice", "author": ["Anacleto"], "venue": null, "citeRegEx": "Anacleto,? \\Q2006\\E", "shortCiteRegEx": "Anacleto", "year": 2006}, {"title": "DBpedia: A nucleus for a web of open data", "author": ["Auer"], "venue": null, "citeRegEx": "Auer,? \\Q2007\\E", "shortCiteRegEx": "Auer", "year": 2007}, {"title": "and Foster", "author": ["F. Bond"], "venue": "R.", "citeRegEx": "Bond and Foster 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["D.G. Bonett", "Wright"], "venue": "A.", "citeRegEx": "Bonett and Wright 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal distributional semantics", "author": ["Tran Bruni", "E. Baroni 2014] Bruni", "N.-K. Tran", "M. Baroni"], "venue": "J. Artif. Intell. Res", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "N", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "Smith"], "venue": "A.", "citeRegEx": "Faruqui et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Finkelstein"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein", "year": 2001}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["Halawi"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Halawi,? \\Q2012\\E", "shortCiteRegEx": "Halawi", "year": 2012}, {"title": "and Mihalcea", "author": ["S. Hassan"], "venue": "R.", "citeRegEx": "Hassan and Mihalcea 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Baroni", "author": ["A. Herda\u01e7delen"], "venue": "M.", "citeRegEx": "Herda\u01e7delen and Baroni 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Rahmann", "author": ["J. K\u00f6ster"], "venue": "S.", "citeRegEx": "K\u00f6ster and Rahmann 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "J", "author": ["Y.-L. Kuo", "J.-C. Lee", "K.-Y. Chiang", "R. Wang", "E. Shen", "C.-W. Chan", "Hsu"], "venue": "Y.-J.", "citeRegEx": "Kuo et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "R", "author": ["D.B. Lenat", "Guha"], "venue": "V.", "citeRegEx": "Lenat and Guha 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3:211\u2013225", "author": ["Goldberg Levy", "O. Dagan 2015] Levy", "Y. Goldberg", "I. Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "and Singh", "author": ["H. Liu"], "venue": "P.", "citeRegEx": "Liu and Singh 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "C", "author": ["M.-T. Luong", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Luong. Socher. and Manning 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space. CoRR abs/1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "author": ["Mostafazadeh"], "venue": "In Proceedings of NAACL: Human Language Technologies,", "citeRegEx": "Mostafazadeh,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh", "year": 2016}, {"title": "and Yamada", "author": ["K. Nakahara"], "venue": "S.", "citeRegEx": "Nakahara and Yamada 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Rosasco Nickel", "M. Poggio 2016] Nickel", "L. Rosasco", "T. Poggio"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "C", "author": ["J. Pennington", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Pennington. Socher. and Manning 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhancing the LexVec distributed word representation model using positional contexts and external memory. arXiv preprint arXiv:1606.01283", "author": ["Idiart Salle", "A. Villavicencio 2016] Salle", "M. Idiart", "A. Villavicencio"], "venue": null, "citeRegEx": "Salle et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salle et al\\.", "year": 2016}, {"title": "and Chin", "author": ["R. Speer"], "venue": "J.", "citeRegEx": "Speer and Chin 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Havasi", "author": ["R. Speer"], "venue": "C.", "citeRegEx": "Speer and Havasi 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Verbosity: a game for collecting common-sense facts", "author": ["Kedia von Ahn", "L. Blum 2006] von Ahn", "M. Kedia", "M. Blum"], "venue": "In Proceedings of the SIGCHI conference on Human Factors in computing systems,", "citeRegEx": "Ahn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2006}, {"title": "and Guo", "author": ["M. Xiao"], "venue": "Y.", "citeRegEx": "Xiao and Guo 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning translation models from monolingual continuous representations", "author": ["Hassan Zhao", "K. Auli 2015] Zhao", "H. Hassan", "M. Auli"], "venue": "In Proceedings of NAACL", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expertcreated resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.", "creator": "LaTeX with hyperref package"}}}