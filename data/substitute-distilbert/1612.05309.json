{"id": "1612.05309", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2016", "title": "Multi-Agent Path Finding with Delay Probabilities", "abstract": "several recently developed multi - agent path finding ( mapf ) solvers scale to large mapf instances by searching for mapf plans span 2 levels : the high - visibility search resolves edges between agents, but the low - level search plans paths for single agents under numerical constraints imposed by the high - level search. we emphasize the following contributions to solve the mapf problem with static plan execution with small average makespans : first, maybe update the stability problem with delay probabilities ( mapf - dp ), define valid mapf - dp losses and propose the use of robust plan - execution policy encoding valid mapf - effect plans to control accordingly each agent proceeds along its path. second, we discuss 2 classes of decentralized robust plan - execution policies ( called fully synchronized policies and structured communication policies ) that prevent instability during plan analysis for symmetric mapf - dp plans. third, we present a 2 - level mapf - dp solver ( called approximate minimization in expectation ) ) generates valid mapf - dp claims.", "histories": [["v1", "Thu, 15 Dec 2016 23:33:41 GMT  (178kb,D)", "http://arxiv.org/abs/1612.05309v1", "To appear in AAAI 2017"]], "COMMENTS": "To appear in AAAI 2017", "reviews": [], "SUBJECTS": "cs.AI cs.MA cs.RO", "authors": ["hang ma 0001", "t k satish kumar", "sven koenig"], "accepted": true, "id": "1612.05309"}, "pdf": {"name": "1612.05309.pdf", "metadata": {"source": "META", "title": "Multi-Agent Path Finding with Delay Probabilities", "authors": ["Hang Ma", "T. K. Satish Kumar", "Sven Koenig"], "emails": ["hangma@usc.edu", "tkskwork@gmail.com", "skoenig@usc.edu"], "sections": [{"heading": "Introduction", "text": "Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths for a given number of agents from their given start locations to their given goal locations in a given environment. MAPF problems arise for aircraft towing vehicles (Morris et al. 2016), office robots (Veloso et al. 2015), video game characters (Silver 2005) and warehouse robots (Wurman, D\u2019Andrea, and Mountz 2008), among others.\nSeveral recently developed MAPF solvers scale to large MAPF instances. However, agents typically cannot execute their MAPF plans perfectly since they often traverse their paths more slowly than intended. Their delay probabilities can be estimated but current MAPF solvers do not use this information, which often leads to frequent and runtimeintensive replanning or plan-execution failures.\nWe thus formalize the MAPF Problem with Delay Probabilities (MAPF-DP), where each agent traverses edges on\n\u2217Our research was supported by NSF under grant numbers 1409987 and 1319966. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the sponsoring organizations, agencies or the U.S. government. Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nan undirected graph (that models the environment) to move from its start vertex to its goal vertex. At any discrete time step, the agent can either execute 1) a wait action, resulting in it staying in its current vertex, or 2) a move action with the intent of traversing an outgoing edge of its current vertex, resulting in it staying in its current vertex with the delay probability and traversing the edge otherwise. The MAPFDP problem is the problem of finding 1) a MAPF-DP plan that consists of a path for each agent from its start vertex to its goal vertex (given by a sequence of wait and move actions) and 2) a plan-execution policy that controls with GO or STOP commands how each agent proceeds along its path such that no collisions occur during plan execution. There are 2 kinds of collisions, namely vertex collisions (where 2 agents occupy the same vertex at the same time step) and edge collisions (where 2 agents traverse the same edge in opposite directions at the same time step).\nWe make the following contributions to solve the MAPFDP problem with small average makespans: First, we formalize the MAPF-DP problem, define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans."}, {"heading": "Background and Related Work", "text": "The MAPF problem is NP-hard to solve optimally for flowtime minimization and to approximate within any constant factor less than 4/3 for makespan minimization (Ma et al. 2016). Search-based MAPF solvers can be optimal, bounded suboptimal or suboptimal (Standley 2010; Luna\nar X\niv :1\n61 2.\n05 30\n9v 1\n[ cs\n.A I]\n1 5\nD ec\n2 01\n6\nand Bekris 2011; Wang and Botea 2011; Goldenberg et al. 2014; Sharon et al. 2013; 2015; Boyarski et al. 2015; Wagner and Choset 2015; Ma and Koenig 2016; Cohen et al. 2016). Current MAPF solvers typically assume perfect plan execution. However, utilizing probabilistic information about imperfect plan execution can reduce frequent and time-intensive replanning and plan-execution failures.\nPartially Observable Markov Decision Processes (POMDPs) are a general probabilistic planning framework. The MAPF-DP problem can be solved with POMDPs but this is tractable only for very few agents in very small environments since the size of the state space is proportional to the size of the environment to the power of the number of agents and the size of the belief space is proportional to the size of the state space to the power of the length of the planning horizon (Kurniawati, Hsu, and Lee 2008; Ma and Pineau 2015). Several specialized probabilistic planning frameworks, such as transition-independent decentralized Markov Decision Processes (DecMDPs) (Becker et al. 2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs. In transition-independent Dec-MDPs, the local state of each agent depends only on its previous local state and the action taken by it (Goldman and Zilberstein 2004). MAPF-DP is indeed transition independent. However, there are interactions among agents since the reward of each agent depends on whether it is involved in a collision and thus on the local states of other agents and the actions taken by them. Fully decentralized probabilistic planning frameworks thus cannot prevent collisions. Fully centralized probabilistic planning frameworks can prevent collisions but are more runtime-intensive and can thus scale poorly. For example, the MAPF-DP problem can be solved with transition-independent MMDPs (Scharpff et al. 2016). In fact, the most closely related research to ours is that on approximating MMDPs (Liu and Michael 2016) although it handles different types of dynamics than we do. The runtime of probabilistic planning frameworks can be reduced by exploiting the problem structure, including when interactions among agents are sparse. For example, decentralized sparse-interaction Markov Decision Processes (Dec-SIMDPs) (Melo and Veloso 2011) assume that interactions among agents occur only in well-defined interaction areas in the environment (which is not the case for MAPF-DP in general), but typically still do not scale to more than 10 agents. The model shaping technique for decentralized POMDPs (Velagapudi et al. 2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions."}, {"heading": "Problem Definition: Planning", "text": "A MAPF-DP instance is characterized by an undirected graph G = (V,E) whose vertices V correspond to locations and whose edges E correspond to transitions between locations. We are given m agents a1, a2 . . . am. Each agent\nai has a unique start vertex si \u2208 V , a unique goal vertex gi \u2208 V and a delay probability pi \u2208 (0, 1). A path for agent ai is expressed by a function li that maps each time index x = 0, 1 . . . Xi to a vertex li(x) \u2208 V such that li(0) = si, consecutive vertices li(x) and li(x + 1) are either identical (when agent ai is scheduled to execute a wait action) or connected by an edge (when agent ai is scheduled to execute a move action from vertex li(x) to vertex li(x + 1)) and li(Xi) = gi. A MAPF plan consists of a path for each agent."}, {"heading": "Problem Definition: Plan Execution", "text": "The local state xti of agent i at time step t = 0, 1 . . .\u221e during plan execution is a time index. We set x0i := 0 and always update its local state such that it is in vertex li(x t i) at time step t. The agent knows its current local state and receives messages from some of the other agents about their local states. At each time step, its plan-execution policy maps this knowledge to one of the commands GO or STOP that control how it proceeds along its path.\n1. If the command is GO at time step t: (a) If xti = Xi, then agent ai executes no action and\nremains in its current vertex li(xti) since it has entered its last local state (and thus the end of its path). We thus update its local state to xt+1i := x t i. (b) If xti 6= Xi and li(xti) = li(xti + 1), then agent ai executes a wait action to remain in its current vertex li(x t i). The execution of wait actions never fails. We\nthus update its local state to xt+1i := x t i + 1 (success).\n(c) If xti 6= Xi and li(xti) 6= li(xti + 1), then agent ai executes a move action from its current vertex li(xti) to vertex li(xti + 1). The execution of move actions fails with delay probability pi with the effect that the agent executes no action and remains delayed in its current vertex li(xti). We thus update its local state to x t+1 i :=\nxti with probability pi (failure) and x t+1 i := x t i +1 with\nprobability 1\u2212 pi (success). 2. If the command is STOP at time step t, then agent ai\nexecutes no action and remains in its current vertex li(xti). We thus update its local state to xt+1i := x t i.\nOur objective is to find a combination of a MAPF plan and a plan-execution policy with small average makespan, which is the average earliest time step during plan execution when all agents have entered their last local states. The MAPF problem is a special case where the delay probabilities of all agents are zero and the plan-execution policies always provide GO commands."}, {"heading": "Valid MAPF-DP Plans", "text": "Definition 1. A valid MAPF-DP plan is a plan with 2 properties:\n1. \u2200i, j, x with i 6= j : li(x) 6= lj(x) [two agents are never scheduled to be in the same vertex at the same time index, that is, the vertices of two agents in the same local state are different]. 2. \u2200i, j, x with i 6= j : li(x + 1) 6= lj(x) [an agent is never scheduled to be in a vertex at a time index x+1 when any other agent is scheduled to be in the same vertex at time index x, that is, the vertex of an agent in a local state x+1\nhas to be different from the vertex of any other agent in local state x].\nFigure 1 shows a sample MAPF-DP instance where the blue agent a1 has to move from its start vertex v3 to its goal vertex v4 and the red agent a2 has to move from its start vertex v2 to its goal vertex v5. Agent a1 has to move north to let agent a2 pass. The paths l1 = \u3008v3, v1, v1, v1, v3, v4\u3009 and l2 = \u3008v2, v2, v3, v4, v5\u3009 form a valid MAPF-DP plan. However, the paths l1 = \u3008v3, v1, v1, v3, v4\u3009 and l2 = \u3008v2, v3, v4, v5\u3009 a valid MAPF plan but not a valid MAPF-DP plan since l2(1) = l1(0) = v3 violates Property 2.\nProperty 1 of Definition 1 is necessary to be able to execute valid MAPF-DP plans without vertex collisions because two agents could otherwise be in the same vertex at the same time step (under perfect or imperfect plan execution). Property 2 is also necessary because an agent could otherwise enter the vertex of some other agent that unsuccessfully tries to leave the same vertex at the same time step (under imperfect plan execution). Property 2 is also necessary to be able to execute valid MAPF-DP plans without edge collisions (under perfect or imperfect plan execution)."}, {"heading": "Robust Plan-Execution Policies", "text": "We study 2 kinds of decentralized robust plan-execution policies for valid MAPF-DP plans, which are plan-execution policies that prevent all collisions during the imperfect plan execution of valid MAPF-DP plans."}, {"heading": "Fully Synchronized Policies (FSPs)", "text": "Fully Synchronized Policies (FSPs) attempt to keep all agents in lockstep as much as possible by providing a GO command to an agent if and only if the agent has not yet entered its last local state and all other agents have either entered their last local states or have left all local states that precede the local state of the agent itself. FSPs can be implemented easily if each agent sends a message to all other agents when it enters a new local state. An agent can implement its FSP simply by counting how many messages it has received from each other agent and providing a GO command to itself in local state x if and only if it has not yet entered its last local state and has received x messages over the course of plan execution from each other agent."}, {"heading": "Minimal Communication Policies (MCPs)", "text": "FSPs have 2 drawbacks. First, agents wait unnecessarily, which results in large average makespans. Second, each agent always needs to know the local states of all other agents, which results in many sent messages. Property 2 of Definition 1 suggests that robust plan-execution policies for valid MAPF-DP plans could provide a GO command to an agent if and only if the agent has not yet entered its last local state and all other agents have left all local states that precede the local state of the agent itself and whose vertices are the same as the vertex of the next local state of the agent itself. This way, it is guaranteed that the vertex of the next local state of the agent is different from the vertices of all other agents in their current local states. Minimal Communication\nPolicies (MCPs) address these drawbacks by identifying such critical dependencies between agents and obeying them during plan execution, an idea that originated in the context of centralized non-robust plan-execution policies (Ho\u0308nig et al. 2016).\nThe local state of an agent ai at any time step during plan execution is a time index x. Since we need to relate the local states of different agents, we use li(x) in the following not only to refer to the vertex assigned to local state x of agent ai but also to the local state x of agent ai itself (instead of x), depending on the context.\nEvery valid MAPF-DP plan defines a total order on the local states of all agents, which we relax to a partial order\u2192 as follows:\n1. \u2200i, x : li(x) \u2192 li(x + 1) [agent ai enters a local state x+ 1 during plan execution only after it enters local state x]. 2. \u2200i, j, x, x\u2032 with i 6= j, x\u2032 < x and l = lj(x\u2032) = li(x+1) : lj(x\n\u2032 + 1)\u2192 li(x+ 1) [agent ai enters a local state x+ 1 with a vertex l during plan execution only after agent aj has left a local state x\u2032 with vertex l (and thus entered local state x\u2032 + 1) that precedes local state x]. Property 1 of the partial order enforces that each agent visits its locations in the same order as in the MAPFDP plan. Property 2 enforces that any two agents visit the same location in the same order as in the MAPFDP plan. We can express the partial order with a directed graph G = (V, E) whose vertices correspond to local states and whose edges correspond to the partial order given by the two properties above. Property 2 specifies the critical dependencies between agents. Edges are redundant and can then be removed from the directed graph when they are implied by the other edges due to transitivity. A transitive reduction of the directed graph minimizes the number of remaining edges. It can be computed in time O(|V||E|) (Aho, Garey, and Ullman 1972), is unique, contains all edges between local states of the same agent (since they are never redundant) and thus minimizes the number of edges between the local states of different agents.\nMCPs can be implemented easily if each agent aj sends a message to each other agent ai when agent aj enters a new local state x\u0304\u2032 (= x\u2032 + 1 in Property 2) if and only if the transitive reduction contains an edge lj(x\u0304\u2032) \u2192 li(x\u0304) for some local state x\u0304 (= x + 1 in Property 2) of agent ai. Since the transitive reduction minimizes the number of edges between the local states of different agents, it also minimizes the number of sent messages. An agent ai can implement its MCP simply by counting how many messages it has received from each other agent and providing a GO command to itself in local state x if and only if it has not yet entered its last local state and has received a number of messages over the course of plan execution from each other agent aj that corresponds to the number of incoming edges from local states of agent aj to its local states 0, 1 . . . x + 1.\nFigure 2 shows a sample partial order on the local states for the MAPF-DP instance from Figure 1 and its valid MAPF-DP plan l1 = \u3008v3, v1, v3, v1, v1, v1, v3, v4\u3009 and l2 = \u3008v2, v2, v2, v2, v3, v4, v5\u3009. l1(1) \u2192 l2(4), for example, is implied by l1(1) \u2192 l1(2) \u2192 l1(3) \u2192 l2(4) and can thus\nbe removed from the directed graph. Figure 3 shows the resulting transitive reduction, which implies that agent a2 has to wait in local state 3 until it has received one message from agent a1 during the course of plan execution but can then proceed through all future local states without waiting."}, {"heading": "Properties of FSPs and MCPs", "text": "Both FSPs and MCPs do not result in deadlocks during the plan execution of valid MAPF-DP plans because there always exists at least one agent that is provided a GO command before all agents have entered their last local states (namely an agent with the smallest local state among all agents that have not yet entered their last local states since an agent can wait only for other agents with smaller local states).\nBoth FSPs and MCPs are robust plan-execution policies due to Properties 1 and 2 of valid MAPF-DP plans. We now provide a proof sketch for the robustness of MCPs.\nFirst, consider a valid MAPF-DF plan and assume that li(x) = lj(y) for two agents ai and aj with i 6= j. Then, 1) y 6= x since li(x) 6= lj(x) according to Property 1 of Definition 1 and 2) y 6= x + 1 since lj(x + 1) 6= li(x) according to Property 2 of Definition 1 (State Property).\nSecond, we show by contradiction that no vertex collisions can occur during plan execution. Assume that a vertex collision occurs between agents ai and aj with i 6= j when agent ai is in local state x and agent aj is in local state y. Assume without loss of generality that x \u2264 y. Then, li(x+1)\u2192 lj(y) according to Property 2 of the partial order \u2192 since li(x) = lj(y) according to our vertex collision assumption and x < y \u2212 1 according to the State Property. Thus, agent aj can leave local state y \u2212 1 only when agent ai reaches local state x+1, which is a contradiction with the vertex collision assumption.\nThird, we show by contradiction that no edge collisions can occur during plan execution. Assume that an edge collision between agents ai and aj with i 6= j occurs when agent ai changes its local state from x to x + 1 and agent aj changes its local state from y to y + 1. Assume without loss of generality that x \u2264 y. Case 1) If x = y, then\nli(x) = lj(y + 1) = lj(x + 1), which is a contradiction with the State Property. Case 2) If x < y, then li(x + 1) \u2192 lj(y+1) according to Property 2 of the partial order\u2192 since li(x) = lj(y+1) according to our edge collision assumption and x < y according to the case assumption. Thus, agent aj can leave local state y only when agent ai reaches local state x + 1, which is a contradiction with the edge collision assumption."}, {"heading": "Approximate Minimization in Expectation", "text": "MCPs are robust plan-execution policies for valid MAPFDP plans that do not stop agents unnecessarily and result in few sent messages. We present a MAPF-DP solver, called Approximate Minimization in Expectation (AME), that determines valid MAPF-DP plans so that their combination with MCPs results in small average makespans.\nAME is a 2-level MAPF-DP solver that is based on Conflict-Based Search (CBS) (Sharon et al. 2015). Its highlevel search imposes constraints on the low-level search that resolve violations of Properties 1 and 2 of Definition 1 (called conflicts). Its low-level search plans paths for single agents that obey these constraints and result in small average makespans. The average makespan of a MAPF-DP plan is the expectation of the maximum of (one or more) random variables that represent the time steps when all agents enter their last local states. Moreover, the average time step when an agent enters a local state is the expectation of the maximum of random variables as well. It is often difficult to obtain good closed-form approximations of the expectation of the maximum of random variables. AME thus approximates it with the maximum over the expectations of the random variables, which typically results in an underestimate but, according to our experimental results, a close approximation. The approximate average time step l\u0303i(x) when agent ai enters a local state x for a given MAPF-DP plan is 0 for x = 0 and\nmax(l\u0303i(x\u2212 1),maxj,x\u2032:i6=j,x\u2032<x,lj(x\u2032)\u2192li(x)(l\u0303j(x \u2032))) + t\u0302i\n= maxj,x\u2032:x\u2032<x,lj(x\u2032)\u2192li(x)(l\u0303j(x \u2032)) + t\u0302i (1)\notherwise since agent ai first enters local state x \u2212 1 at approximate average time step l\u0303i(x \u2212 1), then might have to wait for messages from other agents aj that they send when they enter their local states x\u2032 at approximate average time steps l\u0303j(x\u2032) and finally has to successfully execute one action (perhaps repeatedly) to enter local state x. The average number t\u0302i of time steps that it needs for the successful execution of the action is 1 (for a wait action) if li(x) = li(x \u2212 1) and 1/(1 \u2212 pi) (for a move action) otherwise. The approximate average makespan of the given MAPF-DP plan is then maxi l\u0303i(Xi) since all agents need to enter their last local states. One might be able to obtain better approximations with more runtime-intensive importance sampling or dynamic programming methods but the runtime of the resulting AME variant would be large since it needs to compute many such approximations.\nAlgorithm 1: High-Level Search of AME. 1 Root.constraints := \u2205; 2 Root.plan := \u2205; 3 for each agent ai do 4 if LowLevelSearch(ai , Root, 0) returns no path (nor its labels) then 5 return \u201cNo solution exists\u201d;\n6 Add the returned path (and its labels) to Root.plan;\n7 Root.key := ApproximateAverageMakespan(Root.plan); 8 Priorityqueue := {Root}; 9 while Priorityqueue 6= \u2205 do\n10 N := Priorityqueue.pop(); 11 if FindConflicts(N.plan) returns no conflicts then 12 return \u201cSolution is\u201d N.plan;\n13 Conflict := earliest returned conflict; 14 for each agent ai involved in Conflict do 15 N\u2032 := new node with parent node N ; 16 N\u2032.constraints := N.constraints; 17 N\u2032.plan := N.plan; 18 Add one new constraint for agent ai to N\u2032.constraints (see main text); 19 if LowLevelSearch(ai , N\u2032 , N.key) returns a path (and its labels) then 20 Replace the path (and its labels) of agent ai in N\u2032.plan with the returned path (and its labels); 21 N\u2032.key := ApproximateAverageMakespan(N\u2032.plan); 22 Priorityqueue.insert(N\u2032);\n23 return \u201cNo solution exists\u201d;"}, {"heading": "High-Level Search", "text": "Algorithm 1 shows the high-level search of AME, which is similar to the high-level search of CBS. In the following, we point out the differences. Each high-level node N contains the following items:\n1. A set N.constraints of constraints of the form (ai, l, x) that states that the vertex of agent ai in local state x has to be different from vertex l. 2. A (labeled) MAPF-DP plan N.plan that contains a path li for each agent ai (that obeys the constraints N.constraints) and an approximation l\u0303i(x) (called label) of each average time step when agent ai enters local state x during plan execution with MCPs. 3. The key N.key of high-level node N that encodes its priority (smaller keys have higher priority) and is equal to the approximate average makespan of MAPF-DP plan N.plan given by ApproximateAverageMakespan(N.plan) = maxi l\u0303i(Xi). When a conflict exists in MAPF-DP plan N.plan, then\nthe high-level search creates 2 child nodes of node N [Line 15] whose constraints are initially set to the constraints N.constraints [Line 16] and whose MAPF-DP plan is initially set to MAPF-DP plan N.plan [Line 17]. Assume that the earliest conflict is a violation of Property 1 in Definition 1, in which case the vertices of two agents ai and aj in a local state x are both identical to a vertex l. In this case, AME adds the constraint (ai, l, x) to the constraints of the first child node and the constraint (aj , l, x) to the constraints of the second child node [Line 18], thus preventing the conflict in both cases. Assume that the earliest conflict is a violation of Property 2 in Definition 1, in which case the vertex of an agent ai in a local state x + 1 and the vertex of some other agent aj in the immediately preceding local state x are both identical to a vertex l. In this case, AME adds the constraint (ai, l, x+1) to the constraints of the first child node and the constraint (aj , l, x) to the constraints of\nthe second child node [Line 18], thus preventing the conflict in both cases."}, {"heading": "Low-Level Search", "text": "LowLevelSearch(ai, N , key) finds a new path for agent ai and the labels l\u0303i(x) of this path. It uses the paths of the other agents and their labels in N.plan but does not update them. (The paths are empty directly after the execution of Line 2.) It performs a focal search with re-expansions in a state space whose states correspond to pairs of vertices and local states (except for those pairs ruled out by constraints in N.constraints that pertain to agent ai) and whose edges connect state (l, x) to state (l\u2032, x+1) if and only if l = l\u2032 (for a wait action) or (l, l\u2032) \u2208 E (for a move action). The g-value of a state (l, x) approximates (sic!) the approximate average time step l\u0303i(x). The start state is (si, 0) and its g-value is 0. When the low-level search expands state (l, x \u2212 1), it sets the g-value of its successor (l\u2032, x) according to Equation (1) to the minimum of its current g-value g((l\u2032, x)) and\nmax(g((l, x\u2212 1)),maxj,x\u2032:i6=j,x\u2032<x,lj(x\u2032)\u2192li(x)(l\u0303j(x \u2032))) + t\u0302i,\nwhere t\u0302i is 1 if l = l\u2032 and 1/(1 \u2212 pi) otherwise. The lowlevel search decides which state (l, x) to expand next based on 1) the f-value of the state, which is the sum of its g-value and its h-value, where the h-value is 1/(1 \u2212 pi) times the distance from location l to location gi in graph G (which is an optimistic estimate of the average number of time steps required to move from location l to location gi) and 2) the number of conflicts of the path for agent ai that corresponds to the locations in the states on the found path from the start state to (l, x) with the paths of other agents.\nThe low-level search starts in Phase 1. The objective in this phase is to find a path for agent ai so that it enters its last local state with a reasonably small approximate average number of time steps, namely one that is no larger than the approximate average makespan key of the MAPF-DP plan in the parent node of node N in the high-level search, and has a small number of conflicts. The first part of the objective tries to ensure that the approximate average makespan of the resulting MAPF-DP plan in node N is no larger than the one of the MAPF-DP plan in the parent node of node N , and the second part tries to ensure that the resulting MAPF-DP plan has a small number of conflicts so that the high-level search has a small runtime since it needs to resolve only a small number of conflicts. The low-level search thus repeatedly expands a state with the smallest number of conflicts among all states in the priority queue whose f-values are no larger than key.\nIf no such state exists, then the low-level search switches to Phase 2. The objective in this phase is to find a path for agent ai so that it enters its last local state with a small approximate average number of time steps. This objective tries to ensure that the approximate average makespan of the resulting MAPF-DP plan in node N is not much larger than the one of the MAPF-DP plan in the parent node of node N . The low-level search thus repeatedly expands a state with the smallest f-value among all states in the priority queue.\nThe low-level search terminates successfully when it is about to expand a state (l, x) with l = gi and N.constraints contains no constraints of the form (ai, gi, x\u2032) with x\u2032 > x. It then sets Xi := x, the locations li(x) that form the path of agent ai to the corresponding locations in the states on the found path from the start state to (l, x) and the approximate average time steps l\u0303i(x) to the corresponding g-values of these states. The low-level search terminates unsuccessfully when the priority queue becomes empty. The low-level search currently does not terminate otherwise but we might be able to make it complete by using an upper bound on the smallest average makespan of any valid MAPF-DP plan, similar to upper bounds in the context of valid MAPF plans (Kornhauser, Miller, and Spirakis 1984)."}, {"heading": "Future Work", "text": "The low-level search is currently the weakest part of AME due to the many approximations to keep its runtime small which is important since the high-level search runs many low-level searches. We expect that future work will be able to improve the low-level search substantially. For example, the approximate average time steps l\u0303j(x) for agents aj different from agent ai could be updated before, during or after the local search, which would provide more accurate values for the current and future low-level searches as well as the current high-level search. Once the low-level search finds a path for agent ai and the high-level search replaces the path of agent ai in the MAPF-DP plan in the current high-level node with this path, it could update the approximate average time steps of all agents to the ideal approximate average time steps given by Equations (1), for example as part of the execution of ApproximateAverageMakespan on Lines 7 and 21. Many other improvements are possible as well."}, {"heading": "Experiments", "text": "We evaluate AME with MCPs on a 2.50 GHz Intel Core i52450M PC with 6 GB RAM.\nExperiment 1: MAPF Solvers We compare AME to 2 MAPF solvers, namely 1) Adapted CBS, a CBS variant that assumes perfect plan execution and computes valid MAPF-DP plans, minimizes maxi Xi and breaks ties toward paths with smaller Xi and thus fewer actions and 2) Push and Swap (Luna and Bekris 2011), a MAPF solver that assumes perfect plan execution and computes valid MAPF-DP plans where exactly one agent executes a move action at each time step and all other agents execute wait actions. We generate 10 MAPFDP instances (labeled random 1-10) in 30\u00d730 4-neighbor grids with 10% randomly blocked cells and random but unique start and unique goal cells for 35 agents whose delay probabilities for AME are sampled uniformly at random from the delay probability range (0, 1/2). In the same way, we generate 10 MAPF-DP instances (labeled warehouse 1- 10) in a simulated warehouse environment with random but unique start and unique goal cells on the left and right sides. Figure 4 shows two MAPF-DP instances: random 1 (top) and warehouse 1 (bottom).\nTable 1 reports for each MAPF-DP instance the runtime, the approximate average makespan calculated by AME, the average makespan over 1,000 plan-execution runs with MCPs together with 95%-confidence intervals and the number of sent messages. Dashes indicate that the MAPFDP instance was not solved within a runtime limit of 5 minutes. There is no obvious difference in the numbers of sent messages of the 3 MAPF(-DP) solvers. However, AME seems to find MAPF-DP plans with smaller average makespans than Adapted CBS, which seems to find MAPFDP plans with smaller average makespans than Push and Swap. The approximate average makespans calculated by AME are underestimates but reasonably close to the average makespans. AME and Push and Swap seem to run faster than Adapted CBS. In fact, Adapted CBS did not solve MAPFDP instances with more than 35 agents within the runtime limit while AME and Push and Swap seem to scale to larger numbers of agents than reported here (see also Experiment 3).\nExperiment 2: Delay Probability Ranges We use AME with different delay probability ranges. We repeat Experiment 1 with 19 MAPF-DP instances generated from the MAPF-DP instance labeled \u201crandom 1\u201d in Experiment 1, one for each t\u0302max = 2, 3 . . . 20. For each MAPFDP instance, the delay probabilities pi of all agents are sampled from the delay probability range (0, 1\u22121/t\u0302max) by sampling the average number of time steps t\u0302i = 1/(1 \u2212 pi) needed for the successful execution of single move actions uniformly at random from (1, t\u0302max) and then calculating pi = 1\u2212 1/t\u0302i.\nTable 2 reports the same measures as used in Experiment\n1, and Figure 5 visualizes the results. Larger delay probability ranges seem to result in larger runtimes, approximate average makespans calculated by AME and average makespans (although there is lots of noise). The differences between the approximate average makespans calculated by AME and average makespans are larger as well but remain reasonable.\nExperiment 3: Numbers of Agents\nWe use AME with different numbers of agents. We repeat Experiment 1 with 50 MAPF-DP instances in 30\u00d730 4- neighbor grids generated as in Experiment 1 for each number of agents.\nTable 3 reports the same measures as used in Experiment 1, averaged over all MAPF-DP instances that were solved within a runtime limit of 5 minutes. AME solves most MAPF-DP instances with 50 agents and then degrades gracefully with the number of agents."}, {"heading": "50 0.94 0.166 69.32 75.19 474.62", "text": ""}, {"heading": "100 0.68 4.668 78.48 87.29 1,554.71", "text": ""}, {"heading": "150 0.10 134.155 81.77 96.43 2,940.40", "text": "Experiment 4: Plan-Execution Policies\nWe use AME with 3 plan-execution policies, namely 1) MCPs, 2) FSPs and 3) dummy (non-robust) plan-execution policies that always provide GO commands. We repeat Experiment 1 for each plan-execution policy.\nTable 4 reports for each solved MAPF-DP instance and plan-execution policy the average makespan over 1,000 plan-execution runs together with 95%-confidence intervals, the number of sent messages for MCPs and FSPs and the average number of collisions for dummy plan-execution policies. The number of sent messages is zero (and thus not shown) for dummy plan-execution policies since, different from MCPs and FSPs, they do not prevent collisions. The average makespan for MCPs seems to be only slightly larger than that for dummy plan-execution policies, and the average makespan and number of sent messages for MCPs seem to be smaller than those for FSPs."}, {"heading": "Conclusions", "text": "In this paper, we formalized the Multi-Agent Path-Finding Problem with Delay Probabilities (MAPF-DP) to account for imperfect plan execution and then developed an efficient way of solving it with small average makespans, namely with Approximate Minimization in Expectation (a 2-level MAPF-DP solver for generating valid MAPF-DP plans) and Minimal Communication Policies (decentralized robust plan-execution policies for executing valid MAPF-DP plans without collisions)."}], "references": [{"title": "The transitive reduction of a directed graph", "author": ["A.V. Aho", "M.R. Garey", "J.D. Ullman"], "venue": "SIAM Journal on Computing 1(2):131\u2013137.", "citeRegEx": "Aho et al\\.,? 1972", "shortCiteRegEx": "Aho et al\\.", "year": 1972}, {"title": "Solving transition independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "Journal of Artificial Intelligence Research 22(1):423\u2013455.", "citeRegEx": "Becker et al\\.,? 2004", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "Planning, learning and coordination in multiagent decision processes", "author": ["C. Boutilier"], "venue": "Conference on Theoretical Aspects of Rationality and Knowledge, 195\u2013210.", "citeRegEx": "Boutilier,? 1996", "shortCiteRegEx": "Boutilier", "year": 1996}, {"title": "ICBS: Improved conflict-based search algorithm for multi-agent pathfinding", "author": ["E. Boyarski", "A. Felner", "R. Stern", "G. Sharon", "D. Tolpin", "O. Betzalel", "S.E. Shimony"], "venue": "International Joint Conference on Artificial Intelligence, 740\u2013746.", "citeRegEx": "Boyarski et al\\.,? 2015", "shortCiteRegEx": "Boyarski et al\\.", "year": 2015}, {"title": "Improved solvers for bounded-suboptimal multiagent path finding", "author": ["L. Cohen", "T. Uras", "T.K.S. Kumar", "H. Xu", "N. Ayanian", "S. Koenig"], "venue": "International Joint Conference on Artificial Intelligence, 3067\u20133074.", "citeRegEx": "Cohen et al\\.,? 2016", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Enhanced Partial Expansion A", "author": ["M. Goldenberg", "A. Felner", "R. Stern", "G. Sharon", "N.R. Sturtevant", "R.C. Holte", "J. Schaeffer"], "venue": "Journal of Artificial Intelligence Research 50:141\u2013 187.", "citeRegEx": "Goldenberg et al\\.,? 2014", "shortCiteRegEx": "Goldenberg et al\\.", "year": 2014}, {"title": "Decentralized control of cooperative systems: Categorization and complexity analysis", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research 22:143\u2013174.", "citeRegEx": "Goldman and Zilberstein,? 2004", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2004}, {"title": "Multi-agent path finding with kinematic constraints", "author": ["W. H\u00f6nig", "T.K.S. Kumar", "L. Cohen", "H. Ma", "H. Xu", "N. Ayanian", "S. Koenig"], "venue": "International Conference on Automated Planning and Scheduling, 477\u2013485.", "citeRegEx": "H\u00f6nig et al\\.,? 2016", "shortCiteRegEx": "H\u00f6nig et al\\.", "year": 2016}, {"title": "Coordinating pebble motion on graphs, the diameter of permutation groups, and applications", "author": ["D. Kornhauser", "G. Miller", "P. Spirakis"], "venue": "Annual Symposium on Foundations of Computer Science, 241\u2013250.", "citeRegEx": "Kornhauser et al\\.,? 1984", "shortCiteRegEx": "Kornhauser et al\\.", "year": 1984}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "Robotics: Science and Systems, 65\u201372.", "citeRegEx": "Kurniawati et al\\.,? 2008", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "An MDP-based approximation method for goal constrained multi-MAV planning under action uncertainty", "author": ["L. Liu", "N. Michael"], "venue": "IEEE International Conference on Robotics and Automation, 56\u201362.", "citeRegEx": "Liu and Michael,? 2016", "shortCiteRegEx": "Liu and Michael", "year": 2016}, {"title": "Push and Swap: Fast cooperative path-finding with completeness guarantees", "author": ["R. Luna", "K.E. Bekris"], "venue": "International Joint Conference on Artificial Intelligence, 294\u2013300.", "citeRegEx": "Luna and Bekris,? 2011", "shortCiteRegEx": "Luna and Bekris", "year": 2011}, {"title": "Optimal target assignment and path finding for teams of agents", "author": ["H. Ma", "S. Koenig"], "venue": "International Conference on Autonomous Agents and Multiagent Systems, 1144\u20131152.", "citeRegEx": "Ma and Koenig,? 2016", "shortCiteRegEx": "Ma and Koenig", "year": 2016}, {"title": "Information gathering and reward exploitation of subgoals for POMDPs", "author": ["H. Ma", "J. Pineau"], "venue": "AAAI Conference on Artificial Intelligence, 3320\u20133326.", "citeRegEx": "Ma and Pineau,? 2015", "shortCiteRegEx": "Ma and Pineau", "year": 2015}, {"title": "Multi-agent path finding with payload transfers and the package-exchange robot-routing problem", "author": ["H. Ma", "C. Tovey", "G. Sharon", "T.K.S. Kumar", "S. Koenig"], "venue": "AAAI Conference on Artificial Intelligence, 3166\u20133173.", "citeRegEx": "Ma et al\\.,? 2016", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Decentralized MDPs with sparse interactions", "author": ["F.S. Melo", "M. Veloso"], "venue": "Artificial Intelligence 175(11):1757\u20131789.", "citeRegEx": "Melo and Veloso,? 2011", "shortCiteRegEx": "Melo and Veloso", "year": 2011}, {"title": "Planning, scheduling and monitoring for airport surface operations", "author": ["R. Morris", "C. Pasareanu", "K. Luckow", "W. Malik", "H. Ma", "S. Kumar", "S. Koenig"], "venue": "AAAI-16 Workshop on Planning for Hybrid Systems, 608\u2013614.", "citeRegEx": "Morris et al\\.,? 2016", "shortCiteRegEx": "Morris et al\\.", "year": 2016}, {"title": "Solving transition-independent multiagent MDPs with sparse interactions", "author": ["J. Scharpff", "D.M. Roijers", "F.A. Oliehoek", "M.T.J. Spaan", "M.M. de Weerdt"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Scharpff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Scharpff et al\\.", "year": 2016}, {"title": "The increasing cost tree search for optimal multi-agent pathfinding", "author": ["G. Sharon", "R. Stern", "M. Goldenberg", "A. Felner"], "venue": "Artificial Intelligence 195:470\u2013495.", "citeRegEx": "Sharon et al\\.,? 2013", "shortCiteRegEx": "Sharon et al\\.", "year": 2013}, {"title": "Conflict-based search for optimal multi-agent pathfinding", "author": ["G. Sharon", "R. Stern", "A. Felner", "N.R. Sturtevant"], "venue": "Artificial Intelligence 219:40\u201366.", "citeRegEx": "Sharon et al\\.,? 2015", "shortCiteRegEx": "Sharon et al\\.", "year": 2015}, {"title": "Cooperative pathfinding", "author": ["D. Silver"], "venue": "Artificial Intelligence and Interactive Digital Entertainment, 117\u2013122.", "citeRegEx": "Silver,? 2005", "shortCiteRegEx": "Silver", "year": 2005}, {"title": "Finding optimal solutions to cooperative pathfinding problems", "author": ["T.S. Standley"], "venue": "AAAI Conference on Artificial Intelligence, 173\u2013178.", "citeRegEx": "Standley,? 2010", "shortCiteRegEx": "Standley", "year": 2010}, {"title": "Distributed model shaping for scaling to decentralized POMDPs with hundreds of agents", "author": ["P. Velagapudi", "P. Varakantham", "K.P. Sycara", "P. Scerri"], "venue": "International Conference on Autonomous Agents and Multi-agent Systems, 955\u2013962.", "citeRegEx": "Velagapudi et al\\.,? 2011", "shortCiteRegEx": "Velagapudi et al\\.", "year": 2011}, {"title": "CoBots: Robust symbiotic autonomous mobile service robots", "author": ["M. Veloso", "J. Biswas", "B. Coltin", "S. Rosenthal"], "venue": "International Joint Conference on Artificial Intelligence, 4423\u2013 4429.", "citeRegEx": "Veloso et al\\.,? 2015", "shortCiteRegEx": "Veloso et al\\.", "year": 2015}, {"title": "Subdimensional expansion for multirobot path planning", "author": ["G. Wagner", "H. Choset"], "venue": "Artificial Intelligence 219:1\u201324.", "citeRegEx": "Wagner and Choset,? 2015", "shortCiteRegEx": "Wagner and Choset", "year": 2015}, {"title": "Subdimensional Expansion: A Framework for Computationally Tractable Multirobot Path Planning", "author": ["G. Wagner"], "venue": "Ph.D. Dissertation, Carnegie Mellon University.", "citeRegEx": "Wagner,? 2015", "shortCiteRegEx": "Wagner", "year": 2015}, {"title": "MAPP: a scalable multi-agent path planning algorithm with tractability and completeness guarantees", "author": ["K. Wang", "A. Botea"], "venue": "Journal of Artificial Intelligence Research 42:55\u201390.", "citeRegEx": "Wang and Botea,? 2011", "shortCiteRegEx": "Wang and Botea", "year": 2011}, {"title": "Coordinating hundreds of cooperative, autonomous vehicles in warehouses. AI Magazine 29(1):9\u201320", "author": ["P.R. Wurman", "R. D\u2019Andrea", "M. Mountz"], "venue": null, "citeRegEx": "Wurman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wurman et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 16, "context": "MAPF problems arise for aircraft towing vehicles (Morris et al. 2016), office robots (Veloso et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 23, "context": "2016), office robots (Veloso et al. 2015), video game characters (Silver 2005) and warehouse robots (Wurman, D\u2019Andrea, and Mountz 2008), among others.", "startOffset": 21, "endOffset": 41}, {"referenceID": 20, "context": "2015), video game characters (Silver 2005) and warehouse robots (Wurman, D\u2019Andrea, and Mountz 2008), among others.", "startOffset": 29, "endOffset": 42}, {"referenceID": 14, "context": "The MAPF problem is NP-hard to solve optimally for flowtime minimization and to approximate within any constant factor less than 4/3 for makespan minimization (Ma et al. 2016).", "startOffset": 159, "endOffset": 175}, {"referenceID": 13, "context": "The MAPF-DP problem can be solved with POMDPs but this is tractable only for very few agents in very small environments since the size of the state space is proportional to the size of the environment to the power of the number of agents and the size of the belief space is proportional to the size of the state space to the power of the length of the planning horizon (Kurniawati, Hsu, and Lee 2008; Ma and Pineau 2015).", "startOffset": 369, "endOffset": 420}, {"referenceID": 1, "context": "Several specialized probabilistic planning frameworks, such as transition-independent decentralized Markov Decision Processes (DecMDPs) (Becker et al. 2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs.", "startOffset": 136, "endOffset": 156}, {"referenceID": 2, "context": "2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs.", "startOffset": 56, "endOffset": 72}, {"referenceID": 6, "context": "In transition-independent Dec-MDPs, the local state of each agent depends only on its previous local state and the action taken by it (Goldman and Zilberstein 2004).", "startOffset": 134, "endOffset": 164}, {"referenceID": 17, "context": "For example, the MAPF-DP problem can be solved with transition-independent MMDPs (Scharpff et al. 2016).", "startOffset": 81, "endOffset": 103}, {"referenceID": 10, "context": "In fact, the most closely related research to ours is that on approximating MMDPs (Liu and Michael 2016) although it handles different types of dynamics than we do.", "startOffset": 82, "endOffset": 104}, {"referenceID": 15, "context": "For example, decentralized sparse-interaction Markov Decision Processes (Dec-SIMDPs) (Melo and Veloso 2011) assume that interactions among agents occur only in well-defined interaction areas in the environment (which is not the case for MAPF-DP in general), but typically still do not scale to more than 10 agents.", "startOffset": 85, "endOffset": 107}, {"referenceID": 22, "context": "The model shaping technique for decentralized POMDPs (Velagapudi et al. 2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions.", "startOffset": 53, "endOffset": 77}, {"referenceID": 25, "context": "2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions.", "startOffset": 67, "endOffset": 80}, {"referenceID": 7, "context": "Minimal Communication Policies (MCPs) address these drawbacks by identifying such critical dependencies between agents and obeying them during plan execution, an idea that originated in the context of centralized non-robust plan-execution policies (H\u00f6nig et al. 2016).", "startOffset": 248, "endOffset": 267}, {"referenceID": 19, "context": "AME is a 2-level MAPF-DP solver that is based on Conflict-Based Search (CBS) (Sharon et al. 2015).", "startOffset": 77, "endOffset": 97}, {"referenceID": 11, "context": "We compare AME to 2 MAPF solvers, namely 1) Adapted CBS, a CBS variant that assumes perfect plan execution and computes valid MAPF-DP plans, minimizes maxi Xi and breaks ties toward paths with smaller Xi and thus fewer actions and 2) Push and Swap (Luna and Bekris 2011), a MAPF solver that assumes perfect plan execution and computes valid MAPF-DP plans where exactly one agent executes a move action at each time step and all other agents execute wait actions.", "startOffset": 248, "endOffset": 270}], "year": 2016, "abstractText": "Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.", "creator": "TeX"}}}