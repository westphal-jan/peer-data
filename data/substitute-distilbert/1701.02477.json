{"id": "1701.02477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition", "abstract": "multi - task learning ( mtl ) involves the simultaneous training of multiple adjacent entirely related traits over shared representations. in this work, we apply mtl discrete audio - visual coherent speech recognition ( av - algorithm ). our primary task is to learn a mapping between audio - visual fused features and frame labels obtained from acoustic gmm / hmm model. correlation is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual gmm / hmm system. the mtl model is tested at various levels of babble noise and the results are compared with a base - line hybrid re - hmm av - alignment map. our results indicate that mtl is especially useful at higher means of noise. similar to base - line, upto 7 \\ % relative improvement in prediction is reported at - 3 snr score", "histories": [["v1", "Tue, 10 Jan 2017 08:47:56 GMT  (35kb,D)", "http://arxiv.org/abs/1701.02477v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["abhinav thanda", "shankar m venkatesan"], "accepted": false, "id": "1701.02477"}, "pdf": {"name": "1701.02477.pdf", "metadata": {"source": "CRF", "title": "MULTI-TASK LEARNING OF DEEP NEURAL NETWORKS FOR AUDIO VISUAL AUTOMATIC SPEECH RECOGNITION", "authors": ["Abhinav Thanda", "Shankar M Venkatesan"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Multi-task learning, audio-visual speech recognition, deep neural networks, multi-modal learning, noise robustness\n1. INTRODUCTION\nOne way to build a noise robust Automatic Speech Recognition(ASR) system is to incorporate complementary information from a different modality that is independent of noise in the speech. For example, in an audio-visual ASR(AV-ASR) system visual information from the speaker\u2019s lip region when combined with audio inputs have been shown to provide significant reduction in word error rates(WER) when the audio modality is corrupted by noise. This was inspired by the fact that human perception of speech is dependent on both auditory and visual senses as demonstrated by the famous McGurk effect[1].\nTraditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5]. One way(called decision fusion method) is to model each modality by a separate GMM/HMM and at test time fuse the decisions of each stream by linearly combining the log-likelihoods to get the overall likelihood. In the feature fusion method, audio and visual features are combined usually by concatenation followed by a dimensionality reduction step. The fused features are modeled by a single GMM/HMM model.\n1 September 12, 2016 version, submitted to ICASSP 2017\nWith the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10]. Deep learning based ASR systems tend to outperform GMM/HMM models for several reasons. Firstly, GMM is a mixture model which acts as a \u201dsum of experts\u201d model, whereas DNN is a \u201dproduct of experts\u201d. Also GMM/HMM systems require uncorrelated inputs and do not benefit from multiple frames of input whereas, this is not the case for a DNN[11].Correspondingly, deep learning based AV-ASR systems have been shown to perform better than their GMM/HMM counterparts.\nSimilar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7]. In decision fusion, each modality is modeled by a separate network. Midlevel fusion of features is also possible by fusing the hidden layer outputs of separate audio and visual features deep networks.\nSo far, fusion models based on deep learning have been single task learning(STL) methods in which the fusion network predicts posterior probabilities of labels(usually context dependent tied HMM states) and the state space is common for both audio and visual modalities. An alternative approach, would be to assume different label set for the two modalities and train the network for two tasks simultaneously using a shared representation. This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling. One of the tasks is considered to be a primary task while the other is an auxiliary task. The auxiliary task helps in better feature estimation in the hidden layers and proper generalization of the model. Better generalization results in improved robustness to noise. Once the MTL model is trained, the parameters corresponding to auxiliary task are usually discarded. In this work, we explore the application of MTL to AV-ASR systems. We model the visual stream by a separate GMM/HMM whose states are used as classes for the auxiliary task. We compare our method with a baseline DNN-HMM based model. We show that MTL gives better performance, especially at higher noise levels.\nThe paper is organized as follows: Section 2 describes the feature extraction pipe-line for audio and visual features.\nar X\niv :1\n70 1.\n02 47\n7v 1\n[ cs\n.C L\n] 1\n0 Ja\nn 20\n17\nMTL is explained and is followed by a description of the primary and auxiliary tasks for AV-ASR. The training procedure and experimental results are discussed in section 3. Section 4 discusses the relationship of our work with previous AV-ASR methods. Finally, we summarize our work in section 5.\n2. MTL MODEL"}, {"heading": "2.1. Feature Extraction", "text": "The sampling rate of audio data is converted to 16kHz. For each frame of speech signal of 25ms duration, filter-bank features of 40 dimensions are extracted. Mean and variance normalization is performed.\nThe video frame rate is increased to match the rate of audio frames through interpolation. The region of interest(ROI) corresponding to the area surrounding speaker\u2019s mouth is extracted as follows: Each frame is converted to gray scale and face detection is performed using Viola-Jones algorithm. The 64x64 lip region is extracted by detecting 68 landmark points[20] on the speakers face, and cropping the ROI surrounding speakers mouth and chin. 100 dimensional DCT features are extracted from the ROI. Similar to audio features, we perform mean and variance normalization."}, {"heading": "2.2. MTL-DNN", "text": "In MTL, a primary task along with one or more related secondary/auxiliary tasks are learnt simultaneously over shared representation of the data. The secondary task acts as a type of regularization and results in better generalization of the model. Once the MTL model is trained, the parameters corresponding to secondary task can be ignored. The cost function for an MTL network is given by\nCmtl = Cmain + N\u2211 n=1 \u03bbnCn (1)\nwhere Cmtl, Cmain, C\u03bbn represent cost functions and 0 \u2264 \u03bbn \u2264 1 for n = 1, 2, ...N are parameters indicating the relative importance of the secondary task with respect to the primary task.\nIn this work, the primary as well as secondary tasks are hybrid DNN-HMM models. DNN-HMM hybrid systems are frame level classifiers and estimate the posterior probabilities of tied HMM states given an acoustic feature vector. The frame labels are obtained by first training a bootstrap GMM/HMM model and then aligning the data against it.We train a tri-phone GMM/HMM acoustic model with Linear Discriminant Analysis(LDA) transformed audio features as input. The alignments obtained from the audio feature sequences and acoustic model are used as labels for the primary task of MTL-DNN Correspondingly, for secondary task we train another tri-phone GMM/HMM visual model with LDA\ntransformed visual features. The visual input sequences are aligned against the visual GMM/HMM model.\nHere N = 1 and\nCmain = \u2212 U\u2211 u=1 T\u2211 t=1 logPa(s a ut|Oavut ) (2)\nand\nC1 = \u2212 U\u2211 u=1 T\u2211 t=1 logPv(s v ut|Oavut ) (3)\nare the cross-entropy costs, where smut corresponds to output HMM state for an utterance u at time t for a given modality m \u2208 {a, v}. Pa and Pv correspond to the soft-max outputs for each task. Oavut = [O a ut, O v ut] corresponds to input fused features for an utterance u at time t. The MTL-DNN architecture used in this work, is shown in Figure 1. 40 dimensional audio features and 100 dimensional visual features are fused by simple concatenation. The 140 dimensional audio-visual features are then spliced with 10 frames which provide the contextual information.\nThe MTL-DNN is trained as follows: For the primary task, the inputs are audio-visual fused features in which both audio and visual modalities are present. In addition the primary task is also trained on fused features in which either audio or visual modality is suppressed by setting the corresponding features to very small values. For the secondary task, the inputs are fused features with audio modality suppressed i.e., the inputs correspond to only visual modality.\n3. EXPERIMENTS AND RESULTS\nThe system was trained and tested on GRID audio-visual corpus[21]. The corpus is a collection of audio and video recordings of 34 speakers (18 male, 16 female) each uttering a 1000 sentences. Each utterance is approximately 3 seconds in length. The syntactic structures of all sentences are similar[21]. A simple language model following this structure was built for this work.\nThe dataset in effect consisted of 32692 utterances 90% of the which (containing 29423 utterances) were used for training and cross validation while the remaining (10%) data was used as test set. Both training and test data contain utterances from all of the speakers. Models were trained and tested using Kaldi speech recognition tool kit[22] and Kaldi+PDNN[23].\nBase-line The base-line system is a single task learning feature fusion model(STL-DNN) somewhat similar to the network in [7]. Input to the network is 1540 dimensional vector. Feature extraction is same as described in 2.2. The frame labels are obtained in the same way as the primary task of MTL-DNN described in 2.2. The only difference between MTL-DNN and STL-DNN is the absence of secondary task in STL-DNN.\nPre-training is not performed as it did not provide any noticeable benefit. Cross-entropy loss function is minimized\nusing mini-batch Stochastic Gradient Descent (SGD). The frames are shuffled randomly before each epoch. Batch size is set to 256 and initial learning rate is set to 0.008. New bob learning schedule is adopted, i.e., when the improvement in the cross-validation accuracy between two successive epochs falls below 0.5%, the learning rate is halved.The halving is performed for each subsequent epoch until the increase in frame level accuracy is less than 0.1%. At this point training is halted.\nThe models are tested with four levels of babble noise -3dB SNR, 0dB SNR, 10dB SNR and clean audio. Noise was added to test data artificially by mixing babble noise with clean audio files. At test time, the secondary task outputs are ignored. Decoding is performed over a Weighted Finite State Transducer(WFST) built using GMM/HMM acoustic model, GRID corpus lexicon and the GRID language model. The model is tested once with audio-visual features and again with only audio features i.e., visual features set to small values. The posteriors are converted to log-likelihoods and passed to decoder which outputs the final word sequences.\nThe results are tabulated in table 1. In line with previous AV-ASR systems, when visual modality is turned on, both MTL-DNN and STL-DNN provide significant gains in WER compared to audio only input. The large gains in our case are in part due to the small vocabulary and simple language model of GRID corpus.\nAt low levels of noise (clean audio), the base-line system gives slightly better performance compared to both MTLDNN models.However, as noise increases (0 dB and -3 dB) the difference in performance improvement of MTL-DNN over STL-DNN becomes significant. At -3 SNR dB MTLDNN with \u03bb = 0.3 gives nearly 7.23% relative improvement (from 27.1 to 25.14) in WER over STL-DNN while \u03bb = 0.1 gives 1.6% relative improvement. Among the two MTL-DNN models, the model with \u03bb = 0.3 gives better performance at 0 dB and -3 dB. This is consistent with the idea that regularization due to auxiliary task reduces over-fitting which results in better performance with corrupted inputs. However, in our experiments we found that increasing \u03bb further, resulted in degradation of the performance of primary task.\nThe last row in table 1 corresponds to lip-reading, where audio modality is suppressed. Again MTL-DNN gives better performance compared to STL-DNN with approximately 3% relative gain when \u03bb = 0.3 and 1.6% relative gain when \u03bb = 0.1.\n4. RELATION TO PRIOR WORK\nMulti-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19]. The application of MTL to AV-ASR was suggested in [16] although no results were reported. Our training procedure to some extent similar to [17]. Like [17] we separate the inputs and outputs of two tasks.\nOur work is inspired by [9] and [7]. In [9] the authors employ unsupervised learning methods to obtain a shared representations of the audio and visual modalities which are then used in a separate supervised training step. In our work, learning the shared representation as well as supervised training are taken care by multi-task learning. In contrast to [7] we employ low-level feature fusion. During training we suppress one of the modalities to ensure that the network does not overfit to a particular modality. In addition, our results are re-\nported on GRID corpus which includes digits, alphabets and words different from the continuous digit dataset used in [7].\n5. CONCLUSIONS AND FUTURE WORK\nIn this work, we applied multi-task learning to audio-visual speech recognition. We described the primary and auxiliary tasks to train MTL model. We proposed a network architecture and training protocol for the network. The model was compared with baseline STL-DNN model at various levels of babble noise. Our results indicate that MTL results in the improvement of WER over baseline model, especially at higher levels of noise. In our future work, we would like to compare MTL with ensemble learning for AV-ASR.\n6. REFERENCES\n[1] Harry McGurk and John MacDonald, \u201cHearing lips and seeing voices,\u201d Nature, vol. 264, pp. 746\u2013748, 1976.\n[2] Ste\u0301phane Dupont and Juergen Luettin, \u201cAudio-visual speech modeling for continuous speech recognition,\u201d IEEE transactions on multimedia, vol. 2, no. 3, pp. 141\u2013 151, 2000.\n[3] Samy Bengio, \u201cMultimodal speech processing using asynchronous hidden markov models,\u201d Information Fusion, vol. 5, no. 2, pp. 81\u201389, 2004.\n[4] Jing Huang, Gerasimos Potamianos, and Chalapathy Neti, \u201cImproving audio-visual speech recognition with an infrared headset,\u201d in AVSP 2003-International Conference on Audio-Visual Speech Processing, 2003.\n[5] Ara V Nefian, Luhong Liang, Xiaobo Pi, Xiaoxing Liu, and Kevin Murphy, \u201cDynamic bayesian networks for audio-visual speech recognition,\u201d EURASIP Journal on Advances in Signal Processing, vol. 2002, no. 11, pp. 1\u201315, 2002.\n[6] George E Dahl, Dong Yu, Li Deng, and Alex Acero, \u201cContext-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.\n[7] Jing Huang and Brian Kingsbury, \u201cAudio-visual deep learning for noise robust speech recognition,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 7596\u20137599.\n[8] Youssef Mroueh, Etienne Marcheret, and Vaibhava Goel, \u201cDeep multimodal learning for audio-visual speech recognition,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 2130\u20132134.\n[9] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng, \u201cMultimodal deep learning,\u201d in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.\n[10] Kuniaki Noda, Yuki Yamaguchi, Kazuhiro Nakadai, Hiroshi G Okuno, and Tetsuya Ogata, \u201cAudio-visual speech recognition using deep learning,\u201d Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015.\n[11] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[12] Gerasimos Potamianos, Chalapathy Neti, Guillaume Gravier, Ashutosh Garg, and Andrew W Senior, \u201cRecent advances in the automatic recognition of audiovisual speech,\u201d Proceedings of the IEEE, vol. 91, no. 9, pp. 1306\u20131326, 2003.\n[13] Aggelos K Katsaggelos, Sara Bahaadini, and Rafael Molina, \u201cAudiovisual fusion: Challenges and new approaches,\u201d Proceedings of the IEEE, vol. 103, no. 9, pp. 1635\u20131653, 2015.\n[14] Rich Caruana, \u201cMultitask learning,\u201d in Learning to learn, pp. 95\u2013133. Springer, 1998.\n[15] Ronan Collobert and Jason Weston, \u201cA unified architecture for natural language processing: Deep neural networks with multitask learning,\u201d in Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.\n[16] Dongpeng Chen, Multi-task Learning Deep Neural Networks for Automatic Speech Recognition, Ph.D. thesis, The Hong Kong University of Science and Technology, 2015.\n[17] Georg Heigold, Vincent Vanhoucke, Alan Senior, Patrick Nguyen, M Ranzato, Matthieu Devin, and Jeffrey Dean, \u201cMultilingual acoustic models using distributed deep neural networks,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 8619\u20138623.\n[18] Hui Lin, Li Deng, Dong Yu, Yi-fan Gong, Alex Acero, and Chin-Hui Lee, \u201cA study on multilingual acoustic modeling for large vocabulary asr,\u201d in 2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2009, pp. 4333\u20134336.\n[19] Gueorgui Pironkov, Ste\u0301phane Dupont, and Thierry Dutoit, \u201cMulti-task learning for speech recognition: An overview,\u201d .\n[20] Vahid Kazemi and Josephine Sullivan, \u201cOne millisecond face alignment with an ensemble of regression trees,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1867\u2013 1874.\n[21] Martin Cooke, Jon Barker, Stuart Cunningham, and Xu Shao, \u201cAn audio-visual corpus for speech perception and automatic speech recognition,\u201d The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.\n[22] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number EPFL-CONF-192584.\n[23] Yajie Miao, \u201cKaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn,\u201d arXiv preprint arXiv:1401.6984, 2014.\n[24] Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King, \u201cDeep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4460\u20134464."}], "references": [{"title": "Hearing lips and seeing voices", "author": ["Harry McGurk", "John MacDonald"], "venue": "Nature, vol. 264, pp. 746\u2013748, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Audio-visual speech modeling for continuous speech recognition", "author": ["St\u00e9phane Dupont", "Juergen Luettin"], "venue": "IEEE transactions on multimedia, vol. 2, no. 3, pp. 141\u2013 151, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal speech processing using asynchronous hidden markov models", "author": ["Samy Bengio"], "venue": "Information Fusion, vol. 5, no. 2, pp. 81\u201389, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving audio-visual speech recognition with an infrared headset", "author": ["Jing Huang", "Gerasimos Potamianos", "Chalapathy Neti"], "venue": "AVSP 2003-International Conference on Audio-Visual Speech Processing, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic bayesian networks for audio-visual speech recognition", "author": ["Ara V Nefian", "Luhong Liang", "Xiaobo Pi", "Xiaoxing Liu", "Kevin Murphy"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2002, no. 11, pp. 1\u201315, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["Jing Huang", "Brian Kingsbury"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 7596\u20137599.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 2130\u20132134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio-visual speech recognition using deep learning", "author": ["Kuniaki Noda", "Yuki Yamaguchi", "Kazuhiro Nakadai", "Hiroshi G Okuno", "Tetsuya Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent advances in the automatic recognition of audiovisual speech", "author": ["Gerasimos Potamianos", "Chalapathy Neti", "Guillaume Gravier", "Ashutosh Garg", "Andrew W Senior"], "venue": "Proceedings of the IEEE, vol. 91, no. 9, pp. 1306\u20131326, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Audiovisual fusion: Challenges and new approaches", "author": ["Aggelos K Katsaggelos", "Sara Bahaadini", "Rafael Molina"], "venue": "Proceedings of the IEEE, vol. 103, no. 9, pp. 1635\u20131653, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Learning to learn, pp. 95\u2013133. Springer, 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-task Learning Deep Neural Networks for Automatic Speech Recognition", "author": ["Dongpeng Chen"], "venue": "Ph.D. thesis, The Hong Kong University of Science and Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["Georg Heigold", "Vincent Vanhoucke", "Alan Senior", "Patrick Nguyen", "M Ranzato", "Matthieu Devin", "Jeffrey Dean"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 8619\u20138623.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A study on multilingual acoustic modeling for large vocabulary asr", "author": ["Hui Lin", "Li Deng", "Dong Yu", "Yi-fan Gong", "Alex Acero", "Chin-Hui Lee"], "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2009, pp. 4333\u20134336.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-task learning for speech recognition: An overview", "author": ["Gueorgui Pironkov", "St\u00e9phane Dupont", "Thierry Dutoit"], "venue": ".", "citeRegEx": "19", "shortCiteRegEx": null, "year": 0}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["Vahid Kazemi", "Josephine Sullivan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1867\u2013 1874.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["Martin Cooke", "Jon Barker", "Stuart Cunningham", "Xu Shao"], "venue": "The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number EPFL-CONF-192584.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn", "author": ["Yajie Miao"], "venue": "arXiv preprint arXiv:1401.6984, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Zhizheng Wu", "Cassia Valentini-Botinhao", "Oliver Watts", "Simon King"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4460\u20134464.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This was inspired by the fact that human perception of speech is dependent on both auditory and visual senses as demonstrated by the famous McGurk effect[1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 2, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 3, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 4, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 5, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 7, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 8, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 9, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 10, "context": "Also GMM/HMM systems require uncorrelated inputs and do not benefit from multiple frames of input whereas, this is not the case for a DNN[11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7].", "startOffset": 86, "endOffset": 97}, {"referenceID": 12, "context": "Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7].", "startOffset": 86, "endOffset": 97}, {"referenceID": 6, "context": "Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7].", "startOffset": 86, "endOffset": 97}, {"referenceID": 13, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 16, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 17, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 18, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 19, "context": "The 64x64 lip region is extracted by detecting 68 landmark points[20] on the speakers face, and cropping the ROI surrounding speakers mouth and chin.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "The system was trained and tested on GRID audio-visual corpus[21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The syntactic structures of all sentences are similar[21].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22] and Kaldi+PDNN[23].", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22] and Kaldi+PDNN[23].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "Base-line The base-line system is a single task learning feature fusion model(STL-DNN) somewhat similar to the network in [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 18, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 74, "endOffset": 82}, {"referenceID": 15, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 74, "endOffset": 82}, {"referenceID": 14, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 147, "endOffset": 155}, {"referenceID": 16, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 147, "endOffset": 155}, {"referenceID": 15, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 183, "endOffset": 191}, {"referenceID": 18, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 183, "endOffset": 191}, {"referenceID": 15, "context": "The application of MTL to AV-ASR was suggested in [16] although no results were reported.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Our training procedure to some extent similar to [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Like [17] we separate the inputs and outputs of two tasks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Our work is inspired by [9] and [7].", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Our work is inspired by [9] and [7].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "In [9] the authors employ unsupervised learning methods to obtain a shared representations of the audio and visual modalities which are then used in a separate supervised training step.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In contrast to [7] we employ low-level feature fusion.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "ported on GRID corpus which includes digits, alphabets and words different from the continuous digit dataset used in [7].", "startOffset": 117, "endOffset": 120}], "year": 2017, "abstractText": "Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AVASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7% relative improvement in WER is reported at -3 SNR dB1.", "creator": "LaTeX with hyperref package"}}}