{"id": "1701.02593", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling", "abstract": "we introduce already simple and accurate neural model suggesting dependency - based semantic role labeling. our model predicts predicate - argument dependencies relying on states employing a bidirectional lstm dictionary. the semantic role labeler achieves respectable results on english even without any kind of speech information and only using string inference. however, as automatically predicted part - of - speech judgments are performed as input, np substantially outperforms all previous local models and approaches the best reported results on the conll - 2009 dataset. syntactic parsers are unreliable on out - of - domain conditions, so standard ( i. 4. syntactically - informed ) srl models are hindered when undertaken in this setting. our node - agnostic paradigm appears more robust, resulting in the best reported results on the standard clarity - of - domain test set.", "histories": [["v1", "Tue, 10 Jan 2017 14:01:47 GMT  (62kb,D)", "http://arxiv.org/abs/1701.02593v1", null], ["v2", "Thu, 15 Jun 2017 16:47:47 GMT  (87kb,D)", "http://arxiv.org/abs/1701.02593v2", "To appear in CoNLL 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["diego marcheggiani", "anton frolov", "ivan titov"], "accepted": false, "id": "1701.02593"}, "pdf": {"name": "1701.02593.pdf", "metadata": {"source": "CRF", "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling", "authors": ["Diego Marcheggiani", "Anton Frolov", "Ivan Titov"], "emails": ["titov}@uva.nl", "anton-fr@yandex-team.ru"], "sections": [{"heading": "1 Introduction", "text": "The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves prediction of predicate argument structure, i.e. both identification of arguments as well as their assignment to an underlying semantic role. These representations have been shown beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011). Semantic banks (e.g., PropBank (Palmer et al., 2005)) typically represent arguments as syntactic constituents or, more generally, text spans (Baker et al., 1998). In contrast, CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajic et\n1github.com/diegma/neural-dep-srl\nal., 2009) popularized dependency-based semantic role labeling where the goal is to identify syntactic heads of arguments rather than entire constituents. Figure 1 shows an example of such a dependency-based representation: node labels are senses of predicates (e.g., \u201c01\u201d indicates that the first sense from the PropBank sense repository is used for predicate makes in this sentence) and edge labels are semantic roles (e.g., A0 is a proto-agent, \u2018doer\u2019).\nUntil recently state-of-the-art SRL systems relied on complex sets of lexico-syntactic features (Pradhan et al., 2005) as well as declarative constraints (Punyakanok et al., 2008; Roth and Yih, 2005). Neural SRL models instead exploited feature induction capabilities of neural networks, largely eliminating the need for complex handcrafted features. Initially achieving state-of-theart results only in the multilingual setting, where careful feature engineering is not practical (Titov et al., 2009; Gesmundo et al., 2009; Henderson et al., 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015).\nRecently, it has been shown that an accurate span-based SRL model can be constructed without relying on syntactic features (Zhou and Xu, 2015). Nevertheless, state-of-the-art methods for dependency-based SRL still heavily rely on syntactic features (Roth and Lapata, 2016; FitzGerald et al., 2015; Lei et al., 2015; Roth and Wood-\nar X\niv :1\n70 1.\n02 59\n3v 1\n[ cs\n.C L\n] 1\n0 Ja\nn 20\n17\nsend, 2014; Swayamdipta et al., 2016). In particular, Roth and Lapata (2016) argue that syntactic features are necessary and show that performance of their model degrades dramatically if syntactic paths between arguments and predicates are not provided as an input. In this work, we are the first to show how to construct a very accurate dependency-based semantic role labeler which either does not use any kind of syntactic information or uses very little (automatically predicted part-ofspeech tags). This suggests that our LSTM model can largely capture syntactic information, and this information can, to a large extent, substitute treebank syntax.\nOur model is inspired by recent work in syntactic dependency parsing (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their simplest version, they encoded a sentence by a bidirectional LSTM encoder, and then dependency edges in a candidate dependency tree are scored independently from each other, relying only on the concatenation of two LSTM states, one for the head word and one for the dependent word. We observe that the direct application of this idea does not lead to competitive results on dependencybased SRL. Instead, we find it necessary to use a multi-pass approach where we first identify predicates2 and disambiguate them, then, for each predicate, we re-encode the sentence with an LSTM while indicating (in the input) which word is chosen as a predicate. Finally, for each predicate, arguments and their roles are predicted in the same way as before, i.e. relying on the two LSTM states (a state of the predicate word and a state of the argument word). Intuitively, in this way, on each run, the LSTM encoder does not need to represent all argument-predicate dependencies in its state trajectory but can focus on a single predicate at a time. We hypothesize that this constitutes a more effective way to use the LSTM capacity. This reencoding idea is reminiscent of the region marking features used in the span-based model of Zhou and Xu (2015).\nThe resulting SRL model is very simple. Not only we do not rely on syntax, our model is also local, i.e. we do not globally score or constrain sets of arguments. On the standard in-domain CoNLL2009 benchmark we achieve 87.6 F1 which com-\n2In the CoNLL-2009 benchmark, predicates do not even need to be identified: their positions are provided as input at test time. Consequently, as standard for dependency SRL, we ignore this subtask in further discussion.\npares favorable to the best local model (86.7% F1 for PathLSTM (Roth and Lapata, 2016)) and approaches the best results overall (87.9% for an ensemble of 3 PathLSTM models with a reranker on top). Moreover, as syntactic parsers are not reliable when used out-of-domain, standard (i.e. syntactically-informed) dependency SRL models are crippled when applied to such data. In contrast, our syntax-agnostic model appears to be considerably more robust: we achieve the best result so far on the out-of-domain Brown test set, 77.3% F1. This constitutes a 2% absolute improvement over the comparable previous model (75.3% for the local PathLSTM) and substantially outperforms any previous method (76.5% for the ensemble of 3 PathLSTMs). The key contributions can be summarized as follows:\n\u2022 we propose the first effective syntax-agnostic model for dependency-based SRL;\n\u2022 it achieves the best results among local models on the English in-domain test set;\n\u2022 it substantially outperforms all previous methods on the out-of-domain test set.\nNote that, in this work, we are not arguing that neither global inference nor integration of treebank syntax is not beneficial to SRL. Instead, we leave these questions for future work. In fact, we believe that the proposed SRL model, given its simplicity and efficiency, can be used as a natural building block for future global and syntactically-informed SRL models."}, {"heading": "2 Our Model", "text": "The focus of this paper is on argument identification and labeling, as these are the steps which have been previously believed to require syntactic information. For predicate disambiguation we use a simple LSTM model, described in Section 2.4.\nAs we sketched in the introduction, in order to identify and classify arguments, we use a Bidirectional LSTM (BiLSTM). LSTM takes as input word representations xi of each word wi in a sentence w. LSTM states provide dynamic representation of words and their contexts in a sentence. The actual prediction of roles is done by a classifier which takes as an input the BiLSTM representation of the candidate argument and the BiLSTM representation of the predicate."}, {"heading": "2.1 Word Representation", "text": "We represent each word w as the concatenation of three vectors: a randomly initialized word embedding xre \u2208 Rdw , a pre-trained word embedding xpe \u2208 Rdw , a randomly initialized part-of-speech tag embedding xpos \u2208 Rdp and a randomly initialized lemma embedding xle \u2208 Rdl that is only active if the word is one of the predicates. The randomly initialized embeddings xre, xpos, and xle are fine-tuned during training, while the pretrained ones are kept fixed. The final word representation is given by x = xre \u25e6 xpe \u25e6 xpos \u25e6 xle, where \u25e6 represents the concatenation operator."}, {"heading": "2.2 Bidirectional LSTM Encoder", "text": "One of the most effective ways to model sequences are recurrent neural networks (RNN) (Elman, 1990), more precisely their gated versions, for example, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997).\nFormally, we can define an LSTM as a function LSTM\u03b8(x1:i) that takes as input the sequence x1:i and returns a hidden state hi \u2208 Rdh . This state can be regarded as a representation of the sentence from the start to the position i, or, in other words, it encodes the word at position i along with its left context. Bidirectional LSTMs make use of two LSTMs: one for the forward pass, and another for the backward pass, LSTMF and LSTMB , respectively. In this way the concatenation of forward and backward LSTM states encodes both left and right contexts of a word,BiLSTM(x1:n, i) = LSTMF (x1:i) \u25e6 LSTMB(xn:i). In this work we stack k layers of bidirectional LSTMs, each layer takes the lower layer as its input.\nSince for each word in a sentence we want to predict the semantic role given a predicate, we concatenate the hidden states at the k-th layer of\nthe current word and the predicate word and use them as input to a classifier. Though we experimented with multilayer perceptrons, we obtained the best results with a simple log-linear model. The classifier computes the probability of the role (including special \u2018NULL\u2019 role to indicate that a word is not an argument of the predicate) given the candidate argument and the predicate:\np(r|vi, vp, l) \u221d exp(Wl,r(vi \u25e6 vp)), (1) where vi and vp are hidden state calculated by respectively BiLSTM(x1:n, i) and BiLSTM(x1:n, p), l is the lemma of predicate p and the symbol \u221d signifies proportionality. Instead of using a fixed matrix Wl,r or simply assuming that Wl,r = Wr, we, inspired by FitzGerald et al. (2015), found it beneficial to jointly embed the role r and predicate lemma l using a nonlinear transformation:\nWl,r = ReLU(U(ul \u25e6 vr)), (2) where ReLU is the rectilinear activation function, U is a parameter matrix, whereas ul \u2208 Rd \u2032 l and vr \u2208 Rdr are randomly initialized embeddings of predicate lemmas and roles. In this way each role prediction is predicate-specific, and at the same time we expect to learn a good representation for roles associated to infrequent predicates."}, {"heading": "2.3 Predicate-Specific Encoder", "text": "As we will show in Section 3, although this onepass model, where the sentence is encoded only once, is very effective for syntactic dependency parsing, it does not perform well in SRL (Table 3, \u2018-predicate flag\u2019). Though we found this dramatic drop in performance surprising, the nature of dependencies, especially for nominal predicates, is different here with many arguments being\nfar away from the predicates. Inspired by the spanbased SRL approach of Zhou and Xu (2015), we add a predicate-specific feature to the word representation by concatenating a binary flag to the word representation of Section 2.1. The flag is set to 1 for the word corresponding to the currently considered predicate, it is set to 0 otherwise. In this way, sentences with more than one predicate will be encoded multiple times."}, {"heading": "2.4 Predicate Disambiguation", "text": "We also implemented a syntax-agnostic predicate sense disambiguator. For this subtask, we represented a word as a concatenation of its pretrained word embedding, the predicate word we want to disambiguate, and the predicate flag. This word representation is fed to a single-layer BiLSTM. The concatenation of the hidden state of the predicate and the predicate word embeddings are passed to a linear classifier to obtain the predicate sense. At test time, if a predicate has never been seen during training, the first sense is predicted."}, {"heading": "3 Experiments", "text": "We applied our model to the English CoNLL2009 dataset with the standard split into training, test and development sets. For the semantic role labeler, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al. (2015). Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al., 2015); we replace a word with the unknown token UNK with probability \u03b1fr(w)+\u03b1 , where \u03b1 is an hyper-parameter and fr(w) is the frequency of the word w. We used the predicted POS tags provided by the CoNLL2009 shared-task organizers. For the predicate disambiguator of Section 2.4 we used GloVe embed-\ndings (Pennington et al., 2014). We optimized with Adam (Kingma and Ba, 2015). The hyperparameters tuning and all model selection was performed on the development set, see Table 4 for their values.\nThe results indicate that our full model (with POS tags and re-encoding) significantly outperforms all the local counter-parts on the in-domain tests (see Table 1, 87.6% F1 for our model vs. 86.7% for PathLSTM) and outperforms even ensemble models on the out-of-domain data (77.3% vs. 76.5% for the ensemble of PathLSTMs). The ablation studies (Table 3) demonstrate that POS tag information is beneficial, though not crucial for obtaining competitive performance. In contrast, one-pass processing without re-encoding badly hurts the performance (6% drop in F1 on the development set)."}, {"heading": "4 Conclusions", "text": "Our syntax-agnostic method is simple and fast, and surpasses comparable approaches (no system combination, local inference) on the standard indomain benchmark for English. Moreover, it outperforms all previous methods (including ensembles) in the arguably more realistic out-of-domain setting. In the future, we will consider integration of syntactic information and joint inference as well as experiment with additional languages."}, {"heading": "Acknowledgments", "text": "The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and an Amazon Web Services (AWS) grant. The authors would like to thank Michael Roth for his helpful suggestions."}], "references": [{"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING-ACL \u201998, Au-", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "A high-performance syntactic and semantic dependency parser", "author": ["Anders Bj\u00f6rkelund", "Bernd Bohnet", "Love Hafdell", "Pierre Nugues."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 33\u201336.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2010", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2010}, {"title": "An analysis of open information extraction based on semantic role labeling", "author": ["Janara Christensen", "Mausam", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the 6th International Conference on Knowledge Capture (K-CAP 2011), June 26-29,", "citeRegEx": "Christensen et al\\.,? 2011", "shortCiteRegEx": "Christensen et al\\.", "year": 2011}, {"title": "Incremental parsing with minimal features using bi-directional lstm", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 32\u201337, Berlin, Ger-", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 960\u2013970,", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Dependency-based semantic role labeling using convolutional neural networks", "author": ["William Foland", "James Martin."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 279\u2013288, Denver,", "citeRegEx": "Foland and Martin.,? 2015", "shortCiteRegEx": "Foland and Martin.", "year": 2015}, {"title": "Latent variable model of synchronous syntactic-semantic parsing for multiple languages", "author": ["Andrea Gesmundo", "James Henderson", "Paola Merlo", "Ivan Titov."], "venue": "CoNLL 2009 Shared Task., Conf. on Computational Natural Language Learning, pages", "citeRegEx": "Gesmundo et al\\.,? 2009", "shortCiteRegEx": "Gesmundo et al\\.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational linguistics, 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Multi-lingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo."], "venue": "Computational Linguistics, 39(4).", "citeRegEx": "Henderson et al\\.,? 2013", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "The 3rd International Conference for Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "High-order lowrank tensors for semantic role labeling", "author": ["Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Paul Kingsbury", "Daniel Gildea."], "venue": "Computational Linguistics, 31(1):71\u2013106.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha,", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Semantic role chunking combining complementary syntactic views", "author": ["Sameer Pradhan", "Kadri Hacioglu", "Wayne H. Ward", "James H. Martin", "Daniel Jurafsky."], "venue": "Proceedings of the Ninth Conference on Computational Natural Language Learn-", "citeRegEx": "Pradhan et al\\.,? 2005", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."], "venue": "Computational Linguistics, 34(2):257\u2013287.", "citeRegEx": "Punyakanok et al\\.,? 2008", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Neural semantic role labeling with dependency path embeddings", "author": ["Michael Roth", "Mirella Lapata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1192\u20131202,", "citeRegEx": "Roth and Lapata.,? 2016", "shortCiteRegEx": "Roth and Lapata.", "year": 2016}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Michael Roth", "Kristian Woodsend."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014,", "citeRegEx": "Roth and Woodsend.,? 2014", "shortCiteRegEx": "Roth and Woodsend.", "year": 2014}, {"title": "Integer linear programming inference for conditional random fields", "author": ["Dan Roth", "Wen-tau Yih."], "venue": "Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005, pages", "citeRegEx": "Roth and Yih.,? 2005", "shortCiteRegEx": "Roth and Yih.", "year": 2005}, {"title": "Using semantic roles to improve question answering", "author": ["Dan Shen", "Mirella Lapata."], "venue": "EMNLPCoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language", "citeRegEx": "Shen and Lapata.,? 2007", "shortCiteRegEx": "Shen and Lapata.", "year": 2007}, {"title": "The conll 2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Llu\u0131\u0301s M\u00e0rquez", "Joakim Nivre"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Greedy, joint syntacticsemantic parsing with stack lstms", "author": ["Swabha Swayamdipta", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin,", "citeRegEx": "Swayamdipta et al\\.,? 2016", "shortCiteRegEx": "Swayamdipta et al\\.", "year": 2016}, {"title": "Online projectivisation for synchronous parsing of semantic and syntactic dependencies", "author": ["Ivan Titov", "James Henderson", "Paola Merlo", "Gabriele Musillo."], "venue": "In Proceedings of the Internation Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Titov et al\\.,? 2009", "shortCiteRegEx": "Titov et al\\.", "year": 2009}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "These representations have been shown beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al.", "startOffset": 104, "endOffset": 127}, {"referenceID": 2, "context": "These representations have been shown beneficial in many NLP applications, including question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011).", "startOffset": 155, "endOffset": 181}, {"referenceID": 17, "context": ", PropBank (Palmer et al., 2005)) typically represent arguments as syntactic constituents or, more generally, text spans (Baker et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 0, "context": ", 2005)) typically represent arguments as syntactic constituents or, more generally, text spans (Baker et al., 1998).", "startOffset": 96, "endOffset": 116}, {"referenceID": 7, "context": "The task of semantic role labeling (SRL), pioneered by Gildea and Jurafsky (2002), involves prediction of predicate argument structure, i.", "startOffset": 55, "endOffset": 82}, {"referenceID": 19, "context": "Until recently state-of-the-art SRL systems relied on complex sets of lexico-syntactic features (Pradhan et al., 2005) as well as declarative", "startOffset": 96, "endOffset": 118}, {"referenceID": 20, "context": "constraints (Punyakanok et al., 2008; Roth and Yih, 2005).", "startOffset": 12, "endOffset": 57}, {"referenceID": 23, "context": "constraints (Punyakanok et al., 2008; Roth and Yih, 2005).", "startOffset": 12, "endOffset": 57}, {"referenceID": 27, "context": "art results only in the multilingual setting, where careful feature engineering is not practical (Titov et al., 2009; Gesmundo et al., 2009; Henderson et al., 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al.", "startOffset": 97, "endOffset": 164}, {"referenceID": 8, "context": "art results only in the multilingual setting, where careful feature engineering is not practical (Titov et al., 2009; Gesmundo et al., 2009; Henderson et al., 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al.", "startOffset": 97, "endOffset": 164}, {"referenceID": 10, "context": "art results only in the multilingual setting, where careful feature engineering is not practical (Titov et al., 2009; Gesmundo et al., 2009; Henderson et al., 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al.", "startOffset": 97, "endOffset": 164}, {"referenceID": 6, "context": ", 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015).", "startOffset": 113, "endOffset": 212}, {"referenceID": 21, "context": ", 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015).", "startOffset": 113, "endOffset": 212}, {"referenceID": 26, "context": ", 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015).", "startOffset": 113, "endOffset": 212}, {"referenceID": 7, "context": ", 2013), neural SRL models now also outperform their traditional counterparts on standard benchmarks for English (FitzGerald et al., 2015; Roth and Lapata, 2016; Swayamdipta et al., 2016; Foland and Martin, 2015).", "startOffset": 113, "endOffset": 212}, {"referenceID": 28, "context": "Recently, it has been shown that an accurate span-based SRL model can be constructed without relying on syntactic features (Zhou and Xu, 2015).", "startOffset": 123, "endOffset": 142}, {"referenceID": 21, "context": "ticular, Roth and Lapata (2016) argue that syntactic features are necessary and show that performance of their model degrades dramatically if syntactic paths between arguments and predicates are not provided as an input.", "startOffset": 9, "endOffset": 32}, {"referenceID": 14, "context": "Our model is inspired by recent work in syntactic dependency parsing (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016).", "startOffset": 69, "endOffset": 124}, {"referenceID": 3, "context": "Our model is inspired by recent work in syntactic dependency parsing (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016).", "startOffset": 69, "endOffset": 124}, {"referenceID": 28, "context": "This reencoding idea is reminiscent of the region marking features used in the span-based model of Zhou and Xu (2015).", "startOffset": 99, "endOffset": 118}, {"referenceID": 21, "context": "for PathLSTM (Roth and Lapata, 2016)) and approaches the best results overall (87.", "startOffset": 13, "endOffset": 36}, {"referenceID": 5, "context": "One of the most effective ways to model sequences are recurrent neural networks (RNN) (Elman, 1990), more precisely their gated versions,", "startOffset": 86, "endOffset": 99}, {"referenceID": 11, "context": "for example, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 52, "endOffset": 86}, {"referenceID": 6, "context": "6 FitzGerald et al. (2015) (local) - - 86.", "startOffset": 2, "endOffset": 27}, {"referenceID": 6, "context": "6 FitzGerald et al. (2015) (local) - - 86.7 Roth and Lapata (2016) (local) 88.", "startOffset": 2, "endOffset": 67}, {"referenceID": 6, "context": "Instead of using a fixed matrix Wl,r or simply assuming that Wl,r = Wr, we, inspired by FitzGerald et al. (2015), found it beneficial to jointly embed the role r and predicate lemma l using a non-", "startOffset": 88, "endOffset": 113}, {"referenceID": 6, "context": "6 FitzGerald et al. (2015) (local) - - 75.", "startOffset": 2, "endOffset": 27}, {"referenceID": 28, "context": "Inspired by the spanbased SRL approach of Zhou and Xu (2015), we add a predicate-specific feature to the word representation by concatenating a binary flag to the word representation of Section 2.", "startOffset": 42, "endOffset": 61}, {"referenceID": 12, "context": "Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al., 2015); we replace a word with the unknown token UNK with probability \u03b1 fr(w)+\u03b1 , where \u03b1 is an hyper-parameter and fr(w) is the frequency of the word w.", "startOffset": 66, "endOffset": 86}, {"referenceID": 4, "context": "For the semantic role labeler, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 4, "context": "For the semantic role labeler, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al. (2015). Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al.", "startOffset": 62, "endOffset": 153}, {"referenceID": 4, "context": "For the semantic role labeler, we used external embeddings of Dyer et al. (2015) learned using the structured skip n-gram approach of Ling et al. (2015). Similarly to Kiperwasser and Goldberg (2016) we used word dropout (Iyyer et al.", "startOffset": 62, "endOffset": 199}, {"referenceID": 18, "context": "dings (Pennington et al., 2014).", "startOffset": 6, "endOffset": 31}, {"referenceID": 13, "context": "with Adam (Kingma and Ba, 2015).", "startOffset": 10, "endOffset": 31}], "year": 2017, "abstractText": "We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves respectable performance on English even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the CoNLL-2009 dataset. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e. syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on the standard out-of-domain test set.1", "creator": "TeX"}}}