{"id": "1701.02795", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Bidirectional American Sign Language to English Translation", "abstract": "we outline a cross translation system that converts sentences from american sign language ( asl ) to english, and vice versa. to perform machine translation between asl and english, we utilize a realistic approach. specifically, we employ hardware adjustment to the ibm word - alignment model process ( ibm wam1 ), where we propose language models for english and asl, emphasize translating as a translation model, and attempt to generate a translation that produces the local values defined by local models. then, using these models, we are able to quantify the concepts meaning fluency and faithfulness of a translation between languages.", "histories": [["v1", "Tue, 10 Jan 2017 21:45:56 GMT  (238kb,D)", "http://arxiv.org/abs/1701.02795v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hardie cate", "zeshan hussain"], "accepted": false, "id": "1701.02795"}, "pdf": {"name": "1701.02795.pdf", "metadata": {"source": "CRF", "title": "Bidirectional American Sign Language to English Translator", "authors": ["Hardie Cate", "Zeshan Hussain"], "emails": ["ccate@stanford.edu", "zeshanmh@stanford.edu"], "sections": [{"heading": null, "text": "Bidirectional American Sign Language to English Translator\nHardie Cate ccate@stanford.edu\nZeshan Hussain zeshanmh@stanford.edu\nDecember 11, 2015"}, {"heading": "1 Introduction", "text": "In the US alone, there are approximately 900,000 hearingimpaired people whose primary mode of conversation is sign language. For these people, communication with non-signers is a daily struggle, and they are often disadvantaged when it comes to finding a job, accessing health care, etc. There are a few emerging technologies designed to translate sign language to English in real time, but most of the current research attempts to convert raw signs into English words. This aspect of the translation is certainly necessary, but it does not take into account the grammatical differences between signed languages and spoken languages. In this paper, we outline our bidirectional translation system that converts sentences from American Sign Language (ASL) to English, and vice versa.\nTo perform machine translation between ASL and English, we utilize a generative approach. Specifically, we employ an adjustment to the IBM word-alignment model 1 (IBM WAM1)[14] where we define language models for English and ASL, as well as a translation model, and attempt to generate a translation that maximizes the posterior distribution defined by these models. Using these models, we are able to quantify the concepts of fluency and faithfulness of a translation between languages."}, {"heading": "2 Related Work", "text": "In examining papers and projects related to this topic, we have discovered that translation between signed and spoken languages is still largely an open problem. The few emerging technologies that attempt to tackle this issue are only capable of translating single words or short phrases of ASL into English. In fact, most approaches focus on analyzing videos of signs and converting these into words, which a number of recent CS229 projects have done. However, several teams at the University of Pennsylvania have taken a more grammatical approach [15] to the problem of English-to-ASL translation but not in the other direction. There has also been an Italian team that has designed a tentative translation system for Italian sign language,[7] but there are very few parallels with respect to grammatical structures that we can draw for this project. In short, there has been relatively little em-\nphasis in the literature on the conversion between the grammars of the languages, which is what we examine in this project."}, {"heading": "3 Task Definition", "text": "Let us formalize the notation for this task. For the sake of simplicity, we will use ASL as our \u201csource language\u201d and English as our \u201ctarget language.\u201d In general, S will be used to refer to the sentence in ASL, where S is a sequence of signs S1, . . . , Sm and m is the length of the sentence. Each sign Si is represented by a word with uppercase letters. We also include special \u201dgesture tokens,\u201d which represent gestures or hand movements (e.g., [point], [head shake]) that are not signs in and of themselves but rather modify other signs in the sentence. As a special case, which we address later, commas are also treated as individual sign tokens. Similarly, we will use E to refer to the English sentence, where E is a sequence of words E1, . . . , El and l is the length of the sentence.\nSinput : WRISTWATCH [point], WHO GIVE\u2212YOU? Eoutput : Who gave you that wristwatch?\nIn the backward direction, the input is a grammatically correct English sentence and the output is a sequence of signs represented by words. An example for this input is\nEinput : He fell down.\nSoutput : FELL.\nTo evaluate translations generated by our system as well as the baselines in both directions, we use the BLEU2 score,[10] which takes in a predicted sentence and a reference sentence, both in the target language. Let pn be the the ratio of the number of shared n-grams to the total number of n-grams in the predicted sentence. Then for a predicted sentence p and reference sentence s, the BLEU-N score is given by\nBLEU2(p, r) = e(1\u2212 |r| |p| ) N\u220f i=1 pN .\nThus BLEU-2 is essentially the ratio of shared unigrams and bigrams between the predicted translation and the gold-standard translation to the number that\nar X\niv :1\n70 1.\n02 79\n5v 1\n[ cs\n.C L\n] 1\n0 Ja\nn 20\n17\nappear in the predicted translation. The exponential factor at the beginning of the above expression is called the brevity penalty. Because the rest of the BLEU expression measures the precision of the prediction, it does not penalize the prediction for leaving out words. The role of the brevity penalty is to penalize predicted sentences that are shorter than their reference counterparts. As a concrete example, consider translating \u201dYOU LIKE COLOR BROWN?\u201d to the English \u201dYou like brown color?\u201d with the correct reference \u201dDo you like the color brown?\u201d. The BLEU-2 score of this prediction is e1\u2212\n6 4 ( 44 )( 1 3 ) = 0.202\nThe BLEU-2 metric is used primarily to rate translations at a corpus level, so it does not perform as well when used to evaluate translations of single sentences. Despite that, the BLEU-2 score should be a sufficient evaluation metric for the purposes of this project, since it is still the most common machine translation evaluator and is relatively simple to interpret."}, {"heading": "4 Data", "text": "Our data consists of 579 ASL/English translation pairs scraped from a repository from lifeprint.com,[12] a website dedicated to ASL education. Each pair is a single accurate translation of an ASL sentence or question into English. Many of the ASL sentences contain gesture tokens. These pairs are not specifically designed to be translations from English to ASL and in general, translation between languages is not symmetric. However, we make the simplifying assumption that this translation is symmetric primarily because we do not have a thorough enough understanding of ASL to translate from English to ASL.\nTo test our algorithms, we split the data into two main sets\u2013a training set (80% of original data) and a testing set (remaining 20%). We also have a third set, called the development set, that we use to tune our hyperparameters. This set is constructed by pulling out 10 examples from the training set and treating them as our development set. A more standard convention is to use a 70%-30% split for training and testing data, but we decided to use a larger training set size because we use these signs as our entire corpus for our ASL language model. We also use small subsets of the data to run tests to assess the performance of specific aspects of our system. For example, we construct a test set from the original test set to judge the performance of \u201dcomma-trigram\u201d feature of our ASL language model. We construct another test set to handle gesture tokens. To assess these tests, we use the BLEU-2 metric described in the task definition."}, {"heading": "5 Technical Approach", "text": "Below, we detail the baselines, oracles, and the IBM word alignment method for this problem."}, {"heading": "5.1 Baseline and Oracle", "text": "Because of the bidirectional nature of the problem, we have two baselines. For a baseline in English-to-ASL direction, we use a unigram cost function over English to detect the \u201cmost important\u201d words in the sentence and directly translate each word into ASL, preserving the order of these words. To pick the most important words, our baseline selects the highest cost words above some threshold based on our unigram cost function. In this case, a higher cost implies a lower frequency in the English language, suggesting higher importance. For our baseline in the other direction, we directly translate each sign into English and insert small helper words (such as \u201ca,\u201d \u201cthe,\u201d \u201cand,\u201d etc.), treating this as a search problem that tries to minimize the cost of the English sentence using a bigram cost function. To test these baselines, we utilized the same BLEU score as we did for the results generated by our IBM word alignment system. This approach allowed us to easily compare the baseline results and IBM WAM results, which are reported in the \u2019Results\u2019 section.\nFor our oracles in both directions, we have someone familiar with both languages translate each sentence into the other language. These translations are adequate oracles because they make the most sense from a human standpoint. Furthermore, the oracles serve as appropriate gold standard translations that can be used in the BLEU score calculation."}, {"heading": "5.2 Custom WAM", "text": "The IBM WAM1 is a common machine translation model that we customize to take advantage of common constructions in ASL and allow us to modify how the translation model and language models are constructed. The custom model is meant to give us flexibility in our translations.\nWe now describe the general workflow of our model in the ASL to English direction. This workflow applies to the other direction as well, except with E switched with S: 1). For each potential translation, we calculate p(E) 2). Then, given this channel input, we calculate p(S|E) in the \u201cnoisy channel\u201d 3). Finally, we choose the E that gives the highest posterior probability given these two probabilities (likelihood and prior). The p(E) is represented by the language model while the p(S|E) is represented by the translation model. We will derive this formulation below."}, {"heading": "5.2.1 Modeling", "text": "For concreteness, we will use a running translation example in the descriptions of our models and algorithms. Consider the following gold-standard translation from ASL to English:\nSinput : YOU LIKE LEARN SIGN?\nEoutput : Do you like learning sign language?\nTo derive the models that we will use in the translation, we consider the following optimization problem (note that once again we are writing this derivation in the ASL to English direction but the same derivation applies to the other direction):\nGiven a sentence S in ASL, we want to find an English sentence such that:\nE = arg max E\u2208English\nP (E|S)\nE = arg max E\u2208English\nP (S|E)P (E)\nThe probability P (E) represents the fluency of E in English, i.e. how much the translation makes sense to a native speaker, while P (S|E) represents the faithfulness of the translation from S to E, i.e. if the translation reflects the actual meaning of what is being said. We determine P (E) using a language model for English and P (S|E) using a translation model from English to ASL. (Note that in our overall translation from ASL to English, we model the probability of a given translation from English to ASL.)"}, {"heading": "5.2.2 Language Model", "text": "To compute P (E) for a given English sentence E, we use an n-gram cost function. For instance, in our example sentence E above in the case where n = 3,\np(E) = p3(null,null, \u201ddo\u201d) \u00b7 p3(null, \u201ddo\u201d, \u201dyou\u201d) \u00b7 p3(\u201dlearning\u201d, \u201dsign\u201d, \u201dlanguage?\u201d)\nwhere p3 is a 3-gram cost function. We have imported n-gram cost functions from http://www.ngrams.info/ [8] for n = 2, 3, 4, 5.\nThere are a couple of caveats with the above approach when trying to create a language model for ASL, which is necessary for translation in the other direction (i.e. to calculate P (S)). The first is that our ASL language model is basically a unigram model that has additional components. In addition to each sign having an associated probability, which is a simple unigram model, we add a comma trigram construction. This construction incorporates into the language model the following grammatical structure that is commonly found in ASL,\n([NOUN]|[NOUNPHRASE])[, ][PHRASE]\nIn other words, an infrequent (uncommon) noun will be followed by a comma, which is followed by a phrase that generally describes or refers to the noun. The phrase after the comma usually starts with a more frequent (common) word. The way the comma trigram encapsulates this is by increasing the unigram probability of the second word and decreasing the unigram probability of the first word. This effectively favors a big difference between the word before the comma and the word after the comma, which in turn favors an uncommon word being translated before\nthe comma and a common word after the comma. The second caveat is that we incorporate gesture tokens, which are contained in brackets, into our corpus. Our language model does not, however, treat these gesture tokens any differently than regular signs."}, {"heading": "5.2.3 Translation Model", "text": "Now, we wish to estimate the probability P (S|E) given a trained translation model. We represent this model as a mapping from \u201dsign-English\u201d pairs to probabilities (of those pairs) using a dictionary in python. Training this model requires us to introduce a set A = a1, . . . , aJ of alignment variables where aj represents the index of the word in the English sentence that translates to the jth word sj in S. We also allow these variables to take on a value of 0, which represents a \u201cnull\u201d word in the English sentence. This null word allows for the possibility that there is no word in the English sentence that directly translates to some sign sj in S. In the case of our example above, if A specifies that \u201clearning\u201d in E translates to \u201cLEARN\u201d in S, then a3 = 4, since \u201cLEARN\u201d is the third word in S and \u201clearning\u201d is in the fourth position of E. Thus, A can be thought of as a many-to-one mapping from words in E to signs in S. (In general, this mapping would need to be many-to-many since we sometimes require phrase-to-phrase translation, but for our purposes the many-to-one assumption is reasonable because generally speaking, multiple English words map to only one sign.) Then, to compute P (S|E), we marginalize out the alignment variables\u2211\nA P (S,A|E) = \u2211 A P (S|A,E)P (A|E).\nNow, let I be the number of words in E. For fixed I, we assume that each possible alignment for each length is equally likely, which gives us that P (A|E) =\n(1+I)J . This\nprobability can be thought of as a normalization factor for the probability, P (S|E). Indeed, P (S|A,E) is given by,\np(S|A,E) = J\u220f j=1 t(sj , eaj ),\nwhere t(s, e) is the probability of translating e as s. Therefore, this gives us the following for P (S|E),\np(S|E) = \u2211 A (1 + I)J J\u220f j=1 t(sj , eaj ),\nAfter training on our dataset, constructing a language model for each language using the corpa we have processed, and generating a parameterized translation model using the EM algorithm, we can use our decoder to find the optimal translation. Both the EM and decoding algorithms are described in detail in the next section."}, {"heading": "5.2.4 Algorithms", "text": "The two primary algorithms used in our implementation are an EM algorithm to train our translation model which defines P (S|E) and our decoding algorithm. The decoding algorithm is a modified version of the decoding in IBM WAM1, which uses a variant of beam search to find\narg max E\nP (S|E)P (E).\nFirst we describe the EM algorithm. Our language model maintains a mapping t from pairs (s, e), where s is a sign and e is an English word, to probabilities t(s, e) = p(s|e). Given our training data containing a list of m translation pairs (S,E), we can rewrite our estimate of a single transition probability as\np(s|e) = m\u2211 i=1 \u2211 Ai p(Si, Ai|Ei)\n= m\u2211 i=1 \u2211 Ai p(Ai|Ei) J\u220f j=1 t(sj , eaj )\nwhere Ai is the alignment vector for training example i. To estimate t(s, e) given the data, we need to find the maximum likelihood of this probability. However, with the introduction of these latent variables, it is not possible to find this likelihood in a closed form. Therefore, we use the EM algorithm to estimate this likelihood by computing the probability of each possible alignment given a distribution of t(s, e) for each s and e in the E-step and then adjusting this distribution based on these alignment probability estimates in the M-step. The algorithm runs as follows:\nEM for ASL-to-English translation model while Not converged do\nSet t(s, e) uniform, including t(s,NULL). Set counts of e translating to s as C(s, e) = 0\u2200s, e. for (S,E) in training set do\nfor sign position i = 1, . . . , l do\nP (ai = j|S,E) = t(si,ej)\u2211 j\u2032 t(si,ej\u2032 )\nC(si|eai)+ = P (ai = j|S,E) end for\nend for for all s, e do\nt(s, e) = C(s,e)\u2211 s\u2032 C(s \u2032,e)\nend for end while\nOur decoding algorithm is a modification of the conventional decoding algorithm used in IBM WAM1. We frame the problem of finding the best English translation as a type of search problem. Let l be the length of S. The algorithm is a variant on BEAM search that maintains a list of priority queues q0, q1, . . . , ql of hypotheses. Each hypothesis consists of a list of English words e each with a corresponding sign se from S. Initially, all priority queues are empty, with the exception of q0, which contains the\nempty hypothesis. Then we iterate through the priority queues, and for the top hypothesis h in the current queue, we generate some number of new hypotheses h1, . . . , hk by adding a single English word as the next word in h. We place each new hypothesis h\u2032 in qi, where i is the number of signs in S that have been translated to English words in h\u2032. Each hypotheses h is prioritized according to\nlog |E|\u2211 j=1 p(sej |ej) +W log p(E)\nwhere E = [e1e2 . . . e|E|] is the English sentence contained in h and W is the language model weight whose purpose is explained below. The aspect related to BEAM search is that we only consider the first k hypotheses in each qi before moving on to qi+1. Thus, we effectively prune hypotheses that do not appear among the first k for a given number of translated signs.\nModified decoding algorithm Set q0 to PQ with empty hyp. Set each q1, . . . , ql to empty PQ for i = 0 to l \u2212 1 do\nfor j = 0 to MAX QUEUE SIZE do hyp = qi.pop() for k = 1 to l do\nnewhyp = max over s, e of newhyp(s, e) insert new hyp into appropriate queue\nend for end for\nend for Return ql.pop()\nwhere newhyp(s, e) is the new hypothesis formed by translating s \u2208 S to e and newhyp is the maximum according to the priority measure. Once the algorithm reaches the final priority queue, all l signs have been translated. This implies that the top hypothesis in the priority queue is the hypothesis that translates all signs in S to English (possibly to the NULL word) and maximizes p(S|E)p(E), i.e., the hypothesis with English sentence\nE = arg max e\u2208ql\np(e|S)."}, {"heading": "5.2.5 Algorithms Commentary", "text": "One of the most important aspects of the algorithm is that a hypothesis h can generate a new hypothesis h\u2032 that is placed in the same queue as h if it is generated by translating a sign already translated in h. This is important because it allows us to translate a single sign to multiple English words. In particular, this allows the translated English sentence to be longer than the input sign sentence, which is the case with most English translations.\nAnother important note concerns the language model weight. If this weight is set to 1, the priority is simply p(E|S), i.e., the value we are trying maximize. However, we find that if we try to maximize this value,\nthe language model probabilities \u201doutweigh\u201d the probabilities of the translation model. In effect, the sign sentence is likely to be translated as a set of common English n-grams unrelated to the original sentence. For instance, when we attempt to translate YOUR SISTER SINGLE? as \u201dIs your sister single?\u201d, our top result is \u201din the first\u201d. However, when we introduce this language model weight with W = 0.3, we produce the translation \u201dYour sister single?\u201d. Although this translation might not be as common in English as \u201din the first\u201d, it certainly preserves the meaning of the original sentence much better.\nJust as with BEAM search, our decoding algorithm is not guaranteed to converge to the optimal result (in fact, this problem in NP-complete), but in many cases it provides good results."}, {"heading": "6 Results", "text": "As mentioned in the Data section, we split the dataset into three separate sets: a training set, a development set, and a test set. For both directions, we train our models on a training set, tune the hyperparameters (i.e. the language model weight, the queue size k, and the type of n-gram model we are using for the language model [unigram, bigram, trigram]) on a development set, and finally find the average BLEU-2 score on a test set. Furthermore, our metric for measuring the accuracy of translation on the test set or the development set is the average of individual BLEU-2 scores of the translations. Thus, any BLEU-2 score that is reported in tables or figures will be the mean BLEU-2 score."}, {"heading": "6.1 Hyperparameter Tuning Results", "text": "To find the optimal hyperparamaters, we ran several experiments. The initial set of experiments were run to find the optimal hyperparameters for translation from ASL to English. First, we found the BLEU-2 score on the development set by keeping the queue size constant and adjusting the language model weight using the bigram English model. Next, we performed the same experiment except using the trigram English model. Results of these experiments are reported in tables 2 and 3. We find that the optimal language model weight is 0.1, the optimal queue size for ASL to English translation is 20, and the optimal language model type is a trigram model. Using this combination gives us a BLEU-2 score of 0.276 (see Table 3), which is the highest BLEU-2 score that we obtained while tuning hyperparameters.\nTo provide some intuition on the choice of experiments, we reasoned that we could converge on the optimal hyperparameter combination using a method similar to coordinate ascent, where we optimize a single parameter while keeping the other parameters constant. Then, after finding the optimal value for the parameter, repeat the process with the other parameters while still maintaining the optimal values for the parameters that have been already processed. Using the (20,0.1,\u2019Trigram\u2019) combination, we ran our system on the test set; the BLEU-2 score, reported in Table 1, is 0.1202.\nThe subsequent experiments involved finding the optimal hyperparameters for translation from English to\nASL. Using a similar methodology to the one used in the other translation direction, we ran one set of experiments where the queue size was kept constant while the language model weight was adjusted. Because we only used a unigram ASL language model while translating in this direction, running only this set of experiments is sufficient to find the optimal hyperparameter pair. Results of the experiment are reported in Table 4. We find that the optimal language model weight is 0.1 and the optimal queue size is 20. Using the (20,0.1) combination and running the system on the test set, we obtain a BLEU-2 score, reported in Table 1, of 0.1802."}, {"heading": "6.2 Experiments for ASL Constructions", "text": "The last two sets of experiments test how our system performs on translating specific ASL constructions. Firstly, we perform an experiment on translating only the common comma construction described above in Modeling, by filtering for these test examples in the test set and using the optimal ASL to English hyperparameters when running on the filtered test set. The BLEU-2 score result for this test is .1201 (see Table 1).\nFinally, the last experiment tests how our system translates sign sequences with gesture tokens. We create a test set for this experiment in a similar fashion to what we did for comma constructions, by filtering the original test set to only include sign sequences with gesture tokens. The BLEU-2 score for this test is .1232 (see Table 1)."}, {"heading": "7 Analysis", "text": "As a sanity check for our results, it is important to note that our translation system outperforms the baseline implementations in both directions. We also observe a difference in performance between the two translational directions. The ASL-to-English direction produces an average BLEU-2 score of 0.12, while the English-to-ASL gives a higher score of 0.18. The behavior is consistent with our expectations going into the project since in going from English to ASL, we essentially filter out unnecessary information, whereas the other direction requires generating information in effect.\nAfter running our tuning experiments on the hyperparameters, we noticed a few interesting trends. First, instances in which we use a trigram model for English tend to produce higher BLEU-2 results than with corresponding bigram English models, particularly when the maximum queue size in the decoding algorithm is large. This makes sense because the trigram model inherently captures more information than the bigram model. Furthermore, we see from the two graphs above that for large maximum queue sizes, instances with small language model weights perform better. We can explain this as follows: For large maximum queue sizes, after the first few iterations of the decoding algorithm, we will see hypotheses with common words. If we do not sufficiently dampen the influence of the language model, we will begin to see hypotheses with high priorities resulting from common ngrams, and these will steer the sentence generation away from the original meaning of the sign sentence. Therefore, we conclude that models with high maximum queue sizes and low language model weights generally perform best.\nAlthough it is useful to look at how changing these hyperparameters changes the model, it is perhaps even more important to recognize the shortcoming of our current model. The first issue is with our corpus, which by most measures is far too small to perform accurate machine translation. We also lack large databases containing sign language as text which limited our language model for ASL. Unfortunately, our ASL language model was far too crude to produce accurate results. We oversimplified the model in several ways, in particular with our comma trigram structure and treatment of gestures as equivalent to other signs. Because we are not fluent in ASL, let alone fully understand its underlying structure, we had difficulty in designing its language. In fact, to our knowledge, no one else has designed a text-based language model for any sign language. Finally, machine translation for single sentences is inherently a difficult task, since the sentences lack context. Despite these many challenges and difficulties, our system still translates many sentences effectively between the two languages in both directions."}, {"heading": "8 Conclusion & Future Work", "text": "While we would like our translations to be as accurate as possible, it is important to note that it is virtually impossible, even for an oracle, to come up with a perfect translation. In any language, words and sentences carry contextual meaning that might be impossible to express exactly in another language. The problem of translating between sign languages and natural languages is extremely difficult, and we are likely the first to address it exactly this way. Although we faced numerous challenges, we have shown that this approach is a reasonably effective improvement to our baseline algorithms. With a more thorough understanding of ASL and its grammatical structure, along with a larger training corpus, our approach has the potential to be an effective system of translation between ASL and English."}, {"heading": "9 References", "text": "[1] American Sign Language Dictionary. http://www. signasl.org. Accessed: 2015-12-11.\n[2] American Sign Language Grammar. http://www. lifeprint.com/asl101/pages-layout/grammar.\nhtm. Accessed: 2015-12-11.\n[3] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python. \u201d O\u2019Reilly Media, Inc.\u201d, 2009.\n[4] Introduction to Statistical MT. https://sparkpublic.s3.amazonaws.com/cs124/slides/mt2.\npdf. Accessed: 2015-12-11.\n[5] Machine Translation: Decoding. http : / / www . statmt . org / book / slides / 06 - decoding . pdf. Accessed: 2015-12-11.\n[6] Daniel Ortiz Mart\u0301\u0131nez, Ismael Garc\u0301\u0131a Varea, and Francisco Casacuberta Nolla. \u201cGeneralized stack decoding algorithms for statistical machine translation\u201d. In: Proceedings of the Workshop on Statistical Machine Translation. Association for Computational Linguistics. 2006, pp. 64\u201371.\n[7] Alessandro Mazzei et al. \u201cDeep Natural Language Processing for Italian Sign Language Translation\u201d. In: AI* IA 2013: Advances in Artificial Intelligence. Springer, 2013, pp. 193\u2013204.\n[8] N-grams data. http://www.ngrams.info/. Accessed: 2015-12-11.\n[9] Natural Language Processing: Machine Translation. http : / / web . stanford . edu / class / cs224n /\nhandouts/cs224n- lecture3- MT.pdf. Accessed: 2015-12-11.\n[10] Kishore Papineni et al. \u201cBLEU: a method for automatic evaluation of machine translation\u201d. In: Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics. 2002, pp. 311\u2013318.\n[11] Becky Sue Parton. \u201cSign language recognition and translation: A multidisciplined approach from the field of artificial intelligence\u201d. In: Journal of deaf studies and deaf education 11.1 (2006), pp. 94\u2013101.\n[12] Sign Language Phrases. http://www.lifeprint. com/asl101/index/sign- language- phrases.\nhtm. Accessed: 2015-12-11.\n[13] Signing Savvy. https://www.signingsavvy.com/. Accessed: 2015-12-11.\n[14] Statistical Machine Translation: IBM Model 1 and 2. http://www.cs.columbia.edu/~mcollins/ courses/nlp2011/notes/ibm12.pdf. Accessed: 2015-12-11.\n[15] Liwei Zhao et al. \u201cA machine translation system from English to American Sign Language\u201d. In: Envisioning machine translation in the information future. Springer, 2000, pp. 54\u201367."}], "references": [{"title": "Natural language processing with Python. ", "author": ["Steven Bird", "Ewan Klein", "Edward Loper"], "venue": "O\u2019Reilly Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Generalized stack decoding algorithms for statistical machine translation", "author": ["Daniel Ortiz Mart\u0301\u0131nez", "Ismael Gar\u0107\u0131a Varea", "Francisco Casacuberta Nolla"], "venue": "Proceedings of the Workshop on Statistical Machine Translation. Association for Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Deep Natural Language Processing for Italian Sign Language Translation", "author": ["Alessandro Mazzei"], "venue": "AI* IA 2013: Advances in Artificial Intelligence. Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Sign language recognition and translation: A multidisciplined approach from the field of artificial intelligence", "author": ["Becky Sue Parton"], "venue": "Journal of deaf studies and deaf education", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A machine translation system from English to American Sign Language", "author": ["Liwei Zhao"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}], "referenceMentions": [{"referenceID": 5, "context": "However, several teams at the University of Pennsylvania have taken a more grammatical approach [15] to the problem of English-to-ASL translation but not in the other direction.", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "There has also been an Italian team that has designed a tentative translation system for Italian sign language,[7] but there are very few parallels with respect to grammatical structures that we can draw for this project.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "To evaluate translations generated by our system as well as the baselines in both directions, we use the BLEU2 score,[10] which takes in a predicted sentence and a reference sentence, both in the target language.", "startOffset": 117, "endOffset": 121}], "year": 2017, "abstractText": "In the US alone, there are approximately 900,000 hearingimpaired people whose primary mode of conversation is sign language. For these people, communication with non-signers is a daily struggle, and they are often disadvantaged when it comes to finding a job, accessing health care, etc. There are a few emerging technologies designed to translate sign language to English in real time, but most of the current research attempts to convert raw signs into English words. This aspect of the translation is certainly necessary, but it does not take into account the grammatical differences between signed languages and spoken languages. In this paper, we outline our bidirectional translation system that converts sentences from American Sign Language (ASL) to English, and vice versa. To perform machine translation between ASL and English, we utilize a generative approach. Specifically, we employ an adjustment to the IBM word-alignment model 1 (IBM WAM1)[14] where we define language models for English and ASL, as well as a translation model, and attempt to generate a translation that maximizes the posterior distribution defined by these models. Using these models, we are able to quantify the concepts of fluency and faithfulness of a translation between languages.", "creator": "LaTeX with hyperref package"}}}