{"id": "1701.02925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Question Analysis for Arabic Question Answering Systems", "abstract": "the first step / processing a question in question answering ( qa ) test is to carry out a quick analysis of the question for the purpose of determining as marker is asking for and how to perfectly approach answering it. our question analysis uses several solutions to analyze any question given in natural language : a stanford pos tagger & amp ; parser for arabic language, a named entity recognizer, tokenizer, stop - word removal, question expansion, question evaluation and question focus extraction components. we employ numerous detection systems incorporating trained classifier using features from this analysis to detect important characteristics outside the question, including : 1 ) the portion of the question that is a referring given the meaning ( the focus ) ; 4a ) different readings in the question that identify what line of asking is being asked for ( the lexical answer types ) ; 3 ) question classifications ; 4 ) an process of classifying the question into one or more of several and many types ; and we describe how these elements are formed and evaluate the effect of accurate detection on our question - answering system via the mean reciprocal rank ( mrr ) accuracy measure.", "histories": [["v1", "Wed, 11 Jan 2017 11:12:24 GMT  (379kb)", "http://arxiv.org/abs/1701.02925v1", "10 pages, 3 figures, published article in IJNLC"]], "COMMENTS": "10 pages, 3 figures, published article in IJNLC", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["waheeb ahmed", "dr anto p babu"], "accepted": false, "id": "1701.02925"}, "pdf": {"name": "1701.02925.pdf", "metadata": {"source": "CRF", "title": "QUESTION ANALYSIS FOR ARABIC QUESTION ANSWERING SYSTEMS", "authors": ["Waheeb Ahmed"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121/ijnlc.2016.5603 21\nThe first step of processing a question in Question Answering(QA) Systems is to carry out a detailed analysis of the question for the purpose of determining what it is asking for and how to perfectly approach answering it. Our Question analysis uses several techniques to analyze any question given in natural language: a Stanford POS Tagger & parser for Arabic language, a named entity recognizer, tokenizer, Stop-word removal, Question expansion, Question classification and Question focus extraction components. We employ numerous detection rules and trained classifier using features from this analysis to detect important elements of the question, including: 1) the portion of the question that is a referring to the answer (the focus); 2) different terms in the question that identify what type of entity is being asked for (the lexical answer types); 3) Question expansion ; 4) a process of classifying the question into one or more of several and different types; and We describe how these elements are identified and evaluate the effect of accurate detection on our question-answering system using the Mean Reciprocal Rank(MRR) accuracy measure.\nKEYWORDS\nQuestion Analysis, Question Answering, Information Retrieval, Information Extraction."}, {"heading": "1. INTRODUCTION", "text": "Question analysis is the first stage of any QA system and the accuracy of its results significantly impacts on the following stages of information retrieval and answer extraction. To get a better result, the semantic information available in questions should be extracted for question analysis. The question answering process in most of question-answering systems, starts with a question analysis phase that tries to determine what the question is looking for and how to effectively approach answering it[1]. Generally speaking, question analysis module receives the unstructured text question as input and identifies the syntactical and semantically elements of the question, which are kept as structured information that is used later by the many components of our QA system. Almost all of our QA system components rely in some way on the information generated by question analysis stage[2]. Question analysis is built on the top of parsing, tagging and semantic analysis components. we employ numerous recognition rules and classifiers to identify numerous critical elements of the question. There are a several and variety of such elements, each of which is crucial to different parts of the question processing phase. The most important elements are the focus, answer types (AT), Question Classification, and Question Terms(QTerms). In addition, question is expanded by adding synonyms of its terms to improve the accuracy of the retrieval process. After the question pre-processing and processing steps are done, the final stage is to extract the answer from the retrieved documents."}, {"heading": "2. QUESTION ANALYSIS MODULE", "text": "This module is responsible for analyzing the question carefully before sending it to the Information Retrieval Module. The Question processing module consists of three sub-modules, the Tokenizer, Class Extractor and Focus detector as illustrated in Figure 1. The first module is for Splitting the question into individual tokens, the second module is for identifying the class of the question and the third module for extracting the question\u2019s focus. The focus of the question specifies what the given question is exactly looking for. The following figure shows the architecture of our proposed QA system along with the different sub-modules used for question processing.\nFigure 1. shows the different stages through which the question is handled until the final answer is extracted and generated to the user. The following sections elaborate on the various subtasks applied to the question to extract relevant information that could assist the subsequent stages of the QA system."}, {"heading": "2.1. Question tokenization", "text": "The pre-processing step is tokenization. The first step in question analysis is to identify tokens, or those elementary units which do not require to be decomposed in a subsequent processing. The entity word is defined as one kind of token for Natural Language Processing(NLP) in general and specifically in QA, the most basic one[3]. Tokenization is a crucial step in QA . It can be considered as a preparation stage for all other natural language processing tasks. Tokenization is the task of splitting words (morphemes) from running text [4]. Word Segmentation(tokenization) is getting words from text. The space is a good separator for this purpose but it will not work with special cases as compound words[5]. Some compound words are written with a space in the middle even though they are single words. Therefore, tokenization is a necessary and non-trivial step in natural language processing [6]. It is much related to the morphological analysis but usually it has been considered as an independent process [7]. Arabic words are often ambiguous in their morphological analysis. This is due to Arabic\u2019s rich language of affixes and clitics and the elimination of disambiguating short vowels and other diacritics in standard orthography (\u201cundiacritized orthography\u201d). On average, a word form in the Arabic Tree Bank(ATB) has about 2 morphological analyses [8]. Arabic word can come in the form [Procltics] + [inflected word] +[Enclitics]. Then, tokenization is similar/equivalent to word segmentation in Chinese language where Arabic word is as a sentence in Chinese language[9]. This sub-module splits the question into separate terms so that it can be further processed by subsequent modules in the QA system. For example, the question: \u201c\u061f \u0627 \u0642 \u0627\u0648 \u0627\u0648 \u0627 \u0631 \u0627 \u06be \u201d(\u201cWhat is considered the costliest disaster the insurance industry has ever faced ? \u201d) will be split into the following tokens(\u061f, , \u06be , \u0627 ,\u0642 , \u0627\u0648 , \u0627 ,\u0648 , , \u0627 , \u0631 \u0627)."}, {"heading": "2.2. Stop Words Removal", "text": "This sub-module removes the prepositions, Conjunctions and interrogative words. Since the prepositions and conjunctions occurs very frequently in the documents, these words can add any benefit for the information retrieval IR) module[8ooo][10]. The IR module identifies the target documents by means of the terms that are occurring very less times in the documents. After removing the stop words the remaining thing will be the important terms in the question."}, {"heading": "2.3. Question Expansion", "text": "Traditional keyword based search for information is proved to have some limitations. This include word sense ambiguity, and the question intent ambiguity which can badly affect the precision. To get rid of these limitations we need to adopt semantic information retrieval techniques. These techniques are concentrating on the meaning the user looking for rather than the exact words of the user\u2019s question. We consider four main features that make users prefer semantic based search systems over keyword-based: Handling Generalizations, Handling Morphological Variants, Handling Concept matches, and Handling synonyms with the correct sense (Word Sense Disambiguation)[11][12]. In question expansion, synonyms for nouns and adjectives in the question are added to the list of question terms. Since the documents which may contain the answer for the question may not contain the terms that the user used in his question. Therefore, expanding the user question by adding synonyms to the nouns and adjectives of the\nquestion will increase the chance of getting the answer[13] and for this we used the Arabic WordNet[14]."}, {"heading": "2.4. Class Extraction", "text": "We used a trained Support Vector Machine(VSM) Classifier from our previous work[15]. The classifier will receive the question and give a label to the question. It is trained to produce a label based on two level classification. For example, \u201cWhy do heavier objects travel downhill faster ?\u201d the output of the classifier will be \u201cDESCRIPTON:reason\u201d that is, the question is asking for descriptive answer and this is the coarse grain type of the answer. The fine grain type of the answer is \u201creason\u201d. The class extraction module sends its output to Answer Extraction(AE) module to apply the proper technique for extracting the answer. Table 1 shows the different classes as per the proposed scheme by Li & Roth[16].\nTable 1. Question classes\nQuestion class(Level 1) Question class(Level 2)\nHUMAN\nGroup Individual Title Description\nLOCATION\nCountry State City Mountain other\nNUMERIC\nCount Date Money Distance Speed Percent Other\nDESCRIPTION\nDefinition Manner Reason\nENTITY\nColor Animal Technique Planet other\nFor example:\nQuestion 1: \u201c \u0629 ! \u06be \u061f\u0646 \u0627 \u0631# \u201d (\u201cWhat is a dental root canal ?\u201d ) Question Class=DESCRIPTION:definition\nThe general type of answer for question 1 is \u201cDESCRIPTION\u201d that is the question is looking for description and the type of description is \u201cdefinition\u201d.\nQuestion 2: \u201c $ %& \u0627 '( \u0627 ) \u0627 \u062f+, - \"\u061f/%0 \u0627 \u0644 $ \u0631\u0648+( 2 (\u201cHow many months does it take the moon to revolve around the Earth ?\u201d)\nQuestion Class= NUMBER:count\nIn question 2, the answer type is \u201cNUMBER\u201d and more specifically a \u201ccount\u201d. Hence, the number of months(count) the moon take to revolve around the earth is the required answer."}, {"heading": "2.5. Focus identification", "text": "The question focus is the set of nouns and noun phrases(NPs) available in the question. The question focus information is used by the AE module for ranking the candidate answers.\nFor example:\nQuestion 3: \u201c ( \u0623 \u0644\u0648\u0623 \u06be +45 \u061f\u0621 7 \u0627 \u201d (\u201cWho was the first American in space?\u201d) Class Extraction: HUMAN: individual FOCUS= \" \u0621 7 \u0627 +45 ( \u0623 \u0644\u0648\u0623\" (the first American in space) FOCUS-HEAD = \" \" ( \u0623 (American) FOCUS-MODIFIERS=ADJ \"\u0644\u0648\u0623\"(first), COMP \"\u0621 7 \u0627 +45\" (in space)\nQuestion 4: \u201c\u061f & 8 \u0627 \u0628 4 \u0627 \u0641 0 ; \u0645+8 => \u0627 & \u0627 \u06be \u201d (\u201cName a technique widely used to detect birth defects ?\u201d) Class Extraction: ENTITY:technique FOCUS=\u201d & 8 \u0627 \u0628 4 \u0627 \u0641 0 ; \u0645+8 => \u0627 & \u0627 \u201d(a technique widely used to detect birth defects) FOCUS-HEAD =\u201d & \u0627\"(technique) FOCUS-MODIFIERS=ADV \u201c? \u0627\u0648 @ 0A\u201d(widely), COMP \u201c \u0628 4 \u0627 \u0641 0 ; \u0645+8 => \u0627 & 8 \u0627\u201d(used to detect birth defects)\nTo extract the above information we Once the question terms are tagged , the focus, focus-head, and modifiers of the focus head could be extracted. The FOCUS chunk is extracted by rule-based technique. Where several grammar rules are applied and that is because the noun phrases can come in a variety of forms and for each form a unique grammar rule is used. The rule-based chunking for nouns and noun phrases is based on the POS Tagging information produced by Stanford POS Tagger for Arabic[17].\nQuestion 5: \u201d ( \u0627 \u0644B& ;\u0627 \u0628 $ C D\u062f \u0646 \u0627 A\u0648\u0631\u0648 \u0627 \u0648+ \u0627 E F( G \u0627 +H\u061f \u201d (What two European countries entered the War of American Independence against the British?) Class Extraction: LOCATION:country\nThe POS tags foe question 5:\n/WP \u0648+ \u0627/DTNNS A\u0648\u0631\u0648;\u0627/DTJJ \u0646 \u0627/WP D\u062f/VBD C/IN \u0628 $/NN \u0644B& ;\u0627/DTNN ( ;\u0627/DTJJ +H/NN E F( G /DTNNS\nThe generated syntax tree for question 5:\nFigure 2 shows the different noun phrases identified by NP for question 5. These noun phrases are extracted as question focus. For parsing Arabic questions, Stanford Parser for Arabic language is used[18]."}, {"heading": "3. DOCUMENT RETRIEVAL", "text": "The expanded list of terms extracted from the question along with the synonyms will be sent to the IR module for document retrieval, We implemented our IR module using the Vector Space Model for its simplicity of implementation and also its efficiency[19]. The system first extracts text from the top 10 retrieved documents from which the top three documents are selected for further processing by the AE module."}, {"heading": "4. ANSWER EXTRACTION", "text": "It initiates by processing a document using several procedures: first, the raw text of the document is divided into sentences with the help of a sentence segmenter, and each sentence is further subdivided into words(tokens) using a tokenizer. Next, each sentence is tagged with partof-speech tags, which will help the named entity detection[20]. This module applies different techniques for extracting different types of answers. For example, if the question class given by the class extraction module is \u201cHUMAN:individual\u201d this means the question is looking for a person name. So, the AE module will use Named Entity Recognizer technique to get the answer. Questions which ask for dates a pattern matching technique will be used. Answer selection and\nranking: To select answer from the top 5 generated answers/sentences by the AE module the Answer selection and ranking stage use the question focus for this purpose.\n\u2022 For extracting answer types of \u201cHUMAN\u201d, \u201cLOCATION\u201d we use Named Entity Recognizer. A Named Entity Recognition (NER) system is a significant tool in natural\nlanguage processing (NLP) research since it allows identification of proper nouns in open-domain (i.e., unstructured) text. For the most part, such a system is simply recognizing instances of linguistic patterns and collating them[21]. An important component of a QA system is the named entity recognizer and virtually every QA system incorporates one. Many natural language processing applications require finding named entities (NEs) in textual documents. NEs can be, for example, person or company names, dates and times, and distances. The task of identifying these in a text is called named entity recognition and is performed by a named entity recognizer (NER). The rationale of incorporating a NER as a module in a QA system is that many fact-based answers to questions are entities that can be detected by a NER. Therefore, by incorporating in the QA system a NER, the task of finding some of the answers is simplified considerably.[22] The positive impact of NE recognition in QA is widely acknowledged and there are studies that confirm it The positive impact of NE recognition in QA is widely acknowledged and there are studies that confirm it [23].\n\u2022 For extracting answer types of \u201cNUMERIC\u201d we use Regular Expressions(RE) [24]. where a set of regular expressions for different numeric formats are used.\n\u2022 For extraction answer types of \u201cDESCRIPTION\u201d we use semantic similarity measure between the question terms and the document sentences[25]. We developed answer\nextraction and passage retrieval techniques for Arabic language in our previous\nworks[26][27]. In order to identify the relevance of a likely answer to a question, a semantic similarity calculation was employed to compute the semantic similarity between the question sentence and the answer sentence."}, {"heading": "5. RESULTS AND EVALUATION", "text": "In this study, we presented a combination of techniques to question analysis, employed in a closed-domain question answering system for Arabic language. Our question analysis module consists of several subtasks importantly focus extraction and question classification. For focus extraction, we have multiple rule-based approach based on the output of Stanford POS Tagger for Arabic. Additionally, we described a classification approach for question classification. For question classification, we employed a machine learning classifier which uses a trained model to each class. In addition to the methodology presented, we also used a set of manually annotated questions for testing the system.\nThe assessment methods of answer extraction for the different types of questions supplied to the QA system is based on Text Retrieval Conference(TREC)[28], using MRR (Mean Reciprocal Rank) standards shown in the following formula:\nMRR= \u2211\n(1)\nWhere, n refers to the number of the questions to be tested and ri refers to the position of the first correct answer to the question number i, if there is no correct answer available in candidate sentences, the value will be 0. We used a set of 250 questions translated from TREC-10\nDataset. For each question type a set of 50 questions is used. The corpus is Open-domain(based the world wide web).\nFrom table 1. It is clear that the performance of our QA system has got highest score for questions of type \u201cHUMAN\u201d, e.g., \u201cwho is the director of NASA foundation?\u201d. The system do well in analysing this kind of questions and this indicates the accuracy of the named entity recognizer. The system got low score for questions of type \u201cDESCRIPTION\u201d as this kind of questions look for descriptive answers like reason and manner and it requires more sophisticated techniques at the answer extraction module. As, there several techniques for measuring the similarity between question terms and every sentence in the top returned passages. Also, the variation of the length of the required answer, as some questions requires one sentence answer while some other questions requires 2 or 3 sentences and others full paragraph or passage. And the technique fails sometimes to return the complete answer and this decreases the accuracy value. However, the overall accuracy of the QA system is 65% which is a promising result achieved by an open domain system.\nFrom figure 3. The average MRR achieved is 65%. Different values of MRR for each kind of question is because the analysis requires different amount of information for each kind and the complexity of some questions also requires special handling techniques."}, {"heading": "6. CONCLUSION", "text": "In this paper, we have developed a question analysis module for analyzing a natural language question. Our Question analysis module is mainly concerned with the identification of four important factors , namely, focus, question expansion, Question Classification, and Q terms extraction . This is a comprehensive analysis of question which extracts all the necessary information that will be used as inputs for the other question answering components. We have evaluated our implementation of our module in terms of its performance based on the focus identification and Question Classification tasks, by evaluating its impact on our QA system accuracy. Our proposed method achieved average accuracy of 65% for the five types of questions with total 250 questions submitted to the system."}], "references": [{"title": "A Survey of Arabic Question Answering: challenges, Tasks, Approaches, Tools, and Future Trends", "author": ["Ezzeldin A.M", "Shaheen M"], "venue": "In Proceedings of the 13th International Arab Conference on Information Technology", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Tokenization as The initial phase in NLP", "author": ["J. Jonathan", "K. Chunyu"], "venue": "In Proceedings of COLING-92,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Speech and Language Processing: An Introduction to Natural Language Processing", "author": ["D. Jurafsky", "H. James Martin"], "venue": "Computational Linguistics, and Speech Recognition. Prentice Hall, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Tokenization as Pre-processing for Arabic Tagging System", "author": ["Ahmed H. Aliwy"], "venue": "International Journal of Information and Education Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Natural Language Processing with Python.", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A non-deterministic tokeniser for finite-state parsing", "author": ["J-P Chanod", "P. Tapanainen"], "venue": "ECAI 96. 12th European Conference on Artificial Intelligence, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop", "author": ["N. Habash", "O. Rambow"], "venue": "Proceedings of the 43rd Annual Meeting of the ACL., pp. 573\u2013 580, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Arabic Stop Words:Towards a Generalisation and Standardisation", "author": ["K. Bouzoubaa", "H. Baidouri", "T. Loukili", "T. El Yazidi"], "venue": "In Proceedings of the 13th International Business Information Management Association Conference (IBIMA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Stop-word removal algorithm for Arabic language\u201d, Proceedings", "author": ["R. Al-Shalabi", "G. Kanan", "J.M. Jaam", "Eyad Hailat"], "venue": "Of the International Conference on Information and Communication Technologies: From Theory to Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Semantic Query Expansion for Arabic Information Retrieval", "author": ["A. Mahgoub", "M. Rashwan", "H. Raafat", "Mohamed A. Zahran", "Magda B. Fayek"], "venue": "Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Enhanced semantic arabic Question Answering system based on Khoja stemmer and AWN", "author": ["F. Noha", "M. Hamdy", "E. Ashraf"], "venue": "9th International Computer Engineering Conference (ICENCO),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Semantic Based Query Expansion for Arabic Question Answering Systems", "author": ["A. Hani", "K. Santosh", "S. Khaled"], "venue": "In Proceedings of the First International Conference on Arabic Computational Linguistics (ACLing),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Classification of Arabic Questions Using Multinomial Na\u00efve Bayes And Support Vector Machines", "author": ["A. Waheeb", "A. Babu"], "venue": "International Journal of Latest Trends In Engineering And Technology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Learning Question Classifiers: The Role of Semantic Information", "author": ["X. Li", "D. Roth"], "venue": "Journal of Natural Language Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Automatic Text Processing: the transformation, analysis, and retrieval of information by computer", "author": ["G. Salton"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "A Survey on Approaches to Text Mining using Information Extraction", "author": ["R. Dukhi", "A. Bhattacharya"], "venue": "IOSR Journal of Computer Engineering (IOSR-JCE),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "NERA: Named Entity Recognition for Arabic", "author": ["K. Shaalan", "H. Raza"], "venue": "Journal of The American Society for Information Science & Technology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Named Entity Recognition for Question Answering", "author": ["D. Moll \u0301a", "Menno V", "D. Smith"], "venue": "Proceedings of the 2006 Australasian Language Technology Workshop,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Reducing question answering input data using named entity recognition", "author": ["E. Noguera", "A. Toral", "F. Llopis", "R. Mu\u2019noz"], "venue": "In Proceedings of the 8th International Conference on Text, Speech & Dialogue,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "The Use of Sentence Similarity as a Semantic Relevance Metric for Question Answering", "author": ["M. De Boni", "S. Manandhar"], "venue": "The York Research Database,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Answer Extraction and Passage Retrieval for Question Answering Systems", "author": ["Waheeb", "A. Babu"], "venue": "International Journal of Advanced Research in Computer Engineering & Technology (IJARCET),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Answer Extraction Technique for Question Answering Systems", "author": ["A. Waheeb", "A. Babu"], "venue": "International Journal of Innovative Research in Computer and Communication Engineering(IJIRCCE),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Overview of the TREC 2001 question answering track,", "author": ["E. Voorhees"], "venue": "Proceedings of the 10th Text Retrieval Conference,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Almost all of our QA system components rely in some way on the information generated by question analysis stage[2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "The entity word is defined as one kind of token for Natural Language Processing(NLP) in general and specifically in QA, the most basic one[3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Tokenization is the task of splitting words (morphemes) from running text [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "The space is a good separator for this purpose but it will not work with special cases as compound words[5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Therefore, tokenization is a necessary and non-trivial step in natural language processing [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "It is much related to the morphological analysis but usually it has been considered as an independent process [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "On average, a word form in the Arabic Tree Bank(ATB) has about 2 morphological analyses [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "Then, tokenization is similar/equivalent to word segmentation in Chinese language where Arabic word is as a sentence in Chinese language[9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": "Since the prepositions and conjunctions occurs very frequently in the documents, these words can add any benefit for the information retrieval IR) module[8ooo][10].", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "We consider four main features that make users prefer semantic based search systems over keyword-based: Handling Generalizations, Handling Morphological Variants, Handling Concept matches, and Handling synonyms with the correct sense (Word Sense Disambiguation)[11][12].", "startOffset": 261, "endOffset": 265}, {"referenceID": 10, "context": "We consider four main features that make users prefer semantic based search systems over keyword-based: Handling Generalizations, Handling Morphological Variants, Handling Concept matches, and Handling synonyms with the correct sense (Word Sense Disambiguation)[11][12].", "startOffset": 265, "endOffset": 269}, {"referenceID": 11, "context": "24 question will increase the chance of getting the answer[13] and for this we used the Arabic WordNet[14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "We used a trained Support Vector Machine(VSM) Classifier from our previous work[15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Table 1 shows the different classes as per the proposed scheme by Li & Roth[16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "The expanded list of terms extracted from the question along with the synonyms will be sent to the IR module for document retrieval, We implemented our IR module using the Vector Space Model for its simplicity of implementation and also its efficiency[19].", "startOffset": 251, "endOffset": 255}, {"referenceID": 15, "context": "Next, each sentence is tagged with partof-speech tags, which will help the named entity detection[20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "For the most part, such a system is simply recognizing instances of linguistic patterns and collating them[21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "[22] The positive impact of NE recognition in QA is widely acknowledged and there are studies that confirm it The positive impact of NE recognition in QA is widely acknowledged and there are studies that confirm it [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[22] The positive impact of NE recognition in QA is widely acknowledged and there are studies that confirm it The positive impact of NE recognition in QA is widely acknowledged and there are studies that confirm it [23].", "startOffset": 215, "endOffset": 219}, {"referenceID": 19, "context": "\u2022 For extraction answer types of \u201cDESCRIPTION\u201d we use semantic similarity measure between the question terms and the document sentences[25].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "We developed answer extraction and passage retrieval techniques for Arabic language in our previous works[26][27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "We developed answer extraction and passage retrieval techniques for Arabic language in our previous works[26][27].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "The assessment methods of answer extraction for the different types of questions supplied to the QA system is based on Text Retrieval Conference(TREC)[28], using MRR (Mean Reciprocal Rank) standards shown in the following formula:", "startOffset": 150, "endOffset": 154}], "year": 2016, "abstractText": "The first step of processing a question in Question Answering(QA) Systems is to carry out a detailed analysis of the question for the purpose of determining what it is asking for and how to perfectly approach answering it. Our Question analysis uses several techniques to analyze any question given in natural language: a Stanford POS Tagger & parser for Arabic language, a named entity recognizer, tokenizer, Stop-word removal, Question expansion, Question classification and Question focus extraction components. We employ numerous detection rules and trained classifier using features from this analysis to detect important elements of the question, including: 1) the portion of the question that is a referring to the answer (the focus); 2) different terms in the question that identify what type of entity is being asked for (the lexical answer types); 3) Question expansion ; 4) a process of classifying the question into one or more of several and different types; and We describe how these elements are identified and evaluate the effect of accurate detection on our question-answering system using the Mean Reciprocal Rank(MRR) accuracy measure.", "creator": "PScript5.dll Version 5.2.2"}}}