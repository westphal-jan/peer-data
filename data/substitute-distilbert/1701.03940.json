{"id": "1701.03940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Scalable and Incremental Learning of Gaussian Mixture Models", "abstract": "this work presents a fast and scalable algorithm for incremental learning of gaussian mixture models. by proving rank - one updates preserving its precision matrices and determinants, its asymptotic time complexity are of \\ bigo { mb ^ 2 } for $ n $ data points, $ k $ gaussian components and $ d $ dimensions. the resulting algorithm reaches be applied to high processing tasks, and this theorem confirmed by applying it to convex classification datasets mnist | cifar - 10. additionally, a order to show the operator's applicability to function approximation and control tasks, it is applied to four different learning instruments and its data - efficiency is evaluated.", "histories": [["v1", "Sat, 14 Jan 2017 16:15:44 GMT  (49kb,D)", "http://arxiv.org/abs/1701.03940v1", "13 pages, 1 figure, submitted for peer-review. arXiv admin note: substantial text overlap witharXiv:1506.04422"]], "COMMENTS": "13 pages, 1 figure, submitted for peer-review. arXiv admin note: substantial text overlap witharXiv:1506.04422", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rafael pinto", "paulo engel"], "accepted": false, "id": "1701.03940"}, "pdf": {"name": "1701.03940.pdf", "metadata": {"source": "CRF", "title": "Scalable and Incremental Learning of Gaussian Mixture Models", "authors": ["Rafael Coimbra Pinto", "Paulo Martins Engel"], "emails": ["rcpinto@inf.ufrgs.br", "engel@inf.ufrgs.br"], "sections": [{"heading": null, "text": "( NKD2 ) for N data points, K\nGaussian components and D dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm\u2019s applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated.\nKeywords Gaussian Mixture Models \u00b7 Incremental Learning"}, {"heading": "1 Introduction", "text": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4]. It creates and continually adjusts a probabilistic model of the joint input-output space consistent to all sequentially presented data, after each data point presentation, and without the need to store any past data points. Its learning process is aggressive, meaning that only a single scan through the data is necessary to obtain a consistent model.\nIGMN adopts a Gaussian mixture model of distribution components that can be expanded to accommodate new information from an input data point, or reduced if spurious components are identified along the learning process. Each data point assimilated by the model contributes to the sequential update of the model parameters based on the maximization of the likelihood of the data. The parameters are updated through the accumulation of relevant information extracted from each data point. New points are added directly to existing Gaussian components or new components are created when necessary, avoiding merge and split operations, much like what is seen in the Adaptive Resonance Theory (ART) algorithms [5]. It has been previously shown in [6] that the algorithm is robust even when data is presented in random order, having similar performance and producing similar number of clusters in any order. Also, [4] has\nInstituto de Informa\u0301tica, Universidade Federal do Rio Grande do Sul Av. Bento Gonc\u0327alves, 9500 - Agronomia - Porto Alegre, RS - Zip 91501-970 - Brazil Tel.: +123-45-678910 Fax: +123-45-678910 E-mail: {rcpinto,engel}@inf.ufrgs.br\nar X\niv :1\n70 1.\n03 94\n0v 1\n[ cs\n.L G\n] 1\n4 Ja\nn 20\n17\nshown that the resulting models are very similar to the ones produced by the batch EM algorithm.\nThe IGMN is capable of supervised learning, simply by assigning any of its input vector elements as outputs. In other words, any element can be used to predict any other element, like auto-associative neural networks [8] or missing data imputation [9]. This feature is useful for simultaneous learning of forward and inverse kinematics [10], as well as for simultaneous learning of a value function and a policy in reinforcement learning [11].\nPrevious successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].\nHowever, the IGMN suffers from cubic time complexity due to matrix inversion operations and determinant computations. Its time complexity is of O ( NKD3 ) , where N is the number of data points, K is the number of Gaussian components and D is the problem dimension. It makes the algorithm prohibitive for high-dimensional tasks (like visual tasks) and thus of limited use. One solution would be to use diagonal covariance matrices, but this decreases the quality of the results, as already reported in previous works [6, 12]. In [28], we propose the use of rank-one updates for both inverse matrices and determinants applied to full covariance matrices, thus reducing the time complexity to O ( NKD2 ) for learning while keeping the quality of a full covariance matrix solution.\nFor the specific case of the IGMN algorithm, to the best of our knowledge, this has not been tried before, although we can find similar efforts for related algorithms. In [19], rank-one updates were applied to an iterated linear discriminant analysis algorithm in order to decrease the complexity of the algorithm. Rank-one updates were also used in [20], where Gaussian models are employed for feature selection. Finally, in [21], the same kind of optimization was applied to Maximum Likelihood Linear Transforms (MLLT).\nIn this work, we present improved formulas for the covariance matrix updates, removing the need for two rank-one updates, which increases efficiency and stability. It also presents new promising results in reinforcement learning tasks, showing that this algorithm is not only scalable from the computational point-of-view, but also in terms of data-efficiency, promoting fast learning from few data points / experiences.\nThe next Section describes the algorithm in more detail with the latest improvements to date. Section 3 describes our improvements to the algorithm. Section 4 shows the experiments and results obtained from both versions of the IGMN for comparison, and Section 5 finishes this work with concluding remarks."}, {"heading": "2 Incremental Gaussian Mixture Network", "text": "In the next subsections we describe the IGMN algorithm, a slightly improved version of the one described in [16].\n2.1 Learning\nThe algorithm starts with no components, which are created as necessary (see subsection 2.2). Given input x (a single instantaneous data point), the IGMN algorithm processing step is as follows. First, the squared Mahalanobis distance d2(x, j) for each component j is computed:\nd2M (x, j) = (x\u2212 \u00b5j)T\u03a3\u22121j (x\u2212 \u00b5j) (1)\nwhere \u00b5j is the j th component mean, \u03a3j its full covariance matrix . If any d 2(x, j) is smaller than than \u03c72D,1\u2212\u03b2 (the 1\u2212 \u03b2 percentile of a chi-squared distribution with D degrees-of-freedom, where D is the input dimensionality and \u03b2 is a user defined meta-parameter, e.g., 0.1), an update will occur, and posterior probabilities are calculated for each component as follows:\np(x|j) = 1 (2\u03c0)D/2 \u221a |\u03a3j | exp\n( \u22121\n2 d2M (x, j)\n) (2)\np(j|x) = p(x|j)p(j) K\u2211 k=1 p(x|k)p(k) \u2200j (3)\nwhere K is the number of components. Now, parameters of the algorithm must be updated according to the following equations:\nvj(t) = vj(t\u2212 1) + 1 (4)\nspj(t) = spj(t\u2212 1) + p(j|x) (5)\nej = x\u2212 \u00b5j(t\u2212 1) (6)\n\u03c9j = p(j|x) spj\n(7)\n\u2206\u00b5j = \u03c9jej (8)\n\u00b5j(t) = \u00b5j(t\u2212 1) +\u2206\u00b5j (9)\ne\u2217j = x\u2212 \u00b5j(t) (10)\n\u03a3j(t) = (1\u2212 \u03c9j)\u03a3j(t\u2212 1) + \u03c9je\u2217je\u2217Tj \u2212\u2206\u00b5j\u2206\u00b5Tj (11)\np(j) = spj M\u2211 q=1 spq\n(12)\nwhere spj and vj are the accumulator and the age of component j, respectively, and p(j) is its prior probability. The equations are derived using the Robbins-Monro stochastic approximation [22] for maximizing the likelihood of the model. This derivation can be found in [4, 23].\n2.2 Creating New Components\nIf the update condition in the previous subsection is not met, then a new component j is created and initialized as follows:\n\u00b5j = x; spj = 1; vj = 1; p(j) = 1 K\u2211 i=1 spi ; \u03a3j = \u03c3 2 iniI\nwhere K already includes the new component and \u03c3ini can be obtained by:\n\u03c3ini = \u03b4std(x) (13)\nwhere \u03b4 is a manually chosen scaling factor (e.g., 0.01) and std is the standard deviation of the dataset. Note that the IGMN is an online and incremental algorithm and therefore it may be the case that we do not have the entire dataset to extract descriptive statistics. In this case the standard deviation can be just an estimation (e.g., based on sensor limits from a robotic platform), without impacting the algorithm.\n2.3 Removing Spurious Components\nOptionally, a component j is removed whenever vj > vmin and spj < spmin, where vmin and spmin are manually chosen (e.g., 5.0 and 3.0, respectively). In that case, also, p(k) must be adjusted for all k \u2208 K, k 6= j, using (12). In other words, each component is given some time vmin to show its importance to the model in the form of an accumulation of its posterior probabilities spj . Those components are entirely removed from the model instead of merged with other components, because we assume they represent outliers. Since the removed components have small accumulated activations, it also implies that their removal has almost no negative impact on the model quality, often producing positive impact on generalization performance due to model simplification (a more throughout analysis of parameter sensibility for the IGMN algorithm can be found in [6]).\n2.4 Inference\nIn the IGMN, any element can be predicted by any other element. In other words, inputs and targets are presented together as inputs during training. Thus, inference is done by reconstructing data from the target elements (xt, a slice of the entire input vector x) by estimating the posterior probabilities using only the given elements (xi, also a slice of the entire input vector x), as follows:\np(j|xi) = p(xi|j)p(j) M\u2211 q=1 p(xi|q)p(q) \u2200j (14)\nIt is similar to (3), except that it uses a modified input vector xi with the target elements xt removed from calculations. After that, xt can be reconstructed using the conditional mean equation:\nx\u0302t = M\u2211 j=1 p(j|xi)(\u00b5j,t + \u03a3j,ti\u03a3\u22121j,i (xi \u2212 \u00b5j,i)) (15)\nwhere \u03a3j,ti is the sub-matrix of the jth component covariance matrix associating the unknown and known parts of the data, \u03a3j,i is the sub-matrix corresponding to the known part only and \u00b5j,i is the jth\u2019s component mean without the element corresponding to the target element. This division can be seen below:\n\u03a3j = ( \u03a3j,i \u03a3j,it \u03a3j,ti \u03a3j,t ) It is also possible to estimate the conditional covariance matrix for a given input,\nwhich allows us to obtain error margins for the inference procedure. It is computed according to the following equation:\n\u03a3\u0302(t) = \u03a3j,t \u2212\u03a3j,ti\u03a3\u22121j,i \u03a3j,it (16)"}, {"heading": "3 Fast IGMN", "text": "In this section, the more scalable version of the IGMN algorithm, the Fast Incremental Gaussian Mixture Network (FIGMN) is presented. It is an improvement over the version presented in [28]. The main issue with the IGMN algorithm regarding computational complexity lies in the fact that Equation 1 (the squared Mahalanobis distance) requires a matrix inversion, which has a asymptotic time complexity of\nO ( D3 ) , for D dimensions (O ( Dlog27+O ( 1 )) for the Strassen algorithm or at best\nO ( D2.3728639 ) with the most recent algorithms to date [24]). This renders the entire IGMN algorithm as impractical for high-dimension tasks. Here we show how to work directly with the inverse of covariance matrix (also called the precision or concentration matrix) for the entire procedure, therefore avoiding costly inversions.\nFirstly, let us denote \u03a3\u22121 = \u039b, the precision matrix. Our task is to adapt all equations involving \u03a3 to instead use \u039b.\nWe now proceed to adapt Equation 11 (covariance matrix update). This equation can be seen as a sequence of two rank-one updates to the \u03a3 matrix, as follows:\n\u03a3\u0304j(t) = (1\u2212 \u03c9j)\u03a3j(t\u2212 1) + \u03c9je\u2217je\u2217Tj (17)\n\u03a3j(t) = \u03a3\u0304j(t)\u2212\u2206\u00b5j\u2206\u00b5Tj (18)\nThis allows us to apply the Sherman-Morrison formula [25]:\n(A + uvT )\u22121 = A\u22121 \u2212 A \u22121uvTA\u22121\n1 + vTA\u22121u (19)\nThis formula shows how to update the inverse of a matrix plus a rank-one update. For the second update, which subtracts, the formula becomes:\n(A\u2212 uvT )\u22121 = A\u22121 + A \u22121uvTA\u22121\n1\u2212 vTA\u22121u (20)\nIn the context of IGMN, we have A = (1\u2212 \u03c9)\u03a3j(t\u2212 1) = (1\u2212 \u03c9)\u039b\u22121j (t\u2212 1) and u = v = \u221a \u03c9e\u2217 for the first update, while for the second one we have A = \u03a3\u0304j(t) and u = v = \u2206\u00b5j . Rewriting 19 and 20 we get (for the sake of compactness, assume all subscripts for \u039b and \u2206\u00b5 to be j):\n\u039b\u0304(t) = \u039b(t\u2212 1)\n1\u2212 \u03c9 \u2212\n\u03c9 (1\u2212\u03c9)2\u039b(t\u2212 1)e \u2217e\u2217T\u039b(t\u2212 1) 1 + \u03c91\u2212\u03c9e \u2217T\u039b(t\u2212 1)e\u2217 (21)\n\u039b(t) = \u039b\u0304(t) + \u039b\u0304(t)\u2206\u00b5\u2206\u00b5T \u039b\u0304(t)\n1\u2212\u2206\u00b5T \u039b\u0304(t)\u2206\u00b5 (22)\nThese two equations allow us to update the precision matrix directly, eliminating the need for the covariance matrix \u03a3. They have O ( N2 )\ncomplexity due to matrix-vector products.\nIt is also possible to combine the two rank-one updates into one, and this step was not present in previous works. The first step is to combine 17 and 18 into a single rank-one update, by using equations 6 to 10, resulting in the following:\n\u03a3j(t) = (1\u2212 \u03c9j)\u03a3j(t\u2212 1) + eeT\u03c9(1 + \u03c9(\u03c9 \u2212 3)) (23) Then, by applying the Sherman-Morrison formula to this new update, we arrive at\nthe following precision matrix update formula for the FIGMN:\n\u039b(t) = \u039b(t\u2212 1) 1\u2212 \u03c9 + \u039b(t\u2212 1)eeT\u039b(t\u2212 1) \u03c9(1\u2212 3\u03c9 + \u03c9 2) (\u03c9 \u2212 1)2(\u03c92 \u2212 2\u03c9 \u2212 1) (24)\nAlthough less intuitive than 17 and 18, the above formula is smaller and more efficient, requiring much less vector / matrix operations, making FIGMN yet faster and even more stable (18 depends on the result of 17, which may be a singular matrix).\nFollowing on the adaptation of the IGMN equations, Equation 1 (the squared Mahalanobis distance) allows for a direct substituion, yielding the following new equation:\nd2M (x, j) = (x\u2212 \u00b5j)T\u039bj(x\u2212 \u00b5j) (25) which now has a O ( N2 )\ncomplexity, since there is no matrix inversion as the original equation. Note that the Sherman-Morrison identity is exact, thus the Mahalanobis computation yields exactly the same result, as will be shown in the experiments. After removing the cubic complexity from this step, the determinant computation will be dealt with next.\nSince the determinant of the inverse of a matrix is simply the inverse of the determinant, it is sufficient to invert the result. But computing the determinant itself is also a O ( D3 )\noperation, so we will instead perform rank-one updates using the Matrix Determinant Lemma [26], which states the following:\n|A + uvT | = |A|(1 + vTA\u22121u) (26) |A\u2212 uvT | = |A|(1\u2212 vTA\u22121u) (27)\nSince the IGMN covariance matrix update involves a rank-two update, adding a term and then subtracting one, both rules must be applied in sequence, similar to what has been done with the \u039b equations. Equations 17 and 18 may be reused here, together with the same substitutions previously showed, leaving us with the following new equations for updating the determinant (again, j subscripts were dropped):\n|\u03a3\u0304(t)| = (1\u2212 \u03c9)D|\u03a3(t\u2212 1)| ( 1 + \u03c9\n1\u2212 \u03c9 e\u2217T\u039b(t\u2212 1)e\u2217\n) (28)\n|\u03a3(t)| = |\u03a3\u0304(t)|(1\u2212\u2206\u00b5T \u039b\u0304(t)\u2206\u00b5) (29) Just as with the covariance matrix, a rank-one update for the determinant update is\nalso derived (again, using the definitions from 6 to 10): |\u03a3(t)| = (1\u2212 \u03c9)D|\u03a3(t\u2212 1)| ( 1 + \u03c9(1 + \u03c9(\u03c9 \u2212 3))\n1\u2212 \u03c9 eT\u039b(t\u2212 1)e\n) (30)\nThis was the last source of cubic complexity, which is now quadratic.\nFinishing the adaptation in the learning part of the algorithm, we just need to define the initialization for \u039b for each component. What previously was \u03a3j = \u03c3 2 iniI now becomes \u039bj = \u03c3 \u22122 iniI, the inverse of the variances of the dataset. Since this matrix is diagonal, there are no costly inversions involved. And for initializing the determinant |\u03a3|, just set it to \u220f \u03c32ini, which again takes advantage of the initial diagonal matrix to avoid costly operations. Note that we keep the precision matrix \u039b, but the determinant of the covariance matrix \u03a3 instead. See algorithms 1 to 3 for a summary of the new learning algorithm (excluding pruning, for brevity).\nAlgorithm 1 Fast IGMN Learning\nInput: \u03b4,\u03b2,X K > 0, \u03c3\u22121ini = (\u03b4std(X))\n\u22121,M = \u2205 for all input data vector x \u2208 X do\nif K = 0 or \u2203j, d2M (x, j) < \u03c7 2 D,1\u2212\u03b2 then\nupdate(x) else M \u2190M \u222a create(x)\nend if end for\nAlgorithm 2 update\nInput: x for all Gaussian componentS j \u2208M do d2M (x, j) = (x\u2212 \u00b5j)\nT\u039bj(x\u2212 \u00b5j) p(x|j) = 1\n(2\u03c0)D/2 \u221a |\u03a3j |\nexp ( \u2212 1\n2 d2M (x, j) ) p(j|x) = p(x|j)p(j)\nK\u2211 k=1 p(x|k)p(k) \u2200j\nvj(t) = vj(t\u2212 1) + 1 spj(t) = spj(t\u2212 1) + p(j|x) ej = x\u2212 \u00b5j(t\u2212 1) \u03c9j =\np(j|x) spj\n\u00b5j(t) = \u00b5j(t\u2212 1) + \u03c9jej \u039b(t) =\n\u039b(t\u22121) 1\u2212\u03c9 + \u039b(t\u2212 1)ee T\u039b(t\u2212 1) \u03c9(1\u22123\u03c9+\u03c9 2)\n(\u03c9\u22121)2(\u03c92\u22122\u03c9\u22121) p(j) =\nspj M\u2211 q=1 spq\n|\u03a3(t)| = (1\u2212 \u03c9)D|\u03a3(t\u2212 1)| ( 1 + \u03c9(1+\u03c9(\u03c9\u22123))\n1\u2212\u03c9 e T\u039b(t\u2212 1)e ) end for\nAlgorithm 3 create\nInput: x K \u2190 K + 1 return new Gaussian component K with \u00b5K = x, \u039bK = \u03c3 \u22121 iniI, |\u03a3K | = |\u039bK | \u22121, spj = 1, vj = 1,\np(j) = 1 K\u2211 k=1 spi\nFinally, the inference Equation 15 must also be updated in order to allow the IGMN to work in supervised mode. This can be accomplished by the use of a block matrix decomposition (the i subscripts stand for \u201dinput\u201d, and refers to the input portion of the covariance matrix, i.e., the dimensions corresponding to the known variables; similarly, the t subscripts refer to the \u201dtarget\u201d portions of the matrix, i.e., the unknowns; the it and ti subscripts refer to the covariances between these variables):\n\u039bj = [ \u03a3j,i \u03a3j,it \u03a3j,ti \u03a3j,t ]\u22121 = [ \u039bj,i \u039bj,it \u039bj,ti \u039bj,t ] = [ (\u03a3j,i \u2212\u03a3j,it\u03a3\u22121j,t \u03a3j,ti)\u22121 \u2212\u03a3 \u22121 j,i \u03a3j,it(\u03a3j,t \u2212\u03a3j,ti\u03a3 \u22121 j,i \u03a3j,it) \u22121\n\u2212\u03a3\u22121j,t \u03a3j,ti(\u03a3j,i \u2212\u03a3j,it\u03a3 \u22121 j,t \u03a3j,ti) \u22121 (\u03a3j,t \u2212\u03a3j,ti\u03a3\u22121j,i \u03a3j,it)\u22121 ] (31)\nHere, according to Equation 15, we need \u03a3j,ti and \u03a3 \u22121 j,i . But since the terms that\nconstitute these sub-matrices are relative to the original covariance matrix (which we do not have), they must be extracted from the precision matrix directly. Looking at the decomposition, it is clear that \u039bj,it\u039b \u22121 j,t = \u2212\u03a3 \u22121 j,i \u03a3j,it = \u2212\u03a3j,ti\u03a3 \u22121 j,i (the terms between parenthesis in \u039bj,ti and \u039bj,t cancel each other, while \u03a3j,it = \u03a3 T j,ti due to symmetry). So Equation 15 can be rewritten as:\nx\u0302t = M\u2211 j=1 p(j|xi)(\u00b5j,t \u2212\u039bj,it\u039b\u22121j,t (xi \u2212 \u00b5j,i)) (32)\nwhere \u039bj,it and \u039bj,t can be extracted directly from \u039b. However, we still need to compute the inverse of \u039bj,t. So we can say that this particular implementation has O ( NKD2 ) complexity for learning and O ( NKD3 ) for inference. The reason for us to not worry about that is that d = i+ o, where i is the number of inputs and o is the number of outputs. The inverse computation acts only upon the output portion of the matrix. Since, in general, o i (in many cases even o = 1), the impact is minimal, and the same applies to the \u039bj,it\u039b \u22121 j,t product. In fact, Weka (the data mining platform used in this work [27]) allows for only 1 output, leaving us with just scalar operations. A new conditional variance formula was also derived to use precision matrices, as it was not present in previous works. Looking again at 16, we see that it is the Schur Complement of \u03a3j,i in \u03a3 [7]. By analysing the block decomposition equation, it becomes obvious that, in terms of the precision matrix \u039b, the conditional covariance matrix has the form:\n\u03a3\u0302(t) = \u039b\u22121j,t (33)\nThus, we are now able to compute the conditional covariance matrix during the inference step of the FIGMN algorithm, which can be useful in the reinforcement learning setting (providing error margins for efficient directed exploration). And better yet, \u039b\u22121j,t is already computed in the inference procedure of the FIGMN, which leaves us with no additional computations."}, {"heading": "4 Experiments", "text": "The first experiment was meant to verify that both IGMN implementations produce exactly the same results. They were both applied to 7 standard datasets distributed with the Weka software (table 1). Parameters were set to \u03b4 = 0.5 (chosen by 2-fold cross-validation) and \u03b2 = 4.9E \u2212 324, the smallest possible double precision number available for the Java Virtual Machine (and also the default value for this implementation of the algorithm), such that Gaussian components are created only\nwhen strictly necessary. The same parameters were used for all datasets. Results were obtained from 10-fold cross-validation (resulting in training sets with 90% of the data and test sets with the remaining 10%) and statistical significances came from paired t-tests with p = 0.05. As can be seen in table 2, both IGMN and FIGMN algorithms produced exactly the same results, confirming our expectations. The number of clusters created by them was also the same, and the exact quantity for each dataset is shown in table 3. The Weka packages for both variations of the IGMN algorithm, as well as the datasets used in the experiments can be found at [29]. In order to experiment with high dimensional datasets and confirm the algorithm\u2019s scalability, it was applied to the MNIST1 and CIFAR-102 datasets as well.\nA second experiment was performed in order to evaluate the speed performance of the proposed algorithm, both the original and improved IGMN algorithms, using the parameters \u03b4 = 1 and \u03b2 = 0, such that a single component was created and we could focus on speedups due only to dimensionality (this also made the algorithm highly insensitive to the \u03b4 parameter). They were applied to the 2 highest dimensional datasets in table 1, namely, the MNIST and CIFAR-10 datasets. The MNIST dataset was split into a training set with 60000 data points and a testing set containing 10000 data points, the standard procedure in the machine learning community [30]. Similarlly, the\n1 http://yann.lecun.com/exdb/mnist/ 2 http://www.cs.toronto.edu/~kriz/cifar.html\nCIFAR-10 dataset was split into 50000 training data points and 10000 testing data points, also a standard procedure for this dataset [31].\nResults can be seen in table 4. Training time for the MNIST dataset was 20 times smaller for the fast version while the testing time was 16 times smaller. It makes sense that the testing time has shown a bit less improvement, since inference only takes advantage from the incremental determinant computation but not from the incremental inverse computation. For the CIFAR-10 dataset, it was impractical to run the original IGMN algorithm on the entire dataset, requiring us to estimate the total time, linearly projecting it from 100 data points (note that, since the model always uses only 1 Gaussian component during the entire training, the computation time per data point does not increase over time). It resulted in 32 days of CPU time estimated for the original algorithm against 15545s (\u223c 4h) for the improved algorithm, a speedup above 2 orders of magnitude. Testing time is not available for the original algorithm on this dataset, since the training could not be concluded. Additionally, we compared a pure clustering version of the FIGMN algorithm on the MNIST training set against batch EM (the implementation found in the Weka software). While the FIGMN algorithm took \u223c 7.5h hours to finish, using 208 Gaussian components, the batch EM algorithm took \u223c 1.3h to complete a single iteration (we set the fixed number of components to 208 too) using 4 CPU cores. Besides generally requiring more than one iteration to achieve best results, the batch algorithm required the entire dataset in RAM. The FIGMN memory requirements were much lower.\nFinally, both versions of the IGMN algorithm with \u03b4 = 1 and \u03b2 = 0 were compared on 11 synthetic datasets generated by Weka. All datasets have 1000 data points drawn from a single Gaussian distribution (90% training, 10% testing) and an exponentially growing number of dimensions: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512 and 1024. This experiment was performed in order to compare the scalability of both algorithms. Results for training and testing can be seen in Fig. 1:\nAs predicted, the FIGMN algorithm scales much better in relation to the number of input dimensions of the data.\n4.1 Reinforcement Learning\nAdditionally, the FIGMN algorithm was employed for solving three different classical reinforcement learning tasks in the OpenAI Gym 3 environment: cart-pole, mountain car and acrobot. Reinforcement learning tasks consist in learning sequences of actions from trial and error on diverse environments.\nThe mountain car task consists in controlling an underpowered car in order to reach the top of a hill. It must go up the opposite slope to gain momentum first. The agent has three actions at its disposal, accelerating it leftward, rightward, or no acceleration at all. The agent\u2019s state is made up of two features: current position and speed. The cart-pole task consists in balancing a pole above a small car which can move left or right at each timestep. Four variables are available as observations: current position and speed of the cart and current angle and angular velocity of the pole. Finally, the acrobot task requires a 2-joint robot to reach a certain height with the tip of its \u201darm\u201d. Torque in two directions can be exerted on the 2 joints, resulting in 4 possible actions. Current angle and angular velocity of each joint are provided as observations.\nThe FIGMN algorithm was compared to other 3 algorithms with high scores on OpenAI Gym: Sarsa(\u03bb), Trust Region Policy Optimization (TRPO; a policy gradient method, suitable to continuous states, actions and time, but which works in batch mode and has low data-efficiency) [33] and Dueling Double DQN (an improvement over the DQN algorithm, using two value function approximators with different update rates and generalizing between actions; it is restricted to discrete actions) [34]. Table 5 shows the number of episodes required for each algorithm to reach the required reward threshold for the 3 tasks.\nIt is evident that Q-learning with FIGMN function approximation produces better results than Sarsa(\u03bb) with discretized states. Its results are also superior to TRPO\u2019s by a large margin. But Duel DDQN appears as the most data-efficient algorithm in this comparison (possibly due to its fixed topology which simplifies the learning procedure)."}, {"heading": "5 Conclusion", "text": "We have shown how to work directly with precision matrices in the IGMN algorithm, avoiding costly matrix inversions by performing rank-one updates. The determinant computations were also avoided using a similar method, effectively eliminating any source of cubic complexity from the learning algorithm. While previous works used two rank-one updates for covariance matrices and determinants, this work shows how to perform such updates with single rank-one operations. These improvements resulted in substantial speedups for high-dimensional datasets, turning the IGMN into a good option for this kind of tasks. The inference operation still has cubic complexity, but we argue that it has a much smaller impact on the total runtime of the algorithm, since the number of outputs is usually much smaller than the number of inputs. This was confirmed in the experiments.\nReinforcement learning experiments were also useful for showing that the FIGMN algorithm is data-efficient, i.e., it requires few data points in order to learn a usable model. Thus, besides being computationally fast, it also learns fast.\nIn general, we could see that the fast IGMN is a good option for supervised learning, with low runtimes, good accuracy and high data-efficiency. It should be noted that this is achieved with a single-pass through the data, making it also a valid option for data streams."}], "references": [{"title": "Using a Gaussian mixture neural network for incremental learning and robotics", "author": ["MR Heinen", "PM Engel", "RC. Pinto"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "An incremental gaussian mixture network that learns instantaneously from data flows", "author": ["MR Heinen", "PM Engel", "IGMN Pinto RC."], "venue": "Proc VIII Encontro Nacional de Intelig\u00eancia Artificial", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["AP Dempster", "NM Laird", "DB Rubin"], "venue": "Journal of the Royal Statistical Society Series B (Methodological)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1977}, {"title": "Incremental learning of multivariate gaussian mixture models", "author": ["P Engel", "M. Heinen"], "venue": "Advances in Artificial Intelligence\u2013SBIA", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Competitive learning: From interactive activation to adaptive resonance", "author": ["S. Grossberg"], "venue": "Cognitive science", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1987}, {"title": "A connectionist approach for incremental function approximation and on-line tasks", "author": ["Heinen MR"], "venue": "Universidade Federal do Rio Grande do Sul;", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Parallel distributed processing", "author": ["Rumelhart DE", "McClelland JL"], "venue": "MIT Pr.;", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Supervised learning from incomplete data via an EM approach. In: Advances in neural information processing systems 6. Citeseer", "author": ["Ghahramani Z", "Jordan MI"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "An online algorithm for simultaneously learning forward and inverse kinematics", "author": ["B Damas", "J. Santos-Victor"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "IGMN: An incremental connectionist approach for concept formation, reinforcement learning and robotics", "author": ["Heinen MR", "Engel PM"], "venue": "Journal of Applied Computing Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Echo State Incremental Gaussian Mixture Network for Spatio-Temporal Pattern Processing", "author": ["RC Pinto", "PM Engel", "MR. Heinen"], "venue": "Proceedings of the IX ENIA-Brazilian Meeting on Artificial", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Recursive incremental gaussian mixture network for spatio-temporal pattern processing", "author": ["RC Pinto", "PM Engel", "MR. Heinen"], "venue": "Proc 10th Brazilian Congr Computational Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Autocorrelation and partial autocorrelation functions to improve neural networks models on univariate time series forecasting", "author": ["JHF Flores", "PM Engel", "RC. Pinto"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Dealing with continuous-state reinforcement learning for intelligent control of traffic signals", "author": ["MR Heinen", "AL Bazzan", "PM. Engel"], "venue": "Intelligent Transportation Systems (ITSC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "One-shot learning in the road sign problem", "author": ["RC Pinto", "PM Engel", "MR. Heinen"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning Abstract Behaviors with the Hierarchical Incremental Gaussian Mixture Network", "author": ["R de Pontes Pereira", "PM Engel", "RC. Pinto"], "venue": "Neural Networks (SBRN),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Location-Based Events Detection on Micro-Blogs", "author": ["Santos ADPd", "Wives LK", "Alvares LO"], "venue": "arXiv preprint arXiv:12104008", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Efficient update of the covariance matrix inverse in iterated linear discriminant analysis", "author": ["J Salmen", "M Schlipsing", "C. Igel"], "venue": "Pattern Recognition Letters", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Jointly Informative Feature Selection", "author": ["L Lefakis", "F. Fleuret"], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics;", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Extended mllt for gaussian mixture models", "author": ["Olsen PA", "Gopinath RA"], "venue": "Transactions in Speech and Audio Processing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "A stochastic approximation method", "author": ["H Robbins", "S. Monro"], "venue": "The annals of mathematical statistics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1951}, {"title": "INBC: An incremental algorithm for dataflow segmentation based on a probabilistic approach. INBC: an incremental algorithm for dataflow segmantation based on a probabilistic approach", "author": ["Engel PM"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["Gall FL"], "venue": "arXiv preprint arXiv:14017714", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix", "author": ["Sherman J", "Morrison WJ"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1950}, {"title": "Matrix algebra from a statistician\u2019s perspective", "author": ["Harville DA"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "The WEKA data mining software: an update", "author": ["M Hall", "E Frank", "G Holmes", "B Pfahringer", "P Reutemann", "IH. Witten"], "venue": "ACM SIGKDD explorations newsletter", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "A Fast Incremental Gaussian Mixture Model", "author": ["R.C. Pinto", "P.M. Engel"], "venue": "PloS one, 10(10),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Experiment Data for \u201dA Fast Incremental Gaussian Mixture Model", "author": ["Pinto", "RC"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2030}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["A Krizhevsky", "G. Hinton"], "venue": "Computer Science Department,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "RR. Salakhutdinov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "arXiv preprint", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Dueling Network Architectures for Deep Reinforcement Learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 48, "endOffset": 53}, {"referenceID": 1, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": "New points are added directly to existing Gaussian components or new components are created when necessary, avoiding merge and split operations, much like what is seen in the Adaptive Resonance Theory (ART) algorithms [5].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "It has been previously shown in [6] that the algorithm is robust even when data is presented in random order, having similar performance and producing similar number of clusters in any order.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Also, [4] has", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "In other words, any element can be used to predict any other element, like auto-associative neural networks [8] or missing data imputation [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "In other words, any element can be used to predict any other element, like auto-associative neural networks [8] or missing data imputation [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "This feature is useful for simultaneous learning of forward and inverse kinematics [10], as well as for simultaneous learning of a value function and a policy in reinforcement learning [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "This feature is useful for simultaneous learning of forward and inverse kinematics [10], as well as for simultaneous learning of a value function and a policy in reinforcement learning [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 11, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 12, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 118, "endOffset": 125}, {"referenceID": 13, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 118, "endOffset": 125}, {"referenceID": 0, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 160, "endOffset": 170}, {"referenceID": 14, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 160, "endOffset": 170}, {"referenceID": 15, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 160, "endOffset": 170}, {"referenceID": 16, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 209, "endOffset": 213}, {"referenceID": 5, "context": "One solution would be to use diagonal covariance matrices, but this decreases the quality of the results, as already reported in previous works [6, 12].", "startOffset": 144, "endOffset": 151}, {"referenceID": 10, "context": "One solution would be to use diagonal covariance matrices, but this decreases the quality of the results, as already reported in previous works [6, 12].", "startOffset": 144, "endOffset": 151}, {"referenceID": 26, "context": "In [28], we propose the use of rank-one updates for both inverse matrices and determinants applied to full covariance matrices, thus reducing the time complexity to O ( NKD ) for learning while keeping the quality of a full covariance matrix solution.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [19], rank-one updates were applied to an iterated linear discriminant analysis algorithm in order to decrease the complexity of the algorithm.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Rank-one updates were also used in [20], where Gaussian models are employed for feature selection.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "Finally, in [21], the same kind of optimization was applied to Maximum Likelihood Linear Transforms (MLLT).", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "In the next subsections we describe the IGMN algorithm, a slightly improved version of the one described in [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "The equations are derived using the Robbins-Monro stochastic approximation [22] for maximizing the likelihood of the model.", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "This derivation can be found in [4, 23].", "startOffset": 32, "endOffset": 39}, {"referenceID": 21, "context": "This derivation can be found in [4, 23].", "startOffset": 32, "endOffset": 39}, {"referenceID": 5, "context": "Since the removed components have small accumulated activations, it also implies that their removal has almost no negative impact on the model quality, often producing positive impact on generalization performance due to model simplification (a more throughout analysis of parameter sensibility for the IGMN algorithm can be found in [6]).", "startOffset": 334, "endOffset": 337}, {"referenceID": 26, "context": "It is an improvement over the version presented in [28].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "O ( D ) , for D dimensions (O ( D2 ( 1 )) for the Strassen algorithm or at best O ( D ) with the most recent algorithms to date [24]).", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "This allows us to apply the Sherman-Morrison formula [25]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "But computing the determinant itself is also a O ( D ) operation, so we will instead perform rank-one updates using the Matrix Determinant Lemma [26], which states the following:", "startOffset": 145, "endOffset": 149}, {"referenceID": 25, "context": "In fact, Weka (the data mining platform used in this work [27]) allows for only 1 output, leaving us with just scalar operations.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "The Weka packages for both variations of the IGMN algorithm, as well as the datasets used in the experiments can be found at [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "Dataset Instances (N) Attributes (D) Classes breast-cancer 286 9 2 pima-diabetes 768 8 2 Glass 214 9 7 ionosphere 351 34 2 iris 150 4 3 labor-neg-data 57 16 2 soybean 683 35 19 MNIST [30] 70000 784 10 CIFAR-10 [31] 60000 3072 10", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "Dataset Instances (N) Attributes (D) Classes breast-cancer 286 9 2 pima-diabetes 768 8 2 Glass 214 9 7 ionosphere 351 34 2 iris 150 4 3 labor-neg-data 57 16 2 soybean 683 35 19 MNIST [30] 70000 784 10 CIFAR-10 [31] 60000 3072 10", "startOffset": 210, "endOffset": 214}, {"referenceID": 30, "context": "The neural network is a parallel implementation of a state-of-the-art Dropout Neural Network [32] with 100 hidden neurons, 50% dropout for the hidden layer and 20% dropout for the input layer (this specific implementation can be found at https://github.", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "The MNIST dataset was split into a training set with 60000 data points and a testing set containing 10000 data points, the standard procedure in the machine learning community [30].", "startOffset": 176, "endOffset": 180}, {"referenceID": 29, "context": "CIFAR-10 dataset was split into 50000 training data points and 10000 testing data points, also a standard procedure for this dataset [31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "The FIGMN algorithm was compared to other 3 algorithms with high scores on OpenAI Gym: Sarsa(\u03bb), Trust Region Policy Optimization (TRPO; a policy gradient method, suitable to continuous states, actions and time, but which works in batch mode and has low data-efficiency) [33] and Dueling Double DQN (an improvement over the DQN algorithm, using two value function approximators with different update rates and generalizing between actions; it is restricted to discrete actions) [34].", "startOffset": 271, "endOffset": 275}, {"referenceID": 32, "context": "The FIGMN algorithm was compared to other 3 algorithms with high scores on OpenAI Gym: Sarsa(\u03bb), Trust Region Policy Optimization (TRPO; a policy gradient method, suitable to continuous states, actions and time, but which works in batch mode and has low data-efficiency) [33] and Dueling Double DQN (an improvement over the DQN algorithm, using two value function approximators with different update rates and generalizing between actions; it is restricted to discrete actions) [34].", "startOffset": 478, "endOffset": 482}], "year": 2017, "abstractText": "This work presents a fast and scalable algorithm for incremental learning of Gaussian mixture models. By performing rank-one updates on its precision matrices and determinants, its asymptotic time complexity is of O ( NKD ) for N data points, K Gaussian components and D dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm\u2019s applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated.", "creator": "LaTeX with hyperref package"}}}