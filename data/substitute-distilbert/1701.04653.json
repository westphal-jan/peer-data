{"id": "1701.04653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods", "abstract": "in this paper, we investigate whether text from a community question answering ( qa ) platform can be used the predict and describe real - world attributes. we experiment with predicting a wide web of 62 demographic reports for neighbourhoods of london. we use the text from qa platform of yahoo! answers and compare them results to the ones obtained from twitter microblogs. outcomes show that absolute correlation between the predicted demographic attributes using text from yahoo! answers discussions and the observed demographic attributes can reach an average pearson correlation coefficient of \\ r { ho } = 0. 00, slightly higher than nielsen predictions offered using twitter data. our qualitative analysis indicates that there is semantic relatedness linking the highest correlated terms extracted off both datasets and their relative risk attributes. furthermore, the correlations creates two different indices of the outcomes contained in yahoo! answers and apps. on the former seems to offer a more encyclopedic content, the recent provides information equivalent to the current defining aspects or phenomena.", "histories": [["v1", "Tue, 17 Jan 2017 12:53:19 GMT  (2610kb,D)", "http://arxiv.org/abs/1701.04653v1", "Submitted to ICWSM2017"]], "COMMENTS": "Submitted to ICWSM2017", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["marzieh saeidi", "alessandro venerandi", "licia capra", "sebastian riedel"], "accepted": false, "id": "1701.04653"}, "pdf": {"name": "1701.04653.pdf", "metadata": {"source": "CRF", "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods", "authors": ["Marzieh Saeidi", "Licia Capra", "Alessandro Venerandi", "Sebastian Riedel"], "emails": ["m.saeidi@cs.ucl.ac.uk", "l.capra@ucl.ac.uk", "alessandro.venerandi.12@ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "Introduction", "text": "Recent years have seen a huge boom in the number of different social media platforms available to users. People are increasingly using these platforms to voice their opinions or let others know about their whereabouts and activities. Each of these platforms has its own characteristics and is used for different purposes. The availability of a huge amount of data from many social media platforms has inspired researchers to study the relation between the data generated through the use of these platforms and real-world attributes.\nMany recent studies in this field are particularly inspired by the availability of text-based social media platforms such as blogs and Twitter. Text from Twitter microblogs, in particular, has been widely used as data source to make predictions in many domains. For example, box-office revenues are predicted using text from Twitter (Asur, Huberman, and others 2010). Twitter data has also been used to find correlations between the mood stated in tweets and the value of Dow Jones Industrial Average (DJIA) (Bollen, Mao, and Zeng 2011).\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nPredicting demographics of individual users using their language on social media platforms, especially Twitter, has been the focus of many research works: text from blogs and on-line forum posts are utilised to predict user\u2019s age through the analysis of linguistic features. Results show that the age of users can be predicted where the predicted and observed values reach a Pearson correlation coefficient of almost 0.7. Sociolinguistic associations using geo-tagged Twitter data have been discovered (Eisenstein, Smith, and Xing 2011) and the results indicate that the demographic information of users such as first language, race, and ethnicity can be predicted by using text from Twitter with a correlation up to 0.3. Other research shows that users\u2019 income can also be predicted using tweets with a good prediction accuracy (Preot\u0327iuc-Pietro, Lampos, and Aletras 2015). Text from Twitter microblogs has also been used to discover the relation between the language of users and the deprivation index of neighbourhoods. The collective sentiment extracted from the tweets of users has been shown (Quercia et al. 2012) to have significant correlation (0.35) with the deprivation index of the communities the users belong to.\nData generated on QA platforms have not been used in the past for predicting real-world attributes. Most research work that utilise QA data aim to increase the performance of such platforms in analysing question quality (Li et al. 2012), predicting the best answers (Liu, Liu, and Yang 2010; Tian et al. 2013) or the best responder (Zhao et al. 2012).\nIn this paper, we use the text from the discussions on the QA platform of Yahoo! Answers about neighbourhoods of London to show that the QA text can be used to predict the demographic attributes of the population of those neighbourhoods. We compare the performance of Yahoo! Answers data to the performance of data from Twitter, a platform that has been widely used for predicting many real-world attributes. Unlike many current works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo! Answers and Twitter are semantically related to these attributes and provide examples of sociocultural profiles of neighbourhoods through the interpretation of the coefficients of the predictive models.\nThe contributions of this paper can be summarised as follows:\nar X\niv :1\n70 1.\n04 65\n3v 1\n[ cs\n.C L\n] 1\n7 Ja\nn 20\n17\n\u2022 We show that text from QA discussions can be used to predict real-world attributes such as demographic attributes of the population of neighbourhoods with a performance comparable to Twitter data.\n\u2022 Our analysis highlights the differences between data from a QA platform and Twitter: while QA data offers a more encyclopedic content, the latter provides information related to current sociocultural aspects."}, {"heading": "Datasets", "text": "Yahoo! Answers . Community QA platforms help users to obtain information from a community \u2013 a user can post questions which may then be answered by other users. Discussions can be in depth and in length but the interactions are not spontaneous. The time frame in which users take part in a discussion thread can vary from one day to several days or even to months. Moreover, QA platforms, unlike Twitter and some of the other social media platforms, are not locationbased. Yahoo! Answers is one of the few QA platforms that have emerged in the past decade. Discussions in Yahoo! Answers are not domain specific and can cover a broad range of topics.\nTwitter. Twitter is an on-line microblogging social network where users can post and read short 140-character messages. Twitter is used mostly to share views, opinions and news in real-time. Unlike QA platforms, Twitter is ubiquitous. While some users use Twitter to get updates on the news and their social circle, many others use it as part of their daily routine to talk about their thoughts, whereabouts, activities or sometimes just to share what is going on in their lives. This can be because of the strong tie that there exists between Twitter and smartphones which are nowadays at the centre of many people\u2019s lives. For all these reasons, huge amount of data is constantly being created on this platform. Additionally, Twitter is a location-based platform where people can tag their locations while blogging their tweets.\nYahoo! Answers vs. Twitter. When it comes to neighbourhoods, Yahoo! Answers have been used by many users to ask or to answer questions about different characteristics of many neighbourhoods. While people may not use Twitter in the same way, they may log their tweets while being in different neighbourhoods. In this paper, we investigate the extent to which the discussion on Yahoo! Answers platforms about neighbourhoods and the microblogs that are logged from different neighbourhoods can reflect characteristics of those neighbourhoods. Table 1 shows examples of Yahoo! Answers discussions that contain the names of London neighbourhoods and the term \u201cJewish\u201d. The table also shows examples of tweets that have been blogged from London neighbourhoods (neighbourhood\u2019s name in brackets) and contain the term \u201cJewish\u201d. These examples show some of the differences between the discussions that can be found on Yahoo! Answers and on Twitter. As we can see, the QA discussions are focused on a topic, i.e. Jewish neighbourhoods in London. The answers provide explicit information on neighbourhoods that have a high population of Jewish. On the other hand, microblogs of Twitter do not focus on providing explicit information about the neighbourhoods. But they contain information on user\u2019s activities or observations (e.g. Jewish Museum, Jewish food) while in different locations. These activities can implicitly indicate that Jewish communities inhibit the neighbourhoods that the user is blogging from.\nPopulation Demographic Data. Population demographic data is taken from the UK census provided by the Office for National Statistics. 1 Census surveys in the UK are repeated every 10 years and were last conducted in 2011. Census data is provided for specific geographical units that are created solely for the purpose of census data collection. These are called Lower Layer Super Output areas (LSOAs) and are identified through an alphanumeric ID. Greater London is divided into 4, 835 LSOAs. These are not necessarily equal in size as they have been designed to have a population of around 1, 500. Census data provides statistical information\n1http://www.ons.gov.uk/\non a wide range of categories such as the average house price in a LSOA, population count (or percent) of a religion or an ethnic background, income level, etc. Each category can be subdivided into further attributes. For instance, the category religion contains the attributes Muslim, Christian, Jewish, Hindu, etc."}, {"heading": "Method", "text": ""}, {"heading": "Spatial Unit of Analysis", "text": "The spatial unit of analysis chosen for this work is the neighbourhood. This is identified with a unique name (e.g., Camden) and people normally use this name in QA discussions to refer to specific neighbourhoods. A list of neighbourhoods for London is extracted from the GeoNames gazetteer2, a dataset containing names of geographic places including place names. For each neighbourhood, GeoNames provides its name and a set of geographic coordinates (i.e., latitude and longitude) which roughly represents its centre. Note that geographical boundaries are not provided. GeoNames contains 589 neighbourhoods that fall within the boundaries of the Greater London metropolitan area. In the remainder of the paper, we use the terms \u201cneighbourhood\u201d or \u201carea\u201d to refer to our spatial unit of analysis."}, {"heading": "Pre-processing, Filtering, and Spatial Aggregation", "text": "Yahoo! Answers Data. We collect questions and answers (QAs) from Yahoo! Answers using its public API.3 For each neighbourhood, the query consists of the name of the neighbourhood together with the keywords \u201cLondon\u201d and \u201carea\u201d. This is to prevent obtaining irrelevant QAs for ambiguous entity names such as Victoria. For each neighbourhood, we then take all the QAs that are returned by the API. Each QA consists of a title and a content which is an elaboration on the title. This is followed by a number of answers. In total, we collect 12, 947 QAs across all London neighbourhoods. These QAs span over the last 5 years. It is common for users to discuss characteristics of several neighbourhoods in the same QA thread. This means that the same QA can be assigned to more than one neighbourhood. Figure 1 shows the histogram of the number of QAs for each neighbourhood. As the figure shows, the majority of areas have less than 100 QAs with some areas having less than 10. Only few areas have over 100 QAs.\nFor each neighbourhood, we create one single document by combining all the QA discussions that have been retrieved using the name of such neighbourhood. This document may or may not contain names of other neighbourhoods. We split each document into sentences and remove those neighbourhoods containing less than 40 sentences.\nWe then remove URLs from each document. The document is then converted to tokens and stop words are removed. All the tokens in all the documents are then stemmed. The goal of stemming is to reduce the different grammatical forms of a word to a common base form. Stemming is a special case of text normalisation. For example,\n2http://www.geonames.org/ 3https://developer.yahoo.com/answers/\na stemmer will transform the word \u201cpresumably\u201d to \u201cpresum\u201d and \u201cprovision\u201d to \u201cprovis\u201d. To keep the most frequent words, we remove any token that has appeared less than 5 times in less than 5 unique QAs. This leaves us with 8k distinct tokens.\nTwitter Data. To collect data from Twitter, we use the geographical bounding box of London, defined by the northwest and southeast points of the Greater London region. We then use this bounding box to obtain the tweets that are geotagged and are created within this box through the official Twitter API.4 We stream Twitter data for 6 months between December 2015 and July 2016. At the end, we have around 2, 000, 000 tweets in our dataset.\nTo assign tweets to different neighbourhoods, for each tweet, we calculate the distance between the location that it was blogged from and the centre points of all the neighbourhoods in our dataset. Note that the centre point for each neighbourhood is provided in the gazetteer. We then assign the tweet to the closest neighbourhood that is not further than 1 km from the tweet\u2019s geolocation. At the end of this process, we have a collection of tweets per each neighbourhood and we combine them to create a single document. Figure 2 shows the number of tweets per each neighbourhood. As we can see, the majority of neighbourhoods have less than 1000 tweets.\nWe remove all the target words (words starting with @) from the documents. The pre-processing is then similar to the QA documents. At the end of this process, we obtain 17k distinct frequent tokens for the Twitter corpus.\nPopulation Demographic Data. As we previously explained, each attribute in census data is assigned to spatial units called LSOAs. However, these units do not geographically match our units of analysis which are the neighbourhoods defined trough the gazetteer. A map showing the spatial mismatch is presented in Figure 3. To aggregate the data contained in the LSOAs at the neighbourhood level, we use the following approach.\n4https://dev.twitter.com/streaming/\nOften, when people talk about a neighbourhood, they refer to the area around its centre point. Therefore, the information provided for neighbourhoods in QA discussions should be very related to this geographic point. To keep this level of local information, for each demographic attribute, we assign only the values of the nearby LSOAs to the respective neighbourhood. To do this, we calculate the distance between each neighbourhood and all the LSOAs in London. The distance is calculated between the coordinates of a neighbourhood and the coordinates of each LSOA\u2019s centroid. For each neighbourhood, we then select the 10 closest LSOAs that are not further than one kilometre away. The value of each demographic attribute for each neighbourhood is then computed by averaging the values associated with the LSOAs assigned to it. We apply this procedure to all the demographic attributes."}, {"heading": "Document Representation", "text": "A very popular method for representing a document using its words is the tf-idf approach (Salton, Fox, and Wu 1983). Tfidf is short for term frequency-inverse document frequency where tf indicates the frequency of a term in the document and idf is a function of the number of documents that a terms has appeared in. In a tf-idf representation, the order of the words in the document is not preserved. For each term in a document, the tf-idf value is calculated as below:\ntf-idf(d, t) = tf (d, t)\nlog( Total number of documentsNumber of documents containing the term t ) (1)\nTo discount the bias for areas that have a high number of QAs or tweets, we normalise tf values by the length of each document as below. The length of a document is defined by the number of its tokens (non-distinctive words).\nNormalised tf(d, t) = Frequency of Term t in Document d Number of Tokens in Document d (2)"}, {"heading": "Correlation", "text": "To investigate the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods, we first study whether there are significant, strong and meaningful correlations between the terms present in each corpus and the many neighbourhood attributes through the Pearson correlation coefficient \u03c1. For each term in each corpus, we calculate the correlation between the term and all the selected demographic attributes. To do so, for each term, we define a vector with the dimension of the number of neighbourhoods. The value of each cell in this vector represents the normalised tf-idf value of the term for the corresponding neighbourhood. For each demographic attribute, we also define a vector with the dimension of the number of neighbourhoods. Each cell represents the value for the demographic attribute of the corresponding neighbourhood. We then calculate the Pearson correlation coefficient (\u03c1) between these two vectors to measure the strength of the association between each term and each attribute.\nSince we perform many correlation tests simultaneously, we need to correct the significance values (p-values) for multiple testing. We do so by implementing the Bonferroni correction, a multiple-comparison p-value correction, which is used when several dependent or independent statistical tests are being performed simultaneously. The Bonferroni adjustment ensures an upper bound for the probability of having an erroneous significant result among all the tests. All the p-values showed in this paper are adjusted through the use of the Bonferroni correction."}, {"heading": "Prediction", "text": "We investigate how well the demographic attributes can be predicted by using using Yahoo! Ansewrs and Twitter data. We define the task of predicting a continuous-valued demographic attribute for unseen neighbourhoods as a regression task given their normalised tf-idf document represen-\ntation. A separate regression task is defined for each demographic attribute. We choose linear regression for the prediction tasks as it has been widely used for predictions from text in the literature (Foster and Stine ; Joshi et al. 2010).\nDue to the high number of features (size of vocabulary) and a small number of training points, over-fitting can occur. To avoid this issue, we use elastic net regularisation, a technique that combines the regularisation of the ridge and lasso regressions. The parameters \u03b8 are estimated by minimising the following loss function. Here, yi is the value of an attribute for the i-th neighbourhood, vector xi is its document representation andN is the number of neighbourhoods in the training set.\nL = 1\nN N\u2211 i=1 (yi \u2212 xTi \u03b8)2 + \u03bb1||\u03b8||+ \u03bb2||\u03b8||2 (3)\nEvaluation. To measure the performance of a regression model, residual-based methods such as mean squared error are commonly used. Ranking metrics such as Pearson correlation coefficient have also been used in the literature (Preot\u0327iuc-Pietro et al. 2015; Eisenstein, Smith, and Xing 2011). Using a ranking measure has some advantages compared to a residual-based measure. First, ranking evaluation is more robust against extreme outliers compared to an additive residual-based evaluation measure. Second, ranking metrics are more interpretable than measures such as mean squared error (Rosset, Perlich, and Zadrozny 2005). We thus use this method for evaluating the performance of the regression models in this work.\nAs further performance check, we apply a 10 folds crossvalidation to each regression task. In each fold, we use 75% of the data for training and the remaining 25% for validation. At the end, we report the average performance over all folds together with the standard deviation. For each demographic attribute, i.e. target value, training and validation sets are sampled using Stratified Sampling. This is a sampling method from a population, when sub-populations within this population vary. For instance, in London, there are areas with very high or very low deprivation. In these cases, it is advantageous to sample each sub-population independently and proportionally to its size."}, {"heading": "Results", "text": "Note. There are many attributes across several categories in the census data. Because of space limitation, we conduct most of our experiments on a selected set of attributes. These attributes are taken from religion (population of Jewish%, population of Muslim%, population of Hindu%, population of Buddhist%), ethnicity (population of Black%, population of White%, and population of Asian% ethnicity), Price (average house prices) and deprivation (Index of Multiple Deprivation, IMD5). For a full breakdown of results please refer to the Appendix.\n5https://en.wikipedia.org/wiki/Multiple_ deprivation_index"}, {"heading": "Correlation", "text": "Number of Correlated Terms. The number of significantly correlated terms from both Yahoo! Answers and the Twitter with the selected demographic attributes are shown in Table 2. Note that the number of unique (frequent) words in Twitter (17k) is almost twice as in Yahoo! Answers (8k). The first column shows a demographic attribute and the second column indicates the source, i.e. Yahoo! Answers (Y!A for short) or Twitter. The third column (\u201cAll\u201d) shows the total number of terms that have a significant correlation with each attribute (p-value < 0.01). The following columns show the number of terms that have a significant correlation with the attribute with a \u03c1 in the given ranges. The last column shows the number of terms that are significantly correlated with the attribute with a negative \u03c1. The data source that has the highest number of correlated terms with each attribute is highlighted in bold.\nAs the table shows, terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . These results collectively suggest that there is a wealth of terms, both in Yahoo! Answers and Twitter, which can be used to predict the population demographics.\nSemantic Relatedness. In this section, we observe whether the correlations between terms and attributes are semantically meaningful. Due to the limited space, we select three attributes and their relative top correlated terms extracted from Yahoo! Answers (Table 3) and Twitter (Table 4). We choose the attributes Price and IMD as they show the highest number of correlated terms for both sources. For each source, we then choose one more attribute that has the highest number of strongly correlated terms (\u03c1 > 0.4), i.e. Jewish% for Yahoo! Anwsers and Buddhist% for Twitter.\nWe first examine Table 3 and provide examples of semantic similarity between Yahoo! Answers terms and the selected attributes. Words highlighted in bold are, in our view, the ones most associated with their respective attribute. For the attribute Deprivation, the majority of the terms seem to be linked to issues of deprived areas. \u201cPoverty\u201d, \u201cdrug\u201d, \u201cvictim\u201d, all refer to social issues. \u201cRundown\u201d and \u201cslum\u201d may be associated with the degradation of the surrounding environment. \u201cCockney\u201d is a dialect traditionally spoken by working class, and thus less advantaged, Londoners. For the attribute (High) Price, most terms seem to be related to aspects of places which may offer more expensive housing. Terms such as \u201cfortune\u201d, \u201cdiplomat\u201d, and \u201caristocratic\u201d are often associated with wealth. Others seem to reflect a posh lifestyle and status symbol: \u201ctownhouse\u201d, \u201cexclusive\u201d, \u201cceleb\u201d, \u201cfashionable\u201d, \u201cdesirable\u201d. For the attribute Jewish%, most of the terms seem to reflect aspects of this religion or be linguistically associated with it (i.e., \u201cJew\u201d and \u201cJewish\u201d). \u201cMatzo\u201d and \u201cKosher\u201d are associated with the traditional Jewish cuisine; the former is a type of flat-bread, the latter is a way of preparing food. The \u201cark\u201d is a specific part of the synagogue which contains sacred texts.\nWe now examine Table 4. For the attribute Deprivation, nine words out of ten seem to be linked to more deprived areas. \u201cEast\u201d, \u201ceastlondon\u201d, and \u201ceastend\u201d, for example, pro-\nvide geographical information on where deprivation is more concentrated in London (i.e., East End). Other terms seem to be related to the presence of younger generation of creatives and artists in more deprived neighbourhoods. \u201cYeah\u201d, \u201cshit\u201d, \u201cass\u201d, may all be jargons commonly used by this section of population. \u201cStudio\u201d, \u201ccraftbeer\u201d, \u201cmusic\u201d may instead refer to their main activities and occupations. For what concerns (high) \u201cPrice\u201d, all the terms seem to relate to aspects of expensive areas, e.g. \u201cluxury\u201d, \u201cclassy\u201d, and \u201cstylish\u201d. \u201cTea\u201d, \u201cteatime\u201d, \u201cdelight\u201d, \u201ctruffle\u201d seem to relate to social activities of the upper class. For the attribute \u201cBuddhist%\u201d, five terms out of ten are, in our view, associated with neighbourhoods where the majority of people is Buddhist or practise Buddhism. These terms seems to relate to aspects of this religion, e.g. \u201cthink\u201d, \u201clearn\u201d, \u201cmind\u201d, etc.\nInterestingly, terms extracted from Yahoo! Answers and Twitter seem to offer two different kinds of knowledge. On one side, terms extracted from Yahoo! Answers are more encyclopedic as they tend to offer definitions or renowned aspects for each attribute. \u201cJewish%\u201d is, for example, related to aspects of the Jewish culture such as \u201cmatzo\u201d, \u201charmony\u201d, and \u201ckosher\u201d. \u201cDeprivation\u201d is associated with social issues such as \u201cpoverty\u201d and \u201cdrug\u201d, but also with a degraded urban environment (e.g., \u201crundown\u201d, \u201cslum\u201d). On the other, Twitter words provide a kind of knowledge more related to current sociocultural aspects. This is the case, for example, of the jargon associated with \u201cDeprivation\u201d (e.g., \u201cyeah\u201d, \u201cshit\u201d), or of the culinary habits related to \u201cHigh Prices\u201d (e.g., \u201ctea\u201d, \u201ctruffle\u201d)."}, {"heading": "Prediction", "text": "The results of the regression tasks performed over the selected set of demographic attributes, in terms of Pearson correlation coefficient (\u03c1), are presented in Table 4. Results are averaged over 10 folds and standard deviations are displayed in parenthesis.\nWe can see that on average, performances of Yahoo! Answers and Twitter are very similarly with Yahoo! Answers having a slightly higher performance (4%). Twitter data can predict the majority of the religion-related attributes with a higher correlation coefficient with the exception of population of Jewish%. On the other hand, Yahoo! Answers is superior to Twitter when predicting ethnicity related attributes such as population of White% and Black%. We have seen in Table 2 that Twitter has very few correlated terms with the attributes White (0) and Black (2).\nWe also observe that IMD and Price can be predicted with a high correlation coefficient using both Yahoo! Answers and Twitter. This can be due to the fact that there are many words in our dataset that can be related to the deprivation of a neighbourhood or to how expensive a neighbourhood is. This is also evident in Table 2 where the number of correlated terms from both Yahoo! Answers and Twitter with these attributes are very high. On the other hand, terms that describe a religion or an ethnicity are more specific and lower in frequency. Therefore attributes that are related to religion or ethnicity are predicted with a lower accuracy.\nTable 5 further shows two terms that have the highest coefficients in the regressions models (across the majority of\nfolds) for each attribute and source in the column Terms. These terms are among the strong predictors of their respective attribute. Many of these terms appear to be related to the given demographic attribute (for both Twitter and Yahoo! Answers ) and are also often amongst the top correlated terms presented in Tables 3 and 4. We follow with some examples. According to the regression coefficients for the attribute Muslim%, neighbourhoods inhabited by a Muslim majority may be located in Mile End, an East London district (i.e., Twitter terms \u201cmileend\u201d and \u201ceastlondon\u201d), see the presence of Asian population and barber shops (i.e., Yahoo! Answers terms \u201casian\u201d and \u201cbarber\u201d). According to the terms for Black%, neighbourhoods with a black majority tend to be located in the southern part of London (i.e., Twitter term \u201csouthlondon\u201d) and experience social issues such as presence of criminal groups and drug use (i.e., Yahoo! Answers terms \u201cgang\u201d and \u201cdrug\u201d). According to the terms for IMD, more deprived areas seem to be located in the East End of London (i.e., Twitter term \u201ceastlondon\u201d) where the Cockney dialect is dominant (i.e., Yahoo! Answers term \u201ccockney\u201d). Yahoo! Answers and Twitter seem to complement one another in terms of information they provide through the terms associated with each attribute which in most cases are different. One noticeable difference is that Twitter tends to offer geographical information (e.g., \u201cmileend\u201d, \u201csouthlondon\u201d, \u201cessex\u201d). On the other hand, terms from Yahoo! Answers sometimes match the name of the attribute (i.e. \u201casian\u201d and \u201cJewish\u201d).\nIn the Appendix, in Tables 6 and 7, we show the prediction results for a wide range of 62 demographic attributes using Yahoo! Answers and Twitter. For each attribute, we display two terms with the highest coefficient common between the majority of the folds. Attributes are divided into categories such as Religion, Employment, Education, etc. Overall, the results show that Yahoo! Answers performs slightly better than Twitter with an average 1% increase over all the attributes. Wilxocon signed rank test shows that their results are significantly different from each other (p-value < 0.01). Outcomes in these tables show that on average, a wide range of demographic attributes of the population of neighbourhoods can be predicted using both Yahoo! Answers and Twitter with high performances of 0.54 and 0.53 respectively. While Yahoo! Answers outperforms Twitter in\npredicting attributes related to Ethnicity and Employment, Twitter performs better when predicting attributes relating to the Age Group, and Car Ownership."}, {"heading": "Related Work", "text": "The availability of a huge amount of data from many social media platforms has inspired researchers to study the relation between the data on these platforms and many realworld attributes. Twitter data, in particular, has been widely used as a social media source to make predictions in many domains. For example, box-office revenues are predicted using text from Twitter microblogs (Asur, Huberman, and others 2010). Prediction results have been predicted by performing content analysis on tweets (Tumasjan et al. 2010). It is shown that correlations exist between mood states of the collective tweets to the value of Dow Jones Industrial Average (DJIA) (Bollen, Mao, and Zeng 2011).\nPredicting demographics of individual users using their language on social media platforms, especially Twitter, has been the focus of many research. Text from blogs, telephone conversations, and forum posts are utilised for predicting author\u2019s age (Nguyen, Smith, and Rose\u0301 2011) with a Pearson\u2019s correlation of 0.7. Geo-tagged Twitter data have been used to predict the demographic information of authors such as first language, race, and ethnicity with correlations up to 0.3 (Eisenstein, Smith, and Xing 2011).\nOne aspect of urban area life that has been the focus of many research work in urban data mining is finding correlations between different sources of data and the deprivation index (IMD), of neighbourhoods across a city or a country (Lathia, Quercia, and Crowcroft 2012; Quercia et al. 2012). Cellular data (Smith-Clarke, Mashhadi, and Capra 2014) and the elements present in an urban area (Venerandi et al. 2015) are among non-textual data sources that are shown to have correlations with a deprivation index. Also, flow of public transport data has been used to find correlations (with a correlation coefficient of r = 0.21) with IMD of urban areas available in UK census (Lathia, Quercia, and Crowcroft 2012). Research shows that correlations of r = 0.35 exists between the sentiment expressed in tweets of users in a community and the deprivation index of the community (Quercia et al. 2012).\nSocial media data has been used in many domains to find\nlinks to the real-world attributes. Data generated on QA platforms, however, has not been used in the past for predicting such attributes. In this paper, we use discussions on Yahoo! Answers QA platform to make predictions of demographic attribute of city neighbourhoods. Previous work in this domain has mainly focused on predicting the deprivation index of areas (Quercia et al. 2012). In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. Additionally, work in urban prediction uses geolocation-based platforms such as Twitter. QA data that has been utilised in this paper does not include geolocation information. Utilising such data presents its own challenges."}, {"heading": "Discussion", "text": "In this paper, we investigate predicting values for real-world entities such as demographic attributes of neighbourhoods using discussions from QA platforms. We show that these attributes can be predicted using text features based on Yahoo! Answers discussions about neighbourhoods with a slightly higher correlation coefficient than predictions made using Twitter data."}, {"heading": "Limitations", "text": "Here, we present some of the limitations of our work.\nUnification of the units of analysis. To unify the units of analysis, we take a heuristic approach. We do not crossvalidate our results with other approaches. This is because of the lack of work in using non-geotagged text for predicting attributes of neighbourhoods in the current literature.\nCoverage. Our experiments in this paper is limited to the city of London. London is a cosmopolitan city and a popular destination for travellers and settlers. Therefore, many discussions can be found on Yahoo! Answers regarding its neighbourhoods. The coverage of discussions on QA platforms may not be sufficient for all cities of interest."}], "references": [{"title": "Predicting the future with social media", "author": ["S. Asur", "B Huberman"], "venue": "In Proceedings of the International Conferences on Web Intelligence and Intelligent Agent Technology", "citeRegEx": "Asur and Huberman,? \\Q2010\\E", "shortCiteRegEx": "Asur and Huberman", "year": 2010}, {"title": "Twitter mood predicts the stock market", "author": ["J. Bollen", "H. Mao", "X. Zeng"], "venue": "Journal of Computational Science.", "citeRegEx": "Bollen et al\\.,? 2011", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Discovering sociolinguistic associations with structured sparsity", "author": ["J. Eisenstein", "N.A. Smith", "E.P. Xing"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Eisenstein et al\\.,? 2011", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Movie reviews and revenues: An experiment in text regression", "author": ["M. Joshi", "D. Das", "K. Gimpel", "N.A. Smith"], "venue": "Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Joshi et al\\.,? 2010", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "The hidden image of the city: sensing community well-being from urban mobility", "author": ["N. Lathia", "D. Quercia", "J. Crowcroft"], "venue": "Pervasive computing.", "citeRegEx": "Lathia et al\\.,? 2012", "shortCiteRegEx": "Lathia et al\\.", "year": 2012}, {"title": "Analyzing and predicting question quality in community question answering services", "author": ["B. Li", "T. Jin", "M.R. Lyu", "I. King", "B. Mak"], "venue": "Proceedings of the International Conference on World Wide Web.", "citeRegEx": "Li et al\\.,? 2012", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Predicting best answerers for new questions in community question answering", "author": ["M. Liu", "Y. Liu", "Q. Yang"], "venue": "Proceedings of the International Conference on Web-Age Information Management.", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Author age prediction from text using linear regression", "author": ["D. Nguyen", "N.A. Smith", "C.P. Ros\u00e9"], "venue": "Proceedings of the 5th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities: Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Nguyen et al\\.,? 2011", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Studying user income through language, behaviour and affect in social media", "author": ["D. Preo\u0163iuc-Pietro", "S. Volkova", "V. Lampos", "Y. Bachrach", "N. Aletras"], "venue": "PlOS ONE.", "citeRegEx": "Preo\u0163iuc.Pietro et al\\.,? 2015", "shortCiteRegEx": "Preo\u0163iuc.Pietro et al\\.", "year": 2015}, {"title": "An analysis of the user occupational class through twitter content", "author": ["D. Preo\u0163iuc-Pietro", "V. Lampos", "N. Aletras"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Preo\u0163iuc.Pietro et al\\.,? 2015", "shortCiteRegEx": "Preo\u0163iuc.Pietro et al\\.", "year": 2015}, {"title": "Tracking gross community happiness from tweets", "author": ["D. Quercia", "J. Ellis", "L. Capra", "J. Crowcroft"], "venue": "Proceedings of the International Conference on Computer Supported Cooperative Work and Social Computing.", "citeRegEx": "Quercia et al\\.,? 2012", "shortCiteRegEx": "Quercia et al\\.", "year": 2012}, {"title": "Rankingbased evaluation of regression models", "author": ["S. Rosset", "C. Perlich", "B. Zadrozny"], "venue": "Proceedings of the International Conference on Data Mining.", "citeRegEx": "Rosset et al\\.,? 2005", "shortCiteRegEx": "Rosset et al\\.", "year": 2005}, {"title": "Extended boolean information retrieval", "author": ["G. Salton", "E.A. Fox", "H. Wu"], "venue": "Communications of the ACM.", "citeRegEx": "Salton et al\\.,? 1983", "shortCiteRegEx": "Salton et al\\.", "year": 1983}, {"title": "Poverty on the cheap: Estimating poverty maps using aggregated mobile communication networks", "author": ["C. Smith-Clarke", "A. Mashhadi", "L. Capra"], "venue": "Proceedings of International Conference on Human Factors in Computing Systems.", "citeRegEx": "Smith.Clarke et al\\.,? 2014", "shortCiteRegEx": "Smith.Clarke et al\\.", "year": 2014}, {"title": "Predicting best answerers for new questions: An approach leveraging topic modeling and collaborative voting", "author": ["Y. Tian", "P.S. Kochhar", "E.-P. Lim", "F. Zhu", "D. Lo"], "venue": "Proceedings of the Workshops at the International Conference on Social Informatics.", "citeRegEx": "Tian et al\\.,? 2013", "shortCiteRegEx": "Tian et al\\.", "year": 2013}, {"title": "Predicting elections with twitter: What 140 characters reveal about political sentiment", "author": ["A. Tumasjan", "T.O. Sprenger", "P.G. Sandner", "I.M. Welpe"], "venue": "Proceedings of the International Conference on Web and Social Media.", "citeRegEx": "Tumasjan et al\\.,? 2010", "shortCiteRegEx": "Tumasjan et al\\.", "year": 2010}, {"title": "Measuring urban deprivation from user generated content", "author": ["A. Venerandi", "G. Quattrone", "L. Capra", "D. Quercia", "D. Saez-Trumper"], "venue": "Proceedings of the International Conference on Computer Supported Cooperative Work and Social Computing.", "citeRegEx": "Venerandi et al\\.,? 2015", "shortCiteRegEx": "Venerandi et al\\.", "year": 2015}, {"title": "Predicting best responder in community question answering using topic model method", "author": ["T. Zhao", "C. Li", "M. Li", "S. Wang", "Q. Ding", "L. Li"], "venue": "Proceedings of the International Conferences on Web Intelligence and Intelligent Agent Technology.", "citeRegEx": "Zhao et al\\.,? 2012", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "The collective sentiment extracted from the tweets of users has been shown (Quercia et al. 2012) to have significant correlation (0.", "startOffset": 75, "endOffset": 96}, {"referenceID": 5, "context": "Most research work that utilise QA data aim to increase the performance of such platforms in analysing question quality (Li et al. 2012), predicting the best answers (Liu, Liu, and Yang 2010; Tian et al.", "startOffset": 120, "endOffset": 136}, {"referenceID": 14, "context": "2012), predicting the best answers (Liu, Liu, and Yang 2010; Tian et al. 2013) or the best responder (Zhao et al.", "startOffset": 35, "endOffset": 78}, {"referenceID": 17, "context": "2013) or the best responder (Zhao et al. 2012).", "startOffset": 28, "endOffset": 46}, {"referenceID": 3, "context": "We choose linear regression for the prediction tasks as it has been widely used for predictions from text in the literature (Foster and Stine ; Joshi et al. 2010).", "startOffset": 124, "endOffset": 162}, {"referenceID": 8, "context": "Ranking metrics such as Pearson correlation coefficient have also been used in the literature (Preo\u0163iuc-Pietro et al. 2015; Eisenstein, Smith, and Xing 2011).", "startOffset": 94, "endOffset": 157}, {"referenceID": 15, "context": "Prediction results have been predicted by performing content analysis on tweets (Tumasjan et al. 2010).", "startOffset": 80, "endOffset": 102}, {"referenceID": 10, "context": "One aspect of urban area life that has been the focus of many research work in urban data mining is finding correlations between different sources of data and the deprivation index (IMD), of neighbourhoods across a city or a country (Lathia, Quercia, and Crowcroft 2012; Quercia et al. 2012).", "startOffset": 233, "endOffset": 291}, {"referenceID": 16, "context": "Cellular data (Smith-Clarke, Mashhadi, and Capra 2014) and the elements present in an urban area (Venerandi et al. 2015) are among non-textual data sources that are shown to have correlations with a deprivation index.", "startOffset": 97, "endOffset": 120}, {"referenceID": 10, "context": "35 exists between the sentiment expressed in tweets of users in a community and the deprivation index of the community (Quercia et al. 2012).", "startOffset": 119, "endOffset": 140}, {"referenceID": 10, "context": "Previous work in this domain has mainly focused on predicting the deprivation index of areas (Quercia et al. 2012).", "startOffset": 93, "endOffset": 114}], "year": 2017, "abstractText": "In this paper, we investigate whether text from a Community Question Answering (QA) platform can be used to predict and describe real-world attributes. We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our results to the ones obtained from Twitter microblogs. Outcomes show that the correlation between the predicted demographic attributes using text from Yahoo! Answers discussions and the observed demographic attributes can reach an average Pearson correlation coefficient of \u03c1 = 0.54, slightly higher than the predictions obtained using Twitter data. Our qualitative analysis indicates that there is semantic relatedness between the highest correlated terms extracted from both datasets and their relative demographic attributes. Furthermore, the correlations highlight the different natures of the information contained in Yahoo! Answers and Twitter. While the former seems to offer a more encyclopedic content, the latter provides information related to current sociocultural aspects.", "creator": "TeX"}}}