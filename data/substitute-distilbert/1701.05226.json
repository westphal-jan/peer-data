{"id": "1701.05226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2017", "title": "Reasoning in Non-Probabilistic Uncertainty: Logic Programming and Neural-Symbolic Computing as Examples", "abstract": "relevant article aims to achieve two goals : arguments show that probability is not the only property of dealing with uncertainty ( and obviously more, that there is kinds of uncertainty which are for principled reasons not addressable with probabilistic means ) ; and neither provide means new logic - independent methods can well support reasoning with uncertainty. for the latter claim, two paradigmatic examples are presented : logic programming with kleene semantics for modelling assumptions from uncertainty in a discourse, to an interpretation of the state of affairs of the intended model, and a neural - basis implementation of input / output logic for dealing with uncertainty in dynamic normative contexts.", "histories": [["v1", "Wed, 18 Jan 2017 20:38:55 GMT  (350kb,D)", "http://arxiv.org/abs/1701.05226v1", "Submitted to the Special Issue \"Reasoning with Imperfect Information and Knowledge\" of Minds and Machines (2017)"], ["v2", "Wed, 1 Mar 2017 15:36:37 GMT  (360kb,D)", "http://arxiv.org/abs/1701.05226v2", "Forthcoming with DOI 10.1007/s11023-017-9428-3 in the Special Issue \"Reasoning with Imperfect Information and Knowledge\" of Minds and Machines (2017). The final publication will be available atthis http URL--- Changes to previous version: Fixed some typos and a broken reference"]], "COMMENTS": "Submitted to the Special Issue \"Reasoning with Imperfect Information and Knowledge\" of Minds and Machines (2017)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tarek r besold", "artur d'avila garcez", "keith stenning", "leendert van der torre", "michiel van lambalgen"], "accepted": false, "id": "1701.05226"}, "pdf": {"name": "1701.05226.pdf", "metadata": {"source": "CRF", "title": "Reasoning in Non-Probabilistic Uncertainty Logic Programming and Neural-Symbolic Computing as Examples", "authors": ["Tarek R. Besold", "Artur d\u2019Avila Garcez", "Keith Stenning", "Leendert van der Torre", "Michiel van Lambalgen"], "emails": ["Tarek.Besold@uni-bremen.de", "a.garcez@city.ac.uk", "k.stenning@ed.ac.uk", "leon.vandertorre@uni.lu", "M.vanLambalgen@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "\u201cAlmost all everyday inference is uncertain, and, thus, human reasoning should be assessed using probability theory, the calculus of uncertainty, rather than logic, the calculus of certainty.\u201d [45, p. 308]\nWhile fully agreeing on the premise of the statement\u2014namely the observation that most human reasoning is uncertain in nature\u2014we want to challenge the conclusion Oaksford and Chater [45] (and many others) draw from it: in our view probability theory is neither the perfect solution solving all challenges introduced by reasoning\u2019s inherent uncertainty, nor should logic be overly casually discarded as exclusively fit to deal with reasoning in certainty. In fact, the\n? Submitted to the Special Issue \u201cReasoning with Imperfect Information and Knowledge\u201d of Minds and Machines (2017).\nar X\niv :1\n70 1.\n05 22\n6v 1\n[ cs\n2 conception of monotonic classical logic as \u2018reasoning in certainty\u2019 can be misleading. In order to substantiate these two claims, below we first illustrate how Logic Programming (LP)\u2014as a logic-based reasoning and computation paradigm\u2014can be used to model reasoning which involves the resolution of uncertainties of a kind not amenable to probability theory. This is followed by the presentation of a neural-symbolic approach to LP extended to implement Makinson\u2019s and van der Torre\u2019s Input/Output (I/O) logic [37], which can similarly be used to model non-probabilistic uncertainty in normative reasoning; uncertainty which is compounded by changing or newly added norms (i.e. dynamic environments requiring machine learning).\nAmong the main foundations of our argument is the observation that there are several qualitatively different kinds of uncertainty, for some of which probability\u2014 as convincingly shown by Oaksford and Chater [45]\u2014can be a powerful modelling technique, while others clearly lie outside of the reach of probabilistic approaches. The need for distinguishing kinds of uncertainty has surfaced regularly, for instance, in the study of judgement and decision-making. Knight [29] made an early distinction between \u2018risk\u2019 (which could be modelled in probability) and \u2018uncertainty\u2019 (which could not), in economics. Kahneman and Tversky [27] already discussed several variants of uncertainty, distinguishing, for example, what we might call private uncertainty (\u201cHow sure am I that Rome is south of New York?\u201d where the truth is taken to be already known by others) from communal uncertainty (\u201cHow likely is it at the time of writing that the euro will collapse?\u201d). More recently, [8, page 1225] proposed a taxonomy of uncertainties, in which:\n\u201c[. . . ] ethical, option and state space uncertainty are distinct from state uncertainty, the empirical uncertainty that is typically measured by a probability function on states of the world.\u201d\nThis comes with the claim that a single probability function cannot provide an adequate simultaneous account.6 Mousavi and Gigerenzer [40] expand Knight\u2019s distinctions adding a further type of \u2018utter uncertainty\u2019 which describes cases where models are used in new domains.\nOne way of making the existence of a range of kinds of uncertainty distinct from probability generally plausible, is to think in terms of what a probabilistic model has to specify\u2014and then consider what happens when elements are unavailable. Probabilistic models have to specify an algebra of propositions (variables) which exhaust the \u2018effects\u2019 encompassed by the model. Structural relations of dependence and independence then have to be supplied or assumed. Distributional information has to be attached; and last but not least, an assumption of \u2018stationarity\u2019 has to be made, i.e. the probabilistic relations between the isolated algebra of propositions have to be assumed to remain constant, and without intrusion of extraneous influences.\n6 However, these authors\u2014somewhat paradoxically\u2014in the end come to the view that whatever uncertainty is the topic, probability is the framework for modelling it; cf. Section 2.2 for some considerations on the corresponding argument and conclusion.\n3 Several example phenomena that make this probabilistic schema impossible or unfruitful to apply will recurr throughout this paper: LP modelling of the interpretation of discourse (Section 2); learning (Section 3.4) which contributes various \u2018nonstationarities\u2019 to reasoning systems; motivational phenomena which include both the norms expressed in deontic concepts such as priorities, obligations, permissions, contrary to duties (CTDs) (Section 3.2), but also motivational ones such as goals and values. All these various examples share the requirement for the flexible handling of dynamic contexts, with the ensuing robustness to exceptions, as we illustrate.\nDiscourses are connected language. The process starts with input of discourse sentences, and sequentially builds a unique minimal \u2018preferred\u2019 model at each step.7 The algebra of propositions grows at each step, and when examined carefully, the propositions already in the model change\u2014if only by taking on temporal/causal relations to the new ones that arrived [67]\u2014even when they are not dropped nonmonotonically. The structural \u2018common core\u2019 of the last LP model in this sequence can provide, for instance, the basis for creating a Bayes Net by adding distributional information [46,48]. Still, the propositions had not been identifiable until now, and so, no distributions could be attached, nor judgments made about causal stationarity. Reasoning to this point cannot be probabilistic; and even at this point, it is not entirely clear whether or how the necessary distributional information is available in general. And this is just the beginning of the difficulties since, for example, LP can freely express intentional relations between acts and actors\u2019 multiple goals [68], which can at best be inflexibly \u2018operationalised\u2019 in extensional systems. Extensional systems may be able to formulate propositions that, when they become true, indicate that a goal has been fulfilled. However, they cannot capture what goals are: namely, the abstract flexibility of systems of motivational states and their interactions in unpredictable environments, as we shall see.\nIt is worth noting that the recent success of probabilistic language models based on neural networks [69,20] is orthogonal to the arguments presented in this paper. Yet, within the area of neural-symbolic computing [15], equivalences have been proved between LP and neural networks, which indicate the possibility of reconciling such apparently very distinct representational frameworks. In fact, later in this paper a (non-probabilistic) neural characterisation of LP will be proposed which seeks to combine the requirements identified here of three-valued semantics and spreading of activation (discussed in what follows) with an ability to learn from examples, which is also a requirement of discourse processing in the case of dynamic environments. This neural-symbolic approach will be applied to deontic attitudes which preclude probability, since they are motivational in the extended notion of the word used in this paper: an obligation establishes a goal, even if that goal gets overriden by dynamic changes in circumstance. This will be exemplified in the context of reasoning in uncertainty about an environment\n7 Later, we shall consider an alternative LP semantics based on Answer Sets [17], but we choose Preferred Model Semantics [53] for now because the uniqueness of preferred models is a crucial feature for cognitive processes such as discourse processing.\n4 of changing norms, which might themselves evolve in many as yet indeterminate ways.\nAt this point we take a step back and include what might seem like a mere clarification of terminology, but goes in fact much beyond that: the sense of \u2018extensional\u2019 and \u2018intensional\u2019 we use here are from the psychological decision making literature\u2014stemming from [66]\u2014in which probability is extensional because its predicates are defined as sets, even though its conditional is nontruthfunctional. In that literature, \u2018intensional\u2019 is often synonymous with \u2018informal\u2019 as it lacks a suitable logic. Nonetheless, LP (with the semantics as specified by Stenning and van Lambalgen [56]) is intensional in the general philosophical sense: its predicates are defined in terms of \u2018senses\u2019 which are cashed out as algorithms (completion), and its closed-world reasoning conditionals are \u2018licences for inference\u2019 (roughly contentful inference rules) rather than compound propositions (cf. Section 2 for an introduction). An example of the role this interpretation of the intensional/extensional distinction plays, is the famous \u2018Conjunction Fallacy\u2019 [66] which is supposed to be about judgements of the relative sizes of sets of cases corresponding to predicates and their conjunctions in probability. If a reader\u2019s interpretation is in LP, then this extensional distinction makes no sense. The distinctions required for differentiating qualitatively different kinds of uncertainty can be seen more clearly in the contrast in the semantics of the intensional and extensional systems we discuss below. Stenning et al. [59] use the Linda Task (the origin of the supposed Conjunction Fallacy) to illustrate this intensional option for interpretation, and its consequences. In the large, we believe these contrasts between intensional and extensional systems are at the heart of the contrasts in kinds of uncertainty which are our focus.\nAll these issues concerning the variety of kinds of uncertainty, and the characteristics of different formalisms and representation systems, also resound in the many different types of uncertainty Artificial Intelligence (AI) has to deal with in realistic scenarios: the world might only be partially observable, observation data might be noisy, the actual outcome of actions might be different from the theoretically assumed one (either due to previously unknown or unconsidered factors, or due to independent external influences), or a prediction and assessment of present and future world states, action outcomes, etc. might just practically be outright impossible due to the immense complexity of the environment and scenario. As can be expected, there is an accordingly large AI literature on reasoning in uncertainty, and the variety of systems other than probability which are available. Quite recently Kern-Isberner and Lukasiewicz [28] provided a helpful brief map of approaches to uncertainty\u2014which, among others, demarcates logic programming under answer set semantics as distinctively outside both the currently popular system P [31] and the AGM axioms [1]. Another aid to navigation is offered by Halpern [21] who is concerned with distinguishing different representations of uncertainty, and then studying reasoning from those representations. The example problems he gives in Chapter 1, pages 1 to 4, all have the property that the domain of interpretation and the properties and relations defined thereon are fixed at the outset. This need to\n5 fix interpretation is imposed by classical logic and probability, because of their need to generalise about all assignments of values to the vocabulary. Only the sampling, and with it the epistemic states of the judges of uncertainties, varies. For such problems, Halpern argues convincingly that plausibility measures are a generalisation of probability measures, and that different plausibility measures are more or less appropriate for different problems.8 But contrary to this general line of investigation, our focus is on the more radical kind of uncertainty faced by reasoning to interpretations.\nOur paper thus has a much more modest exploratory goal than Halpern\u2019s monumental work. It seeks to develop two examples in some detail of reasoning to interpretations. So by definition, in these examples, much less is known about the particularities of their domains until their specification is finished, and it is the uncertainty during this process of reasoning which is our focus. A great deal of general knowledge must be applied in the dynamic development of their specification during the period of interpretation whose uncertainties are of interest. The example of narrative discourse interpretation exemplifies this character: At the outset of a story, the hearer may know nothing in particular of what will turn out to be in the preferred model of the story that will develop; not even what range of properties and relations will distinguish the characters and events that unfold, even though the hearer\u2019s knowledgebase must contain a great deal of general knowledge, some of which has to be mobilised to interpret the current input discourse. We nevertheless succeed in reasoning to interpretations of such stories with remarkable, though not inevitable, success. This reasoning is omnipresent in human problem solving and communication [56]. It has to be a precursor to modelling in probability, or plausibility measures more generally, because the vocabulary of predicates and relations has to be established by reasoning to interpretations. Modelling this reasoning to interpretations requires a framework that is more radically nonmonotonic than probability theory, whose underlying propositions to which probabilities are attached, are classical logical propositions. [59] argue that such extensional systems are inherently incapable of providing the requisite flexibility. Understanding how reasoning to interpretations works with reasoning from them can lead to a deeper understanding of both.\nStill at this synoptic level, another difference between Halpern\u2019s plausibility measures and LP doing discourse interpretation is that the former begin with numerical parameters on propositions, and output numerical parameters on inferred propositions. In contrast, LP reasoning to interpretations of discourse need have no numerical parameters, even though numerical properties of the logical structures involved have been shown to be the basis on which reasoners make quantitative judgments of LP conditionals\u2019 reliability in inference [59]. Counting the defeaters for a conditional can predict confidence in inferences from that conditional. For another example, Halpern\u2019s conditionals with plausibility measures treat conditionals as propositions having truth values. In LP with Kleene\n8 In terms of concrete examples, the work by Nilsson [42] comes to mind as a prominent instance falling within the domain of Halpern\u2019s plausibilty measures.\n6 semantics modeling discourse interpretation, conditionals are not propositions (they are licences for inference), and they therefore do not iterate (cf. [56, p. 184, footnote 9]), nor do they become false if they do not apply because of an abnormality condition.\nAt a more technical level, there are a number of contrasts between LP (and also the later discussed I/O logic) and plausibility measures [21] which adopt the KLM axioms. For example, plausibility measures apply to systems with the \u2018OR-rule\u2019. From p\u2192 q, r \u2192 q it follows that (p\u2228r)\u2192 q. This rule does not apply in LP because the \u2018abnormality clauses\u2019 defeat it: p \u2227 \u00acab\u2032 \u2192 q, r \u2227 \u00acab\u2032\u2032 \u2192 q it does not follow that (p \u2228 r)\u2192 q.\nWe close this differentiation of our focus from existing work with an example of a more empirical dissatisfaction with probability as a model of human reasoning. Tversky and Kahneman [65] and Gigerenzer et al. [19,18] have extensively developed evidence that human reasoning is heuristic with regard to probability models. In an especially clear case of this relation, Juslin et al. [26] argue that peoples\u2019 judgements of likelihoods in uncertainty do not obey probabilistic models. However, their concern is with heuristic approximations for combinations of probabilities. They assume that people have access to the probabilities (or their estimates), and produce evidence that people combine them in the conjunctive case by \u2018weighting and adding\u2019, rather than the probabilistically normative procedure of multiplying. This is a different, calculative level of issue with probability than the conceptual differences that concern us here. These authors tiptoe towards the cliff of appreciation that probability might not be the right normative theory:\n\u201cImportantly in this context, to the extent that problems are framed in terms of probability that often requires multiplicative information integration, the strong inclination for linear additive integration is a first sense in which probability theory may often not be a very useful (or useable) guide in life.\u201d [26, p. 861]\nNonetheless, they do not take the leap of offering an intensional framework. Stenning et al. [59] explore an incorporation of such heuristic approaches within LP to provide a simple probability-free model of judgement and decision. This approach offers insights into how intensional reasoning produces structural foundations for subsequent probabilistic modelling through their \u2018common core\u2019 [48]. This functional relation of the two contrasting types of reasoning in uncertainty is of central importance in human reasoning, and is possibly one of its main ingredients which is least represented in current AI.\nIn Section 2 we first focus on the application of LP in building process models of \u201creasoning to an interpretation\u201d as a crucial part of human reasoning. As, for instance, Stenning and van Lambalgen [57] argue, when faced with interpretational uncertainty about the information involved in a problem, we cannot engage the computational complexity of probability, but have to take more accessible inferential paths\u2014which are often quite successful. Here, among other work building on the book by Stenning and van Lambalgen [56], a version of\n7 LP, using a semantics based on three-valued Kleene logics, offers itself as a modelling approach. Since it is a crucial cognitive capacity involved in our daily lives, as already mentioned above, discourse processing will serve as core theme and paradigmatic example in this section.\nSection 3 subsequently takes a more AI-centric view and describes a neuralsymbolic architecture combining the I/O logic proposed by Makinson and van der Torre [37] with artificial neural networks (ANNs), applied to normative reasoning tasks involving uncertainty introduced by changing norms over time. I/O logic seeks to analyse the sometimes subtle asymmetries between what can be fed into, and what can be output from, other logics, wherever there are inputs that cannot occur as outputs, or vice versa. Normative reasoning is chosen because deontic systems have to encode obligations and permissions which will be manipulated as propositions by the \u2018inner logic\u2019, but whose force is not entirely captured by such manipulation. For example, consider \u2018cottage regulations\u2019 often discussed in deontic logic. When the input says that there is a dog, and the output says that there should be no dog, then there is a violation (which could be sanctioned). Alternatively, if the input says that there is no dog, and the output says again that there should be no dog, then there is a fulfilled obligation (which could be rewarded). Moreover, we may also have according-to-duty relations such that the output says that there should be no fence either when the input says there is no dog, or when the input is just a tautology; and contrary-to-duty (CTD) relations in which case the output may say that it is obligatory that there is a fence when the input says that there is a dog. Examples like these will be discussed later. Simply put, the challenge of norm dynamics is to change such\u2014 already by themselves quite complex and often highly interdependent\u2014relations between input and output when new information becomes available that norms have changed. Interestingly, in neural networks, as in I/O logic, what can be fed into and what can be output from a neural network is strictly defined, in contrast to LP. This will be analyzed in detail. The neural part of the neuralsymbolic approach enables the required form of learning, and learning introduces a dynamics to normative systems that also exercises I/O logic, introducing more kinds of uncertainty differentiated from probability.\nSection 4 then concludes our argument and sketches several directions for future work."}, {"heading": "2 Logic Programming Modelling Reasoning to an Interpretation", "text": "In this section we focus on the applicability and advantages of an LP-based approach for modelling reasoning in interpretatively uncertain situations, i.e. situations in which there is uncertainty concerning the relevance or precise meaning/interpretation of propositions. Section 2.1 introduces the relevant form of LP using three-valued Kleene semantics, and conceptually motivates its use as a modelling tool for this human reasoning. This is followed by a more focused\n8 treatment of this type of LP applied to dealing especially with the uncertain aspects of reasoning in Section 2.2."}, {"heading": "2.1 Logic Programming Modelling Reasoning", "text": "LP is a formal system that has been used extensively to model reasoning in a variety of domains, as widely separated as motor control [52], and imitative learning [68]. Its employment in cognitive modelling grew primarily out of an analysis of discourse processing, in particular of interpretation [67,56,58]. People take in sentences online, in fractions of a second, and effortlessly update their current discourse model, fully indexed for co-references of things, times and events, and their temporal and causal relations. This may require far-flung bits of background knowledge, retrieved from a huge knowledgebase (KB) of semantic memory composed of conditional rules understood as licences for inference. Discourse interpretation involves constructing a preferred or intended model9 of the context. A crucial application of LP is modelling the efficient inferential retrieval from long-term memory of the relevant cues needed to construct or decide on an interpretation of the state of affairs as basis for, e.g., choosing a course of action.\nFollowing Stenning and van Lambalgen [56], we view reasoning to be a twostage process. It starts with reasoning to an interpretation (the computation or retrieval of a meaningful model of the current situation), which may be followed by reasoning from the interpretation. In a typical (i.e. cooperative) conversational context this amounts to computing the model the speaker must have intended to convey. The computation of the intended or preferred model by LP complies with cooperative Gricean principles, but is much more efficient than computing directly with those principles. Imagine you are talking to a friend who is telling you her holiday stories, including an outing into the countryside by car. She says: \u201cAnd then I press the brake! And then . . .\u201d. LP is constructed so that it interprets this utterance online under the assumption that the speaker\u2019s goal is to provide everything the hearer needs to know and nothing more, in order to construct a minimal model of the discourse at each point. At this point that model predicts a continuation of the story consistent with the car slowing down, though this may well be retracted at the next point. It does not consider, without positive evidence, that there might have been ice on the road, or that hers functions differently from all other cars, (or any other defeater). LP with negation-as-failure and Kleene\u2019s strong semantics has shown to be a good candidate for a suitable logic of such cooperative intensional reasoning.\nLP conditionals function as \u2018licenses for inference\u2019, rather than sentences compounded by the \u2018if . . . then\u2019 connective. They can be thought of as highly content-specialised rules of inference which are always applicable when a clause\n9 Intended model is the psychological notion which corresponds to minimal model. Model is to be read as semantic model. Preferred model is the logical notion used by Shoham [53]. Keeping in mind the terms belonging to different fields, we use intended, minimal and preferred as synonyms.\n9 matches them. But it has the important consequence, already mentioned, that\u2014 like natural language conditionals\u2014LP conditionals do not iterate, especially in the antecedent, to produce compound propositions, and do not themselves have truth-values: they are assumed applicable on an occasion unless evidence of exception arises. LP as discussed by Stenning and van Lambalgen [56] and van Lambalgen and Hamm [67] formalises a kind of reasoning which uses closedworld assumptions (CWAs) in order to keep the scope of reasoning to manageable dimensions by entities with limited time, storage, and computational resources, though with very large knowledge bases (KBs)\u2014such as we are. Historically, LP is a computational logic designed and developed for automated planning [30,9] which is intrinsically preoccupied with relevance\u2014making for an important qualitative difference between LP and classical logic (CL). Returning to the type of communicative situation as just introduced in the car example, discourse processing in LP is cyclical. When a new input sentence arrives, its terms are searched for in the KB. The existing minimal model of the discourse to this point is then updated with any new relevant information, according to CWAs, and the cycle repeats. CL, in contrast, assesses the validity of inferences from premises to conclusions with respect to all possible models of the premises, so the question of relevance does not even arise in CL, except outside the logic in the framing of problems. So LP is not just a poor man\u2019s way of doing what can be done better but with more difficulty in CL\u2014the two different types of logic simply serve different incompatible reasoning purposes, and in this sense are incommensurable.\nThe basic format of CWAs is the one for reasoning about abnormalities (CWAab), which prescribes that, if there is no positive information that a certain abnormality-event must occur, it is assumed not to occur.10 These abnormalities are with respect to the regularity expressed by a conditional; for example, ice on the road causing a car not to stop although the brake is pressed, is an abnormality with respect to the default functioning of brakes. Potential abnormalities are included in the LP meaning of the conditional \u2192, hence what is labeled \u2018counterexample\u2019 in CL does not invalidate a conditional inference in LP, it is merely treated as an exception. The conditional has an operational semantics as an operator that modifies truth values of atomic formulas. This is the logical mark of its use as an exception-tolerant \u2018licence for inference\u2019. It is represented as p \u2227 \u00acab\u2192 q; it reads as \u2018If p and nothing abnormal is the case, then q\u2019. The\n10 Abnormality is a technical term for exceptions and should not be taken as having any other overtones. Some terminology: in the conditional p\u2227\u00acab\u2192 q, ab is the schematic abnormality clause. A distinct ab is indexed to each conditional, and stands for a disjunction of a list of defeaters for that conditional; CWAab is the CWA applied to abnormality clauses; \u00ac is the 3-valued Kleene connective, whereas negation-as-failure is an inference pattern that results in negative conclusions by CWA reasoning from absence of positive evidence; falsum (\u22a5) and verum (>) are proposition symbols which always take the values false or true respectively; turnstile (`) and semantic turnstile (|=) are symbols indicating syntactic and semantic consequence respectively.\n10\nantecedent p is called the body of the clause, and the consequent q is its head.11 p and q are composed of atomic formulas, with q restricted to literals (atomic formulas or their negations) and p to conjunctions of literals. The only connectives that may occur in the antecedent are \u2227 and \u00ac.\nSets of such conditional clauses constitute a general logic program P (i.e. a program allowing negation in the antecedents of clauses). P can be understood as a recipe for computing a unique model of the discourse it represents, on the basis of information from background knowledge inferentially selected as relevant. Negation in the antecedent requires utilisation of a three-valued semantics.12 We opt for strong Kleene semantics for LP, where the middle truth value u means \u2018currently indeterminate\u2019; it is not a graded truth akin to a probability\u2014 as in Lukasiewicz three-valued logic\u2014but rather a stage of computation of an algorithm which can evolve towards either 0 or 1. This gives LP the sort of semantic mobility which constitutes a crucial reason for claiming that it can capture the particular kind of nonmonotonic flexibility of reasoning required to deal efficiently with interpretational uncertainty (cf. Section 2.2). Facts q can be represented in logic programs as consequents of tautologies, > \u2192 q (for simplicity we write only q). The CWAab requires that for an initial interpretation at least, abnormalities are left \u2018at the back of the reasoners\u2019 minds\u2019, i.e. outside of the minimal model of P. However conditional clauses have conjoined abnormality conditions of the kind r1 \u2192 ab1, . . . rn \u2192 abn; when evidence of r1, . . . rn becomes available, it activates the correspondent abk. If we take P = {If the brake is pressed, the car will slow down}, the fact \u2018there is ice on the road\u2019 is not represented in its minimal model because this model disregards all potential abnormalities without explicit evidence. The information in a minimal model includes all that is currently known to be relevant and nothing more than that; the only relevant information is explicitly mentioned or derivable. The derivation capacities of LP\u2019s search of its KB are what achieves this computation of relevance. If new input is that there was a storm last night, then this, together with other information in the KB about local meteorology, may yield an inference that there is ice on the road. The input does not mention ice, but ice on the road is a relevant defeater for braking causing slowing, that we assume is already in the list of defeaters for the brakes conditional. Of course, the new information will lead to many other potential inferences during the sweep through the KB, but if they do not connect with anything in the current model, these inferences will not be added because there is no evidence of their relevance. Sometimes such retrieval can involve several conditional links. This is a computationally explicit example of what psychology knows as \u2018spreading activation\u2019 models of memory retrieval, but has been neurally implemented for \u2018propositional LP\u2019 as described\n11 Another remark concerning terminology: while in this context the use of the terms \u2018head\u2019 and \u2018body\u2019 is commonplace in computer science, we will in the following restrict ourselves to \u2018antecedent\u2019 and \u2018consequent\u2019 in order to maintain homogeneity also with terminology in philosophy and logic. 12 Cf. [56, chapter 2] for the justification, or the Appendix in [59] for a more succinct version.\n11\nin [56, chapter 8]. Spreading of activation will also be given a slightly different implementation in the neural networks used later in this paper. Such networks will adopt an answer set semantics [17] allowing the use of default negation for implementing CWA explicitly, and classical (sometimes called explicit) negation, which allows for reasoning as intended here, that is, in the absence of a proof of A and its negation, the truth-value of A is \u2018currently indeterminate\u2019.\nLP models embody much information about stereotypes. For a micro example, it is part of the stereotype of our braking scenario, that the car that is conjured is in motion when the brake is pressed, as developed below. Minimal models are at the intersection between the current input (be that heard language, or observations of any other form) and the reasoner\u2019s background knowledge; they contain all the relevant information and nothing more. In logical terms, they are constructed using the two-step rule of completion (comp):\n1. take the disjunction \u2228 of all antecedents p1, . . . pn with the same consequent q in program P; 2. replace \u2192 with \u2194.13\nThus, if P = {p1 \u2192 q, . . . pn \u2192 q}, comp(P) = {p1 \u2228 . . . pn \u2194 q}. The minimal model of P is in fact a model of comp(P). Such a model does not include information that is either not explicitly mentioned in P, or not derivable from it, and in this sense it is minimal. The CWAab provides the notion of valid inference in LP\u2014an inference is valid if it is truth-preserving with respect to the minimal model of the premises. The contrast with CL is noteworthy: in CL an inference is valid if and only if the conclusion is true in all possible models of the premises. Because of this LP turns out much less computationally complex than CL, and therefore is a promising candidate framework for realistic performance models of fast, implicit, or automatic reasoning (online discourse interpretation being the paradigmatic case).14\nOnce a minimal model is constructed, further reasoning from that model ensues according to the derivation rule of resolution, or the reduction of goals to subgoals by means of backwards chaining.15 The syntactic manifestation of the CWAab is the inferential rule of negation-as-failure, which is applied restrictively to proposition letters which are consequents of falsum, i.e. to q occurring in formulae like \u22a5 \u2192 q. Queries or goal clauses initiate derivations.\n\u201cOperationally, one should think of a query ?q as the assumption of the formula \u00acq, the first step in proving q from P using a reductio ad\n13 Here, \u2194 denotes a classical biconditional in the object language. 14 Basically, in LP queries can be answered in time linear with the length of the shortest\ninferential path in the KB. 15 This direction of reasoning is from effect to cause, from goals to subgoals, or simply\nbackwards in time. The typical backward inferences are modus tollens (p\u2192 q,\u00acq |= \u00acp) and affirmation of the consequent (p \u2192 q, q |= p), initiated by the consequent. The psychological findings that these inferences are more difficult might well be a result of the micro-scale of the tasks being used [55], or of the slightly more complex form of the CWAab needed [56, pp. 176\u2013177].\n12\nabsurdum argument. In other words, one tries to show that P,\u00acq |= \u22a5. [. . . ] one rule suffices for this, a rule which reduces a goal to subgoals.\u201d [67, p.230]\nSuppose one wants to reduce the query ?q with respect to the program P = {p \u2227 \u00acab \u2192 q,\u22a5 \u2192 ab}, which contains no positive information about abnormality. Negation-as-failure allows the reduction of q to p in one step, because ab is a consequence of falsum.\nThe formal parameters of LP, e.g., the semantics based on truth in unique minimal models, the nonmonotonic definition of validity, or the syntactic rules, such as negation-as-failure, recommend it for applications in the modelling of human reasoning. In the first place, individuals\u2019 background KBs are sets of exception-tolerant regularities. Further, LP provides a \u2018direct route\u2019 to (the reasoner\u2019s beliefs about) states of the environment which ground inferences: observations can be looked at as the effects of those (beliefs about) states, represented as literals occurring in the consequents of KB clauses. We saw above that the LP syntactic rule of resolution amounts to stepwise backward reasoning from goals (effects) to subgoals (causes), i.e. from queries to their causes represented in the bodies of conditionals. This matches conceptually with the plan-like psychological structure of reasoning strategies: starting from the goal, i.e. a particular desired state of affairs that calls for action, one attempts to derive a behaviour that presumably leads to that state. Similarly, when a certain state is observed, backwards reasoning derives as its cause the (beliefs about) states which appear in the antecedent of the clause whose consequent it is. This \u2018effect to cause\u2019 inference takes place with respect to the minimal model of P, after the operation of completion has been performed; further, it is made under the auspices of CWAab in the syntactic form of negation-as-failure. Therefore the (beliefs about) states are presumably, to the best of the reasoner\u2019s knowledge, relevant for the current inference\u2014they are the most plausible conditions for the observed effect to occur.\nTying back into our overarching theme for this article, it should be noted that LP has some close correspondences with probabilistic models. At least in causal cases, LP produces the structural features of the models which are shared with the structural features of causal Bayes Nets [46,48]. Nevertheless, the computational properties are highly distinct [4,57]. Probability cannot be the basis for what LP does in discourse processing. Discourse processing at a semantic level requires extremely fast reasoning about novel arrangements of properties and goals, currently unknown by the reasoner to be relevant to the discourse, in order to identify the propositions underlying the discourse, and this without knowledge of the distributions required for probability models. Probability has the general computational feature that the heavy computation of critical probabilities has to be done when the information defining the problem is complete (i.e. the relevant propositions identified and refined), and this is not viable in on-line discourse processing. Foreshadowing the discussion in the following subsection, the relevant type of uncertainty here is uncertainty of interpretation, not uncertainty about truth values or their probabilities. We strongly suggest\n13\nthat this state of affairs is not limited to discourse processing, but continues into many of the situations where people must interpret novel information."}, {"heading": "2.2 The Many Logical Faces of Uncertainty, or What Logic Programming Can Do for Reasoning", "text": "We have introduced LP modelling of discourse interpretation in some detail because it is perhaps the extreme example of reasoning to interpretation. Once this contrast with probability models is established, it enables us to understand its kind of uncertainty as qualitatively distinct from probability. For this we resort to comparison of the logical systems that define them. In contrast to most of the proposals of kinds of uncertainty listed in the introduction, we see the necessary focus as on differences in the epistemic relations of the user of a logic to the propositions expressed, rather than in the content of the propositions themselves. To illustrate with examples of the version of LP just described, a speaker and a hearer are modelled as cooperating for the speaker to communicate to the hearer her intended preferred model by uttering a sequence of sentences. As is typical in cooperative action, this is a non-zero sum game. If the hearer doesn\u2019t get the model, then the pair have failed (blame is another matter irrelevant here). Once this cooperative nature is captured in the semantics, it becomes evident that there is a concomitant kind of necessity: one might call it communicative necessity. If the speaker says \u201dOnce upon a time there was a cat and a dog\u201d the hearer can conclude with certainty that the intended model contains two distinct animals at this point. This is not certainty about the real world (whatever that is). It is certainty about the expressed intended model, where the relation of that model to the world is for the time being suspended. However complex the gyrations in the continuation are that perhaps reveal that the cat was actually mistaken for the very dog, so that the model communicated then has only one animal in it, the initial proposition that there are two animals is \u2018communicatively necessary\u2019 in this logic of interpretation. What is certain is the interpretative facts (as long as the discourse is clear, which in turn depends whether the speaker and hearer\u2019s KBs are well aligned in their relevant parts): not any existing situation in the world. Fictional examples are extreme and therefore helpful, but in fact much of our communication starts out nearer to this kind of process than to the construction of a state of affairs that the hearer already knows how to anchor to the world. With \u2018true\u2019 stories, we also do not know what is going to happen, or even who is going to turn up, until they are done. Once the appropriate kind of necessity is grasped, it is easier to see the kind of uncertainty that is involved. The hearer is uncertain about what information will arrive, and what updating of the model will therefore be required to form the propositions that will be in the final model.\nDifferent disciplines over the last half century or so have contributed to a greater understanding of the range of logical systems for treating uncertainty. To see the importance of a qualitative classification of kinds of uncertainty, consider CL as distinct logic also exhibiting a characteristic kind of uncertainty while\n14\nreasoning toward a proof of a conjecture.16 This is reasoning in uncertainty about whether the conjecture is a theorem, and it cannot be measured by probability. A proof dispels this uncertainty with a positive answer, while a counterexample resolves it with a negative answer. For another example, deontic logic defines reasoning in moral uncertainty toward moral necessity. Each logic has its own kind of uncertainty. If there were no uncertainty, the entire motivation behind reasoning would be more than questionable. Also\u2014coming back to the remark concerning the kinds of uncertainty treated by a logic being twinned with the logic\u2019s kind of necessity\u2014the CL example can serve as prime example: CL\u2019s kind of necessity is truth in all models of the premisses, which is distinctively not explicable in terms of probability.\nFirst, a general characterisation of the problem of distinguishing kinds of uncertainty, before returning to the example. Any kind of uncertainty is a three-way relation between a person, their epistemic state, and a proposition. A probability model is one kind of specification of an epistemic state, which assigns probabilities to component propositions. This is not a point about the subjectivity or otherwise of probability. However objective the probability of an event may or may not be, it also depends on the epistemic state of the assigner\u2014what they know or believe that is expressed in the relevant probability model. This epistemic state is also objective. Epistemic agents with different knowledge and belief about the same objective events will generally rationally assign them different probabilities on the basis of different models. Think of the players in a card game who know their own but only their own hand: each has a different probability model assigning different probabilities, to say, the next card played.\nIf the epistemic state of an agent does not permit them to specify a probability model by identifying the propositions relevant to their epistemic state, then their uncertainty will be of a different kind than probability. The authors cited in Section 1 as advocating varieties of uncertainty\u2014with the exception of Mousavi and Gigerenzer [40]\u2014look to identify them through probability models. For example, take the most elaborated classification by Bradley and Drechsler [8]. It starts out promisingly qualitative, as demonstrated by the quote above. But the authors still end up with probabilities. The authors\u2019 argument is only that they cannot be assigned a single probability function. But they do not consider the variety of logics that are required to characterise the epistemic states that constitute the uncertainties. It is the characterisation of epistemic states that requires the different logics; not particularly the nature of the \u2018output proposition\u2019 to which the uncertainty is assigned.\n16 Nota bene: This stands in harsh contrast to Oaksford\u2019s and Chater\u2019s [45] above quoted conventional characterisation of CL as \u2018reasoning in certainty\u2019. When this way of conceptualising monotonic CL in contrast to nonmonotonic logics was introduced in the mid-1970s and 1980s\u2014for instance, in the wake of Minsky\u2019s frames [39] or McCarthy\u2019s circumscription approach [38]\u2014the underlying concern was not with characterising kinds of uncertainty, but with contrasting two systems on the one specific property of (non)monotonicity.\n15\nAgain using discourse processing as example, LP describes the sequence of epistemic states the hearer goes through as they interpret a discourse. In this view, LP\u2014as candidate for a logic underlying cooperative discourse semantics\u2014 specifies a process which takes in new information about the current context of reasoning, and interprets it in the light of background KB of regularities into a unique minimal model which identifies the relevant propositions. Consider the case of a discourse-initial: \u201cMax fell. John pushed him\u201d. First it is crucial to see that the discourse model we construct is more than the set of sentences. The two sentences \u201cMax fell\u201d and \u201cJohn pushed him\u201d are logically unrelated, but the model we derive (perhaps unwittingly) is extensively augmented with new information. For example, the link attributed is causal\u2014Max\u2019s falling follows John\u2019s pushing in time, as its effect\u2014and there is a spatial contact between Max and John. And the model specifies that Max is not John. And so on. This extra material is derived based on our general knowledge about humanon-human pushings and fallings. The example is chosen because here the KB actually induces an interpretation where the sequence of events departs from the sequence of narration, making the reasoning more prominent. Of course, it quickly becomes obvious that here context is everything. If Max and John were fish in water, we would struggle to interpret, because falling in water is hard for a fish, and not the typical result of pushing. If, on the other hand, the next clause were to be a continuation of the second sentence: \u201c[. . . ], or what was left of him after hitting the ground, over the cliff.\u201d, we would have to revise the model in the light of the new information. Now, the pushing is subsequent to the falling (whose causation is unknown), and there is a cliff in the model, near to the protagonists. The uncertainty resides in the fact that we do not know what information is going to be involved; and this information is not available until we have integrated the current input and our background knowledge into the developing model, resolving temporal, causal and referential relations, among others. We will not present an LP formalisation here (cf. [67, p. 131]): our purpose is to point to the omnipresence of such inference, and the richness of the general knowledge that has to be found and applied, generally at speed, and without awareness. Electrophysiological observations on analogous materials strongly corroborate these inferences\u2019 occurrence [47].\nWhat is also important is that\u2014as already stated above\u2014interpretative inferences define their own kind of necessity, namely communicative necessity. They entail nothing about how the world has to be, but they do entail what we have to take as the speaker\u2019s intended model of her discourse. This does not mean we have to take it as truth: we may even have good reason to believe it is part of a deliberate deception. But if we need to understand that deception, we must first understand the intended model to see what pack of lies is being offered, i.e. what has been communicated. The general point is that we must construct our interlocutors\u2019 intended models if we are to communicate.\nBut now, returning to our overarching question, how does this relate to the types of uncertainty probability theory can and cannot cover? As we engage in this little cliff top drama, we are uncertain at every new discourse addition what\n16\nmeanings are involved. Until each new sentence has been successfully incorporated into the model we do not know which sense it expresses. But the arrival of new propositions is just the tip of the iceberg. We also do not know its effect on the other bits of meanings that were there before. They may have disappeared completely, and they probably have changed, if only by the relations that are now determined between the old and the new. In contrast, a probability model has to be founded on a set of propositions where such relations are explicit. So at best, our discourse could invoke a sequence of probability models, one at each step. But there need not be any systematic relation between one and the next. The only one likely to be of much interest as the foundation of a probability model is the \u2018final\u2019 one (cf. [57,4]). And if one wanted a probability model of this last one, then the corresponding LP model provides exactly the common structural core on which the necessary probability information would have to be hung, perhaps even provided by estimates from the conditional frequencies available in the LP net involved (cf. [59]). In other words, while probability theory does not deal with the dynamics of uncertainty about interpretation, LP can serve as a modelling approach for this crucial part of human reasoning (remember the 3-valued Kleene semantics we use for LP, in which the u truth value can evolve during computations towards 1 or 0).\nSo discourse processing provides our first example of a prevalent cognitive process which deals in uncertainties and certainties of a kind not treated by probability. And LP provides an alternative logic for this examples\u2019 analysis."}, {"heading": "3 Neural-Symbolic Computing Modelling Dynamic Normative Contexts", "text": "Following our presentation of LP as a promising approach to modelling reasoning to an interpretation (and resolving the associated communicative uncertainty), in this section\u2014building upon and expanding the ideas from Section 2 conceptually as well as formally\u2014we focus on normative reasoning and the associated form of uncertainty resulting from dynamic changes or expansions of norms. Regarding aspects of uncertainty, norms and norm-based reasoning pose several challenges: among others they tend to be highly sensitive to the context the reasoner finds herself in and her interpretation thereof (e.g., when deciding which norms apply, or\u2014if several options could be chosen from\u2014which would be preferred options, either due to the resulting actions or to abstract value-related considerations), and usually are subject to change over time (e.g., when existing norms are altered in content or interpretation, or new norms are introduced). These properties establish a natural connection to the virtues of LP-based models of reasoning described above: LP\u2019s construction of preferred models can be seen as construction of contexts. Still, while in the previous section we were mostly focused on the application of LP in modelling human reasoning, we now shift emphasis to a more AI-based perspective, considering reasoning in intelligent agents in general. In artificial social systems, norms serve as mechanisms to effectively deal with coordination in multi-agent systems (MAS). Among the\n17\nopen problems relating to the use of norms in these systems is how to equip agents to deal effectively with norms that change over time [5], either due to the introduction of new norms, due to explicit changes made by legislators to already existing norms, or due to different interpretations of the law by judges, referees, and other judicial bodies.17\nIn trying to tackle the difficulties arising from the dynamic nature of norms, we combine I/O logic [37,34,35] with neural-symbolic computation [12] in order to propose a formal framework for reasoning and learning about norms in a dynamic environment. I/O logic is a symbolic formalism\u2014in several ways closely related to LP as will become apparent below\u2014used to represent and reason about norms, providing reasoning mechanisms to produce outputs from the inputs, each of them bearing a specific set of features. The neural-symbolic paradigm of Garcez et al. [12] on the other hand embeds symbolic logic, and in particular LP into ANNs. Neural-symbolic systems provide translation algorithms from symbolic logic to ANNs and vice-versa: the resulting network is used for robust learning and efficient computation within a connectionist framework, while the logic provides background knowledge to help learning, as the logic is translated into the ANN, and high-level explanations for the network models, when the trained ANN is translated into logic. The combination of logic and networks is achieved by representing the I/O logic within the computational model of ANNs, leveraging a similarity between I/O logic and ANNs: both have separate specifications of inputs and outputs. We exploit this analogy to encode symbolic knowledge expressed as I/O logic rules into a standard ANN, and use the resulting ANN to learn new norms in a dynamic environment. Thus, two main steps have to be achieved, namely the translation of I/O logic rules into ANNs, and the evaluation of the ANN learning mechanism at refining normative rules in time.\nWith the exception of game-theoretic approaches (cf., e.g., [51,7,54]), few machine learning techniques have been applied to tackle open problems like revising and learning new norms in open and dynamic environments. We show how to use ANNs to cope with some of the underpinnings of normative reasoning\u2014 namely permissions, CTDs and exceptions\u2014by using the concept of priorities between I/O (or LP) rules, i.e. LP rules with metalevel priorities [2]. Thus, the contribution here is in allowing the handling of the uncertainty associated with norm changes by combining symbolic and sub-symbolic representations to provide a flexible and effective methodology for learning, normative reasoning, and specification in MAS. After a short introduction to neural-symbolic integration and the corresponding conceptual and architectural paradigm in Section 3.1, I/O logic is formally introduced in Section 3.2, explaining abstract normative systems, propositional I/O logic, and the notion of permissions in the corresponding normative framework. Section 3.3 then gives an overview of the neural-symbolic architecture implementing I/O logic, before Section 3.4 finally presents the resulting system for normative connectionist learning and LP.\n17 Terminology yet again: for the purpose of this article we use law, norm, rule, etc. as synonymous.\n18"}, {"heading": "3.1 Neural-Symbolic Systems", "text": "The main purpose of neural-symbolic integration is to bridge the gap between symbolic and sub-symbolic representations. To this end, neural-symbolic systems bring together connectionist networks and symbolic knowledge representation and reasoning [13]. In this way, neural-symbolic systems seek to take advantage of the strengths of each approach whilst hopefully avoiding their drawbacks. For our current purposes, we are particularly interested in three consecutive steps: representing the norms governing a normative system formally and soundly in an ANN, using the network to achieve efficient parallel computation, and finally exploiting the instance learning capacities of ANNs to adapt the norms in the system through learning. This should give rise to a normative system capable of integrating reasoning and learning capacities in an effective way. In what follows, we introduce the basic concepts of ANNs and neural-symbolic systems used in this article, with an emphasis on an extension of the connectionist inductive learning and logic programming (CILP) system by Garcez et al. [12].\nAn ANN is a directed graph with the following structure: a unit (or neurone) in the graph is characterised, at time t, by its input vector Ii(t), its input potential Ui(t), its activation state Ai(t), and its output Oi(t). The units of the network are interconnected via a set of directed and weighted connections such that if there is a connection from unit i to unit j then Wji \u2208 R denotes the weight of this connection. The input potential of neurone i at time t (Ui(t)) is obtained by computing a weighted sum for neurone i such that Ui(t) = \u2211 jWijIi(t) (see Figure 1). The activation state Ai(t) of neurone i at time t\u2014a bounded real or integer number\u2014is then given by the neuron\u2019s activation function hi such that Ai(t) = hi(Ui(t)). Typically, hi is either a linear function, a non-linear (step) function, or a sigmoid function (e.g.: tanh(x)). In addition, \u03b8i (an extra weight with input always fixed at 1) is known as the threshold of neurone i. We say that neurone i is active at time t if Ai(t) > \u03b8i. Finally, the neurone\u2019s output value Oi(t) is given by its output function fi(Ai(t)). Usually, fi is the identity function.\n19\nThe units of an ANN can be organised in layers. A n-layer feedforward network is an acyclic graph. It consists of a sequence of layers and connections between successive layers, containing one input layer, n \u2212 2 hidden layers, and one output layer, where n \u2265 2. When n = 3, we say that the network is a single hidden layer network. When each unit occurring in the i-th layer is connected to each unit occurring in the i + 1-st layer, we say that the network is fully-connected.\nA multilayer feedforward network computes a function \u03d5 : Rr \u2192 Rs, where r and s are the number of units occurring, respectively, in the input and output layers of the network. In the case of single hidden layer networks, the computation of \u03d5 occurs as follows: at time t1, the input vector is presented to the input layer. At time t2, the input vector is propagated through to the hidden layer, and the units in the hidden layer update their input potential and activation state. At time t3, the hidden layer activation state is propagated to the output layer, and the units in the output layer update their input potential and activation state. At time t4, the output vector is read off the output layer. In addition, most neural models have a learning rule, responsible for changing the weights of the network progressively so that it learns to approximate \u03d5 given a number of training examples (input vectors and their respective target output vectors).\nIn the case of backpropagation\u2014probably the most commonly applied neural learning algorithm [50]\u2014an error is calculated as the difference between the network\u2019s actual output vector and the target vector, for each input vector in the set of examples. This error E is then propagated back through the network, and used to calculate the variation of the weights 4W. This calculation is such that the weights vary according to the gradient of the error, i.e. 4W = \u2212\u03b7\u2207E, where 0 < \u03b7 < 1 is called the learning rate. The process is repeated a number of times in an attempt to minimise the error, and thus approximate the network\u2019s actual output to the target output, for each example. In order to try and avoid shallow local minima in the error surface, a common extension of the learning algorithm above takes into account, at any time t, not only the gradient of the error function, but also the variation of the weights at time t \u2212 1, so that 4Wt = \u2212\u03b7\u2207E + \u00b54Wt\u22121, where 0 < \u00b5 < 1 is called the term of momentum. Typically, a subset of the set of examples available for training is left out of the learning process so that it can be used for checking the network\u2019s generalisation ability, i.e. its ability to respond well to examples not seen during training.\nCILP now is a neural-symbolic system based on an ANN that integrates inductive learning and deductive reasoning. In CILP, a translation algorithm maps a logic program P into a single hidden layer ANN N such that N computes the least fixed-point of P [33]. This provides a massively parallel model for computing the stable model semantics of P [16]. In addition, N can be trained with examples using a neural learning algorithm, having P as background knowledge. The knowledge acquired by training can then be extracted [11], closing the learning cycle, as advocated by [64].\nLet us exemplify how CILP\u2019s translation algorithm works. Each rule (rl) of P is mapped from the input layer to the output layer of N through one neurone\n20\n(Nl) in the single hidden layer of N . Intuitively, the translation algorithm from P to N has to implement the following conditions: (c1) the input potential of a hidden neurone Nl can only exceed its threshold \u03b8l, activating Nl, when all the positive antecedents of rl are assigned truth-value true while all the negative antecedents of rl are assigned false; and (c2) the input potential of an output neurone A can only exceed its threshold (\u03b8A), activating A, when at least one hidden neurone Nl that is connected to A is activated.\nExample 1. (CILP) Consider the logic program P = {B\u2227C\u2227 \u223c D \u2192 A,E\u2227F \u2192 A,B}, where \u223c stands for LP\u2019s negation by failure (a.k.a. default negation) [33]. Given P, the CILP translation algorithm produces the network N of Figure 2, setting weights (W ) and thresholds (\u03b8) in a way that conditions (c1) and (c2) above are satisfied. Note that, if N ought to be fully-connected, any other link (not shown in Figure 2) should receive weight zero initially. Each input and output neurone of N is associated with an atom of P. As a result, each input and output vector of N can be associated with an interpretation for P. Note also that each hidden neurone Nl corresponds to a rule rl of P such that neurone N1 will be activated if neurones B and C are activated while neurone D is not; output neurone A will be activated if either N1 or N2 is activated; and output neurone B will be activated if N3 is, while N3 is always activated regardless of the input vector (i.e. B is a fact). To compute the stable models of P, the output vector is recursively given as the next input to the network such that N is used as a recursive network to iterate the fixed-point operator of P as suggested by Garcez et al. [12]. For example, output neurone B should feed input neurone B. N will eventually converge to a stable state which is identical to the stable model of P provided that P is an acceptable program [3]. For example, given any initial activation in the input layer of Nr (i.e. the network of Figure 2 recurrently connected), it always converges to a stable state in which neurone B is activated and all the other neurones are not. We associate this with literal B being assigned truth-value true, while all the other literals are assigned truth-value false, which represents the unique fixed-point of P.\nCILP thereby provides a (provably sound) translation from a symbolic representation into an ANN that can be trained with examples as part of a knowledge evolution process, whereby the original symbolic representation is seen as background knowledge to the network. In what follows, we extend CILP to handle a range of normative rules and prove soundness. Notice how in the standard CILP translation, the weights of the connections linking the hidden and output layers of the network are always positive. As will become clearer in what follows, normative rules require the use of negative weights from the hidden to the output layer of the CILP network as well. This implements priorities in the rules [12] and is responsible for adding alternative paths that enable robustness in the networks also. As we study such different forms of representation in different applications, such as CTD, we are interested in proving soundness, but also in efficient computation and learning, as exemplified later in this article.\n21"}, {"heading": "3.2 Input/Output Logic", "text": "As explained by Makinson and van der Torre [36], I/O logic takes its origin in the study of conditional norms which, either in imperative or indicative form, express obligations under some legal, moral, or practical code, goals, contingency plans, advice, etc. Putting this overall notion in formal terms, Makinson and van der Torre [37] represent rules by ordered pairs (a, x), where the antecedent a is thought of as an input, representing some condition or situation, and the consequent x is thought of as an output, representing what the rule tells us to be desirable, obligatory, or whatever else in that situation.\nConcerning the overall motivation behind the development of I/O logic, in philosophy\u2014but also significant in our current context\u2014norms are commonly distinguished from declarative statements. The latter may bear truth-values, while describing norms as true or false is meaningless. Instead, norms may be respected (or not), can be in force in the current context (or not), or can be assessed from the standpoint of other norms (e.g., when judging a law from a moral point of view). Still, much work addressing deontic formalisms in the study of logic and AI seem to ignore this distinction: most presentations of deontic logic\u2014 whether axiomatic or semantic\u2014treat norms as if they could be subjected to an assessment in terms of truth-values. In particular, the truth-functional connectives \u2018and\u2019, \u2018or\u2019, and \u2018not\u2019 are routinely applied to norms, forming compound norms out of elementary ones. Semantic constructions using possible worlds go further by offering rules to determine, in a model, the truth-value of a norm. I/O logic has its source in precisely this tension between philosophy and studies in formal logic (the reader may identify a similar tension between human reasoning and formal classical logic, as discussed earlier in the case of discourse processing).\nIn the following, we first present abstract normative systems as a general descriptive framework for formal approaches to normative reasoning and basis\n22\nfor the subsequent introduction of propositional I/O logic, both of which then are applied to modelling the three types of permissions commonly encountered in normative context.\nAbstract Normative Systems Modal logic has been the standard for normative reasoning ever since von Wright [70]. Still, for instance in Gabbay et al.\u2019s \u201cHandbook of Deontic Logic and Normative Systems\u201d [10], the classical modal logic framework is mainly confined to the historical chapter. Another chapter presents the alternatives to the modal framework, and three chapters discuss concrete approaches, namely I/O logic, the imperativist approach [22], and the algebraic conceptual implication structures [32].18 Against this multitude of approaches as backdrop, Tosatto et al. [63] proposed abstract normative systems as common framework for comparing and analysing these new proposals.\nAbstract normative systems study frameworks such as I/O logic on a general level, to which Tosatto et al. [63] add two notions. First, each element in the (finite) universe comes with its \u201canti-element\u201d: this is the minimal extension to represent violations, namely elements in the input whose anti-element is in the output. Second, there is an element in the universe called >, contained in every context.\nDefinition 1 (Universe L [63]). Given a finite set of atomic elements E, the universe L is E \u222a {\u223ce | e \u2208 E} \u222a {>}. For e \u2208 E, let a =\u223ce iff a = e, a = e iff a =\u223ce, and undefined iff a = >.\nAn abstract normative system is a directed graph, and a context is a set of nodes of the graph containing >. In abstract normative systems there are three kinds of relations, for the regulative, permissive, and constitutive norms, respectively. We start with the regulative norms only. The edges in an abstract normative system exactly define what a \u201cconditional norm\u201d (with respect to this abstract normative system) is.\nDefinition 2 (ANS \u3008L,N\u3009 [63]). An abstract normative system ANS is a pair \u3008L,N\u3009 with N \u2286 L\u00d7 L a set of pairs of the universe, called conditional norms, and A \u2286 L a subset of the universe such that > \u2208 A, called the context.\nIn a context, an abstract normative system generates or produces an obligation set, a subset of the universe, reflecting the obligatory elements of the universe. The class of deontic operations is specified by their domain and codomain. Some examples of deontic operations are given below.\nDefinition 3 (Deontic operation \u00a9 [63]). A deontic operation \u00a9 is a function from an abstract normative system \u3008L,N\u3009 and a context A to a subset of the universe \u00a9(\u3008L,N\u3009, A) \u2286 L. Since L is always clear from context, we write \u00a9(N,A) for \u00a9(\u3008L,N\u3009, A). 18 Of course this list is non-exhaustive as there are further alternative candidates for a\nnew standard, such as nonmonotonic logic [24] or deontic update semantics [62].\n23\nSimple-minded output or \u00a91 is Makinson and van der Torre\u2019s minimal system. Basic output or \u00a92 allows for reasoning by cases, which now means that if something is obligatory in the context of a and its complement a, then it is obligatory also in the minimal context. Reusable output or \u00a93 allows for deontic detachment, which now corresponds to iteration of the rules. Throughput or \u00a9+i allows for identity. All possible combinations lead to eight input/output operations.\nDefinition 4 (Eight deontic operations [63]). A context A \u2286 L is complete if for all e \u2208 E, it contains either e or e (or both).\n\u00a91(N,A) = N(A) = {x | (a, x) \u2208 N for some a \u2208 A} \u00a92(N,A) = \u2229{N(V ) | A \u2286 V, V complete} \u00a93(N,A) = \u2229{N(B) | A \u2286 B \u2287 N(B)} \u00a94(N,A) = \u2229{N(V ) | A \u2286 V \u2287 N(V ), V complete} \u00a9+i (N,A) =\u00a9i(N \u222a {(a, a) | a \u2208 L}, A)\nEquivalently, \u00a93(N,A) can be defined as N(B) where B is the smallest set containing A and closed under N , i.e. A \u2286 B \u2287 N(B). Moreover, to emphasise symmetry, \u00a91(N,A) can be defined equivalently as \u2229{N(B)|A \u2286 B}.\nAt least since the work of Horty [24], nonmonotonic techniques have been used to deal with reasoning in the context of dilemmas, CTD reasoning, and defeasible norms:\n\u2013 Dilemmas are two (or more) obligations with contradictory content, like the obligation for a and the obligation for a. \u2013 CTD or secondary obligations (a, x) are in force only in case of violation of a primary obligation, e.g., generated using (>, a).19 \u2013 Defeasible deontic logic is concerned with violations and exceptions [60,44].\nPropositional Input/Output Logic As explained by Makinson and van der Torre[37], propositional I/O logic establishes a relatively simple setting, abstracting from important aspects of deontic reasoning, such as CTD reasoning or permissions.20 The construction of the semantics is analogous to the just discussed abstract normative systems, adding the closure of input and output under propositional consequence. As before, N(A) = {x | (a, x) \u2208 N for some a \u2208 A}. 19 To give an intuitive example of a CTD, we report the so-called dog-sign example by\nPrakken and Sergot [49] already hinted at in the introduction: \u201cSuppose that: there must be no dog around the house, and if there is no dog, there must be no warning sign, but if there is a dog, there must be a warning sign.\u201d Obviously, if there is a dog, the conditional obligation that there must be no sign does not become unconditional, since its condition is not fulfilled. On the other hand, it can also be inferred that if no obligations are violated, there will be no sign (modulo exceptions, of course). 20 These require much more involved I/O operations, which we shortly discuss in Section 3.3 below. Cf. the work by Makinson and van der Torre [?] for more detailed treatments.\n24\nDefinition 5 (out [37]). Let L be a propositional logic with Cn the consequence operator of L, > a tautology of L, a complete set one that is either maxiconsistent or equal to L, and let N be a set of ordered pairs of L (called the generators). A generator (a, x) is read as \u2018if input a then output x\u2019. The following logical systems are defined: out1(N,A) = Cn(N(Cn(A)) out2(N,A) = \u2229{Cn(N(V )) : A \u2286 V, V complete} out3(N,A) = \u2229{Cn(N(B)) : A \u2286 B = Cn(B) \u2287 N(B)} out4(N,A) = \u2229{Cn(N(V )) : A \u2286 V \u2287 N(V ), V complete}\nNote that neither in the I/O logic framework, nor in the abstract normative systems framework, does a normative system \u2018imply\u2019 a norm. Norms are used to generate obligation sets; we can axiomatise deontic operations using a proof system based on conditionals, but this does not mean that norms are \u201cimplied\u201d or \u201cderived.\u201d The most we can say is that a norm is \u201caccepted\u201d by a normative system [62], or \u201credundant\u201d in a normative system [61]. The latter point may be related to two philosophical considerations of the I/O logic framework. First, as already explained above, the framework is based on the idea that norms do not have truth values, known as Jo\u0308rgensen\u2019s dilemma in the deontic logic literature [25]. Second, the role of logic is not to create or determine a distinguished set of norms, but rather to prepare information before it goes in as input to such a normative code, to unpack output as it emerges and, if needed, coordinate the two in certain ways. A set of conditional norms is thus seen as a transformation device, and the task of logic is to act as its \u201csecretarial assistant\u201d [37]."}, {"heading": "3.3 Overview of the Architecture", "text": "Our goal is to allow the agent to learn about norms and their interpretation from experience, and to take decisions which respect the norms she is subject to at the respective point in time. Thus, the agent needs to know what is obligatory and forbidden according to norms (conditional rules) in any situation in real time: what is obligatory can eventually become an action of the agent, while what is forbidden inhibits such actions. Also, rules may change as the normative environment changes over time. The agent should be flexible enough to adapt her behaviour to the context using as information the instances of behaviours which have been considered illegal.\nTo allow an intelligent agent to have a internal representation of a normative code, we follow the process visualised in Figure 3. The encoding process is a single and unique task, and we just decompose it in subtasks to give a more detailed explanation. The first step involves encoding a list of normative aspects in terms of priorities and will be described in the next subsection. The second step translates a normative code in I/O logic into an extended logic program (i.e. LP extended with classical negation, a.k.a. explicit negation, which leads to the answer set semantics of LP mentioned earlier in the context of three-valued logics. The third step applies a translation algorithm to convert the logic program\n25\ninto a neural network. The last two steps will be analysed in detail in Section 3.4.\nFigure 4 describes our approach from a more abstract perspective. Note that the encoding of a normative code in an ANN is lumped to a single step. Our framework starts from the symbolic KB of norms contained in the agent, transforming it into an ANN using the encoding introduced in Figure 3 and described below. The ANN is structured as follows: input neurones of the network represent the state of the world, while the output neurones represent the obligations of the agent, or the prohibitions. The ANN is used as part of the controller for the agent and, given its ability to change (i.e. learn from examples), it is expected to give the agent the required flexibility.\nNormative Problems as Priorities Recall, as discussed, that normative reasoning requires agents to deal with specific problems such as dilemmas, exceptions, and CTDs. In what follows, a norm N will be expressed as labelled generators N = (I,O), read \u2018if input I then output O\u2019. In general, I in (I,O) is any propositional formulae, which will be restricted later to conjunctions of literals.\nDilemmas: two obligations are said to be contradictory when they cannot be accomplished together. A possible example of contradictory norms is the dilemma. This usually happens when an agent is subject to different normative codes (i.e. when an agent has to follow the moral and the legal code). How to\n26\novercome dilemmas is left as future work, as we are focusing on how to use priorities to regulate exceptions and CTDs.\nPriorities are used to give a partial ordering between norms. This is useful when, given two applicable norms, we always want one to preempt the other, for instance when dealing with exceptions. We encode priorities among the norms by using negation by failure (\u223c). Given two norms N1 = (A1 \u2227 A3, \u03b21) and N2 = (A2 \u2227 A3, \u03b22) and a priority relation N1 N2 between the norms (such that the first norm has priority), we encode the priority relation by modifying the antecedent of the norm with lower priority. Specifically, we include in the antecedent of the norm with the lower priority the negation-as-failure of the literals in the antecedent of the higher priority norm that does not appear in the antecedent of the lower priority norm. We do so in order to ensure that, in a situation where both (unmodified) norms would be applicable, the newly inserted negation-as-failure atoms in the antecedent of the modified lower-priority norm evaluate to false and make the norm not applicable. Considering for example the two norms given above, we have to modify N2. The only atom appearing in N1\u2019s input and not in N2\u2019s input is A1, and therefore we introduce \u223c A1 as a conjunct in N2\u2019s input. After embedding the priority, the second norm becomes N \u20322 = (A2\u2227 \u223c A1 \u2227A3, \u03b22). Note that in a potentially conflicting situation when A1, A2 and A3 hold, N1 and N2 are applicable, but N \u2032 2 is not, thus avoiding the conflict.\nExceptions occur when, due to particular circumstances, a norm should be followed instead of another. Suppose that a norm N3 = (\u03b1, \u03b2) should be applied in all the situations containing \u03b1. For exceptional situations we consider an additional norm N4 = (\u03b1 \u2227 \u03b3,\u00ac\u03b2). The latter norm should be applied in a subset of situations w.r.t. N3: specifically all those when, in addition to \u03b1, also \u03b3 holds. We can call situations where both \u03b1 and \u03b3 hold exceptional situations. In these exceptional situations both norms could be applied. This would produce two contrasting obligations: \u03b2 and \u00ac\u03b2. To avoid this we add the following priority relation: N4 N3. Therefore we modify the input of the norm with lower priority as described earlier. The result is a new norm N \u20323 = (\u03b1\u2227 \u223c \u03b3, \u03b2), that would not be applied in the exceptional situations, avoiding the problem of contrasting obligations.\nCTDs: An important property of norms is that they are soft constraints and, accordingly, can be violated. CTDs provide additional obligations to be fulfilled when a violation occurs. For example, consider a norm N5 = (\u03b1, \u03b2) that should be applied in all situations containing \u03b1 and producing the obligation \u03b2. As mentioned, norms can be violated, therefore we can also define a norm that produces alternative obligations to be followed in case of a violation. Let this new norm be N6 = (\u03b1 \u2227 \u00ac\u03b2, \u03b3). The latter norm contains in its input both the input of N5 and the negation of its output. In this way it describes which should be the alternative obligation to \u03b2 in the case that it cannot be achieved,\n27\nin this example \u03b3. We use a priority relation between the two norms in order to avoid the generation of the obligation \u03b2 in case it is already known that it is not satisfiable. We add then the following priority relation N6 N5 that modifies the first norm as follows: N \u20325 = (\u03b1\u2227 \u223c \u00ac\u03b2, \u03b2).\nPermissions: An important distinction between obligations and permissions is that the latter will not be explicitly encoded in the ANN. In our approach we consider that something is permitted to the agent if not explicitly forbidden (note that we consider the ought of a negative literal as a prohibition). Due to this, we assume that norms with a permission in their output implicitly have priority over the norms that forbid the same course of action.21 For example, using P in the output of a norm to denote a permission, consider two norms N7 = (A1, P (\u03b21)), N8 = (A2,\u00ac\u03b21). The first norm permits \u03b21 and the second forbids it. In this case, we use the following priority relation: N7 N8."}, {"heading": "3.4 Normative Connectionist Inductive Learning and Logic Programming", "text": "In this section we introduce a new approach for coding a fragment of I/O logic which corresponds to extended LP into ANNs. The main intuition is that, although logic programs in general do not explicitly capture the concepts of inputs and outputs, a neural-symbolic system based on extended logic programming does - on a purely structural level: inputs and outputs in I/O logic correspond to the input and output layers of the ANN - and allows the representation of norms in ANNs.\nAs described above, in I/O logic norms are represented as ordered pairs of formulas like (\u03b1, \u03b2). A peculiarity of I/O logic is that it does not have (\u03b1, \u03b1) for any \u03b1 (i.e. identity is not an axiom). In normative reasoning, the input does not necessarily become an output: the reason is that the output is interpreted as what is obligatory, thus, just because a is in the input, it is not necessarily the case that a is obligatory as well. This I/O perspective corresponds straightforwardly to the general intuition behind an ANN. Activating input neurone A in Figure 2 does not necessarily activate output neurone A also; this is true for any neurone, and it allows a subtle but important distinction between the activation of an input neurone which is derived from the context, that is, the input values provided to the network, and the activation of an output neurone, which is derived from the KB. For example, in Figure 2, the truth-value of B in the input is, at first, obtained from the input to the network (its context), whilst the truth-value of B in the output is true (B is a fact in the KB). Modifying the original CILP algorithm, we first translate I/O logic into an extended logic program to be\n21 Makinson and van der Torre [35] consider three kinds of permissive norms, namely negative, positive, and static positive permission. In this article, we restrict discussions to the above, and should note that much future work is left to be done when it comes to the provision of connectionist representations for normative and deontic reasoning systems\n28\nprocessed by CILP without requiring inputs to be always translated into outputs as well, so that the ANN is allowed different input and output layers. The input \u03b1 of an I/O norm (\u03b1, \u03b2) is subsequently passed as an input vector to the network, producing an output representing what is obligatory (e.g. \u03b2, if the translation to the ANN is proved correct). Only some input appears in the output, if it is made obligatory by a norm. In CILP, output nodes are always connected to input nodes creating a recurrent network, to represent the transitivity of logical rules when computing minimal or stable models. In normative reasoning, transitivity is not always accepted (since if you are obliged to do a and, if a then you are obliged to do b does not imply that you are obliged to do b). Thus, the normative CILP extends CILP also to allow that certain outputs might not be connected to their corresponding inputs (or will not even have a corresponding input as a result of the first change made to CILP earlier).\nMapping Input/Output Logic into Neural Networks We now first introduce a specific fragment of I/O logic relevant for our purposes, then we present an embedding of this fragment into extended logic programs, and finally, how to represent such norms with priorities in ANNs.\nDefinition 6. An extended logic program is a finite set of clauses of the form L0 \u2190 L1, . . . ,\u223c Ln,\u223c Ln+1, . . . ,\u223c Lm, where Li (0 \u2264 i \u2264 n) is a literal i.e. an atom or a classical negation of an atom denoted by \u00ac, and \u223c Lj (n+1 \u2264 j \u2264 m) is called default literal, where \u223c represents negation-as-failure. Following [16], from now on we use \u2018\u2190\u2019 in place of \u2018\u2192\u2019, and say that L0 is true if L1, ..., Lm is true (L0 \u2190 L1, ..., Lm), where L1, ..., Lm denotes a conjunction of literals (with \u2018,\u2019 used in place of \u2018\u2227\u2019).\nGiven an extended logic program P we identify its answer sets [17] as EXT (P ).\nDefinition 7 (I/O Normative Code). A normative code G = \u3008O,P, \u3009 is composed by two sets of rules r : (\u03b1, \u03b2) and a preference relation among those rules. Rules in O are called obligations, while rules in P are permissions. Rules in O are of the type (\u03b1, \u03b2), where:\n\u2013 \u03b1 = \u03b11 \u2228 . . .\u2228\u03b1n is a propositional formula in disjunctive normal form, i.e. \u03b1i (for 0 \u2264 i \u2264 n) is a conjunction of literals (\u00aca\u03b1i1\u2227 . . .\u2227\u00aca\u03b1im\u2227a\u03b1i(m+1)\u2227 . . .\u2227 a\u03b11(m+p)). Without loss of generality we assume that the first m literals are negative while the other p are positive. \u2013 \u03b2 = \u00acb\u03b21 \u2227 . . .\u2227\u00acb\u03b2m \u2227 b\u03b2m+1 \u2227 . . .\u2227 b\u03b2m+p is a finite conjunction of literals.\nRules in P are of type (\u03b1, l), where \u03b1 is the same as for obligations, but l is a literal.\nAs put forward by Boella and van der Torre [6], one of the roles of permissions is to undercut obligations. Informally, suppose to have a normative code G composed of two rules:\n29\n1. b is obligatory (i.e. (>, b) \u2208 O). 2. If a holds, then \u00acb is permitted (i.e. (a,\u00acb) \u2208 P).\nWe say that the rule (a,\u00acb) has priority over (>, b), i.e. b is obligatory as long as a does not hold, otherwise \u00acb is permitted and, therefore b is not obligatory anymore.\nThe fact that we consider only the I/O rules as introduced in Definition 7 permits us to give a natural embedding of this fragment of I/O logic into extended logic programs.\nDefinition 8. Let d\u00b7e denote a function mapping I/O rules (Definition 7) into extended logic programs (Definition 6), as follows:\ndr : (\u03b11 \u2228 . . . \u2228 \u03b1n, \u03b21 \u2227 . . . \u2227 \u03b2m)e = {r11 : (d\u03b21eout \u2190 d\u03b11ein); . . . ; r1m : (d\u03b2meout \u2190 d\u03b11ein); . . . ; rn1 : (d\u03b21eout \u2190 d\u03b1nein); . . . ; rnm : (d\u03b2meout \u2190 d\u03b1nein)}\ndl1 \u2227 . . . \u2227 lnein/out = dl1ein/out, . . . , dlnein/out daein = in a daeout = out a\nd\u00acaein = \u00acin a d\u00acaeout = \u00acout a\nWe call rules rij instances of r, and we informally write rij \u2208 Ints(r).\nNotice that the program resulting from the application of d\u00b7e has a unique model because it is negation-as-failure free.\nLemma 1. Given a set of obligations O = {(\u03b11, \u03b21), . . . , (\u03b1n, \u03b2n)}. Then it holds that:\nIf (\u03b1, \u03b2) \u2208 O then d\u03b2eout \u2208 EXT ({d(\u03b11, \u03b21)e; . . . ; d(\u03b1n, \u03b2n)e} \u222a d\u03b1ein).\nProof. The if direction is trivial while the only if can be proven by showing that every application of the immediate consequence operator T (as defined by [17]) can be encoded into an application of d\u00b7e (Definition 8).\nWe now show how to extend the preference relation w.r.t. rules generated with d\u00b7e.\nDefinition 9. Given a normative code G = \u3008O,P, \u3009 we define a transformation Tro(\u00b7) such that Tro(G) = \u3008dOe,P, \u2032\u3009, where \u2032 is defined as follows: tij \u2032 t\u2032i\u2032j\u2032 , for all tij \u2208 Inst(t) and t\u2032i\u2032j\u2032 \u2208 Inst(t\u2032) for t, t\u2032 \u2208 O such that t t\u2032.\nFor this reason, for a given normative code Tro(G), we introduce a further transformation Trp(\u00b7) as follows:\nDefinition 10. Given a normative code Go = Tro(G) = \u3008dOe,P, \u2032\u3009 we define Trp(Go) = \u3008dOe,P, \u2032\u2032\u3009, where \u2032\u2032 is defined as follows: For all p : (\u03b1, l) \u2208 P, p \u2032\u2032 tij, for all tij : (\u03b1,\u00acl) \u2208 dOe\nWe now recall how to encode (metalevel) preference relations, which define a priority between LP rules into (object-level) extended logic programs [43].\n30\nDefinition 11. (Object-level Priorities) Given a preference relation between ri and r such that ri r for 1 \u2264 i \u2264 j, replace the clause r : Lq+1 \u2190 (L1, ..., Lp) with the clause Lq+1 \u2190 (L1, ..., Lp,\u223c L1p+1, ...,\u223c L1q, ...,\u223c L j p+1, ...,\u223c Ljq), where ri(1\u2264i\u2264j) : (L i q+1 \u2190 Lip+1, ..., Liq).\nExample 2. Take the following normative code:\nG = \u3008{r : (a,\u00acb \u2227 c)}, {p : (d, b)}, {}\u3009.\nThen Tro(G) = {\u3008r11 : (a,\u00acb); r12 : (a, c)}, {p : (d, b)}, {}\u3009, and Trp(Tro(G)) = {\u3008r11 : (a,\u00acb); r12 : (a, c)}, {p : (d, b)}, {p r11\u3009}.\nFor rules with permissions in the output, which are of the form pi : Lim+1 \u2190 (Li1 , . . . , Lin , Lin+1 , . . . , Lim), such that, for any other rule, r : \u00acLim+1 \u2190 (Li1 , . . . , Lin) (resulting from the application of dGe), we impose pi r. As discussed, the role of permissions is to undercut obligations in dGe, and permissions will not be encoded explicitly into the ANN (every output of the ANN counts as an obligation; something is permitted if the contrary is not obligatory, see Section 3.4).\nLemma 2. Let P = {r1, r2, ..., rn} be an extended logic program with an explicit preference relation . Let P denote the translation of P into a program without (Definition 11). It follows that EXT (P ) = EXT (P ).\nWe are particularly interested in the translation of P into P because it is well-known that CILP networks will always compute the unique answer set of P , by converging to a unique stable state, provided that P is well-behaved (i.e. locally stratified, or acyclic, or acceptable, cf. [12]). This will be explored further in the next subsection. Before proceeding, let us use an example to illustrate what has been achieved so far.\nExample 3. (Translation of normative code into extended logic program) Consider the following normative code:\nr1 : (a \u2228 b,O(c)) r2 : (d \u2227 e,O(f)) r3 : (g,P(\u00acf)) r1 r2\nFirst, obligations are decomposed into instances: r1 : (a \u2228 b,O(c)) r2 : (d \u2227 e,O(f)) r3 : (g,P(\u00acf)) r1 r2 r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e r3 : (g,P(\u00acf)) r1 r2\n31\nSecondly, the priorities are decomposed: r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e r3 : (g,P(\u00acf)) r1 r2 r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e r3 : (g,P(\u00acf)) r11 r2 r12 r2\nFinally, the permission-generated priorities are added: r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e r3 : (g,P(\u00acf)) r11 r2 r12 r2 r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e r3 : (g,P(\u00acf)) r11 r2 r12 r2 r3 r2\nAnd the priorities are encoded as norm inputs: r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e r3 : (g,P(\u00acf)) r11 r2 r12 r2 r3 r2 r11 : c\u2190 a r12 : c\u2190 b r2 : f \u2190 d, e,\u223ca,\u223cb,\u223cg\nThe result is an equivalent extended logic program.\nThe N-CILP algorithm In this section we introduce the translation algorithm encoding a normative code into a feedforward ANN (with semi-linear neurones), namely the Normative-CILP (N-CILP) algorithm. The proposed algorithm differs from CILP [12] in how priorities are encoded into the ANN, and it does not assume identity.\nN-CILP Algorith (Input: normative code G; Output: ANN)\n1. G\u2032 = Tro(G);G \u2032\u2032 = Trp(G \u2032) 2. Apply the encoding of priorities as described in Definition 11 to G\u2032\u2032. 3. For each rule Rk = \u03b2o1 \u2190 \u03b1i1 , . . . , \u03b1in ,\u223c \u03b1in+1, . . . ,\u223c \u03b1im /\u2208 P.\n(a) For each literal \u03b1ij (1 \u2264 j \u2264 m) in the input of the rule: if there is no input neurone labeled \u03b1ij in the input level, then add a neurone labeled \u03b1ij in the input layer. (b) Add a neurone labeled Nk in the hidden layer.\n32\n(c) If there is no neurone labeled \u03b2o1 in the output level, then add a neurone labeled \u03b2o1 in the output layer. (d) For each literal \u03b1ij (1 \u2264 j \u2264 n): connect the respective input neurone with the neurone labeled Nk in the hidden layer with a positive weighted arc. (e) For each literal \u223c \u03b1ih (n + 1 \u2264 j \u2264 m): connect the respective input neurone with the neurone labeled Nk in the hidden layer with a negative weighted arc (the connections between these input neurones and the hidden neurone of the rule represent the priorities translated with negation-as-failure). (f) Connect the neurone labeled Ni with the neurone in the output level labeled \u03b2o1 with a positive weighted arc (each output in the rules is considered as a positive atom during the translation; a rule with a negative output \u00ac\u03b2 is translated in the network as output neurone labeled \u03b2\u2032 that has the same meaning of \u00ac\u03b2 but for the purpose of the translation can be treated as a positive output).\nProposition 1. For any normative code in the form of an extended logic program there exists an ANN obtained from the N-CILP translation algorithm such that the network computes the answer set semantics of the code.\nProof. Definition 8 translates a normative code into an extended logic program having a single extension (or answer set). From Lemma 2, the program extended with a priority relation also has a single extension. Garcez et al. [12] show that any extended logic program can be encoded into an ANN. N-CILP performs one such encoding using network weights as defined by Garcez et al. [12]. Hence, NCILP is sound. Since the program has a single extension, the iterative recursive application of input-output patterns to the network will converge to this extension, which is identical to the unique answer set of the program, for any initial input.\nWe end this subsection with a complete example of a translation of a normative code to an ANN. The following captures parts of the rule set a soccer-playing agent might be equipped with regarding the need to stop an opponent from scoring a goal in different situations (as, for instance, potentially encountered in the RoboCup robot soccer competitions):\nR1 = (opponentShooting \u2227 closeToOpponent, O(impactingOpponent)) R2 = (goalkeeper \u2227 insideOwnArea \u2227 closeToOpponent \u2227 opponentHasBall, O(impactingOpponent)) R3 = (haveBall \u2227 closeToGoal \u2227 closeToOpponent, O(impactingOpponent))\nThis set of norms is translated to an extended logic program:\nimpactingOpponent \u2190 opponentShooting \u2227 closeToOpponent impactingOpponent \u2190 goalkeeper, insideOwnArea, closeToOpponent, opponentHasBall\n33\nimpactingOpponent \u2190 haveBall, closeToGoal, closeToOpponent\nWhich, in turn, is embedded in the following ANN:\nopponent shooting\ncloseTo opponent\ngoalkeeper insideOwnArea opponent HasBall\nR1 R2 R3\nimpacting opponent\nhaveBall closeTogoal\nInitial Experimental Evaluation of the N-CILP Algorithm In order to gain a first idea of the performance and properties of the proposed N-CILP algorithm and the resulting networks, it has been implemented in a proof-ofconcept simulator then applied to the above RoboCup example scenario. While the results reported here are still preliminary, they indicate the capabilities of the neural-symbolic approach to normative reasoning and learning under uncertainty.\nIn the simulator, the KB contains the normative rules that an agent knows. We assume that the priorities are embedded in the rules. The KB is then read as input to the N-CILP translation algorithm, which produces a standard ANN trainable with backpropagation (cf., e.g., [23]). The results of training the ANNs are evaluated in the usual way, whereby the performance of a network with random weights initially, i.e. without KB, is compared with that of a network set-up using N-CILP, that is, with KB. Both networks are trained on the same set of examples: pairs of input vectors (opponentShooting, closeToOpponent, etc.) and target output vectors (ImpactingOpponent) with values 1, 0 and -1 denoting, respectively, true, unknown and false. The networks are trained and tested using cross-validation, where the set of examples is divided systematically into a training and a test set, multiple networks are trained and tested on each division (with the test set never seen by the netowork during training), and results are averaged out to produce a better estimate of the network\u2019s ability to generalise to new data, that is, its test set performance.\nIn evaluating the test set performance of the network, two distinct measures are used: tot and part.\ntot =\n\u2211n i=1 I( \u2227k j=1(cij == oij))\nn\n34\npart =\n\u2211n i=1 \u2211k j=1 I(cij == oij)\nn \u2217 k Here, n refers to the cardinality of the test set, k is the number of output neurones in the network, oij is the value of the j-th output of the network for the i-th test instance, cij is the target (desired) value of the j-th literal for the i-th test instance, I(\u00b7) is the indicator (i.e. a function returning 1 if the argument is true, and zero otherwise). The tot measure evaluates how many examples were estimated by the ANN correctly in their entirity (that is w.r.t. the entire target output vector), while part measures the average number of output neurones correctly evaluated by the ANN.\nComparison with a purely connectionist approach: The test-set performance of a network built using N-CILP is compared with that of a nonsymbolic ANN. One of the well known issues in neural-network training is how to decide the number of neurones in the hidden layer. In the case of N-CILP, this number is given by the number of symbolic rules. We adopt the same number of hidden neurones for both networks and do not perform model selection. The difference between the networks is in the values of the connection weights only. As mentioned, the ANN built with N-CILP sets its weights according to the rules in the KB, whilst the non-symbolic network has its weights initialised randomly. The expected advantage of the network built with N-CILP is that, even without any training, it should be capable of estimating correctly the output value of some of the examples by applying the rules contained in the KB (if the translation is correct, as proved, and the KB is relevant to the data classification problem at hand).\nThe network built with N-CILP, thus, has the head-start of a KB containing rules similar to (and including) the ones used in the example given at the end of the previous section. During the training phase, the network tries to learn additional rules provided in the form of training examples (input-output vectors). In the interest of fairness, the non-symbolic network is also provided with training examples derived from the initial rules,22 but has to learn all rules from scratch using backpropagation. The entire set of rules and preference relations used in our experiments, now with multiple outputs, is given below.\nR1 = (kickoff , O(-score)) R2 = (kickoff & MateTouchesBall , P(score)) R3 = (kickoff & MinBallMoved , P(score)) R4 = (True , O(-useHands)) R5 = (goalkeeper & InsideOwnArea , P(useHands)) R6 = (True , O(-contactingOpponent)) R7 = (True , O(-impactingOpponent)) R8 = (impactingOpponent , O(minimizeImpact)) R9 = (contactingOpponent , O(terminateContact))\n22 Given a rule, e.g. B \u2190 A, input and output vectors are created having \u20181\u2019 in the position corresponding to A in the input vector, and \u20181\u2019 in the position corresponding to B in the output vector.\n35\nR10 = (mateInsideOwnArea , O(-insideOwnArea)) R11 = (mateInsideOpponentArea , O(-insideOpponentArea)) R12 = (opponentFreeKick , O(keepDistance)) R13 = (goalkeeper & OpponentPenaltyKick & -ballTouched , O(-getBall)) R14 = (haveBall & OpponentApproaching , O(pass)) R15 = (haveBall & OpponentApproaching & OpponentCloseToMate , O(-pass)) R16 = (haveBall & CloseToGoal , O(shoot)) R17 = (opponentShooting & CloseToOpponent , O(impactingOpponent)) R18 = (goalkeeper & InsideOwnArea & CloseToOpponent & OpponentHasBall , O(impactingOpponent)) R19 = (-goalkeeper & MateInsideOwnArea & OpponentShooting , O(-impactingOpponent)) R20 = (haveBall & CloseToGoal & CloseToOpponent , O(impactingOpponent)) R21 = (opponentHasBall & CloseToOpponent & CloseToGoal , O(-impactingOpponent)) R22 = (-mateInsideOwnArea & CloseToOpponent & OpponentHasBall , O(useHands)) R23 = (insideOwnArea & MateInsideOwnArea & OpponentApproaching , O(impactingOpponent)) R24 = (insideOwnArea & HaveBall , O(pass)) R25 = (opponentFreeKick , O(-canScore)) R26 = (opponentPenaltyKick , O(keepDistance))\nR2 R1 R3 R1 R5 R4 R8 R7 R9 R6 R15 R14 R17 R7 R18 R7 R19 R17\nThe results show that the non-symbolic ANN is not able to achieve the same level of accuracy as the N-CILP network. Using the first 20 rules above (R1 to R20) to set up the ANN with N-CILP and the remaining 6 rules (R21 to R26) for testing produced test-set performances tot = 5.38% and part = 49.19%, while the non-symbolic network achieved tot = 5.13% and part = 45.25%. More importantly, when we evaluate how the N-CILP ANN perform with increasing number of rules in the KB, test-set performances also increase in a consistent way (see Figure 5). This confirms empirically that the ANN is capable of computing the same semantics as given by the rules in the KB (rules R23 and R24 seem to be particularly relevant), and to exploit learning from examples, which allows a normative agent to increase and adjust its knowledge in the face of multiple possible obligations which may change dynamically in time.\nThe test is done incrementally using the same 26 rules. The experiment\u2019s first run starts with a KB containing the first 20 rules, as before. Subsequently, two additional rules are added to the KB, with each consecutive run decreasing\n36\nthe number of unknown rules that the network has to learn by two, as shown in Figure 5. In the last experiment, with 26 rules, the figure reports the network\u2019s traning set performance since there are no rules left from which to derive test set patterns.\nFor the first two experiments, accuracy remains low, while for the last two, performance increases considerably reaching a peak of 98,01% for the part measure and 91,18% for tot.\nLearning CTDs: In a final experiment, we measure the capacity of an ANN built with N-CILP to learn new CTDs. This is done by using a KB with the priority-based orderings that regulate the CTDs left out.\nWe tested the network on learning three different CTDs, again in the robotsoccer context. The first refers to a situation where a robot player should never impact on an opponent (R7), but if a collision route is inevitable, then the robot should make its best to minimise the impact (see R7c below). The second CTD addresses a situation where the soccer robot is in physical contact with an opponent, which for most situations is forbidden by standard soccer rules (R6), and should try to terminate the contact (see R6c below). The third CTD handles a situation where, although generally not being allowed to use its hands (R4), the robot finds itself in the role of the goalkeeper (see R4c below). Rules R4, R6 and R7 are reproduced below for convenience.\nR7 = (> , O(\u00ac impactingOpponent)) R7c = (impactingOpponent , O(minimizeImpact)) R6 = (> , O(\u00accontactingOpponent)) R6c = (contactingOpponent , O(terminateContact)) R4 = (> , O(\u00acuseHands))\n37\nR4c = (goalkeeper & InsideOwnArea , P(useHands))\nRemoving the priority-based orderings results in an incomplete system that produces, in similar situations, both the unfulfillable obligation and the relative obligation to handle the suboptimal situation that is being analysed. Delivering on the promise to be able to deal with this type of uncertainty in the context of norms, what we expect from our approach is the ability to learn the prioritybased orderings that regulate the CTDs. The ANN is trained with a set of examples containing both regular situations (R4, R6, R7) and situations in which the CTD is applied (R4c, R6c, R7c). The resulting network is tested with a test set containing situations where an application of the CTD becomes necessary.\nFor the first CTD, results show a 95% test-set performance by the network, which generated minimizeImpact only when in the suboptimal CTD situation in question. For the two other CTDs, the results show an accuracy of 93% and 87% on their respective test sets. This indicates that N-CILP is capable of learning CTDs not included in the construction of the ANN. It, thus, allows us to avoid a total description of the corresponding domain (which very often turns out overly expensive or simply infeasible) as missing norms can be acquired through learning from examples."}, {"heading": "4 Conclusion", "text": "At the beginning of Section 1 we set out to argue two connected claims. Firstly, we aimed to show that probability is not the only way of dealing with uncertainty (and even more, that there are kinds of uncertainty which are for principled reasons not addressable with probabilistic means). Secondly, we wanted to provide evidence that logic-based methods can well support reasoning with uncertainty, using two paradigmatic examples: LP with Kleene semantics for modelling reasoning from information in a discourse, to an interpretation of the state of affairs of the intended model, and a neural-symbolic implementation of a fragment of I/O logic expressed as extended LP for dealing with uncertainty in dynamic normative contexts. Looking back at what has been reported in the previous sections, we believe that both goals have been met. Even more, while at first sight seeming fairly independent from each other, we hope that also the intrinsic\u2014formal and conceptual\u2014connection between LP for reasoning to an interpretation on the one hand, and the neural-symbolic I/O logic approach combining normative reasoning and learning on the other hand, have become apparent. The neural-symbolic I/O setting presents a natural expansion of the LP approach. In addition, the normative features also (via the additional ANN characteristics) add learning capacities to the previously exclusively reasoningfocused framework.23 Still, it should be clear that the discussed account of LP\n23 The presented approach to LP modelling of discourse does not tackle the learning of KB rules, as discourse comprehension generally is assumed to proceed with a mature\n38\nand neural-symbolic I/O logic are only two examples among several for logicbased methods dealing with forms of uncertainty, and that even for these two the presented work can only be considered initial steps in the direction of fully exploring\u2014and exploiting\u2014the possibilities offered by the respective approaches beyond the use of probabilistic models.\nAs a general insight gained from our described explorations into uncertainty and logical methods, we note that in fact examining the nature of the uncertainty and its twinned necessity in each logic provides at least a semi-systematic method of exploring for species of uncertainty. As we noted, at least LP, and deontic logics provide examples which are clearly interesting for human cognition and the modelling thereof with computational means. These are the first which we have examined in any detail. We do not claim that every logic has its own distinct species, nor that every species enumerated in this way is of any interest to cognitive modelling or AI. However, even from these examples, it is clear that logic can serve as a royal road to the exploration (and handling) of different kinds of uncertainty. The only generalisation we would offer at this point is that logics differ in their kind of uncertainty insofar as they specify distinct kinds of epistemic state. It is the epistemic states that cannot always be matched by other logics that give rise to different kinds of uncertainty, rather than some general property of the inferences that are valid, or the content of their propositions.\nConcerning future work, it seems desirable to also develop an architecture combining the described form of LP modelling with neural-symbolic computing analogous to the I/O logic setting. As discussed, for instance, by Stenning and van Lambalgen [56], there is already a neural implementation for simple LP. Constraint LP is a more expressive logic which includes the Event Calculus, and is required for modelling, among other things, all but the simplest reasoning in the processing of time and causality in narrative discourse [67]. A neural network implementation for this formalism is currently lacking, but would most likely have great advantages: on the one hand, introducing the ANN characteristics as part of the neural-symbolic implementation would allow the introduction of learning capacities into the discourse processing context, expanding the approach and corresponding model in a natural way. On the other hand the availability of such an architecture would further bridge from the currently still (mostly) cognitive modelling-oriented setup to applications of the paradigm in corresponding models in cognitively-inspired AI.\nRegarding the neural-symbolic implementation of I/O logic, we next hope to introduce an explicit notion of context in the neural-symbolic system. In reality, choices are not made by only taking into consideration the current situation, but are usually also influenced by past events. Continuing with the robot soccer example, for instance we might want to consider situations where a robot changes its style of play due to the previous and current history: yellow cards received would make the robot play in a safer way to avoid being sent off; if the current result suggests that the robot team is already winning, they could prefer to play\nKB. But an account of learning is nevertheless an important goal for LP models of discourse.\n39\nmore defensively to prevent the other team equalising. In order to implement those mechanisms, the system must be capable of memorising past events. One way to solve this might be to add external memory to the networks [69]. With this solution the context nodes in the ANN could be added in the same way as for the rules, the difference being that for each context in the input level there would be a correspondent output context which is linked from the output to the input levels, in order to maintain memory if any context modified its status during computation. A related line of potential future research involves the area of argumentation. Argumentation has been proposed, among other things, as a method to help symbolic machine learning. In Mozina et al.\u2019s approach [41], an expert\u2019s reasons for some of the training examples can be used to try and guide the search for hypotheses, in a way similar to our use of background knowledge [14].\nIn a third line of development on the systems-oriented side, we want to take the neural-symbolic architecture for I/O and\u2014reusing insights from the LP approach described in this article\u2014 develop a follow-up framework additionally modelling interpretation-related aspects of reasoning. If successful, this would allow to address the case when one does not know which propositions are actually relevant (i.e. combining reasoning to an interpretation with subsequent normative reasoning while maintaining the ability to deal with dynamically changing sets of rules).\nFrom a conceptual perspective, we would like to get clearer how uses of intensional and extensional systems\u2014as already discussed in Section 1\u2014might work together. Stenning and van Lambalgen [59] argue that systems that use extension sets to capture the meanings of predicates\u2014at least when those systems are used for cognitive modelling\u2014necessarily rely for their foundations on intensional systems that can capture the interplay of motivations of the reasoner (desires, purposes, goals, preferences, . . . ). The extensional systems \u2018precisify\u2019 or perhaps operationalise intensional meanings in specific contexts. But different extensional precisifications of the same intensional concept may be incompatible in having different extensions. Intensional systems can capture the crucial abstractions due to the flexibility of motivational elements, answering the question \u2018Why this extension in this interpretation?\u2019. Extensional systems are important, but their importance cannot be understood without understanding their basis in intensional systems. The issues of operationalising concepts for statistical modelling are commonplace to psychologists, but analogous decisions have to be made in many other related domains, including everyday discourse. If we are reasoning about the reliability of the conditional \u201cIf the brake pedal is pressed, the car slows down.\u201d then the extension of cars excludes ones on the dump. If a mechanic is searching for a spare part, and reasons about the conditional: \u201cIf the car is a 2009 or later, it complies with the emissions regulations.\u201d then the ones on the dump may be exactly the ones that are in the relevant extension. We negotiate extensions for \u2018car\u2019 through our intensional purposes for reasoning, and when we construct them, they do not replace the vague intensional meanings that went into their construction. Reiterating a point already argued in the introduction,\n40\n\u201cintensional\u201d systems like LP with Kleene semantics can express goals in a sense which is not fully possible in \u201cextensional\u201d systems like probability theory. So, once again, it is important to distinguish the different kinds of uncertainty they treat. At the most general level, this paper is an argument for a strategy in understanding uncertainty. The novel kinds of uncertainty exemplified here are of a rather extreme kind. Establishing extreme examples is important. Extreme examples may not make good law, but they greatly aid exploration."}, {"heading": "Acknowledgements", "text": "We want to thank the following people for their indispensable contributions to different parts of the work reported in this article: Guido Boella, Silvano Colombo Tosatto, Valerio Genovese, Laura Martignon, Alan Perotti, and Alexandra Varga."}], "references": [{"title": "On the logic of theory change: Partial meet contraction and revision functions", "author": ["C.E. Alchourr\u00f3n", "P. G\u00e4rdenfors", "D. Makinson"], "venue": "The journal of symbolic logic 50(02), 510\u2013530", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Sceptical logic programming based default reasoning: defeasible logic rehabilitated", "author": ["G. Antoniou", "D. Billington", "M. Maher"], "venue": "Miller, R., Shanahan, M. (eds.) COMMONSENSE 98, The 4th Symposium on Logical Formalizations of Commonsense Reasoning. London", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Reasoning about termination of pure prolog programs", "author": ["K.R. Apt", "D. Pedreschi"], "venue": "Information and Computation 106, 109\u2013157", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "The cognitive interface", "author": ["G. Baggio", "K. Stenning", "M. van Lambalgen"], "venue": "Aloni, M., Dekker, P. (eds.) Cambridge Handbook of Formal Semantics. Cambridge University Press, Cambridge, UK", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Normative framework for normative system change", "author": ["G. Boella", "G. Pigozzi", "L. van der Torre"], "venue": "8th Int. Joint Conf. on Autonomous Agents and Multiagent Systems AAMAS 2009. pp. 169\u2013176. IFAAMAS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Permission and authorization in normative multiagent systems", "author": ["G. Boella", "L. van der Torre"], "venue": "Procs. of Int. Conf. on Artificial Intelligence and Law ICAIL. pp. 236\u2013 237", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A game theoretic approach to contracts in multiagent systems", "author": ["G. Boella", "L. van der Torre"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C 36(1), 68\u2013 79", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Types of uncertainty", "author": ["R. Bradley", "M. Drechsler"], "venue": "Erkenntnis 79, 1225\u20131248", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "From logic to logic programming", "author": ["K. Doets"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Handbook of Deontic Logic and Normative Systems", "author": ["D. Gabbay", "J. Horty", "X. Parent", "R. van der Meyden", "van der Torre", "L. (eds."], "venue": "College Publications, London", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Symbolic knowledge extraction from trained neural networks: A sound approach", "author": ["A. Garcez", "K. Broda", "D.M. Gabbay"], "venue": "Artificial Intelligence 125, 155\u2013207", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural-Symbolic Learning Systems: Foundations and Applications", "author": ["A. Garcez", "K. Broda", "D. Gabbay"], "venue": "Perspectives in Neural Computing, Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural-Symbolic Learning and Reasoning: Contributions and Challenges", "author": ["A. Garcez", "T.R. Besold", "L. de Raedt", "P. F\u00f6ldiak", "P. Hitzler", "T. Icard", "K.U. K\u00fchnberger", "L. Lamb", "R. Miikkulainen", "D. Silver"], "venue": "AAAI Spring 2015 Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. AAAI Technical Reports, vol. SS-15-03. AAAI Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Value-based argumentation frameworks as neural-symbolic learning systems", "author": ["A. Garcez", "D. Gabbay", "L. Lamb"], "venue": "Journal of Logic and Computation 15(6), 1041\u2013 1058", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural-Symbolic Cognitive Reasoning", "author": ["A. Garcez", "L.C. Lamb", "D.M. Gabbay"], "venue": "Cognitive Technologies, Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The stable model semantics for logic programming", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "Proceedings of the 5th Logic Programming Symposium. pp. 1070\u20131080. MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1988}, {"title": "Classical negation in logic programs and disjunctive databases", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "New Generation Computing 9, 365\u2013385", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Heuristics: The foundations of adaptive behavior", "author": ["G. Gigerenzer", "R. Hertwig", "T. Pachur"], "venue": "Oxford University Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Simple heuristics that make us smart", "author": ["G. Gigerenzer", "P.M. Todd", "The ABC Research Group"], "venue": "Oxford University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Reasoning about uncertainty", "author": ["J. Halpern"], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Deontic logics for prioritized imperatives", "author": ["J. Hansen"], "venue": "Artificial Intelligence and Law 14(1-2), 1\u201334", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Prentice Hall", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Deontic logic as founded on nonmonotonic logic", "author": ["J.F. Horty"], "venue": "Ann. Math. Artif. Intell. 9(1-2), 69\u201391", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Imperatives and logic", "author": ["J. J\u00f6rgensen"], "venue": "Erkenntnis 7, 288\u2013296", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1937}, {"title": "Probability theory, not the very guide of life", "author": ["P. Juslin", "H. Nilsson", "A. Winman"], "venue": "Psychological Review 116(4), 856\u2013874", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "The concept of probability in psychological experiments", "author": ["D. Kahneman", "A. Tversky"], "venue": "Kahneman, D., Slovic, P., Tversky, A. (eds.) The concept of probability in psychological experiments, pp. 509\u2013520. Cambridge University Press, Cambridge, UK", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1982}, {"title": "Many facets of reasoning under uncertainty, inconsistency, vagueness, and preferences: A brief survey. K\u00fcnstliche Intelligenz", "author": ["G. Kern-Isberner", "T. Lukasiewicz"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}, {"title": "Risk, uncertainty and profit", "author": ["F. Knight"], "venue": "Hart, Schaffner and Marx, New York", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1921}, {"title": "The early years of logic programming", "author": ["R.A. Kowalski"], "venue": "Communications of the ACM 31, 38\u201342", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1988}, {"title": "Nonmonotonic reasoning, preferential models and cumulative logics", "author": ["S. Kraus", "D. Lehmann", "M. Magidor"], "venue": "Artificial intelligence 44(1), 167\u2013207", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1990}, {"title": "Normative systems and their revision: An algebraic approach", "author": ["L. Lindahl", "J. Odelstad"], "venue": "Artificial Intelligence and Law 11(2-3), 81\u2013104", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Foundations of Logic Programming", "author": ["J.W. Lloyd"], "venue": "Springer-Verlag", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1987}, {"title": "Constraints for input-output logics", "author": ["D. Makinson", "L. van der Torre"], "venue": "Journal of Philosophical Logic 30(2), 155\u2013185", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Permissions from an input-output perspective", "author": ["D. Makinson", "L. van der Torre"], "venue": "Journal of Philosophical Logic 32(4), 391\u2013416", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "What is input/output logic? In: Foundations of the Formal Sciences II: Applications of Mathematical Logic in Philosophy and Linguistics, Trends in Logic, vol", "author": ["D. Makinson", "L. van der Torre"], "venue": "17. Kluwer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Input/output logics", "author": ["D. Makinson", "L. van der Torre"], "venue": "Journal of Philosophical Logic 29(4), 383\u2013408", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Circumscription: ??a form of non-monotonic reasoning", "author": ["J. McCarthy"], "venue": "Artificial Intelligence 13(1), 27\u201339", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1980}, {"title": "A framework for representing knowledge", "author": ["M. Minsky"], "venue": "Tech. Rep. 306, AI Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1974}, {"title": "Risk, uncertainty, and heuristics", "author": ["S. Mousavi", "G. Gigerenzer"], "venue": "Journal of Business Research 67, 1671\u20131678", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Argument based machine learning", "author": ["M. Mozina", "J. Zabkar", "I. Bratko"], "venue": "Artificial Intelligence 171(10-15), 922\u2013937", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic logic", "author": ["N.J. Nilsson"], "venue": "Artificial intelligence 28(1), 71\u201387", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1986}, {"title": "Defeasible logic", "author": ["D. Nute"], "venue": "Gabbay, D., Robinson, J. (eds.) Handbook of Logic in Artificial Intelligence and Logic Programming, vol. 3, pp. 353\u2013396. Oxford University Press", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Defeasible deontic logic, Synthese Library, vol", "author": ["Nute", "D. (ed."], "venue": "263. Kluwer", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Rationality In An Uncertain World: Essays In The Cognitive Science Of Human Understanding", "author": ["M. Oaksford", "N. Chater"], "venue": "Psychology Press", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Causality: Models, Reasoning, and Inferece", "author": ["J. Pearl"], "venue": "Cambridge University Press, Cambridge, UK", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2000}, {"title": "Exceptions and anomalies: An ERP study on context sensitivity in autism", "author": ["J. Pijnacker", "B. Geurts", "M. van Lambalgen", "J. Buitelaar", "P. Hagoort"], "venue": "Neuropsychologia 48, 2940\u20132951", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Dyadic deontic logic and contrary-to-duty obligations", "author": ["H. Prakken", "M. Sergot"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1997}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel Distributed Processing, vol. 1, pp. 318\u2013362. MIT Press", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1986}, {"title": "Emergence of norms through social learning", "author": ["S. Sen", "S. Airiau"], "venue": "Procs. of the 20th International Joint Conference on Artificial Intelligence - IJCAI. pp. 1507\u2013 1512", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinventing Shakey", "author": ["M. Shanahan"], "venue": "Minker, J. (ed.) Logic-based Artificial Intelligence. Kluwer, Dordrecht", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}, {"title": "A semantical approach to non-monotonic logics", "author": ["Y. Shoham"], "venue": "Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI) 1987. pp. 388\u2013392", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1987}, {"title": "On the emergence of social conventions: Modeling, analysis, and simulations", "author": ["Y. Shoham", "M. Tennenholtz"], "venue": "Artificial Intelligence 94(1-2), 139\u2013166", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1997}, {"title": "Causality in thought", "author": ["S. Sloman", "D. Lagnado"], "venue": "The Annual Review of Psychology 66, 1\u201325", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Human reasoning and Cognitive Science", "author": ["K. Stenning", "M. van Lambalgen"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "The logical response to a noisy world", "author": ["K. Stenning", "M. van Lambalgen"], "venue": "Oaksford, M. (ed.) Cognition and Conditionals: Probability and Logic in Human Thought, pp. 85\u2013102. Oxford University Press, Oxford", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Many logics for the many things that people do in reasoning", "author": ["K. Stenning", "A. Varga"], "venue": "Ball, L., Thompson, V. (eds.) International Handbook of Thinking and Reasoning. Psychology Press", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive reasoning: integrating fast and frugal heuristics with a logic of interpretation", "author": ["K. Stenning", "L. Martignon", "A. Varga"], "venue": "Decision", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2017}, {"title": "Reasoning about obligations", "author": ["L. van der Torre"], "venue": "Ph.D. thesis, Erasmus University Rotterdam", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1997}, {"title": "Deontic redundancy: A fundamental challenge for deontic logic", "author": ["L. van der Torre"], "venue": "Deontic Logic in Computer Science, 10th International Conference (\u2206EON 2010). LNCS, vol. 6181, pp. 11\u201332. Springer", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "Deontic update semantics", "author": ["L. van der Torre", "Y. Tan"], "venue": "McNamara, P., Prakken, H. (eds.) Norms, Logics and Information Systems. New Studies on Deontic Logic and Computer Science. IOS Press", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1999}, {"title": "Abstract normative systems: Semantics and proof theory", "author": ["S.C. Tosatto", "G. Boella", "L. van der Torre", "S. Villata"], "venue": "Brewka, G., Eiter, T., McIlraith, S.A. (eds.) KR. AAAI Press", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Knowledge-based artificial neural networks", "author": ["G.G. Towell", "J.W. Shavlik"], "venue": "Artificial Intelligence 70(1), 119\u2013165", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1994}, {"title": "Judgment under uncertainty: Heuristics and biases", "author": ["A. Tversky", "D. Kahneman"], "venue": "Science 185(4157), 1124\u20131131", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1974}, {"title": "Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment", "author": ["A. Tversky", "D. Kahneman"], "venue": "Psychological Review 90(4), 293", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1983}, {"title": "The proper treatment of events", "author": ["M. van Lambalgen", "F. Hamm"], "venue": "Blackwell, Oxford and Boston", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2004}, {"title": "A formal model of infants\u2019 acquisition of practical knowledge from observation", "author": ["A. Varga"], "venue": "Ph.D. thesis, Central European University, Budapest", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "Deontic logic", "author": ["G.H. von Wright"], "venue": "Mind 60, 1\u201315", "citeRegEx": "70", "shortCiteRegEx": null, "year": 1951}], "referenceMentions": [{"referenceID": 44, "context": "While fully agreeing on the premise of the statement\u2014namely the observation that most human reasoning is uncertain in nature\u2014we want to challenge the conclusion Oaksford and Chater [45] (and many others) draw from it: in our view probability theory is neither the perfect solution solving all challenges introduced by reasoning\u2019s inherent uncertainty, nor should logic be overly casually discarded as exclusively fit to deal with reasoning in certainty.", "startOffset": 181, "endOffset": 185}, {"referenceID": 36, "context": "This is followed by the presentation of a neural-symbolic approach to LP extended to implement Makinson\u2019s and van der Torre\u2019s Input/Output (I/O) logic [37], which can similarly be used to model non-probabilistic uncertainty in normative reasoning; uncertainty which is compounded by changing or newly added norms (i.", "startOffset": 151, "endOffset": 155}, {"referenceID": 44, "context": "Among the main foundations of our argument is the observation that there are several qualitatively different kinds of uncertainty, for some of which probability\u2014 as convincingly shown by Oaksford and Chater [45]\u2014can be a powerful modelling technique, while others clearly lie outside of the reach of probabilistic approaches.", "startOffset": 207, "endOffset": 211}, {"referenceID": 28, "context": "Knight [29] made an early distinction between \u2018risk\u2019 (which could be modelled in probability) and \u2018uncertainty\u2019 (which could not), in economics.", "startOffset": 7, "endOffset": 11}, {"referenceID": 26, "context": "Kahneman and Tversky [27] already discussed several variants of uncertainty, distinguishing, for example, what we might call private uncertainty (\u201cHow sure am I that Rome is south of New York?\u201d where the truth is taken to be already known by others) from communal uncertainty (\u201cHow likely is it at the time of writing that the euro will collapse?\u201d).", "startOffset": 21, "endOffset": 25}, {"referenceID": 39, "context": "Mousavi and Gigerenzer [40] expand Knight\u2019s distinctions adding a further type of \u2018utter uncertainty\u2019 which describes cases where models are used in new domains.", "startOffset": 23, "endOffset": 27}, {"referenceID": 65, "context": "The algebra of propositions grows at each step, and when examined carefully, the propositions already in the model change\u2014if only by taking on temporal/causal relations to the new ones that arrived [67]\u2014even when they are not dropped nonmonotonically.", "startOffset": 198, "endOffset": 202}, {"referenceID": 45, "context": "The structural \u2018common core\u2019 of the last LP model in this sequence can provide, for instance, the basis for creating a Bayes Net by adding distributional information [46,48].", "startOffset": 166, "endOffset": 173}, {"referenceID": 66, "context": "And this is just the beginning of the difficulties since, for example, LP can freely express intentional relations between acts and actors\u2019 multiple goals [68], which can at best be inflexibly \u2018operationalised\u2019 in extensional systems.", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "It is worth noting that the recent success of probabilistic language models based on neural networks [69,20] is orthogonal to the arguments presented in this paper.", "startOffset": 101, "endOffset": 108}, {"referenceID": 14, "context": "Yet, within the area of neural-symbolic computing [15], equivalences have been proved between LP and neural networks, which indicate the possibility of reconciling such apparently very distinct representational frameworks.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "7 Later, we shall consider an alternative LP semantics based on Answer Sets [17], but we choose Preferred Model Semantics [53] for now because the uniqueness of preferred models is a crucial feature for cognitive processes such as discourse processing.", "startOffset": 76, "endOffset": 80}, {"referenceID": 51, "context": "7 Later, we shall consider an alternative LP semantics based on Answer Sets [17], but we choose Preferred Model Semantics [53] for now because the uniqueness of preferred models is a crucial feature for cognitive processes such as discourse processing.", "startOffset": 122, "endOffset": 126}, {"referenceID": 64, "context": "At this point we take a step back and include what might seem like a mere clarification of terminology, but goes in fact much beyond that: the sense of \u2018extensional\u2019 and \u2018intensional\u2019 we use here are from the psychological decision making literature\u2014stemming from [66]\u2014in which probability is extensional because its predicates are defined as sets, even though its conditional is nontruthfunctional.", "startOffset": 264, "endOffset": 268}, {"referenceID": 54, "context": "Nonetheless, LP (with the semantics as specified by Stenning and van Lambalgen [56]) is intensional in the general philosophical sense: its predicates are defined in terms of \u2018senses\u2019 which are cashed out as algorithms (completion), and its closed-world reasoning conditionals are \u2018licences for inference\u2019 (roughly contentful inference rules) rather than compound propositions (cf.", "startOffset": 79, "endOffset": 83}, {"referenceID": 64, "context": "An example of the role this interpretation of the intensional/extensional distinction plays, is the famous \u2018Conjunction Fallacy\u2019 [66] which is supposed to be about judgements of the relative sizes of sets of cases corresponding to predicates and their conjunctions in probability.", "startOffset": 129, "endOffset": 133}, {"referenceID": 57, "context": "[59] use the Linda Task (the origin of the supposed Conjunction Fallacy) to illustrate this intensional option for interpretation, and its consequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Quite recently Kern-Isberner and Lukasiewicz [28] provided a helpful brief map of approaches to uncertainty\u2014which, among others, demarcates logic programming under answer set semantics as distinctively outside both the currently popular system P [31] and the AGM axioms [1].", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "Quite recently Kern-Isberner and Lukasiewicz [28] provided a helpful brief map of approaches to uncertainty\u2014which, among others, demarcates logic programming under answer set semantics as distinctively outside both the currently popular system P [31] and the AGM axioms [1].", "startOffset": 246, "endOffset": 250}, {"referenceID": 0, "context": "Quite recently Kern-Isberner and Lukasiewicz [28] provided a helpful brief map of approaches to uncertainty\u2014which, among others, demarcates logic programming under answer set semantics as distinctively outside both the currently popular system P [31] and the AGM axioms [1].", "startOffset": 270, "endOffset": 273}, {"referenceID": 20, "context": "Another aid to navigation is offered by Halpern [21] who is concerned with distinguishing different representations of uncertainty, and then studying reasoning from those representations.", "startOffset": 48, "endOffset": 52}, {"referenceID": 54, "context": "This reasoning is omnipresent in human problem solving and communication [56].", "startOffset": 73, "endOffset": 77}, {"referenceID": 57, "context": "[59] argue that such extensional systems are inherently incapable of providing the requisite flexibility.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "In contrast, LP reasoning to interpretations of discourse need have no numerical parameters, even though numerical properties of the logical structures involved have been shown to be the basis on which reasoners make quantitative judgments of LP conditionals\u2019 reliability in inference [59].", "startOffset": 285, "endOffset": 289}, {"referenceID": 41, "context": "8 In terms of concrete examples, the work by Nilsson [42] comes to mind as a prominent instance falling within the domain of Halpern\u2019s plausibilty measures.", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "At a more technical level, there are a number of contrasts between LP (and also the later discussed I/O logic) and plausibility measures [21] which adopt the KLM axioms.", "startOffset": 137, "endOffset": 141}, {"referenceID": 63, "context": "Tversky and Kahneman [65] and Gigerenzer et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "[19,18] have extensively developed evidence that human reasoning is heuristic with regard to probability models.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[19,18] have extensively developed evidence that human reasoning is heuristic with regard to probability models.", "startOffset": 0, "endOffset": 7}, {"referenceID": 25, "context": "[26] argue that peoples\u2019 judgements of likelihoods in uncertainty do not obey probabilistic models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[59] explore an incorporation of such heuristic approaches within LP to provide a simple probability-free model of judgement and decision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "As, for instance, Stenning and van Lambalgen [57] argue, when faced with interpretational uncertainty about the information involved in a problem, we cannot engage the computational complexity of probability, but have to take more accessible inferential paths\u2014which are often quite successful.", "startOffset": 45, "endOffset": 49}, {"referenceID": 54, "context": "Here, among other work building on the book by Stenning and van Lambalgen [56], a version of", "startOffset": 74, "endOffset": 78}, {"referenceID": 36, "context": "Section 3 subsequently takes a more AI-centric view and describes a neuralsymbolic architecture combining the I/O logic proposed by Makinson and van der Torre [37] with artificial neural networks (ANNs), applied to normative reasoning tasks involving uncertainty introduced by changing norms over time.", "startOffset": 159, "endOffset": 163}, {"referenceID": 50, "context": "LP is a formal system that has been used extensively to model reasoning in a variety of domains, as widely separated as motor control [52], and imitative learning [68].", "startOffset": 134, "endOffset": 138}, {"referenceID": 66, "context": "LP is a formal system that has been used extensively to model reasoning in a variety of domains, as widely separated as motor control [52], and imitative learning [68].", "startOffset": 163, "endOffset": 167}, {"referenceID": 65, "context": "Its employment in cognitive modelling grew primarily out of an analysis of discourse processing, in particular of interpretation [67,56,58].", "startOffset": 129, "endOffset": 139}, {"referenceID": 54, "context": "Its employment in cognitive modelling grew primarily out of an analysis of discourse processing, in particular of interpretation [67,56,58].", "startOffset": 129, "endOffset": 139}, {"referenceID": 56, "context": "Its employment in cognitive modelling grew primarily out of an analysis of discourse processing, in particular of interpretation [67,56,58].", "startOffset": 129, "endOffset": 139}, {"referenceID": 54, "context": "Following Stenning and van Lambalgen [56], we view reasoning to be a twostage process.", "startOffset": 37, "endOffset": 41}, {"referenceID": 51, "context": "Preferred model is the logical notion used by Shoham [53].", "startOffset": 53, "endOffset": 57}, {"referenceID": 54, "context": "LP as discussed by Stenning and van Lambalgen [56] and van Lambalgen and Hamm [67] formalises a kind of reasoning which uses closedworld assumptions (CWAs) in order to keep the scope of reasoning to manageable dimensions by entities with limited time, storage, and computational resources, though with very large knowledge bases (KBs)\u2014such as we are.", "startOffset": 46, "endOffset": 50}, {"referenceID": 65, "context": "LP as discussed by Stenning and van Lambalgen [56] and van Lambalgen and Hamm [67] formalises a kind of reasoning which uses closedworld assumptions (CWAs) in order to keep the scope of reasoning to manageable dimensions by entities with limited time, storage, and computational resources, though with very large knowledge bases (KBs)\u2014such as we are.", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "Historically, LP is a computational logic designed and developed for automated planning [30,9] which is intrinsically preoccupied with relevance\u2014making for an important qualitative difference between LP and classical logic (CL).", "startOffset": 88, "endOffset": 94}, {"referenceID": 8, "context": "Historically, LP is a computational logic designed and developed for automated planning [30,9] which is intrinsically preoccupied with relevance\u2014making for an important qualitative difference between LP and classical logic (CL).", "startOffset": 88, "endOffset": 94}, {"referenceID": 57, "context": "[56, chapter 2] for the justification, or the Appendix in [59] for a more succinct version.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "Such networks will adopt an answer set semantics [17] allowing the use of default negation for implementing CWA explicitly, and classical (sometimes called explicit) negation, which allows for reasoning as intended here, that is, in the absence of a proof of A and its negation, the truth-value of A is \u2018currently indeterminate\u2019.", "startOffset": 49, "endOffset": 53}, {"referenceID": 53, "context": "The psychological findings that these inferences are more difficult might well be a result of the micro-scale of the tasks being used [55], or of the slightly more complex form of the CWAab needed [56, pp.", "startOffset": 134, "endOffset": 138}, {"referenceID": 45, "context": "At least in causal cases, LP produces the structural features of the models which are shared with the structural features of causal Bayes Nets [46,48].", "startOffset": 143, "endOffset": 150}, {"referenceID": 3, "context": "Nevertheless, the computational properties are highly distinct [4,57].", "startOffset": 63, "endOffset": 69}, {"referenceID": 55, "context": "Nevertheless, the computational properties are highly distinct [4,57].", "startOffset": 63, "endOffset": 69}, {"referenceID": 39, "context": "The authors cited in Section 1 as advocating varieties of uncertainty\u2014with the exception of Mousavi and Gigerenzer [40]\u2014look to identify them through probability models.", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "For example, take the most elaborated classification by Bradley and Drechsler [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 44, "context": "16 Nota bene: This stands in harsh contrast to Oaksford\u2019s and Chater\u2019s [45] above quoted conventional characterisation of CL as \u2018reasoning in certainty\u2019.", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "of conceptualising monotonic CL in contrast to nonmonotonic logics was introduced in the mid-1970s and 1980s\u2014for instance, in the wake of Minsky\u2019s frames [39] or McCarthy\u2019s circumscription approach [38]\u2014the underlying concern was not with characterising kinds of uncertainty, but with contrasting two systems on the one specific property of (non)monotonicity.", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "of conceptualising monotonic CL in contrast to nonmonotonic logics was introduced in the mid-1970s and 1980s\u2014for instance, in the wake of Minsky\u2019s frames [39] or McCarthy\u2019s circumscription approach [38]\u2014the underlying concern was not with characterising kinds of uncertainty, but with contrasting two systems on the one specific property of (non)monotonicity.", "startOffset": 198, "endOffset": 202}, {"referenceID": 46, "context": "Electrophysiological observations on analogous materials strongly corroborate these inferences\u2019 occurrence [47].", "startOffset": 107, "endOffset": 111}, {"referenceID": 55, "context": "[57,4]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[57,4]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 57, "context": "[59]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "open problems relating to the use of norms in these systems is how to equip agents to deal effectively with norms that change over time [5], either due to the introduction of new norms, due to explicit changes made by legislators to already existing norms, or due to different interpretations of the law by judges, referees, and other judicial bodies.", "startOffset": 136, "endOffset": 139}, {"referenceID": 36, "context": "In trying to tackle the difficulties arising from the dynamic nature of norms, we combine I/O logic [37,34,35] with neural-symbolic computation [12] in order to propose a formal framework for reasoning and learning about norms in a dynamic environment.", "startOffset": 100, "endOffset": 110}, {"referenceID": 33, "context": "In trying to tackle the difficulties arising from the dynamic nature of norms, we combine I/O logic [37,34,35] with neural-symbolic computation [12] in order to propose a formal framework for reasoning and learning about norms in a dynamic environment.", "startOffset": 100, "endOffset": 110}, {"referenceID": 34, "context": "In trying to tackle the difficulties arising from the dynamic nature of norms, we combine I/O logic [37,34,35] with neural-symbolic computation [12] in order to propose a formal framework for reasoning and learning about norms in a dynamic environment.", "startOffset": 100, "endOffset": 110}, {"referenceID": 11, "context": "In trying to tackle the difficulties arising from the dynamic nature of norms, we combine I/O logic [37,34,35] with neural-symbolic computation [12] in order to propose a formal framework for reasoning and learning about norms in a dynamic environment.", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "[12] on the other hand embeds symbolic logic, and in particular LP into ANNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": ", [51,7,54]), few machine learning techniques have been applied to tackle open problems like revising and learning new norms in open and dynamic environments.", "startOffset": 2, "endOffset": 11}, {"referenceID": 6, "context": ", [51,7,54]), few machine learning techniques have been applied to tackle open problems like revising and learning new norms in open and dynamic environments.", "startOffset": 2, "endOffset": 11}, {"referenceID": 52, "context": ", [51,7,54]), few machine learning techniques have been applied to tackle open problems like revising and learning new norms in open and dynamic environments.", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": "LP rules with metalevel priorities [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 12, "context": "To this end, neural-symbolic systems bring together connectionist networks and symbolic knowledge representation and reasoning [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "In the case of backpropagation\u2014probably the most commonly applied neural learning algorithm [50]\u2014an error is calculated as the difference between the network\u2019s actual output vector and the target vector, for each input vector in the set of examples.", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "In CILP, a translation algorithm maps a logic program P into a single hidden layer ANN N such that N computes the least fixed-point of P [33].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "This provides a massively parallel model for computing the stable model semantics of P [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "The knowledge acquired by training can then be extracted [11], closing the learning cycle, as advocated by [64].", "startOffset": 57, "endOffset": 61}, {"referenceID": 62, "context": "The knowledge acquired by training can then be extracted [11], closing the learning cycle, as advocated by [64].", "startOffset": 107, "endOffset": 111}, {"referenceID": 32, "context": "default negation) [33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "N will eventually converge to a stable state which is identical to the stable model of P provided that P is an acceptable program [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 11, "context": "This implements priorities in the rules [12] and is responsible for adding alternative paths that enable robustness in the networks also.", "startOffset": 40, "endOffset": 44}, {"referenceID": 35, "context": "As explained by Makinson and van der Torre [36], I/O logic takes its origin in the study of conditional norms which, either in imperative or indicative form, express obligations under some legal, moral, or practical code, goals, contingency plans, advice, etc.", "startOffset": 43, "endOffset": 47}, {"referenceID": 36, "context": "Putting this overall notion in formal terms, Makinson and van der Torre [37] represent rules by ordered pairs (a, x), where the antecedent a is thought of as an input, representing some condition or situation, and the consequent x is thought of as an output, representing what the rule tells us to be desirable, obligatory, or whatever else in that situation.", "startOffset": 72, "endOffset": 76}, {"referenceID": 67, "context": "Abstract Normative Systems Modal logic has been the standard for normative reasoning ever since von Wright [70].", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "\u2019s \u201cHandbook of Deontic Logic and Normative Systems\u201d [10], the classical modal logic framework is mainly confined to the historical chapter.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Another chapter presents the alternatives to the modal framework, and three chapters discuss concrete approaches, namely I/O logic, the imperativist approach [22], and the algebraic conceptual implication structures [32].", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "Another chapter presents the alternatives to the modal framework, and three chapters discuss concrete approaches, namely I/O logic, the imperativist approach [22], and the algebraic conceptual implication structures [32].", "startOffset": 216, "endOffset": 220}, {"referenceID": 61, "context": "[63] proposed abstract normative systems as common framework for comparing and analysing these new proposals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[63] add two notions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "Definition 1 (Universe L [63]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 61, "context": "Definition 2 (ANS \u3008L,N\u3009 [63]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 61, "context": "Definition 3 (Deontic operation \u00a9 [63]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "18 Of course this list is non-exhaustive as there are further alternative candidates for a new standard, such as nonmonotonic logic [24] or deontic update semantics [62].", "startOffset": 132, "endOffset": 136}, {"referenceID": 60, "context": "18 Of course this list is non-exhaustive as there are further alternative candidates for a new standard, such as nonmonotonic logic [24] or deontic update semantics [62].", "startOffset": 165, "endOffset": 169}, {"referenceID": 61, "context": "Definition 4 (Eight deontic operations [63]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "At least since the work of Horty [24], nonmonotonic techniques have been used to deal with reasoning in the context of dilemmas, CTD reasoning, and defeasible norms:", "startOffset": 33, "endOffset": 37}, {"referenceID": 58, "context": "\u2013 Defeasible deontic logic is concerned with violations and exceptions [60,44].", "startOffset": 71, "endOffset": 78}, {"referenceID": 43, "context": "\u2013 Defeasible deontic logic is concerned with violations and exceptions [60,44].", "startOffset": 71, "endOffset": 78}, {"referenceID": 36, "context": "Propositional Input/Output Logic As explained by Makinson and van der Torre[37], propositional I/O logic establishes a relatively simple setting, abstracting from important aspects of deontic reasoning, such as CTD reasoning or permissions.", "startOffset": 75, "endOffset": 79}, {"referenceID": 47, "context": "19 To give an intuitive example of a CTD, we report the so-called dog-sign example by Prakken and Sergot [49] already hinted at in the introduction: \u201cSuppose that: there", "startOffset": 105, "endOffset": 109}, {"referenceID": 36, "context": "Definition 5 (out [37]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 60, "context": "\u201d The most we can say is that a norm is \u201caccepted\u201d by a normative system [62], or \u201credundant\u201d in a normative system [61].", "startOffset": 73, "endOffset": 77}, {"referenceID": 59, "context": "\u201d The most we can say is that a norm is \u201caccepted\u201d by a normative system [62], or \u201credundant\u201d in a normative system [61].", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "First, as already explained above, the framework is based on the idea that norms do not have truth values, known as J\u00f6rgensen\u2019s dilemma in the deontic logic literature [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 36, "context": "A set of conditional norms is thus seen as a transformation device, and the task of logic is to act as its \u201csecretarial assistant\u201d [37].", "startOffset": 131, "endOffset": 135}, {"referenceID": 34, "context": "21 Makinson and van der Torre [35] consider three kinds of permissive norms, namely negative, positive, and static positive permission.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Following [16], from now on we use \u2018\u2190\u2019 in place of \u2018\u2192\u2019, and say that L0 is true if L1, .", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Given an extended logic program P we identify its answer sets [17] as EXT (P ).", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "As put forward by Boella and van der Torre [6], one of the roles of permissions is to undercut obligations.", "startOffset": 43, "endOffset": 46}, {"referenceID": 16, "context": "The if direction is trivial while the only if can be proven by showing that every application of the immediate consequence operator T (as defined by [17]) can be encoded into an application of d\u00b7e (Definition 8).", "startOffset": 149, "endOffset": 153}, {"referenceID": 42, "context": "We now recall how to encode (metalevel) preference relations, which define a priority between LP rules into (object-level) extended logic programs [43].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The proposed algorithm differs from CILP [12] in how priorities are encoded into the ANN, and it does not assume identity.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "[12] show that any extended logic program can be encoded into an ANN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": ", [23]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 54, "context": "As discussed, for instance, by Stenning and van Lambalgen [56], there is already a neural implementation for simple LP.", "startOffset": 58, "endOffset": 62}, {"referenceID": 65, "context": "Constraint LP is a more expressive logic which includes the Event Calculus, and is required for modelling, among other things, all but the simplest reasoning in the processing of time and causality in narrative discourse [67].", "startOffset": 221, "endOffset": 225}, {"referenceID": 40, "context": "\u2019s approach [41], an expert\u2019s reasons for some of the training examples can be used to try and guide the search for hypotheses, in a way similar to our use of background knowledge [14].", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "\u2019s approach [41], an expert\u2019s reasons for some of the training examples can be used to try and guide the search for hypotheses, in a way similar to our use of background knowledge [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 57, "context": "Stenning and van Lambalgen [59] argue that systems that use extension sets to capture the meanings of predicates\u2014at least when those systems are used for cognitive modelling\u2014necessarily rely for their foundations on intensional systems that can capture the interplay of motivations of the reasoner (desires, purposes, goals, preferences, .", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "This article aims to achieve two goals: to show that probability is not the only way of dealing with uncertainty (and even more, that there are kinds of uncertainty which are for principled reasons not addressable with probabilistic means); and to provide evidence that logicbased methods can well support reasoning with uncertainty. For the latter claim, two paradigmatic examples are presented: Logic Programming with Kleene semantics for modelling reasoning from information in a discourse, to an interpretation of the state of affairs of the intended model, and a neural-symbolic implementation of Input/Output logic for dealing with uncertainty in dynamic normative contexts.", "creator": "LaTeX with hyperref package"}}}