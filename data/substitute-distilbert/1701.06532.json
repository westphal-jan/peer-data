{"id": "1701.06532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "ENIGMA: Efficient Learning-based Inference Guiding Machine", "abstract": "enigma is a evidence - based method for guiding proper morphological selection in saturation - based theorem publishing. clauses from many proof searches are classified as positive relative negative based on their participation in the proofs. an efficient classification model automatically trained on this data, using fast feature - based characterization of the clauses. the learned model is then tightly linked with the core prover and used as a basis illustrating a statistical rigorous evaluation heuristic that provides fast ranking of all generated clauses. the approach is evaluated whenever the e prover after the casc 2016 aim benchmark, showing a large selection of e's performance.", "histories": [["v1", "Mon, 23 Jan 2017 18:03:52 GMT  (22kb)", "http://arxiv.org/abs/1701.06532v1", "Submitted to LPAR 2017"]], "COMMENTS": "Submitted to LPAR 2017", "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["jan jakub\\r{u}v", "josef urban"], "accepted": false, "id": "1701.06532"}, "pdf": {"name": "1701.06532.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jakubuv@gmail.com", "josef.urban@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n06 53\n2v 1\n[ cs\n.L O\n] 2\n3 Ja\nn 20\n17"}, {"heading": "1 Introduction: Theorem Proving and Learning", "text": "State-of-the-art resolution/superposition automated theorem provers (ATPs) such as Vampire [15] and E [20] are today\u2019s most advanced tools for general reasoning across a variety of mathematical and scientific domains. The stronger the performance of such tools, the more realistic become tasks such as full computer understanding and automated development of complicated mathematical theories, and verification of software, hardware and engineering designs. While performance of ATPs has steadily grown over the past years due to a number of human-designed improvements, it is still on average far behind the performance of trained mathematicians. Their advanced knowledge-based proof finding is an enigma, which is unlikely to be deciphered and programmed completely manually in near future.\nOn large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2]. Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7]. Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5]. An obvious next step is to implement efficient learning-based clause selection also inside the strongest superposition-based ATPs.\nIn this work, we introduce ENIGMA \u2013 Efficient learNing-based Internal Guidance MAchine for state-of-the-art saturation-based ATPs. The method applies fast machine learning algorithms to a large number of proofs, and uses the trained classifier together with simpler heuristics to evaluate the millions of clauses generated during the resolution/superposition proof search. This way, the theorem prover automatically takes into account thousands of previous successes and failures that it has seen in previous problems, similarly to trained humans. Thanks to a carefully chosen efficient learning/evaluation method and its tight integration with the core ATP (in our case the E prover), the penalty for this ubiquitous knowledge-based internal proof guidance is very low. This in turn very significantly improves the performance of E in terms of the number of solved problems in the CASC 2016 AIM benchmark [21]."}, {"heading": "2 Preliminaries", "text": "We use N to denote the set of natural numbers including 0. When S is a finite set then |S| denotes its size, and Sn where n P N is the n-ary Cartesian product of S, that is, the set of all vectors of size n with members from S. When x P Sn then we use notation xris to denote its i-th member, counting indexes from 1. We use xT to denote the transposed vector.\nMultiset M over a set S is represented by a total function from S to N, that is, Mpsq is the count of s P S in M . The union M1 YM2 of two multisets M1 and M2 over S is the multiset represented by function pM1 Y M2qpsq \u201c M1psq ` M2psq for all s P S. We use the notation ts1 \u00de\u00d1 n1, . . . , sk \u00de\u00d1 nku to describe a multiset, omitting the members with count 0.\nWe assume a fixed first-order theory with stable symbol names, and denote \u03a3 its signature, that is, a set of symbols with assigned arities. We use L to range over the set of all first-order literals (Literal), C to range over the set of all first-order clauses (Clause). Finally, we use C to range over sets of clauses (Clauses)."}, {"heading": "3 Training Clause Classifiers", "text": "There are many different machine learning methods, with different function spaces they can explore, different training and evaluation speeds, etc. Based on our previous experiments with premise selection and with guiding leanCoP, we have decided to choose a very fast and scalable learning method for the first ENIGMA instantiation. While more expressive learning methods usually lead to stronger single-strategy ATP results, very important aspects of our domain are that (i) the learning and proving evolve together in a feedback loop [23] where fast learning is useful, and (ii) combinations of multiple strategies \u2013 which can be provided by learning in different ways from different proofs \u2013 usually solve much more problems than the best strategy.\nAfter several experiments, we have chosen LIBLINEAR: open source library [4] for large-scale linear classification. This section describes how we use LIBLINEAR to train a clause classifier to guide given clause selection. Section 3.1 describes how training examples can be obtained from ATP runs. Section 3.2 describes how clauses are represented as fixed-length feature vectors. Finally, Section 3.3 describes how to use LIBLINEAR to train a clause classifier."}, {"heading": "3.1 Extracting Training Examples from ATP Runs", "text": "Suppose we run a saturation-based ATP to prove a conjecture \u03d5 in theory T . When the ATP successfully terminates with a proof, we can extract training examples from this particular proof search as follows. We collect all the clauses that were selected as given clauses during the proof search. From these clauses, those which appear in the final proof are classified as positives while the remaining given clauses as negative. This gives us two sets of clauses, positive clauses C\u2018 and negative clauses Ca.\nRe-running the proof search using the information pC\u2018,Caq to prefer clauses from C\u2018 as given clauses should significantly shorten the proof search. The challenge is to generalize this knowledge to be able to prove new problems which are in some sense similar. To achieve that, the positive and negative clauses extracted from proof runs on many related problems are combined and learned from jointly."}, {"heading": "3.2 Encoding Clauses by Features", "text": "In order to use LIBLINEAR for linear classification (Section 3.3), we need to represent clauses as finite feature vectors. For our purposes, a feature vector x representing a clause C is a fixed-\nlength vector of natural numbers whose i-th member xris specifies how often the i-th feature appears in the clause C.\nSeveral choices of clause features are possible [14], for example sub-terms, their generalizations, or paths in term trees. In this work we use term walks of length 3 as follows. First we construct a feature vector for every literal L in the clause C. We write the literal L as a tree where nodes are labeled by the symbols from \u03a3. In order to deal with possibly infinite number of variables and Skolem symbols, we substitute all variables and Skolem symbols with special symbols. We count for each triple of symbols ps1, s2, s3q P \u03a3\n3, the number of directed node paths of length 3 in the literal tree, provided the trees are oriented from the root. Finally, to construct the feature vector of clause C, we sum the vectors of all literals L P C.\nMore formally as follows. We consider a fixed theory T , hence we have a fixed signature \u03a3. We extend \u03a3 with 4 special symbols for variables (f), Skolem symbols (d), positive literals (\u2018), and negative literals (a). A feature \u03c6 is a triple of symbols from \u03a3. The set of all features is denoted Feature, that is, Feature \u201c \u03a33. Clause (or literal) features \u03a6 is a multiset of features, thus recording for each feature how many times it appears in a literal or a clause. We use \u03a6 to range over literal/clause features and the set of all literal/clause features (that is, feature multisets) is denoted Features. Recall that we represent multisets as total functions from Feature to N. Hence every member \u03a6 P Features is a total function of the type \u201cFeatures \u00d1 N\u201d and we can write \u03a6p\u03c6q to denote the count of \u03c6 in \u03a6.\nNow it is easy to define function features of the type \u201cLiteral \u00d1 Features\u201d which extracts features \u03a6 from a literal L. For a literal L, we construct a rooted feature tree with nodes labeled by the symbols from \u03a3. The feature tree basically corresponds to the tree representing literal L with the following exceptions. The root node of the tree is labeled by \u2018 iff L is a positive literal, otherwise it is labeled by a. Furthermore, all variable nodes are labeled by the symbol f and all nodes corresponding to Skolem symbols are labeled by the symbol d.\nExample 1. Consider the following equality literal L1 : fpx, yq \u201c gpsko1, sko2pxqq with Skolem symbols sko1 and sko2, and with variables x and y. In Figure 1, the tree representation of L1 is depicted on the left, while the corresponding feature tree used to collect features is shown on the right.\nFunction features constructs the feature tree of a literal L and collects all directed paths of length 3. It returns the result as a feature multiset \u03a6.\nExample 2. For literal L2 : P pxq we obtain featurespL2q \u201c tp\u2018, P,fq \u00de\u00d1 1u. For literal L3 : Qpx, yq we have featuresp Qpx, yqq \u201c tpa, Q,fq \u00de\u00d1 2u. Finally, for literal L1 from\nExample 1 we obtain the following multiset.\nt p\u2018,\u201c, fq \u00de\u00d1 1 , p\u2018,\u201c, gq \u00de\u00d1 1 , p\u201c, f,fq \u00de\u00d1 2 , p\u201c, g,dq \u00de\u00d1 2 , pg,d,fq \u00de\u00d1 1 u\nFinally, the function features is extended to clauses (features : Clause \u00d1 Features) by multiset union as featurespCq \u201c \u0164\nLPC featurespLq."}, {"heading": "3.2.1 A Technical Note on Feature Vector Representation", "text": "In order to use LIBLINEAR, we transform the feature multiset \u03a6 to a vector of numbers of length |Feature|. We assign a natural index to every feature and we construct a vector whose i-th member contains the count \u03a6p\u03c6q where i is the index of feature \u03c6. Technically, we construct a bijection sym between \u03a3 and t0, . . . , |\u03a3|\u00b41u which encodes symbols by natural numbers. Then we construct a bijection between Feature and t1, . . . , |Feature|u which encodes features by numbers1. Now it is easy to construct a function vector which translates \u03a6 to a vector from N |Feature| as follows:\nvectorp\u03a6q \u201c x such that xrcodep\u03c6qs \u201c \u03a6p\u03c6q for all \u03c6 P Feature\n3.3 Training Clause Classifiers with LIBLINEAR\nOnce we have the training examples pC\u2018,Caq and encoding of clauses by feature vectors, we can use LIBLINEAR to construct a classification model. LIBLINEAR implements the function train of the type \u201cClauses\u02c6Clauses \u00d1 Model\u201d which takes two sets of clauses (positive and negative examples) and constructs a classification model. Once we have a classification model M \u201c trainpC\u2018,Caq, LIBLINEAR provides a function predict of the type \u201cClause\u02c6Model \u00d1 t\u2018,au\u201d which can be used to predict clause classification as positive (\u2018) or negative (a).\nLIBLINEAR supports several classification methods, but we have so far used only the default solver L2-SVM (L2-regularized L2-loss Support Vector Classification) [3]. Using the functions from the previous section, we can translate the training examples pC\u2018,Caq to the set of instance-label pairs pxi, yiq, where i P t1, . . . , |C \u2018| ` |Ca|u, xi P N |Feature|, yi P t\u2018,au. A training clause Ci is translated to the feature vector xi \u201c vectorpfeaturespCiqq and the corresponding yi is set to \u2018 if Ci P C \u2018 or to a if Ci P C a. Then, LIBLINEAR solves the following optimization problem:\nmin w\n`1\n2 wTw` c\nl \u00ff\ni\u201c1\n\u03bepw,xi, yiq \u02d8\nfor w P R|Feature|, where c \u0105 0 is a penalty parameter and \u03be is the following loss function.\n\u03bepw,xi, yiq \u201c maxp1\u00b4 y 1 iw Txi, 0q 2 where y1i \u201c\n#\n1 iff y1 \u201c \u2018\n\u00b41 otherwise\nLIBLINEAR implements a coordinate descend method [8] and a trust region Newton method [17]. The model computed by LIBLINEAR is basically the vector w obtained by solving the above optimization problem. When computing the prediction for a clause C, the clause is translated to the corresponding feature vector x \u201c vectorpfeaturespCqq and LIBLINEAR classifies C as positive (\u2018) iff wTx \u0105 0. Hence we see that the prediction can be computed in time Opmaxp|Feature|, lengthpCqqq where lengthpCq is the length of clause C (number of symbols).\n1 We use codep\u03c6q \u201c symp\u03c6r1sq \u00a8 |\u03a3| 2 ` symp\u03c6r2sq \u00a8 |\u03a3| ` symp\u03c6r3sq ` 1."}, {"heading": "4 Guiding the Proof Search", "text": "Once we have a LIBLINEAR model (classifier) M, we construct a clause weight function which can be used inside the ATP given-clause loop to evaluate the generated clauses. As usual, clauses with smaller weight are selected before those with a higher weight. First, we define the function predict which assigns a smaller number to positively classified clauses as follows:\npredict-weightpC,Mq \u201c\n#\n1 iff predictpC,Mq \u201c \u2018\n10 otherwise\nIn order to additionally prefer smaller clauses to larger ones, we add the clause length to the above predicted weight. We use lengthpCq to denote the length of C counted as the number of symbols. Furthermore, we use a real-valued parameter \u03b3 to multiply the length as follows.\nweightpC,Mq \u201c \u03b3 \u00a8 lengthpCq ` predict-weightpC,Mq\nThis scheme is designed for the E automated prover which uses clause evaluation functions (CEFs) to select the given clause. A clause evaluation function CEF is a function which assigns a real weight to a clause. The unprocessed clause with the smallest weight is chosen to be the given clause. E allows combining several CEFs to jointly guide the proof search. This is done by specifying a finite number of CEFs together with their frequencies as follows: pf1 \u2039 CEF 1, . . . , fk \u2039 CEF kq. Each frequency fi denotes how often the corresponding CEF i is used to select a given clause in this weighted round-robin scheme. We have implemented learning-based guidance as a new CEF given by the above weight function. We can either use this new CEF alone or combine it with other CEFs already defined in E."}, {"heading": "5 Experimental Evaluation", "text": "We use the AIM2 category of the CASC 2016 competition for evaluation. This benchmark fits our needs as it targets internal guidance in ATPs based on training and testing examples. Before the competition, 1020 training problems were provided for the training of ATPs, while additional 200 problems were used in the competition. Prover9 proofs were provided along with all the training problems. Due to several interesting issues,3 we have decided not to use the training Prover9 proofs yet and instead find as many proofs as possible by a single E strategy.\nUsing fast preliminary evaluation, we have selected a strong E4 strategy S0 (see Appendix A) which can by itself solve 239 training problems with a 30 s timeout. For comparison, E\u2019s auto-schedule mode (using optimized strategy scheduling) can solve 261 problems. We train a clause classifier model M0 (Section 3) on the 239 proofs and then run E enhanced with the classifier M0 in different ways to obtain even more training examples. Either we use the classifier CEF based on M0 (i.e., function weightpC,M0q from Section 4) alone, or combine it with the CEFs from S0 by adding weightpC,M0q to S0 with a grid of frequencies ranging over {1,5,6,7,8,9,10,15,20,30,40,50}. Furthermore, every combination may be run with a different value of the parameter \u03b3 P t0, 0.1, 0.2, 0.4, 0.7, 1, 2, 4, 8u of the function weightpC,M0q. All the methods are run with 30 seconds time limit, leading to the total of 337 solved training problems. As expected, the numbers of processed clauses and the solving times on the previously solved problems are typically very significantly decreased when using weightpC,M0q. This is a good\n2AIM is a long-term project on proving open algebraic conjectures by Kinyon and Veroff. 3E.g., different term orderings, rewriting settings, etc., may largely change the proof search. 4We use E 1.9 and Intel Xeon 2.6GHz workstation for all experiments.\nsign, however, the ultimate test of ENIGMA\u2019s capability to learn and generalize is to evaluate the trained strategies on the testing problems. This is done as follows.\nOn the 337 solved training problems, we (greedily) find that 4 strategies are needed to cover the whole set. The strongest strategy is our classifier weightpC,M0q alone with \u03b3 \u201c 0.2, solving 318 problems. Another 15 problems are added by combining S0 with the trained classifier using frequency 50 and \u03b3 \u201c 0.2. Three problems are contributed by S0 and two by the trained classifier alone using \u03b3 \u201c 0. We take these four strategies and use only the proofs they found to train a new enhanced classifier M1. The proofs yield 6821 positive and 219012 negative examples. Training of M1 by LIBLINEAR takes about 7 seconds \u2013 2 seconds for feature extraction and 5 seconds for learning. The classifier evaluation on the training examples takes about 6 seconds and reaches 97.6% accuracy (ratio of the correctly classified clauses).\nThis means that both the feature generation and the model evaluation times per clause are at the order of 10 microseconds. This is comparable to the speed at which clauses are generated by E on our hardware and evaluated by its built-in heuristics. Our learning-based guidance can thus be quickly trained and used by normal users of E, without expensive training phase or using multiple CPUs or GPUs for clause evaluation.\nThen we use the M1 classifier to attack the 200 competition problems using 180 s time limit as in CASC. We again run several strategies: both weightpC,M1q alone and combined with S0 with different frequencies and parameters \u03b3. All the strategies solve together 52 problems and only 3 of the strategies are needed for this. While S0 solves only 22 of the competition problems, our strongest strategy solves 41 problems, see Table 1. This strategy combines S0 with weightpC,M1q using frequency 30 and \u03b3 \u201c 0.2. 7 more problems are contributed by weightpC,M1q alone with \u03b3 \u201c 0.2 and 4 more problems are added by the E auto-schedule mode. For comparison, Vampire solves 47 problems (compared to our 52 proofs) in 3*180 seconds per problem (simulating 3 runs of the best strategies, each for 180 seconds)."}, {"heading": "5.1 Looping and Boosting", "text": "The recent work on the premise-selection task has shown that typically there is not a single optimal way how to guide proof search. Re-learning from new proofs as introduced by MaLARea and combining proofs and learners usually outperforms a single method. Since we are using a very fast classifier here, we can easily experiment with giving it more and different data.\nFirst such experiment is done as follows. We add the proofs obtained on the solved 52 competition problems to the training data obtained from the 337 solved training problems. Instead of immediately re-learning and re-running (as in the MaLARea loop), we however first boost all positive examples (i.e., clauses participating in the proofs) by repeating them ten times in the training data. This way, we inform the learner to more strongly avoid misclassifying the positive examples as negative, than the other way round. The resulting clasifier M2 has lower\noverall accuracy on all of the data (93% vs. 98% for the unboosted), however, its accuracy on the relatively rare positive data grows significantly, from 12.5% to 81.8%.\nRunning the most successful strategy using M2 instead of M1 indeed helps. In 180 s, it solves additional 5 problems (4 of them not solved by Vampire), all of them in less than 45 s. This raises ENIGMA\u2019s performance on the competition problems to 57 problems (in general in 600 s). Interestingly, the second most useful strategy (now using M2 instead of M1) which is much more focused on doing inferences on the positively classified clauses, solves only two of these new problems, but six times faster. It is clear that we can continue experimenting this way with ENIGMA for long time, producing quickly a large number of strategies that have quite different search properties. In total we have proved 16 problems unsolved by Vampire."}, {"heading": "6 Conclusions", "text": "The first experiments with ENIGMA are extremely encouraging. While the recent work on premise selection and on internal guidance for leanCoP indicated that large improvements are possible, this is the first practical and usable improvement of a state-of-the-art ATP by internal learning-based guidance on a large CASC benchmark. It is clear that a wide range of future improvements are possible: the learning could be dynamically used also during the proof search, training problems selected according to their similarity with the current problem,5 more sophisticated learning and feature characterization methods could be employed, etc.\nThe magnitude of the improvement is unusually big for the ATP field, and similar to the improvements obtained with high-level learning in MaLARea 0.5 over E-LTB (sharing the same underlying engine) in CASC 2013 [13]. We believe that this may well mark the arrival of ENIGMAs \u2013 efficient learning-based inference guiding machines \u2013 to the automated reasoning, as crucial and indispensable technology for building the strongest automated theorem provers."}, {"heading": "7 Acknowledgments", "text": "We thank Stephan Schulz for his open and modular implementation of E and its many features that allowed us to do this work. We also thank the Machine Learning Group at National Taiwan University for making LIBLINEAR openly available. This work was supported by the ERC Consolidator grant no. 649043 AI4REASON."}, {"heading": "A The E Prover Strategy Used in Experiments", "text": "The following fixed E strategy S0, described by its command line arguments, was used in the experiments:\n--definitional-cnf=24 --destructive-er-aggressive --destructive-er --prefer-initial-clauses -F1 --delete-bad-limit=150000000 --forward-context-sr -winvfreqrank -c1 -Ginvfreq -WSelectComplexG --oriented-simul-paramod -tKBO6 -H(1*ConjectureRelativeSymbolWeight(SimulateSOS,0.5,100,100,100,100,1.5,1.5,1),\n4*ConjectureRelativeSymbolWeight(ConstPrio,0.1,100,100,100,100,1.5,1.5,1.5), 1*FIFOWeight(PreferProcessed), 1*ConjectureRelativeSymbolWeight(PreferNonGoals,0.5,100,100,100,100,1.5,1.5,1), 4*Refinedweight(SimulateSOS,3,2,2,1.5,2))"}], "references": [{"title": "A learning-based fact selector for Isabelle/HOL", "author": ["J.C. Blanchette", "D. Greenaway", "C. Kaliszyk", "D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Hammering towards QED", "author": ["J.C. Blanchette", "C. Kaliszyk", "L.C. Paulson", "J. Urban"], "venue": "J. Formalized Reasoning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I. Guyon", "V. Vapnik"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Monte Carlo connection prover", "author": ["M. F\u00e4rber", "C. Kaliszyk", "J. Urban"], "venue": "CoRR, abs/1611.05990,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Global Conference on Artificial Intelligence, GCAI 2015, Tbilisi, Georgia, October 16-19, 2015, volume 36 of EPiC Series in Computing", "author": ["G. Gottlob", "G. Sutcliffe", "A. Voronkov", "editors"], "venue": "EasyChair,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "SEPIA: search for proofs using inferred automata", "author": ["T. Gransden", "N. Walkinshaw", "R. Raman"], "venue": "25th International Conference on Automated Deduction, Berlin,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C. Hsieh", "K. Chang", "C. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "In ICML, volume 307 of ACM International Conference Proceeding Series,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "BliStrTune: hierarchical invention of theorem proving strategies", "author": ["J. Jakubuv", "J. Urban"], "venue": "Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs and Proofs,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "FEMaLeCoP: Fairly efficient machine learning connection prover", "author": ["C. Kaliszyk", "J. Urban"], "venue": "Logic for Programming, Artificial Intelligence, and Reasoning - 20th International Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Machine learner for automated reasoning 0.4 and 0.5", "author": ["C. Kaliszyk", "J. Urban", "J. Vysko\u010dil"], "venue": "CoRR, abs/1402.2359,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Efficient semantic features for automated reasoning over large theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vysko\u010dil"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "MaLeS: A framework for automatic tuning of automated theorem provers", "author": ["D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Trust region newton method for logistic regression", "author": ["C. Lin", "R.C. Weng", "S.S. Keerthi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "leanCoP: lean connection-based theorem proving", "author": ["J. Otten", "W. Bibel"], "venue": "J. Symb. Comput.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "E - A Brainiac Theorem Prover", "author": ["S. Schulz"], "venue": "AI Commun.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "The 8th IJCAR automated theorem proving system competition - CASC-J8", "author": ["G. Sutcliffe"], "venue": "AI Commun.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "MaLARea SG1 - Machine Learner for Automated Reasoning with Semantic Guidance", "author": ["J. Urban", "G. Sutcliffe", "P. Pudl\u00e1k", "J. Vysko\u010dil"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "MaLeCoP: Machine learning connection prover", "author": ["J. Urban", "J. Vysko\u010dil", "P. \u0160t\u011bp\u00e1nek"], "venue": "editors, TABLEAUX,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "State-of-the-art resolution/superposition automated theorem provers (ATPs) such as Vampire [15] and E [20] are today\u2019s most advanced tools for general reasoning across a variety of mathematical and scientific domains.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "State-of-the-art resolution/superposition automated theorem provers (ATPs) such as Vampire [15] and E [20] are today\u2019s most advanced tools for general reasoning across a variety of mathematical and scientific domains.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 11, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 0, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 1, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 8, "context": "Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7].", "startOffset": 126, "endOffset": 141}, {"referenceID": 15, "context": "Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7].", "startOffset": 126, "endOffset": 141}, {"referenceID": 6, "context": "Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7].", "startOffset": 222, "endOffset": 225}, {"referenceID": 17, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 222, "endOffset": 233}, {"referenceID": 10, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 222, "endOffset": 233}, {"referenceID": 4, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 222, "endOffset": 233}, {"referenceID": 19, "context": "This in turn very significantly improves the performance of E in terms of the number of solved problems in the CASC 2016 AIM benchmark [21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "While more expressive learning methods usually lead to stronger single-strategy ATP results, very important aspects of our domain are that (i) the learning and proving evolve together in a feedback loop [23] where fast learning is useful, and (ii) combinations of multiple strategies \u2013 which can be provided by learning in different ways from different proofs \u2013 usually solve much more problems than the best strategy.", "startOffset": 203, "endOffset": 207}, {"referenceID": 3, "context": "After several experiments, we have chosen LIBLINEAR: open source library [4] for large-scale linear classification.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Several choices of clause features are possible [14], for example sub-terms, their generalizations, or paths in term trees.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "LIBLINEAR supports several classification methods, but we have so far used only the default solver L2-SVM (L2-regularized L2-loss Support Vector Classification) [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "LIBLINEAR implements a coordinate descend method [8] and a trust region Newton method [17].", "startOffset": 49, "endOffset": 52}, {"referenceID": 16, "context": "LIBLINEAR implements a coordinate descend method [8] and a trust region Newton method [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "5 over E-LTB (sharing the same underlying engine) in CASC 2013 [13].", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "ENIGMA is a learning-based method for guiding given clause selection in saturationbased theorem provers. Clauses from many proof searches are classified as positive and negative based on their participation in the proofs. An efficient classification model is trained on this data, using fast feature-based characterization of the clauses . The learned model is then tightly linked with the core prover and used as a basis of a new parameterized evaluation heuristic that provides fast ranking of all generated clauses. The approach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing a large increase of E\u2019s performance.", "creator": "easychair.cls-3.4"}}}