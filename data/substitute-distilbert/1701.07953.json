{"id": "1701.07953", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "The Price of Differential Privacy for Online Learning", "abstract": "investigators design differentially private algorithms for preventing problem of online linear optimization in the mutual information and bandit settings with optimal $ \\ low { o } ( \\ sqrt { t } ) $ regret bounds. in the full - information setting, our results demonstrate that $ ( \\ epsilon, \\ delta ) $ - risk privacy limits be ensured for free - in particular, the marginal bounds scale as $ o ( \\ sqrt {? } ) + \\ tilde { { } \\ big ( \\ frac { 1 } { \\ epsilon } \\ log \\ frac { 1 } { \\ delta } \\ big ) $. for bandit linear optimization, and as a summary case, under non - stochastic multi - state bandits, whose proposed variant concerns a regret of $ o \\ big ( \\ frac { \\ sqrt { j \\ * t } } { \\ epsilon } \\ g \\ frac { 1 } { \\ brace } \\ big ) $, while the previously best known bound was $ \\ normal { o } \\ lazy ( \\ frac { 4 ^ { \\ frac { 3 } { 4 } } } { \\ epsilon } \\ beta ) $.", "histories": [["v1", "Fri, 27 Jan 2017 06:17:14 GMT  (26kb)", "http://arxiv.org/abs/1701.07953v1", null], ["v2", "Tue, 13 Jun 2017 21:25:12 GMT  (36kb)", "http://arxiv.org/abs/1701.07953v2", "To appear in the Proceedings of the 34th International Conference on Machine Learning (ICML), Sydney, Australia, 2017"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["naman agarwal", "karan singh"], "accepted": true, "id": "1701.07953"}, "pdf": {"name": "1701.07953.pdf", "metadata": {"source": "CRF", "title": "The Price of Differential Privacy For Online Learning", "authors": ["Naman Agarwal", "Karan Singh"], "emails": ["namana@cs.princeton.edu,", "karans@cs.princeton.edu,"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n07 95\n3v 1\n[ cs\n.L G\n] 2\n7 Ja\nn 20\n\u221a T )1 regret bounds. In the full-information\nsetting, our results demonstrate that (\u03b5, \u03b4)-differential privacy may be ensured for free \u2013 in particular, the regret bounds scale as O( \u221a T ) + O\u0303 (\n1 \u03b5 log 1 \u03b4\n)\n. For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O (\u221a\nT log T \u03b5 log 1 \u03b4\n) , while the previously best known bound was O\u0303 ( T 3 4\n\u03b5\n)\n."}, {"heading": "1 Introduction", "text": "In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees, that is no distributional assumption is made on the environment. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential. While statistical (batch) learning algorithms produce a single predictor as the output, an online learning algorithmmust output a predictor for every time step. As a result, ensuring differential privacy of every data point for online learning is challenging because the change in the data point supplied to the algorithm at the tth time step alters the prediction for all time steps that follow. In this paper, we design differentially private algorithms for online linear optimization with near-optimal regret, both in the full information and partial information (bandit) settings. This result improves the best known regret bounds for a number of important online learning problems \u2013 including prediction from expert advice and non-stochastic multi-armed bandits."}, {"heading": "1.1 Full-Information Setting: Differential Privacy for Free", "text": "For the full-information setting, we design (\u03b5, \u03b4)-differentially private algorithms with regret bounds that scale as O( \u221a T ) + O\u0303 (\n1 \u03b5 log 1 \u03b4\n)\n, resolving an open question posed in [TS13]. A decomposition of the bound on the regret of this form implies that when \u03b5 \u2265 1\u221a\nT we have no\nadditional regret, i.e. differential privacy is free. Moreover even when \u03b5 \u2264 1\u221a T we show a sub constant regret per round as compared to the vacuous constant regret per round guarantee provided by existing results.\n\u2217namana@cs.princeton.edu, Computer Science, Princeton University \u2020karans@cs.princeton.edu, Computer Science, Princeton University 1Here the O\u0303(\u00b7) notation hides polylog(T ) factors.\nAs a concrete example, for the setting of prediction from expert advice with N experts and\nT rounds of play, we show that it is possible to achieve O\u0303 (\u221a\nN logN \u03b5 log 1 \u03b4 +\n\u221a T logN ) regret\nwhile promising (\u03b5, \u03b4)-differential privacy. When \u03b5 = \u03c9 ( \u221a\nN T log 1 \u03b4\n)\n, the leading term of the\nregret bound for the proposed algorithm is same as that for the best possible non-private regret bound of O( \u221a T logN) (upto polylog(T ) factors). In contrast, the previously best known regret bound for the problem was O\u0303 (\u221a\nNT \u03b5\n)\n[TS13]. Our result improves on the previous known both\nin terms of dependency on the differential privacy parameter \u03b5 and the number of experts N . While the previous works[JKT12, TS13] approach the question of regret minimization using the follow-the-regularized-leader (FTRL) paradigm, our algorithm is based on the follow-theperturbed-leader (FTPL) algorithm [KV05], where we demonstrate that the noise required to achieve low regret also ensures differential privacy and vice versa. This novel observation permits our analysis to decompose the regret bound in the desired form."}, {"heading": "1.2 Bandit Feedback: Reduction to the Non-private Setting", "text": "In the partial-information setting of bandit feedback, the online learning algorithm only gets to observe the loss (or the reward) of the prediction it prescribed. We outline a reduction technique that translates a non-private bandit algorithm to a differentially private bandit algorithm, while retaining the O\u0303( \u221a T ) dependency of the regret bound on the number of rounds of play for a number of important problems. This allows us to derive the first (\u03b5, \u03b4)-differentially private algorithm for bandit linear optimization, using the algorithm for the non-private problem from [AHR12]. This positively answers a question from [TS13] asking if O\u0303( \u221a T ) regret is attainable for differentially private linear bandits. An important case of the general bandit linear optimization framework is the non-stochastic multi-armed bandits problem[BCB+12], with applications for website optimization, personalized medicine, advertisement placement and recommendation systems. Here, the proposed (\u03b5, \u03b4)differentially private algorithm (for N arms) enjoys a regret of O\u0303 (\u221a\nNT logN \u03b5 log 1 \u03b4\n)\n, improving\non the previously best attainable regret of O\u0303 ( NT 3 4\n\u03b5\n)\n[TS13]."}, {"heading": "2 Model and Preliminaries", "text": "This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.\nFull information Setting: Online linear optimization [H+16, SS11] involves repeated decision making over T rounds of play. At the beginning of every round (say round t), the algorithm chooses a point in xt \u2208 X , where X \u2286 RN is a (compact) convex set. Subsequently, it observes the loss lt \u2208 Y \u2286 RN and suffers a loss of \u3008lt, xt\u3009. The measure of success of such an algorithm, after T rounds of play, is defined though regret, defined as\nRegret(T ) = E\n[ T \u2211\nt=1\n\u3008lt, xt\u3009 \u2212min x\u2208K\nT \u2211\nt=1\n\u3008lt, x\u3009 ]\nwhere the expectation is over the randomness of the algorithm. In particular, achieving a sub-linear regret (o(T )) corresponds to doing almost as good (averaging across T rounds) as the fixed decision with the least loss in hindsight. In the non-private setting, a number of algorithms have been devised to achieve O( \u221a T ) regret, with additional dependence on the properties of the specific decision X and loss Y sets. (See [H+16] for a survey of results.) An important special case of the above framework is prediction with expert advice. Here the underlying decision is the simplex X = \u2206N = {x \u2208 Rn : xi \u2265 0, \u2211n\ni=1 xi = 1} and the loss vectors are constrained to the unit cube Y = {lt \u2208 RN : \u2016lt\u2016\u221e \u2264 1}. The role of the algorithm in this case can be interpreted as picking one of the N experts to follow it \u2208 [N ] for every round of play. Also lt \u2208 RN can be viewed as the loss on round t corresponding to each expert. In each round, the algorithm suffers a loss that equals the loss of the expert it chose to follow in that round.\nPartial Information Setting: In the setting of bandit feedback, the critical difference is that the algorithm only gets to observe the value \u3008lt, xt\u3009, in contrast to the complete loss vector lt \u2208 RN in the full information scenario. Therefore, the only feedback the algorithm receives is the value of the loss it incurs for the decision it takes. This makes designing algorithms for this feedback model challenging. Nevertheless for the general problem of bandit online optimization, [AHR08] introduced a computationally efficient algorithm that achieves an optimal dependence of the incurred regret of O( \u221a T ) on the number of rounds of play.\nThe non-stochastic multi-armed bandit problem is the bandit version of the prediction with expert advice framework \u2013 namely, in every round, the algorithm only observes the loss it incurs.\nDifferential Privacy: Differential Privacy[DMNS06] is a rigorous framework for establishing guarantees on privacy loss, that admits a number of desirable properties such as graceful degradation of guarantees under composition and robustness to linkage acts [DR+14].\nDefinition 2.1 ((\u03b5, \u03b4)-Differential Privacy). A randomized online learning algorithm A on the action set X and the loss set Y is (\u03b5, \u03b4)-differentially private if for any two sequence of loss vectors L = (l1, . . . lT ) \u2286 YT and L\u2032 = (l\u20321, . . . l\u2032T ) \u2286 YT differing in at most one vector \u2013 that is to say \u2203t0 \u2208 [T ], \u2200t \u2208 [T ]\u2212 {t0}, lt = l\u2032t \u2013 for all S \u2286 X T , it holds that\nP(A(L) \u2208 S) \u2264 e\u03b5P(A(L\u2032) \u2208 S) + \u03b4 (1)\nFor an algorithm to satisfy the above stated definition is to certify that it is not possible to meaningfully infer the presence or absence of a specific loss vector from the sequence of outputs produced by the algorithm.\nNotation: We define \u2016Y\u2016p = max{\u2016lt\u2016p : lt \u2208 Y}, \u2016X\u2016p = max{\u2016x\u2016p : x \u2208 X}, and M = maxl\u2208Y,x\u2208X |\u3008l, x\u3009|, where \u2016 \u00b7 \u2016p is the lp norm. By Holder\u2019s inequality, it is easy to see that M \u2264 \u2016Y\u2016p\u2016X\u2016q for all p, q \u2265 1 with 1p + 1q = 1."}, {"heading": "3 FTPL-based Algorithms: Differential Privacy for Free", "text": "In this section, we outline algorithms based on the follow-the-perturbed-leader template[KV05]. FTPL-based algorithms ensure low-regret by perturbing the cumulative sum of loss vectors with noise from a suitably chosen distribution. We show that the noise added in the process of FTPL is sufficient to ensure differential privacy. More concretely, for the full-information setting, we establish that the regret guarantees obtained scale as O( \u221a T ) + O\u0303(1\u03b5 log 1 \u03b4 ).\nAlgorithm 1 FTPL Template for OLO \u2013 A(D, T ) on the action set X , the loss set Y. 1: Initialize an empty binary tree B to compute differentially private estimates of\n\u2211t s=1 ls.\n2: Sample n10, . . . n \u2308log T \u2309 0 independently from D. 3: L\u03030 \u2190 \u2211\u2308log T\u2309 i=1 n i 0. 4: for t = 1 to T do 5: Choose xt = argminx\u2208X \u3008x, L\u0303t\u22121\u3009. 6: Observe the loss vector lt \u2208 Y, and suffer a loss of \u3008lt, xt\u3009. 7: (L\u0303t, B) \u2190 TreeBasedAggregation(lt, B, t,D, T ). 8: end for\nAlgorithm 2 TreeBasedAggregation(lt, B, t,D, T ) 1: (L\u0303\u2032t, B) \u2190 PrivateSum(lt, B, t,D, T ) \u2013 Algorithm 5 ([JKT12]) with the noise added at each\nnode \u2013 be it internal or leaf \u2013 sampled independently from the distribution D. 2: st \u2190 the binary representation of t as a string. 3: Find the minimum set S of already populated nodes in B that can compute \u2211ts=1 ls. 4: Define (and note) Q = |S| \u2264 \u2308log T \u2309. Define rt = \u2308log T \u2309 \u2212Q. 5: Sample n1t , . . . n rt t independently from D. 6: L\u0303t \u2190 L\u0303\u2032t + \u2211rt i=1 n i t. 7: Output: (L\u0303t, B)."}, {"heading": "3.1 Proof of (\u03b5, \u03b4)-Differential Privacy", "text": "To make formal claims about the quality of privacy, we ensure input differential privacy for the algorithm \u2013 that is, we ensure that the sequence of all partial sums of the loss vectors\n( \u2211t s=1 ls : t \u2208 [T ]) is (\u03b5, \u03b4)-differentially private. This certifies that the sequence of choices made by the algorithm (across all T rounds of play) (it : t \u2208 [T ]) is (\u03b5, \u03b4)-differentially private.\nTheorem 3.1 (Privacy Guarantees with Gaussian Noise). Choose any \u03c32 \u2265 \u2016Y\u2016 2 2\n\u03b52 log 2 T log2 log T\u03b4 .\nWhen Algorithm 1 A(D, T ) is run with D = N (0, \u03c32IN ), the following claims hold true: \u2022 Privacy: The sequence (L\u0303t : t \u2208 [T ]) is (\u03b5, \u03b4)-differentially private. \u2022 Distribution: \u2200t \u2208 [T ], L\u0303t \u223c N ( \u2211t s=1 ls, \u2308logT \u2309\u03c32IN ).\nProof. By Theorem 9 ([JKT12]), we have that the sequence (L\u0303\u2032t : t \u2208 [T ]) is (\u03b5, \u03b4)-differentially private. Now the sequence (L\u0303t : t \u2208 [T ]) is (\u03b5, \u03b4)-differentially private because differential privacy is immune to post-processing[DR+14].\nNote that the PrivateSum algorithm adds exactly |S| independent draws from the distribution D to \u2211ts=1 ls, where S is the minimum set of already populated nodes in the tree that can compute the required prefix sum. Due to Line 6, it is made certain that every prefix sum released is a sum of the true prefix sum and \u2308logT \u2309 independent draws from D.\nFinally, it suffices to note that the outputs of Algorithm 1 are strictly determined by the prefix sum estimates produced by TreeBasedAggregation. Again by post-processing theorem, (\u03b5, \u03b4)-differential privacy of Algorithm 1 is guaranteed."}, {"heading": "3.2 Regret Bounds for FTPL", "text": "While follow-the-perturbed-leader[KV05] has been typically analyzed subject to either uniform or expoenetial noise, recently [ALST14] presented the analysis of FTPL-based algorithms in the form of a general framework that accommodates varied noise structures. For a number of subcases of online linear optimization, with different X and Y, [ALST14] established that Gaussian noise achieves minimax optimal regret (upto multiplicative constants).\nWe utilize these results from [ALST14] to prove low-regret bounds for Algorithm 1. We make a couple of observations before applying these results. Firstly, Algorithm 1 calls the argmin oracle on the perturbed estimates of\n\u2211t s=1 ls, namely L\u0303t. Since these are distributed\nas L\u0303\u2032t \u223c N ( \u2211t s=1 ls, \u2308logT \u2309\u03c32IN), this is equivalent to executing Algorithm 1 ([ALST14]) with Gaussian smoothing with the smoothing parameter \u03b7 = \u03c3 \u221a logT . This realization allows us to measure regret with respect to the true loss vectors, in spite of the proposed algorithm operating on perturbed estimates of the prefix sums. Secondly, note that the perturbations added to the prefix sums will (almost surely) change every round, even while being drawn from a time-invariant distribution. In typical instantiations of the FTPL framework, the noise added is either same over time[KV05] or is sampled independently every round[HP05]. In our setting, neither of the statements hold \u2013 since randomness introduced at any internal node in the PrivateSum algorithm can be reused across multiple rounds \u2013 hence, from the point of view of the FTPL algorithm, the noises sampled across time steps demonstrate some non-zero, yet non-trivial correlations. However, it is crucial to observe that, for the analysis of [ALST14] (in fact, for [KV05] too) to succeed, it is sufficient that the noise is sampled from the same distribution (not necessarily independently) for all time steps.\nRemark 3.2. In fact, the requirement that the noise is sampled from the same distribution can be relaxed too. Using a different magnitude of distribution in each round corresponds to a time-variant smoothing parameter. The results from [ALST14] hold in that setting as well. We do not delve into this point here to ease the presentation.\nTheorem 3.3 (Prediction with Expert Advice.). For the setting of prediction with expert advice, X = \u2206N = {x \u2208 Rn : xi \u2265 0, \u2211n i=1 xi = 1} and Y = {lt \u2208 Rn : \u2016lt\u2016\u221e \u2264 1}. Choosing\n\u03c3 = max{ \u221a\nT log T , \u221a N \u03b5 logT log log T \u03b4 } and D = N (0, \u03c32IN ), we have\nRegretA(D,T )(T ) \u2264 O ( \u221a T logN +\n\u221a N logN\n\u03b5 log1.5 T log\nlogT\n\u03b4\n)\n(2)\nProof. Due to Theorem 8 ([ALST14]), it holds that\nRegretA(D,T )(T ) \u2264 \u221a logN\n(\n\u03c3 \u221a logT + T\n\u03c3 \u221a logT\n)\n(3)\nSubstituting the proposed value of \u03c3, we obtain the stated claim.\nTheorem 3.4 (OLO over Euclidean Balls.). For the setting of online linear optimization over euclidean balls, X = {x \u2208 Rn : \u2016x\u20162 \u2264 1} and Y = {lt \u2208 Rn : \u2016lt\u20162 \u2264 1}. Choosing \u03c3 = max{ \u221a\nT 2N log T , 1 \u03b5 logT log log T \u03b4 } and D = N (0, \u03c32IN ), we have\nRegretA(D,T )(T ) \u2264 O ( \u221a T +\n\u221a N\n\u03b5 log1.5 T log\nlogT\n\u03b4\n)\n(4)\nProof. Due to Theorem 11 ([ALST14]), it holds that\nRegretA(D,T )(T ) \u2264 \u03c3 \u221a N logT + T\n2\u03c3 \u221a N logT\n(5)\nSubstituting the proposed value of \u03c3, we obtain the stated claim.\nRemark 3.5. In this case \u2013 online linear optimization over euclidean balls, it is in fact possible to show that as long as \u221a\nT 2N log T \u2265 1\u03b5 logT log log T \u03b4 , the proposed algorithm achieves the exact\nminimax optimal regret. See Section 4.2 ([ALST14]) for more details.\nTheorem 3.6 (General OLO). Let \u2016X\u20162 = supx\u2208X \u2016x\u20162 and \u2016Y\u20162 = suplt\u2208Y \u2016lt\u20162. Choosing \u03c3 = max{\u2016Y\u20162 \u221a\nT\u221a N log T , \u221a N \u03b5 logT log log T \u03b4 } and D = N (0, \u03c32IN), we have\nRegretA(D,T )(T ) \u2264 O ( N 1 4 \u2016X\u20162\u2016Y\u20162 \u221a T + N\u2016X\u20162 \u03b5 log1.5 T log logT \u03b4 )\n(6)\nProof. As a consequence of Corollary 4 ([ALST14]) and Lemma 13 ([ALST14]), it is true that\nRegretA(D,T )(T ) \u2264 \u03c3 \u221a N logT\u2016X\u20162 + T \u2016X\u20162\u2016Y\u201622 2\u03c3 \u221a logT\n(7)\nSubstituting the proposed value of \u03c3, we obtain the stated claim.\nWhile Theorem 3.6 is valid for all instances of online linear optimization and achieves O( \u221a T )\nregret, it yields sub-optimal dependence on the dimension of the problem.\nAlgorithm 3 A\u2032(A,D) \u2013 Reduction to the Non-private Setting for Bandit Feedback Input: Online Algorithm A, Noise Distribution D. 1: for t = 0 to T do 2: Receive x\u0303t \u2208 X from A and output x\u0303t. 3: Receive a loss value \u3008lt, x\u0303t\u3009 from the adversary. 4: Sample Zt \u223c D. 5: Forward \u3008lt x\u0303t\u3009+ \u3008Zt, x\u0303t\u3009 as input to A. 6: end for"}, {"heading": "4 Bandit Feedback: Reduction to the Non-private Setting", "text": "We begin by describing an algorithmic reduction that takes as input a non-private bandit algorithm and translates it into an algorithm for (\u03b5, \u03b4)-differentially private bandit algorithm. The reduction works in a straight-forward manner by adding the requisite magnitude of Gaussian noise to ensure differential privacy. For the rest of this section for ease of exposition we will assume that both T and N are sufficiently large.\nTheorem 4.1 (Privacy Guarantees). Let it be that each loss vector lt is constrained to be within the set Y \u2286 RN , such that maxt,l\u2208Y | \u3008l,x\u0303t\u3009\u2016x\u0303t\u2016\u221e | \u2264 B. For D = N (0, \u03c3 2 IN ) where \u03c3 2 = B 2 \u03b52 log 2 1 \u03b4 , the sequence of outputs (x\u0303t : t \u2208 [T ]) produced by the Algorithm A\u2032(A,D) is (\u03b5, \u03b4)-differentially private.\nProof. Consider a pair of sequence of loss vectors that differ at exactly one time step \u2013 say L = (l1, . . . lt0 . . . , lT ) and L\n\u2032 = (l1, . . . , l\u2032t0 , . . . lT ). Since the prediction of produced by the algorithm at time step any time t can only depend on the loss vectors in the past (l1, . . . lt\u22121), it is clear that the distribution of the output of the algorithm for the first t0 rounds (x\u03031, . . . x\u0303t0) is unaltered. We claim that \u2200I \u2286 R, it holds that\nP(\u3008lt0 + Zt0 , x\u0303t0\u3009 \u2208 I) \u2264 e\u03b5P(\u3008l\u2032t0 + Zt0 , x\u0303t0\u3009 \u2208 I) + \u03b4\nBefore we justify the claim, let us see how this implies that desired statement. Too see this, note that conditioned on the value fed to the inner algorithm A at time t0, the distribution of all outputs produced by the algorithm are completely determined since the feedback to the algorithm at other time steps (discounting t0) stays the same (in distribution). By the above discussion, it is sufficient to demonstrate (\u03b5, \u03b4)-differential privacy for each input fed (as feedback) to the algorithm A.\nFor the sake of analysis, define lFictt as follows. If x\u0303t = 0, define l Fict t = 0 \u2208 RN . Else, define lFictt \u2208 RN to be such that (lFictt )i = \u3008lt,x\u0303t\u3009x\u0303i if and only if i = argmaxi\u2208[d]|x\u0303i| and 0 otherwise, where argmax breaks ties arbitrarily. Define l\u0303Fictt = l Fict t + Zt. Now note that \u3008l\u0303Fictt , x\u0303t\u3009 = \u3008lt x\u0303t\u3009+ \u3008Zt, x\u0303t\u3009. It suffices to establish that each l\u0303Fictt is \u03b5-differentially private. To argue for this, note that Gaussian mechanism[DR+14] ensures the same, since the l2 norm of l\u0303 Fict t is bounded by B.\nWe show that the regret of the overall algorithm A\u2032 of the true loss vectors is, in expectation, same as that of the regret of the inner algorithm A on some perturbed version of loss vectors. Theorem 4.2 (Noisy Online Optimization). Consider a loss sequence (l1 . . . lT ) and a convex set X . Define a perturbed version of the sequence as random vectors (l\u0303t : t \u2208 [T ]) as l\u0303t = lt +Zt where Zt is a random vector such that {Z1, . . . Zt} are independent and E[Zt] = 0 for all t \u2208 [T ].\nLet A be a full information (or bandit) online algorithm which outputs a sequence (x\u0303t \u2208 X : t \u2208 [T ]) and takes as input l\u0303t (respectively \u3008l\u0303t, x\u0303t\u3009) at time t. Let x\u2217 \u2208 K be a fixed point in the\nconvex set. Then we have that\nE{Zt}\n[\nEA\n[\nT \u2211\nt=1\n(\u3008lt, x\u0303t\u3009 \u2212 \u3008lt, x\u2217\u3009) ]] = E{Zt} [ EA [ T \u2211\nt=1\n( \u3008l\u0303t.x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]]\nWe prove the Theorem in the Appendix in Section A."}, {"heading": "4.1 Differentially Private Bandit Linear Optimization", "text": "To obtain \u03b5-differentially private algorithms for Bandit Linear Optimization, we use the SCRiBLe algorithm from[AHR12] as the inner algorithm A. Before we state this theorem, note that maxt,l\u2208Y | \u3008l,x\u0303t\u3009\u2016x\u0303t\u2016\u221e | \u2264 \u2016Y\u20161 by Holder\u2019s inequality. Therefore, with \u03c3 2 = \u2016Y\u20162 1 \u03b52 log 2 1 \u03b4 , choosing D = N (0, \u03c32IN ) ensures (\u03b5, \u03b4)-differential privacy. Theorem 4.3 (Bandit Linear Optimization). Let X \u2286 RN be a convex set. Fix loss vectors (l1, . . . lT ) such that maxt,x\u2208X |\u3008lt, x\u3009| \u2264 M . We have that Algorithm 3 when run with parameters D = N (0, \u03c32IN ) (with \u03c32 = \u2016Y\u2016 2 1 \u03b52 log 2 1\n\u03b4 ) and algorithm A = SCRiBLe[AHR12] with step parameter \u03b7 = \u221a\n\u03bd log T 2N2T (M2+\u03c32N\u2016X\u20162 2 ) we have the following guarantees that the regret of the\nalgorithm is bounded by\nO\n(\n\u221a T logT \u221a N2\u03bd(M2 + \u03c32N\u2016X\u201622) ) = O\n(\n\u221a\nT logT\n\u221a\nN2\u03bd\n( M2 + \u2016Y\u201621 \u03b52 log2 1 \u03b4 N\u2016X\u201622 )\n)\nBefore delving into the proof we collect some necessary lemmas/theorems. The following is a simple restatement of Theorem 5.1 in [AHR12].\nTheorem 4.4 (Theorem 5.1 in [AHR12]). Fix a loss sequence l1, . . . lT . Let X be a compact convex set and R be a \u03bd-self concordant barrier on X . Assume maxt,x\u2208X |\u3008lt, x\u3009| \u2264 M . Setting \u03b7 \u2264 14NM then the regret of the SCRiBLe algorithm is bounded by\nRegretSCRiBLe \u2264 2\u03b7 \u2211 N\u3008lt, xt\u30092 + \u03bd\u03b7\u22121 logT + 2M\nwhere (xt \u2208 X : t \u2208 [T ]) is the sequence of points played by the algorithm. We now prove Theorem 4.3.\nProof of Theorem 4.3. For the purpose of analysis we define the following pseudo loss vectors l\u0303t = lt + Zt, where by definition Z \u223c N (0, \u03c32IN ). Further note that the following (which is a direct consequence of Fact B.2 proved in the appendix) holds for Zt \u223c N (0, \u03c32IN )\nP(\u2016Zt\u201622 \u2265 10\u03c32N logT ) \u2264 1\nT 2\nTherefore taking a union bound we get that\nP(\u2203t \u2016Zt\u201622 \u2265 10\u03c32N logT ) \u2264 1\nT (8)\nSince the conditions of Theorem 4.2 are satisfied, we have that\nE[Regret] = E{Zt}\n[\nEA\n[\nT \u2211\nt=1\n( \u3008l\u0303t, x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]]\nTherefore we can analyze the second term as a proxy for true regret. We now wish to apply Theorem 4.4 on the term inside the expectation treating l\u0303t as the true losses. However,\nTheorem 4.4 does not apply to loss functions that can be arbitrarily large, therefore we condition the above expectation on the following event F = {\u2203t \u2016Zt\u201622 \u2265 10\u03c3N logT }. We have from (8) that P(F ) \u2264 1T . We now have that\nE[Regret] \u2264 E[Regret|F\u0304 ] + P(F )E[Regret|F ]\nSince the regret is always bounded by T we get that the second term above is at most 1. Therefore we will concern ourselves with bounding the first term above. For the rest of the section we will assume the conditioning on the event F\u0304 . First note that since the noise vectors Zt were independent to begin with they still remain conditionally independent even when conditioned on the event F\u0304 . The following two statements can be seen to follow easily.\n\u2200t E[Zt|F ] = 0 (9)\n\u2200t E[\u2016Zt\u201622|F\u0304 ] \u2264 E[\u2016Zt\u201622] = \u03c32N (10) It follows from Equation 9 that Theorem 4.2 applies even when the noise is sampled from N (0, \u03c32IN ) conditioned on the event F\u0304 . Therefore we have that\nE[Regret|F\u0304 ] = E{Zt} [ EA [ T \u2211\nt=1\n( \u3008l\u0303t, x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]\n\u2223 \u2223 \u2223 \u2223 F\u0304\n]\n(11)\nNow note that since due to the conditioning \u2016Zt\u20162 \u2264 10\u03c32N logT and therefore we have that\nL , maxt,x\u2208X |\u3008Zt, x\u3009| \u2264 4\u03c3\u2016X\u20162 \u221a N logT .\nIt can now be verified that \u03b7 \u2264 14NL . Therefore we can apply Theorem 4.4 obtain that\nE[Regret|F\u0304 ] = E{Zt} [ EA [ T \u2211\nt=1\n( \u3008l\u0303t, x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]\n\u2223 \u2223 \u2223 \u2223 F\u0304\n]\nEquation 11\n\u2264 E{Zt} [ 2\u03b7 \u2211 N2|\u3008l\u0303t, x\u0303t\u3009|2 + \u03bd\u03b7\u22121 logT + 2(M + L) \u2223 \u2223 \u2223\n\u2223\nF\u0304\n]\nTheorem 4.4\n\u2264 E{Zt} [ 2\u03b7 \u2211 N2(|\u3008lt, x\u0303t\u3009|2 + \u2016Zt\u201622\u2016x\u0303t\u201622) + \u03bd\u03b7\u22121 logT + 2(M + L) \u2223 \u2223 \u2223\n\u2223\nF\u0304\n]\n\u2264 2\u03b7TN2(M2 + \u03c32N\u2016X\u201622) + \u03bd\u03b7\u22121 logT + 8\u03c3\u2016X\u20162 \u221a n logT + 2M Equation 10 \u2264 O ( \u221a T logT \u221a N2\u03bd(M2 + \u03c32N\u2016X\u201622) )"}, {"heading": "4.2 Differentially Private Multi-Armed Bandits", "text": "To begin with, we note that maxt,l\u2208Y | \u3008l,x\u0303t\u3009\u2016x\u0303t\u2016\u221e | \u2264 \u2016Y\u2016\u221e \u2264 1 as x\u0303t \u2208 {ei : i \u2208 [N ]}. Therefore, setting \u03c32 = 1\u03b52 log 1 \u03b4 is sufficient to ensure differential privacy.\nAs can be readily seen, Theorem 4.3 does not guarantee optimal regret for Multi-Armed Bandits in terms of the dependence on the dimension as we would like to get a \u221a N dependence on the dimension. The natural idea would be to use EXP3 as A in Algorithm 3 however the standard version of EXP3 is defined on positive losses and the noise we add can potentially make it negative. Therefore we will use the algorithm EXP2 with exploration \u00b5 proposed by ([BCBK+12] Algorithm 2). We give a description of the Algorithm 4 in the Appendix for completeness. The following theorem appears as Theorem 1 in [BCBK+12].\nTheorem 4.5 (Regret Guarantee for EXP2 with exploration \u00b5). Let S be a finite set of N actions. For the EXP2 strategy provided that \u03b7|\u3008s, l\u0303t\u3009| \u2264 1, \u2200s \u2208 S, one has\nRegret \u2264 2\u03b3T + logN \u03b7 + \u03b7E\n[\n\u2211\nt\n\u2211\ns\npt(s)\u3008s, l\u0303t\u30092 ]\nWe now state the regret guarantee for Differentially Private Multi-Armed Bandits.\nTheorem 4.6 (Differentially Private Multi-Armed Bandits). Fix loss vectors (l1 . . . lT ) such that \u2016lt\u2016\u221e \u2264 1. Algorithm 3 when run with parameters D = N (0, \u03c32IN) where \u03c32 = 1\u03b52 log 2 1 \u03b4 and algorithm A =Algorithm 4 with the following parameters. The action set S is {e1 . . . eN}, \u03b7 = \u221a\nlogN 2NT (1+\u03c32 logN) , \u03b3 = \u03b7N\n\u221a\n1 + \u03c32 logNT and the exploration distribution \u00b5(i) = 1N .The\nregret of the algorithm is bounded by\nO ( \u221a NT logN(1 + \u03c32 logNT ) ) = O\n(\u221a NT logN logT\n\u03b5 log\n1\n\u03b4\n)\nProof. The proof follows similar arguments made to prove Theorem 4.3. For the purpose of analysis we define the following pseudo loss vectors\nl\u0303t = lt + Zt\nwhere by definition Zt \u223c N (0, \u03c32IN ). As before, we note that the following fact holds.\nP(\u2016Zt\u20162\u221e \u2265 10\u03c32 logNT ) \u2264 1\nT 2\nThe above follows from Fact B.2 proved in the Appendix. Taking a union bound, we have\nP(\u2203t \u2016Zt\u20162\u221e \u2265 10\u03c32 logNT ) \u2264 1\nT (12)\nTo bound the norm of the loss we will use the same technique as in the proof of Theorem 4.3. Define the event F , {\u2203t \u2016Zt\u20162\u221e \u2265 10\u03c32 logNT }. We have from (8) that P(F ) \u2264 1T . We now have that E[Regret] \u2264 E[Regret|F\u0304 ] + P(F )E[Regret|F ] Since the regret is always bounded by T we get that the second term above is at most 1. Therefore we will concern ourselves with bounding the first term above. Once again we note that Zt remain independent even when conditioned on the event F\u0304 . The following statements also hold\n\u2200t E[Zt|F\u0304 ] = 0 (13) \u2200t E[\u2016Zt\u20162\u221e|F\u0304 ] \u2264 E[\u2016Zt\u20162\u221e] \u2264 4\u03c32 logN (14)\nEquation (13) follows by noting that Zt remains symmetric around the origin even after conditioning. Equation (14) follows from Fact B.1 proved in the Appendix. It is easy to see that Theorem 4.2 still applies even when the noise is sampled from N (0, \u03c32IN ) conditioned under the event F\u0304 (due to Equation 9). Therefore we have that\nE[Regret|F\u0304 ] = E{Zt} [ EA [ T \u2211\nt=1\n( \u3008l\u0303t, x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]\n\u2223 \u2223 \u2223 \u2223 F\u0304\n]\n(15)\nNow note that due to the conditioning \u2016Zt\u20162\u221e \u2264 10\u03c32 logNT and therefore we have that\nL , maxt,x\u2208\u2206N |\u3008Zt, x\u3009| \u2264 4\u03c3 \u221a logNT.\nIt can be seen that the condition \u03b7|\u3008s, l\u0303t\u3009| \u2264 1 in Theorem 4.5 in the setting of the simplex and exploration \u00b5(i) = 1n and under the condition F\u0304 is satisfied as long as we have that\n\u03b7N(1 + 10\u03c3 logNT ) \u2264 \u03b3\nwhich holds by choice of these parameters.\nE[Regret|F\u0304 ] = E{Zt} [ EA [ T \u2211\nt=1\n( \u3008l\u0303t, x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]\n\u2223 \u2223 \u2223 \u2223 F\u0304\n]\nEquation (15)\n\u2264 E{Zt} [ logN\n\u03b7 + \u03b7\nT \u2211\nt=1\nN\u2016l\u0303t\u20162\u221e + 2T\u03b3 \u2223 \u2223 \u2223\n\u2223\nF\u0304\n]\nTheorem 4.5\n\u2264 E{Zt} [ logN\n\u03b7 + 2\u03b7\nT \u2211\nt=1\nN(\u2016lt\u20162\u221e + \u2016Zt\u20162\u221e) + 2T\u03b3 \u2223 \u2223 \u2223\n\u2223\nF\u0304\n]\n\u2264 logN \u03b7 + 2\u03b7TN(1 + \u03c32 logN) + 2T\u03b3 Equation (14) \u2264 O ( \u221a TN logN(1 + \u03c32 logNT ) )"}, {"heading": "A Noisy OCO Theorem Proof", "text": "We now give the proof of Theorem 4.2\nProof. The proof of the lemma is a straightforward calculation. First note that\nE{Zt}\n[\nEA\n[\nT \u2211\nt=1\n(\u3008lt, x\u0303t\u3009 \u2212 \u3008lt, x\u2217\u3009) ]] = E{Zt} [ EA [ T \u2211\nt=1\n( \u3008l\u0303t, x\u0303t\u3009 \u2212 \u3008l\u0303t, x\u2217\u3009 )\n]]\n+ E{Zt}\n[\nEA\n[\nT \u2211\nt=1\n\u3008lt \u2212 l\u0303t, x\u0303t\u3009 ]] \u2212 E{Zt} [ EA [ T \u2211\nt=1\n\u3008lt \u2212 l\u0303t, x\u2217\u3009 ]]\nNow we have that\nE{Zt}\n[\nEA\n[\nT \u2211\nt=1\n\u3008lt \u2212 l\u0303t, x\u0303t\u3009 ]] \u2212 E{Zt} [ EA [ T \u2211\nt=1\n\u3008lt \u2212 l\u0303t, x\u2217\u3009 ]]\n=EA\n[\nT \u2211\nt=1\nE{Zt} [ \u3008lT \u2212 l\u0303t, x\u0303t \u2212 x\u2217\u3009 ]\n]\n=EA\n[\nT \u2211\nt=1\n\u3008E [Zt] ,E[x\u0303t \u2212 x\u2217]\u3009 ]\n=0\nThe first inequality follows because the randomness of the algorithm does not depend on Zt. The second equality follows because xt, being a function of the loss vectors of the previous round, is independent of the noise Zt and the third equality follows from the fact that E[Zt] = 0."}, {"heading": "B Facts about Norms of Gaussian Vectors", "text": "We prove the facts for the case \u03c32 = 1. The generals versions follow immediately.\nFact B.1. Let Z \u223c N (0, IN ), then we have that\nE[\u2016Z\u20162\u221e] \u2264 3 logN\n1\u2212N\u22121\nProof. The proof is a simple application of the standard Moment Generating Function trick. Note that for each i \u2208 [N ], Z(i)2 is a \u03c72 random variable. The MGF for \u03c72 distribution is (1 \u2212 2s)\u22121/2. Therefore we have that for all s \u2264 1/2\nesE[maxi Z(i) 2] \u2264 E[esmaxi Z(i)2 ] Convexity of ex\n= E[max i\nesZ(i) 2 ]\n\u2264 \u2211\ni\nE[esZ(i) 2 ]\n= N(1\u2212 2s)\u22121/2\nTherefore we have that\nE[max i Z(i)2] \u2264 logN \u2212 1 2 log(1\u2212 2s) s\nSetting s = 1\u2212N \u22121\n2 finishes the proof.\nFact B.2. Let Z \u223c N (0, IN ), then we have that\nP(\u2016Z\u20162\u221e \u2265 10 logTN) \u2264 1\nT 2\nProof. We will show that for a fixed i \u2208 [N ]\nP(Z(i)2 \u2265 10 logTN) \u2264 1 NT 2\nThe proof then follows by a simple union bound. To see the above inequality we can use the standard fact about Gaussians, i.e. for n \u223c N (0, 1), we have that\nP(|n| \u2265 t) \u2264 \u221a 2\n\u03c0\ne \u2212t\n2\n2\nt (Mill\u2019s inequality)\nThe proof now follows from substitution."}, {"heading": "C EXP2 with Exploration \u00b5", "text": "Algorithm 4 EXP2 with exploration \u00b5\nInput: learning rate \u03b7; mixing coefficient \u03b3; distribution \u00b5 over the action set S\n1: q1 = ( 1 \u2016S\u2016 . . . 1 \u2016S\u2016 ) \u2208 R|S|. 2: for t = 1,2 . . . T do 3: Let pt = (1\u2212 \u03b3)qt + \u03b3\u00b5 and play st \u223c pt 4: Estimate loss vector lt by l\u0303t = P + t sts T t lt, with Pt = Ept [sts T t ] 5: Update the exponential weights, for all s \u2208 S,\nqt+1(s) = e\u2212\u03b7\u3008s,l\u0303t\u3009qt(s) \u2211\ns\u2032\u2208S e \u2212\u03b7\u3008s\u2032,l\u0303t\u3009qt(s\u2032)\n6: end for"}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Interior-point methods for full-information and bandit online learning", "author": ["Jacob D Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Abernethy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2012}, {"title": "Online linear optimization via smoothing", "author": ["Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2014}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi", "Sham M Kakade", "Shie Mannor", "Nathan Srebro", "Robert C Williamson"], "venue": "In COLT,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["Marcus Hutter", "Jan Poland"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In COLT,", "citeRegEx": "Jain et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "nearly) optimal algorithms for private online learning in full-information and bandit settings", "author": ["Abhradeep Guha Thakurta", "Adam Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Thakurta and Smith.,? \\Q2013\\E", "shortCiteRegEx": "Thakurta and Smith.", "year": 2013}], "referenceMentions": [], "year": 2017, "abstractText": "We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal \u00d5( \u221a T ) regret bounds. In the full-information setting, our results demonstrate that (\u03b5, \u03b4)-differential privacy may be ensured for free \u2013 in particular, the regret bounds scale as O( \u221a T ) + \u00d5 ( 1 \u03b5 log 1 \u03b4 ) . For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O (\u221a T log T \u03b5 log 1 \u03b4 ) , while the previously best known bound was \u00d5 (", "creator": "LaTeX with hyperref package"}}}