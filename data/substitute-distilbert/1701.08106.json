{"id": "1701.08106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Faster Discovery of Faster System Configurations with Spectral Learning", "abstract": "despite the huge spread and economical importance of configurable software capability, training is intuitive support in utilizing the full potential of these systems with respect while finding performance - optimal configurations. prior work on predicting the utilization of software configurations suffered from either ( a ) requiring far too many sample configurations or ( b ) large variances in underlying predictions. both problem problems can be avoided using describing what spectral learner. what's innovation is the use of the spectrum ( eigenvalues ) of the distance matrix between the configurations of a configurable software system, also perform prediction reduction. within that reduced configuration range, many closely associated configurations can be studied by executing only a few sample configurations. for the typical systems studied here, a few dozen samples yield minimal and maximal predictors - less matter 10 % prediction error, with a standard deviation of \u2248 than 2 %. when compared unto the state of the art, what ( a ) creates 2 to 5th times fewer samples to achieve similar ensemble accuracies, and ( b ) its predictions are more stable ( i. e., have lower standard deviation ). furthermore, some demonstrate that predictive judgments generated by what can be used by optimizers to discover system configurations that closely promote guaranteed optimal performance.", "histories": [["v1", "Fri, 27 Jan 2017 16:36:09 GMT  (457kb,D)", "http://arxiv.org/abs/1701.08106v1", "26 pages, 6 figures"], ["v2", "Thu, 3 Aug 2017 21:15:47 GMT  (1281kb,D)", "http://arxiv.org/abs/1701.08106v2", "26 pages, 6 figures"]], "COMMENTS": "26 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.SE cs.LG", "authors": ["vivek nair", "tim menzies", "norbert siegmund", "sven apel"], "accepted": false, "id": "1701.08106"}, "pdf": {"name": "1701.08106.pdf", "metadata": {"source": "CRF", "title": "Faster Discovery of Faster System Configurations with Spectral Learning", "authors": ["Vivek Nair", "Tim Menzies", "Sven Apel"], "emails": ["vivekaxl@gmail.com", "tim.menzies@gmail.com", "norbert.siegmund@uni-passau.de", "apel@uni-passau.de"], "sections": [{"heading": null, "text": "Keywords Performance Prediction \u00b7 Spectral Learning \u00b7 Decision Trees \u00b7 SearchBased Software Engineering \u00b7 Sampling.\nVivek Nair North Carolina State University, Raleigh, USA E-mail: vivekaxl@gmail.com\nTim Menzies North Carolina State University, Raleigh, USA E-mail: tim.menzies@gmail.com\nNorbert Siegmund University of Weimar, Germany E-mail: norbert.siegmund@uni-passau.de\nSven Apel University of Passau, Germany E-mail: apel@uni-passau.de\nar X\niv :1\n70 1.\n08 10\n6v 1\n[ cs\n.S E\n] 2\n7 Ja\nn 20\n17"}, {"heading": "1 Introduction", "text": "Most software systems today are configurable. Despite the undeniable benefits of configurability, large configuration spaces challenge developers, maintainers, and users. In the face of hundreds of configuration options, it is difficult to keep track of the effects of individual configuration options and their mutual interactions. So, predicting the performance of individual system configurations or determining the optimal configuration is often more guess work than engineering. In their recent paper, Xu et al. documented the difficulties developers face with understanding the configuration spaces of their systems [40]. As a result, developers tend to ignore over 5/6ths of the configuration options, which leaves considerable optimization potential untapped and induces major economic cost [40].\nAddressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35]. While gaining some ground, state-of-the-art approaches face two problems: (a) they require far too many sample configurations for learning or (b) they are prone to large variances in their predictions. For example, prior work on predicting performance scores using regression trees had to compile and execute hundreds to thousands of specific system configurations [15]. A more balanced approach by Siegmund et al. is able to learn predictors for configurable systems [35] with low mean errors, but with large variances of prediction accuracy (e.g. in half of the results, the performance predictions for the Apache Web server were up to 50 % wrong). Guo et al. [15] also proposed an incremental method to build a predictor model, which uses incremental random samples with steps equal to the number of configuration options (features) of the system. This approach also suffered from unstable predictions (e.g., predictions had a mean error of up to 22 %, with a standard deviation of up 46 %). Finally, Sarkar et al. [29] proposed a projective-learning approach (using fewer measurements than Guo at al. and Siegmund et al.) to quickly compute the number of sample configurations for learning a stable predictor. However, as we will discuss, after making that prediction, the total number of samples required for learning the predictor is comparatively high (up to hundreds of samples).\nThe problems of large sample sets and large variances in prediction can be avoided using the WHAT spectral learner, which is our main contribution. WHAT\u2019s innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by measuring only a few samples. In a number of experiments, we compared WHAT against the state-of-the-art approaches of Siegmund et al. [35], Guo et al. [15], and Sarkar et al. [29] by means of six real-world configurable systems: Berkeley DB, the Apache Web server, SQLite, the LLVM compiler, and the x264 video encoder. We found that WHAT performs as well or better than prior approaches, while requiring far fewer samples (just a few dozen). This is significant and most surprising, since some of the systems explored here have up to millions of possible configurations.\nOverall, we make the following contributions:\n\u2013 We present a novel sampling and learning approach for predicting the performance of software configurations in the face of large configuration spaces. The approach is based on a spectral learner that uses an approximation to the first principal component of the configuration space to recursively cluster it, relying only on a few points as representatives of each cluster. \u2013 We demonstrate the practicality and generality of our approach by conducting experiments on six real-world configurable software systems (see Figure 1). The results show that our approach is more accurate (lower mean error) and more stable (lower standard deviation) than state-of-the-art approaches. A key finding is the utility of the principal component of a configuration space to find informative samples from a large configuration space. All materials required for reproducing this work are available at https:// goo.gl/689Dve."}, {"heading": "2 Background & Related Work", "text": "A configurable software system has a set X of Boolean configuration options,1 also referred to as features or independent variables in our setting. We denote the number of features of system S as n. The configuration space of S can be represented by a Boolean space Zn2, which is denoted by F . All valid configurations of S belong to a set V , which is represented by vectors Ci (with 1\u2264 i\u2264 |V |) in Zn2. Each element of a configuration represents a feature, which can either be True or False, based on whether the feature is selected or not. Each valid instance of a vector (i.e., a configuration) has a corresponding performance score associated to it.\nThe literature offers two approaches to performance prediction of software configurations: a maximal sampling and a minimal sampling approach: With maximal sampling, we compile all possible configurations and record the associated performance scores. Maximal sampling can be impractically slow. For example, the performance data used in our experiments required 26 days of CPU time for measuring (and much longer, if we also count the time required for compiling the code prior to execution). Other researchers have commented that, in real world scenarios, the cost of acquiring the optimal configuration is overly expensive and time consuming [39].\nIf collecting performance scores of all configurations is impractical, minimal sampling can be used to intelligently select and execute just enough configurations (i.e., samples) to build a predictive model. For example, Zhang et al. [42] approximate the configuration space as a Fourier series, after which they can derive an expression showing how many configurations must be studied to build predictive models with a given error. While a theoretically satisfying result, that approach still needs thousands to hundreds of thousands of executions of sample configurations.\nAnother set of approaches are the four \u201dadditive\u201d minimal sampling methods of Siegmund et al. [35]. Their first method, called feature-wise sampling (FW), is their\n1 In this paper, we concentrate on Boolean options, as they make up the majority of all options; see Siegmund et al., for how to incorporate numeric options [34].\nbasic method. To explain FW, we note that, from a configurable software system, it is theoretically possible to enumerate many or all of the valid configurations2. Since each configuration (Ci) is a vector of n Booleans, it is possible to use this information to isolate examples of how much each feature individually contributes to the total run time:\n1. Find a pair of configurations C1 and C2, where C2 uses exactly the same features as C1, plus one extra feature fi. 2. Set the run time \u03a0( fi) for feature fi to be the difference in the performance scores between C2 and C1. 3. The run time for a new configuration Ci (with 1 \u2264 i \u2264 |V |) that has not been sampled before is then the sum of the run time of its features, as determined before:\n\u03a0(Ci) = \u2211 f j\u2208Ci \u03a0( f j) (1)\nWhen many pairs, such as C1,C2, satisfy the criteria of point 1, Siegmund et al. used the pair that covers the smallest number of features. Their minimal sampling method, FW, compiles and executes only these smallest C1 and C2 configurations. Siegmund et al. also offers three extensions to the basic method, which are based on sampling not just the smallest pairs, but also additional configurations covering certain kinds of interactions between features. All the following minimal sampling policies compile and execute valid configurations selected via one of three heuristics:\nPW (pair-wise): For each pair of features, try to find a configuration that contains the pair and has a minimal number of features selected. HO (higher-order): Select extra configurations, in which three features, f1, f2, f3, are selected if two of the following pair-wise interactions exist: ( f1, f2) and ( f2, f3) and ( f1, f3). HS (hot-spot): Select extra configurations that contain features that are frequently interacting with other features.\nGuo et al. [15] proposed a progressive random sampling approach, which samples the configuration space in steps of the number of features of the software system in question. They used the sampled configurations to train a regression tree, which is then used to predict the performance scores of other system configurations. The termination criterion of this approach is based on a heuristic, similar to the PW heuristics of Siegmund et al.\nSarkar et al. [29] proposed a cost model for predicting the effort (or cost) required to generate an accurate predictive model. The user can use this model to decide whether to go ahead and build the predictive model. This method randomly samples configurations and uses a heuristic based on feature frequencies as termination criterion. The samples are then used to train a regression tree; the accuracy of the model is measured by using a test set (where the size of the training set is equal to size of the test set). One of four projective functions (e.g., exponential) is selected based on how\n2 Though, in practice, this can be very difficult. For example, in models like the Linux Kernel such an enumeration is practically impossible [30].\ncorrelated they are to accuracy measures. The projective function is used to approximate the accuracy-measure curve, and the elbow point of the curve is then used as the optimal sample size. Once the optimal size is known, Sarkar et al. uses the approach of Guo et al. to build the actual prediction model.\nThe advantage of these previous approaches is that, unlike the results of Zhang et al., they require only dozens to hundreds of samples. Also, like our approach, they do not require to enumerate all configurations, which is important for highly configurable software systems. That said, as shown by our experiments (see Section 4), these approaches produce estimates with larger mean errors and partially larger variances than our approach. While sometimes the approach by Sarkar et al. results in models with (slightly) lower mean error rates, it still requires a considerably larger number of samples (up to hundreds), while WHAT requires only few dozen.\n3 Approach\n3.1 Spectral Learning\nThe minimal sampling method we propose here is based on a spectral-learning algorithm that explores the spectrum (eigenvalues) of the distance matrix between configurations in the configuration space. In theory, such spectral learners are an appropriate method to handle noisy, redundant, and tightly inter-connected variables, for the following reasons: When data sets have many irrelevancies or closely associated data parameters d, then only a few eigenvectors e, e d are required to characterize the data. In this reduced space:\n\u2013 Multiple inter-connected variables i, j,k\u2286 d can be represented by a single eigenvector; \u2013 Noisy variables from d are ignored, because they do not contribute to the signal in the data; \u2013 Variables become (approximately) parallel lines in e space. For redundancies i, j \u2208 d, we can ignore j since effects that change over j also change in the same way over i;\nThat is, in theory, samples of configurations drawn via an eigenspace sampling method would not get confused by noisy, redundant, or tightly inter-connected variables. Accordingly, we expect predictions built from that sample to have lower mean errors and lower variances on that error.\nSpectral methods have been used before for a variety of data mining applications [20]. Algorithms, such as PDDP [4], use spectral methods, such as principle component analysis (PCA), to recursively divide data into smaller regions. Softwareanalytics researchers use spectral methods (again, PCA) as a pre-processor prior to data mining to reduce noise in software-related data sets [37]. However, to the best of our knowledge, spectral methods have not been used before as a basis of a minimal sampling method.\nWHAT is somewhat different from other spectral learners explored in, for instance, image processing applications [31]. Work on image processing does not aim\nat defining a minimal sampling policy to predict performance scores. Also, a standard spectral method requires an O(N2) matrix multiplication to compute the components of PCA [18]. Worse, in the case of hierarchical division methods, such as PDDP, the polynomial-time inference must be repeated at every level of the hierarchy. Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystro\u0308m approximation to the first component of PCA [27]).\nWHAT receives N (with 1 \u2264 |N| \u2264 |V |) valid configurations (C), N1,N2, ..., as input and then:\n1. Picks any point Ni (1\u2264 i\u2264 |N|) at random; 2. Finds the point West \u2208 N that is furthest away from Ni; 3. Finds the point East \u2208 N that is furthest from West.\nThe line joining East and West is our approximation for the first principal component. Using the distance calculation shown in Equation 2, we define \u03b4 to be the distance between East (x) and West (y). WHAT uses this distance (\u03b4 ) to divide all the configurations as follows: The value xi is the projection of Ni on the line running from East to West3. We divide the examples based on the median value of the projection of xi. Now, we have two clusters of data divided based on the projection values (of Ni) on the line joining East and West. This process is applied recursively on these clusters until a predefined stopping condition. In our study, the recursive splitting of the Ni\u2019s stops when a sub-region contains less than \u221a |N| examples.\ndist(x,y) =  \u221a \u2211i(xi\u2212 yi)2 if xi and yi is numeric{ 0, if xi = yi 1, otherwise if xi and yi is Boolean (2)\nWe explore this approach for three reasons:\n\u2013 It is very fast: This process requires only 2|n| distance comparisons per level of recursion, which is far less than the O(N2) required by PCA [8] or other algorithms such as K-Means [16]. \u2013 It is not domain-specific: Unlike traditional PCA, our approach is general in that it does not assume that all the variables are numeric. As shown in Equation 2,4 we can approximate distances for both numeric and non-numeric data (e.g., Boolean). \u2013 It reduces the dimensionality problem: This technique explores the underlying dimension (first principal component) without getting confused by noisy, related, and highly associated variables.\n3 The projection of Ni can be calculated in the following way: a = dist(East,Ni);b = dist(West,Ni);xi = \u221a a2\u2212b2+\u03b42 2\u03b4 .\n4 In our study, dist accepts pair of configuration (C) and returns the distance between them. If xi and yi \u2208Rn, then the distance function would be same as the standard Euclidean distance.\n3.2 Spectral Sampling\nWhen the above clustering method terminates, our sampling policy (which we call S1) is then applied:\nRandom sampling (S1): compile and execute one configuration, picked at random, from each leaf cluster;\nWe use this sampling policy, because (as we will show later) it performs better than:\nEast-West sampling (S2): compile and execute the East and West poles of the leaf clusters; Exemplar sampling (S3): compile and execute all items in all leaves and return the one with lowest performance score.\nNote that S3 is not a minimal sampling policy (since it executes all configurations). We use it here as one baseline against which we can compare the other, more minimal, sampling policies. In the results that follow, we also compare our sampling methods against another baseline using information gathered after executing all configurations.\n3.3 Regression-Tree Learning\nAfter collecting the data using one of the sampling policies (S1, S2, or S3), as described in Section 3.2, we use a CART regression-tree learner [5] to build a performance predictor. Regression-tree learners seek the attribute-range split that most increases our ability to make accurate predictions. CART explores splits that divide N samples into two sets A and B, where each set has a standard deviation on the target variable of \u03c31 and \u03c32. CART finds the \u201cbest\u201d split defined as the split that minimizes A N \u03c31 + B N \u03c32. Using this best split, CART divides the data recursively.\nIn summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 compar-\nisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples collected by sampling policy into a run-time prediction model [5].\nThat is,\nWHERE = PDDP \u2212 PCA + FASTMAP\nWHAT = WHERE + { S1,S2,S3 } + CART\nThis unique combination of methods has not been previously explored in the softwareengineering literature.\n4 Experiments\n4.1 Research Questions\nWe formulate our research questions in terms of the challenges of exploring large complex configuration spaces. As our approach explores the spectral space, our hypothesis is that only a small number of samples is required to explore the whole space. However, a prediction model built from a very small sample of the configuration space might be very inaccurate and unstable, that is, it may exhibit very large mean prediction errors and variances on the prediction error.\nAlso, if we learn models from small regions of the training data, it is possible that a learner will miss trends in the data between the sample points. Such trends are useful when building optimizers (i.e., systems that receives one configuration as input and propose an alternate configuration that has, for instance, a better performance). Such optimizers might need to evaluate hundreds to millions of alternate configurations. To speed up that process, optimizers can use a surrogate model 5 that mimics the outputs of a system of interest, while being computationally cheap(er) to evaluate [23]. For example, when optimizing performance scores, we might ask a CART for a performance prediction (rather than compile and execute the corresponding configuration). Note that such surrogate-based reasoning critically depends on how well the surrogate can guide optimization.\nTherefore, to assess feasibility of our sampling policies, we must consider: \u2013 Performance scores generated from our minimal sampling policy; \u2013 The variance of the error rates when comparing predicted performance scores\nwith actual ones; \u2013 The optimization support offered by the performance predictor (i.e., can the model\nwork in tandem with other off-the-shelf optimizers to generate useful solutions). The above considerations lead to four research questions:\nRQ1: Can WHAT generate good predictions after examining only a small number of configurations?\nHere, by \u201cgood\u201d we mean that the predictions made by models that were trained using sampling with WHAT are as accurate, or more accurate, as preditions generated from models supplied with more samples. RQ2: Do less data used in building the predictions models cause larger variances in\nthe predicted performance scores? RQ3: Can \u201cgood\u201d surrogate models (to be used in optimizers) be built from minimal\nsamples? Note that RQ2 and RQ3 are of particular concern with our approach, since our goal is to sample as little as possible from the configuration space. RQ4: How good is WHAT compared to the state of the art of learning performance\npredictors from configurable software systems? To answer RQ4, we will compare WHAT against approaches presented by Siegmund et al. [35], Guo et al. [15], and Sarkar et al. [29].\n5 Also known as response surface methods, meta models, or emulators.\n4.2 Subject Systems\nThe configurable systems we used in our experiments are described in Table 1. Note, with \u201cpredicting performance\u201d, we mean predicting performance scores of the subject systems while executing test suites provided by the developers or the community, as described in Table 1. To compare the predictions of our and prior approaches with actual performance measures, we use data sets that have been obtained by measuring nearly all configurations6. We say nearly all configurations, for the following reasoning: For all except one of our subject systems, the total number of valid configurations was tractable (192 to 2560). However, SQLite has 3,932,160 possible configurations, which is an impractically large number of configurations to test whether our predictions are accurate and stable. Hence, for SQLite, we use the 4500 samples for\n6 http://openscience.us/repo/performance-predict/cpm.html\ntesting prediction accuracy and stability, which we could collect in one day of CPU time. Taking this into account, we will pay particular attention to the variance of the SQLite results.\n4.3 Experimental Rig\nRQ1 and RQ2 require the construction and assessment of numerous runtime predictors from small samples of the data. The following rig implements that construction process.\nFor each configurable software system, we built a table of data, one row per valid configuration. We then ran all configurations of all software systems and recorded the performance scores (i.e., that are invoked by a benchmark). The exception is SQLite for which we measured only the configurations needed to detect interactions and additionally 100 random configurations. To this table, we added a column showing the performance score obtained from the actual measurements for each configuration.\nNote that the following procedure ensures that we never test any prediction model on the data that we used to learn this model. Next, we repeated the following procedure 20 times (the figure of 20 repetitions was selected using the Central Limit Theorem): For each subject system in {BDBC, BDBJ, Apache, SQLite, LLVM, x264}\n\u2013 Randomize the order of the rows in their table of data; \u2013 For X in {10, 20, 30, ... , 90};\n\u2013 Let Train be the first X % of the data \u2013 Let Test be the rest of the data; \u2013 Pass Train to WHAT to select sample configurations; \u2013 Determine the performance scores associated with these configurations. This\ncorresponds to a table lookup, but would entail compiling and executing a system configuration in a practical setting. \u2013 Using the Train data and their performance scores, build a performance predictor using CART. \u2013 Using the Test data, assess the accuracy of the predictor using the error measure of Equation 3 (see below).\nThe validity of the predictors built by CART is verified on testing data. For each test item, we determine how long it actually takes to run the corresponding system configuration and compare the actual measured performance to the prediction from CART. The resulting prediction error is then computed using:\nerror = | predicted\u2212actual |\nactual \u00b7100 (3)\n(Aside: It is reasonable to ask why this metrics and not some of the others proposed in the literature (e.g sum absolute residuals). In short, our results are stable across a range of different metrics. For e.g., the results of this paper have been repeated using sum of absolute residuals and, in those other results, we seen the same ranking of methods; see http://tiny.cc/sumAR).\nRQ2 requires testing the standard deviation of the prediction error rate. To support that test, we:\n\u2013 Determine the X-th point in the above experiments, where all predictions stop improving (elbow point); \u2013 Measure the standard deviation of the error at this point, across our 20 repeats.\nAs shown in Figure 1, all our results plateaued after studying X = 40 % of the valid configurations7. Hence to answer RQ2, we will compare all 20 predictions at X = 40 %.\nRQ3 uses the learned regression tree as a surrogate model within an optimizer;\n\u2013 Take X = 40% of the configurations; \u2013 Apply WHAT to build a CART model using some minimal sample taken from\nthat 40 %; \u2013 Use that CART model within some standard optimizer while searching for con-\nfigurations with least runtime; \u2013 Compare the faster configurations found in this manner with the fastest configu-\nration known for that system.\nThis last item requires access to a ground truth of performance scores for a large number of configurations. For this experiment, we have access to that ground truth (since we have access to all system configurations, except for SQLite). Note that such a ground truth would not be needed when practitioners choose to use WHAT in their own work (it is only for our empirical investigation).\nFor the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system. Normally, it would be reasonable to ask why we used those three, and not the hundreds of other optimizers described in the literature [12, 17]. However, as shown below, all these optimizers in this domain exhibited very similar behavior (all found configurations close to the best case performance). Hence, the specific choice of optimizer is not a critical variable in our analysis."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 RQ1", "text": "Can WHAT generate good predictions after examining only a small number of configurations?\nFigure 1 shows the mean errors of the predictors learned after taking X % of the configurations, then asking WHAT and some sampling method (S1, S2, and S3) to (a) find what configurations to measure; then (b) asking CART to build a predictor using these measurements. The horizontal axis of the plots shows what X % of the configurations are studied; the vertical axis shows the mean relative error (\u00b5) from Equation 3. In this figure:\n7 Just to clarify one frequently asked question about this work, we note that our rig \u201cstudies\u201d 40 % of the data. We do not mean that our predictive models require accessing the performance scores from the 40 % of the data. Rather, by \u201cstudy\u201d we mean reflect on a sample of configurations to determine what minimal subset of that sample deserves to be compiled and executed.\n\u2013 The \u00d7\u2014\u00d7 lines in Figure 1 show a baseline result where data from the performance scores of 100 % of configurations were used by CART to build a runtime predictor.\n\u2013 The other lines show the results using the sampling methods defined in Section 3.2. Note that these sampling methods used runtime data only from a subset of 100 % of the performance scores seen in configurations from 0 to X %.\nIn Figure 1, lower y-axis values are better since this means lower prediction errors. Overall, we find that:\n\u2013 Some of the subject systems exhibit large variances in their error rate, below X = 40 % (e.g., BDBC and BDBJ). \u2013 Above X = 40 %, there is little effect on the overall change of the sampling methods. \u2013 Mostly, S3 shows the highest overall error, so that it cannot be recommended. \u2013 Always, the \u00d7\u2014\u00d7 baseline shows the lowest errors, which is to be expected since\npredictors built on the baseline have access to all data. \u2013 We see a trend that the error of S1 and S2 are within 5 % of the baseline results.\nHence, we can recommend these two minimal sampling methods.\nFigure 2 provides information about which of S1 or S2 we should recommend. This figure displays data taken from the X = 40 % point of Figure 1 and displays how many performance scores of configurations are needed by our sampling methods (while reflecting on the configurations seen in the range 0\u2264 X \u2264 40). Note that:\n\u2013 S3 needs up to thousands of performance scores points, so it cannot be recommended as minimal-sampling policy; \u2013 S2 needs twice as much performance scores as S1 (S2 uses two samples per leaf cluster while S1 uses only one). \u2013 S1 needs performance scores only for a few dozen (or less) configurations to generate the predictions with the lower errors seen in Figure 1.\nCombining the results of Figure 1 and Figure 2, we conclude that:\nS1 is our preferred spectral sampling method. Furthermore, the answer to RQ1 is \u201cyes\u201d, because applying WHAT, we can (a) generate runtime predictors using just a few dozens of sample performance scores; and (b) these predictions have error rates within 5 % of the error rates seen if predictors are built from information about all performance scores."}, {"heading": "5.2 RQ2", "text": ""}, {"heading": "Do less data used in building prediction models cause larger variances in the predicted values?", "text": "Two competing effects can cause increased or decreased variances in performance predictions. In our study, we report standard deviation (\u03c3 ) as a measure of variances in the performance predicitons. The less we sample the configuration space, the less we constrain model generation in that space. Hence, one effect that can be expected is that models learned from too few samples exhibit large variances. But, a compensating effect can be introduced by sampling from the spectral space since that space contains fewer confusing or correlated variables than the raw configuration space.\nFigure 3 reports which one of these two competing effects are dominant. Figure 3 shows that after some initial fluctuations, after seeing X = 40 % of the configurations, the variances in prediction errors reduces to nearly zero, which is similar to the results in figure 1.\nBased of the results of Figure 3, we answer RQ2 with \u201cno\u201d: Selecting a small number of samples does not necessarily increase variance (at least to say, not in this domain)."}, {"heading": "5.3 RQ3", "text": "Can \u201cgood\u201d surrogate models (to be used in optimizers) be built from minimal samples?\nThe results of answering RQ1 and RQ2 suggest to use WHAT (with S1) to build runtime predictors from a small sample of data. RQ3 asks if that predictor can be used by an optimizer to infer what other configurations correspond to system configurations with fast performance scores. To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.\nWhen these three optimizers mutated existing configurations to suggest new ones, these mutations were checked for validity. Any mutants that violated the system\u2019s constraints (e.g., a feature excluding another feature) were rejected and the survivors were \u201cevaluated\u201d by asking the CART surrogate model. These evaluations either rejected the mutant or used it in generation i+ 1, as the basis for a search for more, possibly better mutants.\nFigure 4 shows the configurations found by the three optimizers projected onto the ground truth of the performance scores of nearly all configurations (see Section 4.2). Again note that, while we use that ground truth for the validation of these results, our optimizers used only a small part of that ground-truth data in their search for the fastest configurations (see the WHAT + S1 results of Figure 2).\nThe important information of Figure 4 is that all the optimized configurations fall within 1 % of the fastest configuration according to the ground truth (see all the lefthand-side dots on each plot). Table 2 compares the performance of the optimizers used in this study. Note that the performances are nearly identical, which leads to the following conclusions:\nBased on the results of figure 4 answer to RQ3 is \u201cyes\u201d: For optimizing performance scores, we can use surrogates built from few runtime samples. The choice of the optimizer does not critically effect this conclusion."}, {"heading": "5.4 RQ4", "text": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2. Note that all approaches use\nregression-trees as predictors, except Siegmund\u2019s approach, which uses a regression function derived using linear programming. The results were studied using nonparametric tests, which was also used by Arcuri and Briand at ICSE \u201911 [25]). For testing statistical significance, we used non-parametric bootstrap test 95% confidence [9] followed by an A12 test to check that any observed differences were not\ntrivially small effects; i.e. given two lists X and Y , count how often there are larger numbers in the former list (and there there are ties, add a half mark): a = \u2200x \u2208 X ,y \u2208 Y #(x>y)+0.5\u2217#(x=y)|X |\u2217|Y | (as per Vargha [38], we say that a \u201csmall\u201d effect has a < 0.6). Lastly, to generate succinct reports, we use the Scott-Knott test to recursively divide our optimizers. This recursion used A12 and bootstrapping to group together subsets that are (a) not significantly different and are (b) not just a small effect different to\neach other. This use of Scott-Knott is endorsed by Mittas and Angelis [25] and by Hassan et al. [13].\nAs seen in Figure 5, the FW heuristic of Siegmund et al. (i.e., the sampling approach using the fewest number of configurations) has the higher errors rate and the highest standard deviation on that error rate (four out of six times). Hence, we cannot recommend this method or, if one wishes to use this method, we recommend using the other sampling heuristics (e.g., HO, HS) to make more accurate predictions (but at the cost of much more measurements). Moreover, the size of the standard deviation of this method causes further difficulties in estimating which configurations are those exhibiting a large prediction error.\nAs to the approach of Guo et al (with PW), it does not standout on any of our measurements. Its error results are within 1% of WHAT; its standard deviations are usually larger and it requires much more data than WHAT (Evaluations column of the figure 5).\nIn terms of the number of measure samples required to build a model, the righthand column of Figure 5 shows that WHAT requires the fewest samples except for two cases: the approach of Guo et al. (with 2N) working on BDBC and LLVM. In both these cases, the mean error and standard deviation on the error estimate is larger than WHAT. Furthermore, in the case of BDBC, the error values are \u00b5 = 14%, \u03c3 = 13%, which are much larger than WHAT\u2019s error scores of \u00b5 = 6%, \u03c3 = 5%.\nAlthough the approach of Sarkar et al. produces an error rate that is sometimes less than the one of WHAT, it requires the highest number of measurements. Moreover, WHAT 's accuracy is close to Sarkar's approach (1% to 2%) difference). Hence, we cannot recommend this approach, too.\nTable 3 shows the number of evaluations used by each approaches. We see that most state-of-the-art approaches often require many more samples than WHAT. Using those fewest numbers of samples, WHAT has within 1% to 2 % of the lowest standard deviation rates and within 1 to 2 % of lowest error rates. The exception is Sarkar\u2019s approach, which has 5 % lower mean error rates (in BDBC, see the Mean MRE column of figure 5). However, as shown in right-hand side of Table 3, Sarkar\u2019s approach needs nearly three times more measurements than WHAT. To summarize, there are two cases in Figure 5 where WHAT performs worse than, at least, one other method:\n\u2013 SQLite: The technique proposed by Sarkar et al. does better than WHAT (3.44 vs 5.6) but, as shown in the final column of Figure 5, does so at the cost of 92564 \u2248 15 times more evaluations that WHAT. In this case, a pragmatic engineer could well prefer our solution over that of Sarkar et al. (since number of evaluations performed by Sarkar et al.more than an order of magnitude than WHAT). \u2013 BDBC: Here again, WHAT is not doing the best but, compared to the number of evaluations required by all other solutions, it is not doing particularly bad.\nGiven the overall reduction of the error is small (5 % difference between Sarkar and WHAT in mean error), the cost of tripling the data-collection cost is often not feasible in a practical context and might not justify the small additional benefit in accuracy."}, {"heading": "6 Why does it work?", "text": "In this section, we present an in-depth analysis to understand why our sampling technique (based on a spectral learner) achieves such low mean fault rates while being stable (low variance). We hypothesize that the configuration space of the system configuration lie on a low dimensional manifold.\n6.1 History\nMenzies et. al [24] demonstrated how to exploit the underlying dimension to cluster data to find local homogeneous data regions in an otherwise heterogeneous data space. The authors used an algorithm called WHERE (see section 3.3), which recurses on two dimensions synthesized in linear time using a technique called FASTMAP [11]. The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41]. There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3]. These algorithms use different techniques to identify the underlying, independent/orthogonal dimensions to cluster the data points and differ with respect to the computational complexity and accuracy. We use WHERE since it computationally efficient O(2N), while still being accurate.\n8 WHERE is an approximation of the first principal component\n6.2 Testing Technique\nGiven our hypothesis the configuration space lies in a lower dimensional hyperplane \u2014 it is imperative to demonstrate that the intrinsic dimensionality of the configuration space is less than the actual dimension. To formalize this notion, we borrow the concept of correlation dimension from the domain of physics [14]. The correlation dimension of a dataset with k items is found by computing the number of items found at distance withing radius r (where r is the Euclidean distance between two configurations) while varying r. This is then normalized by the number of connections between k items to find the expected number of neighbors at distance r. This can be written as:\nC(r) = 2\nk(k\u22121)\nn\n\u2211 i=1\nn\n\u2211 j=i+1 I(||xi,x j||< r) (4)\nwhere : I(x < y) = { 1, if x <y 0, otherwise\nGiven the dataset with k items and range of distances [r0\u2013rmax], we estimate the intrinsic dimensionality as the mean slope between ln(C(r)) and ln(r).\n6.3 Evaluation\nOn the configuration space of our subject systems, we observe that the intrinsic dimensionality of the software system is much lower than the actual dimension. Figure 6 presents the intrinsic dimensionality along with the actual dimensions of the software systems. If we take a look at the intrinsic dimensionality and compare it with the actual dimensionality, then it becomes apparent that the configuration space lies on a lower dimensional hyperplane. For example, SQLite has 39 configuration options, but the intrinsic dimensionality of the space is just 1.61 (this is a fractal dimension). At the heart of WHAT is WHERE (a spectral clusterer), which uses the approximation of the first principal component to divide the configuration space and hence can take advantage of the low intrinsic dimensionality.\nAs a summary, our observations indicate that the intrinsic dimension of the configuration space is much lower that its actual dimension. Hence, clustering based on the intrinsic dimensions rather than the actual dimension would be more effective. In other words, configurations with similar performance values lie closer to the intrinsic hyperplane, when compared to the actual dimensions, and may be the reason as to why WHAT achieves empirically good results."}, {"heading": "7 Reliability and Validity", "text": "Reliability refers to the consistency of the results obtained from the research. For example, how well independent researchers could reproduce the study? To increase external reliability, we took care to either clearly define our algorithms or use implementations from the public domain (SciKitLearn) [26]. Also, all the data used in this\nwork are available on-line in the PROMISE9 code repository and all our algorithms are on-line at github.com/ai-se/where.\nValidity refers to the extent to which a piece of research actually investigates what the researcher purports to investigate [33]. Internal validity checks if the differences found in the treatments can be ascribed to the treatments under study.\n9 http://openscience.us/repo/performance-predict/cpm.html\nOne threat to internal validity of our experiments is the choice of training and testing data sets discussed in Figure 1. Recall that, while all our learners used the same testing data set, our untuned learners were only given access to training data.\nAnother threat to internal validity is instrumentation. The very low \u00b5 and \u03c3 error values reported in this study are so small that it is reasonable to ask whether they are due to some instrumentation quirk, rather than due to using a clever sample strategy:\n\u2013 Our low \u00b5 values are consistent with prior work [29]; \u2013 As to our low \u03c3 values, we note that, when the error values are so close to 0 %,\nthe standard deviation of the error is \u201csqueezed\u201d between zero and those errors. Hence, we would expect that experimental rigs that generate error values on the order of 5 % and Equation 3 should have \u03c3 values of 0 \u2264 \u03c3 \u2264 5 (e.g., like those seen in our introduction).\nRegarding SQLite, we cannot measure all possible configurations in reasonable time. Hence, we sampled only 100 configurations to compare prediction and actual performance values. We are aware that this evaluation leaves room for outliers. Also, we are aware that measurement bias can cause false interpretations [24]. Since we aim at predicting performance for a special workload, we do not have to vary benchmarks.\nWe aimed at increasing the external validity by choosing software systems from different domains with different configuration mechanisms and implemented with different programming languages. Furthermore, our subject systems are deployed and used in the real world. Nevertheless, assuming the evaluations to be automatically transferable to all configurable software systems is not fair. To further strengthen external validity, we run the model (generated by WHAT + S1) against other optimizers, such as NSGA-II and differential evolution [36]. That is, we validated whether the learned models are not only applicable for GALE style of perturbation. In Table 2, we see that the models developed are valid for all optimizers, as all optimizers are able to find the near optimal solutions."}, {"heading": "8 Related Work", "text": "In 2000, Shi and Maik [31] claimed the term \u201cspectral clustering\u201d as a reference to their normalized cuts image segmentation algorithm that partitions data through a spectral (eigenvalue) analysis of the Laplacian representation of the similarity graph between instances in the data.\nIn 2003, Kamvar et al. [20] generalized that definition saying that \u201cspectral learners\u201d were any data-mining algorithm that first replaced the raw dimensions with those inferred from the spectrum (eigenvalues) of the affinity (a.k.a. distance) matrix of the data, optionally adjusted via some normalization technique).\nOur clustering based on first principal component splits the data on a approximation to an eigenvector, found at each recursive level of the data (as described in \u00a73.1). Hence, this method is a \u201cspectral clusterer\u201d in the general Kamvar sense. Note that, for our data, we have not found that Kamvar\u2019s normalization matrices are needed.\nRegarding sampling, there are a wide range of methods know as experimental designs or designs of experiments [28]. They usually rely on fractional factorial designs as in the combinatorial testing community [22].\nFurthermore, there is a recent approach that learns performance-influence models for configurable software systems [34]. While this approach can handle even numeric features, it has similar sampling techniques for the Boolean features as reported in their earlier work [35]. Since we already compared to that earlier work and do not consider numeric features, we did not compare our work to performance-influence models."}, {"heading": "9 Conclusions", "text": "Configurable software systems today are widely used in practice, but they impose challenges regarding finding performance-optimal configurations. State-of-the-art approaches require too many measurements or are prone to large variances in their performance predictions. To overcome these limitations, we have proposed a fast spectral learner, called WHAT, along with three new sampling techniques. The key idea of WHAT is to explore the configuration space with eigenvalues of the features used in a configuration to determine exactly those configurations for measurement that reveal key performance characteristics. This way, we can study many closely associated configurations with only a few measurements.\nWe evaluated our approach on six real-world configurable software systems borrowed from the literature. Our approach achieves similar to lower error rates, while being stable when compared to the state of the art. In particular, with the exception of Berkeley DB, our approach is more accurate than the state-of-the-art approaches by Siegmund et al. [35] and Guo et al. [15]. Furthermore, we achieve a similar prediction accuracy and stability as the approach by Sarkar et al [29], while requiring a far smaller number of configurations to be measured. We also demonstrated that our approach can be used to build cheap and stable surrogate prediction models, which can be used by off-the-shelf optimizers to find the performance-optimal configuration. We use the correlation dimension to demonstrate how the high dimensional configuration space of our subject systems has a low intrinsic dimensionality, which might be the reason why WHAT performs so well on these datasets."}], "references": [{"title": "Think locally, act globally: Improving defect and effort prediction models", "author": ["Nicolas Bettenburg", "Meiyappan Nagappan", "Ahmed E Hassan"], "venue": "In Proceedings of the 9th IEEE Working Conference on Mining Software Repositories,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Towards improving statistical modeling of software engineering data: think locally, act globally", "author": ["Nicolas Bettenburg", "Meiyappan Nagappan", "Ahmed E Hassan"], "venue": "Empirical Software Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Principal direction divisive partitioning", "author": ["Daniel Boley"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen"], "venue": "CRC press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["Kalyanmoy Deb", "Amrit Pratap", "Sameer Agarwal", "T Meyarivan"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Using spectral clustering to automate identification and optimization of component structures", "author": ["Constanze Deiters", "Andreas Rausch", "Marco Schindler"], "venue": "In Realizing Artificial Intelligence Synergies in Software Engineering (RAISE),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Low-complexity principal component analysis for hyperspectral image compression", "author": ["Qian Du", "James E Fowler"], "venue": "International Journal of High Performance Computing Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia", "author": ["Christos Faloutsos", "King-Ip Lin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "FastMap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets, volume", "author": ["Christos Faloutsos", "King-Ip Lin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Practical methods of optimization", "author": ["Roger Fletcher"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Revisiting the impact of classification techniques on the performance of defect prediction models", "author": ["B. Ghotra", "S. McIntosh", "A.E. Hassan"], "venue": "IEEE International Conference on Software Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Measuring the strangeness of strange attractors", "author": ["Peter Grassberger", "Itamar Procaccia"], "venue": "In The Theory of Chaotic Attractors,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Variability-aware performance prediction: A statistical learning approach", "author": ["Jianmei Guo", "Krzysztof Czarnecki", "Sven Apel", "Norbert Siegmund", "Andrzej Wasowski"], "venue": "In IEEE/ACM 28th International Conference on Automated Software Engineering,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Search-based software engineering: Trends, techniques and applications", "author": ["Mark Harman", "S Afshin Mansouri", "Yuanyuan Zhang"], "venue": "ACM Computing Surveys,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Practical approaches to principal component analysis in the presence of missing values", "author": ["Alexander Ilin", "Tapani Raiko"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Spectral learning", "author": ["Kamvar Kamvar", "Sepandar Sepandar", "Klein Klein", "Dan Dan", "Manning Manning", "Christopher Christopher"], "venue": "In International Joint Conference of Artificial Intelligence. Stanford InfoLab,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Gale: Geometric active learning for search-based software engineering", "author": ["Joseph Krall", "Tim Menzies", "Misty Davies"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Introduction to combinatorial testing", "author": ["D Richard Kuhn", "Raghu N Kacker", "Yu Lei"], "venue": "CRC press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Surrogate-assisted evolutionary algorithms", "author": ["Ilya Gennadyevich Loshchilov"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Local versus global lessons for defect prediction and effort estimation", "author": ["Tim Menzies", "Andrew Butcher", "David Cok", "Andrian Marcus", "Lucas Layman", "Forrest Shull", "Burak Turhan", "Thomas Zimmermann"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm", "author": ["Nikolaos Mittas", "Lefteris Angelis"], "venue": "IEEE Transactions of Software Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Fastmap, Metricmap, and Landmark MDS are all nystrom algorithms. pages 261\u2013268", "author": ["John Platt"], "venue": "Society for Artificial Intelligence and Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Optimal Design of Experiments, volume 50", "author": ["Friedrich Pukelsheim"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}, {"title": "Cost-efficient sampling for performance prediction of configurable systems", "author": ["Atri Sarkar", "Jianmei Guo", "Norbert Siegmund", "Sven Apel", "Krzysztof Czarnecki"], "venue": "In 30th IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Scalable product line configuration: A straw to break the camel\u2019s back", "author": ["Abdel Salam Sayyad", "Joe Ingram", "Tim Menzies", "Hany Ammar"], "venue": "In IEEE/ACM 28th International Conference on Automated Software Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Views on internal and external validity in empirical software engineering", "author": ["Janet Siegmund", "Norbert Siegmund", "Sven Apel"], "venue": "In Proceedings of the 37th International Conference on Software Engineering,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Performance-influence models for highly configurable systems", "author": ["Norbert Siegmund", "Alexander Grebhahn", "Sven Apel", "Christian K\u00e4stner"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Predicting performance via automated feature-interaction detection", "author": ["Norbert Siegmund", "Sergiy S Kolesnikov", "Christian K\u00e4stner", "Sven Apel", "Don Batory", "Marko Rosenm\u00fcller", "Gunter Saake"], "venue": "In Proceedings of the 34th International Conference on Software Engineering,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Differential evolution\u2013a simple and efficient heuristic for global optimization over continuous spaces", "author": ["Rainer Storn", "Kenneth Price"], "venue": "Journal of global optimization,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1997}, {"title": "Approximating attack surfaces with stack traces", "author": ["Christopher Theisen", "Kim Herzig", "Patrick Morrison", "Brendan Murphy", "Laurie Williams"], "venue": "In Proceedings of the 37th International Conference on Software Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "A Critique and Improvement of the CL Common Language Effect Size Statistics of McGraw and Wong", "author": ["Andr\u00e1s Vargha", "Harold D Delaney"], "venue": "Journal of Educational and Behavioral Statistics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "Maximizing classifier utility when there are data acquisition and modeling costs", "author": ["Gary M Weiss", "Ye Tian"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Talwadker. Hey, you have given me too many knobs!: Understanding and dealing with over-designed configuration in system software", "author": ["Tianyin Xu", "Long Jin", "Xuepeng Fan", "Yuanyuan Zhou", "Shankar Pasupathy", "Rukma"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Cross-project defect prediction using a connectivity-based unsupervised classifier", "author": ["Feng Zhang", "Quan Zheng", "Ying Zou", "Ahmed E. Hassan"], "venue": "In Proceedings of the 38th International Conference on Software Engineering", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Performance prediction of configurable software systems by fourier learning", "author": ["Yi Zhang", "Jianmei Guo", "Eric Blais", "Krzysztof Czarnecki"], "venue": "In 30th IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Active learning for multi-objective optimization", "author": ["Marcela Zuluaga", "Guillaume Sergent", "Andreas Krause", "Markus P\u00fcschel"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}], "referenceMentions": [{"referenceID": 38, "context": "documented the difficulties developers face with understanding the configuration spaces of their systems [40].", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "As a result, developers tend to ignore over 5/6ths of the configuration options, which leaves considerable optimization potential untapped and induces major economic cost [40].", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35].", "startOffset": 204, "endOffset": 214}, {"referenceID": 27, "context": "Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35].", "startOffset": 204, "endOffset": 214}, {"referenceID": 33, "context": "Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35].", "startOffset": 204, "endOffset": 214}, {"referenceID": 14, "context": "For example, prior work on predicting performance scores using regression trees had to compile and execute hundreds to thousands of specific system configurations [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "is able to learn predictors for configurable systems [35] with low mean errors, but with large variances of prediction accuracy (e.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "[15] also proposed an incremental method to build a predictor model, which uses incremental random samples with steps equal to the number of configuration options (features) of the system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] proposed a projective-learning approach (using fewer measurements than Guo at al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35], Guo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15], and Sarkar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] by means of six real-world configurable systems: Berkeley DB, the Apache Web server, SQLite, the LLVM compiler, and the x264 video encoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Other researchers have commented that, in real world scenarios, the cost of acquiring the optimal configuration is overly expensive and time consuming [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "[42] approximate the configuration space as a Fourier series, after which they can derive an expression showing how many configurations must be studied to build predictive models with a given error.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": ", for how to incorporate numeric options [34].", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "[15] proposed a progressive random sampling approach, which samples the configuration space in steps of the number of features of the software system in question.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] proposed a cost model for predicting the effort (or cost) required to generate an accurate predictive model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For example, in models like the Linux Kernel such an enumeration is practically impossible [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "Spectral methods have been used before for a variety of data mining applications [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "Algorithms, such as PDDP [4], use spectral methods, such as principle component analysis (PCA), to recursively divide data into smaller regions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 35, "context": "Softwareanalytics researchers use spectral methods (again, PCA) as a pre-processor prior to data mining to reduce noise in software-related data sets [37].", "startOffset": 150, "endOffset": 154}, {"referenceID": 29, "context": "WHAT is somewhat different from other spectral learners explored in, for instance, image processing applications [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "Also, a standard spectral method requires an O(N2) matrix multiplication to compute the components of PCA [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystr\u00f6m approximation to the first component of PCA [27]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystr\u00f6m approximation to the first component of PCA [27]).", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystr\u00f6m approximation to the first component of PCA [27]).", "startOffset": 251, "endOffset": 255}, {"referenceID": 7, "context": "\u2013 It is very fast: This process requires only 2|n| distance comparisons per level of recursion, which is far less than the O(N2) required by PCA [8] or other algorithms such as K-Means [16].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "2, we use a CART regression-tree learner [5] to build a performance predictor.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "In summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 comparisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples col-", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "In summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 comparisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples col-", "startOffset": 267, "endOffset": 270}, {"referenceID": 22, "context": "In summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 comparisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples col-", "startOffset": 348, "endOffset": 352}, {"referenceID": 4, "context": "lected by sampling policy into a run-time prediction model [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "To speed up that process, optimizers can use a surrogate model 5 that mimics the outputs of a system of interest, while being computationally cheap(er) to evaluate [23].", "startOffset": 164, "endOffset": 168}, {"referenceID": 33, "context": "[35], Guo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15], and Sarkar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 105, "endOffset": 108}, {"referenceID": 19, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 127, "endOffset": 135}, {"referenceID": 41, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 127, "endOffset": 135}, {"referenceID": 11, "context": "Normally, it would be reasonable to ask why we used those three, and not the hundreds of other optimizers described in the literature [12, 17].", "startOffset": 134, "endOffset": 142}, {"referenceID": 15, "context": "Normally, it would be reasonable to ask why we used those three, and not the hundreds of other optimizers described in the literature [12, 17].", "startOffset": 134, "endOffset": 142}, {"referenceID": 19, "context": "To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.", "startOffset": 130, "endOffset": 134}, {"referenceID": 34, "context": "To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.", "startOffset": 156, "endOffset": 159}, {"referenceID": 33, "context": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2.", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2.", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "The results were studied using nonparametric tests, which was also used by Arcuri and Briand at ICSE \u201911 [25]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "For testing statistical significance, we used non-parametric bootstrap test 95% confidence [9] followed by an A12 test to check that any observed differences were not", "startOffset": 91, "endOffset": 94}, {"referenceID": 36, "context": "5\u2217#(x=y) |X |\u2217|Y | (as per Vargha [38], we say that a \u201csmall\u201d effect has a < 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "This use of Scott-Knott is endorsed by Mittas and Angelis [25] and by Hassan et al.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "al [24] demonstrated how to exploit the underlying dimension to cluster data to find local homogeneous data regions in an otherwise heterogeneous data space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "3), which recurses on two dimensions synthesized in linear time using a technique called FASTMAP [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 1, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 6, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 39, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 17, "context": "There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3].", "startOffset": 165, "endOffset": 169}, {"referenceID": 30, "context": "There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3].", "startOffset": 191, "endOffset": 195}, {"referenceID": 2, "context": "There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3].", "startOffset": 218, "endOffset": 221}, {"referenceID": 13, "context": "To formalize this notion, we borrow the concept of correlation dimension from the domain of physics [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "For example, how well independent researchers could reproduce the study? To increase external reliability, we took care to either clearly define our algorithms or use implementations from the public domain (SciKitLearn) [26].", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "Validity refers to the extent to which a piece of research actually investigates what the researcher purports to investigate [33].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "\u2013 Our low \u03bc values are consistent with prior work [29]; \u2013 As to our low \u03c3 values, we note that, when the error values are so close to 0 %, the standard deviation of the error is \u201csqueezed\u201d between zero and those errors.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "Also, we are aware that measurement bias can cause false interpretations [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 34, "context": "To further strengthen external validity, we run the model (generated by WHAT + S1) against other optimizers, such as NSGA-II and differential evolution [36].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "In 2000, Shi and Maik [31] claimed the term \u201cspectral clustering\u201d as a reference to their normalized cuts image segmentation algorithm that partitions data through a spectral (eigenvalue) analysis of the Laplacian representation of the similarity graph between instances in the data.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "[20] generalized that definition saying that \u201cspectral learners\u201d were any data-mining algorithm that first replaced the raw dimensions with those inferred from the spectrum (eigenvalues) of the affinity (a.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Regarding sampling, there are a wide range of methods know as experimental designs or designs of experiments [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "They usually rely on fractional factorial designs as in the combinatorial testing community [22].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "Furthermore, there is a recent approach that learns performance-influence models for configurable software systems [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 33, "context": "While this approach can handle even numeric features, it has similar sampling techniques for the Boolean features as reported in their earlier work [35].", "startOffset": 148, "endOffset": 152}, {"referenceID": 33, "context": "[35] and Guo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Furthermore, we achieve a similar prediction accuracy and stability as the approach by Sarkar et al [29], while requiring a far smaller number of configurations to be measured.", "startOffset": 100, "endOffset": 104}], "year": 2017, "abstractText": "Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT\u2019s innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors\u2014less than 10 % prediction error, with a standard deviation of less than 2 %. When compared to the state of the art, WHAT (a) requires 2 to 10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance.", "creator": "LaTeX with hyperref package"}}}