{"id": "1702.00758", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "HashNet: Deep Learning to Hash by Continuation", "abstract": "learning to exploit has been widely applied to approximate nearest neighbor search for large - scaled multimedia retrieval, due to its computation efficiency and retrieval quality. deep learning to hash, which improves retrieval quality by end - to - least representation learning improves hash encoding, were received increasing attention recently. subject to seemingly reduced gradient difficulty in the optimization with binary activations, existing deep learning to construct methods need to first learn static representations and then generate binary attribute codes in two separated binarization region, whom suffer from substantial loss of retrieval quality. this paper presents hashnet, a novel deep architecture for deep learning to scramble by continuation method, which learns exactly binary hash codes from imbalanced similarity data where the product of component pairs is much smaller then the number three dissimilar pairs. the key idea is to attack the vanishing differentiation problem in providing deep code with non - trivial binary activations by continuation method, in which we begin from learning an easier network with smoothed intersection function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. comprehensive empirical evidence shows that schools can generate exactly binary hash codes and yield state - of - the - art multimedia retrieval techniques on standard benchmarks.", "histories": [["v1", "Thu, 2 Feb 2017 17:29:24 GMT  (2794kb,D)", "http://arxiv.org/abs/1702.00758v1", null], ["v2", "Sun, 23 Apr 2017 22:14:09 GMT  (3001kb,D)", "http://arxiv.org/abs/1702.00758v2", null], ["v3", "Thu, 20 Jul 2017 14:59:45 GMT  (1900kb,D)", "http://arxiv.org/abs/1702.00758v3", null], ["v4", "Sat, 29 Jul 2017 17:55:50 GMT  (1900kb,D)", "http://arxiv.org/abs/1702.00758v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhangjie cao", "mingsheng long", "jianmin wang", "philip s yu"], "accepted": false, "id": "1702.00758"}, "pdf": {"name": "1702.00758.pdf", "metadata": {"source": "CRF", "title": "HashNet: Deep Learning to Hash by Continuation", "authors": ["Zhangjie Cao", "Mingsheng Long", "Jianmin Wang", "Philip S. Yu"], "emails": ["caozhangjie14@gmail.com", "mingsheng@tsinghua.edu.cn", "jimwang@tsinghua.edu.cn", "psyu@uic.edu"], "sections": [{"heading": "1. Introduction", "text": "In the big data era, large-scale and high-dimensional media data has been pervasive in search engines and social networks. To guarantee retrieval quality and computation efficiency, approximate nearest neighbors (ANN) search has attracted increasing attention. Parallel to the traditional indexing methods [21], another advantageous solution is hashing methods [35], which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items. In this paper, we will focus on learning to hash methods [35] that build data-dependent\nhash encoding schemes for efficient image retrieval, which have shown better performance than data-independent hashing methods, e.g. Locality-Sensitive Hashing (LSH) [10].\nMany learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39]. Prior learning to hash methods can be divided into unsupervised methods and supervised methods. While unsupervised methods are more general and can be trained without semantic labels or relevance information, they are subject to the semantic gap dilemma [32] that high-level semantic description of an object often differs from low-level feature descriptors. Supervised methods can incorporate semantic labels or relevances to mitigate the semantic gap and improve the hashing quality, i.e. achieve accurate search with shorter hash codes.\nRecently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions. These deep learning to hash methods have shown state-of-the-art performance on many benchmarks. In particular, it proves crucial to jointly learn similarity-preserving representations and control quantization error of binarizing continuous representations to binary codes [40]. However, a key disadvantage of these deep learning to hash methods is that they need to first learn continuous deep representations, which are then binarized into hash codes in a separated post-step of sign thresholding. By continuous relaxation, i.e. solving the discrete optimization of hash codes with continuous optimization, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash codes in their optimization procedure. Hence, existing deep hashing methods may generate suboptimal binary hash codes for efficient similarity retrieval.\nThere are two key challenges to enabling deep learning to hash truly end-to-end. First, converting deep representations, which are continuous in nature, to exactly binary hash codes, we need to adopt the sign function h = sgn (z) as activation function when generating binary hash codes us-\n1\nar X\niv :1\n70 2.\n00 75\n8v 1\n[ cs\n.L G\n] 2\nF eb\n2 01\n7\ning similarity-preserving learning in deep neural networks. However, the gradient of the sign function is zero for all nonzero inputs, which will make standard back-propagation infeasible. This is known as the vanishing gradient problem, which is the key difficulty in training deep neural networks via back-propagation [14]. Second, the similarity information is often very sparse in real retrieval systems, i.e., the number of similar pairs is much smaller than the number of dissimilar pairs. This will result in the data imbalance problem, making similarity-preserving learning ineffective. Optimizing deep networks with sign activation remains an open problem and a key challenge for deep learning to hash.\nThis paper presents HashNet, a novel architecture for deep learning to hash by continuation, which addresses the data imbalance and vanishing gradient problems in an endto-end learning framework for deep representation learning and binary hash encoding. The architecture is comprised of four key components: (1) Standard convolutional neural network (CNN), e.g. AlexNet and ResNet, for learning deep image representations, (2) a fully-connected hash layer for transforming the deep representations into low-dimensional representations, (3) a sign activation function for binarizing the low-dimensional representations into compact binary hash codes, and (4) a novel weighted pairwise cross-entropy loss function for similarity-preserving learning from data with imbalanced similarity relationships. Specifically, this paper attacks the vanishing gradient problem in the nonconvex optimization of the deep networks with non-smooth sign activation by continuation methods [1], which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize. By gradually reducing the amount of smoothing during the training, it results in a sequence of optimization problems converging to the original optimization problem. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art similarity retrieval performance on standard benchmarks."}, {"heading": "2. Related Work", "text": "Existing learning to hash methods can be roughly put into two categories: unsupervised hashing and supervised hashing. Please refer to [35] for a comprehensive survey.\nUnsupervised hashing methods learn hash functions that encode data points to binary codes by training from unlabeled data. Typical learning criteria include reconstruction error minimization [30, 12, 16] and graph learning[36, 24]. Supervised hashing explores supervised information (e.g. pairwise similarity or relevance feedback) to learn compact hash codes. Binary Reconstruction Embedding (BRE) [19] pursues hash functions by minimizing the squared errors between the distances of data points and the distances of their corresponding hash codes. Minimal Loss Hashing (MLH) [27] and Hamming Distance Metric Learning [28]\nlearn hash codes by minimizing hinge-like loss functions based on similarity of data points. Supervised Hashing with Kernels (KSH) [23] builds compact binary hash codes by minimizing the Hamming distances across similar pairs and maximizing the Hamming distances across dissimilar pairs.\nAs deep convolutional neural network (CNN) [18, 13] yield breakthrough performance on many computer vision tasks, deep learning to hash has attracted attention recently. CNNH [37] adopts a two-stage strategy in which the first stage learns hash codes and the second stage learns a deepnetwork based hash function to fit the codes. DNNH [20] improved the two-stage CNNH with a simultaneous feature learning and hash coding pipeline such that representations and hash codes can be optimized in a joint learning process. DHN [40] further improves DNNH by a cross-entropy loss and a quantization loss which preserve the pairwise similarity and control the quantization error simultaneously. DHN obtains state-of-the-art performance on several benchmarks.\nHowever, existing deep learning to hash methods only learn continuous codes and need a binarization post-step to generate binary codes. By continuous relaxation, these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot keep the codes exactly binary after optimization. Also, they assume the number of similar pairs and dissimilar pairs are balanced, which is not always true in real applications, and if violated, will result in significant performance drop. This paper addresses these problems with a novel HashNet, which can be effectively trained by the continuation to learn exactly binary hash codes from imbalanced similarity data."}, {"heading": "3. HashNet", "text": "In similarity retrieval systems, we are given a training set of N points {xi}Ni=1, each represented by a D-dimensional feature vector xi \u2208 RD. Some pairs of points xi and xj are provided with similarity labels sij , where sij = 1 if xi and xj are similar while sij = 0 if xi and xj are dissimilar. The goal of deep learning to hash is to learn nonlinear hash function f : x 7\u2192 h \u2208 {\u22121, 1}K from input space RD to Hamming space {\u22121, 1}K using deep neural networks, which encodes each point x into compact K-bit binary hash code h = f(x) such that the similarity information between the given pairs S can be preserved in the compact hash codes. In supervised hashing, the similarity set S = {sij} can be constructed from semantic labels of data points or relevance feedback from click-through data in real retrieval systems.\nTo address the data imbalance and vanishing gradient problems in an end-to-end learning framework, this paper presents HashNet, a novel architecture for deep learning to hash by continuation, shown in Figure 1. The architecture accepts pairwise input images {(xi,xj , sij)} and processes them through an end-to-end pipeline of deep representation learning and binary hash coding: (1) a convolutional net-\nwork (CNN) for learning deep representation of each image xi, (2) a fully-connected hash layer (fch) for transforming the deep representation into K-dimensional representation zi \u2208 RK , (3) a sign activation function h = sgn (z) for binarizing the K-dimensional representation zi into K-bit binary hash code hi \u2208 {\u22121, 1}K , and (4) a novel weighted cross-entropy loss for similarity-preserving learning from imbalanced data. We attack the vanishing gradient problem of the non-smooth activation function h = sgn (z) by continuation, which starts with a smoothed activation function y = tanh (\u03b2x) and becomes more non-smooth by increasing \u03b2 as the training proceeds, until eventually goes back to the original, difficult to optimize, sign activation function."}, {"heading": "3.1. Model Formulation", "text": "To perform deep learning to hash from imbalanced data, we jointly preserve similarity information of pairwise images and generate binary hash codes by weighted maximum likelihood [6]. For a pair of binary hash codes hi and hj , there exists a nice relationship between their Hamming distance distH(\u00b7, \u00b7) and inner product \u3008\u00b7, \u00b7\u3009: distH (hi,hj) = 1 2 (K \u2212 \u3008hi,hj\u3009). Hence, the Hamming distance and inner product can be used interchangeably for binary hash codes, and we adopt inner product to quantify pairwise similarity. Given the set of pairwise similarity labels S = {sij}, the Weighted Maximum Likelihood (WML) estimation of the hash codes H = [h1, . . . ,hN ] for all N training points is\nlogP (S|H) = \u2211 sij\u2208S cij logP (sij |hi,hj), (1)\nwhere P (S|H) is the weighted likelihood function, and cij is the weight for each training pair (xi,xj , sij), which is used to tackle the data imbalance problem by weighting the training pairs according to the importance of misclassifying that pair [6]. Since each similarity label in S can only be sij = 1 (similar) or sij = 0 (dissimilar), to account for the\ndata imbalance between similar and dissimilar pairs, we set\ncij = { 1/ |S1| , sij = 1 1/ |S0| , sij = 0\n(2)\nwhere S1 = {sij \u2208 S : sij = 1} is the set of similar pairs and S0 = {sij \u2208 S : sij = 0} is the set of dissimilar pairs. For each pair, P (sij |hi,hj) is the conditional probability of similarity label sij given a pair of hash codes hi and hj , which can be naturally defined as pairwise logistic function,\nP (sij |hi,hj) = { \u03c3 (\u3008hi,hj\u3009) , sij = 1 1\u2212 \u03c3 (\u3008hi,hj\u3009) , sij = 0\n= \u03c3(\u3008hi,hj\u3009)sij (1\u2212 \u03c3 (\u3008hi,hj\u3009))1\u2212sij (3)\nwhere \u03c3 (x) = 1/(1 + e\u2212\u03b1x) is the adaptive sigmoid function with hyper-parameter \u03b1 to control its bandwidth. Note that the sigmoid function with larger \u03b1 will have larger saturation zone where its gradient is zero. To perform more effective back-propagation, we usually require \u03b1 < 1, which is more effective than the typical setting of \u03b1 = 1. Similar to logistic regression, we can see in pairwise logistic regression that the smaller the Hamming distance distH (hi,hj) is, the larger the inner product \u3008hi,hj\u3009 as well as the conditional probability P (1|hi,hj) will be, implying that pair hi and hj should be classified as similar; otherwise, the larger the conditional probability P (0|hi,hj) will be, implying that pair hi and hj should be classified as dissimilar. Hence, Equation (3) is a reasonable extension of the logistic regression classifier to the pairwise classification scenario, which is optimal for binary similarity labels sij \u2208 {0, 1}.\nBy taking Equation (3) into WML estimation in Equation (1), we achieve the optimization problem of HashNet,\nmin W \u2211 sij\u2208S cij (log (1 + exp (\u03b1 \u3008hi,hj\u3009))\u2212 \u03b1sij \u3008hi,hj\u3009),\n(4)\nwhereW denotes the set of all parameters in deep networks. Note that, HashNet directly uses the sign activation function hi = sgn (zi) which converts the K-dimensional representation to exactly binary hash codes, as shown in Figure 1. By optimizing the WML estimation in Equation (4), we can enable deep learning to hash from imbalanced data under a statistically optimal framework. It is noteworthy that our work is the first attempt that extends the WML estimation from pointwise scenario to pairwise scenario. The HashNet can jointly preserve similarity information of pairwise images and generate exactly binary hash codes. Different from HashNet, previous deep-hashing methods need to first learn continuous embeddings, which are binarized in a separated step using the sign function. This will result in substantial quantization errors and significant losses of retrieval quality."}, {"heading": "3.2. Learning by Continuation", "text": "HashNet learns exactly binary hash codes by converting the K-dimensional representation z of the hash layer fch, which is continuous in nature, to binary hash code h taking values of either +1 or \u22121. This binarization process can only be performed by taking the sign function h = sgn (z) as activation function on top of hash layer fch in HashNet,\nh = sgn (z) = { +1, if z > 0 \u22121, otherwise\n(5)\nUnfortunately, as the sign function is non-smooth and nonconvex, its gradient is zero for all nonzero inputs, and is illdefined at zero, which makes the standard back-propagation infeasible for training deep networks. This is known as the vanishing gradient problem, which has been a key difficulty in training deep neural networks via back-propagation [14].\nMany optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [14, 3], dropout [33], batch normalization [15], and deep residual learning [13]. In particular, Rectifier Linear Unit (ReLU) [26] activation function makes deep networks much easier to train and enables end-to-end learning algorithms. However, the sign activation function is so ill-defined that all the above optimization methods may fail. A very recent work, BinaryNet [5], focuses on training deep networks with activations constrained to +1 or \u22121. However, the training algorithm may be hard to converge as the feed-forward pass uses the sign activation (sgn) but the back-propagation pass uses a hard tanh (Htanh) activation. Optimizing deep networks with sign activation remains an open problem and a key challenge for deep learning to hash.\nThis paper attacks the problem of non-convex optimization of deep networks with non-smooth sign activation by starting with a smoothed objective function which becomes more non-smooth as the training proceeds. It is inspired by\nAlgorithm 1: Optimizing HashNet by Continuation Input: A sequence 1 = \u03b20 < \u03b21 < . . . < \u03b2m =\u221e for stage t = 0 to m do\nTrain HashNet (4) with tanh(\u03b2tz) as activation Set converged HashNet as next stage initialization\nend Output: HashNet with sgn(z) as activation, \u03b2m \u2192\u221e\nrecent studies in continuation methods [1], which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize. By gradually reducing the amount of smoothing during the training, it results in a sequence of optimization problems converging to the original optimization problem. Motivated by the continuation methods, we notice there exists a key relationship between the sign function and the scaled tanh function in the concept of limit in mathematics,\nlim\u03b2\u2192\u221e tanh (\u03b2z) = sgn (z) , (6)\nwhere \u03b2 > 0 is a scaling parameter. Increasing \u03b2, the scaled tanh function tanh(\u03b2z) will become more non-smooth and more saturated so that the deep networks using tanh(\u03b2z) as the activation function will be more difficult to optimize, as in Figure 1 (right). But fortunately, as \u03b2 \u2192 \u221e, the optimization problem will converge to the original deep learning to hash problem in (4) with sgn(z) activation function.\nUsing the continuation methods, we design an optimization method for HashNet in Algorithm 1. As deep network with tanh(z) as the activation function can be successfully trained, we start training HashNet with tanh(\u03b2tz) as the activation function, where \u03b20 = 1. For each stage t, after HashNet converges, we increase \u03b2t and train (i.e. fine-tune) HashNet by setting the converged network parameters as the initialization for training the HashNet in the next stage. By evolving tanh(\u03b2tz) with \u03b2t \u2192 \u221e, the network will converge to HashNet with sgn(z) as activation function, which can generate exactly binary hash codes as we desire. The efficacy of continuation in Algorithm 1 can be understood as multi-stage pre-training, i.e., pre-training HashNet with tanh(\u03b2tz) activation function is used to initialize HashNet with tanh(\u03b2t+1z) activation function, which enables easier progressive training of HashNet as the network is becoming non-smooth in later stages by \u03b2t \u2192 \u221e. Using m = 10 we can already achieve fast convergence for training HashNet."}, {"heading": "4. Experiment", "text": "We conduct extensive experiments to evaluate the efficacy of the HashNet approach against several state-of-theart hashing methods on three standard benchmark datasets. The codes and configurations will be made available online."}, {"heading": "4.1. Setup", "text": "The evaluation is conducted on three benchmark image retrieval datasets: ImageNet, NUS-WIDE and MS COCO.\nImageNet1 is a benchmark image dataset for Large Scale Visual Recognition Challenge (ILSVRC 2015) [29]. It contains over 1.2M images in the training set and 50K images in the validation set, where each image is single-labeled by one of the 1,000 categories. We randomly select 100 categories, use all the images of these categories in the training set as the database, and use all the images in the validation set as the queries; furthermore, we randomly select 100 images per category from the database as the training points.\nNUS-WIDE2 [4] is a public Web image dataset which contains 269,648 images downloaded from Flickr.com. Each image is manually annotated by some of the 81 ground truth concepts (categories) for evaluating retrieval models. We follow similar experimental protocols as DHN [40] and randomly sample 5,000 images as queries, with the remaining images used as the database; furthermore, we randomly sample 10,000 images from the database as training points.\nMS COCO3 [22] is an image recognition, segmentation, and captioning dataset. The current release contains 82,783 training images and 40,504 validation images, where each image is labeled by some of the 80 categories. After pruning images with no category information, we obtain 12,2218 images by combining the training and validation images. We randomly sample 5,000 images as queries, with the rest images used as the database; furthermore, we randomly sample 10,000 images from the database as training points.\nFollowing standard evaluation protocol as previous work [37, 20, 40], the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and sij = 1; otherwise, they are dissimilar and sij = 0. Note that, although we use the image labels to construct the similarity information, our proposed HashNet can learn hash codes when only the similarity information is available. By constructing the training data in this way, the ratio between the number of dissimilar pairs and the number of similar pairs is roughly 100, 5, and 1 for ImageNet, NUSWIDE, and MS COCO, respectively. These datasets exhibit the data imbalance phenomenon and can be used to evaluate different hashing methods under data imbalance scenario.\nWe compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40]. To comprehensively compare performance of different methods, we evaluate the retrieval\n1http://image-net.org 2http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm 3http://mscoco.org\nquality based on five standard evaluation metrics: Mean Average Precision (MAP), Precision-Recall curves (PR), Precision curves within Hamming distance 2 (P@H=2), Precision curves with respect to different numbers of top returned samples (P@N), and Histogram of learned codes without binarization. For fair comparison, all the methods use identical training and test sets. We adopt MAP@1000 for ImageNet dataset as each category has at most 1,300 images, and adopt MAP@5000 for the other two datasets as [40].\nFor shallow hashing methods, we use AlexNet-fc7 deep features [7] as input, and for deep hashing methods, we directly use raw image pixels as input. We adopt the AlexNet architecture [18] for all deep hashing methods, and implement the proposed HashNet based on the open-source Caffe framework [17]. We fine-tune convolutional layers conv1\u2013 conv5 and fully-connected layers fc6\u2013fc7 that were copied from the AlexNet model pre-trained on ImageNet 2012, and train the hash layer fch, all through back-propagation. As the fch layer is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use mini-batch stochastic gradient descent (SGD) with 0.9 momentum and the learning rate annealing strategy implemented in Caffe, and cross-validate the learning rate from 10\u22125 to 10\u22123 with a multiplicative step-size 10 1 2 . We fix the mini-batch size of images as 256 and the weight decay parameter as 0.0005."}, {"heading": "4.2. Results", "text": "The MAP results of all methods are demonstrated in Table 1. The proposed HashNet substantially outperforms all the comparison methods. Specifically, compared to the best shallow hashing method using deep features as input, ITQ/ITQ-CCA, we achieve absolute boosts of 13.5%, 15.5%, and 9.1% in average MAP for different bits on ImageNet, NUS-WIDE, and MS COCO, respectively. Compared to the state-of-the-art deep hashing method, DHN, we achieve absolute boosts of 12.4%, 3.7%, 2.9% in average MAP for different bits on the three datasets, respectively. An interesting phenomenon is that the performance boost of HashNet over DHN is significantly different across the three datasets. Specifically, the performance boost on ImageNet is much larger than that on NUS-WIDE and MS COCO by about 10%, which is very impressive. Recall that the ratio between the number of dissimilar pairs and the number of similar pairs is roughly 100, 5, and 1 for ImageNet, NUSWIDE and MS COCO, respectively. This data imbalance problem substantially deteriorates the performance of hashing methods that are trained from pairwise data, including all considered deep hashing methods. The proposed HashNet enables deep learning to hash from pairwise data by Weighted Maximum Likelihood (WML), which is a principled solution to tackling the data imbalance problem. This explains its superior performance especially on highly imbalanced datasets. Among all methods for evaluation, only\nITQ and ITQ-CCA are hashing methods trained from pointwise data, which is less deteriorated by the data imbalance problem. This explains their relatively better performance.\nThe retrieval performance in terms of Precision within Hamming radius 2 (P@H=2) is particularly important for efficient retrieval with binary hash codes, because such Hamming ranking only requires O(1) time cost for each query. As shown in Figures 2(a), 3(a) and 4(a), the proposed HashNet achieves the highest P@H=2 results on all three datasets. In particular, the P@H=2 of HashNet with 32 bits is better than that of DHN with any bit. This result validates that HashNet can learn more compact binary codes than DHN in that shorter codes can already establish more accurate results. It is worth noting that, when using longer hash codes, the Hamming space will become increasingly sparse and very few data points fall within the Hamming ball with radius 2 [9]. This is why HashNet achieves the best performance with reasonably shorter hash codes.\nThe retrieval performance on the three datasets in terms of Precision-Recall curves (PR) and Precision curves with respect to different numbers of top returned samples (P@N)\nare shown in Figures 2(b)\u223c4(b) and Figures 2(c)\u223c4(c), respectively. The proposed HashNet significantly outperforms all comparison methods by large margins in the two metrics. In particular, HashNet achieves much higher precision at lower recall levels or when the number of top returned samples is small. This is desirable for precision-first retrieval, which is widely implemented in practical systems. As an intuitive illustration, Figure 5 shows that HashNet can yield much more relevant and user-desired retrieval results."}, {"heading": "4.3. Discussion", "text": "Ablation Study: We go deeper into the efficacy of the weighted maximum likelihood and the continuation training. To this end, we investigate two variants of HashNet: (1) HashNet-W, the variant using standard maximum likelihood instead of weighted maximum likelihood, i.e. cij = 1,\u2200i, j; (2) HashNet-C, the variant using tanh() as activation function and generating continuous codes (requiring a separated binarization step to generate hash codes), instead of using continuation training with sgn() as activation function. We compare results of the HashNet variants in Table 2.\nBy weighted maximum likelihood estimation, HashNet outperforms HashNet-W by substantially large margins of 10.2%, 2.8% and 0.1% in average MAP for different bits on ImageNet, NUS-WIDE and MS COCO, respectively. The standard maximum likelihood estimation has been widely adopted in previous work [37, 40]. However, this estimation does not account for the data imbalance, and may suffer from performance drop when training data is highly imbalanced (e.g. ImageNet). In contrast, the proposed weighted maximum likelihood estimation (1) is a principled solution to tackling the data imbalance problem by weighting the training pairs according to the importance of misclassifying that pair. Recall that MS COCO is a balanced dataset, hence HashNet and HashNet-W may yield similar MAP results.\nBy training HashNet with continuation, HashNet outperforms HashNet-C by substantial margins of 5.9%, 1.4% and 3.0% in average MAP on ImageNet, NUS-WIDE, and MS COCO, respectively. Due to the vanishing gradient problem, existing deep hashing methods cannot learn exactly binary hash codes using sgn() as activation function. Instead, they need to use surrogate functions of sgn(), e.g. tanh(), as the activation function and learn continuous codes, which require a separated binarization step to generate hash codes. Fundamentally, by continuous relaxation, existing methods solve an optimization problem which may deviate significantly from the original hashing problem, resulting in suboptimal binary codes. The proposed continuation training strategy is a principled solution to deep learning to hash with sgn() as activation function, which can learn lossless binary hash codes to enable high-quality similarity retrieval.\nHistogram of Codes Without Binarization: As discussed previously, the proposed HashNet can learn exactly binary hash codes while previous deep hashing methods can only learn continuous codes and generate binary hash codes by post-step sign thresholding. To verify this key property, we plot the histograms of codes learned by HashNet and\nDHN on the three datasets without post-step binarization. The histograms can be plotted by evenly dividing [0, 1] into 100 bins, and calculating the frequency of codes falling into each bin. To make the histograms more readable, we show absolute code values and squared root of frequency. The histograms in Figure 6 show that the DHN method can only generate continuous codes spanning across the whole range of [0, 1]. This means that if we quantize these continuous codes into binary hash codes (taking values in {\u22121, 1}) in a post-step, we may suffer from large quantization error especially for the codes near zero. On the contrary, the codes of HashNet without binarization are already exactly binary.\nVisualization of Hash Codes: We visualize the t-SNE [7] of hash codes generated by HashNet and DHN on ImageNet (for ease of visualization, we sample 10 categories) in Figure 7. We observe that the hash codes generated by HashNet show clear discriminative structures in that different categories are well separated, while the hash codes generated by DHN do not show such discriminative structures. This suggests that HashNet can learn more discriminative hash codes than DHN for more effective similarity retrieval."}, {"heading": "5. Conclusion", "text": "This paper addressed deep learning to hash from imbalanced similarity data by the continuation method. The proposed HashNet can learn exactly binary hash codes by optimizing a novel weighted pairwise cross-entropy loss function in deep convolutional neural networks. HashNet can be effectively trained by the proposed multi-stage pre-training algorithm carefully crafted from the continuation method. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks."}], "references": [{"title": "Numerical continuation methods: an introduction, volume 13", "author": ["E.L. Allgower", "K. Georg"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Nus-wide: A real-world web image database from national university of singapore", "author": ["T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.-T. Zheng"], "venue": "In ICMR. ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Maximum likelihood in cost-sensitive learning: Model specification, approximations, and upper bounds", "author": ["J.P. Dmochowski", "P. Sajda", "L.C. Parra"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep hashing for compact binary codes learning", "author": ["V. Erin Liong", "J. Lu", "G. Wang", "P. Moulin", "J. Zhou"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["D.J. Fleet", "A. Punjani", "M. Norouzi"], "venue": "In CVPR. IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In VLDB,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Y. Gong", "S. Kumar", "H. Rowley", "S. Lazebnik"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 2016", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "In NIPS, pages 1042\u20131050,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "In CVPR. IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Content-based multimedia information retrieval: State of the art and challenges", "author": ["M.S. Lew", "N. Sebe", "C. Djeraba", "R. Jain"], "venue": "ACM Trans. Multimedia Comput. Commun. Appl.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In CVPR. IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "In ICML. ACM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Hash bit selection: a unified solution for selection problems in hashing", "author": ["X. Liu", "J. He", "B. Lang", "S.-F. Chang"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In J. Fu\u0308rnkranz and T. Joachims, editors,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.M. Blei"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D.M. Blei", "R.R. Salakhutdinov"], "venue": "In NIPS, pages 1061\u20131069,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H. Tao Shen"], "venue": "In CVPR", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Content-based image retrieval at the end of the early years", "author": ["A.W. Smeulders", "M. Worring", "S. Santini", "A. Gupta", "R. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1929}, {"title": "Semi-supervised hashing for large-scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Hashing for similarity search: A survey", "author": ["J. Wang", "H.T. Shen", "J. Song", "J. Ji"], "venue": "Arxiv, 2014", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan"], "venue": "In AAAI, pages 2156\u20132162", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Circulant binary embedding", "author": ["F.X. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "In ICML, pages 353\u2013360", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Supervised hashing with latent factor models", "author": ["P. Zhang", "W. Zhang", "W.-J. Li", "M. Guo"], "venue": "In SIGIR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Deep hashing network for efficient similarity retrieval", "author": ["H. Zhu", "M. Long", "J. Wang", "Y. Cao"], "venue": "In AAAI. AAAI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "Parallel to the traditional indexing methods [21], another advantageous solution is hashing methods [35], which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items.", "startOffset": 45, "endOffset": 49}, {"referenceID": 34, "context": "Parallel to the traditional indexing methods [21], another advantageous solution is hashing methods [35], which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items.", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "In this paper, we will focus on learning to hash methods [35] that build data-dependent hash encoding schemes for efficient image retrieval, which have shown better performance than data-independent hashing methods, e.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "Locality-Sensitive Hashing (LSH) [10].", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 11, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 26, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 8, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 22, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 33, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 24, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 10, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 37, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 38, "context": "Many learning to hash methods have been proposed to enable efficient ANN search of high-dimensional data by ranking the Hamming distance across compact binary hash codes [19, 12, 27, 9, 23, 34, 25, 11, 38, 39].", "startOffset": 170, "endOffset": 209}, {"referenceID": 31, "context": "While unsupervised methods are more general and can be trained without semantic labels or relevance information, they are subject to the semantic gap dilemma [32] that high-level semantic description of an object often differs from low-level feature descriptors.", "startOffset": 158, "endOffset": 162}, {"referenceID": 36, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 40, "endOffset": 59}, {"referenceID": 19, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 40, "endOffset": 59}, {"referenceID": 30, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 40, "endOffset": 59}, {"referenceID": 7, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 40, "endOffset": 59}, {"referenceID": 39, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 40, "endOffset": 59}, {"referenceID": 17, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 187, "endOffset": 194}, {"referenceID": 1, "context": "Recently, deep learning to hash methods [37, 20, 31, 8, 40] have shown that end-to-end learning of feature representation and hash coding can be more effective using deep neural networks [18, 2], which can naturally encode any nonlinear hash functions.", "startOffset": 187, "endOffset": 194}, {"referenceID": 39, "context": "In particular, it proves crucial to jointly learn similarity-preserving representations and control quantization error of binarizing continuous representations to binary codes [40].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "This is known as the vanishing gradient problem, which is the key difficulty in training deep neural networks via back-propagation [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Specifically, this paper attacks the vanishing gradient problem in the nonconvex optimization of the deep networks with non-smooth sign activation by continuation methods [1], which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize.", "startOffset": 171, "endOffset": 174}, {"referenceID": 34, "context": "Please refer to [35] for a comprehensive survey.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "Typical learning criteria include reconstruction error minimization [30, 12, 16] and graph learning[36, 24].", "startOffset": 68, "endOffset": 80}, {"referenceID": 11, "context": "Typical learning criteria include reconstruction error minimization [30, 12, 16] and graph learning[36, 24].", "startOffset": 68, "endOffset": 80}, {"referenceID": 15, "context": "Typical learning criteria include reconstruction error minimization [30, 12, 16] and graph learning[36, 24].", "startOffset": 68, "endOffset": 80}, {"referenceID": 35, "context": "Typical learning criteria include reconstruction error minimization [30, 12, 16] and graph learning[36, 24].", "startOffset": 99, "endOffset": 107}, {"referenceID": 23, "context": "Typical learning criteria include reconstruction error minimization [30, 12, 16] and graph learning[36, 24].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "Binary Reconstruction Embedding (BRE) [19] pursues hash functions by minimizing the squared errors between the distances of data points and the distances of their corresponding hash codes.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "Minimal Loss Hashing (MLH) [27] and Hamming Distance Metric Learning [28] learn hash codes by minimizing hinge-like loss functions based on similarity of data points.", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "Minimal Loss Hashing (MLH) [27] and Hamming Distance Metric Learning [28] learn hash codes by minimizing hinge-like loss functions based on similarity of data points.", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "Supervised Hashing with Kernels (KSH) [23] builds compact binary hash codes by minimizing the Hamming distances across similar pairs and maximizing the Hamming distances across dissimilar pairs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "As deep convolutional neural network (CNN) [18, 13] yield breakthrough performance on many computer vision tasks, deep learning to hash has attracted attention recently.", "startOffset": 43, "endOffset": 51}, {"referenceID": 12, "context": "As deep convolutional neural network (CNN) [18, 13] yield breakthrough performance on many computer vision tasks, deep learning to hash has attracted attention recently.", "startOffset": 43, "endOffset": 51}, {"referenceID": 36, "context": "CNNH [37] adopts a two-stage strategy in which the first stage learns hash codes and the second stage learns a deepnetwork based hash function to fit the codes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "DNNH [20] improved the two-stage CNNH with a simultaneous feature learning and hash coding pipeline such that representations and hash codes can be optimized in a joint learning process.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "DHN [40] further improves DNNH by a cross-entropy loss and a quantization loss which preserve the pairwise similarity and control the quantization error simultaneously.", "startOffset": 4, "endOffset": 8}, {"referenceID": 5, "context": "To perform deep learning to hash from imbalanced data, we jointly preserve similarity information of pairwise images and generate binary hash codes by weighted maximum likelihood [6].", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "where P (S|H) is the weighted likelihood function, and cij is the weight for each training pair (xi,xj , sij), which is used to tackle the data imbalance problem by weighting the training pairs according to the importance of misclassifying that pair [6].", "startOffset": 250, "endOffset": 253}, {"referenceID": 13, "context": "This is known as the vanishing gradient problem, which has been a key difficulty in training deep neural networks via back-propagation [14].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [14, 3], dropout [33], batch normalization [15], and deep residual learning [13].", "startOffset": 187, "endOffset": 194}, {"referenceID": 2, "context": "Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [14, 3], dropout [33], batch normalization [15], and deep residual learning [13].", "startOffset": 187, "endOffset": 194}, {"referenceID": 32, "context": "Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [14, 3], dropout [33], batch normalization [15], and deep residual learning [13].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [14, 3], dropout [33], batch normalization [15], and deep residual learning [13].", "startOffset": 230, "endOffset": 234}, {"referenceID": 12, "context": "Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [14, 3], dropout [33], batch normalization [15], and deep residual learning [13].", "startOffset": 263, "endOffset": 267}, {"referenceID": 25, "context": "In particular, Rectifier Linear Unit (ReLU) [26] activation function makes deep networks much easier to train and enables end-to-end learning algorithms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "A very recent work, BinaryNet [5], focuses on training deep networks with activations constrained to +1 or \u22121.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "recent studies in continuation methods [1], which address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize.", "startOffset": 39, "endOffset": 42}, {"referenceID": 28, "context": "ImageNet1 is a benchmark image dataset for Large Scale Visual Recognition Challenge (ILSVRC 2015) [29].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "NUS-WIDE2 [4] is a public Web image dataset which contains 269,648 images downloaded from Flickr.", "startOffset": 10, "endOffset": 13}, {"referenceID": 39, "context": "We follow similar experimental protocols as DHN [40] and randomly sample 5,000 images as queries, with the remaining images used as the database; furthermore, we randomly sample 10,000 images from the database as training points.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "MS COCO3 [22] is an image recognition, segmentation, and captioning dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "Following standard evaluation protocol as previous work [37, 20, 40], the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and sij = 1; otherwise, they are dissimilar and sij = 0.", "startOffset": 56, "endOffset": 68}, {"referenceID": 19, "context": "Following standard evaluation protocol as previous work [37, 20, 40], the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and sij = 1; otherwise, they are dissimilar and sij = 0.", "startOffset": 56, "endOffset": 68}, {"referenceID": 39, "context": "Following standard evaluation protocol as previous work [37, 20, 40], the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and sij = 1; otherwise, they are dissimilar and sij = 0.", "startOffset": 56, "endOffset": 68}, {"referenceID": 9, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 133, "endOffset": 137}, {"referenceID": 35, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 220, "endOffset": 224}, {"referenceID": 36, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 259, "endOffset": 263}, {"referenceID": 19, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 270, "endOffset": 274}, {"referenceID": 39, "context": "We compare the retrieval performance of HashNet with nine conventional or state-of-the-art hashing methods: unsupervised methods LSH [10], SH [36] and ITQ [12], supervised shallow methods BRE [19], KSH [23], and ITQ-CCA [12], and supervised deep methods CNNH [37], DNNH [20], and DHN [40].", "startOffset": 284, "endOffset": 288}, {"referenceID": 39, "context": "We adopt MAP@1000 for ImageNet dataset as each category has at most 1,300 images, and adopt MAP@5000 for the other two datasets as [40].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "For shallow hashing methods, we use AlexNet-fc7 deep features [7] as input, and for deep hashing methods, we directly use raw image pixels as input.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "We adopt the AlexNet architecture [18] for all deep hashing methods, and implement the proposed HashNet based on the open-source Caffe framework [17].", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "We adopt the AlexNet architecture [18] for all deep hashing methods, and implement the proposed HashNet based on the open-source Caffe framework [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 39, "context": "7362 DHN [40] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "6944 DNNH [20] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "6099 CNNH [37] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "5671 KSH [23] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "5361 ITQ-CCA [12] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "5019 ITQ [12] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "6574 BRE [19] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "6336 SH [36] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "5101 LSH [10] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "It is worth noting that, when using longer hash codes, the Hamming space will become increasingly sparse and very few data points fall within the Hamming ball with radius 2 [9].", "startOffset": 173, "endOffset": 176}, {"referenceID": 36, "context": "The standard maximum likelihood estimation has been widely adopted in previous work [37, 40].", "startOffset": 84, "endOffset": 92}, {"referenceID": 39, "context": "The standard maximum likelihood estimation has been widely adopted in previous work [37, 40].", "startOffset": 84, "endOffset": 92}, {"referenceID": 0, "context": "The histograms can be plotted by evenly dividing [0, 1] into 100 bins, and calculating the frequency of codes falling into each bin.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "The histograms in Figure 6 show that the DHN method can only generate continuous codes spanning across the whole range of [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 6, "context": "Visualization of Hash Codes: We visualize the t-SNE [7] of hash codes generated by HashNet and DHN on ImageNet (for ease of visualization, we sample 10 categories) in Figure 7.", "startOffset": 52, "endOffset": 55}], "year": 2017, "abstractText": "Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the vanishing gradient difficulty in the optimization with binary activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This paper presents HashNet, a novel deep architecture for deep learning to hash by continuation method, which learns exactly binary hash codes from imbalanced similarity data where the number of similar pairs is much smaller than the number of dissimilar pairs. The key idea is to attack the vanishing gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.", "creator": "LaTeX with hyperref package"}}}