{"id": "1702.00783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Pixel Recursive Super Resolution", "abstract": "we present a pixel recursive super resolution model that synthesizes realistic details into images worth enhancing initial resolution. a low resolution image may correspond to two plausible high resolution images, thus modeling the super resolution process whilst a pixel independent conditional model often results in averaging different details - - especially merging edges. by presentation, our model is able to represent a highly conditional trajectory whenever properly modeling the statistical dependencies among the high resolution image pixels, conditioned since pure low scale assumption. we employ a pixelcnn interface to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. human evaluations indicate that samples from our proposed model look more photo realistic than a strong l2 regression baseline.", "histories": [["v1", "Thu, 2 Feb 2017 18:59:17 GMT  (3565kb,D)", "http://arxiv.org/abs/1702.00783v1", null], ["v2", "Wed, 22 Mar 2017 16:13:21 GMT  (4255kb,D)", "http://arxiv.org/abs/1702.00783v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ryan dahl", "mohammad norouzi", "jonathon shlens"], "accepted": false, "id": "1702.00783"}, "pdf": {"name": "1702.00783.pdf", "metadata": {"source": "CRF", "title": "Pixel Recursive Super Resolution", "authors": ["Ryan Dahl", "Mohammad Norouzi", "Jonathon Shlens"], "emails": ["rld@google.com", "mnorouzi@google.com", "shlens@google.com"], "sections": [{"heading": "1. Introduction", "text": "The problem of super resolution entails artificially enlarging a low resolution photograph to recover a plausible high resolution version of it. When the zoom factor is large, the input image does not contain all of the information necessary to accurately construct a high resolution image. Thus, the problem is underspecified and many plausible high resolution images exist that match the low resolution input image. This problem is significant for improving the state-of-the-art in super resolution, and more generally for building better conditional generative models of images.\nA super resolution model must account for the complex variations of objects, viewpoints, illumination, and occlusions, especially as the zoom factor increases. When some details do not exist in the source image, the challenge lies not only in \u2018deblurring\u2019 an image, but also in generating\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency).\nnew image details that appear plausible to a human observer. Generating realistic high resolution images is not possible unless the model draws sharp edges and makes hard decisions about the type of textures, shapes, and patterns present at different parts of an image.\nImagine a low resolution image of a face, e.g., the 8\u00d78 images depicted in the left column of Figure 1\u2013the details\n1\nar X\niv :1\n70 2.\n00 78\n3v 1\n[ cs\n.C V\n] 2\nF eb\n2 01\nof the hair and the skin are missing. Such details cannot be faithfully recovered using simple interpolation techniques such as linear or bicubic. However, by incorporating the prior knowledge of the faces and their typical variations, an artist is able to paint believable details. In this paper, we show how a fully probabilistic model that is trained end-toend can play the role of such an artist by synthesizing 32\u00d732 face images depicted in the middle column of Figure 1. Our super resolution model comprises two components that are trained jointly: a conditioning network, and a prior network. The conditioning network effectively maps a low resolution image to a distribution over corresponding high resolution images, while the prior models high resolution details to make the outputs look more realistic. Our conditioning network consists of a deep stack of ResNet [10] blocks, while our prior network comprises a PixelCNN [28] architecture.\nWe find that standard super resolution metrics such as peak signal-to-noise ratio (pSNR) and structural similarity (SSIM) fail to properly measure the quality of predictions for an underspecified super resolution task. These metrics prefer conservative blurry averages over more plausible photo realistic details, as new fine details often do not align exactly with the original details. Our evaluation studies demonstrate that humans easily distinguish real images from super resolution predictions when regression techniques are used, but they have a harder time telling our samples apart from real images."}, {"heading": "2. Related work", "text": "Super resolution has a long history in computer vision [22]. Methods relying on interpolation [11] are easy to implement and widely used, however these methods suffer from a lack of expressivity since linear models cannot express complex dependencies between the inputs and outputs. In practice, such methods often fail to adequately predict high frequency details leading to blurry high resolution outputs.\nEnhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17]. Much work has been done on algorithms that search a database of patches and combine them to create plausible high frequency details in zoomed images [7, 13]. Recent patch-based work has focused on improving basic interpolation methods by building a dictionary of pre-learned filters on images and selecting the appropriate patches by an efficient hashing mechanism [23]. Such dictionary methods have improved the inference speed while being comparable to state-of-the-art.\nAnother approach for super resolution is to abandon inference speed requirements and focus on constructing the\nhigh resolution images at increasingly higher magnification factors. Convolutional neural networks (CNNs) represent an approach to the problem that avoids explicit dictionary construction, but rather implicitly extracts multiple layers of abstractions by learning layers of filter kernels. Dong et al. [5] employed a three layer CNN with MSE loss. Kim et al. [16] improved accuracy by increasing the depth to 20 layers and learning only the residuals between the high resolution image and an interpolated low resolution image. Most recently, SRResNet [18] uses many ResNet blocks to achieve state of the art pSNR and SSIM on standard super resolution benchmarks\u2013we employ a similar design for our conditional network and catchall regression baseline.\nInstead of using a per-pixel loss, Johnson et al.[14] use Euclidean distance between activations of a pre-trained CNN for model\u2019s predictions vs. ground truth images. Using this so-called preceptual loss, they train feed-forward networks for super resolution and style transfer. Bruna et al. [3] also use perceptual loss to train a super resolution network, but inference is done via gradient propagation to the low-res input (e.g., [9]).\nLedig et al. [18] and Yu et al. [33] use GANs to create compelling super resolution results showing the ability of the model to predict plausible high frequency details. S\u00f8nderby et al. [15] also investigate GANs for super resolution using a learned affine transformation that ensures the models only generate images that downscale back to the low resolution inputs. S\u00f8nderby et al. [15] also explore a masked autoregressive model like PixelCNN [27] but without the gated layers and using a mixture of gaussians instead of a multinomial distribution. Denton et al. [4] use a multi-scale adversarial network for image synthesis, but the architecture also seems beneficial for super resolution.\nPixelRNN and PixelCNN by Oord et al. [27, 28] are probabilistic generative models that impose an order on image pixels representing them as a long sequence. The probability of each pixel is then conditioned on the previous ones. The gated PixelCNN obtained state of the art log-likelihood scores on CIFAR-10 and MNIST, making it one of the most competetive probabilistic generative models.\nSince PixelCNN uses log-likelihood for training, the model is highly penelized if negligible probability is assigned to any of the training examples. By contrast, GANs only learn enough to fool a non-stationary discriminator. One of their common failure cases is mode collapsing were samples are not diverse enough [21]. Furthermore, GANs require careful tuning of hyperparameters to ensure the discriminator and generator are equally powerful and learn at equal rates. PixelCNNs are more robust to hyperparameter changes and usually have a nicely decaying loss curve. Thus, we adopt PixelCNN for super resolution applications."}, {"heading": "3. Probabilistic super resolution", "text": "We aim to learn a probabilistic super resolution model that discerns the statistical dependencies between a high resolution image and a corresponding low resolution image. Let x and y denote a low resolution and a high resolution image, where y\u2217 represents a ground-truth high resolution image. In order to learn a parametric model of p\u03b8(y | x), we exploit a large dataset of pairs of low resolution inputs and ground-truth high resolution outputs, denoted D \u2261 {(x(i),y\u2217(i))}Ni=1. One can easily collect such a large dataset by starting from a set of high resolution images and lowering their resolution as much as needed. To optimize the parameters \u03b8 of the conditional distribution p, we maximize a conditional log-likelihood objective defined as,\nO(\u03b8 | D) = \u2211\n(x,y\u2217)\u2208D\nlog p(y\u2217 | x) . (1)\nThe key problem discussed in this paper is the exact form of p(y | x) that enables efficient learning and inference, while generating realistic non-blurry outputs. We first discuss pixel-independent models that assume that each output pixel is generated with an independent stochastic process given the input. We elaborate why these techniques result in sub-optimal blurry super resolution results. Finally we describe our pixel recursive super resolution model that generates output pixels one at a time to enable modeling the statistical dependencies between the output pixels using PixelCNN [27, 28], and synthesizes sharp images from very blurry input."}, {"heading": "3.1. Pixel independent super resolution", "text": "The simplest form of a probabilistic super resolution model assumes that the output pixels are conditionally independence given the inputs. As such, the conditional distribution of p(y | x) factorizes into a product of independent pixel predictions. Suppose an RGB output y has M pixels each with three color channels, i.e., y \u2208 R3M . Then,\nlog p(y | x) = 3M\u2211 i=1 log p(yi | x) . (2)\nTwo general forms of pixel prediction models have been explored in the literature: Gaussian and multinomial distributions to model continuous and discrete pixel values respectively. In the Gaussian case,\nlog p(yi | x) = \u2212 1\n2\u03b42 \u2016yi \u2212 Ci(x)\u201622 \u2212 log\n\u221a 2\u03b42\u03c0 , (3)\nwhere Ci(x) denotes the ith element of a non-linear transformation of x via a convolutional neural network. Ci(x) is the estimated mean for the ith output pixel yi, and \u03c32 denotes the variance. Often the variance is not learned, in\nHow the dataset was created\nwhich case maximizing the conditional log-likelihood of (1) reduces to minimizing the mean squared error (MSE) between yi and Ci(x) across the pixels and channels throughout the dataset. Super resolution models based on MSE regression (e.g., [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.\nAlternatively, one could use a flexible multinomial distribution as the pixel prediction model, in which case the ouput dimensions are discretized into K possible values (e.g., K = 256) where yi \u2208 {1, . . . ,K}. The pixel prediction model based on a multinomial softmax operator is represented as, log p(yi = k | x) = wjkTCi(x)\u2212log K\u2211 v=1 exp{wjvTCi(x)} , (4) where {wjk}3,Kj=1,k=1 denote the softmax weights for different color channels and different discrete values."}, {"heading": "3.2. Synthetic multimodal task", "text": "To demonstrate how the above pixel independent models can fail at conditional image modeling, we created a synthetic dataset that is explicitly multimodal. For many generative tasks like super resolution, colorization, and depth estimation, models that are able to predict a mode without averaging effects are desirable. For example, in coloriza-\ntion, selecting a strong red or blue for a car is better than selecting a sepia toned average of all of the colors of cars that the model has been exposed to. In this synthetic task, the input is an MNIST digit (1st row of Figure 2), and the output is the same input digit but scaled and translated either into the upper left corner or upper right corner (2nd and 3rd rows of Figure 2). The dataset has an equal ratio of upper left and upper right outputs, which we call the MNIST corners dataset.\nA convolutional network using per pixel squared error loss (Figure 2, L2 Regression) produces two blurry figures. Replacing the continuous loss with a per-pixel crossentropy produces crisper images but also fails to capture the stochastic bimodality because both digits are shown in both corners (Figure 2, cross-entropy). In contrast, a model that explicitly deals with multi-modality, PixelCNN stochastically predicts a digit in the upper-left or bottom-right corners but never predicts both digits simultaneously (Figure 2, PixelCNN).\nSee Figure 5 for examples of our super resolution model predicting different modes on a realistic dataset.\nAny good generative model should be able to make sharp single mode predictions and a dataset like this would be a good starting point for any new models."}, {"heading": "4. Pixel recursive super resolution", "text": "The main issue with the previous probabilistic models (Equations (3) and (4)) for super resolution is the lack of conditional dependency between super resolution pixels. There are two general methods to model statistical correlations between output pixels. One approach is to define the conditional distribution of the output pixels jointly by either a multivariate Gaussian mixture [36] or an undirected graphical model such as conditional random fields [8]. With these approaches one has to commit to a particular form of statistical dependency between the output pixels, for which inference can be computationally expensive. The second approach that we follow in this work, is to factorize the conditional distribution using chain rule as,\nlog p(y | x) = M\u2211 i=1 log p(yi | x,y<i) , (5)\nwhere the generation of each output dimension is conditioned on the input, previous output pixels, and the previous channels of the same output pixel. For simplicity of exposition, we ignore different output channels in our derivations, and use y<i to represent {y1, . . . ,yi\u22121}. The benefits of this approach is that the exact form of the conditional dependencies is flexible and the inference is straightforward. Inspired by the PixelCNN model, we use a multinomial distribution to model discrete pixel values in Eq. (5). Alternatively, one could use an autoregressive prediction model\nwith Gaussian or Logistic (mixture) conditionals as proposed in [24].\nOur model, outlined in Figure 3, comprises two major components that are fused together at a late stage and trained jointly: (1) a conditioning network (2) a prior network. The conditioning network is a pixel independent prediction model that maps a low resolution image to a probabilistic skeleton of a high resolution image, while the prior network is supposed to add natural high resolution details to make the outputs look more realistic.\nGiven an input x \u2208 RL, let Ai(x) : RL \u2192 RK denote a conditioning network predicting a vector of logit values corresponding to the K possible values that the ith output pixel can take. Similarly, let Bi(y<i) : Ri\u22121 \u2192 RK denote a prior network predicting a vector of logit values for the ith output pixel. Our probabilistic model predicts a distribution over the ith output pixel by simply adding the two sets of logits and applying a softmax operator on them,\np(yi | x,y<i) = softmax(Ai(x) +Bi(y<i)) . (6)\nTo optimize the parameters of A and B jointly, we perform stochastic gradient ascent to maximize the conditional log likelihood in (1). That is, we optimize a cross-entropy loss between the model\u2019s predictions in (6) and discrete\nground truth labels y\u2217i \u2208 {1, . . . ,K},\nO1 = \u2211\n(x,y\u2217)\u2208D M\u2211 i=1 ( 1[y\u2217i ] T (Ai(x) +Bi(y \u2217 <i))\n\u2212 lse(Ai(x) +Bi(y\u2217<i)) ) , (7)\nwhere lse(\u00b7) is the log-sum-exp operator corresponding to the log of the denominator of a softmax, and 1[k] denotes a K-dimensional one-hot indicator vector with its kth dimension set to 1.\nOur preliminery experiments indicate that models trained with (7) tend to ignore the conditioning network as the statistical correlation between a pixel and previous high resolution pixels is stronger than its correlation with low resolution inputs. To mitigate this issue, we include an additional loss in our objective to enforce the conditioning network to be optimized. This additional loss measures the cross-entropy between the conditioning network\u2019s predictions via softmax(Ai(x)) and ground truth labels. The total loss that is optimized in our experiments is a sum of two cross-entropy losses formulated as,\nO2 = \u2211\n(x,y\u2217)\u2208D M\u2211 i=1 ( 1[y\u2217i ] T (2Ai(x) +Bi(y \u2217 <i))\n\u2212 lse(Ai(x) +Bi(y\u2217<i))\u2212 lse(Ai(x)) ) . (8)\nOnce the network is trained, sampling from the model is straightforward. Using (6), starting at i = 1, first we sample a high resolution pixel. Then, we proceed pixel by pixel, feeding in the previously sampled pixel values back into the network, and draw new high resolution pixels. The three channels of each pixel are generated sequentially in turn.\nWe additionally consider greedy decoding, where one always selects the pixel value with the largest probability and sampling from a tempered softmax, where the concentration of a distribution p is adjusted by using a temperature parameter \u03c4 > 0,\np\u03c4 = p\u03c4\n\u2016p\u03c4\u20161 .\nTo control the concentration of our sampling distribution p(yi | x,y<i), it suffices to multiply the logits from A and B by a parameter \u03c4 . Note that as \u03c4 goes towards\u221e, the distribution converges to the mode1, and sampling converges to greedy decoding."}, {"heading": "4.1. Implementation details", "text": "The conditioning network is a feed-forward convolutional neural network that takes an 8\u00d78 RGB image through\n1We use a non-standard notion of temperature that represents 1 \u03c4 in the standard notation.\na series of ResNet [10] blocks and transpose convolution layers while maintaining 32 channels throughout. The last layer uses a 1\u00d71 convolution to increase the channels to 256\u00d73 and uses the resulting activations to predict a multinomial distribution over 256 possible sub-pixel values via a softmax operator.\nThis network provides the ability to absorb the global structure of the image in the marginal probability distribution of the pixels. Due to the softmax layer it can capture the rich intricacies of the high resolution distribution, but we have no way to coherently sample from it. Sampling sub-pixels independently will mix the assortment of distributions.\nThe prior network provides a way to tie together the subpixel distributions and allow us to take samples dependent on each other. We use 20 gated PixelCNN layers with 32 channels at each layer. We leave conditioning until the late stages of the network, where we add the pre-softmax activations from the conditioning network and prior network before computing the final joint softmax distribution.\nOur model is built by using TensorFlow [1] and trained across 8 GPUs with synchronous SGD updates. See appendix A for further details."}, {"heading": "5. Experiments", "text": "We assess the effectiveness of the proposed pixel recursive super resolution method on two datasets containing small faces and bedroom images. The first dataset is a version of the CelebA dataset [19] composed of a set of celebrity faces, which are cropped around the face. In the second dataset LSUN Bedrooms [32], images are center cropped. In both datasets we resize the images to 32\u00d732 with bicubic interpolation and again to 8\u00d78, constituting the output and input pairs for training and evaluation. We present representative super resolution examples on held out test sets and report human evaluations of our predictions in Table 1.\nWe compare our results with two baselines: a pixel independent L2 regression (\u201cRegression\u201d) and a nearest neighbors search (\u201cNN\u201c). The architecture used for the regression baseline is identical to the conditioning network used in our model, consisting of several ResNet blocks and upsampling convolutional layers, except that the baseline regression model outputs three channels and has a final tanh(\u00b7) non-linearity instead of ReLU. The regression architecture is similar in design to to SRResNet [18], which reports state of the art scores in image similarity metrics. Furthermore, we train the regression network to predict super resolution residuals instead of the actual pixel values. The residuals are computed based on bicubic interpolation of the input, and are known to work better to provide superior predictions [16]. The nearest neighbors baseline searches the downsampled training set for the nearest example (using eu-\nclidean distance) and returns its high resolution counterpart."}, {"heading": "5.1. Sampling", "text": "Sampling from the model multiple times results in different high resolution images for a given low resolution image (Figure 5). A given model will identify many plausible high resolution images that correspond to a given lower resolution image. Each one of these samples may contain distinct qualitative features and each of these modes is contained within the PixelCNN. Note that the differences between samples for the faces dataset are far less drastic than seen in our synthetic dataset, where failure to cleanly predict modes meant complete failure.\nThe quality of samples is sensitive to the softmax temperature. When the mode is sampled (\u03c4 = \u221e) at each subpixel, the samples are of poor quality, they look smooth with horizontal and vertical line artifacts. Sampling at \u03c4 = 1.0, the exact probability given by the model, tend to be more jittery with high frequency content. It seems in this case there are multiple less certain trajectories and the samples\njump back and forth between them\u2013perhaps this is alleviated with more capacity and training time. Manually tuning the softmax temperature was necessary to find good looking samples\u2013usually a value between 1.1 and 1.3 worked.\nIn Figure 6 are various test predictions with their negative log probability scores listed below each image. Smaller scores means the model has assigned that image a larger probability mass. The greedy, bicubic, and regression faces are preferred by the model despite being worse quality. This is probably because their smooth face-like structure doesn\u2019t contradict the learned distributions. Yet sampling with the proper softmax temperature nevertheless finds realistic looking images."}, {"heading": "5.2. Image similarity", "text": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20]. We quantified the prediction accuracy of our model compared to ground truth using pSNR and MS-SSIM (Table 1). We found that our own visual assessment of the predicted image quality did not correspond to these image similarities metrics. For instance, bicubic interpolation achieved relatively high metrics even though the samples appeared quite poor. This result matches recent observations that suggest that pSNR and SSIM provide poor judgements of super resolution quality [18, 14] when new details are synthesized.\nTo ensure that samples do indeed correspond to the lowresolution input, we measured how consistent the high resolution output image is with the low resolution input image (Table 1, \u2019consistency\u2019). Specifically, we measured the L2 distance between the low-resolution input image and a bicubic downsampled version of the high resolution estimate. Lower L2 distances correspond to high resolutions that are more similar to the original low resolution image. Note that the nearest neighbor high resolution images are less consistent even though we used a database of 3 million training images to search for neighbors in the case of LSUN bedrooms. In contrast, the bicubic resampling and the PixelCNN upsampling methods showed consistently better consistency with the low resolution image. This indicates that\nour samples do indeed correspond to the low-resolution input."}, {"heading": "5.3. Human study", "text": "We presented crowd sourced workers with two images: a true image and the corresponding prediction from our various models. Workers were asked \u201cWhich image, would you guess, is from a camera?\u201d Following the setup in Zhang et al [34], we present each image for one second at a time before allowing them to answer. Workers are started with 10 practice pairs during which they get feedback if they choose correctly or not. The practice pairs not counted in the results. After the practice pairs, they are shown 45 additional pairs, 5 of which are golden questions designed to test if the person is paying attention. The golden question pits a bicubicly upsampled image (very blurry) against the ground truth. Excluding the golden and practice questions, we count fourty answers per session. Sessions in which they missed any golden questions are thrown out. Workers were only allowed to participate in any of our studies once. We continued running sessions until fourty different different workers were tested on each of the four algorithms.\nWe report in Table 1 the percent of the time users choose an algorithm\u2019s output over the ground truth counterpart. Note that 50% would say that an algorithm perfectly confused the subjects."}, {"heading": "6. Conclusion", "text": "As in many image transformation tasks, the central problem of super resolution is in hallucinating sharp details by choosing a mode of the output distribution. We explored this underspecified problem using small images, demonstrating that even the smallest 8\u00d78 images can be enlarged to sharp 32\u00d732 images. We introduced a toy dataset with a small number of explicit modes to demonstrate the failure cases of two common pixel independent likelihood models. In the presented model, the conditioning network gets us most of the way towards predicting a high-resolution image, but the outputs are blurry where the model is uncertain. Combining the conditioning network with a PixelCNN model provides a strong prior over the output pixels, allowing the model to generate crisp predications. Our human evaluations indicate that samples from our model on average look more photo realistic than a strong regression based conditioning network alone."}, {"heading": "B. LSUN bedrooms samples", "text": "Input Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN"}, {"heading": "C. Cropped CelebA faces", "text": "Input Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN\nInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN"}], "references": [{"title": "and X", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu"], "venue": "Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Svdd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "Trans. Sig. Proc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Super-resolution with deep convolutional sufficient statistics", "author": ["J. Bruna", "P. Sprechmann", "Y. LeCun"], "venue": "CoRR, abs/1511.05666", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Image superresolution using deep convolutional networks", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "CoRR, abs/1501.00092", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Image upsampling via imposed edge statistics", "author": ["R. Fattal"], "venue": "ACM Trans. Graph.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Examplebased super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer graphics and Applications", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Markov networks for superresolution", "author": ["W.T. Freeman", "E.C. Pasztor"], "venue": "CISS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "CoRR, abs/1508.06576", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Cubic splines for image interpolation and digital filtering", "author": ["H. Hou", "H. Andrews"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Statistics of natural images and models", "author": ["J. Huang", "D. Mumford"], "venue": "Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on., volume 1. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Single image superresolution from transformed self-exemplars", "author": ["J.-B. Huang", "A. Singh", "N. Ahuja"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["J. Johnson", "A. Alahi", "F. Li"], "venue": "CoRR, abs/1603.08155", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Amortised MAP Inference for Image Superresolution", "author": ["C. Kaae S\u00f8nderby", "J. Caballero", "L. Theis", "W. Shi", "F. Husz\u00e1r"], "venue": "ArXiv e-prints,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Accurate image superresolution using very deep convolutional networks", "author": ["J. Kim", "J.K. Lee", "K.M. Lee"], "venue": "CoRR, abs/1511.04587", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Single-image super-resolution using sparse regression and natural image prior", "author": ["K.I. Kim", "Y. Kwon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(6):1127\u2013 1133", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Photo-realistic single im-  age super-resolution using a generative adversarial network", "author": ["C. Ledig", "L. Theis", "F. Huszar", "J. Caballero", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang", "W. Shi"], "venue": "arXiv:1609.04802", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Group mad competition - a new methodology to compare objective image quality models", "author": ["K. Ma", "Q. Wu", "Z. Wang", "Z. Duanmu", "H. Yong", "H. Li", "L. Zhang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Unrolled generative adversarial networks", "author": ["L. Metz", "B. Poole", "D. Pfau", "J. Sohl-Dickstein"], "venue": "CoRR, abs/1611.02163", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Super-resolution: A comprehensive survey", "author": ["K. Nasrollahi", "T.B. Moeslund"], "venue": "Mach. Vision Appl.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "RAISR: rapid and accurate image super resolution", "author": ["Y. Romano", "J. Isidoro", "P. Milanfar"], "venue": "CoRR, abs/1606.01299", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications. under review at ICLR 2017", "author": ["T. Salimans", "A. Karpathy", "X. Chen", "D.P. Kingma", "Y. Bulatov"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Fast image/video upsampling", "author": ["Q. Shan", "Z. Li", "J. Jia", "C.-K. Tang"], "venue": "ACM Transactions on Graphics (TOG), 27(5):153", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Image super-resolution using gradient profile prior", "author": ["J. Sun", "Z. Xu", "H.-Y. Shum"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "ICML, 2016", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu"], "venue": "NIPS, 2016", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE transactions on image processing, 13(4):600\u2013612", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Z. Wang", "E.P. Simoncelli", "A.C. Bovik"], "venue": "Signals, Systems and Computers, 2004. Conference Record of the Thirty-Seventh Asilomar Conference on, volume 2, pages 1398\u20131402. Ieee", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured face hallucination", "author": ["C.Y. Yang", "S. Liu", "M.H. Yang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "arXiv preprint arXiv:1506.03365", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Ultra-Resolving Face Images by Discriminative Generative Networks", "author": ["X. Yu", "F. Porikli"], "venue": "pages 318\u2013333. Springer International Publishing, Cham", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proceedings of the 2011 International Conference on Computer Vision, ICCV \u201911, pages 479\u2013486, Washington, DC, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "CVPR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "Our conditioning network consists of a deep stack of ResNet [10] blocks, while our prior network comprises a PixelCNN [28] architecture.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "Our conditioning network consists of a deep stack of ResNet [10] blocks, while our prior network comprises a PixelCNN [28] architecture.", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Super resolution has a long history in computer vision [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Methods relying on interpolation [11] are easy to implement and widely used, however these methods suffer from a lack of expressivity since linear models cannot express complex dependencies between the inputs and outputs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 65, "endOffset": 68}, {"referenceID": 34, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 25, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 5, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 11, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 24, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 16, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 6, "context": "Much work has been done on algorithms that search a database of patches and combine them to create plausible high frequency details in zoomed images [7, 13].", "startOffset": 149, "endOffset": 156}, {"referenceID": 12, "context": "Much work has been done on algorithms that search a database of patches and combine them to create plausible high frequency details in zoomed images [7, 13].", "startOffset": 149, "endOffset": 156}, {"referenceID": 22, "context": "Recent patch-based work has focused on improving basic interpolation methods by building a dictionary of pre-learned filters on images and selecting the appropriate patches by an efficient hashing mechanism [23].", "startOffset": 207, "endOffset": 211}, {"referenceID": 4, "context": "[5] employed a three layer CNN with MSE loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] improved accuracy by increasing the depth to 20 layers and learning only the residuals between the high resolution image and an interpolated low resolution image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Most recently, SRResNet [18] uses many ResNet blocks to achieve state of the art pSNR and SSIM on standard super resolution benchmarks\u2013we employ a similar design for our conditional network and catchall regression baseline.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "[14] use Euclidean distance between activations of a pre-trained CNN for model\u2019s predictions vs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] also use perceptual loss to train a super resolution network, but inference is done via gradient propagation to the low-res input (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "[18] and Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] use GANs to create compelling super resolution results showing the ability of the model to predict plausible high frequency details.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] also investigate GANs for super resolution using a learned affine transformation that ensures the models only generate images that downscale back to the low resolution inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] also explore a masked autoregressive model like PixelCNN [27] but without the gated layers and using a mixture of gaussians instead of a multinomial distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[15] also explore a masked autoregressive model like PixelCNN [27] but without the gated layers and using a mixture of gaussians instead of a multinomial distribution.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "[4] use a multi-scale adversarial network for image synthesis, but the architecture also seems beneficial for super resolution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27, 28] are probabilistic generative models that impose an order on image pixels representing them as a long sequence.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[27, 28] are probabilistic generative models that impose an order on image pixels representing them as a long sequence.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "One of their common failure cases is mode collapsing were samples are not diverse enough [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 26, "context": "Finally we describe our pixel recursive super resolution model that generates output pixels one at a time to enable modeling the statistical dependencies between the output pixels using PixelCNN [27, 28], and synthesizes sharp images from very blurry input.", "startOffset": 195, "endOffset": 203}, {"referenceID": 27, "context": "Finally we describe our pixel recursive super resolution model that generates output pixels one at a time to enable modeling the statistical dependencies between the output pixels using PixelCNN [27, 28], and synthesizes sharp images from very blurry input.", "startOffset": 195, "endOffset": 203}, {"referenceID": 4, "context": ", [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.", "startOffset": 2, "endOffset": 13}, {"referenceID": 15, "context": ", [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.", "startOffset": 2, "endOffset": 13}, {"referenceID": 17, "context": ", [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.", "startOffset": 2, "endOffset": 13}, {"referenceID": 35, "context": "One approach is to define the conditional distribution of the output pixels jointly by either a multivariate Gaussian mixture [36] or an undirected graphical model such as conditional random fields [8].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "One approach is to define the conditional distribution of the output pixels jointly by either a multivariate Gaussian mixture [36] or an undirected graphical model such as conditional random fields [8].", "startOffset": 198, "endOffset": 201}, {"referenceID": 27, "context": "The prior network, a PixelCNN [28], makes predictions based on previous stochastic predictions (indicated by dashed line).", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "with Gaussian or Logistic (mixture) conditionals as proposed in [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "a series of ResNet [10] blocks and transpose convolution layers while maintaining 32 channels throughout.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "Our model is built by using TensorFlow [1] and trained across 8 GPUs with synchronous SGD updates.", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "The first dataset is a version of the CelebA dataset [19] composed of a set of celebrity faces, which are cropped around the face.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "In the second dataset LSUN Bedrooms [32], images are center cropped.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "The regression architecture is similar in design to to SRResNet [18], which reports state of the art scores in image similarity metrics.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "The residuals are computed based on bicubic interpolation of the input, and are known to work better to provide superior predictions [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 29, "context": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 17, "context": "This result matches recent observations that suggest that pSNR and SSIM provide poor judgements of super resolution quality [18, 14] when new details are synthesized.", "startOffset": 124, "endOffset": 132}, {"referenceID": 13, "context": "This result matches recent observations that suggest that pSNR and SSIM provide poor judgements of super resolution quality [18, 14] when new details are synthesized.", "startOffset": 124, "endOffset": 132}, {"referenceID": 33, "context": "Workers were asked \u201cWhich image, would you guess, is from a camera?\u201d Following the setup in Zhang et al [34], we present each image for one second at a time before allowing them to answer.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "Consistency lists the MSE between the input low-res image and downsampled samples on a [0, 1] scale.", "startOffset": 87, "endOffset": 93}], "year": 2017, "abstractText": "We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details\u2013 hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.", "creator": "LaTeX with hyperref package"}}}