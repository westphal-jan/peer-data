{"id": "1702.02738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Joint Discovery of Object States and Manipulation Actions", "abstract": "modeling activities involve object structures aiming to modify object state. outcomes of common state changes include full / empty bottle, open / closed door, and attached / detached car trips. in this work, we seek to also discover the states producing objects and the associated grasping actions. given a set of videos for a recurring task, we propose a joint model that learns to control object states and to manipulate state - modifying actions. our model is formulated as a discriminative clustering cost. we assume a consistent temporal order for the changes in object states and manipulating actions, and learn the model without additional supervision. consensus method is defined on a new dataset describing videos depicting real - life object manipulations. we demonstrate their successful discovery of seven manipulating actions and corresponding object states. moreover, we emphasize our joint formulation concepts show continual improvement of perfect state discovery by action recognition and vice versa.", "histories": [["v1", "Thu, 9 Feb 2017 08:04:33 GMT  (2090kb,D)", "https://arxiv.org/abs/1702.02738v1", "12 pages"], ["v2", "Mon, 10 Apr 2017 08:23:00 GMT  (1965kb,D)", "http://arxiv.org/abs/1702.02738v2", "14 pages"], ["v3", "Mon, 28 Aug 2017 08:04:18 GMT  (3190kb,D)", "http://arxiv.org/abs/1702.02738v3", "Appears in: International Conference on Computer Vision 2017 (ICCV 2017). 15 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jean-baptiste alayrac", "josev sivic", "ivan laptev", "simon lacoste-julien"], "accepted": false, "id": "1702.02738"}, "pdf": {"name": "1702.02738.pdf", "metadata": {"source": "CRF", "title": "Joint Discovery of Object States and Manipulation Actions", "authors": ["Jean-Baptiste Alayrac", "Josef Sivic", "Ivan Laptev", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Many of our activities involve changes in object states. We need to open a book to read it, to cut bread before eating it and to lighten candles before taking out a birthday cake. Transitions of object states are often coupled with particular manipulation actions (open, cut, lighten). Moreover, the success of an action is often signified by reaching the desired state of an object (whipped cream, ironed shirt) and avoiding other states (burned shirt). Recognizing object states and manipulation actions is, hence, expected to become a key component of future systems such as wearable automatic assistants or home robots helping people in their daily tasks.\nHuman visual system can easily distinguish different states of objects, such as open/closed bottle or full/empty coffee cup [7]. Automatic recognition of object states and\n\u2217De\u0301partement dinformatique de l\u2019ENS, E\u0301cole normale supe\u0301rieure, CNRS, PSL Research University, 75005 Paris, France. \u2020INRIA \u2021Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague. \u00a7Department of CS & OR (DIRO), Universite\u0301 de Montre\u0301al, Montre\u0301al.\nstate changes, however, presents challenges as it requires distinguishing subtle changes in object appearance such as the presence of a cap on the bottle or screws on the car tire. Despite much work on object recognition and localization, recognition of object states has received only limited attention in computer vision [21].\nOne solution to recognizing object states would be to manually annotate states for different objects, and treat the problem as a supervised fine-grained object classification task [13, 14]. This approach, however, presents two problems. First, we would have to decide a priori on the set of state labels for each object, which can be ambiguous and not suitable for future tasks. Second, for each label we would need to collect a large number of examples, which can be very costly.\nIn this paper we propose to discover object states directly from videos with object manipulations. As state changes are often caused by specific actions, we attempt to jointly discover object states and corresponding manipulations. In our setup we assume that two distinct object states are temporally separated by a manipulation action. For example, the empty and full states of a coffee cup are separated by the \u201cpouring coffee\u201d action, as shown in Figure 1. Equipped with this constraint, we develop a clustering approach that\n1\nar X\niv :1\n70 2.\n02 73\n8v 3\n[ cs\n.C V\n] 2\n8 A\nug 2\n01 7\njointly (i) groups object states with similar appearance and consistent temporal locations with respect to the action and (ii) finds similar manipulation actions separating those object states in the input videos. Our approach exploits the complementarity of both subproblems and finds a joint solution for states and actions. We formulate our problem by adopting a discriminative clustering loss [3] and a joint consistency cost between states and actions. We introduce an effective optimization solution in order to handle the resulting non-convex loss function and the set of spatial-temporal constraints. To evaluate our method, we collect a new video dataset depicting real-life object manipulation actions in realistic videos. Given this dataset for training, our method demonstrates successful discovery of object states and manipulation actions. We also demonstrate that our joint formulation gives an improvement of object state discovery by action recognition and vice versa."}, {"heading": "2. Related work", "text": "Below we review related work on person-object interaction, recognizing object states, action recognition and discriminative clustering that we employ in our model. Person-object interactions. Many daily activities involve person-object interactions. Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46]. Recent work has also focused on building realistic datasets with people manipulating objects, e.g. in instructional videos [2, 31, 36] or while performing daily activities [37]. We build on this work but focus on joint modeling and recognition of actions and object states. States of objects. Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases. Differently from our approach, these works typically focus on classifying still images, do not consider human actions and assume an a priori known list of possible attributes. Closer to our setting, Isola et al. [21] discover object states and transformations between them by analyzing large collections of still images downloaded from the Internet. In contrast, our method does not require annotations of object states. Instead, we use the dynamics of consistent manipulations to discover object states in the video with minimal supervision. In [9], the authors use consistent manipulations to discover task relevant objects. However, they do not consider object states, rely mostly on first person cues (such as gaze) and take advantage of the fact that videos are taken in a single controlled environment. Action recognition. Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43]. This is effective for actions such as dancing or jumping, however, many of our daily activities are best\ndistinguishable by their effect on the environment. For example, opening door and closing door can look very similar using only motion and appearance descriptors but their outcome is completely different. This observation has been used to design action models in [15, 16, 44]. In [44], for example, the authors propose to learn an embedding in which a given action acts as a transformation of features of the video. In our work we localize objects and recognize changes of their states using manipulation actions as a supervisory signal. Related to ours is also the work of Fathi et al. [15] who represent actions in egocentric videos by changes of appearance of objects (also called object states), however, their method requires manually annotated precise temporal localization of actions in training videos. In contrast, we focus on (non-egocentric) Internet videos depicting real-life object manipulations where actions are performed by different people in a variety of challenging indoor/outdoor environments. In addition, our model jointly learns to recognize both actions and object states with only minimal supervision. Discriminative clustering. Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier. Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41]. In particular, we build on the discriminative clustering approach of [3] that has been shown to perform well in a variety of computer vision problems [4]. It leads to a quadratic optimization problem where different forms of supervision can be incorporated in the form of (typically) linear constraints. Building on this formalism, we develop a model that jointly finds object states and temporal locations of actions in the video. Part of our object state model is related to [24], while our action model is related to [6]. However, we introduce new spatialtemporal constraints together with a novel joint cost function linking object states and actions, as well as new effective optimization techniques. Contributions. The contributions of this work are threefold. First, we develop a new discriminative clustering model that jointly discovers object states and temporally localizes associated manipulation actions in video. Second, we introduce an effective optimization algorithm to handle the resulting non-convex constrained optimization problem. Finally, we experimentally demonstrate that our model discovers key object states and manipulation actions from input videos with minimal supervision."}, {"heading": "3. Modeling manipulated objects", "text": "We are given a set of N clips that contain a common manipulation of the same object (such as \u201copen an oyster\u201d). We also assume that we are given an a priori model of the corresponding object in the form of a pre-trained ob-\nject detector [17]. Given these inputs, our goal is twofold: (i) localize the temporal extent of the action and (ii) spatially/temporally localize the manipulated object and identify its states over time. This is achieved by jointly clustering the appearances of an object (such as an \u201coyster\u201d) appearing in all clips into two classes, corresponding to the two different states (such as \u201cclosed\u201d and \u201copen\u201d), while at the same time temporally localizing a consistent \u201copening\u201d action that separates the two states consistently in all clips. More formally, we formulate the problem as a minimization of a joint cost function that ties together the action prediction in time, encoded in the assignment variable Z, with the object state discovery in space and time, defined by the assignment variable Y :\nminimize Y \u2208{0,1}M\u00d72\nZ\u2208{0,1}T\nf(Z) + g(Y ) + d(Z, Y ) (1)\ns.t. Z \u2208 Z\ufe38 \ufe37\ufe37 \ufe38 saliency of action\nAction localization\nand Y \u2208 Y\ufe38 \ufe37\ufe37 \ufe38 ordering + non overlap\nObject state labeling\nwhere f(Z) is a discriminative clustering cost to temporally localize the action in each clip, g(Y ) is a discriminative clustering cost to identify and localize the different object states and d(Z, Y ) is a joint cost that relates object states and actions together. T denotes the total length of all video clips and M denotes the total number of tracked object candidate boxes (tracklets). In addition, we impose constraints Y and Z that encode additional structure of the problem: we localize the action with its most salient time interval per clip (\u201csaliency\u201d); we assume that the ordering\nof object states is consistent in all clips (\u201cordering\u201d) and that only one object is manipulated at a time (\u201cnon overlap\u201d).\nIn the following, we proceed with describing different parts of the model (1). In Sec. 3.1 we describe the cost function for the discovery of object states. In Sec. 3.2 we detail our model for action localization. Finally, in Sec. 3.3 we describe and motivate the joint cost d."}, {"heading": "3.1. Discovering object states", "text": "The goal here is to both (i) spatially localize the manipulated object and (ii) temporally identify its individual states. To address the first goal, we employ pre-trained object detectors. To address the second goal, we formulate the discovery of object states as a discriminative clustering task with constraints. We obtain candidate object detections using standard object detectors pre-trained on large scale existing datasets such as ImageNet [11]. We assume that each clip n is accompanied with a set of Mn tracklets1 of the object of interest.\nWe formalize the task of localizing the states of objects as a discriminative clustering problem where the goal is to find an assignment matrix Yn \u2208 {0, 1}Mn\u00d72, where (Yn)mk = 1 indicates that the m-th tracklet represents the object in state k. We also allow a complete row of Yn to be zero to encode that no state was assigned to the corresponding tracklet. This is to model the possibility of false\n1In this work, we use short tracks of objects (less than one second) that we call tracklet. We want to avoid long tracks that continue across a state change of objects. By using the finer granularity of tracklets, our model has the ability to correct for detection mistakes within a track as well as identify more precisely the state change.\npositive detections of an object, or that another object of the same class appears in the video, but is not manipulated and thus is not undergoing any state change. In detail, we minimize the following discriminative clustering cost [3]:2\ng(Y ) = min Ws\u2208Rds\u00d72\n1\n2M \u2016Y \u2212XsWs\u20162F\ufe38 \ufe37\ufe37 \ufe38\nDiscriminative loss on data\n+ \u00b5\n2 \u2016Ws\u20162F\ufe38 \ufe37\ufe37 \ufe38\nRegularizer\n(2)\nwhere Ws is the object state classifier that we seek to learn, \u00b5 is a regularization parameter and Xs is aM\u00d7ds matrix of features, where each row is a ds-dimensional (state) feature vector storing features for one particular tracklet. The minimization in Ws actually leads to a convex quadratic cost function in Y (see [3]). The first term in (2) is the discriminative loss on the data that measures how easily the input dataXs is classified by the linear classifierWs when the object state assignment is given by matrix Y . In other words, we wish to find a labeling Y for given object tracklets into two states (or no state) so that their appearance features X are easily classified by a linear classifier. To steer the cost towards the right solution, we employ the following constraints (encoded by Y \u2208 Y in (1)). Only one object is manipulated at a time : non overlap constraint. As it is common in instructional videos, we assume that only one object can be manipulated at a given time. However, in practice, it is common to have multiple (spatially diverse) tracklets that occur at the same time, for example, due to a false positive detection in the same frame. To overcome this issue, we impose that at most one tracklet can be labeled as belonging to state 1 or state 2 at any given time. We refer to this constraint as \u201cnon overlap\u201d in problem (1). state 1 \u2192 Action \u2192 state 2: ordering constraints. We assume that the manipulating action transforms the object from an initial state to a final state and that both states are present in each video. This naturally introduces two constraints. The first one is the ordering constraints on the labeling Yn, i.e. the state 1 should occur before state 2 in each video. The second constraint imposes that we have at least one tracklet labeled as state 1 and at least one tracklet labeled as state 2. We call this last constraint the \u201cat least one\u201d constraint in contrast to forcing \u201cexactly one\u201d ordered prediction as previously proposed in a discriminative clustering approach on video for action localization [6]. This new type of constraint brings additional optimization challenges that we address in Section 4.2."}, {"heading": "3.2. Action localization", "text": "Our action model is equivalent to the one of [6] applied to only one action. More precisely, the goal is to find an assignment matrix Zn \u2208 {0, 1}Tn for each clip n, where\n2We concatenate all the variables Yn into one M\u00d72 matrix Y.\nZnt = 1 encodes that the t-th time interval of video is assigned to an action and Znt = 0 encodes that no action is detected in interval t. The cost that we minimize for this problem is similar to the object states cost:\nf(Z) = min Wv\u2208Rdv\n1\n2T \u2016Z \u2212XvWv\u20162F\ufe38 \ufe37\ufe37 \ufe38 Discriminative loss on data\n+ \u03bb\n2 \u2016Wv\u20162F\ufe38 \ufe37\ufe37 \ufe38\nRegularizer\n, (3)\nwhere Wv is the action classifier, \u03bb is a regularization parameter and Xv is a matrix of visual features. We constrain our model to predict exactly one time interval for an action per clip, an approach for actions that was shown to be beneficial in a weakly supervised setting [6] (referred to as \u201caction saliency\u201d constraint). As will be shown in experiments, this model alone is incomplete because the clips in our dataset can contain other actions that do not manipulate the object of interest. Our central contribution is to propose a joint formulation that links this action model with the object state prediction model, thereby resolving the ambiguity of actions. We detail the joint model next."}, {"heading": "3.3. Linking actions and object states", "text": "Actions in our model are directly related to changes in object states. We therefore want to enforce consistency between the two problems. To do so, we design a novel joint cost function that operates on the action video labeling Zn and the state tracklet assignment Yn for each clip. We want to impose a constraint that the action occurs in between the presence of the two different object states. In other words, we want to penalize the fact that state 1 is detected after the action happens, or the fact that state 2 is triggered before the action occurs. Joint cost definition. We propose the following joint symmetric cost function for each clip:\nd(Zn, Yn) = \u2211\ny\u2208S1(Yn)\n[ty \u2212 tZn ]+ + \u2211\ny\u2208S2(Yn)\n[tZn\u2212 ty]+, (4)\nwhere tZn and ty are the times when the action Zn and the tracklet y occur in a clip n, respectively. S1(Yn) and S2(Yn) are the tracklets in the n-th clip that have been assigned to state 1 and state 2, respectively. Finally [x]+ is the positive part of x. In other words, the function penalizes the inconsistent assignment of objects states Yn by the amount of time that separates the incorrectly assigned tracklet and the manipulation action in the clip. The overall joint cost is the sum over all clips weighted by a scaling hyperparameter \u03bd > 0:\nd(Z, Y ) = \u03bd 1\nT N\u2211 n=1 d(Zn, Yn). (5)"}, {"heading": "4. Optimization", "text": "Optimizing problem (1) poses several challenges that need to be addressed. First, we propose a relaxation of the\ninteger constraints and the distortion function (Section 4.1). Second, we optimize this relaxation using Frank-Wolfe with a new dynamic program able to handle our tracklet constraints (Section 4.2). Finally, we introduce a new rounding technique to obtain an integer candidate solution to our problem (Section 4.3)."}, {"heading": "4.1. Relaxation", "text": "Problem (1) is NP-hard in general [30] due to its specific integer constraints. Inspired by the approach of [5] that was successful to approximate combinatorial optimization problems, we propose to use the tightest convex relaxation of the feasible subset of binary matrices by taking its convex hull. As our variables now can take values in [0, 1], we also have to propose a consistent extension for the different cost functions to handle fractional values as input. For the cost functions f and g, we can directly take their expression on the relaxed set as they are already expressed as (convex) quadratic functions. Similarly, for the joint cost function d in (4), we use its natural bilinear relaxation:\nd(Zn, Yn) = Mn\u2211 i=1 Tn\u2211 t=1 ( (Yn)i1Znt[tni \u2212 t]+ +\n(Yn)i2Znt[t\u2212 tni]+ ) , (6)\nwhere tni denotes the video time of tracklet i in clip n. This relaxation is equal to the function (4) on the integer points. However, it is not jointly convex in Y and Z, thus we have to design an appropriate optimization technique to obtain good (relaxed) candidate solutions, as described next."}, {"heading": "4.2. Joint optimization using Frank-Wolfe", "text": "When dealing with a constrained optimization problem for which it is easy to solve linear programs but difficult to project on the feasible set, the Frank-Wolfe algorithm is an excellent choice [22, 28]. It is exactly the case for our relaxed problem, where the linear program over the convex hull of feasible integer matrices can be solved efficiently via dynamic programming. Moreover, [27] recently showed that the Frank-Wolfe algorithm with line-search converges to a stationary point for non-convex objectives at a rate of O(1/ \u221a k). We thus use this algorithm for the joint optimization of (1). As the objective is quadratic, we can perform exact line-search analytically, which speeds up convergence in practice. Finally, in order to get a good initialization for both variables Z and Y , we first optimize separately f(Z) and g(Y ) (without the non-convex d(Z, Y )), which are both convex functions. Dynamic program for the tracklets. In order to apply the Frank-Wolfe algorithm, we need to solve a linear program (LP) over our set of constraints. Previous work has explored \u201cexact one\u201d ordering constraints for time localization problems [5]. Differently here, we have to deal with\nthe spatial (non overlap) constraint and finding \u201cat least one\u201d candidate tracklet per state. To deal with these two requirements, we propose a novel dynamic programming approach. First, the \u201cat least one\u201d constraint is encoded by having a memory variable which indicates whether state 1 or state 2 have already been visited. This variable is used to propose valid state decisions for consecutive tracklets. Second, the challenging \u201cnon-overlap\u201d tracklet constraint is included by constructing valid left-to-right paths in a cost matrix while carefully considering the possible authorized transitions. We provide details of the formulation in Appendix C. In addition, we show in section 5.2 that these new constraints are key for the success of the method."}, {"heading": "4.3. Joint rounding method", "text": "Once we obtain a candidate solution of the relaxed problem, we have to round it to an integer solution in order to make predictions. Previous works [2, 6] have observed that using the learned W \u2217 classifier for rounding gave better results than other possible alternatives. We extend this approach to our joint setup by proposing the following new rounding procedure. We optimize problem (1) but fix the values of W in the discriminative clustering costs. Specifically, we minimize the following cost function over the integer points Z \u2208 Z and Y \u2208 Y:\n1\n2T \u2016Z \u2212XvW \u2217v \u20162F +\n1\n2M \u2016Y \u2212XsW \u2217s \u20162F + d(Z, Y ), (7)\nwhereW \u2217v andW \u2217 s are the classifier weights obtained at the end of the relaxed optimization. Because y2 = y when y is binary, (7) is actually a linear objective over the binary matrix Yn for Zn fixed. Thus we can optimize (7) exactly by solving a dynamic program on Yn for each of the Tn possibilities of Zn, yielding O(MnTn) time complexity per clip (see Appendix D for details)."}, {"heading": "5. Experiments", "text": "In this section, we first describe our dataset, the object tracking pipeline and the feature representation for object tracklets and videos (Section 5.1). We consider two experimental set-ups. In the first weakly-supervised set-up (Section 5.2), we apply our method on a set of video clips which we know contain the action of interest but do not know its precise temporal localization. In the second, more challenging \u201cin the wild\u201d set-up (Section 5.3), the input set of weakly-supervised clips is obtained by automatic processing of text associated with the videos and hence may contain erroneous clips that do not contain the manipulation action of interest. The data and code are available online [1]."}, {"heading": "5.1. Dataset and features", "text": "Dataset of manipulation actions. We build a dataset of manipulation actions by collecting videos from different\nsources: the instructional video dataset introduced in [2], the Charades dataset from [37], and some additional videos downloaded from YouTube. We focus on \u201cthird person\u201d videos (rather than egocentric) as such videos depict a variety of people in different settings and can be obtained on a large scale from YouTube. We annotate the precise temporal extent of seven different actions3 applied to five distinct objects4. This results in 630 annotated occurrences of ground truth manipulation action.\nTo evaluate object state recognition, we define a list of two states for each object. We then run automatic object detector for each involved object, track the detected object occurrences throughout the video and then subdivide the resulting long tracks into short tracklets. Finally, we label ground truth object states for tracklets within \u00b140 seconds of each manipulation action. We label four possible states: state 1, state 2, ambiguous state or false positive detection. The ambiguous state covers the (not so common) in-between cases, such as cup half-full. In total, we have 19,499 fully annotated tracklets out of which: 35% cover state 1 or state 2, 25% are ambiguous, and 40% are false positives. Note that this annotation is only used for evaluation purpose, and not by any of our models. Detailed statistics of the dataset are given in Appendix B. Object detection and tracking. In order to obtain detectors for the five objects, we finetune the FastRCNN network [17] with training data from ImageNet [11]. We use bounding box annotations from ImageNet when available (e.g. the \u201cwheel\u201d class). For the other classes, we manually labeled more than 500 instances per class. In our set-up with only moderate amount of training data, we observed that class-agnostic object proposals combined with FastRCNN performed better than FasterRCNN [35]. In detail, we use geodesic object proposals [26] and set a relatively low object detection threshold (0.4) to have good recall. We track objects using a generic KLT tracker from [4]. The tracks are then post-processed into shorter tracklets that last about one second and thus are likely to have only one object state. Object tracklet representation. For each detected object, represented by a set of bounding boxes over the course of the tracklet, we compute a CNN feature from each (extended) bounding box that we then average over the length of the tracklet to get the final representation. The CNN feature is extracted with a ROI pooling [35] of ResNet50 [19]. The ROI pooling notably allows to capture some context around the object which is important for some cases (e.g. wheel \u201con\u201d or \u201coff\u201d the car). The resulting feature descriptor of each object tracklet is 8,192 dimensional. Representing video for recognizing actions. Following\n3put the wheel on the car (47 clips), withdraw the wheel from the car (46), place a plant inside a pot (27), open an oyster (28), open a refrigerator (234), close a refrigerator (191) and pour coffee (57).\n4car wheel, flower pot, oyster, refrigerator and coffee cup.\nthe approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames. For the motion we use a 2,000 dimensional bag-of-word representation of histogram of local optical flow (HOF) obtained from Improved Dense Trajectories [43]. Following [2], we add an appearance vector that is obtained from a 1,000 dimensional bagof-word vector of conv5 features from VGG16 [39]. This results in a 3,000 dimensional feature vector for each chunk of 10 frames."}, {"heading": "5.2. Weakly supervised object state discovery", "text": "Experimental setup. We first apply our method in a weakly supervised set-up where for each action we provide an input set of clips, where we know the action occurs somewhere in the clip but we do not provide the precise temporal localization. Each clip may contain other actions that affect other objects or actions that do not affect any object at all (e.g. walking / jumping). The input clips are about 20s long and are obtained by taking approximately \u00b1 10s of each annotated manipulation action. Evaluation metric: average precision. For all variants of our method, we use the rounded solution that reached the smallest objective during optimization. We evaluate these predictions with a precision score averaged over all the videos. A temporal action localization is said to be correct if it falls within the ground truth time interval. Similarly, a state prediction is correct if it matches the ground truth state.5 Note that a \u201cprecision\u201d metric is reasonable in our set-up as our method is forced to predict in all videos, i.e. the recall level is fixed to all videos and the method cannot produce high precision with low recall. Hyperparameters. In all methods that involve a discriminative clustering objective, we used \u03bb = 10\u22122 (action localization) and \u00b5 = 10\u22124 (state discovery) for all 7 actions. For joint methods that optimize (1), we set the weight \u03bd of the distortion measure (5) to 1. State discovery results. Results are shown in the top part of Table 1. In the following, we refer to \u201cState only\u201d whenever we use our method without looking at the action cost or the distortion measure (1). We compare to two baselines for the state discovery task. Baseline (a) evaluates chance performance. Baseline (b) performs K-means clustering of the tracklets withK = 3 (2 clusters for the states and 1 for false positives). We report performance of the best assignment for the solution with the lowest objective after 10 different initializations. Baseline (c) is obtained by running our \u201cState only\u201d method while using random features for tracklet representation as well as \u201dat least one ordering\u201d and \u201dnon overlap\u201d constraints. We use random features to avoid nontrivial analytic derivation for the \u201dConstraints only\u201d performance. This baseline reveals the difficulty of the problem\n5In particular, we count \u201cambiguous\u201d labels as incorrect.\nand quantifies improvement brought by the ordering constraints. The next two methods are \u201cState only\u201d variants. Method (d) corresponds to a replacement of the \u201cat least one constraint\u201d by an \u201cexactly one constraint\u201d while the method (e) uses our new constraint. Finally, we report three joint methods that use our new joint rounding technique (7) for prediction. Method (f) corresponds to our joint method that optimizes (1). Method (g) is a simple improvement taking into account object detection score in the objective (details below). Finally, method (h) is our joint method but using the action ground truth labels as video features in order to test the effect of having perfect action localization for the task of object state discovery.\nWe first note that method (e) outperforms (d), thus highlighting the importance of the \u201cat least one\u201d constraint for modeling object states. While the saliency approach (taking only the most confident detection per video) was useful for action modeling in [6], it is less suitable for our setup where multiple tracklets can be in the same state. The joint approach with actions (f) outperforms the \u201cState only\u201d method (e) on 6 out of 7 actions and obtains better average performance, confirming the benefits of joint modeling of actions and object states. Using ground truth action locations further improves results (cf. (h) against (f)). Our weakly supervised approach (f) performs not much lower compared to using ground truth actions (h), except for the states of the coffee cup (empty/full). In this case we observe that a high number of false positive detections confuses our method. A simple way to address this issue is to add the object detection score into the objective of our method, which then prefers to assign object states to higher scoring object candidates further reducing the effect of false positives. This can be done easily by adding a linear cost reflecting the object detection score to objective (1). We denote this modified method \u201c(g) Joint model + det. scores\u201d. This method achieves the best average performance and highlights that additional information can be easily added to our model. Action localization results. We compare our method to three different baselines and give results in the bottom part of Table 1. Baseline (i) corresponds to chance performance,\nwhere the precision for each clip is simply the proportion of the entire clip taken by the ground truth time interval. Baseline (ii) is the method introduced in [6] used here with only one action. It also corresponds to a special case of our method where the object state part of the objective in equation (1) is turned off (salient action only). Interestingly, this baseline is actually worse than chance for several actions. This is because without additional information about objects, this method localizes other common actions in the clip and not the action manipulating the object of interest. This also demonstrates the difficulty of our experimental set-up where the input video clips often contain multiple different actions. To address this issue, we also evaluate baseline (iii), which complements [6] with the additional constraint that the action prediction has to be within the first and the last frame where the object of interest is detected, improving the overall performance above chance. Our joint approach (iv) consistently outperforms these baselines on all actions, thus showing again the strong link between object states and actions. Finally, the approach (v) is the analog of method (g) for action localization where we use ground truth state labels as tracklet features in our joint formulation showing that the action localization can be further improved with better object state descriptors. In addition, we also compare to a supervised baseline. The average obtained performance is 0.58 which is not far from our method. This demonstrates the potential of using object states for action localization. More details on this experiment are provided in Appendix E.\nBenefits of joint object-action modeling. We observe that the joint modeling of object states and actions benefits both tasks. This effect is even stronger for actions. Intuitively, knowing perfectly the object states reduces a lot the search space for action localization. Moreover, despite the recent major progress in object recognition using CNNs, action recognition still remains a hard problem with much room for improvement. Qualitative results are shown in Fig. 3 and failure cases of our method are discussed in F."}, {"heading": "5.3. Object state discovery in the wild", "text": "Towards the discovery of a large number of manipulation actions and state changes, we next apply our method in an automatic setting, where action clips have been obtained using automatic text-based retrieval. Clip retrieval by text. Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions. In this experiment, we keep only such narrated instructional videos from our dataset. This results in the total of 140 videos that are 3 minutes long in average. We extract the narration in the form of subtitles associated with the video. These subtitles have been directly downloaded from YouTube and have been obtained either by Youtube\u2019s Automatic Speech Recognition (ASR) or provided by the users.\nWe use the resulting text to retrieve clip candidates that may contain the action modifying the state of an object. Obtaining the approximate temporal location of actions from the transcribed narration is still very challenging due to ambiguities in language (\u201cundo bolt\u201d and \u201cloosen nut\u201d refer to\nthe same manipulation) and only coarse temporal localization of the action provided by the narration. Given a manipulation action such as \u201cremove tire\u201d, we first find positive and negative sentences relevant for the action from an instruction website such as Wikihow. We then train a linear SVM classifier [8] on bigram text features. Finally, we use the learned classifier to score clips from the input instructional videos. In detail, the classifier is applied in a sliding window of 10 words finding the best scoring window in each input video. The clip candidates are then obtained by trimming the input videos 5 seconds before and 15 seconds after the timing of the best scoring text window to account for the fact that people usually perform the action after having talked about it. We apply our method on the top 20 video clips based on the SVM score for each manipulation action. More details about this process are provided in Appendix A. Results. As shown in Table 2, the pattern of results, where our joint method performs the best, is similar to the weakly supervised set-up described in Sec. 5.2. This highlights the robustness of our model to noisy input data \u2013 an important property for scaling-up the method to Internet scale datasets. To assess how well our joint method could do with perfect retrieval, we also report results for a \u201cCurated\u201d setup where we replace the automatically retrieved clips with the 20s clips used in Sec. 5.2 for the corresponding videos."}, {"heading": "6. Conclusion and future work", "text": "We have described a joint model that relates object states and manipulation actions. Given a set of input videos, our model both localizes the manipulation actions and discovers the corresponding object states. We have demonstrated that our joint approach improves performance of both object state recognition and action recognition. More generally, our work provides evidence that actions should be modeled in the larger context of goals and effects. Finally, our work opens up the possibility of Internet-scale learning of manipulation actions from narrated video sequences.\nAcknowledgments This research was supported in part by a Google Research Award, ERC grants Activia (no. 307574) and LEAP (no. 336845), the CIFAR Learning in Machines & Brains program and ESIF, OP Research, development and education Project IMPACT No. CZ.02.1.01/0.0/0.0/15 003/0000468."}, {"heading": "A. SVM training for clip retrieval", "text": "In Section 5.3 we proposed an automatic method for retrieving video clips with manipulated objects. This method makes use of narrations that come along with instructional videos. Narrations in the form of text are first obtained automatically with Automatic Speech Recognition (ASR)6 and then processed as detailed below.\nLanguage dataset. For each manipulation action, we first find relevant positive and negative sentences on instruction websites such as Wikihow. On average we obtain about 12 positive and 50 negative sentences per action.\nLanguage features. Off-the-shelf methods for text parsing typically fail in the absence of punctuation. To process ASR output, which comes without punctuation, we propose to use the following simple but robust text representation. We represent every 10-word window of the narration by a TF-IDF vector based of uni-grams and bi-grams. We use the same TF-IDF representation to encode text in our Language dataset on the level of sentences.\nSVM training. We train binary linear SVM classifiers to identify manipulation actions using the Language dataset for training and the regularization parameter C = 10. The obtained classifiers are then used to score every 10-word window of text narrations. Video clips with the temporal correspondence to the top-scoring text narrations for each action are then retrieved. To deduce the temporal extent of the video clip given the top-scoring window, we trim 5 seconds before and 15 seconds after the corresponding timing. This is to account for the fact that people are usually doing the action after speaking about it. These are the clips we use for our evaluation in Section 5.3 of the main paper.\n6The ASR transcriptions are directly downloaded from YouTube.\nIllustration. In Figure 3, we provide an illustration of text based clip retrieval. This visualization demonstrates the difficulty of the addressed problem (see caption for details)."}, {"heading": "B. Dataset of manipulated objects", "text": "Table 4 provides statistics for the dataset introduced in Section 5.1 of the main paper. For each object class we indicate associated action classes and the number of video clips for each action. We also provide the list of states and the number of object tracklets with state annotations. In total, we have around 20,000 annotated tracks which we use for the quantitative evaluation of state discovery."}, {"heading": "C. Dynamic program for the tracklets", "text": "The track constraints defined in Section 3.1 introduce new challenges compared to the previous related work [2, 5, 24]. Recall that there are three main components in the constraints. First, we assume that only one object is manipulated at a given time. Thus at most one tracklet can be assigned to a state at a given time. This constraint is referred to as the non-overlap constraint. Second, we have the ordering constraint that imposes that state 1 always happens before state 2. The last constraint imposes that we have at least one tracklet labeled as state 1 and at least one tracklet labeled as state 2. We need to be able to minimize linear functions over this set of constraints in order to use the Frank-Wolfe algorithm. More precisely, as the constraints decompose over the different clips, we can solve independently for each clip n the following linear problem:\nminimize Yn\u2208{0,1}Mn\u00d72\nTr(CTn Yn) (8)\ns.t. Yn \u2208 Yn\ufe38 \ufe37\ufe37 \ufe38 non-overlap + ordering\n+ at least one\n,\nwhere Cn \u2208 RMn\u00d72 is a cost matrix that typically comes from the computation of the gradient of the cost function at the current iterate. In order to solve this problem, we use a dynamic program approach that we explain next. Recall that we are given Mn tracklets (yi)Mni=1 and our goal is to output the Yn matrix that assigns to each of these tracklets either state 1, state 2 or no state at all while respecting the constraints. The whole method is illustrated in Figure 5 with a toy example.\nNon-overlap data structure for the tracklets. We first pre-process the tracklets to build an auxiliary data-structure that is used to enforce the non-overlap constraint between the tracklets, as illustrated in Figure 5a. First, we sort and index each tracklet by their beginning time, and add two fictitious tracklets: y0 as the starting tracklet and yf as the ending tracklet. These two tracklets are used to start and terminate the dynamic program. If all the tracklets were\nsequentially ordered without any overlap in time, then we could simply make a decision for each of them sequentially as was done in previous work on action localization for example (one decision per time step) [5]. To enforce the non-overlap constraint, we force the decision process to choose only one possible successor among the group of overlapping valid immediate successors of a tracklet. For each tracklet yi, we thus define its (smallest) set of \u201cvalid successors\u201d as the earliest tracklet yj7 after yi that is also non-overlapping with yi, as well as any other tracklet yl for l > j that is overlapping with yj (thus giving the earliest valid group of overlapping tracklets). The valid successors\n7Earliest means the smallest j.\nare illustrated by red dotted arrows in Figure 5a. For example, the valid successors of y1 are y3 (the earliest one that is non-overlapping) as well as y4 (which overlaps with y3 thus forming an overlapping group). Skipping a tracklet in this decision process means that we assign it to zero (which trivially always satisfies the non-overlapping constraint); whereas once we choose a tracklet to potentially assign it to state 1 or 2, we cannot visit any overlapping tracklet by construction of the valid successors, thus maintaining the non-overlap constraint.\nDynamic program. The dynamic programming approach is used when we can solve a large problem by solving a sequence of inclusive subproblems that are linked by a simple recursive formula and that use overlapping solutions (which can be stored in a table for efficiency). In terms of implementation, [5] encoded their dynamic program as finding an optimal path inside a cost matrix. This approach is particularly suited when the update cost rule depends only on the arrival entry in the cost matrix as opposed to be transition dependent. As we will show below, we can encode the solution to our problem in a way that satisfies this property. We therefore use the framework of [5] by casting our problem as a search for an optimal path inside a cost matrix\nC\u0303n illustrated in Figure 5b, and where the valid transitions encode the possible constraints.\nOne main difference with [5] is that we have to deal with the challenging at least one constraint in the context of ordered labels. To do so, we can filter further the set of valid decisions by using \u201cmemory states\u201d that encode in which of the following three situations we are: (i) that state 1 has not yet been visited, (ii) that state 1 has already been visited, but state 2 has not yet been visited (and thus that we can either come back to state 1 or go to state 2) and (iii) that both states have been visited. These memory states can be encoded by interleaving complete rows of 0s in between columns of Cn stored as rows, to obtain the 5 \u00d7Mn matrix C\u0303n. These new rows encode the three different memory states previously described when making a prediction of 0 for a specific tracklet, and we enforce the correct memory semantic by only allowing a path to move to the same row or the row immediately below, except for state 1 which can also move directly to state 2 (two rows below), and the middle \u201cbetween state 1/2\u201d row, where one can go up one row additionally to state 1. Finally, the valid transitions between columns (tracklets) are given by the valid successors data structure as given in Figure 5a to encode the non-overlap constraints. Combining these two constraints (at least one ordering and non-overlap), we illustrate with grey arrows in Figure 5b the possible transitions from the states along the path in red. To describe the dynamic program recursion below, we need to go the opposite direction from the successors, and thus we say that yj is a predecessor of yi if and only if yi is a successor of yj .\nTo perform the dynamic program, we maintain a matrix Dn of the same size as C\u0303n whereDn(k, i) contains the minimal valid path cost of going from (1, y0) to (k, yi) inside the cost matrix C\u0303n. To define the cost update recursion to compute Dn(k, i), let P (k, i) be the set of tuples (l, j) for which it is possible to go from (l, j) to (k, yi) according to the rules described above. The update rule is then as follows:\nDn(k, i) = min (l,j)\u2208P (k,i) Dn(l, j) + C\u0303n(k, yi). (9)\nAs we see here, the added cost depends only on the arrival entry C\u0303n(k, yi). We can therefore use the approach of [5] and only consider entry costs rather than edge costs. Thanks to our indexing property (tracklets are sorted by the beginning time), we can update the dynamic program matrix by filling each column of Dn one after the other. Once this update is finished, we back-track to get the best path by starting from the ending track (predecessors of yf ) at the last row (to be sure that both states have been visited) that has the lowest score in the Dn matrix. The total complexity of this algorithm is of order O(Mn)."}, {"heading": "D. Joint cost rounding method", "text": "Recall that we propose to use a convex relaxation approach in order to obtain a candidate solution of main problem (1). Thus, we need to round the relaxed solution afterward in order to get a valid integer solution. We propose here a new rounding that is adapted to our joint problem. We referred to this rounding as the joint cost rounding (see Section 4 of main paper). This rounding is inspired by [6, 2]. They observe that using the learned W \u2217 classifier to round gives them better solutions, both in terms of objective value and performance. We propose to use its natural extension for our joint model. We first fix the classifiers for actions Wa and for states Ws to their relaxed solution (W \u2217a ,W \u2217 s ) and find, for each clip n, the couple (Zn, Yn) that minimizes the joint cost (7). To do so, we observe that we can enumerate all Tn possibilities for Zn, and solve for each of them the minimization of the joint cost with respect to Yn. The minimization with respect to Yn can be addressed as follows. First, we observe that the distortion function (6) is bilinear in (Zn, Yn). Let Zn be a Tn \u00d7 1 vector, and let 12 be a vector of ones of length 2. We can actually write: d(Zn, Yn) = Tr((BnZn1>2 )\n>Yn) for some matrix Bn \u2208 RMn\u00d7Tn . Thus, when Zn is fixed, the joint term d(Zn, Yn) is actually a simple linear function of Yn. In addition, the quadratic term in Yn coming from (2) is also linear over the integer points (using the fact that y2 = y for y \u2208 {0, 1}). Thus, when Zn, W \u2217a and W \u2217s are fixed, the minimization over Yn is a linear program (8) that we solve using our dynamic program from the previous section. The final algorithm is given in Algorithm 1. Its complexity is of order O(TnMn)."}, {"heading": "E. Supervised baselines for Action Localization", "text": "We have run supervised baseline methods with state-ofthe-art features. To be able to compare numbers with our experiment, we used a leave-one-out technique. For each action, we train a binary classifier with SVM on all videos except one. Similarly to our setting, we then select the top scoring time interval of the left alone test video. We repeat this process for all videos and report the metric used in our paper. For baseline (1), we use the same features we are using in the main paper. For baseline (2), we complete our features with all channels of Improved Dense Trajectories (IDT) [43]. Detailed results are given in Table 5.\nAlgorithm 1 Joint cost rounding for video n Get W \u2217s and W \u2217 a from the relaxed problem.\nInitialize Z\u2217, Y \u2217 and val\u2217 = +\u221e. # Loop over all possibilities for Zn (saliency) for t in 1 : Tn do\nZ \u2190 zeros(Tn, 1) # Set the t-th entry of Z to 1 Zt \u2190 1 # Definition of the cost matrix Cn \u2190 12M (ones(Mn, 2)\u2212 2XsWs) + \u03bd T BnZ1 > 2 # Dynamic program for the tracks Ymin \u2190 argminY \u2208Yn Tr(C T n Y ) # Cost computation costZ \u2190 12T \u2016Z \u2212XaWa\u2016 2 F costY \u2190 12M \u2016Ymin \u2212XsWs\u2016 2 F costZY \u2190 \u03bdT d(Z, Ymin) # Update solution if better val\u2190 costZ + costY + costZY if val < val\u2217 then\nZ\u2217 \u2190 Z Y \u2217 \u2190 Ymin val\u2217\u2190 val\nend if end for return Z\u2217, Y \u2217\nWe observe that we obtain results that are on par with our weakly supervised baselines (0.55 versus 0.58), therefore demonstrating the potential of using the information of object states for action localization."}, {"heading": "F. Failure cases", "text": "We observed two main types of failures, illustrated in Figure 6. The first one occurs when a false positive object detection consistently satisfies the hypothesis of our model in multiple videos (the top two rows in Figure 6). The second typical failure mode is due to ambiguous labels (bottom row in Figure 6). This highlights the difficulty in annotating ground truth for long actions such as \u201cpouring coffee\u201d."}], "references": [{"title": "Unsupervised learning from narrated instruction videos", "author": ["J.-B. Alayrac", "P. Bojanowski", "N. Agrawal", "I. Laptev", "J. Sivic", "S. Lacoste Julien"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "DIFFRAC: A discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "NIPS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Finding actors and actions in movies", "author": ["P. Bojanowski", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "ICCV,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "ECCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly-supervised alignment of video with text", "author": ["P. Bojanowski", "R. Lajugie", "E. Grave", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid"], "venue": "ICCV,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting changes in real-world objects: The relationship between visual long-term memory and change blindness", "author": ["T.F. Brady", "T. Konkle", "A. Oliva", "G.A. Alvarez"], "venue": "Communicative and Integrative Biology,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, pages 273\u2013297,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video", "author": ["D. Damen", "T. Leelasawassuk", "O. Haines", "A. Calway", "W. Mayol-Cuevas"], "venue": "BMVA,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning person-object interactions for action recognition in still images", "author": ["V. Delaitre", "J. Sivic", "I. Laptev"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering localized attributes for fine-grained recognition", "author": ["K. Duan", "D. Parikh", "D. Crandall", "K. Grauman"], "venue": "CVPR, pages 3474\u20133481,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I.E. Lim", "D. Hoiem", "D. Forsyth"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling actions through state changes", "author": ["A. Fathi", "J.M. Rehg"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling video evolution for action recognition", "author": ["B. Fernando", "E. Gavves", "M.J. Oramas", "A. Ghodrati", "T. Tuytelaars"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Observing humanobject interactions: Using spatial and functional compatibility for recognition", "author": ["A. Gupta", "A. Kembhavi", "L.S. Davis"], "venue": "PAMI, 31(10):1775\u20131789,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist temporal modeling for weakly supervised action labeling", "author": ["D.-A. Huang", "L. Fei-Fei", "J.C. Niebles"], "venue": "ECCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Discovering states and transformations in image collections", "author": ["P. Isola", "J.J. Lim", "E.H. Adelson"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "ICML,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Representing videos using mid-level discriminative patches", "author": ["A. Jain", "A. Gupta", "M. Rodriguez", "L. Davis"], "venue": "CVPR, pages 2571\u20132578,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient image and video co-localization with Frank-Wolfe algorithm", "author": ["A. Joulin", "K. Tang", "L. Fei-Fei"], "venue": "ECCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual objectaction recognition: Inferring object affordances from human demonstration", "author": ["H. Kjellstr\u00f6m", "J. Romero", "D. Kragi\u0107"], "venue": "CVIU, 115(1):81\u201390,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Geodesic object proposals", "author": ["P. Krhenbhl", "V. Koltun"], "venue": "ECCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Convergence rate of Frank-Wolfe for nonconvex objectives", "author": ["S. Lacoste-Julien"], "venue": "arXiv:1607.00345,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "On the global linear convergence of Frank-Wolfe optimization variants", "author": ["S. Lacoste-Julien", "M. Jaggi"], "venue": "NIPS,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey for the quadratic assignment problem", "author": ["E.M. Loiola", "N.M.M. de Abreu", "P.O. Boaventura-Netto", "P. Hahn", "T. Querido"], "venue": "EJOR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "What\u2019s cookin\u2019? Interpreting cooking videos using text, speech and vision", "author": ["J. Malmaud", "J. Huang", "V. Rathod", "N. Johnston", "A. Rabinovich", "K. Murphy"], "venue": "NAACL,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "ICCV,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "The SUN attribute database: Beyond categories for deeper scene understanding", "author": ["G. Patterson", "C. Xu", "H. Su", "J. Hays"], "venue": "IJCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Detecting activities of daily living in first-person camera views", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised semantic parsing of video collections", "author": ["O. Sener", "A. Zamir", "S. Savarese", "A. Saxena"], "venue": "ICCV,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Hollywood in homes: Crowdsourcing data collection for activity understanding", "author": ["G.A. Sigurdsson", "G. Varol", "X. Wang", "A. Farhadi", "I. Laptev", "A. Gupta"], "venue": "ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["S. Singh", "A. Gupta", "A.A. Efros"], "venue": "ECCV,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Co-localization in real-world images", "author": ["K. Tang", "A. Joulin", "L.-J. Li", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spatiotemporal features with 3D convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "ICCV,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "ICCV,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Actions \u0303 transformations", "author": ["X. Wang", "A. Farhadi", "A. Gupta"], "venue": "CVPR,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "NIPS,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei- Fei"], "venue": "ICCV,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "Human visual system can easily distinguish different states of objects, such as open/closed bottle or full/empty coffee cup [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 18, "context": "Despite much work on object recognition and localization, recognition of object states has received only limited attention in computer vision [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 10, "context": "One solution to recognizing object states would be to manually annotate states for different objects, and treat the problem as a supervised fine-grained object classification task [13, 14].", "startOffset": 180, "endOffset": 188}, {"referenceID": 11, "context": "One solution to recognizing object states would be to manually annotate states for different objects, and treat the problem as a supervised fine-grained object classification task [13, 14].", "startOffset": 180, "endOffset": 188}, {"referenceID": 1, "context": "We formulate our problem by adopting a discriminative clustering loss [3] and a joint consistency cost between states and actions.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 15, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 22, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 31, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 43, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 0, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 24, "endOffset": 35}, {"referenceID": 28, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 24, "endOffset": 35}, {"referenceID": 33, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 24, "endOffset": 35}, {"referenceID": 34, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases.", "startOffset": 58, "endOffset": 70}, {"referenceID": 29, "context": "Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases.", "startOffset": 58, "endOffset": 70}, {"referenceID": 30, "context": "Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases.", "startOffset": 58, "endOffset": 70}, {"referenceID": 18, "context": "[21] discover object states and transformations between them by analyzing large collections of still images downloaded from the Internet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "In [9], the authors use consistent manipulations to discover task relevant objects.", "startOffset": 3, "endOffset": 6}, {"referenceID": 26, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 35, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 39, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 40, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 12, "context": "This observation has been used to design action models in [15, 16, 44].", "startOffset": 58, "endOffset": 70}, {"referenceID": 13, "context": "This observation has been used to design action models in [15, 16, 44].", "startOffset": 58, "endOffset": 70}, {"referenceID": 41, "context": "This observation has been used to design action models in [15, 16, 44].", "startOffset": 58, "endOffset": 70}, {"referenceID": 41, "context": "In [44], for example, the authors propose to learn an embedding in which a given action acts as a transformation of features of the video.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "[15] who represent actions in egocentric videos by changes of appearance of objects (also called object states), however, their method requires manually annotated precise temporal localization of actions in training videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier.", "startOffset": 67, "endOffset": 78}, {"referenceID": 37, "context": "Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier.", "startOffset": 67, "endOffset": 78}, {"referenceID": 42, "context": "Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier.", "startOffset": 67, "endOffset": 78}, {"referenceID": 2, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 17, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 20, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 38, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 1, "context": "In particular, we build on the discriminative clustering approach of [3] that has been shown to perform well in a variety of computer vision problems [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "In particular, we build on the discriminative clustering approach of [3] that has been shown to perform well in a variety of computer vision problems [4].", "startOffset": 150, "endOffset": 153}, {"referenceID": 21, "context": "Part of our object state model is related to [24], while our action model is related to [6].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "Part of our object state model is related to [24], while our action model is related to [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 14, "context": "ject detector [17].", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "We obtain candidate object detections using standard object detectors pre-trained on large scale existing datasets such as ImageNet [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "In detail, we minimize the following discriminative clustering cost [3]:2", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "The minimization in Ws actually leads to a convex quadratic cost function in Y (see [3]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "We call this last constraint the \u201cat least one\u201d constraint in contrast to forcing \u201cexactly one\u201d ordered prediction as previously proposed in a discriminative clustering approach on video for action localization [6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 4, "context": "Our action model is equivalent to the one of [6] applied to only one action.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "We constrain our model to predict exactly one time interval for an action per clip, an approach for actions that was shown to be beneficial in a weakly supervised setting [6] (referred to as \u201caction saliency\u201d constraint).", "startOffset": 171, "endOffset": 174}, {"referenceID": 27, "context": "Problem (1) is NP-hard in general [30] due to its specific integer constraints.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "Inspired by the approach of [5] that was successful to approximate combinatorial optimization problems, we propose to use the tightest convex relaxation of the feasible subset of binary matrices by taking its convex hull.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "When dealing with a constrained optimization problem for which it is easy to solve linear programs but difficult to project on the feasible set, the Frank-Wolfe algorithm is an excellent choice [22, 28].", "startOffset": 194, "endOffset": 202}, {"referenceID": 25, "context": "When dealing with a constrained optimization problem for which it is easy to solve linear programs but difficult to project on the feasible set, the Frank-Wolfe algorithm is an excellent choice [22, 28].", "startOffset": 194, "endOffset": 202}, {"referenceID": 24, "context": "Moreover, [27] recently showed that the Frank-Wolfe algorithm with line-search converges to a stationary point for non-convex objectives at a rate of", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "Previous work has explored \u201cexact one\u201d ordering constraints for time localization problems [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "Previous works [2, 6] have observed that using the learned W \u2217 classifier for rounding gave better results than other possible alternatives.", "startOffset": 15, "endOffset": 21}, {"referenceID": 4, "context": "Previous works [2, 6] have observed that using the learned W \u2217 classifier for rounding gave better results than other possible alternatives.", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "sources: the instructional video dataset introduced in [2], the Charades dataset from [37], and some additional videos downloaded from YouTube.", "startOffset": 55, "endOffset": 58}, {"referenceID": 34, "context": "sources: the instructional video dataset introduced in [2], the Charades dataset from [37], and some additional videos downloaded from YouTube.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "In order to obtain detectors for the five objects, we finetune the FastRCNN network [17] with training data from ImageNet [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "In order to obtain detectors for the five objects, we finetune the FastRCNN network [17] with training data from ImageNet [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 32, "context": "In our set-up with only moderate amount of training data, we observed that class-agnostic object proposals combined with FastRCNN performed better than FasterRCNN [35].", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "In detail, we use geodesic object proposals [26] and set a relatively low object detection threshold (0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 2, "context": "We track objects using a generic KLT tracker from [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 32, "context": "The CNN feature is extracted with a ROI pooling [35] of ResNet50 [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "The CNN feature is extracted with a ROI pooling [35] of ResNet50 [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "the approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.", "startOffset": 16, "endOffset": 25}, {"referenceID": 3, "context": "the approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.", "startOffset": 16, "endOffset": 25}, {"referenceID": 4, "context": "the approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.", "startOffset": 16, "endOffset": 25}, {"referenceID": 40, "context": "For the motion we use a 2,000 dimensional bag-of-word representation of histogram of local optical flow (HOF) obtained from Improved Dense Trajectories [43].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "Following [2], we add an appearance vector that is obtained from a 1,000 dimensional bagof-word vector of conv5 features from VGG16 [39].", "startOffset": 10, "endOffset": 13}, {"referenceID": 36, "context": "Following [2], we add an appearance vector that is obtained from a 1,000 dimensional bagof-word vector of conv5 features from VGG16 [39].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "22 (ii) [6] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "20 (iii) [6] + object cues 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "While the saliency approach (taking only the most confident detection per video) was useful for action modeling in [6], it is less suitable for our setup where multiple tracklets can be in the same state.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Baseline (ii) is the method introduced in [6] used here with only one action.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "To address this issue, we also evaluate baseline (iii), which complements [6] with the additional constraint that the action prediction has to be within the first and the last frame where the object of interest is detected, improving the overall performance above chance.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions.", "startOffset": 21, "endOffset": 32}, {"referenceID": 28, "context": "Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions.", "startOffset": 21, "endOffset": 32}, {"referenceID": 33, "context": "Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions.", "startOffset": 21, "endOffset": 32}, {"referenceID": 6, "context": "We then train a linear SVM classifier [8] on bigram text features.", "startOffset": 38, "endOffset": 41}], "year": 2017, "abstractText": "Many human activities involve object manipulations aiming to modify the object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulation actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost with constraints. We assume a consistent temporal order for the changes in object states and manipulation actions, and introduce new optimization techniques to learn model parameters without additional supervision. We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations. We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.", "creator": "LaTeX with hyperref package"}}}