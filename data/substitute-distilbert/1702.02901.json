{"id": "1702.02901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR)", "abstract": "an big shift that surrounds the transition of brain - computer interfaces ( bcis ) from physiological settings to real - domain applications is the availability of high - performance and robust learning algorithms that can effectively handle individual differences, i. e., ideas that can be applied to a new subject with zero or very little subject - specific calibration data. transfer learning and domain hopping have been extensively used another evaluation purpose. whereas, most previous works focused on classification problems. this paper considers an algorithm regression problem in bci, namely, online driver drowsiness estimation from eeg signals. by integrating fuzzy sets with distance adaptation, to discuss how novel online weighted adaptation regularization for regression ( cot ) algorithm to reduce their amount of subject - specific calibration data, and design a source domain selection ( sds ) approach to save about half behind the computational cost of owarr. using a digital driving dataset with 15 subjects, we show that owarr and owarr - sds can offset significantly smaller estimation errors than several other approaches. we also provide comprehensive calculations on the robustness of cs and owarr - sds.", "histories": [["v1", "Thu, 9 Feb 2017 17:14:15 GMT  (1269kb)", "http://arxiv.org/abs/1702.02901v1", "in press"]], "COMMENTS": "in press", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["dongrui wu", "vernon j lawhern", "stephen gordon", "brent j lance", "chin-teng lin"], "accepted": false, "id": "1702.02901"}, "pdf": {"name": "1702.02901.pdf", "metadata": {"source": "CRF", "title": "Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR)", "authors": ["Dongrui Wu"], "emails": ["drwu09@gmail.com,", "vernon.j.lawhern.civ@mail.mil,", "sgordon@dcscorp.com,", "brent.j.lance.civ@mail.mil,", "Chin-Teng.Lin@uts.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n02 90\n1v 1\n[ cs\n.L G\nIndex Terms\u2014Brain-computer interface, domain adaptation, EEG, ensemble learning, fuzzy sets, transfer learning\nI. INTRODUCTION\nBrain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics. However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50]. This paper focuses on the last challenge, more specifically, how to generalize a BCI algorithm to a new subject, with zero or very little subjectspecific calibration data.\nTransfer learning (TL) [34], which improves learning in a new task by leveraging data or knowledge from other relevant tasks, represents a promising solution to the above challenge. Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].\nHowever, most of the above TL approaches consider only BCI classification problems. Reducing the calibration data requirement in BCI regression problems has been largely understudied. One example is online driver drowsiness estimation from EEG signals, which will be investigated in this paper. This is a very important problem because drowsy driving is among the most important causes of road crashes, following only to alcohol, speeding, and inattention [38]. According to the National Highway Traffic Safety Administration [44], 2.5% of fatal motor vehicle crashes (on average 886/year in the U.S.) and 2.5% of fatalities (on average 1,004/year in the U.S.) between 2005 and 2009 involved drowsy driving. However, to our best knowledge, there have been only two works [51], [54] on TL for drowsiness estimation. Wei et al. [51] showed that selective TL, which selectively turns TL on or off based the level of session generalizability, can achieve better estimation performance than approaches that always turn TL on or off. Wu et al. [54] proposed a domain adaptation with model fusion (DAMF) approach for drowsiness estimation. By making use of data from other subjects in a DA framework, DAMF requires very little subject-specific calibration data, which significantly increases its real-world applicability.\nIn this paper, by making use of fuzzy sets (FSs) [67], we extend our earlier work on online weighted adaptation\n2 regularization [60] from classification to regression to estimate driver drowsiness online from EEG signals. We show that our two proposed algorithms can achieve significantly better estimation performance than the DAMF and two other baseline approaches.\nThe remainder of the paper is organized as follows: Section II introduces the details of the proposed online weighted adaptation regularization for regression (OwARR) algorithm. Section III further introduces a source domain selection (SDS) approach to save the computational cost of OwARR. Section IV presents experimental results and performance comparisons of OwARR and OwARR-SDS with three other approaches. Finally, Section V draws conclusions and points out future research directions."}, {"heading": "II. ONLINE WEIGHTED ADAPTATION REGULARIZATION FOR REGRESSION (OWARR)", "text": "In [60] we have defined two types of calibration in BCI: 1) Offline calibration, in which a pool of unlabeled EEG\nepochs have been obtained a priori, and a subject or an oracle is queried to label some of these epochs, which are then used to train a model to label the remaining epochs in the pool. 2) Online calibration, in which some labeled EEG epochs are obtained on-the-fly, and then a model is trained from them for future (unseen) EEG epochs.\nThe major different between them is that, for offline calibration, the unlabeled EEG epochs can be used to help design the model (e.g., semi-supervised learning), whereas in online calibration there are no unlabeled EEG epochs. Additionally, in offline calibration we can query any epoch in the pool for the label, but in online calibration usually the sequence of the epochs is pre-determined and the subject or oracle has little control on which epochs to see next.\nWe only consider online calibration in this paper. This section introduces the OwARR algorithm, which extends the online weighted adaptation regularization algorithm [60] from classification to regression, by making use of FSs."}, {"heading": "A. Problem Definition", "text": "A domain [23], [34] D in TL consists of a d-dimensional feature space X and a marginal probability distribution P (x), i.e., D = {X , P (x)}, where x \u2208 X . Two domains Dz and Dt are different if X z 6= X t, and/or P z(x) 6= P t(x).\nA task [23], [34] T in TL consists of an output space Y and a conditional probability distribution Q(y|x). Two tasks T z and T t are different if Yz 6= Yt, or Qz(y|x) 6= Qt(y|x).\nGiven the zth source domain Dz with nz samples (xzi , yzi ), i = 1, ..., nz, and a target domain Dt with m calibration samples (xtj , y t j), j = 1, ...,m, DA aims to learn a target prediction function f(x) : x 7\u2192 y with low expected error on Dt, under the assumptions that X z = X t, Yz = Yt, P z(x) 6= P t(x), and Qz(y|x) 6= Qt(y|x).\nIn driver drowsiness estimation from EEG signals, EEG signals from a new subject are in the target domain, while EEG signals from the zth existing subject are in the zth source domain. A single data sample consists of the feature vector for\na single EEG epoch in either domain. Though the features in source and target domains are extracted in the same way, generally their marginal and conditional probability distributions are different, i.e., P z(x) 6= P t(x) and Qz(y|x) 6= Qt(y|x), because different subjects usually have similar but distinct drowsy neural responses. As a result, data from a source domain cannot represent data in the target domain accurately, and must be integrated with some target domain data to induce the target domain regression function."}, {"heading": "B. The Learning Framework", "text": "Because\nf(x) = Q(y|x) = P (x, y) P (x) = Q(x|y)P (y) P (x) , (1)\nto use the data in the zth source domain in the target domain, we need to minimize the distance between the marginal and conditional probability distributions in the two domains by ensuring that1 P z(x) is close to P t(x), and Qz(x|y) is also close to Qt(x|y).\nAssume both the output and each dimension of the input vector have zero mean. Then, the regression function can be written as\nf(x) = \u03b1Tx (2)\nwhere \u03b1 is the regression parameter vector to be found. The learning framework of OwARR is then formulated as:\nf =argmin f\nn \u2211\ni=1\n(yi \u2212 f(xi))2 + wt n+m \u2211\ni=n+1\n(yi \u2212 f(xi))2\n+ \u03bb[d(P z , P t) + d(Qz, Qt)]\u2212 \u03b3r\u03032(y, f(x)) (3) where \u03bb and \u03b3 are non-negative regularization parameters, and wt is the overall weight for target domain samples, which should be larger than 1 so that more emphasis is given to target domain samples than source domain samples.\nBriefly speaking, the first two terms in (3) minimize the sum of squared errors in the source domain and target domain, respectively. The 3rd term minimizes the distance between the marginal and conditional probability distributions in the two domains. The last term maximizes the approximate sample Pearson correlation coefficient between y and f(x), which helps avoid the undesirable situation that the regression output is (nearly) a constant.\nIn the next subsections we will explain how to compute the individual terms in (3)."}, {"heading": "C. Sum of Squared Error Minimization", "text": "Let\nX = [x1, ...,xn+m] T (4)\ny = [y1, ..., yn+m] T (5)\nwhere the first n xi and yi are the column input vectors and the corresponding outputs in the source domain, the next m\n1Strictly speaking, we should also make sure P z(y) is close to P t(y). In this paper we assume P z(y) and P t(y) are close. Our future research will consider the general case that P z(y) and P t(y) are different.\n3 xi and yi are the column input vectors and the corresponding outputs in the target domain, and T is the matrix transpose operation.\nDefine E \u2208 R(n+m)\u00d7(n+m) as a diagonal matrix with\nEii =\n{\n1, 1 \u2264 i \u2264 n wt, n+ 1 \u2264 i \u2264 n+m (6)\nThen, the first two terms in (3) can be rewritten as\nn \u2211\ni=1\n(yi \u2212 f(xi))2 + wt n+m \u2211\ni=n+1\n(yi \u2212 f(xi))2\n=\nn+m \u2211\ni=1\nEii(yi \u2212\u03b1Txi)2\n=(yT \u2212\u03b1TXT )E(y \u2212X\u03b1) (7)\nThe optimal selection of wt is very important to the performance of the OwARR algorithm. In this paper we use\nwt = max(2, \u03c3 \u00b7 n/m) (8)\nwhere \u03c3 is a positive adjustable parameter, based on the following heuristics: 1) when m is small, each target domain sample should have a large weight so that the target domain is not overwhelmed by the source domain; 2) as m increases, the weight on the target domain samples should decrease gradually so that the source domain is not overwhelmed by the target domain; and, 3) the target domain samples should always have larger weights than the source domain samples because eventually the regression model will be applied to the target domain."}, {"heading": "D. Marginal Probability Distribution Adaptation", "text": "As in [23], [36], [60], [61], we compute d(P z, P t) using the maximum mean discrepancy (MMD):\nd(P z , P t) =\n[\n1\nn\nn \u2211\ni=1\nf(xi)\u2212 1\nm\nn+m \u2211\ni=n+1\nf(xi)\n]2\n= \u03b1TXMPX\u03b1 (9)\nwhere MP \u2208 R(n+m)\u00d7(n+m) is the MMD matrix:\n(MP )ij =\n\n  \n  \n1 n2 , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 n\n1 m2 , n+ 1 \u2264 i \u2264 n+m, n+ 1 \u2264 j \u2264 n+m \u22121 nm , otherwise\n(10)"}, {"heading": "E. Conditional Probability Distribution Adaptation", "text": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation. In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].\n1) Conditional Probability Distribution Adaptation for Classification: Let Dzc = {xi|xi \u2208 Dz \u2227 yi = c} be the set of samples in Class c (c = 1, ..., C) of the zth source domain, Dtc = {xi|xi \u2208 Dt \u2227 yi = c} be the set of samples in Class c of the target domain, nc = |Dzc |, and mc = |Dtc|. Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.e.,\nd(Qz , Qt) =\nC \u2211\nc=1\n\n\n1\nnc\n\u2211\nxi\u2208D z c\nf(xi)\u2212 1\nmc\n\u2211\nxi\u2208D t c\nf(xi)\n\n\n2\n(11)\n2) Conditional Probability Distribution Adaptation for Regression: With the help of FSs (background materials are given in Appendix), we can transform the regression problem into a \u201cclassification\u201d problem and hence perform conditional probability distribution adaptation using (11). First, for the nz outputs, {yzi }i=1,...,n, in the zth source domain, we find their 5, 50 and 95 percentile2 values, pz5, p z 50 and p z 95, respectively, and define three triangular FSs3, Smallz, Mediumz and Largez, based on them, as shown in Fig. 1(a). In this way, we can \u201cclassify\u201d the outputs in the zth source domain into three fuzzy classes, Smallz, Mediumz and Largez, corresponding to the different classes in a traditional crisp classification problem. However, note that in the traditional crisp classification problem a sample can only belong to one class. For the fuzzy classes here, a sample can belong to more than one class simultaneously, at different degrees.\nDenote Class Smallz as Class 1, Class Mediumz as Class 2, Class Largez as Class 3, and the membership degree of yzi in Class c as \u00b5zic. We then normalize each \u00b5 z ic according to its class, i.e.,\n\u00b5\u0304zic = \u00b5zic \u2211n\ni=1 \u00b5 z ic\n, i = 1, ..., n; c = 1, 2, 3 (12)\nSimilarly, we also find pt5, p t 50 and p t 95 from the m target domain outputs {yti}i=n+1,...,n+m, define three FSs, Smallt, 2There is a popular regression analysis method called quantile regression [17] in statistics and econometrics, which estimates either the conditional median or other quantiles of the response variable. The percentiles used in this paper are found directly from the data, and they should not be confused with quantile regression.\n3There can be other ways to define these FSs, e.g., we could use Gaussian FSs instead of triangular FSs, use pz\n10 , pz 50 and pz 90 instead of pz 5 , pz 50 and\npz 95 , use other than three FSs in each domain, or use type-2 FSs [30] instead of type-1. Three type-1 triangular FSs are used here for simplicity. More discussions on the sensitivity of the OwARR algorithm to the number of type-1 triangular FSs are given in Section IV-H.\n4 Mediumt and Larget, as shown in Fig. 1(b), and compute the corresponding normalized \u00b5\u0304tic, i = n+1, ..., n+m, c = 1, 2, 3.\nFinally, similar to (11), the distance between the conditional probability distributions in the target domain and the zth source domain is computed as:\nd(Qz, Qt) =\n3 \u2211\nc=1\n\n\n\u2211\nxi\u2208D z\n\u00b5\u0304zicf(xi)\u2212 \u2211\nxi\u2208Dt\n\u00b5\u0304ticf(xi)\n\n\n2\n(13)\nSubstituting (2) into (13), it follows that\nd(Qz, Qt) =\n3 \u2211\nc=1\n\n\n\u2211\nxi\u2208D z\n\u00b5\u0304zic\u03b1 T xi \u2212\n\u2211\nxi\u2208D t\n\u00b5\u0304tic\u03b1 T xi\n\n\n2\n= 3 \u2211\nc=1\n\u03b1 TXMcX\u03b1 = \u03b1 TXMQX\u03b1 (14)\nwhere\nMQ = M1 +M2 +M3 (15)\nin which M1, M2 and M3 are MMD matrices computed as:\n(Mc)ij =\n\n    \n   \n\u00b5\u0304zic\u00b5\u0304 z jc, xi, xj \u2208 Dzc \u00b5\u0304tic\u00b5\u0304 t jc, xi, xj \u2208 Dtc \u2212\u00b5\u0304zic\u00b5\u0304tjc, xi \u2208 Dzc , xj \u2208 Dtc \u2212\u00b5\u0304tic\u00b5\u0304zjc, xi \u2208 Dtc, xj \u2208 Dzc 0, otherwise\n(16)"}, {"heading": "F. Maximize the Approximate Sample Pearson Correlation Coefficient", "text": "The sample Pearson correlation coefficient r(y, f(x)) is defined as [48]:\nr(y, f(x)) = y TX\u03b1\n\u2016 y \u2016 \u00b7 \u2016 X\u03b1 \u2016\n= y TX\u03b1 \u221a\nyTy \u00b7 \u221a \u03b1 TXTX\u03b1 (17)\nand hence\nr2(y, f(x)) = \u03b1\nTXTyyTX\u03b1\nyTy \u00b7 \u03b1TXTX\u03b1 (18)\nNote that r2(y, f(x)) has \u03b1 in the denominator, so it is very challenging to find a closed-form solution to maximize it. However, observe that r2(y, f(x)) increases as \u03b1\nTXTyyTX\u03b1 increases, and decreases as \u03b1TXTX\u03b1 increases. So, instead of maximizing r2(y, f(x)) directly, in this paper we try to maximize the following function:\nr\u03032(y, f(x)) = \u03b1 TXTyyTX\u03b1\u2212\u03b1TXTX\u03b1 yTy\n= \u03b1 TXT (yyT \u2212 I)X\u03b1 yTy\n(19)\nwhere I \u2208 R(n+m)\u00d7(n+m) is an identity matrix. r\u03032(y, f(x)) has the same property as r2(y, f(x)), i.e., r\u03032(y, f(x)) increases as \u03b1TXTyyTX\u03b1 increases, and decreases as \u03b1 TXTX\u03b1 increases."}, {"heading": "G. The Closed-Form Solution", "text": "Substituting (7), (9), (14) and (19) into (3), we can rewrite it as\n\u03b1 =argmin \u03b1\n(yT \u2212\u03b1TXT )E(y \u2212X\u03b1)\n+ \u03bb\u03b1TXT (MP +MQ)X\u03b1\n+ \u03b3 \u03b1 TXT (I \u2212 yyT )X\u03b1 yTy\n(20)\nSetting the derivative of the objective function above to 0 leads to\n\u03b1 =\n[ XT ( E + \u03bbMP + \u03bbMQ + \u03b3 I \u2212 yyT yTy ) X\n]\u22121\nXTEy\n(21)"}, {"heading": "H. The Complete OwARR Algorithm", "text": "The pseudo-code for the complete OwARR algorithm is described in Algorithm 1. We first perform OwARR for each source domain separately, and then construct the final regression model as a weighted average of these base models, where the weight is the inverse of the training accuracy of the corresponding base model. The final regression model will then be applied to future unlabeled data.\nAlgorithm 1: The OwARR algorithm.\nInput: Z source domains, where the zth (z = 1, ..., Z) domain has nz samples {xzi , yzi }i=1,...,nz ; m target domain samples, {xtj , ytj}j=1,...,m; Parameters \u03bb, \u03b3, and \u03c3 in (8). Output: The OwARR regression model. for z = 1, 2, ..., Z do\nConstruct X in (4), y in (5), E in (6), MP in (10), and MQ in (15); Compute \u03b1 by (21) and record it as \u03b1z ; Use \u03b1z to estimate the outputs for the nz +m samples from both domains and record the root mean squared error as az ; Assign the zth regression model a weight wz = 1/az;\nend Return f(x) = \u2211 Z z=1 wz(\u03b1z)Tx\n\u2211 Z\nz=1 wz\n."}, {"heading": "III. OWARR-SDS", "text": "A SDS procedure for online classification problems has been proposed in [60]. In this paper it is extended to regression problems by using the fuzzy classes again defined in Fig. 1. The primary goal of SDS is to reduce the computational cost of OwARR, because when there is a large number of source domains, performing OwARR for each source domain and then aggregating the base models would be very timeconsuming.\nAssume there are Z different source domains. For the zth source domain, we first compute mzc (c = 1, 2, 3), the mean vector of each fuzzy class. Then, we also compute mtc, the\n5 mean vector of each fuzzy class in the target domain, from the m labeled samples. The distance between the two domains is:\nd(z, t) = 3 \u2211\nc=1\n||mzc \u2212mtc|| (22)\nWe next cluster the Z numbers, {d(z, t)}z=1,...,Z, by k-means clustering, and finally choose the cluster that has the smallest centroid, i.e., the source domains that are closest to the target domain. In this way, on average we only need to perform OwARR for Z/k source domains. We used k = 2 in this paper.\nThe pseudo-code for the complete OwARR-SDS algorithm is described in Algorithm 2. We first use SDS to select the Z \u2032 closest source domains, and then perform DA for each selected source domain separately. The final regression model is a weighted average of these base models, with the weight being the inverse of the training accuracy of the corresponding base model.\nAlgorithm 2: The OwARR-SDS algorithm.\nInput: Z source domains, where the zth (z = 1, ..., Z) domain has nz samples {xzi , yzi }i=1,...,nz ; m target domain samples, {xtj , ytj}j=1,...,m; \u03bb, \u03b3, \u03c3 in (8), and k in k-means clustering. Output: The OwARR-SDS regression model. // SDS starts if m == 0 then\nSelect all Z source domains; Go to OwARR.\nelse for z = 1, 2, ..., Z do\nCompute d(z, t), the distance between the target domain and the zth source domain, by (22).\nend Cluster {d(z, t)}z=1,...,Z by k-means clustering; Select the Z \u2032 source domains that belong to the cluster with the smallest centroid.\nend // SDS ends; OwARR starts for z = 1, 2, ..., Z \u2032 do\nConstruct X in (4), y in (5), E in (6), MP in (10), and MQ in (15); Compute \u03b1 by (21) and record it as \u03b1z; Use \u03b1z to estimate the outputs for the nz +m samples from both domains and record the root mean squared error as az; Assign the zth regression model a weight wz = 1/az;\nend // OwARR ends Return f(x) = \u2211 Z \u2032 z=1 wz(\u03b1z)Tx\n\u2211 Z\u2032\nz=1 wz\n."}, {"heading": "IV. EXPERIMENTS AND DISCUSSIONS", "text": "Experimental results on driver drowsiness estimation from EEG signals are presented in this section to demonstrate the performance of OwARR and OwARR-SDS."}, {"heading": "A. Experiment Setup", "text": "We reused the experiment setup and data in [54]. 16 healthy subjects with normal or corrected to normal vision were recruited to participate in a sustained-attention driving experiment [3], [4], which consisted of a real vehicle mounted on a motion platform with 6 degrees of freedom immersed in a 360- degree virtual-reality scene. Each participant read and signed an informed consent form before the experiment began. Each experiment lasted for about 60-90 minutes and was conducted in the afternoon when the circadian rhythm of sleepiness reached its peak. To induce drowsiness during driving, the virtual-reality scenes simulated monotonous driving at a fixed 100 km/h speed on a straight and empty highway. During the experiment, lane-departure events were randomly applied every 5-10 seconds, and participants were instructed to steer the vehicle to compensate for these perturbations as quickly as possible. Subjects\u2019 cognitive states and driving performance were monitored via a surveillance video camera and the vehicle trajectory throughout the experiment. The response time in response to the perturbation was recorded and later converted to drowsiness index. Meanwhile, participants\u2019 scalp EEG signals were recorded using a 32-channel (30-channel EEGs plus 2-channel earlobes) 500 Hz Neuroscan NuAmps Express system (Compumedics Ltd., VIC, Australia).\nThe Institutional Review Board of the Taipei Veterans General Hospital approved the experimental protocol."}, {"heading": "B. Evaluation Process and Performance Measures", "text": "The complete procedure for the application of OwARR for driver drowsiness estimation is shown in Algorithm 3. Compared with Algorithm 1 on OwARR for a generic application, here we also include detailed EEG data pre-processing and feature extraction steps. Observe that although the feature extraction methods for different auxiliary subjects have the same steps, their parameters (channels removed, principal components used, ranges used in normalization) may be different, so we need to record them for each auxiliary subject so that the features in the individual regression models can be computed correctly.\nFrom the experiments we already knew the drowsiness indices for all \u223c1200 epochs. To evaluate the performances of different algorithms, for each subject, we used up to 100 epochs in a randomly chosen continuous block for calibration, and the rest \u223c1100 epochs for testing. Every time when five epochs were acquired, we computed the testing performance to show how the performance of the regression models changed over time. We ran this evaluation process 30 times, each time with a randomly chosen 100-epoch calibration block, to obtain statistically meaningful results. Finally, we repeated this entire process 15 times so that each subject had a chance to be the \u201c15th\u201d subject.\nThe primary performance measured used in this paper is the root mean squared error (RMSE) between the \u223c1100 true drowsiness indices and the corresponding estimates for the testing epochs, which is optimized in the object functions of all algorithms. The secondary performance measure is\n6 Algorithm 3: OwARR for driver drowsiness estimation.\nInput: EEG data and the corresponding RTs; \u03bb, \u03b3, and \u03c3 in OwARR (Algorithm 2); tmax, the duration of the calibration period; \u2206t, the time interval (in second) between two successive epochs in calibration. Output: The OwARR regression model. Set t = 0, and start the calibration; while t < tmax do\nif t \u2265 30 then Compute the drowsiness index y in (5); // Pre-processing Extract 30s EEG data in [t\u2212 30, t]; Band-pass filter the EEG signals to [0, 50] Hz; Down-sample to 250Hz; Re-reference to averaged earlobes; Compute the PSD in the [4, 7.5] Hz theta band for each channel; Convert the PSDs to dB for each channel; end Wait until t = t+\u2206t;\nend // OwARR for z = 1, 2, ..., Z do\n// Feature extraction Concatenate each channel of the powers of the zth subject with the corresponding powers of the new subject; Remove channels which have at least one power larger than a certain threshold; Normalize the powers of each remaining channel to mean 0 and std 1; Put all the powers in a matrix, whose rows represent different EEG channels; Extract a few leading principal components of the power matrix that account for 95% of the variance; Find the corresponding scores of the leading principal components; Normalize each dimension of the scores to [0, 1]; Collect the scores for each epoch as features; Record the parameters of the feature extraction method (channels removed, principal components used, ranges used in normalization) as FEz; // OwARR training Construct X in (4), y in (5), E in (6), MP in (10), and MQ in (15); Compute \u03b1 by (21) and record it as \u03b1z; Use \u03b1z to estimate the outputs for all known samples and record the root mean squared error as az; Assign the zth regression model a weight wz = 1/az;\nend Return The OwARR regression model f(x) = \u2211 Z z=1 wz(\u03b1z)Txz\n\u2211 Z\nz=1 wz , where xz is the feature vector extracted using FEz .\nthe correlation coefficient (CC) between the true drowsiness indices and the estimates."}, {"heading": "C. Preprocessing and Feature Extraction", "text": "The 16 subjects had different lengths of experiment, because the disturbances were presented randomly every 5-10 seconds. Data from one subject was not correctly recorded, so we used only 15 subjects. To ensure fair comparison, we used only the first 3,600 seconds data for each subject.\nWe defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:\ny = max\n{\n0, 1\u2212 e\u2212(\u03c4\u2212\u03c40) 1 + e\u2212(\u03c4\u2212\u03c40)\n}\n(23)\n\u03c40 = 1 was used in this paper, as in [54]. The drowsiness indices were then smoothed using a 90-second square movingaverage window to reduce variations. This does not reduce the sensitivity of the drowsiness index because the cycle lengths of drowsiness fluctuations are longer than 4 minutes [26]. The smoothed drowsiness indices for the 15 subjects are shown in Fig. 2. Observe that each subject had some drowsiness indices at or close to 1, indicating drowsy driving.\nWe used EEGLAB [6] for EEG signal preprocessing. A band-pass filter (1-50 Hz) was applied to remove highfrequency muscle artifacts, line-noise contamination and DC drift. Next the EEG data were downsampled from 500 Hz to 250 Hz and re-referenced to averaged earlobes.\nWe tried to predict the drowsiness index for each subject every three seconds. All 30 EEG channels were used in feature extraction. We epoched 30-second EEG signals right before each sample point, and computed the average power spectral density (PSD) in the theta band (4-7.5 Hz) for each channel using Welch\u2019s method [52], as research [27] has shown that theta band spectrum is a strong indicator of drowsiness. The theta band powers for three selected channels and the corresponding drowsiness index for Subject 1 are shown in Fig. 3(a). Observe that drowsiness index has strong correlations with the theta band powers.\nNext, we converted the 30 theta band powers to dBs. To remove noises or bad channel readings, we removed channels\n7 Time CZ T5 CP4 Drowsiness\n(a)\nTime\nPC1\nPC2\nPC3\nDrowsiness\n(b)\nFig. 3. EEG features and the corresponding drowsiness indices for Subject 1. (a) Theta band powers for three selected channels; (b) The top three principal component (PC) features.\nwhose maximum dBs were larger than 20. We then normalized the dBs of each remaining channel to mean zero and standard deviation one, and extracted a few (usually around 10) leading principal components, which accounted for 95% of the variance. The projections of the theta band powers onto these principal components were then normalized to [0, 1] and used as our features. Three such features for Subject 1 are shown in Fig. 3(b). Observe that the score on the first principal component has obvious correlation with the drowsiness index, suggesting that estimating the drowsiness index from the scores on the principal components is possible."}, {"heading": "D. Algorithms", "text": "We compared the performances of OwARR and OwARRSDS with three other algorithms introduced in [54]:\n1) Baseline 1 (BL1), which combines data from all 14 existing subjects, builds a ridge regression model [10], and applies it to the new subject. That is, BL1 tries to build a subject-independent regression model and ignores data from the new subject completely. 2) Baseline 2 (BL2), which builds a ridge regression model using only subject-specific calibration samples from the new subject. That is, BL2 ignores data from existing subjects completely. 3) DAMF, which builds 14 ridge regression models by combining data from each auxiliary subject with data from the new subject, respectively, and then uses a weighted average to obtain the final regression model. The weights are also the inverse of the training RMSEs, as in Algorithms 1 and 2.\nThe ridge parameter \u03c3 = 0.01 was used in the above three algorithms, as in [54]. For OwARR and OwARR-SDS, we used \u03c3 = 0.2, \u03bb = 10, and \u03b3 = 0.5. However, as will be shown in Section IV-H, OwARR and OwARR-SDS are robust to these three parameters."}, {"heading": "E. Regression Performance Comparison", "text": "The average RMSEs and CCs for the five algorithms across the 15 subjects are shown in Fig. 4, and the RMSEs and CCs for the individual subjects are shown in Fig. 5. Observe that DAMF, OwARR and OwARR-SDS had very similar CCs, and all of them were better than the CCs of BL1 and BL2. Additionally:\n0 20 40 60 80 100 m, number of subject-specific samples\n0.2\n0.25\n0.3\n0.35\n0.4\nR M S E\n(a)\n0 20 40 60 80 100 m, number of subject-specific samples\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nC C\nBL1 BL2 DAMF OwARR OwARR-SDS\n(b)\nIn summary, the three DA based approaches generally had better performance than BL1, which does not use subject-specific data at all, and also BL2, which does not use auxiliary data at all. This suggests that DA is indeed beneficial. Moreover, our proposed OwARR and OwARR-SDS achieved the best overall performances among the five algorithms.\nWe also performed two-way Analysis of Variance (ANOVA) for each m to check if the RMSE differences among the five algorithms were statistically significant, by setting the subjects as a random effect. Two-way ANOVA showed statistically significant differences among them (p < 0.01) for all m. Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2]. The p-values are shown in Table I, where the statistically significant ones are marked in bold. Observe that the differences between OwARR and the other three algorithms (BL1, BL2, and DAMF) were always statistically significant when m > 0, so were the differences between OwARR-SDS and the two baseline algorithms. The difference between OwARR-SDS and DAMF was statistically significant\n9 for m \u2208 [5, 75]. There was no statistically significant difference between OwARR and OwARR-SDS.\nFinally, we can conclude that given the same amount of subject-specific calibration data, OwARR and OwARRSDS can achieve significantly better estimation performance than the other three approaches. Or, in other words, given a desired RMSE, OwARR and OwARR-SDS require significantly less subject-specific calibration data than the other three approaches. For example, in Fig. 4(a), the average RMSE for BL2 when m = 100 was 0.2988, whereas OwARR and OwARR-SDS can achieve even smaller RMSEs without using any subject-specific calibration samples. The average RMSEs for OwARR and OwARR-SDS when m = 5 were 0.2347 and 0.2348, respectively, whereas DAMF needed at least 45 subject-specific samples to achieve these RMSEs, and BL2 needed at least 100 samples."}, {"heading": "F. Computational Cost", "text": "In this subsection we compare the computational cost of the five algorithms, particularly, OwARR and OwARR-SDS, because the primary goal of SDS is to down-select the number of auxiliary subjects and hence to reduce the computational cost of OwARR.\nFig. 6 shows the average number of similar subjects selected by SDS for the 15 subjects. Observe that most of the time fewer than seven subjects (half of the number of auxiliary subjects) were selected.\nTo quantify the computational cost of the five algorithms, we show in Fig. 7 the training times for different m, averaged over 10 runs and across the 15 subjects. The platform was a Dell XPS 13 notebook, with Intel Core i7-5500M CPU@2.40GHz, 8GB memory, and 256GB solid state drive. The software was Matlab R2015b running in 64-bit Windows 10 Pro. Each algorithm was optimized to the best ability of the authors. Observe that the training time of BL1, BL2, and DAMF was almost constant, whereas the training time of OwARR increased monotonically as m increased. Interestingly, the training time of OwARR-SDS decreased slightly as m increased, because Fig. 6 shows that generally the average number of similar subjects selected by SDS decreased as m increased.\nThe computational costs of OwARR and OwARR-SDS were much higher than DAMF, because they used more sophisticated DA approaches. However, except for m = 0, at which point OwARR and OwARR-SDS had identical training time, the training time of OwARR-SDS was on average only about 49% of OwARR. This 51% computation time saving is very worthwhile when the number of source domains is very large and hence computing OwARR for all the source domains is too slow.\n0 20 40 60 80 100\nm, number of subject-specific samples\n0\n0.5\n1 C o m p u ta ti o n ti m e (s )\nBL1 BL2 DAMF OwARR OwARR-SDS\nFig. 7. Average training time of the five algorithms. Note that BL1 overlaps with DAMF.\n0\n2\n20 100\nC om\npu ta\ntio n\ntim e\n(s )\n4\n40 80\nm\n6\nZ\n60 50 80 30\n10100 2\n(a)\n0\n0.2\n20 1000\n0.4\nC om\npu ta\ntio n\ntim e\n(s )\n40 800\nm\n0.6\nn\n60 500 80 200100 50\n(b)\nFig. 8. Scalability of OwARR with respect to (a) the number of source domains (each domain had about 1,200 samples); (b) the number of samples in each source domain (14 source domains were used).\nWe also investigated the scalability of OwARR with respect to Z , the number of source domains, and n, the number of samples in each source domain. Because we only had 14 source domains in this dataset, we bootstrapped them to create additional domains when Z \u2265 14. The results are shown in Fig. 8. Observe from Fig. 8(a) that the computational cost of OwARR increased linearly with the number of source domains, which is intuitive, because OwARR performs DA for each source domain separately and then aggregates the results. However, Fig. 8(b) shows that the computational cost of OwARR increased superlinearly with the number of samples in the source domains. Least-squares curve fitting found that the computation time was about 0.0000021 \u00b7 n1.8 + 0.035 seconds, i.e., the computational cost is O(n1.8) for 14 source domains.\nFinally, it is important to note that the above analyses are only for the training of the algorithms. Once the training is done, the resulting OwARR and OwARR-SDS models can be executed much faster."}, {"heading": "G. Robustness to Noises", "text": "It is also important to study the robustness of the five algorithms to noises. According to [68], there are two types of noises: class noise, which is the noise on the model outputs, and attribute noise, which is the noise on the model inputs. In this subsection we focus on the attribute noise.\nAs in [68], for each model input, we randomly replaced q% (q = 0, 10, ..., 50) of all epochs from the new subject with a uniform noise between its minimum and maximum values. After this was done for both the training and testing data, we trained the five algorithms on the corrupted training data and then tested their performances on the corrupted testing data. The RMSEs for three different m (the number of labeled subject-specific samples), averaged across 15 subjects\n10\nwith five runs per subject, are shown in Fig. 9. Observe that as the noise level q increased, generally all algorithms had worse RMSEs. OwARR and OwARR-SDS still had the smallest RMSEs among the five when q was small. However, when q increased, DAMF became the best. This suggests that OwARR and OwARR-SDS may not be as robust as DAMF with respect to attribute noises, but when the noise level is low, the performance improvement achieved from the sophisticated optimizations in OwARR and OwARR-SDS dominates, and hence they are still the best algorithms among the five. When the noise level is high, we may need some noise handling approaches, e.g., noise correction [68], before applying OwARR and OwARR-SDS."}, {"heading": "H. Parameter Sensitivity Analysis", "text": "The OwARR algorithm has three adjustable parameters: \u03c3, which determines the weight wt for the target domain samples; \u03bb, which is a regularization parameter minimizing the distances between the marginal and conditional probability\ndistributions in the source and target domains; and \u03b3, which maximizes the approximate Pearson correlation coefficient between the true and estimated outputs. It is interesting to study whether all of them are necessary.\nFor this purpose, we constructed three modified versions of the OwARR algorithms by setting \u03c3, \u03bb and \u03b3 to zero, respectively, and compared their average RMSEs with that of the original OwARR. The results are shown in Fig. 10. Observe that the original OwARR had better performance than all three modified versions, suggesting that all three parameters in OwARR contributed to its superior performance.\nNext we studied the sensitivity of OwARR to the three adjustable parameters, \u03c3, \u03bb and \u03b3. The results are shown in Fig. 11(a)-(c). Observe that OwARR is robust to \u03c3 in the range of [0.1, 0.4], to \u03bb in the range of [1, 20], and to \u03b3 in the range of [0.01, 1].\nAdditionally, three type-1 triangular FSs have been used in conditional probability distribution adaptation (Section II-E) in this paper for simplicity. It is also interesting to study the sensitivity of the OwARR algorithm to the number of FSs. The results are shown in Fig. 11(d). Observe that OwARR gives the optimal performance when the number of FSs is between 2 and 5, but its performance gradually deteriorates when the number of FSs further increases. This is intuitive, because the target domain has a limited number of labeled training samples, so as the number of FSs increases, the number of target domain\n11\nsamples that fall into each fuzzy class decreases, and hence the computed fuzzy class means are less reliable. As a result, the distance between the conditional probability distributions [see (14)] cannot be reliably computed.\nAnother interesting questions is: what would be the performance of OwARR if no FSs are used at all, i.e., conditional probability distribution adaptation is disabled? This is corresponding to the left-most slice in Fig. 11(d), where the number of FSs is zero. Observe that this results in worse RMSEs than the case that two to five FSs are used in conditional probability distribution adaptation, suggesting the FS approach is beneficial."}, {"heading": "I. Effectiveness of the Ensemble Fusion Strategy", "text": "From Algorithm 1 (Algorithm 2) it is clear that the final step of OwARR (OwARR-SDS) uses ensemble learning: the base DA models are aggregated using a weighted average to obtain the final regression model, and the weight is inversely proportional to the training RMSE of the corresponding base DA model. In this subsection we study whether this fusion strategy is effective. The performances of the 14 base DA models and the final aggregation model for a typical subject are shown in Fig. 12. Observe that the aggregated model is better than most base DA models, and is also close to the best base DA model (which is unknown in practice), suggesting that the fusion strategy is effective. However, it may be possible that a better fusion strategy can make the final model outperform all base DA models. This will be one of our future research directions."}, {"heading": "V. CONCLUSIONS AND FUTURE RESEARCH", "text": "Transfer learning, which improves learning performance in a new task by leveraging data or knowledge from other relevant tasks, represents a promising solution for handling individual differences in BCI. Previously we have proposed a weighted\nadaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR. In this paper we have proposed an OwARR algorithm to extend the OwAR algorithm from classification to regression, and validated its performance on online estimation of driver drowsiness from EEG signals. Meanwhile, we have also extended the SDS algorithm for classification in [60] to regression problems, and verified that OwARR-SDS can achieve similar performance to OwARR, but save about half of the computation time. Both OwARR and OwARR-SDS use fuzzy sets to perform part of the adaptation regularization, and OwARR-SDS also uses fuzzy sets to select the closest source domains.\nThough OwARR and OwARR-SDS have demonstrated outstanding performance, they can be enhanced in a number of ways, which will be considered in our future research. First, Fig. 5(a) shows that OwARR and OwARR-SDS had worse RMSEs than BL1 for some subjects. This indicates that they still have room for improvement: we could develop a mechanism to switch between BL1 and OwARR (OwARRSDS) so that a more appropriate method is chosen according to the characteristics of the new subject, similar to the idea of selective TL [51]. Second, we will extend OwARR and OwARRSDS to offline calibration, where the goal is to automatically label some initially unlabeled subject-specific samples with a small number of queries [61]. Semi-supervised learning can be used here to enhance the learning performance. Third, in this paper we combine the base learners using a simple weighted average, where the weights of the base learners are inversely proportional to their corresponding training RMSEs. This may not be optimal because what really matter here are the testing RMSEs. In online calibration it is not easy to estimate the testing RMSEs because we do not know what samples will be encountered in the future; however, in offline calibration we can better estimate the testing performances of the base learners using a spectral meta-learner approach [58], and hence a better model fusion strategy could be developed. Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort. Finally, we will apply the online and offline DA algorithms to other regression problems in BCI and beyond to cope with individual differences, e.g., estimating the continuous values of arousal, valence and dominance from\n12\nspeech signals [64] in affective computing."}, {"heading": "ACKNOWLEDGEMENT", "text": "Research was sponsored by the U.S. Army Research Laboratory and was accomplished under Cooperative Agreement Numbers W911NF-10-2-0022 and W911NF-10-D-0002/TO 0023. The views and the conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory or the U.S Government. This work was also partially supported by the Australian Research Council (ARC) under discovery grant DP150101645.\nAPPENDIX FUZZY SETS (FSS)\nFS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.\nA FS X is comprised of a universe of discourse DX of real numbers together with a membership function (MF) \u00b5\nX :\nDX \u2192 [0, 1], i.e.,\nX =\n\u222b\nDX\n\u00b5 X (x)/x (24)\nHere \u222b denotes the collection of all points x \u2208 DX with associated membership degree \u00b5\nX (x). An example of a FS is\nshown in Fig. 13. The membership degrees are \u00b5X(1) = 0, \u00b5X(3) = 0.5, \u00b5X(5) = 1, \u00b5X(6) = 0.8, and \u00b5X(10) = 0. Observe that this is different from traditional (binary) sets, where each element can only belong to a set completely (i.e., with membership degree 1), or does not belong to it at all (i.e., with membership degree 0); there is nothing in between (i.e., with membership degree 0.5).\nFSs are frequently used in modeling concepts in natural language, which may not have clear boundary. For example, we may define a hot day as temperature equal to or above 30\u25e6C, but is 29\u25e6C hot? If we represent hot as a binary set {x|x \u2265 30}, then 29\u25e6C is not hot, because it does not belong to the binary set hot. However, this does not completely agree with people\u2019s intuition: 29\u25e6C is very close to 30\u25e6C, and hence it is somewhat hot. If we represent hot as a FS, we may say 29\u25e6C is hot with a membership degree of 0.9, which sounds more reasonable."}], "references": [{"title": "Improving session-tosession transfer performance of motor imagery-based BCI using adaptive extreme learning machine", "author": ["A. Bamdadian", "C. Guan", "K.K. Ang", "J. Xu"], "venue": "Proc. 35th Annual Int\u2019l Conf. of the IEEE Engineering in Medicine and Biology Society (EMBC), Osaka, Japan, July 2013, pp. 2188\u20132191.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Controlling the false discovery rate: A practical and powerful approach to multiple testing", "author": ["Y. Benjamini", "Y. Hochberg"], "venue": "Journal of the Royal Statistical Society, Series B (Methodological), vol. 57, pp. 289\u2013 300, 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Kinesthesia in a sustained-attention driving task", "author": ["C.-H. Chuang", "L.-W. Ko", "T.-P. Jung", "C.-T. Lin"], "venue": "Neuroimage, vol. 91, pp. 187\u2013202, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Co-modulatory spectral changes in independent brain processes are correlated with task performance", "author": ["S.-W. Chuang", "L.-W. Ko", "Y.-P. Lin", "R.-S. Huang", "T.-P. Jung", "C.-T. Lin"], "venue": "Neuroimage, vol. 62, pp. 1469\u20131477, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Faster self-organizing fuzzy neural network training and a hyperparameter analysis for a braincomputer interface", "author": ["D. Coyle", "G. Prasad", "T. McGinnity"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 6, pp. 1458\u20131471, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis", "author": ["A. Delorme", "S. Makeig"], "venue": "Journal of Neuroscience Methods, vol. 134, pp. 9\u201321, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Multisubject learning for common spatial patterns in motor-imagery BCI", "author": ["D. Devlaminck", "B. Wyns", "M. Grosse-Wentrup", "G. Otte", "P. Santens"], "venue": "Computational intelligence and neuroscience, vol. 20, no. 8, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association, vol. 56, pp. 62\u201364, 1961.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1961}, {"title": "Multiple comparisons using rank sums", "author": ["O. Dunn"], "venue": "Technometrics, vol. 6, pp. 214\u2013252, 1964.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1964}, {"title": "Unsupervised fuzzy cmeans clustering for motor imagery EEG recognition", "author": ["W.-Y. Hsu", "C.-Y. Lin", "W.-F. Kuo", "M. Liou", "Y.-N. Sun", "A.C. hsin Tsai", "H.-J. Hsu", "P.-H. Chen", "I.-R. Chen"], "venue": "Int\u2019l Journal of Innovative Computing, Information and Control, vol. 7, no. 8, pp. 4965\u2013 4976, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Transfer learning in brain-computer interfaces", "author": ["V. Jayaram", "M. Alamgir", "Y. Altun", "B. Scholkopf", "M. Grosse- Wentrup"], "venue": "IEEE Computational Intelligence Magazine, vol. 11, no. 1, pp. 20\u201331, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Composite common spatial pattern for subject-to-subject transfer", "author": ["H. Kang", "Y. Nam", "S. Choi"], "venue": "Signal Processing Letters, vol. 16, no. 8, pp. 683\u2013686, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "DENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction", "author": ["N.K. Kasabov", "Q. Song"], "venue": "IEEE Trans. on Fuzzy Systems, vol. 10, no. 2, pp. 144\u2013154, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Driver drowsiness classification using fuzzy wavelet-packet-based feature-extraction algorithm", "author": ["R. Khushaba", "S. Kodagoda", "S. Lal", "G. Dissanayake"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 58, no. 1, pp. 121\u2013131, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Fuzzy Sets and Fuzzy Logic: Theory and Applications", "author": ["G.J. Klir", "B. Yuan"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Quantile Regression", "author": ["R. Koenker"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Brain-computer interface technologies in the coming decades", "author": ["B.J. Lance", "S.E. Kerick", "A.J. Ries", "K.S. Oie", "K. McDowell"], "venue": "Proc. of the IEEE, vol. 100, no. 3, pp. 1585\u20131599, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of covariate shift adaptation techniques in brain-computer interfaces", "author": ["Y. Li", "H. Kambara", "Y. Koike", "M. Sugiyama"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 57, no. 6, pp. 1318\u20131324, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A framework of adaptive brain computer interfaces", "author": ["Y. Li", "Y. Koike", "M. Sugiyama"], "venue": "Proc. 2nd IEEE Int\u2019l Conf. on Biomedical Engineering and Informatics (BMEI), Tianjin, China, October 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Biosensor technologies for augmented brain-computer interfaces in the next decades", "author": ["L.-D. Liao", "C.-T. Lin", "K. McDowell", "A. Wickenden", "K. Gramann", "T.-P. Jung", "L.-W. Ko", "J.-Y. Chang"], "venue": "Proc. of the IEEE, vol. 100, no. 2, pp. 1553\u20131566, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive EEG-based alertness estimation system by using ICA-based fuzzy neural networks", "author": ["C.-T. Lin", "L.-W. Ko", "I.-F. Chung", "T.-Y. Huang", "Y.-C. Chen", "T.-P. Jung", "S.-F. Liang"], "venue": "IEEE Trans. on Circuits and Systems-I, vol. 53, no. 11, pp. 2469\u20132476, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptation regularization: A general framework for transfer learning", "author": ["M. Long", "J. Wang", "G. Ding", "S.J. Pan", "P.S. Yu"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 5, pp. 1076\u20131089, 2014.  13", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "FuRIA: An inverse solution based feature extraction algorithm using fuzzy set theory for brain-computer interfaces", "author": ["F. Lotte", "A. Lecuyer", "B. Arnaldi"], "venue": "IEEE Trans. on Signal Processing, vol. 57, no. 8, pp. 3253\u2013 3263, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces", "author": ["F. Lotte"], "venue": "Proc. of the IEEE, vol. 103, no. 6, pp. 871\u2013890, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Lapses in alertness: Coherence of fluctuations in performance and EEG spectrum", "author": ["S. Makeig", "M. Inlow"], "venue": "Electroencephalography and Clinical Neurophysiology, vol. 86, pp. 23\u201335, 1993.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}, {"title": "Tonic, phasic and transient EEG correlates of auditory awareness in drowsiness", "author": ["S. Makeig", "T.P. Jung"], "venue": "Cognitive Brain Research, vol. 4, pp. 12\u201325, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Evolving signal processing for brain-computer interfaces", "author": ["S. Makeig", "C. Kothe", "T. Mullen", "N. Bigdely-Shamlo", "Z. Zhang", "K. Kreutz-Delgado"], "venue": "Proc. of the IEEE, vol. 100, no. Special Centennial Issue, pp. 1567\u20131584, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved neural signal classification in a rapid serial visual presentation task using active learning", "author": ["A. Marathe", "V. Lawhern", "D. Wu", "D. Slayback", "B. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 24, no. 3, pp. 333\u2013343, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions", "author": ["J.M. Mendel"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "Perceptual Computing: Aiding People in Making Subjective Judgments", "author": ["J.M. Mendel", "D. Wu"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges", "author": ["C. Muhl", "B. Allison", "A. Nijholt", "G. Chanel"], "venue": "Brain-Computer Interfaces, vol. 1, no. 2, pp. 66\u201384, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Evolutionary fuzzy ARTMAP for autoregressive model order selection and classification of EEG signals", "author": ["R. Palaniappan", "P. Raveendran", "S. Nishida", "N. Saiwaki"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, vol. 5, Nashville, TN, October 2000, pp. 3682\u20133686.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Fuzzy set technology in knowledge discovery", "author": ["W. Pedrycz"], "venue": "Fuzzy Sets and Systems, vol. 98, pp. 279\u2013290, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Large margin transductive transfer learning", "author": ["B. Quanz", "J. Huan"], "venue": "Proc. 18th ACM Conf. on Information and Knowledge Management (CIKM), Hong Kong, November 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy-set social science", "author": ["C.C. Ragin"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Fatigue, sleepiness and reduced alertness as risk factors in driving", "author": ["F. Sagberg", "P. Jackson", "H.-P. Kruger", "A. Muzer", "A. Williams"], "venue": "Institute of Transport Economics, Oslo, Tech. Rep. TOI Report 739/2004, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Transferring subspaces between subjects in brain-computer interfacing", "author": ["W. Samek", "F. Meinecke", "K.-R. Muller"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 60, no. 8, pp. 2289\u20132298, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin\u2013 Madison, Computer Sciences Technical Report 1648, 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Principal component based covariate shift adaption to reduce non-stationarity in a MEG-based brain-computer interface", "author": ["M. Spuler", "W. Rosenstiel", "M. Bogdan"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2012, no. 1, pp. 1\u20137, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamical ensemble learning with model-friendly classifiers for domain adaptation", "author": ["W. Tu", "S. Sun"], "venue": "Proc. 21st Int\u2019l Conf. on Pattern Recognition (ICPR), Tsukuba, Japan, November 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A subject transfer framework for EEG classification", "author": ["W. Tu", "S. Sun"], "venue": "Neurocomputing, vol. 82, pp. 109\u2013116, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Brain-computer interfaces: Beyond medical applications", "author": ["J. van Erp", "F. Lotte", "M. Tangermann"], "venue": "Computer, vol. 45, no. 4, pp. 26\u201334, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy time series approach for disruption prediction in Tokamak reactors", "author": ["M. Versaci", "F.C. Morabito"], "venue": "IEEE Trans. on Magnetics, vol. 39, no. 3, pp. 1503\u20131506, 2003.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward unsupervised adaptation of LDA for brain-computer interfaces", "author": ["C. Vidaurre", "M. Kawanabe", "P.V. Bunau", "B. Blankertz", "K. Muller"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 58, no. 3, pp. 587\u2013597, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Probability & Statistics for Engineers and Scientists, 8th ed", "author": ["R.W. Walpole", "R.H. Myers", "A. Myers", "K. Ye"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2007}, {"title": "A Course in Fuzzy Systems and Control", "author": ["L.-X. Wang"], "venue": "Upper Saddle River, NJ: Prentice Hall,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1997}, {"title": "A review on transfer learning for brain-computer interface classification", "author": ["P. Wang", "J. Lu", "B. Zhang", "Z. Tang"], "venue": "Prof. 5th Int\u2019l Conf. on Information Science and Technology (IC1ST), Changsha, China, April 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Selective transfer learning for EEG-based drowsiness detection", "author": ["C.-S. Wei", "Y.-P. Lin", "Y.-T. Wang", "T.-P. Jung", "N. Bigdely-Shamlo", "C.- T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of fast Fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms", "author": ["P. Welch"], "venue": "IEEE Trans. on Audio Electroacoustics, vol. 15, pp. 70\u2013 73, 1967.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1967}, {"title": "Online driver\u2019s drowsiness estimation using domain adaptation with model fusion", "author": ["D. Wu", "C.-H. Chuang", "C.-T. Lin"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Active transfer learning for reducing calibration data in single-trial classification of visually-evoked potentials", "author": ["D. Wu", "B.J. Lance", "V.J. Lawhern"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, San Diego, CA, October 2014.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering for braincomputer interaction using transfer learning and active class selection", "author": ["D. Wu", "B.J. Lance", "T.D. Parsons"], "venue": "PLoS ONE, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Offline EEG-based driver drowsiness estimation using enhanced batch-mode active learning (EBMAL) for regression", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral meta-learner for regression (SMLR) model aggregation: Towards calibrationless brain-computer interface (BCI)", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Switching EEG headsets made easy: Reducing offline calibration effort using active weighted adaptation regularization", "author": ["D. Wu", "V.J. Lawhern", "W.D. Hairston", "B.J. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, 2016, in press.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing BCI calibration effort in RSVP tasks using online weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing offline BCI calibration effort using weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Linguistic summarization using IF-THEN rules and interval type-2 fuzzy sets", "author": ["D. Wu", "J.M. Mendel"], "venue": "IEEE Trans. on Fuzzy Systems, vol. 19, no. 1, pp. 136\u2013151, 2011.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Inductive transfer learning for handling individual differences in affective computing", "author": ["D. Wu", "T.D. Parsons"], "venue": "Proc. 4th Int\u2019l Conf. on Affective Computing and Intelligent Interaction, vol. 2, Memphis, TN, October 2011, pp. 142\u2013151.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech emotion estimation in 3D space", "author": ["D. Wu", "T.D. Parsons", "E. Mower", "S.S. Narayanan"], "venue": "Proc. IEEE Int\u2019l Conf. on Multimedia & Expo (ICME), Singapore, July 2010, pp. 737\u2013742.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2010}, {"title": "Genetic learning and performance evaluation of type-2 fuzzy logic controllers", "author": ["D. Wu", "W.W. Tan"], "venue": "Engineering Applications of Artificial Intelligence, vol. 19, no. 8, pp. 829\u2013841, 2006.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2006}, {"title": "Database discovery using fuzzy sets", "author": ["R. Yager"], "venue": "International Journal of Intelligent Systems, vol. 11, pp. 691\u2013712, 1996.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1996}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, vol. 8, pp. 338\u2013353, 1965.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1965}, {"title": "Class noise vs. attribute noise: A quantitative study of their impacts", "author": ["X. Zhu", "X. Wu"], "venue": "Artificial Intelligence Review, vol. 22, pp. 177\u2013210, 2004.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 45, "endOffset": 49}, {"referenceID": 42, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 358, "endOffset": 362}, {"referenceID": 23, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 364, "endOffset": 368}, {"referenceID": 26, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 370, "endOffset": 374}, {"referenceID": 47, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 376, "endOffset": 380}, {"referenceID": 32, "context": "Transfer learning (TL) [34], which improves learning in a new task by leveraging data or knowledge from other relevant tasks, represents a promising solution to the above challenge.", "startOffset": 23, "endOffset": 27}, {"referenceID": 47, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 111, "endOffset": 114}, {"referenceID": 11, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 122, "endOffset": 126}, {"referenceID": 39, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 225, "endOffset": 229}, {"referenceID": 52, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 231, "endOffset": 235}, {"referenceID": 59, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 237, "endOffset": 241}, {"referenceID": 0, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 407, "endOffset": 410}, {"referenceID": 39, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 412, "endOffset": 416}, {"referenceID": 44, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 418, "endOffset": 422}, {"referenceID": 40, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 442, "endOffset": 446}, {"referenceID": 41, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 448, "endOffset": 452}, {"referenceID": 50, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 477, "endOffset": 481}, {"referenceID": 56, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 483, "endOffset": 487}, {"referenceID": 57, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 489, "endOffset": 493}, {"referenceID": 36, "context": "This is a very important problem because drowsy driving is among the most important causes of road crashes, following only to alcohol, speeding, and inattention [38].", "startOffset": 161, "endOffset": 165}, {"referenceID": 48, "context": "However, to our best knowledge, there have been only two works [51], [54] on TL for drowsiness estimation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 50, "context": "However, to our best knowledge, there have been only two works [51], [54] on TL for drowsiness estimation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 48, "context": "[51] showed that selective TL, which selectively turns TL on or off based the level of session generalizability, can achieve better estimation performance than approaches that always turn TL on or off.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[54] proposed a domain adaptation with model fusion (DAMF) approach for drowsiness estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "In this paper, by making use of fuzzy sets (FSs) [67], we extend our earlier work on online weighted adaptation", "startOffset": 49, "endOffset": 53}, {"referenceID": 56, "context": "regularization [60] from classification to regression to estimate driver drowsiness online from EEG signals.", "startOffset": 15, "endOffset": 19}, {"referenceID": 56, "context": "ONLINE WEIGHTED ADAPTATION REGULARIZATION FOR REGRESSION (OWARR) In [60] we have defined two types of calibration in BCI: 1) Offline calibration, in which a pool of unlabeled EEG epochs have been obtained a priori, and a subject or an oracle is queried to label some of these epochs, which are then used to train a model to label the remaining epochs in the pool.", "startOffset": 68, "endOffset": 72}, {"referenceID": 56, "context": "This section introduces the OwARR algorithm, which extends the online weighted adaptation regularization algorithm [60] from classification to regression, by making use of FSs.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "Problem Definition A domain [23], [34] D in TL consists of a d-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "Problem Definition A domain [23], [34] D in TL consists of a d-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "A task [23], [34] T in TL consists of an output space Y and a conditional probability distribution Q(y|x).", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "A task [23], [34] T in TL consists of an output space Y and a conditional probability distribution Q(y|x).", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 57, "endOffset": 61}, {"referenceID": 56, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 63, "endOffset": 67}, {"referenceID": 57, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 56, "context": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 57, "context": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 63, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 226, "endOffset": 230}, {"referenceID": 4, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 286, "endOffset": 289}, {"referenceID": 13, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 291, "endOffset": 295}, {"referenceID": 22, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 297, "endOffset": 301}, {"referenceID": 9, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 326, "endOffset": 330}, {"referenceID": 20, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 332, "endOffset": 336}, {"referenceID": 31, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 338, "endOffset": 342}, {"referenceID": 21, "context": "Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.", "startOffset": 191, "endOffset": 195}, {"referenceID": 56, "context": "Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.", "startOffset": 197, "endOffset": 201}, {"referenceID": 57, "context": "Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": ",n+m, define three FSs, Small, 2There is a popular regression analysis method called quantile regression [17] in statistics and econometrics, which estimates either the conditional median or other quantiles of the response variable.", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": ", we could use Gaussian FSs instead of triangular FSs, use p 10 , p 50 and p 90 instead of p 5 , p 50 and p 95 , use other than three FSs in each domain, or use type-2 FSs [30] instead of type-1.", "startOffset": 172, "endOffset": 176}, {"referenceID": 45, "context": "The sample Pearson correlation coefficient r(y, f(x)) is defined as [48]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 56, "context": "A SDS procedure for online classification problems has been proposed in [60].", "startOffset": 72, "endOffset": 76}, {"referenceID": 50, "context": "We reused the experiment setup and data in [54].", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "16 healthy subjects with normal or corrected to normal vision were recruited to participate in a sustained-attention driving experiment [3], [4], which consisted of a real vehicle mounted on a motion platform with 6 degrees of freedom immersed in a 360degree virtual-reality scene.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "16 healthy subjects with normal or corrected to normal vision were recruited to participate in a sustained-attention driving experiment [3], [4], which consisted of a real vehicle mounted on a motion platform with 6 degrees of freedom immersed in a 360degree virtual-reality scene.", "startOffset": 141, "endOffset": 144}, {"referenceID": 47, "context": "Set t = 0, and start the calibration; while t < tmax do if t \u2265 30 then Compute the drowsiness index y in (5); // Pre-processing Extract 30s EEG data in [t\u2212 30, t]; Band-pass filter the EEG signals to [0, 50] Hz; Down-sample to 250Hz; Re-reference to averaged earlobes; Compute the PSD in the [4, 7.", "startOffset": 200, "endOffset": 207}, {"referenceID": 0, "context": ", Z do // Feature extraction Concatenate each channel of the powers of the z subject with the corresponding powers of the new subject; Remove channels which have at least one power larger than a certain threshold; Normalize the powers of each remaining channel to mean 0 and std 1; Put all the powers in a matrix, whose rows represent different EEG channels; Extract a few leading principal components of the power matrix that account for 95% of the variance; Find the corresponding scores of the leading principal components; Normalize each dimension of the scores to [0, 1]; Collect the scores for each epoch as features; Record the parameters of the feature extraction method (channels removed, principal components used, ranges used in normalization) as FE; // OwARR training Construct X in (4), y in (5), E in (6), MP in (10), and MQ in (15); Compute \u03b1 by (21) and record it as \u03b1; Use \u03b1 to estimate the outputs for all known samples and record the root mean squared error as a; Assign the z regression model a weight w = 1/a; end Return The OwARR regression model f(x) = \u2211 Z", "startOffset": 569, "endOffset": 575}, {"referenceID": 48, "context": "We defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:", "startOffset": 22, "endOffset": 26}, {"referenceID": 50, "context": "We defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "We defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:", "startOffset": 86, "endOffset": 92}, {"referenceID": 50, "context": "\u03c40 = 1 was used in this paper, as in [54].", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "This does not reduce the sensitivity of the drowsiness index because the cycle lengths of drowsiness fluctuations are longer than 4 minutes [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 5, "context": "We used EEGLAB [6] for EEG signal preprocessing.", "startOffset": 15, "endOffset": 18}, {"referenceID": 49, "context": "5 Hz) for each channel using Welch\u2019s method [52], as research [27] has shown that theta band spectrum is a strong indicator of drowsiness.", "startOffset": 44, "endOffset": 48}, {"referenceID": 25, "context": "5 Hz) for each channel using Welch\u2019s method [52], as research [27] has shown that theta band spectrum is a strong indicator of drowsiness.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "The projections of the theta band powers onto these principal components were then normalized to [0, 1] and used as our features.", "startOffset": 97, "endOffset": 103}, {"referenceID": 50, "context": "Algorithms We compared the performances of OwARR and OwARRSDS with three other algorithms introduced in [54]: 1) Baseline 1 (BL1), which combines data from all 14 existing subjects, builds a ridge regression model [10], and applies it to the new subject.", "startOffset": 104, "endOffset": 108}, {"referenceID": 50, "context": "01 was used in the above three algorithms, as in [54].", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2].", "startOffset": 246, "endOffset": 249}, {"referenceID": 4, "context": "for m \u2208 [5, 75].", "startOffset": 8, "endOffset": 15}, {"referenceID": 64, "context": "According to [68], there are two types of noises: class noise, which is the noise on the model outputs, and attribute noise, which is the noise on the model inputs.", "startOffset": 13, "endOffset": 17}, {"referenceID": 64, "context": "As in [68], for each model input, we randomly replaced q% (q = 0, 10, .", "startOffset": 6, "endOffset": 10}, {"referenceID": 64, "context": ", noise correction [68], before applying OwARR and OwARR-SDS.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "4], to \u03bb in the range of [1, 20], and to \u03b3 in the range of [0.", "startOffset": 25, "endOffset": 32}, {"referenceID": 18, "context": "4], to \u03bb in the range of [1, 20], and to \u03b3 in the range of [0.", "startOffset": 25, "endOffset": 32}, {"referenceID": 55, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 42, "endOffset": 46}, {"referenceID": 57, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 156, "endOffset": 160}, {"referenceID": 56, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 220, "endOffset": 224}, {"referenceID": 57, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 226, "endOffset": 230}, {"referenceID": 56, "context": "Meanwhile, we have also extended the SDS algorithm for classification in [60] to regression problems, and verified that OwARR-SDS can achieve similar performance to OwARR, but save about half of the computation time.", "startOffset": 73, "endOffset": 77}, {"referenceID": 48, "context": "This indicates that they still have room for improvement: we could develop a mechanism to switch between BL1 and OwARR (OwARRSDS) so that a more appropriate method is chosen according to the characteristics of the new subject, similar to the idea of selective TL [51].", "startOffset": 263, "endOffset": 267}, {"referenceID": 57, "context": "Second, we will extend OwARR and OwARRSDS to offline calibration, where the goal is to automatically label some initially unlabeled subject-specific samples with a small number of queries [61].", "startOffset": 188, "endOffset": 192}, {"referenceID": 54, "context": "In online calibration it is not easy to estimate the testing RMSEs because we do not know what samples will be encountered in the future; however, in offline calibration we can better estimate the testing performances of the base learners using a spectral meta-learner approach [58], and hence a better model fusion strategy could be developed.", "startOffset": 278, "endOffset": 282}, {"referenceID": 27, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 51, "endOffset": 55}, {"referenceID": 51, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 57, "endOffset": 61}, {"referenceID": 55, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 146, "endOffset": 150}, {"referenceID": 53, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 152, "endOffset": 156}, {"referenceID": 60, "context": "speech signals [64] in affective computing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 63, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 40, "endOffset": 44}, {"referenceID": 46, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 130, "endOffset": 134}, {"referenceID": 61, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 58, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 160, "endOffset": 164}, {"referenceID": 62, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 43, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 201, "endOffset": 205}, {"referenceID": 28, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 223, "endOffset": 227}, {"referenceID": 29, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 229, "endOffset": 233}, {"referenceID": 35, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 235, "endOffset": 239}, {"referenceID": 0, "context": "A FS X is comprised of a universe of discourse DX of real numbers together with a membership function (MF) \u03bc X : DX \u2192 [0, 1], i.", "startOffset": 118, "endOffset": 124}], "year": 2017, "abstractText": "One big challenge that hinders the transition of brain-computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation from EEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS.", "creator": "LaTeX with hyperref package"}}}