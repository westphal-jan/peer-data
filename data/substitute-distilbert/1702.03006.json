{"id": "1702.03006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Multi-step Off-policy Learning Without Importance Sampling Ratios", "abstract": "cannot estimate the value functions different policies from exploratory data, most model - free off - partisan algorithms rely on importance sampling, where the use of importance sampling ratios often leads between estimates with larger variance. advantage is thus desirable to learn off - policy without using smoothing ratios. however, such an inequality does not make for multi - step learning with function approximation. in this paper, we introduce the first such algorithm based on temporal - difference ( td ) learning updates. we show as an explicit use of importance sampling ratios can be eliminated by varying in amount of squared in td updates in an action - dependent manner. our new algorithm achieves stability wearing a two - timescale gradient - based td update. a prior output based on lookup likelihood representation called constant backup can also be retrieved using action - dependent convergence, becoming a standard case of our algorithm. in two mutually off - policy tasks, we demonstrate because our algorithm is stable, effectively avoids the variable variance issue, and can perform substantially better than its state - of - the - art counterpart.", "histories": [["v1", "Thu, 9 Feb 2017 22:36:25 GMT  (436kb,D)", "http://arxiv.org/abs/1702.03006v1", "24 pages, 4 figures"]], "COMMENTS": "24 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ashique rupam mahmood", "huizhen yu", "richard s sutton"], "accepted": false, "id": "1702.03006"}, "pdf": {"name": "1702.03006.pdf", "metadata": {"source": "CRF", "title": "Multi-step Off-policy Learning Without Importance Sampling Ratios", "authors": ["A. Rupam Mahmood", "Huizhen Yu", "Richard S. Sutton"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Off-policy learning constitutes an important class of reinforcement learning problems, where the goal is to learn about a designated target policy while behaving according to a different policy. In recent years, off-policy learning has garnered a substantial amount of attention in policy evaluation tasks. When safety and data frugality is paramount, the ability to evaluate a policy without actually following it can be invaluable (cf. Thomas 2015). Learning a large number of off-policy predictions is also considered important for model learning, options learning (Sutton et al. 1999), scalable life-long learning (White, Modayil & Sutton 2012), and knowledge representation (White 2015).\nA number of computationally scalable algorithms have been proposed that can learn offpolicy without requiring to access or estimate the model of the environment and in that sense are model-free (Precup et al. 2000, 2001, Maei & Sutton 2010, Maei 2011, Yu 2012, Geist & Scherrer 2014, Dann et al. 2014, van Hasselt et al. 2014, Mahmood et al. 2014, Yu 2015, 2016, Hallak et al. 2015a, Mahmood et al. 2015, Sutton et al. 2016, White & White 2016a). A core component of these algorithms is a classical Monte Carlo technique called importance sampling (Hammersley & Handscomb 1964, Rubinstein 1981), where samples are scaled by the likelihood ratio of the two policies, also known as the importance sampling ratio, so that they appear to be drawn from the target policy. Although importance sampling plays\nar X\niv :1\n70 2.\n03 00\n6v 1\n[ cs\n.L G\n] 9\nF eb\na key role in correcting the discrepancy between the policies, the highly varying nature of importance sampling ratios often results in large estimation variance (Liu 2001, Koller & Friedman 2009, Dann et al. 2014, Mahmood & Sutton 2015, White & White 2016a). Some off-policy algorithms avoid importance sampling using a model of the environment or combine model-based estimation with importance sampling based estimation. Due to performing model estimation in addition to value function estimation, these algorithms tend to be computationally more complex (Dud\u0301\u0131k et al. 2011, Paduraru 2013, Hallak et al. 2015b, Li et al. 2015, Jiang & Li 2015, Thomas & Brunskill 2016).\nMulti-step learning is one of the most important components of modern temporaldifference learning algorithms. Through multi-step learning, temporal-difference learning algorithms can vary smoothly across a large spectrum of algorithms, including both onestep and Monte Carlo methods. The influence of multi-step learning is the greatest when parametric function approximation is used. In this case, solutions produced by multi-step learning can be much superior to those produced by one-step learning (Tsitsiklis & Van Roy 1997). Unfortunately, the problem of large variance with importance sampling is also the most severe in multi-step learning (White 2015, Mahmood & Sutton 2015). Consequently, multi-step off-policy learning remains problematic and largely unfulfilled.\nAn obvious approach to solve the problem of multi-step off-policy learning would then be to develop an algorithm that avoids using importance sampling ratios. The absence of these ratios will presumably reduce the estimation variance, making long-term multistep learning tenable. Only a few model-free algorithms have been proposed to learn offpolicy without using importance sampling ratios (Precup et al. 2000, van Hasselt 2011, Harutyunyan et al. 2016). However, all these algorithms were introduced either for one-step learning (van Hasselt 2011) or for learning with lookup table representation (Precup et al. 2000, Harutyunyan et al. 2016), that is, without using parametric function approximation. Multi-step learning does not have a lasting influence on performance in this case.\nOur key contribution is to develop an algorithmic technique based on modulating the amount to which the estimates of the subsequent states are used, a concept known as bootstrapping, in an action-dependent manner. It results in an action-dependent bootstrapping parameter, which is a generalization of the state-dependent bootstrapping parameter used in prior works (Maei & Sutton 2010, Sutton et al. 2014). For action-value estimation, we show that importance sampling ratios can be eliminated by varying the action-dependent bootstrapping parameter for different state-action pairs in a particular way. Using this technique, we introduce a new algorithm called ABQ that can achieve much less estimation variance compared to the state-of-the-art off-policy algorithm. ABQ is the first to effectively achieve multi-step function approximation solutions for off-policy learning without explicitly using importance sampling ratios. A prior algorithm, Tree Backup (Precup et al. 2000), can be retrieved as a special case of our algorithm. Furthermore, we show that another off-policy algorithm, Retrace (Munos et al. 2016), can also be derived and extended to the case of function approximation with stability using the action-dependent bootstrapping technique."}, {"heading": "2. Problem formulation and notations", "text": "In this section, we formulate the problem of multi-step off-policy learning with parametric function approximation and establish notations. Consider an agent in a dynamical environment with a finite state space S and action space A. At each time t = 0, 1, . . ., if the present state is s \u2208 S and the agent takes action a \u2208 A, then the next state St+1 is s\u2032 \u2208 S with probability p(s\u2032|s, a), and the agent receives a random reward Rt+1 with mean r(s, a) and finite variance upon the state transition. A randomized stationary policy \u03c0 specifies the probability \u03c0(a|s) of taking action a at state s. Of our interest is a given policy \u03c0, referred to as the target policy, and the performance of the agent if it follows \u03c0. Specifically, our interest in this paper is to estimate the action-value function of \u03c0, defined as the expected sum of discounted rewards for any initial state-action pair (s, a):\nq\u03c0(s, a) def == E\u03c0 [ \u221e\u2211 t=1 \u03b3t\u22121Rt \u2223\u2223\u2223S0 = s,A0 = a] , (1) where \u03b3 < 1 is a discount factor, and E\u03c0 [\u00b7] signifies that all subsequent actions follow policy \u03c0.\nIn off-policy learning, the goal is to estimate q\u03c0 from the observations {(St, At, Rt+1)}t\u22650 obtained by the agent while it follows a (stationary randomized) policy \u00b5 6= \u03c0. We call \u00b5 the behavior policy. (The case \u00b5 = \u03c0 is the on-policy case.) We shall assume that \u00b5 induces an irreducible Markov chain on the state-action space, whose unique invariant distribution we denote by d\u00b5.\nImportant to multi-step learning are multi-step Bellman equations satisfied by the action-value function q\u03c0. We review here such equations for the well-known TD(\u03bb), where \u03bb \u2208 [0, 1] is the bootstrapping parameter. Let P\u03c0 be the transition probability matrix of the Markov chain on S\u00d7A induced by the target policy \u03c0, and let r \u2208 R|S|\u00b7|A| be the vector of expected rewards for different state-action pairs: [r]sa def == r(s, a).1 For \u03bb \u2208 [0, 1], define the multi-step Bellman operator T (\u03bb) \u03c0 by\nT (\u03bb)\u03c0 q def == (I\u2212 \u03b3\u03bbP\u03c0)\u22121[r + \u03b3(1\u2212 \u03bb)P\u03c0q]\nfor all q\u2208R|S|\u00b7|A|, where I is the identity matrix. Then q\u03c0 satisfies the multi-step Bellman equation q\u03c0 = T (\u03bb) \u03c0 q\u03c0, where q\u03c0 stands for the action-value function in vector notation.\nWe approximate the action-value function as a linear function of some given features of state-action pairs: q\u03c0(s, a) \u2248 w>x(s, a), where w \u2208 Rn is the parameter vector to be estimated and x(s, a) \u2208 Rn is the feature vector for state s and action a. In matrix notation we write this approximation as q\u03c0 \u2248 Xw, where X \u2208 R|S|\u00b7|A|\u00d7n is the feature matrix with the rows being the feature vectors for different state-action pairs: [X]sa,: = x(s, a) >.\nThe multi-step solution to the off-policy learning problem with function approximation can be found by solving the fixed point equation: Xw = \u03a0\u00b5T (\u03bb) \u03c0 Xw (when it has a solution), where \u03a0\u00b5 def == X(X>D\u00b5X) \u22121X>D\u00b5 is the projection matrix with D\u00b5 \u2208 R|S|\u00b7|A|\u00d7|S|\u00b7|A| being\n1. We use brackets with subscripts to denote elements of vectors and matrices. We use small letters for vectors and capital letters for matrices, both boldfaced.\na diagonal matrix with diagonal elements d\u00b5(s, a). The Mean Squared Projected Bellman Error (MSPBE) corresponding to this equation is given by:\nJ(w) = \u2225\u2225\u2225\u03a0\u00b5T (\u03bb)\u03c0 Xw \u2212Xw\u2225\u2225\u22252\nD\u00b5 , (2)\nwhere \u2016q\u20162D\u00b5 = q >D\u00b5q, for any q \u2208 R|S|\u00b7|A|. The multi-step asymptotic TD solution associated with the fixed-point equation and the above MSPBE can be expressed as w\u221e def == A\u22121b, when A is an invertible matrix, and A and b are given by\nA def == X>D\u00b5 (I\u2212 \u03b3\u03bbP\u03c0)\u22121 (I\u2212 \u03b3P\u03c0) X, (3)\nb def == X>D\u00b5 (I\u2212 \u03b3\u03bbP\u03c0)\u22121 r. (4)"}, {"heading": "3. The advantage of multi-step learning", "text": "Under the rubric of temporal-difference learning fall a broad spectrum of methods. On one end of the spectrum, we have one-step methods that fully bootstrap using estimates of the next state and use only the immediate rewards as samples. On the other end of the spectrum, we have Monte Carlo methods that do not bootstrap and rather use all future rewards for making updates. Many multi-step learning algorithms incorporate this full spectrum and can vary smoothly between one-step and Monte Carlo updates using the bootstrapping parameter \u03bb. Here 1\u2212\u03bb determines the degree to which bootstrapping is used in the algorithm. With \u03bb = 0, these algorithms achieve one-step TD updates, whereas with \u03bb = 1, they effectively achieve Monte Carlo updates. To contrast with one-step learning, multi-step learning is generally viewed as learning with \u03bb > 0 in TD methods.\nMulti-step learning impacts the efficiency of estimation in two ways. First, it allows more efficient estimation compared to one-step learning with a finite amount of samples. Onestep learning uses the minimal amount of samples, have relatively less variance compared to Monte Carlo updates, but produces biased estimates. Typically, with a finite amount of samples, a value of \u03bb between 0 and 1 reduces the estimation error the most (Sutton & Barto 1998).\nSecond, when function approximation is used and q\u03c0 does not lie in the approximation subspace, multi-step learning can produce superior asymptotic solutions compared to onestep learning. As \u03bb increases, the multi-step Bellman operator approaches the constant operator that maps every q to q\u03c0. This in general leads to better approximations, as suggested by the monotonically improving error bound of asymptotic solutions (Tsitsiklis & Van Roy 1997) in the on-policy case, and as we demonstrate for the off-policy case in Figure 1.\nAlthough multi-step learning is desirable with function approximation, it is more difficult in the off-policy case where the detrimental effect of importance sampling is most pronounced. For this reason, off-policy learning without importance sampling ratios is a naturally appealing and desirable solution to this problem. Prior works on off-policy learning without the ratios (e.g., Precup et al. 2000, Harutyunyan et al. 2016) are given in the lookup table case where the benefit of multi-step learning does not show up, because regardless of \u03bb, the asymptotic solution is q\u03c0. It is in the case of function approximation that multi-step off-policy learning without importance sampling ratios is most needed."}, {"heading": "4. Multi-step off-policy learning with importance sampling ratios", "text": "To set the stage for our work, we describe in this section the canonical multi-step off-policy learning update with importance sampling ratios, and how the ratios introduce variance in off-policy temporal-difference (TD) updates. A TD update is generally constructed based on stochastic approximation methods, where the target of the update is based on returns. Here we consider the off-line update for off-policy TD learning. Although not practical for implementation, off-line updates are useful and develop the foundation for deriving practical and computationally efficient algorithms (Sutton & Barto 1998, Seijen et al. 2016). An offline TD update for off-policy action-value estimation based on multi-step returns can be defined as:\n\u2206wt = \u03b1t ( G\u03bbt \u2212w>xt ) xt, (5)\nwhere \u03b1 > 0 is the step-size parameter, and w is a fixed weight vector. Here, G\u03bbt is the multi-step target, known as \u03bb-return, defined as the sum of TD errors weighted by powers of \u03b3\u03bb and products of importance sampling ratios:\nG\u03bbt def == \u221e\u2211 n=t (\u03b3\u03bb)n\u2212t\u03c1nt+1\u03b4n + w >xt. (6)\nThe TD error \u03b4t is defined as \u03b4t def == Rt+1+\u03b3w >x\u0304t+1\u2212w>xt, with x\u0304t def == \u2211 a \u03c0(a|St)x(St, a). The term \u03c1nt def == \u03a0ni=t\u03c1i is a product of importance sampling ratios \u03c1t def == \u03c0(At|St)\u00b5(At|St) , which\naccounts for the discrepancy due to using the behavior policy instead of the target policy throughout the trajectory. Note that, the update defined by (5) is a forward-view update, that is, it uses samples that only become available in the future from the time the state of the updated estimate is visited. We call this update the off-policy Q(\u03bb) update. It can be shown that the asymptotic multi-step solution corresponding to off-policy Q(\u03bb) is given by w\u221e = A\n\u22121b, (3), and (4), when A is invertible. All existing multi-step off-policy algorithms with importance sampling are of this form or a variant.\nWhen \u03bb = 0, no importance sampling ratios are involved, and this update reduces to that of off-policy expected Sarsa (Sutton & Barto 1998, van Hasselt 2011, Sutton et al. 2014). This one-step update is also closely related to the one-step Q-learning update, where a greedy nonstationary target policy is used instead of a fixed stationary one.\nThe importance sampling ratios play a role when \u03bb > 0, and their influence, including the detrimental impact on variance, is greater with larger \u03bb. The product \u03c1n1 of off-policy Q(\u03bb) in (6) can become as large as 1\n(mins,a \u00b5(a|s))n . Such an exponential growth, when occurred\neven momentarily, can have large impact on the variance of the estimate. If the value of \u03bb is small or very close to zero, the large variance ensuing from the product may be avoided, but it would also be devoid of much of the benefits of multi-step learning."}, {"heading": "5. Avoiding importance sampling ratios", "text": "In this section, we introduce the idea of action-dependent bootstrapping and how it can be used to avoid importance sampling ratios in off-policy estimates. For that, first we introduce an action-dependent bootstrapping parameter \u03bb(s, a) \u2208 [0, 1], which is allowed to vary between different state-action pairs. A closely related idea is state-dependent bootstrapping used by Sutton and Singh (1994) and Sutton et al. (2014) for state-value estimation, and by Maei and Sutton (2010) for action-value estimation. In those works, the degree of bootstrapping was allowed to vary from one state to another by the state-dependent bootstrapping parameter \u03bb(s) \u2208 [0, 1] but was not used as a device to reduce the estimation variance.\nThe variability of the parameter \u03bb(s, a) can be utilized algorithmically on a momentby-moment basis to absorb the detrimental effect of importance sampling and in general to control the impact of importance sampling. Let us use the notational shorthand \u03bbt def == \u03bb(St, At), and define a new \u03bb-return by replacing the constant \u03bb in (6) with variable \u03bb(s, a):\nG\u03bbt = \u221e\u2211 n=t \u03b3n\u2212t\u03bbnt+1\u03c1 n t+1\u03b4n + w >xt, (7)\nwhere \u03bbnt def == \u03a0ni=t\u03bbi. Notice that each importance sampling ratio in (7) is factored with a corresponding bootstrapping parameter: \u03bbt\u03c1t. We can mitigate an explicit use of importance sampling ratios by setting the action-dependent bootstrapping parameter \u03bb(s, a) in the following way:\n\u03bb(s, a) = \u03bd(\u03c8, s, a)\u00b5(a|s), \u03bd(\u03c8, s, a) def== min ( \u03c8,\n1\nmax (\u00b5(a|s), \u03c0(a|s))\n) (8)\nwhere \u03c8 \u2265 0 is a constant. Note that \u03bd(\u03c8, s, a) is upper-bounded by \u03c8max, which is defined as follows:\n\u03c8max def ==\n1\nmins,a max (\u00b5(a|s), \u03c0(a|s)) . (9)\nThe product \u03bbt\u03c1t can then be rewritten as: \u03bbt\u03c1t = \u03bd(\u03c8, St, At)\u00b5t \u03c0t \u00b5t\n= \u03bd(\u03c8, St, At)\u03c0t, dispelling an explicit presence of importance sampling ratios from the update. It is easy to see that, under our proposed scheme, the effective bootstrapping parameter is upper bounded by 1: \u03bbt \u2264 1, and at the same time all the products are also upper bounded by one: \u03bbnt \u03c1\nn t \u2264 1, largely reducing variance. To understand how \u03c8 influences \u03bb(s, a) let us use the following example, where there are only one state and three actions {1, 2, 3} available. The behavior policy probabilities are [0.2, 0.3, 0.5] and the target policy probabilities are [0.2, 0.4, 0.4] for the three actions, respectively. Figure 2 (left) shows how the action-dependent bootstrapping parameter \u03bb for different actions change as \u03c8 is increased from 0 to \u03c8max. Initially, the bootstrapping parameter \u03bb increased linearly for all actions at a different rate depending on their corresponding behavior policy probabilities. The min in the factor \u03bd comes into effect with \u03c8 > \u03c80 def == 1maxs,amax(\u00b5(a|s),\u03c0(a|s)) , and the largest \u03bb at \u03c8 = \u03c80 gets capped first. Eventually, with large enough \u03c8, that is, \u03c8 \u2265 \u03c8max, all \u03bbs get capped. Algorithms with constant \u03bb are typically studied in terms of their parameters by varying \u03bb between [0, 1], which would not be possible for an algorithm based on the above scheme as \u03bb is not a constant any more. For our scheme, the constant tunable parameter is \u03c8, which has three pivotal values: [0, \u03c80, \u03c8max]. For parameter studies, it would be convenient if \u03c8 is scaled to another tunable parameter between [0, 1]. But in that case, we have to make a decision on what value \u03c80 should transform to. In the absence of a clear sense of it, a default choice would be to transform \u03c80 to 0.5. One such scaling is where we set \u03c8 as a function of another constant \u03b6 \u2265 0 in the following way:\n\u03c8(\u03b6) = 2\u03b6\u03c80 + (2\u03b6 \u2212 1)+ \u00d7 (\u03c8max \u2212 2\u03c80) , (10)\nand then vary \u03b6 between [0, 1] as one would vary \u03bb. Here (x)+ = max (0, x). In this case, \u03b6 = 0, 0.5, and 1 correspond to \u03c8(\u03b6) = 0, \u03c80, and \u03c8max, respectively. Note that \u03b6 can be written conversely in terms of \u03c8 as follows:\n\u03b6 = (\u03c8 \u2212 \u03c80)+ \u00b7 2\u03c80 \u2212 \u03c8max\n2 (\u03c8max \u2212 \u03c80)\u03c80 +\n\u03c8\n2\u03c80 . (11)\nThe top margin of Figure 2 (left) shows an alternate x-axis in terms of \u03b6. To form an update using this proposed modification to \u03bb, let us use the following notational shorthands,\n\u03bd\u03b6(s, a) def == \u03bd(\u03c8(\u03b6), s, a), \u03bd\u03b6,t def == \u03bd\u03b6(St, At), (12)\n\u03bb\u03b6(s, a) def == \u03bd(\u03c8(\u03b6), s, a)\u00b5(a|s), \u03bb\u03b6,t def == \u03bb\u03b6(St, At). (13)\nWe use H\u03b6t to denote the \u03bb-return defined by (7) with the bootstrapping parameter set according to \u03bb\u03b6 :\nH\u03b6t = \u221e\u2211 n=t \u03b3n\u2212t\u03bbn\u03b6,t+1\u03c1 n t+1\u03b4n + w >xt = \u221e\u2211 n=t \u03b3n\u2212t\u03bdn\u03b6,t+1\u03c0 n t+1\u03b4n + w >xt, (14)\nRight: The effect of \u03bb on GQ(\u03bb) solutions and \u03b6 on ABQ(\u03b6) solutions. Multi-step (\u03bb > 0) off-policy Q(\u03bb) solutions are superior to the one-step (\u03bb = 0) solution. ABQ(\u03b6) solutions can also achieve a similar multi-step advantage with \u03b6 > 0.\nwhere \u03bbn\u03b6,t def == \u03a0ni=t\u03bb\u03b6,i, \u03bd n \u03b6,t def == \u03a0ni=t\u03bd\u03b6,i, and \u03c0 n t def == \u03a0ni=t\u03c0i. The off-line forward-view update with H\u03b6t as the target can be written as:\n\u2206wt = \u03b1t ( H\u03b6t \u2212w>xt ) xt. (15)\nThe asymptotic solution corresponding to this update, which we call the ABQ(\u03b6) solution, is w\u03b6\u221e def == A\u03b6 \u22121b\u03b6 with\nA\u03b6 def == X>D\u00b5 (I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 (I\u2212 \u03b3P\u03c0) X, (16)\nb\u03b6 def == X>D\u00b5 (I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 r, (17)\nassuming A\u03b6 is invertible. The derivation is given in Appendix A.1. Here \u039b\u03b6 is a diagonal matrix with \u03bb\u03b6(s, a) being the diagonal elements. This is a multi-step solution when the bootstrapping parameter \u03bb\u03b6(s, a) does not uniformly reduce to zero. The drawback of this scheme is that we cannot achieve \u03bb\u03b6(s, a) = 1 for all state-action pairs, which would produce the off-policy Monte Carlo solution. It is the cost of avoiding importance sampling ratios together with its large variance issue. However, much of the multi-step benefits can still be retained by choosing a large \u03b6.\nTo illustrate that ABQ(\u03b6) solutions can retain much of the multi-step benefits, we used a two-state off-policy task similar to the off-policy task by Sutton et al. (2016). In this task, there were two states each with two actions, left and right, leading to one of the two states deterministically. More specifically, p(1|1, left) = p(2|1, right) = p(1|2, left) = p(2|2, right) = 1. There is a deterministic nonzero reward r(2, right) = +1; all other transitions have reward zero. The discount factor \u03b3 was 0.9. The feature vectors were set as x(1, left) = x(1, right) =\n1 and x(2, left) = x(2, right) = 2. The behavior policy was chosen as \u00b5(right|1) = \u00b5(left|2) = 0.9 to break away from the uniform distribution of the original problem. The target policy was chosen as \u03c0(right|\u00b7) = 0.9.\nWe produced different asymptotic solutions defined by (16) and (17), choosing different constant \u03b6 between [0, 1]. We compared ABQ(\u03b6) solutions with off-policy Q(\u03bb) solutions in terms of the Mean Squared Error (MSE) \u2016Xw \u2212 q\u03c0\u20162D\u00b5 normalized by \u2016q\u03c0\u2016 2 D\u00b5 . The results are given in Figure 2 (right). Off-policy Q(\u03bb) solutions with \u03bb > 0 in this task are substantially better than the one-step solution produced with \u03bb = 0. The ABQ(\u03b6) solutions cannot be as good as the off-policy Q(1) solution, as we already anticipated, but much of the benefits of multi-step off-policy solutions can be attained by choosing a large value of \u03b6. Although the tunable parameter \u03b6 of ABQ was set to be a constant, the effective bootstrapping parameter \u03bb\u03b6(s, a) was different for different state-action pairs. Therefore, ABQ solutions cannot be obtained simply by rescaling the constant \u03bb of off-policy Q(\u03bb).\nThe algorithm in principle can be used with either \u03b6 or \u03c8 as the tunable parameter. If we tune with \u03b6 \u2208 [0, 1], we will have to first use (10) to obtain \u03c8(\u03b6), and then (12) and (13) to obtain \u03bbt on a moment-by-moment basis. Computing \u03c8(\u03b6) from \u03b6 requires knowing \u03c80 and \u03c8max which are often known, for example, when the policies are in -greedy form or fixed and known beforehand. In other cases, tuning with \u03c8 directly can be more convenient. As the scaled parameter \u03b6 reflects more clearly the qualitative behavior of ABQ, we use it as the tunable parameter in this work."}, {"heading": "6. The ABQ(\u03b6) algorithm with gradient correction and scalable updates", "text": "In this section, we develop a computationally scalable and stable algorithm corresponding to the ABQ(\u03b6) solution. The update (15) given earlier cannot be computed in a scalable manner, because forward-view updates require an increasing amount of computation as time progresses. Moreover, off-policy algorithms with bootstrapping and function approximation may be unstable (Sutton & Barto 1998), unless machinery for ensuring stability is incorporated. Our goal is to develop a stable update corresponding to ABQ(\u03b6) solutions while keeping it free from an explicit use of importance sampling ratios.\nFirst, we produce the equivalent backward view of (15) so that the updates can be implemented without forming explicitly the \u03bb-return. As we derive in Appendix A.2, these updates are given by\n\u2206wt = \u03b1t\u03b4tet, et = \u03b3\u03bd\u03b6,t\u03c0tet\u22121 + xt. (18)\nHere, et \u2208 Rn is an accumulating trace vector. The above backward-view update achieves equivalence with the forward-view of (15) only for off-line updating, which is typical for all algorithms with accumulating traces. Equivalence for online updating could also be achieved by following the approach taken by van Seijen et al. (2014, 2016), but we leave that out for simplicity.\nWe take the approach proposed by Maei (2011) to develop a stable gradient-based TD algorithm. The key step in deriving a gradient-based TD algorithm is to formulate an associated Mean Squared Projected Bellman Error (MSPBE) and produce its gradient, which can then be sampled to produce stochastic updates.\nThe MSPBE for the update (15) is given by: J(w) = \u2225\u2225\u2225\u03a0\u00b5T (\u039b\u03b6)\u03c0 Xw \u2212Xw\u2225\u2225\u22252\nD\u00b5 (19) = ( X>D\u00b5 ( T (\u039b\u03b6) \u03c0 Xw \u2212Xw ))> (X>D\u00b5X) \u22121X>D\u00b5 ( T (\u039b\u03b6) \u03c0 Xw \u2212Xw ) . (20)\nHere, the Bellman operator corresponding to the bootstrapping matrix \u039b\u03b6 is defined for all q\u2208R|S|\u00b7|A| as\nT (\u039b\u03b6) \u03c0 q def == (I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 [r + \u03b3P\u03c0(I\u2212\u039b\u03b6)q] .\nThen the gradient can be written as:\n\u2207J(w) = \u22121 2\n( X>D\u00b5(I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 (I\u2212 \u03b3P\u03c0) X )> C\u22121g (21)\n= \u22121 2\n( g \u2212 \u03b3H>C\u22121g ) , (22)\nwhere g, C and H are defined in the following way:\ng def == X>D\u00b5 ( T (\u039b\u03b6) \u03c0 Xw \u2212Xw ) , (23)\nC def == (X>D\u00b5X), (24)\nH def == ( X>D\u00b5 (I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 P\u03c0(I\u2212\u039b\u03b6)X ) . (25)\nWhen this gradient is known, the gradient-descent step would be to add the following to the parameter vector:\n\u22121 2 \u03b1t\u2207J(w) = \u03b1t\n( g \u2212 \u03b3H>C\u22121g ) . (26)\nIn model-free learning, the gradient is not available to the learner. However, we can form a stochastic gradient descent update by sampling from this gradient. For that, we can express g and H in an expectation form, and estimate the vector C\u22121g at a faster time scale.\nIn Appendix A.3, we derive the resulting stochastic gradient corrected algorithm in details, which we call the ABQ(\u03b6) algorithm. This algorithm is defined by the following online updates:\n\u03b4t def == Rt+1 + \u03b3w > t x\u0304t+1 \u2212w>t xt, (27)\nx\u0303t+1 def == \u2211 a \u03bd\u03b6(St+1, a)\u03c0(a|St+1)x(St+1, a), (28)\net def == \u03b3\u03bd\u03b6,t\u03c0tet\u22121 + xt, (29)\nwt+1 def ==wt+ \u03b1t ( \u03b4tet\u2212\u03b3e>t ht (x\u0304t+1 \u2212 x\u0303t+1) ) , (30)\nht+1 def == ht + \u03b2t ( \u03b4tet \u2212 ( h>t xt ) xt ) . (31)\nThe iteration (30) carries out stochastic gradient-descent steps to minimize the MSPBE (19), and it differs from (18) as it includes a gradient-correction term \u2212\u03b3e>t ht (x\u0304t+1 \u2212 x\u0303t+1). This\ncorrection term involves an extra vector parameter ht. Note that no importance sampling ratios are needed in the update of ht or in the gradient-correction term. The vector ht is updated according to (31) at a faster timescale than wt by using a second step-size parameter \u03b2t \u03b1t when both step sizes are diminishing. One can achieve that by letting \u03b1t = O(1/t), \u03b2t = O(1/t\nc), c \u2208 (1/2, 1), for instance. In practice, when stability is not a problem, smaller values of \u03b2 often lead to better performance (White & White 2016a)."}, {"heading": "7. Experimental results", "text": "We empirically evaluate ABQ(\u03b6) on three policy evaluation tasks: the two-state off-policy task from Section 5, an off-policy policy evaluation adaptation of the Mountain Car domain (Sutton & Barto 1998), and the 7-star Baird\u2019s counterexample (Baird 1995, White 2015). In the first two tasks we investigated whether ABQ(\u03b6) can produce correct estimates with less variance compared to GQ(\u03bb) (Maei 2011), the state-of-the-art importance sampling based algorithm for action-value estimation with function approximation. We validate the stability of ABQ(\u03b6) in the final task, where off-policy algorithms without a stability guarantee (e.g., off-policy Q(\u03bb)) tend to diverge.\nAlthough the MDP involved in the two-state task is small, off-policy algorithms may suffer severely in this task as the importance sampling ratio, once in a while, can be as large as 9. We simulated both GQ(\u03bb) and ABQ(\u03bb) on this task for 10000 time steps, starting with w0 = 0. We averaged the MSE \u2016Xwt \u2212 q\u03c0\u20162D\u00b5 for the last 5000 time steps. We further averaged this quantity over 100 independent runs. Finally, we divided this error by \u2016q\u03c0\u20162D\u00b5 to obtain the normalized MSE (NMSE).\nFigure 3 (left) shows curves for different combinations of the step-size parameters: \u03b1 \u2208 [0.001, 0.005, 0.01] and \u03b2 \u2208 [0.001, 0.005, 0.01]. Performance is shown in the estimated NMSE, with the corresponding standard error for different values of \u03bb and \u03b6. With \u03bb > 0.6, the error of GQ(\u03bb) increased sharply due to increased influence of importance sampling ratios. It clearly depicts the failure to perform effective multi-step learning by an importance sampling based algorithm when \u03bb is large. The error of ABQ(\u03b6) decreased substantially for most step-size combinations as \u03b6 is increased from 0 to 0.5, and it decreased slightly for some step-size combinations as \u03b6 was increased up to 1. This example clearly shows that ABQ(\u03b6) can perform effective multi-step learning while avoiding importance sampling ratios. On the other hand, the best performance of GQ(\u03bb) was better than ABQ(\u03b6) for the smallest value of the first step size (i.e., \u03b1 = 0.001). When the step size was too small, GQ(\u03bb) in fact benefited from the occasional large scaling from importance sampling, whereas ABQ(\u03b6) remained conservative in its updates to be safe.\nThe Mountain Car domain is typically used for policy improvement tasks, but here we use it for off-policy policy evaluation. We constructed the task in such a way that the importance sampling ratios for GQ can be as large as 30, emphasizing the variance issue regarding ratios. In this task, the car starts in the vicinity of the bottom of the valley with a small nonzero speed. The three actions are: reverse throttle, no throttle, and forward throttle. Rewards are -1 at every time step, and state transitions are deterministic. The discount factor \u03b3 is 0.999. The policies used in the experiments were based on a simple handcrafted policy, which chooses to move forward with full throttle if the velocity of the\nABQ(0.8)\nABQ(0.4)\nfor GQ( ) and \u21e3 for ABQ(\u21e3)\nNorm. Mean\nSquared Error\nGQ(0.4)\nABQ( )\u21e3\nNorm. Mean\nSquared Error\nGQ( )\nTwo-state off-policy task\nGQ(0)/ ABQ(0)\nNumber of episodes\nGQ(0.8)\nMountain Car off-policy task\ncar was nonzero and toward the goal, and chooses to move away from the goal with full throttle otherwise.\nBoth the target policy and the behavior policy are based on this policy but choose to randomly explore the other actions differently. More specifically, when the velocity was nonzero and toward the goal, the behavior policy probabilities for the actions reverse throttle, no throttle, and forward throttle are [ 1\n300 , 1 300 , 298 300\n] , and the target policy probabilities\nare [0.1, 0.1, 0.8], respectively. In the other case, the behavior and the target policy probabilities are [ 298 300 , 1 300 , 1 300 ] and [0.8, 0.1, 0.1], respectively. We set the policies this way so that episodes complete under both policies in a reasonable number of time steps, while the importance sampling ratio may occasionally be as large as 0.1\u00d7 300 = 30.\nThe feature vector for each state-action pair contained 32 features for each action, where only the features corresponding to the given action were nonzero, produced using tile coding with ten 4\u00d74 tilings. Each algorithm ran on the same 100 independent sequences of samples, each consisting 10,000 episodes. The performance was measured in terms of\nthe Mean-Squared-Error (MSE) with respect to the estimated value q\u0302 \u2208 R30 of 30 chosen state-action pairs. These pairs were chosen by running the agent under the behavior policy for 1 million time steps, restarting episodes each time termination occurs, and choosing 30 pairs uniformly randomly from the last half million time steps. The ground truth values for these 30 pairs q\u0302 were estimated by following the target policy 100 times from those pairs and forming the average. The mean-squared error from q\u0302 was normalized by \u2016q\u0302\u201622.\nWe use the mountain car off-policy task to illustrate through learning curves how \u03bb and \u03b6 affect GQ(\u03bb) and ABQ(\u03b6), respectively. Figure 3 (right) shows the learning curves of ABQ(\u03b6) and GQ(\u03bb) with respect to mean squared errors for three difference values of \u03bb and \u03b6: 0, 0.4, and 0.8, and a particular step-size combination: \u03b1 = 0.1/( # of active features) and \u03b2 = 0.0. These learning curves are averages over 100 independent runs. The standard errors of ABQ\u2019s estimated MSE here are smaller than the width of the curves shown. ABQ achieved a significantly lower MSE with \u03b6 > 0 than with \u03b6 = 0. On the other hand, GQ performed unreliably with larger \u03bb. With \u03bb = 0.4, the learning curves are highly varying from each other due to occasionally having the largest ratio value 30, affecting the update at different time steps. Some learning curves even became unstable, causing the MSE to move away further and further with time, and affecting the average MSE, which explains the spikes. When \u03bb = 0.8 was chosen, all learning curves became unstable in few steps.\nIn the 7-star Baird\u2019s counterexample, adopted from White (2015), step sizes were set as \u03b1 = 0.05 and \u03b2 = 0.1, and the bootstrapping parameter \u03b6 was chosen evenly between 0 and 1. The Mean Squared Projected Bellman Error (MSPBE) was estimated by averaging over 50 runs. As shown in Figure 4, ABQ(\u03b6) performed stably with all values of \u03b6 used. This validates empirically the gradient correction in ABQ(\u03b6)."}, {"heading": "8. Action-dependent bootstrapping as a framework for off-policy", "text": "algorithms\nABQ(\u03b6) is a result of this new idea of varying the bootstrapping parameter in an actiondependent manner so that an explicit presence of importance sampling ratios are mitigated from the update. However, it is not the only action-dependent bootstrapping scheme one can devise. It is possible to bound the product \u03bbnt \u03c1 n t by using other action-dependent bootstrapping schemes. Different schemes not only allow us to derive new algorithms, they may also be used to understand some existing algorithms better. Here, we show how the Retrace algorithm by Munos et al. (2016) can be understood in terms of a particular way of setting the action-dependent bootstrapping parameter.\nRetrace is a tabular off-policy algorithm that approaches the variance issue by truncating the importance sampling ratio. We show that such truncations can be understood as varying the action-dependent bootstrapping parameter in a particular manner, first, by constructing a forward-view update with a different action-dependent bootstrapping scheme than ABQ\u2019s, second, by deriving the asymptotic solution corresponding to that forward-view update, and third, by showing that the equivalent backward-view update is the same as the Retrace algorithm. For generality, we take these steps in the linear function approximation case and incorporate gradient corrections for stability. The resulting algorithm, we call AB-Trace(\u03b6) is a stable generalization of Retrace.\nWe construct a new forward-view update similar to (15) with \u03bb\u03b6 = \u03bd\u03b6 (s, a)\u00b5(a|s), where \u03bd is redefined as \u03bd\u03b6 (s, a) def == \u03b6 min\n( 1\n\u03c0(a|s) , 1 \u00b5(a|s)\n) . Here, we treat 1/0 = \u221e and 0 \u00b7 \u221e = 0.\nThen we can directly use Appendix A.1 to derive its asymptotic solution:\nw\u03b6\u221e def == A\u22121\u03b6 b\u03b6 , (32)\nA\u03b6 def == X>D\u00b5 (I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 (I\u2212 \u03b3P\u03c0) X, (33)\nb\u03b6 def == X>D\u00b5 (I\u2212 \u03b3P\u03c0\u039b\u03b6)\u22121 r, (34)\nwhere the diagonal elements of \u039b\u03b6 are \u03bb\u03b6(s, a) = \u03bd\u03b6 (s, a)\u00b5(a|s) = \u03b6 min ( 1 \u03c0(a|s) , 1 \u00b5(a|s) ) \u00b5(a|s) for different state-action pairs. A stable forward-view update with gradient correction corresponding to the above asymptotic solution can be derived by directly using the results of Appendix A.3. The resulting algorithmn, AB-Trace(\u03b6), is given by the following updates:\n\u03b4t def == Rt+1 + \u03b3w > t x\u0304t+1 \u2212w>t xt, (35)\n\u03bd\u03b6 (s, a) def == \u03b6 min\n( 1\n\u03c0(a|s) ,\n1\n\u00b5(a|s)\n) , (36)\nx\u0303t+1 def == \u2211 a \u03bd\u03b6(St+1, a)\u03c0(a|St+1)x(St+1, a), (37)\net def == \u03b3\u03bd\u03b6,t\u03c0tet\u22121 + xt, (38)\nwt+1 def == wt + \u03b1t ( \u03b4tet \u2212 \u03b3e>t ht (x\u0304t+1 \u2212 x\u0303t+1) ) , (39)\nht+1 def == ht + \u03b2t ( \u03b4tet \u2212 h>t xtxt ) , (40)\nNote that, the factor \u03bd\u03b6,t\u03c0t in the eligibility trace vector update can be rewritten as:\n\u03bd\u03b6,t\u03c0t = \u03b6 min\n( 1\n\u03c0t ,\n1\n\u00b5t\n) \u03c0t (41)\n= \u03b6 min (1, \u03c1t) . (42)\nFrom here, it is easy to see that, if the feature representation is tabular and the gradient correction term is dropped, then the AB-Trace algorithm reduces to the Retrace algorithm.\nFinally, we remark that the action-dependent bootstrapping framework provides a principled way of developing stable and efficient off-policy algorithms as well as unifying the existing ones, where AB-Trace and its connection to Retrace is only one instance."}, {"heading": "9. Related works", "text": "A closely related algorithm is Tree Backup by Precup et al. (2000). This algorithm can also be produced as a special case of ABQ(\u03b6), if we remove gradient correction, consider the feature vectors always to be the standard basis, and \u03bd\u03b6 to be always set to a constant, instead of setting it in an action-dependent manner. In the on-policy case, the Tree Backup algorithm fails to achieve the TD(1) solution, whereas ABQ achieves it with \u03b6 = 1.\nOur work is not a trivial generalization of this prior work. The Tree Backup algorithm was developed using a different intuition based on backup diagrams and was introduced only for the lookup table case. Not only does ABQ(\u03b6) extend the Tree Backup algorithm, but the idea of action-dependent bootstrapping also played a crucial role in deriving the ABQ(\u03b6) algorithm with gradient correction in a principled way. Another related algorithm is Retrace, which we have already shown to be a special case of the AB-Trace algorithm. The main differences are that Retrace was introduced and analyzed in the case of tabular representation, and thus Retrace is neither stable nor shown to achieve multi-step solutions in the case of function approximation.\nYet another class of related algorithms are where the bootstrapping parameter is adapted based on past data, for example, the works by White and White (2016b) and Mann et al. (2016). Beside adapting on a state-action-pair basis, the main difference between those algorithms and the ones introduced here under the action-dependent bootstrapping framework is that our algorithms adapt the bootstrapping parameter using only the knowledge of policy probabilities for the current state-action pair whereas the algorithms in the other class involve a separate learning procedure for adapting \u03bb and hence are more complex."}, {"heading": "10. Discussion and conclusions", "text": "In this paper, we have introduced the first model-free off-policy algorithm ABQ(\u03b6) that can produce multi-step function approximation solutions without requiring to use importance sampling ratios. The key to this algorithm is allowing the amount of bootstrapping to vary in an action-dependent manner, instead of keeping them constant or varying only with states. Part of this action-dependent bootstrapping factor mitigates the importance sampling ratios while the rest of the factor is spent achieving multi-step bootstrapping. The resulting effect is that the large variance issue with importance sampling ratios is readily removed without giving up multi-step learning. This makes ABQ(\u03b6) more suitable\nfor practical use. Action-dependent bootstrapping provides an insightful and well-founded framework for deriving off-policy algorithms with reduced variance. The same idea may be applied to state-value estimation. The ratios cannot be eliminated completely in this case; nevertheless, reduction in the variance can be expected. According to an alternative explanation based on backup diagrams (Precup et al. 2000), ABQ updates may be seen as devoid of importance sampling ratios, while the action-dependent bootstrapping framework can now provide us a clearer mechanistic view of how updates without importance sampling ratios can perform off-policy learning.\nThe convergence of this algorithm can be analyzed similarly to the work by Karmakar and Bhatnagar (2015) for diminishing step sizes and Yu (2015) for constant step sizes, by using properties of the eligibility traces and stochastic approximation theory. Investigating the possibility to include true online updates (van Seijen et al. 2016) is also an interesting direction for future work."}], "references": [{"title": "Policy evaluation with temporal differences: a survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Research, 15 :809\u2013883.", "citeRegEx": "Dann et al\\.,? 2014", "shortCiteRegEx": "Dann et al\\.", "year": 2014}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "Proceedings of the 12th International Conference on Machine Learning, pp. 30\u201337.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Doubly robust policy evaluation and learning", "author": ["M. Dud\u0301\u0131k", "J. Langford", "L. Li"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Dud\u0301\u0131k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0301\u0131k et al\\.", "year": 2011}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "Journal of Machine Learning Research, 15 :289\u2013333.", "citeRegEx": "Geist and Scherrer,? 2014", "shortCiteRegEx": "Geist and Scherrer", "year": 2014}, {"title": "Generalized emphatic temporal difference learning: bias-variance analysis", "author": ["A. Hallak", "A. Tamar", "R. Munos", "S. Mannor"], "venue": "arXiv preprint arXiv:1509.05172.", "citeRegEx": "Hallak et al\\.,? 2015a", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Off-policy model-based learning under unknown factored dynamics", "author": ["A. Hallak", "F. Schnitzler", "T. Mann", "S. Mannor"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pp. 711\u2013719.", "citeRegEx": "Hallak et al\\.,? 2015b", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Monte Carlo methods, Methuen & co", "author": ["Hammersley", "D.C.J.M. Handscomb"], "venue": "Ltd., London, pp. 40,", "citeRegEx": "Hammersley and Handscomb,? 1964", "shortCiteRegEx": "Hammersley and Handscomb", "year": 1964}, {"title": "Q (\u03bb) with off-policy corrections", "author": ["A. Harutyunyan", "M.G. Bellemare", "T. Stepleton", "R. Munos"], "venue": "arXiv preprint arXiv:1602.04951.", "citeRegEx": "Harutyunyan et al\\.,? 2016", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "Two timescale stochastic approximation with controlled Markov noise and off-policy temporal difference learning.arXiv preprint arXiv:1503", "author": ["P. Karmakar", "S. Bhatnagar"], "venue": null, "citeRegEx": "Karmakar and Bhatnagar,? \\Q2015\\E", "shortCiteRegEx": "Karmakar and Bhatnagar", "year": 2015}, {"title": "Doubly robust off-policy evaluation for reinforcement learning", "author": ["N. Jiang", "L. Li"], "venue": "arXiv preprint arXiv:1511.03722.", "citeRegEx": "Jiang and Li,? 2015", "shortCiteRegEx": "Jiang and Li", "year": 2015}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press, 2009.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Toward minimax off-policy value estimation", "author": ["L. Li", "R. Munos", "C. Szepesvari"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pp. 608\u2013616.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Monte Carlo strategies in scientific computing", "author": ["J.S. Liu"], "venue": "Berlin, Springer-Verlag. 16", "citeRegEx": "Liu,? 2001", "shortCiteRegEx": "Liu", "year": 2001}, {"title": "GQ(\u03bb): A general gradient algorithm for temporaldifference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proceedings of the Third Conference on Artificial General Intelligence, pp. 91\u201396. Atlantis Press.", "citeRegEx": "Maei and Sutton,? 2010", "shortCiteRegEx": "Maei and Sutton", "year": 2010}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis, University of Alberta.", "citeRegEx": "Maei,? 2011", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A.R. Mahmood", "H. van Hasselt", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R. Mahmood", "R.S. Sutton"], "venue": "Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "Mahmood and Sutton,? 2015", "shortCiteRegEx": "Mahmood and Sutton", "year": 2015}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "European Workshop on Reinforcement Learning 12,arXiv preprint ArXiv:1507. 01569.", "citeRegEx": "Mahmood et al\\.,? 2015", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Adaptive lambda least-squares temporal difference learning", "author": ["T.A. Mann", "H. Penedones", "S. Mannor", "T. Hester"], "venue": "arXiv preprint arXiv:1612.09465.", "citeRegEx": "Mann et al\\.,? 2016", "shortCiteRegEx": "Mann et al\\.", "year": 2016}, {"title": "Off-policy Evaluation in Markov Decision Processes, PhD thesis, McGill University", "author": ["C. Paduraru"], "venue": null, "citeRegEx": "Paduraru,? \\Q2013\\E", "shortCiteRegEx": "Paduraru", "year": 2013}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "Proceedings of the 17th International Conference on Machine Learning, pp. 759\u2013766. Morgan Kaufmann.", "citeRegEx": "Precup et al\\.,? 2000", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "Proceedings of the 18th International Conference on Machine Learning.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Safe and efficient offpolicy reinforcement learning", "author": ["R Munos", "T Stepleton", "A Harutyunyan", "M.G. Bellemare"], "venue": "In Proceedings of Neural Information Processing Systems", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Simulation and the Monte Carlo Method", "author": ["R.Y. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein,? \\Q1981\\E", "shortCiteRegEx": "Rubinstein", "year": 1981}, {"title": "On bias and step size in temporal-difference learning", "author": ["R.S. Sutton", "S.P. Singh"], "venue": "Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp. 91-96.", "citeRegEx": "Sutton and Singh,? 1994", "shortCiteRegEx": "Sutton and Singh", "year": 1994}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence, 112(1), 181\u2013211.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A new Q(\u03bb) with interim forward view and Monte Carlo equivalence", "author": ["R.S. Sutton", "A.R. Mahmood", "D. Precup", "H. van Hasselt"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "Journal of Machine Learning Research 17, (73):1\u2013", "citeRegEx": "Sutton et al\\.,? 2016", "shortCiteRegEx": "Sutton et al\\.", "year": 2016}, {"title": "Safe Reinforcement Learning", "author": ["P.S. Thomas"], "venue": "PhD thesis, University of Massachusetts Amherst.", "citeRegEx": "Thomas,? 2015", "shortCiteRegEx": "Thomas", "year": 2015}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["P.S. Thomas", "E. Brunskill"], "venue": "arXiv preprint arXiv:1604.00923.", "citeRegEx": "Thomas and Brunskill,? 2016", "shortCiteRegEx": "Thomas and Brunskill", "year": 2016}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control, 42(5), 674\u2013690.", "citeRegEx": "Tsitsiklis and Roy,? 1997", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1997}, {"title": "Insights in Reinforcement Learning: formal analysis and empirical evaluation of temporal-difference learning algorithms", "author": ["H. van Hasselt"], "venue": "PhD thesis,", "citeRegEx": "Hasselt,? \\Q2011\\E", "shortCiteRegEx": "Hasselt", "year": 2011}, {"title": "Off-policy TD(\u03bb) with a true online equivalence", "author": ["H. van Hasselt", "A.R. Mahmood", "R.S. Sutton"], "venue": "In Proceedings of the 30th Conference on Uncertainty", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "True online TD(\u03bb)", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Seijen and Sutton,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton", "year": 2014}, {"title": "Scaling life-long off-policy learning", "author": ["A. White", "J. Modayil", "R.S. Sutton"], "venue": "Proceedings of the Second Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics, San Diego, USA.", "citeRegEx": "White et al\\.,? 2012", "shortCiteRegEx": "White et al\\.", "year": 2012}, {"title": "Developing a Predictive Approach to Knowledge", "author": ["A. White"], "venue": "PhD thesis, University of Alberta.", "citeRegEx": "White,? 2015", "shortCiteRegEx": "White", "year": 2015}, {"title": "Investigating practical, linear temporal difference learning", "author": ["A. White", "M. White"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, 494\u2013502.", "citeRegEx": "White and White,? 2016a", "shortCiteRegEx": "White and White", "year": 2016}, {"title": "A greedy approach to adapting the trace parameter for temporal difference learning", "author": ["M. White", "A. White"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, 557\u2013565.", "citeRegEx": "White and White,? 2016b", "shortCiteRegEx": "White and White", "year": 2016}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM Journal on Control and Optimization, 50 (6), 3310\u20133343.", "citeRegEx": "Yu,? 2012", "shortCiteRegEx": "Yu", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "arXiv preprint arXiv:1506.02582; a shorter version appeared in The 28th Annual Conference on Learning Theory (COLT) 2015.", "citeRegEx": "Yu,? 2015", "shortCiteRegEx": "Yu", "year": 2015}, {"title": "Weak convergence properties of constrained emphatic temporal-difference learning with constant and slowly diminishing stepsize", "author": ["H. Yu"], "venue": "Journal of Machine Learning Research 17 (220):1\u201358. 18", "citeRegEx": "Yu,? 2016", "shortCiteRegEx": "Yu", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "Learning a large number of off-policy predictions is also considered important for model learning, options learning (Sutton et al. 1999), scalable life-long learning (White, Modayil & Sutton 2012), and knowledge representation (White 2015).", "startOffset": 116, "endOffset": 136}, {"referenceID": 36, "context": "1999), scalable life-long learning (White, Modayil & Sutton 2012), and knowledge representation (White 2015).", "startOffset": 96, "endOffset": 108}, {"referenceID": 20, "context": "A prior algorithm, Tree Backup (Precup et al. 2000), can be retrieved as a special case of our algorithm.", "startOffset": 31, "endOffset": 51}, {"referenceID": 22, "context": "Furthermore, we show that another off-policy algorithm, Retrace (Munos et al. 2016), can also be derived and extended to the case of function approximation with stability using the action-dependent bootstrapping technique.", "startOffset": 64, "endOffset": 83}, {"referenceID": 22, "context": "A closely related idea is state-dependent bootstrapping used by Sutton and Singh (1994) and Sutton et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 22, "context": "A closely related idea is state-dependent bootstrapping used by Sutton and Singh (1994) and Sutton et al. (2014) for state-value estimation, and by Maei and Sutton (2010) for action-value estimation.", "startOffset": 64, "endOffset": 113}, {"referenceID": 13, "context": "(2014) for state-value estimation, and by Maei and Sutton (2010) for action-value estimation.", "startOffset": 42, "endOffset": 65}, {"referenceID": 26, "context": "To illustrate that ABQ(\u03b6) solutions can retain much of the multi-step benefits, we used a two-state off-policy task similar to the off-policy task by Sutton et al. (2016). In this task, there were two states each with two actions, left and right, leading to one of the two states deterministically.", "startOffset": 150, "endOffset": 171}, {"referenceID": 14, "context": "We take the approach proposed by Maei (2011) to develop a stable gradient-based TD algorithm.", "startOffset": 33, "endOffset": 45}, {"referenceID": 14, "context": "In the first two tasks we investigated whether ABQ(\u03b6) can produce correct estimates with less variance compared to GQ(\u03bb) (Maei 2011), the state-of-the-art importance sampling based algorithm for action-value estimation with function approximation.", "startOffset": 121, "endOffset": 132}, {"referenceID": 1, "context": "In the 7-star Baird\u2019s counterexample, adopted from White (2015), step sizes were set as \u03b1 = 0.", "startOffset": 14, "endOffset": 64}, {"referenceID": 22, "context": "Here, we show how the Retrace algorithm by Munos et al. (2016) can be understood in terms of a particular way of setting the action-dependent bootstrapping parameter.", "startOffset": 43, "endOffset": 63}, {"referenceID": 19, "context": "Related works A closely related algorithm is Tree Backup by Precup et al. (2000). This algorithm can also be produced as a special case of ABQ(\u03b6), if we remove gradient correction, consider the feature vectors always to be the standard basis, and \u03bd\u03b6 to be always set to a constant, instead of setting it in an action-dependent manner.", "startOffset": 60, "endOffset": 81}, {"referenceID": 19, "context": "Related works A closely related algorithm is Tree Backup by Precup et al. (2000). This algorithm can also be produced as a special case of ABQ(\u03b6), if we remove gradient correction, consider the feature vectors always to be the standard basis, and \u03bd\u03b6 to be always set to a constant, instead of setting it in an action-dependent manner. In the on-policy case, the Tree Backup algorithm fails to achieve the TD(1) solution, whereas ABQ achieves it with \u03b6 = 1. Our work is not a trivial generalization of this prior work. The Tree Backup algorithm was developed using a different intuition based on backup diagrams and was introduced only for the lookup table case. Not only does ABQ(\u03b6) extend the Tree Backup algorithm, but the idea of action-dependent bootstrapping also played a crucial role in deriving the ABQ(\u03b6) algorithm with gradient correction in a principled way. Another related algorithm is Retrace, which we have already shown to be a special case of the AB-Trace algorithm. The main differences are that Retrace was introduced and analyzed in the case of tabular representation, and thus Retrace is neither stable nor shown to achieve multi-step solutions in the case of function approximation. Yet another class of related algorithms are where the bootstrapping parameter is adapted based on past data, for example, the works by White and White (2016b) and Mann et al.", "startOffset": 60, "endOffset": 1364}, {"referenceID": 18, "context": "Yet another class of related algorithms are where the bootstrapping parameter is adapted based on past data, for example, the works by White and White (2016b) and Mann et al. (2016). Beside adapting on a state-action-pair basis, the main difference between those algorithms and the ones introduced here under the action-dependent bootstrapping framework is that our algorithms adapt the bootstrapping parameter using only the knowledge of policy probabilities for the current state-action pair whereas the algorithms in the other class involve a separate learning procedure for adapting \u03bb and hence are more complex.", "startOffset": 163, "endOffset": 182}, {"referenceID": 20, "context": "According to an alternative explanation based on backup diagrams (Precup et al. 2000), ABQ updates may be seen as devoid of importance sampling ratios, while the action-dependent bootstrapping framework can now provide us a clearer mechanistic view of how updates without importance sampling ratios can perform off-policy learning.", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "The convergence of this algorithm can be analyzed similarly to the work by Karmakar and Bhatnagar (2015) for diminishing step sizes and Yu (2015) for constant step sizes, by using properties of the eligibility traces and stochastic approximation theory.", "startOffset": 75, "endOffset": 105}, {"referenceID": 8, "context": "The convergence of this algorithm can be analyzed similarly to the work by Karmakar and Bhatnagar (2015) for diminishing step sizes and Yu (2015) for constant step sizes, by using properties of the eligibility traces and stochastic approximation theory.", "startOffset": 75, "endOffset": 146}], "year": 2017, "abstractText": "To estimate the value functions of policies from exploratory data, most model-free offpolicy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart.", "creator": "LaTeX with hyperref package"}}}