{"id": "1702.06120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "On the Consistency of $k$-means++ algorithm", "abstract": "we prove among this paper that the expected value of the objective function of the $ k $ - means + + algorithm for distribution converges to calculate expected value. as $ k $ - means + +, for which, typically with constant factor approximation for $ i $ - means objectives, such an approximation can be achieved for the output with increase of the sample size.", "histories": [["v1", "Mon, 20 Feb 2017 10:09:02 GMT  (303kb,D)", "http://arxiv.org/abs/1702.06120v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1702.06120"}, "pdf": {"name": "1702.06120.pdf", "metadata": {"source": "CRF", "title": "On the Consistency of k-means++ algorithm", "authors": ["Mieczys\u0142aw A. K\u0142opotek"], "emails": [], "sections": [{"heading": null, "text": "ering using subsampling when clustering large data sets (large data bases).\nIndex Terms\u2014 k-means++; consistency; expected value; constant factor approximation; k-means cost function\n1 Introduction\nThe common sense feeling about data mining algorithms is that with increasing sample sizes the tasks should become conceptually easier though more though computationally.\nOne of these areas is surely the cluster analysis. A popular clustering algorithm, k-means, shall be studied in this research with respect to its behaviour \u201din the limit\u201d. It essentially strives to minimise the partition quality function (called also \u201dpartition cost function\u201d)\nJ(U,M) = m\u2211 i=1 k\u2211 j=1 uij\u2016xi \u2212 \u00b5j\u20162 (1)\nar X\niv :1\n70 2.\n06 12\n0v 1\n[ cs\n.L G\n] 2\n0 Fe\nwhere xi, i = 1, . . . ,m are the data points, M is the matrix of cluster centres \u00b5j, j = 1, . . . , k, and U is the cluster membership indicator matrix, consisting of entries uij, where uij is equal 1 if among all of cluster centres \u00b5j is the closest to xi, and is 0 otherwise.\nk-means comes in various variants, most of them differing by the way how the initial hypothetical cluster centres are found. We will concentrate on the so-called k-means++, where the initialisation is driven by candidate probability of being selected proportional to squared distances to the closest cluster centre candidate selected earlier. This distinguishes it from the basic version, called here k-means-random, where the initial candidates are selected randomly with uniform probability distribution over data set. Both are described in detail in the paper [3] by Arthur and Vassilvitskii.\nIn Figure 1 you see the results of clustering for data from a relatively easy probability distribution of 16 identical clusters arranged uniformly in 4 rows of 4 clusters. In Figure 2 you see the results of clustering for exactly the same data samples. It is immediately visible, that even in simple cases the algorithms, for small samples, do not return clusters identical with those for the \u201din the limit\u201d distribution. In particular, k-means-random behaves poorly even for quite large samples (of 33000 data points). k-means++ seems\nto stabilize with sample size, but over multiple repetitions bad cluster split can occur.\nThe behaviour \u201din the limit\u201d is of practical relevance, as some researchers, especially in the realm of database mining, propose to cluster \u201dsufficiently large\u201d samples instead of the whole database content. See for example Bejarano et al. [6] on k-means accelerating via subsampling.\nFor these reasons Pollard [9] investigated already in 1981 the \u201cin the limit\u201d behaviour of k-means, while MacQueen [7] considered it already in 1967.\nThe essential problem consists in the question whether or not we can expect that the cost function value from equation (1), and hence the \u201dbest clustering\u201d, of the population can be approximated by the results of clustering samples (of increasing sizes).\nRegrettably, researchers considered in the past an unrealistic version of k-means, that we shall call here \u201dk-means-ideal\u201d, that is able to find the minimum of (1) for a sample which is NP -hard. This tells nothing about the real-world algorithms, like k-means-random or k-means++, because they are not guaranteed to approach this optimum in any way. In fact, no results for k-means-random are known even for constant factor approximations to\nsample optimal value. So far, only for k-means++ we have this kind of approximation provided by Arthur and Vassilvitskii [3] .\nThe constant factor approximation for a sample does not guarantee the existence of the constant factor approximation of optimal cost function value for the population as the limit of sample constant factor approximations, because the whole proof of in the limit behaviour of k-means-ideal by Pollard relies on the assumption of the existence of unique optimal solution. No direct transposition is possible to any constant factor approximation because there exist potentially infinite many constant factor approximation hence also as many constant factor approximating partitions.\nSo in this paper we address the issue whether or not it is possible to get a constant factor approximation clustering of the population as a limit of constant factor approximation partitions of the samples with increasing sample sizes.\n2 Previous work\nArthur and Vassilvitskii [3] proved the following property of their k-means++ algorithm.\nTheorem 1. [3] The expected value of the partition cost, computed according to equation (1), is delimited in case of k-means++ algorithm by the inequality\nE(J) \u2264 8(ln k + 2)Jopt (2)\nwhere Jopt denotes the optimal value of the partition cost.\n(For k = 3 this amounts to more than 24). Note that the proof in [3] refers to the initialisation step of the algorithm only so that subsequently we assume that k-means++ clustering consists solely of this initialisation. In the full algorithm E(J) is lower than after initialisation step only.\nThere exist a number of other constant factor approximation algorithms for k-means objective beside [3]. But they either decrease the constant below 8(ln k+2) at the expense of an excessive complexity, still with constant factor above 9 [1], by restricting to well-separated cases [10], or by approximating the cost function of k-means with k+l-means, where l is a quite large number, see e.g. the paper [2]. Still most of them rely on the basic approach of [3]. Other approaches, like that of [5], assume that all the near optimal partitions lie close to the optimal solution without actually proving it.\nAll this research attempts to annihilate the fundamental problem behind the k-means-random, namely, as [3] states: \u201dIt is the speed and simplicity\nof the k-means[-random] method that make it appealing, not its accuracy. Indeed, there are many natural examples for which the algorithm generates arbitrarily bad clusterings (i.e., J\nJopt is unbounded even when m and k are\nfixed).\u201d While the aforementioned research direction attempts to improve samplebased cost function results for k-means, there existed and exists interest in strong consistency results concerning variants of k-means algorithm, aiming at proving properties achieved with increasing sample size. Pollard [9] proved an interesting property of a hypothetical k-means-ideal algorithm that would produce a partition yielding Jopt for finite sample. He demonstrated, under assumption of a unique global optimum, the convergence of k-means-ideal algorithm to proper population clustering with increasing sample size. So one can state that such an algorithm would be strongly consistent. MacQueen [7] considered a k-means-ideal algorithm under the settings where there exists no unique clustering reaching the minimum of the cost function for the population. Hence k-means-ideal may be unstable for the sequence of samples of increasing size. Terada [11] considered a version of k-means called \u201dreduced\u201d k-means which seeks a lower dimensional subspace in which k-means can be applied. He again assumes that a k-means-ideal version is available when carrying out his investigations. The problem of the k-means-ideal algorithm is that it is NP -hard [9], even in 2D [8]).\nIn summary, in the mentioned works an abstract global optimiser is referred to rather than an actual practically used algorithm.\nTherefore in this research we study the in the limit behaviour of the realistic k-means++ algorithm. Though it is slow compared to k-meansrandom, various accelerations have been proposed, e.g. [4], hence it is with studying.\n3 In the limit behaviour of k-means++\nSo let us investigate whether or not a realistic algorithm, k-means++, featured by reasonable complexity and reasonable expectation of closeness to optimality, is also in some sense strongly consistent.\nIn what follows we extend the results of Pollard.\n3.1 Notation and assumption\nAssume that we are drawing independent samples of size m = k, k + 1, . . . . from some probability distribution P . Assume further, that the number of disjoint balls of radius b for some b > 0 with non-zero P measure is much\nlarger than k. Furthermore let\u222b \u2016x\u20162(k+1)P (dx)\nbe finite. Given a sample consisting of xi, i = 1, . . . ,m, it will be convenient to consider an empirical probability distribution Pm assigning a probability mass of m\u22121 at each of the sample points xi. Let further M denote any set of up to k points (serving as an arbitrary set of cluster centres).\nLet us introduce the function Jcp(., .), that for any set M of points in Rn and any probability distribution Q over Rn, computes k-means quality function normalised over the probability distribution:\nJcp(M,Q) =\n\u222b D(x,M)2Q(dx) (3)\nwhere D(x,M) = min\u00b5\u2208M \u2016x\u2212 \u00b5\u2016. Then Jcp(M,Pm) can be seen as a version of the function J() from equation (1). Note that for any sets M.M \u2032 such that M \u2282 M \u2032 we have Jcp(M,Q) \u2265 Jcp(M \u2032, Q).\nNote also that if M has been obtained via the initialisation step of kmeans++, then the elements of M can be considered as ordered, so we could use indexes M[p:q] meaning elements \u00b5p, . . . ,\u00b5q from this sequence. As the cluster around M[k] will contain at least one element (\u00b5k itself), we can be sure that Jcp(M[1:k\u22121], Pm) > Jcp(M[1:k], Pm) as no element has a chance to be selected twice. Hence also EM(Jcp(M[1:k\u22121], Pm)) > EM(Jcp(M[1:k], Pm)).\n3.2 Main result\nIf we assume that the distribution P is continuous, then for the expectation over all possible M obtained from initialisation procedure of k-means++, for any j = 2, \u00b7 \u00b7 \u00b7 k the relationship EM(Jcp(M[1:j\u22121], P )) > EM(Jcp(M[1:j], P )) holds. The reason is as follows: The last point, \u00b5k, was selected from a point of non-zero density, so that in ball of radius b > 0 ( b lower than 1/3 of the distance of \u00b5j to each \u00b5 \u2208 M[1:j\u22121]) around it would be re-clustered to \u00b5j diminishing the overall within-cluster variance. As it holds for each set M , so it holds for the expected value too. But our goal here is to show that if we pick the sets M according to kmeans++ (initialization) algorithm, then EM(Jcp(M,Pm))\u2192m\u2192\u221e EM(Jcp(M,P )), and, as a consequence, EM(Jcp(M,P )) \u2264 8(ln k + 2)J.,opt.\nWe show this below. Let Tm,k denote the probability distribution for choosing the set M as cluster centres for a sample from Pm using k-means++ algorithm. From the k-means++ algorithm we know that the probability of choosing \u00b5j given \u00b5 \u2208M[1:j\u22121] have already been picked amounts to Prob(\u00b5j|M[1:j\u22121]) =\nD(\u00b5j ,M[1:j\u22121]) 2\u222b\nD(x,M[1:j\u22121])2Pm(dx) . whereas the point \u00b51 is picked according to the probability distribution Pm. Obviously , the probability of selecting the set M amounts to:\nProb(M) = Prob(\u00b51) \u00b7Prob(\u00b52|M[1]) \u00b7Prob(\u00b53|M[1:2]) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7Prob(\u00b5k|M[1:k\u22121])\n. So the probability density Tm,k may be expressed as\nPm(\u00b51) \u00b7 k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2\u222b D(x,M[1:j\u22121])2Pm(d x)\nFor P we get accordingly the Tk distribution. Now assume we ignore with probability of at most \u03b4 > 0 a \u201cdistant\u201d part of the potential locations of cluster centres from Tk via a procedure described below. Denote withM\u03b4 the subdomain of the set of cluster centres left after ignoring these elements. Instead of expectation of\nEM(Jcp(M,Pm)) = \u222b Jcp(M,Pm)Tm,k(dM)\nwe will be interested in EM,\u03b4(Jcp(M,Pm)) = \u222b M\u2208M\u03b4 Jcp(M,Pm)Tm,k(dM)\nand respectively for P instead of EM(Jcp(M,P )) = \u222b Jcp(M,P )Tk(dM)\nwe want to consider EM,\u03b4(Jcp(M,P )) = \u222b M\u2208M\u03b4 Jcp(M,P )Tk(dM)\nEM(Jcp(M,P )) is apparently finite. Let us concentrate on the difference |EM,\u03b4(Jcp(M,P ))\u2212EM,\u03b4(Jcp(M,Pm))| and show that it converges to 0 when m is increased for a fixed \u03b4, that is\nfor any and \u03b4 there exists such an m\u03b4, that for any larger m almost surely (a.s.) this difference is lower than . Thus by decreasing \u03b4 and we find that the expectation on Pm converges to that on P .\nThe subdomain selection (ignoring \u03b4 share of probability mass of Tm) shall proceed as follows. Let R1 be a radius such that \u222b \u2016x\u2016>R1 \u2016x\u2016\n2P (dx) < \u03b4/k. We reject all M such that \u2016\u00b51\u2016 > R1.\nLet us denote Jm,opt = minM Jcp(M,Pm), J.,opt = minM Jcp(M,P ), where M is a set of cardinality at most k. Let us denote Jm,opt,j = minM Jcp(M,Pm), J.,opt,j = minM Jcp(M,P ), where M be a set of cardinality at most j, j = 1, . . . , k.\nNow observe for l = 2, . . . , k that\u222b \u00b51 \u00b7 \u00b7 \u00b7 \u222b \u00b5l l\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2\u222b D(x,M[1 : j \u2212 1])2P (d x) P (d\u00b5l) . . . P (d\u00b51)\n\u2264 \u222b \u00b51 \u00b7 \u00b7 \u00b7 \u222b \u00b5l l\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2 J.,opt,j\u22121 P (d\u00b5l) . . . P (d\u00b51)\n= ( l\u22121\u220f j=1 1 J.,opt,j )\u222b \u00b51 \u00b7 \u00b7 \u00b7 \u222b \u00b5l l\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (d\u00b5l) . . . P (d\u00b51)\nUpon restricting the integration area on \u00b51, . . . ,\u00b5l\u22121\n\u2264 ( l\u22121\u220f j=1 1 J.,opt,j )\u222b \u2016\u00b51\u2016\u2264R1 \u00b7 \u00b7 \u00b7 \u222b \u2016\u00b5l\u22121\u2016\u2264Rl\u22121 \u222b \u00b5l l\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (d\u00b5l) . . . P (d\u00b51)\nNow substituting the maximum distance between points \u00b51, . . . ,\u00b5l\u22121\n\u2264 ( l\u22121\u220f j=1 1 J.,opt,j )\u222b \u2016\u00b51\u2016\u2264R1 \u00b7 \u00b7 \u00b7 \u222b \u2016\u00b5l\u22121\u2016\u2264Rl\u22121 \u222b \u00b5l ( l\u22121\u220f j=2 (Rj +Rj \u2212 1)2 ) D(\u00b5l,M[1:l\u22121]) 2\nP (d\u00b5l) . . . P (d\u00b51)\n= ( l\u22121\u220f j=1 1 J.,opt,j )( l\u22121\u220f j=2 (Rj +Rj \u2212 1)2 )\u222b \u2016\u00b51\u2016\u2264R1 \u00b7 \u00b7 \u00b7 \u222b \u2016\u00b5l\u22121\u2016\u2264Rl\u22121 \u222b \u00b5l D(\u00b5l,M[1:l\u22121]) 2\nP (d\u00b5l) . . . P (d\u00b51)\nNow substituting the maximum distance to the last point \u00b5l\n\u2264 ( l\u22121\u220f j=1 1 J.,opt,j )( l\u22121\u220f j=2 (Rj +Rj \u2212 1)2 )\u222b \u2016\u00b51\u2016\u2264R1 \u00b7 \u00b7 \u00b7 \u222b \u2016\u00b5l\u22121\u2016\u2264Rl\u22121 \u222b ( \u2016\u00b5l\u2016+Rl\u22121)2\nP (d\u00b5l) . . . P (d\u00b51)\n\u2264 ( l\u22121\u220f j=1 1 J.,opt,j )( l\u22121\u220f j=2 (Rj +Rj \u2212 1)2 )\u222b \u00b5l (\u2016\u00b5l\u2016+Rl\u22121)2P (d\u00b5l)\nLet us choose now an Rl so that( l\u22121\u220f j=1 1 J.,opt,j )( l\u22121\u220f j=2 (Rj +Rj \u2212 1)2 )\u222b \u2016\u00b5l\u2016\u2264Rl (\u2016\u00b5l\u2016+Rl\u22121)2P (d\u00b5l) < \u03b4/k\nIn this way we again reject at most \u03b4/k \u00b5l points. Upon repeating this process for l = 2, . . . , k, we will reject at most \u03b4 share of potential sets of centres under Tk.\nSo let us consider\n|EM,\u03b4(Jcp(M,P ))\u2212 EM,\u03b4(Jcp(M,Pm))| = | \u222b M\u2208M\u03b4 Jcp(M,Pm)Tm,k(dM)\u2212 \u222b M\u2208M\u03b4 Jcp(M,P )Tk(dM)|\n= | \u222b M\u2208M\u03b4 Jcp(M,Pm) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2\u222b D(x,M[1:j\u22121]2Pm(d x) Pm(d\u00b5k) . . . Pm(d\u00b51)\n\u2212 \u222b M\u2208M\u03b4 Jcp(M,P ) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2\u222b D(x,M[1:j\u22121]2P (d x) P (d\u00b5k) . . . P (d\u00b51)|\n\u2264 | \u222b M\u2208M\u03b4 Jcp(M,Pm) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2 Jm,opt,j\u22121 Pm(d\u00b5k) . . . Pm(d\u00b51)\n\u2212 \u222b M\u2208M\u03b4 Jcp(M,P ) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2 J.,opt,j\u22121 P (d\u00b5k) . . . P (d\u00b51)|\nFor a sufficiently large m \u220fk\nj=2\nD(\u00b5j ,M[1:j\u22121]) 2\nJm,opt,j\u22121 differs from \u220fk j=2 D(\u00b5j ,M[1:j\u22121]) 2 J.,opt,j\u22121\nby at most opt. Hence\n\u2264 | k\u220f j=2\n1\nJ.,opt,j\u22121 \u222b M\u2208M\u03b4 Jcp(M,Pm) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2Pm(d\u00b5k) . . . Pm(d\u00b51)\n\u2212( k\u220f j=2\n1\nJ.,opt,j\u22121 +/\u2212 opt) \u222b M\u2208M\u03b4 Jcp(M,P ) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (d\u00b5k) . . . P (d\u00b51)|\n\u2264 opt| \u222b M\u2208M\u03b4 Jcp(M,P ) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (d\u00b5k) . . . P (d\u00b51)|\n+ k\u220f j=2\n1 J.,opt,j\u22121 | \u222b M\u2208M\u03b4 Jcp(M,Pm) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2Pm(d\u00b5k) . . . Pm(d\u00b51)\n\u2212 \u222b M\u2208M\u03b4 Jcp(M,P ) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (d\u00b5k) . . . P (d\u00b51)|\n= opt| \u222b M\u2208M\u03b4 Jcp(M,P ) k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (d\u00b5k) . . . P (d\u00b51)|\n+ k\u220f j=2\n1 J.,opt,j\u22121 | \u222b M\u2208M\u03b4 \u222b x D(x,M)2 k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2Pm(dx)Pm(d\u00b5k) . . . Pm(d\u00b51)\n\u2212 \u222b M\u2208M\u03b4 \u222b x D(x,M)2 k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2P (dx)P (d\u00b5k) . . . P (d\u00b51)|\nThe first summand, the product of an opt and a finite quantity, can be set as low as needed by choosing a sufficiently low opt.\nNow following Pollard, we will decrease as much as required the second summand.\nSo select a finite set T\u03b2 of points from M\u03b4 such that each element of M\u03b4 lies within a distance of \u03b2 from a point of T\u03b2.\nLet us define the function gM(x) = D(x,M)2 \u220fk j=2D(\u00b5j,M[1:j\u22121]) 2.\nLet us introduce also D(x,M) and D(x,M) as follows: M\u2217 be the set of elements from T\u03b2 such that \u2016\u00b5j \u2212M\u2217[j]\u2016 is less than \u03b2 . then D(x,M) = D(x,M\u2217) + \u03b2, D(x,M) = max(D(x,M\u2217)\u2212 \u03b2, 0). Now define the function\ng M (x) = D(x,M)2 k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2\nand\ngM(x) = D(x,M) 2 k\u220f j=2 D(\u00b5j,M[1:j\u22121]) 2\nAs \u2016x,M\u2217[j]\u2016 \u2212 \u03b2 \u2264 \u2016x \u2212 \u00b5j| \u2264 \u2016x,M\u2217[j]\u2016 + \u03b2, it is easily seen that D(x,M) \u2264 D(x,M) \u2264 D(x,M) and hence g\nM (x) \u2264 gM(x) \u2264 gM(x).\nTherefore\n| \u222b M\u2208M\u03b4 \u222b x gM(x)P (dx)P (d\u00b5k) . . . P (d\u00b51))\n\u2212 \u222b M\u2208M\u03b4 \u222b x gM(x)Pm(dx)P (d\u00b5k) . . . P (d\u00b51)|\nis bounded from above by\u222b M\u2208M\u03b4 \u222b x (gM(x)\u2212 gM(x))P (dx)P (d\u00b5k) . . . P (d\u00b51)\n+max(| \u222b M\u2208M\u03b4 \u222b x gM(x)P (dx)P (d\u00b5k) . . . P (d\u00b51)\n\u2212 \u222b M\u2208M\u03b4 \u222b x gM(x)Pm(dx)P (d\u00b5k) . . . P (d\u00b51)|\n, | \u222b M\u2208M\u03b4 \u222b x g M (x)P (dx)P (d\u00b5k) . . . P (d\u00b51)\n\u2212 \u222b M\u2208M\u03b4 \u222b x g M (x)Pm(dx)P (d\u00b5k) . . . P (d\u00b51)|)\nA sufficient increase on m will make the second summand as small as we like. We will show below that using an argument similar to Pollard, one can show that the first summand can be diminished as much as we like to by appropriate choice of \u03b2. Hence the restricted expectation can be shown to converge as claimed.\nFor any R\u222b M\u2208M\u03b4 \u222b x (gM(x)\u2212 gM(x))P (dx)P (d\u00b5k) . . . P (d\u00b51\n= \u222b M\u2208M\u03b4 \u222b \u2016x||\u2264R (gM(x)\u2212 gM(x))P (dx)P (d\u00b5k) . . . P (d\u00b51\n+ \u222b M\u2208M\u03b4 \u222b \u2016x||>R (gM(x)\u2212 gM(x))P (dx)P (d\u00b5k) . . . P (d\u00b51\nThe second summand has the property:\u222b M\u2208M\u03b4 \u222b \u2016x||>R (gM(x)\u2212 gM(x))P (dx)P (d\u00b5k) . . . P (d\u00b51)\n\u2264 \u222b M\u2208M\u03b4 \u222b \u2016x||>R gM(x)P (dx)P (d\u00b5k) . . . P (d\u00b51)\n\u2264 \u222b M\u2208M\u03b4 \u222b \u2016x||>R (2Rk + 2\u03c3) 2k+2(\u2016x\u2016+Rk + 2\u03c3)2P (dx)P (d\u00b5k) . . . P (d\u00b51)\nwhich can be decreased as much as necessary by increasing R. The first summand can be transformed\u222b\nM\u2208M\u03b4\n\u222b \u2016x||\u2264R (gM(x)\u2212 gM(x))P (dx)P (d\u00b5k) . . . P (d\u00b51)\n\u2264 \u222b M\u2208M\u03b4 \u222b \u2016x||\u2264R \u03c3hM(x)P (dx)P (d\u00b5k) . . . P (d\u00b51)\nwhere hM(x) is a function of x and M . As both x and elements of M are of bounded length, hM(x) is limited from above. Hence this summand can be decreased to the desired value by appropriate choice of (small) \u03c3.\nTherefore the whole term can be decreased to a desired value (above zero of course), what we had to prove.\nHence our claim about consistency of k-means++ has been proven.\n4 Conclusions\nWe have shown that the expected value of the realistic k-means++ algorithm for finite sample converges towards the expected value of the realistic k-means++ algorithm for the population, when the sample size increases. Together with Pollard\u2019s results about convergence of k-means-ideal and the properties proved by Arthur and Vassilvitskii for k-means++ on finite samples we know now that k-means++ provides in expectation also with constant factor approximation of the population optimal k-means cost function.\nUnderway we have also shown that the sample based centre group distribution comes close to that of the population distribution so that the partitions of samples yielding constant approximations of k-means cost function come close to partitions yielding constant approximations for the entire population.\nReferences\n[1] In 18th Annual ACM Symposium on Computational Geometry (SoCG\u201902), June 2002, Barcelona, Spain.\n[2] A simple linear time approximation algorithm for geometric k-means problem in any dimension ,author = A. Kumar and Y. Sabharwal and S. Sen , booktitle=FOCS 2004 ,pages= 454\u2013462 , year=2004 , myurl=http://www.cse.iitd.ac.in/ ssen/conf/focs04.pdf .\n[3] D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding. In N. Bansal, K. Pruhs, and C. Stein, editors, Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, pages 1027\u20131035, New Orleans, Louisiana, USA, 7-9 Jan. 2007. SIAM.\n[4] Olivier Bachem, Mario Lucic, S. Hamed Hassani, and Andreas Krause. Approximate k-means++ in sublinear time. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI\u201916, pages 1459\u2013 1467. AAAI Press, 2016.\n[5] Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Approximate clustering without the approximation. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201909, pages 1068\u20131077, Philadelphia, PA, USA, 2009. Society for Industrial and Applied Mathematics.\n[6] Jeremy Bejarano, Koushiki Bose, Tyler Brannan, Anita Thomas, Kofi Adragni, and Nagaraj K. Neerchal. Sampling within k-means algorithm to cluster large datasets. Technical Report HPCF-2011-12, High Performance Computing Facility, University of Maryland, Baltimore County, 2011.\n[7] J. MacQueen. Some methods for classification and analysis of multivariate observations. In Proc. Fifth Berkeley Symp. on Math. Statist. and Prob., volume 1, pages 281\u2013297. Univ. of Calif. Press, 1967.\n[8] Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan. The planar k-means problem is np-hard. In Proceedings of the 3rd International Workshop on Algorithms and Computation, WALCOM \u201909, pages 274\u2013285, Berlin, Heidelberg, 2009. Springer-Verlag.\n[9] David Pollard. Strong consistency of k\u2013means clustering. Ann. Statist., 9(1):135\u2013140, 1981.\n[10] Cheng Tang and Claire Monteleoni. Scalable constant k-means approximation via heuristics on well-clusterable data. In Poster Session of Learning faster from easy data II conference, Montreal, Canada.\n[11] Yoshikazu Terada. Strong consistency of reduced k-means clustering. Scand. J. Stat., 41(4):913\u2013931, 2014."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Approximate k-means++ in sublinear time", "author": ["Olivier Bachem", "Mario Lucic", "S. Hamed Hassani", "Andreas Krause"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Sampling within k-means algorithm to cluster large datasets", "author": ["Jeremy Bejarano", "Koushiki Bose", "Tyler Brannan", "Anita Thomas", "Kofi Adragni", "Nagaraj K. Neerchal"], "venue": "Technical Report HPCF-2011-12,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MacQueen"], "venue": "In Proc. Fifth Berkeley Symp. on Math. Statist. and Prob.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1967}, {"title": "The planar k-means problem is np-hard", "author": ["Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi Varadarajan"], "venue": "In Proceedings of the 3rd International Workshop on Algorithms and Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Strong consistency of k\u2013means clustering", "author": ["David Pollard"], "venue": "Ann. Statist.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1981}, {"title": "Strong consistency of reduced k-means clustering", "author": ["Yoshikazu Terada"], "venue": "Scand. J. Stat.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Both are described in detail in the paper [3] by Arthur and Vassilvitskii.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "[6] on k-means accelerating via subsampling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For these reasons Pollard [9] investigated already in 1981 the \u201cin the limit\u201d behaviour of k-means, while MacQueen [7] considered it already in 1967.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "For these reasons Pollard [9] investigated already in 1981 the \u201cin the limit\u201d behaviour of k-means, while MacQueen [7] considered it already in 1967.", "startOffset": 115, "endOffset": 118}, {"referenceID": 0, "context": "So far, only for k-means++ we have this kind of approximation provided by Arthur and Vassilvitskii [3] .", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "2 Previous work Arthur and Vassilvitskii [3] proved the following property of their k-means++ algorithm.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "[3] The expected value of the partition cost, computed according to equation (1), is delimited in case of k-means++ algorithm by the inequality E(J) \u2264 8(ln k + 2)Jopt (2) where Jopt denotes the optimal value of the partition cost.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that the proof in [3] refers to the initialisation step of the algorithm only so that subsequently we assume that k-means++ clustering consists solely of this initialisation.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "There exist a number of other constant factor approximation algorithms for k-means objective beside [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "Still most of them rely on the basic approach of [3].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Other approaches, like that of [5], assume that all the near optimal partitions lie close to the optimal solution without actually proving it.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "All this research attempts to annihilate the fundamental problem behind the k-means-random, namely, as [3] states: \u201dIt is the speed and simplicity 4", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "Pollard [9] proved an interesting property of a hypothetical k-means-ideal algorithm that would produce a partition yielding Jopt for finite sample.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "MacQueen [7] considered a k-means-ideal algorithm under the settings where there exists no unique clustering reaching the minimum of the cost function for the population.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Terada [11] considered a version of k-means called \u201dreduced\u201d k-means which seeks a lower dimensional subspace in which k-means can be applied.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "The problem of the k-means-ideal algorithm is that it is NP -hard [9], even in 2D [8]).", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "The problem of the k-means-ideal algorithm is that it is NP -hard [9], even in 2D [8]).", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "[4], hence it is with studying.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We prove in this paper that the expected value of the objective function of the k-means++ algorithm for samples converges to population expected value. As k-means++, for samples, provides with constant factor approximation for k-means objectives, such an approximation can be achieved for the population with increase of the sample size. This result is of potential practical relevance when one is considering using subsampling when clustering large data sets (large data bases).", "creator": "LaTeX with hyperref package"}}}