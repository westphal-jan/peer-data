{"id": "1702.06336", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Hybrid Dialog State Tracker with ASR Features", "abstract": "this paper presents a hybrid dialog state tracker enhanced by trainable spoken language understanding ( slu ) for slot - filling dialog systems. scenario architecture is grounded by previously proposed neural - network - based belief - tracking systems. in addition, we extended some parts of our modular architecture with differentiable inputs to allow end - to - end training. we hypothesize that these rules allow simulation tracker to generalize better during original machine - learning based systems. for evaluation, we update the dialog state tracking mechanism ( dstc ) 2 dataset - designed interactive belief tracking kit with dialogs from restaurant information archives. to our knowledge, our hybrid set sets a new state - of - the - pool result in three out of four categories as the dstc2.", "histories": [["v1", "Tue, 21 Feb 2017 11:34:14 GMT  (337kb,D)", "http://arxiv.org/abs/1702.06336v1", "Accepted to EACL 2017"]], "COMMENTS": "Accepted to EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miroslav vodol\\'an", "rudolf kadlec", "jan kleindienst"], "accepted": false, "id": "1702.06336"}, "pdf": {"name": "1702.06336.pdf", "metadata": {"source": "CRF", "title": "Hybrid Dialog State Tracker with ASR Features", "authors": ["Miroslav Vodol\u00e1n", "Rudolf Kadlec", "Jan Kleindienst"], "emails": ["jankle}@cz.ibm.com", "vodolan@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "A belief-state tracker is an important component of dialog systems whose responsibility is to predict user\u2019s goals based on history of the dialog. Belief-state tracking was extensively studied in the Dialog State Tracking Challenge (DSTC) series (Williams et al., 2016) by providing shared testbed for various tracking approaches. The DSTC abstracts away the subsystems of end-toend spoken dialog systems, focusing only on the dialog state tracking. It does so by providing datasets of Automatic Speech Recognition (ASR) and Spoken Language Understanding (SLU) outputs with reference transcriptions, together with annotation on the level of dialog acts and user goals on slot-filling tasks where dialog system tries to fill predefined slots with values from a known ontology (e.g. moderate value for a\npricerange slot). In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodola\u0301n et al. (2015) with 2) a complex neural network based architecture that processes ASR features proposed by Henderson et al. (2014b). Their network consist of two main units. One unit handles generic behaviour that is independent of the actual slot value and the other depends on slot value and can account for common confusions.\nWhen compared to Henderson et al. (2014b) that inspired our work: 1) our model does not require auto-encoder pre-training and shared initial training on all slots which makes the training easier; 2) our approach combines a rule-based core of the tracker and RNNs while their model used only RNNs; 3) we use different NN architecture to process SLU features.\nIn the next section we describe the structure of our model, after that we detail how we evaluated the model on the DSTC2 dataset. We close the paper with a section on the lessons we learned."}, {"heading": "2 Hybrid dialog state tracker model", "text": "The tracker operates separately on the probability distribution for each slot. Each turn, the tracker generates these distributions to reflect the user\u2019s goals based on the last action of the machine, the observed user actions, the probability distributions from the previous turn and an internal hidden state. The probability distribution hst [v] is a distribution over all possible values v from the domain of slot\n1For historical reasons we adopted the hand-coded rules term used throughout the belief tracking community. From another viewpoint, our rules can be seen as a linear combination model.\nar X\niv :1\n70 2.\n06 33\n6v 1\n[ cs\n.C L\n] 2\n1 Fe\nb 20\n17\ns at dialog turn t. The joint belief state is represented by a probability distribution over the Cartesian product of the individual slot domains.\nIn the following notation ist denotes a user action pre-processed into a probability distribution of informed values for the slot s and turn t. During the pre-processing, every Affirm() from the SLU is transformed into Inform(s=v) depending on a machine action of the turn. The ft denotes turn features consisting of unigrams, bigrams, and trigrams extracted from the ASR hypotheses N -best list. They are weighted by the probability of the corresponding hypothesis on the N -best list. The same approach is used in Henderson et al. (2014b). To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix). The batch ASR hypotheses are encoded in the same way as hypotheses from the regular ASR. The confusion matrix information is encoded as weighted unigrams. The last part of the turn features encodes machine-action dialog acts. We are using trigram-like encoding dialogact-slot-value with weight 1.0. The other features are value features fvi created from turn features, which contain occurrence of vi, by replacing occurrence of the value vi and slot name s by a common tag (inform-food-italian\u2192 inform-<slot>-<value>).\nThis technique is called delexicalization by Henderson et al. (2014b).\nFrom a high-level perspective, our model consists of a rule-based core represented by a function R that specifies how the belief state evolves based on new observations. The rules R depend on the output of machine-learned SLU and on transition coefficients2 avi,vj that specify how easy it would be to override a previously internalized slot value vj with a new value vi in the given situation. The avi,vj transition coefficients are computed as a sum of functions F and G where F accounts for generic value-independent behavior which can however be corrected by the valuedependent function G. The structure of the tracker is shown in Figure 1.\nIn the next subsection, we will describe the rule-based component of the Hybrid tracker. Afterwards, in Section 2.2, we will describe the machine-learned part of the tracker followed by the description of the trainable SLU in Section 2.3."}, {"heading": "2.1 Rule-based part", "text": "The rule-based part of our tracker, inspired by Vodola\u0301n et al. (2015), is specified by a function R(hst\u22121, u s t , a) = h s t , which is a function of a slot\u2013 value probability distribution hst\u22121 in the previous turn, the output ust of a trainable SLU and of transition coefficients a which control how the new belief hst is computed. The first equation specifies the belief update rule for the probability assigned to slot value vi:\nhst [vi] = h s t\u22121[vi]\u2212h\u0303st [vi]+ust [vi]\u00b7 \u2211 vj 6=vi hst\u22121[vj ] \u00b7 avivj (1) where h\u0303st [vi] expresses how much probability will be transferred from hst\u22121[vj ] to other slot values in hst . This is computed as:\nh\u0303st [vi] = h s t\u22121[vi] \u00b7 \u2211 vj 6=vi ust [vj ] \u00b7 avjvi (2)\nwhere avivj is called the transition coefficient between values vi and vj . These coefficients are computed by the machine-learned part of our model.\n2These coefficients were modelled by a so called durability function in Kadlec et al. (2014)."}, {"heading": "2.2 Machine-learned part", "text": "The machine-learned part modulates behavior of the rule-based part R by transition coefficients avivj that control the amount of probability which is transferred from hst\u22121[vj ] to h s t [vi] as in Vodola\u0301n et al. (2015). However, our computation of the coefficients involves two different functions:\navivj = F (lt\u22121, ft, vi, vj) +G(ft, vi, vj) (3)\nwhere the function F controls generic behavior of the tracker, which does not take into account any features about vi or vj . On the other hand, function G provides value-dependent corrections to the generic behavior described by F .\nValue Independent Model. F is specified as:\nF (lt\u22121, ft, vi, vj) = { cnew if vi = None coverride if vi 6= vj (4) where the F function takes values of cnew and coverride from a function L. The function \u3008cnew, coverride, lt\u3009 = L(lt\u22121, ft) is a recurrent function that takes its hidden state vector lt\u22121 from the previous turn and the turn features ft as input and it outputs two scalars cnew, coverride and a new hidden state lt. An interpretation of these scalar values is the following:\n\u2022 cnew \u2014 describes how easy it would be to change the belief from hypothesis None to an instantiated slot value,\n\u2022 coverride \u2014 models a goal change, that is, how easily it would be to override the current belief with a new observation.\nIn our implementation, L is formed by 5 LSTM (Hochreiter and Schmidhuber, 1997) cells with tanh activation. We use a recurrent network for L since it can learn to output different values of the c parameters for different parts of the dialog (e.g., it is more likely that a new hypothesis will arise at the beginning of a dialog). This way, the recurrent network influences the rule-based component of the tracker. The function L uses the turn features ft, which encode information from the ASR, machine actions and the currently tracked slot.\nValue Dependent Model. The function G(ft, vi, vj) corrects the generic behavior of F . G is implemented as a multi-layer perceptron\nwith linear activations, that is: G(ft, vi, vj) = MLP (ft, fvi)|vj . The MLP uses turn features ft together with delexicalized features fvi for slot value vi. In our implementation the MLP computes a whole vector with values for each vk at once. However, in this notation we use just the value corresponding to vj . To stress this we use the restriction operator |vj .\n2.3 Spoken Language Understanding part\nThe SLU part of the tracker shown in Figure 2 is inspired by an architecture, proposed in Henderson et al. (2014b), consisting of two separate units. The first unit works with value-independent features fvi where slot values (like indian, italian, north, etc.) from the ontology are replaced by tags. This allows the unit to work with values that have not been seen during training.\nThe features are processed by a bidirectional LSTM B (with 10 tanh activated cells) which enables the model to compare the likelihoods of the values in the user utterance. Even though this is not a standard usage of the LSTM it has proved as crucial especially for estimating the None value which means that no value from the ontology was\nmentioned3. The other benefit of this architecture is that it can weight its output u1 according to how many ontology values have been detected during turn t.\nHowever, not all ontology values can be replaced by tags because of speech-recognition errors or simply because the ontology representation is not the same as the representation in natural language (e.g. dontcare~it does not matter). For this purpose, the model uses a second unit that maps untagged features directly into a value vector u2. Because of its architecture, the unit is able to work only with ontology values seen during training. At the end, outputs u1, u2 of the two units are summed together and turned into a probability distribution u via softmax. Since all parts of our model (R, F , G, SLU) are differentiable, all parameters of the model can be trained jointly by gradient-descent methods."}, {"heading": "3 Evaluation", "text": "Method. From each dialog in the dstc2 train data (1612 dialogs) we extracted training samples for the slots food, pricerange and area and used all of them to train each tracker. The development data dstc2 dev (506 dialogs) were used to select the ft and fv features. We took the 2000 most frequent ft features and the 100 most frequent fv features.\nThe cost that we optimized consists of a tracking cost, which is computed as a cross-entropy between a belief state hst and a goal annotation, and of an SLU cost, which is a cross-entropy between the output of the SLU ust and a semantic annotation. We did not use any regularization on model parameters. We trained the model for 30 epochs by SGD with the AdaDelta (Zeiler, 2012) weightupdate rule and batch size 16 on fully unrolled dialogs. We use the model from the best iteration according to error rate on dstc2 dev. The evaluated model was an ensemble of 10 best trackers (according to the tracking accuracy on dstc2 dev) selected from 62 trained trackers. All trackers used the same training settings with difference in initial parameter weights only). Our tracker did not track the name slot because there are no training data available for it. Therefore, we always set value for the name to None.\n3We also tested other models, such as max-pooling over feature embeddings (to get extra information for None value), however, these performed much worse on the validation dataset.\nResults. This section briefly summarizes results of our tracker on dstc2 test (1117 dialogs) in all DSTC2 categories as can be seen in Table 1. We also provide evaluation of the tracker without specific components to measure their contribution in the overall accuracy.\nIn the standard categories using Batch ASR and ASR features, we set new state-of-the-art results. In the category without ASR features (SLU only) our tracker is slightly behind the best tracker (Lee and Stent, 2016).\nFor completeness, we also evaluated our tracker in the \u201cnon-standard\u201d category that involves trackers using test data for validation. This setup was proposed in Henderson et al. (2014a) where an ensemble was trained from all DSTC2 submissions. However, this methodology discards a direct comparison with the other categories since it can overfit to test data. Our tracker in this category is a weighted4 averaging ensemble of trackers trained for the categories with ASR and batch ASR.\nWe also tested contribution of specialization components G and M by training new ensembles of models without those components. Accuracy of the ensembles can be seen in Table 1. From the results can be seen that removing either of the components hurts the performance in a similar way.\nIn the last part of evaluation we studied importance of the bidirectional LSTM layer B by ensembling models with linear layer instead. From the table we can see a significant drop in accuracy, showing the B is a crucial part of our model."}, {"heading": "4 Lessons learned", "text": "Originally we designed the special SLU unit M with a sigmoid activation inspired by architecture of (Henderson et al., 2014b). However, we found it difficult to train because gradients were propagated poorly through that layer causing its output to resemble priors of ontology values rather than probabilities of informing some ontology value based on corresponding ASR hypotheses as suggested by the network hierarchy. The problem resulted in an inability to learn alternative wordings of ontology values which are often present in the training data. One such example can be \u201casian food\u201d which appears 16 times in the training data as a part of the best ASR hypothesis while 13 times it really informs about \u201casian oriental\u201d ontology value. Measurements on dstc2 dev have shown\n4Validation was used for finding the weights only.\nthat the SLU was not able to recognize this alias anytime. We managed to solve this training issue by simplifying the special SLU sigmoid to linear activation instead. The resulting SLU is able to recognize common alternative wordings as \u201casian food\u201d appearing more than 10 times in training data, as well as rare alternatives like \u201canywhere\u201d (meaning area:dontcare) appearing only 5 times in training data."}, {"heading": "5 Conclusion", "text": "We have presented an end-to-end trainable belief tracker with modular architecture enhanced by differentiable rules. The modular architecture of our tracker outperforms other approaches in almost all standard DSTC categories without large modifications making our tracker successful in a wide\nrange of input-feature settings."}, {"heading": "Acknowledgments", "text": "This work was supported by GAUK grant 1170516 of Charles University in Prague."}], "references": [{"title": "The second dialog state tracking challenge", "author": ["Blaise Thomson", "Jason D. Williams"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dia-", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Knowledge-based dialog state tracking", "author": ["Kadlec et al.2014] Rudolf Kadlec", "Miroslav Vodolan", "Jindrich Libovicky", "Jan Macek", "Jan Kleindienst"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Kadlec et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2014}, {"title": "Task lineages: Dialog state tracking for flexible interaction", "author": ["Lee", "Stent2016] Sungjin Lee", "Amanda Stent"], "venue": "In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Optimizing generative dialog state tracker via cascading gradient descent", "author": ["Lee et al.2014] Byung-Jun Lee", "Woosang Lim", "Daejoong Kim", "Kee-Eung Kim"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dia-", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Neural belief tracker: Data-driven dialogue state tracking. arXiv preprint arXiv:1606.03777", "author": ["Mrk\u0161i\u0107 et al.2016] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "Markovian discriminative modeling for dialog state tracking", "author": ["Ren et al.2014] Hang Ren", "Weiqun Xu", "Yonghong Yan"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Ren et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2014}, {"title": "Comparative error analysis of dialog state tracking", "author": ["Ronnie Smith"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Smith.,? \\Q2014\\E", "shortCiteRegEx": "Smith.", "year": 2014}, {"title": "The sjtu system for dialog state tracking challenge", "author": ["Sun et al.2014] Kai Sun", "Lu Chen", "Su Zhu", "Kai Yu"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Hybrid dialog state tracker", "author": ["Rudolf Kadlec", "Jan Kleindienst"], "venue": "In Proceedings of NIPS 2015 Workshop on Machine Learning for Spoken Language Understanding and Interaction,", "citeRegEx": "Vodol\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vodol\u00e1n et al\\.", "year": 2015}, {"title": "The dialog state tracking challenge series: A review", "author": ["Antoine Raux", "Matthew Henderson"], "venue": "Dialogue & Discourse,", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Constrained Markov Bayesian Polynomial for Efficient Dialogue State Tracking", "author": ["Yu et al.2015] Kai Yu", "Kai Sun", "Lu Chen", "Su Zhu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Belief-state tracking was extensively studied in the Dialog State Tracking Challenge (DSTC) series (Williams et al., 2016) by providing shared testbed for various tracking approaches.", "startOffset": 99, "endOffset": 122}, {"referenceID": 0, "context": "In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodol\u00e1n et al.", "startOffset": 59, "endOffset": 261}, {"referenceID": 0, "context": "In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodol\u00e1n et al. (2015) with 2) a complex neural network based architecture that processes ASR features proposed by Henderson et al.", "startOffset": 59, "endOffset": 287}, {"referenceID": 0, "context": "In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodol\u00e1n et al. (2015) with 2) a complex neural network based architecture that processes ASR features proposed by Henderson et al. (2014b). Their network consist of two main units.", "startOffset": 59, "endOffset": 404}, {"referenceID": 0, "context": "When compared to Henderson et al. (2014b) that inspired our work: 1) our model does not require auto-encoder pre-training and shared initial training on all slots which makes the training easier; 2) our approach combines a rule-based core of the tracker and RNNs while their model used only RNNs; 3) we use different NN architecture to process SLU features.", "startOffset": 17, "endOffset": 42}, {"referenceID": 12, "context": "To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix).", "startOffset": 61, "endOffset": 77}, {"referenceID": 0, "context": "The same approach is used in Henderson et al. (2014b). To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix).", "startOffset": 29, "endOffset": 54}, {"referenceID": 0, "context": "The same approach is used in Henderson et al. (2014b). To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix). The batch ASR hypotheses are encoded in the same way as hypotheses from the regular ASR. The confusion matrix information is encoded as weighted unigrams. The last part of the turn features encodes machine-action dialog acts. We are using trigram-like encoding dialogact-slot-value with weight 1.0. The other features are value features fvi created from turn features, which contain occurrence of vi, by replacing occurrence of the value vi and slot name s by a common tag (inform-food-italian\u2192 inform-<slot>-<value>). This technique is called delexicalization by Henderson et al. (2014b).", "startOffset": 29, "endOffset": 827}, {"referenceID": 10, "context": "The rule-based part of our tracker, inspired by Vodol\u00e1n et al. (2015), is specified by a function R(ht\u22121, u s t , a) = h s t , which is a function of a slot\u2013 value probability distribution ht\u22121 in the previous turn, the output ut of a trainable SLU and of transition coefficients a which control how the new belief ht is computed.", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": "These coefficients were modelled by a so called durability function in Kadlec et al. (2014).", "startOffset": 71, "endOffset": 92}, {"referenceID": 10, "context": "The machine-learned part modulates behavior of the rule-based part R by transition coefficients avivj that control the amount of probability which is transferred from ht\u22121[vj ] to h s t [vi] as in Vodol\u00e1n et al. (2015). However, our computation of the coefficients involves two different functions:", "startOffset": 197, "endOffset": 219}, {"referenceID": 0, "context": "The SLU part of the tracker shown in Figure 2 is inspired by an architecture, proposed in Henderson et al. (2014b), consisting of two separate units.", "startOffset": 90, "endOffset": 115}, {"referenceID": 14, "context": "We trained the model for 30 epochs by SGD with the AdaDelta (Zeiler, 2012) weightupdate rule and batch size 16 on fully unrolled dialogs.", "startOffset": 60, "endOffset": 74}, {"referenceID": 0, "context": "This setup was proposed in Henderson et al. (2014a) where an ensemble was trained from all DSTC2 submissions.", "startOffset": 27, "endOffset": 52}, {"referenceID": 6, "context": "416 Neural Belief Tracker (Mrk\u0161i\u0107 et al., 2016) \u221a .", "startOffset": 26, "endOffset": 47}, {"referenceID": 3, "context": "406 Knowledge-based tracker (Kadlec et al., 2014) .", "startOffset": 28, "endOffset": 49}, {"referenceID": 0, "context": "721 Henderson et al. (2014b) .", "startOffset": 4, "endOffset": 29}, {"referenceID": 6, "context": "433 Smith (2014) .", "startOffset": 4, "endOffset": 17}, {"referenceID": 4, "context": "452 Lee et al. (2014) .", "startOffset": 4, "endOffset": 22}], "year": 2017, "abstractText": "This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slotfilling dialog systems. Our architecture is inspired by previously proposed neuralnetwork-based belief-tracking systems. In addition we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machinelearning based systems. For evaluation we used the Dialog State Tracking Challenge (DSTC) 2 dataset a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new stateof-the-art result in three out of four categories within the DSTC2.", "creator": "LaTeX with hyperref package"}}}